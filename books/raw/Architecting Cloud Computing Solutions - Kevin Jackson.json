{
  "metadata": {
    "title": "Architecting Cloud Computing Solutions - Kevin Jackson",
    "author": "achowdoori",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 367,
    "conversion_date": "2025-12-19T17:19:07.048075",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Architecting Cloud Computing Solutions - Kevin Jackson.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "[ 12 ]",
      "start_page": 26,
      "end_page": 55,
      "detection_method": "regex_chapter",
      "content": "What is Cloud Computing?\nChapter 1\n[ 13 ]\nThe following diagram depicts the various cloud computing phases:\nAs connectivity and services improved and competition continued to drive reliability up\nand costs down, the third age of computing began. The amount of data generated\nexploded. Wireless began to take off, leading to things such as widespread cell phone\nadoption and mobility solutions. Google revolutionized the internet searches by\nautomating it with MapReduce, no-SQL, and the AppEngine (an early Platform-as-a-\nService). During the 90s, the fast growth of wireless networks and the rapid adoption of\nmobile devices led to the rebirth of the thin client in the form of a browser. Better\nconnectivity and browser-based mobile application interfaces enabled much of the\ncomputing to remain on the server side with content and responses sent to the client's\nbrowser. Servers began utilizing a newer form of virtualization (IBM started a form of\nvirtualization on mainframes in 1964) improving resource utilization and changing the way\napplications are developed and deployed.\nToday, applications are loosely coupled architectures that can take advantage of modern\nelastic and scalable infrastructures. As mentioned earlier, virtualization has been around for\na while taking different forms. The real innovation came in the form of modernized billing\nsystems and economic models. The true innovation is that we can now purchase a fraction\nof a core or a fraction of a GB of RAM, consume it for a fraction of a minute and turn it off\nand not pay for it until we need it again. The real power of modern computing is to be able\nto buy what you need, when you need it, and give it back when finished. The model has led\nto entirely new business ideas, business strategies, operational models, economic models,\nand entirely new categories of business.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 14 ]\nTechnology innovations typically run in 10-year hills where we climb through a level of\nadoption and face significant challenges that level off adoption until innovation resolves\nthe current challenges, then we start to climb the next hill of adoption. The second hill is\nusually the largest increase in adoption with major game-changing innovation usually\noccurring in the third hill that restarts the clock. Situational awareness is critical for\nbusiness leaders. Which hill are you climbing? Are you early adopting with significant\nchallenges ahead? Is your company late to the game and about to be washed out by\ninnovation? Leaders must always offset risk with economics.\nLeaders face many challenges as they try to modernize their business and business models.\nFor example, an executive driven initiative to reduce costs through the adoption of cloud\nservices renders current traditional infrastructure-centric security models worthless. To\nremain relevant, enterprise security professionals must now adopt a modern data-centric\nsecurity model. Technical teams traditionally funded as an overhead cost must transform to\nbecome trusted revenue-enabling information technology partners to business leaders. To\nstay relevant, traditional technologists must now update, not only their technical skills but\nalso their non-technical skills. This updated approach includes business risk, economics,\nfinance, and strategy. Innovations such as cloud computing that are driven by economic\ninnovation are lasting and force everyone to adapt. Every known business model is affected\nby cloud computing: strategy, operations, security, economics, risk, deployment, and more.\nCloud computing, also known as IT-as-a-Service, was quickly adopted due to its ability to\ndeliver value in three significant market sectors.\nThe first, and most significant by revenue standards, is the software marketplace where it\nwas able to reduce software consumption costs, especially in the area of application and\nsoftware licensing, and reduced software application support costs. More importantly, this\nis accomplished while simultaneously improving business backend system capabilities.\nThe second marketplace was really in the application development arena. Application\ndevelopment platforms, also known as integrated development environments, were\ndelivered with the embedded support of multiple languages and frameworks. The\nPlatform-as-a-Service model exists in multiple technology environments, with greater\nflexibility. Environment flexibility opened up choice, reduced vendor lock-in fears, and also\ncreated an ability to auto-scale applications based on the number of actual users.\nThe third-most significant IT-as-a-Service marketplace was infrastructure. Infrastructure\nenabled global scale at an affordable price point. Here is where modern converged\nnetworks are used to deliver variable IT capacity pools. It also ushered in the concepts of\ninformation technology self-service, and on-demand capacity.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 15 ]\nAll these models delivered dramatic improvements in cost control, flexibility, speed to\nmarket, reliability, and resilience.\nThe following diagram depicts the various components of IT-as-a-Service:\nCloud computing definition\n\"Cloud computing is a model for enabling ubiquitous, convenient, on-demand network\naccess to a shared pool of configurable computing resources (for example, networks,\nservers, storage, applications, and services) that can be rapidly provisioned and released\nwith minimal management effort or service provider interaction.\"\na US National Institute of Standards and Technology\nThis definition is the most widely quoted and used version globally. Many countries and\nindustries have adopted it, and this is the highly recommended starting point for your\norganization's working definition for the cloud. This definition is so important that we\nshould take a few minutes to review it in detail.\nCloud computing is a model. It is not a specific technology. You cannot go and buy a cloud\ncomputer. The term is used to describe an economic and operational model for the\nprovisioning and consumption of IT infrastructure and associated services. The term can\nalso be extended to cover both business and public-sector mission models. What do these\nmodels enable? Why cloud?\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 16 ]\nThey enable ubiquitous, convenient, on-demand network access to a shared pool of\nconfigurable computing resources. Universal, convenient, on-demand network access\nmeans from anywhere and at any time. The network may include the global public internet,\nbut it may also refer to a global private network. The concepts of ubiquitous, convenient,\nand on-demand take the viewpoint of the cloud service provider's intended users. Shared\npool means that the individual user or organization does not pay for all the resources in the\npool. The end user pays only for what they use when they use it. This concept is the heart\nof the cloud computing economic model. If you need to pay for the resource even when\nyou are not using it, you are not leveraging the cloud computing economic model.\nConfigurable means that the service capability can be changed essentially in real time to\nmeet a specific user's requirements.\nThat final phrase, \"...can be rapidly provisioned and released with minimal management effort or\nservice provider interaction,\" implies a high degree of automation. Cloud service providers\noperate a highly automated, services-oriented platform that requires relatively few people.\nAutomation is enabled through brutal establishment and enforcement of rigorous IT\nstandards. Automation also enables self service, so if a prospective service provider cannot\noffer their capabilities without human interaction, you should be worried.\nEssential characteristics of cloud computing\nWhen the United States National Institute of Standards and Technology (NIST) published\nthe cloud computing definition, they also defined the essential characteristics of this new\nmodel. These have come to be more important than the definition in that the characteristics\nhave helped to define and protect the marketplace against all the marketing hype that has\naccompanied the cloud.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 17 ]\nThe first characteristic of cloud computing is that it is an on-demand, typically self service\nmodel. On-demand, meaning that it can be purchased when needed, for as long as needed,\nand given back when finished. Self service refers to the consumer's ability to buy, deploy\nand shut down services without any assistance from the service provider. This speeds up\nthe process controls cost and moves control to the consumer. (Refer back to earlier\nparagraphs where we discussed the de-centralization and continual innovation of pushing\ncompute and control closer to the edge of the consumer's control and consumer-controlled\ndevices. The same applies here.)\nFrom a security perspective, this has introduced governance challenges about the\nacquisition, provisioning, use, and operation of cloud-based services. Interestingly, these\nnew services may violate existing organizational policies. By its nature, cloud computing\nmay not require procurement, provisioning, or approval from finance due to its low initial\ncost, self-service nature, and immediate deployment options. Cloud infrastructure and\nservices can be provisioned by almost anyone with a credit card, also known as shadow IT.\nFor enterprise customers, this low-entry cost, quickly deployed on-demand model may\nbecome one of the most important characteristics as it instantly wreaks havoc on\ngovernance, security, long-term cost, strategy, internal politics, and collaboration.\nThe second characteristic, broad network access, is required. Ever heard the phrase\nthe network is the cloud? Anything referred to as-a-service requires a network connection.\nHow would it be accessed, managed, operated, or utilized without some network\nconnection, typically to the internet using standard protocols that promote use by disparate\nclient platforms? Because the cloud is an always-on and always-accessible offering, users\nhave immediate access to all available resources, and assets. Think convenient access to\nwhat you want, when you need it, from any location. In theory, all that is required is\ninternet access and relevant credentials. The mobile device and smart device revolution\nhave introduced an interesting dynamic into the cloud conversation within many\norganizations. These devices are often able to access relevant resources that users require;\nhowever, compatibility issues, ineffective security controls and non-standardization of\nplatforms and software systems have made the first adoption climb more difficult for some\nenterprises.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 18 ]\nThe third characteristic, resource pooling, is the characteristic that, in essence, lies at the\nheart of all that is good about cloud computing. Combining many smaller compute\nresources into farms or pools that can serve many consumers simultaneously enables\ndynamic resource allocation and re-allocation, cost predictability, IT resource control, and\nhigher rates of infrastructure utilization. Utilization and consumption patterns directly\naffect cost. Resource pooling enables different physical and virtual resources to be allocated\nand re-allocated according to consumer demand. As mentioned earlier, the true cloud\ninnovation was economic, allowing us to stop billing and give back the resource when\nfinished. More often than not, traditional, non-cloud traditional deployments see low-\nutilization rates for their resources, typically between 10 and 20%. Cloud deployments from\npools used across multiple clients or customer groups can see as high as 80 to 90%\nutilization (100% is not ideal in most cases). Resources can automatically scale and adjust to\ndynamic needs, workload or resource requirements. Cloud service providers or cloud\nsolution providers (CSPs) typically have scores of resources available, from hundreds to\nthousands of servers, network devices, and applications, enabling them to quickly and\neconomically accommodate, prioritize and implement the varied size, and complexities\neach client presents.\nThe fourth essential characteristic of cloud computing centers on elasticity, the ability to\ndynamically match the need. Product and service capabilities are developed, acquired,\npriced, and provisioned elastically, enabling rapid response to continuously changing user\ndemand. To the consumer, capabilities often appear unlimited and easily deployed in any\nquantity at any time. Because cloud services utilize a consumption-based pay-per-use\nmodel, you only pay for what you use. As mentioned earlier, cloud innovation and\nadoption are being driven mainly by economics that affects strategy. For cyclical loads,\napplications with intermittent use, seasonal or event-type business cloud eliminates the\nneed to pay for 100% of a physical server (CAPEX) when only 5% is used 2% of the time\n(OPEX). Think of selling thousands of tickets to an Olympic event. Leading up to the ticket\nrelease date, little to no computing resources are needed; however, when the tickets go on\nsale, they may need to accommodate 100,000 users in the space of 30 minutes-40 minutes.\nThis is where rapid elasticity and cloud computing can be beneficial. Enterprises no longer\nrequire traditional IT deployments with substantial capital expenditure up front (CAPEX)\nto support the temporary project load.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 19 ]\nThe final key characteristic mentioned here is that the cloud is a constantly measured\nservice. Cloud computing natively offers a unique and important component that\ntraditional IT deployments have struggled to providecmeasurement and control of\nresource consumption and utilization. As mentioned often, billing was the big innovation.\nCloud resource consumption needed to be measured and billed for accurately. Once that\nwas possible, the true power of the cloud, which included the ability to shut it off, was\nrealized. The capability enabled automated reporting, monitoring, and alerting which\nprovided much-needed transparency between the provider and the client. Like a metered\nelectricity service or cell phone data usage, consumers have transparent and immediate\naccess to usage data enabling immediate behavior change if needed. Itemized billing\nprovides transparent trendable data providing insight that may lead to needed change.\nProactive organizations can now utilize this well measured, transparent, granular,\ntrendable data to charge departments or business units for their actual consumption. IT,\nproduct development, and finance can now move toward operating collaboratively as a\nrevenue-driving team that can quantify, qualify, and justify exact usage and costs per\ndepartment, by business function, per leader, and so oncsomething that was incredibly\ndifficult to achieve in traditional IT environments.\nThe following diagram is a graphical representation of the five essential characteristics of\ncloud computing:\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 20 ]\nAs a side note: people have been utilizing the cloud for years without realizing it. It is not a\nnew thing, but it has just started to port over into more popular arenas. Let's look at\ninternet access as an example.\nCharacteristic 1: Based on the first characteristic mentioned earlier, how many\npeople dig up the street to put in connectivity when they want to access the\ninternet? None. We pay for it as a service that allows us to use it when we\nchoose.\nCharacteristic 2: Do we need a network to access the internet? Of course.\nCharacteristic 3: Do we have dedicated switches, routers, SONET ring, and so on\nin our living space? No, those resources are pooled by the service provider and\nshared with all the clients in the area or region.\nCharacteristic 4: Can we utilize more if we need it? Absolutely. We only use\nwhat we need with the ability to scale all the way up to the maximum\nperformance for which we are willing to pay. If more is needed, we call and\nchange what performance level we pay for to match up with the changing need.\nCharacteristic 5: Are we paying as we go? Is our service metered and measured?\nDefinitely yes. If we choose to stop paying for the service, the service is shut off.\nWe get a bill every month and often have a portal that we can log in to that\ndetails what we pay for, what we use, performance details, uptime, downtime,\nand so on.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 21 ]\nCloud computing operational models\nThere are many paths to the cloud. Each path is grouped based on how the services are\noffered, deployed, and consumed. The cloud is not a technology. A cloud layer does not\nexist. Each path to the cloud is a response to a requirement or set of needs based on the\nconsumer's current situation, desired future state, available skills, and resources, as well as\ntolerance for risk. Cloud products and services often establish reusable and reoccurring\narchitectural patterns (building blocks) used for designing, building, and managing\napplications and infrastructure.\nThere are primarily three cloud service models: Internet-as-a-Service (IaaS), Platform-as-a-\nService (PaaS), and Software-as-a-Service (SaaS). Deployed as needed, all three models\nrequire network connections to change resource pools that are measured in great detail,\ndynamically. However, each consumption model differs in its approach to a technical\nsolution, economics, complexity risk, and level of acceleration. Deployment models also\ndiffer in that they could be public/shared, private/dedicated, community, and hybrid. Each\nmodel is unique in how it addresses organizational risk tolerance, economic models, and\nmanagement preferences.\nOften the motivator for a move to the cloud is some event that triggers probing questions.\nEvents could be anything from a magazine article, a blog post to a security breach,\ninfrastructure downtime, a complaint about responsiveness, difficulty managing to the\ndesired level of service, or staff/leadership change. Questions can be typically reduced\ndown to three Es: Expectations, Economics, and Execution. As an example, someone\nexpects more delivered work with smaller budgets and less time, or project execution\nunexpectedly fails due to budget and staff constraints.\nAs questions get asked, and solutions considered, strategy details, economics, and\ntechnology must align. Solutions that are technically perfect may be too expensive. Low-\ncost solutions may not match up to chosen strategies going forward. In all cases, economics\nneed to balance or offset chosen risk level. For example, very inexpensive self-managed\npublic cloud servers may not match up to the desired level of isolation and security\nrequired for transactional database servers.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 22 ]\nNext we discuss ways to think through the three primary models available today. How do\nwe recognize situations in which a cloud model should be a consideration? What are the\ncharacteristics of each model? What are the benefits? The following diagram is an overview\nof the three main service models:\nCloud service models\nThe different cloud service models are described here in the following sections.\nIaaS ` background\nAcross the industry, hardware has been largely ignored for a very long time. Servers were\nnot sexy. There was no glory for servers. Servers were just a support for the more important\napplications. Applications got all the credit for solving business challenges. Applications\nwere the things that users interacted with directly. Servers got stuck in dark closets,\nforgotten and neglected until a problem occurred.\nBecause servers received no glory, very little to no maintenance and no budget for\npatching, upgrading, and so on, many servers are now well beyond their service life and\nprone to failure. Incredible amounts of money will be spent over the next several years\nrewriting applications, developing new applications, migrating legacy applications, and\nupdating to new cloud-ready functions needed to replace old applications and neglected\nhardware currently stuck in old closets and worn out in-house data centers.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 23 ]\nIaaS gave many the opportunity to upgrade, refresh infrastructure, and move from\nexcessive capital spending to a monthly pay-as-you-go incremental spend. The shift enables\nstrategy changes, go-to-market changes, differences in software development, and changes\nin handling IT workloads. Think of what hundreds of servers can do in one hour versus one\nserver for hundreds of hours.\nIaaS ` things to consider\nIaaS is often deployed on-demand in small increments (cores, RAM, storage, network) with\nbilling occurring in small increments of time. Instead of spending the capital (CAPEX) for a\nlarge four or eight-core server (which is the smallest currently available from some\nmanufacturers), a right-sized virtual server can be acquired and deployed as a service,\nmatching infrastructure size to cost and immediate need. This flexibility allows for\ninfrastructure to be quickly matched to business strategies and economic constraints.\nIaaS can include many of the infrastructure components included in traditional\ndeployments. Firewalls can be virtual or physical. Compute and storage can be deployed\nacross many different styles and platforms. Each service provider has their unique mix of\ntechnology and services. Ultimately the goal when using IaaS is to forget about\ninfrastructure management and details and acquire the mix of services needed, when\nneeded, solving the requirements at that time.\nIaaS  was one of the first cloud models available and has seen significant adoption in nearly\nall sectors. With IaaS, the user does not manage or control the infrastructure directly, only\nthe software and functions loaded onto it (that is operating system, application). There are\ndifferent types to choose from with varying levels of management and monitoring available\nfor the underlying hardware, virtualization layers, firewalls, SANs, switches, routers,\nnetwork interface cards (NIC), and related service level agreements (SLAs). The \ncontrolled risk and lower economic entry points for IaaS are attractive to new cloud\nadopters as well as savvy veterans. Controllable cost and controllable risk provide extra\nincentive to those trying to modernize through the adoption of the cloud.\nThe delivery of on-demand capacity is typically handled via self-service online customer\nportals. The portal provides complete visibility and control of the IaaS environment. Self-\nservice portals enable and automate functions for adds, moves, changes, managing, and\nreporting without engaging and waiting for other resources internally or within the\nprovider. IaaS can have many different consumption models matching various OPEX and\nCAPEX requirements. When using IaaS, there is no need to invest capital up front based on\ncompute and storage resources forecasts. IaaS enables infrastructure to be purchased in\nincrements, as needed, to match utilization.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 24 ]\nFor organizations, IaaS usage metering provides a higher level of detail used to trend\nutilization and chargeback specific departments or functions based on actual utilization.\nDetailed measurements and reporting also allow for instant, and in some cases, automatic,\nscaling up, and down based on dynamic need requirements. Resource flexibility is\nparticularly useful when there are significant spikes, dips, or cyclical loads for\ninfrastructure.\nA few examples of current IaaS providers are Amazon Web Services, Microsoft Azure, and\nGoogle. They offer many different styles of compute, storage, and network, as well as many\nsupporting solution services. They each offer several different economic models to match\nup to SLAs, OPEX and CAPEX requirements, risk and deployment options.\nSaaS ` background\nAs many small and medium-sized organizations looked for additional ways to control cost,\nmodernize strategy, and consume on-demand solutions, software licensing became a very\ncomplicated issue. An example of this is Oracle, a company that was a bit late to the cloud\nlicensing game. New server configurations were much more substantial with more sockets,\nmore cores, and more RAM. Even with no change in utilization or software configuration,\nOracle client charges increased to over a million dollars due to new server sizing. This\naffected strategy, economics, and eventually technical decisions on how to move forward in\nthe face of shattered budgets and ROI calculations.\nMany organizations lack the skills or resources to create custom software applications.\nFreeware and opensource software helped some organizations, but they still required skills\nsets and significant adjustment for adoption. Software providers are starting looking for\nways to offer online cloud-based solutions at lower cost and universal access. These new\nmodels would center around new licensing models tied to multiple users, a certain level of\naccess, and SLAs rather than the size of the software infrastructure deployment.\nWith SaaS, the subscriber uses the provider's centralized application deployed using cloud\ninfrastructure. SaaS enables access from any approved client device, browser, or custom\ninterface. The user/subscriber does not have access to the underlying infrastructure,\napplication code, or individual application attributes except for a set of named user-specific\napplication configuration settings.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 25 ]\nIn the SaaS space, some applications have stabilized/normalized meaning they are widely\nadopted and have significant competition and innovation driving licensing costs\ndownward, for example, office suites, collaboration software, and communications\nsoftware. Software-as-a-Service providers offer a complete software application to\ncustomers using a license-based model that accesses the application on-demand via a self-\nservice interface.\nSaaS ` things to consider\nUsing SaaS, organizations have potentially limitless possibilities for running applications\nthat may not have been otherwise possible given the limitations of their corporate systems,\ninfrastructure, or resources. If the right middleware and associated components are\ndeployed, SaaS can present massive incentives and benefits. Organizations can quickly\nrealize benefits from scalability, flexibility, and on-demand self-service capabilities.\nCustomer adoption accelerates as access to data and applications can be from virtually\nanywhere, at any time with internet access. Additional benefits include:\nCost control, cost reduction\nLicensing or support becomes a built-in component for the provider and the\nsubscriber benefits from economies of scale\nThe purchasing of up-front bulk licensing and the associated capital expenditure\nis removed and replaced by demand-based pay-as-you-go licensing models\nUser-based internal support requirements reduce significantly as the software\ncloud service provider can typically handle more of the support at scale\nEase of use and limited administration\nAutomatic updates and patch management\nImproved security\nStandardization and compatibility \nGlobal accessibility\nNotable providers of SaaS include Google, Microsoft, Oracle, Salesforce, and SAP.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 26 ]\nPaaS ` background\nPaaS takes both IaaS and SaaS and adds yet another twist on trying to solve the problem.\nAs described earlier, people are trying to control costs, eliminate large major cash outlays,\naccelerate, modernize strategies, and move to only paying for what is needed, when it is\nneeded. IaaS helped but still required a lot of people, skills, and money to support the\napplications. Based on our direct research, software required between 8x and 32x the\nannual cost of the server annually in management, maintenance, monitoring, and support.\nA $6,000 server written down over a 3-year use cycle would cost between $16,000 and\n$64,000 each year for software support. The cost was dependent on the specific software\nand organizational efficiency. These operational changes meant that new infrastructure and\nsoftware models were required to keep businesses innovating and moving forward.\nThe next challenge was that the as-a-service model was not available with every software\npackage. Some software was just not adaptable to modern cloud models. A complicating\nissue was that, for most companies, only about 15%-20% of software was off-the-shelf. Most\nof it was custom developed, homegrown, and built for specific functions and purposes\nwithin each business. Nearly every company still had to develop proprietary applications,\nmiddleware, services, connectors, workflows, and more. Each of those projects required\ndifferent programming languages with different frameworks and libraries. How can things\naccelerate? How can costs be controlled?\nInterestingly, people realized that the combinations of languages, libraries, and frameworks\nused were often the same or very similar. Consumers needed the ability to quickly build\nand deploy applications coded with provider-supported programming languages, services,\nlibraries, and tools. The end user did not want to manage or control the underlying cloud\ninfrastructure, but they did need to control the deployed application's configuration\nsettings. CSPs responded by integrating all of the needed components and subsystems into\na solution stack that could now be offered as a service or rented as-needed. This new PaaS\nenabled faster development at a lower cost. The environment was now ready to use,\nmanaged, and monitored, enabling developers to be productive immediately using the\nlatest components available. This has led to many other variations where raw materials are\nintegrated into environments that enable the building and assembling of new creations\nquickly and cost-effectively.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 27 ]\nPaaS ` things to consider\nCloud PaaS has revolutionized software development and the means through which it is\ndelivered to customers and users. Market entry barriers have been reduced dramatically by\nlower cost, accelerating time to market, and promoting innovative cultures within many\norganizations.\nAs PaaS providers are considered, the languages and frameworks supported are key. A\nprovider that supports multiple relevant languages and frameworks can help avoid\nproductivity pitfalls later. Developers need to write code in their preferred language that\nmeets specified design requirements. Recent advances include options for open source\ndevelopment stacks and many new infrastructure deployment styles including OpenStack\ninfrastructure, various containerization engines, and serverless (FaaS) options. PaaS\nproviders that support multiple languages and deployment options reduce vendor lock-in\nand interoperability issues as applications grow and deployment locations change.\nApplications are never static. They are continually changing, updating, and growing. Being\nable to deploy and move the application across different hosting environments is also a key\nPaaS benefit. Supporting multiple hosting environments helps the developer or\nadministrator easily migrate the application if required. With this option, PaaS can also be\nused for contingency operations and business continuity to ensure continued availability. It\nis important to consider the final environment as platforms used by early users to test for\nfunctionality. These environments transition to a run environment at some point. The final\nenvironment may not be the same as originally intended as many things change\nthroughout the process and testing. Multiple deployment options are an important\nconsideration when picking a platform provider.\nMany platform providers started with the idea of adding value by assembling platforms.\nMake the platform proprietary with their unique workflows, combinations, components\nand create a sort of lock-in mentality. Providers wanted clients to use only their specific\nplatform and direction. The goal was to make things very sticky with limited ability to\ntransition between provider platforms. Recent changes have added much-needed flexibility\nmatching developer needs and requirements. To stay relevant, platform providers needed\nto respond or lose the developers and their communities to more flexible environments and\nopen source options.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 28 ]\nThe application programming interfaces (APIs) are required for nearly every form of\nsoftware in our space today with RESTful being elevated to the de-facto standard in most\ncases. A service provider always offers specified APIs or integration. Developers could run\ntheir application in various environments based on common and standard API structures.\nThis ensured consistency and quality for customers and users. PaaS pushed forward\ninfrastructure concepts like auto-scaling where software could now take the responsibility\nof scale up and scale down and manipulating the infrastructure through APIs, as needed.\nThis would help accommodate cyclical, less predictable demand patterns, seasonal\nbusiness, and event-driven activities. Mother's Day would bring down Hallmark's online\ncard servers every year until they were able to implement auto-scaling. Before auto-scaling,\nHallmark would have to build and engineer infrastructure for a guesstimated utilization\nlevel. With auto-scaling, the platform allocates resources and assigns them these\napplications, as required. This capability is a key driver for any seasonal organizations that\nexperience spikes and drops in usage.\nWhen thinking through platform providers, look for flexibility and future migration\noptions. Look for the right combinations of services and support with expertise in areas\nrelevant to project needs and direction. Look for providers that not only provide the\nplatform but also offer the other versions of the cloud as well. Where your project starts is\nnot where it stays. Plan to move and change. It may not change often, but change happens.\nPlan for it up front.\nNotable PaaS providers include Microsoft, Lightning, and Google.\nOther cloud service models\nYou have probably heard of many other X-as-a-Service offerings such as Storage-as-a-\nService, Desktop-as-a-Service, Network-as-a-Service, Backend-as-a-Service, Function-as-a-\nService. These other models are merely subsets or aggregations of SaaS, IaaS, or PaaS.\nCategorizing them into the three standard models simplifies any cloud conversation you\nmay have.\nCloud deployment models\nWe have discussed the three standard cloud service models. The service model defines the\nwhat. What is unique about each model? What are they trying to solve? What are their\nstrengths and weaknesses? Each of the discussed service models adheres to the five\ncharacteristics mentioned, possibly adhering in different ways. Within each of the service\nmodels, there may also be multiple ways to enable and deploy the service. The service\nmodel is the what, the deployment model is the how.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 29 ]\nMany cloud services are straightforward to comprehend. For example, network as\nmentioned earlier. Most do not realize that the network was one of the very first types of\nIaaS, therefore, one of the original types of cloud. True, there are many types of services,\nand with that, an even greater number of ways to refer to them. In this section, we\nintroduce the deployment models and some of the jargon. The discussion also addresses\nhow the different deployment types are referenced, marketing names, labels, and currently\nused buzz words.\nThis section also demystifies some of the marketing hype which makes it easier to quiet the\nnoise when participating in the often jargon-filled conversations. How is this bare metal\ndifferent to a dedicated cloud? What is a public cloud? What is a private cloud and how is\nprivate different to dedicated? Is it different? Is private on-premises still considered as the\ncloud? Is a private cloud from a service provider also the cloud? If it is called a cloud, it\nmust be a cloud?\nPublic\nPublic is the typical IaaS-compute deployment model most people think of when referring\nto the cloud. A public cloud service provider offers IT resources as-a-service and, as part of\nthe service, is responsible for building, monitoring, and maintaining physical data centers\nand IT resources that are for dynamic public consumption. This IT service environment is\nshared among many customers which normally reduces costs for each customer. By\nleveraging economies of scale, the CSP enables higher average utilization of resources\nthrough the extensive use of virtualization, workload binding, offsetting clients workload\npatterns, and performance tiers.\nThe general public uses a public cloud infrastructure. The infrastructure may be owned,\nmanaged, and operated by a business, academic, or government organization, or some\ncombination. The infrastructure is always on service provider premises as they have taken\nownership of operations and maintenance. Amazon is a good example. The business\nstarted off by selling books. It then started to sell excess server and storage capacity to the\ngeneral public. The infrastructure remained on-premise at Amazon locations.\nA public cloud can fall into two sub-types within IaaS, self managed or fully managed. Both\nof these sub-types are discussed in greater detail later in this chapter. A public cloud is\nhighly scalable, immediately deployed, portal driven, and can be parked or turned off\nwhen not in use.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 30 ]\nPublic cloud benefits include:\nEase of use and inexpensive setup, low cost of entry to the cloud\nStreamlined and easy-to-provision resources via a self-serve portal\nScaled to meet customer needs \nNo wasted resources because customers pay only for what they consume\nBasic security services included\nPublic cloud considerations are:\nHow are noisy neighbors handled?\nDoes security line up with my requirements?\nIs there any network access or storage limitations?\nCost of access or data transfer in/out?\nPortability? Grow into other instance types and service types?\nWhat other services connect to it?\nWhat is the price/performance metric?\nProviders often mentioned in this space include Amazon, Microsoft, and Google, among\nothers.\nPrivate and dedicated\nThere are a lot of marketing, jargon and buzz words that come along with the cloud.\nCompanies are fighting hard to create separation from the pack. Sounding unique and\ndifferent was one way to try and differentiate. What is the difference between a dedicated\nand a private cloud? What is a virtual private cloud? Is a virtual private cloud different to a\nprivate cloud? Do these different versions adhere to the five characteristics of the cloud?\nMany factors drive interest in single-tenant infrastructure. Dedicated and private both, by\ndefinition, are single-tenant environments with the infrastructure only accessible by a\nsingle company or client. The difference ultimately comes down to economic model and\naccess.\n1SJWBUF\u0002DMPVE\nA private cloud is typically an on-client premise solution with the infrastructure leased or\npurchased by the company/entity using it. These environments can also be deployed within\na service provider data center utilizing collocation services. This would still be considered\nan on-premises solution as the collocation space is just another leased location for the\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 31 ]\ninfrastructure owner. A private cloud is typically managed by the organization it serves;\nhowever, outsourcing the general management of this to trusted third parties may also be\nan option. A private cloud is typically available only to the entity or organization, its\nemployees, contractors, and selected third parties. The private cloud is also sometimes\nreferred to as the internal or organizational cloud.\nThe factors driving the use of infrastructure may include legal limitations, trust, and\nsecurity regulations. Private cloud benefits include more control over data, the underlying\nsystems, and applications, ownership, retention of governance controls; and assurance over\ndata location. Private clouds are typically more popular among large, complex\norganizations that have legacy systems and heavily customized environments.\nAdditionally, where significant technology investment has been made, it may be more\nfinancially viable to utilize and incorporate these investments within a private cloud\nenvironment than to discard or retire such devices.\nIs a private cloud really a cloud? It has cloud in the name. Having cloud in the name was\nnot one of the five characteristics of the cloud. It is interesting to debate, you decide.\nCompare a private cloud to the five characteristics and come up with an answer.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 32 ]\n%FEJDBUFE\u0002DMPVE\nA dedicated cloud is very similar to a private cloud. It is also a single-tenant solution.\nOwnership and access differ. In a dedicated cloud, ownership of the infrastructure shifts to\nthe service provider. The infrastructure is housed within the provider's data center. A\ndedicated environment is for use by the single tenant. Network, compute, and storage are\ndedicated to the single tenant.\nThe economic model for dedicated solutions is usually a combination of non-recurring cost\n(NRC) which is a one time fee up front, and monthly recurring cost (MRC), which is a\nmonthly payment paid over a term (number of months or years). Dedicated solutions\nenable a shift from all capital upfront models (CAPEX) to smaller payments over a longer\nperiod (OpEx). Management and operations can continue to be in-house, outsourced, or a\ncombination of both.\nIs a dedicated cloud really a cloud service? It also has cloud in the name. This is also an\ninteresting one to debate, you decide. Compare a private cloud to the five characteristics\nand come up with an answer for your conversations. When sorting out the answer, look at\nthe economics. The cloud is an economic innovation. Ultimately, can you turn it off and\ngive it back? Does billing stop? Do you stop paying for it when you shutdown?\n7JSUVBM\u0002QSJWBUF\u0002DMPVE\nAs with many things in our industry, the lines often get blurred (marketing may have\nsomething to do with it). Dedicated is an isolated environment deployed within the\nprovider's data center. A virtual private cloud (VPC) is a variation of a dedicated cloud. A \nvirtual private cloud combines concepts from a public and a dedicated cloud. VPC takes the\nconcept of shared infrastructure and the economies of scale for servers and storage then\ncombines it with an isolated network.\nThe very high cost of a dedicated cloud and the network challenges of noisy neighbors in a\npublic cloud led service providers to the virtual private approach. VPC is the blending of\nstrengths while trying to control some of the weaknesses. The name drives the blurred line\nin this service. A private cloud is typically deployed on client-owned infrastructure as an\non-client-premise solution. A VPC is deployed on the service provider-owned\ninfrastructure. This service would be more appropriately named a virtual dedicated cloud.\nSome providers have changed the name to eliminate some of the confusion within their\nproduct and solution sets.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 33 ]\n$PNNVOJUZ\nIn a community cloud, an IT infrastructure is provisioned for use by a specified community\nof end users. The community participants are from organizations that have shared concerns\nand governance requirements. Typically, these requirements link to mission, security,\npolicy, or regulatory compliance. It can be owned, managed, or operated by a community\nmember, a third party, or a combination. Community clouds may exist on or off premises.\nA community cloud provides most of the same benefits as a public cloud deployment while\nproviding heightened levels of privacy, security, and regulatory compliance.\n)ZCSJE\nA hybrid cloud is any solution that combines a cloud model with any other cloud or non-\ncloud deployment model. Most organizations tend to gravitate to the hybrid models as no\nsingle model or deployment type matches up to the multiple applications and services\nneeded to support a business. Many applications cannot migrate readily to new services.\nApplication dependencies may introduce additional risk for migrated applications. There is\nrarely a situation where everything is forklift moved all at one time from current state to\nfuture state. Hybrid is the typical path to the cloud for currently deployed applications and\ninfrastructure.\nIn a hybrid IT environment, private clouds, public clouds, community clouds, traditional\ndata centers, and services from service providers can be integrated and interconnected.\nApplications and services can then be deployed to and consumed from the most\nappropriate combination of services and environments.\nKey benefits of the hybrid model are retention of ownership and oversight for critical tasks,\nreuse of earlier technology investments, tighter control over critical business components\nand systems and more cost-effective options for non-critical business functions. Cloud\nbursting and disaster recovery options can also be enhanced by using hybrid cloud\ndeployments.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 34 ]\nThe basic understanding of the cloud deployment models are shown here:\nOther delivery models\nThe cloud, and technology in general, has a notion of fashion to it. Some things are in vogue\nwhile others fade from favor quickly. As cloud conversations happen, it is vital to see ahead\nof the curve and keep in mind overall direction. For decades, a consistent direction has been\nto place more power in the hands of the end user/consumer. Our cell phones today have\nmore compute power and run more applications than many desktop computers sold a few\nyears ago. Think forward a bit to IoT. Significant compute power and data are at our\nfingertips. Connected cars are currently built with nearly 40 processors, almost 100 sensors\nsending out 25 GB of data per hour. Connected cars are described as rolling data centers.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 35 ]\nAs technology continues to innovate and progress at incredible speeds, it is essential to\nraise your awareness of immerging trends. It is also important to quickly distinguish tech\nfashion from tech innovation. Real innovation always has an economic driver that is\nsustainable. Starbucks coffee did not invent coffee. Starbucks was when the first-time coffee\nand culture became inseparable. The iPhone was not the first mobile device; however, it\nwas the first time a mobile device connected many of the most important facets of our\nhome, work, and play lives. The cloud was not the first deployment of virtualization. The\ncloud is the first innovation that has directly connected the concepts of technology,\nstrategy, and economics forever changing the way we build, deploy and consume\ntechnology and services.\nThe shortlist here includes some very innovative ideas that build on and extend the core\nconcepts of cloud computing. These are a few things to watch as they climb the two hills of\ninnovation. More on that later in this book:\nGrid computing: Distributed and parallel computing capability where a virtual\ncomputer is composed of a cluster of networked and loosely coupled individual\ncomputers which act in concert to perform very large tasks.\nFog computing: A distributed computing model that provides IT services closer\nto the fog client. Sources are near-end user edge devices. Fog computing can\nhandle data at the network level, on smart devices, and the end-user client side\ninstead of sending data to a remote location for processing.\nDew computing: Dew computing is positioned at the ground level for the cloud\nand fog computing. When compared to fog computing, which is designed to\nsupport IoT applications that are sensitive to network latency and require real-\ntime and dynamic network reconfigurability, this variant pushes the computing\napplications, data, and low-level services to the end users and away from\ncentralized virtual nodes.\nEdge computing: Edge computing extends cloud computing by pushing the\nprocessing, applications, and data as far away from centralized resources as\npossible. Moving work to the edge also means that devices may not always be\nconnected to the internet (that is, mobile, laptop, tablet). This means that\nprocessing would also need a high level of redundancy with content highly\ndistributed.\nIoT: IoT is connecting the physical world with the logical one. Sensing and\nprocessing technology are being placed where needed when needed. Cloud \ncomputing is rapidly morphing to help catch, process, and utilize the vast\namount of data already being generated by IoT initiatives and projects.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 36 ]\nAI, neural networks, and machine learning: These concepts have been pulled\ntogether because they are hard to separate and sometimes they are used \ninterchangeably even though there are distinct differences. AI has been around\nfor decades, it is not new. However, cloud computing has removed many of the\nbarriers that were slowing innovation; 300 computers for one hour can process a\nstaggering amount of data, connect it, correlate it, learn from it and apply it.\nCloud computing innovation has truly helped this field leap forward recently.\nCloud washing\nCloud washing is a term used to refer to the often deceptive attempt to rebrand an existing\nproduct or service by associating the buzzword cloud with it. Within a financial construct,\nthere is a parallel that describes the practice of inflating financial results for a company's\ncloud business by redefining existing services and products as cloud services. A typical\nexample of this is referring to access to any application or service over the internet through\na browser as cloud computing, just because you are receiving the service over the internet.\nAnother example is the traditional application service provider (ASP) model where a third\nparty offers individuals, and companies access over the internet to applications and\nservices that would normally have been located in their own personal or enterprise\ncomputers. This is often marketed as a SaaS, but there are many significant differences\nbetween the two models. ASP is a software delivery method with a revenue model that is\ndisconnected from the software itself. At its core, these are single-instance, single-tenant\nlegacy software deployments. The revenue model is like renting a server with an\napplication installed on it. This approach failed in the marketplace because it lacks\nscalability for the vendor, too much customization is required, and there is a single\ncustomer for the instantiation. There is also no organic aggregation of data, and no network\neffect data available for collection and aggregation.\nSaaS, on the other hand, is an all-inclusive business architecture that is a value delivery\nmethod. Its built-in multi-tenancy design allows for shared resources and shared\ninfrastructure. SaaS is scalable and offers true economies of scale to the service provider.\nThis approach reduces overall costs, operational complexities, and customization. Multi-\ntenancy can also be leveraged to improve customer service and retention, reduce sales\ncycles, accelerate revenue, gain competitive advantage, and even directly monetize\nadditional services.\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 37 ]\nManaged service arrangements are also sometimes referred to as cloud hosting. The\ndifference here is that the day-to-day functions are outsourced to a particular vendor to\nrealize an increase in efficiency around processes associated with data center operations.\nWhen doing this, the client also pays for all the capital investment (either up front or\nembedded in the recurring fee) and commits to regular payments over a minimum term.\nThese payments are not driven by use but are a calculation related to total operational and\ncustomization costs over the minimum term, financial interest rates and a minimum profit\nfor the service provider. In all cloud service models, the cloud service provider bears all\ncapital cost and offers the same standard service to all marketplace customer. Payment is\nrelated directly to actual customer use, and there is no minimum term commitment.\nCloud computing taxonomy\nThe cloud computing taxonomy was initially developed by the United States National\nInstitute of Standards and Technology (NIST) as a tool for standardizing conversations\naround cloud architectures. Since then, this basic model has been enhanced by the\ncommunity and broadly adopted to discuss basic concepts. The major taxonomy\ncomponents are described here:\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 38 ]\nThe service consumer is the entity (enterprise or end user) that actually uses the cloud\nservice. Users will normally have multiple programming interfaces. These interfaces\npresent themselves like any normal application and the user does not need to understand\nany cloud computing platform details. User interfaces can also provide administrative\nfunctions like virtual machine or storage management.\nThe cloud service provider (CSP) creates, manages, and delivers information technology\nservices to the service consumer. Provider tasks vary based on the service model:\nFor SaaS, the provider installs, manages, and maintains all software. Service\nconsumers only have access to the application.\nFor PaaS, the provider manages and provides a standardized application\ndevelopment environment. This is typically in the form of a development\nlanguage framework.\nFor IaaS, the provider maintains and operates the facilities, hardware, virtual\nmachines, storage, and network associated with the delivery of any information\ntechnology service. The service consumer, however, is responsible for service\ndesign, operations, and delivery.\nCritical to the service provider's operations is the management layer. This layer meters and\nmonitors the use of all services. It also provisions and deprovisions services based on user\ndemand and service provider capacity. Management also includes billing, capacity\nplanning, SLA management, and reporting. Security is applied across all aspects of the\nservice provider's operations.\nThe service developer creates, publishes, and monitors cloud services. Typically, these\nconsist of line-of-business applications delivered directly to end users. During service\ncreation, analytics is used for remote debugging and service testing. When the service is\npublished, analytics is also used to monitor service performance.\nStandards and taxonomies will affect cloud use case scenarios in four different ways:\nWithin each type of cloud service\nAcross the different types of cloud services\nBetween the enterprise and the cloud \nWithin the private cloud of an enterprise\n\n\nWhat is Cloud Computing?\nChapter 1\n[ 39 ]\nWithin each type of cloud service (IaaS, PaaS, or SaaS), open standards help organizations\navoid vendor lock-in by giving users the freedom to move to other cloud service providers\nwithout major application or operational modifications. Standards within an enterprise are\nnormally driven by interoperability, auditability, security, and management requirements.\nSummary\nAs you move forward in cloud conversations, please keep in mind the five characteristics of\nthe cloud. These characteristics will help you stay focused on aligning technology,\neconomics, and strategy. The cloud's big innovation was economic, not technical. Strategy\nchanging economics are driving rapid transformation and digitization. There are many\ndifferent services, different economic and deployment models along with many different\nstrategies to use them. The cloud is another tool in the toolbox. It is not the answer for\neverything, but it is quickly becoming the foundation for everything.\nThe next chapter starts the conversation on governance and change management. You may\nthink that a cloud solutions architect's success depends on the technology chosen. That\nthinking couldn't be further from the truth. While cloud computing is foundational to\ndigital transformation, successful cloud computing solutions must be built on top of an\neffective change management and IT governance foundation.\n\n\n2\nGovernance and Change\nManagement\nSolution adoption requires cultural change. Cultural change requires relevant, insightful\ndata; well-planned governance, and relentless change management. Change is difficult in\nmany ways and for many reasons. Without governance and change management, driving\nadoption for new solutions can be exhausting, unpopular, expensive, and slow.\nGovernance and change management are inseparable but not interchangeable. One is not\nsynonymous with the other. Governance and change management are tightly coupled and\ndependent on one another. Changes in one can certainly affect the other. What are they and\nhow are they different?\nGovernance address the things that need to be accounted for as change is implemented.\nGovernance is the operating agreement for how changes will take place for the things\ninvolved. Governance definitions and operating rules may define organizational structures,\ndecision rights, workflows, processes, stakeholders, authorization points, and toll gates.\nThe goal of governance planning is to talk through how things operate today versus how\nthings will operate during and after the change. Well-planned governance ideally creates a\ntarget workflow that aligns and optimizes the use of business entity resources with the\ngoals and objectives of the business.\nChange management focuses on the people and how people will be assisted through\nchanges being implemented. Changes to our norms can be real challenges. It is difficult to\nveer away from comfortable modes of normal operation, accepted and adopted workflows,\nteam structures, roles and responsibilities, and known rules of engagement. Change\nmanagement helps people transition through changes by providing things such as\nmessaging and communication plans, engaging support, interactive training, and coaching,\nco-ownership in successes and wins.\n\n\nGovernance and Change Management\nChapter 2\n[ 41 ]\nIn this chapter, we will cover the following topics:\nIT governance\nChange management\nIT service management\nArchitecting cloud computing solution catalogs\nIT governance\nGovernance addresses things needed to implement change. Typically, this process will start\nwith a series of questions that determine outcomes, responsibilities, and process. The\ndesired outcome is always first. Without knowing the desired outcomes, it is impossible to\ndetermine who is involved and what is needed to get there.\nThree questions to help start the IT governance process are shown in the following\ndiagram:\n(KIWTG\u0003\u0014\u001d\u0003'aGEVKXG\u0003IQXGTPCPEG\u0003UVCTVU\u0003YKVJ\u0003CFFTGUUKPI\u0003VJTGG\u0003SWGUVKQPU\n\n\nGovernance and Change Management\nChapter 2\n[ 42 ]\nOrganizational leadership and management must articulate desired outcomes, who is\naccountable and responsible for these outcomes, escalation criteria and triggers to progress,\nthe process for progression, and what metrics and counter metrics to be used. The delivery\nof desired outcomes must be continually monitored and evaluated for direction toward\ndesired outcomes, adoption levels, impact to metrics, and affects to counter metrics. Is the \nchange providing the necessary transparency? Is it following expected timelines? Are\nstakeholders and decision makers adjusting accordingly? Scorecards are extremely helpful\nto compare, visualize, and guide as governance is monitored through change.\nCounter metrics \nA second value to watch that may signal unexpected results or behaviors\naway from the actual change. For example, new storage is implemented\nwith much faster throughput. The server is responding much quicker and\ntaking more load. The metric being measured is disk i/o and data\ntransferred statistics for the server. The countermeasures may include\ndramatic changes in load balancing metrics because one server is getting\nall the traffic now due to first responder algorithms being used. Higher\nutilization on network ports, high CPU or RAM utilization may also be\nthe metrics to watch to see if the change is causing adverse behaviors in\nother places not directly affected by the change.\n",
      "page_number": 26
    },
    {
      "number": 2,
      "title": "[ 42 ]",
      "start_page": 56,
      "end_page": 72,
      "detection_method": "regex_chapter",
      "content": "Governance and Change Management\nChapter 2\n[ 43 ]\nThe next diagram shows an example of an IT governance scorecard with notational\noutcomes and metrics:\n(KIWTG\u0003\u0015\u001d\u00030QVCVKQPCN\u0003+6\u0003IQXGTPCPEG\u0003UEQTGECTF\nIT governance does not have a one size fits all approach. Change comes in all shapes and\nsizes with as many or more unique challenges to navigate. IT governance is unique because\nthe same change with the same desired outcome requires different governance when the\nchange is deployed into different operation and deployment models.\nComplete scope and strategy are things to consider when building IT governance plans and\nprocesses. It is important to have a clear understanding of what business processes and\nwhich applications may be affected. This analysis must also consider cost, benefit, risk, and\nshortcomings that may lead to unexpected outcomes as operation models and deployment\nmodels are chosen. Different models will require different governance plans and processes.\nAs mentioned previously, if there are changes to governance, there will also be changes in\nchange management.\n\n\nGovernance and Change Management\nChapter 2\n[ 44 ]\nImplementation strategy\nTo illustrate, there are three main categorical ways to consume IT products and services.\nThe first is in-house. In-house refers to a typical enterprise implementation, where the\norganization pays for ownership of all applicable resources. The enterprise also employs\nthe required operations staff to operate the deployed solutions. In this model, the enterprise\nhas complete and total control of IT governance.\nThe second, managed service provider (MSP), is an arrangement where the enterprise\ncontracts with an outside service provider to provide and/or manage IT resources. In this\nmodel, the enterprise retains some level of IT governance control by negotiating and\nenforcing a binding contract known as a service level agreement. The enterprise also funds\nall the MSP incurred costs plus a mutually agreed to profit. This option can be cheaper than\nthe traditional data center due to economies of scale. If the MSP is more efficient and/or\nmore automated, the same desired outcome may be less expensive but will require some\nchange to governance as some responsibilities will be shifted from in-house enterprise\nresources to the service provider.\nThe third option uses a cloud service provider (CSP). The CSP funds all hardware and\nsoftware. The CSP also pays the salaries and benefits for the required operations staff. The\nrequired IT function is consumed by the enterprise completely as-a-service. In this model,\nthe CSP has complete and total control of IT governance.\nThe following diagram here illustrates a few differences between the outsourcing models,\nMSP, and CSP. When choosing IT strategies, associated governance plans must also adapt\nto service models, deployment models, and implementation options:\n\n\nGovernance and Change Management\nChapter 2\n[ 45 ]\nChange management\nCultural/organizational/policy friction occurs through changes to established norms, not\nbecause of the technology being used. When adopting cloud computing, companies must\nfocus on the benefits from business model changes. Many changes are needed as businesses\nsearch for ways to support new growth, modernized product strategies, and constant\neconomic pressures. The transition to cloud computing can be complex for many different\nreasons. Some of the most common include:\nNew knowledge required\nDynamic nature of infrastructure\nNew cloud service management tools\nRapid innovation and fast-moving market\nNew management models\nDistributed ownership\nContinuous optimization\nDuring cloud transitions, organizations are not only changing from a technology\nperspective, but they are also changing their mindset and culture, simultaneously. The\nfollowing is a list of typical domain changes associated with moving from traditional\nmodels to the cloud. With each of these, there is also a simultaneous change to\norganizational culture, required skill sets, and individual mindsets:\n\n\nGovernance and Change Management\nChapter 2\n[ 46 ]\nIdeally, before cloud services begin to be utilized, focused change management strategies\nshould be implemented. Change management plans should quickly focus on raising\nawareness, understanding the change, accepting the changes, and the commitment of the\norganization to the expected outcomes and the people associated with them. The focused\nchange management and communications plans must relentlessly focus on providing data\nand messaging that support organizational change, modernizes mindset, and skill sets,\nhighlights benefits, and raises awareness. The organization should review metrics and\ncounter metrics to gauge culture change rates and determine if all communication channels\nare being leveraged with correct messaging and correct timing.\nCloud computing is transformational. The hardest part of any transformational strategy is\nchanging the hearts and minds of the people in the organization. It is critical for leaders to\nbe keenly aware of what to look for and to have metrics and counter metrics in place to\nshow changes in organizational culture, transformation rates and progress toward desired\noutcomes. A key question to ask is if each team member knows what the vision is; that is,\ndo they see the whole elephant or do they only see the part they are connected to? Do they\nunderstand how their part is connected to the whole? Refer to the following photograph:\n(KIWTG\u0003\u0016\u001d\u0003&QGU\u0003VJG\u0003UVCa\u0003UGG\u0003VJG\u0003GNGRJCPV\u0003CPF\u0003JQY\u0003VJGKT\u0003RCTV\u0003KU\u0003EQPPGEVGF\u0001\n\n\nGovernance and Change Management\nChapter 2\n[ 47 ]\nIndicative questions may include:\nDo they believe that this transformation is achievable?\nAre customers and service providers on the same page?\nDo they think they have sufficient resources to meet the schedule?\nAre the staff the right kind, in the right place, doing the right work?\nDo the various groups that are working together understand and accept the\ninterfaces and hand-offs?\nAre accepted and consistent vocabularies and definitions being used across the\nenterprise?\nThink of the IT industry as a tribe. The industry has its language, terminology, and\nstandard phrases. The cloud has a bit of its dialect within the IT tribe. There are common\npractices, rituals, and super-secret handshakes among tribe members. As with any other\ntribe or culture, standard practices and dialect enable quick identification of outsiders,\nnewbies, and those that may be a threat to the tribe. Is change likely to be adopted when the\npath forward lacks awareness for current practices or the messaging is communicated used\na completely different vernacular? Not possible.\nAs cloud and things-as-a-service increase in adoption, change management, and\ncommunication plans are paving the way. A deep dive into effective communication\nplanning and IT governance strategy could fill several volumes. Some guidelines for\neffective plans and strategies are:\nCommunicate using the tribe vernacular\nUse industry standard terms and definitions\nProvide authoritative sources if possible\nProvide data and insight with each communication\nReinforce benefits of change at every opportunity\nEnsure leaders are familiar with tribe language, customs, and rituals\nCompanies should also explicitly link demand-side governance with supply-side\ngovernance to improve efficiency and effectiveness as detailed in the following diagram.\nMetrics should be evaluated to create, modify, or delete as part of an enterprise continuous\nimprovement process:\n\n\nGovernance and Change Management\nChapter 2\n[ 48 ]\n(KIWTG\u0003\u0017\u001d\u0003.KPMCIG\u0003DGVYGGP\u0003+6\u0003IQXGTPCPEG\u0003CPF\u0003GbEKGPV\u0003KORNGOGPVCVKQP\nIT service management\nCloud computing solutions are implemented using service management frameworks. In\ndesigning any solution, the cloud solution architect must account for how well the\norganization is prepared to manage, operate, and continually improve that service. The\nmost widely adopted industry standard process for efficiently and effectively providing IT\nservice management (ITSM) is based on the Information Technology Infrastructure\nLibrary (ITIL). ITIL structures ITSM into four domains:\nIT infrastructure: The technology components directly related to an IT service,\nfor example, a Red Hat Enterprise Linux (RHEL) OS instance running on a\nserver.\nSupporting services: The underlying infrastructure required to operate the\ncustomer-facing IT services, for example, the DNS server required to reach the\nRHEL instance by use of the hostname. Supporting services could be referred to\nas IT-internal services.\nIT service: The services requested by customers. Each IT service is implemented\nusing the corresponding IT infrastructure. The IT service, therefore, complements\na set of IT infrastructure components by adding service definitions such as SLA\ninformation and cost. In our example, the RHEL instance could be offered as a\ngold and silver service. Gold might include 24 x 7 support while silver offers 8 x\n5 support at a lower cost.\n\n\nGovernance and Change Management\nChapter 2\n[ 49 ]\nITSM framework: Standards and processes that orchestrate all the activities\nrequired to deploy an IT service. This is not a collection of infrastructure\ncomponents but rather the framework used for the deployment, operation, and\ndecommissioning of IT services. The ITSM framework ties the IT infrastructure,\nsupporting services and IT services together and provides the needed operational\nfunctionality. IT services are presented to potential customers, they, in turn, need\nto be able to order them:\n(KIWTG\u0003\u0018\u001d\u0003\u0003+6\u0003UGTXKEG\u0003OCPCIGOGPV\u0003HTCOGYQTM\n\n\nGovernance and Change Management\nChapter 2\n[ 50 ]\nThe ITSM framework should not be confused with the deployment of an IT infrastructure\nrequired by a service. The focus is on orchestrating all the activities required to deploy an\nIT service as opposed to a collection of infrastructure components. If the customer requests\nthe RHEL gold service, it is not only about deploying the OS image using Red Hat Satellite,\nbut also about modeling the service in the Configuration Management Database (CMDB),\nconfiguring the event and impact management, making sure that the corresponding OLA\nare rolled-up into the promised SLA, and reporting it in a service view on a customer-\nfacing portal. The ITSM framework ties everything together into a coherent service offering.\nIn addition to implementing ITSM, the following best practices should also be practiced:\nEnforcement of brutal standardization across the enterprise: A small number of\nnon-optional constraints is often the most effective means of achieving agile\ngovernance. Jeff Bezos at Amazon famously mandated in 2002 that \"All [Amazon]\nteams will henceforth expose their data and functionality through service interfaces\" that\ncould eventually be exposed to a public-facing market. The form and style of the\ninterfaces were left to the teams to determine, but critically, anyone who didn't\nfollow the edict was subject to termination. Vigorous enforcement of a\nlightweight set of requirements is a recurring theme in successful modern IT\nmanagement. If this is not possible to globally enforce due to organizational\nconsiderations, it becomes even more important to demonstrate success via well-\nscoped pilot projects that can showcase the new model.\nIT standardization: Standardization is critical to gaining operational efficiency,\nreducing overall cost, and reducing the time required to deliver new capabilities.\nAcross all commercial industries, average standardization savings are on the\norder of a 2/3 reduction of servers. Increasing the utilization percentage realizes\nthese. Most of the economic value in this type of transition is accomplished\nthrough standardization in the software development platform and support\npersonnel efficiencies enabled by standardized operational processes, as shown:\n\n\nGovernance and Change Management\nChapter 2\n[ 51 ]\n(KIWTG\u0003\u0019\u001d\u0003#EJKGXKPI\u0003C\u0003UJCTGF\u0003UGTXKEGU\u0003GPXKTQPOGPV\nIT change management standardization (processes and tools): As part of the\nadoption of ITIL, it is critical that the organization has positive knowledge of its\nIT assets. This is typically the first transformational step towards ITIL. Once a\nbaseline of assets is collected, configuration control and processes must be put in\nplace and religiously followed. This is needed to efficiently handle incident\nmanagement (restore services, analyze incident types and trends, and improve\ncommunications), problem management (root cause analysis, document known\nerrors), and change management (reduce unexpected outages, track approvals\nfor compliance, new service support). Variations in these processes are\nproblematic and are easily seen by the customer. The goal is to provide\ntransparent, efficient, and consistent processes and services.\n\n\nGovernance and Change Management\nChapter 2\n[ 52 ]\nTransition from customer-mandated to a customer-focused model: In a\ncustomer-mandated model, the solution design is primarily driven by a targeted\nend user's requirements and dictates. This approach can lead to wide technical\nvariations across operationally similar solutions. In a customer-focused\nenvironment, the architects design solutions that can be used to meet a broad\nmarketplace of users. These offerings are focused on the target audience for the\nprovided services. For example, Amazon Web Services provides only limited\npatched versions of windows and Linux/Unix. This allows them to provide these\nservice solutions at a very inexpensive rate on-demand to customers.\nLeverage all applicable shared services: The interoperability and efficiencies\ngained through cloud-based offerings are depicted in the following diagram. As\nmore and more components become standardized the level of effort to provide\nthem decreases as well as the overall system complexity. As the enterprise and\nthe cloud offering matures, the enterprise performs more and more of the work\nperformed and eventually, the business process-specific applications can be\ndeployed as specialized IT service extensions.\nUse common standardized delivery patterns: Ideally, all cloud computing\nsolutions should be designed as real-time aggregations of existing cloud services.\nThe following diagram depicts the concept of providing a common set of\nstandardized delivery patterns. In this diagram, the far left depicts some\nenterprise-wide patterns or sets of software that could be used to support specific\nlines of business. The diagram also includes the notion of a common\ndevelopment environment. This common development environment can be a\nPaaS or a standardized company environment that uses specified software\ncomponents for targeting specific deployment patterns. By matching a certified\nand accredited development environment to a specific certified and accredited\noperational environment, an organization can rapidly develop and deploy\nindustry-accredited solutions quickly.\n\n\nGovernance and Change Management\nChapter 2\n[ 53 ]\n(KIWTG\u0003\u001b\u001d\u0003%QOOQP\u0003UVCPFCTFK\\GF\u0003FGNKXGT[\u0003RCVVGTPU\nArchitecting cloud computing solution\ncatalogs\nTo align with ITIL recommendations, enterprise IT services should be a customer-facing\nrepresentation of the available technology services. An organization's IT services catalog\nshould, in turn, provide all the information a customer should need to review, select, and\nacquire cloud solutions. Although both top-down approaches (from the business view) and\nbottom-up approaches (based on a technology and an available technology services view)\nhave been used, industry best practices indicate that the bottom-up approach is far more\nefficient, because it is based on something tangiblecthe technology or technology services\navailable to an organization.\n\n\nGovernance and Change Management\nChapter 2\n[ 54 ]\nA mapping of technology services to a cloud solution requires standards regarding both\narchitecture and delivery. Without this standardization, a mapping across the IT external\nand internal boundary is not possible, (for example, if IT doesn't maintain a standardized\nbuild and delivery method for RHEL, the customer would receive a different build every\ntime a RHEL gold service is ordered). A failure by the organization to set and enforce\nstandards across the entire technology space, both regarding architecture and delivery\nwould be contrary to the exact purpose of IT service management.\nThe challenge of the cloud computing solution task is primarily in the structuring and\norganization of available services to allow for a standardized, efficient, and reproducible\nmapping. This is referred to as service design in ITIL. During service design phases, all\naspects of the solution must address new and evolving business needs. Business aspects\noften considered for cloud solutions include:\nBusiness process and the definition of the functional service needs, for example,\ntelesales, invoicing, orders, credit checking\nThe service (or solution) being delivered to the end user or business by the\nservice provider, for example, email, billing\nService level agreements that specify the level, scope, and quality of service to be\nprovided\nInfrastructurecall of the IT equipment necessary to deliver the service to end\nusers which includes servers, networks, switches, client devices, and so on\nThe environmental requirements needed to secure, and operate the infrastructure\nData is necessary to provide the service and deliver the information required to\nexecute the business processes\nThe application needed to manipulate or modify the data and provide the\nfunctional requirement of the business processes\nThe operational level agreements (OLAs) and contracts, as well as any \nfoundational agreements needed to deliver the SLA-dictated quality of service\nSupport services necessary to execute all required operations\nAll processes or procedures needed in the execution and operation of the\ndelivered service\nInternal support teams that provide level 2 and level 3 support to end users and\nany of the service components\nExternal third party suppliers which are necessary to provide level 2 and level 3\nsupport to end users or service components\n\n\nGovernance and Change Management\nChapter 2\n[ 55 ]\nThe cloud is not the answer for everything. Components must first be considered in\nisolation. If the cloud presents advantages for the component in isolation, the component\nshould then be considered among its other relationships, interactions, and dependencies to\nother components and services. If advantages remain the same or increase, the cloud\nbecomes a viable option. This approach will result in an effective, well-researched, and\neasily communicated solution that aligns technical, strategic, and economic business needs\nwithin acceptable risk profiles.\nFollowing ITIL recommendations, each solution component should be documented using\nconfiguration management artifacts across nine categories:\nDescription: Describes the individual solution components by the use of\nstandardized diagrams and a short textual description.\nLife cycle: Defines the organization's specific lifecycle for solution components.\nThat life cycle is different (lagging) from the vendor life cycle because vendor\nproducts may need to be customized to meet internal standards.\nProvisioning: Describes the technical provisioning of solution components, the\nprovisioning process required by solution components is covered in the artifact\nprocesses and runbooks.\nConfiguration management: Document a solution component's requirements\nwith regards to the implementation in a configuration management framework.\nThis includes both CMDB templates as well as configuration management\nreports.\nSecurity: The technical and process aspects of a solution component's security\nrequirements that must be met during the deployment and operating phase.\nMonitoring: Functionality required for monitoring the operational status of the\nsolution components.\nProcesses and runbooks: One-time activities related to a solution component,\nsuch as provisioning. The runbook focuses on normal activities along with any\nanticipated abnormal activities associated with the specific solution component,\nsuch as house cleaning, backup/restore, auditing, failover. Both the process\ndescriptions and the runbook should cover the complete lifecycle of a solution\ncomponent, including provisioning, operations, and decommission.\nFinancials: Technology-based cost for Capital Expenditure (CAPEX) and\nOperational Expenditure (OPEX). This does not include any IT service-oriented\ncost items such as system administrator support, as they can vary depending on\nthe associated IT service offering.\n\n\nGovernance and Change Management\nChapter 2\n[ 56 ]\nBlueprints: Detailed descriptions of the technical implementation of the solution\ncomponent. For the engineering teams, the blueprints define how a component is\nto be developed for customers, they include detailed technical information about\nthe component.\nThe solution itself should be documented in the following 12 categories:\nDescription: Describes the individual solutions by the use of standardized\ndiagrams and a short textual description.\nNon-functional requirements (NFR): Solution requirements defined in a\nstandardized and unambiguous fashion. There are two categories of NFRs:\nCapabilities, which describe the features and functions provided\nby the infrastructure components\nQualities of service requirements that expand the infrastructure \ncapabilities to cover additional end-user expectations\nRecovery Point Objective (RPO) and Recovery Time Objective\n(RTO) are referenced for failover within a data center as well as\nfailover between data centers\nRequired solution components: Solution components required by a solution.\nLife cycle: The solution life cycle is mainly based on the life cycle of the\nindividual solution components required by the solution.\nProvisioning: Provisioning of a solution consists of the provisioning of\nindividual solution components. However, some solutions might require the\nprior provisioning of other solutions.\nConfiguration management: Document a solution's requirements with regards\nto the implementation in a configuration management framework. This includes\nboth CMDB templates as well as configuration management reports.\nSecurity: The security of a solution should be built into the individual solution\ncomponents and not added on a solution level. Most solutions don't require any\nadditional security considerations.\nMonitoring: The monitoring of the overall solution is essentially a collection of\nthe individual solution components monitoring functionality. If applicable, the\nsolution level monitoring should define how events from the different solution\ncomponents are correlated and de-duped. These definitions must correspond the\nscenarios outlined in the resiliency assessment artifacts.\n\n\nGovernance and Change Management\nChapter 2\n[ 57 ]\nResiliency assessment: The description of all possible technical failure scenarios\n(operational errors are out-of-scope). This includes a description of the failure,\nhow the failure is detected by the monitoring components and what remediation\nis possible. The impact of the failure is then captured regarding QoS type NFR,\nmostly downtime and data loss. The worst QoS NFR of all scenarios defines the\nQoS NFR for the solution as a whole.\nProcess and runbooks: A collection of the corresponding artifacts on solution\ncomponent level. Because of the mapping of solutions to IT services, there are IT\nservices-specific process steps which are placed as a wrapper around the\ncomponent processes.\nFinancials: Technology-based cost for CAPEX and OPEX for each component\nused by the solution.\nBlueprints: This artifact lists the blueprints associated with the solution. There is\nno solution-specific blueprint for a solution that consists of solution components\nand doesn't provide technical functionality outside of the solution components.\nIt is essential that the ITSM framework supports the mapping of IT services to solutions\nand maintains the relationship between solution components, solutions, and delivery\npatterns. This can be done in the following manner:\nCustomer orders an IT service\nIT service is mapped to a solution\nSolution components and composite solution components required for a solution\nare identified (solutions can't be deployed as a unit. Solution components are\nwhat is deployed)\nSolution components are deployed as individual components\nSolutions are re-constructed from solution components and mapped to IT\nservices\nSummary\nA cloud computing solution can only be successful if it is built on top of an effective IT\ngovernance and change management foundation. This chapter expanded that critical point\nby addressing cloud implementation strategy options and IT service management details. It\nalso delved into solution catalogs through which service providers present and explain the\navailable technology services.\n\n\n3\nDesign Considerations\nCloud computing is not a technology; it's an economic innovation. There are many ways to\nthink through design, economic models, risk profiles, strategies, and technology decisions.\nThe section will talk through some ways to navigate the thought process, how to eliminate\nsome of the noise surrounding the cloud, and how to stay focused on what the challenges\nare and how solutions get mapped to those business challenges.\nWe will cover the following topics in this chapter:\nFoundation for design d the thought process\nFoundation for design d the cloud is economic, not technical\nFoundation for design d the plans\nUnderstanding business strategy and goals\nFoundation for design ` the thought process\nThe cloud was supposed to be simple. It was supposed to be fast. It was supposed to solve\nmany, if not all, of the problems. As people started digging into cloud designs, they began\nto discover that things were not always as simple, things were not always less expensive,\nand things were not always performing as expected. In many cases, they experienced\nsignificant challenges with social adoption. The design did not match expectations for\nvarious reasons. Throughout this book, we talk about how perceived requirements are\nmerely starting objectives that accelerate towards requirements with the gathering of\nadditional insight and data. Ultimately, successful designs must simultaneously harmonize\neconomics, strategy, technology, and risk. This balance leaves risk and economics offset at\nequilibrium.\n\n\nDesign Considerations\nChapter 3\n[ 59 ]\nCloud computing is one of the rare things in our industry where nearly everyone is\nimpacted or affected by the change. Adoption of the change is the big challenge. People\nmust embrace the transition while they adapt to process and work method changes.\nWithout mental and emotional buy-in, projects can stall, exceed budgets, and, potentially,\nfail. The only path to acceptance and cultural change is through data.\nAs an example, consider a developer who is required to utilize a different cloud provider\nfor all projects going forward. That developer must change processes and working methods\nand, potentially, go through a significant learning curve for new systems, applications, and\ntools. This example is not about the transition to the cloud, but about transitioning between\nclouds. How well does this go over if the developer enjoyed working with the previous\nsupplier? What if that developer was very efficient in using the toolsets and could quickly\nnavigate current processes? What if the developer was with presented data showing that\nthe new provider could provide machines at 30% lower cost? The developer then can\nacquire and utilize three times the number of servers for the same budget. Consequently,\nwhat if productivity then increased tenfold when more compute resources are combined\nwith integrated toolsets and automated processes? Data helps drive adoption in all cases.\nAs we begin to think through design considerations, a consistent thought process is\nrequired. Consistent, methodical, process-oriented thinking will help accomplish several\nthings, such as:\nEliminating the noise\nEnabling quick navigation through the complexity\nMaintaining focus on constraints and objectives\nAllowing fast and accurate interpretation of relevant insight\nAccurately identifying optimal solutions satisfying constraints\nQuickly identifying opportunities to optimize strategy\n",
      "page_number": 56
    },
    {
      "number": 3,
      "title": "[ 59 ]",
      "start_page": 73,
      "end_page": 91,
      "detection_method": "regex_chapter",
      "content": "Design Considerations\nChapter 3\n[ 60 ]\nFoundation for design ` the cloud is\neconomic, not technical\nContrary to what most believe, the cloud is not a technological innovation; it is an economic\none. Although the cloud is based on virtualization technology, virtualization is not a new\ntechnology. Virtualization has been around for more than 50 years, starting with IBM\nmainframes in the early 60s, namely the CP-40. For decades, we have been trying to match\nbetter hardware and system utilization to the task or workload we are working to complete.\nVirtualization started with scientists and mathematicians from both IBM labs and MIT\ntrying to perform complex calculations. They were trying to get more work done and, at the\ntime, one task was limited to one system. IBM came up with the idea of virtualizing\nmemory, which creates separate instances and completing more tasks could take place in\nthe same amount of time. The birth of virtualization happened around 1963, with the first\ncommercially available virtualized system going to market around 1967.\nThis book often discusses the fact that strategy, technology, and economics must align for\nsuccessful design. Cloud computing and virtualization are not technical innovations. They\nare not strategic innovations, as the strategy of utilizing computing to its fullest extent, at its\nlowest possible cost, has been around as long as or longer than virtualization. The cloud is\ntruly an economic innovation. The cloud was not a technical problem; it was a billing\nproblem. The problem was that people could not find a way to accurately bill for a fraction\nof a processor or a fraction of RAM for a fraction of the time; a second, a minute, or so forth.\nThe real innovation came when an instance of compute could be consumed for a period of\ntime at a specified cost, with the ability to shut it off and give it back and only be billed for\nresources and time used (everyone considers Amazon Web Services as the early IaaS pay-\nas-you-go model). It is the ability to consume expensive compute on a pay-as-you-go model\n(OPEX versus CAPEX). It eliminated the need for massive capital up front and extended the\ngrowing movement of as-a-service models.\n\n\nDesign Considerations\nChapter 3\n[ 61 ]\nMuch time and effort are spent aligning strategy, economics, and technology early in the\nbook, because the cloud requires a new skill set, a new thought process, and a new\napproach to design. Successful architectures must simultaneously solve for strategic,\neconomic, and technical requirements. A technically perfect design that is too expensive\nequates to poor design. A technically perfect design that is economically feasible may solve\nfor the wrong strategy, also equating to poor design. All three must simultaneously resolve\nbefore the design can be viewed as successful. Because of this, cloud architects require a\nnew thought process: a process of reflection that systematically navigates in layers utilizing\ndiscriminating attributes and characteristics rather than service names and marketed\nfunctions. Cloud architects need an updated skill set that includes economics and risk\nstrategy, along with their technical prowess. Successful cloud architects must think more\nlike selling CFOs than technical geniuses. Successful cloud architects need to be aware of\nthe associated risks to the business if proposed changes are implemented. What is the\nimpact on the business economically, short-term and long-term? What cultural impact will\nthe changes have in the business, business unit, or division? Will personnel be affected?\nDoes the company structure need to change? As you can see, technical information is no\nlonger enough for the cloud architect. Cloud architects must be as comfortable in strategy\nconversations as they are in a technical one. Business finance and economics training are\nequally as important as, if not more important than, technical training.\nBecause the cloud is an economic innovation, and not a technical one, we quickly see that\ntechnical information will be pushed later towards the end of the thought process. In many\ncases, technical information becomes the tiebreaker, rather than the requirement. If you\nstart with a good set of non-negotiable, and work through basic ideas and their economic\nimpact, you can quickly see what fits strategically and what doesn't. If there are too many\nsolutions, you can change requirements, potentially optimize strategy, revisit economic\nrequirements, and so on. No one is saying that technical information is not required or\nvaluable. The idea is to use technical detail to fine-tune, optimize, and perfect; think of a\nmodern robot-driven, laser-guided surgical scalpel, as opposed to a medieval broadsword.\nThe cloud architect thought process is as shown in the following diagram:\n\n\nDesign Considerations\nChapter 3\n[ 62 ]\nEvery building project requires a solid foundation. Cloud design is no different. It is costly\nand difficult to change directions part-way through. Think of a high-rise building. It is\ntough to be twenty floors into construction and find there was a fatal flaw in the\nfoundation, requiring the whole project to be torn down and started over. In cloud design,\nseveral elements create the base for a successful design. Many modern day designers begin\nwith basics, such as storage, compute, or perhaps the network. They gradually get deeper\nand deeper technically, never knowing if they are headed in the wrong direction or not,\nunless they get told no by someone else who owns strategy or the project budget. This\nframework changes that dynamic. Technical elements come later in the process. This\nupdated cloud architect thought process begins with the base of things that are truly non-\nnegotiable. Non-negotiable constraints include things such as legal requirements,\ngeography, sector and industry-specific requirements, project goals, strategic elements, and\nso on. These limitations form the basis on which everything else is built. If any of these\nelements can change, they are not true requirements and they certainly cannot be labeled\nnon-negotiable. Once an exact set of non-negotiable are set, we can then look at additional\ndetails that will help define overall success factors, including basics such as business\ndrivers, strategy, value prop, economic models, and corporate/industry preferences. Every\ndesign will follow the same thought process, beginning with non-negotiable constraints\nworking upward through each layer as data and insight satisfy the objectives and\nconstraints of the previous tier.\nEvery situation, scenario, design, and component will follow the same thought process and\nthe same line of questioning:\nWhat are the non-negotiable constraints and characteristics?\nWhat are the economic attributes and their impact?\nHow does this affect strategy?\nWhat is the discriminating technical attributes and characteristics?\nIs there an abnormal or excessive risk associated and does it affect economics?\n\n\nDesign Considerations\nChapter 3\n[ 63 ]\nFoundation for design ` the plans\nThere are no pre-built plans. Architects and designers are the ones tasked with developing\nthe plans. Buildings cannot be built without a set of plans. The perimeter boundaries are\nsurveyed. The site plan is created. The dimensions of the building are laid out. All of this is\nestablishing scope and size for the architect or designer, who must stay within those\nconfines. Anything outside of those boundaries will be unacceptable and labeled as poor\ndesign.\nCloud architecture and design operate in much the same way. We must establish a set of\nboundaries to work within. We must identify what is acceptable and what is not. All of our\nquestionings must lead us toward a clear understanding of what leads to success and what\nleads to failure. It is just like building a house; if I put in a five-car garage, but I don't have\nroom for a kitchen, that would be a poor design. I did not account for all requirements and,\nmost likely, missed one that was non-negotiable. The same is true for cloud computing. If\nI'm not considering the correct requirements, my design will fail. So, the question becomes:\nHow do I identify the right requirements?\nWe must discover the correct requirements through progressive questioning. We must start\nwith questions that help us identify wants and needs. We must identify what is non-\nnegotiable. What is the foundation? What is the base? What are the things that, no matter\nwhat changes, those answers are always the same? They will not change based on better\npricing or something else suddenly becoming more interesting because of the latest\nmagazine article. The base foundation elements are truly non-negotiable. They are based on\nwhat you know at this time and on the current data you have. They outline what needs to\nhappen as a result of your design creating our base layer and non-negotiable foundation.\nWe have now drawn the boundaries of our property before we start laying out the\nbuilding. We know where our limits are.\nNon-negotiable must always include a combination of economic, strategic, and technical\nelements. All successful designs must have these elements. Therefore, they must also be\nincluded in the design foundation. For the foundation to be correct, these non-negotiables\nmust harmonize. A balance must be met within these limits and boundaries for the design\nto proceed.\n\n\nDesign Considerations\nChapter 3\n[ 64 ]\nMany factors need to be considered when navigating towards a successful design.\nSuccessful design requires boundaries. It requires an understanding of where the no lines\nare. By trying to find where unacceptable meets acceptable, we are then able to focus on\nwhat does work rather than chasing something that was never going to work from the start.\nIt is a bit like some are essential and non-negotiable. These things are set, they cannot be\nchanged. For example, geography. Geography is typically non-negotiable. Services get\ndeployed where they are needed. If services are needed in California, they are not likely to\nbe moved to Singapore because the price is a bit more favorable. Non-negotiable can\nchange with every solution and every design scenario. The point is to uncover what is the\nfoundation for the design. Find the things that absolutely cannot change and go from there.\nThe following are some of the more common non-negotiable attributes within design:\nEnterprise size: While the size of an enterprise can positively influence certain\nfactors of design, the enterprise size itself does not dictate design. For example,\nthere is no minimum company size required to utilize the cloud. Enterprise size\nmay affect design factors, such as scale, economics, risk, distribution, service mix,\nand so on. Each of these factors must be looked at from a strategic, economic, and\ntechnical point of view to make. A Microsoft study showed that the vast majority\nof organizations of all sizes use both Software-as-a-Service (SaaS) and hosted \ninfrastructure services. Both SaaS and hosted infrastructure services are used\nmost by organizations with less than 100 employees. Smaller companies are also\nmore likely to use Platform-as-a-Service (PaaS).\nRelevant industry sectors: Different industries have different requirements,\ndifferent levels of compliance, and different appetites for risk. Government and\neducation industries lead in the use of SaaS with a Microsoft study showing more\nthan 60% of organizations reporting active use. While information technology is\nthe ubiquitous horizontal layer underlying all industry sector verticals, security\nrequirements strongly influence implementation specifics. This was highlighted\nby a cloud computing adoption study that documented the difference between\nregulated and unregulated industries.\n\n\nDesign Considerations\nChapter 3\n[ 65 ]\nAmong regulated industries, insurance companies prefer private clouds because\nthey are considered more secure than public clouds. Even though many studies\nhave shown this to be a false assessment, this misconception is shared across\nmany industry sectors. Even so, industry association community clouds have\nincreased in popularity. While the banking industry is also concerned about\nsecurity, the industry's forced transition from OS/2 to Windows 7 in 2014 drove a\nrapid adoption of newer and more sophisticated technology. Additional\nupheavals caused by subsequent transitions to Windows 10 have made cloud\ncomputing an attractive option for administrative and back-office processes like\nemail, file sharing, and sharing of notes. While opportunities to use cloud\ncomputing in a variety of ways do exist across the government sector, most users\nmisunderstand them. Today's largest opportunity is in using the public cloud, but\nmany in government also fear security problems. Government-wide efforts such\nas the Federal Risk Authorization and Management Program (FedRAMP),\nhowever, have gone a long way toward educating this sector.\nAcross unregulated industries, the story differs greatly. Cloud implementations in\nthe retail market have been mostly IaaS or PaaS solutions. Security, availability,\nand vendor maturity are all aspects that retailers consider when deciding which\nfunctions to deploy as a cloud service. Media companies have gone all in on\nutilizing cloud computing. Today, the media audience can access any content\nthrough a variety of channels. These new opportunities are why cloud service\nproviders and application developers are exploring a cloud-based ways to enable\nmulti-screen entertainment. Industries are using cloud integrate, automate, and\nenable innovations in logistics, sales support functions, HR, product\ndevelopment, and lifecycle management, as well as some manufacturing\noperations.\nGeographybWhere am I? Where do I need to be? Cloud computing is\ncomposed of physical data centers with five primary considerations influencing\nwhere data centers are built:\nPhysical space required to build the data center buildings\nAvailability of high-capacity network connections\nInexpensive electricity\nApplicable jurisdictional law, policies, and regulations\nThe cloud computing export markets are shown in the following\nfigure:\n\n\nDesign Considerations\nChapter 3\n[ 66 ]\nThe first three are governed by physical constraints based on environmental\nvariables:\nPhysical geography\nWeather and natural disaster risks\nRenewable energy resource availability (such as water, geothermal,\nor wind) for cooling and power\nSafety concerns fueled by crime, terrorism, and corporate\nespionage\nAfter that, proximity to high-capacity internet connections is key, because a data\ncenter's value is measured by the number of users it can support. Proximity to the\ninternet backbone, or the main trunks of the network that carry most of its traffic,\nis another important driver. Data centers consume huge amounts of energy to\ncool servers. Areas with cheap energy are highly attractive. Considerations\nassociated with jurisdictional laws, policies, and regulations are addressed later.\n\n\nDesign Considerations\nChapter 3\n[ 67 ]\nWhile US companies lead the cloud computing marketplace globally today, that\ndoesn't guarantee leadership in the future. Some foreign companies, such\nGermany's SAP or Japan's Fujitsu, have become strong global competitors, while\nothers, like Alibaba in China, have become formidable global competitors.\nPublic cloud services are rapidly becoming more important strategic factors in\nbusiness. Public cloud will constitute more than half of worldwide software,\nserver, and storage spending growth, by 2018, according to IDC. An example of\nthis is General Electric, a global company that currently has over 90% percent of\nits applications in a public cloud. Greater public cloud adoption has also spurred\nwider SaaS usage, accounting for approximately 55% of all public cloud spending\nby 2018.\nLegislation and other external regulations that apply: The laws, policies, and\nregulations of a particular jurisdiction can have a significant impact on the cloud\nprovider and the cloud user. The many law and policy problems that may affect\nits use by a company include:\nSecurity with the assurance that unauthorized access to sensitive\ndata and source code will be guaranteed by the cloud provider\nConfidentiality and privacy of data held by the CSP with an\nexpectation that the cloud provider, third parties, and\ngovernments will not be able to monitor their activities\nClear delineation of liability with regards to operational problems\nProtect of intellectual property\nRegulation, control, and ownership of data that is created or\nmodified using cloud-based services\nFungibility and portability which is described as the ability to\neasily move or transfer data and resources from one cloud service\nto another\nAbility to audit users to verify compliance with regulatory\nrequirements\nA clear understanding of legal jurisdiction\n\n\nDesign Considerations\nChapter 3\n[ 68 ]\nThe manner in which an organization will approach cloud computing policy vary\nand will be driven by organizational priorities. One of the largest challenges will\nbe associated with user security and privacy. Since many data centers are located\nin the United States, some of these concerns will be caused by the USA PATRIOT\nAct, the Homeland Security Act, and other intelligence-gathering instruments\nemployed by the federal government to for release of information. One of the\nmost disturbing aspects of these policies is the restrictions placed on a CSP that\nprevent notification to a user if the government issues a subpoena for a user's\ninformation. Other policy issues that can impact cloud use include Health\nInsurance Portability and Accountability Act (HIPAA), Sarbanes-Oxley Act,\nGramm-Leach-Bliley Act, Stored Communications Act, federal disclosure laws,\nfederal rules of civil procedures, and e-discovery.\nAdopted by the European Commission to strengthen data protection for all\nEuropean Union citizens, the General Data Protection Regulation (GDPR) was\napproved in April 2016. With an effective date of May 25, 2018, GDPR compliance\nis challenging for companies of all sizes. With cloud computing, the problem may\nbe even worse. Studies have shown that only 1 percent of cloud providers have\ndata practices that comply with GDPR regulations. In fact, only 1.2% of cloud\nproviders give users encryption keys that the customer manages and just 2.9%\nhave secure password enforcement that is robust enough to pass GDPR muster.\nOnly 7.2% have proper SAML integration support.\n\n\nDesign Considerations\nChapter 3\n[ 69 ]\nUnderstand business strategy and goals\nThe second layer of the design triangle is having a thorough understanding of\norganizational strategy and goals. The cloud solution design must support and advance the\norganization's strategy and goals or it will be deemed a failure. To ensure alignment, the\nsolution architect must discuss goals and strategy with the business owner and agree on the\nkey metrics and target values. Some of the most popular cloud computing goals are as\nfollows:\nAgility: Cloud computing delivers improved agility because it has on-demand\nself-service and rapid elasticity. These attributes enable enterprises to quickly\ninnovate, introduce new products and services, enter new markets, and adapt to\nchanging circumstances. Business agility also requires the ability to create new\nbusiness processes and change existing ones. Cloud computing can eliminate\nprocurement delays often associated with development and testing by enabling\ndevelopment resources to be available on-demand. Resource scaling enables\nservice levels to be maintained and reduces cost. Cloud-based strategies can also\nhelp an enterprise acquire capabilities without the need for training.\nProductivity: The cloud can provide a more productive environment for\ncollaborative working. The use of cloud-based tools for email, instant messaging,\nvoice communication, information sharing and development, event scheduling,\nand conferencing are well known. The cloud can also provide shared logic in a\nbusiness ecosystem.\nQuality: The cloud can deliver better quality-IT because usage information gives\nthe enterprise an understanding of how IT is operating. Better understanding\nenables effective planning, equitable resource sharing, and increased resource\nefficiency. The ability to use web portals to automatically provision and\nconfigure resources gives the cloud service consumers substantially better\nmanagement capabilities versus non-cloud system. Economies of scale also make\nthe cost and effort required to of duplicate systems for disaster recovery\nrelatively small. Server consolidation, resource optimization, increased asset\nutilization, and thin client use enhances cloud computing efficiency and reduces\nthe carbon footprint.\n\n\nDesign Considerations\nChapter 3\n[ 70 ]\nReduced cost: Cloud computing reduces IT costs by delivering effective resource\noptimization, by being able to move processor, memory, storage, and network\ncapacity among users almost instantly. Significant cost reductions can be\nobtained by replacing expensive client devices with cheaper client devices that\nprovide just a user interface to the application server. Community clouds\nprovide an easier path for a community of enterprises to share the cost of\ncommon resources. If the organizational goal is maximizing financial capital use\nthrough the improved use of the debt and equity funds, the OPEX opportunity\nprovided by cloud computing will directly support that strategy.\nIdentification of new business opportunities through morphing a company into\nbecoming a cloud service provider. A company that excels in the quality of its IT\nmay easily become a public IaaS or PaaS provider. Implementing services in the \ncloud would make them accessible to a large, global market.\nOperational risk/reward balance can be improved by moving low-risk activities\nto an on-demand service environment. Other on-demand business opportunities\nmay be unveiled exist by mitigated risk management through partnerships and\nrisk sharing. High risk can be reduced by sharing selected business process\noperations that are strongly correlated to operational or legal failures. This may\ninclude corporate risks linked to identifiable software applications, infrastructure\ncomponents, or specified services. Corporate benefits should be traded-off\nagainst these corporate risks with consideration of whether business activities\nwith low corporate returns can be commoditized for competitive advantages.\nThis strategic direction could identify opportunities to improve market share,\nrevenue, profit, or cost management through on-demand delivery of cloud\nservices.\nModifications to the business products line could be both transformative and\ndisruptive. Opportunities to exploit existing markets with current products and\nservices could be profitable as a utility or commodity offering. A company may\nbe able to offer existing services in a self-service model that has been augmented\nand enhanced with on-demand features. Unique products services sourced and\ndelivered via an on-demand portal could also be disruptive to existing product\nlines. If this is the case, the cost-benefit exchange here must be analyzed. Rapid\nscaling and expansion of offered services could deliver additional value.\n\n\nDesign Considerations\nChapter 3\n[ 71 ]\nReducing investment in non-differentiating processes through the use of SaaS\ncould significantly improve an organization's bottom line. Areas where this has\nbeen particularly helpful include:\nBusiness management area, which includes skills management,\nbenefits administration, compensation planning, and human\ncapital management (HCM)\nFinancial management (FM) in the areas of accounting, financial\nand compliance reporting, real estate management, Sarbanes-\nOxley (SOX), finance taxes, BASEL II, order to cash, business\nperformance management, and risk management\nCustomer relationship management (CRM) that supports sales,\nbusiness intelligence (BI), customer experience management\n(CEM), business analytics, call center management, campaign\nmanagement, sales force automation (SFA), and sales analytics\nSupply chain management (SCM), especially procurement,\nsupplier relationship management (SRM), inventory\nmanagement, logistics, and import compliance processes\nManufacturing management of product lifecycle (PLM), resource\nand capacity, and workforce\nInformation technology (IT) services associated with IT\narchitecture design, data center operation, and software\ndevelopment\n\n\nDesign Considerations\nChapter 3\n[ 72 ]\nCorporate headquarters activities such as research and\ndevelopment (R&D), communications, strategy, and portfolio\nmanagement, legal, and marketing:\nInternal business process scope and complexity can\nbe adjusted by considering what niche business\nprocesses could be moved to a CSP. This could range\nfrom moving a specific IT operation to an on-\ndemand provider or commoditizing a service for\ncompetitive low-cost advantages. Both large\norganization-wide operations and smaller, localized\nactivities should be considered. Management should\nalso make specific decisions on which specific\nbusiness processes need to stay under the control of\nthe business for competitive advantage or whether\ncomplex processes can be improved by reducing the\nnumber or complexity of the steps involved.\nCloud computing has a history of improving\ncollaboration/information sharing through the use of\non-demand personal productivity tools. This area\ncan also be used to addresses debates around\nwhether personal information and assets created by\nindividuals should be classified as the private\nintellectual property of the corporation.\nCollaboration across a community of users can raise\nsimilar issues, by raising intellectual property issues\nregarding creating assets on a shared platform or\necosystem business service environment. When\naddressing private information, corporate rules that\nidentify and define data ownership and the\npartitioning of corporate information from that\nwhich is private should be part of the strategy\nprocess. This will often drive decisions regarding the\nsuitability of secure storage and access control\nservice.\n\n\nDesign Considerations\nChapter 3\n[ 73 ]\nIn the case of public information, corporate and personal data rules should\nprohibit the storage of, and access to, personal and corporate information from\npublic locations. Local and national legislation affects this as well. Information\nheld publicly must be monitored and managed at a level that meets legal e-\ndiscovery standards. Personal and corporate information must be classified,\npartitioned, and effectively isolated for storage and use in public locations. The\nvarious components of the business process are shown in the following diagram:\n\n\nDesign Considerations\nChapter 3\n[ 74 ]\nAn architect's success in overcoming the two big hills to adoption of cloud computing\nservices will be influenced mostly by a solution's alignment with relevant business drivers\nand strategies. The solution must also pay homage to the intended customer value\nproposition.\nThe most broadly recognized cloud computing business drivers are:\nCost flexibility, which shifts fixed cost to variable cost and allows the\nimplementation of the pay-as-and-when-needed model\nBusiness scalability that provides flexible, cost-effective computing capacity\nMarket adaptability, which enables faster time to market and supports business\nor mission experimentation\nMasked complexity that helps expand product and service sophistication while\nsimultaneously allowing for greater end-user simplicity\nContext-driven variability that is used to better define the user experience which,\nin turn, increases product relevancy\nEcosystem connectivity, which fosters new commerce value nets, which can\ndrive and create new business models\nBusiness strategies most often supported by these business drivers are:\nProduct and service optimization that enhance the customer value proposition\nimprove current industry value chains and uses the cloud to incrementally\nenhance their customer value propositions while improving their organization's\nefficiency.\n\n\nDesign Considerations\nChapter 3\n[ 75 ]\nMarketplace innovation that aims at extending the customer value proposition\nthrough the transformation of industry value chain and improving customer\nvalue. This strategy choice often results in brand-new revenue streams and\necosystem role modifications.\nMarket disruption is focused on inventing new customer value propositions and\ncreating new industry value chains by generating new customer needs and\nsegments.\nValue chain enhancement is accomplished by building new industry value chains or\ndisintermediate existing ones. Cloud adoption can also aid an organization that is\nstruggling to maintain its place within an existing value chain. This is accomplished\nthrough increased efficiency and improved capability to partner and collaborate.\nTransformative organizational goals can be attained through the development of new\noperational capabilities, changing the organization's existing industry role or by deciding to\nenter a completely different marketplace or industry.\nIf organizational goals revolve around improving the customer value proposition,\ncompanies can garner incremental revenue through improvements in current products and\nservices and enhancement of customers' experiences. Cloud computing services can help a\ncompany explore or create new distribution channels or payment methods. This could\nattract existing or adjacent customer segments. This vector could also lead to the creation of\na new marketplace need which would attract new customer segments and generate unique\nrevenue opportunities.\nThe various economic options cloud enables often drives innovative business models. This\nis accomplished by having a model that ensures a dollar of revenue for every dollar of\nexpense. Cloud computing economic payment options include:\nOn demand, in which you only incur cost for the service your organization\nactually consumes\nReserve, where a company is obligated to pay for a predetermined quantity of\nservice provider resources at a discounted price\nSpot market, which is an open marketplace auction model that varies resource\ncost by varying demand for that resource\n\n\nDesign Considerations\nChapter 3\n[ 76 ]\nSummary\nCloud computing requires consistent, methodical, and process-oriented thinking. It is not\nabout any specific technology, but rather operational, economic, and business models built\non highly standardized and automated IT infrastructures. Success depends on establishing\nand working within formalized boundaries. These boundaries must be documented and\nenforced by organizational IT governance. The cloud solution design must support and\nadvance the organization's strategy and goals or it will be deemed a failure. To ensure\nalignment, the solution architect must thoroughly understand the business or mission goals\nand strategy. More importantly, the architect and business/mission owner must agree on\nthe key metrics and target values that will define success.\n\n\n4\nBusiness Drivers, Metrics, and\nUse Cases\nThere are many ways to evaluate projects from a financial perspective. Cloud solutions, like\nmost IT projects, are easily compared with ROI metrics. However, ROI does not typically\ntell the whole story. The cloud can pull many financial levers as well as optimize and\nincrease efficiencies in several related, but not necessarily direct, inputs to ROI calculations.\nThis section looks into some of the considerations when looking at the economic impact of\ncloud solutions.\nReturn on Investment\nReturn on Investment (ROI) is the most often used measurement when making project\ninvestment decisions. It measures the rate of return versus the cost of the investment.\nBecause ROI is a percentage, it is very easy to make quick comparisons when evaluating\nmultiple options. There are only four ways to either increase revenues or lower costs,\nimproving ROI numbers. The four financial levers are the following:\nDecrease investment\nIncrease revenue\nDecrease costs associated with the activity\nReduce the time required to attain revenue\n\n\nBusiness Drivers, Metrics, and Use Cases\nChapter 4\n[ 78 ]\nCloud computing can operate any of these levers but the simultaneous achievement of all\nfour is impossible. The relationship between these factors is the most important aspect of\ncloud ROI, not the absolute values. For example, a project moving to a public cloud may\nresemble the following: initial investment decreases, operating costs may increase, revenue\nmay remain flat, but margins may increase due to lower investment, and return may\naccelerate due to lower overall investment at higher return margins. Increasing revenue in\nthis situation would accelerate the rate of return and shorten the time needed to attain\nrevenue goal.\nThese financial dynamics change with every project, service model, and deployment model.\nPrivate deployments have completely different dynamics. In-house versus external, in-\nhouse versus in-house, each will have different return rates. ROI can be improved, or made\nworse, with strategy choices because of the relationships of revenue and speed of return.\nRevenue may increase by improving features and quality, commanding higher market\nvalues. Automation may help a business scale, driving greater revenues at controlled costs.\nCloud requires balance in its approach. Data-driven methodologies will help to define goals\nand expected outcomes, and identify ways to manage risk. A data-driven approach can\nguide the organization to optimal strategies and identify better choices. There are many\ndata points and fundamental drivers that can impact cloud ROI numbers. Many data points\nare captured from related productivity, speed, scale, and quality measurements. Typical\nROI calculations are straightforward: there is a cost for a given return. ROI for cloud is a bit\ndifferent in that there are other factors, including efficiency improvements, opportunity\ncosts, and investment patterns. When evaluating cloud versus traditional IT strategies,\nadditional layers of data must also be considered, such as the following:\nIncreased turnover and profits due to increased efficiencies\nRevenue loss due to the inability of existing systems to respond to dynamic\ndemands\nCosts of managing a standalone and non-standardized environment\nReduction or avoidance of capital cost related to the purchase, development, and\ndeployment of new systems or services\nSuccess-based growth and investment, as needed\nSmaller increment investment for cloud versus larger capital investment for\ntraditional models\n",
      "page_number": 73
    },
    {
      "number": 4,
      "title": "[ 78 ]",
      "start_page": 92,
      "end_page": 103,
      "detection_method": "regex_chapter",
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 79 ]\nThe factors that drive ROI are the following:\nProductivity, which is enabled by utility-based services that provide on-demand\nprovisioning that meets meet actual customer usage. Increased productivity can\navoid infrastructure capital expenditures, avoid infrastructure investment\nopportunity cost, and improve customer satisfaction through better\nresponsiveness.\nResource utilization, which eliminates the practice of dedicating servers to\nspecific functions or departments by using active management to size and handle\npeak loads that are underutilized at off-peak times.\nUsage-based pricing translates higher provider utilization into lower\ninfrastructure costs for consumers. SaaS does this by reducing the traditional\nlicensing cost associated with ownership, number of users, support, and\nmaintenance costs:\n5QHVYCTG\u0003NKEGPUG\u0003EQUV\u0003CU\u0003VJG[\u0003TGNCVG\u0003VQ\u0003PWODGT\u0003QH\u0003WUGTU\n\n\nBusiness Drivers, Metrics, and Use Cases\nChapter 4\n[ 80 ]\nSpecialization and scale that gives the CSP an ability to drives lower IT costs\nthrough skill specialization and economies of scale by amortizing costs over a\nlarger user base:\n4GXGPWG\u0003IGPGTCVGF\u0003QP\u0003VJG\u0003DCUKU\u0003QH\u0003VJG\u0003EQUV\u0003CPF\u0003VJG\u0003VKOG\nIncreased speed in provisioning IT resources, which enables enterprises to\nacquire the resources they need faster. This model also increases visibility into\nresource configurations, which accelerates the choice when many options are\navailable. This factor can dramatically cut the time to deployment of new\nproducts and services. Elastic provisioning creates a new way for enterprises to\nscale their IT to enable the business to expand. Rapid execution saves time and\nenables new business operating models:\n6TCFKVKQPCN\u0003+6\u0003FGRNQ[OGPV\u0003XGTUWU\u0003CP\u0003+6\u0003FGRNQ[OGPV\u0003WUKPI\u0003ENQWF\u0003EQORWVKPI\n\n\nBusiness Drivers, Metrics, and Use Cases\nChapter 4\n[ 81 ]\nFaster execution of lifetime cost models through increased speed of execution.\nThis factor positively impacts lifetime cost models by reducing the cost of a\nproduct or service as the depreciation cost of purchased assets decreases and \nefficiencies are realized. This higher rate of cost reduction means that\nprofitability increases more quickly, giving shorter payback times and increased\nROI:\n5SBEJUJPOBM\u0002SBUF\u0002PG\u0002DPTU\u0002SFEVDUJPO\u0002WFSTVT\u0002UIBU\u0002XJUI\u0002DMPVE\u0002DPNQVUJOH\nThe IT asset management process accelerates reducing the risk of decoupling IT\nchoices and its impact on long-term operations and maintenance IT service costs.\nThis factor also enables the ability to select hardware, software, and services from\ndefined design configurations to run in production environments. This reduces\nthe design-time/runtime divide while simultaneously optimizing service\nperformance:\n6TCFKVKQPCN\u0003UQHVYCTG\u0003NKEGPUG\u00034GVWTP\u0003QP\u0003+PXGUVOGPV\u0003ITCRJ\n\n\nBusiness Drivers, Metrics, and Use Cases\nChapter 4\n[ 82 ]\nCloud computing is an economic innovation, not a technical one. Infrastructure is aging\nand is going to require significant funding to modernize. How can we optimize the ratio of\nspend to return across the entire asset portfolio? The same traditional deployments are\ngoing to lead to a traditional return. A change in model and economics is required to\nmodernize. Cloud enables an enterprise to change economic models, achieving a faster,\ncost-effective asset management lifecycle for the entire IT portfolio. Designs can utilize\ncurrent capabilities and components, optimizing runtime performance. Cloud services also\nlower entry cost with faster deployment time, quicker time to market, increased\ncompetitiveness, and more business and leads generated across a much larger operational\nscale. Additional high-value services are quickly and efficiently delivered to clients and\ncustomers through the use of cloud-based collaboration services for communication,\ninformation exchange, and virtual meetings.\nCloud economics is creating many new business opportunities that were not possible\npreviously. Opportunity is often associated with the Long Tail shown in the preceding\nfigure. The illustration shows as efficiencies improve, opportunities for a revenue increase\nand margins rise over time. Innovation driven by economics increases efficiency, lowers\ncost, and as a by-product creates additional opportunities for revenue. The revenue\nopportunities may be related to underserved markets that are now accessible. New\nsegments and sectors may now be financially feasible that previously were considered\neconomically undesirable. Opportunities that may have previously been undesirable from a\nrisk point of view may now be interesting revenue opportunities as the lower costs and\nhigher margins may offset perceived risk.\nInterestingly, we see the same behavior in the IT infrastructure market. The cost of\ncomputing resources is dramatically dropping while the cost of managing traditional\ndeployments is rapidly rising. This creates new opportunities within cloud-based\necosystems as well. We are seeing many new cloud service providers entering the market.\nMerger and acquisition activity has increased. Innovations driven by economics are always\ndisruptive. These types of shifts create many opportunities for specialists and related\nenterprises to make big plays as new service providers or acquirers.\nBecause cloud innovation is economic and not technical, new revenue opportunities can be\npeople-based and IT-based. Opportunities may be a new type of service or an existing\nservice with a new economic model. Revenue opportunities may enhance the quality of\nexisting services as reinvestment is now possible due to improved margins, increased scale,\nand larger scope for current operations. Cloud computing enables better utilization of\nresources and assets, increasing efficiency, and accelerating operations and delivery at\nlower cost. Cloud computing is disruptively influencing both buyers and sellers.\n\n\nBusiness Drivers, Metrics, and Use Cases\nChapter 4\n[ 83 ]\nAs available services, combinations of services, and pricing models continually change, the\nquality of the service must be considered. A recent performance comparison of a single\nserver from a single service yielded some surprising data. The provider offered the same\ncloud service out of multiple locations. Two locations were chosen. The same instance type\nand same server configurations were chosen. The only difference was the location. Each\nserver was benchmarked using the same CPU and memory test. Testing reported a 700%\ndifference in performance within the same provider, with the only difference being\nlocation. When the benchmarking was run across multiple providers, using the same size of\nserver with the same configuration, we noticed very different performance, with the price\nvarying by more than 3,200% from low to high, for the same configurations. What is the\ndifference? Why would the deviation be so high? Aren't all clouds equal? Cloud is cloud,\ncorrect? Not exactly.\nBasic ROI calculations are pretty straightforward: cost versus expected return. As we\nexamine the other surrounding benefits of utilizing the cloud, we are reminded of a few\nthings:\nCloud is an economic innovation\nEvery benefit of the cloud has an economic impact\nNext-generation designers, architects, and IT leaders need a blended foundation\nthat includes principles of business and strategy, general economics, and\ntechnology, as well as a good understanding of business risk and economic\nmitigation\nWhy would the same server vary 700% in performance and 3,200% in price for the same\nconfiguration? Other layers and benefits must be accounted for in our cloud ROI\ncalculations. There are many metrics to consider; not all need to be used, for example,\nservice level agreements (SLAs). SLAs are a simplified way to try and express a quality of\nservice numerically. The higher the number, 99.999 versus 99.90, the higher the quality of\nservice is perceived to be. What does the quality of service actually mean? This topic will be\nexplored more throughout the book. Is the service redundant? How resilient is it? Is it\ndelivered via cheap unknown white box machines out of a neighbor's garage or is it within\nan impenetrable fortress using a brand name, high performance, and the latest and greatest\nhardware? How is it supported? How up to date are patching and security? Is it hardened\nOS? Is there 24x7 support? What is the quality and level of the support engineering teams?\nMany things can contribute to pricing differences, including margin.\n\n\nBusiness Drivers, Metrics, and Use Cases\nChapter 4\n[ 84 ]\nCloud computing differentiation is not just through the provisioning of utility computing\nservices, but also higher-level services that enhance and build customer value. These\nattributes are why there is rapid movement from technology-centric services to business-\nvalue-centric services. This change extends to nearly every service and every industry, with\nutility infrastructure services at one end and complete function and application business-\ncentric services being provided by nearly every provider in the market.\nROI metrics\nWhen designing and building a business case for a cloud computing solution, the following\nmetrics can assist with aligning a prospective solution with the business or mission need:\nTime: Cloud solutions optimize time required to deliver or execute business\nprocesses by decreasing the time required to provision resources or time required\nto consider multi-sourcing options. It can also decrease the time required to\nachieve specified goals associated with information technology services. This\nvalue also leads to a faster realization of reduced IT total ownership costs.\nCost: Cloud computing can optimize ownership use by reducing the application\nportfolio total cost of ownership. This is realized through license cost reduction,\nopen source adoption, and SOA reuse adoption. Cloud also optimizes the cost\nassociated with delivering a specified IT service capacity by aligning IT costs\nwith IT usage. The CAPEX versus OPEX utilization balance can be more\neffectively managed with pay-as-you-go savings.\nQuality: Cloud can improve service and product quality through customization\nand enhanced user relevance. It can also reduce ecological damage through\nreduced carbon footprint and advance organizational green sustainability goals.\nOptimizing margin: Metrics associated with the cost to deliver/execute business\nand supply chain cost is reduced, which increases product/service margin.\nIncreased flexibility and choice across providers and feeder services can also help\nto optimize margin.\n\n\nBusiness Drivers, Metrics, and Use Cases\nChapter 4\n[ 85 ]\nKey performance indicators\nThe key performance indicators (KPIs) are used to measure goal attainment. Advance\nagreement on the relevance of the following cloud computing KPIs can assist in\nsolution/business alignment:\nTime:\nAvailability versus recovery SLA: Indicator of availability\nperformance compared to current levels\nTimeliness: Degree of service responsiveness, which can be used\nto indicate rapidity of service choice determination\nThroughput: Transaction latency or the volume per unit of time\nthroughput which measures workload efficiency\nPeriodicity: Frequency of demand and supply activity or the\namplitude of the demand and supply activity\nTemporal: The event frequency to real-time action and outcome\nresult\nCost:\nWorkload-predictable cost: Indicator of CAPEX cost of on-\npremises ownership versus cloud\nWorkload-variable cost: Indicator of OPEX cost for on-premises\nownership versus cloud\nCAPEX versus OPEX cost: Indicator of on-premises physical asset\nTCO versus cloud TCO\nServer consolidation ratio: Ratio of servers in legacy infrastructure\nto the number used in the cloud infrastructure\nWorkload versus utilization percentage: Indicator of cost-effective\ncloud workload utilization\nWorkload type allocations:\nWorkload size versus memory/processor distribution: Measures\npercentage of IT asset workloads using cloud\nInstance to asset ratio: Measures percentage and cost of IT\nconsolidation\nDegree of complexity reduction (%): Measures the number of\nguest operating system instances versus the number of physical\nresource assets\nTenancy to instance ratio: Measures tenants per resource, which\nmeasures CPU and memory utilization\n\n\nBusiness Drivers, Metrics, and Use Cases\nChapter 4\n[ 86 ]\nEcosystem (supply chain) optionality: Tracks the use of\ncommodity assets used to deliver company services after the\nfunction is migrated to a CSP\nQuality:\nExperientialcthe quality of the perceived user experience of the\nservice\nBasic quality of service metrics (availability, reliability,\nrecoverability, responsiveness, throughput, manageability,\nsecurity)\nUser satisfaction\nCustomer retention\nRevenue efficiencies\nMargin increase per unit revenue:\nRate of increase of annuity income\nSLA response error rate\nFrequency of defective responses\nIntelligent automationcthe level of automated response (agent)\nMargin:\nRevenue efficienciescability to generate margin increase per\nrevenue.\nRate of annuity improvement\nMarket disruption ratecrate of revenue growth versus rate of new\nproduct customer acquisition\nBusiness goal key performance indicators\nBusiness goal KPIs that should be considered include the following:\nThe speed of cost reduction\nCost of adoption/de-adoption\nOptimizing ownership use\nRapid provisioning\nIncreasing margins\nDynamic usage\nElastic provisioning and service management\nRisk and compliance improvement\n\n\nBusiness Drivers, Metrics, and Use Cases\nChapter 4\n[ 87 ]\nEconomic goal metric\nIn a similar vein, measurable economic goal metrics include the following:\nCapital expenditure avoidance\nConsumption billed as a utility\nLower barriers to market entry\nShared infrastructure costs\nLower management overheads\nImmediate access to the application\nImmediate termination option\nEnforceable SLAs\nHigh benefit-cost ratio\nThere are also metrics for performance and price-to-performance that can provide very\nclear comparisons between providers, services, and strategies. Using these next-level\nmetrics can move the ROI conversation forward quickly when aligning solutions to an\norganization's goals and expected outcomes. This type of transparent data also helps build\nconsensus, eliminate politics, and facilitate cultural adoption within the enterprise.\nGeneral use cases\nYou can refer to a baseline set of use cases a cloud solution architect can use to identify the\nbest solution target for the enterprise at IUUQT\u001c\u0011\u0011XXX\u0010TDSJCE\u0010DPN\u0011EPDVNFOU\u0011\u0013\u0019\u001b\u0014\u001b\u0015\u001b\u0016\u0011\n$MPVE\u000f$PNQVUJOH\u000f6TF\u000f$BTFT\u000f8IJUFQBQFS\n4BWFE\u0002DPQZ\u000b.\nThese are illustrative examples of the most typical cloud use cases and are not meant to be\nan exhaustive list. Active use case components are shown in color.\nAdditional details on the operational requirements summarized here are\nprovided in $IBQUFS\u0002\u0013\u0013, Operational Requirements.\n\n\nBusiness Drivers, Metrics, and Use Cases\nChapter 4\n[ 88 ]\nSummary\nROI is always a key topic when organizations invest. Investing in cloud computing\nsolutions is no different. The architect must be clear on how the proposed solution will\ndeliver value and that value must be described in business or mission terms. The metrics\noutlined in this chapter have been effectively used across many industries. Key business\ndrivers must be identified early and a direct linkage to solution functions and capabilities\nmade clear. General use cases are useful for outlining day in the life scenarios, which, in turn,\ncan be effectively leveraged when communicating solution value to business or mission\nowners.\n\n\n5\nArchitecture Executive\nDecisions\nThe cloud is changing everything. Change occurs every day in the cloud services market.\nChanges to solutions, services, pricing models, consumption models, and locations all lead\nto different strategies, technology choices, economic impacts, and risk profiles.\nToday, it is a normal process for the consumer to express what solution and solution\ncomponents they require in the form of an RFI, RFP, RFQ, or some other type of\nrequirements document, emailing a spreadsheet, for example. The consumer expresses\nwhat their experts need based on a mix of data sources, including business requirements,\ncurrent state information, and the latest innovative articles read by a partially involved IT\nleader. Requirements are sent to one or more providers with the providers expected to\nrespond to what is requested. Pricing is normally added and the response returned with\nminimal, if any, insight given. There may be a few phone calls and some back-and-forth,\nenabling the service provider to respond as accurately as possible.\nCurrent processes feel very transactional: I want a blue shirt, or I want a French cuff,\nbutton-down, blue dress shirt; please show me the blue shirts you have and how much they\nare. This process pattern is very manual and very slow. The process is filled with stop-and-\ngo workflows and communication patterns. This kind of process is also serial, meaning that\neach step must be completed before the next can start. Tools used to communicate and\nshare data are also manual, disconnected, and slow.\n\n\nArchitecture Executive Decisions\nChapter 5\n[ 90 ]\nToday's process looks something like the following diagram. This view is from the service\nprovider side as they formulate a response. Everyone involved in the process (consumer,\nprovider, integrator, consultant, and channel partner) follows virtually the same\nengagement pattern:\nThe cloud is completely turning this engagement method upside down and sideways. This\nmethod no longer works. The market is changing too fast. Products and services are\nreleased, updated, changed, or retired almost daily. How can the consumer side possibly\nbuild accurate solutions in a vacuum and pass it to a service provider for a response? It is\nimpossible. The service providers can barely keep up with the pace of knowing their own\nproduct catalogs. There is no way the consumer can keep up with the thousands of\navailable products and services from a service provider. Multiply that by the hundreds and\nthousands of potential service providers available in the market. Talking to two or three\nsuppliers is no longer a representative sample set. Multiply those numbers by the hundreds\nof locations available globally. There are trillions of potential combinations available. How\ncan a consuming enterprise possibly gather the data, normalize it, compare it, optimize\ndesign, and pick a strategic partner using the slow, manual, disconnected tools, and\nprocesses available today?\n",
      "page_number": 92
    },
    {
      "number": 5,
      "title": "[ 90 ]",
      "start_page": 104,
      "end_page": 115,
      "detection_method": "regex_chapter",
      "content": "Architecture Executive Decisions\nChapter 5\n[ 91 ]\nWhat needs to change? As stated, the process needs to be completely inverted. The tools\nneed to be automated. Collaboration needs to be in real time. Insight needs to become a\nrequirement, not an unexpected differentiator. Instead of pricing requirements, we need to\ndesign with economics in mind. Instead of building technical designs, we need to\ncollaborate for solutions that align technology, strategy, economics, and risk. Instead of\ntelling service providers what is required, business challenges and risks should be\ncommunicated. The thinking must change and begin to map solutions to business\nchallenges and match technology to tasks.\nWith changes in mindset, process, and approach, executives can accelerate organizations,\nmotivate teams, and increase control over strategy, economics, and risk. Engaging in\ninsightful, collaborative ways can transform the previous slow, serial, stop-and-go methods\ninto the following high-velocity, streamlined, parallel operating method:\nInvert for insight ` process\nToday, we are bound by a process that no longer matches our industry, nor does it align\nwith industry direction. As stated, consumers express what they believe they require,\nconfining service providers to respond with services they have that match the request. The \nRFI process allows for some insight to be given, yet is still void of any real collaboration. It\nis still a version of a question and answer session. If the questions go unasked, or the wrong\nquestions are asked, opportunities for greater insight are missed.\n\n\nArchitecture Executive Decisions\nChapter 5\n[ 92 ]\nReal-time collaboration\nWith the level of change, and the rapid pace of change, in the current market, anything less\nthan real-time collaboration will miss the mark. It is no longer possible to keep up with the\nmoving pieces in our industry. The constant changes to product strategies, economic and\nbusiness models, consumption models, deployment models, and pricing can't be accurately\ngathered, normalized, and compared for one or two vendors, certainly not the hundreds\navailable today.\nExpress challenges, not requirements\nCloud is changing everything. Cloud allows us to buy a fraction of a resource needed for a\nfraction of time, consume it, and return it when finished. The length of time could be in\nseconds, hours, days, months, or years. The beauty is that we cannot align what we need to\nwhen we need it and directly match it to a specific challenge or situation. Thinking can now\nshift away from acquiring solutions then mapping as many problems as we can to it, to\nnow expressing well-defined challenges and only acquiring what is needed, when it is\nneeded, to satisfy the challenge and be able to give it back when we are done with it,\neliminating much of the cost.\nExpressing challenges also improves relationships and partnerships with those wanting to\nhelp solve them. Service providers listen and respond to global market needs every day.\nThe challenge for a service provider is trying to interpret requirements and translating\nthem into challenges. Service providers rarely participate in conversations directly\nexpressing client challenges. The normal conversation covers requirements, not business-\nfocused needs. Consumers avoiding deep discussions related to their challenges may occur\nfor many reasons: internal politics, ego, inexperience, poor planning, oversight, policies,\nconfining processes, and so on. Service providers have learned that insightful, proactive,\ncollaborative engagements are the most successful, with the highest levels of loyalty and\ncommitment. Express challenges to receive insight.\n\n\nArchitecture Executive Decisions\nChapter 5\n[ 93 ]\nAutomate and enable\nExecutives continually balance investment, return, and risk. Today, many of the tools and\nprocesses we use are manual, disconnected, and slow. In a rapidly moving market,\nexecutives need more data and insight quicker. Automation, integration, and enablement\nare critical for success in today's cloud universe. Any platforms, systems, tools, and\nprocesses invested in should align strategy, technology, economics, and risk in an\nautomated integrated way. Collections of data do nothing without mapping, matching, and\ncomparing to expose real-time insight used to make aligned decisions.\nStop talking technology ` Strategy\nMany people start talking about technology choices very early in conversations. There may\nhave been significant recent investigation aimed at a technology component or direction.\nThere may be team structures aimed at supporting technology commitments previously\nmade. Financially, there may be a perception that change is going to be difficult and\nexpensive. There may also be those that are afraid of change as it may politically change\ntheir value, roles may change, team structures could change along with current\nresponsibilities, or skill sets may need significant changes for those late in their career path.\nStrategy determines direction; technology implements it. Technology can influence\nstrategy, but cannot dictate it. Before the cloud, projects were designed and engineered for\nthe anticipated high-water mark of utilization, even if it was for 1 minute. The anticipated\nworkload determined infrastructure size and scale. Cloud has now enabled us to design\nand architect for the low side baseline workloads and dynamically burst as needed into\ndemand-based configurations using automation and scripting, improving economics, and\nmatching technology to strategy.\nEconomics, not pricing ` Economics\nIt is important to switch the thinking from what the cost is for assembled technical answers\nand move to using economic data as an integral part of the solution design process. Today,\nwe go through the design process four or five times, first for assembling a reasonably\ncorrect technical answer then pulling it apart in layers, trying to back down to something\nthat is within economic reach yet in a technically desired direction.\n\n\nArchitecture Executive Decisions\nChapter 5\n[ 94 ]\nIf we switch that model and move to looking at designing with economics from the very\nbeginning, we can eliminate nearly all recirculating effort as designs are built then\nredesigned in the current process. It is also important to keep in mind that much of the\ntechnical detail affects implementation details more than it does strategy or economics. If it\ndoes not materially affect strategy or dramatically change economics, move on to\nimplementation discussions after confirming a solution strategy and economic direction. If\nshowstoppers rise during implementation conversations, you are still money and time\nahead, able to switch directions as needed since you have not done the four or five rounds\nin traditional processes:\nThe perfect technical solutions are sometimes unaffordable or miss the strategy. Low-cost\nsolutions may not balance with technical requirements or introduce too much risk. All\nfactors must balance for successful solution design.\nSolutions, not servers ` Technology\nTechnology no longer needs to drive economic and strategic conversations. Due to the\neconomic innovation of cloud, technology is now better able to align with strategy,\neconomic, and risk requirements. Enterprises can now simultaneously align choices on\nstrategy and direction with technology and economics. As an example, businesses with\ncyclical needs, meaning that everything is not needed all the time, can leverage services and\njust-in-time infrastructure deployments as needed to satisfy cycles as needed. Very little\ninvestment is required up front, with scalable growth available programmatically.\n\n\nArchitecture Executive Decisions\nChapter 5\n[ 95 ]\nSuccessful solutions will utilize services as needed. These services may be internally\nprovided or externally consumed. Strategy and economics will determine the best path\nforward. In almost all cases, consuming solutions as a service is optimal across nearly all\ndecision metrics and scorecards.\nLower costs can be bad for business ` Risk\nExecutives often need to balance economics and risk. The more risk assumed, the lower is\nthe cost. Risk and cost seem to have an inverse relationship. Cloud is an opportunity to\nchange paradigms. The shift comes from a change in perspective. Because cloud is an\neconomic innovation, much of its pricing model builds on economies of scale. An entire\nteam of administrators, security experts, technicians, and engineers can be acquired as part\nof a service for a server priced at pennies per hour. Because of the specialization, deep\nknowledge, 24x7 operation, automation, and employed best practices, one could argue that\nthis situation is a much lower risk than the overworked, underpaid frustrated internal IT\nguru who has virtually no training in cybersecurity and is sick of answering calls after\nhours.\nAlso mentioned earlier was the situation where the same configured server represented a\n3,200% difference in price from a low-cost provider to a high-cost one. It is a guarantee that\nevery provider along that continuum from low to high offers a different level of service\nquality, automation, support, security, patching, management, and monitoring for that\nprice. Because cloud appears to be lower cost, it does not mean that cloud is optimal for\nyour unique situation and business challenges. Cloud is a tool in the tool bag. Screwdrivers\nshould not get used when driving nails. Cloud is the right tool when used for the right job.\n\n\nArchitecture Executive Decisions\nChapter 5\n[ 96 ]\nAdoption is optional ` Culture\nIf you play in the middle of the road, you are eventually going to get run over. Change is\nhard; adopting change sometimes seems impossible. Cloud is something that goes much\nbetter with a firm commitment to it. It is not something to do halfway. You can't maybe,\nkinda, sorta your way into using cloud services. Cloud is a strategic change with very\npowerful economic levers alongside it. Cloud can transform enterprises in many ways.\nCloud is something that you can start at your own pace. There is no reason to lift and shift\neverything at once. Hybrid strategies are among the safest, easiest to control, and cost-\neffective ways to beginning the cloud journey. Cloud journeys are not sprints; they do not\nneed to be marathons either. Based on the real-time data, analytics, scenario planning,\nmodels, normalized data, and side-by-side comparisons, it should be clear whether cloud\nservices make sense for the given situation. Start with the smallest scope that makes sense\nand go from there.\nCloud changes can have a rippling effect. They can also have a polarizing effect. It is critical\nto gather, process, and effectively communicate the right data in the right messaging at the\nright time. Data helps change the mindset, increase adoption, and, ultimately, remove any\ncultural challenges.\nPeople get comfortable with routine, processes, structures, roles, and responsibilities.\nEverything is familiar. A big part of changing mindset is helping others understand the\nbenefits and believe the change is needed. Disruption is difficult when skill sets are lacking.\nChange is also difficult when the reasoning for the change is unknown or misunderstood. It\nis very uncomfortable to change processes that people have used for some time. It is hard to\nreorganize teams or shift responsibilities to others. It is critical to communicate the right\ndata within the right message at the right time. Change management is a huge part of cloud\nsuccess. A relentless focus on communicating change, retraining, refocusing, and\nprioritizing is core to any successful culture and mindset change. Every cloud initiative fails\nwithout cultural adoption.\nUpdating skill sets is another big key to success. The market is moving fast. Technology is\nrapidly changing. Most team members may get one class per year, maybe two if they are\nlucky. Most have to do it on their own to stay relevant. With technical skills lagging behind,\nit makes it very difficult when cloud services begin to take hold. Cloud is economics,\nstrategy, and risk. Most technical team members have very little training or, quite honestly,\npatience to deal with politics, strategy, risk conversations, financial analysis, and metrics.\nMost technical types would gladly jump off the nearest high bridge. The next generation of\narchitects are going to need a mix of business, financial, technical, and political skills. The\nnext generation of high-value designers and architects will need to think more like CFOs\nthan technical administrators.\n\n\nArchitecture Executive Decisions\nChapter 5\n[ 97 ]\nTechnology for the executives\nArchitecting cloud solutions will also require executives to become familiar with a few\nmodels as cloud solutions are evaluated and implemented. It is important to have a\ncomfortable level of familiarity with these different concepts as strategies and economics\nare discussed alongside technology choices and risk profiles. Service models (IaaS, PaaS,\nand SaaS) dictate the direction for any cloud services consumed and implemented for the\nscope or that project.\nCloud service models for executives\nService model choices are driven by the consuming organization's employee skill set. For\nexample, system administrators manage infrastructure. If IaaS is the chosen service model,\nthe enterprise will need to maintain and grow this group's knowledge and capabilities.\nBecause the base infrastructure gets consumed as a service, administrators can refocus on\nnew career growth opportunities beneficial to the enterprise in the longer term. An example\nmight include learning infrastructure scripting skills with languages such as Chef and\nPuppet:\n\n\nArchitecture Executive Decisions\nChapter 5\n[ 98 ]\nHeavy software development shops may choose to consume PaaS services. The platform\ncarries all of the components, frameworks, drivers, pieces, and parts, enabling developers\nto be productive immediately. There are many different types of platforms for building\nmany different things. Software development is not the only choice or industry with PaaS\navailable, but it is the most prevalent. As an example, in-house developers need to be\ncomfortable with the platform chosen. If Java is their forte, a Microsoft Azure PaaS service\nmay be slow in the adoption of new Java capabilities. Teams may need additional and\nongoing training depending on the platform chosen. Additional investment may outweigh\nthe benefits if the platform is too much of a mismatch to current skill sets or lacking\ncapabilities for strategies chosen.\nThere are opportunities where no infrastructure is needed separately. The software is\nprebuilt with all the features and functions needed already included for a fee. Licensing is\nconsumed per some unit, usually per enterprise or per user. The SaaS model is rapid to\ndeploy with rapid adoption rates. SaaS does not have an answer for everything, but very\nhelpful when solutions are available.\nDeployment models for executives\nOnce the organization chooses a cloud service mode, relevant metrics are selected, and\ntargeted values finalized, the architect must then determine the appropriate\nrecommendations for deployment and implementation styles. Each service model may\nhave multiple deployment and implementation models. For example, IaaS is a service\nmodel. Within that model, infrastructure is deployed in many ways, including private,\npublic, dedicated, or shared. The deployment of public cloud infrastructure services can\ntraverse many different configuration options as well as different consumption and\neconomic models.\nOrganizational risk tolerance drives cloud deployment model selection. Risk domains cut\nacross operational, economic, technological, and security domains. Operational risk is a\nconsideration for every deployment model. Private deployments are considered lower risk\nas they are owned and managed by the consumer. Right or wrong, humans tend to trust\nthemselves more than others. That thinking has been proven invalid many times, yet this\nreflexive attitude remains.\nWhen choosing deployment options, updating and patching the infrastructure is a key part\nof cybersecurity. Updates are rolled out proactively when service providers are managing\nthe base infrastructure. In-house management may choose not to install patches or security\nupdates for many reasons. Deployment model choices may need to change depending on\nthe desired outcomes.\n\n\nArchitecture Executive Decisions\nChapter 5\n[ 99 ]\nWhen internally managing software and systems, upgrades can take more time, but it does\ngive the owner/manager control to decide how and when the upgrades and updates are\nimplemented, along with as much time as needed for preparation and testing. Another\noperational risk may include the inability to access a cloud service due to lack of internet\naccess. As cloud deployments grow, typically quicker than traditional deployments, they\ntend to sprawl, with control becoming more challenging as the number of services\nconsumed grows. If not monitored and managed correctly, this leads to end-of-month\ncloud sticker shock.\nIn IaaS solutions, the cloud service provider makes all the decisions regarding technology\nchoices for deployed services, including the type of server, the brand of storage, and the\nCPU manufacturer. These choices are abstracted away from the end user, with limited\nvisibility into technical compatibility. The separation from the technology choices may lead\nto portability and interoperability risks in some situations. There can be some concern\nabout data security. Most of these concerns are quickly handled through good design and\nsecurity-minded questioning. Putting information into services that are accessible over the\npublic internet means that criminals have a potential target. Security is a never-ending\nbattle, with threats externally, internally, and sometimes from the least-expected places.\nRemember TJX and the HVAC entry point.\nImplementation models and IT governance for\nexecutives\nImplementation models are largely determined by IT governance. Governance for all cloud\nservices, other than under a private deployment model, is outside of the consuming\norganization's purview. For this reason, cloud services may introduce some level of legal,\nsecurity, regulatory, and jurisdictional risks. Realizing there may be some risk associated,\ncloud services can increase levels of adoption and enforcement of information technology\nstandards. Additional investment commitments in infrastructure, training, and time may be\nrequired, depending on the compliance standards being met. A failure to obtain initial\ncompliance and sustaining the level of commitment required will cause failure of even the\nbest cloud computing solution.\nOnce executive decisions have been finalized, and investment commitments confirmed, a\ncloud computing adoption strategy can then move forward. The most broadly followed\ncloud adoption strategies are the following:\nBuilding competitive advantage, which typically targets organizational strategic\nreinvention of customer relationships, rapidly innovating products and services\nrapidly or building new or improved business models.\n\n\nArchitecture Executive Decisions\nChapter 5\n[ 100 ]\nBetter decisions strategic vector that extensively leverages analytics to pull\ninsights from big data. This also requires the seamless sharing of data across\napplications and the exploitation of data-driven and evidence-based decisions.\nDeep collaboration aims to make it easier to locate and use expert knowledge\nacross a business ecosystem. This strategy requires improved integration\nbetween development and operations and collaboration across the organization\nand extended ecosystem.\nBusiness model change strategies that can be used to support these adoption strategies are\nthe following:\nAddressing more mission/business requirements via a customer self-service\nmodel\nTransitioning from single/limited IT providers into a multiple IT provider\necosystem\nChanging the design/build/maintain and identify/adapt/adopt technology mix\nfrom 80/20 to 20/80\nTransitioning from labor-driven HW/SW integration to value-driven IT service\nmanagement\nTransitioning from dedicated single tenancy to shared service multitenancy\nTransitioning from a cost center that provides support to a business center that\ndelivers tangible value\nSummary\nEnterprise executives will make final decisions on just about every aspect of a cloud\ndeployment. Required investment levels and organizational impact makes this inevitable.\nAs a key team member, the solution architect must always understand and manage the\nviewpoints of any involved executive. These tasks require two-way communication and the\neffective transfer of difficult concepts and new business, operational, and economic models.\nIn almost all instances, the solution architect tends to be cast to serve the role of lead\nexecutive instructor. The challenge of this role was made clear in early 2018 when Facebook\nCEO Mark Zuckerberg testified before the United States Senate on social media data\nprotection concerns. Although these senior leaders were seriously contemplating the\nimplementation of new social media laws and regulations, their questions on basic cloud\ncomputing business models and technologies betrayed a sometimes comical ignorance of\nmodern technology. This chapter should go a long way in assisting you in the performance\nof your educational duties.\n\n\n6\nArchitecting for Transition\n\"Victorious warriors win first and then go to war, while defeated warriors go to war first\nand then seek to win.\"\na The Art of War, Sun Tzu\nMany people mistake The Art of War for a book teaching strategies on how to fight wars and\ncritical battles. To the contrary, The Art of War is about how to avoid the fight. Long, drawn-\nout battles are expensive, slow, and very hard to control. In the first part of the opening\nquote, Victorious warriors win first, Sun Tzu focuses the student on preparation, situational\nawareness, a controllable environment, attention to relevant details, and determining the\noutcome before starting. The lessons in the book are about knowing the environment and\nhaving situational awareness. Do you have the latest, most accurate information? Do you\nhave reliable sources? Are you thinking clearly? Are you controlling your emotions and\nbiases? Are external influences and detractors in check? Do you see the data for what it is or\nfor what you think it should say?\nCloud transitions, while not wars, can certainly feel like battles. They can feel a bit like\nreligious crusades where believers are willing to do anything for the cause. Cloud\ntransitions do not have a particular pattern, shape, or size. Cloud transitions require the\nmost up-to-date and accurate data possible. Successful cloud transitions are successful\nbefore they ever start. They require the same clear focus, preparation, environmental\ncontrol, and situational awareness. In Sun Tzu, if cloud transitions do not have a clear\nfocus, detailed preparation, and careful planning, the transition will fail before it starts.\n\n\nArchitecting for Transition\nChapter 6\n[ 102 ]\nIn this chapter, we will cover the following topics:\nUser characteristics\nApplication workload\nUse of application programming interfaces (APIs)\nUser characteristics\nClouds are like children. They have unique personalities, quirks, strengths, and\nweaknesses. No two are the same. Just like children, and like some siblings, they behave\ndifferently and seem to follow different rules, and sometimes, they misbehave at the worst\ntime.\nCloud solutions are specific to each provider. Successful transitions require due diligence\nand a thorough understanding of the cloud provider. Cloud transitions require the\nconsumer to know the provider more than the provider needing a deep understanding of\nthe consumer. The traditional IT acquisition process begins with the consumer providing all\nof the details and requirements for inspection by the provider. The provider then responds\nwith a proposed design that meets said requirements. Cloud solution acquisition cycles\nnow reverse that thinking. Up front, service providers must effectively communicate their\ncapabilities, characteristics, attributes, and supporting services for consideration by\nconsumers. This complete reversal in process and approach is core to the cultural challenge\nassociated with cloud transitions. Strategic, economic, and technical choices are now\ncompletely held by the consumer side. Previously, service providers were responsible for\nnearly all solution due diligence, making recommendations, controlling solution\neconomics, and in nearly all cases conservatively over-engineering solutions as excess\ncapacity was better than too little when needed.\n",
      "page_number": 104
    },
    {
      "number": 6,
      "title": "[ 102 ]",
      "start_page": 116,
      "end_page": 136,
      "detection_method": "regex_chapter",
      "content": "Architecting for Transition\nChapter 6\n[ 103 ]\nThe examination of several characteristics and attributes is now the responsibility of the\nconsumer. These investigative deep-dives often require internal cooperation, and often\nunwanted politics, across multiple organizations and divisions. The service providers are\nalso adapting in real time as they are now required to share deeper-level detail on how\nsolutions are designed, built, supported, and maintained. Some details potentially requiring\nadditional investigation may include the following:\nApplication characteristics\nApplication dependencies\nAPI requirements\nTechnology service consumption requirements\nConsuming organization's ability to support IT automation\nConsuming organization's use of scalable design techniques\nConsuming organization's data security and control requirements\nConsuming organization's transformational readiness\nCloud architecture starts by understanding the end user. Cloud service providers use\ntargeted utilization and usage patterns to guide their infrastructure designs. Consumers\ngain value from a provider's service only if that consumer's operational patterns match the\nprovider's target.\nEconomy of scale is how CSPs create profit margin. To accomplish this, resource pools are\nused to share resources across multiple tenants. Both physical and virtual resources are\ndynamically provisioned and de-provisioned based on user demand. Resource pooling also\nprovides location diversity. For SaaS, the user has no control or knowledge of hardware\nlocation. In some cases, the consumer can specify general location aspects such as country\nor region.\n\n\nArchitecting for Transition\nChapter 6\n[ 104 ]\nCloud computing economics typically uses virtualization to automate information\ntechnology resource provisioning. This often leads to an assumption that virtualization\ndefines cloud computing. In truth, cloud computing economic depends on customer\npopulation metrics, namely the Number of Unique Customer Sets (n), Customer Set Duty\nCycles (b,f), Relative Duty Cycle Displacement (t) and Customer Set Load (L):\n\n\nArchitecting for Transition\nChapter 6\n[ 105 ]\nThese metrics set the minimum physical IT resource requirements needed to serve a\nspecified maximum level of demand. The preceding figure illustrates this concept. While\nthe three scenarios show different customer sets, each demanding a 1-unit maximum load,\nand the consumer demand duty cycles are similar, the duty cycle displacement values in\neach scenario are different. This small difference translates into significant operational\ndifferences:\nMaximum demand in Scenario A and Scenario B is 30% more than that in\nScenario C.\nScenario B has a zero minimum demand.\nScenario C transitive load requirements are 50% less than the other two.\nIf each user set individually owned their resources, three units would be\nrequired. Scenario A and Scenario B would require the same total number of\nunits. Scenario C, on the other hand, would only need a maximum of 2 units to\nsupport the same demand, resulting in a 30% resource savings.\nIn leveraging the cloud economic model, CSPs must continuously monitor key user metrics\nin near real time to enable any required changes in the underlying physical infrastructure.\nThis leads to the desired illusion of infinite resources that is characteristic of the cloud\ncomputing service experience:\n\n\nArchitecting for Transition\nChapter 6\n[ 106 ]\nUnderstanding and knowledge of target user base characteristics will enable a better\nsolution design. The most appropriate service providers are those that provide the best cost\nand benefit during critical operational periods and duration. If implemented correctly, the\ncloud computing economic model can materially improve return on investment over on-\npremises deployments. The preceding figure illustrates this model in a private cloud\nconstruct. A 2009 Booz Allen Hamilton study concluded that a cloud computing strategy\ncould save from 50%-67% on the life cycle cost of a 1,000-server deployment.\nThe study also showed increased benefit-cost ratios when the cloud transition included\nmore servers or a faster migration schedule, as shown in the following figure. A separate\nDeloitte study shows that cloud deployments deliver more return on investment with\nshorter payback periods when compared to the traditional data center option:\n\n\nArchitecting for Transition\nChapter 6\n[ 107 ]\nA cloud solution architect should make every attempt to document and verify the following\nuser population characteristics:\nThe expected number of concurrent users for every application or service\nUser growth rates over an annual period\nThe variability of user demand and for any apparent temporal cycle (that is, time\nof day, day of the week, week of the month, month of the year)\nUser consumption differences based on the user's geographic location\nBreadth across the population and frequency in the use of mobile devices and\nmobile applications\nTypes, models, and operating systems of mobile devices used\nDevice ownership options\nVariability of user characteristics based on functional entity association,\nincluding any economic variable measured or tracked by separate organizational\nentities or consolidated across the entire user base\nImportant business continuity or disaster recovery concerns are driven by end\nuser location, end user devices, operational process considerations, or the usage\ncycle\nApplication design\nMany organizations want to leverage cloud computing as a means of reducing the\noperational costs associated with maintaining legacy applications. In these cases, the cloud\nsolution architect is faced with an application migration task where application maturity\nmay become a decisive factor in whether a cloud environment is beneficial or detrimental.\nApplication characteristics could also profoundly influence the decision regarding CSP\nchoice. Many legacy applications, for example, are tightly coupled to data, specific\nprocesses, and other related applications. This makes them very difficult to transition to a\ncommodity-based technology service model. Embedded dependencies and unusually\nundocumented assumptions don't easily fit into a strictly standardized and not easily\ntailored environment.\n\n\nArchitecting for Transition\nChapter 6\n[ 108 ]\nCloud-friendly applications are loosely coupled, with RESTful interfaces and modular\ndesign. This design approach makes them more amenable to modern cloud-based\ninfrastructures. This difference must also be addressed when applications are newly\ndeveloped for a cloud deployment. Developers have traditionally had the luxury of\nexploring new design approaches that often leverage the unique capabilities of a specific\nand targeted technology or vendor. These new approaches are often foundational to a\ncompany's market differentiation or customer value proposition. The resultant\ninfrastructure customization or configurations are then presented to the supporting\ninfrastructure system administrators as minimum requirements or must-haves. If the\norganization is organically responsible for deployment, these minimum requirements are\ntranslated into specific technology procurement and technical configurations. If the \ndeployment is to be outsourced to a managed service provider or system integrator, the\nprocurement official translates these requirements into a request for proposal (RFP) or\nrequest for quote (RFQ). Competitive vendors then follow the specified requirements to\ndesign and present a priced solution. After technical evaluations and cost-benefit analysis\nare completed, a contract is awarded and funded for the delivery of a selected solution.\nThis traditional approach is fatally flawed if the targeted vendor is a cloud service provider.\nFirst, cloud computing environments are designed and managed as a commodity service by\nthe cloud service provider. This will typically negate most technology or configuration\ndictates from the end customer. It also prevents exploitation of most vendor- or technology-\nspecific capabilities designated as minimum requirements or must-haves by the developer.\nSecondly, the solution design is fixed by the CSP based on previously finalized technology\ndecisions and CSP-funded acquisitions. The technology service price is also dictated by the\nCSP based on its target marketplace dynamics, not the price sensitivity of any particular\ncustomer. In short, RFP or RFQ requirements have virtually zero effect on available CSP\ntechnology services or technology service price.\nCloud service acquisition fundamentally represents a 180e shift from traditional IT\nacquisition practices. This can not only wreck existing procurement process oversight and\ncontrol, but it will also lead to unexpected deployment challenges, cost, application design\nchanges, and failed migration projects. This pattern has repeatedly been observed across\nthe IT industry. Its root cause is the cloud solution architect's failure to address\nprocurement process differences between traditional IT service acquisition and cloud IT\nservice acquisition.\n\n\nArchitecting for Transition\nChapter 6\n[ 109 ]\nApplication migration\nApplication screening is the first step in identifying whether a specific application is ready\nto move into the cloud. This is normally accomplished using an interview process, with the\nsystem/application owner designed to identify cloud migration readiness and the value a\nmigration could provide. As a data discovery exercise, this process will help identify\napplications for migration, while ensuring that the existing IT and security architecture is\nwell understood, and this also helps to mitigate many complications that may occur when\nexecuting a migration strategy. Interviewers should leverage a consistent set of evaluation\ntested questions that will help triage an organization's application portfolio. Responses\nshould be analysed with a focus on deciding the following:\nThe most appropriate target deployment environment, which varies from\nphysical hardware in a user-owned and -operated data center to a virtualized\nplatform, a private cloud, or a public cloud\nEach application's SPAR benefit (scalability, performance, accessibility,\nreliability)\nEach application's SOAR readiness (security, organization, architecture\nresilience).\nThis triage effort should also highlight the most influential business or mission drivers, key\nreadiness strengths, key benefit weaknesses, and key readiness weaknesses.\nAfter identifying the applications that should be moved into the cloud, a data classification\n(PII, classified information, and so on) of the information processed by these systems\nshould be completed. This should be done with input from the relevant SMEs and the\nGovernance, Risk Management, and Compliance (GRC) team. This is an important step to\nunderstand because CSPs operate on a shared-responsibility model. The CSP will provide\nsecurity of the cloud and the customer is responsible securing the information that put in\nthe cloud. Data classification will help determine what information will remain on-\npremises and what information will be moved into the cloud, and it also helps to ensure\nthat compliance requirements can be achieved.\n\n\nArchitecting for Transition\nChapter 6\n[ 110 ]\nApplication portfolio data should then be compiled across all relevant or interrelated\ndomains. The selection of a service model (SaaS, PaaS, or IaaS) and a deployment model\n(public, community, or hybrid) should be driven by organizational goals and compliance\nneeds. It is important the think about where data will be stored, is encryption required,\nhow information will be encrypted at rest, how information will be encrypted in motion,\nand who will manage the encryption. Answers to these types of questions will inform your\nselection. Screening output should also provides data to inform long-term application\nstrategy decisions. Long-term options typically include retirement, refactoring, rebuilding,\nor lift and shift.\nApplication workloads\nApplication workload, dictated by user characteristics, represents a major factor in cloud\ncomputing solution design. Application scalability and cost efficiency are enhanced by the\nsolution architect's ability to leverage the automated provisioning and de-provisioning of\ncloud technology services. Dynamic instantiation and decommissioning of virtual machines\nbased on workload variations is often ignored or delayed when a lift and shift application\nmigration strategy is pursued. Failing to design or redesign solutions with application\nworkload variability in mind can nullify any positive return on a cloud deployment\ninvestment.\nStatic workloads\nStatic workloads show flat resource utilization over time within particular boundaries.\nApplications with static workloads are less likely to see benefit from an elastic cloud that\nuses a pay-per-use model because resource requirements are constant. Periodic workloads\nare very common in real deployments. Examples include monthly paychecks, monthly\ntelephone bills, yearly car checkups, weekly status reports, or the daily use of public\ntransport during the rush hour. Tasks occur in distinct patterns, and the customer realizes\ncost-savings from the ability to de-provision resources during non-peak times.\nOnce-in-a-lifetime workloads\nOnce-in-a-lifetime workloads are a special case of periodic workload in which the peaks of\nperiodic utilization occurs only once in a very long time frame. This peak is often known in\nadvance as it correlates to an individual event or task. Cloud elasticity is used to obtain\nnecessary IT resources. The provisioning and decommissioning of IT resources in this use\ncase can often be done manually since it can be done at a known point in time.\n\n\nArchitecting for Transition\nChapter 6\n[ 111 ]\nUnpredictable and random workloads\nUnpredictable and random workloads are a generalization of periodic workloads as they\nrequire elasticity but are not predictable. Unplanned provisioning and de-provisioning of\nIT resources is required. The necessary provisioning and decommissioning of IT resources\nare, therefore, automated to align the resource numbers to the changing workload. Many\napplications experience a long-term change in workload that can be characterized as a\ncontinuously changing workload. This is normally seen as an ongoing continuous growth\nor decline of use. The elasticity of clouds enables applications experiencing continuously\nchanging workloads to provision or decommission resources with the same rate as the\nworkload changes.\nThe following table provides a cross-reference between the solution use cases outlined in\n$IBQUFS\u0002\u0016, Business Drivers, Metrics, and Use Cases, application workloads described here,\nand the standard operational requirements also addressed earlier. Once application user\nrequirements are established, the table can be used to develop a documented initial draft of\nyour cloud solution operational requirements:\n\n\nArchitecting for Transition\nChapter 6\n[ 112 ]\nApplication categories\nThe more value an application or business process can exploit from the basic cloud model\nvalue proposition, the more valuable a cloud transition becomes. This fact emphasizes the\nneed to categorize enterprise applications by important organizational goals and target\ndeployment options. Several factors can be used to categorize applications. This\ncategorization often drives the selection of an appropriate deployment model for an\napplication being moved to the cloud. These factors include security privacy regulation\nneeds, unique technology requirements, agility, and elasticity needs. The process of\ncategorizing applications is unique for each enterprise, but the cloud solution architect\nshould educate, lead, and advise throughout the process:\n\n\nArchitecting for Transition\nChapter 6\n[ 113 ]\nUse of a categorization framework can form the basis for establishing a structured approach\nfor assessing a given application's overall relative cloud migration value against the ease or\ndifficulty in making that migration. It will also identify how valuable essential cloud\ncharacteristics are to the application's operational value:\nIn this example diagram, circle size represents overall cloud affinity. Cloud migration pain\nis affected by several factors, such as application dependencies, application cloud\nfriendliness, associated security and compliance requirements, and other factors. The gain\nis derived from scalability, agility, elasticity, and overall organizational cloud transition\nmotivation. Some of the most popular SaaS application categories are shown in the\nfollowing table:\nSaaS application\nDescription\nCustomer relationship\nmanagement\nAutomate marketing and track sales\nEnterprise resource management\nImprove workflow and productivity visibility\nAccounting\nAccurately organize and track finances\n\n\nArchitecting for Transition\nChapter 6\n[ 114 ]\nProject management\nTrack project scope, requirements, progress, changes,\ncommunications, and delivery deadlines\nEmail marketing\nAutomate and optimize marketing and relationship\nbuilding\nBilling and invoicing\nReduce billing process time\nCollaboration\nImprove communications and employee productivity\nWeb hosting and electronic\ncommerce (e -commerce)\nAutomate internet-based business processes\nHuman resource management\nMore efficient scheduling, payroll automation, and\nrecruiting\nPublic sector, compliance, and EDI Monitor and enforce regulations to improve\ncompliance and communications\nIndustry vertical applications\nIndustry-specific application deployment\nFinancial transaction processing\nExchange of financial assets\nApplication dependencies\nApplication dependencies increase the difficulty associated with putting an application on\nthe cloud. They can dictate application migration order or even determine migration\nfeasibility. Critical application dependencies could include the following:\nShared communication channel\nShared architecture\nIdentity and access management\nShared data\nThe solution architect must explore and gain a consensus on the most appropriate\nresolution options, which may include the following:\nCreating and deploying a shared service layer\nReplicating the service in the cloud environment\nReplacing legacy services with an available cloud services.\n\n\nArchitecting for Transition\nChapter 6\n[ 115 ]\nUse of APIs\nApplication APIs are the glue that connects applications. They manage the virtual\ndiscussions between users and the cloud services being consumed. APIs enable business\nagility, flexibility, and interoperability. These software modules are more than just\nconnective tissue on the web, they are business model drivers and represent organizational\ncore assets that can be reused, shared, and monetized. Using APIs, companies extend the\nreach of existing services or provide new revenue streams. In some instances, they are\nactually end products that, in turn, provide access legacy and third-party systems and data.\nInfrastructure APIs are used to provision, de-provision, and scale cloud computing\nresources. As crucial solution components, the cloud solution architecting process should\nalso consider the following:\nCreation and publication of a service endpoint as an API\nDeployment of APIs on-premises or in the cloud\nUse of versioning to control APIs throughout the solution life cycle\nManagement and monitoring of solution-related web services\nThe predominant API design styles are Simple Object Access Protocol (SOAP) and\nRepresentational State Transfer (REST).\nSOAP\nSOAP uses eXtensible Markup Language (XML), which is very complex when used for\nrequests and responses. Requests are often created manually, leading to fragile applications\ndue to the intolerance of SOAP to errors. Web Services Description Language (WSDL) is\nused with SOAP to define web service usage. With WSDL, the Integrated Development\nEnvironment (IDE) can fully automate the process. Because of this connection, the\ndifficulty in using SOAP depends on the programming language. An important SOAP\nfeature is its error handling. If a request has an error, the response embeds information that\ncan be used to correct the problem. Error reporting also has standard codes that can be used\nto automate error handling.\n\n\nArchitecting for Transition\nChapter 6\n[ 116 ]\nREST\nREST is a lightweight SOAP alternative. It uses a simple URL and can process four different\ntasks (GET, POST, PUT, and DELETE). REST flexibility lies in its ability to return data using\nJavaScript Object Notation (JSON), Comma Separated Value (CSV), and Really Simple\nSyndication (RSS). This means output can be delivered in just about any desired parsing\nformat.\nAdvantages of SOAP and REST\nThe advantages of SOAP and REST are given in the following table:\nSOAP advantages\nREST advantages\nPlatform, language, and transport\nindependence\nNo expensive tools are required to interact\nwith the web service\nBetter support for distributed enterprise\nenvironments\nSmaller learning curve\nBetter standardization\nEfficient (SOAP uses XML for all messages,\nREST can use smaller message formats)\nSignificant pre-built extensibility sing WS*\nstandards\nFast (no extensive processing required)\nBuilt-in error handling\nCloser to other web technologies in design\nphilosophy\nMore automation with certain languages\n\n\nArchitecting for Transition\nChapter 6\n[ 117 ]\nTechnical architecture requirements\nCloud computing services are consumed using an on-demand model. Service providers,\ntherefore, meter and monitor the consumption of every resource by every user. Every\nresource, in turn, uses specific metrics and measurement units to bill users. Most\norganizations do not measure IT usage in this way. Neither do they typically place resource\nmeasurement sensors across their IT platform. IT is usually seen as a shared cost center\nwith only the total cost and total capacity requirements tracked. This makes it extremely\ndifficult to estimate service usage rates and expected cost when an application is\ntransitioned to a CSP. Infrastructure analysis is used to get estimates of resource usage by\nparticular applications, business processes, and organizational segments. This is normally\none of the most difficult aspects of developing the business case and resultant cost-benefit\nanalysis. Responses provide insight into the economic comparison between IaaS service\noptions, managed infrastructure service options, and enterprise-owned data center options.\nLegal/regulatory/security requirements\nAs the global nature of technology continues to evolve, the complexity of adhering to both\nglobal and local laws and regulations becomes greater. The most significant trend in the US\ngovernment market is the move from security compliance to security risk management.\nRelevant guidance addressing this move is contained in the following:\nThe Federal Risk and Authorization Management Program (FedRAMP), which\nprovides a standard approach for security assessments, authorizations, and\ncontinuous monitoring of cloud computing products and services.\nThe Department of Defense (DoD) cloud computing security requirements,\nwhich extend the FedRAMP security requirements to meet the unique\nrequirements of the DoD.\nICD 503, which replaces DCID 6/3 and 6/5 and establishes intelligence\ncommunity policies for security risk management of information technology\nsystems. This includes security certification and accreditation.\nEnsuring compliance with these is challenging within a cloud computing environment\nincreases significantly. Legal, regulatory, and security requirements are used to screen\ncloud service provider options, select appropriate CSP regions, and verify necessary\nsecurity controls.\n\n\nArchitecting for Transition\nChapter 6\n[ 118 ]\nBusiness continuity and disaster recovery `\nBCDR\nCloud infrastructure has some characteristics that can be distinct advantages in realizing\nBCDR, depending on the scenario:\nRapid elasticity and on-demand self-service provides flexible infrastructure that\ncan be quickly deployed to execute an actual disaster recovery without\nunexpected ceilings.\nBroad network connectivity reduces operational risk.\nCloud infrastructure providers have a highly automated and resilient\ninfrastructure backing all offered services.\nPay-per-use can mean tremendous cost savings and no capital expenditures to\nsupport a BCDR strategy.\nScenarios that should be considered when considering cloud services are the following:\nOn-premises, cloud as BCDR: An existing, on-premises infrastructure, which\nmay or may not have a BCDR plan already, where a cloud provider is considered\nas the provider of alternative facilities should a disaster strike at the on-premises\ninfrastructure.\nCloud consumer, primary provider BCDR: The infrastructure under\nconsideration is already located at a cloud provider. The risk being considered is\nthat of a failure of part of the infrastructure of that cloud provider, for example,\none of their regions or availability zones. The business continuity strategy then\nfocuses on restoration of service or failover to another part of that same cloud\nprovider infrastructure.\nCloud consumer, alternative provider BCDR: A scenario similar the previous\none except instead of the restoration of service from the same provider, the\nservice has to be restored from a different provider. This also addresses the risk\nof complete cloud provider failure.\nDisaster recovery (DR) almost by definition requires replication. The key difference\nbetween these scenarios is where the replication happens. Business continuity (BC)\nquestions are used to identify any critical BC or DR issues that may require more detailed\nanalysis.\n\n\nArchitecting for Transition\nChapter 6\n[ 119 ]\nEconomics\nCloud service providers typically offer three options to pay for their service:\nOn-demand: You pay for what you use as you use it.\nReserved: You commit to using a specific amount of a service over a specified\nperiod.\nSpot: You use a market auction model to match price with demand.\nEach has advantages and disadvantages, but all require an understanding of operational\nrequirements and customer sensitivity to price fluctuations. Economic screening questions\nare used to gauge customer sensitivity to cloud service cost and importance of the various\neconomic payment models.\nOrganizational assessment\nDigital transformation and cloud computing migrations typically involve transitioning\nfrom a non-standardized, minimally documented environment into a highly standardized,\nrigidly documented one. This is a highly challenging transition that requires effective\norganizational governance. It may also present significant change management challenges.\nTo be successful, any corporate cloud computing transition strategy needs to be paired with\na focused training and education program. This assessment is designed to identify whether\nsuch a program is in place or, if not, identify the appropriate organizational POC.\nOrganizational governance defines organizational structures, decision rights, workflow,\nand authorization points to create a target workflow that optimally uses a business entity's\nresources in alignment with the goals and objectives of the firm. Effective governance can\nonly succeed if the organization has defined the desired outcomes and the proper metrics.\nOrganizational leadership and management must be able to articulate what the desired\noutcomes are, who is accountable and responsible for these outcomes, what the escalation\ncriteria or triggers to move a decision to the next level are, and what metrics will be used to\nmonitor that the system is delivering the desired outcomes. The entire governance process\nneeds to be continuously evaluated to determine that it provided the necessary\ntransparency and timeliness to the decision makers and adjusted accordingly.\n\n\nArchitecting for Transition\nChapter 6\n[ 120 ]\nDuring transitions, the organization must simultaneously abandon business as usual and\nembrace the following across multiple dimensions:\nSecurity framework: Infrastructure-centric to data-centric\nApplication development: Tightly coupled to loosely coupled\nData: Mostly structured to mostly unstructured\nBusiness processes: Mostly serial to mostly parallel\nSecurity controls: Enterprise responsibility to shared responsibility\nEconomic model: Mostly CAPEX to mostly OPEX\nInfrastructure: Mostly physical to mostly virtual\nIT operations: Mostly manual to mostly automated\nTechnology operational scope: Local/regional to international/global:\nBefore embarking on any cloud transition program, the enterprise should implement a\nfocused organizational change management strategy. There needs to be broad awareness,\nunderstanding, acceptance, and commitment across the organization of what is expected\nand when is it to be delivered by implanting focused organizational change management\nprocesses and procedures. The organization should regularly ask, is the corporate culture\nchanging at the necessary rate and are all communication channels are being leveraged so that the\nright stuff is being communicated?\n\n\nArchitecting for Transition\nChapter 6\n[ 121 ]\nSummary\nCloud computing is transformational in many ways. Cloud transformation can have a\nripple effect throughout the entire organization. The hardest part of any transformational\nstrategy is changing the hearts and minds of the people in the organization. Proper\nplanning, diligent preparation, situational awareness, and environmental control will create\na solid foundation for transformation success. Pairing solution strategy, economics,\ntechnology choices, and risk profiles with matching cloud service providers will maximize\ncloud transition value. Change management and communication plans are critical during\nany major change. Leadership continuously measures and compares accurate, relevant\nmetrics and data, gauging progress, adoption, potential risks, and the pace of cultural\nchange. If progress slows, adoption may follow. If adoption slows, project timelines may\nslide. Stay environmentally and situationally aware. Continuously monitor, measure, and\ncompare the current situation to transition goals. Preparation is key. Avoid being a\ndefeated warrior going into battle seeking a win.\n\n\n7\nBaseline Cloud Architectures\nCloud transitions can be difficult to begin. As discussed in $IBQUFS\u0002\u0018, Architecting for\nTransition, transitions can be difficult to design and plan, as much of the diligence now falls\non the consumer side. This change is a double-edged sword; it cuts both ways. It enables\nthe consumer to have significantly more control over designs, technical choices, economics,\nand risk. It also places the significantly more of the design and architecture burden on the\nconsumer, who may not have the level of solution design experience that many service\nproviders do.\nBaseline cloud architectures are foundational building blocks to cornerstone design ideas.\nThese common design arrangements can be used to jump-start solution efforts. Baseline\narchitectures are useful when leveraging standard cloud computing patterns. Patterns\nrepresent cloud service requirements, while baseline architectures provide useful models\nfor handling common architectural components and their associated requirements.\nEach of the following sections will build on the section previous. The baseline compute\ncomponent takes into account a web layer, application layer, and database layer, each\nhaving some level of storage. Storage attributes will change based on design requirements.\nNearly all modern designs will have web, app, and database layers in their designs.\nThis type of layering is called tiering. Most designs will have three or four tiers. Tiers are\ntypically the number of individual isolated layers between the environment entry point and\nthe destination data. As an example, a three-tier architecture has a web layer, app layer, and\ndatabase layer. A single-server architecture will have all three layers residing on the same\nvirtual or physical server.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 123 ]\nIn this chapter, we will cover the following topics:\nBaseline architecture types\nOSI model and layer description\nComplex architecture types\nArchitecting for hybrid clouds\nBaseline architecture types\nThe various types of baseline architectures are as follows.\nSingle server\nSingle server templates represent the use of one server, virtual or physical, that contains a\nweb server, an application, and a database. An example is the LAMP Stack (Linux,\nApache, MySQL, PHP). Single server architectures are not very common, as they have\ninherent security risks as one compromise can compromise all. These architectures are\ncommonly deployed for development work, allowing developers to quickly build\nfunctionality without having to deal with connectivity and communication issues between\ndifferent servers, potentially in different locations.\n",
      "page_number": 116
    },
    {
      "number": 7,
      "title": "[ 123 ]",
      "start_page": 137,
      "end_page": 157,
      "detection_method": "regex_chapter",
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 124 ]\nSingle-site\nSingle-site architectures take the single server architecture and split all of the layers into\ntheir own compute instances, creating the three-tier architecture mentioned. With all\ncompute resources located in the same location, a single-site architecture is created. There\nare two versions of single-site architectures: non- redundant and redundant.\nNon-redundant three-tier architectures\nNon-redundant three-tier architectures are used to save on costs and resources but must\naccept a higher risk. A single failure in any component, a single point of failure, can stop\ntraffic flowing correctly into or out of the environment. This approach is commonly used\nfor development or testing environments only. The following figure shows each layer, or\ntier, as a separate server, virtual or physical. Using this type of design for production\nenvironments is not recommended:\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 125 ]\nRedundant three-tier architectures\nRedundant three-tier architectures add another set of the same components for\nredundancy. Additional design components do increase complexity, but are required if\ndesigning for failover and recovery protection. Designing redundant infrastructures\nrequires a well thought out plan for the components within each layer (horizontal scaling),\nas well as a plan for how the traffic will flow from one layer to another (vertical scaling).\n4JOHMF\u0002QPJOUT\u0002PG\u0002GBJMVSF\nIn redundant architectures, duplicate components eliminate the single point of failure\npresent when only one device or component is present in the layer. With one component in\na layer, there is only one way in and one way out. A second device adds multiple ingress\nand egress points to the design, eliminating the single point of failure associated with\nsingle-component layer designs.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 126 ]\n3FEVOEBODZ\u0002WFSTVT\u0002SFTJMJFODZ\nRedundancy and resiliency are often confused. They are related, but not interchangeable.\nRedundancy is something that is done to prevent failure, implying that it happens before\nan issue happens. Resiliency, from the word resolve, relates to how to find solutions after a\nproblem has occurred. Redundancy is before the issue. Resiliency is after. For example,\nredundant databases with replication can be utilized. Multiple components and copies of\ndata create a redundant design. If the primary side of the database pair fails, the secondary\nside will promote to primary and begin to pick up the load while the failed side self-repairs.\nThe failover and self-healing functions are resiliency. Both are related, but not\ninterchangeable.\n)PSJ[POUBM\u0002TDBMJOH\nWorking outside in, the XYZ website has a single web server. A recent outage has suddenly\nidentified available budget money for redundant components at each layer of the current\ndesign. By the way, every company is one major outage away from adding budget money\nfor redundancy plans. One web server is currently used in the design. To add redundancy,\nwe must horizontally scale the web server layer by adding additional web servers,\neliminating the single point of failure. How will traffic be passed to both servers? How does\nthe packet on the wire know which web server to go to and which path to take in and out,\nand how is all of this physically connected?\nLoad balancing is a major design component when adding redundancy to designs. A single\nload balancer will help delegate traffic across multiple servers, but a single load balancer\ncreates another single point of failure. For redundancy, two or more load balancers are\nadded to designs. Load balancers control traffic patterns. There are many interesting\nconfigurations to consider when deciding how to control and distribute traffic. Distribution\nmay relate to traffic type, content, traffic patterns, or the ability of the servers to respond to\nrequests. Load balancers help to handle traffic logically; how is the traffic handled at the\nphysical layer?\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 127 ]\nOSI model and layer description\nThe OSI stack is a great tool when working with complex designs. Every layer in the OSI\nstack must be considered within the design and have a purposeful answer. Designs always\nstart at the physical layer, working up the stack from the bottom to the top. See the\nfollowing diagram. Many load balancers today work at all layers of the OSI stack. Back to\nthe question: how are multiple load balancers physically connected to multiple servers\ncreating multiple ingress and egress paths? Multiple switches may also be required. Today\nmany load balancers combine the port density of switches, the routing capability of routers,\nand the logical functions of load balancers, all in a single device simplifying designs and\nsaving a bit of budget money.\nThe web layer and application layers can often be collapsed into the same server. From a\nsecurity perspective, this can be an issue. If the server is compromised, both services are\npotentially compromised. Many designs collapse these two layers, as they are tightly\nintegrated, and performance can significantly increase using system bus speeds instead of\nslower network connections and additional devices.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 128 ]\nFrom single server designs to single site to single site redundant, each design builds on the\none previous. The following figure adds the additional components, servers, and load\nbalancers to illustrate a baseline architecture for single site designs with redundancy. The\nfollowing redundant design collapses both web and app onto the same virtual or physical\nserver. Load balancers are added to the design to delegate the load across multiple servers.\nDatabase servers are shown as primary-backup with replication between them. This\nredundant architecture can protect against issues with applications due to system\nunavailability and downtime. Resiliency considerations may include RAID configurations\nfor database drives, how databases are backed up and restored, how applications and\ndevices handle state and session information, and how databases rebuild after data or drive\nloss.\nLogical and physical designs\nDesigns can be logical or physical. It is very important to remain clear on what is\nrepresented. Logical diagrams illustrate how things logically flow through the design.\nEliminating some of the physical connections may help the viewer focus on logical flows\nthrough the design. Conversely, physical layouts may not include many of the logical\ndetails and configurations to focus the viewer on physical characteristics and attributes of\nthe design. The illustrations in this section are logical unless specifically called out as\nphysical.\nAutoscaling architecture\nA key benefit of cloud computing is the ability to consume what is needed when it is\nneeded. Autoscaling describes the ability to scale horizontally (that is, shrink or grow the\nnumber of running server instances) as application user demand changes over time.\nAutoscaling is often utilized within web/app tiers within the baseline architectures\nmentioned. In the following figure, an additional server is dynamically added based on\ndemand and threshold settings.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 129 ]\nLoad balancers must be preconfigured or configured dynamically to handle the new\nservers that are added.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 130 ]\nComplex architecture types\nThe various types of complex architectures are as follows.\nMulti-data center architecture\nRedundant single site designs can often handle many of the more prevalent issues that\ncause downtime within infrastructure layers. What happens if the entire site is not\nreachable? What happens if something gets misconfigured in DNS that sends traffic the\nwrong direction? The single site is now unreachable. Traditionally, the answer to this\nchallenge has been very expensive. The redundant single site design nearly doubled the\ncost of infrastructure. For geographic redundancy, a second site is needed. This second site\neffectively doubles the budget of the first site, which already doubled when redundancy\nwas added to the first design.\nCloud solutions are dramatically changing the way we design redundancy, resiliency, and\ndisaster recovery. The cloud changes the fundamentals of base designs. For example, we\nare now able to design for low-side, base-level traffic flows, instead of designing for\nanticipated high-watermark levels. The cloud enables dramatic changes in footprint size\nand the amount of redundant infrastructure needed in single site and multiple site designs.\nThe cloud is also changing the consumption patterns for infrastructure. Some applications\ntraditionally deployed in-house have transformed to SaaS offerings, eliminating the need\nfor the associated in-house infrastructure. The reduction in single site footprints can also\nreduce the size of second site footprints, helping redundant strategies fit into budgets easier\nthan traditional deployments.\nWhen planning redundancy across multiple data centers, new design challenges need\nconsideration. How is traffic sent to one location or the other? Is one site active and one\nbackup? Are both active? How does fail-back to the primary get handled after the failure\noccurs? What changes in resiliency plans are needed? How is data synchronization handled\nbefore and after failover?\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 131 ]\nGlobal server load balancing\nThere are many mechanisms to handle the flow of traffic between multiple sites. Nearly all\nof them rely on the manipulation of DNS information. DNS information can sometimes\ntake hours to update across the globe. If production sites must failover to redundant sites,\nwaiting hours for traffic to pass again is not an option. Global server load balancing\nenabled the configuration of pre-planned actions to take place in the event of failure. GSLB\nrequired expensive publicly accessible devices at each site. Security experts were also\nrequired as part of a successful solution to keep devices safe from continuous hacking\nattempts.\nExpensive, traditional, device-based GSLB deployments can be deployed as cloud GSLB\nservices, where GSLB is consumed as a managed service for a monthly fee. Providers are\nalso offering additional options including regional deployments and separated availability\nzones to help handle geographic diversity and failover. It is up to the consumer to decide\nthe level of redundancy and speed of failover required. Zone level redundancy is different\nthan regional deployments.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 132 ]\nDatabase resiliency\nPrimary-secondary or master-slave database relationships are common, but have some\nchallenges when failures occur in high-transaction, heavy traffic environments. Databases\nare taking lots of requests and transactions are being written and read continuously.\nBackup processes can be taxing and time-consuming. Restoration and synchronization can\ntake significant time. Heavy demand environments can benefit from an active-active \ndatabase configuration with bi-direction replication to keep data synchronized on both\ndatabase servers. This type of design does add more complexity but also adds greater levels\nof redundancy and resiliency within a single site, or across multiple sites, depending on\nconfiguration.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 133 ]\nCaching and databases\nContent types can also affect architectures. As an example, caching techniques can change\nthe load on database servers, load balancing design, database server sizing, storage type,\nstorage speed, how storage is handled and replicated, as well as network connectivity, and\nbandwidth requirements. Current estimates place 80%-90% of enterprise data in\nunstructured categories.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 134 ]\nAlert-based and queue-based scalable setup\nSince multiple server arrays can be attached to the same deployment, a dual scalable\narchitecture can be implemented. This delivers a scalable front-end and back-end server\nwebsite array.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 135 ]\nHybrid cloud site architectures\nA hybrid cloud site architecture can protect your application or site redundancy by\nleveraging multiple public/private cloud infrastructures or dedicated hosted servers. This\nwill require data and infrastructure portability between selected service providers. A\nhybrid approach requires an ability to launch identically functioning servers into multiple\npublic/private clouds. This architecture can be used to avoid cloud service provider lock-in.\nIt is also used to take advantage of multiple cloud resource pools. The hybrid approach can\nbe used in both hybrid cloud and hybrid IT situations.\nScalable multi-cloud architecture\nA multi-cloud architecture, offers the flexibility of primarily hosting an application in a\nprivate cloud infrastructure, with the ability to cloudburst into a public cloud for additional\ncapacity as necessary.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 136 ]\nFailover multi-cloud architecture\nA second cloud service provider could be used to provide business continuity for a primary\ncloud provider if the same server templates and scripts could be used to configure and\nlaunch resources into either provider. Factors to be considered when using this option\ninclude public versus private IP addresses and provider service level agreements. If there is\na problem or failure requires switching clouds, a multi-cloud architecture would make this\na relatively easy migration.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 137 ]\nSending and receiving data securely between servers on two different cloud service\nprovider platforms can be done using a VPN wrapped around the public IP address. In this\napproach, any data transmitted between the various cloud infrastructures (except if used\nbetween private clouds) is sent over the public IP. In the following diagram, two different\nclouds are connected using an encrypted VPN:\nCloud and dedicated hosting architecture\nHybrid cloud solutions can use public and private cloud resources as a supplement for\ninternal or external data center servers. This can be used to comply with data physical\nlocation requirements. If the database cannot be transitioned to a cloud computing\nplatform, other application tiers may not have the same restrictions. In these situations,\nhybrid architecture can use a virtual private network (VPN) to implement an encrypted\ntunnel across a public IP between cloud and dedicated servers.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 138 ]\nArchitecting for hybrid clouds\nThe various concepts of a hybrid cloud are explained in the following sections.\nHybrid user interface\nVarying user group workload interacts asynchronously with an application hosted in an\nelastic environment while the rest of the application resides in a static environment. An\napplication responds to user groups with different workload behavior. One user group\npresents a static workload, while the other user group presents periodic, once-in-a-lifetime,\nunpredictable, or continuously changing workloads. Since user group size and workload\nbehavior is unpredictable, this interface ensures that unexpected peak workloads do not\naffect application performance while each user group is handled by the most suitable\nenvironment. The user interface component serving varying workload users is hosted in an\nelastic cloud environment. Other application components that are in a static environment.\nThe user interface in the elastic cloud is integrated with the rest of the application in a\ndecoupled manner using messaging to ensure loose coupling.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 139 ]\nHybrid processing\nProcessing functionality with the varying workload is in an elastic cloud while the\nremainder of the application is in a static environment. A distributed application provides\nprocessing functions with different workload behavior. The user group accessing the\napplication is predictable in size but accesses the functions differently. Although most\nfunctions are used equally and experience static workload, some processing components\nexperience periodic, unpredictable, or continuously changing workloads. The processing\ncomponents with varying workloads are provisioned in an elastic cloud. Loose coupling is\nensured by asynchronously exchanging information between the hosting environments via\nmessages.\nHybrid data\nData of varying size is in an elastic cloud while the rest of an application is in a static\nenvironment. A distributed application handles data with drastically varying size. Large\namounts of data may be periodically generated and then deleted, data may increase and\ndecrease randomly, or data may display a general increase or decrease. During these\nchanges, the user number and application accesses can be static resulting in a static\nworkload on the other application components. Elastic cloud storage offerings handle data\nwith varying size that are unsuitable for static environment hosting. Data is accessed either\nby data access components hosted in the static environment or by data access components\nin the elastic environment.\nHybrid backup\nFor disaster recovery, data is periodically extracted from an application and archived in an\nelastic cloud. Requirements regarding business resiliency and business continuity are\nchallenging. There are also laws and regulations that make businesses liable to archive data\nfor audits over very long periods of time. A distributed application is in a local static\nenvironment. Data handled by stateful components is extracted periodically and replicated\nto cloud storage.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 140 ]\nHybrid backend\nBackend functionality is made up of data-intensive processing and data storage with\nvarying workloads is hosted in an elastic cloud while all other components reside in a static\ndata center. A distributed application provides processing with different workload\nbehaviors. Support for a mainly static workload needs to available, but some processing\ncomponents experience periodic, unpredictable, or continuously changing workloads.\nApplication components that have varying workloads should be in an elastic environment.\nThese components, however, need to access large amounts of data during execution\nmaking them very dependent on availability and timely access to data. The processing\ncomponents with varying workloads are in an elastic cloud together with the data accessed\nduring operation. Asynchronous messages exchanged from the static environment are used\nto trigger the processing components in the elastic cloud through via message-oriented\nmiddleware message queues. A static environment data access component ensures that\ndata required by elastic processing components is in storage offerings The data location\nmay then be passed to the elastic processing components via messages. Data not required\nby the backend functionality may still be stored in stateful components in the static data\ncenter.\nHybrid application functions\nSome application functions provided by user interfaces, processing, and data handling is\nexperienced varying workload and is in an elastic cloud while other application functions\nof the same type are in a static environment. Distributed application components\nexperience varying workloads on all layers of the application stack: user interface,\nprocessing, and data access. All components provide functionality to the application user\ngroup, but user groups access functionality differently. In addition to the workload\nrequirements, other issues may limit the environments to which an application component\nmay be provisioned. Application components are grouped based on similar requirements\nand are deployed into the best fitting environments. Components interdependencies are\nreduced by exchanging data with asynchronous messaging to ensure loose coupling.\nDepending on the function accessed, a load balancer seamlessly redirects user accesses to\nthe different environments.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 141 ]\nHybrid multimedia web application\nWebsite content is primarily provided from a static environment. Multimedia files that\ncannot be cached efficiently are provided from a large distributed elastic environment for\nhigh-performance access. A distributed application provides website access to a globally\ndistributed user group. While most of the website has static content, there is a significant\namount of multimedia content that needs to be streamed to users. Static website content is\nin a static environment where users access it. The streaming content is in an elastic cloud\nenvironment where it is accessed from a user interface component. Static content is\ndelivered to users' client software which references the multimedia content. Streaming\ncontent retrieval is often handled directly by the users' browser software.\nHybrid development environment\nA runtime environment for production is replicated and mocked in an elastic environment\nfor new applications development and testing. Applications have different runtime\nenvironment requirements during the development, testing, and production phases.\nDuring development, hardware requirements vary, so hardware resources need to be\nflexible and able to extend resources as necessary. During the test phase, diverse test\nsystems are needed in order to verify proper application functionality on various operating\nsystems or while being accessed with different client software. Large numbers of resources\nare also required for load tests. In production, other factors, such as security and\navailability are of greater importance than resource flexibility. The application production\nenvironment is simulated in the development and test environment using equivalent\naddressing, similar data, and equivalent functionality. Applications migration of is ensured\nthrough the transformation of application components or the compatibility of runtimes.\nSome testing resources are exclusively provided in the development environment to verify\nthe application behavior under different circumstances.\n\n\nBaseline Cloud Architectures\nChapter 7\n[ 142 ]\nEach pattern employs certain characteristics and attributes. These help solution architects\naccurately visualize interoperability, and models, and compare the impact of economics,\ntechnology choices, and potential strategies. Pattern attributes and their associated metrics\ncan also be used to models and test solutions using computing aided design tools. Aligning\npattern characteristics, attributes, and metrics with organizational requirements and goals\nwill normally lead to successful solution deployments.\nSummary\nSuccessful design requires a simultaneous balance between desired strategic, economic,\ntechnical, and risk attributes. Complex designs are not necessarily better and can introduce\nadditional risk rather than mitigate it. Defined requirements are where design starts, not\nwhere it finishes. As architectures are designed, evaluated, and compared, insight is\nrevealed. Insight often provides a feedback loop for requirements to update or change.\nUpdated requirements lead to new design scenarios and, potentially, more insight. When\nstrategy, economics, technology, and risk align, objections will subside or will be negotiated\naway. Only add design complexity if non-negotiable requirements dictate it.\n\n\n8\nSolution Reference\nArchitectures\nThe reference architecture summaries presented in this chapter are meant to be starting\npoints for your solution design. They outline the minimum components and processes to\naddress the topic requirement but do not recommend specific technologies or cloud vendor\nsolutions. They also cannot possibly address any unique organizational requirements or\nconcerns. Real, deployable solutions require the addition of actual enterprise requirements,\na solution architect's insight, modifications driven by the selected cloud service provider,\nand organizational team collaboration.\nThese summaries were created from complete reference architectures developed by the\nCloud Standards Customer Council (CSCC\nTM). The CSCC\nTM is dedicated to accelerating\nthe successful adoption of cloud computing. It provides users with the opportunity to drive\nrequirements into standard development organizations and deliver materials that assist\nother enterprises. Complete reference architectures are available online for zero cost at\nIUUQ\u001c\u0011\u0011XXX\u0010DMPVE\u000fDPVODJM\u0010PSH\u0011SFTPVSDF\u000fIVC\u0010IUN.\nIn this chapter, we will cover the following topics:\nApplication Security\nWeb application hosting\nPublic network\nAPI management\nE-commerce\nBig data and analytics\nBlockchain\nArchitecture for IoT\nArchitecture for hybrid integration\n\n\nSolution Reference Architectures\nChapter 8\n[ 144 ]\nApplication security\nThis reference architecture summary presents the key components needed to secure any\napplication or process in a cloud service provider's environment. Cloud service usage\nrequires a clear understanding of security services, components, and options. This\nknowledge is paired with a clear architecture which covers development, deployment, and\noperations, as depicted in the following diagram:\n(KIWTG\u0003\u0014\u001d\u0003#TEJKVGEVWTG\u0003HQT\u0003VJG\u0003UGEWTKV[\u0003QH\u0003ENQWF\u0003UGTXKEG\u0003UQNWVKQPU\nFigure 1 is a high-level architecture for the roles and components needed in the security\narchitecture for cloud service solutions. The solution is divided into three domains based\non the applicable network. These networks are normally separately secured: public\nnetwork, cloud provider network, and the enterprise network.\n",
      "page_number": 137
    },
    {
      "number": 8,
      "title": "[ 144 ]",
      "start_page": 158,
      "end_page": 204,
      "detection_method": "regex_chapter",
      "content": "Solution Reference Architectures\nChapter 8\n[ 145 ]\nThe public network (typically the internet) includes the parties that interact with the cloud\nsolution, their end user devices, and the associated applications.\nFigure 1 also shows three main roles: application users, cloud administrators, and cloud\ndevelopers.\nThe enterprise network contains the existing (non-cloud) enterprise components. These are\nusually required by the cloud solution and include the user directory, the applications, and\nthe data systems.\nThe cloud provider network contains the major components of the cloud-based solution,\nrunning in cloud servicescthe cloud applications, the data services, the runtime services,\nand the infrastructure services. Security services are associated with these components (the\nnumbers correspond to the numbers in Figure 1):\nIdentity and access management: Manage identity and access for your cloud\n1.\nadministrators, application developers, and application users.\nInfrastructure security: Handle network security, secure connectivity, and secure\n2.\ncompute infrastructure.\nApplication security: Address application threats, security measures, and\n3.\nvulnerabilities.\nData security: Discover, categorize, and protect data and information assets,\n4.\nincluding protection of data at rest and in transit.\nSecure DevOps: Securely acquire, develop, deploy, and maintain cloud services,\n5.\napplications, and infrastructure.\nSecurity monitoring and vulnerability: Provide visibility into cloud\n6.\ninfrastructure, data, and applications in real time and manage security incidents.\nSecurity governance, risk, and compliance: Maintain security policy, audit, and\n7.\ncompliance measures, meeting corporate policies, solution-specific regulations,\nand governing laws.\n\n\nSolution Reference Architectures\nChapter 8\n[ 146 ]\nWeb application hosting\nThe web application-hosting architecture delivers web pages that contain static and\ndynamic content using HTTP or HTTPS. Static content uses standardized web page text\nwith specialized content held in document, image, video, and sound clip files. Dynamic\ncontent is created in real time based on visitor input. The response is based on the request\nand content derived from linked databases. The core component is the web application\nserver. Other components can include life cycle management, operations management, and\ngovernance:\n(KIWTG\u0003\u0015\u001d\u00039GD\u0003CRRNKECVKQP\u0003JQUVKPI\u0003ENQWF\u0003CTEJKVGEVWTG\n\n\nSolution Reference Architectures\nChapter 8\n[ 147 ]\nPublic network\nPublic network components contain users and edge services. Users can interact with the\nweb application using various devices and systems. Edge services have the capability to\ndeliver application content and normally include the firewall, DNS server, load balancer,\nand content delivery network (CDN).\nCloud provider network components\nThe cloud provider network components are as follows:\nWeb service tier\nThe CSP normally hosts the web services tier. This tier holds the program logic for\ngenerating dynamic web content. Web and application servers can also be deployed in a\nthree-tiered design. This uses load balancers to connect separate pools of web and\napplication servers. Other components include file repository, web application servers, user\ndirectory, and cache.\nAPI management: API management presents all available service endpoints\nneeded for access to the application. Services can include security, scalability,\ncomposition, access, governance, analytics, deployment, and management.\nTransformation and connectivity: Transformation and connectivity ensures\nsecure connection to legacy enterprise systems. It can also filter, aggregate, or\nmodify data as it transitions between web components and legacy enterprise\nsystems. In this reference architecture, the transformation and connectivity\ncomponent are located between the web and enterprise tiers and can include\nenterprise data connectivity, data transformation, and enterprise secure\nconnectivity.\nEnterprise network components\nEnterprises normally host many applications that typically deliver critical business\nsolutions. This is done in conjunction with providing infrastructure support. Applications\nalso have data sources that are extracted and integrated with cloud-based services. Analysis\nis performed in the cloud, and output is delivered to on-premises systems.\n\n\nSolution Reference Architectures\nChapter 8\n[ 148 ]\nService tier\nThe service tier holds the enterprise applications, enterprise user directory, and enterprise\ndata. The enterprise user directory provides storage for and access to user information to\nsupport user access. Enterprise data includes metadata about the data as well as systems of\nrecord for enterprise application authentication, authorization, or profile data. Enterprise\napplications consume cloud provider data and analytics to produce results that address\nbusiness goals and objectives.\nSecurity components\nSecurity components include identity and access management, data and application\nprotection, and security intelligence.\nAPI management\nAPIs are central to any cloud computing solution so they also must be managed. This\nfunction must not only address the multiple personas associated with API delivery and use,\nbut all the different services, devices, and applications as well.\nFor in-depth information on API management, refer\nto: IUUQT\u001c\u0011\u0011TMJEFMFHFOE\u0010DPN\u0011DTDD\u000fDMPVE\u000fDVTUPNFS\u000fBSDIJUFDUVSF\u000fGPS\u000fBQJ\u000fNBOBHFNFOU@\n\u0017\u001bGD\u0019\u0016\u0012\u0012\u0013\u0019\u0014\u0015EE\u0018BF\u0016E\u001b\u0019\u0013\u001a\u0013\u0010IUNM.\nE-commerce\nThe following diagram shows the e-commerce solution across three domains: public\nnetworks, cloud service provider, and enterprise networks. The public network domain\ncontains commerce users and their e-commerce channel that supports user interaction. The\nedge services handle traffic between the pubic network and the CSP. The cloud service\nprovider can host comprehensive e-commerce capabilities, such as merchandising, location\nawareness, B2B2C commerce, payment processing, customer care, distributed order\nmanagement, supply chain management, and warehouse management.\nThe enterprise network domain represents existing enterprise systems. It includes\norganizational applications, data stores, and user directories. Results are delivered using\ntransformation and connectivity components.\n\n\nSolution Reference Architectures\nChapter 8\n[ 149 ]\n(KIWTG\u0003\u0016\u001d\u0003%NQWF\u0003EQORQPGPV\u0003TGNCVKQPUJKRU\u0003HQT\u0003G\u0010EQOOGTEG\nPublic network components\nThe public network contains data sources and APIs, users, and the edge services. An e-\ncommerce user accesses the commerce solutions via the cloud provider platform or\norganizational network. The channel provides a seamless, personalized experience\nindependent of the customer access mode or channel. Key capabilities in this domain\ninclude the following:\nWebsite\nMobile\nConnected devices\n\n\nSolution Reference Architectures\nChapter 8\n[ 150 ]\nEdge services are needed to transfer data safely from the internet into the CSP and on to the\nenterprise. Edge services also support end user applications. Key capabilities include the\nfollowing:\nDomain name system server\nCDNs\nFirewall\nLoad balancers\nCloud provider components\nBy allowing direct ordering from a manufacturer, ecommerce applications have extended\nthe supplier's ability to tap into new markets and channels. Having a retailer participate as\na delivery channel for a supplier has also provided convenience to customers and has\nallowed them to reach new customers and promote their brand. Key capabilities include\nthe following:\nMobile digital and store\nProduct search and personalization\nCatalog\nOrder capture\nMarketplace\nA robust digital experience is the key to engaging customers, and the functional capabilities\nshould include the following:\nContent\nFederated search\nSocial engagement\nDigital messaging\n\n\nSolution Reference Architectures\nChapter 8\n[ 151 ]\nA gateway is also critical because it allows smart devices to search, shop, and pay.\nCustomer care assists the customer throughout the entire transaction life cycle and across\nall commerce channels. Online customer care should be offered in real time, often through\nchat facilities, cued by user behaviors such as abandoning a shopping cart or alternating\nbetween pages multiple times. Cognitive computing and natural-language processing have\ngreatly enhanced customer care functions.\nKey capabilities in this domain include the following:\nCustomer relationship management (CRM)\nLoyalty management\nPayment processing handles payment transactions that use credit cards or electronic fund\ntransfers (EFT) from the following roles:\nMerchant\nCustomer\nMerchant payment-processing service provider\nMerchant bank, if different than payment processor\nCustomer's bank, or bank issuing credit or purchase card\nThe payment gateway, on the other hand, is the mediator between the e-commerce\ntransaction and the payment-processing service. Security requirements prohibit the direct\ninformation transmission from the website to the payment processor. Payment gateways\nare offered by payment-processing vendors or contracted from vendors who only offer a\ngateway as a service.\nDistributed order management orchestrates the workflow of orders from distribution\ncenters and warehouses through direct fulfillment at stores. It can deliver superior\ncustomer experiences across an extended supply chain network and provide flexible order\nmanagement across multiple channels. Key capabilities include the following:\nOrder management and orchestration\nGlobal inventory visibility\nReturns management\n\n\nSolution Reference Architectures\nChapter 8\n[ 152 ]\nSupply chain management is used to plan and manage product life cycle, supply network,\ninventory, distribution, and partner alliances. Logistics management addresses internal\nlogistics for purchasing, production, warehousing, and transportation. Key capabilities in\nthis domain include the following:\nSupply chain management\nProduct life cycle management (PLM) and manufacturing\nSourcing and procurement\nSupplier and partner data communications\nTransactional event ledgers\nTransportation management and optimization\nWarehouse management enables efficient warehouse management operations. Modern\norganizations combine warehouse management wireless networks, mobile computers,\nRadio Frequency Identification (RFID) technology, voice-picking applications, and\nbarcoding. This can fully extend an enterprise to the mobile worker, increase efficiency, and\nenhance customer service. Capabilities here include the following:\nWarehouse inventory management\nInventory optimization\nInventory\nMerchandising planning is the management of merchandise or service-marketing rights.\nThe goal is optimizing margin, gross revenue, or product shelf life. Domain key capabilities\ninclude the following:\nAssortment management\nPricing management and optimization\nProduct placement\nCommerce analytics optimizes the shopper's journey to improve sales and business\nrevenue. This component should drive the next best action at the right time and the best\nchannel. Personalization is enabled through having a comprehensive customer view and\npredictive analytics. Key capabilities in this domain include the following:\nDigital analytics\nCross-channel analytics\nSocial commerce and sentiment analytics\nMerchandise analytics and optimization\n\n\nSolution Reference Architectures\nChapter 8\n[ 153 ]\nThe marketing domain supports the customer from product exploration through the\npurchase decision to transaction completion by delivering personalized offers, content, and\nproduct presentations. Understanding consumer consumption and shopping behavior is\nnow key to building and growing market share. Key capabilities are the following:\nMarketing resource management (MRM)\nCampaign management\nReal-time recommendations\nData services deliver the ability to access, replicate, and synchronize data. These services\naid in the management of merchandise inventory and distributing transportation. Other\ndata services can also be used to generate and aggregate the reports from the enterprise\ndata and applications.\nThe business performance component delivers important alerts, metrics, and Key\nPerformance Indicators (KPIs) used to monitor commerce activities, tracking progress\nagainst goals, and adjusting offerings in response to market variations and demand. Data is\nusually displayed using dashboards that have been tailored for specific management roles.\nCommerce analytics and data services support real-time visibility of customer activity and\nprovide the ability to drill down to individual transactions.\nRetailers typically rely on a few fundamental metrics to provide an accurate view of their\nperformance. These retail KPIs are as follows:\nNumber of customers interacting in a store or via a website\nConversion rateschow many store or website visitors actually make a purchase\nAverage sales value of items purchased\nThe size of a shopping basket\nGross margin\nThe transformation and connectivity component provides secure connections to enterprise\nsystems. It also provides the ability to filter, aggregate, modify, or reformat data. Key\ncapabilities in this domain include the following:\nEnterprise secure connectivity\nData transformations\nEnterprise data connectivity\nExtract, transform, and load\n\n\nSolution Reference Architectures\nChapter 8\n[ 154 ]\nEnterprise network components\nEnterprise network components support on-premises systems and users. Key domain\ncapabilities include the following:\nIn-store\nCall center\nEnterprise applications are important data sources in commerce solutions. Enterprise\napplications use cloud services and host legacy applications. Three key applications are the\nfollowing:\nFinance\nHuman resources\nContract management\nEnterprise data\nThe enterprise data component hosts applications that deliver critical business solutions\nand their supporting infrastructure. These applications are key data sources that are\nextracted and integrated with analytics services. Key capabilities in this domain include the\nfollowing:\nReference data\nTransactional data\nActivity/big data\nOperation master data\nThe enterprise user directory provides user profile access for both the cloud and enterprise\nusers. A user profile provides a login account and access control lists. The security services\nand edge services use this directory to control access to the enterprise network and services\nor to cloud provider services.\n\n\nSolution Reference Architectures\nChapter 8\n[ 155 ]\nSecurity\nSecurity services enable identity and access management, protection of data and\napplications, and actionable security intelligence across cloud and enterprise environments.\nThey use the catalog and user directory to understand the location and classification of the\ndata they are protecting. Key capabilities in this domain include the following:\nIdentity and access management\nApplication and data protection\nData encryption\nInfrastructure and network protection\nApplication security\nData activity monitoring\nData lineage\nSecurity intelligence\nMobile\nThe architectural elements described in this summary are used to instantiate mobile hosting\nenvironments with cloud service providers. Mobile applications have time-variable usage\npatterns that are well supported by the scalability and elasticity characteristics of cloud\ncomputing. Mobile applications also tend to make use of server-side data.\nThe frequency and volume of data access common with mobile apps can sometimes be\ndifficult for traditional enterprise systems. Elastic provisioning and support of application-\nspecific databases is an important and relevant cloud-computing capability. Using\napplication-specific databases can also reduce the need to access enterprise systems and the\nassociated resources.\nMobile architecture components\nFigure 1 illustrates the high-level architecture of a mobile cloud solution. The architecture\nhas four tiers:\nMobile computing devices\nPublic network that connects a device to the cloud services\nProvider cloud environment that hosts the necessary services\n\n\nSolution Reference Architectures\nChapter 8\n[ 156 ]\nThe following diagram depicts an enterprise network that contains legacy enterprise\napplications, services, and data:\n(KIWTG\u0003\u0017\u001d\u0003%NQWF\u0003EWUVQOGT\u0003OQDKNG\u0003CTEJKVGEVWTG\n\n\nSolution Reference Architectures\nChapter 8\n[ 157 ]\nMobile device components\nMobile applications are the core vehicle for user engagement with services on mobile\ndevices. Mobile applications communicate with backend services via APIs, normally based\non REST interfaces. The two key mobile app components are the following:\nVendor frameworks\nEnterprise software development kits (SDKs)\nManagement agents apply enterprise policies. The agent is the SDK component that stores,\nenforces, and manages policies on the device. Offline capabilities give the application the\nability to store and sync data securely on devices. A mobile app may use offline capabilities\nto access and store secure data.\nPublic network Components\nEdge services connect the mobile device and its applications to the mobile gateway using\nWi-Fi or mobile provider networks. These include the following:\nDomain name system server\nFirewall\nLoad balancers\nCDNs\nThe mobile provider network owns or controls the elements necessary to sell and deliver\nservices to an end user. These typically include radio spectrum (IUUQ\u001c\u0011\u0011FO\u0010XJLJQFEJB\u0010\nPSH\u0011XJLJ\u00113BEJP@TQFDUSVN) allocation, wireless network (IUUQ\u001c\u0011\u0011FO\u0010XJLJQFEJB\u0010PSH\u0011XJLJ\u0011\n8JSFMFTT@OFUXPSL) infrastructure, back haul (IUUQ\u001c\u0011\u0011FO\u0010XJLJQFEJB\u0010PSH\u0011XJLJ\u0011#BDLIBVM@\n\u0007\u0014\u001aUFMFDPNNVOJDBUJPOT\u0007\u0014\u001b) infrastructure, billing, customer care, provisioning (IUUQ\u001c\u0011\u0011\nFO\u0010XJLJQFEJB\u0010PSH\u0011XJLJ\u00111SPWJTJPOJOH) computer systems, and marketing and repair\norganizations. \n\n\nSolution Reference Architectures\nChapter 8\n[ 158 ]\nProvider cloud service components\nThe mobile gateway is the entry point from a mobile application to the mobile-specific\nsolution services. It may also use data services and the enterprise user directory. A mobile\ngateway can be implemented using a common gateway across all channels into an API\necosystem. It provides the following:\nAuthentication/authorization\nPolicy enforcement\nAPI/invocation analytics\nAPI/reverse proxy\nThe mobile backend delivers runtime services to mobile applications in implementing\nserver-side logic, maintaining data, and using mobile services. It provides an environment\nto run application logic and APIs. Here, application logic can communicate with the\nenterprise network and other applications that reside outside of the service provider. It\nprovides the following:\nApplication logic/API implementation\nMobile app operational analytics\nPush notifications\nLocation services\nMobile data sync\nMobile app security\nMobile device management (MDM) manages mobile devices and provides services to\ntrack enterprise-owned devices. It also manages devices that connect to corporate networks.\nMDM provides the following:\nEnterprise app distribution\nMobile device security\nDevice management\nDevice analytics\n\n\nSolution Reference Architectures\nChapter 8\n[ 159 ]\nMobile business applications are the enterprise- or industry-specific capabilities needed to\nconduct business on the mobile devices. These can provide a gateway to enterprise\napplications and data, and may include analytics components that track usage. They can\ninclude the following:\nProximity services and analytics\nCampaign management\nBusiness analytics and reporting\nWorkflow/rules\nAPI management advertises available service endpoints and provides API discovery,\ncatalogs, APIs that connect to management capabilities, and service implementations, such\nas API versioning. The capabilities are the following:\nAPI discovery/documentation\nManagement\nData services enable data to be stored and accessed in a form suitable for rapid access. This\nmay include extracts of enterprise data. Data services can include the following:\nMobile app data/NoSQL\nFile repositories\nCaches\nSecurity services ensure that only authorized users are able to access mobile cloud services.\nThis also protects the data and enables the visibility required to have actionable security\nintelligence across all environments. Components include the following:\nIdentity and access management\nData and application protection\nSecurity intelligence\nEnterprise transformation and connectivity\nEnterprise security connectivity\nTransformation\n\n\nSolution Reference Architectures\nChapter 8\n[ 160 ]\nEnterprise network components\nEnterprise network components provide backend connectivity to enterprise business\nservices and include the following:\nEnterprise user directory\nEnterprise data\nEnterprise applications\nEnterprise social collaboration\nThe enterprise social collaboration architecture addresses services that support an\nenterprise social platform. This design also includes the internal and external extension\npoints needed for data and services integration. These capabilities can be applied in\nmodules. The interfaces between the social collaboration platform and on-premises systems\nare important when defining the final system architecture.\nCloud customer reference architecture for\nenterprise social collaboration\nThe cloud customer reference architecture for enterprise social collaboration is explained in\nthe following sections.\n\n\nSolution Reference Architectures\nChapter 8\n[ 161 ]\nArchitecture Overview\n(KIWTG\u0018\u001d\u0003'NGOGPVU\u0003QH\u0003GPVGTRTKUG\u0003UQEKCN\u0003EQNNCDQTCVKQP\nUser network\nThe user network allows end users to interact with cloud provider services. These services\nare classified as desktop (client), mobile, and web applications. The end user enters requests\nfor enterprise social collaboration services and receives results after processing has been\ncompleted. Services can also include end users' interactions. The key focus is social network\ninteraction between the following:\nWeb applications (via web browser)\nMobile application\nDesktop rich client\n\n\nSolution Reference Architectures\nChapter 8\n[ 162 ]\nService consumer\nEnd users may not always interact directly with social services through the user network.\nThey could also consume a service through other application interfaces. A service consumer\ndescribes this as a pattern where an end user consumes and contributes to the social\nservices indirectly through the following patterns:\nIntegrated digital experience, which is integrated capabilities that deliver an\nengaging, personalized, relevant, and meaningful digital presence to the user.\nKey capabilities may include the following:\nContent\nDigital messaging\nSocial engagement\nFederated search\nPersonalization\nAnalytics\nPeer cloud services used for core business processes or to meet just-in-time\nrequirements may also be required consume content, services, and interfaces\nfrom the social services that could include the following:\nSoftware-as-a-Service (SaaS)\nCloud services (API)\nPlatform-as-a-Service (PaaS)\nProvider network\nProvider cloud components include the following capabilities.\nEdge services deliver connectivity options to end users and the capabilities needed to allow\nthe safe flow of data between the internet, the provider cloud, and the enterprise.\nCapabilities in this domain include the following:\nDomain Name System (DNS)\nCDNs\nFirewall\nLoad balancers\n\n\nSolution Reference Architectures\nChapter 8\n[ 163 ]\nEnterprise social services represent a collaborative information exchange using secure\nsocial applications. This capability can blend an integrated user experience that is infused\ninto business processes, integrated with other applications, and aggregated with other\nexperiences. Key capabilities in this domain include the following:\nNetworking\nCommunities\nFile sync\nLive collaboration\nMessaging\nUser directory\nAs part of a social implementation, peer services may be consumed in order to deliver\nfunctionality provided by an external solution. These can also be integrated into social\nservices and can be hosted within the service provider cloud. Peer services rely on the\ncloud service provider's cloud governance and security models and they can deliver the\nfollowing:\nExtended capabilities: Functional experiences integrated into defined extension\npoints of the enterprise social services experience\nEnhanced experiences: Additional tools and extensions to the existing enterprise\nsocial service\nFoundation services: Enhance the underlying functions of the social service\nThe information governance component assures enforcement of organizational policies by\nfocusing on procedures that govern access to capabilities and information sharing. These\ntypically include the following:\nSign-on/on-boarding approval process\nLegal compliance\nRegulatory compliance (that is, PII, PCI, HIPPA, FINRA, FedRAMP, and so on)\nAudit reporting\nData loss protection\nCorporate policies\n\n\nSolution Reference Architectures\nChapter 8\n[ 164 ]\nSecurity\nAs an integrated suite, the enterprise social collaboration may have some unique security\nconsiderations that need to be addressed. Some of the more prevalent of these are as\nfollows.\nAuthentication to the service is needed to ensure that only authorized users have access to\ndata, tools, and applications, while simultaneously blocking unauthorized access.\nSynchronizing enterprise user directories is beneficial in extending the on-premises\nenvironment to the cloud. The enterprise social services facilitate this by providing the\nfollowing:\nOn-/off-boarding of users\nUser bulk provisioning and updates\nProvisioning of user through an administrative tool\nFederated Identity Management uses Single Sign-On (SSO) to protect the transfer of user\ncredentials across networks. Using SSO, authorized users can use different applications\nwithout additional authentication.\nSecurity Assertion Markup Language (SAML) is used to facilitate SSO with other parties\nor enterprise directories. SAML is a widely used standard that leverages signed assertion\ndocuments instead of passwords as identity credentials. Customers maintain passwords\ninternally for web application resources which help organizations do the following:\nManage password requirements.\nManage two-factor authentication requirements.\nSet password change intervals.\nUse open authorization (OAUTH), which supports web applications, desktop\napplications, and third-party extensions. This is an open source methodology for\nAPI authorization.\n\n\nSolution Reference Architectures\nChapter 8\n[ 165 ]\nData security ensures only authorized users have secure access to customer data. This\nrequires protection of the relevant data against service vulnerabilities and physical breach\nof data centers. Security requires the layered use of a combination of technology coupled\nwith standard CSP processes and procedures. These can include the following:\nPlatform and process\nSecurity checklist against every release\nSecurity compliance with ongoing automated health checks\nData center\nRedundancy: Redundant systems to prevent a single point of failure in providing\nservices, including application, power, network, and so on.\nMonitoring of the physical environment, which includes the logging of staff\nactivities\nAccess controls and fire-prevention systems\nNetwork and infrastructure defenses\nLayered firewall infrastructure\nDeployed network intrusion detection\nProcess for people\nSeparation of duty definitions\nSegregation of activities, including personnel with change access to the code base\nand those with operational configuration control\nCode reviews prior to deployment\nRegular ethical hacking penetration testing\nAudit logs and analysis of security-related events\nData privacy and data ownership policies\nEncryption and email security\nData in transit\nData at rest\nReal-time antivirus at application and server levels\nAnti-spam protection on email messages\n\n\nSolution Reference Architectures\nChapter 8\n[ 166 ]\nTransformation and connectivity ensures secure connections to backend enterprise systems.\nThis also enables data filtering, aggregation, modification, or reformatting. Key capabilities\nin this domain include the following:\nEnterprise secure connectivity\nTransformations\nEnterprise data connectivity\nExtract, transform, and load\nEnterprise network\nThe enterprise network contains the on-premises systems components. While integration\nbetween the enterprise social services and enterprise applications may not be necessary,\nsome use cases may need it. Services include the following:\nUser directory\nEnterprise data\nEnterprise applications are existing applications that accomplish business goals and\nobjectives. These may also need to interact with cloud services. Organizational email is a\ncommon example.\nBig data and analytics\nBig data analytics (BDA) is used to build competitive advantage, drive innovations, and\nenhance revenue. This capability offers a cost-effective cloud-based solution for data\nanalytics. As big data grows in importance, organizations are striving to derive meaningful\ninsight from this data. This is crucial to enabling an ability to respond to real business needs\nin a timely manner.\nThe following diagram illustrates a simplified enterprise cloud architecture for a BDA\nenvironment and contains three network zones: public, cloud service provider, and\nenterprise:\n\n\nSolution Reference Architectures\nChapter 8\n[ 167 ]\n(KIWTG\u0003\u0019\u001d\u0003$&#\u0003UQNWVKQPU\u0003KP\u0003VJG\u0003ENQWF\nThis architecture is similar to that of a data lake deployment. Both structured and non-\nstructured data are used as sources. Data is then staged and transformed by data\nintegration and stream computing engines, after which it is stored in various repositories.\nThe data may also be augmented, transformed, correlated, and summarized until it is\nfinally made available to consumers through APIs. Cognitive computing technologies such\nas machine learning and natural language processing can also be used to automate\ningestion, integration, discovery, and exploration.\nThis architectural approach is applicable to the entire analytics life cycle and can be applied\nas a data lake solution for a DevOps environment. The latter is achieved by adding\nmetadata and semantic definitions to the enterprise data repository descriptions that are\nstored in a service catalog. These catalog entries are then augmented with governance\nclassifications, rules, and policies that automate data management as it flows in, out, and\nthrough the data lake. The following sections give a summary of each of the required\ncomponents.\n\n\nSolution Reference Architectures\nChapter 8\n[ 168 ]\nPublic network components\nThe public network contains cloud users, SaaS applications, data sources, and edge\nservices.\nA cloud user connects to the analytics cloud solution via the network. Human and non-\nhuman users may represent one or more of the following roles:\nKnowledge worker and citizen analyst\nData scientist\nApplication developer\nData engineer\nChief Data Officer (CDO)\nAll the different personas have the following common characteristics:\nSelf-service is desired.\nThey require access to sometime large volumes of data needed to accomplish an\nanalytical task with associated data quality and provenance metrics.\nThey require multiple tools and capabilities that may be open-source and\nconsumed from on-demand services.\nCollaboration is required.\nData sources can be external, public, and varied, with multiple information sources\ncontained within a typical big-data system. High velocity, volume, variety, and data\ninconsistency are often the norm. Edge analytics services may also be required. Data\nsources normally include the following:\nMachine and sensor\nImage and video\nSocial\nInternet datasets\nWeather data\nThird party\nEdge services allow data to flow safely and securely from the internet into the data\nanalytics processing system. The data flow may also require domain name system (DNS)\nservers, CDNs, firewalls, and load balancer services before entering the cloud service\nprovider's data integration or data-streaming service points.\n\n\nSolution Reference Architectures\nChapter 8\n[ 169 ]\nProvider cloud components\nThe cloud service provider delivers the analytics solution and hosts the required\ncomponents. These functions are used to prepare data for analytics, storage, and results\nprocessing. Cloud service provider elements include the following:\nAPI management\nData repositories\nStreaming computing\nSaaS applications\nCognitive assisted data integration\nCognitive analytics discovery and exploration\nTransformation and connectivity\nCognitive actionable analytics\nThe data access component is used to express the many capabilities for interacting with the\ndata repositories. This serves customer data access needs and includes the following\nservices:\nData access\nData virtualization\nData federation\nOpen APIs\nStream processing is used to ingest and process large volumes of highly dynamic, time-\nsensitive, and continuous data streams from inputs such as sensors, messaging systems,\nand real-time feeds. The traditional store-and-pull data processing model is not usable for\nmeeting low-latency or real-time streaming applications. Capabilities include the following:\nStreaming analytics\nComplex Event Processing (CEP)\nData enrichment\nReal-time ingestion\n\n\nSolution Reference Architectures\nChapter 8\n[ 170 ]\nThe cognitive-assisted data integration component deals with the capture, qualification,\nprocessing, and movement of data into analytical data lake repositories. Here, it is shared\nwith the discovery and exploration and actionable insights components using the data\naccess component. Various cognitive technologies such as machine learning and natural\nlanguage processing can be used to automate data ingestion and integration. Data\nintegration capabilities include the following:\nBatch ingestion\nChange data capture\nDocument interpretation and classification\nData quality analysis\nThe data repositories are a set of secure locations used to store data prior to it being\nconsumed by analytics tools and end users. The repositories are the core of the analytics\nenvironment. Operational and transactional data stores (such as OLTP, ECM, and so on) are\nnot part of this component. They are help within the data sources component. Data\nrepository types include the following:\nLanding zone and data archive\nHistory\nDeep and exploratory analytics\nSandboxes\nData warehouses and data marts\nPredictive analytics\nCognitive analytics discovery and exploration enables end users to collaborate about and\neasily interact with complicated data repositories using modern data science techniques.\nThey also enable an ability to semantically search across both structured and unstructured\ncontent in order to glean new insights and obtain a complete data ontology view.\nFunctionally, these provide the following:\nData science\nSearch and survey / shopping for data\nThe cognitive actionable insight component cohesively analyzes data from multiple sources\nin order to derive meaningful and actionable insight for the business. Techniques used\nwithin this function include the following:\nVisualization and storyboarding\nReporting, analysis, and content analytics\nDecision management\n\n\nSolution Reference Architectures\nChapter 8\n[ 171 ]\nPredictive analytics and modeling\nCognitive analytics\nInsight as a Service\nCloud service provider SaaS applications are often used to enable, manage or augment the\nfollowing:\nCustomer experience\nNew business models\nFinancial performance\nRisk\nFraud and preparation\nIT economics\nThe transformation and connectivity component ensures secure connections to backend\nenterprise systems while also enabling data filtering, aggregation, modification, or re-\nformatting. Capabilities include the following:\nEnterprise security connectivity\nTransformations\nEnterprise data connectivity\nEnterprise network\nThe enterprise network is where the on-premises systems and users are located and\nincludes users and applications. Enterprise data is also included and holds metadata on the\ndata as well as systems of record for enterprise applications. It may flow directly to data\nintegration or the data repositories and includes the following:\nReference data\nMaster data\nTransactional data\nApplication data\nLog data\nEnterprise content data\nHistorical data\nArchived data\n\n\nSolution Reference Architectures\nChapter 8\n[ 172 ]\nThe enterprise user directory contains the user profiles for both the cloud and enterprise\nusers. A user profile provides a login account and access control lists. Security and edge\nservices use this to manage data access.\nUbiquitous services that interact across the entire environment include the following.\nInformation management and governance components maintain a trusted, standardized,\nand accurate view of critical business data and include the following:\nData life cycle management\nMaster and entity data\nReference data\nData catalog\nData models\nData quality rules\nSecurity\nThe security component protects the data and delivers the ability to mask/hide data at a\ngranular level and the following capabilities:\nData security\nIdentity and access management\nInfrastructure security\nApplication security\nSecure DevOps\nSecurity monitoring and intelligence\nSecurity governance\nSystem management refers to all activities performed to plan, design, deliver, operate, and\ncontrol IT and cloud-based services typically addressed by service level agreements\n(SLAs).\n\n\nSolution Reference Architectures\nChapter 8\n[ 173 ]\nBlockchain\nBlockchain technology features an immutable distributed ledger accessed across a\ndecentralized cryptographically secured network. This architecture allows for the sharing\nof an electronic ledger, through peer-to-peer replication. This ledger is updated every time\na block of transactions is committed. This technology can radically alter the way business is\nconducted and how transactions are processed. With blockchain technology, participants\ncan engage in transparent business transactions across geographical boundaries.\nFrom a business perspective, a blockchain is an exchange network that facilitates transfer of\nvalue, assets, or other entities between willing and mutually agreeing participants, ensuring\nprivacy and control of data to stakeholders.\nFrom a legal perspective, blockchain ledger transactions are validated, indisputable\ntransactions, which do not require intermediaries or trusted third-party legal entities.\nFrom a technical perspective, a blockchain is a replicated, distributed ledger of transactions\nwith ledger entries referencing other data stores. Cryptography ensures that network\nparticipants see only the parts of the ledger that are relevant to them, and that transactions\nare secure, authenticated, and verifiable, in the context of permissioned business\nblockchains.\nBlockchain Reference Architecture Capabilities\nThe following diagram presents the typical node capabilities needed to participate in a\nblockchain architecture and is depicted across three networkscpublic, cloud, and\nenterprise. The capability locations are representative of industry best practice, but any\ncapability can be implemented in any network based on the solution needs:\n\n\nSolution Reference Architectures\nChapter 8\n[ 174 ]\n(KIWTG\u0003\u001a\u001d\u0003$NQEMEJCKP\u0003TGHGTGPEG\u0003CTEJKVGEVWTG\u0003ECRCDKNKVKGU\nPublic network\nThe public network contains the wide-area networks peer cloud systems, and edge services.\nEdge services allow data to flow securely and safely from the network into the cloud\nservice provider and onto the enterprise. Edge services support end user applications and\ninclude the following:\nDomain name system server (DNSS)\nCDNs\nFirewall\nLoad balancers\n\n\nSolution Reference Architectures\nChapter 8\n[ 175 ]\nUsers are the blockchain participants who create and distribute blockchain applications.\nThose performing operations using the blockchain may include the following:\nDevelopers\nAdministrators\nOperators\nAuditors\nBusiness users\nCloud network\nBlockchain applications present business capabilities to blockchain system end users.\nApplications may also serve other users who have different roles. Blockchain applications\ncan be web services or end user device(s) applications, or connected to server-side\napplication services. These applications and services interface using the platform APIs. The\napplications may also have access to other resources such as databases if needed to\nimplement capabilities.\nAPI management capabilities publish catalogs and update APIs, enabling developers and\nend users to leverage discovery and reuse of existing data, analytics, and services to rapidly\nassemble solutions. Blockchain applications interface with the blockchain network by using\nthe blockchain programming interfaces.\nThe blockchain platform supports essential capabilities via a blockchain network node or\nenterprise. Although each blockchain platform is implemented differently, core capabilities\nthat should be considered are the following:\nConsensus\nLedger\nMembership services\nTransactions\nEvent distribution\nCommunication protocol\nCryptographic services\nSmart contract\nSecure runtime environment\n\n\nSolution Reference Architectures\nChapter 8\n[ 176 ]\nTypical system integration methods include API adapters and an Enterprise Service Bus\n(ESB) connection between the blockchain platform and the organization's internal systems.\nTransformation and connectivity capability enables secure connections to backend\nenterprise systems. It also provides filtering, aggregation, or data modification or data re-\nformatting as it moves between the cloud, blockchain components, and enterprise systems.\nThis component includes the following capabilities:\nEnterprise secure connectivity\nTransformation\nEnterprise data connectivity\nThe enterprise network provides the enterprise user directory, enterprise applications, and\nenterprise data. Enterprise data includes metadata as well as systems of record for\nenterprise applications. Enterprise data relating to blockchain includes the following:\nTransactional data\nApplication data\nLog data\nBlockchain services\nBlockchain foundational services include the following:\nGovernance\nSecurity\nMonitoring and intelligence\nThe blockchain network management component provides visibility across all blockchain\nnetwork operations. This visibility includes business process, performance, and capacity\ndata metrics. It also delivers the management interface for changing configurations and\nother parameters.\n\n\nSolution Reference Architectures\nChapter 8\n[ 177 ]\nOther important blockchain concepts include permissions options:\nPermissionless networks are open to any participant. Transactions are verified\nagainst the pre-existing network rules. Any participant, even those that are\nanonymous, can view ledger transactions.\nPermissioned networks are limited to participants within a given business\nnetwork. Participants are only allowed to view transactions relevant to them and\ncan only perform operations for which they are permissioned.\nWhen using blockchain, only a small amount of the transaction data is stored directly in the\nblockchain ledger. Other transaction is stored separately but is referenced by the entry. This\napproach avoids overwhelming the blockchain ledger with large data amounts. Storage\noptions include the following:\nLedger storage\nData storage\nBlockchain interaction options are varied and include the following:\nCommand line interface (CLI)\nClient SDK\nSoftware Development Kit (SDK)\nArchitecture for IoT\nThe IoT links physical entities (things) with information technology systems in order to\nderive information. This information is used to drive multiple applications and services.\nSince IoT covers applications that integrate systems from traditionally different\ncommunities, they must have architectures that are capable of accommodating many\nunique requirements.\n\n\nSolution Reference Architectures\nChapter 8\n[ 178 ]\nIoT systems include sensors for gathering information about objects and human activities.\nThey can also monitor actuators acting on other physical objects. Unique IoT architecture\nimplementation aspects are shown in the following table:\nArchitecture aspects\nDescription\nScalability\nThe numbers of sensors and actuators connected to the system, the\nnetworks which connect them together, the amount of data\nassociated with the system, data speed of movement, and the\namount of required processing power.\nBig data\nThe ability to gather new insights from mining existing data.\nCloud computing\nThe use of large amounts of resources in terms of data storage and\nscalable processing.\nReal time\nReal-time data flow support and an ability to produce timely\nresponses based on a continuous stream of events. There is also a\nparallel need to detected and avoid the use of corrupted data.\nHighly distributed\nWidely distributed devices, systems and data.\nHeterogeneous\nsystems\nHeterogeneous set of devices that include sensors and actuators,\ntypes of networks, and variety of processing components.\nSecurity and privacy\nData protection combined with significant data privacy protection.\nCompliance\nRegulatory compliance across specific industries, sectors, and\nverticals.\nIntegration\nAbility to connect to existing operational technology systems such\nas factory systems, building control systems, and other physical\nmanagement systems.\n\n\nSolution Reference Architectures\nChapter 8\n[ 179 ]\nThe following diagram shows the capabilities and relationships for supporting IoT using\ncloud computing:\n(KIWTG\u0003\u001b\u001d\u0003%NQWF\u0003EQORQPGPVU\u0003HQT\u0003VJG\u0003+Q6\nThe cloud components of IoT architecture are normally positioned in a three-tier\narchitecture composed of edge, platform, and enterprise tiers.\nEdge tier\nProximity networks and public networks are in the edge tier. Here, data is received and\ntransmitted to user devices. Data flows through the IoT gateway or through edge services\ninto the CSP.\n\n\nSolution Reference Architectures\nChapter 8\n[ 180 ]\nThe cloud service provider in in the platform tier. The provider receives, processes, and\nanalyzes data flows. The CSP also provides API management and visualization and can\ninitiate control commands.\nThe enterprise network represents the enterprise tier, which is comprised of enterprise\ndata, enterprise user directory, and enterprise applications.\nAn IoT system uses application logic and control logic across many locations, depending on\ntimescales and datasets. Some code may actually execute directly on the IoT devices or in\nthe gateways. Edge computing or fog computing is applied to the case where code executes in\nthe IoT gateways or the devices.\nIoT users and end user applications are in the user layer. The proximity network has all\nphysical entities that interact with IoT system physical entities. The physical entity is the\nsubject of sensor measurements or actuator behavior.\nDevices contain sensor and actuator. The attached network connection enables interaction\nwith the extended IoT system. The device may also be the physical entity being monitored\nby the sensors. Other important components include sensor/actuator, agent, and firmware.\nThe network connection provides connectivity from the device to the IoT system. This\ndevice often has low power and low range to reduce power requirements. Alternative\ncommunication mechanisms can include Bluetooth, Bluetooth Low Energy (BTLE), Wi-Fi\nor wide area networking using 2G, 3G, and 4G LTE. The user interface supports user\ninteraction with applications, agents, actuators, and sensors.\nThe IoT gateway connects devices to the public network. If the devices have limited\nnetwork connectivity, the local IoT gateway enables the needed communications. The IoT\ngateway can also filter and react to data. The IoT gateway contains the following:\nApp logic\nAnalytics\nAgent\nDevice data store\n\n\nSolution Reference Architectures\nChapter 8\n[ 181 ]\nPublic network\nThe public network contains the wide area networks, other cloud systems, and edge\nservices. Large IoT systems may combine a series of smaller IoT systems that each address a\nspecific part of the solution. These systems of systems include connections between other\nclouds.\nEdge services enable secure data flow into the CSP and onto the enterprise network. They\ninclude the following:\nDomain name system server\nCDNs\nFirewall\nLoad balancers\nCloud service provider\nThe CSP delivers core IoT applications and services. These can include data storage,\nanalytics, IoT system process management, and data visualizations. IoT transformations\nand connectivity delivers secure connectivity between all IoT devices. This component\nmust handle and transform high volumes of messages and route them to the correct\nsolution components. The transformation and connectivity component includes the\nfollowing:\nSecure connectivity\nScalable messaging\nScalable transformation\nThe application logic holds the core application components that co-ordinate the IoT device\ndata handling and other services that support user applications. An event-based\nprogramming model is often used. Application logic can include workflow and control\nlogic.\nVisualization helps users explore and interact with. Visualization capabilities include the\nfollowing:\nEnd user UI\nAdmin UI\nDashboard\n\n\nSolution Reference Architectures\nChapter 8\n[ 182 ]\nAnalytics is used to discover and communicate meaningful IoT data information patterns.\nThese patterns are used to describe, predict, and improve business performance. IoT\ncapabilities include the following:\nAnalytics data repository\nCognitive\nActionable insight\nStreaming computing\nA critical component is the device data store. This stores data from the IoT devices so that it\ncan be integrated with other processes and applications. Devices can generate large\namounts of real-time data which requires an elastic and scalable device data store. Device\nmanagement is used to efficiently manage and connect devices securely and reliably to the\ncloud. Device management also contains provisioning, remote administration, software\nupdating, device remote control, and device monitoring.\nTransformation and connectivity enables secure connections to enterprise systems and the\nability to filter, aggregate, or modify data or its format as it moves between the cloud and\nIoT system components. The transformation and connectivity component includes the\nfollowing:\nEnterprise secure connectivity\nTransformation\nEnterprise data connectivity\nEnterprise network\nThe enterprise network hosts business-specific enterprise applications. These include\nenterprise data, user directory, and applications. Key IoT applications might include\ncustomer experience, financial performance, or operations and fraud.\nSecurity\nSecurity and privacy in IoT deployments always need to address both Information\nTechnology (IT), security, and Operations Technology (OT) security elements. The level of\nattention to security varies based on the application environment, business pattern, and risk\nassessment.\n\n\nSolution Reference Architectures\nChapter 8\n[ 183 ]\nThere are many challenges in securing an IoT solution. Use of oversight and procedures is\nnecessary to ensure that there is a means and mechanism for addressing new vulnerabilities\nand threats. An important difference in IoT systems is that exploits and failures have the\npotential to cause serious harm to humans, property, and the environment. Additionally,\nequipment is often installed in locations where change or replacement is not possible. IoT\nsystems must, therefore, be designed and deployed with strong\nchange/update/modification governance in mind. The cloud service provider components\nmay also be subject to change over time. Appropriate governance must be in place to\nensure that changes to these components are addressed as well.\nArchitecture for hybrid integration\nIT environments are now normally hybrid in nature. Integration across an ever-changing\nenvironment, delivered at the pace of modern digital innovation initiatives, is a significant\nchallenge. This makes the hybrid integration platform is crucial. Hybrid cloud integration\nscenarios include the following:\nViewing customer information between cloud-based CRM systems and on-\npremises Enterprise Resource Planning (ERP) applications\nEmployee data integration between cloud-based human capital management\nsystems and back-office applications\nThe hybrid integration reference architecture explores these and other common patterns\nobserved in enterprises addressing these issues. It addresses the following considerations:\nConnectivity: Connecting systems and devices with other systems and devices.\nThis integration may require low-level connectivity to Systems of Record (SoR)\nand the need to leverage cloud native systems as well.\nDeployment: Modern systems are deployed across a broad landscape so the\naccompanying integration components must have flexible deployment options.\nComponents should also be able to run directly on bare metal, in virtual\nmachines, or in containers.\nRoles: IT can be operated as bi-modal or multi-modal, where independent teams\nare working at different velocities. Integration needs to address these disparate\nvelocities. Hybrid integration therefore expands beyond the IT organization\nbusiness users and shadow IT departments that may be aligned with the line of\nbusiness. Complex integrations are now collaborative, and APIs become the\nbuilding blocks for collaboration between many different teams.\n\n\nSolution Reference Architectures\nChapter 8\n[ 184 ]\nStyles: Enterprise integration can be combined with APIs, events, and data to\ncreate seamless business processes and flows.\nThe hybrid integration architecture components are illustrated in the following diagram:\n(KIWTG\u0003\u001c\u001d\u0003%NQWF\u0003EWUVQOGT\u0003J[DTKF\u0003KPVGITCVKQP\u0003CTEJKVGEVWTG\nHybrid integration delivers a seamless platform for applications to interchangeably\nconsume services that, in turn, deliver end-to-end comprehensive mission-critical business\ncapabilities. The reference architecture has three tiers:\nPublic network\nCloud service provider\nEnterprise network\nSecurity is a cross-cutting theme that is applicable to all three tiers.\n\n\nSolution Reference Architectures\nChapter 8\n[ 185 ]\nPublic network\nThe public network contains user access applications that reside on the cloud provider\nnetwork. They are accessed using a browser or via a mobile native app. Edge services\ninclude capabilities needed to deliver function and content to the users via the internet.\nThese include the following:\nDNS server\nCDN\nFirewall\nLoad balancer\nCloud provider network\nThe cloud provider network holds many of the key application and API services. The\nCloud application component represents cloud-native applications that have been designed\nand developed within the cloud environment. These applications normally use modern\ntechniques such as micro services architecture, lightweight run times, container technology,\nand DevOps methodologies. The application's services are often exposed using APIs and\nmay need to access data from other systems via API calls, messaging, and data integration\nservices.\nInteraction APIs provide access to enterprise capabilities. They are maintained by the lines\nof business and are composed from lower-level system APIs. These APIs are business-led,\nmay be exposed externally, and might even be monetized with a funding model based on\ntheir usage. This component is the API gateway into the enterprise network. The interaction\nAPIs also advertises the available services endpoints to which the cloud application has\naccess. This component provides API discovery, catalogs, and connection of offered APIs to\nservice implementations and management capabilities, such as API versioning.\n\n\nSolution Reference Architectures\nChapter 8\n[ 186 ]\nCloud messaging gives fast, scalable, high throughput event delivery services with the\nenterprise network. This component should support multiple open event protocols. It\nshould also abstract away any proprietary non-standard protocols of the enterprise\nmessaging. This component is the event gateway into the enterprise network and should be\nable to do the following:\nEnable large-scale message processing\nSupport a microservices framework and event-driven applications\nEnable hybrid messaging\nPerform batch and real-time analytics\nAccelerate applications and data integration\nCloud integration services deliver rapid, simple, and flexible integration capabilities.\nUnlike traditional Enterprise Application Integration (EAI) and ETL solutions, this\ncomponent provides simple integration tooling with targeted capabilities. Customization is\ndone performed via configuration and not by writing software code. This component is the\ngateway into SoR within the enterprise network and includes the following:\nPreparing/moving data to the cloud\nExtending business operations\nAccessing mainframe data and services\nMaintaining data consistency across applications\nConnecting on-premises apps and data to the cloud\nThe transformation and connectivity component enables secure enterprise connections.\nThis component includes the following capabilities:\nEnterprise secure connectivity\nTransformation\nEnterprise data connectivity\nEnterprise network\nThe enterprise network holds legacy applications, data, and APIs.System APIs give access\nto enterprise applications and data. They are maintained by the corporate IT team and are\nnormally lower-level, fine-grained APIs. Multiple interaction components might consume\nthese APIs to compose higher-level capabilities. This component also provides API\ndiscovery, catalogs, and connection of offered APIs to service implementations and\nmanagement capabilities.\n\n\nSolution Reference Architectures\nChapter 8\n[ 187 ]\nEnterprise messaging is the enterprise messaging backbone. This is the primary messaging\ninterface into the enterprise for the cloud messaging component and does the following:\nProvides secure and reliable messaging\nSupports heterogeneous application platforms\nProvides high-performance and scalable message transfer\nProvides simplified management and control\nEnterprise integration services depict a broad variety of integration capabilities including\nenterprise data warehouse (ETL) systems, application integration components, and\nbusiness process management systems. This is the primary integration interface into the\nenterprise for the cloud integration services component. Capabilities include the following:\nPreparing/moving data to the cloud\nExtending business operations\nAccessing mainframe data and services\nMaintaining data consistency across applications\nConnecting on-premises apps and data to the cloud\nEnterprise applications are applications that run enterprise business processes and logic\nwithin existing enterprise systems, while enterprise data represents transactional data or\ndata warehouses that represent the existing data in the enterprise.\nSecurity for hybrid integration addresses the following needs:\nData integrity\nThreat management\nSolution compliance\nCapabilities include the following:\nIdentity and access management\nData and application protection\nSecurity intelligence\n\n\nSolution Reference Architectures\nChapter 8\n[ 188 ]\nSummary\nThis chapter provides the baseline architectures for some key modern business solutions.\nThey are meant to serve as starting points for effective solutions the reader may be tasked\nto deploy. While the solutions are not readily deployable as documented, they do provide\nguidance and industry best practice recommendations for a solution delivery team. When\nused collaboratively across the solution delivery team, these reference architectures will\naccelerate and improve almost any solution design job.\n\n\n9\nCloud Environment Key Tenets\nand Virtualization\nDesigning cloud computing solutions is not about transplanting the same enterprise data\ncenter design onto another platform. To properly leverage the highly automated and\ndynamic cloud computing platform, an architect must use the environment's elasticity and\nscalability to improve operational efficiency or economics. The concepts in this section are\nessential tools for accomplishing that task and modifying the data center architecture\ndesign to leverage these approaches will be the key to a profitable cloud computing\ntransition.\nThe following topics will be covered in this chapter:\nElastic infrastructure\nElastic platform\nNode-based availability\nEnvironment-based availability\nTechnology service consumption model\nDesign balance\nVirtualization\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 190 ]\nElastic infrastructure\nElastic infrastructures deliver preconfigured virtual machine servers, storage services, and\nnetwork connectivity using a self-service interface. This type of infrastructure provides the\nproper amount of dynamically-adjusted IT resources necessary for a stated level of service.\nAs the core capability of an IaaS service, this runtime infrastructure supports dynamic\nprovisioning and de-provisioning of servers, disk storage, and network connectivity. Real-\ntime monitoring of resource utilization is provided to enable traceable billing and\nautomation of all associated management tasks.\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 191 ]\nElastic platform\nThe provisioning of shared resources is an essential characteristic of cloud computing.\nSharing enables economies of scale associated with the cloud. Elastic platforms extend\nresource sharing to the operating systems and middleware. The extension enhances\neconomies of scale by increasing the utilization rates of these resources. This PaaS service\ndelivers application components to various customers on a shared middleware platform.\nMaintained by the service provider, the environment is also referred to as an integrated\ndevelopment environment (IDE). Customers build and deploy custom application\ncomponents to the middleware platform with a self-service interface. This enables resource\nsharing and automated middleware management.\n",
      "page_number": 158
    },
    {
      "number": 9,
      "title": "[ 191 ]",
      "start_page": 205,
      "end_page": 216,
      "detection_method": "regex_chapter",
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 192 ]\nNode-based availability\nWhen either an elastic infrastructure or platform is offered by a provider, service\navailability is defined by the conditions that must be fulfilled by the offering and the time-\nframe within which service availability is assured. These two factors enable the\ncomputation of hosted application availability. With node-based availability, the service\nprovider guarantees the availability of each application component.\nAvailable is generally defined as being reachable and capable of performing its advertised\nfunction. This availability time-frame is expressed as a percentage. As an example, an\navailability of 99.95% means that a component will be available 99.95% of the time. The\nfollowing figure depicts this example:\nEnvironment-based availability\nWhen using environment-based availability, a cloud provider expresses the availability of\nan elastic infrastructure or platform as a whole. Availability of an individual application\ncomponent using this type of expression is not defined. By communicating availability in\nthis way, a customer is better able to match how end users commonly express availability.\nAn example of this is expressing the availability of an at-least-once provisioned component\nor virtual server and the ability to provide a replacement if the first element fails. The\nfollowing figure depicts this example:\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 193 ]\nTechnology service consumption model\nThe IT service consumption is at the core of the industries embracing the OPEX expenditure\neconomic model. It also highlights the intersection of rapidly-growing demands, ever-\nevolving technology, and the IT-enabled business model. IT is no longer just a cost center. It\nallows business processes and transactions that weren't possible before. This\ntransformation from the traditional IT model to the pay-by-use contract avoids the direct\npurchase of equipment purchases and all expenses associated with operations and\nmaintenance. Today, the equipment acquisition model is risky and costly due to rapid\ntechnology and consumer demand changes.\nCompanies now have many cloud computing options. They can adopt a full private cloud\nstrategy, consume a single enterprise application from a public cloud, or take a hybrid\nroute and source-specific service assets as a complement to traditional data center solutions.\nSuccessful IT organizations must cope with a dynamic cloud service vendor landscape that\ncan deliver unprecedented choice.\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 194 ]\nIT and business leaders must team in the shaping of IT service consumption. When\nparticipating in this partnership, the IT organization must be both flexible and business\nmodel-aware. The previously ubiquitous approach of mandating technology equipment\nand use is no longer viable. In the face of this change, IT must still, however, maintain a\ncentral role across all stages of enterprise IT consumption. The IT team must become a\ntrusted broker of technology services to the LOB. Their role will be as a critical\nintermediary and orchestrator, managing services, procurement, and delivery. They will\nalso provide technical support and information technology security.\nThe consumption model shift requires the following:\nThe consolidation of IT and corporate financial management strategies in a way\nthat can also serve user needs through a consumption-based process across\ntraditional infrastructure as well as public, private and hybrid clouds.\nImplementing an enterprise-wide set of financial and operational controls.\nFlexible IT planning and governance that measures and bundles usage into\nbusiness-relevant IT service catalogs. This alignment will improve LOB resource\nvisibility and selection.\nThe use of a consistent and repeatable process for the timely capturing of \nconsumption data.\nData streamlines invoicing and revenue recognition.\nLink reporting views directly to resource usage and the transaction owner.\nThe use of fixed, variable, tiered, scheduled, and resource state pricing features.\nThe ability to track detailed usage and costs that also shows customer cost at a\ngranular level.\nThe establishment of alerts that capture and analyze data that compares allocated\nversus used capacity, usage trends, and forecasted usage across the entire\nenterprise.\nDesign balance\nWhen designing a cloud computing solution, the goal is balance across four specific\norganizational guidelines:\nEconomics targets\nOperational goals\nTechnological compatibility\nEnterprise governance (risks)\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 195 ]\nThe solution architect must understand, respect, and document each of these limits. These\nguidelines will set the barriers, boundaries, and expectations of just about every\nconversation and meeting you will encounter. Everyone will want it all, but your toughest\njob will be the developing, presenting, and explaining the data in a way that leads to\ncompromise and agreement.\nVirtualization\nVirtualization enables the high utilization and high efficiencies associated with cloud\ncomputing. This technology approach is used through the computing stack. This section\nprovides the background needed for the architect to understand how to use compute,\nnetwork, data, and application virtualization.\nCompute virtualization\nA hypervisor enables the sharing of common underlying physical hardware between\ndifferent applications. The hypervisor will also reduce an application's dependency on a\nspecific physical server by abstracting the hardware into virtualized instantiations. This\nallows various operating systems and middleware to be installed on the same physical\nserver while maintaining isolation regarding the use of resources such as central\nprocessing units (CPUs), memory, disk storage, and networking.\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 196 ]\nAlso known as a virtual machine monitor (VMM), the hypervisor may be software,\nfirmware, or hardware. The hypervisor manages requests from virtual machines.\nHypervisors come in two types:\nThe type 1 hypervisor runs directly on a bare-metal physical server. Sometimes\nreferred to as a bare-metal, embedded, or native hypervisor, type 1 has direct\naccess to the hardware. This type does not use a preloaded operating system.\nCompletely independent from the operating system, the hypervisor is small and\ncan monitor operating systems that run above it. Any problems manifesting in\none virtual machine or guest operating system do not affect the other running\noperating systems.\nExamples include VMware vSphere/ESXi product, XenServer, Red Hat\nEnterprise Virtualization (RHEV), open-source KVM (Kernel-based Virtual\nMachine), and Microsoft Hyper-V.\nType 2 hypervisors load inside an operating system like any other application.\nThe OS manages them, and its virtual machines will be slower than on the type 1\nvariety. These are also known as hosted hypervisors and are entirely dependent\non its host operating system for all operations. A type 2 installed on an operating\nsystem can also support other operating systems above it. Although the base\noperating system can allow better policy specification, any security\nvulnerabilities in the host operating system will affect the entire system,\nincluding the hypervisor running. Widely used type 2 hypervisors include\nVMware workstation, Microsoft Virtual PC, Oracle VirtualBox, and Parallels.\nType 1 hypervisors have dominated the server marketplace while type 2 hypervisors are\nmostly used on clients. Type 1 hypervisors running on client devices, however, are gaining \nmarket traction to support virtual desktop infrastructure (VDI) solutions).\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 197 ]\nNetwork virtualization\nThere are three major approaches to the virtualization of network services:\nThe first, referred to as network virtualization (NV), is a network tunnel. This\nversion is used to create tunnels through an existing network to connect two\nseparate domains. This is done as an alternative to physically connecting two\nnetwork domains. Using tunnels is valuable because it avoids physical labor\nassociated with physically installing new domain connection. This concept is\neven more critical when needed to connect virtual machines. NV also efficiently\nleverages capital investments in existing infrastructure thus avoiding additional\ncapital outlays. When network virtualization is used with high-performance x86\nplatforms, movement of VMs can be done independently of any existing\ninfrastructure connections, avoiding any physical network reconfiguration.\nThe second network virtualization approach, network functions virtualization\n(NFV), uses best practices as initial policies and configurations for all network\nelements. One widespread use is adding firewalls and IDS/IPS systems. NFV\nenables the addition of functions on selected network tunnels, allowing the\ncreation of virtual machine service profiles or flow. This approach avoids manual\nnetwork provisioning and any associated training cost. It can also eliminate the\nneed to practice network over provisioning of firewall or IDS/IPS services. By\ncustomizing these services for each instantiated network tunnel, initial CAPEX is\nreduced while simultaneously enhancing operational flexibility.\nThe third option, software defined networking (SDN) is used to program \nnetwork deployments through the use of a control plane and data plane. The\ncontrol plane dictates what data packets should go to which destination. The\ndata plane transports those packets and uses switches that are programmed\nusing an SDN controller. One industry standard control protocol is OpenFlow.\nNV and NFV both add virtual tunnels and functions to the physical network whereas SDN\nchanges the physical network. That makes SDN an externally-driven method for\nprovisioning and managing the network. Use cases include data flows to different ports\n(for example, from 1GE port to a 10GE port) or aggregating multiple small flows into a\nsingle port. SDN is implemented using network switches as opposed to using x86 servers\nfor NV and NFV.\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 198 ]\nNetwork virtualization technologies address mobility and agility. NV and NFV are used on\nexisting networks and reside on servers to interact with traffic directed to them. SDN is a\nnew network construct that uses switches to implement separate data and control plane\nfunctions.\nData virtualization\nData virtualization is used to retrieve and manipulate data without requiring the related\ntechnical data details like format or location (federated/heterogeneous data joins).\nAbstracted technical include API specifications and access language. This option can\nfacilitate connections to heterogeneous data sources making all data accessible from a single\nlocation. Data federation can also be accomplished with data virtualization by using it to\ncombine data result sets across multiple sources. When operational data and\nprocessed/cleansed data is needed to support of real-time data requirements, data\nvirtualization software is used for data integration, business process integration, service-\noriented architecture data services, and enterprise search is ideal.\nIn a cloud computing environment, data virtualization decouples data analytics and\napplications from physical data structures. This minimizes end-user impact if data\ninfrastructure is changed. Another cloud use is linking between NoSQL sources and\nrelational data sources.\nCloud solution architects should always architect with an enterprise view. With evolving\norganizational data virtualization requirements, these solutions can become less agile and\ndeliver lower performance as more layers and objects are added. Duplicate business logic\nand dependencies may also affect performance. In mitigating these challenges, the cloud\nsolution architect should design a layered view approach that isolates business logic.\nAlways use consistent naming standards and common rules for reusability and layer\nisolation and push down processing requirements to the source as much as possible.\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 199 ]\nWith data virtualization serving a gateway into corporate data assets, it should be governed\nas such. Data virtualization concepts and capabilities must also be implemented\nconsistently across the enterprise. Data security will strongly impact data virtualization\nsecurity management so data security managers should determine applicable regulatory\nguidance (such as HIPAA, SOX, and so on). In many situations, data virtualization should\nbe used to limit access for specific users or user groups.\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 200 ]\nMany data virtualization tools can be used to display and export data lineage information.\nThis forms an important component of business process metadata. This tool can can be\nused to analyze and resolve data quality issues. Data virtualization is a powerful\ntechnology, but enterprise governance that balances data management structure, data\nvirtualization uptake, and innovation must be in place.\nApplication virtualization\nApplication virtualization segregates computer programs from the underlying operating\nsystem. They do not install like a normal application, but execute as if they were. Contrary\nto how normal computing applications, when using application virtualization, each\napplication sets its configurations at runtime. This leaves the host operating system and\nexisting environments unaltered. The application will behave like it is interfacing directly\nwith the operating system. Application virtualization is another technology that allows for\nthe dynamic distribution of computing resources. Application virtualization allows\napplications to run in non-native environments.\n\n\nCloud Environment Key Tenets and Virtualization\nChapter 9\n[ 201 ]\nSummary\nSuccessful cloud computing solutions align with the cloud environment key tenets outlined\nin this chapter. They also use compute, network, and application virtualization to delivery\nscalability and elasticity to the business or mission model. In the end, this is why cloud\ncomputing has revolutionized every industry vertical. The cloud solution architect must be\nintimately familiar with the purpose and tenor of every one of the environment tenets\noutlined in this chapter, because their job will depend on how they deliver value.\n\n\n10\nCloud Clients and Key Cloud\nServices\nOne of the five essential cloud computing characteristics is ubiquitous access. Network\nconnectivity and the user's device client are necessary to enable this capability. Client\nselection typically depends on a decision between using a native application or a web app.\nIn this chapter, we will cover the following topics:\nCloud computing clients\nIaaS\nCommunications services\nAuditing\nCloud computing clients\nNative applications are built and designed for specific mobile devices. They install directly\non the hardware after being downloaded from app stores or marketplaces. These \napplications are designed to be compatible with native features of the target device\nhardware and can work as standalone entities. An important drawback, however, is that\nusers need to continually update the app.\nWeb apps are accessible via the mobile device web browser and are not downloaded onto\nthe user's device. They can only access a limited number of the device's native features and\nupdate themselves without user intervention. This development option uses languages\nsuch as JavaScript, HTML 5, or CSS3 but no standardization or SDK is available. Web apps\nmay also lead to higher maintenance cost across multiple mobile platforms.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 203 ]\nFrom a user point of view, both options look and operate in similar manners. The choice is\nnormally between deploying a user-centric app or an application-centric app. Sometimes,\nboth native and web apps are developed to expand user reach and provide a better overall\nuser experience. Client developers must also choose between using a thin or thick client.\nThe following table shows the difference between a thin and a thick client:\nA terminal emulator emulates a video terminal within another display architecture. Usually\nsynonymous with a shell or text terminal, the term refers to all remote terminals. A terminal\nemulator that resides inside a graphical user interface (GUI) is called a terminal window. It\nallows user access to text terminal applications like command-line interfaces (CLI) and\ntext-user interface (TUI).\nRapidly growing in popularity as a cloud computing concept is the Internet of Things\n(IoT). A customer does not buy an IoT, but customers purchase solutions that use IoT\ncomponents. An IoT solution will typically tap into an ecosystem of partners that span a\nvalue chain from sensors to the application.\nThe genesis of an IoT solution is making things smart by combining sensors, connectivity,\nand software. Machine-to-machine (M2M) solutions focus on connectivity, where the goal\nis to deliver intelligence by connecting items. Reporting data from these devices provided\nhistorical business intelligence versus real-time insight. IoT solutions focus on smart versus\nconnected and near real-time analysis. The goal here is insight and action rather than\nreporting.\n",
      "page_number": 205
    },
    {
      "number": 10,
      "title": "[ 203 ]",
      "start_page": 217,
      "end_page": 237,
      "detection_method": "regex_chapter",
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 204 ]\nWhen designing IoT solutions, the architect should identify a precise problem that has a\nquantifiable response. Early IoT solutions used instrumented devices that sent data back to\nthe cloud for processing. This approached proved impractical because physical-world\nsolutions required a certain amount of processing at the edge for various reasons. Data\nneeded to be processed and acted upon immediately using new concepts, such as fog\ncomputing. It could also be difficult to send large volumes of data over the network.\nIoT solutions also need to be able to do correlation and analytics at the edge. Therefore,\narchitects need to develop strategies that define when and where to process data. They\nmust also select equipment with this type of edge processing in mind. The most valuable\nsystems will offer machine learning that identifies patterns quickly and delivers timely and\nrelevant information to its users.\nMake sure the solution also preserves sensor battery life. Hardwiring an electrical line to\neach sensor is cost-prohibitive, but battery-powered sensing is useless if you need to\nexpend labor to replace batteries too often. End sensing devices must, therefore, have\nsoftware that optimizes battery life when processing and delivering data. IoT security can\nalso be more challenging. Encryption is widely used and identity and authentication should\nprovide additional protections. The network, hardware, and people accessing the data\nshould all be trusted entities. IoT solutions must also be field-upgradable.\nIaaS\nInfrastructure-as-a-Service delivers compute, storage, communications (which includes\nnetwork services), metering/monitoring, and auditing services.\nCompute services\nCompute combines CPUs, memory, and disks virtual equipment to create virtual machines.\nThis is done by virtualizing physical servers and storage devices shared by multiple users.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 205 ]\nA virtual machine is a computer software file, called an image, that acts like a real\ncomputer. It runs in a segregated environment, just like any other software program, and\ngives the end user the same experience as they would have on a traditional host operating\nsystem. The software inside a virtual machine is sandboxed and cannot escape or tamper\nwith the physical computer. Multiple virtual machines can operate simultaneously on the\nsame physical computer. This is called a multi-tenant environment, Multiple operating\nsystems run side-by-side on the hypervisor, which manages them. Each virtual machine\nindependently provides its virtual hardware. The virtual hardware is mapped to the actual\nhardware on the physical machine. This mapping saves costs by reducing physical\nhardware quantities, associated maintenance costs, power consumption, and cooling\ndemands. Virtual servers scale quickly but may have reduced performance when compared\nto bare-metal servers.\nA bare-metal server is a single tenant physical server that is dedicated to one customer. This\nprevents server performance from being impacted by other workloads. This service is\ntypically used for latency-sensitive workloads that require a significant amount of raw\nprocessing power. Bare-metal clouds provision bare-metal servers with on-demand access,\nhigh scalability, and pay-as-you-go features. Bare-metal cloud economics can be compelling\nif the solution architect is faced with high load factors. They can be cheaper on a per-\nworkload basis in environments where the virtual machines are large and continually\nheavily loaded.\nCloud service providers typically provide a choice of operating systems to their customers.\nUsually, this involves different versions of Linux (RHEL, Ubuntu, CENTOS, Freebird) or\nMicrosoft Windows, Solaris, or IoS. The architect should poll organizational users and\nrequirements to select the most appropriate OS. CSPs will package their compute offering\nwith variations on the number of computing cores, amount of RAM, IOPs, and available\nephemeral storage. Autoscaling is used to automatically change the number of\ncomputational resources deployed to a server farm. This is usually measured in the number\nof active servers. This number rises or falls automatically based on the farm workload.\nStorage services\nIaaS storage services are either ephemeral or persistent. Ephemeral storage persists only\nwhen a specific virtual machine is live. If that machine is de-provisioned, any data in its\nephemeral data store is lost. The random access memory (RAM) and cache are typically\nnon-persistent storage technologies.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 206 ]\nTransfer all ephemeral data to a persistent data store to prevent data loss. Persistent\nstorage, as its name implies, persists after a virtual machine is de-provisioned and is\nsometimes referred to as non-volatile storage. This storage type is typically backed by\nmechanical hard disk drives or solid state drives in either a storage area networks (SANs)\nor network attached storage (NAS) schema. This can be in the form of file, block, or object\nstorage.\nFailure tolerance in a storage offering duplicates data across multiple copies. These copies\nstore the same set of data. If one of these copied versions is lost, data is still recoverable\nfrom the other copies. Storage consistency is a fundamental concept in cloud computing\nand describes the time it takes for all data copies to be the same. Strict consistency ensures\nthat all copies of the data have been duplicated among all relevant copies to increase\navailability. A subset of the data copies is accessed by read and write operations. The ratio\nof replicas versus the number of replicas accessed during read and write operations can\nguarantee consistency across all copies. In eventual consistency, the consistency of data is\nrelaxed.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 207 ]\nThis reduces the number of replicas that have to be accessed during read and write\noperations and reduces the overhead required to maintain strict consistency. With eventual\nconsistency, data changes are eventually transferred to all data copies through the\nasynchronous propagation via the network.\nVolume/block storage\nVirtual and physical servers can be managed more efficiently if they don't store state\ninformation locally. This makes provisioning, de-provisioning, and failure handling much\nmore manageable. Volume/block device storage is centralized storage that is accessed by\nservers as if it was a local hard drive.\nObject/blob storage\nDistributed cloud applications are widely used to handle large data elements. Also referred\nto as binary large objects (blobs), some examples include virtual server images, pictures,\nor videos.\nThese types of data elements are organized in a folder hierarchy where each data element\nhas a unique identifier that includes its location and a file name. This globally unique\nidentifier is passed to the storage offerings to retrieve data over the network.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 208 ]\nKey-value storage\nFor higher availability and performance, storage offerings distributed data across different\nIT resources and locations. This can change storage requirements and increase the demand\nfor a more flexible data structure. In these situations, data structure validation during\nqueries may require high-performance connectivity between the distributed resources.\nPerformance issues are avoided by storing identifiers (keys) and associated data (values)\npairs and not enforcing data structure. Data query complexities are reduced significantly\nwhile simultaneously enhancing scalability and configurability. Semi-structured and\nunstructured data can also be scaled out among many IT resources without needing to\naccess them in order to evaluate expressive queries.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 209 ]\nArchival storage\nArchival storage is very long-term data storage that uses SAN, optical, or magnetic tape\ntechnologies. This service is used to meet regulatory or legal retention requirements and is\nused to store data that does not require quick access.\nCommunications services\nCommunication services encompass all of the functions normally associated with the\nnetwork. These services are metered, based on the amount of data throughput or the\nnumber of input/output operations.\nVirtual networks\nVirtual networks support application components deployed on elastic infrastructures and\nplatforms. These virtual communications resources rely on physical network hardware to\ncommunicate. Customers are, however, isolated from each other on this networking layer.\nPhysical resources, such as networking interface cards (NICs), switches, and routers, are \nabstracted into virtualized equivalents that can be managed by service provider customers.\nUsing the self-service interface and CSP applications, customers can design, implement,\nand configure virtual circuits, firewalls, load balancers, network address translations\n(NAT), and network cross-connects.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 210 ]\nMessage oriented middleware\nWith distributed applications, application components hosted on different cloud resources\nneed to exchange information. Often, this also requires integration with other cloud and\nnon-cloud applications. Using message-oriented middleware, communication partners can\nexchange information asynchronously using messages. This service handles addressing, the\navailability of communication partners, and message format transformation.\nExactly-once delivery\nThis service is used for systems in which duplicate messages are unacceptable. For these\ncases, the messaging system ensures delivery of each message only once by automatically\nfiltering all possible duplicates. When created, each message is tagged with a unique\nidentifier. This identifier filters out message duplicates during transmission from sender to\nreceiver.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 211 ]\nAt-least-once delivery\nIn some solutions, message-oriented middleware handles message duplicity. The system\nstill, however, needs assurances that the message is received. With at-once-delivery, a\nservice acknowledgment is sent back to the message sender for each message retrieved by a\nreceiver. If this response is not received within a specific time frame, the message is resent.\nTransaction-based delivery\nAlthough message-oriented middleware can manage message traversal, an assurance that\ntransmissions are received may also be required. With a transaction-based delivery service,\nboth the message-oriented middleware and the receiving client participate in the\ntransaction. All message communications operations are, therefore, performed under a\nsingle transactional context guaranteeing ACID (short for Atomic, Consistent, Isolated,\nDurable) behavior.\nTimeout-based delivery\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 212 ]\nTimeout-based delivery service assures that a client receives a message before being deleted\nfrom a message queue. This is done by not deleting the message immediately after a client\nhas read it, but only marking is as being invisible. After the client has read a message, it\nsends an acknowledgment to the message queue and the message is deleted.\nMetering/monitoring\nThe dynamic nature of the cloud and its pay-by-use economic model makes monitoring a\ncrucial architectural component. Monitoring also forms the basis for metering which\nmeasures how use resources are used to support the charges levied. Metering also enables\nthe automation of variable service quantities to support large variances in customer\nrequirements.\nMetrics used in metering services include the following:\nPer-unit of time for services\nPer-unit of data\nPer-transaction\nPer-user\nOne-time charges\nServices are monitored and metered across the operating system, network, and application\ncategories. The operating system is fundamental to all monitoring and some of the\nfundamental tools are as follows:\nSyslog: Access to log entries that contain a description of the application that\ngenerated the message, severity level, time stamp, and the message\nVmstat: Virtual memory statistics\nMpstat: Processor-related statistics, activities of each available processor, and\nglobal averages\nTop: Linux tasks and a system summary\nWell known open source tools include:\nNagios: Availability, CPU load, memory, disk use, users logged in, and processes\nMunin: Performance monitoring tool that provides detailed graphs of system\nperformance over a web interface\nThe primary network metering tool is Netstat, which provides data on network\nconfigurations, connection activity, and usage statistics.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 213 ]\nApplication monitoring needs to identify when a problem occurs, while analysis will locate\nrelated application structure issues. This is primarily an operations team responsibility\nwhile performance analysis is a development team responsibility. Performance analysis\nincludes profiling, which requires insight into the application structure. Analysis tools\nshould be able to capture output over time for use as input to monitoring applications.\nMonitoring results need to be actionable, but limited control over the production system\nmight not afford the capability to reproduce problems or to do in-depth analysis. Tools that\nenable profiling can, however, significantly reduce application performance.\nAuditing\nInformation technology audits fall into either the internal or external categories. Internal\naudits address work done by the organization employees. These look at organizational\nprocesses and primarily focus on process optimization and risk management. External\naudits look at an organization's ability to meet legal and regulatory requirements from an\noutside perspective. Audits can also evaluate data availability, integrity, and confidentiality\nissues. A cloud solution requires a three-way negotiation among service organizations,\ncloud service providers (CSPs), and end users. The goal is to ensure productivity while\nmaintaining an acceptable degree of security.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 214 ]\nCloud security audits look at whether security-relevant data is transparent to CSP\ncustomers, data encryption policies, and protections that address the co-located customer\nenvironment. The scale, scope, and complexity of cloud computing audits are also\nsignificantly different than a traditional enterprise equivalent. A significant challenge,\nhowever, lies in an auditor's cloud computing knowledge. Cloud security auditors must\nknow cloud computing terminology and have a working knowledge of a cloud system's\nservice design and delivery method.\nCloud security audits must make sure that all security-relevant data is available to CSP\ncustomers. Transparency enables rapid identification of potential security risks and threats.\nIt also helps in the creation and development of appropriate enterprise countermeasures\nand recommendations. Access to accurate information reduces the risk of cyber security\nthreats.\nData should be encrypted at rest, in motion, and, if possible, when in use. Encryption may\nnot always be the most efficient solution and encryption key management options aren't\nalways acceptable. Encryption and decryption performance shortcomings may make\nencryption at rest non-viable. Data in motion is usually encrypted using transport layer\ntechnologies like secure socket layer. Homomorphic encryption or encryption in use can\nallow encrypted queries to search encrypted texts without search engine decryption. It has\nthe potential to solve the security issue of encrypted data at rest in both traditional IT and\ncloud infrastructures, but performance is still lacking.\nWhile co-location enables the economic advantages of the multi-tenant environment, it also\nintroduces some significant security concerns. An audit must ensure that the CSP\nhypervisors can reliably insulate virtual machines (VMs) from the physical computing\nhardware. A CSP must balance the multiple ways to build and manage cloud infrastructure\nhypervisors each with business needs and relevant security issues. In spite of the need to\nestablish standardize cloud computing structures and multi-tenant security, no official\nstandard exists.\nWith cloud computing, a single physical machine will typically host many virtual\nmachines. Hosting multiple VMs can drastically increase the number of hosts that need to\nbe audited. This increase can make the scale, scope, and complexity of cloud audits\noverwhelming. Standardization can dramatically assist in making the auditing process\nsmoother and faster despite the larger scale of cloud computing. Another critical factor to\nconsider is an adjustment of the audit scope.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 215 ]\nWhile increased numbers of IT elements requiring audit drive scale issues, new technology\ntypes cause scope increases. An example is the examination of hypervisor security in the\nmulti-tenant environment. Also, many cloud environments include intangible and logical\nelements that also require an audit. Auditors must be aware of these differences and take\nthis complexity into account.\nCloud service performance can vary based on the specific CSP. Within the same CSP,\nperformance can also be dependent on service configuration, time (time of day, the day of\nthe week, week of the month, and so on) and geographic location. Performance variance of\nover 1000% in compute services alone has been observed. Since pricing is typically a fixed\nrate tied to a specific metric, this will often lead to widely-differing price/performance\nvalues\nCompute metrics recommended for auditing are CPU and input/output (I/O) performance.\nNetwork metrics such as latency and bandwidth allocation should also be measured from\nmultiple CSP locations\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 216 ]\nService level agreement\nThe service level agreement (SLA) serves as both the blueprint and warranty for cloud\ncomputing services. Its purpose is to document specific parameters minimum service levels\nand remedies for any failure to meet the specified requirements. It should also affirm data\nownership and specify data return and destruction details. Other important SLA points to\nconsider include the following:\nCloud system infrastructure details and security standards\nCustomer right to audit legal and regulatory compliance by the CSP\nRights and cost associated with continuing and discontinuing service use\nOther important criteria\nService availability\nService performance\nData security and privacy\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 217 ]\nDisaster recovery processes\nData location\nData access\nData portability\nProblems identification and resolution expectations\nChange management processes\nDispute mediation processes\nExit strategy\nCustomers should read the cloud provider's SLA very carefully and validate them against\ncommon outage scenarios. Organizations should also have contingency plans in place to\nsupport worse case scenarios.\nPaaS\nPlatform-as-a-Service (PaaS) is an execution runtime environments offered in a multi-\ntenant environment. The underlying assumption is that applications often use similar\nfunctions and that these components can be shared with other applications. Sharing this\ncommon functionality should also result in higher environment utilization rates.\nCommon application functionality is offered in an execution environment that delivers\nplatform libraries for custom application implementations using middleware solutions. The\nmost common of these are databases and integrated development environments (IDEs).\nDatabase\nDatabase PaaS services typically align as either a SQL/relational form or NoSQL/non-\nrelational type.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 218 ]\nSQL/relational databases handled data comprising large numbers of similar data elements.\nThese elements have identifiable dependencies among each other. When this structured\ndata is queried, users make certain assumptions about the data structure and the\nrelationship consistency between the retrieved data elements.\nData elements are recorded in tables, where each column represents a data element\nattribute. Table columns may also embed dependencies for how entries in one table column\nrelate to a corresponding column in a different table. These dependencies are strictly\nenforced during any data manipulation.\nIn No-SQL/non-relational databases (Mongo, Map Reduce, and so on), an enforced\ndatabase structure does not exist. This is useful when processing large data sets and the\nprocess is split up and mapped to multiple application components. This is often the case\nwith cloud applications, usually handle enormous amounts of data, which need to be\nprocessed efficiently. As distributed applications are scaled out, data processing is similarly\ndistributed among multiple components.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 219 ]\nThe data processing components simultaneously execute the query to be performed on the\nassigned data chunks. Afterwards, the processing results are consolidated or reduced into\none result data set. During this reduction, additional functions (sums, average values, and\nso on) can also be applied.\nIntegrated Development Environment\nAn IDE provides an application development environment for developers that is managed\nby a cloud service provider. This eliminates the complexities associated with maintaining\nand operating the application development infrastructure. Developers can access and\nadminister PaaS services via a web browser or IDE plugin. Some common PaaS IDEs\ninclude:\nElastic Beanstalk, native to Amazon Web Services (AWS). The code is uploaded\nand the PaaS automatically deploys the WAR file to one or more EC2.\nHeroku, which uses standard libraries with application servers (such as Tomcat\nand Jetty) but is extensible and natively supports Ruby, Node, Python, Java,\nClojure, Go, Groovy, Scala, and PHP.\nRed Hat OpenShift, which supports Java, Ruby, Node, Python, PHP, and Perl.\nIBM Bluemix, which is based on CloudFoundry, is extensible, and natively\nsupports Java, Node.js, PHP, and Python.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 220 ]\nGoogle App Engine (GAE) which runs applications in a sandboxed environment\nand requests are distributed across multiple servers. This is specifically used for\nthe building and deployment of applications onto Google's infrastructure.\nSaaS\nSoftware-as-a-Service provides a fully managed application. The consuming organization\nmanages the user base, access to the service, and the governance of the data inputted by\norganizational users. The CSP has full responsibility for the architecture, security, and\navailability of the service. Some of the most popular SaaS service offering categories are as\nfollows:\nCRM software: Customer information management, marketing automation, and\nsales pipeline tracking.\nERP software: Improved process efficiency and organizational information\nsharing combined with improved management insight into workflow and\nproductivity.\nAccounting software: Improved financial organization and tracking\nProject management software: Project/program scope, requirements, and\nprogress management. The tracking of changes, communications, and deadlines\nin a way that meets stakeholder requirements.\nEmail marketing software: Automate email marketing and relationship\nbuilding, while optimizing message delivery.\nBilling and invoicing software: Billing and invoicing automation. Implementing\ncustomer self-service payment options. The reduction of data entry costs, and\nelimination of billing errors.\nCollaboration software: Improved organizational communications that\nempower employees to more easily follow complex interactions. More efficient\ncommunications and enhanced enterprise productivity.\nWeb hosting and e-commerce: Web hosting, content management systems,\nmessage boards, shopping carts, and so on.\nHR software: Employee time tracking. Improved recruiting and hiring.\nAutomate payroll and more efficient management of human resources.\nTransaction processing: Credit cards and bank transfer processing. Publish and\ntrack coupons, in support of loyalty rewards programs.\n\n\nCloud Clients and Key Cloud Services\nChapter 10\n[ 221 ]\nSummary\nDelivering the desired cloud service through the appropriate client device is the desired\nresult of any cloud computing solution. This chapter reviewed the basics needed to achieve\nthat goal. Clients must seamlessly align with the expected end user consumption\nrequirements while the services, as always, must align with the business or mission model.\nThe services reviewed in this chapter are the essential solution building blocks.\n\n\n11\nOperational Requirements\nUntil recently, enterprises designed data centers with the mindset of supplying hosting,\ncompute, storage, or other services with typical or standard organization types in mind.\nThis mindset is a problem when considering modern-day data centers and cloud service\nofferings. As cloud-based application development continues to gain popularity and\nwidespread adoption, it is important for us to recognize the benefits and efficiencies, along\nwith the challenges and complexities. Cloud development typically includes integrated\ndevelopment environments, application lifecycle management components, along with\napplication security testing. Unlike traditional deployments within a data center or even a\nhosted solution where network controls are ubiquitous and compensating perimeter\ncontrols are sometimes depended upon to offer application security, cloud applications\noften run in a comparatively unprotected fashion.\nIn this chapter, we will cover the following topics:\nApplication programming interface\nCommon infrastructure file formats d VMs\nData and application federation\nDeployment\nFederated identity\nIdentity management\nPortability and interoperability\nLifecycle management\nLocation awareness\nMetering and monitoring\nOpen client\nAvailability\nPrivacy\nResiliency\nAuditability\n\n\nOperational Requirements\nChapter 11\n[ 223 ]\nPerformance\nManagement and governance\nTransaction and concurrency across clouds\nSLAs and benchmarks\nProvider exit\nSecurity\nSecurity controls\nDistributed computing reference model\nApplication programming interface\nOrganizations and practitioners alike need to understand and appreciate that cloud-based\ndevelopment and applications can vary from traditional or on-premise development. When\nconsidering an application for cloud deployment, one must remember that applications can\nbe broken down into the following sub-components:\nData\nFunctions\nProcesses\n\n\nOperational Requirements\nChapter 11\n[ 224 ]\nThese components can be further broken up, so that portions that have sensitive data run in\na traditional data center and less sensitive data runs in a cloud computing environment. It\nis also important for developers to understand that, in many cloud environments, access is\nacquired through the means of an application programming interface (API). These APIs\nwill consume tokens rather than traditional usernames and passwords. APIs can be broken\ndown into two formats:\nRepresentational state transfer (REST)\nSimple object access protocol (SOAP)\nREST defines a set of constraints and properties based on HTTP. These are referred to as\nRESTful web services and conform to the REST architectural style. By doing this they\nprovide interoperability when computers communicate across the Internet. REST-\ncompliant services allow the requesting systems to access and manipulate textual\nrepresentations of web resources by using a standard set of stateless operations. SOAP, also\nreferred to as simple object access protocol, is a more structured messaging protocol\nspecification used predominately for exchanging structured information through web\nservices across computer networks. The purpose of SOAP is to deliver extensibility,\nneutrality, and independence. It uses the XML information set for its message format and\nrelies on application layer protocols, usually Hypertext Transfer Protocol (HTTP) or\nSimple Mail Transfer Protocol (SMTP), for message negotiation and transmission.\nThe application programming interfaces (APIs) are a means for a company to expose\nfunctionality to applications. Some benefits of APIs include the following:\nProgrammatic control and access\nAutomation\nIntegration with third-party tools\nConsumption of APIs can lead to the use of insecure products by a company. Organizations\nmust also consider the security of software (and APIs) outside of their corporate\nboundaries. Consumption of external APIs should go through the same approval process\nused for all other software being consumed by the organization. When leveraging APIs,\nensure that API access is secured. This requires the use of SSL (REST) or message-level\ncrypto (SOAP), access authentication, and logging of API usage.\n",
      "page_number": 217
    },
    {
      "number": 11,
      "title": "[ 224 ]",
      "start_page": 238,
      "end_page": 249,
      "detection_method": "regex_chapter",
      "content": "Operational Requirements\nChapter 11\n[ 225 ]\nAPI levels and categories\nThere are four levels of APIs that developers must work with. For more information refer\nto: IUUQ\u001c\u0011\u0011XXX\u0010KBTPOHBVESFBV\u0010DPN\u0011\u0014\u0012\u0013\u0014\u0011\u0012\u001a\u0011DMPVE\u000fDPNQVUJOH\u000fVTF\u000fDBTF\u000fQBSU\u000f\u0015\u0010IUNM\u0001\u0001\u0001\nIJTUPSZ\u001f\u0012BNQ\u001dQGJE\u001f\u0013BNQ\u001dTBNQMF\u001f\u0014\u0017BNQ\u001dSFG\u001f\u0013.\nCommon APIs for cloud storage\nData is central to operations so establishing standard APIs for accessing cloud storage\nservices, databases, and other middleware services is a must. The use of custom code\nwithin a solution locks the enterprise into a proprietary design, eliminating portability, and\neliminating the financial benefits and flexibility afforded by cloud computing.\nCommon cloud middleware API\nAPIs needed to support creating and dropping databases and tables, connecting to message\nqueues and other middleware operations should be consistent across the enterprise.\nEmbedded restrictions in a database vendor product can significantly increase processing\nresource requirements when dealing with large datasets. Examples include restrictions on\njoins across tables and an inability to support a valid database schema. Such restrictions\ncreate significant challenges when contemplating a move to a different database.\nLimitations are especially applicable for applications built on a relational model.\nMiddleware services such as message queues are more straightforward and will typically\nnot present such a significant challenge.\nAdditional concerns\nA cloud solution architect must focus on meeting an organization's operational\nrequirements. Lessons learned from previous cloud deployments have highlighted the\ncriticality of addressing each of the requirements listed in this section adequately.\n\n\nOperational Requirements\nChapter 11\n[ 226 ]\nCommon infrastructure file formats ` VMs\nVirtual machine portability is a significant concern in a cloud computing environment.\nConcerns are especially valid in a hybrid IT deployment. Any enterprise solution should\naddress possible differences in both the VM file format and the process for attaching\nstorage to VMs.\nData and application federation\nWhen combining data from multiple cloud-based sources, enterprise applications need to\ncoordinate the applications activities that may span multiple platforms; cloud managed\nservice provider and traditional data centers. Hybrid environments require implementing\ndata federation and virtualization techniques across the various environments.\nDeployment\nCloud application deployment involves both programming interfaces and cloud-specific\npackaging technologies. This operational requirement may include traditional packaging\nmechanisms like EAR/WAR files and .Net assemblies.\nBuilding and deploying a VM image should be simple and portable between different\nhybrid infrastructure environments. Any required compensations should be well known\nand mechanisms for attaching storage to VMs well understood.\nFederated identity\nWhen operating in a hybrid environment, the idea is to have the user maintain\nresponsibility for a single ID with the infrastructure federating all other required identities.\nThis federation would include the primary identity needed by an end user and all\nassociated enterprise roles that the user is likely to hold within the enterprise.\n\n\nOperational Requirements\nChapter 11\n[ 227 ]\nIdentity management\nMost cloud computing solutions can leverage industry-specific identity management\nstandards and protocols, such as SAML and OAuth. These may also need to interact with\ntraditional standards such as RosettaNet or OASIS. Although the specific standard may\nvary between applications, the solution must be able to handle all access and data\nauthorization scenarios efficiently.\nPortability and interoperability\nThe cloud computing era brings with it the need to design, build, and manage a business-\nfocused ecosystem. Efficient communication and interaction across such an ecosystem\nrequire interoperability between the enterprise and its ecosystem partners. Since a universal\nset of standards does not exist and most likely won't exist shortly, these ecosystems can\nencounter a significant risk of vendor lock-in. An ecosystem's ability to use reusable\ncomponents to build systems that work together out of the box depends on the enforcement\nof portability and interoperability governance. A particular concern for in-cloud computing\nthis is critical during the deployment or migration of systems to a cloud service provider. A\ntypical scenario is an inability to migrate some components to the cloud due to data\nmanagement or data sovereignty regulations. Cloud migration requires portability of all\nmigrating components as well as interoperability of those components with systems that\nremain on-premise.\nSpecific technology categories where portability and interoperability standards should be\nspecified include the following:\nData: Enabling the reuse of data components across different applications. Since\ndata interoperability interfaces do not currently exist, this may require the use of\ndata virtualization techniques.\nApplications: This focuses on interoperability between application components.\nThese have SaaS deployed components, application modules leveraged in a PaaS,\nor infrastructure components consumed as IaaS. Similar issues arise in a hybrid\nenvironment when interfacing with a traditional enterprise IT environment or\nwith client endpoint devices. Application portability enables the re-use of all\napplication components across the entire hybrid IT environment.\nPlatforms: This category addresses the re-use of service bundles that may contain\ninfrastructure, middleware, or application components along with any associated\ndata.\n\n\nOperational Requirements\nChapter 11\n[ 228 ]\nInfrastructure: Interoperability and portability associated with various hardware\nvirtualization technologies and architectures.\nManagement: Management interoperability is interoperability between cloud\nservices (SaaS, PaaS, or IaaS) and programs concerned with the implementation\nof on-demand self-service. Management may also include application programs\nconcerned with the deployment, configuration, provisioning, and operation of\ncloud resources.\nPublication and acquisition: The self-service aspect of cloud computing gives\nend users the ability to acquire software, data, infrastructure and various other\ncloud services. Developers can also publish applications, data, and cloud services\nvia online marketplaces. This category addresses interoperability between\nplatforms and cloud service marketplaces, including app stores.\nLifecycle management\nLifecycle management of applications and documentation is a continuous challenge to all\norganizations. Tasks that fall within this requirement include versioning, data retention,\nand destruction and information discovery. Legal liabilities can be substantial if due\ndiligence is not effective in identifying regulatory and legal restriction in this area.\nLocation awareness\nNational data sovereignty laws are expanding globally. These new requirements not only\napply to how an organization handles data but it also equally apply to data managed on the\norganization's behalf. The associated requirement may include legal restrictions on the\nlocation of the physical server when organizational data is present. Meeting location-\ndependent legal requirements may require the use of APIs that determine the location of\nthe physical hardware associated with the delivery of all cloud services.\nMetering and monitoring\nThe pay-as-you-go cloud computing model requires consistent and ubiquitous metering\nand monitoring of all cloud services. This capability is essential to an effective cost control,\ninternal charge-backs, and service provisioning process.\n\n\nOperational Requirements\nChapter 11\n[ 229 ]\nOpen client\nUbiquitous access to cloud services levies a requirement for the use of open clients and\nendpoint devices. The use of vendor-specific endpoints violates this essential requirement\nas cloud services should not require the use of vendor-specific platforms or technologies.\nAvailability\nCloud service availability describes the degree to which a specific service is in a specified\noperable and committable state if a provisioning a request at a random time. Availability is\nusually expressed as a percentage and stated in the CSP service level agreement. The CSP\nsets availability, but additional payments can enhance this value. The solution architect\nshould be aware of all service availability rates and advise mission/business owners on the\nservice's ability to meet organizational goals.\nPrivacy\nPrivacy addresses the condition of being free from observation or disturbance by others.\nCloud computing has led to the establishment and strict enforcement of many new data\nprivacy laws. One of the most evasive of these is the General Data Protection Regulation\n(GDPR). Approved by the EU parliament on 14 April 2016, its enforcement date is 25 May\n2018. A non-compliant organization can face hefty fines. GDPR invalidates the Data\nProtection Directive 95/46/EC and has a goal of harmonizing European data privacy laws,\nprotecting and empowering all EU citizens' data privacy, and reshaping the how regional\norganizations approach data privacy. It applies to all personal data processing of EU\nresidents, regardless of the company's location. It also covers data processed outside of the\nterritorial limits of the EU and to the processing of personal data for EU citizens by non-EU\ncompanies when selling goods or services to an EU citizens. Penalties can be up to 4% of\nannual global revenue or f20 million, whichever is greater.\nResiliency\nResiliency refers to the ability of a cloud service to recover from service delivery difficulties\nor failure. The CSP sets resiliency levels, but additional payments can enhance the property.\nThe solution architect should be aware of all service resiliency specifics and advise\nmission/business owners on the service's ability to meet organizational goals.\n\n\nOperational Requirements\nChapter 11\n[ 230 ]\nAuditability\nAuditability describes the extent to which a cloud service consumer can conduct a thorough\nand accurate assessment of the cloud service provider's ability to deliver and appropriately\naccount for the cost of delivering a cloud service. This sort of data is typically driven by\nlegal or regulatory requirements and is often foundational to an organization's ability to use\na service at all. The solution architect should be aware of all audit requirements and advise\nmission/business owners on the service's ability to meet them.\nPerformance\nWhile the service level agreement outlines the minimum level of service expected from a\nprovider, performance may still vary widely across any specified parameter set. Service\ncomponents that lie entirely outside the provider or consumer's control may drive\nvariability. Things like network bandwidth limitations or abnormally large service\nprovisioning request can dramatically affect the cost or availability of a service.\nPerformance variability and auditing should, therefore, be directly addressed by the cloud\nsolution architect.\nManagement and governance\nThe ease of use associated with to opening an account and using cloud services creates the\nrisk of abuse in the provisioning and consumption of cloud-based services. Cloud industry\nleaders often highlight this risk as a significant security risk. Organizations must, therefore,\nestablish strict management and governance procedures. Recommendations are to include\ntracking for initiation and use of cloud services like storage, databases, and message queue\nvolumes. Establishment and enforcement of governance are critical to successfully\nfollowing government regulations, as well as industry and geography-specific policies.\nTransaction and concurrency across clouds\nWhen operating across a cloud ecosystem, the sharing of applications and data drives the\nrequirement for ACID transactions and concurrency. Any changes made by any member of\nthe ecosystem must be visible, auditable and reliable. Specific to this requirement is an\nexpanding use of blockchain and related technologies across the cloud computing industry.\n\n\nOperational Requirements\nChapter 11\n[ 231 ]\nSLAs and benchmarks\nCompanies that sign SLA-backed contracts should also establish a standard way of\nbenchmarking CSP performance. SLA should not only specify minimum requirement and\nvariability expectations, but it should also specify appropriate remedies to the consumer \nshould the CSP fail to meet a service level or restore services to the appropriate level within\na specified period. Service definitions and metrics should be unambiguous.\nProvider exit\nThe cloud solution architect should prioritize risk mitigation as part of any solution design.\nSetting this as a priority dictates a carefully designed provider exit strategy plan before\nconsuming any cloud service. Risk mitigation requires the identification and verification of\nsecondary, and in some cases, a tertiary, supplier for all cloud service deemed crucial to the\nenterprise.\nSecurity\nCloud computing security is always a significant concern but focuses primarily on user\ndata privacy. When using cloud services, end users do not have control of storage location.\nApart from SLA-specified limitations, they also lack specific knowledge of storage location.\nSecurity controls\nA security control acts as a tool to restrict a list of possible actions down to those that are\nallowed or permitted. An industry group, called the The Cloud Security Alliance, has\ndocumented a complete list of data security controls in a reference called the Cloud Control\nMatrix. This matrix is an important tool and is designed to help the security professional\nidentify and selected data security controls, based on the applicable industry regulations or\nsecurity governance environment.\n\n\nOperational Requirements\nChapter 11\n[ 232 ]\nControls are generally described as being within one of three categories:\nAdministrative: regulations, policies, laws, guidelines, and practices governing\nthe overall information security requirements and controls\nLogical: Virtual technical and application controls such as firewalls, encryption,\nanti-virus software, and maker/checker routines\nPhysical: used to manage physical access like a key to a door. Other physical\ncontrols include gates and barricades, video surveillance systems, the use of\nguards, and remote backup facilities\nThese three elements are crucial to an effective control environment but do not give clear\nguidance on the degree to which a risk is mitigated.\nData management controls can be classified as directive or deterrent.\nDirective controls cause or encourage a desirable event to occur, such as\nemployees meeting objectives effectively. Formally written procedure manuals\nwould be a directive control in this case because they would encourage\nemployees to carry out particular functions in an effective manner.\nDeterrent controls are designed to discourage potential attackers by sending the\nmessage that it is better not to attack, but even if you do, the target can defend\nitself. Examples of deterrent controls include notices of monitoring and logging\nas well as the visible practice of sound information security management.\nControls that could be considered as mitigating controls include the following:\nPreventive controls which prevent data loss or harm from occurring\nDetective controls that monitor activity in order to identify where practices or\nprocedures are not properly followed\nCorrective controls which restore the process or system back to a prior pre-\nincident state\nControls that extend data protection include:\nRecovery controls, which restore lost computing resources or capabilities and\nhelp the organization to return to normal operations and recover monetary loss\ncaused by a security violation or incident.\nCompensating controls, which reinforce or replace normal controls that are\nunavailable for any reason. These are typically back-up controls and usually\ninvolve higher levels of supervision and/or contingency plans.\n\n\nOperational Requirements\nChapter 11\n[ 233 ]\nControls should also be identified as manual or automated.\nSince security is a critical operational element, the solution architect should also take all of\nthe following aspects into account when developing a comprehensive enterprise solution. It\nshould also be noted that many of these components actually fall under corporate\ngovernance and, as such, fall outside of the technical solution design. A failure to address\nthese elements, however, could prevent the deployment and success of any cloud\ncomputing solution. As stated before, cultural change is an essential part of any transition\nto cloud. Effective IT governance is foundational to cultural change.\nThe Cloud Security Alliance has categorized these controls into industry\nstandard control groups. Control group descriptions are provided in the\nfollowing documentation: IUUQT\u001c\u0011\u0011EPXOMPBET\u0010DMPVETFDVSJUZBMMJBODF\u0010\nPSH\u0011JOJUJBUJWFT\u0011DDN\u0011$4\"@$$.@W\u0015\u0010\u0012\u0010YMTY.\n\n\nOperational Requirements\nChapter 11\n[ 234 ]\nDistributed computing reference model\nThe various cloud service models expose applications, platform and infrastructure\ncomponents in many different and unique ways. The different interfaces between the\nvarious components create a foundation for the distributed computing reference model.\nThe open group created the model as a means for identifying and managing the\ninteroperability and portability of cloud computing solutions. In offering the DCRM as a\nvital cloud solution architect tool, this chapter describes its components and processes. The\narchitect should also note that the execution of all interactions is through industry\nstandards, user-developed or vendor-specific APIs, or web services.\nYou can find more information at: IUUQ\u001c\u0011\u0011XXX\u0010PQFOHSPVQ\u0010PSH\u0011DMPVE\u0011DMPVE@JPQ\u0011Q\u0017\u0010IUN.\nSummary\nCloud computing represents a new operational model for delivering information\ntechnology services. This brings with it a number of unique operational requirements. This\nchapter explains each of those within an operational context so that they can be\nappropriately included in every architected cloud solution. Security controls are equally\ncrucial due to the importance associated with managing and controlling access to all data.\nThe DCRM is presented in here, in this chapter, as a communicative tool. While the other\nmodels presented in this text are specific to cloud computing, this one represents a more\ngeneralized approach more suitable when evaluating portability and interoperability when\noperating across different cloud computing and traditional designs.\n\n\n12\nCSP Performance\nCloud service providers (CSPs) are not all the same. Service performance from the same\nservice provider can vary from day to day. From a consumer point of view, critical\nperformance characteristics vary based on the cloud service model. For end users, SaaS\nperformance measures are perceived as business transaction response times and\nthroughput, technical service reliability and availability, and application scalability. PaaS\nperformance measures, on the other hand, are indirectly perceived by users and defined as\nthroughput, transaction response times, technical service reliability, availability, and the\nscalability of the middleware. Infrastructure performance, capacity, reliability, availability,\nand scalability typically define IaaS performance.\nIn this chapter, we will cover the following topics:\nCSP performance metrics\nCSP benchmarks\nCSP performance metrics\nIn general, performance measures characteristics of the higher service layers depend on\nthose of the underlying technology components. Since consumers typically have no\nvisibility into technology specifics, they can be clueless concerning performance\nexpectations.\n\n\nCSP Performance\nChapter 12\n[ 236 ]\nThe typical user operational metrics used to evaluate cloud service providers are the\nfollowing:\nService response time (delay): The latency time between service request and\nservice completion\nService throughput: The number of jobs processed by the service provider\nwithin a set time unit\nService availability: The probability that the service provider accepts a customer\nservice request at any time\nSystem utilization: The percentage of system resources being used for service\nprovisioning\nSystem resilience: The stability of system performance over time, especially\nunder bursty loads\nSystem scalability: The ability of a system to perform well with size or volume\nchanges\nSystem elasticity: The ability of a system to adapt to changes in its loads\nThe interplay between multiple CSP performance drivers determines these metrics. One of\nthe most important is the user's geographic proximity to the provisioning CSP data center.\nThe relative geographic location affects the following:\nService response time (delay): Affected by the physical distance between the\ndata center and the number of consuming customers\nService throughput: Dependent on the nature of the network topology and\nnetwork technology between the data center and the consuming customer\nService availability: Dependent on the service capacity and the number of\nconsuming customers from that location\nNon-geographic performance drivers, mostly driven by data center physical and logical\ndesign choices, include the following:\nSystem utilization: The usage rate as a percentage of a system's maximum IT\nservice capabilities\nSystem resilience: System's capacity to recover quickly from a failure\nSystem scalability: Ability to increase or decrease resources applied to a specific\ncustomer's request\n",
      "page_number": 238
    },
    {
      "number": 12,
      "title": "[ 236 ]",
      "start_page": 250,
      "end_page": 259,
      "detection_method": "regex_chapter",
      "content": "CSP Performance\nChapter 12\n[ 237 ]\nSystem elasticity: System responsiveness to a customer's request to increase or\ndecrease applied resources\nUnderlying technology variability: Level of technology consistency across a\nprovider's global system\nRate limiting: Overt action by the service provider to limit the amount, quality,\nor responsiveness of a requested service\nLatency: Service delay incurred following an instruction for its consumption or\nexecution\nCSP benchmarks\nAs the number and variety of cloud service providers continue to expand, consumers face a\ngrowing information disparity concerning the level of service they should expect. This\nchallenge is especially perplexing when the consumer tackles the complexity of selecting\ncloud services and service configurations that best meet the price and performance\nrequirements of applications selected for cloud deployment.\nFor a cloud solution architect, this challenge is managed by comparing prospective CSP\nservice levels and capabilities against industry benchmarks for those services. Challenges to\nbeing able to establish useful cloud computing industry benchmarks include the following:\nThe sheer number of cloud service providers and the variety of cloud services in\nthe market\nThe broad geographic expanse of CSP platforms that typically span many\ndifferent locations\nGeopolitical requirements and restrictions\nWide area networking performance\nVariety of CSP business, pricing, and service models\nMultiplicity of service price changes\nVariability of performance within the same service at different times and from\ndifferent locations\nAdditionally, services can be consumed by the hour, month, annually, or through a spot\nmarket. New products are introduced on an almost daily basis, and pricing changes\nweekly. Amazon, for instance, is known to make price changes monthly. One of the best\nindustry benchmark studies was a collaboration between Rice University and Burstorm\nInc., the result of which was the industry's first comprehensive and continuous price-\nperformance benchmark:\n\n\nCSP Performance\nChapter 12\n[ 238 ]\nUsing a highly automated process, the first benchmark was across seven suppliers\n(Amazon, Rackspace, Google, Microsoft, HP, IBM, and Linode), across three continents\n(North America, Asia, and Europe) , as shown in the preceding tables, with a total of 266\ncompute products spread over three locations per vendor. The benchmark was executed\nevery day, for 15 days. The results were normalized to a 720-hour, monthly pricing model\nto establish the price-performance metrics. These results showed the following:\n\n\nCSP Performance\nChapter 12\n[ 239 ]\nThe range of performance within a single provider can vary widely:\n\n\nCSP Performance\nChapter 12\n[ 240 ]\nThe diversity of platforms and solutions offered by different CSPs can result in a\n1-core instance performance variance of as much as 622%:\n\n\nCSP Performance\nChapter 12\n[ 241 ]\nThere is a 4-core compute price performance variation of 1,000%:\n\n\nCSP Performance\nChapter 12\n[ 242 ]\nThere can be instance performance fluctuations of as much as 60% over time:\nThere is a wide variance in the availability and performance of instance types\nwhen measured at different locations.\nRapid changes over time in instance types, pricing, performance, and availability of\nservices by location demonstrates that benchmarking of a small set of instance types in a\nunique event is not sufficient for cloud computing. Even within the short span of this study,\nGoogle updated their infrastructure and pricing:\n\n\nCSP Performance\nChapter 12\n[ 243 ]\nService level agreements\nThe Cloud Standards Customer Council has developed industry best practices for the\ndesign and enforcement of cloud service level agreements. You can find a convenient\nsummary of these recommendations at IUUQ\u001c\u0011\u0011XXX\u0010DMPVE\u000fDPVODJM\u0010PSH\u0011EFMJWFSBCMFT\u0011\nQSBDUJDBM\u000fHVJEF\u000fUP\u000fDMPVE\u000fTFSWJDF\u000fBHSFFNFOUT\u0010IUN.\nSummary\nPerformance metrics are often overlooked when designing and deploying cloud computing\nsolutions. This is especially troubling because service level agreements should be used to\ndefine and track these.\nAs the number and variety of cloud service providers continue to expand, consumers must\nbe proactive in how they identify and select performance metrics, as these should form the\nbasis for the service level agreement. Cloud solution architects must also be able to compare\nprospective CSP service level metrics and capabilities against industry benchmarks for\nthose services.\n\n\n13\nCloud Application Development\nThe adoption of the cloud computing model invariably leads to changes in an\norganization's application development process. Changes are due to the cloud service\nconsumer's inability to have any real control over the cloud application's underlying\ninfrastructure. In the traditional software development life cycle, application developers\ncan exert direction and sometimes complete control over the hardware used. When an\napplication is destined for deployment to a CSP, neither control nor even visibility into the\nunderlying infrastructure is possible. Critical aspects of security, including responsibility\nfor executing and monitoring required security controls, is left to the service provider. Data\nsecurity is the primary reason for adherence to the fundamental design principle for cloud\napplication development.\nWe will cover the following topics in this chapter:\nCore application characteristics\nCloud application components\nDevOps\nMicroservices and serverless architectures\nApplication migration planning\n\n\nCloud Application Development\nChapter 13\n[ 245 ]\nCore application characteristics\nAll cloud computing applications should adhere to the following basic characteristics.\nLoose coupling\nThe loose coupling of application components operating in the cloud maximizes the ability\nfor each component to be individually self-contained. This approach logically separates\ncomponents and leads to more straightforward and less numerous interactions, which\nimproves application resiliency and portability. Interactions should not be time-critical as\ncommunication latency between cloud-based components cannot be reliably predicted.\n\n\nCloud Application Development\nChapter 13\n[ 246 ]\nService orientation\nService orientation is a design approach that focuses on the linkage between services and\nservice-based development and the outcomes of those services. It is referred to as service-\noriented architecture (SOA). A service does the following:\nLogically represents a repeatable business activity that has a specified outcome\n(for example, check customer credit, provide weather data, consolidate drilling\nreports)\nIs designed to be self-contained\nIs often composed of multiple different services\nHas its technical details abstracted from the service consumer\nCloud applications are organized as a service, or a set of services, and may use other\nservices. Their most common characteristics include the following:\nStable interfaces: Cloud application interfaces should not vary over time. Any\nvariations should be backward-compatible. Component interface changes could\nrequire significant re-integration with other components, which could negatively\naffect lifetime cost.\nDescribed interfaces: Cloud application interfaces must be human- and\nmachine-readable and describable. Human-readability is needed to support\ncomponent acquisition and integration. Machine-readability is needed for\ndynamic service discovery and composition.\nUse of marketplaces: Application marketplaces afford easy and rapid access to\ncloud-based products and services. Using a marketplace, high product quality\nand consistent device compatibility are assured. They also provide user freedom\nof choice between competing products and reinforce application and data\nportability and interoperability.\nREST: Representational State Transfer (REST) uses uniform interfaces to\nprovide cacheable, stateless, and layered client-server interactions. Using REST,\nevery client-to-server request contains all the information needed to execute the\nrequest. It also enables robust, scalable, and loosely coupled services that contain\nstable interfaces.\n",
      "page_number": 250
    },
    {
      "number": 13,
      "title": "[ 246 ]",
      "start_page": 260,
      "end_page": 268,
      "detection_method": "regex_chapter",
      "content": "Cloud Application Development\nChapter 13\n[ 247 ]\nBase transactions: Cloud applications are usually designed to perform\ntransactions with BASE properties: basic availability, soft-state, and eventually\nconsistent. Traditional transactions follow ACID properties: atomicity,\nconsistency, isolation, and durability. This means that transactions are reliable,\nbut consistency is not possible without sacrificing availability. With BASE,\nreplicated resources are allowed and at least one copy is available but other\ncopies are temporarily in different states. Synchronization will, however, drive\nthem eventually into consistency. Some applications require ACID\ntransactionality. In others, components providing parts of the transaction can use\nBASE transactionality for interoperability with other components:\n\n\nCloud Application Development\nChapter 13\n[ 248 ]\nCloud application components\nCloud applications consist of the software and programming language combinations used\nto create a web or mobile application. Software application components, client-side and\nserver-side, are also known as frontend and backend. Each application layer builds on the\nfeatures of the one below it, creating an application development stack. The following\nfigure shows the major building blocks of a typical stack:\nServer side\nOn the server side, three development models, or stacks, are widely used in the cloud\ncomputing industry:\nLAMP stack (Linux/Apache/MySQL/PHP)\nWISA stack (Windows/IIS/SQL Server/ASP.NET)\nJava web application stack (Linux or Solaris/Tomcat/MySQL/JSP)\n\n\nCloud Application Development\nChapter 13\n[ 249 ]\nLAMP\nLAMP is considered an open source development stack. Usually, there is no direct cost\nassociated with licensing, installation, setup, or deployment open source software, but the\nprocess does require expertise, and without it, could be very time-consuming. There are\nmany LAMP variations in the market. When deployed, LAMP stack products work\ntogether straightforwardly. Linux has fast performance, but the PHP layer does present\nsome limitations. Since PHP is an interpreted language, the server interprets each PHP\nscript, every time it runs. While this provides benefit by not needing compilation and\ngaining some language perks, the downside is performance. Alternative PHP Cache (APC)\nor other accelerators can enhance performance.\nWISA stack\nThe WISA stack core is the .NET Framework. It is a widely deployed standard and the\nmarketplace has an abundance of certified professionals. This can be very appealing for\nenterprises that enforce Microsoft standards. Benefits include clustering, failover, security,\nautomated administration, and business intelligence features. The .NET framework is\ncompiled just-in-time (JIT), which enables code hiding and increased performance.\nAlthough compiling the .NET architecture only once provides significant performance\nbenefits, it does bring with it a lack of portability. The integrated development\nenvironment (IDE) for WISA is Visual Studio.\nJava\nThe largest Java stack market shares are held by Red Hat and JBoss. Although stack\ncomponents vary, the use of Java code is consistent. Performance and development differ\ndrastically, however. Red Hat stack uses the Linux Tomcat server and is fully configurable\nusing XML configuration files. Because servlets are compiled into JARs, they offer\ninformation hiding and a performance benefit that scripted technologies don't have.\nMySQL works well in a web application. MySQL scales well for particular (that is, read-\nonly) web applications, but lacks DBMS functionality.\nJBoss web server blends an enterprise application resource (EAR) server and a web server\ninto a single product. It also uses the Tomcat server but the database is not specified. JBoss\napplications use the Hibernate persistence manager. Applications are written once and\ndeployed anywhere. The J2EE standard provides many enterprise-level components such\nas transactions and pooling.\n\n\nCloud Application Development\nChapter 13\n[ 250 ]\nClient side\nClient-side scripting options are many and varied. Device capability and developer skill set\ndictate selection and use. Some of the more common options are the following:\nJavaScript client-side scripting language\nREXX, which is an IBM mainframes scripting language\nTool Command Language (TCL), which processes strings and passes commands\nto interactive programs\nActive Server Pages (ASP) by Microsoft, which uses scripted pages on the web\nserver and acts as an interpreted interface between the backend application and\nbrowser\nJava Server Pages (JSP) from by Sun Microsystems, which uses scripted pages\nthat are compiled to run on the server as small programs called servlets\nPHP, an open source server scripting language embedded into HTML\nAsynchronous JavaScript And XML (AJAX), which is not a programming\nlanguage but uses XM and HTTP to request data from a web server\nHTML5cthe current version of the HTML standard used for structuring and\npresenting web content\nDevOps\nDevOps combines cultural philosophies, practices, and tools to increase an organization's\nability to deliver applications and services at high velocity. Accelerated velocity is\naccomplished by evolving and improving products at a faster pace than organizations\nusing traditional software development and infrastructure management processes. DevOps\nuses a concept referred to as Infrastructure as a Code where scripting is used to instantiate\ncloud-based infrastructure through the use of APIs. The primary tools for configuration\nmanagement are the following:\nChef: Uses Ruby, a domain-specific language (DSL), to write system\nconfiguration recipes. Chef installation uses a workstation to control the master.\nAgents are installed using the knife tool with SSH. Managed nodes authenticate\nwith the master using certificates.\nPuppet: Used to manage data center orchestration. It works with many operating\nsystems and provides operational support tools for major OSes. Setup requires a\nmaster server and client agents on each managed system. Modules and\nconfigurations use a Puppet-specific language based on Ruby.\n\n\nCloud Application Development\nChapter 13\n[ 251 ]\nAnsible: Ansible requires no node agent installation to manage configurations. It\nuses Python with an installation through a GIT repository clone. All functions\nuse SSH. Ansible node management requires appending SSH authorized keys to\neach node.\nSalt: A command-line interface (CLI) tool that uses a push method for client\ncommunication. It is installed on Git or the package management system. Salt\ncommunicates through general SSH. It also includes an asynchronous file server\nfor speeding up file serving. Python or PyDSL are used to write custom modules.\nMicroservices and serverless architectures\nAs cloud computing advances, application design techniques are also advancing. One\nsignificant development is the microservice architectural style. Applications are structured\nan as a collection of loosely coupled services that combine to implement business\ncapabilities. The microservice architecture is used to support the continuous\ndelivery/deployment of large, complex applications. It also enables an organization to\nevolve its technology stack. Another important new approach is the exclusive use of third-\nparty services that provide ephemeral containers using a just-in-time infrastructure\nprovisioning model, called serverless computing. With this execution model, the cloud\nprovider dynamically manages the allocation of machine resources. Pricing is based on the\nactual amount of resources consumed by an application, rather than on pre-purchased units\nof capacity.\nApplication migration planning\nWhile developing applications for the cloud is an essential area for the architect to\nunderstand, most large enterprises have some existing applications targeted for migration\nto the cloud environment. The cloud solution architect is intimately involved in these\nactivities as well. Application migrations typically go through four distinct phases:\nOrganizational assessment\nSolution definition and design\nApplication migration\nApplication operations\n\n\nCloud Application Development\nChapter 13\n[ 252 ]\nDuring the assessment phase, the migration team assesses the organization's infrastructure\nand targeted applications for readiness to transition to a cloud computing environment.\nSystems and applications owner interviews are core to this assessment. Application owners\nanswer questions about their current application state as a prerequisite. Cloud adoption\nitself is an application portfolio activity. Interactions and dependencies between business\napplications may be more important than the data or application itself. Dependencies make\nthe upfront screening, analysis, and hybrid infrastructure design crucial to the cloud\nreadiness prescreening. This screening process captures as-is the application architecture\nand current sustainment costs. It also should provide the baseline data for migration option\ncost-benefit analysis to support the stakeholder's decision process. Migrating applications\nare typically targeted for one of four transition processes:\nLift and shift: Required infrastructure is rebuilt using appropriate CSP services,\nand the application is transitioned as-is with no modifications.\nRefactor: Applications designed to operate on customized infrastructure are\nmodified to leverage available cloud services before migrating.\nRebuild: Applications that are still required by the organization but cannot be\nmodified to use available cloud-based services. These applications are redesigned\nand rebuilt before transitioning the process to the cloud.\nRetire: Applications that are no longer operationally or economically viable to\nthe organizations. Associated processes are either eliminated or replaced with\navailable SaaS.\nIn the solution definition and design phase, organizational requirements and associated\nmetrics are used to define, design, and compare candidate solutions. A multi-cloud\nanalysis platform (MCAP) is typically used to support this stage. An MCAP enables\nenterprises, service providers, and systems integrators to model, design, benchmark, and\noptimizes information technology infrastructures. They are also used to design and model\nprospective system architecture alternatives. During this phase, to-be architecture options\nare reviewed to gain insight into mission suitability and an understanding of how\nmigration impacts application performance, security, and scalability. This phase also\nencompasses finalizing all data security control requirements. Operational needs, laws, or\nindustry regulatory dictates drive security requirements. Since security is a shared\nresponsibility between the organization and selected cloud service providers, this activity\nidentifies all required security controls and their application within the solution design.\nIn the application migration phase, applications first migrate into a sandboxed environment\nto complete functional and security testing. After verifying functional capabilities and\nsecurity controls, they are promoted into the production environment.\n\n\nCloud Application Development\nChapter 13\n[ 253 ]\nIn the application operations phase, the final operational entity starts managing the\ninfrastructure and application. Users must continually monitor CSP adherence to service\nlevel agreements (SLA) throughout this phase. Continuous monitoring of all cloud-based\nresources is also necessary. The organization should continually recalibrate cost by\ncomparing planned versus actual and should recommends policies to streamline cloud\nusage. As requirements and available market services improve, the transition to other\nservice providers may be a more optimal choice.\nSummary\nCloud applications consist of the software and programming language combinations used\nto create a web or mobile application. These applications should also adhere to cloud-\nfriendly characteristics such as loose coupling, stable and described interfaces,\nmarketplaces, and REST. The most prevalent server-side stacks include LAMP, WISA, and\nJava. Many enterprises have accelerated the application development process by adopting\nDevOps. It uses a concept referred to as Infrastructure as a Code, where scripting is used to\ninstantiate cloud-based infrastructure through the use of APIs. While developing\napplications for the cloud is an essential area for the architect to understand, most large\nenterprises have some existing applications targeted for migration to the cloud\nenvironment. Cloud solution architects should use organizational requirements and\nassociated metrics to define, design, and compare candidate solutions.\n\n\n14\nData Security\nThe explosion of cloud computing and consumer IT means that your data, as well as data\nabout you, can be virtually anywhere. This expanding concept of data mobility means that\ntraditional security concepts, which focus in depth on infrastructure defense, no longer\napply. Implementing security requirements associated with transitioning from an\ninfrastructure-centric data security model to the data-centric cloud computing security\nmodel are challenging to every modern organization. As the cloud solution architect,\nhowever, your job focuses on how to address this issue without disrupting your end users'\nworkflows. A data-centric security model ensures that the most important asset of the\nbusinesscthe datacis always protected.Having your data everywhere is also critical to the\nsuccess of the cloud computing business model. A data-centric security solution must target\nthe direct protection of the data, not the endpoint devices. Device protection requirements\ninvariably mean additional fortification of the corporate security measures that are\ncurrently in place. Cloud solutions focus on protecting data, files, documents, and folders\nstored and used by the user community throughout its life cycle. They should also protect\nthe data in motion and distributed to employees internally, externally, and to partner\norganizations. By embracing public clouds such as Box, Dropbox, OneDrive, and Google\nDrive, enterprises are embracing these services as opportunities to work smarter, faster,\ncollaboratively, and efficiently. The data itself is an essential component of this digital\ncommerce because it holds intellectual property, employee information, and customer data.\nDeveloping a comprehensive data-centric security program, including data discovery,\nclassification, encryption, and file protection, can uniquely position your organization to\nprotect its data, and make security move with your data to comply with global regulations\nsuch as General Data Protection Regulation (GDPR).\nIn this chapter, we will cover the following topics:\nData classification\nData privacy\n\n\nData Security\nChapter 14\n[ 255 ]\nData security life cycle\nThe secure data life cycle has six phases:\nCreate: The generation or acquisition of new digital content, or the\nalteration/updating of existing content. Creation can happen internally in the\ncloud or externally after the data is imported into the cloud. The creation phase is\nthe preferred time to classify content according to its sensitivity and value to the\norganization. Careful classification is necessary because weak security controls\ncould be implemented if the content is classified incorrectly.\nStore: Committing digital data to a storage repository; typically occurs nearly\nsimultaneously with creation. When storing data, protection should align with its\nclassification level and controls, such as encryption, access policy, monitoring,\nand logging, and backups should be implemented to avoid data threats. Content\ncan be vulnerable to attackers if access control lists (ACLs) are not well\nimplemented, or files are not scanned for threats or classified incorrectly.\nUse: Viewing or processing, or otherwise used in some activity, not including\nmodification. Data in use is most vulnerable because it might be transported to\nunsecured locations such as workstations.\nShare: Information made accessible to others, such as between users, to\ncustomers, and to partners. Since shared data is no longer under the\norganization's control, maintaining security can be difficult. Data loss prevention\ntechnologies can be used to detect unauthorized sharing, and data rights\nmanagement technologies can be used to maintain control over the information.\nArchive: Data leaves active use and enters long-term storage. Considerations of\ncost versus availability can affect data access procedures. Data placed in an\narchive must still be protected according to its classification. Regulatory\nrequirements must also be addressed, and different tools and providers might be\npart of this phase.\nDestroy: The permanent destruction of data using physical or digital means (for\nexample, crypto-shredding). The destroy phase can have different technical\nmeanings according to usage, data content, and applications used. Data can be\ndestroyed through the logical erasure of pointers or via permanent data\ndestruction using physical or digital means. Consideration should be given\naccording to regulation, type of cloud being used (IaaS versus SaaS), and the\nclassification of the data:\n",
      "page_number": 260
    },
    {
      "number": 14,
      "title": "[ 255 ]",
      "start_page": 269,
      "end_page": 277,
      "detection_method": "regex_chapter",
      "content": "Data Security\nChapter 14\n[ 256 ]\nAlthough this life cycle addresses the phases data passes through, as shown in the previous\nfigure, it does not address data location, how the data is accessed (device or channel), the\nfunctions that can be performed with the data, or the process of authorizing a given actor\n(person or system) to have access to the data. A secure cloud solution must address all\nthese aspects.The data security life cycle should be managed as a series of smaller life cycles\nrunning in different operating environments. Data can, and does, constantly move into, out\nof, and between these environments. Regulatory, legal, contractual, and other jurisdictional\nissues make keeping track of the physical and logical locations of data a high-priority issue.\nThese aspects also control who is authorized to use the data and, often, the device and\ncommunications channel that can be used. Devices and channels have different security\ncharacteristics and may use different applications or clients.When accessed, a given datum\ncan be acted upon through three specific functions:\nAccess: View/access the data. Access includes creating, copying, dissemination,\nand file transfers.\nProcess: Performing a transaction on data. This includes updating it or using it in\na business processing transaction.\nStore: Store the data for future use (that is, in a file or database).\n\n\nData Security\nChapter 14\n[ 257 ]\nFunctions are performed in a location, by an actor (person, application, or system/process,\nas opposed to the access device). Protecting the datum requires the selection,\nimplementation, and enforcement of security controls. Controls restrict a list of possible\nactions down to those who are allowed. The appropriate governance regime typically\ndrives control selection. Applicable governance regimes include the following:\nGDPR: The General Data Protection Regulation (Regulation (EU) 2016/679) is a\nregulation by which the European Parliament, the Council of the European\nUnion, and the European Commission have unified and strengthened data\nprotection for all European Union (EU) individuals.\nSOX: The Sarbanes-Oxley Act of 2002 controls data access to reduce corporate\nfraud.\nHIPPA: The Health Insurance Portability and Accountability Act of 1996 is\nUnited States legislation that provides data privacy and security provisions for\nsafeguarding medical information.\nFedRAMP: The Federal Risk and Authorization Management Program is a\nUnited States government-wide program that provides a standardized approach\nto security assessment.\nPCI DSS: The Payment Card Industry Data Security Standard is a set of policies\nand procedures designed to optimize credit, debit, and cash card transactions\nsecurity. It protects cardholders against misuse of their personal information.\nFERPA:  The Family Educational Rights and Privacy Act is a United States\nfederal privacy law that protects parents and their children's education records\n(that is, report cards, transcripts, disciplinary records, contact and family\ninformation, and class schedules).\nThe Cloud Security Alliance provides a reference known as the Cloud Control Matrix\n(IUUQT\u001c\u0011\u0011DMPVETFDVSJUZBMMJBODF\u0010PSH\u0011EPXOMPBE\u0011DMPVE\u000fDPOUSPMT\u000fNBUSJY\u000fW\u0015\u000f\u0012\u000f\u0013\u0011) that\nlists required data security controls for these and many other industry governance regimes.\nThe cloud solution architect is responsible for identifying all required data controls and\nensuring that the implemented solution enforces the required control on the data, no matter\nwhere the data is located or what actor attempts to access it.\n\n\nData Security\nChapter 14\n[ 258 ]\nData classification\nGiven the importance of protecting data at all times and in all places, the most critical data\nmanagement task is data classification. Ideally, data is classified immediately upon creation\nby the entity that creates the data. If this is not done, data needs to be reviewed and\nclassified by others based on the organization's information governance guidelines.\nInformation governance represents the policies and procedures for managing all data and\nshould include the following:\nInformation classification: High-level descriptions of critical information\ncategories. The goal is to define high-level categories to determine appropriate\nsecurity controls.\nInformation management policies: Policies that define allowed activities for\ndifferent data types.\nLocation and jurisdictional policies: Where data can be located geographically.\nLegal and regulatory restrictions drive this.\nAuthorizations: Define which employee/user types are allowed to use or access\nwhich types of information.\nOwnership: The ultimately responsible party for the protection of information.\nCustodianship: Who is responsible for managing the information, at the\ndirection of the owner.\nWhen classifying data, best practice suggests that the schema used should, at a minimum,\naddress the following eight key areas:\nData type (format, structure)\nInformation context\nJurisdiction and other legal constraints\nData ownership\nTrust levels and source of origin\nContractual obligations or business constraints\nValue, sensitivity, and criticality of data to the organization\nObligation for retention and preservation\nThe classification categories should match the data controls used.\n\n\nData Security\nChapter 14\n[ 259 ]\nData privacy\nCompliance with the relevant privacy and data protection (P & DP) laws (by geography)\nrepresents, both for customers and service providers, an essential factor for the success of\nany cloud computing services implementation. Cloud service customers and cloud service\nproviders must work together to find viable solutions by implementing appropriate\nagreements and controls. The outcome should focus on ensuring defined roles and the\nattribution of due care and due diligence responsibilities. The P & DP regulations affect not\njust those whose personal data is processed in the cloud (the data subjects), but also those\n(the cloud service customers) using cloud computing to process others' data, and those\nproviding cloud services used to process that data (the cloud service providers). Key data\nprivacy roles are the following:\nData subject: An identifiable subject is one who can be identified, directly or\nindirectly, in particular by reference to an identification number or one or more\nfactors specific to his physical, physiological, mental, economic, cultural, or social\nidentity [telephone number, IP address].\nController: The entity which alone, or jointly with others, determines the\npurposes and means of the processing of personal data. When national or\ncommunity laws or regulations determine the purposes and means of processing\ndata, the controller may be designated by national or community law.\nProcessor: A natural or legal person, public authority, agency, or any other body\nthat processes personal data on behalf of the controller.\nData owner: An entity that can authorize or deny access to data and is the\nauthority responsible for its accuracy, integrity, and timeliness.\nIn a cloud deployment, your organization may play any or all of these roles. Your data\nsecurity controls must protect all data as dictated by the data owner. Meeting privacy and\ndata protection laws may require addressing the following data classification aspects:\nP & DP law for any relevant countries or jurisdictions\nScope and purpose of the processing\nCategories of the personal data to be processed\nCategories of the processing to be performed\nData location allowed\nCategories of user allowed\n\n\nData Security\nChapter 14\n[ 260 ]\nData retention constraints\nSecurity measures to be ensured\nData breach constraints\nStatus\nPersonally Identifiable Information ` PII\nThe personally Identifiable Information (PII) is data that can be used singularly or with\nother data to identify, contact, or locate a single person. The classification of any specific\ndatum as PII is dictated by laws or regulations of the relevant government or jurisdiction.\nPII can be divided into two categories: linked information and linkable information. Linked\ninformation can be used to identify an individual and includes the following:\nFull name\nHome address\nEmail address\nSocial security number\nPassport number\nDriver's license number\nCredit card numbers\nDate of birth\nTelephone number\nLogin details\nLinkable information is information that, by itself, cannot be used to identify a person but\nthat when combined with another piece of information, could identify, trace, or locate a\nperson. Examples include the following:\nCountry, state, city, postcode\nFirst or last name\nGender\nNon-specific age\nRace\nJob position and workplace\n\n\nData Security\nChapter 14\n[ 261 ]\nNon-personally identifiable information (non-PII) is data that cannot be used on its own\nto identify, trace, or identify a person. Examples of this are the following:\nDevice IDs\nIP addresses\nCookies\nSummary\nData security is the core of cloud security. The solution architect must establish and\nmaintain that mindset across the entire organization. Having your data everywhere is also\ncritical to the success of the cloud computing business model and the most critical data\nmanagement task is data classification. An ever changing aspect of data classification is the\nfluid nature of global privacy and data protection (P&DP) laws. Every solution must\nrecognize the multiple data protection roles and ensure that all required security controls,\nfor whatever role the organization plays, are in place.\n\n\n15\nApplication Security\nIn this chapter, we will cover the following topics:\nThe application security management process\nApplication security risks\nCloud computing threats\nThe application security management\nprocess\nThe ISO 27034-1 standard provides a very valuable framework for implementing cloud\napplication security. The standard's underlying principles include the following:\nSecurity requirements are defined and analyzed throughout the application's life\ncycle and managed continually.\nApplication risks are influenced by security requirement type and scope, which\nare driven by (1) business; (2) regulatory; and (3) technological domains.\nApplication security controls and audit measurements costs should align with\nthe targeted level of trust.\nAuditing process should verify that implemented controls are delivering\nmanagement's targeted level of trust.\n\n\nApplication Security\nChapter 15\n[ 263 ]\nISO 27034-1 also lays out the components, processes, and frameworks to help organizations\nacquire, implement, and use trustworthy applications, at an acceptable (or tolerable)\nsecurity cost. These components, processes, and frameworks provide verifiable evidence\nthat applications have reached and maintained a targeted level of trust. The recommended\ntop-level processes are as follows:\nThe Organization Normative Framework (ONF) management process, used for\nmanaging the application security-related aspects of the ONF.\nThe Application Security Management Process (ASMP), used for managing\nsecurity for each application used by an organization. This process is performed\nin five steps, shown in the following diagram:\n\n\nApplication Security\nChapter 15\n[ 264 ]\nThe ONF stores all the organization's application security best practices, or those from\nwhich they will be refined or derived. It comprises essential components, processes that\nutilize these components, and processes for managing the ONF itself. It will contains\nregulations, laws, best practices, and roles and responsibilities accepted by the\norganization. The ONF is a bidirectional process meant to create a continuous improvement\nloop. Innovations that result from securing a single application are returned to the ONF to\nstrengthen all organization application security in the future. Its specific IT governance\ncomponents include the following:\nBusiness context: Includes all application security policies, standards, and best\npractices adopted by the organization.\nRegulatory context: Includes all standards, laws, and regulations that affect\napplication security.\nTechnical context: Includes required and available technologies that apply to\napplication security.\nSpecifications: Documents the organization's IT functional requirements and the\nsolutions that are appropriate to address these requirements.\nRoles, responsibilities, and qualifications: Documents the actors within an\norganization who are related to IT applications. Includes processes related to\napplication security.\nApplication security control library: Contains the approved controls that are\nrequired to protect an application based on the identified threats, the context,\nand the targeted level of trust.\nThe application security risk assessment, the second step in the risk management process,\napplies the risk assessment process at the application level. Its primary purpose is to obtain\nthe organization's approval for a target level of trust through specific application-oriented\nrisk analysis.\nAs the third step of the process, the Application Normative Framework (ANF) is a subset\nof the ONF that contains the information required for a specific application to match the\nrequired targeted level of trust as set by the application owner. It identifies the relevant\nelements from the ONF which are applicable to the target business project.\n",
      "page_number": 269
    },
    {
      "number": 15,
      "title": "[ 264 ]",
      "start_page": 278,
      "end_page": 282,
      "detection_method": "regex_chapter",
      "content": "Application Security\nChapter 15\n[ 265 ]\nThe ONF to ANF is a one-to-many relationship, where one ONF will be used as the basis to\ncreate multiple ANFs:\nProvisioning and operating the application is the fourth step of the ASMP, which involves\nthe deployment and follow-up within the application project. It actually implements the\nsecurity activities contained in the ANF. Application Security Audit is the ASMP fifth step\nand deals with the verification and recording of the supporting evidence regarding whether\na specific application has attained its targeted level of Trust.\n\n\nApplication Security\nChapter 15\n[ 266 ]\nA descriptive graphic of the entire ASMP is provided as follows:\n\n\nApplication Security\nChapter 15\n[ 267 ]\nApplication security risks\nAfter all appropriate data controls are identified and designed into the cloud computing\nsolution, applications themselves need to be hardened against attack. The best guidance for\nthis hardening process is the OWASP top 10, which lists the 10 most critical web\napplication security risks. Refer to IUUQT\u001c\u0011\u0011XXX\u0010PXBTQ\u0010PSH\u0011JNBHFT\u0011\u0019\u0011\u0019\u0014\u001108\"41@5PQ@\u0013\u0012\u000f\n\u0014\u0012\u0013\u0019@\u0007\u0014\u001aFO\u0007\u0014\u001b\u0010QEG\u0010QEG for a detailed list along with the description of each risk.\nCloud computing threats\nThe most critical threats to cloud-based applications have been enumerated by the Cloud\nSecurity Alliance. Referred to as the Treacherous Twelve, a secure cloud solution must\nprotect all applications and processes against these attack vectors. Refer to IUUQT\u001c\u0011\u0011\nEPXOMPBET\u0010DMPVETFDVSJUZBMMJBODF\u0010PSH\u0011BTTFUT\u0011SFTFBSDI\u0011UPQ\u000fUISFBUT\u00115SFBDIFSPVT\u000f\u0013\u0014@\n$MPVE\u000f$PNQVUJOH@5PQ\u000f5ISFBUT\u0010QEG for a detailed description of each of these critical\nthreats.\nSummary\nApplication security requires data security supported by a standardized and consistent\napplication development process. The ONF and ANF provide the standardization and the\nASMP delivers the consistency. Cloud computing is often referred to as the industrialization\nof IT, but that value can be irretrievably damaged by a flawed application development\nprocess. Applications are key to an ability to deliver value with any solution so the architect\nshould definitely drive toward the ideals presented in this chapter.\n\n\n16\nRisk Management and\nBusiness Continuity\nCloud computing solutions balance the risk of data and information loss against the\nbusiness and mission value of using the cloud. This chapter gives you the tools needed to\npresent both sides of the equation to management so that they can make the many complex\ndecisions embedded throughout the cloud solution architecting process.\nIn this chapter, we will be covering the following topics:\nFraming risk\nAssessing risk\nMonitoring risk\nBusiness continuity and disaster recovery\nFraming risk\nThe risk in cloud computing is multifaceted and involves multiple participants. The entire\nmodel relies, in fact, on a shared risk model between the providers and the consumers.\nEnterprises using cloud assume risks as part of an interrelated service ecosystem that may\nnot be controlled by the internal IT department. Traditional risk management design is\ntargeted for low uncertainty environments that have few interconnections. The risk in\ntoday's networked world, however, is managed in an environment of high uncertainty and\ndynamically changing, interconnected systems. Key cloud computing risks include the\nfollowing:\nFailure to meet financial objectives\nInability to work within the context of corporate organization and culture\n\n\nRisk Management and Business Continuity\nChapter 16\n[ 269 ]\nUnsurmountable difficulties in integrating the cloud services involved\nInability to comply with legal, contractual, and moral obligations\nInability to recover from a disaster\nTechnically inadequate cloud service\nInadequate solution quality\nA definition for each of your organization's specific risks should be developed and agreed\nupon at the start of any transition to cloud computing.\nAssessing risk\nThe first step in managing an organization's risk of adopting cloud computing is an\nassessment. The assessment should evaluate financial, culture, service integration,\nregulatory compliance, business continuity, and business or mission system quality.\nThe impact of financial risk is always critical as it directly drives the return on all\ninvestments associated with a cloud computing transition. When using cloud services, costs\nare directly related to workload and revenue. While this model does reduce some financial\nrisk, it affects other factors differently. The critical assessment factors for cloud ROI risk\nprobability are the following:\nUtilization\nSpeed\nScale\nQuality\nThese four factors drive ROI directly because they affect revenue, cost, and the time\nrequired to realize any investment return. Differences between actual and projected values\nindicate a likely failure to achieve the desired ROI.\n",
      "page_number": 278
    },
    {
      "number": 16,
      "title": "[ 269 ]",
      "start_page": 283,
      "end_page": 288,
      "detection_method": "regex_chapter",
      "content": "Risk Management and Business Continuity\nChapter 16\n[ 270 ]\nManaging the cultural impact of a cloud transition is, in many ways, the most challenging\naspect of deploying a cloud computing solution. When adopting any cloud-enabled\nbusiness processes, organizational executives must project clear vision, direction, and\nsupport for all associated business transformations. Establishing a precise procurement and\nimplementation roadmap is imperative and may require significant training of the\nacquisition and legal team. Stakeholder coordination and reconciliation between competing\nstrategies are needed to build internal consensus for storage, computing, and networking\nservices. Tasks associated with migrating applications require a thorough understanding of\ncustomer demand. Pilots and demonstrations should be used to create confidence and to\nbuild buy-in and cloud service usage in the user community. They also help in building\nrequired transition skills and cloud technology knowledge. The organization's financial\ngovernance and acquisition processes may need to be modified to effectively and efficiently\nleverage the cloud computing economic model.\nOrganizations of any size typically consume services from three or more cloud service\nproviders. This Service integration risk requires both process and technology integration\nefforts. There is a risk that this integration does not deliver the expected results. An\nassessment of service integration risk consists of an evaluation of technical interface details,\nthe organization's ability to modify the existing system, and the skill sets available within\nthe team. Interface details provide data to support integration costs. Solution architects can\nobtain an initial qualitative estimate by classifying all interface points as required using one\nof the following:\nSyntactic conversion, which is relatively straightforward\nSemantic compatibility modification, characterized as possible but expensive\nProcess model changes, which would be required if the services have radically\ndifferent process models\n\n\nRisk Management and Business Continuity\nChapter 16\n[ 271 ]\nA similar triage process is used when evaluating the organization's ability to change the\nexisting system. Risks are high in any redevelopment effort.\nA significant source of regulatory compliance risks is mandatory or required interfaces with\nexternal services or systems. Driven by regulations or company policy, these usually\nrestrict data to particular geographical areas or legal jurisdictions. There may also be a\nminimum set of security, integrity, or confidentiality controls. Online or offline retention\nperiods are also often dictated. These types of restriction are particularly applicable to\npersonal and financial data. While the impact of failing to meet such regulations varies, it\ntypically includes financial penalties and operationally detrimental enforcement actions.\nExternal cloud supplier dependencies can increase the probability of non-compliance, even\nif compensating contracts clauses are in place, because force majeure may prevent the\nsupplier from honoring them.\nBusiness continuity management risk can arise from external services, internal systems, or\nphysical disasters. Business events such as mergers and acquisitions of suppliers,\nunforeseen bankruptcy, or contract cancellations could also affect operational continuity.\nCloud computing models can make it harder to respond to these types of changes due to\nthe reduced level of direct control. As part of a risk analysis, assess the probabilities and\nimpact of unplanned events that could harm the enterprise. Also make general provision\nfor unforeseen events that disrupt the cloud services that are used, or damage their data.\nHaving first identified the risks, build into the solution design elements that reduce their\nprobability or mitigate their effects.\nThis is always a risk that the solution fails to live up to the end user's expectations.\nClassified as system quality risk, the impact can be seen in reduced margin and loss of ROI.\nSpecific quality areas of concern are the following:\nFunctionality: Risk associated with the use of external cloud-based systems, or\nnon-cloud-related factors such as the quality of the solution specification.\nPerformance: Failure to meet required operational or technical metrics.\nAvailability and reliability: Insufficient reliability as measured by mean time\nbetween failure (MTBF) and the mean time to repair (MTTR).\nFault tolerance: Excessive availability risk caused by a single point of failure\n(SPOF) or an inability to accommodate multiple failures within a specified\nservice window.\nRecoverability: Inability to recover from a failure or excessive data loss should a\nfailure occur.\nResponsiveness: Solution that is not sufficiently responsive as measured by user\nresponse times and response variability specifications, primarily if the degraded\nresponse is due to throughput overload.\n\n\nRisk Management and Business Continuity\nChapter 16\n[ 272 ]\nManageability: Factors of configurability, reporting, and fault management,\nmainly when associated with the provisioning of cloud services.\nSecurity: Risk associated with internet accessibility and the shared security\ncontrol model. Failure to meet security requirements can result in financial loss,\ndata unavailability, sensitive information leakage, reputational damage, and\nfailure to meet privacy regulations. The use of multiple CSPs can also lead to\nelaborate security arrangements, introducing the possibility of gaps in the data\nsecurity defenses.\nMonitoring risk\nRisk management is an integral part of the solution architecture development. Risk\nassessments should, therefore, be repeated at every significant decision stage of the\narchitecture development process. This ensures that the levels of risk exposure continue to\nbe acceptable. Since cloud service procurement is an operational expenditure and not a\ncapital expenditure, cloud solutions must include a continuous service monitoring\ncomponent.\nCloud service risk assessment for suitability is completed at the start and throughout the\nsolution's lifetime. Services should also be reevaluated if the service provider introduces\nchanges or if alternative service options are made available in the broader marketplace.\nThis requirement is the basis for maintaining and updating industry benchmarks for every\ncritical cloud solution service. Industry benchmark data is also an important input to CSP\nservice level agreement (SLA) negotiations.\nBusiness continuity and disaster recovery\nA solution is worthless if it cannot deliver service to its intended consumers. This is why\nbusiness continuity and disaster recovery should always be included when architecting a\ncloud computing solution. Although the solution architect may exert minimal influence on\na solution's operational deployment, the good solution architect considers the following\nkey BCDR questions before presenting a recommended solution:\nCan the recommended cloud service provider deliver the required service\nelasticity if BCDR is invoked?\nAre any other CSPs capable of delivering all the required services under a similar\nSLA?\n\n\nRisk Management and Business Continuity\nChapter 16\n[ 273 ]\nDoes the recommended CSP have available network bandwidth for timely\nreplication of data?\nWill there be available bandwidth between the impacted user base and the BCDR\nlocations?\nAre there any legal or licensing constraints that prohibit the data or functionality\nto be present in any CSP data center location?\nCloud solution disaster recovery options fit into three broad categories:\nOn-premises data center uses a CSP to support BCDR requirements\nCloud service consumer depends on the CSPs redundant infrastructure to\nsupport BCDR requirements\nThe cloud service consumer moves from the primary CSP to a secondary CSP to\nsupport BCDR requirements\nThe cloud solution architect should recommend the most practical of these options as the\nBCDR path for any recommended cloud solution. The impact of a BCDR scenario on all\ncritical risk elements should be considered as supporting data in consideration of the\nfollowing planning factors:\nEnumeration of the importance and priority of data and critical organizational\nprocess assets\nThe current locations of these assets\nNetwork bandwidth and transport cost between data assets and all relevant\nprocessing sites\nActual and potential location of enterprise workforce and business partners\nEnumeration and prioritization of anticipated disaster events and scenarios\nProcess for initiating BCDR activities for each anticipated event or scenario\nReturn to normal process for each event or scenario\nSummary\nCloud computing brings risk management to the forefront of information technology. The\nrisk-averse reflexive management decisions of the past will result in rapid business failure\nin today's world. The smart use of other people's infrastructure, also known as cloud\nservice providers, demands a robust risk management process, with continuous monitoring\nand rapid reaction. BCDR itself is a risk management process that should also leverage CSP\ncapabilities.\n\n\n17\nHands-On Lab 1  Basic Cloud\nDesign (Single Server)\nCloud architecture can be difficult; at times, we make it more difficult than we need to.\nCloud is shifting everything because it is an economic innovation, not a technical one.\nCloud is driven by economics rather than technology. Each new service continues to drive\nprogression via economics by enabling the realignment of strategy, technology, and\neconomics. Containers and serverless are using new economic models to change the way\ninfrastructure and software are deployed. Because the cloud is primarily economics and\nstrategy, it requires updates to skill sets and additional data for decisions.\nThe cloud is an answer, but not the answer to everything. Cloud does not make bad\ndecisions better. The cloud is a tool. The cloud is a philosophy, a strategy, a mindset, and an\nattitude. Above all, cloud is a process. A single aggressive move from CAPEX to OPEX is\nlikely to be expensive; it will probably fail, and probably will not solve much. Fork-lifting\nthe same design from an on-premises data center to an off-premises service provider will\nmove the problem, but not solve it. Cloud success requires research, change management,\ngovernance, and comparative design. Every design choice affects economics, strategy,\ntechnology, and risk.\nHands-on labs and exercises\nThe next three chapters will discuss the impact of design choices at increasing levels of\ncomplexity. These chapters are meant to be used as a step-by-step hands-on guide that will\nnavigate through designs and design choices, yielding real-time insight each step of the\nway.\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 275 ]\nThis chapter will start with a single-server infrastructure, then we will accelerate into more\ncomplex insight and scenarios in $IBQUFS\u0002\u0013\u001b, Hands-On Lab a Advanced Cloud Design Insight\nand $IBQUFS\u0002\u0014\u0012, Hands-On Lab 3 a Optimizing Current State (12 Months Later). It is suggested\nto navigate these example chapters in order, as each one builds on the previous.\nComplexity with each example grows, adding considerations for applications, application\nstacking, utilization, and general market and current trends. The examples and exercises in\nthe book will also be accessible via the Burstorm platform, with unlimited use for 30 days.\nComplexity\nCloud is typically associated with outcomes such as lower cost, speed, and simplicity, yet\ncloud can be very complex even in its most basic form. For example, a single server can\nhave many attributes that must be considered. How many cores? How much RAM? How\nmuch storage? Is it a virtual server or a physical one? What operating system? What type of\nconnectivity? Is the server on a shared or dedicated environment? What about going\nserverless? What about containers?\nThe answers to these seemingly simple questions have a drastically different economic\nimpact and a huge effect on strategy. Each attribute feels somewhat technical in nature, yet\nthey are more about economics and how economics affect strategy. Why would virtual\nservers be chosen over physical? Better utilization? Isn't utilization really about maximizing\nthe use of an expensive resource? Virtualization allows for the acquisition of only what is\nneeded for as long as needed. Not really. Virtualization has been around since the 1960s.\nRecent billing innovations are what allow partial resources to be consumed in very short\nincrements of time. Virtual servers can be deployed faster. True,  but why does that matter?\nPhysical deployments are very manual, expensive, time-consuming, and potentially filled\nwith human error. Virtual machines can be deployed very quickly and programmatically,\neliminating much of the expense, time, and effort associated with deployments.\n",
      "page_number": 283
    },
    {
      "number": 17,
      "title": "[ 275 ]",
      "start_page": 289,
      "end_page": 308,
      "detection_method": "regex_chapter",
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 276 ]\nVirtualization and its benefits are well known. Designs using virtualization have been\naround for several years now. What is so different? For the first time, we see economic\nmodels driving design decisions, for example, reserve instance versus current market rate.\nReserve instances require a large upfront fee with a very low monthly fee. What situations\nare better suited for a longer-term commitment with significant money up front? What\nstrategy does this line up with? How does this affect risk? With the high fees up front and\nlonger commitment requirements, reserve instances are better suited for persistent\nworkloads with fairly flat traffic patterns. Cyclical or seasonal traffic patterns do not fit\nhere, as resources would be paid for when they were not being utilized fully. A major\nchange, as mentioned earlier, is that designs can now be created for the low point with\nburst or up-cycle moves to support increases in the traffic pattern.\nEliminating the noise\nSuccessful next-generation designers are able to quickly triage true requirements from\nwants and wishes. Much of the truth is drowned by emotions, agendas, hype, marketing,\nand other forms of distracting noise. Simplify, then build. Quickly get to the lowest and\nsimplest common denominator and add where truly needed. Every server and GB of\nstorage requires monitoring, administration, management, and all other care-and-feeding\ntype activities. Poor choices at basic infrastructure levels can dramatically affect economics\nas all of the other requirements are piled on.\nA single server is not as simple as it sounds. The following diagram shows a set of basic\noptions that can be applied to any server. There are a number of options for each attribute,\nof which one is chosen. The chart shows almost 6.3 billion potential combinations for this\nsingle server. Considerations for other attributes, such as external storage, port\nconfiguration, software, patch level, and so on, have not been accounted for. The potential\ncombinations can quickly reach into the trillions for a single server when all attributes are\nconsidered. Add in additional combinations when adding in additional servers, licensing\noptions, additional devices, additional potential locations, potential providers, pricing\noptions, business models, consumption rules, deployment rules, and the many other\nnuances that permeate every solution design.\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 277 ]\nIn the following example, we see three different term options. This may equate to 12-, 24-,\nor 36-month terms, with only one term being chosen. We see that cores, in this example, can\nbe any number between 1 and 12. RAM could be anything between 1 and 16. Obviously,\nthere are many other options and add-ons, such as monitoring, management, licensing, and\nso on. But just basic server configuration choices already place this single server into the 6+\nbillion combination range:\nBurstorm lab 1 ` background (NeBu Systems)\nAll of the hands-on exercises will be for a company named NeBu Systems. NeBu creates\nsoftware for the automotive industry. New cars have almost as much processing power\nwithin them as full data centers in recent years. With all of the sensors gathering IoT data\nand the tremendous compute power available for processing, NeBu is trying to transition\naway from large monolithic legacy applications to highly flexible cloud-based modular\nfunctions aimed at changing the automotive experience. The goal is to be positioned to\nadapt as some functions become widely adopted while others are driven to satisfy certain\nniche markets. Ideally, functions are added as custom apps similar to adding apps to cell\nphones or picking car colors and upholstery types.\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 278 ]\nIn this first lab, NeBu is developing a new application that will be engineered for the cloud\nfrom the beginning. No legacy code to deal with. No legacy dependencies or specific\nhardware requirements complicating things. The code will be written using modern\nlanguages, eliminating concern over hardware compatibility.\nBurstorm lab 1 ` getting started\nPlease send an email to TVQQPSU!CVSTUPSN\u0010DPN with the following:\n(Required) current email address (must work as initial password information\nwill be sent to this address)\n(Required) full name\nPlease include the following code within the subject of the email: NeBu214495\n#VSTUPSN\u0002MBC\u0002\u0013\u0002`\u0002DSFBUJOH\u0002OFX\u0002NPEFM\nPlease go to IUUQ\u001c\u0011\u0011BQQ\u0010CVSTUPSN\u0010DPN\u0011MPHJO and enter your email address and\n1.\ntemporary password, which will need to be changed once you have logged in\nFrom the dashboard/home screen, please click on Design:\n2.\nClick on New Project | Model:\n3.\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 279 ]\nA dialog box will appear, asking for basic information to be entered:\n4.\nPlease enter a Model Name\n1.\nPlease change the View field from My Organization to Myself\n2.\nScroll to the bottom and select Create:\n3.\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 280 ]\nCongratulations, your model has been created. In this platform, a model is figuratively a\nscope or a problem that is trying to be solved. There may be multiple ways to solve a\nproblem or series of problems. NeBu is trying to transform from large monolithic apps to\nsmaller functional blocks of code and smaller targeted apps.\nThere are many ways to navigate that scope or problem. Should NeBu deploy on existing\ninfrastructure? Should it be deployed on-premises or off-premises? Should NeBu deploy it\nwithin existing collocation environments? What about virtual machines as a cloud service?\nAll are potential options. How do we start sorting it out?\nAs discussed in this book, there are many things to consider when assessing current cloud\nreadiness, developing new applications using new styles of infrastructure consumption and\ndeployment patterns, and recycling/up-cycling existing code bases. This book has discussed\nmany approaches and frameworks that can be used. After several internal meetings,\ndiscovery sessions, and planning conversations, NeBu has determined a path forward.\nNeBu has chosen to develop new code that will be deployed on Linux servers and begin to\nembrace more of the open source community.\nAs code begins to navigate through the development lifecycle, resource requirements tend\nto increase with each stage. Initial development is handled by relatively few people,\nrequiring minimal infrastructure when building and testing initial rounds of code. As the\ncode progresses, more people and infrastructure are needed to perform logical and\nresource testing. During testing and each development stage, developers must determine\ninfrastructure requirements for initial deployment and anticipated capacity plans.\nResponsible testing should yield logical and resource constraints that will determine initial\ndeployment and growth increments. Wouldn't these answers change based on the provider\nand infrastructure chosen?\nHow can initial anticipated performance level and basic resource requirements be\ndetermined if the infrastructure options, pricing, and performance are unknown? Based on\nBurstorm ongoing benchmark data, the same instance type within the same provider, at\ntwo different locations, has shown up to 700% different in performance tests. These\nperformance differences can dramatically change infrastructure requirements, deployment\nstyles, and associated solution economics. In the next exercise, we will begin examining\ncharacteristics and attributes that will help determine a short list of potential providers and\ninstance sizes that match up to technical, strategic, and economic requirements.\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 281 ]\nReturn to the initial model created and verify. The model should appear as a blank drawing\nboard as shown in the following screenshot. In the following screenshot, a model named\nSingle Server (Reference) is shown. This model was created and shared as a follow-along\nmodel that can be viewed if you choose to do the configuration work later. The reference\nmodel is also meant to be used as a reference to check progress and see the results expected:\n#VSTUPSN\u0002MBC\u0002\u0013\u0002`\u0002DSFBUJOH\u0002B\u0002EFTJHO\u0002TDFOBSJP\nAs mentioned, understanding available solution components, where they are available,\nwhat they cost, and how they can be combined with other services is very helpful when\nused from the very beginning. In this example, a single server design will be created to help\nidentify potential providers, configurations, and services that may affect final solutions as \ndevelopment cycles move closer to production deployment at NeBu Systems:\nStarting at the top center of the page, please click and drag the Design icon onto\n1.\nany blank space on the drawing board.\nThe application will create a new scenario as shown in the following screenshot:\n2.\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 282 ]\nPlease enter a name for the scenario. In this example, I have used 4JOHMF\n3.\n4FSWFS. The name can be anything you choose that is helpful as you try to\nremember what it is for:\nService providers deploy products and services in specific locations. Services are\n4.\nnot generally available anywhere, although some can be (for example, equipment\ndeployed on client premises). In-house services are also only available in very\nspecific locations. For this reason, one of the initial scenario defining\ncharacteristics is location. NeBu Systems has chosen to deploy the new\napplication in a central location within the US. The Midwest has additional\nbenefits, with lower risk factors than higher-profile, more densely populated\ncities such as New York and Los Angeles. The threat of natural disaster is much\nlower than California. Please enter the initial location as $IJDBHP\u000e\u0002*-, shown in\nthe following screenshot. Chicago also has very good connectivity options as\nmany of the carriers pass through in large telco hotels.\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 283 ]\nPlease add $IJDBHP\u000e\u0002*- to the Locality field. This will set the general search\n5.\nepicenter for potential solution products and services:\nNot every product or service will be located in Chicago, IL. NeBu Systems does\n6.\nnot have a firm requirement that forces it to locate within Chicago. Since other\npotential providers and locations may be available within an acceptable distance\nfrom Chicago, please enter \u0015\u0012\u0012 as the search radius to be used when mapping\nand matching acceptable providers, products, and services:\nProducts and services not only have specific locations, but they also have\n7.\npredetermined business and consumption models. As an example, reserve\ninstances from AWS have an economic and consumption model that requires a\nminimum commitment of a 12-month term with significant non-recurring cost\n(NRC) due up front. Once the upfront (NRC) costs have been paid, a smaller\nongoing monthly payment is required for each month of the specified\ncommitment term. NeBu systems have strategic interest and a financial policy\nthat desires more emphasis on preserving capital favoring operational expense\n(OpEx) driven solutions.\nPlease leave the Term field blank. This will indicate that the minimum term is\n8.\nnot specified and will allow economic models with no minimum term to\nrespond. If a reserve instance model were required, a minimum of 12 months\nwould be entered instead of leaving the field blank:\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 284 ]\nPlease leave the % NRC field set to the default of Any NRC as shown in the\n9.\nfollowing screenshot. This example will not limit the NRC amount:\nPlease leave Compliance set as default and choose Create at the bottom.\n10.\nThe result should show a single design scenario on the board titled with the\n11.\nname previously entered:\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 285 ]\nNeBu chose to deploy on Linux servers. Application and code design\nconversations have not been able to provide a consensus for a single provider, or\nshortlist of providers. Different stakeholders have their own agendas and\npriorities that are being brought to the table. One of the administrators wants it to\nbe housed at Google because he has good relationships from engagements\ncompleted with Google Cloud at previous employers. NeBu developers like the\nidea of using AWS due to the scope of all available cutting-edge services. Sales\nlikes the idea of using Azure as many of the clients are comfortable with Azure\nand like where the direction and progress Azure has made recently.\nWhat is needed to resolve this debate? Data. Real-time analytics and performance\ndata will help in many ways. Many people may consider RFP, RFI, or RFQ type\nprocesses. For this exercise, a Linux server can be pulled into the scenario and\nprovide real-time data that may help navigate the dynamics of this internal\ndebate as a provider, or set of providers, are chosen.\nPlease click on Compute near the top left of the design board as shown in the\n12.\nfollowing screenshot:\nA series of preconfigured compute options will show across the grey ribbon, as\n13.\nshown in the following screenshot. Please click on the first icon, named Linux,\nand drag it into any empty space within the scenario box created earlier:\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 286 ]\nThe Linux icon should now appear in the box with a dialog window open on the\n14.\nright side of the screen:\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 287 ]\nNeBu Systems anticipates the new application workloads, and workload types to\n15.\nutilize far less compute power when compared to RAM requirements. The\ncurrent plan will utilize virtual servers on a shared platform to continue proving\nthe concept, test code, and baseline initial performance characteristics. The initial\nconfiguration will start at 1 core and 8 GB of RAM. We will address storage at a\nlater step.\nPer the NeBu requirement, please update the RAM from \u0013 to \u0019 (yes, the earlier\n16.\nstatement mentioned 8, please use 7). Also, please clear the storage amount from\nthe storage line. Storage will be addressed in a later step. Please refer to the\nfollowing screenshot to verify the configurations match:\nPlease click the blue Advanced bar to drop down the additional options. Please\n17.\nverify that Is VM? and Is Shared? are set to Yes. NeBu's initial solution requires\na virtual server from one of the shared IaaS providers available in the current\nmarket:\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 288 ]\nPlease click the Save button at the bottom. The server configuration should\n18.\nupdate to match the following screenshot:\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 289 ]\n#VSTUPSN\u0002MBC\u0002\u0013\u0002`\u0002EFTJHO\u0002TDFOBSJP\u0002TPMVUJPO\u0002SFTVMUT\nFor NeBu to move forward most effectively, they have requirements to control cost while\nacquiring as much performance as possible per dollar spent. There are many providers in\nthe world, with more arriving daily it seems. Each provider has their own personality,\ndeployment style, consumption model, pricing model, and unique combination of available\nproducts and services. How can a shortlist of favored providers be assembled? Two\nrequirements and prioritizing characteristics have been stated previously: cost and\nperformance. We can start there:\nPlease click on the hamburger menu in the top-right corner of the design scenario\n1.\nbox and choose BurstormIQ from the drop-down menu:\nIn real time, the platform will return a set of results based on real-time API\nconnected providers, their available products, services, consumption rules,\ndeployment rules, and pricing. Results may vary depending on when you access\nthe real-time data. Data updates/changes often:\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 290 ]\nIn order to choose a provider and technology partner as NeBu moves forward\n2.\nwith testing and deploying the new functions and applications, NeBu would like\nto see more provider data from a wider set of providers. Please click on the blue\nbar with the scenario name in it. This will open the characteristics and attributes\nof the scenario itself. Please change the distance to \u0017\u0012\u0012 miles. Click Save at the\nbottom:\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 291 ]\nInstantly, NeBu has access to more providers and potential service options that\nmatch the stated requirements for strategy, technology, and economics:\n#VSTUPSN\u0002MBC\u0002\u0013\u0002`\u0002IJHI\u000fMFWFM\u0002SBQJE\u0002JOTJHIUT\nSuccessful cloud deployment requires not only good strategic, technical, and financial\ndecisions; it also requires significant change management and governance. The real-time\ndata presented in this single view contains solution options from multiple providers and\nsome very interesting insight that NeBu Systems can use to not only support decisions but\nalso help promote effective change management and establish governance.\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 292 ]\nNeBu expressed interest in being near Chicago, IL. Truly, it was a desire to be in the\nMidwest to avoid many of the potential disasters associated with the higher-profile cities\nand regions in the US. NeBu also required good connectivity options, as clients would be\naccessing from several locations across the country. Resiliency was important, along with\ncontrolling costs and gaining as much performance as possible for money invested.\nData shown within Bustorm has been automatically normalized so that it can be compared\nin real time. One of the largest challenges with architecting cloud solutions is gathering and\nnormalizing relevant data. Designers, architects, strategists, and stakeholders must gather,\nnormalize, and compare data for the current state in various forms. Current state billing\ndata is a fairly common starting point. Billing data is compared against deployed\ninformation, which is then also compared to actual consumed detail and ultimately\ncompared to potential future state options.\nNeBu Systems chose to start greenfield as strategically there was not enough value in trying\nto repurpose or up-cycle what has already been deployed. Current state still needed to be\nconsidered, normalized, and compared. Starting with a new environment does not mean\ncurrent state is completely ignored. In many cases, current state must be evaluated as an\noption until it is proven to be less than desirable.\nArchitecting cloud solutions is about alignment and balance. Successful solutions require\nrisk to be offset by economics. Technology helps accomplish strategy requirements, with\nstrategy influencing technology choices. At times, technology choices can also dramatically\nimpact economics while economics can certainly influence technology choices. In the\nsolution results side of the design board, a greater than 300% difference in price can be seen\nfrom low to high. There is a 50% difference that separates the three lowest-cost providers.\nThe request was the same; why are the prices so different? Is it performance difference?\nResiliency? Location? Size of infrastructure? Brand value? There are many factors that can\naffect cost. Many of these questions will be answered in the next chapter as we dive deeper\ninto the insight needed for successful design and architecture.\nAnother interesting insight the data presents is that none of the three lowest-cost providers\n(Google, AWS, and Azure) are in Chicago. If NeBu Systems' requirement was to be in\nChicago, CenturyLink and Rackspace become the only options available. Data also shows\nAWS to be the highest performing of the solutions available based on the components\nrequested. Performance is a big requirement for NeBu Systems. Quickly spotting that detail\nis covered in depth in the next chapter.\n\n\nHands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 293 ]\nSummary\nSolution design and architecture can be filled with lots of unnecessary noise and\ndistractions. Much of the information can be misleading and is often misrepresented.\nReturning to the basics and starting with things that are non-negotiable to establish baseline\nrequirements is the best way to begin. Start with a few high-level requirements and let the\ninsight become the foundation, not the technology. Build upon the insight, which enables\nproper alignment and balance.\nIn this chapter, NeBu Systems has been able to start with a very basic set of requirements,\nquickly assess initial economic impact, identify a short list of providers to focus on, and\nquickly confirm some of the strategic and technical pieces. Beginning with non-negotiable\nrequirements helps solution designers and architects avoid unnecessary complexity and\nscope-creep. This iterative method allows data to expose additional insights that may affect\ndirection and choices that would have been otherwise missed.\nThe next chapter will explore deeper-level detail and additional insight that helps refine\ncloud solution design.\n\n\n18\nHands-On Lab 2  Advanced\nCloud Design Insight\nSuccessful cloud design requires good data. Successful cloud design, more importantly,\nrequires the effective communication of decision supporting data. Many transformation\nprojects fail from poor communication, poor execution, and a lack of adoption, all\npotentially resolved with well-executed change management and communication plans.\nSuccessful solutions require real-time data; the same data that is very beneficial when used\nfor change management and communication plans.\nIn this chapter, real-time data takes center stage. Additional scenario data and insights will\nbe examined in greater depth. Additional infrastructure design options and ideas will be\nexplored. Later in the chapter, additional services and application data will be factored into\nthe options reviewed and decisions made.\nWe will learn about the following topics in this chapter:\nData-driven design\nBurstorm lab 2\nData-driven design\nIn the previous chapter, complexity was painted somewhat as a villain. Complexity itself is\nnot the villain; complexity without data to support it is. While implementing cool features\nbased on the data outlined in successful stories within popular blog posts and magazine\narticles could be considered data-driven, it is not really a data-driven approach that leads to\nsuccess most often. Transformations are hard enough without trying to recreate someone\nelse's story. Why are transformations so hard? Cloud is supposed to make things much\neasier to align and implement.\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 295 ]\nHow tough can cloud transformation be? Choose the number of cores and RAM. Add\nstorage. Pick a virtual server with the desired OS. Give it some bandwidth and start loading\napplications. Easy. Even easier: put it in a container. It spins up fast, is very portable and\ncheap. Awesome. Wait, better yet, go serverless. That removes the server, correct? (Insert\nlaughing audio file here).\nTransformations are difficult because of the data. Not necessarily a lack of data, but the\nchallenges associated with identifying relevant data and making it useful. Today, it is\nassumed that excruciating amounts of data at mind-numbing levels of detail must be\ngathered to describe the current state accurately. Today, most also assume that equally\nexcruciating amounts of mind-numbing potential future state details are required for\ncredible solutions to be built and accurate decisions to be made. Today, most assume that\nresults will fall short of expectations if sufficient levels of detail are not considered. These\nassumptions lead to another assumption: that these deep, drawn-out investigations require\na lot of time and can be accelerated by throwing more people and expense at the problem.\nIn other words, we dive way too deep, way too fast, and wrap ourselves around the axle\nand get completely stuck.\nAll data is useful; maybe not\nCloud transitions fail most often because the data used is not the most relevant, both when\ndesigning the solution and managing change. What does relevant data mean? How do we\nknow what data is relevant and what is not? How should data be triaged and prioritized?\nRelevance implies that there is a level of focus brought by comparing data to a set of\ncriteria. The criteria used must eliminate extraneous noise and unwanted distractions.\nThroughout this book, it is often stated that the simultaneous alignment of strategy,\neconomics, technology, and risk is critical to success. These four segments become the\ncriteria for both filtering and communicating solution data. To correctly triage and\nprioritize information, the data must have a significant impact across all four segments.\nAny data that does not impact all four segments simultaneously should be addressed at a\nlater stage or as part of implementation planning. An example of this may include NeBu\nSystems' interest in moving away from physical servers and monolithic apps to virtual\nservers with functions and services loaded. Can this be considered a strategic choice? Is this\na technical choice? Does the use of virtual machines and outsourced services affect\neconomics? Does the use of virtual machines provided by a service provider change the risk\nprofile?\n",
      "page_number": 289
    },
    {
      "number": 18,
      "title": "[ 295 ]",
      "start_page": 309,
      "end_page": 322,
      "detection_method": "regex_chapter",
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 296 ]\nNeBu Systems made a few initial decisions based on a few non-negotiable concepts:\nMove away from monolithic applications and coding methods\nMove away from physical servers in favor of current virtualization methods\nMinimize risk associated with natural and man-made disasters by locating\ninfrastructure in the midwest\nThe application will likely be more RAM intensive than processor\nEconomics impact is weighted the heaviest of all decision criteria if all others are\nequal\nLinux is a requirement with an emphasis placed on embracing open source when\npossible\nOpEx model is required\nThe project will be greenfield with portions of the current state infrastructure\nsubject to sunset after new deployment and go-live\nIn the last chapter, part I of the lab could quickly be created with incredible amounts of\ndetail and relevant insights instantly revealed. The simple input data was enough to get\nenough relevant data that the project could move forward collaboratively without any\nadditional delay while waiting for more potentially useful input data.\nTwo to three providers clearly showed promise, even with the limited input data used. The\nprovider data is normalized, compared, and ranked, giving proper focus to where time and\neffort should be invested if any additional investigation is required. Based on results from\nthe initial scenario, the following insights are shown:\nGoogle offers the lowest cost for the requirements provided\nAWS appeared to be the best performer (compute)\nAzure and AWS are virtually the same cost, making Azure a viable option as\nwell\nNone of the three lowest-cost options were in Chicago, but all were in the\nMidwest\nResponses show a 300% difference between low- and high-cost providers for the\nsame request\nVirtually no difference in cost between AWS and Azure\nPrice-to-performance cost is 50% different between AWS and Azure; this gets\nvery interesting since the price was virtually the same.\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 297 ]\nBurstorm lab 2 ` advanced insight (NeBu\nSystems)\nThe second part of this lab is diving into additional data and insights that can help shape\nnext-level decisions that may include the following:\nInfrastructure choices\nApplication stacking\nApplication layout\nInfrastructure footprint\nVarious types of optimization that become interesting due to data and insight\nrevealed\nBased on the response data, next-level decisions can be made to quickly refine solution\nchoices and turn them into building blocks for the final solution design. In the next series of\nsteps, each change will continue to expose deeper-level details that can confirm choices or\nhighlight potential alternatives that may provide a better fit with strategy, technology, and\neconomic requirements.\nBurstorm lab 2 - accessing additional detail\nPlease click on Details in the gray ribbon above the results side of the window.\n1.\nPlease use the following screenshot as a reference for where the tab is located:\nOnce clicked, additional solution detais will be shown based on the results\nreturned from each provider. Please refer to the following screenshot:\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 298 ]\nOverview of the Details tab\nThere is a lot of data shown in a very confined space. The layout allows for this data to be\nused for many types of comparisons quickly. The data is presented visually so interesting\nconnections can be made by referencing consistent data locations.\nUnder the provider's name, a table of data for matching solution products and services\nshows actual product details in the middle, any pricing information on the right side of the\ntab, and any performance data shown to the left. The green ovals contain the total cost for\nthe entire solution, accounting for the term length requested for that scenario. Since NeBu\nSystems did not specify the term, a standard of 720 hours is used as a standard month.\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 299 ]\nIn this view, details can be compared visually for quick decision-making. The segmented\nview is Google's response to the design created in lab 1; details for quantity, pricing, specs,\nand performance are shown along with the total price and real-time benchmarked\nperformance data. In this example, Google can be quickly identified as a less expensive\nalternative to AWS (for this solution combination) by comparing pricing in the green oval.\nAWS can be quickly identified as the faster alternative based on performance data. Google\nprovides the smallest infrastructure size based on specs listed in the response (though not\nenough difference to truly impact performance related to the NeBu Systems use case).\nBurstorm lab 2 ` selecting for direct\ncomparison\nData by itself does not tell us much at all. Data is only helpful when it can be compared to\nsomething. The comparison then leads to insight. In this portion of the lab, comparisons\nwill be made that will lead to insights that shape solution decisions:\nPlease check the Compare box for both AWS and Azure as shown in the\n1.\nfollowing screenshot:\nAfter checking the boxes for the solutions to be directly compared, please click on\n2.\nthe Compare tab as shown in the following screenshot. The Compare tab is next\nto the Details tab described in the previous section:\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 300 ]\nThe following view will help confirm that the first two steps have been completed correctly.\nThe view should change to one comparing the two selected solutions side by side:\nThis view allows solutions to be aligned and compared, line by line. Each line matches\nexactly for each objective in the design scenario. In NeBu Systems' current design, there is\nonly one objective defined. Imagine a solution with many lines that need to be mapped,\nmatched, and compared. It takes a lot of time to normalize and compare data using manual\nmethods today. The current NeBu Systems design and comparison only took a few clicks.\nInsightful data is shown in an instant, accelerating design and decision processes.\nComparing by price\nCloud solutions require insightful data. Comparing on price alone leads to trouble quickly.\nCheap solutions may not be the best fit strategically or have the right level of performance.\nThroughout this book, the economic impact has been mentioned as a requirement, but it is\nnot the only requirement. In many places throughout the platform, a red number is shown.\nThis number is a normalized number that takes pricing data and performance data and\nruns a calculation that normalizes the data and presents a consistent and dependable\nnumber that allows the viewer to compare the several performance and pricing metrics for\nsolution and solution component decisions:\nIn the upper-right corner of the Compare window, there is a drop-down box.\n1.\nPlease verify the box contains By Price. This drop-down menu is a selectable way\nto reorder and prioritize how the data is presented based on what is most\nimportant to the viewer at that moment. The default is set to By Price:\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 301 ]\nIn the comparison just created for NeBu Systems, which provider is the lowest\n2.\ncost? Is the lower-cost provider on the left or the right? There are several visual\nclues to help quickly identify the optimal solution based on the prioritization\nmethod chosen. When comparing AWS and Azure, as shown below, the lowest-\ncost provider for the current solution requested is AWS. This insight is shown in\na few ways. First, the green oval within the AWS response also shows best below\nthe price as the optimization method chosen was by price. The optimal solution\noption is always shown on the left; again, in this situation, the optimal solution is\nfrom AWS based on price alone:\nThere are a few other indicators to help the viewer quickly find highly relevant insights.\nWithin the green oval on the Azure side, there is a red number below the price. The red\nnumber, in this case, states that the price difference between the two solutions is 1% or less. \nAnother quick visual indicator is the red or green indicator at the end of each line item\nwithin each response. Within the AWS solution, it again says best, but using green text. In\nthe Azure response, the red number indicates the difference in price for that line. Please see\nthe following screenshot:\nBased on the requirements and responses, pricing difference between the two providers is\n1% or less. This difference is too close to make an informed provider choice. Additional\ndata is needed to choose the right path forward.\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 302 ]\nComparing by performance\nIn this step, additional data points can be quickly added for consideration. In the previous\nstep, 1% difference in cost is not enough to clearly choose which solution is optimal. In a\nlater step, price distribution will also be considered:\nPlease click the drop-down box that currently shows By Price and change it to By\nPerformance:\nThe view will change to now prioritize views based on normalized, real-time performance\nbenchmark data. This additional data enables the comparison of multiple solutions based\non performance as the priority data set to organize the views and calculate differences.\nPlease confirm the current views match the following screenshot:\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 303 ]\nSome may have noticed that the view did not change. There are a couple of reasons for this.\nFirst, in the bottom-left oval, the performance numbers presented are pulled from\nBurstorm's ongoing cloud benchmark service that randomly and continuously tests cloud\nproviders in real time. For this scenario, the performance number 15.7 for AWS is greater\nthan the 9.8 for Azure. The AWS performance is greater than Azure and presented on the\nleft. No change in view is needed. Second, there is only a single line item in the solution;\nnone of the data needed to be changed or reordered since AWS is the lowest cost and the\nhighest-performing in this scenario. Again, no change is needed. If Azure was higher-\nperforming with AWS still the lowest cost, this view would have moved Azure to the left\nside, as it was the higher-performing solution, and prioritized the data based on \nperformance, as indicated in the drop-down list at the top right.\nWith this additional data, AWS is slightly lower in terms of cost but appears to be\nsignificantly faster. The text in the bottom of each oval on the left side will again visually\nindicate which is best and what the difference between them is. Please see the following\nscreenshot. In this scenario, the difference is 38%:\nPricing did not give much indication of which would be optimal. The infrastructure sizes\nalso appear to be equal, with both showing as 2x8 machines.\nPlease note, in the scenario when created, the requested infrastructure size was one core\nand 7 GB of RAM (1x7). The platform automatically corrects to match how products and\nservices are sold by the providers, how they are meant to be consumed, and how the\nproducts and services are deployed.\nIn this scenario, both AWS and Azure would deploy the requested compute resources as a\ntwo-core and 8 GB of RAM (2x8) virtual instance. See the following original request from\nthe lab part 1:\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 304 ]\nPlease compare the requested details to the response details from each provider shown as\nfollows:\n\u0002\nComparing by price-to-performance\nInfrastructure size has not provided any meaningful differentiation. Pricing has not shown\nany major benefit from one provider to the other. The performance looked to be\nsignificantly different, with AWS appearing more favorable. Another very helpful indicator\nwhen comparing potential cloud solutions is price-to-performance benchmarks. In many\ncases, the price may clearly indicate a provider or two are optimal with performance\nshowing a different provider, or set of providers, as optimal. Price-to-performance\nbenchmarks enable value-oriented comparisons that will clearly show which provider has\nthe lowest cost for the highest level of performance. Of course, other factors may still\ninfluence final decisions, but the data can help build strong cases as solution designs\nprogress:\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 305 ]\nPlease go again to the top right, click the drop-down list, and choose Price\n1.\nPerformance as the priority when viewing data:\nAll red numbers should change to match the following screenshot. A $BCU is\nnow shown within each line item. This is the normalized price per unit of\nperformance for each line item. In the case of AWS, the cost for every unit of\nperformance is $4.26. For Azure, it is $6.91, which is a 38% higher cost per unit of\nperformance for the same size infrastructure than AWS:\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 306 ]\nBased on this additional data, it appears that AWS is likely the optimal answer\nbased on the data points considered to this point. It takes very little to make the\nsame comparison between AWS and Google. Google was the original low-cost\nprovider based on the requirements included in the scenario to this point.\nPlease return to the Details tab and uncheck Azure, and check Google for the\n2.\ncomparison. It is perfectly fine if you check the box for all three and compare\nthem side by side. The only challenge is being able to see all the data as you must\nscroll left and right to see all the data when you select three or more for\ncomparison. Please see the AWS and Google comparison in the following\nscreenshot:\nWhich provider is the lowest cost?\nWhich provider has the highest performance?\nWhich provider would be optimal based on price-to-performance\ndata?\nGoogle is 28% less expensive based on what was requested. By price alone, this would look\noptimal. AWS is higher-performing, with a 38% difference. It is an interesting side note\nthat, if you felt that the difference was 50% or so, that calculation would be a margin\nnumber that is different than the amount of difference. Viewing the data based on price-\nperformance shows AWS as a slight favorite (10%) even though Google was 28% lower cost\nin this scenario.\n\n\nHands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 307 ]\nSummary\nThere are many factors to consider as cloud solutions are built. In this section, several\nadditional important data points were examined:\nRequested infrastructure sizing\nUpdated infrastructure sizing based on provider deployment sizing\nNormalized infrastructure detail based on consumption and business models\nNormalized details based on updated pricing matching updated sizing based on\ndeployment rules\nPerformance data and analytics\nPrice-to-performance data and analytics\nBecause price-to-performance is so important to the process of building cloud solution\ndesigns, an in-depth paper from Burstorm can be found at: IUUQT\u001c\u0011\u0011TMJEFY\u0010UJQT\u0011\nEPXOMPBE\u0011DMPVE\u000fDPNQVUJOH\u000fCFODINBSL. Many of the aspects of why and how are included\nin the paper. Price-to-performance is a critical dataset that must be included as solution\noptions are evaluated.\nThere are many more details to consider as solutions are normalized, compared, and\nchosen. Based on very high-level details, a path can be chosen and focused on. Additional\ndata points can then be added and compared to either confirm the right path is chosen or\nclearly illustrate that a different path is needed. This allows projects and decisions to\nprogress quickly while reducing the level of effort to compare all data across all potential\nproviders. Starting with infrastructure also allows for the creation of a solid foundation to\nbuild on. It can be very expensive to manage environments that sprawl and spread\nunnecessarily. It can also be very hard to change directions if a solution goes too far too fast\nin the wrong direction.\nBased on the data to this point, NeBu Systems chooses to utilize AWS at this stage of the\nproject. The next chapter will examine how NeBu Systems' choice to utilize AWS has\nprogressed. How has their solution worked out strategically and technically, and how is it\ncurrently affecting the economics? Using the same concepts from this chapter, what can\nNeBu Systems do differently? What changes should be made? Are there better options that\nshould be considered?\n\n\n19\nHands-On Lab 3  Optimizing\nCurrent State (12 Months Later)\nIn the last chapters, data and insight quickly identified Google, AWS, and Azure as well-\nsuited providers for the infrastructure and services NeBu Systems required in the very\nearly stages of their transformation. Based on price, performance, and price-to-performance\ndata, AWS was chosen as NeBu Systems' initial cloud service provider.\nAs with many transformations, NeBu Systems has had challenges with change\nmanagement and governance, which, in turn, have slowed adoption. Many questions are\nbeing asked as infrastructure costs have escalated. NeBu Systems has chosen to examine the\ncurrent state in a little more detail. NeBu has imported one of their most recent AWS billing\nfiles. The plan is to quickly identify ways to optimize current state and control costs.\nIn this chapter, real-time data and insight will continue to provide a solid foundation for\nevaluating next steps, options, and decisions.\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 309 ]\nVisualizing current state data\nCurrent state data is typically spread across different locations, several different tools, and\noften many curators. A couple of the many challenges with trying to work with current\nstate data is that the data itself is not interactive or insightful. Collections of data do not\nreally do anything helpful until comparisons are made. Comparing data is revealing and\ninsightful. As an example, a lease can provide details regarding how much is being paid\nand the amount of time left on the lease. It would be much more helpful to compare the\nlease information to current market costs and other solution options. It may be beneficial to\nterminate the lease early and refresh technology through a more cost-effective current\nsolution or better simply benefit from fast-moving markets and current market economics,\nas an example.\nVisualizing data is the quickest path to insight. Many have stated that human perception is\n75%-85% based on sight. If you ask a chef the same question, it would be 75%-85% through\nolfactory nerve (smell). If you ask someone practicing shiatsu massage, that same\npercentage would most likely come from touch. Humans are experts at making data match\npurpose. Science has proven that light travels much faster than sound; about 1 million\ntimes faster than sound. This supports the fact that sight is the fastest of the human senses.\nOur other senses in order from fastest to slowest are sound, touch, smell, and lastly, taste.\nIn my rudimentary thinking, I know that I personally struggle with tasting and smelling\ncurrent state data. The best way for me to process it and quickly identify insight is visual\ninteraction. It has been well documented that data represented visually leads to accurate\ninsight much quicker than any other method.\nAs mentioned previously, in this chapter, the lab will visualize data from an AWS bill.\nThese different visualization steps will create many opportunities to make visual\ncomparisons that will yield many insights that can be used to optimize or transform the\ncurrent state. This lab section will focus on identifying insight that will help NeBu Systems\noptimize current state to better match up with the strategy, technology, economic impact,\nand risk profiles for the current project.\n",
      "page_number": 309
    },
    {
      "number": 19,
      "title": "[ 309 ]",
      "start_page": 323,
      "end_page": 353,
      "detection_method": "regex_chapter",
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 310 ]\nHands-on lab 3 ` visualizing the data\nFor convenience, an AWS bill has already been imported for the lab. The import was done\nusing the import function at the top right of the design board. Please see the following\nscreenshot:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 311 ]\nAfter the import, a new project will show in the left window. After a billing file is imported,\nit is added to the project list as an existing state project (green letters). Please confirm that\nAWS Feb and AWS March are within the list in the left window and accessible by your\nuser. Please confirm your view matches the following screenshot:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 312 ]\nPlease click on the project name AWS Feb (green letters) in the left window pane, shown in\nthe preceding screenshot. Clicking the name opens the project in the main drawing board\nwindow. The view should match the following screenshot:\nThe imported billing data has created a visualization of the current state data included in\nthe billing file. The visualization includes all infrastructure and services along with any \nAWS user-created tags that were included in the billing file details. The view is\nautomatically split by location. US West 1 and 2, US East 1, and Europe were included in\nthis file.\nHands-on lab 3 ` NeBu Systems' transformation\nprogress update\nNeBu Systems has made a lot of progress in a very short time. While things appear to be\nsuccessful at first glance, rapid growth and transformation have many challenges.\nAdoption can be difficult. Proper change management and governance are critical to\nsuccess but very hard to do well.\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 313 ]\nInitially, NeBu Systems did a very good job with change management. Unfortunately,\npeople have moved on. As the transition has progressed, teams have been shuffled a bit to\nrealign people with current strategy, technology, and economics. Change management has\nsuffered for the last few months, slowing adoption significantly. People are gravitating\nback to old, comfortable methods rather than embracing the new processes, infrastructure,\nand updated services.\nNeBu Systems saw significant early adoption, which led to rapid growth in the\ninfrastructure supporting the growing user base. With rapid growth, details get missed.\nCosts have escalated beyond where initial budgets were set. Scope-creep has become a\nproblem. Leaders want to re-examine where they are today, reset to align infrastructure\nwith the current strategy, and do a better job of controlling costs.\nHands-on lab 3 ` Current billing file\nWhat is in the current billing file? How can it be compared to the current market? Today,\nanalyzing a cloud billing file is very difficult. Billing files are very detailed. They usually\nhave many different services, with different locations, different billing methods, different\nterms, quantities, and very cryptic ways of identifying exactly what product or service is\nbeing referenced. Today, many try to download spreadsheets and CSV files to analyze\nthem line by line. This is very time-consuming and prone to error. Most automated tools do\nnot have the ability to compare and drive insight across the entire market. Many efforts\ntake days and weeks to normalize and compare billing data. Cloud solutions have services\nthat last fractions of a second, hours, and days. Taking weeks to analyze, compare, and\ndesign solutions is not acceptable in the cloud industry. Automation and enablement is a\nrequirement.\nIn the bottom left of the design board, there are a few icons that can start leading\n1.\nus to visually-driven insight. The first icon should be currently selected. The first\nicon shows the logical visualization of everything in the file:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 314 ]\nThis billing file has four main locations as described earlier, three in the US and\n2.\none in Europe. Please click on the icon located furthest to the right in that same\nrow at the bottom left of the design board:\nThis view visualizes the billing file line by line with a total at the bottom. The\nfollowing screenshot shows a partial view of the Bill of Material (BOM) view.\nPlease confirm you have selected the correct tab by comparing to the following\nscreenshot:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 315 ]\nPlease scroll to the bottom of the page, using the small slider on the right side of\n3.\nthe page or use the mouse wheel to scroll. At the bottom of the page, a green oval\nwill hold the total for the billing term specified in the billing file loaded as well as\na second green oval with the total monthly recurring cost (MRC). Since this is an\nAWS bill that has services for a term of one month or less, the monthly (MRC)\nwill match the total. The oval labeled NRC has a total of $0.00. This confirms that\nno reserve instances are being consumed. Please confirm that views match up to\nthe following screenshot:\nThe imported billing file shows a total of $65,337.13. This is the rolled-up total for\nall locations contained in the billing file. It is important to be able to understand\nthe stories the data is telling. It is also very important to understand what\nquestions still need to be asked and what answers still need to be found. For\nexample, how much of this bill is allocated to each site? Which site is primary?\nWhat products and services are currently deployed at each site?\nPlease click on the icon in the bottom left of the screen; this time, please choose\n4.\nthe icon located second from the right:\nThis icon will bring up a list that can be searched and filtered, again enabling\nquick visual insight utilizing various ways to align and compare data. Please\nconfirm views have changed to the correct location by comparing to the following\nscreenshot:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 316 ]\nThe columns can be sorted by clicking the heading for each column. Please\n5.\nconfirm which service has the highest MRC cost by clicking MRC twice. The first\ntime will arrange it from low to high. The second click will reverse the order and\narrange it from high to low. Please confirm via the following screenshot. Which\nservice is the most expensive? Which site is it deployed in? What is the second\nhighest and where is it deployed?\nAmazonElasticCache appears to be the highest-cost line item in the bill. This\nservice is currently deployed in USW1 (AWS San Jose). Some interesting\nquestions regarding optimization surface now that we know caching is the\nhighest-cost item in the entire billing file. Caching is typically a service that is\nemployed to keep the cost of other services down:\nIs caching working as planned?\nIs it deployed correctly?\nIs it refreshing content and removing stale content working as\nplanned?\nDoes the caching service offset other more expensive services as\nintended?\nShould this much content in San Jose be caching this often?\nIs San Jose the primary location that should be serving a majority\nof the content?\nQuickly visualizing data in this way enables attention, focus, and effort to be\nplaced in the most effective way based on insight revealed. Cloud architecture\nrequires a keen sense of utilizing only what is needed only when it is needed.\nCloud architecture is being as mindful of economic impact as required for\ntechnical details.\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 317 ]\nThe bill also has AWSDirectConnect as the second most expensive line item. This\nline item is deployed in a different location from the caching service.\nAWSDirectConnect is deployed from US East 1 (Northern Virginia), not US West\n1. AWSDirectConnect is used to connect client locations directly to AWS. What\ntypes of questions surface knowing these details?\nHow does the direct-connected location on the east coast relate to\nthe west coast location that appears to be caching a lot of content?\nIs the US East 1 location the primary location or is the US West 1\nlocation primary?\nThere were four sites represented in the billing file. Is one of the\nother locations primary?\nWhy is there 2445 GB of data being transferred in one month\nacross the AWSDirectConnect link? Big transfer or backup job?\n2445 GB transferred in less than 720 hours per month equates to a\nfully utilized 7 Mbps-8 Mbps line. Is there a more cost-effective\nsolution for low bandwidth connectivity?\nWhat location does the AWS US East 1 location directly connect to?\nCan/should the services connecting to AWS be moved into a cloud\nservice to eliminate the monthly cost associated with\nAWSDirectConnect?\nAt current market pricing, direct-connect to a 10G port is\n$2.41/hour. If using 720 hours as a standard month, the monthly\ncost shown in the bill would equate to a total of three 10GE ports\nsending a total of 7 Mbps. What is the story that this is telling?\nThe actual cost is for transfer out. AWS does not charge inbound.\nAt an average of $0.02 currently per GB of outbound transfer, 2448\nGB should account for less than $50.00 total for the month. Again,\nwhat is the story that is behind such an anomaly?\nAs great cloud architects, diving deeper is a must. There is more to this story.\n6.\nPlease click on the Contract column header at the far left:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 318 ]\nBy clicking this header, you can sort the table by this column using alphabetical\norder. Clicking one time will sort A-Z. Clicking a second time will sort Z-A.\nPlease click one time only. Please scroll down to find USE1 for US East 1. Please\nconfirm the view matches the following screenshot:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 319 ]\nMore very interesting data points rise in this view. Unfortunately, at this time, we appear to\nbe finding more questions than answers. Please look at the types of services (second\ncolumn) and the monthly costs (last column on the right). What stands out? What is the\nstory being told?\nFairly normal infrastructure is deployed that could be used in either primary or\nbackup locations including DB, block storage, compute, S3, DNS, and so on\nCosts are minimal and in some cases $0.00\nIt appears that this site would be set up as a redundant site; maybe a warm site\nthat has some data, but not thousands of GBs worth of data\nSome questions appear when looking at some of the additional detail:\nWhy are there thousands of GBs and thousands of dollars' worth of data being\ntransferred out of this site when there is very little data stored in this location?\nThe amount of data stored in this redundant/backup location does not appear to\nmatch up with what is expected of a $68,000 per month consumer of AWS cloud\nservices.\nCompute costs are zero, or close to it. Has the data that is replicated there been\nvalidated? Has it been verified to work as planned? When was the last time it\nwas checked and tested?\nThe solutions have both DynamoDB as well as RDS. In some cases, particularly in the cloud\nrealm, different types of databases can be utilized for different purposes. For example,\nDynamoDB is only a NoSQL database where RDS can be one of six types. DynamoDB is a\nmulti-tenant database solution with much lower costs. RDS is a single tenant solution at\nmuch higher costs. Both have completely different pricing models.\nAt the top of the same page in current view, there is a Text Search box. Please\n1.\ntype EZO in the search box:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 320 ]\nThe filtered results immediately change to only show locations with DynamoDB\ndeployed. Please confirm that views match the following screenshot:\nThe filtered detail shows that DynamoDB is deployed, or at least enabled, in all\nfour locations in the billing file. There is very little, if any, activity in the last\nmonth, or maybe longer:\nWhy are these services enabled and not used, or used very little?\nDo these services present any added risk, as they are likely\npartially configured, or set to basic defaults, and not locked down\nat this point?\nHow do these relate to RDS, if at all? Is RDS also partially\nconfigured?\nWhich service is primary for the business?\nWhich site is primary and backup for the database service that is\nsupposed to be utilized?\nPlease replace EZO in the Text Search box with 3%4:\n2.\nThe filtered results immediately change to only show locations with RDS\ndeployed. Please confirm that views match the following screenshot:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 321 ]\nThe filtered detail shows that RDS is only deployed in two locations based on the\ndata in the billing file. US West 1 appears to be the primary location, with nearly\n$3000.00 in monthly spend associated. The only other site is in Europe with less\nthan $250.00 in monthly spend. Again, with some answers found, more questions\nare added to the list:\nRDS can be set up in a multi-zone deployment. Based on the data,\nit does not appear to be true for this deployment. Should this be\nverified?\nHow does a single-zone deployment of RDS affect suggestions for\nthe future state?\nHow would a multi-zone RDS deployment affect economics and\nrisk?\nWhich site appears to be primary, based on database activity?\nThis may not be ideal when trying to find which location is\nproduction but has a very high probability based on the details\nseen so far.\nAs a cloud architect, many hats must be worn. We are investigators at times. The\naccountant, technician, risk manager, and strategist hats are never far away. Modern cloud\narchitects must have as much or more skill in business finance and economics as they do\ntechnical prowess.\nAs the investigation into NeBu Systems' current state has progressed, the details examined\ncontinually must work to align NeBu Systems strategically, economically, and technically.\nNeBu appears to be overspending in areas and potentially not spending enough in others.\nThe technical mix appears to be solid for the most part, with definite areas to improve. As\ndescribed by NeBu Systems earlier, things do feel like they have grown quickly without the\nbest governance and limited change management.\nUp to this point, most of the examination has been across all locations included in the\nbilling file. This has helped NeBu Systems gain a better understanding of where they are\noverall and what they are consuming, and has identified some ways to focus optimization\nefforts that may help control service sprawl and escalating costs.\nIn the next section, a deeper dive into individual locations within the billing file is needed.\nComparing existing deployments to the current market will quickly provide insight that\nwill help solidify direction and next steps for NeBu Systems as they continue their\ntransformation to cloud.\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 322 ]\nPlease click the first icon in the bottom-left corner to switch back to the design view. In the\nnext section, current state data for the primary location will be considered and compared to\ncurrent market real-time data to help expose additional insight:\nThe view should switch to display all four NeBu Systems locations, with any infrastructure\nand services currently deployed at each location. Please confirm the view matches the\nfollowing screenshot. From this view, each NeBu location and the services deployed there\ncan be individually compared to the current market to identify options that NeBu can use\nto optimize designs item by item.\nEach service has its own characteristics, deployment size, level of utilization, technical\ndetail, and economic impact. Each compute service has its own performance characteristics\nand reliability/availability trends over time. Each of these data points will help the NeBu\nSystems cloud architects align strategy, economics, and technical requirements:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 323 ]\nThe primary locations appear to be USW1 and USW2. This next section will focus on the\noptimization of USW1. The current view shows compute, storage, services, and\nconnectivity. Please click on the affectionately named hamburger menu at the top right of\nthe USW1 current state design. Please confirm menu location in the following screenshot:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 324 ]\nIt will take a minute or two for the view to change. In real time, every line item is analyzed\nand compared to the current market. Once the view changes, a BOM view should show on\nthe right, with the design visualization on the left. Please confirm the view has changed as\nshown in the following screenshot:\nPlease scroll through the line items on the right to the bottom of the page. Three ovals\nshould now be visible. Please confirm the current view matches the following screenshot,\nwith the three ovals now visible at the bottom of the page:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 325 ]\nAgain, what stories can be told with the data?\nThe billing data shows nearly $31,000.00 total spent for this location during the\nbilling period\nThe monthly recurring cost (MRC) is $31,000.00\nThe MRC equals the total, meaning that all services have a term of one month or\nless\nNone of the spend is NRC, meaning NeBu Systems is not currently utilizing any\nreserve instances\nThe data mentioned provides a good high-level overview of the current state services and\ncurrent state spending for NeBu Systems. More detail is needed as optimization efforts are\nexplored:\nWhat is driving the cost of the solution?\nAre there any strategic, economic, or technical factors that highlight where the\nfocus should be placed as future state considerations are made?\nHow does performance factor in?\nCan the footprint be consolidated to help control cost?\nAre the correct or optimal instance types being used?\nAre consumption models matching up with strategy?\nPlease click on the middle icon at the bottom left of the screen. This will change the view to\nshow how each service contributes to the overall cost of the solution. Larger blocks mean\nthat items with larger block size account for larger portions of the overall spend. This\nprovides a visual way for cloud architects to quickly identify places to focus and find\nalternatives to re-align strategy, economics, and technology:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 326 ]\nPlease confirm that the view has changed to match the following screenshot. A couple of\nvery large blocks quickly points out that a small number of services are contributing to a \nmajority of the cost in the current state:\nThe purple block is Other Costs. These costs are AWS-specific services that may lead to\nvendor lock-in. These services are generally not the same from provider to provider. There\nmay be alternatives that could be used by other providers. Additional time and effort are\nneeded to investigate each of these further. The Other Costs block accounts for 31% of the\ntotal solution cost monthly.\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 327 ]\nThe second large block (dark blue) is associated with the N\u0015\u000fYMBSHF\u000fMJOVY instance type.\nThis single instance type is contributing 28% to the overall solution cost each month. There\nmay be more than one instance deployed, but this type of instance is contributing\nsignificantly to the overall NeBu Systems solution in US West 1. Some interesting questions\ncome to mind based on these two additional data points:\nWhat services are AWS-specific?\nIs lock-in to AWS an issue? Does it need to be resolved?\nThe M3 instance types are older instances that have now been updated to newer\nversions. Should these be upgraded?\nWhy have the M3 instances not been upgraded to a more current version?\nM3 instances are a general use compute type with SSD storage. Is it better to split\nthe applications into more cost-effective compute types that match the\napplications?\nWould smaller instance types match NeBu System strategy better technically\nand/or economically?\nWhat are these instances doing? Are they still critical to the solution?\nAs upgrades and changes for a future state are considered, what new services\nmay align better strategically, economically, and technically to NeBu Systems'\ncurrent direction?\nA general idea is now understood regarding how NeBu Systems has deployed their\ninfrastructure and services. Several questions have been raised with very good\nopportunities for optimization coming into focus quickly.\nPlease click on the first icon at the bottom left of the screen to change the view to the IQ\nview:\nThe view should change to show the Summary tab by default. This view provides high-\nlevel details such as location, cost, and price-to-performance for the entire solution. Please\nconfirm the view has changed to match the following screenshot:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 328 ]\nIn this view, the location is confirmed as San Jose, CA, which is US West 1. Again, the total\ncost is shown. Two new pieces of data are shown in this view. First, it is shown as (from\ncontract) in the first column under Solution Set. This distinguishes data that is from the\ncurrent state billing file versus comparison market data that is compared in real time. The\nsecond new piece of data is the $/BCU red text in the middle of the screen. This red number\nis the Burstorm Compute Unit (BCU), an average cost per unit of performance based on\nthe benchmark data discussed in depth at the end of the previous chapter. $/BCU will be\nused in later steps to compare solutions and individual solution components to current\nmarket options available. These comparisons will help quickly identify options that have\nlower price-to-performance ratios. Lower $/BCU numbers are more desirable if all other\ncriteria are equal.\nPlease click on the Details tab in the gray bar at the top of the provider response window.\nThe location of the tab is shown in the following screenshot:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 329 ]\nThe view should now have detailed solution data for each line item in the current state\nsolution. Please confirm that the solution detail is shown. The following screenshot is\nincluded for reference:\nThe preceding view shows the services portion of the current state bill related to Amazon-\nspecific services. 30% of NeBu Systems' current monthly spend ($9,424.00) is associated\nwith the AWS-specific services. Some of these services have already been discussed in\ndetail earlier in this chapter.\nScrolling down through the same window shows the same level of detail for the\ninfrastructure components and services. Please match views with the following screenshot.\nScrolling to the bottom exposes two ovals. The green oval shows the total for the\ninfrastructure components ($21,510.00). The second oval contains price-to-performance\ndata. This view, by default, is set to prioritize based on price. The price-to-performance oval\nwill show performance data as a cumulative total for the current state solution (2086.1):\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 330 ]\nIn the preceding compute details, some answers to previous questions can be answered.\nNeBu Systems noticed a very large portion of the bill was committed to N\u0015\u000fYMBSHF\u000fMJOVY\ninstances. In this view, 21 instances are shown. The detail for each is also shown (4 cores, 15\nGB RAM, 80 GB of storage). Quick math shows this to be the largest grouping of total cores\nand RAM (81 cores and 315 GB RAM). Depending on application requirements and the\nnumber of applications, this group may be able to be changed to more cost-effective and\nmore specialized workloads that match the application and NeBu Systems' strategy better:\nWhich instance type would be more beneficial based on performance and pricing\ndata?\nIs there a way to re-stack applications to utilize a more advantageous instance\ntype and/or size?\nWhat does this cost to deploy on updated infrastructure?\nAre there any applications that can now be purchased as a service?\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 331 ]\nPlease change from prioritizing on price to prioritizing on price performance. Changing the\nprioritization of the data, compute can be compared looking for opportunities to optimize\nthe instance type. Depending on actual utilization data, an N\u0015\u000fMBSHF may be more\nbeneficial than an N\u0015\u000fYMBSHF. If RAM utilization is low, the N\u0015 may be the instance of\nchoice:\nAdditional data that may also be helpful is how the individual types rank based on price\nperformance data. The following real-time data is available by looking through the ongoing\nbenchmark data. The arrows have been placed over the price-to-performance details for the\nN\u0015\u000fYMBSHF and the N\u0015\u000fMBSHF instance types:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 332 ]\nNeBu Systems applications tend to be more RAM-intensive. N\u0015\u000fMBSHF may not be the right\ninstance type based on the workload type. What does the current market have available? Is\nthere a high-performing, lower-cost instance type that matches up to the NeBu Systems\nworkload?\nPlease click on the hamburger menu above the word Summary as follows:\nA menu will appear with a switch for Exact-Match. The switch should be on by default.\nPlease click to flip the switch to OFF. Please use the following screenshot as a reference:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 333 ]\nExact-Match | OFF asks the platform to compare the solution to external solution\nproviders. Showing a provider that is not an exact 100% match is allowed when the switch\nis off. Once the app has refreshed the new data view, the following screenshot should now\nmatch the current view:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 334 ]\nThe new data allows for comparisons to be made using current market data. As in previous\nchapters, Google is lower cost than several others, including Azure and AWS. The\nfollowing screenshot shows a few interesting insights:\nGoogle is the low-cost provider\nAzure and AWS are very similar in cost\nAWS, again, appears to be the higher-performing solution with a lower $/BCU\nThe lowest-cost AWS solution is in US West 2 (Boardman, OR), not the NeBu\ncurrent location of US West 1 (San Jose, CA)\nCloud architects must often weigh risk and economics. This book has discussed that\neconomics must offset risk. The higher the risk, the lower the cost must be to make it worth\nabsorbing the risk. Migrating to Boardman may feel a bit risky. However, it is still in the\nsame region with the same provider. If the cost is significantly less and/or the performance\nis significantly high where applications can be consolidated or re-stacked, the move may be\nworth the effort. NeBu Systems has a strategy of getting the highest performance at the\nlowest cost. The move to Boardman may be a foundation piece for realigning strategy,\neconomics, and technology:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 335 ]\nThere should be a list of several solutions from several providers that match the following\nscreenshot. In this section, a comparison between the current state billing file data for US\nWest 1 and the current market. Please check the Compare box for AWS and the Compare\nbox for (from contract) toward the bottom of the list. Please see the marked boxes in the\nfollowing screenshot for reference:\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 336 ]\nBefore comparing, there are a couple of interesting data points worth mentioning in this\nview. NeBu Systems is focused on finding the best performance at the lowest cost.\nChanging providers is an option if the price and/or performance is worth the risk:\nThe current state bill is one of the most expensive options presented\nThe current state option is $10,000+ higher than current market with the same\nprovider\nThe current state services are potentially much slower than current services from\nthe same provider\nThe data in this view makes comparisons very easy for NeBu Systems. This data alone may\nlead NeBu to conclude that focusing on staying with AWS is the right option and migrating\nto Boardman may make a lot of sense as well. A direct comparison between the two AWS\nlocations is the next logical step.\nPlease change to the Compare view at the top of the provider response table as follows:\nThe view will immediately change to align each unique line item side by side. The\nfollowing screenshot is shown for reference if needed:\nIt becomes very clear quickly that staying with AWS and migrating to Boardman has many\nbenefits. Please scroll to the bottom and look at the ovals with the summarized cost and\nprice-to-performance data:\nBoardman has a much lower $/BCU ratio, $3.50 versus $8.83 for current state\nBoardman is lower infrastructure cost, $11,295 versus $21,510 for current state\nChanging the prioritized view from price performance to performance only\nshows that only Boardman is significantly faster, 2698.2 versus 2086.1 for current\nstate\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 337 ]\nIf these comparisons are difficult, refer to the first couple of sections of the hands-on labs, as\neach of these comparisons were detailed in those sections.\nIt is also very interesting to look at some of the side-by-side comparisons to see what is\nsuggested based on the current state data available. Please look at the following example:\nThe first line is the current state solution. A total of 21 N\u0015\u000fYMBSHF instances were\ndeployed accounting for $8,279.04 with a performance score of 413.1.\nThe second line is the potential future state solution utilizing 21 U\u0014\u000fYMBSHF\ninstances for only $2848.27.\nThe difference between the two options is 66% less cost and a 20% increase in\nperformance.\nT2 instances may work very well for NeBu Systems' strategy as most of the\napplications are RAM-intensive, not CPU-intensive. The T2 series instances could\nstay at base CPU performance levels, controlling costs quite well. The T2 prices\nare very depending on how CPU performance and load increases. Staying at base\nperformance would allow NeBu systems to utilize the RAM fully without\nincreasing costs. Key note: understanding how economics and technology relate\nenables the simultaneous alignment of strategy, economics, and technology:\nSummary\nNeBu Systems needed to revisit their transformation in progress and current deployment\nthat has grown quickly. In this lab, price-to-performance became the key differentiator for\nnearly all the choices made.\nThese hands-on labs intentionally took a very infrastructure-centric view of the world.\nInfrastructure has long been ignored in favor of more sexy and endearing things such as\napplications. Applications are what users interact with. They are the things that are most\noften seen and commented on, not the infrastructure underneath.\n\n\nHands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 338 ]\nInfrastructure is getting cheaper by the day. The race-to-zero is just beginning for compute.\nNetwork has been on its way for a while. Storage is also beginning the run. With\ninfrastructure declining in price and the cost of management and operations rising\nexponentially, the cost of a mistake at the infrastructure level can be very costly, with\nchanges in strategy and technical direction nearly cost-prohibitive. Get the foundation set\nright. Then build on the foundation.\nThis lab started with data at very high levels, abstracting most of the detail away. By doing\nthis, the strategy could be quickly analyzed and confirmed. The first part of the lab was\ninvestigative work, matching up the stories the billing data was telling with the\nexpectations of what NeBu Systems thought they were doing. The middle portion of the lab\nwas used to look for deeper-level data to confirm stories and find opportunities to change\nthe narrative. The last part of the lab showed how to confirm direction by answering\nquestions raised during the beginning and middle of the lab.\nNot all questions raised were answered. That was never the intent. The intent was to build\na process and pattern for thinking, raising questions, searching for relevant data, and\nanswering questions that help accomplish the simultaneous alignment of strategy,\neconomics, technology, and risk. Cloud architecture is about quickly triaging data,\nidentifying relevance, and remaining keenly aware of real-time insight.\n\n\n20\nCloud Architecture  Lessons\nLearned\nIf you are completely successful in navigating the complex and competing priorities levied\non every cloud computing solution, your efforts will fail unless a successful\nimplementation follows. Although implementation is outside the scope of this solution\ndesign text, we would like to share with you the lessons we have learned in these early\nyears of cloud computing architecting:\nTo be a successful cloud solution architect, you need to obtain and maintain\nexecutive sponsorship. Make sure governance control points are built into the\ntransition process (an example control point: ensure financial controls on pay-as-\nyou-go elastic compute model do not result in runaway costs!).\nAnalysis of an organization's application portfolio as a whole is key to efficiency\nand the breadth of value delivered by transitioning to a cloud platform.\nManagement oversight and review processes for legacy application transitions to\nIaaS platforms should be modified to reflect their software-only nature.\nMost customers are only aware of a few large cloud service providers (for\nexample, AWS, Azure, Google, Salesforce, and IBM). They also may be limited to\nselecting a single CSP platform (that is, C2S). This does not reduce or eliminate\nthe need to evaluate the economics and performance aspects of a transition\nstrategy to the broader marketplace.\n\n\nCloud Architecture  Lessons Learned\nChapter 20\n[ 340 ]\nLack of IT standards or a failure to enforce those standards results in differences\nbetween your development, test, and production environments. This\nsignificantly reduces your ability to leverage automated testing tools and delays\nyour cloud transition. Developers must be educated on Application\nPerformance Monitoring (APM) capabilities, service management/monitoring\ncapabilities, web and mobile analytics, and alerting and notification solutions. \nThe most severe challenge when adopting cloud computing is around cultural\nchange. A focused and dedicated informational and educational campaign\nshould be in place to support this type of transition. A lack of cloud computing\neducation and understanding is one of the most significant organizational risks.\n",
      "page_number": 323
    },
    {
      "number": 20,
      "title": "[ 340 ]",
      "start_page": 354,
      "end_page": 367,
      "detection_method": "regex_chapter",
      "content": "Epilogue\nWhere should NeBu Systems go next? They have successfully broken apart their monolithic\napplication and now utilize cloud services. Is it a case of congratulations on finishing? Or is\nit congratulations on starting? Did NeBu successfully migrate or transform? \nMigrations are a series of things that get done. Migrations seem tangible: from this to that,\nfrom here to there. Transformations, interestingly, are mental and emotional.\nTransformations require a change in mindset. Transformations require constant data  that\ncan be continuously compared to expose insights and establish perceived value. \nMigrations are planned and executed. Transformations are adopted. Without adoption,\ntransformation fails. Adoption requires a change in mindset, often created from a\ncontinuous digestion of highly valued relevant data and insight. This means continuously\nsensing the environment and continuously changing your actions to better align with goals,\nwhich are also changing continuously. We, the authors, call this being senso-morphic.\nBusinesses and people tasked with adapting and driving change must become senso-\nmorphic.\nToday, many are flooded with data, yet remain uninformed. Many know they are in the\nwrong place, yet struggle to know where they are. The only sustainable path for positive\ntransformation is to become senso-morphic. In the world of cloud computing, this means\nbeing senso-morphic across many domains, simultaneously. The senso-morphic domains\nare shown in the following table:\n\n\nEpilogue\n[ 342 ]\nThis book is but the first step in a long journey. Our wish is that we've prepared you well.\nGood luck.\n\n\nOther Books You May Enjoy\nIf you enjoyed this book, you may be interested in these other books by Packt:\nGoogle Cloud Platform Cookbook\nLegorie Rajan PS\nISBN: 978-1-78829-199-6\nHost a Python application on Google Compute Engine\nHost an application using Google Cloud Functions\nMigrate a MySQL DB to Cloud Spanner\nConfigure a network for a highly available application on GCP\nLearn simple image processing using Storage and Cloud Functions\nAutomate security checks using Policy Scanner\nUnderstand tools for monitoring a production environment in GCP\nLearn to manage multiple projects using service accounts\n\n\nOther Books You May Enjoy\n[ 344 ]\nArchitecting Microsoft Azure Solutions ` Exam Guide 70-535\nSjoukje Zaal\nISBN: 978-1-78899-173-5\nUse Azure Virtual Machines to design effective VM deployments\nImplement architecture styles, like serverless computing and microservices\nSecure your data using different security features and design effective security\nstrategies\nDesign Azure storage solutions using various storage features\nCreate identity management solutions for your applications and resources\nArchitect state-of-the-art solutions using Artificial Intelligence, IoT, and Azure\nMedia Services\nUse different automation solutions that are incorporated in the Azure platform\n\n\nOther Books You May Enjoy\n[ 345 ]\nLeave a review - let other readers know what\nyou think\nPlease share your thoughts on this book with others by leaving a review on the site that you\nbought it from. If you purchased the book from Amazon, please leave us an honest review\non this book's Amazon page. This is vital so that other potential readers can see and use\nyour unbiased opinion to make purchasing decisions, we can understand what our\ncustomers think about our products, and our authors can see your feedback on the title that\nthey have worked with Packt to create. It will only take a few minutes of your time, but is\nvaluable to other potential customers, our authors, and Packt. Thank you!\n\n\nIndex\nA\naccess control lists (ACLs)  \u0014\u0017\u0017\nActive Server Pages (ASP)  \u0014\u0017\u0012\nAI  \u0015\u0018\nAlternative PHP Cache (APC)  \u0014\u0016\u001b\nAmazon Web Services (AWS)  \u0014\u0013\u001b\nAnsible  \u0014\u0017\u0013\nAPI management\n   about  \u0013\u0016\u001a\n   reference  \u0013\u0016\u001a\napplicable governance regimes\n   FedRAMP  \u0014\u0017\u0019\n   FERPA  \u0014\u0017\u0019\n   GDPR  \u0014\u0017\u0019\n   HIPPA  \u0014\u0017\u0019\n   PCI DSS  \u0014\u0017\u0019\n   SOX  \u0014\u0017\u0019\napplication category  \u0013\u0013\u0014, \u0013\u0013\u0015\napplication dependencies  \u0013\u0013\u0016\napplication design  \u0013\u0012\u0019\napplication federation  \u0014\u0014\u0018\napplication migration  \u0013\u0012\u001b\napplication migration planning\n   about  \u0014\u0017\u0013\n   processes  \u0014\u0017\u0014\nApplication Normative Framework (ANF)  \u0014\u0018\u0016\nApplication Performance Monitoring (APM)  \u0015\u0016\u0012\napplication programming interface (API)\n   about  \u0014\u001a, \u0014\u0014\u0016\n   benefits  \u0014\u0014\u0016\n   categories  \u0014\u0014\u0017\n   cloud middleware API  \u0014\u0014\u0017\n   for cloud storage  \u0014\u0014\u0017\n   levels  \u0014\u0014\u0017\n   Representational state transfer (REST)  \u0013\u0013\u0018, \u0014\u0014\u0016\n   Simple object access protocol (SOAP)  \u0013\u0013\u0017, \u0014\u0014\u0016\n   using  \u0013\u0013\u0017\nApplication Security Management Process (ASMP) \n\u0014\u0018\u0014, \u0014\u0018\u0016, \u0014\u0018\u0017\napplication security risks\n   reference  \u0014\u0018\u0019\napplication service provider (ASP)  \u0015\u0018\napplication virtualization  \u0014\u0012\u0012\napplication workload\n   about  \u0013\u0013\u0012\n   once-in-a-lifetime workloads  \u0013\u0013\u0012\n   static workloads  \u0013\u0013\u0012\n   unpredictable and random workloads  \u0013\u0013\u0013\narchitecture executive decisions\n   automation  \u001b\u0015\n   challenges, expressing  \u001b\u0014\n   culture  \u001b\u0018\n   economics  \u001b\u0015\n   process  \u001b\u0013\n   real-time collaboration  \u001b\u0014\n   risk  \u001b\u0017\n   strategy  \u001b\u0015\n   technology  \u001b\u0016\nAsynchronous JavaScript And XML (AJAX)  \u0014\u0017\u0012\nAtomic, Consistent, Isolated, Durable (ACID)  \u0014\u0013\u0013\nauditability  \u0014\u0015\u0012\nauditing  \u0014\u0013\u0015, \u0014\u0013\u0016, \u0014\u0013\u0017\nB\nbaseline architecture\n   single server architecture  \u0013\u0014\u0015\n   single-site architecture  \u0013\u0014\u0016\n   types  \u0013\u0014\u0015\nbenchmarks  \u0014\u0015\u0013\nbig data analytics (BDA)\n   about  \u0013\u0018\u0018\n   enterprise network  \u0013\u0019\u0013\n\n\n[ 347 ]\n   provider cloud components  \u0013\u0018\u001b, \u0013\u0019\u0012\n   public network components  \u0013\u0018\u001a\n   security  \u0013\u0019\u0014\nBill of Material (BOM)  \u0015\u0013\u0016\nbinary large objects (blobs)  \u0014\u0012\u0019\nblockchain  \u0013\u0019\u0015\nblockchain reference architecture capabilities\n   about  \u0013\u0019\u0015\n   blockchain services  \u0013\u0019\u0018\n   cloud network  \u0013\u0019\u0017\n   enterprise data connectivity  \u0013\u0019\u0018\n   public network  \u0013\u0019\u0016\nBluetooth Low Energy (BTLE)  \u0013\u001a\u0012\nBurstorm Compute Unit (BCU)  \u0015\u0014\u001a\nBurstorm lab 1\n   background  \u0014\u0019\u0019\n   design scenario solution results  \u0014\u001a\u001b, \u0014\u001b\u0012\n   design scenario, creating  \u0014\u001a\u0013, \u0014\u001a\u0014, \u0014\u001a\u0017, \u0014\u001a\u0019\n   high-level rapid insights  \u0014\u001b\u0013, \u0014\u001b\u0014\n   model, creating  \u0014\u0019\u001a, \u0014\u0019\u001b, \u0014\u001a\u0012\nBurstorm lab 2\n   additional detail, accessing  \u0014\u001b\u0019\n   advanced insight  \u0014\u001b\u0019\n   Details tab  \u0014\u001b\u001a, \u0014\u001b\u001b\n   selection, for direct comparison  \u0014\u001b\u001b, \u0015\u0012\u0012\nbusiness continuity (BC)  \u0013\u0013\u001a, \u0014\u0019\u0014\nbusiness continuity and disaster recovery (BCDR) \n\u0013\u0013\u001a\nbusiness goal KPIs  \u001a\u0018\nC\nCapital Expenditure (CAPEX)  \u0017\u0017\nchange management  \u0016\u0017, \u0016\u0018, \u0016\u0019\nChef  \u0014\u0017\u0012\nChief Data Officer (CDO)  \u0013\u0018\u001a\ncloud application deployment  \u0014\u0014\u0018\ncloud applications\n   characteristics  \u0014\u0016\u0018, \u0014\u0016\u0019\n   client-side  \u0014\u0017\u0012\n   components  \u0014\u0016\u001a\n   server side  \u0014\u0016\u001a\ncloud computing security  \u0014\u0015\u0013\ncloud computing solution catalogs\n   architecting  \u0017\u0015, \u0017\u0017, \u0017\u0019\ncloud computing solution\n   best practices  \u0015\u0015\u001b\n   designing, goals  \u0013\u001b\u0016\ncloud computing threats  \u0014\u0018\u0019\ncloud computing\n   about  \u001a\u0014\n   business drivers  \u0019\u0016\n   business strategies  \u0019\u0016, \u0019\u0017\n   characteristics  \u0013\u0018, \u0013\u0019, \u0013\u001a, \u0013\u001b, \u0014\u0012\n   clients  \u0014\u0012\u0014, \u0014\u0012\u0015, \u0014\u0012\u0016\n   defining  \u0013\u0017, \u0013\u0018\n   goals  \u0018\u001b, \u0019\u0012, \u0019\u0013, \u0019\u0014\n   history  \u0013\u0014\n   operational models  \u0014\u0013\n   phases  \u0013\u0015\n   risks  \u0014\u0018\u001a\n   taxonomy  \u0015\u0019, \u0015\u001a\n   use cases  \u001a\u0019\nCloud Control Matrix\n   reference  \u0014\u0017\u0019\ncloud customer reference architecture\n   enterprise network  \u0013\u0018\u0018\n   for enterprise social collaboration  \u0013\u0018\u0012\n   overview  \u0013\u0018\u0012\n   provider network  \u0013\u0018\u0014\n   security  \u0013\u0018\u0016\n   service consumer  \u0013\u0018\u0014\n   user network  \u0013\u0018\u0013\ncloud delivery models  \u0015\u0016\ncloud deployment models  \u0014\u001a\ncloud middleware API\n   additional concerns  \u0014\u0014\u0017\ncloud network  \u0013\u0019\u0017\ncloud provider network components\n   about  \u0013\u0016\u0019\n   web service tier  \u0013\u0016\u0019\ncloud security threat\n   reference  \u0014\u0018\u0019\ncloud service availability  \u0014\u0014\u001b\ncloud service models\n   about  \u0014\u0013, \u0014\u0014, \u0014\u001a\n   for executives  \u001b\u0019, \u001b\u001a\n   IaaS  \u0014\u0014\n   PaaS  \u0014\u0018\n   SaaS  \u0014\u0016\ncloud service providers (CSPs)  \u0015\u001a, \u0016\u0016, \u0014\u0013\u0015, \u0014\u0015\u0017\n\n\n[ 348 ]\ncloud service solutions\n   application security  \u0013\u0016\u0017\n   data security  \u0013\u0016\u0017\n   identity and access management  \u0013\u0016\u0017\n   infrastructure security  \u0013\u0016\u0017\n   secure DevOps  \u0013\u0016\u0017\n   security  \u0013\u0016\u0016\n   security governance, risk and compliance  \u0013\u0016\u0017\n   security monitoring and vulnerability  \u0013\u0016\u0017\ncloud service\n   economics  \u0013\u0013\u001b\ncloud services, BCDR\n   scenarios  \u0013\u0013\u001a\ncloud solution providers (CSPs)  \u0013\u001a\nCloud Standards Customer Council (CSCCTM) \n\u0013\u0016\u0015\ncloud washing  \u0015\u0018\ncloud\n   complexity  \u0014\u0019\u0017\n   concurrency  \u0014\u0015\u0012\n   noise, eliminating  \u0014\u0019\u0018\n   transaction  \u0014\u0015\u0012\nComma Separated Value (CSV)  \u0013\u0013\u0018\nCommand line interface (CLI)  \u0013\u0019\u0019\ncommand-line interfaces (CLI)  \u0014\u0012\u0015\ncommon infrastructure file formats  \u0014\u0014\u0018\ncommunications services\n   about  \u0014\u0012\u001b\n   virtual networks  \u0014\u0012\u001b\ncommunity cloud  \u0015\u0015\ncomplex architecture\n   alert-based scalable setup  \u0013\u0015\u0016\n   caching  \u0013\u0015\u0015\n   cloud hosting architecture  \u0013\u0015\u0019\n   database resiliency  \u0013\u0015\u0014\n   databases  \u0013\u0015\u0015\n   dedicated hosting architecture  \u0013\u0015\u0019\n   failover multi-cloud architecture  \u0013\u0015\u0018\n   global server load balancing  \u0013\u0015\u0013\n   hybrid cloud site architecture  \u0013\u0015\u0017\n   multi-data center architecture  \u0013\u0015\u0012\n   queue-based scalable setup  \u0013\u0015\u0016\n   scalable multi-cloud architecture  \u0013\u0015\u0017\n   types  \u0013\u0015\u0012\nComplex Event Processing (CEP)  \u0013\u0018\u001b\ncomponents, Organization Normative Framework\n(ONF)\n   application security control library  \u0014\u0018\u0016\n   business context  \u0014\u0018\u0016\n   qualifications  \u0014\u0018\u0016\n   regulatory context  \u0014\u0018\u0016\n   responsibilities  \u0014\u0018\u0016\n   roles  \u0014\u0018\u0016\n   specifications  \u0014\u0018\u0016\n   technical context  \u0014\u0018\u0016\ncompute virtualization  \u0013\u001b\u0017\nConfiguration Management Database (CMDB)  \u0017\u0012\nconfiguration management tools\n   Ansible  \u0014\u0017\u0013\n   Chef  \u0014\u0017\u0012\n   Puppet  \u0014\u0017\u0012\n   Salt  \u0014\u0017\u0013\ncontent delivery network (CDN)  \u0013\u0016\u0019\ncore application characteristics\n   loose coupling  \u0014\u0016\u0017\n   service orientation  \u0014\u0016\u0018\nCSP benchmarks  \u0014\u0015\u001a, \u0014\u0016\u0013\nCSP performance metrics  \u0014\u0015\u0017, \u0014\u0015\u0018\ncurrent state data\n   visualizing  \u0015\u0012\u001b\ncustomer relationship management (CRM)  \u0013\u0017\u0013\nD\ndata classification\n   about  \u0014\u0017\u001a\n   authorizations  \u0014\u0017\u001a\n   custodianship  \u0014\u0017\u001a\n   information classification  \u0014\u0017\u001a\n   information management policies  \u0014\u0017\u001a\n   jurisdictional policies  \u0014\u0017\u001a\n   location policies  \u0014\u0017\u001a\n   ownership  \u0014\u0017\u001a\ndata federation  \u0014\u0014\u0018\ndata privacy  \u0014\u0017\u001b\ndata security\n   life cycle  \u0014\u0017\u0017, \u0014\u0017\u0018\ndata virtualization  \u0013\u001b\u001a\ndata-driven design  \u0014\u001b\u0016, \u0014\u001b\u0017\ndatabase  \u0014\u0013\u0019, \u0014\u0013\u001a, \u0014\u0013\u001b\ndatabase resiliency  \u0013\u0015\u0014\n\n\n[ 349 ]\ndedicated cloud  \u0015\u0012, \u0015\u0014\nDepartment of Defense (DoD)  \u0013\u0013\u0019\ndeployment models\n   for executives  \u001b\u001a\ndesign consideration, cloud computing\n   economic  \u0018\u0012, \u0018\u0013, \u0018\u0014\n   plans  \u0018\u0015, \u0018\u0016, \u0018\u0017, \u0018\u0019, \u0018\u001a\n   thought process  \u0017\u001a, \u0017\u001b\nDevOps  \u0014\u0017\u0012\ndew computing  \u0015\u0017\ndirect comparison selection, Burstorm lab 2\n   performance  \u0015\u0012\u0014, \u0015\u0012\u0015\n   price  \u0015\u0012\u0012, \u0015\u0012\u0013\n   price-to-performance  \u0015\u0012\u0016, \u0015\u0012\u0017, \u0015\u0012\u0018\ndisaster recovery (DR)  \u0013\u0013\u001a, \u0014\u0019\u0015\ndistributed computing reference model\n   about  \u0014\u0015\u0016\n   reference link  \u0014\u0015\u0016\ndomain name system (DNS)  \u0013\u0018\u001a\nDomain Name System (DNS)  \u0013\u0018\u0014\nDomain name system server (DNSS)  \u0013\u0019\u0016\ndomain-specific language (DSL)  \u0014\u0017\u0012\nE\ne-commerce\n   about  \u0013\u0016\u001a\n   cloud provider components  \u0013\u0017\u0012\n   enterprise network components  \u0013\u0017\u0016\n   public network components  \u0013\u0016\u001b\nedge computing  \u0015\u0017\nedge tier, IoT\n   cloud service provider  \u0013\u001a\u0013\n   enterprise network  \u0013\u001a\u0014\n   public network  \u0013\u001a\u0013\n   security  \u0013\u001a\u0014\nElastic infrastructure  \u0013\u001b\u0012\nElastic platform  \u0013\u001b\u0013\nelectronic fund transfers (EFT)  \u0013\u0017\u0013\nEnterprise Application Integration (EAI)  \u0013\u001a\u0018\nenterprise application resource (EAR)  \u0014\u0016\u001b\nenterprise network components\n   about  \u0013\u0016\u0019, \u0013\u0017\u0016\n   enterprise data  \u0013\u0017\u0016\n   security  \u0013\u0017\u0017\n   service tier  \u0013\u0016\u001a\nEnterprise Resource Planning (ERP)  \u0013\u001a\u0015\nEnterprise Service Bus (ESB)  \u0013\u0019\u0018\nenterprise social collaboration\n   about  \u0013\u0018\u0012\n   cloud customer reference architecture, for\nenterprise social collaboration  \u0013\u0018\u0012\nenvironment-based availability  \u0013\u001b\u0014\neXtensible Markup Language (XML)  \u0013\u0013\u0017\nF\nFederal Risk and Authorization Management\nProgram (FedRAMP)  \u0013\u0013\u0019\nFederal Risk Authorization and Management\nProgram (FedRAMP)  \u0018\u0017\nfederated identity  \u0014\u0014\u0018\nfinancial levers  \u0019\u0019\nfog computing  \u0015\u0017\nfunctions, date security\n   access  \u0014\u0017\u0018\n   process  \u0014\u0017\u0018\n   store  \u0014\u0017\u0018\nG\nGeneral Data Protection Regulation (GDPR)  \u0018\u001a,\n\u0014\u0014\u001b, \u0014\u0017\u0016\nglobal server load balancing  \u0013\u0015\u0013\nGoogle App Engine (GAE)  \u0014\u0014\u0012\nGovernance, Risk Management, and Compliance\n(GRC)  \u0013\u0012\u001b\ngraphical user interface (GUI)  \u0014\u0012\u0015\ngrid computing  \u0015\u0017\nH\nHands-on lab 3\n   current billing file  \u0017, \u0015\u0013\u0015, \u0015\u0013\u0016, \u0015\u0013\u0017, \u0015\u0013\u0019, \u0015\u0013\u001a,\n\u0015\u0013\u001b, \u0015\u0014\u0013, \u0015\u0014\u0015, \u0015\u0014\u0016, \u0015\u0014\u0018, \u0015\u0014\u0019, \u0015\u0014\u001a, \u0015\u0014\u001b, \u0015\u0015\u0012,\n\u0015\u0015\u0015, \u0015\u0015\u0017, \u0015\u0015\u0019\n   data visualization  \u0015\u0013\u0012, \u0015\u0013\u0013, \u0015\u0013\u0014\n   NeBu Systems transformation progress update \n\u0015\u0013\u0014, \u0015\u0013\u0015\nHealth Insurance Portability and Accountability Act\n(HIPAA)  \u0018\u001a\nhybrid cloud\n   about  \u0015\u0015\n\n\n[ 350 ]\n   application functions  \u0013\u0016\u0012\n   architecting  \u0013\u0015\u001a\n   backend  \u0013\u0016\u0012\n   backup  \u0013\u0015\u001b\n   data  \u0013\u0015\u001b\n   development environment  \u0013\u0016\u0013\n   multimedia web application  \u0013\u0016\u0013\n   processing  \u0013\u0015\u001b\n   user interface  \u0013\u0015\u001a\nhybrid integration\n   architecture  \u0013\u001a\u0015\n   cloud provider network  \u0013\u001a\u0017\n   connectivity  \u0013\u001a\u0015\n   deployment  \u0013\u001a\u0015\n   enterprise network  \u0013\u001a\u0018\n   public network  \u0013\u001a\u0017\n   roles  \u0013\u001a\u0015\n   styles  \u0013\u001a\u0016\nHypertext Transfer Protocol (HTTP)  \u0014\u0014\u0016\nhypervisor\n   type 1 hypevisor  \u0013\u001b\u0018\n   type 2 hypervisor  \u0013\u001b\u0018\nI\nIaaS\n   about  \u0014\u0012\u0016\n   background  \u0014\u0014\n   compute services  \u0014\u0012\u0016, \u0014\u0012\u0017\n   considerations  \u0014\u0015\n   storage services  \u0014\u0012\u0017, \u0014\u0012\u0018, \u0014\u0012\u0019\nidentity management  \u0014\u0014\u0019\nimplementation models\n   for executives  \u001b\u001b\nimplementation strategy  \u0016\u0016\nInformation Technology (IT)  \u0013\u001a\u0015\nInformation Technology Infrastructure Library (ITIL) \n\u0016\u001a\nIntegrated Development Environment (IDE)  \u0013\u0013\u0017\nintegrated development environment (IDE)  \u0013\u001b\u0013,\n\u0014\u0013\u001b, \u0014\u0016\u001b\nInternet of Things (IoT)  \u0014\u0012\u0015\ninteroperability\n   about  \u0014\u0014\u0019\n   applications  \u0014\u0014\u0019\n   data  \u0014\u0014\u0019\n   infrastructure  \u0014\u0014\u001a\n   management  \u0014\u0014\u001a\n   platforms  \u0014\u0014\u0019\n   publication and acquisition  \u0014\u0014\u001a\nIoT\n   about  \u0015\u0017\n   architecture  \u0013\u0019\u0019, \u0013\u0019\u001b\n   architecture, aspects  \u0013\u0019\u001a\n   edge tier  \u0013\u0019\u001b\nIT governance  \u0016\u0013, \u0016\u0014, \u0016\u0015, \u001b\u001b\nIT service management  \u0016\u001a\nIT service management (ITSM)  \u0016\u001a\nIT service management framework  \u0017\u0014\nIT-as-a-Service\n   components  \u0013\u0017\nJ\nJava  \u0014\u0016\u001b\nJava Server Pages (JSP)  \u0014\u0017\u0012\nJavaScript Object Notation (JSON)  \u0013\u0013\u0018\njust-in-time (JIT)  \u0014\u0016\u001b\nK\nkey performance indicators (KPIs)\n   about  \u001a\u0017\n   business goal KPIs  \u001a\u0018\n   economic goal metric  \u001a\u0019\nKVM (Kernel-based Virtual Machine)  \u0013\u001b\u0018\nL\nLAMP  \u0014\u0016\u001b\nLAMP Stack (Linux, Apache, MySQL, PHP)  \u0013\u0014\u0015\nlegal requisites  \u0013\u0013\u0019\nlife cycle, data security\n   archive  \u0014\u0017\u0017\n   create  \u0014\u0017\u0017\n   destroy  \u0014\u0017\u0017\n   share  \u0014\u0017\u0017\n   store  \u0014\u0017\u0017\n   use  \u0014\u0017\u0017\nlifecycle management  \u0014\u0014\u001a\nlocation awareness  \u0014\u0014\u001a\nloose coupling  \u0014\u0016\u0017\n\n\n[ 351 ]\nM\nmachine learning  \u0015\u0018\nmanaged service provider (MSP)  \u0016\u0016\nmarketing resource management (MRM)  \u0013\u0017\u0015\nmean time between failure (MTBF)  \u0014\u0019\u0013\nmean time to repair (MTTR)  \u0014\u0019\u0013\nmetering  \u0014\u0014\u001a\nmicroservice architecture  \u0014\u0017\u0013\nmobile applications\n   about  \u0013\u0017\u0017\n   cloud service components  \u0013\u0017\u001a\n   enterprise network components  \u0013\u0018\u0012\n   mobile device components  \u0013\u0017\u0019\n   public network components  \u0013\u0017\u0019\nmobile architecture components  \u0013\u0017\u0017\nMobile device management (MDM)  \u0013\u0017\u001a\nmonitoring  \u0014\u0014\u001a\nmonthly recurring cost (MRC)  \u0015\u0013\u0017\nmulti-cloud analysis platform (MCAP)  \u0014\u0017\u0014\nmulti-data center architecture  \u0013\u0015\u0012\nN\nNational Institute of Standards and Technology\n(NIST)  \u0013\u0018\nnative applications  \u0014\u0012\u0014\nNeBu Systems\n   decisions, based on non-negotiable concepts \n\u0014\u001b\u0018\nnetwork address translations (NAT)  \u0014\u0012\u001b\nnetwork attached storage (NAS)  \u0014\u0012\u0018\nnetwork functions virtualization (NFV)  \u0013\u001b\u0019\nnetwork virtualization (NV)  \u0013\u001b\u0019\nnetworking interface cards (NICs)  \u0014\u0012\u001b\nneural networks  \u0015\u0018\nnode-based availability  \u0013\u001b\u0014\nNon-personally identifiable information (non-PII) \n\u0014\u0018\u0013\nnon-redundant three-tier architecture  \u0013\u0014\u0016\nO\nonce-in-a-lifetime workloads  \u0013\u0013\u0012\nopen authorization (OAUTH)  \u0013\u0018\u0016\nopen client  \u0014\u0014\u001b\nOperational Expenditure (OPEX)  \u0017\u0017\noperational level agreements (OLAs)  \u0017\u0016\nOperations Technology (OT)  \u0013\u001a\u0015\nOrganization Normative Framework (ONF)\nmanagement process  \u0014\u0018\u0015\norganizational assessment  \u0013\u0013\u001b\norganizations\n   governance  \u0014\u0015\u0012\n   management  \u0014\u0015\u0012\nOSI model and layer description\n   about  \u0013\u0014\u0019\n   autoscaling architecture  \u0013\u0014\u001a\n   logical and physical design  \u0013\u0014\u001a\nP\nperformance  \u0014\u0015\u0012\npersonally identifiable information (PII)  \u0014\u0018\u0012\nPlatform-as-a-Service (PaaS)\n   about  \u0018\u0016, \u0013\u0018\u0014, \u0014\u0013\u0019\n   background  \u0014\u0018\n   considerations  \u0014\u0019, \u0014\u001a\nportability\n   about  \u0014\u0014\u0019\n   applications  \u0014\u0014\u0019\n   data  \u0014\u0014\u0019\n   infrastructure  \u0014\u0014\u001a\n   management  \u0014\u0014\u001a\n   platforms  \u0014\u0014\u0019\n   publication and acquisition  \u0014\u0014\u001a\nprivacy addresses  \u0014\u0014\u001b\nprivacy and data protection (P and DP) laws  \u0014\u0017\u001b\nprivate cloud  \u0015\u0012\nProduct life cycle management (PLM)  \u0013\u0017\u0014\nprovider exit strategy plan  \u0014\u0015\u0013\npublic cloud\n   about  \u0014\u001b\n   benefits  \u0015\u0012\n   considerations  \u0015\u0012\npublic network\n   about  \u0013\u0016\u0019\n   cloud provider network components  \u0013\u0016\u0019\n   enterprise network components  \u0013\u0016\u0019\n   security components  \u0013\u0016\u001a\nPuppet  \u0014\u0017\u0012\n\n\n[ 352 ]\nR\nRadio Frequency Identification (RFID)  \u0013\u0017\u0014\nrandom access memory (RAM)  \u0014\u0012\u0017\nReally Simple Syndication (RSS)  \u0013\u0013\u0018\nRecovery Point Objective (RPO)  \u0017\u0018\nRecovery Time Objective (RTO)  \u0017\u0018\nRed Hat Enterprise Linux (RHEL)  \u0016\u001a\nRed Hat Enterprise Virtualization (RHEV)  \u0013\u001b\u0018\nredundant three-tier architecture\n   about  \u0013\u0014\u0017\n   horizontal scaling  \u0013\u0014\u0018\n   redundancy, versus resiliency  \u0013\u0014\u0018\n   single points of failure  \u0013\u0014\u0017\nregulatory requisites  \u0013\u0013\u0019\nRepresentational State Transfer (REST)  \u0013\u0013\u0017, \u0014\u0016\u0018\nrequest for proposal (RFP)  \u0013\u0012\u001a\nrequest for quote (RFQ)  \u0013\u0012\u001a\nresiliency  \u0014\u0014\u001b\nREST\n   about  \u0013\u0013\u0018\n   advantages  \u0013\u0013\u0018\nReturn on Investment (ROI)\n   about  \u0019\u0019, \u0019\u001a\n   driving factors  \u0019\u001b, \u001a\u0012, \u001a\u0013\nrisk\n   assessing  \u0014\u0018\u001b, \u0014\u0019\u0012, \u0014\u0019\u0013\n   framing  \u0014\u0018\u001a\n   monitoring  \u0014\u0019\u0014\nROI metrics\n   cost  \u001a\u0016\n   margin optimization  \u001a\u0016\n   quality  \u001a\u0016\n   time  \u001a\u0016\nrole, data privacy\n   controller  \u0014\u0017\u001b\n   data owner  \u0014\u0017\u001b\n   data subject  \u0014\u0017\u001b\n   processor  \u0014\u0017\u001b\nS\nSalt  \u0014\u0017\u0013\nSecurity Assertion Markup Language (SAML)  \u0013\u0018\u0016\nsecurity components  \u0013\u0016\u001a\nsecurity control\n   about  \u0014\u0015\u0013, \u0014\u0015\u0015\n   administrative  \u0014\u0015\u0014\n   logical  \u0014\u0015\u0014\n   physical  \u0014\u0015\u0014\nsecurity\n   requisites  \u0013\u0013\u0019\nserver side, cloud applications\n   JAVA  \u0014\u0016\u001b\n   LAMP  \u0014\u0016\u001b\n   WISA stack  \u0014\u0016\u001b\nserverless architecture  \u0014\u0017\u0013\nservice level agreements (SLAs)  \u0014\u0015, \u001a\u0015, \u0013\u0019\u0014, \u0014\u0013\u0018,\n\u0014\u0013\u0019, \u0014\u0016\u0015, \u0014\u0017\u0015, \u0014\u0019\u0014\nservice orientation  \u0014\u0016\u0018\nservice tier  \u0013\u0016\u001a\nservice-oriented architecture (SOA)  \u0014\u0016\u0018\nshadow IT  \u0013\u0019\nSimple Mail Transfer Protocol (SMTP)  \u0014\u0014\u0016\nSimple Object Access Protocol (SOAP)  \u0013\u0013\u0017\nsingle point of failure (SPOF)  \u0014\u0019\u0013\nSingle Sign-On (SSO)  \u0013\u0018\u0016\nsingle-site architecture\n   about  \u0013\u0014\u0016\n   non-redundant three-tier architecture  \u0013\u0014\u0016\n   redundant three-tier architecture  \u0013\u0014\u0017\nSLAs  \u0014\u0015\u0013\nSOAP\n   about  \u0013\u0013\u0017\n   advantages  \u0013\u0013\u0018\nsoftware defined networking (SDN)  \u0013\u001b\u0019\nSoftware Development Kit (SDK)  \u0013\u0019\u0019\nsoftware development kits (SDKs)  \u0013\u0017\u0019\nSoftware-as-a-Service (SaaS)  \u0013\u0018\u0014\n   about  \u0018\u0016\n   background  \u0014\u0016\n   considerations  \u0014\u0017\n   offering  \u0014\u0014\u0012\nstatic workloads  \u0013\u0013\u0012\nstorage area networks (SAN)  \u0014\u0012\u0018\nstorage services, IaaS\n   archival storage  \u0014\u0012\u001b\n   key-value storage  \u0014\u0012\u001a\n   object/blob storage  \u0014\u0012\u0019\n   volume/block storage  \u0014\u0012\u0019\n\n\nT\ntechnical architecture\n   requisites  \u0013\u0013\u0019\ntechnology service consumption model  \u0013\u001b\u0015, \u0013\u001b\u0016\nterminal emulator  \u0014\u0012\u0015\nterminal window  \u0014\u0012\u0015\ntext-user interface (TUI)  \u0014\u0012\u0015\nthin client\n   versus thick client  \u0014\u0012\u0015\nTool Command Language (TCL)  \u0014\u0017\u0012\nTreacherous Twelve  \u0014\u0018\u0019\nU\nunpredictable and random workloads  \u0013\u0013\u0013\nuser characteristics  \u0013\u0012\u0014, \u0013\u0012\u0017, \u0013\u0012\u0018\nV\nvirtual desktop infrastructure (VDI)  \u0013\u001b\u0018\nvirtual machine  \u0014\u0012\u0017\nvirtual machine monitor (VMM)  \u0013\u001b\u0018\nvirtual networks\n   at-least-once delivery  \u0014\u0013\u0013\n   exactly-once delivery  \u0014\u0013\u0012\n   message oriented middleware  \u0014\u0013\u0012\n   metering/monitoring  \u0014\u0013\u0014, \u0014\u0013\u0015\n   timeout-based delivery  \u0014\u0013\u0014\n   transaction-based delivery  \u0014\u0013\u0013\nvirtual private cloud  \u0015\u0014\nvirtual private network (VPN)  \u0013\u0015\u0019\nvirtualization\n   about  \u0013\u001b\u0017\n   application virtualization  \u0014\u0012\u0012\n   compute virtualization  \u0013\u001b\u0017\n   data virtualization  \u0013\u001b\u001a\n   network virtualization  \u0013\u001b\u0019\nW\nweb application hosting  \u0013\u0016\u0018\nweb apps  \u0014\u0012\u0014\nweb service tier\n   about  \u0013\u0016\u0019\n   API management  \u0013\u0016\u0019\n   transformation and connectivity  \u0013\u0016\u0019\nWeb Services Description Language (WSDL)  \u0013\u0013\u0017\nWISA stack  \u0014\u0016\u001b\n",
      "page_number": 354
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Architecting Cloud Computing\nSolutions\n#VJME\u0002DMPVE\u0002TUSBUFHJFT\u0002UIBU\u0002BMJHO\u0002UFDIOPMPHZ\u0002BOE\u0002FDPOPNJDT\nXIJMF\u0002F`FDUJWFMZ\u0002NBOBHJOH\u0002SJTL\nKevin L. Jackson\nScott Goessling\nBIRMINGHAM - MUMBAI\n",
      "content_length": 182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Architecting Cloud Computing Solutions\nCopyright a 2018 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form\nor by any means, without the prior written permission of the publisher, except in the case of brief quotations\nembedded in critical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented.\nHowever, the information contained in this book is sold without warranty, either express or implied. Neither the\nauthors, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to\nhave been caused directly or indirectly by this book.\nPackt Publishing has endeavored to provide trademark information about all of the companies and products\nmentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy\nof this information.\nCommissioning Editor: Vijin Boricha\nAcquisition Editor: Shrilekha Inani\nContent Development Editor: Devika Battike\nTechnical Editor: Mohd Riyan Khan\nCopy Editor: Safis Editing\nProject Coordinator: Judie Jose\nProofreader: Safis Editing\nIndexer: Tejal Daruwale Soni\nGraphics: Tom Scaria\nProduction Coordinator: Deepika Naik\nFirst published: May 2018\nProduction reference: 1280518\nPublished by Packt Publishing Ltd.\nLivery Place\n35 Livery Street\nBirmingham\nB3 2PB, UK.\nISBN 978-1-78847-242-5\nXXX\u0010QBDLUQVC\u0010DPN\n",
      "content_length": 1490,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "Contributors\nAbout the authors\nKevin L. Jackson is a globally recognized cloud computing expert, technology thought\nleader, and CEO/founder of GovCloud Network, LLC. Mr. Jacksonbs commercial experience\nincludes being vice president of J.P. Morgan Chase and worldwide sales executive at IBM.\nHe has deployed mission applications to the US Intelligence Community cloud computing\nenvironment (IC ITE), and he has authored and published several cloud computing courses\nand books. He is a Certified Information System Security Professional (CISSP) and Certified\nCloud Security Professional (CCSP).\nThank you to my coauthor, Scott Goessling, whose knowledge and insight has greatly\nenhanced me both professionally and personally. My love and sincere admiration go out to\nmy children Lauren, Lance, and Karl, who, in their journey through life, fill me with pride\nevery day. And finally, AMLAD to the best part of my own life`s journey, my wife, Lisa.\nYou are my everything!\nScott Goessling is the COO/CTO for Burstorm and helped create the worldbs first\nautomated Cloud Solution Design platform. He has lived and worked in the Philippines,\nJapan, India, Mexico, France, and the US. Being an expert in many technologies, Scott has\nalso been a part of several successful start-ups, including a network hardware innovator\nthat was acquired for over $8B.\nScott's perspectives combine many real-world experiences. He is interested in nutrition\ntherapy, home renovation, custom car restoration, cooking, photography, sculpture, and, \nmost importantly, parenting.\nThank you to my co-author Kevin Jackson whose knowledge, experience, and patience\ncontinue to enhance me both professionally and personally. None of this would have been\npossible without the endless love, understanding, and support from my wife, Laura, and\nyoung son Grayson. Thank you for your unconditional love.\n",
      "content_length": 1866,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "About the reviewers\nSivagurunathan has over 10 years of experience in establishing and managing successful\ntechnology companies with strong expertise in cloud computing, Virtualization,\nnetworking, and security. Having co-founded his first start-up at the age of 21 and\nbootstrapped it into a multi-million dollar venture within 3 years. Siva is an alumni of IIM\nBangalore and also has a double major in engineering from Bits, Pilani. He currently\nfocuses on hybrid cloud initiatives bridging public clouds and on-premise data-centers.\nTravis Truman has 20+ years of experience in the technology industry. His previous roles\ninclude software engineering, software product architecture, SaaS platform architecture,\nand VP of engineering in several Philadelphia-area start-ups. Travis is a regular contributor\nto open source software, and he has contributed code to OpenStack, Ansible, GopherCloud,\nTerraform, Packer, Consul, and many other projects powering modern cloud computing.\nHe currently works as a cloud architect focused on OpenStack, AWS, and Azure for a\nFortune 50 media and technology company based in Philadelphia.\nPackt is searching for authors like you\nIf you're interested in becoming an author for Packt, please visit BVUIPST\u0010QBDLUQVC\u0010DPN\nand apply today. We have worked with thousands of developers and tech professionals,\njust like you, to help them share their insight with the global tech community. You can\nmake a general application, apply for a specific hot topic that we are recruiting an author\nfor, or submit your own idea.\n",
      "content_length": 1550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "NBQU\u0010JP\nMapt is an online digital library that gives you full access to over 5,000 books and videos, as\nwell as industry leading tools to help you plan your personal development and advance\nyour career. For more information, please visit our website.\nWhy subscribe?\nSpend less time learning and more time coding with practical eBooks and Videos\nfrom over 4,000 industry professionals\nImprove your learning with Skill Plans built especially for you\nGet a free eBook or video every month\nMapt is fully searchable\nCopy and paste, print, and bookmark content\nPacktPub.com\nDid you know that Packt offers eBook versions of every book published, with PDF and\nePub files available? You can upgrade to the eBook version at XXX\u00101BDLU1VC\u0010DPN and as a\nprint book customer, you are entitled to a discount on the eBook copy. Get in touch with us\nat TFSWJDF!QBDLUQVC\u0010DPN for more details.\nAt XXX\u00101BDLU1VC\u0010DPN, you can also read a collection of free technical articles, sign up for a\nrange of free newsletters, and receive exclusive discounts and offers on Packt books and\neBooks.\n",
      "content_length": 1065,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "Table of Contents\nPreface\n1\nPrologue\n7\nChapter 1: What is Cloud Computing?\n11\nCloud computing history\n12\nCloud computing definition\n15\nEssential characteristics of cloud computing\n16\nCloud computing operational models\n21\nCloud service models\n22\nIaaS  background\n22\nIaaS  things to consider\n23\nSaaS  background\n24\nSaaS  things to consider\n25\nPaaS  background\n26\nPaaS  things to consider\n27\nOther cloud service models\n28\nCloud deployment models\n28\nPublic\n29\nPrivate and dedicated\n30\nPrivate cloud\n30\nDedicated cloud\n32\nVirtual private cloud\n32\nCommunity\n33\nHybrid\n33\nOther delivery models\n34\nCloud washing\n36\nCloud computing taxonomy\n37\nSummary\n39\nChapter 2: Governance and Change Management\n40\nIT governance\n41\nImplementation strategy\n44\nChange management\n45\nIT service management\n48\nArchitecting cloud computing solution catalogs\n53\nSummary\n57\nChapter 3: Design Considerations\n58\nFoundation for design  the thought process\n58\nFoundation for design  the cloud is economic, not technical\n60\n",
      "content_length": 997,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "Table of Contents\n[ ii ]\nFoundation for design  the plans\n63\nUnderstand business strategy and goals\n69\nSummary\n76\nChapter 4: Business Drivers, Metrics, and Use Cases\n77\nReturn on Investment\n77\nROI metrics\n84\nKey performance indicators\n85\nBusiness goal key performance indicators\n86\nEconomic goal metric\n87\nGeneral use cases\n87\nSummary\n88\nChapter 5: Architecture Executive Decisions\n89\nInvert for insight  process\n91\nReal-time collaboration\n92\nExpress challenges, not requirements\n92\nAutomate and enable\n93\nStop talking technology  Strategy\n93\nEconomics, not pricing  Economics\n93\nSolutions, not servers  Technology\n94\nLower costs can be bad for business  Risk\n95\nAdoption is optional  Culture\n96\nTechnology for the executives\n97\nCloud service models for executives\n97\nDeployment models for executives\n98\nImplementation models and IT governance for executives\n99\nSummary\n100\nChapter 6: Architecting for Transition\n101\nUser characteristics\n102\nApplication design\n107\nApplication migration\n109\nApplication workloads\n110\nStatic workloads\n110\nOnce-in-a-lifetime workloads\n110\nUnpredictable and random workloads\n111\nApplication categories\n112\nApplication dependencies\n114\nUse of APIs\n115\nSOAP\n115\nREST\n116\nAdvantages of SOAP and REST\n116\nTechnical architecture requirements\n117\n",
      "content_length": 1279,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "Table of Contents\n[ iii ]\nLegal/regulatory/security requirements\n117\nBusiness continuity and disaster recovery  BCDR\n118\nEconomics\n119\nOrganizational assessment\n119\nSummary\n121\nChapter 7: Baseline Cloud Architectures\n122\nBaseline architecture types\n123\nSingle server\n123\nSingle-site\n123\nNon-redundant three-tier architectures\n124\nRedundant three-tier architectures\n125\nSingle points of failure\n125\nRedundancy versus resiliency\n126\nHorizontal scaling\n126\nOSI model and layer description\n127\nLogical and physical designs\n128\nAutoscaling architecture\n128\nComplex architecture types\n130\nMulti-data center architecture\n130\nGlobal server load balancing\n131\nDatabase resiliency\n132\nCaching and databases\n133\nAlert-based and queue-based scalable setup\n134\nHybrid cloud site architectures\n135\nScalable multi-cloud architecture\n135\nFailover multi-cloud architecture\n136\nCloud and dedicated hosting architecture\n137\nArchitecting for hybrid clouds\n138\nHybrid user interface\n138\nHybrid processing\n139\nHybrid data\n139\nHybrid backup\n139\nHybrid backend\n140\nHybrid application functions\n140\nHybrid multimedia web application\n141\nHybrid development environment\n141\nSummary\n142\nChapter 8: Solution Reference Architectures\n143\nApplication security\n144\nWeb application hosting\n146\nPublic network\n147\nCloud provider network components\n147\nWeb service tier\n147\nEnterprise network components\n147\n",
      "content_length": 1373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "Table of Contents\n[ iv ]\nService tier\n148\nSecurity components\n148\nAPI management\n148\nE-commerce\n148\nPublic network components\n149\nCloud provider components\n150\nEnterprise network components\n154\nEnterprise data\n154\nSecurity\n155\nMobile\n155\nMobile architecture components\n155\nMobile device components\n157\nPublic network Components\n157\nProvider cloud service components\n158\nEnterprise network components\n160\nEnterprise social collaboration\n160\nCloud customer reference architecture for enterprise social collaboration\n160\nArchitecture Overview\n161\nUser network\n161\nService consumer\n162\nProvider network\n162\nSecurity\n164\nEnterprise network\n166\nBig data and analytics\n166\nPublic network components\n168\nProvider cloud components\n169\nEnterprise network\n171\nSecurity\n172\nBlockchain\n173\nBlockchain Reference Architecture Capabilities\n173\nPublic network\n174\nCloud network\n175\nEnterprise data connectivity\n176\nBlockchain services\n176\nArchitecture for IoT\n177\nEdge tier\n179\nPublic network\n181\nCloud service provider\n181\nEnterprise network\n182\nSecurity\n182\nArchitecture for hybrid integration\n183\nPublic network\n185\nCloud provider network\n185\nEnterprise network\n186\nSummary\n188\n",
      "content_length": 1164,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "Table of Contents\n[ v ]\nChapter 9: Cloud Environment Key Tenets and Virtualization\n189\nElastic infrastructure\n190\nElastic platform\n191\nNode-based availability\n192\nEnvironment-based availability\n192\nTechnology service consumption model\n193\nDesign balance\n194\nVirtualization\n195\nCompute virtualization\n195\nNetwork virtualization\n197\nData virtualization\n198\nApplication virtualization\n200\nSummary\n201\nChapter 10: Cloud Clients and Key Cloud Services\n202\nCloud computing clients\n202\nIaaS\n204\nCompute services\n204\nStorage services\n205\nVolume/block storage\n207\nObject/blob storage\n207\nKey-value storage\n208\nArchival storage\n209\nCommunications services\n209\nVirtual networks\n209\nMessage oriented middleware\n210\nExactly-once delivery\n210\nAt-least-once delivery\n211\nTransaction-based delivery\n211\nTimeout-based delivery\n211\nMetering/monitoring\n212\nAuditing\n213\nService level agreement\n216\nPaaS\n217\nDatabase\n217\nIntegrated Development Environment\n219\nSaaS\n220\nSummary\n221\nChapter 11: Operational Requirements\n222\nApplication programming interface\n223\nAPI levels and categories\n225\nCommon APIs for cloud storage\n225\nCommon cloud middleware API\n225\nAdditional concerns\n225\n",
      "content_length": 1160,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "Table of Contents\n[ vi ]\nCommon infrastructure file formats  VMs\n226\nData and application federation\n226\nDeployment\n226\nFederated identity\n226\nIdentity management\n227\nPortability and interoperability\n227\nLifecycle management\n228\nLocation awareness\n228\nMetering and monitoring\n228\nOpen client\n228\nAvailability\n229\nPrivacy\n229\nResiliency\n229\nAuditability\n230\nPerformance\n230\nManagement and governance\n230\nTransaction and concurrency across clouds\n230\nSLAs and benchmarks\n231\nProvider exit\n231\nSecurity\n231\nSecurity controls\n231\nDistributed computing reference model\n234\nSummary\n234\nChapter 12: CSP Performance\n235\nCSP performance metrics\n235\nCSP benchmarks\n237\nService level agreements\n243\nSummary\n243\nChapter 13: Cloud Application Development\n244\nCore application characteristics\n245\nLoose coupling\n245\nService orientation\n246\nCloud application components\n248\nServer side\n248\nLAMP\n249\nWISA stack\n249\nJava\n249\nClient side\n250\nDevOps\n250\nMicroservices and serverless architectures\n251\nApplication migration planning\n251\n",
      "content_length": 1018,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "Table of Contents\n[ vii ]\nSummary\n253\nChapter 14: Data Security\n254\nData security life cycle\n255\nData classification\n258\nData privacy\n259\nPersonally Identifiable Information  PII\n260\nSummary\n261\nChapter 15: Application Security\n262\nThe application security management process\n262\nApplication security risks\n267\nCloud computing threats\n267\nSummary\n267\nChapter 16: Risk Management and Business Continuity\n268\nFraming risk\n268\nAssessing risk\n269\nMonitoring risk\n272\nBusiness continuity and disaster recovery\n272\nSummary\n273\nChapter 17: Hands-On Lab 1  Basic Cloud Design (Single Server)\n274\nHands-on labs and exercises\n274\nComplexity\n275\nEliminating the noise\n276\nBurstorm lab 1  background (NeBu Systems)\n277\nBurstorm lab 1  getting started\n278\nBurstorm lab 1  creating new model\n278\nBurstorm lab 1  creating a design scenario\n281\nBurstorm lab 1  design scenario solution results\n289\nBurstorm lab 1  high-level rapid insights\n291\nSummary\n293\nChapter 18: Hands-On Lab 2  Advanced Cloud Design Insight\n294\nData-driven design\n294\nAll data is useful; maybe not\n295\nBurstorm lab 2  advanced insight (NeBu Systems)\n297\nBurstorm lab 2 - accessing additional detail\n297\nOverview of the Details tab\n298\nBurstorm lab 2  selecting for direct comparison\n299\nComparing by price\n300\nComparing by performance\n302\nComparing by price-to-performance\n304\nSummary\n307\n",
      "content_length": 1357,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "Table of Contents\n[ viii ]\nChapter 19: Hands-On Lab 3  Optimizing Current State (12 Months\nLater)\n308\nVisualizing current state data\n309\nHands-on lab 3  visualizing the data\n310\nHands-on lab 3  NeBu Systems' transformation progress update\n312\nHands-on lab 3  Current billing file\n313\nSummary\n337\nChapter 20: Cloud Architecture  Lessons Learned\n339\nEpilogue\n341\nOther Books You May Enjoy\n343\nIndex\n346\n",
      "content_length": 406,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "Preface\nCloud adoption is a core component of digital transformation. Organizations must align\nmodern technology and current economic models to business strategy. Transformation\nrequires a new approach that balances cost and technology choices with company direction\nand client consumption models. Architecting Cloud Computing Solutions presents and\nexplains many critical Cloud solution design considerations and technology decisions\nrequired to successfully consume the right cloud service and deployment models based on\nstrategic, economic, and technology requirements.\nThis book starts with the fundamentals of cloud computing and its architectural concepts. It\nthen navigates through cloud service models (IaaS, PaaS, and SaaS), deployment models\n(public, private, community, and hybrid), and implementation options (Enterprise, MSP,\nand CSP). Each section exposes and discusses key considerations and challenges that\norganizations face during cloud migration. In later chapters, this book dives into how to\nleverage DevOps, Cloud-Native, and Serverless architectures in your Cloud environment.\nDiscussions include industry best practices for scaling your cloud environment, as well as\ndetails for managing essential cloud technology service components such as data storage,\nsecurity controls, and disaster recovery. By the end of this book, you will be well versed in\nall the design considerations and operational trades needed to adopt cloud services no\nmatter which cloud service provider you choose.\nChinese lanterns symbolize wishes for a brighter, more prosperous future. The lanterns on\nthe book cover symbolize our desire to help you and your organization attain that brighter\nand more prosperous future with cloud computing.\n",
      "content_length": 1739,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "Preface\n[ 2 ]\nWho this book is for\nThis book teaches you how to architect effective and organizationally aligned cloud\ncomputing solutions by addressing cloud computing fundamentals, cloud architecture\nconsiderations, cloud technology service selection, and cloud computing security controls.\nIt is ideal for the following people:\nIT administrators, Cloud architects, or a solution architects looking to lead an\norganization through cloud adoption\nSmall business owners, managers, or consultants looking to develop and execute\na goal-oriented cloud computing strategy\nSoftware developers leading or participating in DevOps or DevSecOps processes\nNo prior knowledge of Cloud computing is needed, but a basic understanding of current\ninformation technology operations and practice is highly desired.\nWhat this book covers\n1SPMPHVF, Ground rules, covers baseline assumptions the authors took when writing this\nbook.\nPart 1: What you hear about cloud computing\n$IBQUFS\u0002\u0013, What is Cloud Computing? explains foundational definitions and explanations.\n$IBQUFS\u0002\u0014, Governance and Change Management, explains how organizational governance\nand change management affect cloud computing transitions.\nPart 2: How a cloud architect sees cloud\ncomputing\n$IBQUFS\u0002\u0015, Design Considerations, provides direction on how to think through design,\neconomic models, risk profiles, strategies, and technology decisions.\n$IBQUFS\u0002\u0016,   Business Drivers, Metrics, and Use Cases, provides key considerations when\nlooking at the economic impact of cloud solutions.\n",
      "content_length": 1532,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "Preface\n[ 3 ]\n$IBQUFS\u0002\u0017, Architecture Executive Decisions, explains how organizational executives lead the\nchanges in mindset, process, and approach in order to accelerate organizations, motivate\nteams, and increase control over cloud computing strategy, economics, and risk.   \n$IBQUFS\u0002\u0018, Architecting for Transition, discusses about interpreting the current environment\nand maintaining situational awareness during the cloud transition process.\n$IBQUFS\u0002\u0019, Baseline Cloud Architectures, explains how to use the baseline cloud architectures\nas foundational building blocks to cornerstone design ideas. \n$IBQUFS\u0002\u001a, Solution Reference Architectures, discusses about blending different deployment\nand service models to deliver on organizational goals.\nPart 3: Technology Services ` Itas not about the\ntechnology\n$IBQUFS\u0002\u001b, Cloud Environment Key Tenets and Virtualization, explains essential elements used\nto modify existing architectures, application layouts, and solution dependencies to\nmodernize deployments at lower risk.\n$IBQUFS\u0002\u0013\u0012, Cloud Clients and Key Cloud Services, discusses important cloud services and\nservice access methods.\n$IBQUFS\u0002\u0013\u0013, Operational Requirements, explains Cloud operational levers used to create\nbusiness opportunities and alternatives. It discusses standards for interoperability and\nportability related to cloud computing applications, ecosystems, and applications.\n$IBQUFS\u0002\u0013\u0014, CSP Performance, discusses how to measure, evaluate, and compare service\nproviders.\n$IBQUFS\u0002\u0013\u0015, Cloud Application Development, discusses key concepts to address when\ndeveloping for cloud-based applications.\nPart 4: Cloud Security ` itas all about the data\n$IBQUFS\u0002\u0013\u0016, Data Security, explains security planning from a data-centric point of view.\n$IBQUFS\u0002\u0013\u0017, Application Security, discusses challenges that need to be considered when\ndeveloping cloud-related applications.  \n",
      "content_length": 1880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "Preface\n[ 4 ]\n$IBQUFS\u0002\u0013\u0018, Risk Management and Business Continuity, explains how to manage both the risk\nand risk mitigation when making future state choices.\nPart 5: Capstone ` end-to-end design exercise\n$IBQUFS\u0002\u0013\u0019, Hands-On Lab 1 a Basic Cloud Design (Single Server), provides an example of a\nsmall cloud solution design.\n$IBQUFS\u0002\u0013\u001a, Hands-On Lab 2 a Advanced Cloud Design Insight, provides an example of a\nsmall-medium solution design.\n$IBQUFS\u0002\u0013\u001b, Hands-On Lab 3 a Optimizing Current State (12 Months Later), provides an\nexample of a small large solution design.\n$IBQUFS\u0002\u0014\u0012, Cloud Architecture a Lessons Learned, discusses important solution design\nlessons.\n&QJMPHVF, Sensomorphic. \nTo get the most out of this book\nThis book is designed to guide organization digital transformation and cloud transition. To\nget the most out of this guide, the main goal should be to design and build architectures\nsupporting specified business or mission use cases. The target should be the use and\naggregation of cloud architectures to deploy and securely use business and/or mission\nsoftware applications.\nConventions used\nThere are a number of text conventions used throughout this book.\n$PEF*O5FYU: Indicates code words in text, database table names, folder names, filenames,\nfile extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an\nexample: \"Please type EZO in the search box\"\n",
      "content_length": 1395,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "Preface\n[ 5 ]\nBold: Indicates a new term, an important word, or words that you see onscreen. For\nexample, words in menus or dialog boxes appear in the text like this. Here is an example:\n\"At the top of the same page in current view, there is a Text Search box.\"\nWarnings or important notes appear like this.\nTips and tricks appear like this.\nGet in touch\nFeedback from our readers is always welcome.\nGeneral feedback: Email GFFECBDL!QBDLUQVC\u0010DPN and mention the book title in the\nsubject of your message. If you have questions about any aspect of this book, please email\nus at RVFTUJPOT!QBDLUQVC\u0010DPN.\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes\ndo happen. If you have found a mistake in this book, we would be grateful if you would\nreport this to us. Please visit XXX\u0010QBDLUQVC\u0010DPN\u0011TVCNJU\u000fFSSBUB, selecting your book,\nclicking on the Errata Submission Form link, and entering the details.\nPiracy: If you come across any illegal copies of our works in any form on the Internet, we\nwould be grateful if you would provide us with the location address or website name.\nPlease contact us at DPQZSJHIU!QBDLUQVC\u0010DPN with a link to the material.\nIf you are interested in becoming an author: If there is a topic that you have expertise in\nand you are interested in either writing or contributing to a book, please visit\nBVUIPST\u0010QBDLUQVC\u0010DPN.\n",
      "content_length": 1378,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "Preface\n[ 6 ]\nReviews\nPlease leave a review. Once you have read and used this book, why not leave a review on\nthe site that you purchased it from? Potential readers can then see and use your unbiased\nopinion to make purchase decisions, we at Packt can understand what you think about our\nproducts, and our authors can see your feedback on their book. Thank you!\nFor more information about Packt, please visit QBDLUQVC\u0010DPN.\n",
      "content_length": 423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "Prologue\n140 million results are returned in 0.48 seconds when you search for cloud computing in a\nGoogle search. With that much information available, and that many conversations active\naround the globe, do we really know what cloud is? Are we confident in knowing what\ncloud can do? Can we explain why the cloud is changing everything?\nIf 10 people were asked what cloud computing is and why it is important, we would get at\nleast 12 different answers. Where is the disconnect? We know leaders want it. CFOs\nsupport it. Strategists recommend it. Technical teams request it. Users demand it. Isn't\ncloud easy? Cloud is often associated with acceleration, cost control, added flexibility,\nincreased agility, lower complexity, and rapid innovation. Cloud is never described as easy.\nIt takes an incredible amount of work and planning to be simple. CIOs are stating that\ncloud skills are a top hiring priority in 2018. What do we need to stay relevant? How do we\nkeep up with an industry that is changing every day?\nCloud computing is changing strategies and enabling innovation at every turn. Cloud is\nchanging IT economics. Cloud is blurring the lines and breaking down traditional silos.\nCloud is blending roles and redefining boundaries. Regardless of which industry we are in,\nor the position we hold, cloud computing is changing everything: how we work, how we\nplay, and how we communicate.\nGround rules\nThis book is meant as a guide to help you sort through the noise related to cloud\ncomputing. We intend to help explain what cloud really is and how it affects strategy,\neconomics, and technology simultaneously, and discuss how to stay professionally relevant\nthrough the tremendous changes occurring daily in our industry.\nOur targeted audience is anyone involved in the cloud conversation. There are many\npeople asking questions and many more trying to find and/or provide answers.\nThree of your closest provider friends are not representative of the\nmarket. Now combine the extremely high cost of failed implementations\nand incorrect strategies and you quickly realize that the old way of\nthinking, the manual tools of Excel, PowerPoint, and email, can no longer\nhelp someone trying to embrace cloud or lead any kind of digital\ntransformation within their organization.\n",
      "content_length": 2280,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "Prologue\n[ 8 ]\nIn designing this book, we've taken an agnostic view of the market. Digital transformation\nand cloud adoption have many paths. Just like any other journey, there are multiple ways\nto reach the destination, with some paths more optimal than others, depending on what is\nrequired for a successful outcome. Many books are extremely technical, detailing\ntechnology knobs and dials, configuration formulas, and technical tasks. Many books\ndiscuss various concepts for business and strategy. Our approach was to create a book that\nsimultaneously aligns strategy, technology, and economics. Fundamentally, we believe\nthese cannot be separated. Technically optimal solutions may not fit within economic goals.\nStrategy requirements may alter technical choices. Risk always needs to be offset by\neconomics. We believe our industry needs a way to reference and apply these.\nThere are thousands of service providers in the market offering services out of tens of\nthousands of locations. Many of them have thousands of products with what seems like\nendless potential combinations The combinatorial effect of options equates to trillions of\npossible solutions available in the market today. How do we begin to sort through the data?\nHow do we analyze a representative sample size to make a well informed, market-\nconscious decision?\nIn construction, it would be costly to finish half a building just to have a fatal flaw cause\nyou to tear it down. The high cost of failure is astronomical. Not to mention the costly\nperson-hours expended in the preliminary work needed to design, architect, and fund such\na project. Cost is why every major modern design process today uses computer-aided\ndesign, which is particularly the case if the marketplace has a high cost of design coupled\nwith a high-cost failure. Industries from computer chip design using Cadence to\nconstruction using Autodesk have evolved in this way. Can you imagine trying to develop\ncomputer chips using paper and pencil?\nWe see the same thing in the IT industry. IT today is all about hybrid platforms and cloud\ncomputing. Failure is very, very expensive. Going back to the construction idea mentioned\npreviously, it is very cumbersome and expensive to have finished with 40 stories of a\nskyscraper and realize it must be torn down due to a fatal flaw in the foundation.\n",
      "content_length": 2339,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "Prologue\n[ 9 ]\nToday's market is rapidly changing, with new options, pricing, locations, concepts, and\nsolutions appearing almost daily. The opportunities to identify, evaluate, and apply new\nsolutions and strategies are endless. Today, we have limited data and no way to work in\nreal time as we compare, optimize, and choose between options and strategies. The tools\nwe use are usually manual, disconnected, expensive, and filled with stop and go serial-built\nprocesses. Working with limited data in a disconnected way makes it virtually impossible\nto make good decisions as the market has changed before we can act confidently. In a\nsituation with little automation, limited data, and disconnected processes, it is impossible to\nalign strategy, technology, and economics to move forward quickly. These challenges mean\nwe need automation, connected ecosystems, and computer-aided design in our IT toolbox:\nThis book will teach a modern approach to IT solution design. In our design process,\neconomic, technical, and strategic attributes must always align. Executives have always\nneeded to balance economics, return, and risk. As executives modernize their thinking, they\nare having to blend deeper technical detail with the economic and strategic aspects.\nSolution designers and architects must also modernize their thinking by adding economics,\nrisk, and strategy with technical detail to make high-value recommendations. Product\nmanagers updating skill sets and processes require the same level of strategic and economic\nprowess as they do technical. As solutions and answers to modern IT challenges are\nassembled, they must simultaneously align strategy, economics, and technology.\n",
      "content_length": 1686,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "Prologue\n[ 10 ]\nThe examples in this book will use enterprise and mission goals to visualize, map, match,\nand compare solution design patterns modeled using modern computer-aided design\nplatforms. Hybrid is not just a term to describe cloud solutions. It is also used to describe\nthe modernization of skill sets required when architecting business, economic, technical,\nand risk strategies, new business models, and leading-edge technology solutions.\nThis book is not meant to be read front to back. Just like architecting strategy or architecting\ntechnical solutions, the journey can take different paths, with things of interest moving you\nin one direction or another. The examples in this book help the reader progress through\narchitecting interactions in compounding layers. As choices, not decisions, are made,\nscenarios, insights, comparative analysis, and outcomes change. The examples in this book\nwill show how multiple choices, and combinations of choices, can derive additional data,\npresent unique insights, and identify optimal scenarios that simultaneously satisfy risk,\neconomic, strategic, and technical requirements.\nAs discussed, architecting is done at many layers and levels. Our examples in this book will\nshow how to do the following:\nCollect accurate real-time solution design data and analytics\nLeverage automation and high-speed solution design tools\nUpdate your skill set to include business fundamentals, economics, and risk\nmanagement\nRapidly model solutions, quickly interpret insights, and gain advantage through\nsafe failures leading to success\nA final important aspect of our approach is that we are looking at cloud computing with an\narchitecting focus, not a solution design focus. It will often be stated throughout the book\nthat successful architectures, not designs, must satisfy strategic, economic, and technical\nrequirements simultaneously. Designs do not. The focus of this book is on cloud computing\nand the various architectures associated with it. There are many moving pieces and\nintertwining facets for successful cloud architectures. The design is one important part.\n",
      "content_length": 2115,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "1\nWhat is Cloud Computing?\nWe hear that the cloud simplifies, yet it makes things more complicated (at first). It saves us\nmoney, yet unusually high bills have surprised many IT leaders and executives. The cloud\nis flexible, agile, and nimble, yet many get locked into single providers with less than\noptimal architectures and with substantial migration costs to change. We also hear that the\ncloud is not as secure as our data center, even though this has been proven false time and\nagain.\nIn many ways, cloud computing reflects human nature. Everyone believes their idea is best.\nPeople sometimes blindly follow their beliefs regardless of the data. No single cloud\nprovider, cloud service, or cloud architecture is perfect. They all have things that we wish\nwere different. They have rules to follow, and they all peddle the line of being the only way\nto the truth and the promised land.\nThe cloud is not an answer for everything. It is a tool in the toolbox that has a purpose.\nWhen used appropriately, it is an incredible addition. When used incorrectly it can be\npainful, expensive, and career altering. Let's sort out what it is and what it is not.\nWe cover the formal definition later, but in essence, cloud computing is a new business model\nfor the consumption and provisioning of information technology software, infrastructures, and\nrelated services. Additionally, this chapter covers:\nCloud computing history\nCloud computing definition\nEssential characteristics of cloud computing\nCloud service models\nCloud deployment models\nSimilar technology models\nCloud washing\n",
      "content_length": 1578,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "What is Cloud Computing?\nChapter 1\n[ 12 ]\nCloud computing history\nFor our purposes, the first age of computing was the 1970s when the focus was on big\ninfrastructure. Green-screen terminals, in vogue back then, eventually evolved into\npersonal computers. Networks went from a centralized, hierarchical design to a\ndecentralized design. Decentralization moved the processing closer to the user meaning\napplications moved from thin client (processing on the server) to thick client (processing on\nthe user/client side). Green screens were tightly coupled interfaces to the data-laden\nbackend. Decentralization enabled developers to track process steps and state information\non the server side while allowing client-side computers to do much more of the processing.\nThe period was the birth of client-server architectures, which are central to today's modern\ntechnology-driven business.\nWith much of the processing moving closer to the actual user, the connectivity of the user\nbecame the main limitation. Lack of connectivity led to the second age of computing. The\n80s heralded the rise of the internet. Better connectivity between distributed computing\nsystems quickly led to the development and near ubiquity of easy-to-use, visually attractive\ncomputing devices. Businesses moved quickly in exploiting this new Internet Protocol (IP)-\nbased connectivity as local area networks expanded to globally inter-connected wide area\nnetworks. Users, however, became frustrated with poor application performance, network\nlatency, and application timeouts. Developers were again forced to place more compute\nload closer to the user. Tightly coupled centralized applications did not have the functions,\nflexibility, or the responsiveness of a well-designed well-built decentralized application.\nAdditionally, the late 80s gave way to a major shakeup in the telecommunications industry.\nThe then monopolized local exchanges were mandated to separate into independent\ncompeting companies. The competition forced faster innovation, lower costs, and higher\nlevels of reliability and service.\n",
      "content_length": 2078,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "What is Cloud Computing?\nChapter 1\n[ 13 ]\nThe following diagram depicts the various cloud computing phases:\nAs connectivity and services improved and competition continued to drive reliability up\nand costs down, the third age of computing began. The amount of data generated\nexploded. Wireless began to take off, leading to things such as widespread cell phone\nadoption and mobility solutions. Google revolutionized the internet searches by\nautomating it with MapReduce, no-SQL, and the AppEngine (an early Platform-as-a-\nService). During the 90s, the fast growth of wireless networks and the rapid adoption of\nmobile devices led to the rebirth of the thin client in the form of a browser. Better\nconnectivity and browser-based mobile application interfaces enabled much of the\ncomputing to remain on the server side with content and responses sent to the client's\nbrowser. Servers began utilizing a newer form of virtualization (IBM started a form of\nvirtualization on mainframes in 1964) improving resource utilization and changing the way\napplications are developed and deployed.\nToday, applications are loosely coupled architectures that can take advantage of modern\nelastic and scalable infrastructures. As mentioned earlier, virtualization has been around for\na while taking different forms. The real innovation came in the form of modernized billing\nsystems and economic models. The true innovation is that we can now purchase a fraction\nof a core or a fraction of a GB of RAM, consume it for a fraction of a minute and turn it off\nand not pay for it until we need it again. The real power of modern computing is to be able\nto buy what you need, when you need it, and give it back when finished. The model has led\nto entirely new business ideas, business strategies, operational models, economic models,\nand entirely new categories of business.\n",
      "content_length": 1852,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "What is Cloud Computing?\nChapter 1\n[ 14 ]\nTechnology innovations typically run in 10-year hills where we climb through a level of\nadoption and face significant challenges that level off adoption until innovation resolves\nthe current challenges, then we start to climb the next hill of adoption. The second hill is\nusually the largest increase in adoption with major game-changing innovation usually\noccurring in the third hill that restarts the clock. Situational awareness is critical for\nbusiness leaders. Which hill are you climbing? Are you early adopting with significant\nchallenges ahead? Is your company late to the game and about to be washed out by\ninnovation? Leaders must always offset risk with economics.\nLeaders face many challenges as they try to modernize their business and business models.\nFor example, an executive driven initiative to reduce costs through the adoption of cloud\nservices renders current traditional infrastructure-centric security models worthless. To\nremain relevant, enterprise security professionals must now adopt a modern data-centric\nsecurity model. Technical teams traditionally funded as an overhead cost must transform to\nbecome trusted revenue-enabling information technology partners to business leaders. To\nstay relevant, traditional technologists must now update, not only their technical skills but\nalso their non-technical skills. This updated approach includes business risk, economics,\nfinance, and strategy. Innovations such as cloud computing that are driven by economic\ninnovation are lasting and force everyone to adapt. Every known business model is affected\nby cloud computing: strategy, operations, security, economics, risk, deployment, and more.\nCloud computing, also known as IT-as-a-Service, was quickly adopted due to its ability to\ndeliver value in three significant market sectors.\nThe first, and most significant by revenue standards, is the software marketplace where it\nwas able to reduce software consumption costs, especially in the area of application and\nsoftware licensing, and reduced software application support costs. More importantly, this\nis accomplished while simultaneously improving business backend system capabilities.\nThe second marketplace was really in the application development arena. Application\ndevelopment platforms, also known as integrated development environments, were\ndelivered with the embedded support of multiple languages and frameworks. The\nPlatform-as-a-Service model exists in multiple technology environments, with greater\nflexibility. Environment flexibility opened up choice, reduced vendor lock-in fears, and also\ncreated an ability to auto-scale applications based on the number of actual users.\nThe third-most significant IT-as-a-Service marketplace was infrastructure. Infrastructure\nenabled global scale at an affordable price point. Here is where modern converged\nnetworks are used to deliver variable IT capacity pools. It also ushered in the concepts of\ninformation technology self-service, and on-demand capacity.\n",
      "content_length": 3033,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "What is Cloud Computing?\nChapter 1\n[ 15 ]\nAll these models delivered dramatic improvements in cost control, flexibility, speed to\nmarket, reliability, and resilience.\nThe following diagram depicts the various components of IT-as-a-Service:\nCloud computing definition\n\"Cloud computing is a model for enabling ubiquitous, convenient, on-demand network\naccess to a shared pool of configurable computing resources (for example, networks,\nservers, storage, applications, and services) that can be rapidly provisioned and released\nwith minimal management effort or service provider interaction.\"\na US National Institute of Standards and Technology\nThis definition is the most widely quoted and used version globally. Many countries and\nindustries have adopted it, and this is the highly recommended starting point for your\norganization's working definition for the cloud. This definition is so important that we\nshould take a few minutes to review it in detail.\nCloud computing is a model. It is not a specific technology. You cannot go and buy a cloud\ncomputer. The term is used to describe an economic and operational model for the\nprovisioning and consumption of IT infrastructure and associated services. The term can\nalso be extended to cover both business and public-sector mission models. What do these\nmodels enable? Why cloud?\n",
      "content_length": 1330,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "What is Cloud Computing?\nChapter 1\n[ 16 ]\nThey enable ubiquitous, convenient, on-demand network access to a shared pool of\nconfigurable computing resources. Universal, convenient, on-demand network access\nmeans from anywhere and at any time. The network may include the global public internet,\nbut it may also refer to a global private network. The concepts of ubiquitous, convenient,\nand on-demand take the viewpoint of the cloud service provider's intended users. Shared\npool means that the individual user or organization does not pay for all the resources in the\npool. The end user pays only for what they use when they use it. This concept is the heart\nof the cloud computing economic model. If you need to pay for the resource even when\nyou are not using it, you are not leveraging the cloud computing economic model.\nConfigurable means that the service capability can be changed essentially in real time to\nmeet a specific user's requirements.\nThat final phrase, \"...can be rapidly provisioned and released with minimal management effort or\nservice provider interaction,\" implies a high degree of automation. Cloud service providers\noperate a highly automated, services-oriented platform that requires relatively few people.\nAutomation is enabled through brutal establishment and enforcement of rigorous IT\nstandards. Automation also enables self service, so if a prospective service provider cannot\noffer their capabilities without human interaction, you should be worried.\nEssential characteristics of cloud computing\nWhen the United States National Institute of Standards and Technology (NIST) published\nthe cloud computing definition, they also defined the essential characteristics of this new\nmodel. These have come to be more important than the definition in that the characteristics\nhave helped to define and protect the marketplace against all the marketing hype that has\naccompanied the cloud.\n",
      "content_length": 1911,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "What is Cloud Computing?\nChapter 1\n[ 17 ]\nThe first characteristic of cloud computing is that it is an on-demand, typically self service\nmodel. On-demand, meaning that it can be purchased when needed, for as long as needed,\nand given back when finished. Self service refers to the consumer's ability to buy, deploy\nand shut down services without any assistance from the service provider. This speeds up\nthe process controls cost and moves control to the consumer. (Refer back to earlier\nparagraphs where we discussed the de-centralization and continual innovation of pushing\ncompute and control closer to the edge of the consumer's control and consumer-controlled\ndevices. The same applies here.)\nFrom a security perspective, this has introduced governance challenges about the\nacquisition, provisioning, use, and operation of cloud-based services. Interestingly, these\nnew services may violate existing organizational policies. By its nature, cloud computing\nmay not require procurement, provisioning, or approval from finance due to its low initial\ncost, self-service nature, and immediate deployment options. Cloud infrastructure and\nservices can be provisioned by almost anyone with a credit card, also known as shadow IT.\nFor enterprise customers, this low-entry cost, quickly deployed on-demand model may\nbecome one of the most important characteristics as it instantly wreaks havoc on\ngovernance, security, long-term cost, strategy, internal politics, and collaboration.\nThe second characteristic, broad network access, is required. Ever heard the phrase\nthe network is the cloud? Anything referred to as-a-service requires a network connection.\nHow would it be accessed, managed, operated, or utilized without some network\nconnection, typically to the internet using standard protocols that promote use by disparate\nclient platforms? Because the cloud is an always-on and always-accessible offering, users\nhave immediate access to all available resources, and assets. Think convenient access to\nwhat you want, when you need it, from any location. In theory, all that is required is\ninternet access and relevant credentials. The mobile device and smart device revolution\nhave introduced an interesting dynamic into the cloud conversation within many\norganizations. These devices are often able to access relevant resources that users require;\nhowever, compatibility issues, ineffective security controls and non-standardization of\nplatforms and software systems have made the first adoption climb more difficult for some\nenterprises.\n",
      "content_length": 2541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "What is Cloud Computing?\nChapter 1\n[ 18 ]\nThe third characteristic, resource pooling, is the characteristic that, in essence, lies at the\nheart of all that is good about cloud computing. Combining many smaller compute\nresources into farms or pools that can serve many consumers simultaneously enables\ndynamic resource allocation and re-allocation, cost predictability, IT resource control, and\nhigher rates of infrastructure utilization. Utilization and consumption patterns directly\naffect cost. Resource pooling enables different physical and virtual resources to be allocated\nand re-allocated according to consumer demand. As mentioned earlier, the true cloud\ninnovation was economic, allowing us to stop billing and give back the resource when\nfinished. More often than not, traditional, non-cloud traditional deployments see low-\nutilization rates for their resources, typically between 10 and 20%. Cloud deployments from\npools used across multiple clients or customer groups can see as high as 80 to 90%\nutilization (100% is not ideal in most cases). Resources can automatically scale and adjust to\ndynamic needs, workload or resource requirements. Cloud service providers or cloud\nsolution providers (CSPs) typically have scores of resources available, from hundreds to\nthousands of servers, network devices, and applications, enabling them to quickly and\neconomically accommodate, prioritize and implement the varied size, and complexities\neach client presents.\nThe fourth essential characteristic of cloud computing centers on elasticity, the ability to\ndynamically match the need. Product and service capabilities are developed, acquired,\npriced, and provisioned elastically, enabling rapid response to continuously changing user\ndemand. To the consumer, capabilities often appear unlimited and easily deployed in any\nquantity at any time. Because cloud services utilize a consumption-based pay-per-use\nmodel, you only pay for what you use. As mentioned earlier, cloud innovation and\nadoption are being driven mainly by economics that affects strategy. For cyclical loads,\napplications with intermittent use, seasonal or event-type business cloud eliminates the\nneed to pay for 100% of a physical server (CAPEX) when only 5% is used 2% of the time\n(OPEX). Think of selling thousands of tickets to an Olympic event. Leading up to the ticket\nrelease date, little to no computing resources are needed; however, when the tickets go on\nsale, they may need to accommodate 100,000 users in the space of 30 minutes-40 minutes.\nThis is where rapid elasticity and cloud computing can be beneficial. Enterprises no longer\nrequire traditional IT deployments with substantial capital expenditure up front (CAPEX)\nto support the temporary project load.\n",
      "content_length": 2749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "What is Cloud Computing?\nChapter 1\n[ 19 ]\nThe final key characteristic mentioned here is that the cloud is a constantly measured\nservice. Cloud computing natively offers a unique and important component that\ntraditional IT deployments have struggled to providecmeasurement and control of\nresource consumption and utilization. As mentioned often, billing was the big innovation.\nCloud resource consumption needed to be measured and billed for accurately. Once that\nwas possible, the true power of the cloud, which included the ability to shut it off, was\nrealized. The capability enabled automated reporting, monitoring, and alerting which\nprovided much-needed transparency between the provider and the client. Like a metered\nelectricity service or cell phone data usage, consumers have transparent and immediate\naccess to usage data enabling immediate behavior change if needed. Itemized billing\nprovides transparent trendable data providing insight that may lead to needed change.\nProactive organizations can now utilize this well measured, transparent, granular,\ntrendable data to charge departments or business units for their actual consumption. IT,\nproduct development, and finance can now move toward operating collaboratively as a\nrevenue-driving team that can quantify, qualify, and justify exact usage and costs per\ndepartment, by business function, per leader, and so oncsomething that was incredibly\ndifficult to achieve in traditional IT environments.\nThe following diagram is a graphical representation of the five essential characteristics of\ncloud computing:\n",
      "content_length": 1574,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "What is Cloud Computing?\nChapter 1\n[ 20 ]\nAs a side note: people have been utilizing the cloud for years without realizing it. It is not a\nnew thing, but it has just started to port over into more popular arenas. Let's look at\ninternet access as an example.\nCharacteristic 1: Based on the first characteristic mentioned earlier, how many\npeople dig up the street to put in connectivity when they want to access the\ninternet? None. We pay for it as a service that allows us to use it when we\nchoose.\nCharacteristic 2: Do we need a network to access the internet? Of course.\nCharacteristic 3: Do we have dedicated switches, routers, SONET ring, and so on\nin our living space? No, those resources are pooled by the service provider and\nshared with all the clients in the area or region.\nCharacteristic 4: Can we utilize more if we need it? Absolutely. We only use\nwhat we need with the ability to scale all the way up to the maximum\nperformance for which we are willing to pay. If more is needed, we call and\nchange what performance level we pay for to match up with the changing need.\nCharacteristic 5: Are we paying as we go? Is our service metered and measured?\nDefinitely yes. If we choose to stop paying for the service, the service is shut off.\nWe get a bill every month and often have a portal that we can log in to that\ndetails what we pay for, what we use, performance details, uptime, downtime,\nand so on.\n",
      "content_length": 1413,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "What is Cloud Computing?\nChapter 1\n[ 21 ]\nCloud computing operational models\nThere are many paths to the cloud. Each path is grouped based on how the services are\noffered, deployed, and consumed. The cloud is not a technology. A cloud layer does not\nexist. Each path to the cloud is a response to a requirement or set of needs based on the\nconsumer's current situation, desired future state, available skills, and resources, as well as\ntolerance for risk. Cloud products and services often establish reusable and reoccurring\narchitectural patterns (building blocks) used for designing, building, and managing\napplications and infrastructure.\nThere are primarily three cloud service models: Internet-as-a-Service (IaaS), Platform-as-a-\nService (PaaS), and Software-as-a-Service (SaaS). Deployed as needed, all three models\nrequire network connections to change resource pools that are measured in great detail,\ndynamically. However, each consumption model differs in its approach to a technical\nsolution, economics, complexity risk, and level of acceleration. Deployment models also\ndiffer in that they could be public/shared, private/dedicated, community, and hybrid. Each\nmodel is unique in how it addresses organizational risk tolerance, economic models, and\nmanagement preferences.\nOften the motivator for a move to the cloud is some event that triggers probing questions.\nEvents could be anything from a magazine article, a blog post to a security breach,\ninfrastructure downtime, a complaint about responsiveness, difficulty managing to the\ndesired level of service, or staff/leadership change. Questions can be typically reduced\ndown to three Es: Expectations, Economics, and Execution. As an example, someone\nexpects more delivered work with smaller budgets and less time, or project execution\nunexpectedly fails due to budget and staff constraints.\nAs questions get asked, and solutions considered, strategy details, economics, and\ntechnology must align. Solutions that are technically perfect may be too expensive. Low-\ncost solutions may not match up to chosen strategies going forward. In all cases, economics\nneed to balance or offset chosen risk level. For example, very inexpensive self-managed\npublic cloud servers may not match up to the desired level of isolation and security\nrequired for transactional database servers.\n",
      "content_length": 2339,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "What is Cloud Computing?\nChapter 1\n[ 22 ]\nNext we discuss ways to think through the three primary models available today. How do\nwe recognize situations in which a cloud model should be a consideration? What are the\ncharacteristics of each model? What are the benefits? The following diagram is an overview\nof the three main service models:\nCloud service models\nThe different cloud service models are described here in the following sections.\nIaaS ` background\nAcross the industry, hardware has been largely ignored for a very long time. Servers were\nnot sexy. There was no glory for servers. Servers were just a support for the more important\napplications. Applications got all the credit for solving business challenges. Applications\nwere the things that users interacted with directly. Servers got stuck in dark closets,\nforgotten and neglected until a problem occurred.\nBecause servers received no glory, very little to no maintenance and no budget for\npatching, upgrading, and so on, many servers are now well beyond their service life and\nprone to failure. Incredible amounts of money will be spent over the next several years\nrewriting applications, developing new applications, migrating legacy applications, and\nupdating to new cloud-ready functions needed to replace old applications and neglected\nhardware currently stuck in old closets and worn out in-house data centers.\n",
      "content_length": 1384,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "What is Cloud Computing?\nChapter 1\n[ 23 ]\nIaaS gave many the opportunity to upgrade, refresh infrastructure, and move from\nexcessive capital spending to a monthly pay-as-you-go incremental spend. The shift enables\nstrategy changes, go-to-market changes, differences in software development, and changes\nin handling IT workloads. Think of what hundreds of servers can do in one hour versus one\nserver for hundreds of hours.\nIaaS ` things to consider\nIaaS is often deployed on-demand in small increments (cores, RAM, storage, network) with\nbilling occurring in small increments of time. Instead of spending the capital (CAPEX) for a\nlarge four or eight-core server (which is the smallest currently available from some\nmanufacturers), a right-sized virtual server can be acquired and deployed as a service,\nmatching infrastructure size to cost and immediate need. This flexibility allows for\ninfrastructure to be quickly matched to business strategies and economic constraints.\nIaaS can include many of the infrastructure components included in traditional\ndeployments. Firewalls can be virtual or physical. Compute and storage can be deployed\nacross many different styles and platforms. Each service provider has their unique mix of\ntechnology and services. Ultimately the goal when using IaaS is to forget about\ninfrastructure management and details and acquire the mix of services needed, when\nneeded, solving the requirements at that time.\nIaaS  was one of the first cloud models available and has seen significant adoption in nearly\nall sectors. With IaaS, the user does not manage or control the infrastructure directly, only\nthe software and functions loaded onto it (that is operating system, application). There are\ndifferent types to choose from with varying levels of management and monitoring available\nfor the underlying hardware, virtualization layers, firewalls, SANs, switches, routers,\nnetwork interface cards (NIC), and related service level agreements (SLAs). The \ncontrolled risk and lower economic entry points for IaaS are attractive to new cloud\nadopters as well as savvy veterans. Controllable cost and controllable risk provide extra\nincentive to those trying to modernize through the adoption of the cloud.\nThe delivery of on-demand capacity is typically handled via self-service online customer\nportals. The portal provides complete visibility and control of the IaaS environment. Self-\nservice portals enable and automate functions for adds, moves, changes, managing, and\nreporting without engaging and waiting for other resources internally or within the\nprovider. IaaS can have many different consumption models matching various OPEX and\nCAPEX requirements. When using IaaS, there is no need to invest capital up front based on\ncompute and storage resources forecasts. IaaS enables infrastructure to be purchased in\nincrements, as needed, to match utilization.\n",
      "content_length": 2888,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "What is Cloud Computing?\nChapter 1\n[ 24 ]\nFor organizations, IaaS usage metering provides a higher level of detail used to trend\nutilization and chargeback specific departments or functions based on actual utilization.\nDetailed measurements and reporting also allow for instant, and in some cases, automatic,\nscaling up, and down based on dynamic need requirements. Resource flexibility is\nparticularly useful when there are significant spikes, dips, or cyclical loads for\ninfrastructure.\nA few examples of current IaaS providers are Amazon Web Services, Microsoft Azure, and\nGoogle. They offer many different styles of compute, storage, and network, as well as many\nsupporting solution services. They each offer several different economic models to match\nup to SLAs, OPEX and CAPEX requirements, risk and deployment options.\nSaaS ` background\nAs many small and medium-sized organizations looked for additional ways to control cost,\nmodernize strategy, and consume on-demand solutions, software licensing became a very\ncomplicated issue. An example of this is Oracle, a company that was a bit late to the cloud\nlicensing game. New server configurations were much more substantial with more sockets,\nmore cores, and more RAM. Even with no change in utilization or software configuration,\nOracle client charges increased to over a million dollars due to new server sizing. This\naffected strategy, economics, and eventually technical decisions on how to move forward in\nthe face of shattered budgets and ROI calculations.\nMany organizations lack the skills or resources to create custom software applications.\nFreeware and opensource software helped some organizations, but they still required skills\nsets and significant adjustment for adoption. Software providers are starting looking for\nways to offer online cloud-based solutions at lower cost and universal access. These new\nmodels would center around new licensing models tied to multiple users, a certain level of\naccess, and SLAs rather than the size of the software infrastructure deployment.\nWith SaaS, the subscriber uses the provider's centralized application deployed using cloud\ninfrastructure. SaaS enables access from any approved client device, browser, or custom\ninterface. The user/subscriber does not have access to the underlying infrastructure,\napplication code, or individual application attributes except for a set of named user-specific\napplication configuration settings.\n",
      "content_length": 2445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "What is Cloud Computing?\nChapter 1\n[ 25 ]\nIn the SaaS space, some applications have stabilized/normalized meaning they are widely\nadopted and have significant competition and innovation driving licensing costs\ndownward, for example, office suites, collaboration software, and communications\nsoftware. Software-as-a-Service providers offer a complete software application to\ncustomers using a license-based model that accesses the application on-demand via a self-\nservice interface.\nSaaS ` things to consider\nUsing SaaS, organizations have potentially limitless possibilities for running applications\nthat may not have been otherwise possible given the limitations of their corporate systems,\ninfrastructure, or resources. If the right middleware and associated components are\ndeployed, SaaS can present massive incentives and benefits. Organizations can quickly\nrealize benefits from scalability, flexibility, and on-demand self-service capabilities.\nCustomer adoption accelerates as access to data and applications can be from virtually\nanywhere, at any time with internet access. Additional benefits include:\nCost control, cost reduction\nLicensing or support becomes a built-in component for the provider and the\nsubscriber benefits from economies of scale\nThe purchasing of up-front bulk licensing and the associated capital expenditure\nis removed and replaced by demand-based pay-as-you-go licensing models\nUser-based internal support requirements reduce significantly as the software\ncloud service provider can typically handle more of the support at scale\nEase of use and limited administration\nAutomatic updates and patch management\nImproved security\nStandardization and compatibility \nGlobal accessibility\nNotable providers of SaaS include Google, Microsoft, Oracle, Salesforce, and SAP.\n",
      "content_length": 1797,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "What is Cloud Computing?\nChapter 1\n[ 26 ]\nPaaS ` background\nPaaS takes both IaaS and SaaS and adds yet another twist on trying to solve the problem.\nAs described earlier, people are trying to control costs, eliminate large major cash outlays,\naccelerate, modernize strategies, and move to only paying for what is needed, when it is\nneeded. IaaS helped but still required a lot of people, skills, and money to support the\napplications. Based on our direct research, software required between 8x and 32x the\nannual cost of the server annually in management, maintenance, monitoring, and support.\nA $6,000 server written down over a 3-year use cycle would cost between $16,000 and\n$64,000 each year for software support. The cost was dependent on the specific software\nand organizational efficiency. These operational changes meant that new infrastructure and\nsoftware models were required to keep businesses innovating and moving forward.\nThe next challenge was that the as-a-service model was not available with every software\npackage. Some software was just not adaptable to modern cloud models. A complicating\nissue was that, for most companies, only about 15%-20% of software was off-the-shelf. Most\nof it was custom developed, homegrown, and built for specific functions and purposes\nwithin each business. Nearly every company still had to develop proprietary applications,\nmiddleware, services, connectors, workflows, and more. Each of those projects required\ndifferent programming languages with different frameworks and libraries. How can things\naccelerate? How can costs be controlled?\nInterestingly, people realized that the combinations of languages, libraries, and frameworks\nused were often the same or very similar. Consumers needed the ability to quickly build\nand deploy applications coded with provider-supported programming languages, services,\nlibraries, and tools. The end user did not want to manage or control the underlying cloud\ninfrastructure, but they did need to control the deployed application's configuration\nsettings. CSPs responded by integrating all of the needed components and subsystems into\na solution stack that could now be offered as a service or rented as-needed. This new PaaS\nenabled faster development at a lower cost. The environment was now ready to use,\nmanaged, and monitored, enabling developers to be productive immediately using the\nlatest components available. This has led to many other variations where raw materials are\nintegrated into environments that enable the building and assembling of new creations\nquickly and cost-effectively.\n",
      "content_length": 2589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "What is Cloud Computing?\nChapter 1\n[ 27 ]\nPaaS ` things to consider\nCloud PaaS has revolutionized software development and the means through which it is\ndelivered to customers and users. Market entry barriers have been reduced dramatically by\nlower cost, accelerating time to market, and promoting innovative cultures within many\norganizations.\nAs PaaS providers are considered, the languages and frameworks supported are key. A\nprovider that supports multiple relevant languages and frameworks can help avoid\nproductivity pitfalls later. Developers need to write code in their preferred language that\nmeets specified design requirements. Recent advances include options for open source\ndevelopment stacks and many new infrastructure deployment styles including OpenStack\ninfrastructure, various containerization engines, and serverless (FaaS) options. PaaS\nproviders that support multiple languages and deployment options reduce vendor lock-in\nand interoperability issues as applications grow and deployment locations change.\nApplications are never static. They are continually changing, updating, and growing. Being\nable to deploy and move the application across different hosting environments is also a key\nPaaS benefit. Supporting multiple hosting environments helps the developer or\nadministrator easily migrate the application if required. With this option, PaaS can also be\nused for contingency operations and business continuity to ensure continued availability. It\nis important to consider the final environment as platforms used by early users to test for\nfunctionality. These environments transition to a run environment at some point. The final\nenvironment may not be the same as originally intended as many things change\nthroughout the process and testing. Multiple deployment options are an important\nconsideration when picking a platform provider.\nMany platform providers started with the idea of adding value by assembling platforms.\nMake the platform proprietary with their unique workflows, combinations, components\nand create a sort of lock-in mentality. Providers wanted clients to use only their specific\nplatform and direction. The goal was to make things very sticky with limited ability to\ntransition between provider platforms. Recent changes have added much-needed flexibility\nmatching developer needs and requirements. To stay relevant, platform providers needed\nto respond or lose the developers and their communities to more flexible environments and\nopen source options.\n",
      "content_length": 2501,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "What is Cloud Computing?\nChapter 1\n[ 28 ]\nThe application programming interfaces (APIs) are required for nearly every form of\nsoftware in our space today with RESTful being elevated to the de-facto standard in most\ncases. A service provider always offers specified APIs or integration. Developers could run\ntheir application in various environments based on common and standard API structures.\nThis ensured consistency and quality for customers and users. PaaS pushed forward\ninfrastructure concepts like auto-scaling where software could now take the responsibility\nof scale up and scale down and manipulating the infrastructure through APIs, as needed.\nThis would help accommodate cyclical, less predictable demand patterns, seasonal\nbusiness, and event-driven activities. Mother's Day would bring down Hallmark's online\ncard servers every year until they were able to implement auto-scaling. Before auto-scaling,\nHallmark would have to build and engineer infrastructure for a guesstimated utilization\nlevel. With auto-scaling, the platform allocates resources and assigns them these\napplications, as required. This capability is a key driver for any seasonal organizations that\nexperience spikes and drops in usage.\nWhen thinking through platform providers, look for flexibility and future migration\noptions. Look for the right combinations of services and support with expertise in areas\nrelevant to project needs and direction. Look for providers that not only provide the\nplatform but also offer the other versions of the cloud as well. Where your project starts is\nnot where it stays. Plan to move and change. It may not change often, but change happens.\nPlan for it up front.\nNotable PaaS providers include Microsoft, Lightning, and Google.\nOther cloud service models\nYou have probably heard of many other X-as-a-Service offerings such as Storage-as-a-\nService, Desktop-as-a-Service, Network-as-a-Service, Backend-as-a-Service, Function-as-a-\nService. These other models are merely subsets or aggregations of SaaS, IaaS, or PaaS.\nCategorizing them into the three standard models simplifies any cloud conversation you\nmay have.\nCloud deployment models\nWe have discussed the three standard cloud service models. The service model defines the\nwhat. What is unique about each model? What are they trying to solve? What are their\nstrengths and weaknesses? Each of the discussed service models adheres to the five\ncharacteristics mentioned, possibly adhering in different ways. Within each of the service\nmodels, there may also be multiple ways to enable and deploy the service. The service\nmodel is the what, the deployment model is the how.\n",
      "content_length": 2645,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "What is Cloud Computing?\nChapter 1\n[ 29 ]\nMany cloud services are straightforward to comprehend. For example, network as\nmentioned earlier. Most do not realize that the network was one of the very first types of\nIaaS, therefore, one of the original types of cloud. True, there are many types of services,\nand with that, an even greater number of ways to refer to them. In this section, we\nintroduce the deployment models and some of the jargon. The discussion also addresses\nhow the different deployment types are referenced, marketing names, labels, and currently\nused buzz words.\nThis section also demystifies some of the marketing hype which makes it easier to quiet the\nnoise when participating in the often jargon-filled conversations. How is this bare metal\ndifferent to a dedicated cloud? What is a public cloud? What is a private cloud and how is\nprivate different to dedicated? Is it different? Is private on-premises still considered as the\ncloud? Is a private cloud from a service provider also the cloud? If it is called a cloud, it\nmust be a cloud?\nPublic\nPublic is the typical IaaS-compute deployment model most people think of when referring\nto the cloud. A public cloud service provider offers IT resources as-a-service and, as part of\nthe service, is responsible for building, monitoring, and maintaining physical data centers\nand IT resources that are for dynamic public consumption. This IT service environment is\nshared among many customers which normally reduces costs for each customer. By\nleveraging economies of scale, the CSP enables higher average utilization of resources\nthrough the extensive use of virtualization, workload binding, offsetting clients workload\npatterns, and performance tiers.\nThe general public uses a public cloud infrastructure. The infrastructure may be owned,\nmanaged, and operated by a business, academic, or government organization, or some\ncombination. The infrastructure is always on service provider premises as they have taken\nownership of operations and maintenance. Amazon is a good example. The business\nstarted off by selling books. It then started to sell excess server and storage capacity to the\ngeneral public. The infrastructure remained on-premise at Amazon locations.\nA public cloud can fall into two sub-types within IaaS, self managed or fully managed. Both\nof these sub-types are discussed in greater detail later in this chapter. A public cloud is\nhighly scalable, immediately deployed, portal driven, and can be parked or turned off\nwhen not in use.\n",
      "content_length": 2523,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "What is Cloud Computing?\nChapter 1\n[ 30 ]\nPublic cloud benefits include:\nEase of use and inexpensive setup, low cost of entry to the cloud\nStreamlined and easy-to-provision resources via a self-serve portal\nScaled to meet customer needs \nNo wasted resources because customers pay only for what they consume\nBasic security services included\nPublic cloud considerations are:\nHow are noisy neighbors handled?\nDoes security line up with my requirements?\nIs there any network access or storage limitations?\nCost of access or data transfer in/out?\nPortability? Grow into other instance types and service types?\nWhat other services connect to it?\nWhat is the price/performance metric?\nProviders often mentioned in this space include Amazon, Microsoft, and Google, among\nothers.\nPrivate and dedicated\nThere are a lot of marketing, jargon and buzz words that come along with the cloud.\nCompanies are fighting hard to create separation from the pack. Sounding unique and\ndifferent was one way to try and differentiate. What is the difference between a dedicated\nand a private cloud? What is a virtual private cloud? Is a virtual private cloud different to a\nprivate cloud? Do these different versions adhere to the five characteristics of the cloud?\nMany factors drive interest in single-tenant infrastructure. Dedicated and private both, by\ndefinition, are single-tenant environments with the infrastructure only accessible by a\nsingle company or client. The difference ultimately comes down to economic model and\naccess.\n1SJWBUF\u0002DMPVE\nA private cloud is typically an on-client premise solution with the infrastructure leased or\npurchased by the company/entity using it. These environments can also be deployed within\na service provider data center utilizing collocation services. This would still be considered\nan on-premises solution as the collocation space is just another leased location for the\n",
      "content_length": 1892,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "What is Cloud Computing?\nChapter 1\n[ 31 ]\ninfrastructure owner. A private cloud is typically managed by the organization it serves;\nhowever, outsourcing the general management of this to trusted third parties may also be\nan option. A private cloud is typically available only to the entity or organization, its\nemployees, contractors, and selected third parties. The private cloud is also sometimes\nreferred to as the internal or organizational cloud.\nThe factors driving the use of infrastructure may include legal limitations, trust, and\nsecurity regulations. Private cloud benefits include more control over data, the underlying\nsystems, and applications, ownership, retention of governance controls; and assurance over\ndata location. Private clouds are typically more popular among large, complex\norganizations that have legacy systems and heavily customized environments.\nAdditionally, where significant technology investment has been made, it may be more\nfinancially viable to utilize and incorporate these investments within a private cloud\nenvironment than to discard or retire such devices.\nIs a private cloud really a cloud? It has cloud in the name. Having cloud in the name was\nnot one of the five characteristics of the cloud. It is interesting to debate, you decide.\nCompare a private cloud to the five characteristics and come up with an answer.\n",
      "content_length": 1361,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "What is Cloud Computing?\nChapter 1\n[ 32 ]\n%FEJDBUFE\u0002DMPVE\nA dedicated cloud is very similar to a private cloud. It is also a single-tenant solution.\nOwnership and access differ. In a dedicated cloud, ownership of the infrastructure shifts to\nthe service provider. The infrastructure is housed within the provider's data center. A\ndedicated environment is for use by the single tenant. Network, compute, and storage are\ndedicated to the single tenant.\nThe economic model for dedicated solutions is usually a combination of non-recurring cost\n(NRC) which is a one time fee up front, and monthly recurring cost (MRC), which is a\nmonthly payment paid over a term (number of months or years). Dedicated solutions\nenable a shift from all capital upfront models (CAPEX) to smaller payments over a longer\nperiod (OpEx). Management and operations can continue to be in-house, outsourced, or a\ncombination of both.\nIs a dedicated cloud really a cloud service? It also has cloud in the name. This is also an\ninteresting one to debate, you decide. Compare a private cloud to the five characteristics\nand come up with an answer for your conversations. When sorting out the answer, look at\nthe economics. The cloud is an economic innovation. Ultimately, can you turn it off and\ngive it back? Does billing stop? Do you stop paying for it when you shutdown?\n7JSUVBM\u0002QSJWBUF\u0002DMPVE\nAs with many things in our industry, the lines often get blurred (marketing may have\nsomething to do with it). Dedicated is an isolated environment deployed within the\nprovider's data center. A virtual private cloud (VPC) is a variation of a dedicated cloud. A \nvirtual private cloud combines concepts from a public and a dedicated cloud. VPC takes the\nconcept of shared infrastructure and the economies of scale for servers and storage then\ncombines it with an isolated network.\nThe very high cost of a dedicated cloud and the network challenges of noisy neighbors in a\npublic cloud led service providers to the virtual private approach. VPC is the blending of\nstrengths while trying to control some of the weaknesses. The name drives the blurred line\nin this service. A private cloud is typically deployed on client-owned infrastructure as an\non-client-premise solution. A VPC is deployed on the service provider-owned\ninfrastructure. This service would be more appropriately named a virtual dedicated cloud.\nSome providers have changed the name to eliminate some of the confusion within their\nproduct and solution sets.\n",
      "content_length": 2487,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "What is Cloud Computing?\nChapter 1\n[ 33 ]\n$PNNVOJUZ\nIn a community cloud, an IT infrastructure is provisioned for use by a specified community\nof end users. The community participants are from organizations that have shared concerns\nand governance requirements. Typically, these requirements link to mission, security,\npolicy, or regulatory compliance. It can be owned, managed, or operated by a community\nmember, a third party, or a combination. Community clouds may exist on or off premises.\nA community cloud provides most of the same benefits as a public cloud deployment while\nproviding heightened levels of privacy, security, and regulatory compliance.\n)ZCSJE\nA hybrid cloud is any solution that combines a cloud model with any other cloud or non-\ncloud deployment model. Most organizations tend to gravitate to the hybrid models as no\nsingle model or deployment type matches up to the multiple applications and services\nneeded to support a business. Many applications cannot migrate readily to new services.\nApplication dependencies may introduce additional risk for migrated applications. There is\nrarely a situation where everything is forklift moved all at one time from current state to\nfuture state. Hybrid is the typical path to the cloud for currently deployed applications and\ninfrastructure.\nIn a hybrid IT environment, private clouds, public clouds, community clouds, traditional\ndata centers, and services from service providers can be integrated and interconnected.\nApplications and services can then be deployed to and consumed from the most\nappropriate combination of services and environments.\nKey benefits of the hybrid model are retention of ownership and oversight for critical tasks,\nreuse of earlier technology investments, tighter control over critical business components\nand systems and more cost-effective options for non-critical business functions. Cloud\nbursting and disaster recovery options can also be enhanced by using hybrid cloud\ndeployments.\n",
      "content_length": 1983,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "What is Cloud Computing?\nChapter 1\n[ 34 ]\nThe basic understanding of the cloud deployment models are shown here:\nOther delivery models\nThe cloud, and technology in general, has a notion of fashion to it. Some things are in vogue\nwhile others fade from favor quickly. As cloud conversations happen, it is vital to see ahead\nof the curve and keep in mind overall direction. For decades, a consistent direction has been\nto place more power in the hands of the end user/consumer. Our cell phones today have\nmore compute power and run more applications than many desktop computers sold a few\nyears ago. Think forward a bit to IoT. Significant compute power and data are at our\nfingertips. Connected cars are currently built with nearly 40 processors, almost 100 sensors\nsending out 25 GB of data per hour. Connected cars are described as rolling data centers.\n",
      "content_length": 855,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "What is Cloud Computing?\nChapter 1\n[ 35 ]\nAs technology continues to innovate and progress at incredible speeds, it is essential to\nraise your awareness of immerging trends. It is also important to quickly distinguish tech\nfashion from tech innovation. Real innovation always has an economic driver that is\nsustainable. Starbucks coffee did not invent coffee. Starbucks was when the first-time coffee\nand culture became inseparable. The iPhone was not the first mobile device; however, it\nwas the first time a mobile device connected many of the most important facets of our\nhome, work, and play lives. The cloud was not the first deployment of virtualization. The\ncloud is the first innovation that has directly connected the concepts of technology,\nstrategy, and economics forever changing the way we build, deploy and consume\ntechnology and services.\nThe shortlist here includes some very innovative ideas that build on and extend the core\nconcepts of cloud computing. These are a few things to watch as they climb the two hills of\ninnovation. More on that later in this book:\nGrid computing: Distributed and parallel computing capability where a virtual\ncomputer is composed of a cluster of networked and loosely coupled individual\ncomputers which act in concert to perform very large tasks.\nFog computing: A distributed computing model that provides IT services closer\nto the fog client. Sources are near-end user edge devices. Fog computing can\nhandle data at the network level, on smart devices, and the end-user client side\ninstead of sending data to a remote location for processing.\nDew computing: Dew computing is positioned at the ground level for the cloud\nand fog computing. When compared to fog computing, which is designed to\nsupport IoT applications that are sensitive to network latency and require real-\ntime and dynamic network reconfigurability, this variant pushes the computing\napplications, data, and low-level services to the end users and away from\ncentralized virtual nodes.\nEdge computing: Edge computing extends cloud computing by pushing the\nprocessing, applications, and data as far away from centralized resources as\npossible. Moving work to the edge also means that devices may not always be\nconnected to the internet (that is, mobile, laptop, tablet). This means that\nprocessing would also need a high level of redundancy with content highly\ndistributed.\nIoT: IoT is connecting the physical world with the logical one. Sensing and\nprocessing technology are being placed where needed when needed. Cloud \ncomputing is rapidly morphing to help catch, process, and utilize the vast\namount of data already being generated by IoT initiatives and projects.\n",
      "content_length": 2684,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "What is Cloud Computing?\nChapter 1\n[ 36 ]\nAI, neural networks, and machine learning: These concepts have been pulled\ntogether because they are hard to separate and sometimes they are used \ninterchangeably even though there are distinct differences. AI has been around\nfor decades, it is not new. However, cloud computing has removed many of the\nbarriers that were slowing innovation; 300 computers for one hour can process a\nstaggering amount of data, connect it, correlate it, learn from it and apply it.\nCloud computing innovation has truly helped this field leap forward recently.\nCloud washing\nCloud washing is a term used to refer to the often deceptive attempt to rebrand an existing\nproduct or service by associating the buzzword cloud with it. Within a financial construct,\nthere is a parallel that describes the practice of inflating financial results for a company's\ncloud business by redefining existing services and products as cloud services. A typical\nexample of this is referring to access to any application or service over the internet through\na browser as cloud computing, just because you are receiving the service over the internet.\nAnother example is the traditional application service provider (ASP) model where a third\nparty offers individuals, and companies access over the internet to applications and\nservices that would normally have been located in their own personal or enterprise\ncomputers. This is often marketed as a SaaS, but there are many significant differences\nbetween the two models. ASP is a software delivery method with a revenue model that is\ndisconnected from the software itself. At its core, these are single-instance, single-tenant\nlegacy software deployments. The revenue model is like renting a server with an\napplication installed on it. This approach failed in the marketplace because it lacks\nscalability for the vendor, too much customization is required, and there is a single\ncustomer for the instantiation. There is also no organic aggregation of data, and no network\neffect data available for collection and aggregation.\nSaaS, on the other hand, is an all-inclusive business architecture that is a value delivery\nmethod. Its built-in multi-tenancy design allows for shared resources and shared\ninfrastructure. SaaS is scalable and offers true economies of scale to the service provider.\nThis approach reduces overall costs, operational complexities, and customization. Multi-\ntenancy can also be leveraged to improve customer service and retention, reduce sales\ncycles, accelerate revenue, gain competitive advantage, and even directly monetize\nadditional services.\n",
      "content_length": 2623,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "What is Cloud Computing?\nChapter 1\n[ 37 ]\nManaged service arrangements are also sometimes referred to as cloud hosting. The\ndifference here is that the day-to-day functions are outsourced to a particular vendor to\nrealize an increase in efficiency around processes associated with data center operations.\nWhen doing this, the client also pays for all the capital investment (either up front or\nembedded in the recurring fee) and commits to regular payments over a minimum term.\nThese payments are not driven by use but are a calculation related to total operational and\ncustomization costs over the minimum term, financial interest rates and a minimum profit\nfor the service provider. In all cloud service models, the cloud service provider bears all\ncapital cost and offers the same standard service to all marketplace customer. Payment is\nrelated directly to actual customer use, and there is no minimum term commitment.\nCloud computing taxonomy\nThe cloud computing taxonomy was initially developed by the United States National\nInstitute of Standards and Technology (NIST) as a tool for standardizing conversations\naround cloud architectures. Since then, this basic model has been enhanced by the\ncommunity and broadly adopted to discuss basic concepts. The major taxonomy\ncomponents are described here:\n",
      "content_length": 1307,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "What is Cloud Computing?\nChapter 1\n[ 38 ]\nThe service consumer is the entity (enterprise or end user) that actually uses the cloud\nservice. Users will normally have multiple programming interfaces. These interfaces\npresent themselves like any normal application and the user does not need to understand\nany cloud computing platform details. User interfaces can also provide administrative\nfunctions like virtual machine or storage management.\nThe cloud service provider (CSP) creates, manages, and delivers information technology\nservices to the service consumer. Provider tasks vary based on the service model:\nFor SaaS, the provider installs, manages, and maintains all software. Service\nconsumers only have access to the application.\nFor PaaS, the provider manages and provides a standardized application\ndevelopment environment. This is typically in the form of a development\nlanguage framework.\nFor IaaS, the provider maintains and operates the facilities, hardware, virtual\nmachines, storage, and network associated with the delivery of any information\ntechnology service. The service consumer, however, is responsible for service\ndesign, operations, and delivery.\nCritical to the service provider's operations is the management layer. This layer meters and\nmonitors the use of all services. It also provisions and deprovisions services based on user\ndemand and service provider capacity. Management also includes billing, capacity\nplanning, SLA management, and reporting. Security is applied across all aspects of the\nservice provider's operations.\nThe service developer creates, publishes, and monitors cloud services. Typically, these\nconsist of line-of-business applications delivered directly to end users. During service\ncreation, analytics is used for remote debugging and service testing. When the service is\npublished, analytics is also used to monitor service performance.\nStandards and taxonomies will affect cloud use case scenarios in four different ways:\nWithin each type of cloud service\nAcross the different types of cloud services\nBetween the enterprise and the cloud \nWithin the private cloud of an enterprise\n",
      "content_length": 2134,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "What is Cloud Computing?\nChapter 1\n[ 39 ]\nWithin each type of cloud service (IaaS, PaaS, or SaaS), open standards help organizations\navoid vendor lock-in by giving users the freedom to move to other cloud service providers\nwithout major application or operational modifications. Standards within an enterprise are\nnormally driven by interoperability, auditability, security, and management requirements.\nSummary\nAs you move forward in cloud conversations, please keep in mind the five characteristics of\nthe cloud. These characteristics will help you stay focused on aligning technology,\neconomics, and strategy. The cloud's big innovation was economic, not technical. Strategy\nchanging economics are driving rapid transformation and digitization. There are many\ndifferent services, different economic and deployment models along with many different\nstrategies to use them. The cloud is another tool in the toolbox. It is not the answer for\neverything, but it is quickly becoming the foundation for everything.\nThe next chapter starts the conversation on governance and change management. You may\nthink that a cloud solutions architect's success depends on the technology chosen. That\nthinking couldn't be further from the truth. While cloud computing is foundational to\ndigital transformation, successful cloud computing solutions must be built on top of an\neffective change management and IT governance foundation.\n",
      "content_length": 1417,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "2\nGovernance and Change\nManagement\nSolution adoption requires cultural change. Cultural change requires relevant, insightful\ndata; well-planned governance, and relentless change management. Change is difficult in\nmany ways and for many reasons. Without governance and change management, driving\nadoption for new solutions can be exhausting, unpopular, expensive, and slow.\nGovernance and change management are inseparable but not interchangeable. One is not\nsynonymous with the other. Governance and change management are tightly coupled and\ndependent on one another. Changes in one can certainly affect the other. What are they and\nhow are they different?\nGovernance address the things that need to be accounted for as change is implemented.\nGovernance is the operating agreement for how changes will take place for the things\ninvolved. Governance definitions and operating rules may define organizational structures,\ndecision rights, workflows, processes, stakeholders, authorization points, and toll gates.\nThe goal of governance planning is to talk through how things operate today versus how\nthings will operate during and after the change. Well-planned governance ideally creates a\ntarget workflow that aligns and optimizes the use of business entity resources with the\ngoals and objectives of the business.\nChange management focuses on the people and how people will be assisted through\nchanges being implemented. Changes to our norms can be real challenges. It is difficult to\nveer away from comfortable modes of normal operation, accepted and adopted workflows,\nteam structures, roles and responsibilities, and known rules of engagement. Change\nmanagement helps people transition through changes by providing things such as\nmessaging and communication plans, engaging support, interactive training, and coaching,\nco-ownership in successes and wins.\n",
      "content_length": 1858,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "Governance and Change Management\nChapter 2\n[ 41 ]\nIn this chapter, we will cover the following topics:\nIT governance\nChange management\nIT service management\nArchitecting cloud computing solution catalogs\nIT governance\nGovernance addresses things needed to implement change. Typically, this process will start\nwith a series of questions that determine outcomes, responsibilities, and process. The\ndesired outcome is always first. Without knowing the desired outcomes, it is impossible to\ndetermine who is involved and what is needed to get there.\nThree questions to help start the IT governance process are shown in the following\ndiagram:\n(KIWTG\u0003\u0014\u001d\u0003'aGEVKXG\u0003IQXGTPCPEG\u0003UVCTVU\u0003YKVJ\u0003CFFTGUUKPI\u0003VJTGG\u0003SWGUVKQPU\n",
      "content_length": 707,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "Governance and Change Management\nChapter 2\n[ 42 ]\nOrganizational leadership and management must articulate desired outcomes, who is\naccountable and responsible for these outcomes, escalation criteria and triggers to progress,\nthe process for progression, and what metrics and counter metrics to be used. The delivery\nof desired outcomes must be continually monitored and evaluated for direction toward\ndesired outcomes, adoption levels, impact to metrics, and affects to counter metrics. Is the \nchange providing the necessary transparency? Is it following expected timelines? Are\nstakeholders and decision makers adjusting accordingly? Scorecards are extremely helpful\nto compare, visualize, and guide as governance is monitored through change.\nCounter metrics \nA second value to watch that may signal unexpected results or behaviors\naway from the actual change. For example, new storage is implemented\nwith much faster throughput. The server is responding much quicker and\ntaking more load. The metric being measured is disk i/o and data\ntransferred statistics for the server. The countermeasures may include\ndramatic changes in load balancing metrics because one server is getting\nall the traffic now due to first responder algorithms being used. Higher\nutilization on network ports, high CPU or RAM utilization may also be\nthe metrics to watch to see if the change is causing adverse behaviors in\nother places not directly affected by the change.\n",
      "content_length": 1451,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "Governance and Change Management\nChapter 2\n[ 43 ]\nThe next diagram shows an example of an IT governance scorecard with notational\noutcomes and metrics:\n(KIWTG\u0003\u0015\u001d\u00030QVCVKQPCN\u0003+6\u0003IQXGTPCPEG\u0003UEQTGECTF\nIT governance does not have a one size fits all approach. Change comes in all shapes and\nsizes with as many or more unique challenges to navigate. IT governance is unique because\nthe same change with the same desired outcome requires different governance when the\nchange is deployed into different operation and deployment models.\nComplete scope and strategy are things to consider when building IT governance plans and\nprocesses. It is important to have a clear understanding of what business processes and\nwhich applications may be affected. This analysis must also consider cost, benefit, risk, and\nshortcomings that may lead to unexpected outcomes as operation models and deployment\nmodels are chosen. Different models will require different governance plans and processes.\nAs mentioned previously, if there are changes to governance, there will also be changes in\nchange management.\n",
      "content_length": 1085,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "Governance and Change Management\nChapter 2\n[ 44 ]\nImplementation strategy\nTo illustrate, there are three main categorical ways to consume IT products and services.\nThe first is in-house. In-house refers to a typical enterprise implementation, where the\norganization pays for ownership of all applicable resources. The enterprise also employs\nthe required operations staff to operate the deployed solutions. In this model, the enterprise\nhas complete and total control of IT governance.\nThe second, managed service provider (MSP), is an arrangement where the enterprise\ncontracts with an outside service provider to provide and/or manage IT resources. In this\nmodel, the enterprise retains some level of IT governance control by negotiating and\nenforcing a binding contract known as a service level agreement. The enterprise also funds\nall the MSP incurred costs plus a mutually agreed to profit. This option can be cheaper than\nthe traditional data center due to economies of scale. If the MSP is more efficient and/or\nmore automated, the same desired outcome may be less expensive but will require some\nchange to governance as some responsibilities will be shifted from in-house enterprise\nresources to the service provider.\nThe third option uses a cloud service provider (CSP). The CSP funds all hardware and\nsoftware. The CSP also pays the salaries and benefits for the required operations staff. The\nrequired IT function is consumed by the enterprise completely as-a-service. In this model,\nthe CSP has complete and total control of IT governance.\nThe following diagram here illustrates a few differences between the outsourcing models,\nMSP, and CSP. When choosing IT strategies, associated governance plans must also adapt\nto service models, deployment models, and implementation options:\n",
      "content_length": 1794,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "Governance and Change Management\nChapter 2\n[ 45 ]\nChange management\nCultural/organizational/policy friction occurs through changes to established norms, not\nbecause of the technology being used. When adopting cloud computing, companies must\nfocus on the benefits from business model changes. Many changes are needed as businesses\nsearch for ways to support new growth, modernized product strategies, and constant\neconomic pressures. The transition to cloud computing can be complex for many different\nreasons. Some of the most common include:\nNew knowledge required\nDynamic nature of infrastructure\nNew cloud service management tools\nRapid innovation and fast-moving market\nNew management models\nDistributed ownership\nContinuous optimization\nDuring cloud transitions, organizations are not only changing from a technology\nperspective, but they are also changing their mindset and culture, simultaneously. The\nfollowing is a list of typical domain changes associated with moving from traditional\nmodels to the cloud. With each of these, there is also a simultaneous change to\norganizational culture, required skill sets, and individual mindsets:\n",
      "content_length": 1145,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "Governance and Change Management\nChapter 2\n[ 46 ]\nIdeally, before cloud services begin to be utilized, focused change management strategies\nshould be implemented. Change management plans should quickly focus on raising\nawareness, understanding the change, accepting the changes, and the commitment of the\norganization to the expected outcomes and the people associated with them. The focused\nchange management and communications plans must relentlessly focus on providing data\nand messaging that support organizational change, modernizes mindset, and skill sets,\nhighlights benefits, and raises awareness. The organization should review metrics and\ncounter metrics to gauge culture change rates and determine if all communication channels\nare being leveraged with correct messaging and correct timing.\nCloud computing is transformational. The hardest part of any transformational strategy is\nchanging the hearts and minds of the people in the organization. It is critical for leaders to\nbe keenly aware of what to look for and to have metrics and counter metrics in place to\nshow changes in organizational culture, transformation rates and progress toward desired\noutcomes. A key question to ask is if each team member knows what the vision is; that is,\ndo they see the whole elephant or do they only see the part they are connected to? Do they\nunderstand how their part is connected to the whole? Refer to the following photograph:\n(KIWTG\u0003\u0016\u001d\u0003&QGU\u0003VJG\u0003UVCa\u0003UGG\u0003VJG\u0003GNGRJCPV\u0003CPF\u0003JQY\u0003VJGKT\u0003RCTV\u0003KU\u0003EQPPGEVGF\u0001\n",
      "content_length": 1507,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "Governance and Change Management\nChapter 2\n[ 47 ]\nIndicative questions may include:\nDo they believe that this transformation is achievable?\nAre customers and service providers on the same page?\nDo they think they have sufficient resources to meet the schedule?\nAre the staff the right kind, in the right place, doing the right work?\nDo the various groups that are working together understand and accept the\ninterfaces and hand-offs?\nAre accepted and consistent vocabularies and definitions being used across the\nenterprise?\nThink of the IT industry as a tribe. The industry has its language, terminology, and\nstandard phrases. The cloud has a bit of its dialect within the IT tribe. There are common\npractices, rituals, and super-secret handshakes among tribe members. As with any other\ntribe or culture, standard practices and dialect enable quick identification of outsiders,\nnewbies, and those that may be a threat to the tribe. Is change likely to be adopted when the\npath forward lacks awareness for current practices or the messaging is communicated used\na completely different vernacular? Not possible.\nAs cloud and things-as-a-service increase in adoption, change management, and\ncommunication plans are paving the way. A deep dive into effective communication\nplanning and IT governance strategy could fill several volumes. Some guidelines for\neffective plans and strategies are:\nCommunicate using the tribe vernacular\nUse industry standard terms and definitions\nProvide authoritative sources if possible\nProvide data and insight with each communication\nReinforce benefits of change at every opportunity\nEnsure leaders are familiar with tribe language, customs, and rituals\nCompanies should also explicitly link demand-side governance with supply-side\ngovernance to improve efficiency and effectiveness as detailed in the following diagram.\nMetrics should be evaluated to create, modify, or delete as part of an enterprise continuous\nimprovement process:\n",
      "content_length": 1964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "Governance and Change Management\nChapter 2\n[ 48 ]\n(KIWTG\u0003\u0017\u001d\u0003.KPMCIG\u0003DGVYGGP\u0003+6\u0003IQXGTPCPEG\u0003CPF\u0003GbEKGPV\u0003KORNGOGPVCVKQP\nIT service management\nCloud computing solutions are implemented using service management frameworks. In\ndesigning any solution, the cloud solution architect must account for how well the\norganization is prepared to manage, operate, and continually improve that service. The\nmost widely adopted industry standard process for efficiently and effectively providing IT\nservice management (ITSM) is based on the Information Technology Infrastructure\nLibrary (ITIL). ITIL structures ITSM into four domains:\nIT infrastructure: The technology components directly related to an IT service,\nfor example, a Red Hat Enterprise Linux (RHEL) OS instance running on a\nserver.\nSupporting services: The underlying infrastructure required to operate the\ncustomer-facing IT services, for example, the DNS server required to reach the\nRHEL instance by use of the hostname. Supporting services could be referred to\nas IT-internal services.\nIT service: The services requested by customers. Each IT service is implemented\nusing the corresponding IT infrastructure. The IT service, therefore, complements\na set of IT infrastructure components by adding service definitions such as SLA\ninformation and cost. In our example, the RHEL instance could be offered as a\ngold and silver service. Gold might include 24 x 7 support while silver offers 8 x\n5 support at a lower cost.\n",
      "content_length": 1466,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "Governance and Change Management\nChapter 2\n[ 49 ]\nITSM framework: Standards and processes that orchestrate all the activities\nrequired to deploy an IT service. This is not a collection of infrastructure\ncomponents but rather the framework used for the deployment, operation, and\ndecommissioning of IT services. The ITSM framework ties the IT infrastructure,\nsupporting services and IT services together and provides the needed operational\nfunctionality. IT services are presented to potential customers, they, in turn, need\nto be able to order them:\n(KIWTG\u0003\u0018\u001d\u0003\u0003+6\u0003UGTXKEG\u0003OCPCIGOGPV\u0003HTCOGYQTM\n",
      "content_length": 593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "Governance and Change Management\nChapter 2\n[ 50 ]\nThe ITSM framework should not be confused with the deployment of an IT infrastructure\nrequired by a service. The focus is on orchestrating all the activities required to deploy an\nIT service as opposed to a collection of infrastructure components. If the customer requests\nthe RHEL gold service, it is not only about deploying the OS image using Red Hat Satellite,\nbut also about modeling the service in the Configuration Management Database (CMDB),\nconfiguring the event and impact management, making sure that the corresponding OLA\nare rolled-up into the promised SLA, and reporting it in a service view on a customer-\nfacing portal. The ITSM framework ties everything together into a coherent service offering.\nIn addition to implementing ITSM, the following best practices should also be practiced:\nEnforcement of brutal standardization across the enterprise: A small number of\nnon-optional constraints is often the most effective means of achieving agile\ngovernance. Jeff Bezos at Amazon famously mandated in 2002 that \"All [Amazon]\nteams will henceforth expose their data and functionality through service interfaces\" that\ncould eventually be exposed to a public-facing market. The form and style of the\ninterfaces were left to the teams to determine, but critically, anyone who didn't\nfollow the edict was subject to termination. Vigorous enforcement of a\nlightweight set of requirements is a recurring theme in successful modern IT\nmanagement. If this is not possible to globally enforce due to organizational\nconsiderations, it becomes even more important to demonstrate success via well-\nscoped pilot projects that can showcase the new model.\nIT standardization: Standardization is critical to gaining operational efficiency,\nreducing overall cost, and reducing the time required to deliver new capabilities.\nAcross all commercial industries, average standardization savings are on the\norder of a 2/3 reduction of servers. Increasing the utilization percentage realizes\nthese. Most of the economic value in this type of transition is accomplished\nthrough standardization in the software development platform and support\npersonnel efficiencies enabled by standardized operational processes, as shown:\n",
      "content_length": 2260,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "Governance and Change Management\nChapter 2\n[ 51 ]\n(KIWTG\u0003\u0019\u001d\u0003#EJKGXKPI\u0003C\u0003UJCTGF\u0003UGTXKEGU\u0003GPXKTQPOGPV\nIT change management standardization (processes and tools): As part of the\nadoption of ITIL, it is critical that the organization has positive knowledge of its\nIT assets. This is typically the first transformational step towards ITIL. Once a\nbaseline of assets is collected, configuration control and processes must be put in\nplace and religiously followed. This is needed to efficiently handle incident\nmanagement (restore services, analyze incident types and trends, and improve\ncommunications), problem management (root cause analysis, document known\nerrors), and change management (reduce unexpected outages, track approvals\nfor compliance, new service support). Variations in these processes are\nproblematic and are easily seen by the customer. The goal is to provide\ntransparent, efficient, and consistent processes and services.\n",
      "content_length": 936,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "Governance and Change Management\nChapter 2\n[ 52 ]\nTransition from customer-mandated to a customer-focused model: In a\ncustomer-mandated model, the solution design is primarily driven by a targeted\nend user's requirements and dictates. This approach can lead to wide technical\nvariations across operationally similar solutions. In a customer-focused\nenvironment, the architects design solutions that can be used to meet a broad\nmarketplace of users. These offerings are focused on the target audience for the\nprovided services. For example, Amazon Web Services provides only limited\npatched versions of windows and Linux/Unix. This allows them to provide these\nservice solutions at a very inexpensive rate on-demand to customers.\nLeverage all applicable shared services: The interoperability and efficiencies\ngained through cloud-based offerings are depicted in the following diagram. As\nmore and more components become standardized the level of effort to provide\nthem decreases as well as the overall system complexity. As the enterprise and\nthe cloud offering matures, the enterprise performs more and more of the work\nperformed and eventually, the business process-specific applications can be\ndeployed as specialized IT service extensions.\nUse common standardized delivery patterns: Ideally, all cloud computing\nsolutions should be designed as real-time aggregations of existing cloud services.\nThe following diagram depicts the concept of providing a common set of\nstandardized delivery patterns. In this diagram, the far left depicts some\nenterprise-wide patterns or sets of software that could be used to support specific\nlines of business. The diagram also includes the notion of a common\ndevelopment environment. This common development environment can be a\nPaaS or a standardized company environment that uses specified software\ncomponents for targeting specific deployment patterns. By matching a certified\nand accredited development environment to a specific certified and accredited\noperational environment, an organization can rapidly develop and deploy\nindustry-accredited solutions quickly.\n",
      "content_length": 2106,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "Governance and Change Management\nChapter 2\n[ 53 ]\n(KIWTG\u0003\u001b\u001d\u0003%QOOQP\u0003UVCPFCTFK\\GF\u0003FGNKXGT[\u0003RCVVGTPU\nArchitecting cloud computing solution\ncatalogs\nTo align with ITIL recommendations, enterprise IT services should be a customer-facing\nrepresentation of the available technology services. An organization's IT services catalog\nshould, in turn, provide all the information a customer should need to review, select, and\nacquire cloud solutions. Although both top-down approaches (from the business view) and\nbottom-up approaches (based on a technology and an available technology services view)\nhave been used, industry best practices indicate that the bottom-up approach is far more\nefficient, because it is based on something tangiblecthe technology or technology services\navailable to an organization.\n",
      "content_length": 799,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "Governance and Change Management\nChapter 2\n[ 54 ]\nA mapping of technology services to a cloud solution requires standards regarding both\narchitecture and delivery. Without this standardization, a mapping across the IT external\nand internal boundary is not possible, (for example, if IT doesn't maintain a standardized\nbuild and delivery method for RHEL, the customer would receive a different build every\ntime a RHEL gold service is ordered). A failure by the organization to set and enforce\nstandards across the entire technology space, both regarding architecture and delivery\nwould be contrary to the exact purpose of IT service management.\nThe challenge of the cloud computing solution task is primarily in the structuring and\norganization of available services to allow for a standardized, efficient, and reproducible\nmapping. This is referred to as service design in ITIL. During service design phases, all\naspects of the solution must address new and evolving business needs. Business aspects\noften considered for cloud solutions include:\nBusiness process and the definition of the functional service needs, for example,\ntelesales, invoicing, orders, credit checking\nThe service (or solution) being delivered to the end user or business by the\nservice provider, for example, email, billing\nService level agreements that specify the level, scope, and quality of service to be\nprovided\nInfrastructurecall of the IT equipment necessary to deliver the service to end\nusers which includes servers, networks, switches, client devices, and so on\nThe environmental requirements needed to secure, and operate the infrastructure\nData is necessary to provide the service and deliver the information required to\nexecute the business processes\nThe application needed to manipulate or modify the data and provide the\nfunctional requirement of the business processes\nThe operational level agreements (OLAs) and contracts, as well as any \nfoundational agreements needed to deliver the SLA-dictated quality of service\nSupport services necessary to execute all required operations\nAll processes or procedures needed in the execution and operation of the\ndelivered service\nInternal support teams that provide level 2 and level 3 support to end users and\nany of the service components\nExternal third party suppliers which are necessary to provide level 2 and level 3\nsupport to end users or service components\n",
      "content_length": 2397,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "Governance and Change Management\nChapter 2\n[ 55 ]\nThe cloud is not the answer for everything. Components must first be considered in\nisolation. If the cloud presents advantages for the component in isolation, the component\nshould then be considered among its other relationships, interactions, and dependencies to\nother components and services. If advantages remain the same or increase, the cloud\nbecomes a viable option. This approach will result in an effective, well-researched, and\neasily communicated solution that aligns technical, strategic, and economic business needs\nwithin acceptable risk profiles.\nFollowing ITIL recommendations, each solution component should be documented using\nconfiguration management artifacts across nine categories:\nDescription: Describes the individual solution components by the use of\nstandardized diagrams and a short textual description.\nLife cycle: Defines the organization's specific lifecycle for solution components.\nThat life cycle is different (lagging) from the vendor life cycle because vendor\nproducts may need to be customized to meet internal standards.\nProvisioning: Describes the technical provisioning of solution components, the\nprovisioning process required by solution components is covered in the artifact\nprocesses and runbooks.\nConfiguration management: Document a solution component's requirements\nwith regards to the implementation in a configuration management framework.\nThis includes both CMDB templates as well as configuration management\nreports.\nSecurity: The technical and process aspects of a solution component's security\nrequirements that must be met during the deployment and operating phase.\nMonitoring: Functionality required for monitoring the operational status of the\nsolution components.\nProcesses and runbooks: One-time activities related to a solution component,\nsuch as provisioning. The runbook focuses on normal activities along with any\nanticipated abnormal activities associated with the specific solution component,\nsuch as house cleaning, backup/restore, auditing, failover. Both the process\ndescriptions and the runbook should cover the complete lifecycle of a solution\ncomponent, including provisioning, operations, and decommission.\nFinancials: Technology-based cost for Capital Expenditure (CAPEX) and\nOperational Expenditure (OPEX). This does not include any IT service-oriented\ncost items such as system administrator support, as they can vary depending on\nthe associated IT service offering.\n",
      "content_length": 2489,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "Governance and Change Management\nChapter 2\n[ 56 ]\nBlueprints: Detailed descriptions of the technical implementation of the solution\ncomponent. For the engineering teams, the blueprints define how a component is\nto be developed for customers, they include detailed technical information about\nthe component.\nThe solution itself should be documented in the following 12 categories:\nDescription: Describes the individual solutions by the use of standardized\ndiagrams and a short textual description.\nNon-functional requirements (NFR): Solution requirements defined in a\nstandardized and unambiguous fashion. There are two categories of NFRs:\nCapabilities, which describe the features and functions provided\nby the infrastructure components\nQualities of service requirements that expand the infrastructure \ncapabilities to cover additional end-user expectations\nRecovery Point Objective (RPO) and Recovery Time Objective\n(RTO) are referenced for failover within a data center as well as\nfailover between data centers\nRequired solution components: Solution components required by a solution.\nLife cycle: The solution life cycle is mainly based on the life cycle of the\nindividual solution components required by the solution.\nProvisioning: Provisioning of a solution consists of the provisioning of\nindividual solution components. However, some solutions might require the\nprior provisioning of other solutions.\nConfiguration management: Document a solution's requirements with regards\nto the implementation in a configuration management framework. This includes\nboth CMDB templates as well as configuration management reports.\nSecurity: The security of a solution should be built into the individual solution\ncomponents and not added on a solution level. Most solutions don't require any\nadditional security considerations.\nMonitoring: The monitoring of the overall solution is essentially a collection of\nthe individual solution components monitoring functionality. If applicable, the\nsolution level monitoring should define how events from the different solution\ncomponents are correlated and de-duped. These definitions must correspond the\nscenarios outlined in the resiliency assessment artifacts.\n",
      "content_length": 2198,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "Governance and Change Management\nChapter 2\n[ 57 ]\nResiliency assessment: The description of all possible technical failure scenarios\n(operational errors are out-of-scope). This includes a description of the failure,\nhow the failure is detected by the monitoring components and what remediation\nis possible. The impact of the failure is then captured regarding QoS type NFR,\nmostly downtime and data loss. The worst QoS NFR of all scenarios defines the\nQoS NFR for the solution as a whole.\nProcess and runbooks: A collection of the corresponding artifacts on solution\ncomponent level. Because of the mapping of solutions to IT services, there are IT\nservices-specific process steps which are placed as a wrapper around the\ncomponent processes.\nFinancials: Technology-based cost for CAPEX and OPEX for each component\nused by the solution.\nBlueprints: This artifact lists the blueprints associated with the solution. There is\nno solution-specific blueprint for a solution that consists of solution components\nand doesn't provide technical functionality outside of the solution components.\nIt is essential that the ITSM framework supports the mapping of IT services to solutions\nand maintains the relationship between solution components, solutions, and delivery\npatterns. This can be done in the following manner:\nCustomer orders an IT service\nIT service is mapped to a solution\nSolution components and composite solution components required for a solution\nare identified (solutions can't be deployed as a unit. Solution components are\nwhat is deployed)\nSolution components are deployed as individual components\nSolutions are re-constructed from solution components and mapped to IT\nservices\nSummary\nA cloud computing solution can only be successful if it is built on top of an effective IT\ngovernance and change management foundation. This chapter expanded that critical point\nby addressing cloud implementation strategy options and IT service management details. It\nalso delved into solution catalogs through which service providers present and explain the\navailable technology services.\n",
      "content_length": 2087,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "3\nDesign Considerations\nCloud computing is not a technology; it's an economic innovation. There are many ways to\nthink through design, economic models, risk profiles, strategies, and technology decisions.\nThe section will talk through some ways to navigate the thought process, how to eliminate\nsome of the noise surrounding the cloud, and how to stay focused on what the challenges\nare and how solutions get mapped to those business challenges.\nWe will cover the following topics in this chapter:\nFoundation for design d the thought process\nFoundation for design d the cloud is economic, not technical\nFoundation for design d the plans\nUnderstanding business strategy and goals\nFoundation for design ` the thought process\nThe cloud was supposed to be simple. It was supposed to be fast. It was supposed to solve\nmany, if not all, of the problems. As people started digging into cloud designs, they began\nto discover that things were not always as simple, things were not always less expensive,\nand things were not always performing as expected. In many cases, they experienced\nsignificant challenges with social adoption. The design did not match expectations for\nvarious reasons. Throughout this book, we talk about how perceived requirements are\nmerely starting objectives that accelerate towards requirements with the gathering of\nadditional insight and data. Ultimately, successful designs must simultaneously harmonize\neconomics, strategy, technology, and risk. This balance leaves risk and economics offset at\nequilibrium.\n",
      "content_length": 1530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "Design Considerations\nChapter 3\n[ 59 ]\nCloud computing is one of the rare things in our industry where nearly everyone is\nimpacted or affected by the change. Adoption of the change is the big challenge. People\nmust embrace the transition while they adapt to process and work method changes.\nWithout mental and emotional buy-in, projects can stall, exceed budgets, and, potentially,\nfail. The only path to acceptance and cultural change is through data.\nAs an example, consider a developer who is required to utilize a different cloud provider\nfor all projects going forward. That developer must change processes and working methods\nand, potentially, go through a significant learning curve for new systems, applications, and\ntools. This example is not about the transition to the cloud, but about transitioning between\nclouds. How well does this go over if the developer enjoyed working with the previous\nsupplier? What if that developer was very efficient in using the toolsets and could quickly\nnavigate current processes? What if the developer was with presented data showing that\nthe new provider could provide machines at 30% lower cost? The developer then can\nacquire and utilize three times the number of servers for the same budget. Consequently,\nwhat if productivity then increased tenfold when more compute resources are combined\nwith integrated toolsets and automated processes? Data helps drive adoption in all cases.\nAs we begin to think through design considerations, a consistent thought process is\nrequired. Consistent, methodical, process-oriented thinking will help accomplish several\nthings, such as:\nEliminating the noise\nEnabling quick navigation through the complexity\nMaintaining focus on constraints and objectives\nAllowing fast and accurate interpretation of relevant insight\nAccurately identifying optimal solutions satisfying constraints\nQuickly identifying opportunities to optimize strategy\n",
      "content_length": 1920,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "Design Considerations\nChapter 3\n[ 60 ]\nFoundation for design ` the cloud is\neconomic, not technical\nContrary to what most believe, the cloud is not a technological innovation; it is an economic\none. Although the cloud is based on virtualization technology, virtualization is not a new\ntechnology. Virtualization has been around for more than 50 years, starting with IBM\nmainframes in the early 60s, namely the CP-40. For decades, we have been trying to match\nbetter hardware and system utilization to the task or workload we are working to complete.\nVirtualization started with scientists and mathematicians from both IBM labs and MIT\ntrying to perform complex calculations. They were trying to get more work done and, at the\ntime, one task was limited to one system. IBM came up with the idea of virtualizing\nmemory, which creates separate instances and completing more tasks could take place in\nthe same amount of time. The birth of virtualization happened around 1963, with the first\ncommercially available virtualized system going to market around 1967.\nThis book often discusses the fact that strategy, technology, and economics must align for\nsuccessful design. Cloud computing and virtualization are not technical innovations. They\nare not strategic innovations, as the strategy of utilizing computing to its fullest extent, at its\nlowest possible cost, has been around as long as or longer than virtualization. The cloud is\ntruly an economic innovation. The cloud was not a technical problem; it was a billing\nproblem. The problem was that people could not find a way to accurately bill for a fraction\nof a processor or a fraction of RAM for a fraction of the time; a second, a minute, or so forth.\nThe real innovation came when an instance of compute could be consumed for a period of\ntime at a specified cost, with the ability to shut it off and give it back and only be billed for\nresources and time used (everyone considers Amazon Web Services as the early IaaS pay-\nas-you-go model). It is the ability to consume expensive compute on a pay-as-you-go model\n(OPEX versus CAPEX). It eliminated the need for massive capital up front and extended the\ngrowing movement of as-a-service models.\n",
      "content_length": 2200,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "Design Considerations\nChapter 3\n[ 61 ]\nMuch time and effort are spent aligning strategy, economics, and technology early in the\nbook, because the cloud requires a new skill set, a new thought process, and a new\napproach to design. Successful architectures must simultaneously solve for strategic,\neconomic, and technical requirements. A technically perfect design that is too expensive\nequates to poor design. A technically perfect design that is economically feasible may solve\nfor the wrong strategy, also equating to poor design. All three must simultaneously resolve\nbefore the design can be viewed as successful. Because of this, cloud architects require a\nnew thought process: a process of reflection that systematically navigates in layers utilizing\ndiscriminating attributes and characteristics rather than service names and marketed\nfunctions. Cloud architects need an updated skill set that includes economics and risk\nstrategy, along with their technical prowess. Successful cloud architects must think more\nlike selling CFOs than technical geniuses. Successful cloud architects need to be aware of\nthe associated risks to the business if proposed changes are implemented. What is the\nimpact on the business economically, short-term and long-term? What cultural impact will\nthe changes have in the business, business unit, or division? Will personnel be affected?\nDoes the company structure need to change? As you can see, technical information is no\nlonger enough for the cloud architect. Cloud architects must be as comfortable in strategy\nconversations as they are in a technical one. Business finance and economics training are\nequally as important as, if not more important than, technical training.\nBecause the cloud is an economic innovation, and not a technical one, we quickly see that\ntechnical information will be pushed later towards the end of the thought process. In many\ncases, technical information becomes the tiebreaker, rather than the requirement. If you\nstart with a good set of non-negotiable, and work through basic ideas and their economic\nimpact, you can quickly see what fits strategically and what doesn't. If there are too many\nsolutions, you can change requirements, potentially optimize strategy, revisit economic\nrequirements, and so on. No one is saying that technical information is not required or\nvaluable. The idea is to use technical detail to fine-tune, optimize, and perfect; think of a\nmodern robot-driven, laser-guided surgical scalpel, as opposed to a medieval broadsword.\nThe cloud architect thought process is as shown in the following diagram:\n",
      "content_length": 2600,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "Design Considerations\nChapter 3\n[ 62 ]\nEvery building project requires a solid foundation. Cloud design is no different. It is costly\nand difficult to change directions part-way through. Think of a high-rise building. It is\ntough to be twenty floors into construction and find there was a fatal flaw in the\nfoundation, requiring the whole project to be torn down and started over. In cloud design,\nseveral elements create the base for a successful design. Many modern day designers begin\nwith basics, such as storage, compute, or perhaps the network. They gradually get deeper\nand deeper technically, never knowing if they are headed in the wrong direction or not,\nunless they get told no by someone else who owns strategy or the project budget. This\nframework changes that dynamic. Technical elements come later in the process. This\nupdated cloud architect thought process begins with the base of things that are truly non-\nnegotiable. Non-negotiable constraints include things such as legal requirements,\ngeography, sector and industry-specific requirements, project goals, strategic elements, and\nso on. These limitations form the basis on which everything else is built. If any of these\nelements can change, they are not true requirements and they certainly cannot be labeled\nnon-negotiable. Once an exact set of non-negotiable are set, we can then look at additional\ndetails that will help define overall success factors, including basics such as business\ndrivers, strategy, value prop, economic models, and corporate/industry preferences. Every\ndesign will follow the same thought process, beginning with non-negotiable constraints\nworking upward through each layer as data and insight satisfy the objectives and\nconstraints of the previous tier.\nEvery situation, scenario, design, and component will follow the same thought process and\nthe same line of questioning:\nWhat are the non-negotiable constraints and characteristics?\nWhat are the economic attributes and their impact?\nHow does this affect strategy?\nWhat is the discriminating technical attributes and characteristics?\nIs there an abnormal or excessive risk associated and does it affect economics?\n",
      "content_length": 2165,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "Design Considerations\nChapter 3\n[ 63 ]\nFoundation for design ` the plans\nThere are no pre-built plans. Architects and designers are the ones tasked with developing\nthe plans. Buildings cannot be built without a set of plans. The perimeter boundaries are\nsurveyed. The site plan is created. The dimensions of the building are laid out. All of this is\nestablishing scope and size for the architect or designer, who must stay within those\nconfines. Anything outside of those boundaries will be unacceptable and labeled as poor\ndesign.\nCloud architecture and design operate in much the same way. We must establish a set of\nboundaries to work within. We must identify what is acceptable and what is not. All of our\nquestionings must lead us toward a clear understanding of what leads to success and what\nleads to failure. It is just like building a house; if I put in a five-car garage, but I don't have\nroom for a kitchen, that would be a poor design. I did not account for all requirements and,\nmost likely, missed one that was non-negotiable. The same is true for cloud computing. If\nI'm not considering the correct requirements, my design will fail. So, the question becomes:\nHow do I identify the right requirements?\nWe must discover the correct requirements through progressive questioning. We must start\nwith questions that help us identify wants and needs. We must identify what is non-\nnegotiable. What is the foundation? What is the base? What are the things that, no matter\nwhat changes, those answers are always the same? They will not change based on better\npricing or something else suddenly becoming more interesting because of the latest\nmagazine article. The base foundation elements are truly non-negotiable. They are based on\nwhat you know at this time and on the current data you have. They outline what needs to\nhappen as a result of your design creating our base layer and non-negotiable foundation.\nWe have now drawn the boundaries of our property before we start laying out the\nbuilding. We know where our limits are.\nNon-negotiable must always include a combination of economic, strategic, and technical\nelements. All successful designs must have these elements. Therefore, they must also be\nincluded in the design foundation. For the foundation to be correct, these non-negotiables\nmust harmonize. A balance must be met within these limits and boundaries for the design\nto proceed.\n",
      "content_length": 2403,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "Design Considerations\nChapter 3\n[ 64 ]\nMany factors need to be considered when navigating towards a successful design.\nSuccessful design requires boundaries. It requires an understanding of where the no lines\nare. By trying to find where unacceptable meets acceptable, we are then able to focus on\nwhat does work rather than chasing something that was never going to work from the start.\nIt is a bit like some are essential and non-negotiable. These things are set, they cannot be\nchanged. For example, geography. Geography is typically non-negotiable. Services get\ndeployed where they are needed. If services are needed in California, they are not likely to\nbe moved to Singapore because the price is a bit more favorable. Non-negotiable can\nchange with every solution and every design scenario. The point is to uncover what is the\nfoundation for the design. Find the things that absolutely cannot change and go from there.\nThe following are some of the more common non-negotiable attributes within design:\nEnterprise size: While the size of an enterprise can positively influence certain\nfactors of design, the enterprise size itself does not dictate design. For example,\nthere is no minimum company size required to utilize the cloud. Enterprise size\nmay affect design factors, such as scale, economics, risk, distribution, service mix,\nand so on. Each of these factors must be looked at from a strategic, economic, and\ntechnical point of view to make. A Microsoft study showed that the vast majority\nof organizations of all sizes use both Software-as-a-Service (SaaS) and hosted \ninfrastructure services. Both SaaS and hosted infrastructure services are used\nmost by organizations with less than 100 employees. Smaller companies are also\nmore likely to use Platform-as-a-Service (PaaS).\nRelevant industry sectors: Different industries have different requirements,\ndifferent levels of compliance, and different appetites for risk. Government and\neducation industries lead in the use of SaaS with a Microsoft study showing more\nthan 60% of organizations reporting active use. While information technology is\nthe ubiquitous horizontal layer underlying all industry sector verticals, security\nrequirements strongly influence implementation specifics. This was highlighted\nby a cloud computing adoption study that documented the difference between\nregulated and unregulated industries.\n",
      "content_length": 2385,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "Design Considerations\nChapter 3\n[ 65 ]\nAmong regulated industries, insurance companies prefer private clouds because\nthey are considered more secure than public clouds. Even though many studies\nhave shown this to be a false assessment, this misconception is shared across\nmany industry sectors. Even so, industry association community clouds have\nincreased in popularity. While the banking industry is also concerned about\nsecurity, the industry's forced transition from OS/2 to Windows 7 in 2014 drove a\nrapid adoption of newer and more sophisticated technology. Additional\nupheavals caused by subsequent transitions to Windows 10 have made cloud\ncomputing an attractive option for administrative and back-office processes like\nemail, file sharing, and sharing of notes. While opportunities to use cloud\ncomputing in a variety of ways do exist across the government sector, most users\nmisunderstand them. Today's largest opportunity is in using the public cloud, but\nmany in government also fear security problems. Government-wide efforts such\nas the Federal Risk Authorization and Management Program (FedRAMP),\nhowever, have gone a long way toward educating this sector.\nAcross unregulated industries, the story differs greatly. Cloud implementations in\nthe retail market have been mostly IaaS or PaaS solutions. Security, availability,\nand vendor maturity are all aspects that retailers consider when deciding which\nfunctions to deploy as a cloud service. Media companies have gone all in on\nutilizing cloud computing. Today, the media audience can access any content\nthrough a variety of channels. These new opportunities are why cloud service\nproviders and application developers are exploring a cloud-based ways to enable\nmulti-screen entertainment. Industries are using cloud integrate, automate, and\nenable innovations in logistics, sales support functions, HR, product\ndevelopment, and lifecycle management, as well as some manufacturing\noperations.\nGeographybWhere am I? Where do I need to be? Cloud computing is\ncomposed of physical data centers with five primary considerations influencing\nwhere data centers are built:\nPhysical space required to build the data center buildings\nAvailability of high-capacity network connections\nInexpensive electricity\nApplicable jurisdictional law, policies, and regulations\nThe cloud computing export markets are shown in the following\nfigure:\n",
      "content_length": 2392,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "Design Considerations\nChapter 3\n[ 66 ]\nThe first three are governed by physical constraints based on environmental\nvariables:\nPhysical geography\nWeather and natural disaster risks\nRenewable energy resource availability (such as water, geothermal,\nor wind) for cooling and power\nSafety concerns fueled by crime, terrorism, and corporate\nespionage\nAfter that, proximity to high-capacity internet connections is key, because a data\ncenter's value is measured by the number of users it can support. Proximity to the\ninternet backbone, or the main trunks of the network that carry most of its traffic,\nis another important driver. Data centers consume huge amounts of energy to\ncool servers. Areas with cheap energy are highly attractive. Considerations\nassociated with jurisdictional laws, policies, and regulations are addressed later.\n",
      "content_length": 833,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "Design Considerations\nChapter 3\n[ 67 ]\nWhile US companies lead the cloud computing marketplace globally today, that\ndoesn't guarantee leadership in the future. Some foreign companies, such\nGermany's SAP or Japan's Fujitsu, have become strong global competitors, while\nothers, like Alibaba in China, have become formidable global competitors.\nPublic cloud services are rapidly becoming more important strategic factors in\nbusiness. Public cloud will constitute more than half of worldwide software,\nserver, and storage spending growth, by 2018, according to IDC. An example of\nthis is General Electric, a global company that currently has over 90% percent of\nits applications in a public cloud. Greater public cloud adoption has also spurred\nwider SaaS usage, accounting for approximately 55% of all public cloud spending\nby 2018.\nLegislation and other external regulations that apply: The laws, policies, and\nregulations of a particular jurisdiction can have a significant impact on the cloud\nprovider and the cloud user. The many law and policy problems that may affect\nits use by a company include:\nSecurity with the assurance that unauthorized access to sensitive\ndata and source code will be guaranteed by the cloud provider\nConfidentiality and privacy of data held by the CSP with an\nexpectation that the cloud provider, third parties, and\ngovernments will not be able to monitor their activities\nClear delineation of liability with regards to operational problems\nProtect of intellectual property\nRegulation, control, and ownership of data that is created or\nmodified using cloud-based services\nFungibility and portability which is described as the ability to\neasily move or transfer data and resources from one cloud service\nto another\nAbility to audit users to verify compliance with regulatory\nrequirements\nA clear understanding of legal jurisdiction\n",
      "content_length": 1860,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "Design Considerations\nChapter 3\n[ 68 ]\nThe manner in which an organization will approach cloud computing policy vary\nand will be driven by organizational priorities. One of the largest challenges will\nbe associated with user security and privacy. Since many data centers are located\nin the United States, some of these concerns will be caused by the USA PATRIOT\nAct, the Homeland Security Act, and other intelligence-gathering instruments\nemployed by the federal government to for release of information. One of the\nmost disturbing aspects of these policies is the restrictions placed on a CSP that\nprevent notification to a user if the government issues a subpoena for a user's\ninformation. Other policy issues that can impact cloud use include Health\nInsurance Portability and Accountability Act (HIPAA), Sarbanes-Oxley Act,\nGramm-Leach-Bliley Act, Stored Communications Act, federal disclosure laws,\nfederal rules of civil procedures, and e-discovery.\nAdopted by the European Commission to strengthen data protection for all\nEuropean Union citizens, the General Data Protection Regulation (GDPR) was\napproved in April 2016. With an effective date of May 25, 2018, GDPR compliance\nis challenging for companies of all sizes. With cloud computing, the problem may\nbe even worse. Studies have shown that only 1 percent of cloud providers have\ndata practices that comply with GDPR regulations. In fact, only 1.2% of cloud\nproviders give users encryption keys that the customer manages and just 2.9%\nhave secure password enforcement that is robust enough to pass GDPR muster.\nOnly 7.2% have proper SAML integration support.\n",
      "content_length": 1621,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "Design Considerations\nChapter 3\n[ 69 ]\nUnderstand business strategy and goals\nThe second layer of the design triangle is having a thorough understanding of\norganizational strategy and goals. The cloud solution design must support and advance the\norganization's strategy and goals or it will be deemed a failure. To ensure alignment, the\nsolution architect must discuss goals and strategy with the business owner and agree on the\nkey metrics and target values. Some of the most popular cloud computing goals are as\nfollows:\nAgility: Cloud computing delivers improved agility because it has on-demand\nself-service and rapid elasticity. These attributes enable enterprises to quickly\ninnovate, introduce new products and services, enter new markets, and adapt to\nchanging circumstances. Business agility also requires the ability to create new\nbusiness processes and change existing ones. Cloud computing can eliminate\nprocurement delays often associated with development and testing by enabling\ndevelopment resources to be available on-demand. Resource scaling enables\nservice levels to be maintained and reduces cost. Cloud-based strategies can also\nhelp an enterprise acquire capabilities without the need for training.\nProductivity: The cloud can provide a more productive environment for\ncollaborative working. The use of cloud-based tools for email, instant messaging,\nvoice communication, information sharing and development, event scheduling,\nand conferencing are well known. The cloud can also provide shared logic in a\nbusiness ecosystem.\nQuality: The cloud can deliver better quality-IT because usage information gives\nthe enterprise an understanding of how IT is operating. Better understanding\nenables effective planning, equitable resource sharing, and increased resource\nefficiency. The ability to use web portals to automatically provision and\nconfigure resources gives the cloud service consumers substantially better\nmanagement capabilities versus non-cloud system. Economies of scale also make\nthe cost and effort required to of duplicate systems for disaster recovery\nrelatively small. Server consolidation, resource optimization, increased asset\nutilization, and thin client use enhances cloud computing efficiency and reduces\nthe carbon footprint.\n",
      "content_length": 2267,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "Design Considerations\nChapter 3\n[ 70 ]\nReduced cost: Cloud computing reduces IT costs by delivering effective resource\noptimization, by being able to move processor, memory, storage, and network\ncapacity among users almost instantly. Significant cost reductions can be\nobtained by replacing expensive client devices with cheaper client devices that\nprovide just a user interface to the application server. Community clouds\nprovide an easier path for a community of enterprises to share the cost of\ncommon resources. If the organizational goal is maximizing financial capital use\nthrough the improved use of the debt and equity funds, the OPEX opportunity\nprovided by cloud computing will directly support that strategy.\nIdentification of new business opportunities through morphing a company into\nbecoming a cloud service provider. A company that excels in the quality of its IT\nmay easily become a public IaaS or PaaS provider. Implementing services in the \ncloud would make them accessible to a large, global market.\nOperational risk/reward balance can be improved by moving low-risk activities\nto an on-demand service environment. Other on-demand business opportunities\nmay be unveiled exist by mitigated risk management through partnerships and\nrisk sharing. High risk can be reduced by sharing selected business process\noperations that are strongly correlated to operational or legal failures. This may\ninclude corporate risks linked to identifiable software applications, infrastructure\ncomponents, or specified services. Corporate benefits should be traded-off\nagainst these corporate risks with consideration of whether business activities\nwith low corporate returns can be commoditized for competitive advantages.\nThis strategic direction could identify opportunities to improve market share,\nrevenue, profit, or cost management through on-demand delivery of cloud\nservices.\nModifications to the business products line could be both transformative and\ndisruptive. Opportunities to exploit existing markets with current products and\nservices could be profitable as a utility or commodity offering. A company may\nbe able to offer existing services in a self-service model that has been augmented\nand enhanced with on-demand features. Unique products services sourced and\ndelivered via an on-demand portal could also be disruptive to existing product\nlines. If this is the case, the cost-benefit exchange here must be analyzed. Rapid\nscaling and expansion of offered services could deliver additional value.\n",
      "content_length": 2514,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "Design Considerations\nChapter 3\n[ 71 ]\nReducing investment in non-differentiating processes through the use of SaaS\ncould significantly improve an organization's bottom line. Areas where this has\nbeen particularly helpful include:\nBusiness management area, which includes skills management,\nbenefits administration, compensation planning, and human\ncapital management (HCM)\nFinancial management (FM) in the areas of accounting, financial\nand compliance reporting, real estate management, Sarbanes-\nOxley (SOX), finance taxes, BASEL II, order to cash, business\nperformance management, and risk management\nCustomer relationship management (CRM) that supports sales,\nbusiness intelligence (BI), customer experience management\n(CEM), business analytics, call center management, campaign\nmanagement, sales force automation (SFA), and sales analytics\nSupply chain management (SCM), especially procurement,\nsupplier relationship management (SRM), inventory\nmanagement, logistics, and import compliance processes\nManufacturing management of product lifecycle (PLM), resource\nand capacity, and workforce\nInformation technology (IT) services associated with IT\narchitecture design, data center operation, and software\ndevelopment\n",
      "content_length": 1220,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "Design Considerations\nChapter 3\n[ 72 ]\nCorporate headquarters activities such as research and\ndevelopment (R&D), communications, strategy, and portfolio\nmanagement, legal, and marketing:\nInternal business process scope and complexity can\nbe adjusted by considering what niche business\nprocesses could be moved to a CSP. This could range\nfrom moving a specific IT operation to an on-\ndemand provider or commoditizing a service for\ncompetitive low-cost advantages. Both large\norganization-wide operations and smaller, localized\nactivities should be considered. Management should\nalso make specific decisions on which specific\nbusiness processes need to stay under the control of\nthe business for competitive advantage or whether\ncomplex processes can be improved by reducing the\nnumber or complexity of the steps involved.\nCloud computing has a history of improving\ncollaboration/information sharing through the use of\non-demand personal productivity tools. This area\ncan also be used to addresses debates around\nwhether personal information and assets created by\nindividuals should be classified as the private\nintellectual property of the corporation.\nCollaboration across a community of users can raise\nsimilar issues, by raising intellectual property issues\nregarding creating assets on a shared platform or\necosystem business service environment. When\naddressing private information, corporate rules that\nidentify and define data ownership and the\npartitioning of corporate information from that\nwhich is private should be part of the strategy\nprocess. This will often drive decisions regarding the\nsuitability of secure storage and access control\nservice.\n",
      "content_length": 1660,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "Design Considerations\nChapter 3\n[ 73 ]\nIn the case of public information, corporate and personal data rules should\nprohibit the storage of, and access to, personal and corporate information from\npublic locations. Local and national legislation affects this as well. Information\nheld publicly must be monitored and managed at a level that meets legal e-\ndiscovery standards. Personal and corporate information must be classified,\npartitioned, and effectively isolated for storage and use in public locations. The\nvarious components of the business process are shown in the following diagram:\n",
      "content_length": 591,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "Design Considerations\nChapter 3\n[ 74 ]\nAn architect's success in overcoming the two big hills to adoption of cloud computing\nservices will be influenced mostly by a solution's alignment with relevant business drivers\nand strategies. The solution must also pay homage to the intended customer value\nproposition.\nThe most broadly recognized cloud computing business drivers are:\nCost flexibility, which shifts fixed cost to variable cost and allows the\nimplementation of the pay-as-and-when-needed model\nBusiness scalability that provides flexible, cost-effective computing capacity\nMarket adaptability, which enables faster time to market and supports business\nor mission experimentation\nMasked complexity that helps expand product and service sophistication while\nsimultaneously allowing for greater end-user simplicity\nContext-driven variability that is used to better define the user experience which,\nin turn, increases product relevancy\nEcosystem connectivity, which fosters new commerce value nets, which can\ndrive and create new business models\nBusiness strategies most often supported by these business drivers are:\nProduct and service optimization that enhance the customer value proposition\nimprove current industry value chains and uses the cloud to incrementally\nenhance their customer value propositions while improving their organization's\nefficiency.\n",
      "content_length": 1365,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "Design Considerations\nChapter 3\n[ 75 ]\nMarketplace innovation that aims at extending the customer value proposition\nthrough the transformation of industry value chain and improving customer\nvalue. This strategy choice often results in brand-new revenue streams and\necosystem role modifications.\nMarket disruption is focused on inventing new customer value propositions and\ncreating new industry value chains by generating new customer needs and\nsegments.\nValue chain enhancement is accomplished by building new industry value chains or\ndisintermediate existing ones. Cloud adoption can also aid an organization that is\nstruggling to maintain its place within an existing value chain. This is accomplished\nthrough increased efficiency and improved capability to partner and collaborate.\nTransformative organizational goals can be attained through the development of new\noperational capabilities, changing the organization's existing industry role or by deciding to\nenter a completely different marketplace or industry.\nIf organizational goals revolve around improving the customer value proposition,\ncompanies can garner incremental revenue through improvements in current products and\nservices and enhancement of customers' experiences. Cloud computing services can help a\ncompany explore or create new distribution channels or payment methods. This could\nattract existing or adjacent customer segments. This vector could also lead to the creation of\na new marketplace need which would attract new customer segments and generate unique\nrevenue opportunities.\nThe various economic options cloud enables often drives innovative business models. This\nis accomplished by having a model that ensures a dollar of revenue for every dollar of\nexpense. Cloud computing economic payment options include:\nOn demand, in which you only incur cost for the service your organization\nactually consumes\nReserve, where a company is obligated to pay for a predetermined quantity of\nservice provider resources at a discounted price\nSpot market, which is an open marketplace auction model that varies resource\ncost by varying demand for that resource\n",
      "content_length": 2130,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "Design Considerations\nChapter 3\n[ 76 ]\nSummary\nCloud computing requires consistent, methodical, and process-oriented thinking. It is not\nabout any specific technology, but rather operational, economic, and business models built\non highly standardized and automated IT infrastructures. Success depends on establishing\nand working within formalized boundaries. These boundaries must be documented and\nenforced by organizational IT governance. The cloud solution design must support and\nadvance the organization's strategy and goals or it will be deemed a failure. To ensure\nalignment, the solution architect must thoroughly understand the business or mission goals\nand strategy. More importantly, the architect and business/mission owner must agree on\nthe key metrics and target values that will define success.\n",
      "content_length": 810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "4\nBusiness Drivers, Metrics, and\nUse Cases\nThere are many ways to evaluate projects from a financial perspective. Cloud solutions, like\nmost IT projects, are easily compared with ROI metrics. However, ROI does not typically\ntell the whole story. The cloud can pull many financial levers as well as optimize and\nincrease efficiencies in several related, but not necessarily direct, inputs to ROI calculations.\nThis section looks into some of the considerations when looking at the economic impact of\ncloud solutions.\nReturn on Investment\nReturn on Investment (ROI) is the most often used measurement when making project\ninvestment decisions. It measures the rate of return versus the cost of the investment.\nBecause ROI is a percentage, it is very easy to make quick comparisons when evaluating\nmultiple options. There are only four ways to either increase revenues or lower costs,\nimproving ROI numbers. The four financial levers are the following:\nDecrease investment\nIncrease revenue\nDecrease costs associated with the activity\nReduce the time required to attain revenue\n",
      "content_length": 1073,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 78 ]\nCloud computing can operate any of these levers but the simultaneous achievement of all\nfour is impossible. The relationship between these factors is the most important aspect of\ncloud ROI, not the absolute values. For example, a project moving to a public cloud may\nresemble the following: initial investment decreases, operating costs may increase, revenue\nmay remain flat, but margins may increase due to lower investment, and return may\naccelerate due to lower overall investment at higher return margins. Increasing revenue in\nthis situation would accelerate the rate of return and shorten the time needed to attain\nrevenue goal.\nThese financial dynamics change with every project, service model, and deployment model.\nPrivate deployments have completely different dynamics. In-house versus external, in-\nhouse versus in-house, each will have different return rates. ROI can be improved, or made\nworse, with strategy choices because of the relationships of revenue and speed of return.\nRevenue may increase by improving features and quality, commanding higher market\nvalues. Automation may help a business scale, driving greater revenues at controlled costs.\nCloud requires balance in its approach. Data-driven methodologies will help to define goals\nand expected outcomes, and identify ways to manage risk. A data-driven approach can\nguide the organization to optimal strategies and identify better choices. There are many\ndata points and fundamental drivers that can impact cloud ROI numbers. Many data points\nare captured from related productivity, speed, scale, and quality measurements. Typical\nROI calculations are straightforward: there is a cost for a given return. ROI for cloud is a bit\ndifferent in that there are other factors, including efficiency improvements, opportunity\ncosts, and investment patterns. When evaluating cloud versus traditional IT strategies,\nadditional layers of data must also be considered, such as the following:\nIncreased turnover and profits due to increased efficiencies\nRevenue loss due to the inability of existing systems to respond to dynamic\ndemands\nCosts of managing a standalone and non-standardized environment\nReduction or avoidance of capital cost related to the purchase, development, and\ndeployment of new systems or services\nSuccess-based growth and investment, as needed\nSmaller increment investment for cloud versus larger capital investment for\ntraditional models\n",
      "content_length": 2482,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 79 ]\nThe factors that drive ROI are the following:\nProductivity, which is enabled by utility-based services that provide on-demand\nprovisioning that meets meet actual customer usage. Increased productivity can\navoid infrastructure capital expenditures, avoid infrastructure investment\nopportunity cost, and improve customer satisfaction through better\nresponsiveness.\nResource utilization, which eliminates the practice of dedicating servers to\nspecific functions or departments by using active management to size and handle\npeak loads that are underutilized at off-peak times.\nUsage-based pricing translates higher provider utilization into lower\ninfrastructure costs for consumers. SaaS does this by reducing the traditional\nlicensing cost associated with ownership, number of users, support, and\nmaintenance costs:\n5QHVYCTG\u0003NKEGPUG\u0003EQUV\u0003CU\u0003VJG[\u0003TGNCVG\u0003VQ\u0003PWODGT\u0003QH\u0003WUGTU\n",
      "content_length": 927,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 80 ]\nSpecialization and scale that gives the CSP an ability to drives lower IT costs\nthrough skill specialization and economies of scale by amortizing costs over a\nlarger user base:\n4GXGPWG\u0003IGPGTCVGF\u0003QP\u0003VJG\u0003DCUKU\u0003QH\u0003VJG\u0003EQUV\u0003CPF\u0003VJG\u0003VKOG\nIncreased speed in provisioning IT resources, which enables enterprises to\nacquire the resources they need faster. This model also increases visibility into\nresource configurations, which accelerates the choice when many options are\navailable. This factor can dramatically cut the time to deployment of new\nproducts and services. Elastic provisioning creates a new way for enterprises to\nscale their IT to enable the business to expand. Rapid execution saves time and\nenables new business operating models:\n6TCFKVKQPCN\u0003+6\u0003FGRNQ[OGPV\u0003XGTUWU\u0003CP\u0003+6\u0003FGRNQ[OGPV\u0003WUKPI\u0003ENQWF\u0003EQORWVKPI\n",
      "content_length": 870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 81 ]\nFaster execution of lifetime cost models through increased speed of execution.\nThis factor positively impacts lifetime cost models by reducing the cost of a\nproduct or service as the depreciation cost of purchased assets decreases and \nefficiencies are realized. This higher rate of cost reduction means that\nprofitability increases more quickly, giving shorter payback times and increased\nROI:\n5SBEJUJPOBM\u0002SBUF\u0002PG\u0002DPTU\u0002SFEVDUJPO\u0002WFSTVT\u0002UIBU\u0002XJUI\u0002DMPVE\u0002DPNQVUJOH\nThe IT asset management process accelerates reducing the risk of decoupling IT\nchoices and its impact on long-term operations and maintenance IT service costs.\nThis factor also enables the ability to select hardware, software, and services from\ndefined design configurations to run in production environments. This reduces\nthe design-time/runtime divide while simultaneously optimizing service\nperformance:\n6TCFKVKQPCN\u0003UQHVYCTG\u0003NKEGPUG\u00034GVWTP\u0003QP\u0003+PXGUVOGPV\u0003ITCRJ\n",
      "content_length": 984,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 82 ]\nCloud computing is an economic innovation, not a technical one. Infrastructure is aging\nand is going to require significant funding to modernize. How can we optimize the ratio of\nspend to return across the entire asset portfolio? The same traditional deployments are\ngoing to lead to a traditional return. A change in model and economics is required to\nmodernize. Cloud enables an enterprise to change economic models, achieving a faster,\ncost-effective asset management lifecycle for the entire IT portfolio. Designs can utilize\ncurrent capabilities and components, optimizing runtime performance. Cloud services also\nlower entry cost with faster deployment time, quicker time to market, increased\ncompetitiveness, and more business and leads generated across a much larger operational\nscale. Additional high-value services are quickly and efficiently delivered to clients and\ncustomers through the use of cloud-based collaboration services for communication,\ninformation exchange, and virtual meetings.\nCloud economics is creating many new business opportunities that were not possible\npreviously. Opportunity is often associated with the Long Tail shown in the preceding\nfigure. The illustration shows as efficiencies improve, opportunities for a revenue increase\nand margins rise over time. Innovation driven by economics increases efficiency, lowers\ncost, and as a by-product creates additional opportunities for revenue. The revenue\nopportunities may be related to underserved markets that are now accessible. New\nsegments and sectors may now be financially feasible that previously were considered\neconomically undesirable. Opportunities that may have previously been undesirable from a\nrisk point of view may now be interesting revenue opportunities as the lower costs and\nhigher margins may offset perceived risk.\nInterestingly, we see the same behavior in the IT infrastructure market. The cost of\ncomputing resources is dramatically dropping while the cost of managing traditional\ndeployments is rapidly rising. This creates new opportunities within cloud-based\necosystems as well. We are seeing many new cloud service providers entering the market.\nMerger and acquisition activity has increased. Innovations driven by economics are always\ndisruptive. These types of shifts create many opportunities for specialists and related\nenterprises to make big plays as new service providers or acquirers.\nBecause cloud innovation is economic and not technical, new revenue opportunities can be\npeople-based and IT-based. Opportunities may be a new type of service or an existing\nservice with a new economic model. Revenue opportunities may enhance the quality of\nexisting services as reinvestment is now possible due to improved margins, increased scale,\nand larger scope for current operations. Cloud computing enables better utilization of\nresources and assets, increasing efficiency, and accelerating operations and delivery at\nlower cost. Cloud computing is disruptively influencing both buyers and sellers.\n",
      "content_length": 3073,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 83 ]\nAs available services, combinations of services, and pricing models continually change, the\nquality of the service must be considered. A recent performance comparison of a single\nserver from a single service yielded some surprising data. The provider offered the same\ncloud service out of multiple locations. Two locations were chosen. The same instance type\nand same server configurations were chosen. The only difference was the location. Each\nserver was benchmarked using the same CPU and memory test. Testing reported a 700%\ndifference in performance within the same provider, with the only difference being\nlocation. When the benchmarking was run across multiple providers, using the same size of\nserver with the same configuration, we noticed very different performance, with the price\nvarying by more than 3,200% from low to high, for the same configurations. What is the\ndifference? Why would the deviation be so high? Aren't all clouds equal? Cloud is cloud,\ncorrect? Not exactly.\nBasic ROI calculations are pretty straightforward: cost versus expected return. As we\nexamine the other surrounding benefits of utilizing the cloud, we are reminded of a few\nthings:\nCloud is an economic innovation\nEvery benefit of the cloud has an economic impact\nNext-generation designers, architects, and IT leaders need a blended foundation\nthat includes principles of business and strategy, general economics, and\ntechnology, as well as a good understanding of business risk and economic\nmitigation\nWhy would the same server vary 700% in performance and 3,200% in price for the same\nconfiguration? Other layers and benefits must be accounted for in our cloud ROI\ncalculations. There are many metrics to consider; not all need to be used, for example,\nservice level agreements (SLAs). SLAs are a simplified way to try and express a quality of\nservice numerically. The higher the number, 99.999 versus 99.90, the higher the quality of\nservice is perceived to be. What does the quality of service actually mean? This topic will be\nexplored more throughout the book. Is the service redundant? How resilient is it? Is it\ndelivered via cheap unknown white box machines out of a neighbor's garage or is it within\nan impenetrable fortress using a brand name, high performance, and the latest and greatest\nhardware? How is it supported? How up to date are patching and security? Is it hardened\nOS? Is there 24x7 support? What is the quality and level of the support engineering teams?\nMany things can contribute to pricing differences, including margin.\n",
      "content_length": 2597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 84 ]\nCloud computing differentiation is not just through the provisioning of utility computing\nservices, but also higher-level services that enhance and build customer value. These\nattributes are why there is rapid movement from technology-centric services to business-\nvalue-centric services. This change extends to nearly every service and every industry, with\nutility infrastructure services at one end and complete function and application business-\ncentric services being provided by nearly every provider in the market.\nROI metrics\nWhen designing and building a business case for a cloud computing solution, the following\nmetrics can assist with aligning a prospective solution with the business or mission need:\nTime: Cloud solutions optimize time required to deliver or execute business\nprocesses by decreasing the time required to provision resources or time required\nto consider multi-sourcing options. It can also decrease the time required to\nachieve specified goals associated with information technology services. This\nvalue also leads to a faster realization of reduced IT total ownership costs.\nCost: Cloud computing can optimize ownership use by reducing the application\nportfolio total cost of ownership. This is realized through license cost reduction,\nopen source adoption, and SOA reuse adoption. Cloud also optimizes the cost\nassociated with delivering a specified IT service capacity by aligning IT costs\nwith IT usage. The CAPEX versus OPEX utilization balance can be more\neffectively managed with pay-as-you-go savings.\nQuality: Cloud can improve service and product quality through customization\nand enhanced user relevance. It can also reduce ecological damage through\nreduced carbon footprint and advance organizational green sustainability goals.\nOptimizing margin: Metrics associated with the cost to deliver/execute business\nand supply chain cost is reduced, which increases product/service margin.\nIncreased flexibility and choice across providers and feeder services can also help\nto optimize margin.\n",
      "content_length": 2087,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 85 ]\nKey performance indicators\nThe key performance indicators (KPIs) are used to measure goal attainment. Advance\nagreement on the relevance of the following cloud computing KPIs can assist in\nsolution/business alignment:\nTime:\nAvailability versus recovery SLA: Indicator of availability\nperformance compared to current levels\nTimeliness: Degree of service responsiveness, which can be used\nto indicate rapidity of service choice determination\nThroughput: Transaction latency or the volume per unit of time\nthroughput which measures workload efficiency\nPeriodicity: Frequency of demand and supply activity or the\namplitude of the demand and supply activity\nTemporal: The event frequency to real-time action and outcome\nresult\nCost:\nWorkload-predictable cost: Indicator of CAPEX cost of on-\npremises ownership versus cloud\nWorkload-variable cost: Indicator of OPEX cost for on-premises\nownership versus cloud\nCAPEX versus OPEX cost: Indicator of on-premises physical asset\nTCO versus cloud TCO\nServer consolidation ratio: Ratio of servers in legacy infrastructure\nto the number used in the cloud infrastructure\nWorkload versus utilization percentage: Indicator of cost-effective\ncloud workload utilization\nWorkload type allocations:\nWorkload size versus memory/processor distribution: Measures\npercentage of IT asset workloads using cloud\nInstance to asset ratio: Measures percentage and cost of IT\nconsolidation\nDegree of complexity reduction (%): Measures the number of\nguest operating system instances versus the number of physical\nresource assets\nTenancy to instance ratio: Measures tenants per resource, which\nmeasures CPU and memory utilization\n",
      "content_length": 1704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 86 ]\nEcosystem (supply chain) optionality: Tracks the use of\ncommodity assets used to deliver company services after the\nfunction is migrated to a CSP\nQuality:\nExperientialcthe quality of the perceived user experience of the\nservice\nBasic quality of service metrics (availability, reliability,\nrecoverability, responsiveness, throughput, manageability,\nsecurity)\nUser satisfaction\nCustomer retention\nRevenue efficiencies\nMargin increase per unit revenue:\nRate of increase of annuity income\nSLA response error rate\nFrequency of defective responses\nIntelligent automationcthe level of automated response (agent)\nMargin:\nRevenue efficienciescability to generate margin increase per\nrevenue.\nRate of annuity improvement\nMarket disruption ratecrate of revenue growth versus rate of new\nproduct customer acquisition\nBusiness goal key performance indicators\nBusiness goal KPIs that should be considered include the following:\nThe speed of cost reduction\nCost of adoption/de-adoption\nOptimizing ownership use\nRapid provisioning\nIncreasing margins\nDynamic usage\nElastic provisioning and service management\nRisk and compliance improvement\n",
      "content_length": 1182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 87 ]\nEconomic goal metric\nIn a similar vein, measurable economic goal metrics include the following:\nCapital expenditure avoidance\nConsumption billed as a utility\nLower barriers to market entry\nShared infrastructure costs\nLower management overheads\nImmediate access to the application\nImmediate termination option\nEnforceable SLAs\nHigh benefit-cost ratio\nThere are also metrics for performance and price-to-performance that can provide very\nclear comparisons between providers, services, and strategies. Using these next-level\nmetrics can move the ROI conversation forward quickly when aligning solutions to an\norganization's goals and expected outcomes. This type of transparent data also helps build\nconsensus, eliminate politics, and facilitate cultural adoption within the enterprise.\nGeneral use cases\nYou can refer to a baseline set of use cases a cloud solution architect can use to identify the\nbest solution target for the enterprise at IUUQT\u001c\u0011\u0011XXX\u0010TDSJCE\u0010DPN\u0011EPDVNFOU\u0011\u0013\u0019\u001b\u0014\u001b\u0015\u001b\u0016\u0011\n$MPVE\u000f$PNQVUJOH\u000f6TF\u000f$BTFT\u000f8IJUFQBQFS\n4BWFE\u0002DPQZ\u000b.\nThese are illustrative examples of the most typical cloud use cases and are not meant to be\nan exhaustive list. Active use case components are shown in color.\nAdditional details on the operational requirements summarized here are\nprovided in $IBQUFS\u0002\u0013\u0013, Operational Requirements.\n",
      "content_length": 1371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "Business Drivers, Metrics, and Use Cases\nChapter 4\n[ 88 ]\nSummary\nROI is always a key topic when organizations invest. Investing in cloud computing\nsolutions is no different. The architect must be clear on how the proposed solution will\ndeliver value and that value must be described in business or mission terms. The metrics\noutlined in this chapter have been effectively used across many industries. Key business\ndrivers must be identified early and a direct linkage to solution functions and capabilities\nmade clear. General use cases are useful for outlining day in the life scenarios, which, in turn,\ncan be effectively leveraged when communicating solution value to business or mission\nowners.\n",
      "content_length": 700,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "5\nArchitecture Executive\nDecisions\nThe cloud is changing everything. Change occurs every day in the cloud services market.\nChanges to solutions, services, pricing models, consumption models, and locations all lead\nto different strategies, technology choices, economic impacts, and risk profiles.\nToday, it is a normal process for the consumer to express what solution and solution\ncomponents they require in the form of an RFI, RFP, RFQ, or some other type of\nrequirements document, emailing a spreadsheet, for example. The consumer expresses\nwhat their experts need based on a mix of data sources, including business requirements,\ncurrent state information, and the latest innovative articles read by a partially involved IT\nleader. Requirements are sent to one or more providers with the providers expected to\nrespond to what is requested. Pricing is normally added and the response returned with\nminimal, if any, insight given. There may be a few phone calls and some back-and-forth,\nenabling the service provider to respond as accurately as possible.\nCurrent processes feel very transactional: I want a blue shirt, or I want a French cuff,\nbutton-down, blue dress shirt; please show me the blue shirts you have and how much they\nare. This process pattern is very manual and very slow. The process is filled with stop-and-\ngo workflows and communication patterns. This kind of process is also serial, meaning that\neach step must be completed before the next can start. Tools used to communicate and\nshare data are also manual, disconnected, and slow.\n",
      "content_length": 1554,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 90 ]\nToday's process looks something like the following diagram. This view is from the service\nprovider side as they formulate a response. Everyone involved in the process (consumer,\nprovider, integrator, consultant, and channel partner) follows virtually the same\nengagement pattern:\nThe cloud is completely turning this engagement method upside down and sideways. This\nmethod no longer works. The market is changing too fast. Products and services are\nreleased, updated, changed, or retired almost daily. How can the consumer side possibly\nbuild accurate solutions in a vacuum and pass it to a service provider for a response? It is\nimpossible. The service providers can barely keep up with the pace of knowing their own\nproduct catalogs. There is no way the consumer can keep up with the thousands of\navailable products and services from a service provider. Multiply that by the hundreds and\nthousands of potential service providers available in the market. Talking to two or three\nsuppliers is no longer a representative sample set. Multiply those numbers by the hundreds\nof locations available globally. There are trillions of potential combinations available. How\ncan a consuming enterprise possibly gather the data, normalize it, compare it, optimize\ndesign, and pick a strategic partner using the slow, manual, disconnected tools, and\nprocesses available today?\n",
      "content_length": 1415,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 91 ]\nWhat needs to change? As stated, the process needs to be completely inverted. The tools\nneed to be automated. Collaboration needs to be in real time. Insight needs to become a\nrequirement, not an unexpected differentiator. Instead of pricing requirements, we need to\ndesign with economics in mind. Instead of building technical designs, we need to\ncollaborate for solutions that align technology, strategy, economics, and risk. Instead of\ntelling service providers what is required, business challenges and risks should be\ncommunicated. The thinking must change and begin to map solutions to business\nchallenges and match technology to tasks.\nWith changes in mindset, process, and approach, executives can accelerate organizations,\nmotivate teams, and increase control over strategy, economics, and risk. Engaging in\ninsightful, collaborative ways can transform the previous slow, serial, stop-and-go methods\ninto the following high-velocity, streamlined, parallel operating method:\nInvert for insight ` process\nToday, we are bound by a process that no longer matches our industry, nor does it align\nwith industry direction. As stated, consumers express what they believe they require,\nconfining service providers to respond with services they have that match the request. The \nRFI process allows for some insight to be given, yet is still void of any real collaboration. It\nis still a version of a question and answer session. If the questions go unasked, or the wrong\nquestions are asked, opportunities for greater insight are missed.\n",
      "content_length": 1587,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 92 ]\nReal-time collaboration\nWith the level of change, and the rapid pace of change, in the current market, anything less\nthan real-time collaboration will miss the mark. It is no longer possible to keep up with the\nmoving pieces in our industry. The constant changes to product strategies, economic and\nbusiness models, consumption models, deployment models, and pricing can't be accurately\ngathered, normalized, and compared for one or two vendors, certainly not the hundreds\navailable today.\nExpress challenges, not requirements\nCloud is changing everything. Cloud allows us to buy a fraction of a resource needed for a\nfraction of time, consume it, and return it when finished. The length of time could be in\nseconds, hours, days, months, or years. The beauty is that we cannot align what we need to\nwhen we need it and directly match it to a specific challenge or situation. Thinking can now\nshift away from acquiring solutions then mapping as many problems as we can to it, to\nnow expressing well-defined challenges and only acquiring what is needed, when it is\nneeded, to satisfy the challenge and be able to give it back when we are done with it,\neliminating much of the cost.\nExpressing challenges also improves relationships and partnerships with those wanting to\nhelp solve them. Service providers listen and respond to global market needs every day.\nThe challenge for a service provider is trying to interpret requirements and translating\nthem into challenges. Service providers rarely participate in conversations directly\nexpressing client challenges. The normal conversation covers requirements, not business-\nfocused needs. Consumers avoiding deep discussions related to their challenges may occur\nfor many reasons: internal politics, ego, inexperience, poor planning, oversight, policies,\nconfining processes, and so on. Service providers have learned that insightful, proactive,\ncollaborative engagements are the most successful, with the highest levels of loyalty and\ncommitment. Express challenges to receive insight.\n",
      "content_length": 2083,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 93 ]\nAutomate and enable\nExecutives continually balance investment, return, and risk. Today, many of the tools and\nprocesses we use are manual, disconnected, and slow. In a rapidly moving market,\nexecutives need more data and insight quicker. Automation, integration, and enablement\nare critical for success in today's cloud universe. Any platforms, systems, tools, and\nprocesses invested in should align strategy, technology, economics, and risk in an\nautomated integrated way. Collections of data do nothing without mapping, matching, and\ncomparing to expose real-time insight used to make aligned decisions.\nStop talking technology ` Strategy\nMany people start talking about technology choices very early in conversations. There may\nhave been significant recent investigation aimed at a technology component or direction.\nThere may be team structures aimed at supporting technology commitments previously\nmade. Financially, there may be a perception that change is going to be difficult and\nexpensive. There may also be those that are afraid of change as it may politically change\ntheir value, roles may change, team structures could change along with current\nresponsibilities, or skill sets may need significant changes for those late in their career path.\nStrategy determines direction; technology implements it. Technology can influence\nstrategy, but cannot dictate it. Before the cloud, projects were designed and engineered for\nthe anticipated high-water mark of utilization, even if it was for 1 minute. The anticipated\nworkload determined infrastructure size and scale. Cloud has now enabled us to design\nand architect for the low side baseline workloads and dynamically burst as needed into\ndemand-based configurations using automation and scripting, improving economics, and\nmatching technology to strategy.\nEconomics, not pricing ` Economics\nIt is important to switch the thinking from what the cost is for assembled technical answers\nand move to using economic data as an integral part of the solution design process. Today,\nwe go through the design process four or five times, first for assembling a reasonably\ncorrect technical answer then pulling it apart in layers, trying to back down to something\nthat is within economic reach yet in a technically desired direction.\n",
      "content_length": 2332,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 94 ]\nIf we switch that model and move to looking at designing with economics from the very\nbeginning, we can eliminate nearly all recirculating effort as designs are built then\nredesigned in the current process. It is also important to keep in mind that much of the\ntechnical detail affects implementation details more than it does strategy or economics. If it\ndoes not materially affect strategy or dramatically change economics, move on to\nimplementation discussions after confirming a solution strategy and economic direction. If\nshowstoppers rise during implementation conversations, you are still money and time\nahead, able to switch directions as needed since you have not done the four or five rounds\nin traditional processes:\nThe perfect technical solutions are sometimes unaffordable or miss the strategy. Low-cost\nsolutions may not balance with technical requirements or introduce too much risk. All\nfactors must balance for successful solution design.\nSolutions, not servers ` Technology\nTechnology no longer needs to drive economic and strategic conversations. Due to the\neconomic innovation of cloud, technology is now better able to align with strategy,\neconomic, and risk requirements. Enterprises can now simultaneously align choices on\nstrategy and direction with technology and economics. As an example, businesses with\ncyclical needs, meaning that everything is not needed all the time, can leverage services and\njust-in-time infrastructure deployments as needed to satisfy cycles as needed. Very little\ninvestment is required up front, with scalable growth available programmatically.\n",
      "content_length": 1650,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 95 ]\nSuccessful solutions will utilize services as needed. These services may be internally\nprovided or externally consumed. Strategy and economics will determine the best path\nforward. In almost all cases, consuming solutions as a service is optimal across nearly all\ndecision metrics and scorecards.\nLower costs can be bad for business ` Risk\nExecutives often need to balance economics and risk. The more risk assumed, the lower is\nthe cost. Risk and cost seem to have an inverse relationship. Cloud is an opportunity to\nchange paradigms. The shift comes from a change in perspective. Because cloud is an\neconomic innovation, much of its pricing model builds on economies of scale. An entire\nteam of administrators, security experts, technicians, and engineers can be acquired as part\nof a service for a server priced at pennies per hour. Because of the specialization, deep\nknowledge, 24x7 operation, automation, and employed best practices, one could argue that\nthis situation is a much lower risk than the overworked, underpaid frustrated internal IT\nguru who has virtually no training in cybersecurity and is sick of answering calls after\nhours.\nAlso mentioned earlier was the situation where the same configured server represented a\n3,200% difference in price from a low-cost provider to a high-cost one. It is a guarantee that\nevery provider along that continuum from low to high offers a different level of service\nquality, automation, support, security, patching, management, and monitoring for that\nprice. Because cloud appears to be lower cost, it does not mean that cloud is optimal for\nyour unique situation and business challenges. Cloud is a tool in the tool bag. Screwdrivers\nshould not get used when driving nails. Cloud is the right tool when used for the right job.\n",
      "content_length": 1831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 96 ]\nAdoption is optional ` Culture\nIf you play in the middle of the road, you are eventually going to get run over. Change is\nhard; adopting change sometimes seems impossible. Cloud is something that goes much\nbetter with a firm commitment to it. It is not something to do halfway. You can't maybe,\nkinda, sorta your way into using cloud services. Cloud is a strategic change with very\npowerful economic levers alongside it. Cloud can transform enterprises in many ways.\nCloud is something that you can start at your own pace. There is no reason to lift and shift\neverything at once. Hybrid strategies are among the safest, easiest to control, and cost-\neffective ways to beginning the cloud journey. Cloud journeys are not sprints; they do not\nneed to be marathons either. Based on the real-time data, analytics, scenario planning,\nmodels, normalized data, and side-by-side comparisons, it should be clear whether cloud\nservices make sense for the given situation. Start with the smallest scope that makes sense\nand go from there.\nCloud changes can have a rippling effect. They can also have a polarizing effect. It is critical\nto gather, process, and effectively communicate the right data in the right messaging at the\nright time. Data helps change the mindset, increase adoption, and, ultimately, remove any\ncultural challenges.\nPeople get comfortable with routine, processes, structures, roles, and responsibilities.\nEverything is familiar. A big part of changing mindset is helping others understand the\nbenefits and believe the change is needed. Disruption is difficult when skill sets are lacking.\nChange is also difficult when the reasoning for the change is unknown or misunderstood. It\nis very uncomfortable to change processes that people have used for some time. It is hard to\nreorganize teams or shift responsibilities to others. It is critical to communicate the right\ndata within the right message at the right time. Change management is a huge part of cloud\nsuccess. A relentless focus on communicating change, retraining, refocusing, and\nprioritizing is core to any successful culture and mindset change. Every cloud initiative fails\nwithout cultural adoption.\nUpdating skill sets is another big key to success. The market is moving fast. Technology is\nrapidly changing. Most team members may get one class per year, maybe two if they are\nlucky. Most have to do it on their own to stay relevant. With technical skills lagging behind,\nit makes it very difficult when cloud services begin to take hold. Cloud is economics,\nstrategy, and risk. Most technical team members have very little training or, quite honestly,\npatience to deal with politics, strategy, risk conversations, financial analysis, and metrics.\nMost technical types would gladly jump off the nearest high bridge. The next generation of\narchitects are going to need a mix of business, financial, technical, and political skills. The\nnext generation of high-value designers and architects will need to think more like CFOs\nthan technical administrators.\n",
      "content_length": 3081,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 97 ]\nTechnology for the executives\nArchitecting cloud solutions will also require executives to become familiar with a few\nmodels as cloud solutions are evaluated and implemented. It is important to have a\ncomfortable level of familiarity with these different concepts as strategies and economics\nare discussed alongside technology choices and risk profiles. Service models (IaaS, PaaS,\nand SaaS) dictate the direction for any cloud services consumed and implemented for the\nscope or that project.\nCloud service models for executives\nService model choices are driven by the consuming organization's employee skill set. For\nexample, system administrators manage infrastructure. If IaaS is the chosen service model,\nthe enterprise will need to maintain and grow this group's knowledge and capabilities.\nBecause the base infrastructure gets consumed as a service, administrators can refocus on\nnew career growth opportunities beneficial to the enterprise in the longer term. An example\nmight include learning infrastructure scripting skills with languages such as Chef and\nPuppet:\n",
      "content_length": 1123,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 98 ]\nHeavy software development shops may choose to consume PaaS services. The platform\ncarries all of the components, frameworks, drivers, pieces, and parts, enabling developers\nto be productive immediately. There are many different types of platforms for building\nmany different things. Software development is not the only choice or industry with PaaS\navailable, but it is the most prevalent. As an example, in-house developers need to be\ncomfortable with the platform chosen. If Java is their forte, a Microsoft Azure PaaS service\nmay be slow in the adoption of new Java capabilities. Teams may need additional and\nongoing training depending on the platform chosen. Additional investment may outweigh\nthe benefits if the platform is too much of a mismatch to current skill sets or lacking\ncapabilities for strategies chosen.\nThere are opportunities where no infrastructure is needed separately. The software is\nprebuilt with all the features and functions needed already included for a fee. Licensing is\nconsumed per some unit, usually per enterprise or per user. The SaaS model is rapid to\ndeploy with rapid adoption rates. SaaS does not have an answer for everything, but very\nhelpful when solutions are available.\nDeployment models for executives\nOnce the organization chooses a cloud service mode, relevant metrics are selected, and\ntargeted values finalized, the architect must then determine the appropriate\nrecommendations for deployment and implementation styles. Each service model may\nhave multiple deployment and implementation models. For example, IaaS is a service\nmodel. Within that model, infrastructure is deployed in many ways, including private,\npublic, dedicated, or shared. The deployment of public cloud infrastructure services can\ntraverse many different configuration options as well as different consumption and\neconomic models.\nOrganizational risk tolerance drives cloud deployment model selection. Risk domains cut\nacross operational, economic, technological, and security domains. Operational risk is a\nconsideration for every deployment model. Private deployments are considered lower risk\nas they are owned and managed by the consumer. Right or wrong, humans tend to trust\nthemselves more than others. That thinking has been proven invalid many times, yet this\nreflexive attitude remains.\nWhen choosing deployment options, updating and patching the infrastructure is a key part\nof cybersecurity. Updates are rolled out proactively when service providers are managing\nthe base infrastructure. In-house management may choose not to install patches or security\nupdates for many reasons. Deployment model choices may need to change depending on\nthe desired outcomes.\n",
      "content_length": 2741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 99 ]\nWhen internally managing software and systems, upgrades can take more time, but it does\ngive the owner/manager control to decide how and when the upgrades and updates are\nimplemented, along with as much time as needed for preparation and testing. Another\noperational risk may include the inability to access a cloud service due to lack of internet\naccess. As cloud deployments grow, typically quicker than traditional deployments, they\ntend to sprawl, with control becoming more challenging as the number of services\nconsumed grows. If not monitored and managed correctly, this leads to end-of-month\ncloud sticker shock.\nIn IaaS solutions, the cloud service provider makes all the decisions regarding technology\nchoices for deployed services, including the type of server, the brand of storage, and the\nCPU manufacturer. These choices are abstracted away from the end user, with limited\nvisibility into technical compatibility. The separation from the technology choices may lead\nto portability and interoperability risks in some situations. There can be some concern\nabout data security. Most of these concerns are quickly handled through good design and\nsecurity-minded questioning. Putting information into services that are accessible over the\npublic internet means that criminals have a potential target. Security is a never-ending\nbattle, with threats externally, internally, and sometimes from the least-expected places.\nRemember TJX and the HVAC entry point.\nImplementation models and IT governance for\nexecutives\nImplementation models are largely determined by IT governance. Governance for all cloud\nservices, other than under a private deployment model, is outside of the consuming\norganization's purview. For this reason, cloud services may introduce some level of legal,\nsecurity, regulatory, and jurisdictional risks. Realizing there may be some risk associated,\ncloud services can increase levels of adoption and enforcement of information technology\nstandards. Additional investment commitments in infrastructure, training, and time may be\nrequired, depending on the compliance standards being met. A failure to obtain initial\ncompliance and sustaining the level of commitment required will cause failure of even the\nbest cloud computing solution.\nOnce executive decisions have been finalized, and investment commitments confirmed, a\ncloud computing adoption strategy can then move forward. The most broadly followed\ncloud adoption strategies are the following:\nBuilding competitive advantage, which typically targets organizational strategic\nreinvention of customer relationships, rapidly innovating products and services\nrapidly or building new or improved business models.\n",
      "content_length": 2742,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "Architecture Executive Decisions\nChapter 5\n[ 100 ]\nBetter decisions strategic vector that extensively leverages analytics to pull\ninsights from big data. This also requires the seamless sharing of data across\napplications and the exploitation of data-driven and evidence-based decisions.\nDeep collaboration aims to make it easier to locate and use expert knowledge\nacross a business ecosystem. This strategy requires improved integration\nbetween development and operations and collaboration across the organization\nand extended ecosystem.\nBusiness model change strategies that can be used to support these adoption strategies are\nthe following:\nAddressing more mission/business requirements via a customer self-service\nmodel\nTransitioning from single/limited IT providers into a multiple IT provider\necosystem\nChanging the design/build/maintain and identify/adapt/adopt technology mix\nfrom 80/20 to 20/80\nTransitioning from labor-driven HW/SW integration to value-driven IT service\nmanagement\nTransitioning from dedicated single tenancy to shared service multitenancy\nTransitioning from a cost center that provides support to a business center that\ndelivers tangible value\nSummary\nEnterprise executives will make final decisions on just about every aspect of a cloud\ndeployment. Required investment levels and organizational impact makes this inevitable.\nAs a key team member, the solution architect must always understand and manage the\nviewpoints of any involved executive. These tasks require two-way communication and the\neffective transfer of difficult concepts and new business, operational, and economic models.\nIn almost all instances, the solution architect tends to be cast to serve the role of lead\nexecutive instructor. The challenge of this role was made clear in early 2018 when Facebook\nCEO Mark Zuckerberg testified before the United States Senate on social media data\nprotection concerns. Although these senior leaders were seriously contemplating the\nimplementation of new social media laws and regulations, their questions on basic cloud\ncomputing business models and technologies betrayed a sometimes comical ignorance of\nmodern technology. This chapter should go a long way in assisting you in the performance\nof your educational duties.\n",
      "content_length": 2259,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "6\nArchitecting for Transition\n\"Victorious warriors win first and then go to war, while defeated warriors go to war first\nand then seek to win.\"\na The Art of War, Sun Tzu\nMany people mistake The Art of War for a book teaching strategies on how to fight wars and\ncritical battles. To the contrary, The Art of War is about how to avoid the fight. Long, drawn-\nout battles are expensive, slow, and very hard to control. In the first part of the opening\nquote, Victorious warriors win first, Sun Tzu focuses the student on preparation, situational\nawareness, a controllable environment, attention to relevant details, and determining the\noutcome before starting. The lessons in the book are about knowing the environment and\nhaving situational awareness. Do you have the latest, most accurate information? Do you\nhave reliable sources? Are you thinking clearly? Are you controlling your emotions and\nbiases? Are external influences and detractors in check? Do you see the data for what it is or\nfor what you think it should say?\nCloud transitions, while not wars, can certainly feel like battles. They can feel a bit like\nreligious crusades where believers are willing to do anything for the cause. Cloud\ntransitions do not have a particular pattern, shape, or size. Cloud transitions require the\nmost up-to-date and accurate data possible. Successful cloud transitions are successful\nbefore they ever start. They require the same clear focus, preparation, environmental\ncontrol, and situational awareness. In Sun Tzu, if cloud transitions do not have a clear\nfocus, detailed preparation, and careful planning, the transition will fail before it starts.\n",
      "content_length": 1649,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "Architecting for Transition\nChapter 6\n[ 102 ]\nIn this chapter, we will cover the following topics:\nUser characteristics\nApplication workload\nUse of application programming interfaces (APIs)\nUser characteristics\nClouds are like children. They have unique personalities, quirks, strengths, and\nweaknesses. No two are the same. Just like children, and like some siblings, they behave\ndifferently and seem to follow different rules, and sometimes, they misbehave at the worst\ntime.\nCloud solutions are specific to each provider. Successful transitions require due diligence\nand a thorough understanding of the cloud provider. Cloud transitions require the\nconsumer to know the provider more than the provider needing a deep understanding of\nthe consumer. The traditional IT acquisition process begins with the consumer providing all\nof the details and requirements for inspection by the provider. The provider then responds\nwith a proposed design that meets said requirements. Cloud solution acquisition cycles\nnow reverse that thinking. Up front, service providers must effectively communicate their\ncapabilities, characteristics, attributes, and supporting services for consideration by\nconsumers. This complete reversal in process and approach is core to the cultural challenge\nassociated with cloud transitions. Strategic, economic, and technical choices are now\ncompletely held by the consumer side. Previously, service providers were responsible for\nnearly all solution due diligence, making recommendations, controlling solution\neconomics, and in nearly all cases conservatively over-engineering solutions as excess\ncapacity was better than too little when needed.\n",
      "content_length": 1668,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "Architecting for Transition\nChapter 6\n[ 103 ]\nThe examination of several characteristics and attributes is now the responsibility of the\nconsumer. These investigative deep-dives often require internal cooperation, and often\nunwanted politics, across multiple organizations and divisions. The service providers are\nalso adapting in real time as they are now required to share deeper-level detail on how\nsolutions are designed, built, supported, and maintained. Some details potentially requiring\nadditional investigation may include the following:\nApplication characteristics\nApplication dependencies\nAPI requirements\nTechnology service consumption requirements\nConsuming organization's ability to support IT automation\nConsuming organization's use of scalable design techniques\nConsuming organization's data security and control requirements\nConsuming organization's transformational readiness\nCloud architecture starts by understanding the end user. Cloud service providers use\ntargeted utilization and usage patterns to guide their infrastructure designs. Consumers\ngain value from a provider's service only if that consumer's operational patterns match the\nprovider's target.\nEconomy of scale is how CSPs create profit margin. To accomplish this, resource pools are\nused to share resources across multiple tenants. Both physical and virtual resources are\ndynamically provisioned and de-provisioned based on user demand. Resource pooling also\nprovides location diversity. For SaaS, the user has no control or knowledge of hardware\nlocation. In some cases, the consumer can specify general location aspects such as country\nor region.\n",
      "content_length": 1635,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "Architecting for Transition\nChapter 6\n[ 104 ]\nCloud computing economics typically uses virtualization to automate information\ntechnology resource provisioning. This often leads to an assumption that virtualization\ndefines cloud computing. In truth, cloud computing economic depends on customer\npopulation metrics, namely the Number of Unique Customer Sets (n), Customer Set Duty\nCycles (b,f), Relative Duty Cycle Displacement (t) and Customer Set Load (L):\n",
      "content_length": 457,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "Architecting for Transition\nChapter 6\n[ 105 ]\nThese metrics set the minimum physical IT resource requirements needed to serve a\nspecified maximum level of demand. The preceding figure illustrates this concept. While\nthe three scenarios show different customer sets, each demanding a 1-unit maximum load,\nand the consumer demand duty cycles are similar, the duty cycle displacement values in\neach scenario are different. This small difference translates into significant operational\ndifferences:\nMaximum demand in Scenario A and Scenario B is 30% more than that in\nScenario C.\nScenario B has a zero minimum demand.\nScenario C transitive load requirements are 50% less than the other two.\nIf each user set individually owned their resources, three units would be\nrequired. Scenario A and Scenario B would require the same total number of\nunits. Scenario C, on the other hand, would only need a maximum of 2 units to\nsupport the same demand, resulting in a 30% resource savings.\nIn leveraging the cloud economic model, CSPs must continuously monitor key user metrics\nin near real time to enable any required changes in the underlying physical infrastructure.\nThis leads to the desired illusion of infinite resources that is characteristic of the cloud\ncomputing service experience:\n",
      "content_length": 1279,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "Architecting for Transition\nChapter 6\n[ 106 ]\nUnderstanding and knowledge of target user base characteristics will enable a better\nsolution design. The most appropriate service providers are those that provide the best cost\nand benefit during critical operational periods and duration. If implemented correctly, the\ncloud computing economic model can materially improve return on investment over on-\npremises deployments. The preceding figure illustrates this model in a private cloud\nconstruct. A 2009 Booz Allen Hamilton study concluded that a cloud computing strategy\ncould save from 50%-67% on the life cycle cost of a 1,000-server deployment.\nThe study also showed increased benefit-cost ratios when the cloud transition included\nmore servers or a faster migration schedule, as shown in the following figure. A separate\nDeloitte study shows that cloud deployments deliver more return on investment with\nshorter payback periods when compared to the traditional data center option:\n",
      "content_length": 985,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "Architecting for Transition\nChapter 6\n[ 107 ]\nA cloud solution architect should make every attempt to document and verify the following\nuser population characteristics:\nThe expected number of concurrent users for every application or service\nUser growth rates over an annual period\nThe variability of user demand and for any apparent temporal cycle (that is, time\nof day, day of the week, week of the month, month of the year)\nUser consumption differences based on the user's geographic location\nBreadth across the population and frequency in the use of mobile devices and\nmobile applications\nTypes, models, and operating systems of mobile devices used\nDevice ownership options\nVariability of user characteristics based on functional entity association,\nincluding any economic variable measured or tracked by separate organizational\nentities or consolidated across the entire user base\nImportant business continuity or disaster recovery concerns are driven by end\nuser location, end user devices, operational process considerations, or the usage\ncycle\nApplication design\nMany organizations want to leverage cloud computing as a means of reducing the\noperational costs associated with maintaining legacy applications. In these cases, the cloud\nsolution architect is faced with an application migration task where application maturity\nmay become a decisive factor in whether a cloud environment is beneficial or detrimental.\nApplication characteristics could also profoundly influence the decision regarding CSP\nchoice. Many legacy applications, for example, are tightly coupled to data, specific\nprocesses, and other related applications. This makes them very difficult to transition to a\ncommodity-based technology service model. Embedded dependencies and unusually\nundocumented assumptions don't easily fit into a strictly standardized and not easily\ntailored environment.\n",
      "content_length": 1874,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "Architecting for Transition\nChapter 6\n[ 108 ]\nCloud-friendly applications are loosely coupled, with RESTful interfaces and modular\ndesign. This design approach makes them more amenable to modern cloud-based\ninfrastructures. This difference must also be addressed when applications are newly\ndeveloped for a cloud deployment. Developers have traditionally had the luxury of\nexploring new design approaches that often leverage the unique capabilities of a specific\nand targeted technology or vendor. These new approaches are often foundational to a\ncompany's market differentiation or customer value proposition. The resultant\ninfrastructure customization or configurations are then presented to the supporting\ninfrastructure system administrators as minimum requirements or must-haves. If the\norganization is organically responsible for deployment, these minimum requirements are\ntranslated into specific technology procurement and technical configurations. If the \ndeployment is to be outsourced to a managed service provider or system integrator, the\nprocurement official translates these requirements into a request for proposal (RFP) or\nrequest for quote (RFQ). Competitive vendors then follow the specified requirements to\ndesign and present a priced solution. After technical evaluations and cost-benefit analysis\nare completed, a contract is awarded and funded for the delivery of a selected solution.\nThis traditional approach is fatally flawed if the targeted vendor is a cloud service provider.\nFirst, cloud computing environments are designed and managed as a commodity service by\nthe cloud service provider. This will typically negate most technology or configuration\ndictates from the end customer. It also prevents exploitation of most vendor- or technology-\nspecific capabilities designated as minimum requirements or must-haves by the developer.\nSecondly, the solution design is fixed by the CSP based on previously finalized technology\ndecisions and CSP-funded acquisitions. The technology service price is also dictated by the\nCSP based on its target marketplace dynamics, not the price sensitivity of any particular\ncustomer. In short, RFP or RFQ requirements have virtually zero effect on available CSP\ntechnology services or technology service price.\nCloud service acquisition fundamentally represents a 180e shift from traditional IT\nacquisition practices. This can not only wreck existing procurement process oversight and\ncontrol, but it will also lead to unexpected deployment challenges, cost, application design\nchanges, and failed migration projects. This pattern has repeatedly been observed across\nthe IT industry. Its root cause is the cloud solution architect's failure to address\nprocurement process differences between traditional IT service acquisition and cloud IT\nservice acquisition.\n",
      "content_length": 2821,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "Architecting for Transition\nChapter 6\n[ 109 ]\nApplication migration\nApplication screening is the first step in identifying whether a specific application is ready\nto move into the cloud. This is normally accomplished using an interview process, with the\nsystem/application owner designed to identify cloud migration readiness and the value a\nmigration could provide. As a data discovery exercise, this process will help identify\napplications for migration, while ensuring that the existing IT and security architecture is\nwell understood, and this also helps to mitigate many complications that may occur when\nexecuting a migration strategy. Interviewers should leverage a consistent set of evaluation\ntested questions that will help triage an organization's application portfolio. Responses\nshould be analysed with a focus on deciding the following:\nThe most appropriate target deployment environment, which varies from\nphysical hardware in a user-owned and -operated data center to a virtualized\nplatform, a private cloud, or a public cloud\nEach application's SPAR benefit (scalability, performance, accessibility,\nreliability)\nEach application's SOAR readiness (security, organization, architecture\nresilience).\nThis triage effort should also highlight the most influential business or mission drivers, key\nreadiness strengths, key benefit weaknesses, and key readiness weaknesses.\nAfter identifying the applications that should be moved into the cloud, a data classification\n(PII, classified information, and so on) of the information processed by these systems\nshould be completed. This should be done with input from the relevant SMEs and the\nGovernance, Risk Management, and Compliance (GRC) team. This is an important step to\nunderstand because CSPs operate on a shared-responsibility model. The CSP will provide\nsecurity of the cloud and the customer is responsible securing the information that put in\nthe cloud. Data classification will help determine what information will remain on-\npremises and what information will be moved into the cloud, and it also helps to ensure\nthat compliance requirements can be achieved.\n",
      "content_length": 2130,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "Architecting for Transition\nChapter 6\n[ 110 ]\nApplication portfolio data should then be compiled across all relevant or interrelated\ndomains. The selection of a service model (SaaS, PaaS, or IaaS) and a deployment model\n(public, community, or hybrid) should be driven by organizational goals and compliance\nneeds. It is important the think about where data will be stored, is encryption required,\nhow information will be encrypted at rest, how information will be encrypted in motion,\nand who will manage the encryption. Answers to these types of questions will inform your\nselection. Screening output should also provides data to inform long-term application\nstrategy decisions. Long-term options typically include retirement, refactoring, rebuilding,\nor lift and shift.\nApplication workloads\nApplication workload, dictated by user characteristics, represents a major factor in cloud\ncomputing solution design. Application scalability and cost efficiency are enhanced by the\nsolution architect's ability to leverage the automated provisioning and de-provisioning of\ncloud technology services. Dynamic instantiation and decommissioning of virtual machines\nbased on workload variations is often ignored or delayed when a lift and shift application\nmigration strategy is pursued. Failing to design or redesign solutions with application\nworkload variability in mind can nullify any positive return on a cloud deployment\ninvestment.\nStatic workloads\nStatic workloads show flat resource utilization over time within particular boundaries.\nApplications with static workloads are less likely to see benefit from an elastic cloud that\nuses a pay-per-use model because resource requirements are constant. Periodic workloads\nare very common in real deployments. Examples include monthly paychecks, monthly\ntelephone bills, yearly car checkups, weekly status reports, or the daily use of public\ntransport during the rush hour. Tasks occur in distinct patterns, and the customer realizes\ncost-savings from the ability to de-provision resources during non-peak times.\nOnce-in-a-lifetime workloads\nOnce-in-a-lifetime workloads are a special case of periodic workload in which the peaks of\nperiodic utilization occurs only once in a very long time frame. This peak is often known in\nadvance as it correlates to an individual event or task. Cloud elasticity is used to obtain\nnecessary IT resources. The provisioning and decommissioning of IT resources in this use\ncase can often be done manually since it can be done at a known point in time.\n",
      "content_length": 2529,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "Architecting for Transition\nChapter 6\n[ 111 ]\nUnpredictable and random workloads\nUnpredictable and random workloads are a generalization of periodic workloads as they\nrequire elasticity but are not predictable. Unplanned provisioning and de-provisioning of\nIT resources is required. The necessary provisioning and decommissioning of IT resources\nare, therefore, automated to align the resource numbers to the changing workload. Many\napplications experience a long-term change in workload that can be characterized as a\ncontinuously changing workload. This is normally seen as an ongoing continuous growth\nor decline of use. The elasticity of clouds enables applications experiencing continuously\nchanging workloads to provision or decommission resources with the same rate as the\nworkload changes.\nThe following table provides a cross-reference between the solution use cases outlined in\n$IBQUFS\u0002\u0016, Business Drivers, Metrics, and Use Cases, application workloads described here,\nand the standard operational requirements also addressed earlier. Once application user\nrequirements are established, the table can be used to develop a documented initial draft of\nyour cloud solution operational requirements:\n",
      "content_length": 1206,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "Architecting for Transition\nChapter 6\n[ 112 ]\nApplication categories\nThe more value an application or business process can exploit from the basic cloud model\nvalue proposition, the more valuable a cloud transition becomes. This fact emphasizes the\nneed to categorize enterprise applications by important organizational goals and target\ndeployment options. Several factors can be used to categorize applications. This\ncategorization often drives the selection of an appropriate deployment model for an\napplication being moved to the cloud. These factors include security privacy regulation\nneeds, unique technology requirements, agility, and elasticity needs. The process of\ncategorizing applications is unique for each enterprise, but the cloud solution architect\nshould educate, lead, and advise throughout the process:\n",
      "content_length": 821,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "Architecting for Transition\nChapter 6\n[ 113 ]\nUse of a categorization framework can form the basis for establishing a structured approach\nfor assessing a given application's overall relative cloud migration value against the ease or\ndifficulty in making that migration. It will also identify how valuable essential cloud\ncharacteristics are to the application's operational value:\nIn this example diagram, circle size represents overall cloud affinity. Cloud migration pain\nis affected by several factors, such as application dependencies, application cloud\nfriendliness, associated security and compliance requirements, and other factors. The gain\nis derived from scalability, agility, elasticity, and overall organizational cloud transition\nmotivation. Some of the most popular SaaS application categories are shown in the\nfollowing table:\nSaaS application\nDescription\nCustomer relationship\nmanagement\nAutomate marketing and track sales\nEnterprise resource management\nImprove workflow and productivity visibility\nAccounting\nAccurately organize and track finances\n",
      "content_length": 1065,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "Architecting for Transition\nChapter 6\n[ 114 ]\nProject management\nTrack project scope, requirements, progress, changes,\ncommunications, and delivery deadlines\nEmail marketing\nAutomate and optimize marketing and relationship\nbuilding\nBilling and invoicing\nReduce billing process time\nCollaboration\nImprove communications and employee productivity\nWeb hosting and electronic\ncommerce (e -commerce)\nAutomate internet-based business processes\nHuman resource management\nMore efficient scheduling, payroll automation, and\nrecruiting\nPublic sector, compliance, and EDI Monitor and enforce regulations to improve\ncompliance and communications\nIndustry vertical applications\nIndustry-specific application deployment\nFinancial transaction processing\nExchange of financial assets\nApplication dependencies\nApplication dependencies increase the difficulty associated with putting an application on\nthe cloud. They can dictate application migration order or even determine migration\nfeasibility. Critical application dependencies could include the following:\nShared communication channel\nShared architecture\nIdentity and access management\nShared data\nThe solution architect must explore and gain a consensus on the most appropriate\nresolution options, which may include the following:\nCreating and deploying a shared service layer\nReplicating the service in the cloud environment\nReplacing legacy services with an available cloud services.\n",
      "content_length": 1425,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "Architecting for Transition\nChapter 6\n[ 115 ]\nUse of APIs\nApplication APIs are the glue that connects applications. They manage the virtual\ndiscussions between users and the cloud services being consumed. APIs enable business\nagility, flexibility, and interoperability. These software modules are more than just\nconnective tissue on the web, they are business model drivers and represent organizational\ncore assets that can be reused, shared, and monetized. Using APIs, companies extend the\nreach of existing services or provide new revenue streams. In some instances, they are\nactually end products that, in turn, provide access legacy and third-party systems and data.\nInfrastructure APIs are used to provision, de-provision, and scale cloud computing\nresources. As crucial solution components, the cloud solution architecting process should\nalso consider the following:\nCreation and publication of a service endpoint as an API\nDeployment of APIs on-premises or in the cloud\nUse of versioning to control APIs throughout the solution life cycle\nManagement and monitoring of solution-related web services\nThe predominant API design styles are Simple Object Access Protocol (SOAP) and\nRepresentational State Transfer (REST).\nSOAP\nSOAP uses eXtensible Markup Language (XML), which is very complex when used for\nrequests and responses. Requests are often created manually, leading to fragile applications\ndue to the intolerance of SOAP to errors. Web Services Description Language (WSDL) is\nused with SOAP to define web service usage. With WSDL, the Integrated Development\nEnvironment (IDE) can fully automate the process. Because of this connection, the\ndifficulty in using SOAP depends on the programming language. An important SOAP\nfeature is its error handling. If a request has an error, the response embeds information that\ncan be used to correct the problem. Error reporting also has standard codes that can be used\nto automate error handling.\n",
      "content_length": 1948,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "Architecting for Transition\nChapter 6\n[ 116 ]\nREST\nREST is a lightweight SOAP alternative. It uses a simple URL and can process four different\ntasks (GET, POST, PUT, and DELETE). REST flexibility lies in its ability to return data using\nJavaScript Object Notation (JSON), Comma Separated Value (CSV), and Really Simple\nSyndication (RSS). This means output can be delivered in just about any desired parsing\nformat.\nAdvantages of SOAP and REST\nThe advantages of SOAP and REST are given in the following table:\nSOAP advantages\nREST advantages\nPlatform, language, and transport\nindependence\nNo expensive tools are required to interact\nwith the web service\nBetter support for distributed enterprise\nenvironments\nSmaller learning curve\nBetter standardization\nEfficient (SOAP uses XML for all messages,\nREST can use smaller message formats)\nSignificant pre-built extensibility sing WS*\nstandards\nFast (no extensive processing required)\nBuilt-in error handling\nCloser to other web technologies in design\nphilosophy\nMore automation with certain languages\n",
      "content_length": 1047,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "Architecting for Transition\nChapter 6\n[ 117 ]\nTechnical architecture requirements\nCloud computing services are consumed using an on-demand model. Service providers,\ntherefore, meter and monitor the consumption of every resource by every user. Every\nresource, in turn, uses specific metrics and measurement units to bill users. Most\norganizations do not measure IT usage in this way. Neither do they typically place resource\nmeasurement sensors across their IT platform. IT is usually seen as a shared cost center\nwith only the total cost and total capacity requirements tracked. This makes it extremely\ndifficult to estimate service usage rates and expected cost when an application is\ntransitioned to a CSP. Infrastructure analysis is used to get estimates of resource usage by\nparticular applications, business processes, and organizational segments. This is normally\none of the most difficult aspects of developing the business case and resultant cost-benefit\nanalysis. Responses provide insight into the economic comparison between IaaS service\noptions, managed infrastructure service options, and enterprise-owned data center options.\nLegal/regulatory/security requirements\nAs the global nature of technology continues to evolve, the complexity of adhering to both\nglobal and local laws and regulations becomes greater. The most significant trend in the US\ngovernment market is the move from security compliance to security risk management.\nRelevant guidance addressing this move is contained in the following:\nThe Federal Risk and Authorization Management Program (FedRAMP), which\nprovides a standard approach for security assessments, authorizations, and\ncontinuous monitoring of cloud computing products and services.\nThe Department of Defense (DoD) cloud computing security requirements,\nwhich extend the FedRAMP security requirements to meet the unique\nrequirements of the DoD.\nICD 503, which replaces DCID 6/3 and 6/5 and establishes intelligence\ncommunity policies for security risk management of information technology\nsystems. This includes security certification and accreditation.\nEnsuring compliance with these is challenging within a cloud computing environment\nincreases significantly. Legal, regulatory, and security requirements are used to screen\ncloud service provider options, select appropriate CSP regions, and verify necessary\nsecurity controls.\n",
      "content_length": 2373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "Architecting for Transition\nChapter 6\n[ 118 ]\nBusiness continuity and disaster recovery `\nBCDR\nCloud infrastructure has some characteristics that can be distinct advantages in realizing\nBCDR, depending on the scenario:\nRapid elasticity and on-demand self-service provides flexible infrastructure that\ncan be quickly deployed to execute an actual disaster recovery without\nunexpected ceilings.\nBroad network connectivity reduces operational risk.\nCloud infrastructure providers have a highly automated and resilient\ninfrastructure backing all offered services.\nPay-per-use can mean tremendous cost savings and no capital expenditures to\nsupport a BCDR strategy.\nScenarios that should be considered when considering cloud services are the following:\nOn-premises, cloud as BCDR: An existing, on-premises infrastructure, which\nmay or may not have a BCDR plan already, where a cloud provider is considered\nas the provider of alternative facilities should a disaster strike at the on-premises\ninfrastructure.\nCloud consumer, primary provider BCDR: The infrastructure under\nconsideration is already located at a cloud provider. The risk being considered is\nthat of a failure of part of the infrastructure of that cloud provider, for example,\none of their regions or availability zones. The business continuity strategy then\nfocuses on restoration of service or failover to another part of that same cloud\nprovider infrastructure.\nCloud consumer, alternative provider BCDR: A scenario similar the previous\none except instead of the restoration of service from the same provider, the\nservice has to be restored from a different provider. This also addresses the risk\nof complete cloud provider failure.\nDisaster recovery (DR) almost by definition requires replication. The key difference\nbetween these scenarios is where the replication happens. Business continuity (BC)\nquestions are used to identify any critical BC or DR issues that may require more detailed\nanalysis.\n",
      "content_length": 1963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "Architecting for Transition\nChapter 6\n[ 119 ]\nEconomics\nCloud service providers typically offer three options to pay for their service:\nOn-demand: You pay for what you use as you use it.\nReserved: You commit to using a specific amount of a service over a specified\nperiod.\nSpot: You use a market auction model to match price with demand.\nEach has advantages and disadvantages, but all require an understanding of operational\nrequirements and customer sensitivity to price fluctuations. Economic screening questions\nare used to gauge customer sensitivity to cloud service cost and importance of the various\neconomic payment models.\nOrganizational assessment\nDigital transformation and cloud computing migrations typically involve transitioning\nfrom a non-standardized, minimally documented environment into a highly standardized,\nrigidly documented one. This is a highly challenging transition that requires effective\norganizational governance. It may also present significant change management challenges.\nTo be successful, any corporate cloud computing transition strategy needs to be paired with\na focused training and education program. This assessment is designed to identify whether\nsuch a program is in place or, if not, identify the appropriate organizational POC.\nOrganizational governance defines organizational structures, decision rights, workflow,\nand authorization points to create a target workflow that optimally uses a business entity's\nresources in alignment with the goals and objectives of the firm. Effective governance can\nonly succeed if the organization has defined the desired outcomes and the proper metrics.\nOrganizational leadership and management must be able to articulate what the desired\noutcomes are, who is accountable and responsible for these outcomes, what the escalation\ncriteria or triggers to move a decision to the next level are, and what metrics will be used to\nmonitor that the system is delivering the desired outcomes. The entire governance process\nneeds to be continuously evaluated to determine that it provided the necessary\ntransparency and timeliness to the decision makers and adjusted accordingly.\n",
      "content_length": 2150,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "Architecting for Transition\nChapter 6\n[ 120 ]\nDuring transitions, the organization must simultaneously abandon business as usual and\nembrace the following across multiple dimensions:\nSecurity framework: Infrastructure-centric to data-centric\nApplication development: Tightly coupled to loosely coupled\nData: Mostly structured to mostly unstructured\nBusiness processes: Mostly serial to mostly parallel\nSecurity controls: Enterprise responsibility to shared responsibility\nEconomic model: Mostly CAPEX to mostly OPEX\nInfrastructure: Mostly physical to mostly virtual\nIT operations: Mostly manual to mostly automated\nTechnology operational scope: Local/regional to international/global:\nBefore embarking on any cloud transition program, the enterprise should implement a\nfocused organizational change management strategy. There needs to be broad awareness,\nunderstanding, acceptance, and commitment across the organization of what is expected\nand when is it to be delivered by implanting focused organizational change management\nprocesses and procedures. The organization should regularly ask, is the corporate culture\nchanging at the necessary rate and are all communication channels are being leveraged so that the\nright stuff is being communicated?\n",
      "content_length": 1250,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "Architecting for Transition\nChapter 6\n[ 121 ]\nSummary\nCloud computing is transformational in many ways. Cloud transformation can have a\nripple effect throughout the entire organization. The hardest part of any transformational\nstrategy is changing the hearts and minds of the people in the organization. Proper\nplanning, diligent preparation, situational awareness, and environmental control will create\na solid foundation for transformation success. Pairing solution strategy, economics,\ntechnology choices, and risk profiles with matching cloud service providers will maximize\ncloud transition value. Change management and communication plans are critical during\nany major change. Leadership continuously measures and compares accurate, relevant\nmetrics and data, gauging progress, adoption, potential risks, and the pace of cultural\nchange. If progress slows, adoption may follow. If adoption slows, project timelines may\nslide. Stay environmentally and situationally aware. Continuously monitor, measure, and\ncompare the current situation to transition goals. Preparation is key. Avoid being a\ndefeated warrior going into battle seeking a win.\n",
      "content_length": 1148,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "7\nBaseline Cloud Architectures\nCloud transitions can be difficult to begin. As discussed in $IBQUFS\u0002\u0018, Architecting for\nTransition, transitions can be difficult to design and plan, as much of the diligence now falls\non the consumer side. This change is a double-edged sword; it cuts both ways. It enables\nthe consumer to have significantly more control over designs, technical choices, economics,\nand risk. It also places the significantly more of the design and architecture burden on the\nconsumer, who may not have the level of solution design experience that many service\nproviders do.\nBaseline cloud architectures are foundational building blocks to cornerstone design ideas.\nThese common design arrangements can be used to jump-start solution efforts. Baseline\narchitectures are useful when leveraging standard cloud computing patterns. Patterns\nrepresent cloud service requirements, while baseline architectures provide useful models\nfor handling common architectural components and their associated requirements.\nEach of the following sections will build on the section previous. The baseline compute\ncomponent takes into account a web layer, application layer, and database layer, each\nhaving some level of storage. Storage attributes will change based on design requirements.\nNearly all modern designs will have web, app, and database layers in their designs.\nThis type of layering is called tiering. Most designs will have three or four tiers. Tiers are\ntypically the number of individual isolated layers between the environment entry point and\nthe destination data. As an example, a three-tier architecture has a web layer, app layer, and\ndatabase layer. A single-server architecture will have all three layers residing on the same\nvirtual or physical server.\n",
      "content_length": 1771,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 123 ]\nIn this chapter, we will cover the following topics:\nBaseline architecture types\nOSI model and layer description\nComplex architecture types\nArchitecting for hybrid clouds\nBaseline architecture types\nThe various types of baseline architectures are as follows.\nSingle server\nSingle server templates represent the use of one server, virtual or physical, that contains a\nweb server, an application, and a database. An example is the LAMP Stack (Linux,\nApache, MySQL, PHP). Single server architectures are not very common, as they have\ninherent security risks as one compromise can compromise all. These architectures are\ncommonly deployed for development work, allowing developers to quickly build\nfunctionality without having to deal with connectivity and communication issues between\ndifferent servers, potentially in different locations.\n",
      "content_length": 884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 124 ]\nSingle-site\nSingle-site architectures take the single server architecture and split all of the layers into\ntheir own compute instances, creating the three-tier architecture mentioned. With all\ncompute resources located in the same location, a single-site architecture is created. There\nare two versions of single-site architectures: non- redundant and redundant.\nNon-redundant three-tier architectures\nNon-redundant three-tier architectures are used to save on costs and resources but must\naccept a higher risk. A single failure in any component, a single point of failure, can stop\ntraffic flowing correctly into or out of the environment. This approach is commonly used\nfor development or testing environments only. The following figure shows each layer, or\ntier, as a separate server, virtual or physical. Using this type of design for production\nenvironments is not recommended:\n",
      "content_length": 930,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 125 ]\nRedundant three-tier architectures\nRedundant three-tier architectures add another set of the same components for\nredundancy. Additional design components do increase complexity, but are required if\ndesigning for failover and recovery protection. Designing redundant infrastructures\nrequires a well thought out plan for the components within each layer (horizontal scaling),\nas well as a plan for how the traffic will flow from one layer to another (vertical scaling).\n4JOHMF\u0002QPJOUT\u0002PG\u0002GBJMVSF\nIn redundant architectures, duplicate components eliminate the single point of failure\npresent when only one device or component is present in the layer. With one component in\na layer, there is only one way in and one way out. A second device adds multiple ingress\nand egress points to the design, eliminating the single point of failure associated with\nsingle-component layer designs.\n",
      "content_length": 926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 126 ]\n3FEVOEBODZ\u0002WFSTVT\u0002SFTJMJFODZ\nRedundancy and resiliency are often confused. They are related, but not interchangeable.\nRedundancy is something that is done to prevent failure, implying that it happens before\nan issue happens. Resiliency, from the word resolve, relates to how to find solutions after a\nproblem has occurred. Redundancy is before the issue. Resiliency is after. For example,\nredundant databases with replication can be utilized. Multiple components and copies of\ndata create a redundant design. If the primary side of the database pair fails, the secondary\nside will promote to primary and begin to pick up the load while the failed side self-repairs.\nThe failover and self-healing functions are resiliency. Both are related, but not\ninterchangeable.\n)PSJ[POUBM\u0002TDBMJOH\nWorking outside in, the XYZ website has a single web server. A recent outage has suddenly\nidentified available budget money for redundant components at each layer of the current\ndesign. By the way, every company is one major outage away from adding budget money\nfor redundancy plans. One web server is currently used in the design. To add redundancy,\nwe must horizontally scale the web server layer by adding additional web servers,\neliminating the single point of failure. How will traffic be passed to both servers? How does\nthe packet on the wire know which web server to go to and which path to take in and out,\nand how is all of this physically connected?\nLoad balancing is a major design component when adding redundancy to designs. A single\nload balancer will help delegate traffic across multiple servers, but a single load balancer\ncreates another single point of failure. For redundancy, two or more load balancers are\nadded to designs. Load balancers control traffic patterns. There are many interesting\nconfigurations to consider when deciding how to control and distribute traffic. Distribution\nmay relate to traffic type, content, traffic patterns, or the ability of the servers to respond to\nrequests. Load balancers help to handle traffic logically; how is the traffic handled at the\nphysical layer?\n",
      "content_length": 2147,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 127 ]\nOSI model and layer description\nThe OSI stack is a great tool when working with complex designs. Every layer in the OSI\nstack must be considered within the design and have a purposeful answer. Designs always\nstart at the physical layer, working up the stack from the bottom to the top. See the\nfollowing diagram. Many load balancers today work at all layers of the OSI stack. Back to\nthe question: how are multiple load balancers physically connected to multiple servers\ncreating multiple ingress and egress paths? Multiple switches may also be required. Today\nmany load balancers combine the port density of switches, the routing capability of routers,\nand the logical functions of load balancers, all in a single device simplifying designs and\nsaving a bit of budget money.\nThe web layer and application layers can often be collapsed into the same server. From a\nsecurity perspective, this can be an issue. If the server is compromised, both services are\npotentially compromised. Many designs collapse these two layers, as they are tightly\nintegrated, and performance can significantly increase using system bus speeds instead of\nslower network connections and additional devices.\n",
      "content_length": 1230,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 128 ]\nFrom single server designs to single site to single site redundant, each design builds on the\none previous. The following figure adds the additional components, servers, and load\nbalancers to illustrate a baseline architecture for single site designs with redundancy. The\nfollowing redundant design collapses both web and app onto the same virtual or physical\nserver. Load balancers are added to the design to delegate the load across multiple servers.\nDatabase servers are shown as primary-backup with replication between them. This\nredundant architecture can protect against issues with applications due to system\nunavailability and downtime. Resiliency considerations may include RAID configurations\nfor database drives, how databases are backed up and restored, how applications and\ndevices handle state and session information, and how databases rebuild after data or drive\nloss.\nLogical and physical designs\nDesigns can be logical or physical. It is very important to remain clear on what is\nrepresented. Logical diagrams illustrate how things logically flow through the design.\nEliminating some of the physical connections may help the viewer focus on logical flows\nthrough the design. Conversely, physical layouts may not include many of the logical\ndetails and configurations to focus the viewer on physical characteristics and attributes of\nthe design. The illustrations in this section are logical unless specifically called out as\nphysical.\nAutoscaling architecture\nA key benefit of cloud computing is the ability to consume what is needed when it is\nneeded. Autoscaling describes the ability to scale horizontally (that is, shrink or grow the\nnumber of running server instances) as application user demand changes over time.\nAutoscaling is often utilized within web/app tiers within the baseline architectures\nmentioned. In the following figure, an additional server is dynamically added based on\ndemand and threshold settings.\n",
      "content_length": 1988,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 129 ]\nLoad balancers must be preconfigured or configured dynamically to handle the new\nservers that are added.\n",
      "content_length": 152,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 130 ]\nComplex architecture types\nThe various types of complex architectures are as follows.\nMulti-data center architecture\nRedundant single site designs can often handle many of the more prevalent issues that\ncause downtime within infrastructure layers. What happens if the entire site is not\nreachable? What happens if something gets misconfigured in DNS that sends traffic the\nwrong direction? The single site is now unreachable. Traditionally, the answer to this\nchallenge has been very expensive. The redundant single site design nearly doubled the\ncost of infrastructure. For geographic redundancy, a second site is needed. This second site\neffectively doubles the budget of the first site, which already doubled when redundancy\nwas added to the first design.\nCloud solutions are dramatically changing the way we design redundancy, resiliency, and\ndisaster recovery. The cloud changes the fundamentals of base designs. For example, we\nare now able to design for low-side, base-level traffic flows, instead of designing for\nanticipated high-watermark levels. The cloud enables dramatic changes in footprint size\nand the amount of redundant infrastructure needed in single site and multiple site designs.\nThe cloud is also changing the consumption patterns for infrastructure. Some applications\ntraditionally deployed in-house have transformed to SaaS offerings, eliminating the need\nfor the associated in-house infrastructure. The reduction in single site footprints can also\nreduce the size of second site footprints, helping redundant strategies fit into budgets easier\nthan traditional deployments.\nWhen planning redundancy across multiple data centers, new design challenges need\nconsideration. How is traffic sent to one location or the other? Is one site active and one\nbackup? Are both active? How does fail-back to the primary get handled after the failure\noccurs? What changes in resiliency plans are needed? How is data synchronization handled\nbefore and after failover?\n",
      "content_length": 2026,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 131 ]\nGlobal server load balancing\nThere are many mechanisms to handle the flow of traffic between multiple sites. Nearly all\nof them rely on the manipulation of DNS information. DNS information can sometimes\ntake hours to update across the globe. If production sites must failover to redundant sites,\nwaiting hours for traffic to pass again is not an option. Global server load balancing\nenabled the configuration of pre-planned actions to take place in the event of failure. GSLB\nrequired expensive publicly accessible devices at each site. Security experts were also\nrequired as part of a successful solution to keep devices safe from continuous hacking\nattempts.\nExpensive, traditional, device-based GSLB deployments can be deployed as cloud GSLB\nservices, where GSLB is consumed as a managed service for a monthly fee. Providers are\nalso offering additional options including regional deployments and separated availability\nzones to help handle geographic diversity and failover. It is up to the consumer to decide\nthe level of redundancy and speed of failover required. Zone level redundancy is different\nthan regional deployments.\n",
      "content_length": 1179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 132 ]\nDatabase resiliency\nPrimary-secondary or master-slave database relationships are common, but have some\nchallenges when failures occur in high-transaction, heavy traffic environments. Databases\nare taking lots of requests and transactions are being written and read continuously.\nBackup processes can be taxing and time-consuming. Restoration and synchronization can\ntake significant time. Heavy demand environments can benefit from an active-active \ndatabase configuration with bi-direction replication to keep data synchronized on both\ndatabase servers. This type of design does add more complexity but also adds greater levels\nof redundancy and resiliency within a single site, or across multiple sites, depending on\nconfiguration.\n",
      "content_length": 781,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 133 ]\nCaching and databases\nContent types can also affect architectures. As an example, caching techniques can change\nthe load on database servers, load balancing design, database server sizing, storage type,\nstorage speed, how storage is handled and replicated, as well as network connectivity, and\nbandwidth requirements. Current estimates place 80%-90% of enterprise data in\nunstructured categories.\n",
      "content_length": 444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 134 ]\nAlert-based and queue-based scalable setup\nSince multiple server arrays can be attached to the same deployment, a dual scalable\narchitecture can be implemented. This delivers a scalable front-end and back-end server\nwebsite array.\n",
      "content_length": 278,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 135 ]\nHybrid cloud site architectures\nA hybrid cloud site architecture can protect your application or site redundancy by\nleveraging multiple public/private cloud infrastructures or dedicated hosted servers. This\nwill require data and infrastructure portability between selected service providers. A\nhybrid approach requires an ability to launch identically functioning servers into multiple\npublic/private clouds. This architecture can be used to avoid cloud service provider lock-in.\nIt is also used to take advantage of multiple cloud resource pools. The hybrid approach can\nbe used in both hybrid cloud and hybrid IT situations.\nScalable multi-cloud architecture\nA multi-cloud architecture, offers the flexibility of primarily hosting an application in a\nprivate cloud infrastructure, with the ability to cloudburst into a public cloud for additional\ncapacity as necessary.\n",
      "content_length": 919,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 136 ]\nFailover multi-cloud architecture\nA second cloud service provider could be used to provide business continuity for a primary\ncloud provider if the same server templates and scripts could be used to configure and\nlaunch resources into either provider. Factors to be considered when using this option\ninclude public versus private IP addresses and provider service level agreements. If there is\na problem or failure requires switching clouds, a multi-cloud architecture would make this\na relatively easy migration.\n",
      "content_length": 560,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 137 ]\nSending and receiving data securely between servers on two different cloud service\nprovider platforms can be done using a VPN wrapped around the public IP address. In this\napproach, any data transmitted between the various cloud infrastructures (except if used\nbetween private clouds) is sent over the public IP. In the following diagram, two different\nclouds are connected using an encrypted VPN:\nCloud and dedicated hosting architecture\nHybrid cloud solutions can use public and private cloud resources as a supplement for\ninternal or external data center servers. This can be used to comply with data physical\nlocation requirements. If the database cannot be transitioned to a cloud computing\nplatform, other application tiers may not have the same restrictions. In these situations,\nhybrid architecture can use a virtual private network (VPN) to implement an encrypted\ntunnel across a public IP between cloud and dedicated servers.\n",
      "content_length": 983,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 138 ]\nArchitecting for hybrid clouds\nThe various concepts of a hybrid cloud are explained in the following sections.\nHybrid user interface\nVarying user group workload interacts asynchronously with an application hosted in an\nelastic environment while the rest of the application resides in a static environment. An\napplication responds to user groups with different workload behavior. One user group\npresents a static workload, while the other user group presents periodic, once-in-a-lifetime,\nunpredictable, or continuously changing workloads. Since user group size and workload\nbehavior is unpredictable, this interface ensures that unexpected peak workloads do not\naffect application performance while each user group is handled by the most suitable\nenvironment. The user interface component serving varying workload users is hosted in an\nelastic cloud environment. Other application components that are in a static environment.\nThe user interface in the elastic cloud is integrated with the rest of the application in a\ndecoupled manner using messaging to ensure loose coupling.\n",
      "content_length": 1124,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 139 ]\nHybrid processing\nProcessing functionality with the varying workload is in an elastic cloud while the\nremainder of the application is in a static environment. A distributed application provides\nprocessing functions with different workload behavior. The user group accessing the\napplication is predictable in size but accesses the functions differently. Although most\nfunctions are used equally and experience static workload, some processing components\nexperience periodic, unpredictable, or continuously changing workloads. The processing\ncomponents with varying workloads are provisioned in an elastic cloud. Loose coupling is\nensured by asynchronously exchanging information between the hosting environments via\nmessages.\nHybrid data\nData of varying size is in an elastic cloud while the rest of an application is in a static\nenvironment. A distributed application handles data with drastically varying size. Large\namounts of data may be periodically generated and then deleted, data may increase and\ndecrease randomly, or data may display a general increase or decrease. During these\nchanges, the user number and application accesses can be static resulting in a static\nworkload on the other application components. Elastic cloud storage offerings handle data\nwith varying size that are unsuitable for static environment hosting. Data is accessed either\nby data access components hosted in the static environment or by data access components\nin the elastic environment.\nHybrid backup\nFor disaster recovery, data is periodically extracted from an application and archived in an\nelastic cloud. Requirements regarding business resiliency and business continuity are\nchallenging. There are also laws and regulations that make businesses liable to archive data\nfor audits over very long periods of time. A distributed application is in a local static\nenvironment. Data handled by stateful components is extracted periodically and replicated\nto cloud storage.\n",
      "content_length": 2005,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 140 ]\nHybrid backend\nBackend functionality is made up of data-intensive processing and data storage with\nvarying workloads is hosted in an elastic cloud while all other components reside in a static\ndata center. A distributed application provides processing with different workload\nbehaviors. Support for a mainly static workload needs to available, but some processing\ncomponents experience periodic, unpredictable, or continuously changing workloads.\nApplication components that have varying workloads should be in an elastic environment.\nThese components, however, need to access large amounts of data during execution\nmaking them very dependent on availability and timely access to data. The processing\ncomponents with varying workloads are in an elastic cloud together with the data accessed\nduring operation. Asynchronous messages exchanged from the static environment are used\nto trigger the processing components in the elastic cloud through via message-oriented\nmiddleware message queues. A static environment data access component ensures that\ndata required by elastic processing components is in storage offerings The data location\nmay then be passed to the elastic processing components via messages. Data not required\nby the backend functionality may still be stored in stateful components in the static data\ncenter.\nHybrid application functions\nSome application functions provided by user interfaces, processing, and data handling is\nexperienced varying workload and is in an elastic cloud while other application functions\nof the same type are in a static environment. Distributed application components\nexperience varying workloads on all layers of the application stack: user interface,\nprocessing, and data access. All components provide functionality to the application user\ngroup, but user groups access functionality differently. In addition to the workload\nrequirements, other issues may limit the environments to which an application component\nmay be provisioned. Application components are grouped based on similar requirements\nand are deployed into the best fitting environments. Components interdependencies are\nreduced by exchanging data with asynchronous messaging to ensure loose coupling.\nDepending on the function accessed, a load balancer seamlessly redirects user accesses to\nthe different environments.\n",
      "content_length": 2378,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 141 ]\nHybrid multimedia web application\nWebsite content is primarily provided from a static environment. Multimedia files that\ncannot be cached efficiently are provided from a large distributed elastic environment for\nhigh-performance access. A distributed application provides website access to a globally\ndistributed user group. While most of the website has static content, there is a significant\namount of multimedia content that needs to be streamed to users. Static website content is\nin a static environment where users access it. The streaming content is in an elastic cloud\nenvironment where it is accessed from a user interface component. Static content is\ndelivered to users' client software which references the multimedia content. Streaming\ncontent retrieval is often handled directly by the users' browser software.\nHybrid development environment\nA runtime environment for production is replicated and mocked in an elastic environment\nfor new applications development and testing. Applications have different runtime\nenvironment requirements during the development, testing, and production phases.\nDuring development, hardware requirements vary, so hardware resources need to be\nflexible and able to extend resources as necessary. During the test phase, diverse test\nsystems are needed in order to verify proper application functionality on various operating\nsystems or while being accessed with different client software. Large numbers of resources\nare also required for load tests. In production, other factors, such as security and\navailability are of greater importance than resource flexibility. The application production\nenvironment is simulated in the development and test environment using equivalent\naddressing, similar data, and equivalent functionality. Applications migration of is ensured\nthrough the transformation of application components or the compatibility of runtimes.\nSome testing resources are exclusively provided in the development environment to verify\nthe application behavior under different circumstances.\n",
      "content_length": 2090,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "Baseline Cloud Architectures\nChapter 7\n[ 142 ]\nEach pattern employs certain characteristics and attributes. These help solution architects\naccurately visualize interoperability, and models, and compare the impact of economics,\ntechnology choices, and potential strategies. Pattern attributes and their associated metrics\ncan also be used to models and test solutions using computing aided design tools. Aligning\npattern characteristics, attributes, and metrics with organizational requirements and goals\nwill normally lead to successful solution deployments.\nSummary\nSuccessful design requires a simultaneous balance between desired strategic, economic,\ntechnical, and risk attributes. Complex designs are not necessarily better and can introduce\nadditional risk rather than mitigate it. Defined requirements are where design starts, not\nwhere it finishes. As architectures are designed, evaluated, and compared, insight is\nrevealed. Insight often provides a feedback loop for requirements to update or change.\nUpdated requirements lead to new design scenarios and, potentially, more insight. When\nstrategy, economics, technology, and risk align, objections will subside or will be negotiated\naway. Only add design complexity if non-negotiable requirements dictate it.\n",
      "content_length": 1269,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "8\nSolution Reference\nArchitectures\nThe reference architecture summaries presented in this chapter are meant to be starting\npoints for your solution design. They outline the minimum components and processes to\naddress the topic requirement but do not recommend specific technologies or cloud vendor\nsolutions. They also cannot possibly address any unique organizational requirements or\nconcerns. Real, deployable solutions require the addition of actual enterprise requirements,\na solution architect's insight, modifications driven by the selected cloud service provider,\nand organizational team collaboration.\nThese summaries were created from complete reference architectures developed by the\nCloud Standards Customer Council (CSCC\nTM). The CSCC\nTM is dedicated to accelerating\nthe successful adoption of cloud computing. It provides users with the opportunity to drive\nrequirements into standard development organizations and deliver materials that assist\nother enterprises. Complete reference architectures are available online for zero cost at\nIUUQ\u001c\u0011\u0011XXX\u0010DMPVE\u000fDPVODJM\u0010PSH\u0011SFTPVSDF\u000fIVC\u0010IUN.\nIn this chapter, we will cover the following topics:\nApplication Security\nWeb application hosting\nPublic network\nAPI management\nE-commerce\nBig data and analytics\nBlockchain\nArchitecture for IoT\nArchitecture for hybrid integration\n",
      "content_length": 1325,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "Solution Reference Architectures\nChapter 8\n[ 144 ]\nApplication security\nThis reference architecture summary presents the key components needed to secure any\napplication or process in a cloud service provider's environment. Cloud service usage\nrequires a clear understanding of security services, components, and options. This\nknowledge is paired with a clear architecture which covers development, deployment, and\noperations, as depicted in the following diagram:\n(KIWTG\u0003\u0014\u001d\u0003#TEJKVGEVWTG\u0003HQT\u0003VJG\u0003UGEWTKV[\u0003QH\u0003ENQWF\u0003UGTXKEG\u0003UQNWVKQPU\nFigure 1 is a high-level architecture for the roles and components needed in the security\narchitecture for cloud service solutions. The solution is divided into three domains based\non the applicable network. These networks are normally separately secured: public\nnetwork, cloud provider network, and the enterprise network.\n",
      "content_length": 855,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "Solution Reference Architectures\nChapter 8\n[ 145 ]\nThe public network (typically the internet) includes the parties that interact with the cloud\nsolution, their end user devices, and the associated applications.\nFigure 1 also shows three main roles: application users, cloud administrators, and cloud\ndevelopers.\nThe enterprise network contains the existing (non-cloud) enterprise components. These are\nusually required by the cloud solution and include the user directory, the applications, and\nthe data systems.\nThe cloud provider network contains the major components of the cloud-based solution,\nrunning in cloud servicescthe cloud applications, the data services, the runtime services,\nand the infrastructure services. Security services are associated with these components (the\nnumbers correspond to the numbers in Figure 1):\nIdentity and access management: Manage identity and access for your cloud\n1.\nadministrators, application developers, and application users.\nInfrastructure security: Handle network security, secure connectivity, and secure\n2.\ncompute infrastructure.\nApplication security: Address application threats, security measures, and\n3.\nvulnerabilities.\nData security: Discover, categorize, and protect data and information assets,\n4.\nincluding protection of data at rest and in transit.\nSecure DevOps: Securely acquire, develop, deploy, and maintain cloud services,\n5.\napplications, and infrastructure.\nSecurity monitoring and vulnerability: Provide visibility into cloud\n6.\ninfrastructure, data, and applications in real time and manage security incidents.\nSecurity governance, risk, and compliance: Maintain security policy, audit, and\n7.\ncompliance measures, meeting corporate policies, solution-specific regulations,\nand governing laws.\n",
      "content_length": 1763,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "Solution Reference Architectures\nChapter 8\n[ 146 ]\nWeb application hosting\nThe web application-hosting architecture delivers web pages that contain static and\ndynamic content using HTTP or HTTPS. Static content uses standardized web page text\nwith specialized content held in document, image, video, and sound clip files. Dynamic\ncontent is created in real time based on visitor input. The response is based on the request\nand content derived from linked databases. The core component is the web application\nserver. Other components can include life cycle management, operations management, and\ngovernance:\n(KIWTG\u0003\u0015\u001d\u00039GD\u0003CRRNKECVKQP\u0003JQUVKPI\u0003ENQWF\u0003CTEJKVGEVWTG\n",
      "content_length": 660,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "Solution Reference Architectures\nChapter 8\n[ 147 ]\nPublic network\nPublic network components contain users and edge services. Users can interact with the\nweb application using various devices and systems. Edge services have the capability to\ndeliver application content and normally include the firewall, DNS server, load balancer,\nand content delivery network (CDN).\nCloud provider network components\nThe cloud provider network components are as follows:\nWeb service tier\nThe CSP normally hosts the web services tier. This tier holds the program logic for\ngenerating dynamic web content. Web and application servers can also be deployed in a\nthree-tiered design. This uses load balancers to connect separate pools of web and\napplication servers. Other components include file repository, web application servers, user\ndirectory, and cache.\nAPI management: API management presents all available service endpoints\nneeded for access to the application. Services can include security, scalability,\ncomposition, access, governance, analytics, deployment, and management.\nTransformation and connectivity: Transformation and connectivity ensures\nsecure connection to legacy enterprise systems. It can also filter, aggregate, or\nmodify data as it transitions between web components and legacy enterprise\nsystems. In this reference architecture, the transformation and connectivity\ncomponent are located between the web and enterprise tiers and can include\nenterprise data connectivity, data transformation, and enterprise secure\nconnectivity.\nEnterprise network components\nEnterprises normally host many applications that typically deliver critical business\nsolutions. This is done in conjunction with providing infrastructure support. Applications\nalso have data sources that are extracted and integrated with cloud-based services. Analysis\nis performed in the cloud, and output is delivered to on-premises systems.\n",
      "content_length": 1909,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "Solution Reference Architectures\nChapter 8\n[ 148 ]\nService tier\nThe service tier holds the enterprise applications, enterprise user directory, and enterprise\ndata. The enterprise user directory provides storage for and access to user information to\nsupport user access. Enterprise data includes metadata about the data as well as systems of\nrecord for enterprise application authentication, authorization, or profile data. Enterprise\napplications consume cloud provider data and analytics to produce results that address\nbusiness goals and objectives.\nSecurity components\nSecurity components include identity and access management, data and application\nprotection, and security intelligence.\nAPI management\nAPIs are central to any cloud computing solution so they also must be managed. This\nfunction must not only address the multiple personas associated with API delivery and use,\nbut all the different services, devices, and applications as well.\nFor in-depth information on API management, refer\nto: IUUQT\u001c\u0011\u0011TMJEFMFHFOE\u0010DPN\u0011DTDD\u000fDMPVE\u000fDVTUPNFS\u000fBSDIJUFDUVSF\u000fGPS\u000fBQJ\u000fNBOBHFNFOU@\n\u0017\u001bGD\u0019\u0016\u0012\u0012\u0013\u0019\u0014\u0015EE\u0018BF\u0016E\u001b\u0019\u0013\u001a\u0013\u0010IUNM.\nE-commerce\nThe following diagram shows the e-commerce solution across three domains: public\nnetworks, cloud service provider, and enterprise networks. The public network domain\ncontains commerce users and their e-commerce channel that supports user interaction. The\nedge services handle traffic between the pubic network and the CSP. The cloud service\nprovider can host comprehensive e-commerce capabilities, such as merchandising, location\nawareness, B2B2C commerce, payment processing, customer care, distributed order\nmanagement, supply chain management, and warehouse management.\nThe enterprise network domain represents existing enterprise systems. It includes\norganizational applications, data stores, and user directories. Results are delivered using\ntransformation and connectivity components.\n",
      "content_length": 1913,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "Solution Reference Architectures\nChapter 8\n[ 149 ]\n(KIWTG\u0003\u0016\u001d\u0003%NQWF\u0003EQORQPGPV\u0003TGNCVKQPUJKRU\u0003HQT\u0003G\u0010EQOOGTEG\nPublic network components\nThe public network contains data sources and APIs, users, and the edge services. An e-\ncommerce user accesses the commerce solutions via the cloud provider platform or\norganizational network. The channel provides a seamless, personalized experience\nindependent of the customer access mode or channel. Key capabilities in this domain\ninclude the following:\nWebsite\nMobile\nConnected devices\n",
      "content_length": 521,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "Solution Reference Architectures\nChapter 8\n[ 150 ]\nEdge services are needed to transfer data safely from the internet into the CSP and on to the\nenterprise. Edge services also support end user applications. Key capabilities include the\nfollowing:\nDomain name system server\nCDNs\nFirewall\nLoad balancers\nCloud provider components\nBy allowing direct ordering from a manufacturer, ecommerce applications have extended\nthe supplier's ability to tap into new markets and channels. Having a retailer participate as\na delivery channel for a supplier has also provided convenience to customers and has\nallowed them to reach new customers and promote their brand. Key capabilities include\nthe following:\nMobile digital and store\nProduct search and personalization\nCatalog\nOrder capture\nMarketplace\nA robust digital experience is the key to engaging customers, and the functional capabilities\nshould include the following:\nContent\nFederated search\nSocial engagement\nDigital messaging\n",
      "content_length": 973,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "Solution Reference Architectures\nChapter 8\n[ 151 ]\nA gateway is also critical because it allows smart devices to search, shop, and pay.\nCustomer care assists the customer throughout the entire transaction life cycle and across\nall commerce channels. Online customer care should be offered in real time, often through\nchat facilities, cued by user behaviors such as abandoning a shopping cart or alternating\nbetween pages multiple times. Cognitive computing and natural-language processing have\ngreatly enhanced customer care functions.\nKey capabilities in this domain include the following:\nCustomer relationship management (CRM)\nLoyalty management\nPayment processing handles payment transactions that use credit cards or electronic fund\ntransfers (EFT) from the following roles:\nMerchant\nCustomer\nMerchant payment-processing service provider\nMerchant bank, if different than payment processor\nCustomer's bank, or bank issuing credit or purchase card\nThe payment gateway, on the other hand, is the mediator between the e-commerce\ntransaction and the payment-processing service. Security requirements prohibit the direct\ninformation transmission from the website to the payment processor. Payment gateways\nare offered by payment-processing vendors or contracted from vendors who only offer a\ngateway as a service.\nDistributed order management orchestrates the workflow of orders from distribution\ncenters and warehouses through direct fulfillment at stores. It can deliver superior\ncustomer experiences across an extended supply chain network and provide flexible order\nmanagement across multiple channels. Key capabilities include the following:\nOrder management and orchestration\nGlobal inventory visibility\nReturns management\n",
      "content_length": 1728,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "Solution Reference Architectures\nChapter 8\n[ 152 ]\nSupply chain management is used to plan and manage product life cycle, supply network,\ninventory, distribution, and partner alliances. Logistics management addresses internal\nlogistics for purchasing, production, warehousing, and transportation. Key capabilities in\nthis domain include the following:\nSupply chain management\nProduct life cycle management (PLM) and manufacturing\nSourcing and procurement\nSupplier and partner data communications\nTransactional event ledgers\nTransportation management and optimization\nWarehouse management enables efficient warehouse management operations. Modern\norganizations combine warehouse management wireless networks, mobile computers,\nRadio Frequency Identification (RFID) technology, voice-picking applications, and\nbarcoding. This can fully extend an enterprise to the mobile worker, increase efficiency, and\nenhance customer service. Capabilities here include the following:\nWarehouse inventory management\nInventory optimization\nInventory\nMerchandising planning is the management of merchandise or service-marketing rights.\nThe goal is optimizing margin, gross revenue, or product shelf life. Domain key capabilities\ninclude the following:\nAssortment management\nPricing management and optimization\nProduct placement\nCommerce analytics optimizes the shopper's journey to improve sales and business\nrevenue. This component should drive the next best action at the right time and the best\nchannel. Personalization is enabled through having a comprehensive customer view and\npredictive analytics. Key capabilities in this domain include the following:\nDigital analytics\nCross-channel analytics\nSocial commerce and sentiment analytics\nMerchandise analytics and optimization\n",
      "content_length": 1763,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "Solution Reference Architectures\nChapter 8\n[ 153 ]\nThe marketing domain supports the customer from product exploration through the\npurchase decision to transaction completion by delivering personalized offers, content, and\nproduct presentations. Understanding consumer consumption and shopping behavior is\nnow key to building and growing market share. Key capabilities are the following:\nMarketing resource management (MRM)\nCampaign management\nReal-time recommendations\nData services deliver the ability to access, replicate, and synchronize data. These services\naid in the management of merchandise inventory and distributing transportation. Other\ndata services can also be used to generate and aggregate the reports from the enterprise\ndata and applications.\nThe business performance component delivers important alerts, metrics, and Key\nPerformance Indicators (KPIs) used to monitor commerce activities, tracking progress\nagainst goals, and adjusting offerings in response to market variations and demand. Data is\nusually displayed using dashboards that have been tailored for specific management roles.\nCommerce analytics and data services support real-time visibility of customer activity and\nprovide the ability to drill down to individual transactions.\nRetailers typically rely on a few fundamental metrics to provide an accurate view of their\nperformance. These retail KPIs are as follows:\nNumber of customers interacting in a store or via a website\nConversion rateschow many store or website visitors actually make a purchase\nAverage sales value of items purchased\nThe size of a shopping basket\nGross margin\nThe transformation and connectivity component provides secure connections to enterprise\nsystems. It also provides the ability to filter, aggregate, modify, or reformat data. Key\ncapabilities in this domain include the following:\nEnterprise secure connectivity\nData transformations\nEnterprise data connectivity\nExtract, transform, and load\n",
      "content_length": 1956,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "Solution Reference Architectures\nChapter 8\n[ 154 ]\nEnterprise network components\nEnterprise network components support on-premises systems and users. Key domain\ncapabilities include the following:\nIn-store\nCall center\nEnterprise applications are important data sources in commerce solutions. Enterprise\napplications use cloud services and host legacy applications. Three key applications are the\nfollowing:\nFinance\nHuman resources\nContract management\nEnterprise data\nThe enterprise data component hosts applications that deliver critical business solutions\nand their supporting infrastructure. These applications are key data sources that are\nextracted and integrated with analytics services. Key capabilities in this domain include the\nfollowing:\nReference data\nTransactional data\nActivity/big data\nOperation master data\nThe enterprise user directory provides user profile access for both the cloud and enterprise\nusers. A user profile provides a login account and access control lists. The security services\nand edge services use this directory to control access to the enterprise network and services\nor to cloud provider services.\n",
      "content_length": 1135,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "Solution Reference Architectures\nChapter 8\n[ 155 ]\nSecurity\nSecurity services enable identity and access management, protection of data and\napplications, and actionable security intelligence across cloud and enterprise environments.\nThey use the catalog and user directory to understand the location and classification of the\ndata they are protecting. Key capabilities in this domain include the following:\nIdentity and access management\nApplication and data protection\nData encryption\nInfrastructure and network protection\nApplication security\nData activity monitoring\nData lineage\nSecurity intelligence\nMobile\nThe architectural elements described in this summary are used to instantiate mobile hosting\nenvironments with cloud service providers. Mobile applications have time-variable usage\npatterns that are well supported by the scalability and elasticity characteristics of cloud\ncomputing. Mobile applications also tend to make use of server-side data.\nThe frequency and volume of data access common with mobile apps can sometimes be\ndifficult for traditional enterprise systems. Elastic provisioning and support of application-\nspecific databases is an important and relevant cloud-computing capability. Using\napplication-specific databases can also reduce the need to access enterprise systems and the\nassociated resources.\nMobile architecture components\nFigure 1 illustrates the high-level architecture of a mobile cloud solution. The architecture\nhas four tiers:\nMobile computing devices\nPublic network that connects a device to the cloud services\nProvider cloud environment that hosts the necessary services\n",
      "content_length": 1618,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "Solution Reference Architectures\nChapter 8\n[ 156 ]\nThe following diagram depicts an enterprise network that contains legacy enterprise\napplications, services, and data:\n(KIWTG\u0003\u0017\u001d\u0003%NQWF\u0003EWUVQOGT\u0003OQDKNG\u0003CTEJKVGEVWTG\n",
      "content_length": 214,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "Solution Reference Architectures\nChapter 8\n[ 157 ]\nMobile device components\nMobile applications are the core vehicle for user engagement with services on mobile\ndevices. Mobile applications communicate with backend services via APIs, normally based\non REST interfaces. The two key mobile app components are the following:\nVendor frameworks\nEnterprise software development kits (SDKs)\nManagement agents apply enterprise policies. The agent is the SDK component that stores,\nenforces, and manages policies on the device. Offline capabilities give the application the\nability to store and sync data securely on devices. A mobile app may use offline capabilities\nto access and store secure data.\nPublic network Components\nEdge services connect the mobile device and its applications to the mobile gateway using\nWi-Fi or mobile provider networks. These include the following:\nDomain name system server\nFirewall\nLoad balancers\nCDNs\nThe mobile provider network owns or controls the elements necessary to sell and deliver\nservices to an end user. These typically include radio spectrum (IUUQ\u001c\u0011\u0011FO\u0010XJLJQFEJB\u0010\nPSH\u0011XJLJ\u00113BEJP@TQFDUSVN) allocation, wireless network (IUUQ\u001c\u0011\u0011FO\u0010XJLJQFEJB\u0010PSH\u0011XJLJ\u0011\n8JSFMFTT@OFUXPSL) infrastructure, back haul (IUUQ\u001c\u0011\u0011FO\u0010XJLJQFEJB\u0010PSH\u0011XJLJ\u0011#BDLIBVM@\n\u0007\u0014\u001aUFMFDPNNVOJDBUJPOT\u0007\u0014\u001b) infrastructure, billing, customer care, provisioning (IUUQ\u001c\u0011\u0011\nFO\u0010XJLJQFEJB\u0010PSH\u0011XJLJ\u00111SPWJTJPOJOH) computer systems, and marketing and repair\norganizations. \n",
      "content_length": 1452,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "Solution Reference Architectures\nChapter 8\n[ 158 ]\nProvider cloud service components\nThe mobile gateway is the entry point from a mobile application to the mobile-specific\nsolution services. It may also use data services and the enterprise user directory. A mobile\ngateway can be implemented using a common gateway across all channels into an API\necosystem. It provides the following:\nAuthentication/authorization\nPolicy enforcement\nAPI/invocation analytics\nAPI/reverse proxy\nThe mobile backend delivers runtime services to mobile applications in implementing\nserver-side logic, maintaining data, and using mobile services. It provides an environment\nto run application logic and APIs. Here, application logic can communicate with the\nenterprise network and other applications that reside outside of the service provider. It\nprovides the following:\nApplication logic/API implementation\nMobile app operational analytics\nPush notifications\nLocation services\nMobile data sync\nMobile app security\nMobile device management (MDM) manages mobile devices and provides services to\ntrack enterprise-owned devices. It also manages devices that connect to corporate networks.\nMDM provides the following:\nEnterprise app distribution\nMobile device security\nDevice management\nDevice analytics\n",
      "content_length": 1278,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "Solution Reference Architectures\nChapter 8\n[ 159 ]\nMobile business applications are the enterprise- or industry-specific capabilities needed to\nconduct business on the mobile devices. These can provide a gateway to enterprise\napplications and data, and may include analytics components that track usage. They can\ninclude the following:\nProximity services and analytics\nCampaign management\nBusiness analytics and reporting\nWorkflow/rules\nAPI management advertises available service endpoints and provides API discovery,\ncatalogs, APIs that connect to management capabilities, and service implementations, such\nas API versioning. The capabilities are the following:\nAPI discovery/documentation\nManagement\nData services enable data to be stored and accessed in a form suitable for rapid access. This\nmay include extracts of enterprise data. Data services can include the following:\nMobile app data/NoSQL\nFile repositories\nCaches\nSecurity services ensure that only authorized users are able to access mobile cloud services.\nThis also protects the data and enables the visibility required to have actionable security\nintelligence across all environments. Components include the following:\nIdentity and access management\nData and application protection\nSecurity intelligence\nEnterprise transformation and connectivity\nEnterprise security connectivity\nTransformation\n",
      "content_length": 1360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "Solution Reference Architectures\nChapter 8\n[ 160 ]\nEnterprise network components\nEnterprise network components provide backend connectivity to enterprise business\nservices and include the following:\nEnterprise user directory\nEnterprise data\nEnterprise applications\nEnterprise social collaboration\nThe enterprise social collaboration architecture addresses services that support an\nenterprise social platform. This design also includes the internal and external extension\npoints needed for data and services integration. These capabilities can be applied in\nmodules. The interfaces between the social collaboration platform and on-premises systems\nare important when defining the final system architecture.\nCloud customer reference architecture for\nenterprise social collaboration\nThe cloud customer reference architecture for enterprise social collaboration is explained in\nthe following sections.\n",
      "content_length": 898,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "Solution Reference Architectures\nChapter 8\n[ 161 ]\nArchitecture Overview\n(KIWTG\u0018\u001d\u0003'NGOGPVU\u0003QH\u0003GPVGTRTKUG\u0003UQEKCN\u0003EQNNCDQTCVKQP\nUser network\nThe user network allows end users to interact with cloud provider services. These services\nare classified as desktop (client), mobile, and web applications. The end user enters requests\nfor enterprise social collaboration services and receives results after processing has been\ncompleted. Services can also include end users' interactions. The key focus is social network\ninteraction between the following:\nWeb applications (via web browser)\nMobile application\nDesktop rich client\n",
      "content_length": 620,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "Solution Reference Architectures\nChapter 8\n[ 162 ]\nService consumer\nEnd users may not always interact directly with social services through the user network.\nThey could also consume a service through other application interfaces. A service consumer\ndescribes this as a pattern where an end user consumes and contributes to the social\nservices indirectly through the following patterns:\nIntegrated digital experience, which is integrated capabilities that deliver an\nengaging, personalized, relevant, and meaningful digital presence to the user.\nKey capabilities may include the following:\nContent\nDigital messaging\nSocial engagement\nFederated search\nPersonalization\nAnalytics\nPeer cloud services used for core business processes or to meet just-in-time\nrequirements may also be required consume content, services, and interfaces\nfrom the social services that could include the following:\nSoftware-as-a-Service (SaaS)\nCloud services (API)\nPlatform-as-a-Service (PaaS)\nProvider network\nProvider cloud components include the following capabilities.\nEdge services deliver connectivity options to end users and the capabilities needed to allow\nthe safe flow of data between the internet, the provider cloud, and the enterprise.\nCapabilities in this domain include the following:\nDomain Name System (DNS)\nCDNs\nFirewall\nLoad balancers\n",
      "content_length": 1328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "Solution Reference Architectures\nChapter 8\n[ 163 ]\nEnterprise social services represent a collaborative information exchange using secure\nsocial applications. This capability can blend an integrated user experience that is infused\ninto business processes, integrated with other applications, and aggregated with other\nexperiences. Key capabilities in this domain include the following:\nNetworking\nCommunities\nFile sync\nLive collaboration\nMessaging\nUser directory\nAs part of a social implementation, peer services may be consumed in order to deliver\nfunctionality provided by an external solution. These can also be integrated into social\nservices and can be hosted within the service provider cloud. Peer services rely on the\ncloud service provider's cloud governance and security models and they can deliver the\nfollowing:\nExtended capabilities: Functional experiences integrated into defined extension\npoints of the enterprise social services experience\nEnhanced experiences: Additional tools and extensions to the existing enterprise\nsocial service\nFoundation services: Enhance the underlying functions of the social service\nThe information governance component assures enforcement of organizational policies by\nfocusing on procedures that govern access to capabilities and information sharing. These\ntypically include the following:\nSign-on/on-boarding approval process\nLegal compliance\nRegulatory compliance (that is, PII, PCI, HIPPA, FINRA, FedRAMP, and so on)\nAudit reporting\nData loss protection\nCorporate policies\n",
      "content_length": 1523,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "Solution Reference Architectures\nChapter 8\n[ 164 ]\nSecurity\nAs an integrated suite, the enterprise social collaboration may have some unique security\nconsiderations that need to be addressed. Some of the more prevalent of these are as\nfollows.\nAuthentication to the service is needed to ensure that only authorized users have access to\ndata, tools, and applications, while simultaneously blocking unauthorized access.\nSynchronizing enterprise user directories is beneficial in extending the on-premises\nenvironment to the cloud. The enterprise social services facilitate this by providing the\nfollowing:\nOn-/off-boarding of users\nUser bulk provisioning and updates\nProvisioning of user through an administrative tool\nFederated Identity Management uses Single Sign-On (SSO) to protect the transfer of user\ncredentials across networks. Using SSO, authorized users can use different applications\nwithout additional authentication.\nSecurity Assertion Markup Language (SAML) is used to facilitate SSO with other parties\nor enterprise directories. SAML is a widely used standard that leverages signed assertion\ndocuments instead of passwords as identity credentials. Customers maintain passwords\ninternally for web application resources which help organizations do the following:\nManage password requirements.\nManage two-factor authentication requirements.\nSet password change intervals.\nUse open authorization (OAUTH), which supports web applications, desktop\napplications, and third-party extensions. This is an open source methodology for\nAPI authorization.\n",
      "content_length": 1555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "Solution Reference Architectures\nChapter 8\n[ 165 ]\nData security ensures only authorized users have secure access to customer data. This\nrequires protection of the relevant data against service vulnerabilities and physical breach\nof data centers. Security requires the layered use of a combination of technology coupled\nwith standard CSP processes and procedures. These can include the following:\nPlatform and process\nSecurity checklist against every release\nSecurity compliance with ongoing automated health checks\nData center\nRedundancy: Redundant systems to prevent a single point of failure in providing\nservices, including application, power, network, and so on.\nMonitoring of the physical environment, which includes the logging of staff\nactivities\nAccess controls and fire-prevention systems\nNetwork and infrastructure defenses\nLayered firewall infrastructure\nDeployed network intrusion detection\nProcess for people\nSeparation of duty definitions\nSegregation of activities, including personnel with change access to the code base\nand those with operational configuration control\nCode reviews prior to deployment\nRegular ethical hacking penetration testing\nAudit logs and analysis of security-related events\nData privacy and data ownership policies\nEncryption and email security\nData in transit\nData at rest\nReal-time antivirus at application and server levels\nAnti-spam protection on email messages\n",
      "content_length": 1406,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "Solution Reference Architectures\nChapter 8\n[ 166 ]\nTransformation and connectivity ensures secure connections to backend enterprise systems.\nThis also enables data filtering, aggregation, modification, or reformatting. Key capabilities\nin this domain include the following:\nEnterprise secure connectivity\nTransformations\nEnterprise data connectivity\nExtract, transform, and load\nEnterprise network\nThe enterprise network contains the on-premises systems components. While integration\nbetween the enterprise social services and enterprise applications may not be necessary,\nsome use cases may need it. Services include the following:\nUser directory\nEnterprise data\nEnterprise applications are existing applications that accomplish business goals and\nobjectives. These may also need to interact with cloud services. Organizational email is a\ncommon example.\nBig data and analytics\nBig data analytics (BDA) is used to build competitive advantage, drive innovations, and\nenhance revenue. This capability offers a cost-effective cloud-based solution for data\nanalytics. As big data grows in importance, organizations are striving to derive meaningful\ninsight from this data. This is crucial to enabling an ability to respond to real business needs\nin a timely manner.\nThe following diagram illustrates a simplified enterprise cloud architecture for a BDA\nenvironment and contains three network zones: public, cloud service provider, and\nenterprise:\n",
      "content_length": 1444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "Solution Reference Architectures\nChapter 8\n[ 167 ]\n(KIWTG\u0003\u0019\u001d\u0003$&#\u0003UQNWVKQPU\u0003KP\u0003VJG\u0003ENQWF\nThis architecture is similar to that of a data lake deployment. Both structured and non-\nstructured data are used as sources. Data is then staged and transformed by data\nintegration and stream computing engines, after which it is stored in various repositories.\nThe data may also be augmented, transformed, correlated, and summarized until it is\nfinally made available to consumers through APIs. Cognitive computing technologies such\nas machine learning and natural language processing can also be used to automate\ningestion, integration, discovery, and exploration.\nThis architectural approach is applicable to the entire analytics life cycle and can be applied\nas a data lake solution for a DevOps environment. The latter is achieved by adding\nmetadata and semantic definitions to the enterprise data repository descriptions that are\nstored in a service catalog. These catalog entries are then augmented with governance\nclassifications, rules, and policies that automate data management as it flows in, out, and\nthrough the data lake. The following sections give a summary of each of the required\ncomponents.\n",
      "content_length": 1199,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "Solution Reference Architectures\nChapter 8\n[ 168 ]\nPublic network components\nThe public network contains cloud users, SaaS applications, data sources, and edge\nservices.\nA cloud user connects to the analytics cloud solution via the network. Human and non-\nhuman users may represent one or more of the following roles:\nKnowledge worker and citizen analyst\nData scientist\nApplication developer\nData engineer\nChief Data Officer (CDO)\nAll the different personas have the following common characteristics:\nSelf-service is desired.\nThey require access to sometime large volumes of data needed to accomplish an\nanalytical task with associated data quality and provenance metrics.\nThey require multiple tools and capabilities that may be open-source and\nconsumed from on-demand services.\nCollaboration is required.\nData sources can be external, public, and varied, with multiple information sources\ncontained within a typical big-data system. High velocity, volume, variety, and data\ninconsistency are often the norm. Edge analytics services may also be required. Data\nsources normally include the following:\nMachine and sensor\nImage and video\nSocial\nInternet datasets\nWeather data\nThird party\nEdge services allow data to flow safely and securely from the internet into the data\nanalytics processing system. The data flow may also require domain name system (DNS)\nservers, CDNs, firewalls, and load balancer services before entering the cloud service\nprovider's data integration or data-streaming service points.\n",
      "content_length": 1505,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "Solution Reference Architectures\nChapter 8\n[ 169 ]\nProvider cloud components\nThe cloud service provider delivers the analytics solution and hosts the required\ncomponents. These functions are used to prepare data for analytics, storage, and results\nprocessing. Cloud service provider elements include the following:\nAPI management\nData repositories\nStreaming computing\nSaaS applications\nCognitive assisted data integration\nCognitive analytics discovery and exploration\nTransformation and connectivity\nCognitive actionable analytics\nThe data access component is used to express the many capabilities for interacting with the\ndata repositories. This serves customer data access needs and includes the following\nservices:\nData access\nData virtualization\nData federation\nOpen APIs\nStream processing is used to ingest and process large volumes of highly dynamic, time-\nsensitive, and continuous data streams from inputs such as sensors, messaging systems,\nand real-time feeds. The traditional store-and-pull data processing model is not usable for\nmeeting low-latency or real-time streaming applications. Capabilities include the following:\nStreaming analytics\nComplex Event Processing (CEP)\nData enrichment\nReal-time ingestion\n",
      "content_length": 1222,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "Solution Reference Architectures\nChapter 8\n[ 170 ]\nThe cognitive-assisted data integration component deals with the capture, qualification,\nprocessing, and movement of data into analytical data lake repositories. Here, it is shared\nwith the discovery and exploration and actionable insights components using the data\naccess component. Various cognitive technologies such as machine learning and natural\nlanguage processing can be used to automate data ingestion and integration. Data\nintegration capabilities include the following:\nBatch ingestion\nChange data capture\nDocument interpretation and classification\nData quality analysis\nThe data repositories are a set of secure locations used to store data prior to it being\nconsumed by analytics tools and end users. The repositories are the core of the analytics\nenvironment. Operational and transactional data stores (such as OLTP, ECM, and so on) are\nnot part of this component. They are help within the data sources component. Data\nrepository types include the following:\nLanding zone and data archive\nHistory\nDeep and exploratory analytics\nSandboxes\nData warehouses and data marts\nPredictive analytics\nCognitive analytics discovery and exploration enables end users to collaborate about and\neasily interact with complicated data repositories using modern data science techniques.\nThey also enable an ability to semantically search across both structured and unstructured\ncontent in order to glean new insights and obtain a complete data ontology view.\nFunctionally, these provide the following:\nData science\nSearch and survey / shopping for data\nThe cognitive actionable insight component cohesively analyzes data from multiple sources\nin order to derive meaningful and actionable insight for the business. Techniques used\nwithin this function include the following:\nVisualization and storyboarding\nReporting, analysis, and content analytics\nDecision management\n",
      "content_length": 1915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "Solution Reference Architectures\nChapter 8\n[ 171 ]\nPredictive analytics and modeling\nCognitive analytics\nInsight as a Service\nCloud service provider SaaS applications are often used to enable, manage or augment the\nfollowing:\nCustomer experience\nNew business models\nFinancial performance\nRisk\nFraud and preparation\nIT economics\nThe transformation and connectivity component ensures secure connections to backend\nenterprise systems while also enabling data filtering, aggregation, modification, or re-\nformatting. Capabilities include the following:\nEnterprise security connectivity\nTransformations\nEnterprise data connectivity\nEnterprise network\nThe enterprise network is where the on-premises systems and users are located and\nincludes users and applications. Enterprise data is also included and holds metadata on the\ndata as well as systems of record for enterprise applications. It may flow directly to data\nintegration or the data repositories and includes the following:\nReference data\nMaster data\nTransactional data\nApplication data\nLog data\nEnterprise content data\nHistorical data\nArchived data\n",
      "content_length": 1103,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "Solution Reference Architectures\nChapter 8\n[ 172 ]\nThe enterprise user directory contains the user profiles for both the cloud and enterprise\nusers. A user profile provides a login account and access control lists. Security and edge\nservices use this to manage data access.\nUbiquitous services that interact across the entire environment include the following.\nInformation management and governance components maintain a trusted, standardized,\nand accurate view of critical business data and include the following:\nData life cycle management\nMaster and entity data\nReference data\nData catalog\nData models\nData quality rules\nSecurity\nThe security component protects the data and delivers the ability to mask/hide data at a\ngranular level and the following capabilities:\nData security\nIdentity and access management\nInfrastructure security\nApplication security\nSecure DevOps\nSecurity monitoring and intelligence\nSecurity governance\nSystem management refers to all activities performed to plan, design, deliver, operate, and\ncontrol IT and cloud-based services typically addressed by service level agreements\n(SLAs).\n",
      "content_length": 1114,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "Solution Reference Architectures\nChapter 8\n[ 173 ]\nBlockchain\nBlockchain technology features an immutable distributed ledger accessed across a\ndecentralized cryptographically secured network. This architecture allows for the sharing\nof an electronic ledger, through peer-to-peer replication. This ledger is updated every time\na block of transactions is committed. This technology can radically alter the way business is\nconducted and how transactions are processed. With blockchain technology, participants\ncan engage in transparent business transactions across geographical boundaries.\nFrom a business perspective, a blockchain is an exchange network that facilitates transfer of\nvalue, assets, or other entities between willing and mutually agreeing participants, ensuring\nprivacy and control of data to stakeholders.\nFrom a legal perspective, blockchain ledger transactions are validated, indisputable\ntransactions, which do not require intermediaries or trusted third-party legal entities.\nFrom a technical perspective, a blockchain is a replicated, distributed ledger of transactions\nwith ledger entries referencing other data stores. Cryptography ensures that network\nparticipants see only the parts of the ledger that are relevant to them, and that transactions\nare secure, authenticated, and verifiable, in the context of permissioned business\nblockchains.\nBlockchain Reference Architecture Capabilities\nThe following diagram presents the typical node capabilities needed to participate in a\nblockchain architecture and is depicted across three networkscpublic, cloud, and\nenterprise. The capability locations are representative of industry best practice, but any\ncapability can be implemented in any network based on the solution needs:\n",
      "content_length": 1746,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "Solution Reference Architectures\nChapter 8\n[ 174 ]\n(KIWTG\u0003\u001a\u001d\u0003$NQEMEJCKP\u0003TGHGTGPEG\u0003CTEJKVGEVWTG\u0003ECRCDKNKVKGU\nPublic network\nThe public network contains the wide-area networks peer cloud systems, and edge services.\nEdge services allow data to flow securely and safely from the network into the cloud\nservice provider and onto the enterprise. Edge services support end user applications and\ninclude the following:\nDomain name system server (DNSS)\nCDNs\nFirewall\nLoad balancers\n",
      "content_length": 473,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "Solution Reference Architectures\nChapter 8\n[ 175 ]\nUsers are the blockchain participants who create and distribute blockchain applications.\nThose performing operations using the blockchain may include the following:\nDevelopers\nAdministrators\nOperators\nAuditors\nBusiness users\nCloud network\nBlockchain applications present business capabilities to blockchain system end users.\nApplications may also serve other users who have different roles. Blockchain applications\ncan be web services or end user device(s) applications, or connected to server-side\napplication services. These applications and services interface using the platform APIs. The\napplications may also have access to other resources such as databases if needed to\nimplement capabilities.\nAPI management capabilities publish catalogs and update APIs, enabling developers and\nend users to leverage discovery and reuse of existing data, analytics, and services to rapidly\nassemble solutions. Blockchain applications interface with the blockchain network by using\nthe blockchain programming interfaces.\nThe blockchain platform supports essential capabilities via a blockchain network node or\nenterprise. Although each blockchain platform is implemented differently, core capabilities\nthat should be considered are the following:\nConsensus\nLedger\nMembership services\nTransactions\nEvent distribution\nCommunication protocol\nCryptographic services\nSmart contract\nSecure runtime environment\n",
      "content_length": 1445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "Solution Reference Architectures\nChapter 8\n[ 176 ]\nTypical system integration methods include API adapters and an Enterprise Service Bus\n(ESB) connection between the blockchain platform and the organization's internal systems.\nTransformation and connectivity capability enables secure connections to backend\nenterprise systems. It also provides filtering, aggregation, or data modification or data re-\nformatting as it moves between the cloud, blockchain components, and enterprise systems.\nThis component includes the following capabilities:\nEnterprise secure connectivity\nTransformation\nEnterprise data connectivity\nThe enterprise network provides the enterprise user directory, enterprise applications, and\nenterprise data. Enterprise data includes metadata as well as systems of record for\nenterprise applications. Enterprise data relating to blockchain includes the following:\nTransactional data\nApplication data\nLog data\nBlockchain services\nBlockchain foundational services include the following:\nGovernance\nSecurity\nMonitoring and intelligence\nThe blockchain network management component provides visibility across all blockchain\nnetwork operations. This visibility includes business process, performance, and capacity\ndata metrics. It also delivers the management interface for changing configurations and\nother parameters.\n",
      "content_length": 1332,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "Solution Reference Architectures\nChapter 8\n[ 177 ]\nOther important blockchain concepts include permissions options:\nPermissionless networks are open to any participant. Transactions are verified\nagainst the pre-existing network rules. Any participant, even those that are\nanonymous, can view ledger transactions.\nPermissioned networks are limited to participants within a given business\nnetwork. Participants are only allowed to view transactions relevant to them and\ncan only perform operations for which they are permissioned.\nWhen using blockchain, only a small amount of the transaction data is stored directly in the\nblockchain ledger. Other transaction is stored separately but is referenced by the entry. This\napproach avoids overwhelming the blockchain ledger with large data amounts. Storage\noptions include the following:\nLedger storage\nData storage\nBlockchain interaction options are varied and include the following:\nCommand line interface (CLI)\nClient SDK\nSoftware Development Kit (SDK)\nArchitecture for IoT\nThe IoT links physical entities (things) with information technology systems in order to\nderive information. This information is used to drive multiple applications and services.\nSince IoT covers applications that integrate systems from traditionally different\ncommunities, they must have architectures that are capable of accommodating many\nunique requirements.\n",
      "content_length": 1384,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "Solution Reference Architectures\nChapter 8\n[ 178 ]\nIoT systems include sensors for gathering information about objects and human activities.\nThey can also monitor actuators acting on other physical objects. Unique IoT architecture\nimplementation aspects are shown in the following table:\nArchitecture aspects\nDescription\nScalability\nThe numbers of sensors and actuators connected to the system, the\nnetworks which connect them together, the amount of data\nassociated with the system, data speed of movement, and the\namount of required processing power.\nBig data\nThe ability to gather new insights from mining existing data.\nCloud computing\nThe use of large amounts of resources in terms of data storage and\nscalable processing.\nReal time\nReal-time data flow support and an ability to produce timely\nresponses based on a continuous stream of events. There is also a\nparallel need to detected and avoid the use of corrupted data.\nHighly distributed\nWidely distributed devices, systems and data.\nHeterogeneous\nsystems\nHeterogeneous set of devices that include sensors and actuators,\ntypes of networks, and variety of processing components.\nSecurity and privacy\nData protection combined with significant data privacy protection.\nCompliance\nRegulatory compliance across specific industries, sectors, and\nverticals.\nIntegration\nAbility to connect to existing operational technology systems such\nas factory systems, building control systems, and other physical\nmanagement systems.\n",
      "content_length": 1474,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "Solution Reference Architectures\nChapter 8\n[ 179 ]\nThe following diagram shows the capabilities and relationships for supporting IoT using\ncloud computing:\n(KIWTG\u0003\u001b\u001d\u0003%NQWF\u0003EQORQPGPVU\u0003HQT\u0003VJG\u0003+Q6\nThe cloud components of IoT architecture are normally positioned in a three-tier\narchitecture composed of edge, platform, and enterprise tiers.\nEdge tier\nProximity networks and public networks are in the edge tier. Here, data is received and\ntransmitted to user devices. Data flows through the IoT gateway or through edge services\ninto the CSP.\n",
      "content_length": 540,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "Solution Reference Architectures\nChapter 8\n[ 180 ]\nThe cloud service provider in in the platform tier. The provider receives, processes, and\nanalyzes data flows. The CSP also provides API management and visualization and can\ninitiate control commands.\nThe enterprise network represents the enterprise tier, which is comprised of enterprise\ndata, enterprise user directory, and enterprise applications.\nAn IoT system uses application logic and control logic across many locations, depending on\ntimescales and datasets. Some code may actually execute directly on the IoT devices or in\nthe gateways. Edge computing or fog computing is applied to the case where code executes in\nthe IoT gateways or the devices.\nIoT users and end user applications are in the user layer. The proximity network has all\nphysical entities that interact with IoT system physical entities. The physical entity is the\nsubject of sensor measurements or actuator behavior.\nDevices contain sensor and actuator. The attached network connection enables interaction\nwith the extended IoT system. The device may also be the physical entity being monitored\nby the sensors. Other important components include sensor/actuator, agent, and firmware.\nThe network connection provides connectivity from the device to the IoT system. This\ndevice often has low power and low range to reduce power requirements. Alternative\ncommunication mechanisms can include Bluetooth, Bluetooth Low Energy (BTLE), Wi-Fi\nor wide area networking using 2G, 3G, and 4G LTE. The user interface supports user\ninteraction with applications, agents, actuators, and sensors.\nThe IoT gateway connects devices to the public network. If the devices have limited\nnetwork connectivity, the local IoT gateway enables the needed communications. The IoT\ngateway can also filter and react to data. The IoT gateway contains the following:\nApp logic\nAnalytics\nAgent\nDevice data store\n",
      "content_length": 1906,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "Solution Reference Architectures\nChapter 8\n[ 181 ]\nPublic network\nThe public network contains the wide area networks, other cloud systems, and edge\nservices. Large IoT systems may combine a series of smaller IoT systems that each address a\nspecific part of the solution. These systems of systems include connections between other\nclouds.\nEdge services enable secure data flow into the CSP and onto the enterprise network. They\ninclude the following:\nDomain name system server\nCDNs\nFirewall\nLoad balancers\nCloud service provider\nThe CSP delivers core IoT applications and services. These can include data storage,\nanalytics, IoT system process management, and data visualizations. IoT transformations\nand connectivity delivers secure connectivity between all IoT devices. This component\nmust handle and transform high volumes of messages and route them to the correct\nsolution components. The transformation and connectivity component includes the\nfollowing:\nSecure connectivity\nScalable messaging\nScalable transformation\nThe application logic holds the core application components that co-ordinate the IoT device\ndata handling and other services that support user applications. An event-based\nprogramming model is often used. Application logic can include workflow and control\nlogic.\nVisualization helps users explore and interact with. Visualization capabilities include the\nfollowing:\nEnd user UI\nAdmin UI\nDashboard\n",
      "content_length": 1418,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "Solution Reference Architectures\nChapter 8\n[ 182 ]\nAnalytics is used to discover and communicate meaningful IoT data information patterns.\nThese patterns are used to describe, predict, and improve business performance. IoT\ncapabilities include the following:\nAnalytics data repository\nCognitive\nActionable insight\nStreaming computing\nA critical component is the device data store. This stores data from the IoT devices so that it\ncan be integrated with other processes and applications. Devices can generate large\namounts of real-time data which requires an elastic and scalable device data store. Device\nmanagement is used to efficiently manage and connect devices securely and reliably to the\ncloud. Device management also contains provisioning, remote administration, software\nupdating, device remote control, and device monitoring.\nTransformation and connectivity enables secure connections to enterprise systems and the\nability to filter, aggregate, or modify data or its format as it moves between the cloud and\nIoT system components. The transformation and connectivity component includes the\nfollowing:\nEnterprise secure connectivity\nTransformation\nEnterprise data connectivity\nEnterprise network\nThe enterprise network hosts business-specific enterprise applications. These include\nenterprise data, user directory, and applications. Key IoT applications might include\ncustomer experience, financial performance, or operations and fraud.\nSecurity\nSecurity and privacy in IoT deployments always need to address both Information\nTechnology (IT), security, and Operations Technology (OT) security elements. The level of\nattention to security varies based on the application environment, business pattern, and risk\nassessment.\n",
      "content_length": 1731,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "Solution Reference Architectures\nChapter 8\n[ 183 ]\nThere are many challenges in securing an IoT solution. Use of oversight and procedures is\nnecessary to ensure that there is a means and mechanism for addressing new vulnerabilities\nand threats. An important difference in IoT systems is that exploits and failures have the\npotential to cause serious harm to humans, property, and the environment. Additionally,\nequipment is often installed in locations where change or replacement is not possible. IoT\nsystems must, therefore, be designed and deployed with strong\nchange/update/modification governance in mind. The cloud service provider components\nmay also be subject to change over time. Appropriate governance must be in place to\nensure that changes to these components are addressed as well.\nArchitecture for hybrid integration\nIT environments are now normally hybrid in nature. Integration across an ever-changing\nenvironment, delivered at the pace of modern digital innovation initiatives, is a significant\nchallenge. This makes the hybrid integration platform is crucial. Hybrid cloud integration\nscenarios include the following:\nViewing customer information between cloud-based CRM systems and on-\npremises Enterprise Resource Planning (ERP) applications\nEmployee data integration between cloud-based human capital management\nsystems and back-office applications\nThe hybrid integration reference architecture explores these and other common patterns\nobserved in enterprises addressing these issues. It addresses the following considerations:\nConnectivity: Connecting systems and devices with other systems and devices.\nThis integration may require low-level connectivity to Systems of Record (SoR)\nand the need to leverage cloud native systems as well.\nDeployment: Modern systems are deployed across a broad landscape so the\naccompanying integration components must have flexible deployment options.\nComponents should also be able to run directly on bare metal, in virtual\nmachines, or in containers.\nRoles: IT can be operated as bi-modal or multi-modal, where independent teams\nare working at different velocities. Integration needs to address these disparate\nvelocities. Hybrid integration therefore expands beyond the IT organization\nbusiness users and shadow IT departments that may be aligned with the line of\nbusiness. Complex integrations are now collaborative, and APIs become the\nbuilding blocks for collaboration between many different teams.\n",
      "content_length": 2461,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "Solution Reference Architectures\nChapter 8\n[ 184 ]\nStyles: Enterprise integration can be combined with APIs, events, and data to\ncreate seamless business processes and flows.\nThe hybrid integration architecture components are illustrated in the following diagram:\n(KIWTG\u0003\u001c\u001d\u0003%NQWF\u0003EWUVQOGT\u0003J[DTKF\u0003KPVGITCVKQP\u0003CTEJKVGEVWTG\nHybrid integration delivers a seamless platform for applications to interchangeably\nconsume services that, in turn, deliver end-to-end comprehensive mission-critical business\ncapabilities. The reference architecture has three tiers:\nPublic network\nCloud service provider\nEnterprise network\nSecurity is a cross-cutting theme that is applicable to all three tiers.\n",
      "content_length": 684,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "Solution Reference Architectures\nChapter 8\n[ 185 ]\nPublic network\nThe public network contains user access applications that reside on the cloud provider\nnetwork. They are accessed using a browser or via a mobile native app. Edge services\ninclude capabilities needed to deliver function and content to the users via the internet.\nThese include the following:\nDNS server\nCDN\nFirewall\nLoad balancer\nCloud provider network\nThe cloud provider network holds many of the key application and API services. The\nCloud application component represents cloud-native applications that have been designed\nand developed within the cloud environment. These applications normally use modern\ntechniques such as micro services architecture, lightweight run times, container technology,\nand DevOps methodologies. The application's services are often exposed using APIs and\nmay need to access data from other systems via API calls, messaging, and data integration\nservices.\nInteraction APIs provide access to enterprise capabilities. They are maintained by the lines\nof business and are composed from lower-level system APIs. These APIs are business-led,\nmay be exposed externally, and might even be monetized with a funding model based on\ntheir usage. This component is the API gateway into the enterprise network. The interaction\nAPIs also advertises the available services endpoints to which the cloud application has\naccess. This component provides API discovery, catalogs, and connection of offered APIs to\nservice implementations and management capabilities, such as API versioning.\n",
      "content_length": 1568,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "Solution Reference Architectures\nChapter 8\n[ 186 ]\nCloud messaging gives fast, scalable, high throughput event delivery services with the\nenterprise network. This component should support multiple open event protocols. It\nshould also abstract away any proprietary non-standard protocols of the enterprise\nmessaging. This component is the event gateway into the enterprise network and should be\nable to do the following:\nEnable large-scale message processing\nSupport a microservices framework and event-driven applications\nEnable hybrid messaging\nPerform batch and real-time analytics\nAccelerate applications and data integration\nCloud integration services deliver rapid, simple, and flexible integration capabilities.\nUnlike traditional Enterprise Application Integration (EAI) and ETL solutions, this\ncomponent provides simple integration tooling with targeted capabilities. Customization is\ndone performed via configuration and not by writing software code. This component is the\ngateway into SoR within the enterprise network and includes the following:\nPreparing/moving data to the cloud\nExtending business operations\nAccessing mainframe data and services\nMaintaining data consistency across applications\nConnecting on-premises apps and data to the cloud\nThe transformation and connectivity component enables secure enterprise connections.\nThis component includes the following capabilities:\nEnterprise secure connectivity\nTransformation\nEnterprise data connectivity\nEnterprise network\nThe enterprise network holds legacy applications, data, and APIs.System APIs give access\nto enterprise applications and data. They are maintained by the corporate IT team and are\nnormally lower-level, fine-grained APIs. Multiple interaction components might consume\nthese APIs to compose higher-level capabilities. This component also provides API\ndiscovery, catalogs, and connection of offered APIs to service implementations and\nmanagement capabilities.\n",
      "content_length": 1946,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "Solution Reference Architectures\nChapter 8\n[ 187 ]\nEnterprise messaging is the enterprise messaging backbone. This is the primary messaging\ninterface into the enterprise for the cloud messaging component and does the following:\nProvides secure and reliable messaging\nSupports heterogeneous application platforms\nProvides high-performance and scalable message transfer\nProvides simplified management and control\nEnterprise integration services depict a broad variety of integration capabilities including\nenterprise data warehouse (ETL) systems, application integration components, and\nbusiness process management systems. This is the primary integration interface into the\nenterprise for the cloud integration services component. Capabilities include the following:\nPreparing/moving data to the cloud\nExtending business operations\nAccessing mainframe data and services\nMaintaining data consistency across applications\nConnecting on-premises apps and data to the cloud\nEnterprise applications are applications that run enterprise business processes and logic\nwithin existing enterprise systems, while enterprise data represents transactional data or\ndata warehouses that represent the existing data in the enterprise.\nSecurity for hybrid integration addresses the following needs:\nData integrity\nThreat management\nSolution compliance\nCapabilities include the following:\nIdentity and access management\nData and application protection\nSecurity intelligence\n",
      "content_length": 1454,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "Solution Reference Architectures\nChapter 8\n[ 188 ]\nSummary\nThis chapter provides the baseline architectures for some key modern business solutions.\nThey are meant to serve as starting points for effective solutions the reader may be tasked\nto deploy. While the solutions are not readily deployable as documented, they do provide\nguidance and industry best practice recommendations for a solution delivery team. When\nused collaboratively across the solution delivery team, these reference architectures will\naccelerate and improve almost any solution design job.\n",
      "content_length": 562,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "9\nCloud Environment Key Tenets\nand Virtualization\nDesigning cloud computing solutions is not about transplanting the same enterprise data\ncenter design onto another platform. To properly leverage the highly automated and\ndynamic cloud computing platform, an architect must use the environment's elasticity and\nscalability to improve operational efficiency or economics. The concepts in this section are\nessential tools for accomplishing that task and modifying the data center architecture\ndesign to leverage these approaches will be the key to a profitable cloud computing\ntransition.\nThe following topics will be covered in this chapter:\nElastic infrastructure\nElastic platform\nNode-based availability\nEnvironment-based availability\nTechnology service consumption model\nDesign balance\nVirtualization\n",
      "content_length": 802,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 190 ]\nElastic infrastructure\nElastic infrastructures deliver preconfigured virtual machine servers, storage services, and\nnetwork connectivity using a self-service interface. This type of infrastructure provides the\nproper amount of dynamically-adjusted IT resources necessary for a stated level of service.\nAs the core capability of an IaaS service, this runtime infrastructure supports dynamic\nprovisioning and de-provisioning of servers, disk storage, and network connectivity. Real-\ntime monitoring of resource utilization is provided to enable traceable billing and\nautomation of all associated management tasks.\n",
      "content_length": 678,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 191 ]\nElastic platform\nThe provisioning of shared resources is an essential characteristic of cloud computing.\nSharing enables economies of scale associated with the cloud. Elastic platforms extend\nresource sharing to the operating systems and middleware. The extension enhances\neconomies of scale by increasing the utilization rates of these resources. This PaaS service\ndelivers application components to various customers on a shared middleware platform.\nMaintained by the service provider, the environment is also referred to as an integrated\ndevelopment environment (IDE). Customers build and deploy custom application\ncomponents to the middleware platform with a self-service interface. This enables resource\nsharing and automated middleware management.\n",
      "content_length": 820,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 192 ]\nNode-based availability\nWhen either an elastic infrastructure or platform is offered by a provider, service\navailability is defined by the conditions that must be fulfilled by the offering and the time-\nframe within which service availability is assured. These two factors enable the\ncomputation of hosted application availability. With node-based availability, the service\nprovider guarantees the availability of each application component.\nAvailable is generally defined as being reachable and capable of performing its advertised\nfunction. This availability time-frame is expressed as a percentage. As an example, an\navailability of 99.95% means that a component will be available 99.95% of the time. The\nfollowing figure depicts this example:\nEnvironment-based availability\nWhen using environment-based availability, a cloud provider expresses the availability of\nan elastic infrastructure or platform as a whole. Availability of an individual application\ncomponent using this type of expression is not defined. By communicating availability in\nthis way, a customer is better able to match how end users commonly express availability.\nAn example of this is expressing the availability of an at-least-once provisioned component\nor virtual server and the ability to provide a replacement if the first element fails. The\nfollowing figure depicts this example:\n",
      "content_length": 1427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 193 ]\nTechnology service consumption model\nThe IT service consumption is at the core of the industries embracing the OPEX expenditure\neconomic model. It also highlights the intersection of rapidly-growing demands, ever-\nevolving technology, and the IT-enabled business model. IT is no longer just a cost center. It\nallows business processes and transactions that weren't possible before. This\ntransformation from the traditional IT model to the pay-by-use contract avoids the direct\npurchase of equipment purchases and all expenses associated with operations and\nmaintenance. Today, the equipment acquisition model is risky and costly due to rapid\ntechnology and consumer demand changes.\nCompanies now have many cloud computing options. They can adopt a full private cloud\nstrategy, consume a single enterprise application from a public cloud, or take a hybrid\nroute and source-specific service assets as a complement to traditional data center solutions.\nSuccessful IT organizations must cope with a dynamic cloud service vendor landscape that\ncan deliver unprecedented choice.\n",
      "content_length": 1139,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 194 ]\nIT and business leaders must team in the shaping of IT service consumption. When\nparticipating in this partnership, the IT organization must be both flexible and business\nmodel-aware. The previously ubiquitous approach of mandating technology equipment\nand use is no longer viable. In the face of this change, IT must still, however, maintain a\ncentral role across all stages of enterprise IT consumption. The IT team must become a\ntrusted broker of technology services to the LOB. Their role will be as a critical\nintermediary and orchestrator, managing services, procurement, and delivery. They will\nalso provide technical support and information technology security.\nThe consumption model shift requires the following:\nThe consolidation of IT and corporate financial management strategies in a way\nthat can also serve user needs through a consumption-based process across\ntraditional infrastructure as well as public, private and hybrid clouds.\nImplementing an enterprise-wide set of financial and operational controls.\nFlexible IT planning and governance that measures and bundles usage into\nbusiness-relevant IT service catalogs. This alignment will improve LOB resource\nvisibility and selection.\nThe use of a consistent and repeatable process for the timely capturing of \nconsumption data.\nData streamlines invoicing and revenue recognition.\nLink reporting views directly to resource usage and the transaction owner.\nThe use of fixed, variable, tiered, scheduled, and resource state pricing features.\nThe ability to track detailed usage and costs that also shows customer cost at a\ngranular level.\nThe establishment of alerts that capture and analyze data that compares allocated\nversus used capacity, usage trends, and forecasted usage across the entire\nenterprise.\nDesign balance\nWhen designing a cloud computing solution, the goal is balance across four specific\norganizational guidelines:\nEconomics targets\nOperational goals\nTechnological compatibility\nEnterprise governance (risks)\n",
      "content_length": 2059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 195 ]\nThe solution architect must understand, respect, and document each of these limits. These\nguidelines will set the barriers, boundaries, and expectations of just about every\nconversation and meeting you will encounter. Everyone will want it all, but your toughest\njob will be the developing, presenting, and explaining the data in a way that leads to\ncompromise and agreement.\nVirtualization\nVirtualization enables the high utilization and high efficiencies associated with cloud\ncomputing. This technology approach is used through the computing stack. This section\nprovides the background needed for the architect to understand how to use compute,\nnetwork, data, and application virtualization.\nCompute virtualization\nA hypervisor enables the sharing of common underlying physical hardware between\ndifferent applications. The hypervisor will also reduce an application's dependency on a\nspecific physical server by abstracting the hardware into virtualized instantiations. This\nallows various operating systems and middleware to be installed on the same physical\nserver while maintaining isolation regarding the use of resources such as central\nprocessing units (CPUs), memory, disk storage, and networking.\n",
      "content_length": 1274,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 196 ]\nAlso known as a virtual machine monitor (VMM), the hypervisor may be software,\nfirmware, or hardware. The hypervisor manages requests from virtual machines.\nHypervisors come in two types:\nThe type 1 hypervisor runs directly on a bare-metal physical server. Sometimes\nreferred to as a bare-metal, embedded, or native hypervisor, type 1 has direct\naccess to the hardware. This type does not use a preloaded operating system.\nCompletely independent from the operating system, the hypervisor is small and\ncan monitor operating systems that run above it. Any problems manifesting in\none virtual machine or guest operating system do not affect the other running\noperating systems.\nExamples include VMware vSphere/ESXi product, XenServer, Red Hat\nEnterprise Virtualization (RHEV), open-source KVM (Kernel-based Virtual\nMachine), and Microsoft Hyper-V.\nType 2 hypervisors load inside an operating system like any other application.\nThe OS manages them, and its virtual machines will be slower than on the type 1\nvariety. These are also known as hosted hypervisors and are entirely dependent\non its host operating system for all operations. A type 2 installed on an operating\nsystem can also support other operating systems above it. Although the base\noperating system can allow better policy specification, any security\nvulnerabilities in the host operating system will affect the entire system,\nincluding the hypervisor running. Widely used type 2 hypervisors include\nVMware workstation, Microsoft Virtual PC, Oracle VirtualBox, and Parallels.\nType 1 hypervisors have dominated the server marketplace while type 2 hypervisors are\nmostly used on clients. Type 1 hypervisors running on client devices, however, are gaining \nmarket traction to support virtual desktop infrastructure (VDI) solutions).\n",
      "content_length": 1857,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 197 ]\nNetwork virtualization\nThere are three major approaches to the virtualization of network services:\nThe first, referred to as network virtualization (NV), is a network tunnel. This\nversion is used to create tunnels through an existing network to connect two\nseparate domains. This is done as an alternative to physically connecting two\nnetwork domains. Using tunnels is valuable because it avoids physical labor\nassociated with physically installing new domain connection. This concept is\neven more critical when needed to connect virtual machines. NV also efficiently\nleverages capital investments in existing infrastructure thus avoiding additional\ncapital outlays. When network virtualization is used with high-performance x86\nplatforms, movement of VMs can be done independently of any existing\ninfrastructure connections, avoiding any physical network reconfiguration.\nThe second network virtualization approach, network functions virtualization\n(NFV), uses best practices as initial policies and configurations for all network\nelements. One widespread use is adding firewalls and IDS/IPS systems. NFV\nenables the addition of functions on selected network tunnels, allowing the\ncreation of virtual machine service profiles or flow. This approach avoids manual\nnetwork provisioning and any associated training cost. It can also eliminate the\nneed to practice network over provisioning of firewall or IDS/IPS services. By\ncustomizing these services for each instantiated network tunnel, initial CAPEX is\nreduced while simultaneously enhancing operational flexibility.\nThe third option, software defined networking (SDN) is used to program \nnetwork deployments through the use of a control plane and data plane. The\ncontrol plane dictates what data packets should go to which destination. The\ndata plane transports those packets and uses switches that are programmed\nusing an SDN controller. One industry standard control protocol is OpenFlow.\nNV and NFV both add virtual tunnels and functions to the physical network whereas SDN\nchanges the physical network. That makes SDN an externally-driven method for\nprovisioning and managing the network. Use cases include data flows to different ports\n(for example, from 1GE port to a 10GE port) or aggregating multiple small flows into a\nsingle port. SDN is implemented using network switches as opposed to using x86 servers\nfor NV and NFV.\n",
      "content_length": 2451,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 198 ]\nNetwork virtualization technologies address mobility and agility. NV and NFV are used on\nexisting networks and reside on servers to interact with traffic directed to them. SDN is a\nnew network construct that uses switches to implement separate data and control plane\nfunctions.\nData virtualization\nData virtualization is used to retrieve and manipulate data without requiring the related\ntechnical data details like format or location (federated/heterogeneous data joins).\nAbstracted technical include API specifications and access language. This option can\nfacilitate connections to heterogeneous data sources making all data accessible from a single\nlocation. Data federation can also be accomplished with data virtualization by using it to\ncombine data result sets across multiple sources. When operational data and\nprocessed/cleansed data is needed to support of real-time data requirements, data\nvirtualization software is used for data integration, business process integration, service-\noriented architecture data services, and enterprise search is ideal.\nIn a cloud computing environment, data virtualization decouples data analytics and\napplications from physical data structures. This minimizes end-user impact if data\ninfrastructure is changed. Another cloud use is linking between NoSQL sources and\nrelational data sources.\nCloud solution architects should always architect with an enterprise view. With evolving\norganizational data virtualization requirements, these solutions can become less agile and\ndeliver lower performance as more layers and objects are added. Duplicate business logic\nand dependencies may also affect performance. In mitigating these challenges, the cloud\nsolution architect should design a layered view approach that isolates business logic.\nAlways use consistent naming standards and common rules for reusability and layer\nisolation and push down processing requirements to the source as much as possible.\n",
      "content_length": 2011,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 199 ]\nWith data virtualization serving a gateway into corporate data assets, it should be governed\nas such. Data virtualization concepts and capabilities must also be implemented\nconsistently across the enterprise. Data security will strongly impact data virtualization\nsecurity management so data security managers should determine applicable regulatory\nguidance (such as HIPAA, SOX, and so on). In many situations, data virtualization should\nbe used to limit access for specific users or user groups.\n",
      "content_length": 563,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 200 ]\nMany data virtualization tools can be used to display and export data lineage information.\nThis forms an important component of business process metadata. This tool can can be\nused to analyze and resolve data quality issues. Data virtualization is a powerful\ntechnology, but enterprise governance that balances data management structure, data\nvirtualization uptake, and innovation must be in place.\nApplication virtualization\nApplication virtualization segregates computer programs from the underlying operating\nsystem. They do not install like a normal application, but execute as if they were. Contrary\nto how normal computing applications, when using application virtualization, each\napplication sets its configurations at runtime. This leaves the host operating system and\nexisting environments unaltered. The application will behave like it is interfacing directly\nwith the operating system. Application virtualization is another technology that allows for\nthe dynamic distribution of computing resources. Application virtualization allows\napplications to run in non-native environments.\n",
      "content_length": 1159,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "Cloud Environment Key Tenets and Virtualization\nChapter 9\n[ 201 ]\nSummary\nSuccessful cloud computing solutions align with the cloud environment key tenets outlined\nin this chapter. They also use compute, network, and application virtualization to delivery\nscalability and elasticity to the business or mission model. In the end, this is why cloud\ncomputing has revolutionized every industry vertical. The cloud solution architect must be\nintimately familiar with the purpose and tenor of every one of the environment tenets\noutlined in this chapter, because their job will depend on how they deliver value.\n",
      "content_length": 607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "10\nCloud Clients and Key Cloud\nServices\nOne of the five essential cloud computing characteristics is ubiquitous access. Network\nconnectivity and the user's device client are necessary to enable this capability. Client\nselection typically depends on a decision between using a native application or a web app.\nIn this chapter, we will cover the following topics:\nCloud computing clients\nIaaS\nCommunications services\nAuditing\nCloud computing clients\nNative applications are built and designed for specific mobile devices. They install directly\non the hardware after being downloaded from app stores or marketplaces. These \napplications are designed to be compatible with native features of the target device\nhardware and can work as standalone entities. An important drawback, however, is that\nusers need to continually update the app.\nWeb apps are accessible via the mobile device web browser and are not downloaded onto\nthe user's device. They can only access a limited number of the device's native features and\nupdate themselves without user intervention. This development option uses languages\nsuch as JavaScript, HTML 5, or CSS3 but no standardization or SDK is available. Web apps\nmay also lead to higher maintenance cost across multiple mobile platforms.\n",
      "content_length": 1261,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 203 ]\nFrom a user point of view, both options look and operate in similar manners. The choice is\nnormally between deploying a user-centric app or an application-centric app. Sometimes,\nboth native and web apps are developed to expand user reach and provide a better overall\nuser experience. Client developers must also choose between using a thin or thick client.\nThe following table shows the difference between a thin and a thick client:\nA terminal emulator emulates a video terminal within another display architecture. Usually\nsynonymous with a shell or text terminal, the term refers to all remote terminals. A terminal\nemulator that resides inside a graphical user interface (GUI) is called a terminal window. It\nallows user access to text terminal applications like command-line interfaces (CLI) and\ntext-user interface (TUI).\nRapidly growing in popularity as a cloud computing concept is the Internet of Things\n(IoT). A customer does not buy an IoT, but customers purchase solutions that use IoT\ncomponents. An IoT solution will typically tap into an ecosystem of partners that span a\nvalue chain from sensors to the application.\nThe genesis of an IoT solution is making things smart by combining sensors, connectivity,\nand software. Machine-to-machine (M2M) solutions focus on connectivity, where the goal\nis to deliver intelligence by connecting items. Reporting data from these devices provided\nhistorical business intelligence versus real-time insight. IoT solutions focus on smart versus\nconnected and near real-time analysis. The goal here is insight and action rather than\nreporting.\n",
      "content_length": 1649,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 204 ]\nWhen designing IoT solutions, the architect should identify a precise problem that has a\nquantifiable response. Early IoT solutions used instrumented devices that sent data back to\nthe cloud for processing. This approached proved impractical because physical-world\nsolutions required a certain amount of processing at the edge for various reasons. Data\nneeded to be processed and acted upon immediately using new concepts, such as fog\ncomputing. It could also be difficult to send large volumes of data over the network.\nIoT solutions also need to be able to do correlation and analytics at the edge. Therefore,\narchitects need to develop strategies that define when and where to process data. They\nmust also select equipment with this type of edge processing in mind. The most valuable\nsystems will offer machine learning that identifies patterns quickly and delivers timely and\nrelevant information to its users.\nMake sure the solution also preserves sensor battery life. Hardwiring an electrical line to\neach sensor is cost-prohibitive, but battery-powered sensing is useless if you need to\nexpend labor to replace batteries too often. End sensing devices must, therefore, have\nsoftware that optimizes battery life when processing and delivering data. IoT security can\nalso be more challenging. Encryption is widely used and identity and authentication should\nprovide additional protections. The network, hardware, and people accessing the data\nshould all be trusted entities. IoT solutions must also be field-upgradable.\nIaaS\nInfrastructure-as-a-Service delivers compute, storage, communications (which includes\nnetwork services), metering/monitoring, and auditing services.\nCompute services\nCompute combines CPUs, memory, and disks virtual equipment to create virtual machines.\nThis is done by virtualizing physical servers and storage devices shared by multiple users.\n",
      "content_length": 1931,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 205 ]\nA virtual machine is a computer software file, called an image, that acts like a real\ncomputer. It runs in a segregated environment, just like any other software program, and\ngives the end user the same experience as they would have on a traditional host operating\nsystem. The software inside a virtual machine is sandboxed and cannot escape or tamper\nwith the physical computer. Multiple virtual machines can operate simultaneously on the\nsame physical computer. This is called a multi-tenant environment, Multiple operating\nsystems run side-by-side on the hypervisor, which manages them. Each virtual machine\nindependently provides its virtual hardware. The virtual hardware is mapped to the actual\nhardware on the physical machine. This mapping saves costs by reducing physical\nhardware quantities, associated maintenance costs, power consumption, and cooling\ndemands. Virtual servers scale quickly but may have reduced performance when compared\nto bare-metal servers.\nA bare-metal server is a single tenant physical server that is dedicated to one customer. This\nprevents server performance from being impacted by other workloads. This service is\ntypically used for latency-sensitive workloads that require a significant amount of raw\nprocessing power. Bare-metal clouds provision bare-metal servers with on-demand access,\nhigh scalability, and pay-as-you-go features. Bare-metal cloud economics can be compelling\nif the solution architect is faced with high load factors. They can be cheaper on a per-\nworkload basis in environments where the virtual machines are large and continually\nheavily loaded.\nCloud service providers typically provide a choice of operating systems to their customers.\nUsually, this involves different versions of Linux (RHEL, Ubuntu, CENTOS, Freebird) or\nMicrosoft Windows, Solaris, or IoS. The architect should poll organizational users and\nrequirements to select the most appropriate OS. CSPs will package their compute offering\nwith variations on the number of computing cores, amount of RAM, IOPs, and available\nephemeral storage. Autoscaling is used to automatically change the number of\ncomputational resources deployed to a server farm. This is usually measured in the number\nof active servers. This number rises or falls automatically based on the farm workload.\nStorage services\nIaaS storage services are either ephemeral or persistent. Ephemeral storage persists only\nwhen a specific virtual machine is live. If that machine is de-provisioned, any data in its\nephemeral data store is lost. The random access memory (RAM) and cache are typically\nnon-persistent storage technologies.\n",
      "content_length": 2679,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 206 ]\nTransfer all ephemeral data to a persistent data store to prevent data loss. Persistent\nstorage, as its name implies, persists after a virtual machine is de-provisioned and is\nsometimes referred to as non-volatile storage. This storage type is typically backed by\nmechanical hard disk drives or solid state drives in either a storage area networks (SANs)\nor network attached storage (NAS) schema. This can be in the form of file, block, or object\nstorage.\nFailure tolerance in a storage offering duplicates data across multiple copies. These copies\nstore the same set of data. If one of these copied versions is lost, data is still recoverable\nfrom the other copies. Storage consistency is a fundamental concept in cloud computing\nand describes the time it takes for all data copies to be the same. Strict consistency ensures\nthat all copies of the data have been duplicated among all relevant copies to increase\navailability. A subset of the data copies is accessed by read and write operations. The ratio\nof replicas versus the number of replicas accessed during read and write operations can\nguarantee consistency across all copies. In eventual consistency, the consistency of data is\nrelaxed.\n",
      "content_length": 1253,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 207 ]\nThis reduces the number of replicas that have to be accessed during read and write\noperations and reduces the overhead required to maintain strict consistency. With eventual\nconsistency, data changes are eventually transferred to all data copies through the\nasynchronous propagation via the network.\nVolume/block storage\nVirtual and physical servers can be managed more efficiently if they don't store state\ninformation locally. This makes provisioning, de-provisioning, and failure handling much\nmore manageable. Volume/block device storage is centralized storage that is accessed by\nservers as if it was a local hard drive.\nObject/blob storage\nDistributed cloud applications are widely used to handle large data elements. Also referred\nto as binary large objects (blobs), some examples include virtual server images, pictures,\nor videos.\nThese types of data elements are organized in a folder hierarchy where each data element\nhas a unique identifier that includes its location and a file name. This globally unique\nidentifier is passed to the storage offerings to retrieve data over the network.\n",
      "content_length": 1155,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 208 ]\nKey-value storage\nFor higher availability and performance, storage offerings distributed data across different\nIT resources and locations. This can change storage requirements and increase the demand\nfor a more flexible data structure. In these situations, data structure validation during\nqueries may require high-performance connectivity between the distributed resources.\nPerformance issues are avoided by storing identifiers (keys) and associated data (values)\npairs and not enforcing data structure. Data query complexities are reduced significantly\nwhile simultaneously enhancing scalability and configurability. Semi-structured and\nunstructured data can also be scaled out among many IT resources without needing to\naccess them in order to evaluate expressive queries.\n",
      "content_length": 832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 209 ]\nArchival storage\nArchival storage is very long-term data storage that uses SAN, optical, or magnetic tape\ntechnologies. This service is used to meet regulatory or legal retention requirements and is\nused to store data that does not require quick access.\nCommunications services\nCommunication services encompass all of the functions normally associated with the\nnetwork. These services are metered, based on the amount of data throughput or the\nnumber of input/output operations.\nVirtual networks\nVirtual networks support application components deployed on elastic infrastructures and\nplatforms. These virtual communications resources rely on physical network hardware to\ncommunicate. Customers are, however, isolated from each other on this networking layer.\nPhysical resources, such as networking interface cards (NICs), switches, and routers, are \nabstracted into virtualized equivalents that can be managed by service provider customers.\nUsing the self-service interface and CSP applications, customers can design, implement,\nand configure virtual circuits, firewalls, load balancers, network address translations\n(NAT), and network cross-connects.\n",
      "content_length": 1208,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 210 ]\nMessage oriented middleware\nWith distributed applications, application components hosted on different cloud resources\nneed to exchange information. Often, this also requires integration with other cloud and\nnon-cloud applications. Using message-oriented middleware, communication partners can\nexchange information asynchronously using messages. This service handles addressing, the\navailability of communication partners, and message format transformation.\nExactly-once delivery\nThis service is used for systems in which duplicate messages are unacceptable. For these\ncases, the messaging system ensures delivery of each message only once by automatically\nfiltering all possible duplicates. When created, each message is tagged with a unique\nidentifier. This identifier filters out message duplicates during transmission from sender to\nreceiver.\n",
      "content_length": 902,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 211 ]\nAt-least-once delivery\nIn some solutions, message-oriented middleware handles message duplicity. The system\nstill, however, needs assurances that the message is received. With at-once-delivery, a\nservice acknowledgment is sent back to the message sender for each message retrieved by a\nreceiver. If this response is not received within a specific time frame, the message is resent.\nTransaction-based delivery\nAlthough message-oriented middleware can manage message traversal, an assurance that\ntransmissions are received may also be required. With a transaction-based delivery service,\nboth the message-oriented middleware and the receiving client participate in the\ntransaction. All message communications operations are, therefore, performed under a\nsingle transactional context guaranteeing ACID (short for Atomic, Consistent, Isolated,\nDurable) behavior.\nTimeout-based delivery\n",
      "content_length": 938,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 212 ]\nTimeout-based delivery service assures that a client receives a message before being deleted\nfrom a message queue. This is done by not deleting the message immediately after a client\nhas read it, but only marking is as being invisible. After the client has read a message, it\nsends an acknowledgment to the message queue and the message is deleted.\nMetering/monitoring\nThe dynamic nature of the cloud and its pay-by-use economic model makes monitoring a\ncrucial architectural component. Monitoring also forms the basis for metering which\nmeasures how use resources are used to support the charges levied. Metering also enables\nthe automation of variable service quantities to support large variances in customer\nrequirements.\nMetrics used in metering services include the following:\nPer-unit of time for services\nPer-unit of data\nPer-transaction\nPer-user\nOne-time charges\nServices are monitored and metered across the operating system, network, and application\ncategories. The operating system is fundamental to all monitoring and some of the\nfundamental tools are as follows:\nSyslog: Access to log entries that contain a description of the application that\ngenerated the message, severity level, time stamp, and the message\nVmstat: Virtual memory statistics\nMpstat: Processor-related statistics, activities of each available processor, and\nglobal averages\nTop: Linux tasks and a system summary\nWell known open source tools include:\nNagios: Availability, CPU load, memory, disk use, users logged in, and processes\nMunin: Performance monitoring tool that provides detailed graphs of system\nperformance over a web interface\nThe primary network metering tool is Netstat, which provides data on network\nconfigurations, connection activity, and usage statistics.\n",
      "content_length": 1814,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 213 ]\nApplication monitoring needs to identify when a problem occurs, while analysis will locate\nrelated application structure issues. This is primarily an operations team responsibility\nwhile performance analysis is a development team responsibility. Performance analysis\nincludes profiling, which requires insight into the application structure. Analysis tools\nshould be able to capture output over time for use as input to monitoring applications.\nMonitoring results need to be actionable, but limited control over the production system\nmight not afford the capability to reproduce problems or to do in-depth analysis. Tools that\nenable profiling can, however, significantly reduce application performance.\nAuditing\nInformation technology audits fall into either the internal or external categories. Internal\naudits address work done by the organization employees. These look at organizational\nprocesses and primarily focus on process optimization and risk management. External\naudits look at an organization's ability to meet legal and regulatory requirements from an\noutside perspective. Audits can also evaluate data availability, integrity, and confidentiality\nissues. A cloud solution requires a three-way negotiation among service organizations,\ncloud service providers (CSPs), and end users. The goal is to ensure productivity while\nmaintaining an acceptable degree of security.\n",
      "content_length": 1439,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 214 ]\nCloud security audits look at whether security-relevant data is transparent to CSP\ncustomers, data encryption policies, and protections that address the co-located customer\nenvironment. The scale, scope, and complexity of cloud computing audits are also\nsignificantly different than a traditional enterprise equivalent. A significant challenge,\nhowever, lies in an auditor's cloud computing knowledge. Cloud security auditors must\nknow cloud computing terminology and have a working knowledge of a cloud system's\nservice design and delivery method.\nCloud security audits must make sure that all security-relevant data is available to CSP\ncustomers. Transparency enables rapid identification of potential security risks and threats.\nIt also helps in the creation and development of appropriate enterprise countermeasures\nand recommendations. Access to accurate information reduces the risk of cyber security\nthreats.\nData should be encrypted at rest, in motion, and, if possible, when in use. Encryption may\nnot always be the most efficient solution and encryption key management options aren't\nalways acceptable. Encryption and decryption performance shortcomings may make\nencryption at rest non-viable. Data in motion is usually encrypted using transport layer\ntechnologies like secure socket layer. Homomorphic encryption or encryption in use can\nallow encrypted queries to search encrypted texts without search engine decryption. It has\nthe potential to solve the security issue of encrypted data at rest in both traditional IT and\ncloud infrastructures, but performance is still lacking.\nWhile co-location enables the economic advantages of the multi-tenant environment, it also\nintroduces some significant security concerns. An audit must ensure that the CSP\nhypervisors can reliably insulate virtual machines (VMs) from the physical computing\nhardware. A CSP must balance the multiple ways to build and manage cloud infrastructure\nhypervisors each with business needs and relevant security issues. In spite of the need to\nestablish standardize cloud computing structures and multi-tenant security, no official\nstandard exists.\nWith cloud computing, a single physical machine will typically host many virtual\nmachines. Hosting multiple VMs can drastically increase the number of hosts that need to\nbe audited. This increase can make the scale, scope, and complexity of cloud audits\noverwhelming. Standardization can dramatically assist in making the auditing process\nsmoother and faster despite the larger scale of cloud computing. Another critical factor to\nconsider is an adjustment of the audit scope.\n",
      "content_length": 2666,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 215 ]\nWhile increased numbers of IT elements requiring audit drive scale issues, new technology\ntypes cause scope increases. An example is the examination of hypervisor security in the\nmulti-tenant environment. Also, many cloud environments include intangible and logical\nelements that also require an audit. Auditors must be aware of these differences and take\nthis complexity into account.\nCloud service performance can vary based on the specific CSP. Within the same CSP,\nperformance can also be dependent on service configuration, time (time of day, the day of\nthe week, week of the month, and so on) and geographic location. Performance variance of\nover 1000% in compute services alone has been observed. Since pricing is typically a fixed\nrate tied to a specific metric, this will often lead to widely-differing price/performance\nvalues\nCompute metrics recommended for auditing are CPU and input/output (I/O) performance.\nNetwork metrics such as latency and bandwidth allocation should also be measured from\nmultiple CSP locations\n",
      "content_length": 1087,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 216 ]\nService level agreement\nThe service level agreement (SLA) serves as both the blueprint and warranty for cloud\ncomputing services. Its purpose is to document specific parameters minimum service levels\nand remedies for any failure to meet the specified requirements. It should also affirm data\nownership and specify data return and destruction details. Other important SLA points to\nconsider include the following:\nCloud system infrastructure details and security standards\nCustomer right to audit legal and regulatory compliance by the CSP\nRights and cost associated with continuing and discontinuing service use\nOther important criteria\nService availability\nService performance\nData security and privacy\n",
      "content_length": 760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 217 ]\nDisaster recovery processes\nData location\nData access\nData portability\nProblems identification and resolution expectations\nChange management processes\nDispute mediation processes\nExit strategy\nCustomers should read the cloud provider's SLA very carefully and validate them against\ncommon outage scenarios. Organizations should also have contingency plans in place to\nsupport worse case scenarios.\nPaaS\nPlatform-as-a-Service (PaaS) is an execution runtime environments offered in a multi-\ntenant environment. The underlying assumption is that applications often use similar\nfunctions and that these components can be shared with other applications. Sharing this\ncommon functionality should also result in higher environment utilization rates.\nCommon application functionality is offered in an execution environment that delivers\nplatform libraries for custom application implementations using middleware solutions. The\nmost common of these are databases and integrated development environments (IDEs).\nDatabase\nDatabase PaaS services typically align as either a SQL/relational form or NoSQL/non-\nrelational type.\n",
      "content_length": 1168,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 218 ]\nSQL/relational databases handled data comprising large numbers of similar data elements.\nThese elements have identifiable dependencies among each other. When this structured\ndata is queried, users make certain assumptions about the data structure and the\nrelationship consistency between the retrieved data elements.\nData elements are recorded in tables, where each column represents a data element\nattribute. Table columns may also embed dependencies for how entries in one table column\nrelate to a corresponding column in a different table. These dependencies are strictly\nenforced during any data manipulation.\nIn No-SQL/non-relational databases (Mongo, Map Reduce, and so on), an enforced\ndatabase structure does not exist. This is useful when processing large data sets and the\nprocess is split up and mapped to multiple application components. This is often the case\nwith cloud applications, usually handle enormous amounts of data, which need to be\nprocessed efficiently. As distributed applications are scaled out, data processing is similarly\ndistributed among multiple components.\n",
      "content_length": 1147,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 219 ]\nThe data processing components simultaneously execute the query to be performed on the\nassigned data chunks. Afterwards, the processing results are consolidated or reduced into\none result data set. During this reduction, additional functions (sums, average values, and\nso on) can also be applied.\nIntegrated Development Environment\nAn IDE provides an application development environment for developers that is managed\nby a cloud service provider. This eliminates the complexities associated with maintaining\nand operating the application development infrastructure. Developers can access and\nadminister PaaS services via a web browser or IDE plugin. Some common PaaS IDEs\ninclude:\nElastic Beanstalk, native to Amazon Web Services (AWS). The code is uploaded\nand the PaaS automatically deploys the WAR file to one or more EC2.\nHeroku, which uses standard libraries with application servers (such as Tomcat\nand Jetty) but is extensible and natively supports Ruby, Node, Python, Java,\nClojure, Go, Groovy, Scala, and PHP.\nRed Hat OpenShift, which supports Java, Ruby, Node, Python, PHP, and Perl.\nIBM Bluemix, which is based on CloudFoundry, is extensible, and natively\nsupports Java, Node.js, PHP, and Python.\n",
      "content_length": 1264,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 220 ]\nGoogle App Engine (GAE) which runs applications in a sandboxed environment\nand requests are distributed across multiple servers. This is specifically used for\nthe building and deployment of applications onto Google's infrastructure.\nSaaS\nSoftware-as-a-Service provides a fully managed application. The consuming organization\nmanages the user base, access to the service, and the governance of the data inputted by\norganizational users. The CSP has full responsibility for the architecture, security, and\navailability of the service. Some of the most popular SaaS service offering categories are as\nfollows:\nCRM software: Customer information management, marketing automation, and\nsales pipeline tracking.\nERP software: Improved process efficiency and organizational information\nsharing combined with improved management insight into workflow and\nproductivity.\nAccounting software: Improved financial organization and tracking\nProject management software: Project/program scope, requirements, and\nprogress management. The tracking of changes, communications, and deadlines\nin a way that meets stakeholder requirements.\nEmail marketing software: Automate email marketing and relationship\nbuilding, while optimizing message delivery.\nBilling and invoicing software: Billing and invoicing automation. Implementing\ncustomer self-service payment options. The reduction of data entry costs, and\nelimination of billing errors.\nCollaboration software: Improved organizational communications that\nempower employees to more easily follow complex interactions. More efficient\ncommunications and enhanced enterprise productivity.\nWeb hosting and e-commerce: Web hosting, content management systems,\nmessage boards, shopping carts, and so on.\nHR software: Employee time tracking. Improved recruiting and hiring.\nAutomate payroll and more efficient management of human resources.\nTransaction processing: Credit cards and bank transfer processing. Publish and\ntrack coupons, in support of loyalty rewards programs.\n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "Cloud Clients and Key Cloud Services\nChapter 10\n[ 221 ]\nSummary\nDelivering the desired cloud service through the appropriate client device is the desired\nresult of any cloud computing solution. This chapter reviewed the basics needed to achieve\nthat goal. Clients must seamlessly align with the expected end user consumption\nrequirements while the services, as always, must align with the business or mission model.\nThe services reviewed in this chapter are the essential solution building blocks.\n",
      "content_length": 498,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "11\nOperational Requirements\nUntil recently, enterprises designed data centers with the mindset of supplying hosting,\ncompute, storage, or other services with typical or standard organization types in mind.\nThis mindset is a problem when considering modern-day data centers and cloud service\nofferings. As cloud-based application development continues to gain popularity and\nwidespread adoption, it is important for us to recognize the benefits and efficiencies, along\nwith the challenges and complexities. Cloud development typically includes integrated\ndevelopment environments, application lifecycle management components, along with\napplication security testing. Unlike traditional deployments within a data center or even a\nhosted solution where network controls are ubiquitous and compensating perimeter\ncontrols are sometimes depended upon to offer application security, cloud applications\noften run in a comparatively unprotected fashion.\nIn this chapter, we will cover the following topics:\nApplication programming interface\nCommon infrastructure file formats d VMs\nData and application federation\nDeployment\nFederated identity\nIdentity management\nPortability and interoperability\nLifecycle management\nLocation awareness\nMetering and monitoring\nOpen client\nAvailability\nPrivacy\nResiliency\nAuditability\n",
      "content_length": 1310,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "Operational Requirements\nChapter 11\n[ 223 ]\nPerformance\nManagement and governance\nTransaction and concurrency across clouds\nSLAs and benchmarks\nProvider exit\nSecurity\nSecurity controls\nDistributed computing reference model\nApplication programming interface\nOrganizations and practitioners alike need to understand and appreciate that cloud-based\ndevelopment and applications can vary from traditional or on-premise development. When\nconsidering an application for cloud deployment, one must remember that applications can\nbe broken down into the following sub-components:\nData\nFunctions\nProcesses\n",
      "content_length": 597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "Operational Requirements\nChapter 11\n[ 224 ]\nThese components can be further broken up, so that portions that have sensitive data run in\na traditional data center and less sensitive data runs in a cloud computing environment. It\nis also important for developers to understand that, in many cloud environments, access is\nacquired through the means of an application programming interface (API). These APIs\nwill consume tokens rather than traditional usernames and passwords. APIs can be broken\ndown into two formats:\nRepresentational state transfer (REST)\nSimple object access protocol (SOAP)\nREST defines a set of constraints and properties based on HTTP. These are referred to as\nRESTful web services and conform to the REST architectural style. By doing this they\nprovide interoperability when computers communicate across the Internet. REST-\ncompliant services allow the requesting systems to access and manipulate textual\nrepresentations of web resources by using a standard set of stateless operations. SOAP, also\nreferred to as simple object access protocol, is a more structured messaging protocol\nspecification used predominately for exchanging structured information through web\nservices across computer networks. The purpose of SOAP is to deliver extensibility,\nneutrality, and independence. It uses the XML information set for its message format and\nrelies on application layer protocols, usually Hypertext Transfer Protocol (HTTP) or\nSimple Mail Transfer Protocol (SMTP), for message negotiation and transmission.\nThe application programming interfaces (APIs) are a means for a company to expose\nfunctionality to applications. Some benefits of APIs include the following:\nProgrammatic control and access\nAutomation\nIntegration with third-party tools\nConsumption of APIs can lead to the use of insecure products by a company. Organizations\nmust also consider the security of software (and APIs) outside of their corporate\nboundaries. Consumption of external APIs should go through the same approval process\nused for all other software being consumed by the organization. When leveraging APIs,\nensure that API access is secured. This requires the use of SSL (REST) or message-level\ncrypto (SOAP), access authentication, and logging of API usage.\n",
      "content_length": 2255,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "Operational Requirements\nChapter 11\n[ 225 ]\nAPI levels and categories\nThere are four levels of APIs that developers must work with. For more information refer\nto: IUUQ\u001c\u0011\u0011XXX\u0010KBTPOHBVESFBV\u0010DPN\u0011\u0014\u0012\u0013\u0014\u0011\u0012\u001a\u0011DMPVE\u000fDPNQVUJOH\u000fVTF\u000fDBTF\u000fQBSU\u000f\u0015\u0010IUNM\u0001\u0001\u0001\nIJTUPSZ\u001f\u0012BNQ\u001dQGJE\u001f\u0013BNQ\u001dTBNQMF\u001f\u0014\u0017BNQ\u001dSFG\u001f\u0013.\nCommon APIs for cloud storage\nData is central to operations so establishing standard APIs for accessing cloud storage\nservices, databases, and other middleware services is a must. The use of custom code\nwithin a solution locks the enterprise into a proprietary design, eliminating portability, and\neliminating the financial benefits and flexibility afforded by cloud computing.\nCommon cloud middleware API\nAPIs needed to support creating and dropping databases and tables, connecting to message\nqueues and other middleware operations should be consistent across the enterprise.\nEmbedded restrictions in a database vendor product can significantly increase processing\nresource requirements when dealing with large datasets. Examples include restrictions on\njoins across tables and an inability to support a valid database schema. Such restrictions\ncreate significant challenges when contemplating a move to a different database.\nLimitations are especially applicable for applications built on a relational model.\nMiddleware services such as message queues are more straightforward and will typically\nnot present such a significant challenge.\nAdditional concerns\nA cloud solution architect must focus on meeting an organization's operational\nrequirements. Lessons learned from previous cloud deployments have highlighted the\ncriticality of addressing each of the requirements listed in this section adequately.\n",
      "content_length": 1692,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "Operational Requirements\nChapter 11\n[ 226 ]\nCommon infrastructure file formats ` VMs\nVirtual machine portability is a significant concern in a cloud computing environment.\nConcerns are especially valid in a hybrid IT deployment. Any enterprise solution should\naddress possible differences in both the VM file format and the process for attaching\nstorage to VMs.\nData and application federation\nWhen combining data from multiple cloud-based sources, enterprise applications need to\ncoordinate the applications activities that may span multiple platforms; cloud managed\nservice provider and traditional data centers. Hybrid environments require implementing\ndata federation and virtualization techniques across the various environments.\nDeployment\nCloud application deployment involves both programming interfaces and cloud-specific\npackaging technologies. This operational requirement may include traditional packaging\nmechanisms like EAR/WAR files and .Net assemblies.\nBuilding and deploying a VM image should be simple and portable between different\nhybrid infrastructure environments. Any required compensations should be well known\nand mechanisms for attaching storage to VMs well understood.\nFederated identity\nWhen operating in a hybrid environment, the idea is to have the user maintain\nresponsibility for a single ID with the infrastructure federating all other required identities.\nThis federation would include the primary identity needed by an end user and all\nassociated enterprise roles that the user is likely to hold within the enterprise.\n",
      "content_length": 1554,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "Operational Requirements\nChapter 11\n[ 227 ]\nIdentity management\nMost cloud computing solutions can leverage industry-specific identity management\nstandards and protocols, such as SAML and OAuth. These may also need to interact with\ntraditional standards such as RosettaNet or OASIS. Although the specific standard may\nvary between applications, the solution must be able to handle all access and data\nauthorization scenarios efficiently.\nPortability and interoperability\nThe cloud computing era brings with it the need to design, build, and manage a business-\nfocused ecosystem. Efficient communication and interaction across such an ecosystem\nrequire interoperability between the enterprise and its ecosystem partners. Since a universal\nset of standards does not exist and most likely won't exist shortly, these ecosystems can\nencounter a significant risk of vendor lock-in. An ecosystem's ability to use reusable\ncomponents to build systems that work together out of the box depends on the enforcement\nof portability and interoperability governance. A particular concern for in-cloud computing\nthis is critical during the deployment or migration of systems to a cloud service provider. A\ntypical scenario is an inability to migrate some components to the cloud due to data\nmanagement or data sovereignty regulations. Cloud migration requires portability of all\nmigrating components as well as interoperability of those components with systems that\nremain on-premise.\nSpecific technology categories where portability and interoperability standards should be\nspecified include the following:\nData: Enabling the reuse of data components across different applications. Since\ndata interoperability interfaces do not currently exist, this may require the use of\ndata virtualization techniques.\nApplications: This focuses on interoperability between application components.\nThese have SaaS deployed components, application modules leveraged in a PaaS,\nor infrastructure components consumed as IaaS. Similar issues arise in a hybrid\nenvironment when interfacing with a traditional enterprise IT environment or\nwith client endpoint devices. Application portability enables the re-use of all\napplication components across the entire hybrid IT environment.\nPlatforms: This category addresses the re-use of service bundles that may contain\ninfrastructure, middleware, or application components along with any associated\ndata.\n",
      "content_length": 2416,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "Operational Requirements\nChapter 11\n[ 228 ]\nInfrastructure: Interoperability and portability associated with various hardware\nvirtualization technologies and architectures.\nManagement: Management interoperability is interoperability between cloud\nservices (SaaS, PaaS, or IaaS) and programs concerned with the implementation\nof on-demand self-service. Management may also include application programs\nconcerned with the deployment, configuration, provisioning, and operation of\ncloud resources.\nPublication and acquisition: The self-service aspect of cloud computing gives\nend users the ability to acquire software, data, infrastructure and various other\ncloud services. Developers can also publish applications, data, and cloud services\nvia online marketplaces. This category addresses interoperability between\nplatforms and cloud service marketplaces, including app stores.\nLifecycle management\nLifecycle management of applications and documentation is a continuous challenge to all\norganizations. Tasks that fall within this requirement include versioning, data retention,\nand destruction and information discovery. Legal liabilities can be substantial if due\ndiligence is not effective in identifying regulatory and legal restriction in this area.\nLocation awareness\nNational data sovereignty laws are expanding globally. These new requirements not only\napply to how an organization handles data but it also equally apply to data managed on the\norganization's behalf. The associated requirement may include legal restrictions on the\nlocation of the physical server when organizational data is present. Meeting location-\ndependent legal requirements may require the use of APIs that determine the location of\nthe physical hardware associated with the delivery of all cloud services.\nMetering and monitoring\nThe pay-as-you-go cloud computing model requires consistent and ubiquitous metering\nand monitoring of all cloud services. This capability is essential to an effective cost control,\ninternal charge-backs, and service provisioning process.\n",
      "content_length": 2048,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "Operational Requirements\nChapter 11\n[ 229 ]\nOpen client\nUbiquitous access to cloud services levies a requirement for the use of open clients and\nendpoint devices. The use of vendor-specific endpoints violates this essential requirement\nas cloud services should not require the use of vendor-specific platforms or technologies.\nAvailability\nCloud service availability describes the degree to which a specific service is in a specified\noperable and committable state if a provisioning a request at a random time. Availability is\nusually expressed as a percentage and stated in the CSP service level agreement. The CSP\nsets availability, but additional payments can enhance this value. The solution architect\nshould be aware of all service availability rates and advise mission/business owners on the\nservice's ability to meet organizational goals.\nPrivacy\nPrivacy addresses the condition of being free from observation or disturbance by others.\nCloud computing has led to the establishment and strict enforcement of many new data\nprivacy laws. One of the most evasive of these is the General Data Protection Regulation\n(GDPR). Approved by the EU parliament on 14 April 2016, its enforcement date is 25 May\n2018. A non-compliant organization can face hefty fines. GDPR invalidates the Data\nProtection Directive 95/46/EC and has a goal of harmonizing European data privacy laws,\nprotecting and empowering all EU citizens' data privacy, and reshaping the how regional\norganizations approach data privacy. It applies to all personal data processing of EU\nresidents, regardless of the company's location. It also covers data processed outside of the\nterritorial limits of the EU and to the processing of personal data for EU citizens by non-EU\ncompanies when selling goods or services to an EU citizens. Penalties can be up to 4% of\nannual global revenue or f20 million, whichever is greater.\nResiliency\nResiliency refers to the ability of a cloud service to recover from service delivery difficulties\nor failure. The CSP sets resiliency levels, but additional payments can enhance the property.\nThe solution architect should be aware of all service resiliency specifics and advise\nmission/business owners on the service's ability to meet organizational goals.\n",
      "content_length": 2254,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "Operational Requirements\nChapter 11\n[ 230 ]\nAuditability\nAuditability describes the extent to which a cloud service consumer can conduct a thorough\nand accurate assessment of the cloud service provider's ability to deliver and appropriately\naccount for the cost of delivering a cloud service. This sort of data is typically driven by\nlegal or regulatory requirements and is often foundational to an organization's ability to use\na service at all. The solution architect should be aware of all audit requirements and advise\nmission/business owners on the service's ability to meet them.\nPerformance\nWhile the service level agreement outlines the minimum level of service expected from a\nprovider, performance may still vary widely across any specified parameter set. Service\ncomponents that lie entirely outside the provider or consumer's control may drive\nvariability. Things like network bandwidth limitations or abnormally large service\nprovisioning request can dramatically affect the cost or availability of a service.\nPerformance variability and auditing should, therefore, be directly addressed by the cloud\nsolution architect.\nManagement and governance\nThe ease of use associated with to opening an account and using cloud services creates the\nrisk of abuse in the provisioning and consumption of cloud-based services. Cloud industry\nleaders often highlight this risk as a significant security risk. Organizations must, therefore,\nestablish strict management and governance procedures. Recommendations are to include\ntracking for initiation and use of cloud services like storage, databases, and message queue\nvolumes. Establishment and enforcement of governance are critical to successfully\nfollowing government regulations, as well as industry and geography-specific policies.\nTransaction and concurrency across clouds\nWhen operating across a cloud ecosystem, the sharing of applications and data drives the\nrequirement for ACID transactions and concurrency. Any changes made by any member of\nthe ecosystem must be visible, auditable and reliable. Specific to this requirement is an\nexpanding use of blockchain and related technologies across the cloud computing industry.\n",
      "content_length": 2182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "Operational Requirements\nChapter 11\n[ 231 ]\nSLAs and benchmarks\nCompanies that sign SLA-backed contracts should also establish a standard way of\nbenchmarking CSP performance. SLA should not only specify minimum requirement and\nvariability expectations, but it should also specify appropriate remedies to the consumer \nshould the CSP fail to meet a service level or restore services to the appropriate level within\na specified period. Service definitions and metrics should be unambiguous.\nProvider exit\nThe cloud solution architect should prioritize risk mitigation as part of any solution design.\nSetting this as a priority dictates a carefully designed provider exit strategy plan before\nconsuming any cloud service. Risk mitigation requires the identification and verification of\nsecondary, and in some cases, a tertiary, supplier for all cloud service deemed crucial to the\nenterprise.\nSecurity\nCloud computing security is always a significant concern but focuses primarily on user\ndata privacy. When using cloud services, end users do not have control of storage location.\nApart from SLA-specified limitations, they also lack specific knowledge of storage location.\nSecurity controls\nA security control acts as a tool to restrict a list of possible actions down to those that are\nallowed or permitted. An industry group, called the The Cloud Security Alliance, has\ndocumented a complete list of data security controls in a reference called the Cloud Control\nMatrix. This matrix is an important tool and is designed to help the security professional\nidentify and selected data security controls, based on the applicable industry regulations or\nsecurity governance environment.\n",
      "content_length": 1681,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "Operational Requirements\nChapter 11\n[ 232 ]\nControls are generally described as being within one of three categories:\nAdministrative: regulations, policies, laws, guidelines, and practices governing\nthe overall information security requirements and controls\nLogical: Virtual technical and application controls such as firewalls, encryption,\nanti-virus software, and maker/checker routines\nPhysical: used to manage physical access like a key to a door. Other physical\ncontrols include gates and barricades, video surveillance systems, the use of\nguards, and remote backup facilities\nThese three elements are crucial to an effective control environment but do not give clear\nguidance on the degree to which a risk is mitigated.\nData management controls can be classified as directive or deterrent.\nDirective controls cause or encourage a desirable event to occur, such as\nemployees meeting objectives effectively. Formally written procedure manuals\nwould be a directive control in this case because they would encourage\nemployees to carry out particular functions in an effective manner.\nDeterrent controls are designed to discourage potential attackers by sending the\nmessage that it is better not to attack, but even if you do, the target can defend\nitself. Examples of deterrent controls include notices of monitoring and logging\nas well as the visible practice of sound information security management.\nControls that could be considered as mitigating controls include the following:\nPreventive controls which prevent data loss or harm from occurring\nDetective controls that monitor activity in order to identify where practices or\nprocedures are not properly followed\nCorrective controls which restore the process or system back to a prior pre-\nincident state\nControls that extend data protection include:\nRecovery controls, which restore lost computing resources or capabilities and\nhelp the organization to return to normal operations and recover monetary loss\ncaused by a security violation or incident.\nCompensating controls, which reinforce or replace normal controls that are\nunavailable for any reason. These are typically back-up controls and usually\ninvolve higher levels of supervision and/or contingency plans.\n",
      "content_length": 2224,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "Operational Requirements\nChapter 11\n[ 233 ]\nControls should also be identified as manual or automated.\nSince security is a critical operational element, the solution architect should also take all of\nthe following aspects into account when developing a comprehensive enterprise solution. It\nshould also be noted that many of these components actually fall under corporate\ngovernance and, as such, fall outside of the technical solution design. A failure to address\nthese elements, however, could prevent the deployment and success of any cloud\ncomputing solution. As stated before, cultural change is an essential part of any transition\nto cloud. Effective IT governance is foundational to cultural change.\nThe Cloud Security Alliance has categorized these controls into industry\nstandard control groups. Control group descriptions are provided in the\nfollowing documentation: IUUQT\u001c\u0011\u0011EPXOMPBET\u0010DMPVETFDVSJUZBMMJBODF\u0010\nPSH\u0011JOJUJBUJWFT\u0011DDN\u0011$4\"@$$.@W\u0015\u0010\u0012\u0010YMTY.\n",
      "content_length": 957,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "Operational Requirements\nChapter 11\n[ 234 ]\nDistributed computing reference model\nThe various cloud service models expose applications, platform and infrastructure\ncomponents in many different and unique ways. The different interfaces between the\nvarious components create a foundation for the distributed computing reference model.\nThe open group created the model as a means for identifying and managing the\ninteroperability and portability of cloud computing solutions. In offering the DCRM as a\nvital cloud solution architect tool, this chapter describes its components and processes. The\narchitect should also note that the execution of all interactions is through industry\nstandards, user-developed or vendor-specific APIs, or web services.\nYou can find more information at: IUUQ\u001c\u0011\u0011XXX\u0010PQFOHSPVQ\u0010PSH\u0011DMPVE\u0011DMPVE@JPQ\u0011Q\u0017\u0010IUN.\nSummary\nCloud computing represents a new operational model for delivering information\ntechnology services. This brings with it a number of unique operational requirements. This\nchapter explains each of those within an operational context so that they can be\nappropriately included in every architected cloud solution. Security controls are equally\ncrucial due to the importance associated with managing and controlling access to all data.\nThe DCRM is presented in here, in this chapter, as a communicative tool. While the other\nmodels presented in this text are specific to cloud computing, this one represents a more\ngeneralized approach more suitable when evaluating portability and interoperability when\noperating across different cloud computing and traditional designs.\n",
      "content_length": 1605,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "12\nCSP Performance\nCloud service providers (CSPs) are not all the same. Service performance from the same\nservice provider can vary from day to day. From a consumer point of view, critical\nperformance characteristics vary based on the cloud service model. For end users, SaaS\nperformance measures are perceived as business transaction response times and\nthroughput, technical service reliability and availability, and application scalability. PaaS\nperformance measures, on the other hand, are indirectly perceived by users and defined as\nthroughput, transaction response times, technical service reliability, availability, and the\nscalability of the middleware. Infrastructure performance, capacity, reliability, availability,\nand scalability typically define IaaS performance.\nIn this chapter, we will cover the following topics:\nCSP performance metrics\nCSP benchmarks\nCSP performance metrics\nIn general, performance measures characteristics of the higher service layers depend on\nthose of the underlying technology components. Since consumers typically have no\nvisibility into technology specifics, they can be clueless concerning performance\nexpectations.\n",
      "content_length": 1159,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "CSP Performance\nChapter 12\n[ 236 ]\nThe typical user operational metrics used to evaluate cloud service providers are the\nfollowing:\nService response time (delay): The latency time between service request and\nservice completion\nService throughput: The number of jobs processed by the service provider\nwithin a set time unit\nService availability: The probability that the service provider accepts a customer\nservice request at any time\nSystem utilization: The percentage of system resources being used for service\nprovisioning\nSystem resilience: The stability of system performance over time, especially\nunder bursty loads\nSystem scalability: The ability of a system to perform well with size or volume\nchanges\nSystem elasticity: The ability of a system to adapt to changes in its loads\nThe interplay between multiple CSP performance drivers determines these metrics. One of\nthe most important is the user's geographic proximity to the provisioning CSP data center.\nThe relative geographic location affects the following:\nService response time (delay): Affected by the physical distance between the\ndata center and the number of consuming customers\nService throughput: Dependent on the nature of the network topology and\nnetwork technology between the data center and the consuming customer\nService availability: Dependent on the service capacity and the number of\nconsuming customers from that location\nNon-geographic performance drivers, mostly driven by data center physical and logical\ndesign choices, include the following:\nSystem utilization: The usage rate as a percentage of a system's maximum IT\nservice capabilities\nSystem resilience: System's capacity to recover quickly from a failure\nSystem scalability: Ability to increase or decrease resources applied to a specific\ncustomer's request\n",
      "content_length": 1798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "CSP Performance\nChapter 12\n[ 237 ]\nSystem elasticity: System responsiveness to a customer's request to increase or\ndecrease applied resources\nUnderlying technology variability: Level of technology consistency across a\nprovider's global system\nRate limiting: Overt action by the service provider to limit the amount, quality,\nor responsiveness of a requested service\nLatency: Service delay incurred following an instruction for its consumption or\nexecution\nCSP benchmarks\nAs the number and variety of cloud service providers continue to expand, consumers face a\ngrowing information disparity concerning the level of service they should expect. This\nchallenge is especially perplexing when the consumer tackles the complexity of selecting\ncloud services and service configurations that best meet the price and performance\nrequirements of applications selected for cloud deployment.\nFor a cloud solution architect, this challenge is managed by comparing prospective CSP\nservice levels and capabilities against industry benchmarks for those services. Challenges to\nbeing able to establish useful cloud computing industry benchmarks include the following:\nThe sheer number of cloud service providers and the variety of cloud services in\nthe market\nThe broad geographic expanse of CSP platforms that typically span many\ndifferent locations\nGeopolitical requirements and restrictions\nWide area networking performance\nVariety of CSP business, pricing, and service models\nMultiplicity of service price changes\nVariability of performance within the same service at different times and from\ndifferent locations\nAdditionally, services can be consumed by the hour, month, annually, or through a spot\nmarket. New products are introduced on an almost daily basis, and pricing changes\nweekly. Amazon, for instance, is known to make price changes monthly. One of the best\nindustry benchmark studies was a collaboration between Rice University and Burstorm\nInc., the result of which was the industry's first comprehensive and continuous price-\nperformance benchmark:\n",
      "content_length": 2049,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "CSP Performance\nChapter 12\n[ 238 ]\nUsing a highly automated process, the first benchmark was across seven suppliers\n(Amazon, Rackspace, Google, Microsoft, HP, IBM, and Linode), across three continents\n(North America, Asia, and Europe) , as shown in the preceding tables, with a total of 266\ncompute products spread over three locations per vendor. The benchmark was executed\nevery day, for 15 days. The results were normalized to a 720-hour, monthly pricing model\nto establish the price-performance metrics. These results showed the following:\n",
      "content_length": 544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "CSP Performance\nChapter 12\n[ 239 ]\nThe range of performance within a single provider can vary widely:\n",
      "content_length": 102,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "CSP Performance\nChapter 12\n[ 240 ]\nThe diversity of platforms and solutions offered by different CSPs can result in a\n1-core instance performance variance of as much as 622%:\n",
      "content_length": 175,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "CSP Performance\nChapter 12\n[ 241 ]\nThere is a 4-core compute price performance variation of 1,000%:\n",
      "content_length": 100,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "CSP Performance\nChapter 12\n[ 242 ]\nThere can be instance performance fluctuations of as much as 60% over time:\nThere is a wide variance in the availability and performance of instance types\nwhen measured at different locations.\nRapid changes over time in instance types, pricing, performance, and availability of\nservices by location demonstrates that benchmarking of a small set of instance types in a\nunique event is not sufficient for cloud computing. Even within the short span of this study,\nGoogle updated their infrastructure and pricing:\n",
      "content_length": 546,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "CSP Performance\nChapter 12\n[ 243 ]\nService level agreements\nThe Cloud Standards Customer Council has developed industry best practices for the\ndesign and enforcement of cloud service level agreements. You can find a convenient\nsummary of these recommendations at IUUQ\u001c\u0011\u0011XXX\u0010DMPVE\u000fDPVODJM\u0010PSH\u0011EFMJWFSBCMFT\u0011\nQSBDUJDBM\u000fHVJEF\u000fUP\u000fDMPVE\u000fTFSWJDF\u000fBHSFFNFOUT\u0010IUN.\nSummary\nPerformance metrics are often overlooked when designing and deploying cloud computing\nsolutions. This is especially troubling because service level agreements should be used to\ndefine and track these.\nAs the number and variety of cloud service providers continue to expand, consumers must\nbe proactive in how they identify and select performance metrics, as these should form the\nbasis for the service level agreement. Cloud solution architects must also be able to compare\nprospective CSP service level metrics and capabilities against industry benchmarks for\nthose services.\n",
      "content_length": 940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "13\nCloud Application Development\nThe adoption of the cloud computing model invariably leads to changes in an\norganization's application development process. Changes are due to the cloud service\nconsumer's inability to have any real control over the cloud application's underlying\ninfrastructure. In the traditional software development life cycle, application developers\ncan exert direction and sometimes complete control over the hardware used. When an\napplication is destined for deployment to a CSP, neither control nor even visibility into the\nunderlying infrastructure is possible. Critical aspects of security, including responsibility\nfor executing and monitoring required security controls, is left to the service provider. Data\nsecurity is the primary reason for adherence to the fundamental design principle for cloud\napplication development.\nWe will cover the following topics in this chapter:\nCore application characteristics\nCloud application components\nDevOps\nMicroservices and serverless architectures\nApplication migration planning\n",
      "content_length": 1048,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "Cloud Application Development\nChapter 13\n[ 245 ]\nCore application characteristics\nAll cloud computing applications should adhere to the following basic characteristics.\nLoose coupling\nThe loose coupling of application components operating in the cloud maximizes the ability\nfor each component to be individually self-contained. This approach logically separates\ncomponents and leads to more straightforward and less numerous interactions, which\nimproves application resiliency and portability. Interactions should not be time-critical as\ncommunication latency between cloud-based components cannot be reliably predicted.\n",
      "content_length": 621,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "Cloud Application Development\nChapter 13\n[ 246 ]\nService orientation\nService orientation is a design approach that focuses on the linkage between services and\nservice-based development and the outcomes of those services. It is referred to as service-\noriented architecture (SOA). A service does the following:\nLogically represents a repeatable business activity that has a specified outcome\n(for example, check customer credit, provide weather data, consolidate drilling\nreports)\nIs designed to be self-contained\nIs often composed of multiple different services\nHas its technical details abstracted from the service consumer\nCloud applications are organized as a service, or a set of services, and may use other\nservices. Their most common characteristics include the following:\nStable interfaces: Cloud application interfaces should not vary over time. Any\nvariations should be backward-compatible. Component interface changes could\nrequire significant re-integration with other components, which could negatively\naffect lifetime cost.\nDescribed interfaces: Cloud application interfaces must be human- and\nmachine-readable and describable. Human-readability is needed to support\ncomponent acquisition and integration. Machine-readability is needed for\ndynamic service discovery and composition.\nUse of marketplaces: Application marketplaces afford easy and rapid access to\ncloud-based products and services. Using a marketplace, high product quality\nand consistent device compatibility are assured. They also provide user freedom\nof choice between competing products and reinforce application and data\nportability and interoperability.\nREST: Representational State Transfer (REST) uses uniform interfaces to\nprovide cacheable, stateless, and layered client-server interactions. Using REST,\nevery client-to-server request contains all the information needed to execute the\nrequest. It also enables robust, scalable, and loosely coupled services that contain\nstable interfaces.\n",
      "content_length": 1977,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "Cloud Application Development\nChapter 13\n[ 247 ]\nBase transactions: Cloud applications are usually designed to perform\ntransactions with BASE properties: basic availability, soft-state, and eventually\nconsistent. Traditional transactions follow ACID properties: atomicity,\nconsistency, isolation, and durability. This means that transactions are reliable,\nbut consistency is not possible without sacrificing availability. With BASE,\nreplicated resources are allowed and at least one copy is available but other\ncopies are temporarily in different states. Synchronization will, however, drive\nthem eventually into consistency. Some applications require ACID\ntransactionality. In others, components providing parts of the transaction can use\nBASE transactionality for interoperability with other components:\n",
      "content_length": 806,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "Cloud Application Development\nChapter 13\n[ 248 ]\nCloud application components\nCloud applications consist of the software and programming language combinations used\nto create a web or mobile application. Software application components, client-side and\nserver-side, are also known as frontend and backend. Each application layer builds on the\nfeatures of the one below it, creating an application development stack. The following\nfigure shows the major building blocks of a typical stack:\nServer side\nOn the server side, three development models, or stacks, are widely used in the cloud\ncomputing industry:\nLAMP stack (Linux/Apache/MySQL/PHP)\nWISA stack (Windows/IIS/SQL Server/ASP.NET)\nJava web application stack (Linux or Solaris/Tomcat/MySQL/JSP)\n",
      "content_length": 749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "Cloud Application Development\nChapter 13\n[ 249 ]\nLAMP\nLAMP is considered an open source development stack. Usually, there is no direct cost\nassociated with licensing, installation, setup, or deployment open source software, but the\nprocess does require expertise, and without it, could be very time-consuming. There are\nmany LAMP variations in the market. When deployed, LAMP stack products work\ntogether straightforwardly. Linux has fast performance, but the PHP layer does present\nsome limitations. Since PHP is an interpreted language, the server interprets each PHP\nscript, every time it runs. While this provides benefit by not needing compilation and\ngaining some language perks, the downside is performance. Alternative PHP Cache (APC)\nor other accelerators can enhance performance.\nWISA stack\nThe WISA stack core is the .NET Framework. It is a widely deployed standard and the\nmarketplace has an abundance of certified professionals. This can be very appealing for\nenterprises that enforce Microsoft standards. Benefits include clustering, failover, security,\nautomated administration, and business intelligence features. The .NET framework is\ncompiled just-in-time (JIT), which enables code hiding and increased performance.\nAlthough compiling the .NET architecture only once provides significant performance\nbenefits, it does bring with it a lack of portability. The integrated development\nenvironment (IDE) for WISA is Visual Studio.\nJava\nThe largest Java stack market shares are held by Red Hat and JBoss. Although stack\ncomponents vary, the use of Java code is consistent. Performance and development differ\ndrastically, however. Red Hat stack uses the Linux Tomcat server and is fully configurable\nusing XML configuration files. Because servlets are compiled into JARs, they offer\ninformation hiding and a performance benefit that scripted technologies don't have.\nMySQL works well in a web application. MySQL scales well for particular (that is, read-\nonly) web applications, but lacks DBMS functionality.\nJBoss web server blends an enterprise application resource (EAR) server and a web server\ninto a single product. It also uses the Tomcat server but the database is not specified. JBoss\napplications use the Hibernate persistence manager. Applications are written once and\ndeployed anywhere. The J2EE standard provides many enterprise-level components such\nas transactions and pooling.\n",
      "content_length": 2404,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "Cloud Application Development\nChapter 13\n[ 250 ]\nClient side\nClient-side scripting options are many and varied. Device capability and developer skill set\ndictate selection and use. Some of the more common options are the following:\nJavaScript client-side scripting language\nREXX, which is an IBM mainframes scripting language\nTool Command Language (TCL), which processes strings and passes commands\nto interactive programs\nActive Server Pages (ASP) by Microsoft, which uses scripted pages on the web\nserver and acts as an interpreted interface between the backend application and\nbrowser\nJava Server Pages (JSP) from by Sun Microsystems, which uses scripted pages\nthat are compiled to run on the server as small programs called servlets\nPHP, an open source server scripting language embedded into HTML\nAsynchronous JavaScript And XML (AJAX), which is not a programming\nlanguage but uses XM and HTTP to request data from a web server\nHTML5cthe current version of the HTML standard used for structuring and\npresenting web content\nDevOps\nDevOps combines cultural philosophies, practices, and tools to increase an organization's\nability to deliver applications and services at high velocity. Accelerated velocity is\naccomplished by evolving and improving products at a faster pace than organizations\nusing traditional software development and infrastructure management processes. DevOps\nuses a concept referred to as Infrastructure as a Code where scripting is used to instantiate\ncloud-based infrastructure through the use of APIs. The primary tools for configuration\nmanagement are the following:\nChef: Uses Ruby, a domain-specific language (DSL), to write system\nconfiguration recipes. Chef installation uses a workstation to control the master.\nAgents are installed using the knife tool with SSH. Managed nodes authenticate\nwith the master using certificates.\nPuppet: Used to manage data center orchestration. It works with many operating\nsystems and provides operational support tools for major OSes. Setup requires a\nmaster server and client agents on each managed system. Modules and\nconfigurations use a Puppet-specific language based on Ruby.\n",
      "content_length": 2148,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "Cloud Application Development\nChapter 13\n[ 251 ]\nAnsible: Ansible requires no node agent installation to manage configurations. It\nuses Python with an installation through a GIT repository clone. All functions\nuse SSH. Ansible node management requires appending SSH authorized keys to\neach node.\nSalt: A command-line interface (CLI) tool that uses a push method for client\ncommunication. It is installed on Git or the package management system. Salt\ncommunicates through general SSH. It also includes an asynchronous file server\nfor speeding up file serving. Python or PyDSL are used to write custom modules.\nMicroservices and serverless architectures\nAs cloud computing advances, application design techniques are also advancing. One\nsignificant development is the microservice architectural style. Applications are structured\nan as a collection of loosely coupled services that combine to implement business\ncapabilities. The microservice architecture is used to support the continuous\ndelivery/deployment of large, complex applications. It also enables an organization to\nevolve its technology stack. Another important new approach is the exclusive use of third-\nparty services that provide ephemeral containers using a just-in-time infrastructure\nprovisioning model, called serverless computing. With this execution model, the cloud\nprovider dynamically manages the allocation of machine resources. Pricing is based on the\nactual amount of resources consumed by an application, rather than on pre-purchased units\nof capacity.\nApplication migration planning\nWhile developing applications for the cloud is an essential area for the architect to\nunderstand, most large enterprises have some existing applications targeted for migration\nto the cloud environment. The cloud solution architect is intimately involved in these\nactivities as well. Application migrations typically go through four distinct phases:\nOrganizational assessment\nSolution definition and design\nApplication migration\nApplication operations\n",
      "content_length": 2012,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "Cloud Application Development\nChapter 13\n[ 252 ]\nDuring the assessment phase, the migration team assesses the organization's infrastructure\nand targeted applications for readiness to transition to a cloud computing environment.\nSystems and applications owner interviews are core to this assessment. Application owners\nanswer questions about their current application state as a prerequisite. Cloud adoption\nitself is an application portfolio activity. Interactions and dependencies between business\napplications may be more important than the data or application itself. Dependencies make\nthe upfront screening, analysis, and hybrid infrastructure design crucial to the cloud\nreadiness prescreening. This screening process captures as-is the application architecture\nand current sustainment costs. It also should provide the baseline data for migration option\ncost-benefit analysis to support the stakeholder's decision process. Migrating applications\nare typically targeted for one of four transition processes:\nLift and shift: Required infrastructure is rebuilt using appropriate CSP services,\nand the application is transitioned as-is with no modifications.\nRefactor: Applications designed to operate on customized infrastructure are\nmodified to leverage available cloud services before migrating.\nRebuild: Applications that are still required by the organization but cannot be\nmodified to use available cloud-based services. These applications are redesigned\nand rebuilt before transitioning the process to the cloud.\nRetire: Applications that are no longer operationally or economically viable to\nthe organizations. Associated processes are either eliminated or replaced with\navailable SaaS.\nIn the solution definition and design phase, organizational requirements and associated\nmetrics are used to define, design, and compare candidate solutions. A multi-cloud\nanalysis platform (MCAP) is typically used to support this stage. An MCAP enables\nenterprises, service providers, and systems integrators to model, design, benchmark, and\noptimizes information technology infrastructures. They are also used to design and model\nprospective system architecture alternatives. During this phase, to-be architecture options\nare reviewed to gain insight into mission suitability and an understanding of how\nmigration impacts application performance, security, and scalability. This phase also\nencompasses finalizing all data security control requirements. Operational needs, laws, or\nindustry regulatory dictates drive security requirements. Since security is a shared\nresponsibility between the organization and selected cloud service providers, this activity\nidentifies all required security controls and their application within the solution design.\nIn the application migration phase, applications first migrate into a sandboxed environment\nto complete functional and security testing. After verifying functional capabilities and\nsecurity controls, they are promoted into the production environment.\n",
      "content_length": 2999,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "Cloud Application Development\nChapter 13\n[ 253 ]\nIn the application operations phase, the final operational entity starts managing the\ninfrastructure and application. Users must continually monitor CSP adherence to service\nlevel agreements (SLA) throughout this phase. Continuous monitoring of all cloud-based\nresources is also necessary. The organization should continually recalibrate cost by\ncomparing planned versus actual and should recommends policies to streamline cloud\nusage. As requirements and available market services improve, the transition to other\nservice providers may be a more optimal choice.\nSummary\nCloud applications consist of the software and programming language combinations used\nto create a web or mobile application. These applications should also adhere to cloud-\nfriendly characteristics such as loose coupling, stable and described interfaces,\nmarketplaces, and REST. The most prevalent server-side stacks include LAMP, WISA, and\nJava. Many enterprises have accelerated the application development process by adopting\nDevOps. It uses a concept referred to as Infrastructure as a Code, where scripting is used to\ninstantiate cloud-based infrastructure through the use of APIs. While developing\napplications for the cloud is an essential area for the architect to understand, most large\nenterprises have some existing applications targeted for migration to the cloud\nenvironment. Cloud solution architects should use organizational requirements and\nassociated metrics to define, design, and compare candidate solutions.\n",
      "content_length": 1549,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "14\nData Security\nThe explosion of cloud computing and consumer IT means that your data, as well as data\nabout you, can be virtually anywhere. This expanding concept of data mobility means that\ntraditional security concepts, which focus in depth on infrastructure defense, no longer\napply. Implementing security requirements associated with transitioning from an\ninfrastructure-centric data security model to the data-centric cloud computing security\nmodel are challenging to every modern organization. As the cloud solution architect,\nhowever, your job focuses on how to address this issue without disrupting your end users'\nworkflows. A data-centric security model ensures that the most important asset of the\nbusinesscthe datacis always protected.Having your data everywhere is also critical to the\nsuccess of the cloud computing business model. A data-centric security solution must target\nthe direct protection of the data, not the endpoint devices. Device protection requirements\ninvariably mean additional fortification of the corporate security measures that are\ncurrently in place. Cloud solutions focus on protecting data, files, documents, and folders\nstored and used by the user community throughout its life cycle. They should also protect\nthe data in motion and distributed to employees internally, externally, and to partner\norganizations. By embracing public clouds such as Box, Dropbox, OneDrive, and Google\nDrive, enterprises are embracing these services as opportunities to work smarter, faster,\ncollaboratively, and efficiently. The data itself is an essential component of this digital\ncommerce because it holds intellectual property, employee information, and customer data.\nDeveloping a comprehensive data-centric security program, including data discovery,\nclassification, encryption, and file protection, can uniquely position your organization to\nprotect its data, and make security move with your data to comply with global regulations\nsuch as General Data Protection Regulation (GDPR).\nIn this chapter, we will cover the following topics:\nData classification\nData privacy\n",
      "content_length": 2099,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "Data Security\nChapter 14\n[ 255 ]\nData security life cycle\nThe secure data life cycle has six phases:\nCreate: The generation or acquisition of new digital content, or the\nalteration/updating of existing content. Creation can happen internally in the\ncloud or externally after the data is imported into the cloud. The creation phase is\nthe preferred time to classify content according to its sensitivity and value to the\norganization. Careful classification is necessary because weak security controls\ncould be implemented if the content is classified incorrectly.\nStore: Committing digital data to a storage repository; typically occurs nearly\nsimultaneously with creation. When storing data, protection should align with its\nclassification level and controls, such as encryption, access policy, monitoring,\nand logging, and backups should be implemented to avoid data threats. Content\ncan be vulnerable to attackers if access control lists (ACLs) are not well\nimplemented, or files are not scanned for threats or classified incorrectly.\nUse: Viewing or processing, or otherwise used in some activity, not including\nmodification. Data in use is most vulnerable because it might be transported to\nunsecured locations such as workstations.\nShare: Information made accessible to others, such as between users, to\ncustomers, and to partners. Since shared data is no longer under the\norganization's control, maintaining security can be difficult. Data loss prevention\ntechnologies can be used to detect unauthorized sharing, and data rights\nmanagement technologies can be used to maintain control over the information.\nArchive: Data leaves active use and enters long-term storage. Considerations of\ncost versus availability can affect data access procedures. Data placed in an\narchive must still be protected according to its classification. Regulatory\nrequirements must also be addressed, and different tools and providers might be\npart of this phase.\nDestroy: The permanent destruction of data using physical or digital means (for\nexample, crypto-shredding). The destroy phase can have different technical\nmeanings according to usage, data content, and applications used. Data can be\ndestroyed through the logical erasure of pointers or via permanent data\ndestruction using physical or digital means. Consideration should be given\naccording to regulation, type of cloud being used (IaaS versus SaaS), and the\nclassification of the data:\n",
      "content_length": 2433,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "Data Security\nChapter 14\n[ 256 ]\nAlthough this life cycle addresses the phases data passes through, as shown in the previous\nfigure, it does not address data location, how the data is accessed (device or channel), the\nfunctions that can be performed with the data, or the process of authorizing a given actor\n(person or system) to have access to the data. A secure cloud solution must address all\nthese aspects.The data security life cycle should be managed as a series of smaller life cycles\nrunning in different operating environments. Data can, and does, constantly move into, out\nof, and between these environments. Regulatory, legal, contractual, and other jurisdictional\nissues make keeping track of the physical and logical locations of data a high-priority issue.\nThese aspects also control who is authorized to use the data and, often, the device and\ncommunications channel that can be used. Devices and channels have different security\ncharacteristics and may use different applications or clients.When accessed, a given datum\ncan be acted upon through three specific functions:\nAccess: View/access the data. Access includes creating, copying, dissemination,\nand file transfers.\nProcess: Performing a transaction on data. This includes updating it or using it in\na business processing transaction.\nStore: Store the data for future use (that is, in a file or database).\n",
      "content_length": 1379,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "Data Security\nChapter 14\n[ 257 ]\nFunctions are performed in a location, by an actor (person, application, or system/process,\nas opposed to the access device). Protecting the datum requires the selection,\nimplementation, and enforcement of security controls. Controls restrict a list of possible\nactions down to those who are allowed. The appropriate governance regime typically\ndrives control selection. Applicable governance regimes include the following:\nGDPR: The General Data Protection Regulation (Regulation (EU) 2016/679) is a\nregulation by which the European Parliament, the Council of the European\nUnion, and the European Commission have unified and strengthened data\nprotection for all European Union (EU) individuals.\nSOX: The Sarbanes-Oxley Act of 2002 controls data access to reduce corporate\nfraud.\nHIPPA: The Health Insurance Portability and Accountability Act of 1996 is\nUnited States legislation that provides data privacy and security provisions for\nsafeguarding medical information.\nFedRAMP: The Federal Risk and Authorization Management Program is a\nUnited States government-wide program that provides a standardized approach\nto security assessment.\nPCI DSS: The Payment Card Industry Data Security Standard is a set of policies\nand procedures designed to optimize credit, debit, and cash card transactions\nsecurity. It protects cardholders against misuse of their personal information.\nFERPA:  The Family Educational Rights and Privacy Act is a United States\nfederal privacy law that protects parents and their children's education records\n(that is, report cards, transcripts, disciplinary records, contact and family\ninformation, and class schedules).\nThe Cloud Security Alliance provides a reference known as the Cloud Control Matrix\n(IUUQT\u001c\u0011\u0011DMPVETFDVSJUZBMMJBODF\u0010PSH\u0011EPXOMPBE\u0011DMPVE\u000fDPOUSPMT\u000fNBUSJY\u000fW\u0015\u000f\u0012\u000f\u0013\u0011) that\nlists required data security controls for these and many other industry governance regimes.\nThe cloud solution architect is responsible for identifying all required data controls and\nensuring that the implemented solution enforces the required control on the data, no matter\nwhere the data is located or what actor attempts to access it.\n",
      "content_length": 2175,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "Data Security\nChapter 14\n[ 258 ]\nData classification\nGiven the importance of protecting data at all times and in all places, the most critical data\nmanagement task is data classification. Ideally, data is classified immediately upon creation\nby the entity that creates the data. If this is not done, data needs to be reviewed and\nclassified by others based on the organization's information governance guidelines.\nInformation governance represents the policies and procedures for managing all data and\nshould include the following:\nInformation classification: High-level descriptions of critical information\ncategories. The goal is to define high-level categories to determine appropriate\nsecurity controls.\nInformation management policies: Policies that define allowed activities for\ndifferent data types.\nLocation and jurisdictional policies: Where data can be located geographically.\nLegal and regulatory restrictions drive this.\nAuthorizations: Define which employee/user types are allowed to use or access\nwhich types of information.\nOwnership: The ultimately responsible party for the protection of information.\nCustodianship: Who is responsible for managing the information, at the\ndirection of the owner.\nWhen classifying data, best practice suggests that the schema used should, at a minimum,\naddress the following eight key areas:\nData type (format, structure)\nInformation context\nJurisdiction and other legal constraints\nData ownership\nTrust levels and source of origin\nContractual obligations or business constraints\nValue, sensitivity, and criticality of data to the organization\nObligation for retention and preservation\nThe classification categories should match the data controls used.\n",
      "content_length": 1702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "Data Security\nChapter 14\n[ 259 ]\nData privacy\nCompliance with the relevant privacy and data protection (P & DP) laws (by geography)\nrepresents, both for customers and service providers, an essential factor for the success of\nany cloud computing services implementation. Cloud service customers and cloud service\nproviders must work together to find viable solutions by implementing appropriate\nagreements and controls. The outcome should focus on ensuring defined roles and the\nattribution of due care and due diligence responsibilities. The P & DP regulations affect not\njust those whose personal data is processed in the cloud (the data subjects), but also those\n(the cloud service customers) using cloud computing to process others' data, and those\nproviding cloud services used to process that data (the cloud service providers). Key data\nprivacy roles are the following:\nData subject: An identifiable subject is one who can be identified, directly or\nindirectly, in particular by reference to an identification number or one or more\nfactors specific to his physical, physiological, mental, economic, cultural, or social\nidentity [telephone number, IP address].\nController: The entity which alone, or jointly with others, determines the\npurposes and means of the processing of personal data. When national or\ncommunity laws or regulations determine the purposes and means of processing\ndata, the controller may be designated by national or community law.\nProcessor: A natural or legal person, public authority, agency, or any other body\nthat processes personal data on behalf of the controller.\nData owner: An entity that can authorize or deny access to data and is the\nauthority responsible for its accuracy, integrity, and timeliness.\nIn a cloud deployment, your organization may play any or all of these roles. Your data\nsecurity controls must protect all data as dictated by the data owner. Meeting privacy and\ndata protection laws may require addressing the following data classification aspects:\nP & DP law for any relevant countries or jurisdictions\nScope and purpose of the processing\nCategories of the personal data to be processed\nCategories of the processing to be performed\nData location allowed\nCategories of user allowed\n",
      "content_length": 2239,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "Data Security\nChapter 14\n[ 260 ]\nData retention constraints\nSecurity measures to be ensured\nData breach constraints\nStatus\nPersonally Identifiable Information ` PII\nThe personally Identifiable Information (PII) is data that can be used singularly or with\nother data to identify, contact, or locate a single person. The classification of any specific\ndatum as PII is dictated by laws or regulations of the relevant government or jurisdiction.\nPII can be divided into two categories: linked information and linkable information. Linked\ninformation can be used to identify an individual and includes the following:\nFull name\nHome address\nEmail address\nSocial security number\nPassport number\nDriver's license number\nCredit card numbers\nDate of birth\nTelephone number\nLogin details\nLinkable information is information that, by itself, cannot be used to identify a person but\nthat when combined with another piece of information, could identify, trace, or locate a\nperson. Examples include the following:\nCountry, state, city, postcode\nFirst or last name\nGender\nNon-specific age\nRace\nJob position and workplace\n",
      "content_length": 1105,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "Data Security\nChapter 14\n[ 261 ]\nNon-personally identifiable information (non-PII) is data that cannot be used on its own\nto identify, trace, or identify a person. Examples of this are the following:\nDevice IDs\nIP addresses\nCookies\nSummary\nData security is the core of cloud security. The solution architect must establish and\nmaintain that mindset across the entire organization. Having your data everywhere is also\ncritical to the success of the cloud computing business model and the most critical data\nmanagement task is data classification. An ever changing aspect of data classification is the\nfluid nature of global privacy and data protection (P&DP) laws. Every solution must\nrecognize the multiple data protection roles and ensure that all required security controls,\nfor whatever role the organization plays, are in place.\n",
      "content_length": 833,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "15\nApplication Security\nIn this chapter, we will cover the following topics:\nThe application security management process\nApplication security risks\nCloud computing threats\nThe application security management\nprocess\nThe ISO 27034-1 standard provides a very valuable framework for implementing cloud\napplication security. The standard's underlying principles include the following:\nSecurity requirements are defined and analyzed throughout the application's life\ncycle and managed continually.\nApplication risks are influenced by security requirement type and scope, which\nare driven by (1) business; (2) regulatory; and (3) technological domains.\nApplication security controls and audit measurements costs should align with\nthe targeted level of trust.\nAuditing process should verify that implemented controls are delivering\nmanagement's targeted level of trust.\n",
      "content_length": 863,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "Application Security\nChapter 15\n[ 263 ]\nISO 27034-1 also lays out the components, processes, and frameworks to help organizations\nacquire, implement, and use trustworthy applications, at an acceptable (or tolerable)\nsecurity cost. These components, processes, and frameworks provide verifiable evidence\nthat applications have reached and maintained a targeted level of trust. The recommended\ntop-level processes are as follows:\nThe Organization Normative Framework (ONF) management process, used for\nmanaging the application security-related aspects of the ONF.\nThe Application Security Management Process (ASMP), used for managing\nsecurity for each application used by an organization. This process is performed\nin five steps, shown in the following diagram:\n",
      "content_length": 760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "Application Security\nChapter 15\n[ 264 ]\nThe ONF stores all the organization's application security best practices, or those from\nwhich they will be refined or derived. It comprises essential components, processes that\nutilize these components, and processes for managing the ONF itself. It will contains\nregulations, laws, best practices, and roles and responsibilities accepted by the\norganization. The ONF is a bidirectional process meant to create a continuous improvement\nloop. Innovations that result from securing a single application are returned to the ONF to\nstrengthen all organization application security in the future. Its specific IT governance\ncomponents include the following:\nBusiness context: Includes all application security policies, standards, and best\npractices adopted by the organization.\nRegulatory context: Includes all standards, laws, and regulations that affect\napplication security.\nTechnical context: Includes required and available technologies that apply to\napplication security.\nSpecifications: Documents the organization's IT functional requirements and the\nsolutions that are appropriate to address these requirements.\nRoles, responsibilities, and qualifications: Documents the actors within an\norganization who are related to IT applications. Includes processes related to\napplication security.\nApplication security control library: Contains the approved controls that are\nrequired to protect an application based on the identified threats, the context,\nand the targeted level of trust.\nThe application security risk assessment, the second step in the risk management process,\napplies the risk assessment process at the application level. Its primary purpose is to obtain\nthe organization's approval for a target level of trust through specific application-oriented\nrisk analysis.\nAs the third step of the process, the Application Normative Framework (ANF) is a subset\nof the ONF that contains the information required for a specific application to match the\nrequired targeted level of trust as set by the application owner. It identifies the relevant\nelements from the ONF which are applicable to the target business project.\n",
      "content_length": 2165,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "Application Security\nChapter 15\n[ 265 ]\nThe ONF to ANF is a one-to-many relationship, where one ONF will be used as the basis to\ncreate multiple ANFs:\nProvisioning and operating the application is the fourth step of the ASMP, which involves\nthe deployment and follow-up within the application project. It actually implements the\nsecurity activities contained in the ANF. Application Security Audit is the ASMP fifth step\nand deals with the verification and recording of the supporting evidence regarding whether\na specific application has attained its targeted level of Trust.\n",
      "content_length": 577,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "Application Security\nChapter 15\n[ 266 ]\nA descriptive graphic of the entire ASMP is provided as follows:\n",
      "content_length": 105,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "Application Security\nChapter 15\n[ 267 ]\nApplication security risks\nAfter all appropriate data controls are identified and designed into the cloud computing\nsolution, applications themselves need to be hardened against attack. The best guidance for\nthis hardening process is the OWASP top 10, which lists the 10 most critical web\napplication security risks. Refer to IUUQT\u001c\u0011\u0011XXX\u0010PXBTQ\u0010PSH\u0011JNBHFT\u0011\u0019\u0011\u0019\u0014\u001108\"41@5PQ@\u0013\u0012\u000f\n\u0014\u0012\u0013\u0019@\u0007\u0014\u001aFO\u0007\u0014\u001b\u0010QEG\u0010QEG for a detailed list along with the description of each risk.\nCloud computing threats\nThe most critical threats to cloud-based applications have been enumerated by the Cloud\nSecurity Alliance. Referred to as the Treacherous Twelve, a secure cloud solution must\nprotect all applications and processes against these attack vectors. Refer to IUUQT\u001c\u0011\u0011\nEPXOMPBET\u0010DMPVETFDVSJUZBMMJBODF\u0010PSH\u0011BTTFUT\u0011SFTFBSDI\u0011UPQ\u000fUISFBUT\u00115SFBDIFSPVT\u000f\u0013\u0014@\n$MPVE\u000f$PNQVUJOH@5PQ\u000f5ISFBUT\u0010QEG for a detailed description of each of these critical\nthreats.\nSummary\nApplication security requires data security supported by a standardized and consistent\napplication development process. The ONF and ANF provide the standardization and the\nASMP delivers the consistency. Cloud computing is often referred to as the industrialization\nof IT, but that value can be irretrievably damaged by a flawed application development\nprocess. Applications are key to an ability to deliver value with any solution so the architect\nshould definitely drive toward the ideals presented in this chapter.\n",
      "content_length": 1482,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "16\nRisk Management and\nBusiness Continuity\nCloud computing solutions balance the risk of data and information loss against the\nbusiness and mission value of using the cloud. This chapter gives you the tools needed to\npresent both sides of the equation to management so that they can make the many complex\ndecisions embedded throughout the cloud solution architecting process.\nIn this chapter, we will be covering the following topics:\nFraming risk\nAssessing risk\nMonitoring risk\nBusiness continuity and disaster recovery\nFraming risk\nThe risk in cloud computing is multifaceted and involves multiple participants. The entire\nmodel relies, in fact, on a shared risk model between the providers and the consumers.\nEnterprises using cloud assume risks as part of an interrelated service ecosystem that may\nnot be controlled by the internal IT department. Traditional risk management design is\ntargeted for low uncertainty environments that have few interconnections. The risk in\ntoday's networked world, however, is managed in an environment of high uncertainty and\ndynamically changing, interconnected systems. Key cloud computing risks include the\nfollowing:\nFailure to meet financial objectives\nInability to work within the context of corporate organization and culture\n",
      "content_length": 1270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "Risk Management and Business Continuity\nChapter 16\n[ 269 ]\nUnsurmountable difficulties in integrating the cloud services involved\nInability to comply with legal, contractual, and moral obligations\nInability to recover from a disaster\nTechnically inadequate cloud service\nInadequate solution quality\nA definition for each of your organization's specific risks should be developed and agreed\nupon at the start of any transition to cloud computing.\nAssessing risk\nThe first step in managing an organization's risk of adopting cloud computing is an\nassessment. The assessment should evaluate financial, culture, service integration,\nregulatory compliance, business continuity, and business or mission system quality.\nThe impact of financial risk is always critical as it directly drives the return on all\ninvestments associated with a cloud computing transition. When using cloud services, costs\nare directly related to workload and revenue. While this model does reduce some financial\nrisk, it affects other factors differently. The critical assessment factors for cloud ROI risk\nprobability are the following:\nUtilization\nSpeed\nScale\nQuality\nThese four factors drive ROI directly because they affect revenue, cost, and the time\nrequired to realize any investment return. Differences between actual and projected values\nindicate a likely failure to achieve the desired ROI.\n",
      "content_length": 1371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "Risk Management and Business Continuity\nChapter 16\n[ 270 ]\nManaging the cultural impact of a cloud transition is, in many ways, the most challenging\naspect of deploying a cloud computing solution. When adopting any cloud-enabled\nbusiness processes, organizational executives must project clear vision, direction, and\nsupport for all associated business transformations. Establishing a precise procurement and\nimplementation roadmap is imperative and may require significant training of the\nacquisition and legal team. Stakeholder coordination and reconciliation between competing\nstrategies are needed to build internal consensus for storage, computing, and networking\nservices. Tasks associated with migrating applications require a thorough understanding of\ncustomer demand. Pilots and demonstrations should be used to create confidence and to\nbuild buy-in and cloud service usage in the user community. They also help in building\nrequired transition skills and cloud technology knowledge. The organization's financial\ngovernance and acquisition processes may need to be modified to effectively and efficiently\nleverage the cloud computing economic model.\nOrganizations of any size typically consume services from three or more cloud service\nproviders. This Service integration risk requires both process and technology integration\nefforts. There is a risk that this integration does not deliver the expected results. An\nassessment of service integration risk consists of an evaluation of technical interface details,\nthe organization's ability to modify the existing system, and the skill sets available within\nthe team. Interface details provide data to support integration costs. Solution architects can\nobtain an initial qualitative estimate by classifying all interface points as required using one\nof the following:\nSyntactic conversion, which is relatively straightforward\nSemantic compatibility modification, characterized as possible but expensive\nProcess model changes, which would be required if the services have radically\ndifferent process models\n",
      "content_length": 2062,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "Risk Management and Business Continuity\nChapter 16\n[ 271 ]\nA similar triage process is used when evaluating the organization's ability to change the\nexisting system. Risks are high in any redevelopment effort.\nA significant source of regulatory compliance risks is mandatory or required interfaces with\nexternal services or systems. Driven by regulations or company policy, these usually\nrestrict data to particular geographical areas or legal jurisdictions. There may also be a\nminimum set of security, integrity, or confidentiality controls. Online or offline retention\nperiods are also often dictated. These types of restriction are particularly applicable to\npersonal and financial data. While the impact of failing to meet such regulations varies, it\ntypically includes financial penalties and operationally detrimental enforcement actions.\nExternal cloud supplier dependencies can increase the probability of non-compliance, even\nif compensating contracts clauses are in place, because force majeure may prevent the\nsupplier from honoring them.\nBusiness continuity management risk can arise from external services, internal systems, or\nphysical disasters. Business events such as mergers and acquisitions of suppliers,\nunforeseen bankruptcy, or contract cancellations could also affect operational continuity.\nCloud computing models can make it harder to respond to these types of changes due to\nthe reduced level of direct control. As part of a risk analysis, assess the probabilities and\nimpact of unplanned events that could harm the enterprise. Also make general provision\nfor unforeseen events that disrupt the cloud services that are used, or damage their data.\nHaving first identified the risks, build into the solution design elements that reduce their\nprobability or mitigate their effects.\nThis is always a risk that the solution fails to live up to the end user's expectations.\nClassified as system quality risk, the impact can be seen in reduced margin and loss of ROI.\nSpecific quality areas of concern are the following:\nFunctionality: Risk associated with the use of external cloud-based systems, or\nnon-cloud-related factors such as the quality of the solution specification.\nPerformance: Failure to meet required operational or technical metrics.\nAvailability and reliability: Insufficient reliability as measured by mean time\nbetween failure (MTBF) and the mean time to repair (MTTR).\nFault tolerance: Excessive availability risk caused by a single point of failure\n(SPOF) or an inability to accommodate multiple failures within a specified\nservice window.\nRecoverability: Inability to recover from a failure or excessive data loss should a\nfailure occur.\nResponsiveness: Solution that is not sufficiently responsive as measured by user\nresponse times and response variability specifications, primarily if the degraded\nresponse is due to throughput overload.\n",
      "content_length": 2883,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "Risk Management and Business Continuity\nChapter 16\n[ 272 ]\nManageability: Factors of configurability, reporting, and fault management,\nmainly when associated with the provisioning of cloud services.\nSecurity: Risk associated with internet accessibility and the shared security\ncontrol model. Failure to meet security requirements can result in financial loss,\ndata unavailability, sensitive information leakage, reputational damage, and\nfailure to meet privacy regulations. The use of multiple CSPs can also lead to\nelaborate security arrangements, introducing the possibility of gaps in the data\nsecurity defenses.\nMonitoring risk\nRisk management is an integral part of the solution architecture development. Risk\nassessments should, therefore, be repeated at every significant decision stage of the\narchitecture development process. This ensures that the levels of risk exposure continue to\nbe acceptable. Since cloud service procurement is an operational expenditure and not a\ncapital expenditure, cloud solutions must include a continuous service monitoring\ncomponent.\nCloud service risk assessment for suitability is completed at the start and throughout the\nsolution's lifetime. Services should also be reevaluated if the service provider introduces\nchanges or if alternative service options are made available in the broader marketplace.\nThis requirement is the basis for maintaining and updating industry benchmarks for every\ncritical cloud solution service. Industry benchmark data is also an important input to CSP\nservice level agreement (SLA) negotiations.\nBusiness continuity and disaster recovery\nA solution is worthless if it cannot deliver service to its intended consumers. This is why\nbusiness continuity and disaster recovery should always be included when architecting a\ncloud computing solution. Although the solution architect may exert minimal influence on\na solution's operational deployment, the good solution architect considers the following\nkey BCDR questions before presenting a recommended solution:\nCan the recommended cloud service provider deliver the required service\nelasticity if BCDR is invoked?\nAre any other CSPs capable of delivering all the required services under a similar\nSLA?\n",
      "content_length": 2221,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "Risk Management and Business Continuity\nChapter 16\n[ 273 ]\nDoes the recommended CSP have available network bandwidth for timely\nreplication of data?\nWill there be available bandwidth between the impacted user base and the BCDR\nlocations?\nAre there any legal or licensing constraints that prohibit the data or functionality\nto be present in any CSP data center location?\nCloud solution disaster recovery options fit into three broad categories:\nOn-premises data center uses a CSP to support BCDR requirements\nCloud service consumer depends on the CSPs redundant infrastructure to\nsupport BCDR requirements\nThe cloud service consumer moves from the primary CSP to a secondary CSP to\nsupport BCDR requirements\nThe cloud solution architect should recommend the most practical of these options as the\nBCDR path for any recommended cloud solution. The impact of a BCDR scenario on all\ncritical risk elements should be considered as supporting data in consideration of the\nfollowing planning factors:\nEnumeration of the importance and priority of data and critical organizational\nprocess assets\nThe current locations of these assets\nNetwork bandwidth and transport cost between data assets and all relevant\nprocessing sites\nActual and potential location of enterprise workforce and business partners\nEnumeration and prioritization of anticipated disaster events and scenarios\nProcess for initiating BCDR activities for each anticipated event or scenario\nReturn to normal process for each event or scenario\nSummary\nCloud computing brings risk management to the forefront of information technology. The\nrisk-averse reflexive management decisions of the past will result in rapid business failure\nin today's world. The smart use of other people's infrastructure, also known as cloud\nservice providers, demands a robust risk management process, with continuous monitoring\nand rapid reaction. BCDR itself is a risk management process that should also leverage CSP\ncapabilities.\n",
      "content_length": 1966,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "17\nHands-On Lab 1  Basic Cloud\nDesign (Single Server)\nCloud architecture can be difficult; at times, we make it more difficult than we need to.\nCloud is shifting everything because it is an economic innovation, not a technical one.\nCloud is driven by economics rather than technology. Each new service continues to drive\nprogression via economics by enabling the realignment of strategy, technology, and\neconomics. Containers and serverless are using new economic models to change the way\ninfrastructure and software are deployed. Because the cloud is primarily economics and\nstrategy, it requires updates to skill sets and additional data for decisions.\nThe cloud is an answer, but not the answer to everything. Cloud does not make bad\ndecisions better. The cloud is a tool. The cloud is a philosophy, a strategy, a mindset, and an\nattitude. Above all, cloud is a process. A single aggressive move from CAPEX to OPEX is\nlikely to be expensive; it will probably fail, and probably will not solve much. Fork-lifting\nthe same design from an on-premises data center to an off-premises service provider will\nmove the problem, but not solve it. Cloud success requires research, change management,\ngovernance, and comparative design. Every design choice affects economics, strategy,\ntechnology, and risk.\nHands-on labs and exercises\nThe next three chapters will discuss the impact of design choices at increasing levels of\ncomplexity. These chapters are meant to be used as a step-by-step hands-on guide that will\nnavigate through designs and design choices, yielding real-time insight each step of the\nway.\n",
      "content_length": 1603,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 275 ]\nThis chapter will start with a single-server infrastructure, then we will accelerate into more\ncomplex insight and scenarios in $IBQUFS\u0002\u0013\u001b, Hands-On Lab a Advanced Cloud Design Insight\nand $IBQUFS\u0002\u0014\u0012, Hands-On Lab 3 a Optimizing Current State (12 Months Later). It is suggested\nto navigate these example chapters in order, as each one builds on the previous.\nComplexity with each example grows, adding considerations for applications, application\nstacking, utilization, and general market and current trends. The examples and exercises in\nthe book will also be accessible via the Burstorm platform, with unlimited use for 30 days.\nComplexity\nCloud is typically associated with outcomes such as lower cost, speed, and simplicity, yet\ncloud can be very complex even in its most basic form. For example, a single server can\nhave many attributes that must be considered. How many cores? How much RAM? How\nmuch storage? Is it a virtual server or a physical one? What operating system? What type of\nconnectivity? Is the server on a shared or dedicated environment? What about going\nserverless? What about containers?\nThe answers to these seemingly simple questions have a drastically different economic\nimpact and a huge effect on strategy. Each attribute feels somewhat technical in nature, yet\nthey are more about economics and how economics affect strategy. Why would virtual\nservers be chosen over physical? Better utilization? Isn't utilization really about maximizing\nthe use of an expensive resource? Virtualization allows for the acquisition of only what is\nneeded for as long as needed. Not really. Virtualization has been around since the 1960s.\nRecent billing innovations are what allow partial resources to be consumed in very short\nincrements of time. Virtual servers can be deployed faster. True,  but why does that matter?\nPhysical deployments are very manual, expensive, time-consuming, and potentially filled\nwith human error. Virtual machines can be deployed very quickly and programmatically,\neliminating much of the expense, time, and effort associated with deployments.\n",
      "content_length": 2156,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 276 ]\nVirtualization and its benefits are well known. Designs using virtualization have been\naround for several years now. What is so different? For the first time, we see economic\nmodels driving design decisions, for example, reserve instance versus current market rate.\nReserve instances require a large upfront fee with a very low monthly fee. What situations\nare better suited for a longer-term commitment with significant money up front? What\nstrategy does this line up with? How does this affect risk? With the high fees up front and\nlonger commitment requirements, reserve instances are better suited for persistent\nworkloads with fairly flat traffic patterns. Cyclical or seasonal traffic patterns do not fit\nhere, as resources would be paid for when they were not being utilized fully. A major\nchange, as mentioned earlier, is that designs can now be created for the low point with\nburst or up-cycle moves to support increases in the traffic pattern.\nEliminating the noise\nSuccessful next-generation designers are able to quickly triage true requirements from\nwants and wishes. Much of the truth is drowned by emotions, agendas, hype, marketing,\nand other forms of distracting noise. Simplify, then build. Quickly get to the lowest and\nsimplest common denominator and add where truly needed. Every server and GB of\nstorage requires monitoring, administration, management, and all other care-and-feeding\ntype activities. Poor choices at basic infrastructure levels can dramatically affect economics\nas all of the other requirements are piled on.\nA single server is not as simple as it sounds. The following diagram shows a set of basic\noptions that can be applied to any server. There are a number of options for each attribute,\nof which one is chosen. The chart shows almost 6.3 billion potential combinations for this\nsingle server. Considerations for other attributes, such as external storage, port\nconfiguration, software, patch level, and so on, have not been accounted for. The potential\ncombinations can quickly reach into the trillions for a single server when all attributes are\nconsidered. Add in additional combinations when adding in additional servers, licensing\noptions, additional devices, additional potential locations, potential providers, pricing\noptions, business models, consumption rules, deployment rules, and the many other\nnuances that permeate every solution design.\n",
      "content_length": 2467,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 277 ]\nIn the following example, we see three different term options. This may equate to 12-, 24-,\nor 36-month terms, with only one term being chosen. We see that cores, in this example, can\nbe any number between 1 and 12. RAM could be anything between 1 and 16. Obviously,\nthere are many other options and add-ons, such as monitoring, management, licensing, and\nso on. But just basic server configuration choices already place this single server into the 6+\nbillion combination range:\nBurstorm lab 1 ` background (NeBu Systems)\nAll of the hands-on exercises will be for a company named NeBu Systems. NeBu creates\nsoftware for the automotive industry. New cars have almost as much processing power\nwithin them as full data centers in recent years. With all of the sensors gathering IoT data\nand the tremendous compute power available for processing, NeBu is trying to transition\naway from large monolithic legacy applications to highly flexible cloud-based modular\nfunctions aimed at changing the automotive experience. The goal is to be positioned to\nadapt as some functions become widely adopted while others are driven to satisfy certain\nniche markets. Ideally, functions are added as custom apps similar to adding apps to cell\nphones or picking car colors and upholstery types.\n",
      "content_length": 1346,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 278 ]\nIn this first lab, NeBu is developing a new application that will be engineered for the cloud\nfrom the beginning. No legacy code to deal with. No legacy dependencies or specific\nhardware requirements complicating things. The code will be written using modern\nlanguages, eliminating concern over hardware compatibility.\nBurstorm lab 1 ` getting started\nPlease send an email to TVQQPSU!CVSTUPSN\u0010DPN with the following:\n(Required) current email address (must work as initial password information\nwill be sent to this address)\n(Required) full name\nPlease include the following code within the subject of the email: NeBu214495\n#VSTUPSN\u0002MBC\u0002\u0013\u0002`\u0002DSFBUJOH\u0002OFX\u0002NPEFM\nPlease go to IUUQ\u001c\u0011\u0011BQQ\u0010CVSTUPSN\u0010DPN\u0011MPHJO and enter your email address and\n1.\ntemporary password, which will need to be changed once you have logged in\nFrom the dashboard/home screen, please click on Design:\n2.\nClick on New Project | Model:\n3.\n",
      "content_length": 974,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 279 ]\nA dialog box will appear, asking for basic information to be entered:\n4.\nPlease enter a Model Name\n1.\nPlease change the View field from My Organization to Myself\n2.\nScroll to the bottom and select Create:\n3.\n",
      "content_length": 279,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 280 ]\nCongratulations, your model has been created. In this platform, a model is figuratively a\nscope or a problem that is trying to be solved. There may be multiple ways to solve a\nproblem or series of problems. NeBu is trying to transform from large monolithic apps to\nsmaller functional blocks of code and smaller targeted apps.\nThere are many ways to navigate that scope or problem. Should NeBu deploy on existing\ninfrastructure? Should it be deployed on-premises or off-premises? Should NeBu deploy it\nwithin existing collocation environments? What about virtual machines as a cloud service?\nAll are potential options. How do we start sorting it out?\nAs discussed in this book, there are many things to consider when assessing current cloud\nreadiness, developing new applications using new styles of infrastructure consumption and\ndeployment patterns, and recycling/up-cycling existing code bases. This book has discussed\nmany approaches and frameworks that can be used. After several internal meetings,\ndiscovery sessions, and planning conversations, NeBu has determined a path forward.\nNeBu has chosen to develop new code that will be deployed on Linux servers and begin to\nembrace more of the open source community.\nAs code begins to navigate through the development lifecycle, resource requirements tend\nto increase with each stage. Initial development is handled by relatively few people,\nrequiring minimal infrastructure when building and testing initial rounds of code. As the\ncode progresses, more people and infrastructure are needed to perform logical and\nresource testing. During testing and each development stage, developers must determine\ninfrastructure requirements for initial deployment and anticipated capacity plans.\nResponsible testing should yield logical and resource constraints that will determine initial\ndeployment and growth increments. Wouldn't these answers change based on the provider\nand infrastructure chosen?\nHow can initial anticipated performance level and basic resource requirements be\ndetermined if the infrastructure options, pricing, and performance are unknown? Based on\nBurstorm ongoing benchmark data, the same instance type within the same provider, at\ntwo different locations, has shown up to 700% different in performance tests. These\nperformance differences can dramatically change infrastructure requirements, deployment\nstyles, and associated solution economics. In the next exercise, we will begin examining\ncharacteristics and attributes that will help determine a short list of potential providers and\ninstance sizes that match up to technical, strategic, and economic requirements.\n",
      "content_length": 2706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 281 ]\nReturn to the initial model created and verify. The model should appear as a blank drawing\nboard as shown in the following screenshot. In the following screenshot, a model named\nSingle Server (Reference) is shown. This model was created and shared as a follow-along\nmodel that can be viewed if you choose to do the configuration work later. The reference\nmodel is also meant to be used as a reference to check progress and see the results expected:\n#VSTUPSN\u0002MBC\u0002\u0013\u0002`\u0002DSFBUJOH\u0002B\u0002EFTJHO\u0002TDFOBSJP\nAs mentioned, understanding available solution components, where they are available,\nwhat they cost, and how they can be combined with other services is very helpful when\nused from the very beginning. In this example, a single server design will be created to help\nidentify potential providers, configurations, and services that may affect final solutions as \ndevelopment cycles move closer to production deployment at NeBu Systems:\nStarting at the top center of the page, please click and drag the Design icon onto\n1.\nany blank space on the drawing board.\nThe application will create a new scenario as shown in the following screenshot:\n2.\n",
      "content_length": 1205,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 282 ]\nPlease enter a name for the scenario. In this example, I have used 4JOHMF\n3.\n4FSWFS. The name can be anything you choose that is helpful as you try to\nremember what it is for:\nService providers deploy products and services in specific locations. Services are\n4.\nnot generally available anywhere, although some can be (for example, equipment\ndeployed on client premises). In-house services are also only available in very\nspecific locations. For this reason, one of the initial scenario defining\ncharacteristics is location. NeBu Systems has chosen to deploy the new\napplication in a central location within the US. The Midwest has additional\nbenefits, with lower risk factors than higher-profile, more densely populated\ncities such as New York and Los Angeles. The threat of natural disaster is much\nlower than California. Please enter the initial location as $IJDBHP\u000e\u0002*-, shown in\nthe following screenshot. Chicago also has very good connectivity options as\nmany of the carriers pass through in large telco hotels.\n",
      "content_length": 1087,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 283 ]\nPlease add $IJDBHP\u000e\u0002*- to the Locality field. This will set the general search\n5.\nepicenter for potential solution products and services:\nNot every product or service will be located in Chicago, IL. NeBu Systems does\n6.\nnot have a firm requirement that forces it to locate within Chicago. Since other\npotential providers and locations may be available within an acceptable distance\nfrom Chicago, please enter \u0015\u0012\u0012 as the search radius to be used when mapping\nand matching acceptable providers, products, and services:\nProducts and services not only have specific locations, but they also have\n7.\npredetermined business and consumption models. As an example, reserve\ninstances from AWS have an economic and consumption model that requires a\nminimum commitment of a 12-month term with significant non-recurring cost\n(NRC) due up front. Once the upfront (NRC) costs have been paid, a smaller\nongoing monthly payment is required for each month of the specified\ncommitment term. NeBu systems have strategic interest and a financial policy\nthat desires more emphasis on preserving capital favoring operational expense\n(OpEx) driven solutions.\nPlease leave the Term field blank. This will indicate that the minimum term is\n8.\nnot specified and will allow economic models with no minimum term to\nrespond. If a reserve instance model were required, a minimum of 12 months\nwould be entered instead of leaving the field blank:\n",
      "content_length": 1486,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 284 ]\nPlease leave the % NRC field set to the default of Any NRC as shown in the\n9.\nfollowing screenshot. This example will not limit the NRC amount:\nPlease leave Compliance set as default and choose Create at the bottom.\n10.\nThe result should show a single design scenario on the board titled with the\n11.\nname previously entered:\n",
      "content_length": 397,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 285 ]\nNeBu chose to deploy on Linux servers. Application and code design\nconversations have not been able to provide a consensus for a single provider, or\nshortlist of providers. Different stakeholders have their own agendas and\npriorities that are being brought to the table. One of the administrators wants it to\nbe housed at Google because he has good relationships from engagements\ncompleted with Google Cloud at previous employers. NeBu developers like the\nidea of using AWS due to the scope of all available cutting-edge services. Sales\nlikes the idea of using Azure as many of the clients are comfortable with Azure\nand like where the direction and progress Azure has made recently.\nWhat is needed to resolve this debate? Data. Real-time analytics and performance\ndata will help in many ways. Many people may consider RFP, RFI, or RFQ type\nprocesses. For this exercise, a Linux server can be pulled into the scenario and\nprovide real-time data that may help navigate the dynamics of this internal\ndebate as a provider, or set of providers, are chosen.\nPlease click on Compute near the top left of the design board as shown in the\n12.\nfollowing screenshot:\nA series of preconfigured compute options will show across the grey ribbon, as\n13.\nshown in the following screenshot. Please click on the first icon, named Linux,\nand drag it into any empty space within the scenario box created earlier:\n",
      "content_length": 1465,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 286 ]\nThe Linux icon should now appear in the box with a dialog window open on the\n14.\nright side of the screen:\n",
      "content_length": 178,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 287 ]\nNeBu Systems anticipates the new application workloads, and workload types to\n15.\nutilize far less compute power when compared to RAM requirements. The\ncurrent plan will utilize virtual servers on a shared platform to continue proving\nthe concept, test code, and baseline initial performance characteristics. The initial\nconfiguration will start at 1 core and 8 GB of RAM. We will address storage at a\nlater step.\nPer the NeBu requirement, please update the RAM from \u0013 to \u0019 (yes, the earlier\n16.\nstatement mentioned 8, please use 7). Also, please clear the storage amount from\nthe storage line. Storage will be addressed in a later step. Please refer to the\nfollowing screenshot to verify the configurations match:\nPlease click the blue Advanced bar to drop down the additional options. Please\n17.\nverify that Is VM? and Is Shared? are set to Yes. NeBu's initial solution requires\na virtual server from one of the shared IaaS providers available in the current\nmarket:\n",
      "content_length": 1040,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 288 ]\nPlease click the Save button at the bottom. The server configuration should\n18.\nupdate to match the following screenshot:\n",
      "content_length": 193,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 289 ]\n#VSTUPSN\u0002MBC\u0002\u0013\u0002`\u0002EFTJHO\u0002TDFOBSJP\u0002TPMVUJPO\u0002SFTVMUT\nFor NeBu to move forward most effectively, they have requirements to control cost while\nacquiring as much performance as possible per dollar spent. There are many providers in\nthe world, with more arriving daily it seems. Each provider has their own personality,\ndeployment style, consumption model, pricing model, and unique combination of available\nproducts and services. How can a shortlist of favored providers be assembled? Two\nrequirements and prioritizing characteristics have been stated previously: cost and\nperformance. We can start there:\nPlease click on the hamburger menu in the top-right corner of the design scenario\n1.\nbox and choose BurstormIQ from the drop-down menu:\nIn real time, the platform will return a set of results based on real-time API\nconnected providers, their available products, services, consumption rules,\ndeployment rules, and pricing. Results may vary depending on when you access\nthe real-time data. Data updates/changes often:\n",
      "content_length": 1087,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 290 ]\nIn order to choose a provider and technology partner as NeBu moves forward\n2.\nwith testing and deploying the new functions and applications, NeBu would like\nto see more provider data from a wider set of providers. Please click on the blue\nbar with the scenario name in it. This will open the characteristics and attributes\nof the scenario itself. Please change the distance to \u0017\u0012\u0012 miles. Click Save at the\nbottom:\n",
      "content_length": 485,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 291 ]\nInstantly, NeBu has access to more providers and potential service options that\nmatch the stated requirements for strategy, technology, and economics:\n#VSTUPSN\u0002MBC\u0002\u0013\u0002`\u0002IJHI\u000fMFWFM\u0002SBQJE\u0002JOTJHIUT\nSuccessful cloud deployment requires not only good strategic, technical, and financial\ndecisions; it also requires significant change management and governance. The real-time\ndata presented in this single view contains solution options from multiple providers and\nsome very interesting insight that NeBu Systems can use to not only support decisions but\nalso help promote effective change management and establish governance.\n",
      "content_length": 691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 292 ]\nNeBu expressed interest in being near Chicago, IL. Truly, it was a desire to be in the\nMidwest to avoid many of the potential disasters associated with the higher-profile cities\nand regions in the US. NeBu also required good connectivity options, as clients would be\naccessing from several locations across the country. Resiliency was important, along with\ncontrolling costs and gaining as much performance as possible for money invested.\nData shown within Bustorm has been automatically normalized so that it can be compared\nin real time. One of the largest challenges with architecting cloud solutions is gathering and\nnormalizing relevant data. Designers, architects, strategists, and stakeholders must gather,\nnormalize, and compare data for the current state in various forms. Current state billing\ndata is a fairly common starting point. Billing data is compared against deployed\ninformation, which is then also compared to actual consumed detail and ultimately\ncompared to potential future state options.\nNeBu Systems chose to start greenfield as strategically there was not enough value in trying\nto repurpose or up-cycle what has already been deployed. Current state still needed to be\nconsidered, normalized, and compared. Starting with a new environment does not mean\ncurrent state is completely ignored. In many cases, current state must be evaluated as an\noption until it is proven to be less than desirable.\nArchitecting cloud solutions is about alignment and balance. Successful solutions require\nrisk to be offset by economics. Technology helps accomplish strategy requirements, with\nstrategy influencing technology choices. At times, technology choices can also dramatically\nimpact economics while economics can certainly influence technology choices. In the\nsolution results side of the design board, a greater than 300% difference in price can be seen\nfrom low to high. There is a 50% difference that separates the three lowest-cost providers.\nThe request was the same; why are the prices so different? Is it performance difference?\nResiliency? Location? Size of infrastructure? Brand value? There are many factors that can\naffect cost. Many of these questions will be answered in the next chapter as we dive deeper\ninto the insight needed for successful design and architecture.\nAnother interesting insight the data presents is that none of the three lowest-cost providers\n(Google, AWS, and Azure) are in Chicago. If NeBu Systems' requirement was to be in\nChicago, CenturyLink and Rackspace become the only options available. Data also shows\nAWS to be the highest performing of the solutions available based on the components\nrequested. Performance is a big requirement for NeBu Systems. Quickly spotting that detail\nis covered in depth in the next chapter.\n",
      "content_length": 2849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "Hands-On Lab 1  Basic Cloud Design (Single Server)\nChapter 17\n[ 293 ]\nSummary\nSolution design and architecture can be filled with lots of unnecessary noise and\ndistractions. Much of the information can be misleading and is often misrepresented.\nReturning to the basics and starting with things that are non-negotiable to establish baseline\nrequirements is the best way to begin. Start with a few high-level requirements and let the\ninsight become the foundation, not the technology. Build upon the insight, which enables\nproper alignment and balance.\nIn this chapter, NeBu Systems has been able to start with a very basic set of requirements,\nquickly assess initial economic impact, identify a short list of providers to focus on, and\nquickly confirm some of the strategic and technical pieces. Beginning with non-negotiable\nrequirements helps solution designers and architects avoid unnecessary complexity and\nscope-creep. This iterative method allows data to expose additional insights that may affect\ndirection and choices that would have been otherwise missed.\nThe next chapter will explore deeper-level detail and additional insight that helps refine\ncloud solution design.\n",
      "content_length": 1180,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "18\nHands-On Lab 2  Advanced\nCloud Design Insight\nSuccessful cloud design requires good data. Successful cloud design, more importantly,\nrequires the effective communication of decision supporting data. Many transformation\nprojects fail from poor communication, poor execution, and a lack of adoption, all\npotentially resolved with well-executed change management and communication plans.\nSuccessful solutions require real-time data; the same data that is very beneficial when used\nfor change management and communication plans.\nIn this chapter, real-time data takes center stage. Additional scenario data and insights will\nbe examined in greater depth. Additional infrastructure design options and ideas will be\nexplored. Later in the chapter, additional services and application data will be factored into\nthe options reviewed and decisions made.\nWe will learn about the following topics in this chapter:\nData-driven design\nBurstorm lab 2\nData-driven design\nIn the previous chapter, complexity was painted somewhat as a villain. Complexity itself is\nnot the villain; complexity without data to support it is. While implementing cool features\nbased on the data outlined in successful stories within popular blog posts and magazine\narticles could be considered data-driven, it is not really a data-driven approach that leads to\nsuccess most often. Transformations are hard enough without trying to recreate someone\nelse's story. Why are transformations so hard? Cloud is supposed to make things much\neasier to align and implement.\n",
      "content_length": 1531,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 295 ]\nHow tough can cloud transformation be? Choose the number of cores and RAM. Add\nstorage. Pick a virtual server with the desired OS. Give it some bandwidth and start loading\napplications. Easy. Even easier: put it in a container. It spins up fast, is very portable and\ncheap. Awesome. Wait, better yet, go serverless. That removes the server, correct? (Insert\nlaughing audio file here).\nTransformations are difficult because of the data. Not necessarily a lack of data, but the\nchallenges associated with identifying relevant data and making it useful. Today, it is\nassumed that excruciating amounts of data at mind-numbing levels of detail must be\ngathered to describe the current state accurately. Today, most also assume that equally\nexcruciating amounts of mind-numbing potential future state details are required for\ncredible solutions to be built and accurate decisions to be made. Today, most assume that\nresults will fall short of expectations if sufficient levels of detail are not considered. These\nassumptions lead to another assumption: that these deep, drawn-out investigations require\na lot of time and can be accelerated by throwing more people and expense at the problem.\nIn other words, we dive way too deep, way too fast, and wrap ourselves around the axle\nand get completely stuck.\nAll data is useful; maybe not\nCloud transitions fail most often because the data used is not the most relevant, both when\ndesigning the solution and managing change. What does relevant data mean? How do we\nknow what data is relevant and what is not? How should data be triaged and prioritized?\nRelevance implies that there is a level of focus brought by comparing data to a set of\ncriteria. The criteria used must eliminate extraneous noise and unwanted distractions.\nThroughout this book, it is often stated that the simultaneous alignment of strategy,\neconomics, technology, and risk is critical to success. These four segments become the\ncriteria for both filtering and communicating solution data. To correctly triage and\nprioritize information, the data must have a significant impact across all four segments.\nAny data that does not impact all four segments simultaneously should be addressed at a\nlater stage or as part of implementation planning. An example of this may include NeBu\nSystems' interest in moving away from physical servers and monolithic apps to virtual\nservers with functions and services loaded. Can this be considered a strategic choice? Is this\na technical choice? Does the use of virtual machines and outsourced services affect\neconomics? Does the use of virtual machines provided by a service provider change the risk\nprofile?\n",
      "content_length": 2721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 296 ]\nNeBu Systems made a few initial decisions based on a few non-negotiable concepts:\nMove away from monolithic applications and coding methods\nMove away from physical servers in favor of current virtualization methods\nMinimize risk associated with natural and man-made disasters by locating\ninfrastructure in the midwest\nThe application will likely be more RAM intensive than processor\nEconomics impact is weighted the heaviest of all decision criteria if all others are\nequal\nLinux is a requirement with an emphasis placed on embracing open source when\npossible\nOpEx model is required\nThe project will be greenfield with portions of the current state infrastructure\nsubject to sunset after new deployment and go-live\nIn the last chapter, part I of the lab could quickly be created with incredible amounts of\ndetail and relevant insights instantly revealed. The simple input data was enough to get\nenough relevant data that the project could move forward collaboratively without any\nadditional delay while waiting for more potentially useful input data.\nTwo to three providers clearly showed promise, even with the limited input data used. The\nprovider data is normalized, compared, and ranked, giving proper focus to where time and\neffort should be invested if any additional investigation is required. Based on results from\nthe initial scenario, the following insights are shown:\nGoogle offers the lowest cost for the requirements provided\nAWS appeared to be the best performer (compute)\nAzure and AWS are virtually the same cost, making Azure a viable option as\nwell\nNone of the three lowest-cost options were in Chicago, but all were in the\nMidwest\nResponses show a 300% difference between low- and high-cost providers for the\nsame request\nVirtually no difference in cost between AWS and Azure\nPrice-to-performance cost is 50% different between AWS and Azure; this gets\nvery interesting since the price was virtually the same.\n",
      "content_length": 1994,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 297 ]\nBurstorm lab 2 ` advanced insight (NeBu\nSystems)\nThe second part of this lab is diving into additional data and insights that can help shape\nnext-level decisions that may include the following:\nInfrastructure choices\nApplication stacking\nApplication layout\nInfrastructure footprint\nVarious types of optimization that become interesting due to data and insight\nrevealed\nBased on the response data, next-level decisions can be made to quickly refine solution\nchoices and turn them into building blocks for the final solution design. In the next series of\nsteps, each change will continue to expose deeper-level details that can confirm choices or\nhighlight potential alternatives that may provide a better fit with strategy, technology, and\neconomic requirements.\nBurstorm lab 2 - accessing additional detail\nPlease click on Details in the gray ribbon above the results side of the window.\n1.\nPlease use the following screenshot as a reference for where the tab is located:\nOnce clicked, additional solution detais will be shown based on the results\nreturned from each provider. Please refer to the following screenshot:\n",
      "content_length": 1185,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 298 ]\nOverview of the Details tab\nThere is a lot of data shown in a very confined space. The layout allows for this data to be\nused for many types of comparisons quickly. The data is presented visually so interesting\nconnections can be made by referencing consistent data locations.\nUnder the provider's name, a table of data for matching solution products and services\nshows actual product details in the middle, any pricing information on the right side of the\ntab, and any performance data shown to the left. The green ovals contain the total cost for\nthe entire solution, accounting for the term length requested for that scenario. Since NeBu\nSystems did not specify the term, a standard of 720 hours is used as a standard month.\n",
      "content_length": 794,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 299 ]\nIn this view, details can be compared visually for quick decision-making. The segmented\nview is Google's response to the design created in lab 1; details for quantity, pricing, specs,\nand performance are shown along with the total price and real-time benchmarked\nperformance data. In this example, Google can be quickly identified as a less expensive\nalternative to AWS (for this solution combination) by comparing pricing in the green oval.\nAWS can be quickly identified as the faster alternative based on performance data. Google\nprovides the smallest infrastructure size based on specs listed in the response (though not\nenough difference to truly impact performance related to the NeBu Systems use case).\nBurstorm lab 2 ` selecting for direct\ncomparison\nData by itself does not tell us much at all. Data is only helpful when it can be compared to\nsomething. The comparison then leads to insight. In this portion of the lab, comparisons\nwill be made that will lead to insights that shape solution decisions:\nPlease check the Compare box for both AWS and Azure as shown in the\n1.\nfollowing screenshot:\nAfter checking the boxes for the solutions to be directly compared, please click on\n2.\nthe Compare tab as shown in the following screenshot. The Compare tab is next\nto the Details tab described in the previous section:\n",
      "content_length": 1389,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 300 ]\nThe following view will help confirm that the first two steps have been completed correctly.\nThe view should change to one comparing the two selected solutions side by side:\nThis view allows solutions to be aligned and compared, line by line. Each line matches\nexactly for each objective in the design scenario. In NeBu Systems' current design, there is\nonly one objective defined. Imagine a solution with many lines that need to be mapped,\nmatched, and compared. It takes a lot of time to normalize and compare data using manual\nmethods today. The current NeBu Systems design and comparison only took a few clicks.\nInsightful data is shown in an instant, accelerating design and decision processes.\nComparing by price\nCloud solutions require insightful data. Comparing on price alone leads to trouble quickly.\nCheap solutions may not be the best fit strategically or have the right level of performance.\nThroughout this book, the economic impact has been mentioned as a requirement, but it is\nnot the only requirement. In many places throughout the platform, a red number is shown.\nThis number is a normalized number that takes pricing data and performance data and\nruns a calculation that normalizes the data and presents a consistent and dependable\nnumber that allows the viewer to compare the several performance and pricing metrics for\nsolution and solution component decisions:\nIn the upper-right corner of the Compare window, there is a drop-down box.\n1.\nPlease verify the box contains By Price. This drop-down menu is a selectable way\nto reorder and prioritize how the data is presented based on what is most\nimportant to the viewer at that moment. The default is set to By Price:\n",
      "content_length": 1755,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 301 ]\nIn the comparison just created for NeBu Systems, which provider is the lowest\n2.\ncost? Is the lower-cost provider on the left or the right? There are several visual\nclues to help quickly identify the optimal solution based on the prioritization\nmethod chosen. When comparing AWS and Azure, as shown below, the lowest-\ncost provider for the current solution requested is AWS. This insight is shown in\na few ways. First, the green oval within the AWS response also shows best below\nthe price as the optimization method chosen was by price. The optimal solution\noption is always shown on the left; again, in this situation, the optimal solution is\nfrom AWS based on price alone:\nThere are a few other indicators to help the viewer quickly find highly relevant insights.\nWithin the green oval on the Azure side, there is a red number below the price. The red\nnumber, in this case, states that the price difference between the two solutions is 1% or less. \nAnother quick visual indicator is the red or green indicator at the end of each line item\nwithin each response. Within the AWS solution, it again says best, but using green text. In\nthe Azure response, the red number indicates the difference in price for that line. Please see\nthe following screenshot:\nBased on the requirements and responses, pricing difference between the two providers is\n1% or less. This difference is too close to make an informed provider choice. Additional\ndata is needed to choose the right path forward.\n",
      "content_length": 1548,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 302 ]\nComparing by performance\nIn this step, additional data points can be quickly added for consideration. In the previous\nstep, 1% difference in cost is not enough to clearly choose which solution is optimal. In a\nlater step, price distribution will also be considered:\nPlease click the drop-down box that currently shows By Price and change it to By\nPerformance:\nThe view will change to now prioritize views based on normalized, real-time performance\nbenchmark data. This additional data enables the comparison of multiple solutions based\non performance as the priority data set to organize the views and calculate differences.\nPlease confirm the current views match the following screenshot:\n",
      "content_length": 756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 303 ]\nSome may have noticed that the view did not change. There are a couple of reasons for this.\nFirst, in the bottom-left oval, the performance numbers presented are pulled from\nBurstorm's ongoing cloud benchmark service that randomly and continuously tests cloud\nproviders in real time. For this scenario, the performance number 15.7 for AWS is greater\nthan the 9.8 for Azure. The AWS performance is greater than Azure and presented on the\nleft. No change in view is needed. Second, there is only a single line item in the solution;\nnone of the data needed to be changed or reordered since AWS is the lowest cost and the\nhighest-performing in this scenario. Again, no change is needed. If Azure was higher-\nperforming with AWS still the lowest cost, this view would have moved Azure to the left\nside, as it was the higher-performing solution, and prioritized the data based on \nperformance, as indicated in the drop-down list at the top right.\nWith this additional data, AWS is slightly lower in terms of cost but appears to be\nsignificantly faster. The text in the bottom of each oval on the left side will again visually\nindicate which is best and what the difference between them is. Please see the following\nscreenshot. In this scenario, the difference is 38%:\nPricing did not give much indication of which would be optimal. The infrastructure sizes\nalso appear to be equal, with both showing as 2x8 machines.\nPlease note, in the scenario when created, the requested infrastructure size was one core\nand 7 GB of RAM (1x7). The platform automatically corrects to match how products and\nservices are sold by the providers, how they are meant to be consumed, and how the\nproducts and services are deployed.\nIn this scenario, both AWS and Azure would deploy the requested compute resources as a\ntwo-core and 8 GB of RAM (2x8) virtual instance. See the following original request from\nthe lab part 1:\n",
      "content_length": 1963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 304 ]\nPlease compare the requested details to the response details from each provider shown as\nfollows:\n\u0002\nComparing by price-to-performance\nInfrastructure size has not provided any meaningful differentiation. Pricing has not shown\nany major benefit from one provider to the other. The performance looked to be\nsignificantly different, with AWS appearing more favorable. Another very helpful indicator\nwhen comparing potential cloud solutions is price-to-performance benchmarks. In many\ncases, the price may clearly indicate a provider or two are optimal with performance\nshowing a different provider, or set of providers, as optimal. Price-to-performance\nbenchmarks enable value-oriented comparisons that will clearly show which provider has\nthe lowest cost for the highest level of performance. Of course, other factors may still\ninfluence final decisions, but the data can help build strong cases as solution designs\nprogress:\n",
      "content_length": 989,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 305 ]\nPlease go again to the top right, click the drop-down list, and choose Price\n1.\nPerformance as the priority when viewing data:\nAll red numbers should change to match the following screenshot. A $BCU is\nnow shown within each line item. This is the normalized price per unit of\nperformance for each line item. In the case of AWS, the cost for every unit of\nperformance is $4.26. For Azure, it is $6.91, which is a 38% higher cost per unit of\nperformance for the same size infrastructure than AWS:\n",
      "content_length": 561,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 306 ]\nBased on this additional data, it appears that AWS is likely the optimal answer\nbased on the data points considered to this point. It takes very little to make the\nsame comparison between AWS and Google. Google was the original low-cost\nprovider based on the requirements included in the scenario to this point.\nPlease return to the Details tab and uncheck Azure, and check Google for the\n2.\ncomparison. It is perfectly fine if you check the box for all three and compare\nthem side by side. The only challenge is being able to see all the data as you must\nscroll left and right to see all the data when you select three or more for\ncomparison. Please see the AWS and Google comparison in the following\nscreenshot:\nWhich provider is the lowest cost?\nWhich provider has the highest performance?\nWhich provider would be optimal based on price-to-performance\ndata?\nGoogle is 28% less expensive based on what was requested. By price alone, this would look\noptimal. AWS is higher-performing, with a 38% difference. It is an interesting side note\nthat, if you felt that the difference was 50% or so, that calculation would be a margin\nnumber that is different than the amount of difference. Viewing the data based on price-\nperformance shows AWS as a slight favorite (10%) even though Google was 28% lower cost\nin this scenario.\n",
      "content_length": 1388,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "Hands-On Lab 2  Advanced Cloud Design Insight\nChapter 18\n[ 307 ]\nSummary\nThere are many factors to consider as cloud solutions are built. In this section, several\nadditional important data points were examined:\nRequested infrastructure sizing\nUpdated infrastructure sizing based on provider deployment sizing\nNormalized infrastructure detail based on consumption and business models\nNormalized details based on updated pricing matching updated sizing based on\ndeployment rules\nPerformance data and analytics\nPrice-to-performance data and analytics\nBecause price-to-performance is so important to the process of building cloud solution\ndesigns, an in-depth paper from Burstorm can be found at: IUUQT\u001c\u0011\u0011TMJEFY\u0010UJQT\u0011\nEPXOMPBE\u0011DMPVE\u000fDPNQVUJOH\u000fCFODINBSL. Many of the aspects of why and how are included\nin the paper. Price-to-performance is a critical dataset that must be included as solution\noptions are evaluated.\nThere are many more details to consider as solutions are normalized, compared, and\nchosen. Based on very high-level details, a path can be chosen and focused on. Additional\ndata points can then be added and compared to either confirm the right path is chosen or\nclearly illustrate that a different path is needed. This allows projects and decisions to\nprogress quickly while reducing the level of effort to compare all data across all potential\nproviders. Starting with infrastructure also allows for the creation of a solid foundation to\nbuild on. It can be very expensive to manage environments that sprawl and spread\nunnecessarily. It can also be very hard to change directions if a solution goes too far too fast\nin the wrong direction.\nBased on the data to this point, NeBu Systems chooses to utilize AWS at this stage of the\nproject. The next chapter will examine how NeBu Systems' choice to utilize AWS has\nprogressed. How has their solution worked out strategically and technically, and how is it\ncurrently affecting the economics? Using the same concepts from this chapter, what can\nNeBu Systems do differently? What changes should be made? Are there better options that\nshould be considered?\n",
      "content_length": 2115,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "19\nHands-On Lab 3  Optimizing\nCurrent State (12 Months Later)\nIn the last chapters, data and insight quickly identified Google, AWS, and Azure as well-\nsuited providers for the infrastructure and services NeBu Systems required in the very\nearly stages of their transformation. Based on price, performance, and price-to-performance\ndata, AWS was chosen as NeBu Systems' initial cloud service provider.\nAs with many transformations, NeBu Systems has had challenges with change\nmanagement and governance, which, in turn, have slowed adoption. Many questions are\nbeing asked as infrastructure costs have escalated. NeBu Systems has chosen to examine the\ncurrent state in a little more detail. NeBu has imported one of their most recent AWS billing\nfiles. The plan is to quickly identify ways to optimize current state and control costs.\nIn this chapter, real-time data and insight will continue to provide a solid foundation for\nevaluating next steps, options, and decisions.\n",
      "content_length": 973,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 309 ]\nVisualizing current state data\nCurrent state data is typically spread across different locations, several different tools, and\noften many curators. A couple of the many challenges with trying to work with current\nstate data is that the data itself is not interactive or insightful. Collections of data do not\nreally do anything helpful until comparisons are made. Comparing data is revealing and\ninsightful. As an example, a lease can provide details regarding how much is being paid\nand the amount of time left on the lease. It would be much more helpful to compare the\nlease information to current market costs and other solution options. It may be beneficial to\nterminate the lease early and refresh technology through a more cost-effective current\nsolution or better simply benefit from fast-moving markets and current market economics,\nas an example.\nVisualizing data is the quickest path to insight. Many have stated that human perception is\n75%-85% based on sight. If you ask a chef the same question, it would be 75%-85% through\nolfactory nerve (smell). If you ask someone practicing shiatsu massage, that same\npercentage would most likely come from touch. Humans are experts at making data match\npurpose. Science has proven that light travels much faster than sound; about 1 million\ntimes faster than sound. This supports the fact that sight is the fastest of the human senses.\nOur other senses in order from fastest to slowest are sound, touch, smell, and lastly, taste.\nIn my rudimentary thinking, I know that I personally struggle with tasting and smelling\ncurrent state data. The best way for me to process it and quickly identify insight is visual\ninteraction. It has been well documented that data represented visually leads to accurate\ninsight much quicker than any other method.\nAs mentioned previously, in this chapter, the lab will visualize data from an AWS bill.\nThese different visualization steps will create many opportunities to make visual\ncomparisons that will yield many insights that can be used to optimize or transform the\ncurrent state. This lab section will focus on identifying insight that will help NeBu Systems\noptimize current state to better match up with the strategy, technology, economic impact,\nand risk profiles for the current project.\n",
      "content_length": 2360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 310 ]\nHands-on lab 3 ` visualizing the data\nFor convenience, an AWS bill has already been imported for the lab. The import was done\nusing the import function at the top right of the design board. Please see the following\nscreenshot:\n",
      "content_length": 306,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 311 ]\nAfter the import, a new project will show in the left window. After a billing file is imported,\nit is added to the project list as an existing state project (green letters). Please confirm that\nAWS Feb and AWS March are within the list in the left window and accessible by your\nuser. Please confirm your view matches the following screenshot:\n",
      "content_length": 422,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 312 ]\nPlease click on the project name AWS Feb (green letters) in the left window pane, shown in\nthe preceding screenshot. Clicking the name opens the project in the main drawing board\nwindow. The view should match the following screenshot:\nThe imported billing data has created a visualization of the current state data included in\nthe billing file. The visualization includes all infrastructure and services along with any \nAWS user-created tags that were included in the billing file details. The view is\nautomatically split by location. US West 1 and 2, US East 1, and Europe were included in\nthis file.\nHands-on lab 3 ` NeBu Systems' transformation\nprogress update\nNeBu Systems has made a lot of progress in a very short time. While things appear to be\nsuccessful at first glance, rapid growth and transformation have many challenges.\nAdoption can be difficult. Proper change management and governance are critical to\nsuccess but very hard to do well.\n",
      "content_length": 1030,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 313 ]\nInitially, NeBu Systems did a very good job with change management. Unfortunately,\npeople have moved on. As the transition has progressed, teams have been shuffled a bit to\nrealign people with current strategy, technology, and economics. Change management has\nsuffered for the last few months, slowing adoption significantly. People are gravitating\nback to old, comfortable methods rather than embracing the new processes, infrastructure,\nand updated services.\nNeBu Systems saw significant early adoption, which led to rapid growth in the\ninfrastructure supporting the growing user base. With rapid growth, details get missed.\nCosts have escalated beyond where initial budgets were set. Scope-creep has become a\nproblem. Leaders want to re-examine where they are today, reset to align infrastructure\nwith the current strategy, and do a better job of controlling costs.\nHands-on lab 3 ` Current billing file\nWhat is in the current billing file? How can it be compared to the current market? Today,\nanalyzing a cloud billing file is very difficult. Billing files are very detailed. They usually\nhave many different services, with different locations, different billing methods, different\nterms, quantities, and very cryptic ways of identifying exactly what product or service is\nbeing referenced. Today, many try to download spreadsheets and CSV files to analyze\nthem line by line. This is very time-consuming and prone to error. Most automated tools do\nnot have the ability to compare and drive insight across the entire market. Many efforts\ntake days and weeks to normalize and compare billing data. Cloud solutions have services\nthat last fractions of a second, hours, and days. Taking weeks to analyze, compare, and\ndesign solutions is not acceptable in the cloud industry. Automation and enablement is a\nrequirement.\nIn the bottom left of the design board, there are a few icons that can start leading\n1.\nus to visually-driven insight. The first icon should be currently selected. The first\nicon shows the logical visualization of everything in the file:\n",
      "content_length": 2137,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 314 ]\nThis billing file has four main locations as described earlier, three in the US and\n2.\none in Europe. Please click on the icon located furthest to the right in that same\nrow at the bottom left of the design board:\nThis view visualizes the billing file line by line with a total at the bottom. The\nfollowing screenshot shows a partial view of the Bill of Material (BOM) view.\nPlease confirm you have selected the correct tab by comparing to the following\nscreenshot:\n",
      "content_length": 545,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 315 ]\nPlease scroll to the bottom of the page, using the small slider on the right side of\n3.\nthe page or use the mouse wheel to scroll. At the bottom of the page, a green oval\nwill hold the total for the billing term specified in the billing file loaded as well as\na second green oval with the total monthly recurring cost (MRC). Since this is an\nAWS bill that has services for a term of one month or less, the monthly (MRC)\nwill match the total. The oval labeled NRC has a total of $0.00. This confirms that\nno reserve instances are being consumed. Please confirm that views match up to\nthe following screenshot:\nThe imported billing file shows a total of $65,337.13. This is the rolled-up total for\nall locations contained in the billing file. It is important to be able to understand\nthe stories the data is telling. It is also very important to understand what\nquestions still need to be asked and what answers still need to be found. For\nexample, how much of this bill is allocated to each site? Which site is primary?\nWhat products and services are currently deployed at each site?\nPlease click on the icon in the bottom left of the screen; this time, please choose\n4.\nthe icon located second from the right:\nThis icon will bring up a list that can be searched and filtered, again enabling\nquick visual insight utilizing various ways to align and compare data. Please\nconfirm views have changed to the correct location by comparing to the following\nscreenshot:\n",
      "content_length": 1541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 316 ]\nThe columns can be sorted by clicking the heading for each column. Please\n5.\nconfirm which service has the highest MRC cost by clicking MRC twice. The first\ntime will arrange it from low to high. The second click will reverse the order and\narrange it from high to low. Please confirm via the following screenshot. Which\nservice is the most expensive? Which site is it deployed in? What is the second\nhighest and where is it deployed?\nAmazonElasticCache appears to be the highest-cost line item in the bill. This\nservice is currently deployed in USW1 (AWS San Jose). Some interesting\nquestions regarding optimization surface now that we know caching is the\nhighest-cost item in the entire billing file. Caching is typically a service that is\nemployed to keep the cost of other services down:\nIs caching working as planned?\nIs it deployed correctly?\nIs it refreshing content and removing stale content working as\nplanned?\nDoes the caching service offset other more expensive services as\nintended?\nShould this much content in San Jose be caching this often?\nIs San Jose the primary location that should be serving a majority\nof the content?\nQuickly visualizing data in this way enables attention, focus, and effort to be\nplaced in the most effective way based on insight revealed. Cloud architecture\nrequires a keen sense of utilizing only what is needed only when it is needed.\nCloud architecture is being as mindful of economic impact as required for\ntechnical details.\n",
      "content_length": 1548,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 317 ]\nThe bill also has AWSDirectConnect as the second most expensive line item. This\nline item is deployed in a different location from the caching service.\nAWSDirectConnect is deployed from US East 1 (Northern Virginia), not US West\n1. AWSDirectConnect is used to connect client locations directly to AWS. What\ntypes of questions surface knowing these details?\nHow does the direct-connected location on the east coast relate to\nthe west coast location that appears to be caching a lot of content?\nIs the US East 1 location the primary location or is the US West 1\nlocation primary?\nThere were four sites represented in the billing file. Is one of the\nother locations primary?\nWhy is there 2445 GB of data being transferred in one month\nacross the AWSDirectConnect link? Big transfer or backup job?\n2445 GB transferred in less than 720 hours per month equates to a\nfully utilized 7 Mbps-8 Mbps line. Is there a more cost-effective\nsolution for low bandwidth connectivity?\nWhat location does the AWS US East 1 location directly connect to?\nCan/should the services connecting to AWS be moved into a cloud\nservice to eliminate the monthly cost associated with\nAWSDirectConnect?\nAt current market pricing, direct-connect to a 10G port is\n$2.41/hour. If using 720 hours as a standard month, the monthly\ncost shown in the bill would equate to a total of three 10GE ports\nsending a total of 7 Mbps. What is the story that this is telling?\nThe actual cost is for transfer out. AWS does not charge inbound.\nAt an average of $0.02 currently per GB of outbound transfer, 2448\nGB should account for less than $50.00 total for the month. Again,\nwhat is the story that is behind such an anomaly?\nAs great cloud architects, diving deeper is a must. There is more to this story.\n6.\nPlease click on the Contract column header at the far left:\n",
      "content_length": 1900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 318 ]\nBy clicking this header, you can sort the table by this column using alphabetical\norder. Clicking one time will sort A-Z. Clicking a second time will sort Z-A.\nPlease click one time only. Please scroll down to find USE1 for US East 1. Please\nconfirm the view matches the following screenshot:\n",
      "content_length": 372,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 319 ]\nMore very interesting data points rise in this view. Unfortunately, at this time, we appear to\nbe finding more questions than answers. Please look at the types of services (second\ncolumn) and the monthly costs (last column on the right). What stands out? What is the\nstory being told?\nFairly normal infrastructure is deployed that could be used in either primary or\nbackup locations including DB, block storage, compute, S3, DNS, and so on\nCosts are minimal and in some cases $0.00\nIt appears that this site would be set up as a redundant site; maybe a warm site\nthat has some data, but not thousands of GBs worth of data\nSome questions appear when looking at some of the additional detail:\nWhy are there thousands of GBs and thousands of dollars' worth of data being\ntransferred out of this site when there is very little data stored in this location?\nThe amount of data stored in this redundant/backup location does not appear to\nmatch up with what is expected of a $68,000 per month consumer of AWS cloud\nservices.\nCompute costs are zero, or close to it. Has the data that is replicated there been\nvalidated? Has it been verified to work as planned? When was the last time it\nwas checked and tested?\nThe solutions have both DynamoDB as well as RDS. In some cases, particularly in the cloud\nrealm, different types of databases can be utilized for different purposes. For example,\nDynamoDB is only a NoSQL database where RDS can be one of six types. DynamoDB is a\nmulti-tenant database solution with much lower costs. RDS is a single tenant solution at\nmuch higher costs. Both have completely different pricing models.\nAt the top of the same page in current view, there is a Text Search box. Please\n1.\ntype EZO in the search box:\n",
      "content_length": 1810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 320 ]\nThe filtered results immediately change to only show locations with DynamoDB\ndeployed. Please confirm that views match the following screenshot:\nThe filtered detail shows that DynamoDB is deployed, or at least enabled, in all\nfour locations in the billing file. There is very little, if any, activity in the last\nmonth, or maybe longer:\nWhy are these services enabled and not used, or used very little?\nDo these services present any added risk, as they are likely\npartially configured, or set to basic defaults, and not locked down\nat this point?\nHow do these relate to RDS, if at all? Is RDS also partially\nconfigured?\nWhich service is primary for the business?\nWhich site is primary and backup for the database service that is\nsupposed to be utilized?\nPlease replace EZO in the Text Search box with 3%4:\n2.\nThe filtered results immediately change to only show locations with RDS\ndeployed. Please confirm that views match the following screenshot:\n",
      "content_length": 1028,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 321 ]\nThe filtered detail shows that RDS is only deployed in two locations based on the\ndata in the billing file. US West 1 appears to be the primary location, with nearly\n$3000.00 in monthly spend associated. The only other site is in Europe with less\nthan $250.00 in monthly spend. Again, with some answers found, more questions\nare added to the list:\nRDS can be set up in a multi-zone deployment. Based on the data,\nit does not appear to be true for this deployment. Should this be\nverified?\nHow does a single-zone deployment of RDS affect suggestions for\nthe future state?\nHow would a multi-zone RDS deployment affect economics and\nrisk?\nWhich site appears to be primary, based on database activity?\nThis may not be ideal when trying to find which location is\nproduction but has a very high probability based on the details\nseen so far.\nAs a cloud architect, many hats must be worn. We are investigators at times. The\naccountant, technician, risk manager, and strategist hats are never far away. Modern cloud\narchitects must have as much or more skill in business finance and economics as they do\ntechnical prowess.\nAs the investigation into NeBu Systems' current state has progressed, the details examined\ncontinually must work to align NeBu Systems strategically, economically, and technically.\nNeBu appears to be overspending in areas and potentially not spending enough in others.\nThe technical mix appears to be solid for the most part, with definite areas to improve. As\ndescribed by NeBu Systems earlier, things do feel like they have grown quickly without the\nbest governance and limited change management.\nUp to this point, most of the examination has been across all locations included in the\nbilling file. This has helped NeBu Systems gain a better understanding of where they are\noverall and what they are consuming, and has identified some ways to focus optimization\nefforts that may help control service sprawl and escalating costs.\nIn the next section, a deeper dive into individual locations within the billing file is needed.\nComparing existing deployments to the current market will quickly provide insight that\nwill help solidify direction and next steps for NeBu Systems as they continue their\ntransformation to cloud.\n",
      "content_length": 2316,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 322 ]\nPlease click the first icon in the bottom-left corner to switch back to the design view. In the\nnext section, current state data for the primary location will be considered and compared to\ncurrent market real-time data to help expose additional insight:\nThe view should switch to display all four NeBu Systems locations, with any infrastructure\nand services currently deployed at each location. Please confirm the view matches the\nfollowing screenshot. From this view, each NeBu location and the services deployed there\ncan be individually compared to the current market to identify options that NeBu can use\nto optimize designs item by item.\nEach service has its own characteristics, deployment size, level of utilization, technical\ndetail, and economic impact. Each compute service has its own performance characteristics\nand reliability/availability trends over time. Each of these data points will help the NeBu\nSystems cloud architects align strategy, economics, and technical requirements:\n",
      "content_length": 1075,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 323 ]\nThe primary locations appear to be USW1 and USW2. This next section will focus on the\noptimization of USW1. The current view shows compute, storage, services, and\nconnectivity. Please click on the affectionately named hamburger menu at the top right of\nthe USW1 current state design. Please confirm menu location in the following screenshot:\n",
      "content_length": 421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 324 ]\nIt will take a minute or two for the view to change. In real time, every line item is analyzed\nand compared to the current market. Once the view changes, a BOM view should show on\nthe right, with the design visualization on the left. Please confirm the view has changed as\nshown in the following screenshot:\nPlease scroll through the line items on the right to the bottom of the page. Three ovals\nshould now be visible. Please confirm the current view matches the following screenshot,\nwith the three ovals now visible at the bottom of the page:\n",
      "content_length": 625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 325 ]\nAgain, what stories can be told with the data?\nThe billing data shows nearly $31,000.00 total spent for this location during the\nbilling period\nThe monthly recurring cost (MRC) is $31,000.00\nThe MRC equals the total, meaning that all services have a term of one month or\nless\nNone of the spend is NRC, meaning NeBu Systems is not currently utilizing any\nreserve instances\nThe data mentioned provides a good high-level overview of the current state services and\ncurrent state spending for NeBu Systems. More detail is needed as optimization efforts are\nexplored:\nWhat is driving the cost of the solution?\nAre there any strategic, economic, or technical factors that highlight where the\nfocus should be placed as future state considerations are made?\nHow does performance factor in?\nCan the footprint be consolidated to help control cost?\nAre the correct or optimal instance types being used?\nAre consumption models matching up with strategy?\nPlease click on the middle icon at the bottom left of the screen. This will change the view to\nshow how each service contributes to the overall cost of the solution. Larger blocks mean\nthat items with larger block size account for larger portions of the overall spend. This\nprovides a visual way for cloud architects to quickly identify places to focus and find\nalternatives to re-align strategy, economics, and technology:\n",
      "content_length": 1444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 326 ]\nPlease confirm that the view has changed to match the following screenshot. A couple of\nvery large blocks quickly points out that a small number of services are contributing to a \nmajority of the cost in the current state:\nThe purple block is Other Costs. These costs are AWS-specific services that may lead to\nvendor lock-in. These services are generally not the same from provider to provider. There\nmay be alternatives that could be used by other providers. Additional time and effort are\nneeded to investigate each of these further. The Other Costs block accounts for 31% of the\ntotal solution cost monthly.\n",
      "content_length": 691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 327 ]\nThe second large block (dark blue) is associated with the N\u0015\u000fYMBSHF\u000fMJOVY instance type.\nThis single instance type is contributing 28% to the overall solution cost each month. There\nmay be more than one instance deployed, but this type of instance is contributing\nsignificantly to the overall NeBu Systems solution in US West 1. Some interesting questions\ncome to mind based on these two additional data points:\nWhat services are AWS-specific?\nIs lock-in to AWS an issue? Does it need to be resolved?\nThe M3 instance types are older instances that have now been updated to newer\nversions. Should these be upgraded?\nWhy have the M3 instances not been upgraded to a more current version?\nM3 instances are a general use compute type with SSD storage. Is it better to split\nthe applications into more cost-effective compute types that match the\napplications?\nWould smaller instance types match NeBu System strategy better technically\nand/or economically?\nWhat are these instances doing? Are they still critical to the solution?\nAs upgrades and changes for a future state are considered, what new services\nmay align better strategically, economically, and technically to NeBu Systems'\ncurrent direction?\nA general idea is now understood regarding how NeBu Systems has deployed their\ninfrastructure and services. Several questions have been raised with very good\nopportunities for optimization coming into focus quickly.\nPlease click on the first icon at the bottom left of the screen to change the view to the IQ\nview:\nThe view should change to show the Summary tab by default. This view provides high-\nlevel details such as location, cost, and price-to-performance for the entire solution. Please\nconfirm the view has changed to match the following screenshot:\n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 328 ]\nIn this view, the location is confirmed as San Jose, CA, which is US West 1. Again, the total\ncost is shown. Two new pieces of data are shown in this view. First, it is shown as (from\ncontract) in the first column under Solution Set. This distinguishes data that is from the\ncurrent state billing file versus comparison market data that is compared in real time. The\nsecond new piece of data is the $/BCU red text in the middle of the screen. This red number\nis the Burstorm Compute Unit (BCU), an average cost per unit of performance based on\nthe benchmark data discussed in depth at the end of the previous chapter. $/BCU will be\nused in later steps to compare solutions and individual solution components to current\nmarket options available. These comparisons will help quickly identify options that have\nlower price-to-performance ratios. Lower $/BCU numbers are more desirable if all other\ncriteria are equal.\nPlease click on the Details tab in the gray bar at the top of the provider response window.\nThe location of the tab is shown in the following screenshot:\n",
      "content_length": 1148,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 329 ]\nThe view should now have detailed solution data for each line item in the current state\nsolution. Please confirm that the solution detail is shown. The following screenshot is\nincluded for reference:\nThe preceding view shows the services portion of the current state bill related to Amazon-\nspecific services. 30% of NeBu Systems' current monthly spend ($9,424.00) is associated\nwith the AWS-specific services. Some of these services have already been discussed in\ndetail earlier in this chapter.\nScrolling down through the same window shows the same level of detail for the\ninfrastructure components and services. Please match views with the following screenshot.\nScrolling to the bottom exposes two ovals. The green oval shows the total for the\ninfrastructure components ($21,510.00). The second oval contains price-to-performance\ndata. This view, by default, is set to prioritize based on price. The price-to-performance oval\nwill show performance data as a cumulative total for the current state solution (2086.1):\n",
      "content_length": 1098,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 330 ]\nIn the preceding compute details, some answers to previous questions can be answered.\nNeBu Systems noticed a very large portion of the bill was committed to N\u0015\u000fYMBSHF\u000fMJOVY\ninstances. In this view, 21 instances are shown. The detail for each is also shown (4 cores, 15\nGB RAM, 80 GB of storage). Quick math shows this to be the largest grouping of total cores\nand RAM (81 cores and 315 GB RAM). Depending on application requirements and the\nnumber of applications, this group may be able to be changed to more cost-effective and\nmore specialized workloads that match the application and NeBu Systems' strategy better:\nWhich instance type would be more beneficial based on performance and pricing\ndata?\nIs there a way to re-stack applications to utilize a more advantageous instance\ntype and/or size?\nWhat does this cost to deploy on updated infrastructure?\nAre there any applications that can now be purchased as a service?\n",
      "content_length": 1003,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 331 ]\nPlease change from prioritizing on price to prioritizing on price performance. Changing the\nprioritization of the data, compute can be compared looking for opportunities to optimize\nthe instance type. Depending on actual utilization data, an N\u0015\u000fMBSHF may be more\nbeneficial than an N\u0015\u000fYMBSHF. If RAM utilization is low, the N\u0015 may be the instance of\nchoice:\nAdditional data that may also be helpful is how the individual types rank based on price\nperformance data. The following real-time data is available by looking through the ongoing\nbenchmark data. The arrows have been placed over the price-to-performance details for the\nN\u0015\u000fYMBSHF and the N\u0015\u000fMBSHF instance types:\n",
      "content_length": 750,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 332 ]\nNeBu Systems applications tend to be more RAM-intensive. N\u0015\u000fMBSHF may not be the right\ninstance type based on the workload type. What does the current market have available? Is\nthere a high-performing, lower-cost instance type that matches up to the NeBu Systems\nworkload?\nPlease click on the hamburger menu above the word Summary as follows:\nA menu will appear with a switch for Exact-Match. The switch should be on by default.\nPlease click to flip the switch to OFF. Please use the following screenshot as a reference:\n",
      "content_length": 600,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 333 ]\nExact-Match | OFF asks the platform to compare the solution to external solution\nproviders. Showing a provider that is not an exact 100% match is allowed when the switch\nis off. Once the app has refreshed the new data view, the following screenshot should now\nmatch the current view:\n",
      "content_length": 363,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 334 ]\nThe new data allows for comparisons to be made using current market data. As in previous\nchapters, Google is lower cost than several others, including Azure and AWS. The\nfollowing screenshot shows a few interesting insights:\nGoogle is the low-cost provider\nAzure and AWS are very similar in cost\nAWS, again, appears to be the higher-performing solution with a lower $/BCU\nThe lowest-cost AWS solution is in US West 2 (Boardman, OR), not the NeBu\ncurrent location of US West 1 (San Jose, CA)\nCloud architects must often weigh risk and economics. This book has discussed that\neconomics must offset risk. The higher the risk, the lower the cost must be to make it worth\nabsorbing the risk. Migrating to Boardman may feel a bit risky. However, it is still in the\nsame region with the same provider. If the cost is significantly less and/or the performance\nis significantly high where applications can be consolidated or re-stacked, the move may be\nworth the effort. NeBu Systems has a strategy of getting the highest performance at the\nlowest cost. The move to Boardman may be a foundation piece for realigning strategy,\neconomics, and technology:\n",
      "content_length": 1223,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 335 ]\nThere should be a list of several solutions from several providers that match the following\nscreenshot. In this section, a comparison between the current state billing file data for US\nWest 1 and the current market. Please check the Compare box for AWS and the Compare\nbox for (from contract) toward the bottom of the list. Please see the marked boxes in the\nfollowing screenshot for reference:\n",
      "content_length": 474,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 336 ]\nBefore comparing, there are a couple of interesting data points worth mentioning in this\nview. NeBu Systems is focused on finding the best performance at the lowest cost.\nChanging providers is an option if the price and/or performance is worth the risk:\nThe current state bill is one of the most expensive options presented\nThe current state option is $10,000+ higher than current market with the same\nprovider\nThe current state services are potentially much slower than current services from\nthe same provider\nThe data in this view makes comparisons very easy for NeBu Systems. This data alone may\nlead NeBu to conclude that focusing on staying with AWS is the right option and migrating\nto Boardman may make a lot of sense as well. A direct comparison between the two AWS\nlocations is the next logical step.\nPlease change to the Compare view at the top of the provider response table as follows:\nThe view will immediately change to align each unique line item side by side. The\nfollowing screenshot is shown for reference if needed:\nIt becomes very clear quickly that staying with AWS and migrating to Boardman has many\nbenefits. Please scroll to the bottom and look at the ovals with the summarized cost and\nprice-to-performance data:\nBoardman has a much lower $/BCU ratio, $3.50 versus $8.83 for current state\nBoardman is lower infrastructure cost, $11,295 versus $21,510 for current state\nChanging the prioritized view from price performance to performance only\nshows that only Boardman is significantly faster, 2698.2 versus 2086.1 for current\nstate\n",
      "content_length": 1635,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 337 ]\nIf these comparisons are difficult, refer to the first couple of sections of the hands-on labs, as\neach of these comparisons were detailed in those sections.\nIt is also very interesting to look at some of the side-by-side comparisons to see what is\nsuggested based on the current state data available. Please look at the following example:\nThe first line is the current state solution. A total of 21 N\u0015\u000fYMBSHF instances were\ndeployed accounting for $8,279.04 with a performance score of 413.1.\nThe second line is the potential future state solution utilizing 21 U\u0014\u000fYMBSHF\ninstances for only $2848.27.\nThe difference between the two options is 66% less cost and a 20% increase in\nperformance.\nT2 instances may work very well for NeBu Systems' strategy as most of the\napplications are RAM-intensive, not CPU-intensive. The T2 series instances could\nstay at base CPU performance levels, controlling costs quite well. The T2 prices\nare very depending on how CPU performance and load increases. Staying at base\nperformance would allow NeBu systems to utilize the RAM fully without\nincreasing costs. Key note: understanding how economics and technology relate\nenables the simultaneous alignment of strategy, economics, and technology:\nSummary\nNeBu Systems needed to revisit their transformation in progress and current deployment\nthat has grown quickly. In this lab, price-to-performance became the key differentiator for\nnearly all the choices made.\nThese hands-on labs intentionally took a very infrastructure-centric view of the world.\nInfrastructure has long been ignored in favor of more sexy and endearing things such as\napplications. Applications are what users interact with. They are the things that are most\noften seen and commented on, not the infrastructure underneath.\n",
      "content_length": 1855,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "Hands-On Lab 3  Optimizing Current State (12 Months Later)\nChapter 19\n[ 338 ]\nInfrastructure is getting cheaper by the day. The race-to-zero is just beginning for compute.\nNetwork has been on its way for a while. Storage is also beginning the run. With\ninfrastructure declining in price and the cost of management and operations rising\nexponentially, the cost of a mistake at the infrastructure level can be very costly, with\nchanges in strategy and technical direction nearly cost-prohibitive. Get the foundation set\nright. Then build on the foundation.\nThis lab started with data at very high levels, abstracting most of the detail away. By doing\nthis, the strategy could be quickly analyzed and confirmed. The first part of the lab was\ninvestigative work, matching up the stories the billing data was telling with the\nexpectations of what NeBu Systems thought they were doing. The middle portion of the lab\nwas used to look for deeper-level data to confirm stories and find opportunities to change\nthe narrative. The last part of the lab showed how to confirm direction by answering\nquestions raised during the beginning and middle of the lab.\nNot all questions raised were answered. That was never the intent. The intent was to build\na process and pattern for thinking, raising questions, searching for relevant data, and\nanswering questions that help accomplish the simultaneous alignment of strategy,\neconomics, technology, and risk. Cloud architecture is about quickly triaging data,\nidentifying relevance, and remaining keenly aware of real-time insight.\n",
      "content_length": 1564,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "20\nCloud Architecture  Lessons\nLearned\nIf you are completely successful in navigating the complex and competing priorities levied\non every cloud computing solution, your efforts will fail unless a successful\nimplementation follows. Although implementation is outside the scope of this solution\ndesign text, we would like to share with you the lessons we have learned in these early\nyears of cloud computing architecting:\nTo be a successful cloud solution architect, you need to obtain and maintain\nexecutive sponsorship. Make sure governance control points are built into the\ntransition process (an example control point: ensure financial controls on pay-as-\nyou-go elastic compute model do not result in runaway costs!).\nAnalysis of an organization's application portfolio as a whole is key to efficiency\nand the breadth of value delivered by transitioning to a cloud platform.\nManagement oversight and review processes for legacy application transitions to\nIaaS platforms should be modified to reflect their software-only nature.\nMost customers are only aware of a few large cloud service providers (for\nexample, AWS, Azure, Google, Salesforce, and IBM). They also may be limited to\nselecting a single CSP platform (that is, C2S). This does not reduce or eliminate\nthe need to evaluate the economics and performance aspects of a transition\nstrategy to the broader marketplace.\n",
      "content_length": 1380,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "Cloud Architecture  Lessons Learned\nChapter 20\n[ 340 ]\nLack of IT standards or a failure to enforce those standards results in differences\nbetween your development, test, and production environments. This\nsignificantly reduces your ability to leverage automated testing tools and delays\nyour cloud transition. Developers must be educated on Application\nPerformance Monitoring (APM) capabilities, service management/monitoring\ncapabilities, web and mobile analytics, and alerting and notification solutions. \nThe most severe challenge when adopting cloud computing is around cultural\nchange. A focused and dedicated informational and educational campaign\nshould be in place to support this type of transition. A lack of cloud computing\neducation and understanding is one of the most significant organizational risks.\n",
      "content_length": 817,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "Epilogue\nWhere should NeBu Systems go next? They have successfully broken apart their monolithic\napplication and now utilize cloud services. Is it a case of congratulations on finishing? Or is\nit congratulations on starting? Did NeBu successfully migrate or transform? \nMigrations are a series of things that get done. Migrations seem tangible: from this to that,\nfrom here to there. Transformations, interestingly, are mental and emotional.\nTransformations require a change in mindset. Transformations require constant data  that\ncan be continuously compared to expose insights and establish perceived value. \nMigrations are planned and executed. Transformations are adopted. Without adoption,\ntransformation fails. Adoption requires a change in mindset, often created from a\ncontinuous digestion of highly valued relevant data and insight. This means continuously\nsensing the environment and continuously changing your actions to better align with goals,\nwhich are also changing continuously. We, the authors, call this being senso-morphic.\nBusinesses and people tasked with adapting and driving change must become senso-\nmorphic.\nToday, many are flooded with data, yet remain uninformed. Many know they are in the\nwrong place, yet struggle to know where they are. The only sustainable path for positive\ntransformation is to become senso-morphic. In the world of cloud computing, this means\nbeing senso-morphic across many domains, simultaneously. The senso-morphic domains\nare shown in the following table:\n",
      "content_length": 1510,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "Epilogue\n[ 342 ]\nThis book is but the first step in a long journey. Our wish is that we've prepared you well.\nGood luck.\n",
      "content_length": 121,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "Other Books You May Enjoy\nIf you enjoyed this book, you may be interested in these other books by Packt:\nGoogle Cloud Platform Cookbook\nLegorie Rajan PS\nISBN: 978-1-78829-199-6\nHost a Python application on Google Compute Engine\nHost an application using Google Cloud Functions\nMigrate a MySQL DB to Cloud Spanner\nConfigure a network for a highly available application on GCP\nLearn simple image processing using Storage and Cloud Functions\nAutomate security checks using Policy Scanner\nUnderstand tools for monitoring a production environment in GCP\nLearn to manage multiple projects using service accounts\n",
      "content_length": 606,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": "Other Books You May Enjoy\n[ 344 ]\nArchitecting Microsoft Azure Solutions ` Exam Guide 70-535\nSjoukje Zaal\nISBN: 978-1-78899-173-5\nUse Azure Virtual Machines to design effective VM deployments\nImplement architecture styles, like serverless computing and microservices\nSecure your data using different security features and design effective security\nstrategies\nDesign Azure storage solutions using various storage features\nCreate identity management solutions for your applications and resources\nArchitect state-of-the-art solutions using Artificial Intelligence, IoT, and Azure\nMedia Services\nUse different automation solutions that are incorporated in the Azure platform\n",
      "content_length": 671,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "Other Books You May Enjoy\n[ 345 ]\nLeave a review - let other readers know what\nyou think\nPlease share your thoughts on this book with others by leaving a review on the site that you\nbought it from. If you purchased the book from Amazon, please leave us an honest review\non this book's Amazon page. This is vital so that other potential readers can see and use\nyour unbiased opinion to make purchasing decisions, we can understand what our\ncustomers think about our products, and our authors can see your feedback on the title that\nthey have worked with Packt to create. It will only take a few minutes of your time, but is\nvaluable to other potential customers, our authors, and Packt. Thank you!\n",
      "content_length": 697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": "Index\nA\naccess control lists (ACLs)  \u0014\u0017\u0017\nActive Server Pages (ASP)  \u0014\u0017\u0012\nAI  \u0015\u0018\nAlternative PHP Cache (APC)  \u0014\u0016\u001b\nAmazon Web Services (AWS)  \u0014\u0013\u001b\nAnsible  \u0014\u0017\u0013\nAPI management\n   about  \u0013\u0016\u001a\n   reference  \u0013\u0016\u001a\napplicable governance regimes\n   FedRAMP  \u0014\u0017\u0019\n   FERPA  \u0014\u0017\u0019\n   GDPR  \u0014\u0017\u0019\n   HIPPA  \u0014\u0017\u0019\n   PCI DSS  \u0014\u0017\u0019\n   SOX  \u0014\u0017\u0019\napplication category  \u0013\u0013\u0014, \u0013\u0013\u0015\napplication dependencies  \u0013\u0013\u0016\napplication design  \u0013\u0012\u0019\napplication federation  \u0014\u0014\u0018\napplication migration  \u0013\u0012\u001b\napplication migration planning\n   about  \u0014\u0017\u0013\n   processes  \u0014\u0017\u0014\nApplication Normative Framework (ANF)  \u0014\u0018\u0016\nApplication Performance Monitoring (APM)  \u0015\u0016\u0012\napplication programming interface (API)\n   about  \u0014\u001a, \u0014\u0014\u0016\n   benefits  \u0014\u0014\u0016\n   categories  \u0014\u0014\u0017\n   cloud middleware API  \u0014\u0014\u0017\n   for cloud storage  \u0014\u0014\u0017\n   levels  \u0014\u0014\u0017\n   Representational state transfer (REST)  \u0013\u0013\u0018, \u0014\u0014\u0016\n   Simple object access protocol (SOAP)  \u0013\u0013\u0017, \u0014\u0014\u0016\n   using  \u0013\u0013\u0017\nApplication Security Management Process (ASMP) \n\u0014\u0018\u0014, \u0014\u0018\u0016, \u0014\u0018\u0017\napplication security risks\n   reference  \u0014\u0018\u0019\napplication service provider (ASP)  \u0015\u0018\napplication virtualization  \u0014\u0012\u0012\napplication workload\n   about  \u0013\u0013\u0012\n   once-in-a-lifetime workloads  \u0013\u0013\u0012\n   static workloads  \u0013\u0013\u0012\n   unpredictable and random workloads  \u0013\u0013\u0013\narchitecture executive decisions\n   automation  \u001b\u0015\n   challenges, expressing  \u001b\u0014\n   culture  \u001b\u0018\n   economics  \u001b\u0015\n   process  \u001b\u0013\n   real-time collaboration  \u001b\u0014\n   risk  \u001b\u0017\n   strategy  \u001b\u0015\n   technology  \u001b\u0016\nAsynchronous JavaScript And XML (AJAX)  \u0014\u0017\u0012\nAtomic, Consistent, Isolated, Durable (ACID)  \u0014\u0013\u0013\nauditability  \u0014\u0015\u0012\nauditing  \u0014\u0013\u0015, \u0014\u0013\u0016, \u0014\u0013\u0017\nB\nbaseline architecture\n   single server architecture  \u0013\u0014\u0015\n   single-site architecture  \u0013\u0014\u0016\n   types  \u0013\u0014\u0015\nbenchmarks  \u0014\u0015\u0013\nbig data analytics (BDA)\n   about  \u0013\u0018\u0018\n   enterprise network  \u0013\u0019\u0013\n",
      "content_length": 1737,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "[ 347 ]\n   provider cloud components  \u0013\u0018\u001b, \u0013\u0019\u0012\n   public network components  \u0013\u0018\u001a\n   security  \u0013\u0019\u0014\nBill of Material (BOM)  \u0015\u0013\u0016\nbinary large objects (blobs)  \u0014\u0012\u0019\nblockchain  \u0013\u0019\u0015\nblockchain reference architecture capabilities\n   about  \u0013\u0019\u0015\n   blockchain services  \u0013\u0019\u0018\n   cloud network  \u0013\u0019\u0017\n   enterprise data connectivity  \u0013\u0019\u0018\n   public network  \u0013\u0019\u0016\nBluetooth Low Energy (BTLE)  \u0013\u001a\u0012\nBurstorm Compute Unit (BCU)  \u0015\u0014\u001a\nBurstorm lab 1\n   background  \u0014\u0019\u0019\n   design scenario solution results  \u0014\u001a\u001b, \u0014\u001b\u0012\n   design scenario, creating  \u0014\u001a\u0013, \u0014\u001a\u0014, \u0014\u001a\u0017, \u0014\u001a\u0019\n   high-level rapid insights  \u0014\u001b\u0013, \u0014\u001b\u0014\n   model, creating  \u0014\u0019\u001a, \u0014\u0019\u001b, \u0014\u001a\u0012\nBurstorm lab 2\n   additional detail, accessing  \u0014\u001b\u0019\n   advanced insight  \u0014\u001b\u0019\n   Details tab  \u0014\u001b\u001a, \u0014\u001b\u001b\n   selection, for direct comparison  \u0014\u001b\u001b, \u0015\u0012\u0012\nbusiness continuity (BC)  \u0013\u0013\u001a, \u0014\u0019\u0014\nbusiness continuity and disaster recovery (BCDR) \n\u0013\u0013\u001a\nbusiness goal KPIs  \u001a\u0018\nC\nCapital Expenditure (CAPEX)  \u0017\u0017\nchange management  \u0016\u0017, \u0016\u0018, \u0016\u0019\nChef  \u0014\u0017\u0012\nChief Data Officer (CDO)  \u0013\u0018\u001a\ncloud application deployment  \u0014\u0014\u0018\ncloud applications\n   characteristics  \u0014\u0016\u0018, \u0014\u0016\u0019\n   client-side  \u0014\u0017\u0012\n   components  \u0014\u0016\u001a\n   server side  \u0014\u0016\u001a\ncloud computing security  \u0014\u0015\u0013\ncloud computing solution catalogs\n   architecting  \u0017\u0015, \u0017\u0017, \u0017\u0019\ncloud computing solution\n   best practices  \u0015\u0015\u001b\n   designing, goals  \u0013\u001b\u0016\ncloud computing threats  \u0014\u0018\u0019\ncloud computing\n   about  \u001a\u0014\n   business drivers  \u0019\u0016\n   business strategies  \u0019\u0016, \u0019\u0017\n   characteristics  \u0013\u0018, \u0013\u0019, \u0013\u001a, \u0013\u001b, \u0014\u0012\n   clients  \u0014\u0012\u0014, \u0014\u0012\u0015, \u0014\u0012\u0016\n   defining  \u0013\u0017, \u0013\u0018\n   goals  \u0018\u001b, \u0019\u0012, \u0019\u0013, \u0019\u0014\n   history  \u0013\u0014\n   operational models  \u0014\u0013\n   phases  \u0013\u0015\n   risks  \u0014\u0018\u001a\n   taxonomy  \u0015\u0019, \u0015\u001a\n   use cases  \u001a\u0019\nCloud Control Matrix\n   reference  \u0014\u0017\u0019\ncloud customer reference architecture\n   enterprise network  \u0013\u0018\u0018\n   for enterprise social collaboration  \u0013\u0018\u0012\n   overview  \u0013\u0018\u0012\n   provider network  \u0013\u0018\u0014\n   security  \u0013\u0018\u0016\n   service consumer  \u0013\u0018\u0014\n   user network  \u0013\u0018\u0013\ncloud delivery models  \u0015\u0016\ncloud deployment models  \u0014\u001a\ncloud middleware API\n   additional concerns  \u0014\u0014\u0017\ncloud network  \u0013\u0019\u0017\ncloud provider network components\n   about  \u0013\u0016\u0019\n   web service tier  \u0013\u0016\u0019\ncloud security threat\n   reference  \u0014\u0018\u0019\ncloud service availability  \u0014\u0014\u001b\ncloud service models\n   about  \u0014\u0013, \u0014\u0014, \u0014\u001a\n   for executives  \u001b\u0019, \u001b\u001a\n   IaaS  \u0014\u0014\n   PaaS  \u0014\u0018\n   SaaS  \u0014\u0016\ncloud service providers (CSPs)  \u0015\u001a, \u0016\u0016, \u0014\u0013\u0015, \u0014\u0015\u0017\n",
      "content_length": 2287,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "[ 348 ]\ncloud service solutions\n   application security  \u0013\u0016\u0017\n   data security  \u0013\u0016\u0017\n   identity and access management  \u0013\u0016\u0017\n   infrastructure security  \u0013\u0016\u0017\n   secure DevOps  \u0013\u0016\u0017\n   security  \u0013\u0016\u0016\n   security governance, risk and compliance  \u0013\u0016\u0017\n   security monitoring and vulnerability  \u0013\u0016\u0017\ncloud service\n   economics  \u0013\u0013\u001b\ncloud services, BCDR\n   scenarios  \u0013\u0013\u001a\ncloud solution providers (CSPs)  \u0013\u001a\nCloud Standards Customer Council (CSCCTM) \n\u0013\u0016\u0015\ncloud washing  \u0015\u0018\ncloud\n   complexity  \u0014\u0019\u0017\n   concurrency  \u0014\u0015\u0012\n   noise, eliminating  \u0014\u0019\u0018\n   transaction  \u0014\u0015\u0012\nComma Separated Value (CSV)  \u0013\u0013\u0018\nCommand line interface (CLI)  \u0013\u0019\u0019\ncommand-line interfaces (CLI)  \u0014\u0012\u0015\ncommon infrastructure file formats  \u0014\u0014\u0018\ncommunications services\n   about  \u0014\u0012\u001b\n   virtual networks  \u0014\u0012\u001b\ncommunity cloud  \u0015\u0015\ncomplex architecture\n   alert-based scalable setup  \u0013\u0015\u0016\n   caching  \u0013\u0015\u0015\n   cloud hosting architecture  \u0013\u0015\u0019\n   database resiliency  \u0013\u0015\u0014\n   databases  \u0013\u0015\u0015\n   dedicated hosting architecture  \u0013\u0015\u0019\n   failover multi-cloud architecture  \u0013\u0015\u0018\n   global server load balancing  \u0013\u0015\u0013\n   hybrid cloud site architecture  \u0013\u0015\u0017\n   multi-data center architecture  \u0013\u0015\u0012\n   queue-based scalable setup  \u0013\u0015\u0016\n   scalable multi-cloud architecture  \u0013\u0015\u0017\n   types  \u0013\u0015\u0012\nComplex Event Processing (CEP)  \u0013\u0018\u001b\ncomponents, Organization Normative Framework\n(ONF)\n   application security control library  \u0014\u0018\u0016\n   business context  \u0014\u0018\u0016\n   qualifications  \u0014\u0018\u0016\n   regulatory context  \u0014\u0018\u0016\n   responsibilities  \u0014\u0018\u0016\n   roles  \u0014\u0018\u0016\n   specifications  \u0014\u0018\u0016\n   technical context  \u0014\u0018\u0016\ncompute virtualization  \u0013\u001b\u0017\nConfiguration Management Database (CMDB)  \u0017\u0012\nconfiguration management tools\n   Ansible  \u0014\u0017\u0013\n   Chef  \u0014\u0017\u0012\n   Puppet  \u0014\u0017\u0012\n   Salt  \u0014\u0017\u0013\ncontent delivery network (CDN)  \u0013\u0016\u0019\ncore application characteristics\n   loose coupling  \u0014\u0016\u0017\n   service orientation  \u0014\u0016\u0018\nCSP benchmarks  \u0014\u0015\u001a, \u0014\u0016\u0013\nCSP performance metrics  \u0014\u0015\u0017, \u0014\u0015\u0018\ncurrent state data\n   visualizing  \u0015\u0012\u001b\ncustomer relationship management (CRM)  \u0013\u0017\u0013\nD\ndata classification\n   about  \u0014\u0017\u001a\n   authorizations  \u0014\u0017\u001a\n   custodianship  \u0014\u0017\u001a\n   information classification  \u0014\u0017\u001a\n   information management policies  \u0014\u0017\u001a\n   jurisdictional policies  \u0014\u0017\u001a\n   location policies  \u0014\u0017\u001a\n   ownership  \u0014\u0017\u001a\ndata federation  \u0014\u0014\u0018\ndata privacy  \u0014\u0017\u001b\ndata security\n   life cycle  \u0014\u0017\u0017, \u0014\u0017\u0018\ndata virtualization  \u0013\u001b\u001a\ndata-driven design  \u0014\u001b\u0016, \u0014\u001b\u0017\ndatabase  \u0014\u0013\u0019, \u0014\u0013\u001a, \u0014\u0013\u001b\ndatabase resiliency  \u0013\u0015\u0014\n",
      "content_length": 2347,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 363,
      "content": "[ 349 ]\ndedicated cloud  \u0015\u0012, \u0015\u0014\nDepartment of Defense (DoD)  \u0013\u0013\u0019\ndeployment models\n   for executives  \u001b\u001a\ndesign consideration, cloud computing\n   economic  \u0018\u0012, \u0018\u0013, \u0018\u0014\n   plans  \u0018\u0015, \u0018\u0016, \u0018\u0017, \u0018\u0019, \u0018\u001a\n   thought process  \u0017\u001a, \u0017\u001b\nDevOps  \u0014\u0017\u0012\ndew computing  \u0015\u0017\ndirect comparison selection, Burstorm lab 2\n   performance  \u0015\u0012\u0014, \u0015\u0012\u0015\n   price  \u0015\u0012\u0012, \u0015\u0012\u0013\n   price-to-performance  \u0015\u0012\u0016, \u0015\u0012\u0017, \u0015\u0012\u0018\ndisaster recovery (DR)  \u0013\u0013\u001a, \u0014\u0019\u0015\ndistributed computing reference model\n   about  \u0014\u0015\u0016\n   reference link  \u0014\u0015\u0016\ndomain name system (DNS)  \u0013\u0018\u001a\nDomain Name System (DNS)  \u0013\u0018\u0014\nDomain name system server (DNSS)  \u0013\u0019\u0016\ndomain-specific language (DSL)  \u0014\u0017\u0012\nE\ne-commerce\n   about  \u0013\u0016\u001a\n   cloud provider components  \u0013\u0017\u0012\n   enterprise network components  \u0013\u0017\u0016\n   public network components  \u0013\u0016\u001b\nedge computing  \u0015\u0017\nedge tier, IoT\n   cloud service provider  \u0013\u001a\u0013\n   enterprise network  \u0013\u001a\u0014\n   public network  \u0013\u001a\u0013\n   security  \u0013\u001a\u0014\nElastic infrastructure  \u0013\u001b\u0012\nElastic platform  \u0013\u001b\u0013\nelectronic fund transfers (EFT)  \u0013\u0017\u0013\nEnterprise Application Integration (EAI)  \u0013\u001a\u0018\nenterprise application resource (EAR)  \u0014\u0016\u001b\nenterprise network components\n   about  \u0013\u0016\u0019, \u0013\u0017\u0016\n   enterprise data  \u0013\u0017\u0016\n   security  \u0013\u0017\u0017\n   service tier  \u0013\u0016\u001a\nEnterprise Resource Planning (ERP)  \u0013\u001a\u0015\nEnterprise Service Bus (ESB)  \u0013\u0019\u0018\nenterprise social collaboration\n   about  \u0013\u0018\u0012\n   cloud customer reference architecture, for\nenterprise social collaboration  \u0013\u0018\u0012\nenvironment-based availability  \u0013\u001b\u0014\neXtensible Markup Language (XML)  \u0013\u0013\u0017\nF\nFederal Risk and Authorization Management\nProgram (FedRAMP)  \u0013\u0013\u0019\nFederal Risk Authorization and Management\nProgram (FedRAMP)  \u0018\u0017\nfederated identity  \u0014\u0014\u0018\nfinancial levers  \u0019\u0019\nfog computing  \u0015\u0017\nfunctions, date security\n   access  \u0014\u0017\u0018\n   process  \u0014\u0017\u0018\n   store  \u0014\u0017\u0018\nG\nGeneral Data Protection Regulation (GDPR)  \u0018\u001a,\n\u0014\u0014\u001b, \u0014\u0017\u0016\nglobal server load balancing  \u0013\u0015\u0013\nGoogle App Engine (GAE)  \u0014\u0014\u0012\nGovernance, Risk Management, and Compliance\n(GRC)  \u0013\u0012\u001b\ngraphical user interface (GUI)  \u0014\u0012\u0015\ngrid computing  \u0015\u0017\nH\nHands-on lab 3\n   current billing file  \u0017, \u0015\u0013\u0015, \u0015\u0013\u0016, \u0015\u0013\u0017, \u0015\u0013\u0019, \u0015\u0013\u001a,\n\u0015\u0013\u001b, \u0015\u0014\u0013, \u0015\u0014\u0015, \u0015\u0014\u0016, \u0015\u0014\u0018, \u0015\u0014\u0019, \u0015\u0014\u001a, \u0015\u0014\u001b, \u0015\u0015\u0012,\n\u0015\u0015\u0015, \u0015\u0015\u0017, \u0015\u0015\u0019\n   data visualization  \u0015\u0013\u0012, \u0015\u0013\u0013, \u0015\u0013\u0014\n   NeBu Systems transformation progress update \n\u0015\u0013\u0014, \u0015\u0013\u0015\nHealth Insurance Portability and Accountability Act\n(HIPAA)  \u0018\u001a\nhybrid cloud\n   about  \u0015\u0015\n",
      "content_length": 2259,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "[ 350 ]\n   application functions  \u0013\u0016\u0012\n   architecting  \u0013\u0015\u001a\n   backend  \u0013\u0016\u0012\n   backup  \u0013\u0015\u001b\n   data  \u0013\u0015\u001b\n   development environment  \u0013\u0016\u0013\n   multimedia web application  \u0013\u0016\u0013\n   processing  \u0013\u0015\u001b\n   user interface  \u0013\u0015\u001a\nhybrid integration\n   architecture  \u0013\u001a\u0015\n   cloud provider network  \u0013\u001a\u0017\n   connectivity  \u0013\u001a\u0015\n   deployment  \u0013\u001a\u0015\n   enterprise network  \u0013\u001a\u0018\n   public network  \u0013\u001a\u0017\n   roles  \u0013\u001a\u0015\n   styles  \u0013\u001a\u0016\nHypertext Transfer Protocol (HTTP)  \u0014\u0014\u0016\nhypervisor\n   type 1 hypevisor  \u0013\u001b\u0018\n   type 2 hypervisor  \u0013\u001b\u0018\nI\nIaaS\n   about  \u0014\u0012\u0016\n   background  \u0014\u0014\n   compute services  \u0014\u0012\u0016, \u0014\u0012\u0017\n   considerations  \u0014\u0015\n   storage services  \u0014\u0012\u0017, \u0014\u0012\u0018, \u0014\u0012\u0019\nidentity management  \u0014\u0014\u0019\nimplementation models\n   for executives  \u001b\u001b\nimplementation strategy  \u0016\u0016\nInformation Technology (IT)  \u0013\u001a\u0015\nInformation Technology Infrastructure Library (ITIL) \n\u0016\u001a\nIntegrated Development Environment (IDE)  \u0013\u0013\u0017\nintegrated development environment (IDE)  \u0013\u001b\u0013,\n\u0014\u0013\u001b, \u0014\u0016\u001b\nInternet of Things (IoT)  \u0014\u0012\u0015\ninteroperability\n   about  \u0014\u0014\u0019\n   applications  \u0014\u0014\u0019\n   data  \u0014\u0014\u0019\n   infrastructure  \u0014\u0014\u001a\n   management  \u0014\u0014\u001a\n   platforms  \u0014\u0014\u0019\n   publication and acquisition  \u0014\u0014\u001a\nIoT\n   about  \u0015\u0017\n   architecture  \u0013\u0019\u0019, \u0013\u0019\u001b\n   architecture, aspects  \u0013\u0019\u001a\n   edge tier  \u0013\u0019\u001b\nIT governance  \u0016\u0013, \u0016\u0014, \u0016\u0015, \u001b\u001b\nIT service management  \u0016\u001a\nIT service management (ITSM)  \u0016\u001a\nIT service management framework  \u0017\u0014\nIT-as-a-Service\n   components  \u0013\u0017\nJ\nJava  \u0014\u0016\u001b\nJava Server Pages (JSP)  \u0014\u0017\u0012\nJavaScript Object Notation (JSON)  \u0013\u0013\u0018\njust-in-time (JIT)  \u0014\u0016\u001b\nK\nkey performance indicators (KPIs)\n   about  \u001a\u0017\n   business goal KPIs  \u001a\u0018\n   economic goal metric  \u001a\u0019\nKVM (Kernel-based Virtual Machine)  \u0013\u001b\u0018\nL\nLAMP  \u0014\u0016\u001b\nLAMP Stack (Linux, Apache, MySQL, PHP)  \u0013\u0014\u0015\nlegal requisites  \u0013\u0013\u0019\nlife cycle, data security\n   archive  \u0014\u0017\u0017\n   create  \u0014\u0017\u0017\n   destroy  \u0014\u0017\u0017\n   share  \u0014\u0017\u0017\n   store  \u0014\u0017\u0017\n   use  \u0014\u0017\u0017\nlifecycle management  \u0014\u0014\u001a\nlocation awareness  \u0014\u0014\u001a\nloose coupling  \u0014\u0016\u0017\n",
      "content_length": 1868,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "[ 351 ]\nM\nmachine learning  \u0015\u0018\nmanaged service provider (MSP)  \u0016\u0016\nmarketing resource management (MRM)  \u0013\u0017\u0015\nmean time between failure (MTBF)  \u0014\u0019\u0013\nmean time to repair (MTTR)  \u0014\u0019\u0013\nmetering  \u0014\u0014\u001a\nmicroservice architecture  \u0014\u0017\u0013\nmobile applications\n   about  \u0013\u0017\u0017\n   cloud service components  \u0013\u0017\u001a\n   enterprise network components  \u0013\u0018\u0012\n   mobile device components  \u0013\u0017\u0019\n   public network components  \u0013\u0017\u0019\nmobile architecture components  \u0013\u0017\u0017\nMobile device management (MDM)  \u0013\u0017\u001a\nmonitoring  \u0014\u0014\u001a\nmonthly recurring cost (MRC)  \u0015\u0013\u0017\nmulti-cloud analysis platform (MCAP)  \u0014\u0017\u0014\nmulti-data center architecture  \u0013\u0015\u0012\nN\nNational Institute of Standards and Technology\n(NIST)  \u0013\u0018\nnative applications  \u0014\u0012\u0014\nNeBu Systems\n   decisions, based on non-negotiable concepts \n\u0014\u001b\u0018\nnetwork address translations (NAT)  \u0014\u0012\u001b\nnetwork attached storage (NAS)  \u0014\u0012\u0018\nnetwork functions virtualization (NFV)  \u0013\u001b\u0019\nnetwork virtualization (NV)  \u0013\u001b\u0019\nnetworking interface cards (NICs)  \u0014\u0012\u001b\nneural networks  \u0015\u0018\nnode-based availability  \u0013\u001b\u0014\nNon-personally identifiable information (non-PII) \n\u0014\u0018\u0013\nnon-redundant three-tier architecture  \u0013\u0014\u0016\nO\nonce-in-a-lifetime workloads  \u0013\u0013\u0012\nopen authorization (OAUTH)  \u0013\u0018\u0016\nopen client  \u0014\u0014\u001b\nOperational Expenditure (OPEX)  \u0017\u0017\noperational level agreements (OLAs)  \u0017\u0016\nOperations Technology (OT)  \u0013\u001a\u0015\nOrganization Normative Framework (ONF)\nmanagement process  \u0014\u0018\u0015\norganizational assessment  \u0013\u0013\u001b\norganizations\n   governance  \u0014\u0015\u0012\n   management  \u0014\u0015\u0012\nOSI model and layer description\n   about  \u0013\u0014\u0019\n   autoscaling architecture  \u0013\u0014\u001a\n   logical and physical design  \u0013\u0014\u001a\nP\nperformance  \u0014\u0015\u0012\npersonally identifiable information (PII)  \u0014\u0018\u0012\nPlatform-as-a-Service (PaaS)\n   about  \u0018\u0016, \u0013\u0018\u0014, \u0014\u0013\u0019\n   background  \u0014\u0018\n   considerations  \u0014\u0019, \u0014\u001a\nportability\n   about  \u0014\u0014\u0019\n   applications  \u0014\u0014\u0019\n   data  \u0014\u0014\u0019\n   infrastructure  \u0014\u0014\u001a\n   management  \u0014\u0014\u001a\n   platforms  \u0014\u0014\u0019\n   publication and acquisition  \u0014\u0014\u001a\nprivacy addresses  \u0014\u0014\u001b\nprivacy and data protection (P and DP) laws  \u0014\u0017\u001b\nprivate cloud  \u0015\u0012\nProduct life cycle management (PLM)  \u0013\u0017\u0014\nprovider exit strategy plan  \u0014\u0015\u0013\npublic cloud\n   about  \u0014\u001b\n   benefits  \u0015\u0012\n   considerations  \u0015\u0012\npublic network\n   about  \u0013\u0016\u0019\n   cloud provider network components  \u0013\u0016\u0019\n   enterprise network components  \u0013\u0016\u0019\n   security components  \u0013\u0016\u001a\nPuppet  \u0014\u0017\u0012\n",
      "content_length": 2231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "[ 352 ]\nR\nRadio Frequency Identification (RFID)  \u0013\u0017\u0014\nrandom access memory (RAM)  \u0014\u0012\u0017\nReally Simple Syndication (RSS)  \u0013\u0013\u0018\nRecovery Point Objective (RPO)  \u0017\u0018\nRecovery Time Objective (RTO)  \u0017\u0018\nRed Hat Enterprise Linux (RHEL)  \u0016\u001a\nRed Hat Enterprise Virtualization (RHEV)  \u0013\u001b\u0018\nredundant three-tier architecture\n   about  \u0013\u0014\u0017\n   horizontal scaling  \u0013\u0014\u0018\n   redundancy, versus resiliency  \u0013\u0014\u0018\n   single points of failure  \u0013\u0014\u0017\nregulatory requisites  \u0013\u0013\u0019\nRepresentational State Transfer (REST)  \u0013\u0013\u0017, \u0014\u0016\u0018\nrequest for proposal (RFP)  \u0013\u0012\u001a\nrequest for quote (RFQ)  \u0013\u0012\u001a\nresiliency  \u0014\u0014\u001b\nREST\n   about  \u0013\u0013\u0018\n   advantages  \u0013\u0013\u0018\nReturn on Investment (ROI)\n   about  \u0019\u0019, \u0019\u001a\n   driving factors  \u0019\u001b, \u001a\u0012, \u001a\u0013\nrisk\n   assessing  \u0014\u0018\u001b, \u0014\u0019\u0012, \u0014\u0019\u0013\n   framing  \u0014\u0018\u001a\n   monitoring  \u0014\u0019\u0014\nROI metrics\n   cost  \u001a\u0016\n   margin optimization  \u001a\u0016\n   quality  \u001a\u0016\n   time  \u001a\u0016\nrole, data privacy\n   controller  \u0014\u0017\u001b\n   data owner  \u0014\u0017\u001b\n   data subject  \u0014\u0017\u001b\n   processor  \u0014\u0017\u001b\nS\nSalt  \u0014\u0017\u0013\nSecurity Assertion Markup Language (SAML)  \u0013\u0018\u0016\nsecurity components  \u0013\u0016\u001a\nsecurity control\n   about  \u0014\u0015\u0013, \u0014\u0015\u0015\n   administrative  \u0014\u0015\u0014\n   logical  \u0014\u0015\u0014\n   physical  \u0014\u0015\u0014\nsecurity\n   requisites  \u0013\u0013\u0019\nserver side, cloud applications\n   JAVA  \u0014\u0016\u001b\n   LAMP  \u0014\u0016\u001b\n   WISA stack  \u0014\u0016\u001b\nserverless architecture  \u0014\u0017\u0013\nservice level agreements (SLAs)  \u0014\u0015, \u001a\u0015, \u0013\u0019\u0014, \u0014\u0013\u0018,\n\u0014\u0013\u0019, \u0014\u0016\u0015, \u0014\u0017\u0015, \u0014\u0019\u0014\nservice orientation  \u0014\u0016\u0018\nservice tier  \u0013\u0016\u001a\nservice-oriented architecture (SOA)  \u0014\u0016\u0018\nshadow IT  \u0013\u0019\nSimple Mail Transfer Protocol (SMTP)  \u0014\u0014\u0016\nSimple Object Access Protocol (SOAP)  \u0013\u0013\u0017\nsingle point of failure (SPOF)  \u0014\u0019\u0013\nSingle Sign-On (SSO)  \u0013\u0018\u0016\nsingle-site architecture\n   about  \u0013\u0014\u0016\n   non-redundant three-tier architecture  \u0013\u0014\u0016\n   redundant three-tier architecture  \u0013\u0014\u0017\nSLAs  \u0014\u0015\u0013\nSOAP\n   about  \u0013\u0013\u0017\n   advantages  \u0013\u0013\u0018\nsoftware defined networking (SDN)  \u0013\u001b\u0019\nSoftware Development Kit (SDK)  \u0013\u0019\u0019\nsoftware development kits (SDKs)  \u0013\u0017\u0019\nSoftware-as-a-Service (SaaS)  \u0013\u0018\u0014\n   about  \u0018\u0016\n   background  \u0014\u0016\n   considerations  \u0014\u0017\n   offering  \u0014\u0014\u0012\nstatic workloads  \u0013\u0013\u0012\nstorage area networks (SAN)  \u0014\u0012\u0018\nstorage services, IaaS\n   archival storage  \u0014\u0012\u001b\n   key-value storage  \u0014\u0012\u001a\n   object/blob storage  \u0014\u0012\u0019\n   volume/block storage  \u0014\u0012\u0019\n",
      "content_length": 2129,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "T\ntechnical architecture\n   requisites  \u0013\u0013\u0019\ntechnology service consumption model  \u0013\u001b\u0015, \u0013\u001b\u0016\nterminal emulator  \u0014\u0012\u0015\nterminal window  \u0014\u0012\u0015\ntext-user interface (TUI)  \u0014\u0012\u0015\nthin client\n   versus thick client  \u0014\u0012\u0015\nTool Command Language (TCL)  \u0014\u0017\u0012\nTreacherous Twelve  \u0014\u0018\u0019\nU\nunpredictable and random workloads  \u0013\u0013\u0013\nuser characteristics  \u0013\u0012\u0014, \u0013\u0012\u0017, \u0013\u0012\u0018\nV\nvirtual desktop infrastructure (VDI)  \u0013\u001b\u0018\nvirtual machine  \u0014\u0012\u0017\nvirtual machine monitor (VMM)  \u0013\u001b\u0018\nvirtual networks\n   at-least-once delivery  \u0014\u0013\u0013\n   exactly-once delivery  \u0014\u0013\u0012\n   message oriented middleware  \u0014\u0013\u0012\n   metering/monitoring  \u0014\u0013\u0014, \u0014\u0013\u0015\n   timeout-based delivery  \u0014\u0013\u0014\n   transaction-based delivery  \u0014\u0013\u0013\nvirtual private cloud  \u0015\u0014\nvirtual private network (VPN)  \u0013\u0015\u0019\nvirtualization\n   about  \u0013\u001b\u0017\n   application virtualization  \u0014\u0012\u0012\n   compute virtualization  \u0013\u001b\u0017\n   data virtualization  \u0013\u001b\u001a\n   network virtualization  \u0013\u001b\u0019\nW\nweb application hosting  \u0013\u0016\u0018\nweb apps  \u0014\u0012\u0014\nweb service tier\n   about  \u0013\u0016\u0019\n   API management  \u0013\u0016\u0019\n   transformation and connectivity  \u0013\u0016\u0019\nWeb Services Description Language (WSDL)  \u0013\u0013\u0017\nWISA stack  \u0014\u0016\u001b\n",
      "content_length": 1070,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}