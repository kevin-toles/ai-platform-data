{
  "metadata": {
    "title": "Microservices from Day One - Build robust and scalable software from the start",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 258,
    "conversion_date": "2025-12-19T17:36:46.756809",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Microservices from Day One - Build robust and scalable software from the start.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "Microservices from Day One\n\nBuild robust and scalable software from the start — Cloves Carneiro Jr. Tim Schmelmer\n\nMicroservices from Day One\n\nBuild robust and scalable software from the start\n\nCloves Carneiro Jr.\n\nTim Schmelmer\n\nMicroservices from Day One: Build robust and scalable software from the start\n\nCloves Carneiro Jr. Hollywood, Florida, USA\n\nTim Schmelmer Broomfield, Colorado, USA\n\nISBN-13 (pbk): 978-1-4842-1936-2 DOI 10.1007/978-1-4842-1937-9\n\nISBN-13 (electronic): 978-1-4842-1937-9\n\nLibrary of Congress Control Number: 2016961230\n\nCopyright © 2016 by Cloves Carneiro Jr. and Tim Schmelmer\n\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.\n\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every occurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion and to the benefit of the trademark owner, with no intention of infringement of the trademark.\n\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\n\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.\n\nManaging Director: Welmoed Spahr Acquisitions Editor: Louise Corrigan Development Editor: Chris Nelson Technical Reviewer: Tom Copeland Editorial Board: Steve Anglin, Pramila Balan, Laura Berendson, Aaron Black, Louise Corrigan, Jonathan Gennick, Todd Green, Robert Hutchinson, Celestin Suresh John, Nikhil Karkal, James Markham, Susan McDermott, Matthew Moodie, Natalie Pao, Gwenan Spearing\n\nCoordinating Editor: Nancy Chen Copy Editor: James A. Compton Compositor: SPi Global Indexer: SPi Global Artist: SPi Global, image courtesy of Freepik\n\nDistributed to the book trade worldwide by Springer Science+Business Media New York, 233 Spring Street, 6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders-ny@springer-sbm.com, or visit www.springer.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation.\n\nFor information on translations, please e-mail rights@apress.com, or visit www.apress.com.\n\nApress and friends of ED books may be purchased in bulk for academic, corporate, or promotional use. eBook versions and licenses are also available for most titles. For more information, reference our Special Bulk Sales–eBook Licensing web page at www.apress.com/bulk-sales.\n\nAny source code or other supplementary materials referenced by the author in this text are available to readers at www.apress.com. For detailed information about how to locate your book’s source code, go to www.apress.com/source-code/. Readers can also access source code at SpringerLink in the Supplementary Material section for each chapter.\n\nPrinted on acid-free paper\n\nTo my wife Safora, for all her love, and for her support during the hundreds of hours that I spent on this book while she ran the family.\n\n—Tim Schmelmer\n\nFor my family, Jane, Noah, and Sofia.\n\n—Cloves Carneiro Jr.\n\nContents at a Glance\n\nAbout the Authors ����������������������������������������������������������������������������������������������������xv\n\nAbout the Technical Reviewer �������������������������������������������������������������������������������xvii\n\nAcknowledgments ��������������������������������������������������������������������������������������������������xix\n\nIntroduction ������������������������������������������������������������������������������������������������������������xxi\n\n■Part I: Service Oriented Architectures ������������������������������������������������ 1\n\n■Chapter 1: Microservices: The What and the Why �������������������������������������������������3\n\n■Chapter 2: Rules of Engagement �������������������������������������������������������������������������19\n\n■Part II: APIs ��������������������������������������������������������������������������������������� 23\n\n■Chapter 3: Partitioning Your Business into Services �������������������������������������������25\n\n■Chapter 4: Designing Your APIs ��������������������������������������������������������������������������37\n\n■Chapter 5: Defining APIs ��������������������������������������������������������������������������������������59\n\n■Chapter 6: Consuming Your APIs �������������������������������������������������������������������������75\n\n■Chapter 7: Optimizing Your APIs �������������������������������������������������������������������������83\n\n■Part III: Development and Deployment ������������������������������������������� 103\n\n■Chapter 8: Development Environment and Workflow ����������������������������������������105\n\n■Chapter 9: Testing with Services �����������������������������������������������������������������������127\n\n■Chapter 10: Deploying and Running Microservices �������������������������������������������151\n\nv\n\n■ Contents at a GlanCe\n\n■Part IV: Putting Everything Together ���������������������������������������������� 175\n\n■Chapter 11: Polyglot Services ���������������������������������������������������������������������������177\n\n■Chapter 12: Single-Client Application—Mobile and Web ����������������������������������185\n\n■Chapter 13: Monitoring Your Services ���������������������������������������������������������������211\n\n■Chapter 14: Operational Excellence with Microservices �����������������������������������221\n\nIndex ���������������������������������������������������������������������������������������������������������������������243\n\nvi\n\nContents\n\nAbout the Authors ����������������������������������������������������������������������������������������������������xv\n\nAbout the Technical Reviewer �������������������������������������������������������������������������������xvii\n\nAcknowledgments ��������������������������������������������������������������������������������������������������xix\n\nIntroduction ������������������������������������������������������������������������������������������������������������xxi\n\n■Part I: Service Oriented Architectures ������������������������������������������������ 1\n\n■Chapter 1: Microservices: The What and the Why �������������������������������������������������3\n\nAll Successful Software Undergoes Change ��������������������������������������������������������������������3\n\nRemember Service-Oriented Architectures? ��������������������������������������������������������������������3\n\nWhat Is a Service? ������������������������������������������������������������������������������������������������������������4\n\nWhat Are Microservices? ������������������������������������������������������������������������������������������������������������������������4\n\nMicroservices Are Not “CORBA over HTTP” ��������������������������������������������������������������������������������������������5\n\nIndustry Adoption of Microservices ����������������������������������������������������������������������������������6\n\nWhy Use Microservices? ��������������������������������������������������������������������������������������������������9\n\nAdvantages as Seen by Industry Experts �����������������������������������������������������������������������������������������������9\n\nOur Reasons to Adopt a Microservices Architecture ���������������������������������������������������������������������������12\n\nPotential Pitfalls and How to Avoid Them �����������������������������������������������������������������������13\n\nWhen Should I Start with a Microservices Architecture? �����������������������������������������������17\n\nSummary ������������������������������������������������������������������������������������������������������������������������18\n\n■Chapter 2: Rules of Engagement �������������������������������������������������������������������������19\n\nCustomer-Facing Applications Cannot Directly Touch Any Data Store ���������������������������19\n\nNo Service Accesses Another Service’s Data Store �������������������������������������������������������20\n\nInvest in Making Spinning Up New Services Trivial �������������������������������������������������������20\n\nvii\n\n■ Contents\n\nBuild Services in a Consistent Way ��������������������������������������������������������������������������������21\n\nDefine API Contracts in Code ������������������������������������������������������������������������������������������21\n\nEvery Service Has at Least a Development and a Production Instance �������������������������22\n\nExisting Code Bases �����������������������������������������������������������������������������������������������������������������������������22\n\nSummary ������������������������������������������������������������������������������������������������������������������������22\n\n■Part II: APIs ��������������������������������������������������������������������������������������� 23\n\n■Chapter 3: Partitioning Your Business into Services �������������������������������������������25\n\nGeneral Approaches for Application Decomposition and Aggregation ���������������������������25\n\nApplication Decomposition �������������������������������������������������������������������������������������������������������������������25\n\nApplication Aggregation������������������������������������������������������������������������������������������������������������������������28\n\nWhen to Use Decomposition versus Aggregation? ���������������������������������������������������������30\n\nDecomposing and Aggregating a Sample Application ����������������������������������������������������31\n\nWelcome to Our Bookstore! ������������������������������������������������������������������������������������������������������������������31\n\nPartitioning the Bookstore ��������������������������������������������������������������������������������������������������������������������32\n\nSummary ������������������������������������������������������������������������������������������������������������������������36\n\n■Chapter 4: Designing Your APIs ��������������������������������������������������������������������������37\n\nIntroducing APIs �������������������������������������������������������������������������������������������������������������37\n\nThe Significance of API Design �������������������������������������������������������������������������������������������������������������38\n\nWhy is Designing a Good API Important? ����������������������������������������������������������������������������������������������38\n\nWhat Makes an API Good?����������������������������������������������������������������������������������������������39\n\nGeneral Characteristics ������������������������������������������������������������������������������������������������������������������������39\n\nHypermedia Type ����������������������������������������������������������������������������������������������������������������������������������40\n\nAPI Design Guide ����������������������������������������������������������������������������������������������������������������������������������41\n\nNonfunctional Requirements ����������������������������������������������������������������������������������������������������������������42\n\nImplementing APIs the Right Way ����������������������������������������������������������������������������������43\n\nGather Requirements ����������������������������������������������������������������������������������������������������������������������������43\n\nMaximize Information Hiding ����������������������������������������������������������������������������������������������������������������44\n\nAPIs Change ������������������������������������������������������������������������������������������������������������������������������������������45\n\nviii",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary",
      "content": "■ Contents\n\nDesigning / Splitting Up by Example ������������������������������������������������������������������������������49\n\nThe Bookstore Home Page �������������������������������������������������������������������������������������������������������������������49\n\nThe Book Detail Page����������������������������������������������������������������������������������������������������������������������������52\n\nSummary ������������������������������������������������������������������������������������������������������������������������57\n\n■Chapter 5: Defining APIs ��������������������������������������������������������������������������������������59\n\nInterface Definition Languages (IDLs) ����������������������������������������������������������������������������59\n\nA Matter of Style: RPC versus REST �������������������������������������������������������������������������������60\n\nRPC-Style Interface Definition Languages ��������������������������������������������������������������������������������������������60\n\nRESTful Interface Definition Languages �����������������������������������������������������������������������������������������������60\n\nOur Stance ��������������������������������������������������������������������������������������������������������������������������������������������61\n\nDescribing APIs using Swagger ��������������������������������������������������������������������������������������61\n\nHosting Swagger Specification as JSON Schemas ������������������������������������������������������������������������������62\n\nThe Swagger User Interface �����������������������������������������������������������������������������������������������������������������65\n\nGenerating Code from Swagger Definitions �����������������������������������������������������������������������������������������68\n\nGenerating the Swagger JSON Specifications ��������������������������������������������������������������������������������������71\n\nSummary ������������������������������������������������������������������������������������������������������������������������74\n\n■Chapter 6: Consuming Your APIs �������������������������������������������������������������������������75\n\nWhat Are Client Libraries? ����������������������������������������������������������������������������������������������75\n\nWhat Not to Do ���������������������������������������������������������������������������������������������������������������76\n\nDon’t wait eternally ������������������������������������������������������������������������������������������������������������������������������76\n\nDon’t make the clients smart ���������������������������������������������������������������������������������������������������������������76\n\nService Orchestration �����������������������������������������������������������������������������������������������������77\n\nCaching in the Client ������������������������������������������������������������������������������������������������������78\n\nBuild Caching into Your Client Library ��������������������������������������������������������������������������������������������������78\n\nError Handling ����������������������������������������������������������������������������������������������������������������78\n\nTimeouts �����������������������������������������������������������������������������������������������������������������������������������������������79\n\nCircuit Breakers ������������������������������������������������������������������������������������������������������������������������������������79\n\nPerformance �������������������������������������������������������������������������������������������������������������������80\n\nUpdating Client Libraries ������������������������������������������������������������������������������������������������81\n\nix\n\n■ Contents\n\nService Discovery �����������������������������������������������������������������������������������������������������������81\n\nSummary ������������������������������������������������������������������������������������������������������������������������82\n\n■Chapter 7: Optimizing Your APIs �������������������������������������������������������������������������83\n\nMeasuring Performance �������������������������������������������������������������������������������������������������83\n\nService Access Pattern-Based Improvements ���������������������������������������������������������������85\n\nThe Blessing and the Curse of Caching ��������������������������������������������������������������������������86\n\nCaching in the API Consumer ���������������������������������������������������������������������������������������������������������������86\n\nCaching in the API Provider ������������������������������������������������������������������������������������������������������������������86\n\nMake Your Service Work Less ����������������������������������������������������������������������������������������87\n\nUse the Standard ����������������������������������������������������������������������������������������������������������������������������������87\n\nFramework Support for HTTP Conditional GET �������������������������������������������������������������������������������������89\n\nCaching Proxies ��������������������������������������������������������������������������������������������������������������92\n\nSize Matters �������������������������������������������������������������������������������������������������������������������93\n\nResult Representations�������������������������������������������������������������������������������������������������������������������������93\n\nPartial Resources ����������������������������������������������������������������������������������������������������������������������������������94\n\nHTTP Compression �������������������������������������������������������������������������������������������������������������������������������95\n\nSemantic Paging �����������������������������������������������������������������������������������������������������������������������������������96\n\nOptimizing for Efficiency of Development and Support ��������������������������������������������������98\n\nTalk Less, Listen More ����������������������������������������������������������������������������������������������������99\n\nSummary ����������������������������������������������������������������������������������������������������������������������101\n\n■Part III: Development and Deployment ������������������������������������������� 103\n\n■Chapter 8: Development Environment and Workflow ����������������������������������������105\n\nThe Premise ����������������������������������������������������������������������������������������������������������������105\n\nThe Development Environment Wish List ���������������������������������������������������������������������105\n\nEase of Development Tool Setup ��������������������������������������������������������������������������������������������������������106\n\nEase of Application Setup �������������������������������������������������������������������������������������������������������������������106\n\nAutomation of Application Tests ���������������������������������������������������������������������������������������������������������106\n\nAvailability of Sample Data �����������������������������������������������������������������������������������������������������������������106\n\nUp-to-Date Dependencies ������������������������������������������������������������������������������������������������������������������106\n\nx\n\n■ Contents\n\nFast Developer Machines �������������������������������������������������������������������������������������������������������������������107\n\nConcurrency of Development �������������������������������������������������������������������������������������������������������������107\n\nCollaborative Development �����������������������������������������������������������������������������������������������������������������107\n\nPrivacy of Development ����������������������������������������������������������������������������������������������������������������������107\n\nImmediacy of Change Feedback���������������������������������������������������������������������������������������������������������108\n\nEase of Service Generation�����������������������������������������������������������������������������������������������������������������108\n\nAccessibility of Documentation ����������������������������������������������������������������������������������������������������������108\n\nMaking the Wishes Come True �������������������������������������������������������������������������������������108\n\nAutomating Developer Machine Set-Up ����������������������������������������������������������������������������������������������108\n\nAutomated Application Set-Up ������������������������������������������������������������������������������������������������������������109\n\nMaking Testing Efficient with Common Tools �������������������������������������������������������������������������������������111\n\nShipping Sample Data with the Code �������������������������������������������������������������������������������������������������117\n\nMaintaining a Development Sandbox �������������������������������������������������������������������������������������������������120\n\nUsing a Rewriting Proxy for Edge Device Development ���������������������������������������������������������������������123\n\nCodifying Internal Standards via Generation Tools �����������������������������������������������������������������������������125\n\nMaking Documentation Search-Efficient ��������������������������������������������������������������������������������������������125\n\nSummary ����������������������������������������������������������������������������������������������������������������������126\n\n■Chapter 9: Testing with Services �����������������������������������������������������������������������127\n\nLayers of Microservice Testing �������������������������������������������������������������������������������������127\n\nThe Five Layers of Testing �����������������������������������������������������������������������������������������������������������������128\n\nThe Test Pyramid ��������������������������������������������������������������������������������������������������������������������������������133\n\nTesting Examples ���������������������������������������������������������������������������������������������������������134\n\nIn-Process Stubs for Component and Unit Testing �����������������������������������������������������������������������������134\n\nOut-Of-Process Component Testing����������������������������������������������������������������������������������������������������141\n\nExamples of End-to-End Tests ������������������������������������������������������������������������������������������������������������145\n\nContract Testing Examples �����������������������������������������������������������������������������������������������������������������147\n\nSummary ����������������������������������������������������������������������������������������������������������������������150\n\n■Chapter 10: Deploying and Running Microservices �������������������������������������������151\n\nAutomation Is Key ��������������������������������������������������������������������������������������������������������151\n\nContinuously Integrating Microservices �����������������������������������������������������������������������152\n\nxi\n\n■ Contents\n\nA Brief History �������������������������������������������������������������������������������������������������������������������������������������152\n\nThe Advantages of Using CI Tools �������������������������������������������������������������������������������������������������������152\n\nUsing CI for Microservices ������������������������������������������������������������������������������������������������������������������153\n\nContinuous Code Delivery ��������������������������������������������������������������������������������������������154\n\nHow Do You Know You Are Doing CD? ������������������������������������������������������������������������������������������������154\n\nBreaking the Release Process into Stages �����������������������������������������������������������������������������������������154\n\nCD and Microservices �������������������������������������������������������������������������������������������������������������������������156\n\nTool Support for Continuous Delivery �������������������������������������������������������������������������������������������������156\n\nMaturing Code Preproduction ��������������������������������������������������������������������������������������159\n\nStaging as a Preproduction Environment �������������������������������������������������������������������������������������������159\n\nTool Support for Staging Environments ����������������������������������������������������������������������������������������������161\n\nSafer Releases ������������������������������������������������������������������������������������������������������������������������������������162\n\nAssigning Resources to Microservices ������������������������������������������������������������������������164\n\nTenets for Microservice Deployments ������������������������������������������������������������������������������������������������165\n\nOptions for Running Microservices ����������������������������������������������������������������������������������������������������166\n\nSummary ����������������������������������������������������������������������������������������������������������������������173\n\n■Part IV: Putting Everything Together ���������������������������������������������� 175\n\n■Chapter 11: Polyglot Services ���������������������������������������������������������������������������177\n\nDimensions of Change �������������������������������������������������������������������������������������������������177\n\nAdvantages of Using Different Languages and Technologies ���������������������������������������177\n\nSpecialization �������������������������������������������������������������������������������������������������������������������������������������178\n\nExperimentation ���������������������������������������������������������������������������������������������������������������������������������178\n\nEvolution ���������������������������������������������������������������������������������������������������������������������������������������������179\n\nNavigating the Pitfalls of Being Polyglot ����������������������������������������������������������������������179\n\nA Process for Adopting New Technologies �������������������������������������������������������������������180\n\nDocument All Findings ������������������������������������������������������������������������������������������������������������������������180\n\nStatement of Motivation and Goals�����������������������������������������������������������������������������������������������������180\n\nImplementation Planning ��������������������������������������������������������������������������������������������������������������������181\n\nGo-Live Criteria �����������������������������������������������������������������������������������������������������������������������������������182\n\nxii\n\n■ Contents\n\nA Case Study: Moving books-service from Rails to Rust ����������������������������������������������182\n\nSummary ����������������������������������������������������������������������������������������������������������������������184\n\n■Chapter 12: Single-Client Application—Mobile and Web ����������������������������������185\n\nApplication Overview����������������������������������������������������������������������������������������������������185\n\nHome Page �������������������������������������������������������������������������������������������������������������������186\n\nFeatured Categories ���������������������������������������������������������������������������������������������������������������������������192\n\nUsers Should Never See Internal Errors ���������������������������������������������������������������������������������������������195\n\nCircuit Breakers ����������������������������������������������������������������������������������������������������������������������������������197\n\nBook Detail Page ����������������������������������������������������������������������������������������������������������197\n\nThe Shopping Cart Page �����������������������������������������������������������������������������������������������203\n\nExercises for You ����������������������������������������������������������������������������������������������������������206\n\nService Orchestration ���������������������������������������������������������������������������������������������������207\n\nPerformance Considerations ����������������������������������������������������������������������������������������208\n\nSummary ����������������������������������������������������������������������������������������������������������������������209\n\n■Chapter 13: Monitoring Your Services ���������������������������������������������������������������211\n\nMonitor All Services �����������������������������������������������������������������������������������������������������211\n\nWhat Do You Need to Monitor? �����������������������������������������������������������������������������������������������������������212\n\nFocus on Perceived Performance �������������������������������������������������������������������������������������������������������212\n\nAdhere to SLAs �����������������������������������������������������������������������������������������������������������������������������������213\n\nBusiness Metrics Are Important ���������������������������������������������������������������������������������������������������������213\n\nIdentifying Requests �����������������������������������������������������������������������������������������������������214\n\nLogging All Requests ��������������������������������������������������������������������������������������������������������������������������215\n\nReading Metrics ����������������������������������������������������������������������������������������������������������������������������������216\n\nAdjusting Alarms���������������������������������������������������������������������������������������������������������������������������������218\n\nSummary ����������������������������������������������������������������������������������������������������������������������219\n\n■Chapter 14: Operational Excellence with Microservices �����������������������������������221\n\nWhat is Operational Support? ��������������������������������������������������������������������������������������221\n\nEmbracing Ownership ��������������������������������������������������������������������������������������������������221\n\nxiii\n\n■ Contents\n\nCustomers First ����������������������������������������������������������������������������������������������������������������������������������222\n\nNo Code without an Owning Team ������������������������������������������������������������������������������������������������������223\n\nBoosting Team Cohesion���������������������������������������������������������������������������������������������������������������������223\n\nWhat It Means to Be On-Call ����������������������������������������������������������������������������������������224\n\nDefining the Role and Process for Being On-Call �������������������������������������������������������������������������������224\n\nThe On-Call Person in Relation to the Team ���������������������������������������������������������������������������������������227\n\nBeing On-Call in a Microservices Environment ����������������������������������������������������������������������������������228\n\nTracking and Scheduling Operations Support Work �����������������������������������������������������229\n\nA Lightweight Process Proposal ���������������������������������������������������������������������������������������������������������229\n\nFeatures to Look for When Picking a Trouble Ticket System ��������������������������������������������������������������230\n\nDashboards to Support Operations ������������������������������������������������������������������������������232\n\nOutage Etiquette �����������������������������������������������������������������������������������������������������������234\n\nEvery Pager Alarm is a Failure ������������������������������������������������������������������������������������������������������������235\n\nPlaybooks for Operational Support �����������������������������������������������������������������������������������������������������235\n\nWhat to Do When the Pager Goes Off �������������������������������������������������������������������������������������������������236\n\nSupporting Tools ���������������������������������������������������������������������������������������������������������������������������������237\n\nHow to Learn from Outages:—Post-Mortems ��������������������������������������������������������������238\n\nWhen Is a Post-Mortem the Right Tool? ���������������������������������������������������������������������������������������������238\n\nStructural Elements of a Post-Mortem �����������������������������������������������������������������������������������������������238\n\nPost-Mortem Ownership ���������������������������������������������������������������������������������������������������������������������240\n\nDefining and Advertising Team Policy ��������������������������������������������������������������������������240\n\nImpact Severity and SLAs �������������������������������������������������������������������������������������������������������������������240\n\nShare the Steps of Your Process ��������������������������������������������������������������������������������������������������������241\n\nSummary ����������������������������������������������������������������������������������������������������������������������242\n\nIndex ���������������������������������������������������������������������������������������������������������������������243\n\nxiv\n\nAbout the Authors\n\nCloves Carneiro Jr. is a senior software engineer with over 15 years of experience creating software for companies in many fields. In recent years, Cloves was first introduced to the idea of distributed services at Amazon.com, where he was exposed to services with challenging scalability requirements for the Kindle and AWS teams. In 2001, he joined the amazing Engineering team at LivingSocial, and has focused on building microservices as scalable pieces of software to help improve the reliability and availability of the systems at the company. In his free time, Cloves enjoys spending time with his family in sunny South Florida, and is an avid—yet slow—triathlete. Cloves has also previously written for Apress, authoring Beginning Rails, Beginning Rails 3, and Beginning Rails 4.\n\nAfter abandoning his childhood dream of becoming a firefighter, Tim Schmelmer did the next best thing and became a software developer. He has spent the past 19.75 years extinguishing fires while working for technology consulting companies, telecommunications equipment manufacturers, building control systems for research telescopes, and selling things to people via a web browser and mobile apps. Tim found his love for building microservices systems while working at Amazon.com, and he has most recently been working on a team that does surprisingly little firefighting while building and maintaining LivingSocial.com’s core services platform. When he is not hacking away on his keyboard, or trying to be a good husband, father of two girls, and tamer of an overly energetic puppy, Tim loves to run, bike, and hike in the mountains near beautiful Boulder, Colorado.\n\nxv\n\nAbout the Technical Reviewer\n\nTom Copeland is a Rails developer at LivingSocial.com. He has programmed in Ruby since 2004, consulted at InfoEther on Ruby on Rails applications since 2007, and received a “Ruby Hero” award in 2008. Before focusing on Ruby, he wrote two books about Java programming (PMD Applied and Generating Parsers with JavaCC, both from Centennial Books) and was named Oracle’s “Open Source Developer of the Year” in 2003.\n\nxvii",
      "page_number": 9
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "detection_method": "topic_boundary",
      "content": "Acknowledgments\n\nWriting a book is never solely the accomplishment of the authors listed on its cover, and that is particularly true for the one you are holding in your hands right now. We would like to acknowledge that by expressing our gratitude to the people and companies that made this book possible.\n\nFirst of all, we would like to thank the team at Apress Media LLC for helping us through this process.\n\nIt all started with Ben Renow-Clark reaching out to us and planting the idea of writing another book in our heads. We are unsure how many times Nancy Chen, our coordinating editor, who did an amazing job getting all ducks in a row and chasing us for our content when chapter deadlines came approaching fast, must have cursed Ben for signing us on, though. We believe that the only person at Apress who might have had to show even more patience with us than Nancy must have been our development editor Chris Nelson, whom we owe many thanks for countless corrections, suggestions, requests for clarifications, and for pretending not to mind that neither of the authors is a native speaker of the English language.\n\nIf you find that reading this book has some positive bearing on your job in the field of software, then chances are that our technical reviewer Tom Copeland is largely responsible for that. Other than being a Ruby Hero (https://rubyheroes.com/heroes/2008), book author (https://pragprog.com/book/cbdepra/ deploying-rails), as well as generally an exceptional engineer with oodles of experience, Tom gave invaluable feedback and practical tips on how to improve this book so that it matters for practitioners.\n\nThank you also to Toby Clemson at ThoughtWorks for letting us use some of his excellent diagrams illustrating testing strategies in a microservices environment. His excellent article on this topic, published on Martin Fowler’s site, was the inspiration for much of the content in Chapter 9.\n\nCloves and Tim share very formative years of work experience at two corporations that have implemented microservices architectures: Amazon.com and LivingSocial. Both of us consider Amazon. com to be simultaneously the maverick and the gold standard for the architectural style that is now most commonly referred to as microservices. We had the privilege of working with some of the most talented engineers in the world in an environment that formed our understanding of how massively distributed services can be tamed and how they can provide value. If nothing else, then the rules of engagement we formulate in Chapter 2 are a direct result of our time at the world’s number one e-commerce store. We consider ourselves very fortunate to have had the opportunity to contribute to Amazon’s internal tech community.\n\nAlthough both of us overlapped briefly during our time as Amazonians, the first time we met in person\n\nwas when we both started at LivingSocial in late 2011. We have since worked together for the best part of five years, helping in LivingSocial’s move from a monolithic Ruby on Rails application to a microservices setup, breaking some world records on the way (http://www.guinnessworldrecords.com/world-records/most- online-vouchers-sold-in-24-hours). Not only have we learned many lessons during this—sometimes painful—transition, but we also have LivingSocial to thank for being supportive by allowing us to describe some of their systems in this book. Also, we would never have met the amazing Tom Copeland if it had not been for our time on the LivingSocial engineering team.\n\nAnother very gracious contribution to this book was LivingSocial’s sponsorship of Cloves’ and Tim’s attendance at AbrilPro Ruby (http://tropicalrb.com/2014/en/) and Rocky Mountain Ruby (http://rockymtnruby2014.busyconf.com/schedule) in 2014. The materials for, and feedback from, the workshops we held at both conferences laid the foundation for the ideas mentioned in this book. We will\n\nxix\n\n■ aCknowledGments\n\nnever possess the stamina, grit, and organizational talent it takes to arrange internationally renowned software development conferences, so we tip our heads to the organizers of such events.\n\nLast but not least, both Cloves and Tim are family men with multiple children below the age of ten. We cannot imagine how any book project would ever see the light of day without the unwavering support of the authors’ families. Tim would like to thank Safora, his wife and friend of more than 10 years, for keeping the ship that is their family afloat while he was often physically, and even more often mentally, unavailable and preoccupied with finishing this book. Cloves would also like to thank his family, Jane, Noah and Sofia, who were always present and understanding of the effort required to write another book.\n\nxx\n\nIntroduction\n\nAs you have surely deduced from its title, Microservices from Day One touches on many aspects of adopting microservices as the underlying principle of structuring your application.\n\nYour choice to read a book on microservices may have any of several different motivations. If you have heard the buzzwords and general hype surrounding the budding trend in the industry to move toward this architectural style, and now you want to find out more about what really hides behind the curtains, then this book is certainly for you.\n\nIf you are working on a team where the decision has already been made, or if you are exploring the option, to decompose your application in a fine-grained manner with many small, interdependent services to help with ease of scaling and maintainability, and you are looking for practical advice and best practices for how to get started, then this book will also deliver what you need. It is meant to serve as a guide for developers who have a desire to build robust and scalable online applications in smaller, more focused and manageable chunks, but do not know how to get started.\n\nAs the book’s title also indicates, you will get most the leverage out of this publication if you are in a situation where you are at liberty to design your business around microservices from the beginning. Microservices from Day One explains what microservices architecture are about, their advantages, how to avoid stumbling blocks along the way of implementing them, and the reasons why developers should think about using them when starting a new project.\n\nIt is complex, expensive, and at times frustrating to move a large, monolithic system to a microservices-\n\noriented approach when the monolith has reached a sizable code base and traffic load, or has become business-critical in its mission. We know this to be true from multiple years of professional experience dealing with such a metamorphosis. It is not unusual that companies have to declare technological bankruptcy on their existing monolith, and then engage in re-architecture projects that involve 95% of their software development teams simply to break such large systems apart into applications of more manageable size. One of the main arguments of this book is that employing microservices from an early stage of development not only helps you avoid such a scenario, but is also feasible, convenient, and cost-effective if you follow the advice and recipes laid out herein.\n\nBut even though this book does not specifically focus on questions of transitioning from a monolith\n\nto microservices, a reader who is faced with exactly that predicament might still find a large number of the concepts described in this book helpful and relevant.\n\nAnother property of this book is its emphasis on practicality. While some parts certainly explain\n\nconcepts in theory, priority is placed on showing these concepts in practice. This means that we will discuss sample code in the book, most of which is written in the Ruby programming language. This sample code is largely taken from an accompanying set of service and front-end applications that we developed while writing this book, simply to illustrate and put into practice most the theory discussed herein. These services together make up an imaginary bookstore business “_Books.com_”.\n\nFrom a content perspective, the book’s 14 chapters are organized into 4 parts. The first part discusses the basics of microservices architectures. Chapter 1 tries to answer the question\n\nof what microservices are, doing so by digging into the history of this style of building applications and looking at how they are implemented at companies that should count as early adopters. By Chapter 2, we are becoming much more opinionated when we are laying out a set of six fundamental rules that we see as necessary properties for any successful architecture based on microservices.\n\nxxi\n\n■ IntroduCtIon\n\nPart II describes how microservices adoption will allow you to view your business as a set of APIs. Chapter 3 helps with the task of decomposing a problem domain into very basic services and also provides patterns for reaggregating information from these core microservices into secondary, more special- purpose services and applications. We show this using the practical example of our bookstore example, developing decomposition and aggregation patterns as we go.\n\nChapter 4 starts discussing the APIs as the interfaces exposed by the services in your business, with a special focus on explaining what all the properties are that together make up a high-quality design of your APIs. Here again we go into detail by show an example API from our bookstore application, and how it was designed.\n\nChapter 5 is a deep dive into how to use automated tools and IDLs to define, generate, explore and consume the APIs you build inside your business. We will again place high importance on showing real-life examples of how this can be done, using the fictional bookstore we designed for this book.\n\nThe main topics addressed in Chapter 6 are practical tips surrounding the dos and don’ts for designing and building client libraries for the services in a microservices environment. We list the concerns we believe need to be part of any decent client-side code that accesses any of your service resources.\n\nAfter the API basics that were discussed in Chapter 4 through 6, we close out Part II by presenting\n\nan in-depth exploration of API optimization strategies in Chapter 7. In this context, optimization should not be understood as restricted to performance aspects, but also taking into account client convenience, maintainability, extensibility and immediacy of change notification.\n\nIn Part III the focus is on the particularities of setting up development environments and deployment processes in a microservices infrastructure. In Chapter 8 we define a wish list of 12 items to be successful in a microservices environment, and we show which tools can be used to check off every one of them. Chapter 9 identifies and explains five separate layers at which testing can and should be done to make sure your microservices architecture is sound, and we show concrete examples and code for automated test scenarios that exercise one of the services for our fictitious bookstore business. Chapter 10 is dedicated to all aspects of automated delivery, deployment, and releasing of microservice units, and we show practical examples of tools and strategies to achieve continuous integration and delivery in a safe and reversible manner. Along the way, we define seven tenets for deployment and all the currently available options for assigning computing resources to the various services in your infrastructure.\n\nThe fourth and final part of this book serves as a potpourri of practical techniques for making it easier\n\nto run microservices in production. Chapter 11 discusses the benefits and potential stumbling stones of adopting a heterogeneous technology stack using a hands-on scenario, and then Chapter 12 describes in detail one of the consumer-facing applications in our bookstore example. In Chapter 13 we move on to describe which parts of your microservices architecture deserve monitoring and alarming treatment, explaining some of the available tools. The book concludes with Chapter 14 on how to breed excellence in operational support in a self-learning organization that is structured around ownership of microservices.\n\nOur aim was to have this book’s topics be relevant, its structure conducive to your learning, its pace and scope appropriate, and—most of all—its focus on practical examples immediately useful to your career and business.\n\nLast but not least, we hope that you enjoy reading the book as much as we loved writing it, which means\n\nthat you will have a blast!\n\nxxii\n\nPART I\n\nService Oriented Architectures\n\nCHAPTER 1\n\nMicroservices: The What and the Why\n\nThis first part of the book discusses what we mean when we talk about service-oriented architectures in general, and microservices in particular. We will spend some time explaining the benefits, as well as touching on potential pitfalls to be avoided, involved in adopting this architectural style.\n\nIn this chapter, we begin by showing the relevance of taking the microservices approach. After defining\n\nsome general terms like service, service-oriented architecture, and microservices, we list examples of companies that successfully use them in the wild, and we discuss some of the reasons why these companies have adopted this architectural pattern. Next, we will look at some of the stumbling blocks that you encounter when utilizing microservices. We conclude the chapter by discussing the advantages of starting out with a service orientation compared to beginning with a single, monolithic application.\n\nAll Successful Software Undergoes Change If your goal is to have a software application that is successful, there will be a point in the life of your application where it will have to undergo change.\n\nTaking just web applications as an example, one change that comes to mind is that you might need\n\nto add an interface for administrators of your application data. What if you decide to add a mobile version of your site (or native mobile apps) after its initial launch? What will you do when you need to add more business intelligence into the mix and run regular business (or even continuous, real-time) metrics reports? What about the point in time when your customers request public API access to your application?\n\nIn short, it’s not a question of if you will need to invest in changing your application, but when to make\n\nthe change and how expensive it will be. Investing in building your application to be based on microservices from the beginning is your best chance at making changes expedient, safe, and inexpensive.\n\nRemember Service-Oriented Architectures? In order to understand what microservices are all about, it seems necessary to discuss how they relate to their older cousin: an architectural style commonly referred to as service-oriented architecture (SOA).\n\n3\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nAlthough SOAs were all the rage in the early 2000s, there are a lot of often -conflicting ideas out there\n\naround what an SOA is really all about. Martin Fowler raises this point in a 2005 blog post,1 and his attempt to classify the prevailing opinions can be summarized this way:\n\n\n\nAn SOA is designed to expose software through web services, usually involving a common backbone—usually using HTTP—for an organization’s applications to work with.\n\n\n\nIt is meant to make the monolithic applications disappear, and to replace them with a set of core services that expose business functionality and data. Data and logic provided by the core services will be aggregated in separate user interfaces.\n\n\n\nFor many, an SOA necessarily involves an asynchronous message bus technology, which communicates business events via “documents” (sometimes called messages) between the systems involved.\n\nFor the purpose of this book, let’s define SOA as an architectural principle in which there is a bias to\n\nbuild systems through separately implemented, deployed, evolving, and maintained services.\n\nIn a set-up like that, any client-facing “front-end” applications then solely rely on a set of (back-end)\n\nservices which expose the functionalities that the front-end applications consume in order to do their jobs.\n\nWhat Is a Service? To us, the definition of a software service is very close to the Oxford American Dictionary (nontechnical) definition of any service, in that it is “a system supplying a public need.”\n\nSoftware services should serve the needs of one or more internal or external client applications. The\n\ninterface of such services should therefore be designed from a client perspective, optimizing for maximum usefulness for its API consumer(s).\n\nWhile ideally every service is different as it optimizes for its clients’ use cases, it can generally be said\n\nthat a service consists of a piece of functionality and its associated set of data.\n\nIf the service is also the authoritative and definitive source for the current state of the data it holds and\n\nvends, then we call such a service a system of record. An examples of services where this system-of-record test does not pass could be a service that aggregates data and functionality from other services, which in turn could themselves be the systems of record for this data (for example, in an attempt to improve the usability or performance of the entire system).\n\nWhat Are Microservices? It is hard to determine exactly who first used the term microservice and when. While it seems most likely that although the term first appeared on the scene around 2011, some companies adopted this style much earlier. One example is Amazon, which broke apart its previously monolithic Obidos application into many\n\nsmaller services. In a 2006 interview with Microsoft Fellow Jim Gray for ACM Queue,2 Amazon CTO Werner Vogels describes as a general “service orientation” what comes very close to our definition of microservices:\n\n“For us service orientation means encapsulating the data with the business logic that operates on the data, with the only access through a published service interface. No direct database access is allowed from outside the service, and there’s no data sharing among the services.\"\n\n1Martin Fowler, “ServiceOrientedAmbiguity,” July 1, 2005. http://martinfowler.com/bliki/ ServiceOrientedAmbiguity.html 2Werner Vogels, “A Conversation with Werner Vogels: Learning from the Amazon technology platform,” interview by Jim Gray. ACM Queue, Volume 4, Issue 4. June 30, 2006. https://queue.acm.org/detail.cfm?id=1142065. 4\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nFormer eBay and Google executive Randy Shoup summarizes3 microservice characteristics as having\n\nthree main features:\n\n\n\nThey have a very focused scope that concentrates on a very few functionalities (at times a single one), expressed by a small and also well-defined API.\n\n\n\nThey are all very modular, and independently deployable, essentially elevating the rules of software encapsulation and modularity from the program to the deployment unit level.\n\n\n\nFinally, microservices have isolated persistence, meaning that they do not share persistent storage, such as a common database.\n\nGiven the similarities to the earlier description of an SOA, how are microservices really different? Is the\n\nterm just a more modern, trendier name for the same service-oriented architecture?\n\nThe main difference we have found is that microservices have the common trait of decomposing a large application into a set of small, focused, versioned, and loosely-coupled services, which are very inexpensive to spin up and maintain.\n\nAn excellent definition of microservices is that of Fowler and Lewis,4 as an architectural style that\n\n\"… is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies.\"\n\nThis approach is a significant departure from old-school SOA implementations, which ended up being much more accurately described as a set of medium-size to large monolithic applications, stitched together via expensive, and often complex, enterprise service bus ( ESB) solutions. This often led to much more tightly coupled systems, where many of the interapplication contracts were not explicit, but rather implicit and hidden inside the ESB “glue layer.”\n\nBy contrast, microservice architectures tend to work with very explicitly published, small and rather simple contracts between the single service applications. Most successful microservice implementations apply Postel’s Law of being “conservative in what you do, be liberal in what you accept from others” to all services that make up the entire system.\n\nMicroservices Are Not “CORBA over HTTP” Another difference between traditional SOAs and microservice architectures is that the latter approach seems to have thus far avoided the pitfalls of advocating distributed objects.\n\nThe idea of distributed objects, which was very popular during the time when SOAs were first developed, is to try to hide the difference between objects that live in the same process as the client code, or remotely in a different process—whether that is running on the same CPU or on an altogether separate machine. Technologies such as CORBA or Microsoft’s DCOM would go to great lengths to hide these differences.\n\n3Randy Shoup, “From the Monolith to Microservices: Lessons from Google and eBay”, April 24th, 2015, Craft Con 2015, http://www.ustream.tv/recorded/61479577 4Martin Fowler and James Lewis, “Microservices: a definition of this new architectural term.” March 25, 2014. http:// martinfowler.com/articles/microservices.html.\n\n5",
      "page_number": 17
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 25-32)",
      "start_page": 25,
      "end_page": 32,
      "detection_method": "topic_boundary",
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nThe problem with distributed objects is that they are based on a fallacy that invalidates the entire approach: the boundary between in-process and out-of-process may be hidden to the developer, but it still always exists. The implications of a remote call simply cannot be removed. Out-of-process calls are always slower than in-process calls, and their likelihood and ways of failing need to be addressed by a distributed system’s architecture.\n\nWell-designed microservice contracts tend to reflect this fact by not pretending to be “Remote Method Invocation over HTTP.” Whereas CORBA and similar RMI technologies tried hard to enable client applications to allow for very fine-grained calls, RESTful microservice APIs often favor coarse-grained, or even batch, endpoints to reduce the impact of the unavoidable latency introduced by out-of-process and HTTP calls, or the transfer of documents via lightweight ESBs.\n\nIndustry Adoption of Microservices While more and more companies are starting to migrate their monolith application infrastructure to a set of smaller, more manageable services, or are continuing to do so, some prominent earlier adopters stick out. Most likely the earliest large-scale adoption of what would now be called a microservices architecture\n\nwas started at Amazon.com during the very early part of the third millennium.\n\nIn the 2006 interview cited earlier, Amazon’s CTO Werner Vogels explains their evolution from a\n\nmonolithic web application called Obidos to a large-scale service-oriented architecture. Obidos, which held “all the business logic, all the display logic, and all the functionality” for Amazon’s web shop, had undergone years of extensions and performance tuning, until Amazon came to the conclusion in 2001 that its monolith- based architecture would simply not scale anymore. The same held true not just for their actual web application, but also for their efforts to optimize and scale their backing database resources.\n\nTo quote Vogels,\n\n“the parts that needed to scale independently were tied into sharing resources with other unknown code paths. There was no isolation and, as a result, no clear ownership.\"\n\nVogels goes on to disclose more details about the way and scale at which services are implemented at\n\nAmazon, mentioning that when a consumer hits Amazon.com, more than a hundred services contribute data to the page rendered.\n\nIt is also worth mentioning that Amazon.com does not necessarily expose the same APIs to its external customers as it exposes internally. While most internal services are streamlined to conform to an optimized version of WSDL (called Amazon Service Definition Language, or ASDL), externally exposed Amazon web services usually offer both RESTful, as well as WSDL-based, APIs in order to maximize customer convenience.\n\nAmazon’s process for developing and launching new features relies heavily on the use of small, isolated\n\nservices. Amazon has built an infrastructure in which services are so commoditized that they can be easily spun up, soft-launched, measured, and iterated upon, until the problem domain and the new feature’s business impact are well understood.\n\nAnother key component of Amazon’s service-oriented architecture is the company’s approach to operational support. Vogels describes this as: “you build it, you run it,” in stark contrast to a more traditional approach that erects organizational walls between software development and technical operations departments. This not only exposes developers to the operational aspects of their product, it also incentivizes them to build robust systems.\n\n6\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nAccording to Randy Shoup, former employee of both Google and eBay, both companies have also shifted significantly toward using microservices in the past years. In his talk at the 2015 CRAFT Conference cited earlier, Randy describes in great detail how quite a few large companies have migrated in three to five steps from a monolithic architecture to a current state that encompasses hundreds to sometimes thousands of microservices. The interplay between these services is not organized in a strict tier-based hierarchy. It is rather best represented in the shape of an ad-hoc direct graph, where the edges of communication evolve over time, based on the entire system’s changing requirements and use cases.\n\nRandy observes that in this graph, companies like Google, eBay, Twitter, Amazon and others who have\n\nbeen successful in adopting microservices have concentrated on standardizing the “edges, rather than the nodes” of their directed services graph. They unified communication (like network protocols, data formats, and specifying APIs in a unified way) and infrastructure support (for source control, configuration management, and monitoring and altering), rather than any particular programming language, application framework, or persistence mechanism. While it is essential that these communication and infrastructure choices are internally supported by excellent, automated tooling, there is usually no top-down mandate to use a particular tool chain. The result of what is actually in use is more like the outcome of a healthy competition for the most useful and expedient results.\n\nAccording to Randy, another essential part of a microservices architecture is that it has no up-front master plan or architecture it aims to implement. In his experience, the architecture is constantly in flux and evolves over time, adapting to the pressures of usefulness: if a service no longer proves to be useful to any client, it is retired due to lack of use.\n\nSimilar to Vogels’ points earlier, Shoup mentions that Google and eBay also place great importance on\n\nservice ownership from cradle to grave. While teams there generally have a high degree of autonomy over their technology choices, they also carry responsibility for the functionality, quality, performance, reliability, and maintainability of what they produce. Their work on a service is not done until the service is unused and fully retired.\n\nAn interesting difference he brings up between Google and eBay compared to Amazon is that the former two tech juggernauts appear to have something like an internal marketplace for services, where clients are charged (mostly in a fictional, internal currency) for their usage of other teams’ service resources. This often serves as an incentive to be more mindful of potential performance issues that a client’s usage can cause, leading to greater cross-company efficiency.\n\nAnother well-known company to bet on microservices early is Netflix. Adrian Cockcroft, in his London QCon talk,5 offers many insights into Netflix’ migration to microservices in the public cloud. Netflix is one of Amazon Web Services’ largest customers, and much of the tooling they developed in-house has been open- sourced under the moniker of NetflixOSS.\n\nMuch as Shoup reports from eBay and Google, Cockcroft describes that a service at Netflix is not done once the code is ready and deployed, but rather when the service is finished being useful, and hence can be retired.\n\nHe also delivers other valuable organizational insights about the Netflix internal structure. For example,\n\nhe explains that at Netflix, decisions are made first about what a particular service architecture should look like, and only then are team structures built out to best match an ownership structure around this architectural layout.\n\nCockcroft calls this approach the “Inverse of Conway’s Law.” He adds that much of why the resulting teams\n\nwork at a very high level of efficiency and creativity stems from the “high trust, low process” environment in which developers can operate. Efficiency there is greatly helped by the fact that all hardware is commoditized to such a degree that Netflix owns no data centers but relies entirely on cloud offerings of Amazon and (more recently) Google. This even holds true for its development-stage environments, so that developers will never have to run more service applications than the one they are currently working on modifying.\n\n5Adrian Cockcroft, “Migrating to Microservices,” QCon London. March 6, 2014. www.infoq.com/presentations/ migration-cloud-native.\n\n7\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nThe trust invested in developers also comes with a price: much like Amazon, Netflix lets its software engineers “run what they wrote.” Every developer has production service root-level access privileges, but they also all participate in operational support and carry pagers for being on call, in case any operational issues arise.\n\nCockcroft’s talk also addresses some practical design choices that appear to have crystallized over the\n\ncourse of Netflix’ experience with implementing a microservices architecture. There appears to be no actual limit imposed on the number of services they run. They instead use rule-of-thumb guidelines to decide when to spin up a new service. One such guideline is that more mature code should be kept separate from innovative or unproven features.\n\nAnother rule about service granularity appears to be that each “verb” ( functionality) should be factored\n\ninto a separate service. Cockcroft quotes the example of a service whose sole purpose it is to use a given customer ID and movie ID as inputs to return a list of other movie IDs that should be recommended to the customer based on these inputs.\n\nThe Netflix approach to service versioning is to run several instances of the same service\n\nsimultaneously, but with distinct versions. Instances of older versions will only be shut down once all clients have been convinced that they need to migrate off such an existing, older version.\n\nOther defaults chosen at Netflix are to make service interactions non-blocking, using an Observable Set / Reactive model for communications. Netflix even open-sources its internal tool for this purpose, called RxJava (https://github.com/ReactiveX/RxJava). Use of a message bus at Netflix seems to be restricted to “fire-and-forget” messaging, where no response is needed for the sending client. Cockcroft cites the example of a “delete this from all caches” message that one service can send on a reliable message bus topic, for other applications to consume and act on appropriately.\n\nRegarding decisions about data storage implications of the CAP theorem, the guidelines at Netflix appear to favor “available when partitioned” over consistency, unless having data availability downtime has explicitly been mentioned as an acceptable trade-off in a service’s nonfunctional requirements.\n\nIn a May 2015 blog post,6 Martin Fowler coined the term microservices premium to describe the measures that an organization needs to take to address the added complexity introduced by distributing functionality across multiple services.\n\nNetflix has certainly paid this premium by building, and additionally open-sourcing, many of its\n\ninternal tools for automating and simplifying the additional overhead incurred by employing a large service- oriented architecture. Netflix uses Karyon as a basis for all services to be developed, and Zuul as an edge service component that fronts all of its applications. Edda serves as a change-tracking tool for Netflix’ AWS server configurations, and Eureka is a load-balancing AWS service registry. To further enable reliability and resilience, Netflix also developed Hystrix, a Java library to help placing circuit breakers7 around a service’s points of remote system access. For a full list of Netflix’ open-source tools, see https://netflix.github.io/.\n\nThe engineering team at SoundCloud (https://soundcloud.com/) made a similar move from a monolithic Ruby on Rails application to microservices written in Clojure, Scala, and Ruby. Phil Calçado, formerly director of engineering for SoundCloud’s core team, described their experiences in another August 2015 QCon talk.8\n\nThe first feature that was pulled out of the monolith was search, but even at this very early stage they started wondering whether they were ready for the paradigm shift to microservices, which he calls a “microservices explosion,” where a huge amount of new services were added within a matter of weeks.\n\nOne way they prepared was to hone their provisioning process. SoundCloud runs its own data-centers, but started mimicking the structure of Heroku (https://www.heroku.com/), using the same toolchain that Heroku is built on. As a result of running on a physical data center, SoundCloud encountered issues resulting from the nonoptimal resource sharing of co-hosted applications, paired with “naive scheduling.”\n\n6Martin Fowler, “Microservice Premium.” May 13, 2015. http://martinfowler.com/bliki/MicroservicePremium.html. 7Martin Fowler, “CircuitBreaker” March 6, 2014 . http://martinfowler.com/bliki/CircuitBreaker.html. 8Phil Calçado, “No Free Lunch, Indeed: Three Years of Microservices at SoundCloud,” QCon. August 2015. www.infoq. com/presentations/soundcloud-microservices.\n\n8\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nOther choices they made during the migration to an SOA were to move applications written in Ruby,\n\nNode.js, and Go over to JVM-based languages like Clojure, Scala, and JRuby.\n\nSoundCloud started employing Tumblr’s Collins tool (https://tumblr.github.io/collins/) for automating their rapid provisioning process, packaging all their microservices into Docker (https://www. docker.com/) containers.\n\nSoundCloud’s telemetry approach was also home-built, and resulted in a tool called Prometheus\n\n(http://prometheus.io/), which had integrated alarm facilities via PagerDuty (https://www. pagerduty.com/). For operational support, SoundCloud made sure that all applications have uniformity of administration endpoint URL schemes, so that on-call engineers have an easy time determining, or modifying, application status.\n\nSoundCloud also made sure to integrate request tracing into their API infrastructure, so that the\n\ncomplete graph of service dependencies can be determined and visualized more easily.\n\nTo automate deployment, SoundCloud adopted a continuous integration approach, combined with continuous deployment directly to Docker containers that are pushed to production servers in SoundCloud’s data centers. SoundCloud’s main challenge was to iteratively adopt an SOA and its necessary preconditions.\n\nWhy Use Microservices? The list of reasons for employing microservices is long, and before summarizing the main advantages from our own viewpoint, we will first summarize the reasons each of the companies and consultants mentioned earlier have given.\n\nAdvantages as Seen by Industry Experts Martin Fowler’s “Microservice Trade-Offs” article9 distils his view of the advantages down to three points:\n\n\n\nStrong Module Boundaries: In his view, designing and implementing any piece of software can benefit from applying a modular structure. Divvying up your business’ IT architecture into microservices reinforces such modular structures, which become increasingly important as a company’s development team grows. Organizations can form tight-knit teams around the ownership structure of a set of small services, which leads to clearer interfaces with stronger boundaries, not just between the teams, but as a consequence also between the software (sub) systems these teams are building. Decoupling subsystems in a monolith is of course also possible, but it is rarely done, and even less often upheld for any length of time, because it is all too easy to violate such module boundaries in a monolith. In Fowler’s words, violating subsystems boundaries “… can be a useful tactical shortcut to getting features built quickly, but done widely they undermine the modular structure and trash the team’s productivity. Putting the modules into separate services makes the boundaries firmer, making it much harder to find these cancerous workarounds.” Another advantageous side effect of erecting service- based boundaries is that, if done right, the data storage and access should also be decentralized and become entirely opaque. All access to data will be at the “service model” layer, where the actual persistence structure is hidden behind the services’ exposed APIs.\n\n9Martin Fowler, “Microservice Trade-Offs,” July 1, 2015. http://martinfowler.com/articles/microservice- trade-offs.html.\n\n9\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\n\n\nIndependent Deployment: Fowler praises the benefits of a Continuous Delivery approach to software development.10 Where releasing software used to be a “once a quarter,” or—at best—a monthly event, many teams using microservices have trimmed down their release cycles to happen several times per day. The main competitive advantage that results from the ability to deploy at a high frequency comes from the increased speed of responding to the market, or offering a new feature faster than your competitors. Deploying large monoliths makes such a deployment frequency very hard to maintain, as any botched change to such large code bases can mean that the entire deployment will fail. If, on the other hand, microservices are designed to be autonomous, and hence independently deployable, components, then the risk of a complete system failure due to a deployment of any of its (mircroservice-encapsulated) parts is greatly reduced.\n\n\n\nTechnology diversity: In Fowler’s view, the main advantages of this point are not only the more obvious greater ease to “choose an appropriate tool for the job”: it is evident that, in a microservices environment, you can more easily introduce, and experiment with, more diversity in programming languages, frameworks, and data stores. Another often overlooked advantage is the ability to take a more fine-grained, less all-or-nothing approach to upgrading libraries and other common dependencies. As Fowler puts it: “Dealing with library versioning issues is one of those problems that gets exponentially harder as the code base gets bigger.”\n\nWerner Vogels’ arguments for why Amazon switched to what today would be called a microservices\n\narchitecture are as follows:\n\nDevelopers have a deeper feeling of software ownership and control, which leads to more innovation and focus on system stability and performance than was possible when all code lived in a single monolith.\n\n\n\nA fine-grained, service-oriented architecture gives Amazon a “level of isolation” between each of its pieces of software that allows the company to build its components independently, and hence more quickly than before.\n\nNew ideas can more easily be implemented, launched, tried out, and potentially discarded, both at a developer level and in operational support.\n\nWhile the number and (lack of) structure of microservices employed at Amazon may appear chaotic when viewed from close up, they make sense at a larger scale. They are simply an acknowledgement of the fact that large-scale distributed systems possess “many organic and emerging,” and hence nondeterministic, properties. These nondeterministic properties are not seen as problematic at Amazon, but rather as a fact of life that helps their innovation cycles.\n\n\n\nAmazon now considers it a competitive and strategic advantage to be able to build large, complex applications via composition of “primitive” services. Vogels claims that Amazon can scale its “operation independently, maintain unparalleled system availability, and introduce new services quickly without the need for massive reconfiguration.”\n\n10Martin Fowler, “Continuous Delivery,” May 31, 2013. http://martinfowler.com/bliki/ContinuousDelivery.html.\n\n10\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nUsing a microservices architecture also provides organizational advantages. Teams at Amazon are created to form around a set of services they build and own cradle to grave, making the owners “completely responsible for the service—from scoping out the functionality, to architecting it, to building it, and operating it.” Amazon employs a “you build it, you run it” approach to software development, exposing developers to the needs of production support for all the software they launch, as well as to the customers (be they internal to Amazon or external) that rely on the functionalities provided by the respective service. The resulting customer feedback loop is a key element for improving the service’s “quality of the service.”\n\nRandy Shoup quotes many points that he sees as disadvantages of running a business on top of a monolith:\n\n\n\nAs a company’s software development team grows, coordination of the developers becomes increasingly challenging. Developers will start to step onto each other’s feet, and become much less productive as build-times grow with the size of the application.\n\nWhile it’s not impossible, it is much harder to enforce well-modularized software design in a monolith, where all code eventually has direct access to all resources without having to pass through a layer of abstraction. On the other hand, dividing a system into separate services with well-defined interfaces and resources that are owned exclusively by such services is a very potent facilitator for enforcing modularity.\n\nMonoliths leave very little room for adaptable scaling strategies. If all code lives in a single, large application, then the only way to add performance is via vertically scaling the resources providing the application. While it is again possible, breaking up a single, underlying data store into well-modularized units takes much more discipline in a monolithic system. The database easily becomes a point of strong coupling between the various parts of the application, and tuning a database for all the different use cases quickly becomes impossible.\n\nHaving a single unit of deployment for your system makes every change, however minute, carry an enormous risk for your business, as the entire application can stop functioning because of a single botched code change. Additionally, times for deploying, and potentially reverting, code to the production environment have the potential to grow to a point where they become a burden and productivity impediment for the entire business.\n\n\n\nShoup similarly and inversely lists the advantages of adopting microservices as follows:\n\n\n\nEach single service making up the application is a much smaller piece of the entire system to understand, and to reason about.\n\n\n\nServices become independently deployable, thereby reducing risk and speed of deployments. Additionally, each of the services becomes testable in isolation.\n\n\n\nScaling services individually allows for more options to choose from when the need to performance-tune the application arises.\n\n\n\nLarge-scale organizations like Google and Amazon, which are trying very hard to optimize for development velocity, have many projects moving in parallel, with little to no coordination overhead. Organizing their teams and business capabilities around small services enables this desire to move faster than many of their competitors of a similar scale.\n\n11\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nAdrian Cockcroft has three main arguments for microservices:\n\n\n\nThe removal of much friction from product development cycles increases the speed of the business as a whole.\n\n\n\nAs a staunch proponent of cloud-based microservices architectures, Cockcroft points out that adding horizontal scaling resources can be completed extremely quickly; when done right, scaling up in the cloud can be done within a matter of minutes.\n\n\n\nSpeed and availability of the overall system are improved, while at the same time cost, size, and risk of software development to the business are reduced. One enabler is the much shortened feedback loop for code that is deployed via small, isolated services, which means the overall rate of change to the system can be increased relatively safely. As Cockcroft puts it, the “Observe, Orient, Develop, Act” loop is shortened.\n\nAccording to Phil Calçado’s blog-post about SoundCloud’s move to microservices, its motivation “had\n\nto do much more with productivity than pure technical matters.”\n\nWhile implementing the “Next SoundCloud” project on top of an existing Ruby on Rails monolith\n\n(lovingly dubbed “the mothership”), they found themselves mostly in scaling issues in which their team processes posed the largest bottleneck to their velocity. Changes to their monolithic code base were so risky, and had caused so many large-scale outages, that they introduced a process of mandatory, intensive code reviews for pull requests. Reviewing such changes took an expert in the existing code base often several hours of their time. In Calçado’s words: “Larger changes required a lot of time to be reviewed and, due to the spaghetti nature of our Rails code, were very risky. People avoided these big Pull Requests like the plague.”\n\nAnd even after such intensive reviews, there remained a significant risk of causing unforeseen negative\n\nside effects. To quote Calçado again: “Turns out that the monolithic code base we had was so massive and so broad no one knew all of it. People had developed their own areas of expertise and custodianship around submodules of the application.”\n\nSoundCloud found that their switch to smaller, independent deployment units, organized as services\n\nwith specific focus on a small set of functionalities, helped them overcome their process scaling issues, as well. They built teams with true system ownership around these smaller services: “These teams were explicitly told that they had full ownership over the modules they owned. This meant that they would be on call for any outages related to those, but would also have the freedom to evolve these in any shape they thought reasonable. If they decided to keep something in the monolith, it would be their call. They were the ones maintaining that code anyway.”\n\nOur Reasons to Adopt a Microservices Architecture Our personal experiences with the advantages of microservices are very close to what many of the experts we’ve seen cite as reasons to employ them.\n\nWhen done right, services have much smaller code bases than monolithic applications. They are therefore easier to maintain, as they are easier to read, reason about, and to independently maintain, upgrade, and evolve.\n\nUsing services can be compared to employing the “single responsibility principle” of object-oriented\n\ndesign on an application design level. Services implement a self-contained, well-defined and documented set of functionalities, which they expose only via versioned APIs.\n\nThey can more easily be designed to be a true system of record for all data they access from a data\n\nstore (such as an underlying database). No other service or application has direct read or write access to the underlying data store. This enforces true decoupling, while at the same time achieving a high level of transparency of the implementation details of data storage from information access.\n\n12\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nIn most cases, the use of services also makes it easier to scale your application. For one, you can hone the design of your services to be scaled by their traffic pattern: read-intensive API endpoints can be cached independently from write-intensive functionality. You can also more naturally separate cache expiry based on the tolerance for staleness of information based on your individual endpoints, or even by particular clients requesting information (for example, information about the descriptive text on any given inventory item should in most cases have a higher tolerance for staleness than the quantity of the item left to be purchased.)\n\nIn Chapters 6 and 7 of this book, we will show how to implement such caching strategies client-side, as well as taking advantage of the HTTP protocol’s conditional GET facilities service-side. You will see how this can be paired with the use of reverse proxies, precalculating results service-side, as well as employing dedicated database read-only replicas for resource-intensive queries.\n\nPotential Pitfalls and How to Avoid Them It would be irresponsible and almost reckless to pretend that microservices are a silver bullet without any potential challenges or trade-offs. This section will try to summarize the trade-offs and concerns mentioned by the experts we have previously quoted, and it will conclude with our own views on impediments to introducing microservices.\n\nMartin Fowler is the expert who has been most explicit in discussing some of the trade-offsa that come\n\nwith the use of microservices:\n\nDistribution: Distributed systems are harder to program, since remote calls are slow and are always at risk of failure.\n\n\n\nEventual Consistency: Maintaining strong consistency is extremely difficult for a distributed system, which means everyone has to manage eventual consistency.\n\nOperational Complexity: You need a mature operations team to manage lots of services, which are being redeployed regularly.\n\nIn addition to these technological considerations, Fowler and his colleague James Lewis agree that there are also many political factors in a company’s surrounding culture and environment that can affect the chances of a successful microservices adoption. We will later explain our own experience and the cultural landscape that is needed for such an approach to thrive.\n\nPhil Calçado agrees with Fowler and Lewis on many points. In his August 2015 QCon talk, cited in the\n\npreceding section, he emphasizes the need for any organization using microservices to have rapid resource- provisioning, basic monitoring and telemetry, and rapid deployment in place.\n\nParticularly important to him is the monitoring and alarming component. In his words: “Tracking down\n\nissues in a monolith is easier … it’s in some component of the monolith.” When trying to find the root cause of an issue in a distributed environment, it is much harder for any operational support personnel to figure out where in the entire system a problem that has arisen is rooted. Calçado suggests that this be addressed by unifying dashboards for the monitoring data of all services and apps involved, to more easily identify which services were acting up.\n\nAnother hurdle is to aid application developers during their implementation and test efforts.\n\nDevelopment becomes challenging and cumbersome in an environment where the application you work on is never run by itself, because it has dependencies on many other services. SoundCloud decided to address this by having all applications ship as Docker images that could be installed on a single server, or even a developer’s laptop.\n\naMartin Fowler, “Microservice Trade-Offs”, July 1, 2015, http://martinfowler.com/articles/microservice- trade-offs.html.\n\n13",
      "page_number": 25
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 33-40)",
      "start_page": 33,
      "end_page": 40,
      "detection_method": "topic_boundary",
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nIn his “Microservice Prerequisites” article,11 Fowler also lists ways in which a mature organization can address the potential drawbacks via good planning. He sees three main conditions that need to exist in any successful implementation of a microservices architecture:\n\n\n\nRapid provisioning: while he thinks that companies can gradually approach a full automation of such expedited server provisioning, he is convinced that any organization that is serious about microservices adoption will need to get to a state where new servers can be fired up in a matter of hours. Hosting such servers in a (private or public) cloud environment will make this more natural in his view, but he says that this can also be achieved without a full switch to cloud computing.\n\n\n\nBasic Monitoring: Because fully simulating or testing an environment where often hundreds of independent service components interact is very difficult, he considers it “essential” to have monitoring and alarming in place to quickly be notified about any issues that impact the production environment. Such monitoring should not be restricted to technical problems (like errors or drops in availability or performance), but should extend to business metrics (such as order drops, revenue decrease, or click-through rates).\n\n\n\nRapid application deployment: when the number of services in an organization increases significantly, the need for very short deployment (and rollback) cycles arises, both for the test, as well as for the production stages. This usually involves automated, or even continuous, deployments that can change the code running in any particular stage in less than an hour. Again, Fowler concedes that some initial adoption phases can tolerate manual processes for application deployments, but he also says that full automation should be a near-term goal.\n\nFowler makes the point that especially monitoring and fast deployments go hand in hand:\n\n“This collaboration is needed to ensure that provisioning and deployment can be done rapidly, it's also important to ensure you can react quickly when your monitoring indicates a problem. In particular any incident management needs to involve the development team and operations, both in fixing the immediate problem and the root-cause analysis to ensure the underlying problems are fixed. … There’s also the shift to product centered teams that needs to be started. You’ll need to organize your development environment so developers can easily swap between multiple repositories, libraries, and languages.\"\n\nRandy Shoup’s talk mentions a good list of pitfalls of microservices, and he also adds occasional tips on\n\nhow to avoid them.\n\nMuch like Fowler, he explains that while each small service itself becomes simpler and more intuitive to understand, coordinating the simple parts becomes complex, and understanding the whole distributed system and its information flows becomes more complex than in a monolith. Additionally, the introduction of network latencies between the services needs to be considered, and addressed by the development processes and standards employed in an organization.\n\nA switch to microservices also often results in the active use of hundreds of code repositories. That number is large enough to justify spending time on devising a strategy for managing them in a unified way; for example, to address refactoring concerns across such a large number of repositories.\n\n11Martin Fowler, “Microservice Prerequisites,” August 29, 2014. http://martinfowler.com/bliki/ MicroservicePrerequisites.html.\n\n14\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nAs the many successful examples of microservices adoption prove, the problems we’ve just seen can certainly be managed, but companies will need more sophisticated tooling to address the challenges than was previously needed in a monolithic environment.\n\nShoup continues by defining what he calls “microservices anti-patterns\":\n\n\n\nThe Mega-Service: This is an instance of failure to adopt the right granularity for a service extraction. An example of this anti-pattern is a service application that hosts too much functionality, or too many areas of responsibility, to maintain the advantage of a code base that can be easily understood. But a service that has too many up- or downstream dependencies can also be seen as an instance of this anti- pattern, as it counteracts the advantage of greater stability and availability of the overall system architecture.\n\n\n\nShared Persistence: Having several services or applications share the same data store “under the hood” breaks the isolation advantages that switching to a loosely coupled collaboration of small services promises to bring. Even if the external interfaces of these coupled services are distinct, tying them back together at the database level leads to near-invisible coupling.\n\n\n\nLeaky Abstraction Service: In a well-designed service, its interface needs to reflect the client’s needs, so the API model exposed should cater to its customers’ needs. Implementations of the leaky abstraction service anti-pattern, on the other hand, expose an API design “from the internals outward,” thereby leaking the underlying persistence model of the API provider’s constraints instead.\n\nAnother concern Shoup raises is that in his experience, companies that have not reached sufficient size will not benefit from a microservices architecture. In his view, there are certain “stable points” in the size of companies as they evolve. The first stable point is when all employees still “fit around a conference table.” The next stable size point is twenty to twenty-five people on the payroll, and the step after that will be when the number of employees reaches around 100 people. Shoup’s advice is to switch to microservices when the second stage (of 20–25 people) is reached.\n\nShoup lays out his rationale for this advice in an interview with InfoQ:\n\n“Google and Amazon are very intentionally trying to optimize for velocity of large-scale organizations, which means lots of things moving in parallel with little coordination. They behave like 1,000 tiny companies rather than one monster company and that is why those companies versus a bunch of even larger organizations, move fast and the other one do not. But in the case of smaller organizations, if you are all one team, do not subdivide.\"\n\nAdrian Cockcroft’s “Migrating to Microservices” QCon talk does not reveal much information about his concerns regarding any potential pitfalls with microservices adoption. The main take-away from his talk appears to be that he whole-heartedly supports commoditization of server resources in the cloud, as opposed to owning and maintaining your own data centers. To summarize his advice in his own catch phrase: “Don’t be a server hugger!”\n\nWerner Vogels’ interview also only touches on a very few points that lay out the challenges of adopting a\n\nlarge-scale SOA:\n\nUnified service access: Vogels makes the case for a single, commonly adopted service access protocol. It is Amazon.com’s experience that such a mechanism is a precondition to easily aggregate services, and to enable monitoring, request tracking, and other more advanced “infrastructure techniques.” Amazon implements this in its home-grown ASDL standard.\n\n15\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nDeveloper productivity: A fair amount of up-front thought should be invested to ensure efficiency of feature development in a large SOA architecture. Amazon addresses this by making sure that developers only ever need to run locally the code bases for the particular services they are currently trying to modify or extend. All other system dependencies run in a development-stage server fabric, which is also maintained and supported 24/7.\n\nQuality assurance: integration testing in a widely distributed environment becomes a large-scale challenge for an organization’s quality assurance measures. To quote Vogels:\n\n“If you have this decentralized organization where everybody is developing things in parallel, how can you make sure that all the pieces work together as intended, now and in the future? An important part of that is, for example, testing. How do you test in an environment like Amazon? Do we build another Amazon.test somewhere, which has the same number of machines, the same number of data centers, the same number of customers, and the same data sets? These are very hard questions, and we have no simple answers. Testing in a very large-scale distributed setting is a major challenge.\"\n\nOur own experience from working at Amazon.com in the years after Vogels’ interview was that the company has heavily invested in doing just what Vogels alludes to above: production-like server fabrics have been spun up and populated with realistic test data, which a sizeable QA team uses to verify (automatically, as well as manually) larger software changes before they are deployed to the production stage.\n\nOur own views on pitfalls to avoid, and what measures to implement to counteract them, overlap with\n\nmany of the experts quoted in this chapter. Martin Fowler’s and Phil Calçado’s points about the need for monitoring and alarming approaches are particularly necessary, based on our own extensive experience in working in SOA environments.\n\nIt is also important to realize that working in a truly distributed environment introduces a large number of new failure modes, which include but are not limited to handling dependency timeouts or downtime. The need to avoid cascading failures of any system, which can be easily caused by any of the dependencies being unavailable or slow, must be engrained into every developer’s workflow and thinking.\n\nAnother absolutely essential point for us was touched on by Werner Vogels: the need to unburden the individual developer of the complexity and inefficiency involved in running the entire system locally. Most changes are to be made only to a small portion (usually, a single service component) at any given time, so requiring any developer to set up, provision, or run any more than just the component(s) under change means continual grief and friction during the development cycle.\n\nWe do, however, explicitly disagree with Randy Shoup and others in that we do not think that\n\nmicroservices requires a certain level of company or development team size; after all, this book is dedicated to adopting the approach that it is not just possible, but actually preferable, to start out with a microservices architecture.\n\nRandy’s description of “microservices anti-patterns” hits the nail on the head, though. In our own experience, the most harmful of these is the “Shared Persistence” anti-pattern. Applying this defeats the entire purpose of splitting large applications into smaller decoupled parts, precisely because the resulting parts are no longer truly decoupled: they are tied together by having to adhere to the same underlying database schemas and—potentially even worse—shared data access model logic.\n\n16\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\nThe second most dangerous anti-pattern to employ is Shoup’s “mega-service.” Service repositories of that kind are often born out of an irrational fear (or lazyness) of being potentially “slowed down” by spinning up separate code repositories and deployment units. This approach defeats the fundamental advantages of applying a “single responsibility principle” at a service application level, robbing the implementers of the chance to build a self-contained, easy-to-understand, and inexpensive-to-maintain service code base.\n\nThe rest of this book will present practical tips and examples for handling these measures.\n\nWhen Should I Start with a Microservices Architecture? You should not be surprised to find that we strongly recommend employing microservices from a very early stage in your application: after all, you have bought a book entitled Microservices from Day One.\n\nIf you are looking for more information about why other practitioners advocate the approach of starting\n\na project with a monolith before deciding whether to switch to microservices, we recommend Martin Fowler’s “Monolith First” blog post.12\n\nAlso posted on Fowler’s treasure trove of a site is an article by Stefan Tilkov that takes the exact\n\nopposite stance, titled “Don’t start with a monolith.”13 The following points from this article very accurately summarize our own experiences and the reasons we recommend a “services first” approach:\n\n\n\nTilkov states that the point at which you start building a new system is also exactly the point in time when good design practices demand that you think about “carving it up into pieces,” in order to build them as decoupled from each other as is possible. Postponing such system modularization to any time later in the lifecycle of a system should be strongly discouraged, because “you make it hard—or at least harder—to do the wrong thing: Namely, connecting parts that shouldn’t be connected, and coupling those that need to be connected too tightly.\"\n\n\n\nTilkov’s experience that it is a challenging task , and sometimes well-nigh impossible, to divvy up an existing monolith into coherent modules entirely matches our own experience. When you begin developing in a monolith, each part will access shared domain logic and shared data access model, and assume that database transactions are always a possibility. This leads to very tight coupling inside the application. He mentions that owners of a monolith are “tempted to assume there are a number of nicely separated microservices hiding in your monolith, just waiting to be extracted. In reality, though, it’s extremely hard to avoid creating lots of connections, planned and unplanned. In fact the whole point of the microservices approach is to make it hard to create something like this.”\n\nHe concedes the “monolith-first” camp’s argument that one needs to know the problem domain very well before being able to correctly break it down into subdomains small enough constitute functionalities for microservices. The main reason is the difficulty of making significant changes in the system composition. As he puts it: “Refactoring in the small becomes easier, refactoring in the large becomes much harder.”\n\n\n\nTo him, “the ideal scenario is one where you’re building a second version of an existing system.” This observation also coincides with our own experience: we recommend building a proof-of-concept first to understand the business domain well enough. But, as any proof-of-concept, such a prototype needs to be entirely considered a throw away effort, and not “production-worthy.” The second system can, and should, be built with services in mind.\n\n12Martin Fowler, “MonolithFirst,” June 2, 2015. http://martinfowler.com/bliki/MonolithFirst.html. 13Stefan Tilkov, “Don’t start with a monolith,” June 9, 2015. http://martinfowler.com/articles/dont-start- monolith.html.\n\n17\n\nChapter 1 ■ MiCroserviCes: the What and the Why\n\n\n\nTilkov also argues the counter-intuitive point that developing in more than one code base actually increases efficiency and the speed of tangible results. His point is that microservices are an enabler for parallel development, because they establish “a hard-to-cross boundary between different parts of your system,” and hence are a forcing function for developers to discuss interfaces up-front. The parts that are being developed in parallel can also be delivered independently from each other, so that the feedback loop is shortened.\n\nMoreover, Tilkov states that concentrating on a microservices-based approach, where each subsystem “with its own development, deployment, and delivery cycle, and (the possibility of) its own internal architecture, is a very powerful concept that can help you to deliver a system in the first place.”\n\nThe main point of this book is to show that adopting a microservices-based approach from the\n\nbeginning does not have to be much more expensive than just going the ordinary (monolithic) “one app to rule them all” way.\n\nIn our view, the investment is very comparable to the “write tests or not?” question: investing in repeatable, automated pre-launch testing will pay off almost immediately by sparing companies the cost of addressing bugs found post-release.\n\nSimilarly, starting out with small, more focused services in the early phases of your applications life cycle will prevent you from involving very large parts of your engineering resources in ripping apart and rewriting your platform in a year from now, or so.\n\nAnother advantage to starting out with microservices is that an organization will be forced very early in a system’s lifecycle to address the infrastructure concerns that are prerequisites to succeeding with such an architecture. Making an investment in development, testing, continuous integration and deployment, as well as monitoring and alarming strategies in the early phases will make the total amount of time and resources invested smaller, and the organization will reap the efficiency benefits of these measures much earlier.\n\nSummary In this chapter we laid out what services are and what a service-oriented architecture (SOA) is. We discussed what microservices are, and also what specifically they are not. We looked at four technology industry juggernauts (Amazon, Netflix, Google, and eBay), and discussed their respective reasons for relying on microservices, as well as the advantages such adoption brings in our view.\n\nWe moved on to describing some of the potential pitfalls that result from distributing systems at a large\n\nscale, and suggested remedies, again based on what industry leaders suggest, and what our experience tells us. We closed the chapter with an appeal to start using microservices from the earliest stages of your application development, having mid- to long-term efficiency and effectiveness in mind.\n\nThe next chapter will introduce some ground rules and best practices that are going to make it easier for\n\nyou to get to such an architectural style once you are ready to adopt it.\n\nThen, in part II of this book, we will look at ways and guidelines around decomposing your business application into microservices, and how to design, define, consume, and finally optimize your services’ APIs.\n\nThe third and final part of this book will focus on the development environment, deployment\n\ninfrastructure, testing strategies and monitoring & alarming aspects of running a microservices architecture. All parts will be tied together by a set of example services implementing a fictitious online bookstore.\n\n18\n\nCHAPTER 2\n\nRules of Engagement\n\nIn this chapter, we’ll go over what we call our “Rules of Engagement,” which is a set of principles and best practices we find valuable to follow to ensure an efficient microservices environment. It would be correct to state that this entire book is a collection of what we consider best practices; but the principles we outline in this chapter are a foundation of many of the tools and design ideas we will discuss throughout the book. We have six Rules of Engagement for successfully building applications in a microservices environment. Those rules are based on our years of experience, and are phrased from a developer’s point-of-view.\n\nAs with most rules, there may be cases where you will have a very good reason to move away from them, which is completely acceptable; however, make sure the set of principles that you—and your organization— believe in are easily accessible to your technical team when questions about software architecture arise. We believe that guiding principles like these help your development team spend less time thinking about high- level architecture, and more time writing business-related code and providing value to stakeholders. Our rules of engagement follow in order of importance, with the most important ones coming first.\n\nCustomer-Facing Applications Cannot Directly Touch Any Data Store “Consumer-facing applications” are interfaces used by an organization’s customers. They are usually but not always web sites, administrative interfaces, and native mobile applications; there could even be public- facing APIs that you expose. Consumer-facing applications will be a mere mashup of data retrieved from authoritative systems-of-record, and will never have a database or any other mechanism that persists state. Such systems-of-record will always be service applications with well-defined, versionable interfaces. Apart from the fact that some nasty security issues will be easier to address this way (SQL injection should be a thing of the past), different consumer-facing apps that work on the same information exposed by the owning service will be able to evolve independently, regardless of changes in the underlying data schemas of your data store.\n\nAn important side effect of this rule is that all the features in consumer-facing applications are\n\nimplemented via service calls to microservices.\n\nThis is a rule that if ever broken will take you on a slippery slope you will regret for a really long time. We’ve seen many features being implemented with direct database access “because it’s quicker,” but quickly become a nightmare to evolve because of tight coupling from one client. Do not believe it when someone—possibly you—says that “this will be extracted it into a service later”; you may have good intentions, but shifting business needs may mean that you will not be able to follow up on fixing your application’s design later.\n\n19\n\nChapter 2 ■ rules of engagement\n\nNo Service Accesses Another Service’s Data Store In 2002, Jeff Bezos sent out an email at Amazon saying that “all teams will henceforth expose their data and functionality through service interfaces” and “there will be no other form of inter-process communication allowed,” and that’s how Amazon adopted and built an extremely efficient service-oriented architecture. This principle is mainly about implementation encapsulation; however, we mention data stores in our version because that’s where we’ve quite often seen this rule being broken, causing all sorts of implementation leakage that ends up slowing down service evolution.\n\nSimilarly, all inter-service interactions should happen only through well defined, versioned APIs. While a service has ownership of the data for which it is itself the system-of-record (including direct read and write access), it can only access other information via its dependent authoritative services.\n\nInvest in Making Spinning Up New Services Trivial This point, combined with the preceding rule, is very important; it’s essential to have both your developers and your TechOps teams embrace the “SOA way” inside your company. Driven, smart people tend not to tolerate being seemingly held back, and from a business viewpoint, they are too expensive an asset to be slowed down.\n\nMake everyone happy by having a small accepted list of technologies to support. Invest in creating\n\ntemplates for service code. And, last but not least, introduce rules and guidelines for your interfaces: consider the use of an Interface Definition Language to define APIs, and think about the structure of your RESTful interfaces (what consistent error status codes will you expose, what is in the headers and what is in URI parameters, how will authentication / authorization work? and similar questions).\n\nThese rules stem from more than a decade of combined personal experience (both from the ground up,\n\nas well as via refactoring existing monoliths). With a growing number of deployable units (that is, services); it’s important to be serious about automation (DevOps) —of all aspects of your infrastructure. In a perfect scenario, most code changes would kick off a build that runs a suite of automated tests. When a successful build happens, the change should be deployed to a shared development environment, and then later deployed to production without manual intervention in each step of the way.\n\nWhen a developer—or development team—decides that a new service is required, a lot of steps are\n\nneeded to get that new service live. Some of those steps are:\n\nDefining the language stack to be used based on requirements\n\nCreating a repository\n\nDefining a data-store\n\n\n\nSetting up continuous integration,\n\nDefining deployment\n\n\n\ndeploying To development/staging/production\n\n\n\nSetting up monitoring and logging\n\n... and so on. If your company has an environment where most of those steps are automated, then\n\ncreating one—or multiple—services doesn’t incur any sort of time penalty from the point of view of development, and you help your team keep code that needs to be separated in different services. By the same token, if spinning up a new service requires “asking for permission” and takes a few weeks, then it’s likely that your development team will bunch APIs into existing services to avoid those delays in getting services up and running.\n\n20\n\nChapter 2 ■ rules of engagement\n\nWhen a developer shoehorns new APIs into an existing service just because it’s less friction than\n\nstarting a new service, he or she ends up breaking the single-responsibility concern that should be applied to microservices, which can lead to confusion and disrupt a focused system into becoming a set of seemingly unrelated APIs. The famous “broken windows theory” well fits this situation. It becomes hard to keep proper separation of concern when one API that is a bad fit goes into a service; it will undoubtedly invite similar behavior from other developers.\n\nBuild Services in a Consistent Way All services you build should behave consistently in many aspects. In the perfect scenarios, all your services will:\n\nCommunicate using the same protocol and format—for example, JSON over HTTP\n\nDefine APIs in a consistent manner\n\n\n\nLog request access using a standard log format\n\n\n\nBe monitored with the same tools\n\n\n\n...\n\nThe keyword of this rule is “consistency; surprises are evil and slow everyone down. This rule doesn’t\n\nmean that you put the brakes on innovation and don’t try new languages or libraries. You should always be assessing new technologies and practices, but in an organized way. It’s well known that, in the software development lifecycle, software spends most of its time in the operations and maintenance phase, and you really want to avoid snowflake services that require very specific knowledge for maintenance because some technologies were tested with no process for either applying those technologies broadly or retiring them.\n\nDefine API Contracts in Code An API is generally defined by its inputs and outputs, which should ideally be described using an artifact that computers and people—other developers—can parse. We really mean, at any point in time, the API definition must be clearly defined, and clients of that API can validate and test it unambiguously. In practice, a mechanism known to achieve this is an IDL, Interface Definition Language, a term for a language that allows developers to define the characteristics of an interface—in our case API endpoints. Using IDLs, developers can programmatically define accepted input and output parameters—including data types—for an interface.\n\nAll your services will expose and consume data via APIs; so documenting them is a necessity. At every\n\npoint in time, there should be a clear way of describing your endpoints in detail. You need to find out the expected inputs and outputs of each endpoint, and having those items in code that is exercised often is a way of making sure that you really know what is being exposed. In this book’s sample project, we will be using Swagger, a specification used to define RESTful APIs, and you will see some of the benefits of using an IDL, including the usage of tools built around standards, which help to increase developer productivity.\n\nWe’ll go into a lot of detail about API development in Part II.\n\n21",
      "page_number": 33
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 41-48)",
      "start_page": 41,
      "end_page": 48,
      "detection_method": "topic_boundary",
      "content": "Chapter 2 ■ rules of engagement\n\nEvery Service Has at Least a Development and a Production Instance When developing in SOA, you cannot expect your development and QA teams to run all the various environments for all the services to be deployed, on their local development machines, and on any host for that matter. Not only will this slow down their computers (and hence negatively impact their velocity), but nobody should be expected to update potentially several dozen service applications (and their test data) locally on a regular basis.\n\nNo developer should ever need to run, on their personal development environment, any system other than the one(s) that need modification in the context of their task. Dependent services should have standard, well-known URIs for server instances to which all consuming applications point by default. Ideally, you should use a service-discovery service to take away a lot of the pain involved in knowing how to connect to each service in your entire environment. We will show some techniques to achieve this in Part III.\n\nExisting Code Bases Most of the content of this book is geared toward green-field development, and it may sound too opinionated and hard to apply to existing code bases. Even if you work on an existing code base that doesn’t follow those principles, however, you may still be able to apply them selectively, in order to improve your existing architecture.\n\nThe rule of thumb for applying those principles to a suboptimal code base is to optimize your ROI. For example, if you have a couple of services that share the same database, it will probably be a very large effort to split tables into two separate databases; your existing code could even make it impossible because of joins and reliance on too many tables; so, you’d be better served by properly documenting your existing APIs. On a similar note, if you have a talented DevOps team, then your best improvement may be to invest in making it easy to spin up your next service. We recommend you analyze and use common sense to start making progress toward a more sane microservices environment.\n\nSummary In this chapter, we outlined our six rules of engagement for using microservices:\n\nCustomer-facing applications cannot directly touch any data store.\n\nNo service accesses another service’s data store.\n\n\n\nInvest in making spinning up new services trivial.\n\n\n\nBuild services in a consistent way.\n\nDefine API contracts in code.\n\n\n\nEvery service has at least a development and a production instance.\n\nWe hope all of these Rules of Engagement make sense at this point. They may seem quite opinionated\n\nand complex to follow now. If some of the rules seem blurry to you, that’s almost to be expected at this point. We will refer to them often in the rest of the book, and hope they will make more sense as we go deeper into each subject.\n\nIn Chapter 3, we will dive into how you can decompose your business model into services. We’ll define\n\nsome easy-to-follow rules for defining how to split parts of an application into services, and will use our sample project as an example to put those rules into practice.\n\n22\n\nPART II\n\nAPIs\n\nCHAPTER 3\n\nPartitioning Your Business into Services\n\nThis chapter presents some guidelines for composing a larger business application out of a set of microservices. We start by explaining some general criteria by which to partition a software system into a set of core services. We then explain how these core services can be composed into one or more layers of higher-level services, which eventually are the building blocks for the entire application. Finally, we will apply some of these generic principles to begin building our example bookstore application.\n\nGeneral Approaches for Application Decomposition and Aggregation This section offers some general guidelines for breaking down an entire application into services components. Many of the principles laid out in here are based on the excellent book Service-Oriented Design with Ruby and Rails by Paul Dix (Addison-Wesley Professional 2010), which uses a social feed reader application as an example for demonstrating these principles. In that application, users can subscribe to feeds, comment on articles in the feed, as well as vote them up or down, and follow other users’ feed commenting and voting activities.\n\nApplication Decomposition The first set of methods for breaking apart an application domain into a list of services has in common that they decompose the larger scope of the entire application into small, low-level services.\n\nDecomposing by Functionality\n\nThe first, and possibly most natural, principle to apply when splitting up a problem domain to be addressed with a larger software application is to analyze the functionalities it is meant to provide. Once you have identified the individual pieces of functionality the whole system is to provide, start by grouping them together into bundles that fit together logically.\n\nSome such general candidates for functionalities to factor out into a separate service can be email\n\nprocessing (sending or receiving), application resource monitoring, or connecting to and consuming external data sources (for example, scraping external sites for data, or collecting data via other site’s APIs).\n\n25\n\nChapter 3 ■ partitioning Your Business into serviCes\n\nDix’s book example of a social feed reader application shows that the logic function of “updating feeds”\n\nsticks out as a separable functionally, as its “only purpose is to download XML files, parse them, and insert [them] … into the data store.” Another example he quotes is the feed and feed entry storage functionality: The users in his sample scenarios share feed and article data, and they subscribe to feeds. But these feeds and article in and of themselves do not exhibit an ownership relationship with the subscribing users. These are good characteristics for factoring the feed functionalities into their own service component.\n\nAs mentioned in Chapter 1, Netflix also takes a functionality-driven approach to divvying up its system into services. In fact, one could argue that Netflix is following this approach quite to the extreme, in that they have a guideline to have a single functionality per service, such as recommending a list of movies based on a given user and her past ratings for different movies.\n\nMore low-level functionalities that could be candidates for splitting out into separate services could be a system’s messaging component, or even a key-value–based data store. Amazon’s Web Services business unit capitalizes on this approach by offering its Simple Storage Service and Simple Queue Service.\n\nGoing the other way, it is also possible to find service candidates at a much higher level. More than just partitioning out data and business logic into a service, we can also isolate UI components rendered in a larger application. Many projects have been successful in splitting out components at the presentation logic layer. Paul Dix gives the example of bundling typical Ruby on Rails framework-based views and controllers into separate services that render consistent HTML (like a “comment service” that renders HTML snippets of a comment section on multifarious pages). To give the look of a single web-application, the controller-and- view based applications can be stitched together via Edge-Side Includes (ESI). The same effect could even be achieved via a fronting Ruby-On-Rails or Rack application instead, where this application’s goal is just to combine the routes of all the backing HTML-component rendering applications into includes as modules.\n\nWhen German etailer OTTO (https://www.otto.de/) re-engineered the second generation of its entire online presence in 2013 and 2014 as a “from-scratch” service-oriented system,1 the engineering department followed an approach of what they coined “vertical decomposition.”\n\nThey split their system according to business subdomains. As part of this process, they divided the online shop into 11 different verticals; for example, their back office, product, and ordering verticals, to name just three. Each of these verticals is serving its own web UI. Other verticals’ web UIs are referenced by explicitly linking to them via HTML links. The goal of integrating the verticals into a coherent, single web application from the end user’s viewpoint is achieved via ESI includes, which a fronting varnish proxy server resolves and pulls from all respective verticals into a single, top-level HTML view.\n\nDecomposing by Maturity\n\nAnother method of identifying parts of a prospective application that are good candidates for successfully combining into a service is to look for parts that have reached a similar level of maturity.\n\nA company’s business organization typically has had several iterations of defining goals and specifications, and these are reflected in the functionalities and behaviors of parts of the application in question. Parts whose functional and nonfunctional requirements are well understood should be bundled together, while things that are less stable and subject to more and faster iterations might move into other separate, versioned services (or not split out into separate services at all, initially).\n\nDix’s example for a core, and fairly static, part of his sample social feed reader application is the storing\n\nand updating the feeds, as it “will exist regardless of other iterations.” On the other hand, the services for users, comments, and votes components will be iterated upon more frequently, so they should not be part of the same service as the feed-handling.\n\n1Guido Steinaker, “On Monoliths and Monoliths and Microservices,” September 30, 2015. http://dev.otto.de/2015/ 09/30/on-monoliths-and-microservices/\n\n26\n\nChapter 3 ■ partitioning Your Business into serviCes\n\nThe online marketplace LivingSocial has moved to this approach whenever pilot efforts for new business initiatives are being explored. A recent example is the Restaurants Plus initiative (https://www. livingsocial.com/restaurants-plus-rewards/), which was launched early to a set of LivingSocial employees and early-adopters to gather feedback and refine the product before a wider launch. Given the need for speedy and frequent iterations on the nascent business model while at the same time avoiding impact to the existing business, the decision was made to implement all new functionalities in dedicated applications and services.\n\nDecomposing by Data-Access Pattern\n\nThe next method of partitioning an application into service components focuses on data retrieval and persistence efficiency. The advice here is to analyze expected storage access for the various functionalities and classify them into “read intensive,” “write intensive,” and “balanced read and write” buckets.\n\nThe advantage of this approach is that a service’s schemas, access queries, or even the data store engine\n\n(whether RDBMS, key-value store, document-based, or other method) can be optimized for either read- or write-intensive access in isolation. Similarly, different data caching strategies can be applied for each of the services. One might, for example, decide not to introduce the complexity involved with caching and cache invalidation at all if a service deals mainly with write-intensive data.\n\nSometimes, partitioning an application by the data-access pattern leads to different results than trying\n\nto decompose along lines of related functionality. To illustrate this, let’s again consider Dix’s social-feed reader example: user profile data and a user’s reading list are functionally closely related, but the profile data is often read, while the reading list is much more often updated than it is accessed for displaying.\n\nSpecial care needs to be taken with “balanced read and write” data. Dix’s example is the voting data in the feed reader, which is updated about as often as it is read. Modularizing this functionality into a dedicated service enables the service owners to experiment with different optimization strategies to optimize performance for vending voting aggregates at both the user and the feed entry level. Another way to optimize this particular scenario would be to store voting statistics with the users and the feed entries themselves. While this could certainly result in performance improvements, that strategy does not come for free: one will have to live with the implications of only eventually achieving consistency of vote aggregate data duplicated in this way.\n\nDecomposing by Context\n\nSometimes it makes sense to implement multiple services that seem to focus on exactly the same entity. This is appropriate if they represent different distinct meanings in separate bounded contexts, one of the key concepts of an architectural design approach called domain-driven design.\n\nMartin Fowler’s article on the matter of bounded context2 explains that, for example, a customer can be polysemic. A customer can mean two different things in different business contexts; the attributes and business logic associated with a customer will differ greatly for the customer support unit of an organization from the attributes and associations in the sales unit’s business context.\n\nSuch a separation of concerns could go so far that the only shared data between such customer-focused services would be a common, unique customer identifier (best in the form of a universally unique identifier, or UUID). Both such domain-specific customer services could also cross-reference a third, lower-level customer-service for shared data (such as the customer’s address or other contact information).\n\nLivingSocial’s infrastructure serves information about cities in which the company runs voucher-based\n\noffers out of a service called city-service. This core data includes information about the city’s location, its relative size, its name, and so on.\n\n2Martin Fowler, “BoundedContext,” January 15, 2014. http://martinfowler.com/bliki/BoundedContext.html\n\n27\n\nChapter 3 ■ partitioning Your Business into serviCes\n\nIn addition, there are at least two other services that map such city entities one-to-one to model objects in different contexts: a market-area-service that is used in the context of sending user-subscribed promotional emails, as well as another service that attaches information to cities that is needed only in the context of a new business initiative.\n\nBoth services refer back to the original, core city entities via a shared city ID. In his December 2015 talk at GOTO Berlin, “DDD & Microservices: At Last, Some Boundaries!”3, Eric Evans—the inventor of domain-driven design —confirms that bounded contexts and microservices play well together. In his words, microservices “are a good vehicle for domain-driven design,” because they provide “real isolation for the work within these different teams,” where teams can work in different bounded contexts. In his mind, employing microservices to implement the bounded contexts makes it easier to erect context boundaries and to subsequently enforce them.\n\nApplication Aggregation The opposite of the previously mentioned approach, which arrived at service components to make up an entire application system via decomposition, is to reaggregate smaller, lower-level services into new larger services. The following sections attempt to classify the set of motivations that can make such aggregations desirable.\n\nAggregation for a Derived Functionality\n\nOne motivation for such an approach could be that a desired functionality can best be provided as a mash-up of other existing, more basic core services. An example of such derived functionality could be a recommendation service for an e-commerce website. Such a service could be used to vend a set of personalized product recommendations for a given user. The data upon which the recommendation engine bases its suggestions can come from several, more basic services. One example of a more elementary service that provides input to the recommendation service might be a purchases service, which exposes data about a user’s previous purchases. Another input for data to be mashed up inside the recommendation engine can be a product service, which categorizes products and potentially relates them to complementary products. Combined, those two basic services’ APIs can be combined to recommend other “up-sell” products that are either in the same category as the user’s previous purchases or complement such previous purchase activities.\n\nThis approach is often taken when new features of an application are conceived in the later stages of the application’s lifecycle. Encapsulating such features into their own service enables the business to receive quick feedback, while the engineering team is able to launch the changes in relative isolation.\n\nGoing this route, and again using the previous example of a recommendation service, the business can\n\nmore easily test the effectiveness of new recommendation algorithms, for example, adding in data about purchases of other users that have bought similar products as the user for whom a recommendation is to be made.\n\nAggregation for Business Intelligence\n\nAnother good reason to aggregate a set of services into a secondary layer of services is to provide organizations with tools to gather and evaluate business metrics.\n\nMany companies aggregate data from other core sources in extremely large data stores, such as Apache\n\nHadoop. Systems often called “data warehouses” are built as a service-layer on top of this data, providing “Extract, Transform, and Load” (ETL) functions for use by business analysts.\n\n3A recording of the talk can be found at https://www.youtube.com/watch?v=yPvef9R3k-M\n\n28\n\nChapter 3 ■ partitioning Your Business into serviCes\n\nA popular use case for such a system is financial reporting and forecasting, and other monitoring of business-related key performance indicators. Another common use case for this type of data analysis and data mining approach is to build fraud detection services on top of the large data sets available in such aggregated data stores. Similarly, the tasks of personalizing functionalities to the granularity of a single consumer of an e-commerce site often rely on post-processing large amounts of other data, from disparate sources.\n\nTypically, most of the business intelligence capabilities relying on such large data are not updated in\n\nreal time. While a service that provides this data could be called as part of an HTTP request cycle from a consumer’s browser, the underlying logic often is based on precalculated data.\n\nAggregation for Client Convenience\n\nAnother common motivation for composing services is to make life for client applications as easy as feasibly possible.\n\nDesigning service APIs with the maximum level of utility to the prospective clients in mind can\n\nsignificantly contribute to an application’s success. Tailoring the service endpoints such that they present a coherent design and only expose data and functionality that a client truly needs is crucial.\n\nHiding implementation details of the underlying data design of the service’s internals helps not only\n\nwith usability, but also with security aspects, especially for publicly accessible APIs.\n\nOften this service aggregation pattern is also applied to provide a more coherent, or generally just more\n\nuseful, presentation layer of data that a client application needs to perform its own duties. One example of this use case could be a “mobile façade service.”\n\nNative mobile applications (such as iOS or Android apps) often have a lifetime that spans multiple\n\nyears, and there usually is no good way to force all client devices to upgrade to newer versions of the application. This means that the service APIs used by such native mobile apps are publicly exposed, and hence need to be virtually frozen once any app version uses them. Unless you want to run the risk of causing breakages in the installed base of already shipped clients, those APIs should not be changed. Newer versions of the mobile apps, which want to take advantage of new features or changes in the APIs that break existing API contracts, will need to use an entirely new version of the previously exposed API. Over time as the number of supported features and mobile app versions grows, so does the number of APIs that need supporting separately and almost eternally.\n\nLivingSocial has such a mobile façade service at the edge between the public network and the application provider’s internal network. One reason for this choice was to shield native clients from the complexities and potential inconsistencies in other downstream services’ API contracts. This way, it is possible to introduce coherence and provide guarantees about data formats: not having to support the many different possible data formats for calendar dates, times, or currency value can greatly reduce the complexity of client applications. This edge service also performs validations of data it vends to native applications (for example, via enforcing that currency data conform to a schema complying with ISO 4217, dates that conform to ISO 8601), so that publicly shipped native applications can be more stable themselves. It also is a central point of dealing with the above-mentioned need for long-term support for publicly exposed APIs.\n\nAggregation to Aid System Performance\n\nA final purpose for service aggregation is to use it to improve response times or availability of the overall application.\n\nSome out-of-the box solutions exist for such efforts, for example caching proxy-services like Varnish (https://www.varnish-cache.org/). Architecturally, Varnish can be introduced as a caching layer to front one or more HTTP-based services, exposing those services’ endpoints directly to clients that otherwise would have directly produced traffic to the back-end services. Varnish is lightning-fast, and it has highly configurable caching and data aggregation capabilities, which include support for ESI and HTTP conditional GET header processing.\n\n29\n\nChapter 3 ■ partitioning Your Business into serviCes\n\nBesides serving as a proxy that can aggregate and cache service-exposed data for performance improvements, Varnish can also be configured to aid system availability, as it can continue to serve data from its caches even if the back-end services fronted by varnish are currently unavailable.\n\nAnother example of aggregation for performance tuning is the case of services that expose aggregated\n\ndata in different data representations, or are optimized to provide search APIs.\n\nOne example from our work experience at LivingSocial is an internal service for the company’s overall\n\nproduct catalog. While each of the different product types is owned by a dedicated “systems-of-record” service, this catalog-service aggregates data about all product types. This data is stored in a denormalized fashion, and special database indexes are built to allow for very fast search and retrieval of product data by a configurable set of criteria.\n\nYet another service at LivingSocial whose main raison d’être is to improve overall performance is called\n\ninventory-service. It is a service that serves almost the exact same information as catalog-service, but which is specifically optimized for fast geo-location based product searches.\n\nAdditionally, the service provides a way to define special purpose representations of the product data, which can be tailored to the respective client’s needs. This way, the payload received about a set of products can be greatly reduced, so that network transmission time and latency, as well as service-side data serialization and client-side deserialization, can be kept to an absolute minimum.\n\nPaul Dix’s book about service-oriented design also speaks about improving efficiency by aggregating data that is often accessed together to provide information for service clients. He calls this “partitioning by join-frequency,” as this information is often generated by joining data from several database tables.\n\nWhile joining database tables can often be costly, combining data procured by multiple, potentially\n\nsequential, service calls across process and network boundaries can become even more prohibitive to meeting performance requirements. While this realization is surely easy to understand, the problem is much harder to avoid: all nontrivial applications usually expose functionalities that require almost all data to be joined. Dix formulates a few guidelines to impact performance as little as possible:\n\n\n\nAnalyze the functionalities of your application up-front, and consider the expected frequency of joining the various data objects. Looking at the results, you can make decisions about placing data and logic that is most frequently joined inside the same service and data store.\n\n\n\nIf the previous approach impedes the rest of your application design too much, you can denormalize the joined data, and then push it (for example, via a messaging bus) to all the services that want to join against it. This will give you the best performance improvements if the joined data is often accessed. Otherwise, the cost and complexity involved with broadcasting to, and storing it in, all joining services might not be justifiable.\n\nWhen to Use Decomposition versus Aggregation? After spending a significant amount of time trying to research, we have failed to come up with hard-and- fast rules or checklists for which of the above strategies to use under given conditions. Many of these design decisions are made weighing the pros and cons of the given characteristics of each of the approaches laid out above.\n\nThe best general rule of thumb we have been able to identify during a decade building and maintaining\n\nservices is that favoring decomposition techniques over aggregation techniques appears to be a cyclic decision. We have found that decomposition of a domain into small services is often how a business should start out, while aggregating services appears to be more needed and helpful to address the issues that arise later in the lifecycle of an application. Once a business is established and wants to diversify, that situation often triggers another phase of breaking down new ideas into additional small core services, which will then again be composed (often, together with other existing services) to form larger, aggregated services.\n\n30",
      "page_number": 41
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 49-57)",
      "start_page": 49,
      "end_page": 57,
      "detection_method": "topic_boundary",
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nWhile we realize that these observations and considerations appear rather general, we hope that the\n\nexample business case discussed next might help to provide additional guidance.\n\nDecomposing and Aggregating a Sample Application To illustrate many of the concepts laid out in this book, we have decided to put them into practice in a sample application that is partitioned into several small services.\n\nThe following section first lays out the main features of this application, and then explains decisions about how the application could be structured into a set of microservices, based on the previous section’s guidelines.\n\nWelcome to Our Bookstore! The sample domain chosen to accompany this book is that of an online bookstore. Throughout the rest of this book, we will use this fictitious bookstore to illustrate the concepts laid out in this book. These illustrations include partially implementing some of the services and applications that together could make up the entire system that supports this sample domain, discussing the resulting code samples, and deploying them to a cloud infrastructure.\n\nHere is a list of features that we see as needed for our bookstore business:\n\n\n\nVisitors to the bookstore’s site will be able to view, read reviews about, and search for, books (by category, title, author, or topic), and place them in a personal shopping cart.\n\nOnce registered on the site, such visitors become users, who can also buy existing books for themselves, or as gifts for others, and preorder books soon to be released.\n\nUsers can also sign up for promotional email newsletters and notifications about events regarding order status or activities of other users (see the social component described shortly). Users can also choose to receive personalized recommendations based on previous reading or shopping activity, category or author preferences, and recommendations by users they follow.\n\n\n\nSite-wide and category-specific bestseller lists will be maintained and displayed in the store.\n\n\n\nThe bookstore is aiming to serve all of North America, and therefore not only is obliged to follow Canada’s bilingual regulations, but also Quebec’s strict rules about the use of the French and English languages.\n\n\n\nBooks can exist in different formats: a physical book in paperback or library binding, as well as e-books in PDF, Mobi, or epub formats. Physical books can be bought, using a range of different payment instruments. Physical books are shipped to a set of user-associated addresses, and book returns are permitted within 7 work days after delivery. Electronic books are delivered electronically (via a download link on the user’s account summary page) and cannot be returned once purchased and delivered.\n\n\n\nThe bookstore will also have several social features: users can have reading lists and wish lists for books. They can write comments and reviews of books on the site, up- or down-vote books they have read, and follow other users’ reading lists, commenting, and reviewing activity.\n\n31\n\nChapter 3 ■ partitioning Your Business into serviCes\n\n\n\nA fraud detection system integrated in the online bookstore monitors purchasing, review, and voting activities on the site. The system can flag suspicious activities to site administrators, as well as autonomously take action regarding user, vote and review activation statuses.\n\nWriting reviews about books and voting on them, are ways for a user to receive in- store credits, which can be used to purchase more books on the site.\n\n\n\nAn internal web interface to the bookstore will allow bookstore personnel access to various administrative tasks, based on the fine-grained access control restrictions. Examples of such tasks are adding, removing, or altering data about the books (for example, descriptions, pricing, category, keyword, or author-related data), and administering user-related data (like email and physical addresses, in-store credits, resetting user passwords, changing a user’s registered payment instruments, and toggling the user’s activation status) or curating comments and reviews on the site.\n\n\n\nAdditionally, business intelligence reports can be run on all data entities that are part of the application, for example, about users and their activities, book sales and review performance, current and projected financials.\n\n\n\nLast but not least, most of the consumer-facing functionalities are also exposed in native mobile applications running on iOS and Android devices. Other mobile devices are supported by a mobile web interface.\n\nPartitioning the Bookstore Let us start out with a few words of warning: as should be clear from the first section of this chapter about guidelines, decisions about the exact set of services to make up the entire system are very often design trade- offs to be made by the team(s) involved. There really are no black-or-white answers to this task, or anything like a “correct” partitioning, and even less so one that is optimal under all circumstances now and for the entire lifecycle of the bookstore.\n\nAlso, the bookstore business we’ll describe next has a rather large feature set and complexity; much\n\nmore than anything we are intending to implement in the accompanying example code. Do not be overwhelmed by the number of services proposed in this section. While we advocate having a rather large number of services for any application (very often larger than the number of developers working on them), it should be clear by now that we do not recommend rushing into a microservices architecture approach head-long or naively; adopting strong supporting development, deployment, and monitoring tools prior to engaging in such an architecture remains a necessary prerequisite.\n\nWith that said, we mentioned in the first section of this chapter that decomposing the bookstore\n\nbusiness along lines of functionality is probably the most natural way to start.\n\nDecomposing into Core Services\n\nKey and core functionalities are most obviously the data and logic directly related to the books that are sold on the site, so envisioning the existence of a books-service seems very appropriate. It seems very prudent to spin up an authoritative service for all attributes and direct functionalities directly associated with the products to be sold, commented and voted on, and reviewed on the site. The various versions of a book, in their respective formats (paperback, library binding, epub, and so on) will be called “book variants.”\n\n32\n\nChapter 3 ■ partitioning Your Business into serviCes\n\nTo help performance, the books-service will vend product information in several different formats, or representations, each of which has a different subset of the entire set of book attributes. As an example, the bestseller list displayed in the web application will request a representation called a tile, which contains only a book’s title, author, a default price, and thumbnail URL for the cover image. In contrast to the page that lists bestsellers, a book’s detail page will request the “full” representation, which also contains the number of up- and down-votes, a full descriptive text, a summary, pricing and other details for all different variants of the book, and more.\n\nIt is important to note that books (much like any other entity in the store) are never truly deleted from the data store. They might be marked as “out of stock” or “out of print” or otherwise unavailable for purchase, but they will always be available in the data store via their direct resource UUID. This ensures that things like a user’s purchase history will be able to refer to them.\n\nAnother key entity is surely the user on the site; so for similar reasons as for books, implementing a\n\nusers-service seems a strategy in line with partitioning the application domain by functionality, as well.\n\nA user service necessarily handles the core data attributes that are associated with a user (for example,\n\nher name, email address, user handle, unique UUID, activation status, and potentially her salted and encrypted password) and the core logic to create, change, or read some or all of these attributes. In addition, it is also often tasked with authenticating a site user. This most likely means that a session cookie containing an encrypted user ID is generated and vended at an authentication endpoint used by browsers. For inter- service authentication, which mostly serves to authenticate the acting user for sessionless API requests, such a users-service often employs OAuth 2.04 tokens.\n\nIn other contexts, it might seem more prudent to separate the authentication functionality out of the core user service into a separate service. Sometimes, such an authentication service even lives outside the actual application domain. OAuth authentication via providers like Google, Facebook, Amazon, LinkedIn, or Twitter, to name but a few, is quite commonplace in today’s online applications.\n\nLooking at the fact that administrative tasks also need to be performed on the site by authenticated\n\npersonnel, a users-service might not just be responsible for consumer customers, but also for handling company employees. Depending on how fine-grained access control will be to various administrative options on the site, a simple, Boolean admin flag to identify administrative personnel might not be sufficient. In this case, many sites apply more or less sophisticated systems for role-based access control (RBAC). Such a service could either be integrated into a users-service, or developed as a dedicated authorization service. Given the scenario for our fictitious bookstore described earlier, we are making the decision to combine\n\nthe core user data and authentication functionalities into a single users-service, while isolating the RBAC functionality into a separate service. The main reason for this decision is the expected access pattern for these separate functionalities. The store will have a consumer-facing set of applications, which will be kept separate from the ones to be used by administrative staff. Both the consumer applications and the administrative interfaces will need to create, alter, and read core user data, as well as authenticate users. RBAC, however, mainly applies in the realm of the administrative context; consumer-facing applications are not envisioned to have much use for fine-grained access control. We only want to prevent consumers from modifying other consumers’ information or viewing other users’ sensitive information.\n\nRelated to users is another entity: addresses. We have decided to also dedicate a separate addresses-\n\nservice to these core data objects. Among the main reasons for this decision are data sensitivity and compliance concerns; billing and shipping addresses, for example, are personally identifiable information (PII), and therefore protected by specific laws in many jurisdictions. Many companies therefore decide to keep such data in separate, specifically protected environments, so that any potential security breaches to less restrictive environments do not lead to loss of personal information.\n\n4Internet Engineering Task Force (IETF), Request For Comment 6749, “The OAuth 2.0 Authorization Framework,” October 2012. https://tools.ietf.org/html/rfc6749\n\n33\n\nChapter 3 ■ partitioning Your Business into serviCes\n\nApplications and Aggregated Services\n\nThere will be three major applications that are directly at the edges of the bookstore business, catering to external customers, and hence are exposed to the public network:\n\n\n\nThe application that renders HTML for desktop web users (let’s call it desktop-web),\n\nOne application that serves HTML to mobile web clients (called mobile-web)\n\n\n\nA mobile façade service exposing public APIs for access by the iOS and Android mobile applications (named mobile-service)\n\nAdministrative applications for company personal, including the reporting tools for running business\n\nintelligence queries against the data warehouse, will only be exposed inside the company’s firewall.\n\nImportant to note here is that none of these “pixel-rendering” edge applications will have access to any authoritative data store. While they might use some local caching technologies, they will rely solely on data that they retrieve from other non-edge service applications. This holds true even for the mobile API façade service, and—if there will ever be such an extension to the bookstore—for any other publicly facing APIs for external customers’ apps to consume.\n\nThe introduction of the “one-stop shop” mobile-service is done following the previously mentioned\n\npattern to aggregate services further downstream for the convenience of the Android and iOS apps. Localizations are done in this service, and attention is paid that all data is transformed into formats that can be easily and efficiently processed by the native mobile apps. As there is no way to ever truly force mobile devices to upgrade the bookstore apps, this mobile façade will need to implement strict versioning for the exposed APIs, with multi-year support for each of the versions introduced.\n\nFor maximum stability of the native apps, all data formats vended by the service will be specified using\n\nJSON Schema (http://json-schema.org/), and the service’s integration test suite will verify that all the versioned APIs conform to the schema specifications.\n\nAdditionally, mobile-service will serve as an aggregator to help overall system performance, adding HTTP conditional GET and caching headers like ETags and Cache Control max-age.5 This is a very effective and unobtrusive way to help native mobile applications improve performance, as the most used iOS and Android HTTP libraries support honoring and inserting the respective headers without any additional configuration or coding efforts.\n\nSupporting Platform Services\n\nGiven that the bookstore will operate in Canada, translations in at least the Canadian English and Canadian French locales will be required. In order not to duplicate the responsibility for correct localization of any given string across all edge applications mentioned above (for example, desktop-web, mobile-web, and mobile-service), a central translations-service will be developed. Translators and other company personnel will use an internal web UI to define strings via unique keys, and provide translations into all needed locales. The translations-service will store this data and might decide to expose it via RESTful APIs. The main way to communicate these translations internally to the bookstore infrastructure will be by publishing any changes or new translations on a system-wide message bus topic. Because of its message delivery guarantees and replay options, we will opt for using Apache Kafka.\n\nAny (existing and future) edge application that needs to render localized text will consume the translation’s\n\ntopic in a dedicated consumption process, and then update an internal storage engine (for which we will use a simple, file-based Berkeley database) to have the latest translations available inside the edge application. Purchasing books will also be handled by a separate service, called purchase-service, which encapsulates this functionality. It plays together very directly with two other services: cart-service and payments-service.\n\n5See https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html\n\n34\n\nChapter 3 ■ partitioning Your Business into serviCes\n\nA user can populate her cart with a number of book variants in various quantities. The cart-service will therefore have a dependency on the books-service, as it will reference the book variants by the UUID identifying each book option.\n\nThe payments-service encapsulates the various accepted payment instruments a user can register with her account on the site. The bookstore will initially accept the major credit cards, but encapsulating this functionality into its own service will make it much easier to accommodate foreseeable extensions like the desire to also support PayPal payments, or maybe even in-store credits a user could accrue (for example, in return for reviewing many books). To simplify PCI compliance requirements, the service acts as a gateway an for external charging service (such as Braintree), never actually storing more than an inconspicuous payment instrument foreign key (as opposed to actual credit card numbers, or the like).\n\nThe steps that happen during purchase as part of the checkout process are as follows:\n\n\n\nThe book variants’ prices are retrieved from the books-service, multiplied by the selected quantities, and summed up to retrieve the totals of the cart’s value.\n\n\n\nAn attempt is made to charge the payment instrument chosen for the cart’s value using payments-service.\n\n\n\nThe cart data, cart total, and payment instrument ID are denormalized and then sent as inputs to purchase-service to create a record of the sale.\n\n\n\nA message about such a successful sale is placed on the appropriate Kafka message topic to inform dependent systems (like a service vending best-seller lists based on sales numbers) of this purchase event.\n\nTo allow for customer refunds, we implement a refund-service. It relies on historical purchase data, and the fact that a purchase-service record entirely describes the sale transaction (including the pricing information that was current at the time of purchase, as well as the payment instrument to which to refund the sale total). Refunds are also events that are published to a Kafka topic, so that other systems (for example, a data warehouse used for financial reporting queries, or again the best-seller list service) can update their own internal states accordingly.\n\nThe fraud-service, which detects suspicious or clearly fraudulent consumer behavior, has no directly\n\ncallable endpoints. It autonomously monitors purchasing and other user activities, and subsequently notifies administrative personnel. Depending on the severity of the fraud suspicion and impact, the service can deactivate users, votes, or reviews without prior human approval.\n\nFeature-Focused Services\n\nThe bestsellers-service exposes only a single search endpoint that responds to the HTTP GET method to vend the various bestseller lists based on criteria like a book category, an author, the book format, the language a book was written in, or the year it was published. This service’s backing data store can be optimized based on its purely read-intensive data access pattern to use a key-value store, or a search- optimized back end like Elasticsearch (https://www.elastic.co/products/elasticsearch). Updates to this data store will happen in a separate message-consuming process that listens to the topics published purchase-service and refund-service, while querying other data sources (like books-service) for additional data, like a book option’s title, author, category, format, language, and so on.\n\nThe social features mentioned in the bookstore scenario will also mostly be implemented using\n\ndedicated services. When partitioning these features into service components, we are placing special care on the data-access patterns and data join frequencies, in order to ensure that these features function efficiently at scale.\n\nSeparate services for comments, voting, and reviews functionalities are justified by their separate\n\nfunctionalities and potential maturity and iteration speeds.\n\n35\n\nChapter 3 ■ partitioning Your Business into serviCes\n\nA decision that is harder to make is this: which service should own the information about the set of users any given customer follows? From a functionality-driven perspective, the most natural place to store and retrieve such “user A follows users B, C, and D” data would be inside the users-service. But, given that we also have a requirement to notify all users of commenting, voting, and review activities by the users they follow, an activity-notifications-service (which will be in charge of notifying all following users) would have to constantly make service calls to the users-service to retrieve the current list of users to notify. Therefore, instead of keeping the “follows” relationship inside the users-service, we decide to keep this information inside the activity-notifications-service data store, so main cross-service data joins can be avoided.\n\nThe recommendations-service vends personalized recommendations and is a good example of a service that exposes a functionality that is mostly a mash-up of other, more basic service information. Recommendations will be based on purchase history data retrieved from the purchase-service application, combined with current cart-service information, and joined against books-service information about categories and authors.\n\nA promotions-service is an example of an entirely out-of-request-cycle service, which exposes no synchronous API endpoints. It operates entirely as a background process that sends promotional emails and push notifications to users based on data from the recommendations-service, as well as a users-service, once every week, or during holidays specials.\n\nSummary This chapter has focused on ways to break down a business domain and its related features into a set of smaller web applications and microservices. We looked at the two general approaches of application decomposition and application aggregation, and identified a set of patterns for them.\n\nWe examined and explained the following decomposition approaches:\n\nDecomposing by functionality\n\nDecomposing by maturity\n\nDecomposing by data access pattern\n\nDecomposing by context\n\nWe discussed the following aggregation patterns and the effects of their properties:\n\n\n\nAggregation for a derived functionality\n\n\n\nAggregation for business intelligence\n\n\n\nAggregation for client convenience\n\n\n\nAggregation to aid system performance\n\nWe concluded the chapter by outlining a fictitious online bookstore business application and its desired\n\nfeatures. We then applied some of the patterns and techniques discussed in this chapter to partition the bookstore into a set of core services, supporting services, and user-facing applications.\n\nChapter 4 will focus on the design aspects involved in exposing specific service APIs, both internal to a company and externally to the public, and we will describe a set of characteristics that we think make up a good API.\n\n36\n\nCHAPTER 4\n\nDesigning Your APIs\n\nIn this chapter we will start explaining in detail how APIs should be designed. We will define what APIs are and will describe a set of characteristics that we think make up a good API, using what you now recognize as our opinionated approach.\n\nWe'll then explain how you should design your own APIs by describing a set of APIs from our bookstore\n\nsample application, listing each use case and describing our thought process when defining inputs and outputs for each API.\n\nIntroducing APIs To answer the question of what we understand by the term API, let’s look at some quotes from standard web resources that we think capture our understanding well and clearly.\n\nAPI (application program interface) is a set of routines, protocols, and tools for building software applications. The API specifies how software components should interact and APIs are used when programming graphical user interface (GUI) components.\n\n—Webopedia1\n\nAn API expresses a software component in terms of its operations, inputs, outputs, and underlying types, defining functionalities that are independent of their respective implementations, which allows definitions and implementations to vary without compromising the interface. A good API makes it easier to develop a program by providing all the building blocks, which are then put together by the programmer.\n\n—Wikipedia2\n\nOne aspect often overlooked when describing any API is its nonfunctional requirements. You will see why it is generally important to understand those requirements early on in the design process, and to include them in each individual API description.\n\nThe use of APIs is not exclusive to microservices or even other web-based applications. Many different\n\nkinds of computer systems employ them to enable software developers to build applications for these systems based on published programming interface specifications. As an example, a programmer who develops apps for Android will use Android APIs to interact with the system to build screens and access phone capabilities and hardware, like the front camera of an Android-based device.\n\n1“API—application program interface”, Webopedia (August 24, 2016). http://www.webopedia.com/TERM/A/API.html 2“Application programming interface”, Wikipedia, referenced on May 5, 2016. https://en.wikipedia.org/wiki/ Application_programming_interface\n\n37\n\nChapter 4 ■ Designing Your apis\n\nAPIs are especially important in the realm of microservices because most communication between applications happens via API calls. Each of your microservices will expose its functionalities via a set of endpoints that have a well-defined set of inputs and outputs, and that perform the task(s) the service is responsible for.\n\nThe Significance of API Design\n\nAPI Design is easy … Good API Design is HARD\n\n—David Carver3\n\nDepending on your line of business, your API may be the actual product you sell and expose to your customer; if so, you have to take as much time as you need before you expose that service to your clients. If possible, come up with a process that helps you start developing and exposing your APIs early to a small subset of your users, so that you start to validate those APIs iteratively.\n\nWhen you decide to use an external API, you acknowledge that it takes a certain amount of time for your development team to learn that API and understand its interaction points. One needs to read documentation and write code, sometimes using trial and error, to take advantage of an API's functionalities. With that in mind, you should strive for providing an API that is well documented, behaves in a consistent manner, and performs well. Creating APIs that are inconsistent across different endpoints or perform poorly will frustrate the developers that have decided to invest in your API, so you really want to get that right the first time.\n\nAt Amazon Web Services, new services are usually extracted from known internal services at Amazon that are already successfully used by a set of developers. Once a service is identified as a possible external product, then getting a first beta of the to-be-public API to a very small set of Amazon customers is the next step in the product development cycle, because that allows some validation of that API. After the service team is happy that internal customers have successfully deployed solutions with the beta API, then a small set of external (to Amazon) customers are selected to be private beta users of the new service, to further validate the service API. Of course, at the scale of AWS, it's probably impossible to fix a bad API after it goes live; so, they have a validation process to make sure that their APIs will be useful in the designed state before they become public.\n\nWhy is Designing a Good API Important?\n\nOnce customers started building their applications and systems using our APIs, changing those APIs becomes impossible, as we would be impacting our customer’s business operations if we would do so. We knew that designing APIs was a very important task as we’d only have one chance to get it right.\n\n—Werner Vogels4\n\nOur goal as API designers is to deliver a product that is easy to learn and use, easy to maintain and\n\nextend, and perform its functions correctly and according to the client’s nonfunctional requirements.\n\n3“API Design is easy … Good API Design is HARD”, David Carver (October 20, 2009). https://intellectual- cramps.wordpress.com/2009/10/20/api-design-is-easy-good-api-design-is-hard/ 4Werner Vogels, “10 Lessons from 10 Years of Amazon Web Services,” March 11, 2016. http://www. allthingsdistributed.com/2016/03/10-lessons-from-10-years-of-aws.html\n\n38\n\nChapter 4 ■ Designing Your apis\n\nJoshua Bloch, formerly the lead designer of many APIs and features of the Java platform at Sun\n\nMicrosystems makes the following key point about placing high importance on API design:\n\nThinking in terms of API Design improves code quality\n\n—Joshua Bloch5\n\nOn the opposite side of the equation, bad API design will cause you a lot of issues that will require you\n\nto make—often big—changes to your API design, which in turn will potentially break the code of your clients who are using the initial poorly defined API. There's nothing worse than asking your clients to rewrite code for no good reason. Another reason to avoid rushing your API designs is that you will probably still have to support the old versions of an API for clients who cannot upgrade to your new version when you want them to, so you may end up having to support both versions for a long time.\n\nWhat Makes an API Good? Now that we understand why API design is very important for microservices, we will talk about a set of properties and best practices that we think make a good API.\n\nGeneral Characteristics Any API designer should pay special attention to the big picture of the interfaces that the system under design exposes. In our experience, the following properties have proven particularly important to the success of APIs. Please note that we see all these characteristics as equally essential, so we do not rank or prioritize any one over others in this list.\n\nClear Naming\n\nEvery API should have an obvious name that makes sense to its users. It's known that naming is one of the hardest problems in software engineering, but it's especially important when you're creating an endpoint that other developers will call to make it very easy to remember. If you manage to give your API a name that developers can guess just based on its functionality, then you've nailed it.\n\nFocus\n\nAPIs should do one thing and do it well; the purpose of each API has to be clear to all its clients. If you make the reason for an API easy to understand, remember, and use, then you achieve this goal.\n\nCompleteness\n\nAn API needs to be able to fulfil its raison d'être; the functionality you advertise has to be properly implemented. This may sounds contrary to the previous characteristic, but is actually complementary.\n\n5Joshua Bloch, “How to Design a Good API and Why it Matters”, JavaPolis 2005, Antwerp, Belgium. https://www. infoq.com/presentations/effective-api-design\n\n39",
      "page_number": 49
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 58-67)",
      "start_page": 58,
      "end_page": 67,
      "detection_method": "topic_boundary",
      "content": "Chapter 4 ■ Designing Your apis\n\nIntuitiveness\n\nThe best APIs are the ones you can call without having to think a lot about them. If an API consumer can guess what your API does by glancing at your documentation, then you've achieved this goal.\n\nConsistency\n\nWithin your company, it's imperative that you have a set of consistent rules around API development. When you adopt consistent naming conventions for how you call your APIs, and how inputs/outputs are expected in the same way across your APIs, using them will become easy and intuitive.\n\nHypermedia Type In the name of consistency, you need a set of rules that any team working on APIs should follow. In our experience, organizations that let each developer and/or development team build APIs as they see fit will end up with a ton of inconsistencies in inputs, outputs, error treatment, and response envelopes. Over time, those inconsistencies will cause headaches and slow down your development effort overall. We've seen cases of APIs that were very inconsistent within the same service. This causes a lot of frustration for API clients who can't reliably reuse code to handle basic things like error-handling, because those endpoints don't follow a standard.\n\nMike Amundsen, author of the 2011 O’Reilly book Building Hypermedia APIs with HTML5 and Node,\n\nmakes an important point in an article called “Hypermedia Types” about the characteristics of hypermedia types and their relationship of the flow of an application using them:\n\nHypermedia Types are MIME media types that contain native hyper-linking semantics that induce application flow. For example, HTML is a hypermedia type; XML is not. 6\n\n—Mike Amundsen\n\nWe think it's vital that your organization picks one hypermedia type option as a standard for API communication, and sticks with it on all your APIs, and it will be the answer to questions that could cause inconsistencies. Some options are:\n\nHAL (https://tools.ietf.org/html/draft-kelly-json-hal-06)\n\n\n\nJSON API (http://jsonapi.org/)\n\n\n\nJSON-LD (http://json-ld.org/)\n\n\n\nSiren (https://github.com/kevinswiber/siren)\n\n\n\nCollections+JSON (http://amundsen.com/media-types/collection/examples/)\n\nEach of these hypermedia types has its rules that describe the exact format that requests and responses\n\nshould conform to, with varying degrees of robustness, verbosity, and strictness. We have decided to use JSON API in this book because it's a complete solution for describing what our APIs are supposed to look like, and it provides the consistency we're looking for. It could be that your preferred programming language has better support for other hypermedia types, and we recommend you do some research to pick the tool that helps you be most efficient.\n\n6“Hypermedia Types”, Mike Amundsen (2010). http://amundsen.com/hypermedia/\n\n40\n\nChapter 4 ■ Designing Your apis\n\nSome hypermedia types have fairly long and strict specifications, and some leave a lot of room for\n\ndevelopers to maneuver around their definitions. We prefer hypermedia types that leave no room for guessing how APIs should behave and recommend ones that take all of the following into consideration:\n\n\n\nRequest/response envelopes\n\n\n\nAuthentication\n\n\n\nResponse code\n\n\n\nError codes/messages\n\n\n\nPagination parameters\n\n\n\nSorting/ordering responses\n\n\n\nVersioning\n\nWhile we will not discuss the pros and cons of each of the hypermedia type options noted earlier, there\n\nare some very good articles that go over many of the characteristics of each type, and we recommend you experiment to find a specification you're happy with.7 The important part is that you make it obvious what hypermedia type you are using and make sure that it's respected by all teams in your company. By doing that, you won't have surprises when calling services, making your developers happier. As a general rule, we highly recommend making sure to research the tool chain that exists for the hypermedia types you choose with regard to the languages and the rest of your technology stack.\n\nWe realize that choosing a common hypermedia type across team borders is often a challenge, and that\n\nthe preferred choice can and will vary over time. In such scenarios it will often be necessary to introduce breaking changes to existing APIs, and we will present a process to address such evolution later in this chapter.\n\nAPI Design Guide After you have selected a hypermedia type to standardize on, we recommend you create an API Design Guide document (often also referred to as a Style Guide) that describes your organization's best practices regarding API design/development. You should aim for a document that is easy to read, searchable, concise, and explains your practices well. Make sure to update your API Design Guide as you improve your design practices based on what you’ve learned, and provide guidance about prioritizing potentially conflicting goals.\n\nA good API Design Guide should touch on all of these:\n\nHypermedia type to adopt\n\n\n\nRequest formats\n\n\n\nResponse envelopes\n\nConsistent attribute names\n\n\n\nExposed IDs\n\nOther concerns commons across your services\n\n7Chapter 7, which focuses on optimizing APIs, will show the use and virtues of a few hypermedia types in more detail.\n\n41\n\nChapter 4 ■ Designing Your apis\n\nYou've probably noticed that hypermedia types may dictate some of the things that go into your Design\n\nGuide; selecting a complete hypermedia type will make your API Design Guide simpler to write, as a lot of concerns are already covered in the hypermedia type specification.\n\nA great example to follow—and find inspiration in—is Heroku's HTTP API Design Guide (https:// geemus.gitbooks.io/http-api-design/content/en/index.html). It's a very easy to read document that describes Heroku's set of HTTP+JSON API design practices. You will see that it answers quite a few questions about how APIs are expected to behave, and again, removes the need for developers to guess about many aspects of API development, making APIs consistent across the company.\n\nNonfunctional Requirements Often, most of the discussion around APIs navigates toward the inputs, outputs, and side effects of API calls. In our experience, the nonfunctional requirements for API calls—which usually relate to availability, performance, scalability, capacity, recoverability, and reliability—can end up being ignored or not seen as crucial. We, of course, disagree with that emphasis and think it’s essential to be explicit about those requirements when exposing and consuming APIs. We'll discuss some of the most important nonfunctional requirements of APIs in this section.\n\nService-Level Agreements\n\nMake sure that you have service-level agreements—SLAs—as part of your service API contracts. For example, you can have service-level availability agreements that apply to all the endpoints exposed by your service, and can go into detail about specific endpoints that have more fine-grained requirements.\n\nAvailability\n\nAvailability is probably the most important nonfunctional requirement a service owner needs to guarantee, because an unavailable service is not very useful; so your SLA should have an availability clause that explicitly states your service's availability guarantee. The simple fact of thinking about availability will make you think about the various scenarios that can impact availability, and necessary steps to guarantee that your service can be back online in a short time. It may be that, because of their responsibilities, specific services can be unavailable for extended periods of time without affecting a business; if so, make sure that is documented as well. For a public-facing commercial service, a sample availability clause in a SLA could read:\n\nAmazingAPICo will use commercially reasonable efforts to make sure its API is available 99.95% of the time. In the event AmazingAPICo does not meet the goal of 99.95% API availability, you will be eligible to receive a service credit.\n\nFor an internal service within a corporation, the clause could be as simple as this:\n\nServiceA should be available 99.5% of the time. In the event ServiceA is continually unavailable for more than 5 minutes, please page TeamA.\n\nBy being explicit about your nonfunctional requirements, you make sure that your clients are aware of the type of service they will receive, they can plan for fallback measures during outages, and you have measuring sticks for your monitoring and alarming setups. Also, SLAs for internal-only APIs can be used to estimate resources and justify operational expenditures (for example, for fail-over hardware if a given uptime requirement cannot otherwise be met).\n\n42\n\nChapter 4 ■ Designing Your apis\n\nPerformance\n\nPerformance is another extremely important nonfunctional requirement, and it can affect the actual implementation of your service; therefore, you don't want to start thinking about performance only when your service is about to go live, because you may need to make substantial changes if its performance is not acceptable for its clients.\n\nWe go into a lot more detail about performance monitoring and alarming in Chapter 13, but the\n\nimportant aspect here is to make sure performance is not an afterthought. Your SLA should state some metrics that you want to respect during the operation of your service; this could be an average or p908 response time for all endpoints in your service, or an endpoint-specific performance metric for endpoints that have stricter performance requirements.\n\nScalability, Capacity, Recoverability, and Reliability\n\nWe will not go into detail about every aspect of your nonfunctional requirements, as those may vary widely. Our recommendation is to make sure you give them the attention they deserve and spend some time thinking about how to answer most of the following questions:\n\nHow do we scale the service? How do we measure scalability on day 1?\n\nHow many requests can the service support while maintaining its performance? How do we add extra capacity?\n\nHow fast can we recover from known failures?\n\nHow reliable is the service? Do we have specific reliability concerns? Is eventual consistency acceptable?\n\nIdeally, you should have a template SLA that can easily be adopted by your services with little\n\nadaptation, to avoid making “adding an SLA” feel like a complex task that requires weeks of analysis.\n\nImplementing APIs the Right Way Now that you know what an API is, it's time to dive into how APIs should be implemented. We'll outline a fairly simple set of steps to follow when you sit down to think about a problem you're solving by implementing one—or many—endpoints. After we describe those steps, we will move to a set of examples based on our sample project that will illustrate our thought process in building some APIs.\n\nGather Requirements The first thing you need describe when thinking about writing a new API is a use case that describes the functionality required. We use the term use case informally; we really want to come up with a nontechnical description of the business functionality required, ideally with no hints at implementation or technical constraint.\n\nA sample use case can be as simple as this: “As a User, I want to be able to save a file to storage for later\n\nretrieval.” This description of the functionality tells us who the main actor/participant is (User), the action he wants to take (save a file), and the input to the function (the file).\n\n8The p90 value is the number of milliseconds latency in which 90% of the requests reaching the API are serviced. In other words, no more than 10% of all requests take longer for the service to process and respond to.\n\n43\n\nChapter 4 ■ Designing Your apis\n\nThe approach just described is known as a contract-first design style, which means we completely ignored all aspects of implementation and focused on what the API should do at a conceptual level and what inputs and outputs it requires. After you understand the requirements of your API, follow these steps to deliver it:\n\n\n\nPick one use case you understand well.\n\nDefine one API that fulfills an aspect of that use case by describing what it does, its inputs and outputs, and its error cases.\n\n\n\nPublish the API using a known protocol to your team members, and to the team that will consume your APIs. Your design approval process can vary from formal to informal based on the characteristics of the teams involved in the process.\n\nOnce the API documentation is vetted, it is ready to be implemented, and then verified via client integration.\n\nUsing this process, you will have involved multiple teams/individuals in helping you design your API, and you can start implementation knowing that you didn't miss obvious inputs/outputs, speeding up your overall development effort.\n\nMaximize Information Hiding\n\nIn software development, a leaky abstraction is an abstraction that exposes to its users details and limitations of its underlying implementation that should ideally be hidden away.\n\n—Wikipedia9\n\nOne of the main advantages of exposing functionality via APIs is that you provide a public interface to\n\na functionality—described by the API name and its parameters list, and are able to make changes to your implementation without having to disrupt your API’s clients; therefore, APIs are an “information hiding mechanism” for the code you are sharing via APIs. With that in mind, it becomes very important that you only expose enough attributes to make your response fulfil its goal, and nothing more.\n\nSome frameworks make it easy to generate API responses by serializing all attributes of a model in a JSON response by default; this seems to save the developer time deciding what attributes should be exposed, but it turns out to be an anti-pattern. In the long run, those easy-to-generate responses end up restricting the API developer when changes to the API implementation require database schema tweaks, which could break the actual API. Make sure that your APIs expose only attributes that are obviously useful to your clients, and make an effort not to expose artifacts that are too tied to your implementation, which creates what we call “leaky abstractions.” Another drawback is that clients will come to rely on implementation details you didn't need—or mean—to expose; you will have a hard time requesting clients to stop using those attributes, or will have to support them forever.\n\nThe rule of thumb for API design is to “hide” as much information as possible, and only expose data that is needed by clients, by using a vocabulary that is a level above the one you use in your implementation. Using our bookstore, a very simple example of this is a feature of the book detail API called has_errata?, which initially has the following Ruby implementation:\n\ndef has_errata? confirmed_errata.any? end\n\n9“Leaky abstraction,” Wikipedia, referenced on May 2, 2016. https://en.wikipedia.org/wiki/Leaky_abstraction\n\n44\n\nChapter 4 ■ Designing Your apis\n\nIn this case, we could have taken a shortcut and exposed an errata array with all the data we had and let the API users figure out whether a book has any errata, which would have exposed our implementation to outside clients; to avoid this, stick to exposing an attribute called has_errata? which helps clients extract the same information without exposing too much information. If you were to expose your raw data—the errata array— then in the future, when you want your errata entries to have a status that could be one of “submitted,” “verified,” “invalid,” or “corrected,” this would cause issues with clients that rely on any record in errata to mean that a book had errors; so, exposing your errata array broadly is a design mistake that you should avoid.\n\nBy this point, you're starting to understand that consistency is a key aspect of good API design. Here are some aspects of your API where consistency means that there will be no surprises, and the development team will be able to make progress without guessing how each API behaves.\n\nWhen we talk about RESTful APIs, we already picture “JSON over HTTP” described with a verb and a resource. One thing that REST does not describe is how to standardize requests and responses, which will cause developers to be inconsistent across different services and endpoints, causing confusion in day-to- day development when, for example, the set of parameters required to query service A differ from those for service B, or one service returns an array while another returns a hash with an element called data.\n\nAPIs Change The one aspect of API design that is a constant is that APIs will change over time, and often during your development process. Like any other piece of software, APIs are never considered “complete” until the day they are retired.\n\nAcknowledge and Accept Changes\n\nThe Agile Manifesto10—like its practices—is widely accepted in our industry; it has the following principle that also applies well to APIs:\n\nWelcome changing requirements, even late in development. Agile processes harness change for the customer's competitive advantage.\n\nAll code bases go through many evolution steps during development, even when requirements are not a moving target. As soon as one accepts the fact that some code changes will happen over time, and in many layers, then changing an API just becomes part of the job of owning a service. As we mentioned before, APIs describe a contract that has to be followed for data exchange. Changes may sometimes break that contract in incompatible ways. Those are the cases where an API change needs to be thought out extremely well because not only is a service changing, but it's also requiring its clients to make changes.\n\nNonbreaking changes are API changes that, when deployed, will not break the expected behavior for\n\nexisting clients. Nonbreaking changes happen, for example, when one or more data attributes are added to an API response. If your API uses JSON to describe its responses, then adding attributes to clients that are not expecting them will have no effect on how your clients are accessing your API.\n\nBreaking changes are API changes that, when deployed, cause failures for existing clients. Some of the\n\nreasons that can cause breaking changes are:\n\nChange in the endpoint URL, which will cause clients to get 404 errors\n\nChange in the list of required input, which will cause client requests to fail for the lack of that attribute\n\n10“Principles behind the Agile Manifesto,”. http://agilemanifesto.org/principles.html\n\n45\n\nChapter 4 ■ Designing Your apis\n\nChange in the format of the response, which will cause client requests to fail because the previously expected data structure is no longer available\n\nDepending on the size of your development team and the scope of your breaking changes, you can possibly update all clients in a short time, and not cause a lot of disruption; however, by the same token, you could require changes in applications that belong to other busy teams, and it may take months until your new API is used by all its intended audience.\n\nThe Development and Communication Process for Breaking Changes\n\nWhen possible, breaking changes should be avoided because they cause disruption of existing client calls, which may require a nontrivial amount of time to be updated across possibly many clients. When breaking changes cannot be avoided, it's crucial that you have a process to communicate that those changes are coming to your clients. Assuming that your clients can still use the current version of your API, a simple process to track API changes should include these:\n\nDocumentation about the change, explaining exactly what has changed, how important it is that your clients move to the updated API, and when you'd expect all clients to migrate.\n\n\n\nTimelines for when the new version of the API will be available for testing and in other environments, such as staging and production.\n\n\n\nIdeally, some sample code that communicates how the change in the API can be consumed. This is especially valuable in an organization that standardizes in a specific technology, where all API clients will benefit from seeing code changes.\n\nIn small companies, it's possible that developers who are making breaking changes are expected to also\n\nchange the code of the clients that consume those APIs. This is efficient when you have a small team; so, if your environment allows you to do so, it may be part of your process for introducing breaking changes.\n\nAdd a New API\n\nWhen a breaking change alters some of the fundamentals of how your existing API behaves, it may make sense to explain—and expose—your API change as a new API altogether. This can be seen when large companies release new versions of the APIs/libraries that completely break backward-compatibility with the existing version. As software evolves, some large refactoring may be unavoidable, making the new version of the API look like a new API, and you should consider doing that in cases where it’s likely that you will need to make changes in various endpoints because of design decisions taken in early stages of development. Of course, creating a new API—or set of APIs—is a very disrupting type of breaking change, and should be avoided when possible.\n\nVersion Your APIs\n\nAs we’ve said before, change happens! Over time, new requirements for your service will arise. Causing breakages in existing client applications—whether internal or external—can and should be avoided completely. In the vast majority of cases, upgrading your service and all its clients in lock-step is simply not possible.\n\n46\n\nChapter 4 ■ Designing Your apis\n\nThis situation is best addressed by versioning your APIs. Put very simply, you can bump the version\n\nnumber of existing API endpoints that change. If you continue supporting the previous version of such API endpoints ( “API V1”), then current clients will continue to function unchanged, while new clients can choose to call the updated API endpoints at the new version, API V2.\n\nNegotiating the API Version\n\nClients need to communicate to the service which version of an API resource they would like to receive, while the service needs to indicate which version it serves up. There are many options for specifying a version for the API. Let’s look at the most common approaches we have seen used.\n\nUsing HTTP Headers\n\nIn this approach, the negotiations happen based on HTTP headers. The most HTTP standard-compliant way to do this is to use the Accept and Content-Type headers to request, and respond with, a specific version of an API resource.\n\nThe requesting client will specify the acceptable content type and version in an Accept header, for\n\nexample:\n\nAccept: application/vnd.books.com; version=2 GET /books\n\nThe service will then include an appropriate version of the result in its response, indicating the type and\n\nversion in the Content-Type response header, as shown here:\n\nHTTP/1.1 200 Success Date: Wed, 26 Aug 2016 06:25:24 GMT ... Content-Type: application/vnd.books.com; version=2\n\nA variation of this versioning strategy is to accept a nonstandard, application-specific header in place of the Accept header in requests destined for your service. One example for accepting (and including in service responses) is to support a header like, for example, X-BookStore-API-Version:\n\nX-BookStore-API-Version: 2 GET /books\n\nThe service response could then look like this:\n\nHTTP/1.1 200 Success Date: Wed, 26 Aug 2016 06:25:24 GMT ... X-BookStore-API-Version: 2 Content-Type: application/json\n\nUsing HTTP headers for versioning is the way most REST “purists” will probably choose. The argument\n\nfor this is that the version of a resource should not be part of the resource’s URI, as it is in essence still the same resource that is addressed. A disadvantage of this approach is the loss of convenience in quickly testing an API endpoint from a browser, as most browsers need additional plugins or extensions to allow for specifying HTTP request headers.\n\n47\n\nChapter 4 ■ Designing Your apis\n\nUsing the Request Path\n\nSome APIs accept the version specification as part of their endpoint’s URLs, usually as a prefix before specifying the resource in the path. When translated to our bookstore example, the request to the list of all books in version 2 would look like this:\n\nGET /v2/books\n\nWhile it’s not uncontroversial, many API designers usually use this approach because of its simplicity,\n\ncompatibility with browsers, and intuitiveness for handling of versions. If your web application framework’s request routing mechanism supports it, we suggest that you at least consider allowing for specifying the resource version as part of the request path, in addition to relying on HTTP headers as discussed earlier.\n\nUsing a Query Parameter\n\nAnother approach to specify the requested version is to accept it as a query parameter, as shown here:\n\nGET /books?version=2\n\nSome API designers choose this approach, but we do not recommend it. We have found that this strategy seems the least intuitive of the approaches described here. Using a query parameter for the version of a resource muddies the waters around other query parameters that are better placed there, such as representations, search terms, and the like.\n\nDetermining What Numbering Scheme to Use\n\nFor the concrete values that signify a list of versions, most implementations we have observed use either consecutive integers (v1, v2, v3, …), or dates (v20160424, v20150811, …). Another popular software versioning scheme is semantic versioning (http://semver.org/), but we have rarely seen it in use for web- based APIs.\n\nWhile the details can certainly cater to personal preferences, we encourage using something that allows\n\nfor easy and efficient analysis by the server to evaluate which API is the later version. In other words, we discourage using schemes like v_new, v_old, vXmasEdition, or even v08112015 (as using dates in the ISO 8601 format yields larger integers for later date values).\n\nSupporting and Deprecating API Versions\n\nAny published API can potentially live for longer than its designer. This point is especially true when exposing an API publicly, where third-party entities decide how long they will attempt to use it. But even for private, company-internal APIs, time and thought invested in considering support and deprecation policies prior to launch pays off.\n\nA very important step is to make sure that there is a way to identify your clients for any given version of your API. One approach we have taken to help with this is to require an API key parameter (preferably as a special header) for every API call.11 If you do so, make sure to log such client information. You will find numerous opportunities to use it while your service is in production, even if it is just to perform usage pattern analysis.12\n\n11For internal APIs, the security and authentication features surrounding the authenticity of this API key can be quite relaxed; that is, clients can usually be trusted to reliably self-identify. More stringent rules for authentication, authoriza- tion, expiry, and encryption need to apply for externally accessible API keys. 12We will discuss aspects of monitoring your services in more detail in Chapter 13.\n\n48\n\nChapter 4 ■ Designing Your apis\n\nYou should also think about how to contact and notify your clients about any updates to your APIs. Do\n\nyou have email addresses or other reliable contact details for all owners of client applications? Internal to your company you could use a simple mailing list for communicating changes, bug fixes, and so on, that are part of a new API version.\n\nAlso, be sure to publish your API’s deprecation and support policy alongside any API release. That way,\n\nclients can plan ahead, and you are in a better position to tell them to get off a version for which you are about to drop support. Teams that strive to support no more than two versions of the same API, where an outdated version is only available for a small number of weeks after the updated version has been released, can focus more time and resources on development tasks.13\n\nFor internal APIs, be sure to set expectations regarding which team is responsible for upgrading the\n\nclient applications when new (potentially breaking) API changes occur. Is the client application team responsible for acting on it, or will the service-owning team be on the hook?14\n\nDesigning / Splitting Up by Example In this section, we will take a practical approach to API design. We'll talk about two major interaction points of our bookstore project, and will describe some of the features of a couple of its pages, to help us identify the attributes that each page requires to be properly rendered. We'll then group those attributes and talk about the services and endpoints we should hit in each case, and we’ll design a few request/response examples that will hopefully help drive home some of the concepts we touched on in this chapter.\n\nFor all examples in our sample project, we decided to adopt the JSON API hypermedia type, so some of\n\nthe characteristics of our responses and requests are defined by that specification.\n\nThe Bookstore Home Page The bookstore home page has a fairly complex set of features distributed across some of our services (see Figure 4-1). In this section, we will describe those features and talk about the API calls needed to properly render the home page.\n\n13Some companies, like Netflix, on the other hand opt to keep old versions of internal APIs around until no client can be identified as using them. See Chapter 1 for more details on the Netflix approach. 14We discuss this and other topics of operational support in a microservices environment at length in Chapter 14.\n\n49",
      "page_number": 58
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 68-75)",
      "start_page": 68,
      "end_page": 75,
      "detection_method": "topic_boundary",
      "content": "Chapter 4 ■ Designing Your apis\n\nFigure 4-1. The sample application home page\n\nCart Information\n\nThe cart widget, which contains the price and number of elements added to the cart, appears in most pages of the web application. We have decided that carts are saved on a per-user basis, and therefore require the widget’s clients to pass in a UUID representing the current user, who could be logged in or not. In order to retrieve the user_id, we will ask users-service for the ID of the current user, which can be extracted from a secure cookie holding the user's authentication token.\n\nGET /users?token=6BbW0pxO0YENxn38HMUbcQ== Accept: application/vnd.api+json\n\nThe response returns a match that lists some information about the user who matched the token we\n\npassed in as a parameter.\n\n50\n\nChapter 4 ■ Designing Your apis\n\n{ \"links\": { \"self\": \"/users/d83d6c58-e05a-43a7-8d58-5995f68e1ba9\" }, \"data\": { \"type\": \"users\", \"id\": \"d83d6c58-e05a-43a7-8d58-5995f68e1ba9\", attributes: { \"name\": \"Cloves Carneiro Jr\", \"email\": \"cloves.carneiro@example.org\", ... \"phone\": \"\", } } }\n\nNow that we have a user ID handy, we can go back to cart-service and request the carts detail information, but we don't need all the details of the cart; we just want a really small subset of its attributes, the current total price, and number of products. The JSON API specification has a feature called sparse fieldsets, which states that an endpoint can return only specific fields in the response on a per-type basis by including a fields[TYPE] parameter:\n\nGET /cart?user_id=d83d6c58-e05a-43a7-8d58-5995f68e1ba9&fields[carts]=total_price,products_count Accept: application/vnd.api+json\n\nThe response returns just the data we requested, increasing efficiency both on the client and on the server.\n\n{ \"links\": { \"self\": \"/carts/178c3c65-16ee-46e8-8931-cdef1fb0568c\" }, \"data\": { \"type\": \"carts\", \"id\": \"178c3c65-16ee-46e8-8931-cdef1fb0568c\", attributes: { \"total_price\": 2259, \"products_count\": 1, } } }\n\nUser Recommendations\n\nIt's common that e-commerce landing pages display a set of products that are recommended to the logged- in user. Our proposed recommendations-service takes as parameters the user_id of the currently logged- in user and the cart_id, so that it can look up recommendations based on the user's previous purchases and current items in cart. Although recommendations-service sounds complex, it knows how to do its job and requires only a simple API call with the parameters we mentioned. We also know that we only want to display four recommendations in our home page, so we pass in page[size] with a value of 4.\n\n51\n\nChapter 4 ■ Designing Your apis\n\nGET /recommendations?user_id=d83d6c58-e05a-43a7-8d58-5995f68e1ba9&cart_id=178c3c65-16ee- 46e8-8931-cdef1fb0568c&page[size]=4 Accept: application/vnd.api+json\n\n{ \"links\": { \"self\": \"/recommendations?used_id=d83d6c58-e05a-43a7-8d58-5995f68e1ba9&cart_id=178c3c65-\n\n16ee-46e8-8931-cdef1fb0568c&page[size]=4\"\n\n\"next\": \"/recommendations?used_id=d83d6c58-e05a-43a7-8d58-5995f68e1ba9&cart_id=178c3c65-\n\n16ee-46e8-8931-cdef1fb0568c&page[size]=4&page[number]=2\",\n\n}, \"data\": { \"type\": \"books\", \"id\": \"5ea3d5c2-13a7-4282-9d33-693ae38c48e0\", attributes: { \"title\": \"Beginning Rails 4 (Expert's Voice in Web Development)\", }, \"relationships\": [ \"main_author\": { \"links\": {\"self\": \"http://books-service.books.com/authors/ec84275e-0e8a-495c-9ba7-\n\n68e791b0fad7\"\"}\n\n\"data\": {\"type\": \"authors\", \"id\": \"ec84275e-0e8a-495c-9ba7-68e791b0fad7\"} \"attributes\": {\"name\": \"Cloves Carneiro Jr\"} }, \"cover\": { \"links\": {\"self\": \"http://images-service.books.com/images/e54c65b6-2916-4573-a494-\n\n1e71188ae754\"}\n\n\"data\": {\"type\": \"images\", \"id\": \"e54c65b6-2916-4573-a494-1e71188ae754\"} \"attributes\": { \"file_name_url\": \"https://bucket.s3.amazonaws.com/book/5ea3d5c2/c421cdd8-865f-\n\n4bc6-8db5-107ccb1d4459.jpeg\"\n\n} } ] } }\n\nThe response served by recommendations-service provides books resources with a limited set of book attributes, only enough to display basic information about a book, which the user can then click on to get to a book detail page.\n\nThe Book Detail Page Let's now think about the book detail page of our sample application. It has most of the features customers would expect from a bookstore website; some of the obvious pieces of information it needs to render are the actual book details, all images for a book, and book reviews (see Figure 4-2).\n\n52\n\nChapter 4 ■ Designing Your apis\n\nFigure 4-2. The sample application’s book detail page\n\nBook Details\n\nIn the context of the home page, books-service returned a list of books displaying a limited number of attributes for each book, because at that level, there wasn't a requirement for displaying much more than the book title, author, and image. In this case, the book detail page takes the opposite approach; its job is to display as much information as possible about a book to help customers make educated purchase decisions; thus, we can say that we will need all the book data available in that service.\n\nWe've used a feature we call representations to define a well-known set of attributes with a name that conveys the type of data that will be returned. In this case, we will be requesting the “full” representation view of a book detail API endpoint. It will provide the following attributes:\n\n\n\nTitle\n\n\n\nISBN\n\n\n\nAuthor(s)\n\n\n\nExcerpt\n\nDescription\n\n53\n\nChapter 4 ■ Designing Your apis\n\n\n\nTable of contents (TOC)\n\n\n\nPage count\n\n\n\nLanguage\n\n\n\nPublisher\n\n\n\nEdition\n\n\n\nRelease date\n\nDimensions\n\nWeight\n\n\n\nPrice\n\nOriginal price (if the current price is promotional)\n\n\n\nFormats (paperback, hardcover, ebook)\n\n\n\nTop category\n\n\n\nSub-category\n\n\n\nTags\n\nThe book details endpoint will be a resource identified by the URI /books/{isbn}, where isbn will be the ISBN that uniquely identifies each book; so, when a client application needs to access the detail endpoint for a book, it will send the following HTTP request:\n\nGET /books/978-1430260349?representation=full Accept: application/vnd.api+json\n\nIt will receive a response that—using the JSON API hypermedia type—looks like this:\n\n{ \"links\": { \"self\": \"http://example.com/books/978-1430260349\" }, \"data\": { \"type\": \"books\", \"id\": \"5ea3d5c2-13a7-4282-9d33-693ae38c48e0\", attributes: { \"title\": \"Beginning Rails 4 (Expert's Voice in Web Development)\", \"page_count\": 232, \"excerpt\": \"\", \"description\": \"\", ... \"tags\": [] },\n\n54\n\nChapter 4 ■ Designing Your apis\n\n\"relationships\": [ \"authors\": [ { \"links\": {\"self\": \"http://example.com/authors/UUID\"} \"data\": {\"type\": \"author\", \"id\": \"UUID\"} \"attributes\": {\"name\": \"Cloves Carneiro Jr\"} } ], ] } }\n\nYou see that the top-level data element is what describes the book—the resource—we are interested in.\n\nIt's a JSON API requirement that responses have a data element. It can include a hash, as in this case, or an array when the response describes multiple resources instead of one, which is the case with the reviews and image requests this page will make.\n\nSome of the resources associated with the book are returned in that response—all under\n\nrelationships, as is the case of the authors array, which returns us enough information to display a list of authors, and links to URLs where we can load more details about those authors when necessary.\n\nReviews\n\nThe same book details page wants to list a small number of highly relevant reviews associated with the book the customer is looking at. For that, it will request that data from the reviews-service by sending an HTTP request to:\n\nGET /reviews/5ea3d5c2-13a7-4282-9d33-693ae38c48e0?page[size]=3 Accept: application/vnd.api+json\n\nYou can see that, in this request, we requested only the first three reviews associated with the book by using the page[size] parameter to limit the number of resources we want to see. We assume that the reviews-service will default to its most important sort order, and hence have not provided an explicit value for the sort parameter.\n\n{ \"links\": { \"self\": \"/reviews/5ea3d5c2-13a7-4282-9d33-693ae38c48e0?page[size]=3\", \"next\": \"/reviews/5ea3d5c2-13a7-4282-9d33-693ae38c48e0?page[size]=3&page[number]=2\", }, \"data\": [ { \"type\": \"reviews\", \"id\": \"UUID\", \"attributes\": { \"title\": \"Everything you need as a Rails programmer\", \"rating\": 5, \"name\": \"Jane Doe\", \"content\": \"Lorem ipsum ...\" } },\n\n55\n\nChapter 4 ■ Designing Your apis\n\n{ \"type\": \"reviews\", \"id\": \"UUID\", \"attributes\": { \"title\": \"When learning is a pleasure\", \"rating\": 5, \"name\": \"Martin Z\", \"content\": \"Lorem ipsum ...\" } }, ], \"meta\": { \"records\": 22 } }\n\nThat is a response that packs a ton of information, and it displays some of the great features of the JSON\n\nAPI. In this response, we have:\n\n\n\nThe list of reviews we requested, under data\n\n\n\nThe location where to get the next page of reviews, in links#next\n\n\n\nThe total number of review that match our request, in meta#records\n\nFor books that do not have any reviews, the only difference in the response would be an empty data\n\narray:\n\n{ \"links\": { \"self\": \"/reviews/5ea3d5c2-13a7-4282-9d33-693ae38c48e0?page[size]=3\", }, \"data\": [] }\n\nImages\n\nThe same book details page needs to display an image for each available version of the book the customer is looking at. For that, it will request that data from the images-service by sending an HTTP request to:\n\nGET /images/5ea3d5c2-13a7-4282-9d33-693ae38c48e0 Accept: application/vnd.api+json\n\n{ \"links\": { \"self\": \"/images/5ea3d5c2-13a7-4282-9d33-693ae38c48e0\" },\n\n56\n\nChapter 4 ■ Designing Your apis\n\n\"data\": [ { \"type\": \"images\", \"id\": \"UUID\", attributes: { \"file_name_url\": \"https://bucket.s3.amazonaws.com/book/5ea3d5c2/a1d4efb5-7908-4683-\n\na1b9-20438c16eedb.jpeg\",\n\n} }, { \"type\": \"images\", \"id\": \"UUID\", attributes: { \"file_name_url\": \"https://bucket.s3.amazonaws.com/book/5ea3d5c2/c421cdd8-865f-4bc6-\n\n8db5-107ccb1d4459.jpeg\",\n\n} } ] }\n\nIn this response, we can see that there are two images associated with the book, which we can display in\n\nthe book detail page.\n\nAs you can see, we were able to load enough data to render all the details needed in the book detail\n\npage by calling three services, each responsible for a type of data associated with the book.\n\nSummary In this chapter, we covered a lot of ground related to designing APIs. We started by stating the importance of good API design, and listed a set of principles we think are important for your API process. We touched on the fact that you also need to think about your API’s nonfunctional requirements and listed a set of properties to consider during your design process. Tips on how to deal with breaking changes and API version control were explained next. We ended the chapter with some hands-on examples of how we defined APIs in our sample book project, describing some of the pages that customers see and endpoints that are hit to render those pages.\n\nIn the next chapter, we will go into more detail about implementing APIs. We will introduce approaches\n\nand tools for describing your services' APIs in a machine-readable format, by introducing the Swagger specification and some of its toolset.\n\n57",
      "page_number": 68
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 76-84)",
      "start_page": 76,
      "end_page": 84,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 5\n\nDefining APIs\n\nThis chapter will introduce approaches and tools for describing your services' APIs in a machine-readable format. First we will introduce the concepts behind Interface Definition Languages (IDLs) and will briefly introduce a number of such languages. Next, we will introduce the Swagger IDL framework and its associated tool chain in more detail, and we will conclude the chapter by illustrating how to use Swagger to define the APIs of one of the services in our bookstore application.\n\nInterface Definition Languages (IDLs) An IDL is a language for specifying any software component's API in a format that is easy for computers to process. The size of the component described in an IDL can vary widely; some IDLs describe the APIs of a single object, whereas others aim at defining the APIs of an entire application.\n\nThere are many advantages in describing your APIs in such a manner. As shown in the previous chapter, an iterative definition process that involves early drafting, sharing, and soliciting and incorporating feedback helps to increase the chances that your API will be useful to its clients.\n\nAnother advantageous trait of IDLs is that they are usually platform- and language-independent; clients of the APIs do not need to be implemented on top of the same operating system, or even in the same programming language as the service providing the APIs.\n\nThe biggest selling point in our eyes for the use of a machine-readable IDL revolves about the possibilities for developing a supporting chain of tools. Automatic processing of API definitions means that tools for generating client and/or service stub code in many programming languages can be created. Additionally, code for the validation of client requests and service responses based on the IDL specifications can be introduced. Some IDLs also come with tools for generating documentation, or with automatically generated web sites that let you explore the APIs based on their definition.\n\nThe first implementations of Interface Defining Languages date back to the early to mid-1980s,\n\nwhen Sun Microsystems introduced the Remote Procedure Call protocol. Sun’s first version of what is now called Open Network Computing RPC was specified in RFC 1057 and adopted in 1988. The style of defining interprocess communication in ONC RPC was firmly rooted in the Unix operating system and C programming language traditions. To exchange data between clients and services, ONC RPC used the External Data Representation (EDR) serialization protocol. This standardization strategy enabled even this very early IDL to develop a tool chain that included rpcgen for generating C language client libraries and service stubs, which were available across multiple operating systems (Unix derivatives and Microsoft Windows).\n\nThe list of IDLs used in the industry has grown large since these early beginnings. Some IDL frameworks (like Apache Thrift or Google’s Protocol Buffers) introduce special binary formats for transmitting data over the wire, favoring terseness and the resulting speed of transmission and decoding. Other IDLs (for example, JSON-based approaches like Swagger or API Blueprint) favor text-based protocols because of their human-readable format, and despite their implicit inefficiencies the growing availability of mass storage and bandwidth has made such protocols gain large popularity.\n\n59\n\nChapter 5 ■ Defining apis\n\nA Matter of Style: RPC versus REST Most IDLs can be classified into two categories based on which architectural style of interprocess communications they most naturally support: Remote Procedure Calls (RPC) or REpresentational State Transfer (REST).\n\nWe will not go into detail about the specific differences between RPC and REST. For our purposes here, it is sufficient to say that we consider RPC a style that focuses mostly on traditional object-oriented method invocation and message passing. The approach is usually implemented via invoking a method on an object stub that exists in the local process address space of the client. Under the hood, this local invocation triggers a network call to a remote service, in whose process the actual object method is executed instead. The main point is to make the remote aspect of the procedure execution as transparent as feasibly possible.\n\nIn a RESTfully styled API, a set of particular constraints is imposed on the client and the service. Here,\n\nclients can retrieve information about, and perform a very limit set of actions (also known as verbs) on, a set of resources that are exposed by the service’s APIs. Services provide clients with multiple representations of the API-defined resources, and the client can use the action verbs either to retrieve a particular new representation of a resource, or to manipulate the resources. Additionally, communication between clients and the service is stateless. In almost all practical implementations, RESTful APIs are implemented on top of the HTTP protocol, which itself can be seen as the prototypical implementation of the REST architectural style; its set of methods (GET, PUT, POST, PATCH, DELETE, and so on) can be seen as the verbs that are applied to resources, identified via Uniform Resource Identifiers (URIs).\n\nRPC-Style Interface Definition Languages The first IDLs available all followed the RPC style, focusing mostly on allowing for invoking methods on remote objects. In addition to the already-mentioned ONC RPC, another early RPC IDL was the Common Object Request Broker Architecture (CORBA). This architecture, standardized by the Object Management Group (OMG), was a big step forward from ONC RPC for usage of an IDL outside a LAN and in more polyglot environments that demanded language and OS interoperability. Later, technologies like XML-RPC and WSDL, and their close younger cousin JSON-RPC, implemented remote method calling mechanisms on top of the HTTP protocol. Today, technologies like Apache Thrift (originally developed by Facebook), Google's Protocol Buffers,1 and Apache Avro2 have entered the arena of RPC IDLs, and each is used in production by many large companies.\n\nRESTful Interface Definition Languages RESTful IDLs probably started when Sun Microsystems first drafted the Web Application Description Language (WADL) in late 2005. WADL lets you describe REST APIs using an XML-based language, and much like WSDL, which added REST support with its version 2.0, WADL is mainly used in the Java ecosystem, especially in the JAX-RS APIs and its Jersey reference implementation. Another XML-based API definition language that developed fairly recently is RESTful Service Definition Language (RSDL), which is trying to improve on some drawbacks that practitioners found with WADL (mainly the perceived tight coupling between the service and the client caused by insufficient support for hypermedia-driven design).\n\n1https://developers.google.com/protocol-buffers/ 2http://avro.apache.org/\n\n60\n\nChapter 5 ■ Defining apis\n\nOther, more modern and less Java ecosystem-centric IDLs seem to find the use of XML to express\n\nthe API specifications too cumbersome and verbose. One such example is the RESTful API Modeling Language (RAML),3 which expresses API specifications in YAML, and which has an increasingly large set of community-maintained tools surrounding it. API Blueprint4 is a similarly active IDL project, which chose a Markdown dialect as the language in which to write down API descriptions.\n\nAnother notable modern format in which to describe your APIs is JSON, and a significant number\n\nof RESTful IDLs specify service endpoints using it. An important part of such API descriptions is the specification of returned and accepted data types, and JSON Schema5 currently seems to be crystalizing as the common approach for this. The main advantage that JSON Schema appears to be providing is that there is a well-defined, community-supported specification format that has found wide adoption amongst RESTful IDLs.\n\nCloud application platform provider Heroku describes in a 2014 blog post their extensive open- source RESTful IDL tool chain, using JSON Schema as a central technology. Similar to what Heroku’s API Design Guide6 aims at, the JSON-API project tries to codify guidelines about sane choices for API design conventions. The aim is to foster consistency and similarity between APIs described using JSON Schema.\n\nProbably the most mature JSON Schema-compatible IDL framework in existence today is Swagger. Swagger has recently been donated to a consortium called the OpenAPI Initiative7 and has hence been renamed the OpenAPI Specification. Swagger sports an impressive tool chain, some of which we will be exploring in the next section of this chapter.\n\nOur Stance To conclude this section we would like to mention that we have taken sides and decided to design most of our service APIs in a RESTful manner. The REST approach seems more natural to the world of microservices, in which APIs are designed around resources, reachable via the (RESTful) HTTP protocol, and owned by systems of record and authority. This style also appears to make the undeniable fact more obvious that in-process object method invocations are different in many ways from remote, cross-network boundary service calls; external calls are much less reliable, have more failure scenarios, and expose very different performance characteristics, all of which should have an impact on API design.\n\nDescribing APIs using Swagger As explained in the general section on IDLs, the main benefit is that once you have defined your API operations, a computer can automatically process the IDL definitions to produce tooling for you: it can generate documentation from that IDL, it can generate a complete test suite that exhaustively exercises your API endpoints, it can generate load testing scripts for the endpoints, and handle many similar tasks.\n\nAs mentioned in the previous section, Swagger is focused on specifying RESTful web services. The tool chain around Swagger has matured greatly in recent years, and the framework now helps not only with describing, but also with producing, consuming, visualizing, and testing RESTful web services in a multitude of programming languages.\n\nWhile the main focus is surely on allowing computers to understand and discover the service APIs, the\n\ntooling also makes it easier for humans to comprehend and explore the described service.\n\n3http://raml.org/ 4https://apiblueprint.org/ 5http://json-schema.org/ 6https://github.com/interagent/http-api-design 7https://openapis.org/\n\n61\n\nChapter 5 ■ Defining apis\n\nHosting Swagger Specification as JSON Schemas Let's start by looking at the concrete example of the JSON Schema definitions of one of the sample services' APIs shown in Listing 5-1—the users-service hosts this specification itself at the /swagger/api path.\n\n■ Note show you tools later in this chapter that help you with JsOn schema generation and upkeep.\n\nalthough it is of course possible to produce and maintain these api definitions manually, we will\n\nListing 5-1. Swagger API JSON specification of the users-service\n\ntim$ curl http://localhost:5002/swagger/api | jsonpp { \"swagger\": \"2.0\", \"info\": { \"title\": \"Users-Service\", \"description\": \"Service that owns the concern of user accounts for the bookstore\", \"version\": \"1.0\" }, \"host\": \"localhost:5002\", \"basePath\": \"/\", \"paths\": { \"/users\": { \"post\": { \"tags\": [ \"Users API\" ], \"operationId\": \"Users API-create\", \"parameters\": [ { \"name\": \"email\", \"description\": \"Email address for the new user, needs to be unique.\", \"required\": true, \"in\": \"query\", \"type\": \"string\" }, { \"name\": \"name\", \"description\": \"Full name of the user.\", \"required\": false, \"in\": \"query\", \"type\": \"string\" }, { \"name\": \"password\", \"description\": \"Password for the user.\", \"required\": true,\n\n62\n\n\"in\": \"query\", \"type\": \"string\" } ], \"responses\": { \"default\": { \"description\": \"\", \"schema\": { \"$ref\": \"#/definitions/User\" } }, \"422\": { \"description\": \"User data invalid\" } }, \"description\": \"Creates a new user for the given data\", \"summary\": \"Creates a new user for the given data\", \"x-controller\": \"users\", \"x-action\": \"create\" }, \"get\": { \"tags\": [ \"Users API\" ], \"operationId\": \"Users API-index\", \"parameters\": [], \"responses\": { \"default\": { \"description\": \"\", \"schema\": { \"type\": \"array\", \"items\": { \"$ref\": \"#/definitions/User\" } } } }, \"description\": \"Returns all users' data\", \"summary\": \"Returns all users' data\", \"x-controller\": \"users\", \"x-action\": \"index\" } }, \"/users/{id}\": { \"get\": { \"tags\": [ \"Users API\" ],\n\nChapter 5 ■ Defining apis\n\n63\n\nChapter 5 ■ Defining apis\n\n\"operationId\": \"Users API-show\", \"parameters\": [ { \"name\": \"id\", \"description\": \"Scope response to id\", \"required\": true, \"in\": \"path\", \"type\": \"string\" } ], \"responses\": { \"default\": { \"description\": \"\", \"schema\": { \"$ref\": \"#/definitions/User\" } }, \"404\": { \"description\": \"User not found\" } }, \"description\": \"Returns a user's data for a given user by uuid\", \"summary\": \"Returns a user's data for a given user by uuid\", \"x-controller\": \"users\", \"x-action\": \"show\" } } }, \"definitions\": { \"User\": { \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"string\", \"description\": \"UUID for the user's ID\" }, \"name\": { \"type\": \"string\", \"description\": \"the name for user\" }, \"email\": { \"type\": \"string\", \"description\": \"user's email address\" } }, \"required\": [ \"user_id\", \"email\" ] } },\n\n64\n\nChapter 5 ■ Defining apis\n\n\"tags\": [ { \"name\": \"Users API\", \"description\": \"This document describes the API for creating and reading user\n\naccounts.\"\n\n} ], \"securityDefinitions\": {} }\n\nAfter some initial declarations about the Swagger version used, and general information about the users\n\nservice, the host and base path for reaching its APIs are declared.\n\nWhat follows is the top-level paths key, where the value is another JSON hash in which each entry represents one of the resources exposed by the users service. This simple service only exposes two resources called /users (for the general list of all accounts for the bookstore application), as well as /users/{id} (which is a parameterized resource that refers to each single user account).\n\nBefore we dig deeper into the resources, let’s look at the next top-level entry, called definitions. The\n\ndefinitions key holds a hash with an entry for each API model’s schema definition. Here, the only API model to describe is the User model. As you can see, this model consists of two required string attributes (email and user_id), as well as an optional name string.\n\nThe Swagger JSON specifications of our users service hash are concluded by the tags entry, which is a\n\nlist of name and description key-value-pairs for the APIs, as well as a securityDefinitions section that can specify authentication and authorization requirements for the APIs.\n\nGetting back to the resource and paths definitions, we can see that they are organized by the HTTP action verbs they support: you can use an HTTP POST request with required input query parameters email and password, as well as an optional name. The results of this user account creation endpoint are captured in the responses section: the operation returns either the newly created User object, or an error response with a 422 status code when the user data passed in was insufficient.\n\nAfter a short description and summary section, you can find x-controller and x-action. These entries are interesting in that they illustrate that the Swagger JSON-Schema is extensible. In this case, the x- prefixed fields are used by a LivingSocial internal tool to better identify the code sections in a Ruby on Rails application that implements the API endpoint to create a new user.\n\nThe next endpoint defined is the parameterless GET /users, which simply returns an array of all bookstore account holders as User API model objects and is tied to the Ruby on Rails users#index action of the users service.\n\nFinally, the JSON schema defines the GET /users/{id} operation, which takes as an input from the URI’s path the user’s UUID as a string, and returns either the identified user’s data as a User object, or an error that indicates that a user by that UUID cannot be found.\n\nThe Swagger User Interface As mentioned earlier, one of the major selling points of Swagger over other RESTful IDL frameworks is the large list of tools surrounding it.\n\nA real gem in this tool chain is the Swagger UI. Swagger UI is a purely HTML/CSS and JavaScript-based tool that helps to visualize the JSON specifications of an API, as well as to interactively explore the described APIs.\n\nOur users service is hosting the Swagger UI at http://localhost:5002/swagger/doc, shown in\n\nFigure 5-1.\n\n65\n\nChapter 5 ■ Defining apis\n\nFigure 5-1. Swagger UI for users-service\n\nAs you can see in Figure 5-1, the UI provides access to and descriptions of all API endpoints, as well as\n\nAPI models, based entirely on the API specifications hosted locally at http://localhost:5002/swagger/ api.json.\n\nAdditionally, the UI allows for specifying the APIs’ input parameters in a web form, and exposes “Try\n\nit out!” buttons to send requests to the respective endpoint. Each request triggered this way results in displaying the response body, code, and headers in separate sections of the UI. Sample cURL statements and resulting request URLs can also be found on the Swagger UI page.\n\n66\n\nChapter 5 ■ Defining apis\n\nThe schemas of API request and response models are shown right above the section where input parameters are described and can be entered before making requests. Boldface type indicates that the named parameter is required, and there are also definitions about the data type and request location (that is, whether in the request body, the URL path, or the URL query) for each parameter.\n\nAt the end of each entry describing an API endpoint is a list of declared response messages, detailing\n\npotential error codes and conditions, models, and headers.\n\nLet’s look at the results from calling the GET /users/{id} API endpoint via the Swagger UI (Figure 5-2).\n\nFigure 5-2. Swagger UI for GET /iusers/{id}\n\n67",
      "page_number": 76
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 85-94)",
      "start_page": 85,
      "end_page": 94,
      "detection_method": "topic_boundary",
      "content": "Chapter 5 ■ Defining apis\n\nOnce the request triggered by the “Try it out!” button completes, new sections appear. The “Curl”\n\nsection details the exact Unix curl command-line statement equivalent to that made by the Swagger UI.\n\n■ Note it. internal, inside-Vpn-only facing apis should probably at least make sure that an api key is required to authenticate and authorize access. externally-facing apis will need more protection; we recommend Oauth 2.0 via JsOn Web token (JWt). see https://tools.ietf.org/html/rfc7523 for more details.\n\nfor simplicity of illustration, the api exposed here has no authentication or authorization built into\n\nThis new section shows the request URL, a formatted version of the JSON response body, and the response code and headers returned from the service.\n\nSwagger UI is just one of many tools surrounding the framework, and the official Swagger website lists a large number of core, as well as community supported tools. The list is still growing, and it is already too large to explain each one of them in this book. Therefore, we will just focus on two more categories of tools: automatic code generation from Swagger JSON specifications, and generating the JSON schema specification itself based on more human-readable DSLs.\n\nGenerating Code from Swagger Definitions Starting out with code generation, we are using the swagger-codegen tool (https://github.com/swagger- api/swagger-codegen), which is included as a core project in the Swagger toolset. This tool allows automatic generation of client libraries to communicate with APIs based on the service's Swagger API specifications. Another option that the tool provides is to generate an actual service stub to vend the APIs based on a predefined JSON schema specification. Both client and service generation can be configured to generate code in around 35 languages and service frameworks.\n\nLet’s look at an example of how to use this tool to generate a Ruby language gem (Ruby’s equivalent to\n\nother languages’ libraries) for our users service:\n\nswagger-codegen generate -i http://localhost:5002/swagger/api -l ruby -o /tmp/users-client/\n\nThe directory tree of the code generated by this command can be seen in Figure 5-3.\n\n68\n\nChapter 5 ■ Defining apis\n\nFigure 5-3. Directory tree of the Ruby client gem\n\nListing 5-2 demonstrates how this auto-generated client library can be used.\n\nListing 5-2. Using the generated Ruby client gem\n\nusers-client tim$ bundle exec rake console irb -I lib -r swagger_client.rb -r console_defaults.rb 2.3.0 :001 > client = SwaggerClient::UsersAPIApi.new => #<SwaggerClient::UsersAPIApi:0x007fb0c4938b00 @api_client=#<SwaggerClient::ApiClient:0x 007fb0c4938ad8 ...>> 2.3.0 :002 > all_users = client.users_api_index\n\n69\n\nChapter 5 ■ Defining apis\n\n=> [#<SwaggerClient::User:0x007fb0c488b298 @user_id=\"3a07468a-f97d-4242-9057-8437385efb29\", @name=\"Tom Copeland\", @email=\"tom@books.com\">, #<SwaggerClient::User:0x007fb0c488ae10 @user_ id=\"943f3586-8f8e-4513-90ff-9da59e86f013\", @email=\"cloves@books,com\">, #<SwaggerClient::User :0x007fb0c488aa00 @user_id=\"b635e0b4-14db-4737-9672-2b5c678156bb\", @email=\"tim@books.com\">] 2.3.0 :003 > tom = client.users_api_show(all_users.first.user_id) => #<SwaggerClient::User:0x007fb0c3a89230 @user_id=\"3a07468a-f97d-4242-9057-8437385efb29\", @name=\"Tom Copeland\", @email=\"tom@books.com\"> 2.3.0 :004 > tom.to_hash => {:user_id=>\"3a07468a-f97d-4242-9057-8437385efb29\", :name=>\"Tom Copeland\", :email=>\"tom@ books.com\"}\n\nIf you’re familiar with idiomatic Ruby code you can see here that using the client gem is rather easy and\n\nwithout surprises. After creating a client object, you call methods on that client that correspond to the API endpoints, accepting as arguments the parameters laid out in the Swagger JSON specification. The API call results also match auto-generated Ruby classes for the API models from the Swagger specification. We made the complete source of the generated Ruby client available at https://github.com/microservices-from- day-one/user-client.\n\nIf your language of choice is Clojure, the only tweak to generating a client library is the language for the\n\nl option of the swagger-codegen command-line tool:\n\nswagger-codegen generate -i http://localhost:5002/swagger/api -l clojure -o /tmp/users- client-clj/\n\nThe code structure generated looks like Figure 5-4.\n\nFigure 5-4. Directory tree of the Clojure client library\n\nUsing the client code from the Clojure REPL is also very idiomatic, as shown in see Listing 5-3.\n\n70\n\nChapter 5 ■ Defining apis\n\nListing 5-3. Using the generated Clojure library\n\nuser-client-clj tim$ lein repl nREPL server started on port 51209 REPL-y 0.1.10 Clojure 1.7.0 Exit: Control+D or (exit) or (quit) Commands: (user/help) Docs: (doc function-name-here) (find-doc \"part-of-name-here\") Source: (source function-name-here) (user/sourcery function-name-here) Javadoc: (javadoc java-object-or-class-here) Examples from clojuredocs.org: [clojuredocs or cdoc] (user/clojuredocs name-here) (user/clojuredocs \"ns-here\" \"name-here\") user=> (require '[users-service.api.users-api :as u]) nil user=> (u/users-api-index) ({:user_id \"3a07468a-f97d-4242-9057-8437385efb29\", :name \"Tom Copeland\", :email \"tom@ books.com\"} {:user_id \"943f3586-8f8e-4513-90ff-9da59e86f013\", :name nil, :email \"cloves@ books,com\"} {:user_id \"b635e0b4-14db-4737-9672-2b5c678156bb\", :name nil, :email \"tim@books. com\"}) user=> (u/users-api-show \"3a07468a-f97d-4242-9057-8437385efb29\") {:user_id \"3a07468a-f97d-4242-9057-8437385efb29\", :name \"Tom Copeland\", :email \"tom@books. com\"}\n\nAgain, all API endpoints are accessible via unsurprisingly named functions. The choice of data type\n\nfor the User API model is a simple Clojure map, not a record structure, which is also available in the language. We made the complete source of the generated Clojure client available at https://github.com/ microservices-from-day-one/users-client-clj.\n\nGenerating the Swagger JSON Specifications The last category of tools we discuss aims at making it easier to generate the Swagger JSON specifications for an API from within the project that implements the service.\n\nGiven that our main language of choice for implementing the sample services is Ruby, we are showing\n\ntwo Ruby tools for this task.\n\nSwagger-Blocks\n\nThe first tool we would like to demonstrate is swagger-blocks (https://github.com/fotinakis/swagger- blocks/). Including the swagger-blocks gem into your Ruby on Rails lets you specify your API endpoints using a DSL instead of JSON.\n\nListing 5-4 shows sample documentation that describes the UsersController in our users service, and\n\nwhich generates part of the JSON specification shown earlier in this chapter.\n\nListing 5-4. Swagger-blocks description of /users/{id}\n\nclass UsersController < ApplicationController include Swagger::Blocks\n\n71\n\nChapter 5 ■ Defining apis\n\nswagger_path '/users/{id}' do operation :get do key :description, \"Returns a user's data for a given user by uuid\" key :operationId, 'Users API-show' key :tags, [ 'Users API' ] key :summary, \"Returns a user's data for a given user by uuid\", key :'x-controller', \"users\" key :'x-action', \"show\" parameter do key :name, :id key :in, :path key :description, 'Scope response to id' key :required, true key :type, :string end response 404 do key :description, 'User not found' end response :default do key :description, '' schema do key :'$ref', :User end end end end # Here comes the actual code that implements the `users#show` API def show # ... end\n\n#... end\n\nWhile the swagger-blocks tool has some very nice features and supports the latest Swagger version\n\nv2.0, we chose a different tool to specify our Rails APIs.\n\nOne reason we chose a different tool is that the structure of the swagger-blocks DSL very closely matches the structure of the JSON schema specifications. There doesn’t appear to be a lot of simplification or lowering of the abstraction barrier as a result of choosing this tool.\n\nAdditionally, mixing the specification DSL Ruby code with the Ruby code that implements the actual\n\nAPI endpoints is confusing to read.\n\nSwagger-YARD\n\nThe LivingSocial-maintained open-source tool swagger_yard (https://github.com/livingsocial/ swagger_yard/) approaches the documentation is a different way; the Swagger JSON schema is generated from comments in the code, rather than Ruby code itself. Under the hood, swagger_yard uses the YARD gem that serves as the Ruby community's standard tool for generating code documentation. YARD uses @-prefixed tag definitions to annotate regular Ruby code methods and classes.\n\n72\n\nChapter 5 ■ Defining apis\n\nSwagger_yard specializes the general YARD tags to provide tags like @resource, @path, @parameter, @response_type, @error_message, @model, and @property that describe the respective related Swagger specification entities.\n\nIn Listing 5-5 you can see the full JSON specification again describing the UsersController, but this\n\ntime expressed using swagger_yard.\n\nListing 5-5. Swagger_yard description of the UserController\n\n# @resource Users API # # This document describes the API for creating, and reading user accounts. # class UsersController < ApplicationController ## # Creates a new user for the given data # # @path [POST] /users # @parameter email(required) [string] Email address for the new user, needs to be unique. # @parameter name [string] Full name of the user. # @parameter password(required) [string] Password for the user. # # @response_type [User] # # @error_message 422 User data invalid def create ... end\n\n## # Returns a user's data for a given user by uuid # # @path [GET] /users/{id} # @response_type [User] # # @error_message 404 User not found # def show ... end\n\n## # Returns all users' data # # @path [GET] /users # @response_type [array<User>] # def index ... end end\n\n73\n\nChapter 5 ■ Defining apis\n\nThe User API model (see Listing 5-6)referenced in the API endpoint definitions just shown is described\n\nas a leading comment associated with another Ruby class (in this case the ActiveRecord model used to persist a user to the database).\n\nListing 5-6. Swagger_yard description of the User API model\n\n# @model User # # @property user_id(required) [string] UUID for the user's ID # @property name [string] the name for user # @property email(required) [string] user's email address # class User < ActiveRecord::Base include ActiveUUID::UUID\n\nhas_secure_password\n\nvalidates :email, presence: true, uniqueness: true validates :password, presence: true\n\n... end\n\nSummary We hope that this chapter helped to illustrate the advantages of defining your APIs in a computer-readable format. Adopting the use of an IDL and tools like Swagger very early on in your development process allows for soliciting feedback during the API design process. This enables a very iterative approach to API development.\n\nIn our daily work, we find ourselves defining API drafts almost every day using swagger_yard before\n\nimplementing a single line of business logic.\n\nWe can then proceed very quickly and share the API drafts via the Swagger UI with the teams that will\n\nbe consuming the APIs. The resulting feedback loop is very tight and fast, and we can iterate on change requests within minutes, speeding up the entire development cycle.\n\nAdditionally, involving the client teams early and defining APIs with their help leads to much more useful and robust APIs, as they are closely focused on the clients’ needs and hence much more likely to contribute to your business goals.\n\nWe will focus on aspects that deserve more consideration when consuming such service APIs in the\n\nnext chapter.\n\n74\n\nCHAPTER 6\n\nConsuming Your APIs\n\nThe size ofclient-side libraries, and the features included in them, can vary widely. We will discuss the pros and cons of including various responsibilities, like exposing network errors, or making service calls, validating service responses, caching expensive API calls, handling errors, and alarming when things go wrong. Key topics in this chapter include\n\nHow much code should live in a client library?\n\n\n\nLibraries should not have business logic, ever.\n\nHandling errors is a must, and depends on the nature of the endpoint being called.\n\nWhat Are Client Libraries? In a microservices environment, we call client libraries, the application code used to interact with a service. It’s a set of code, that, when added to an application, will give developers access to the APIs of the service it was written for. We believe that client libraries should be a very thin service access layer that implements a convenient set of objects that mimic specific service APIs. For example, a client library for books-service should expose a Book class that has methods that correspond to the API response of the service, returning values of the correct type for each attribute, making it palatable and convenient to interact with the API.\n\nIn general, your client libraries will have a set of plumbing features that should be common to all client libraries, they provide functionality that is not associated with each underlying API, but is required in order to provide all the tools developers need to interact with services. We call the following plumbing features:\n\nNetwork access. The main characteristic of distributed systems is that APIs require a network access layer in order to be accessed. That network access layer is implemented in client libraries; so, that developers don’t need to write network code in all their applications.\n\nObject serialization/deserialization. APIs use a serialization format to retrieve and expose data, which needs to be handled at the client library level. It’s expected that client libraries will abstract the details of the serialization format to its users, making responses look like regular objects, as expected by users of the language in which the library is implemented.\n\n\n\nLow level errors, like timeouts. Client libraries have to handle expected errors, such as access to invalid resources, or network timeouts. Depending on the programming language, client libraries will handle those type of errors, and will raise exceptions, or set specific response codes to service calls, allowing its users to write error handling code according to each failure coming from a service.\n\n75\n\nChapter 6 ■ Consuming Your apis\n\n\n\nExpose application errors. Client libraries can choose to translate some service response into code that is common in programming languages. For example, when a service returns a “404” to a request for a nonexistent resource, the client library may decide to expose that as a ResourceNotFound exception, which may make programming more natural, instead of forcing API users to check response codes.\n\nCaching. Client libraries may have code to cache service responses, thus increasing overall system performance. The caching layer in client libraries can have custom caching code, or it can just be HTTP-compliant to send and honor certain headers, performing tasks such as storing response based on ETags or expiry headers.\n\n\n\nService discovery. A client library needs to know how to connect to the services it exposes, and allow its users to override its default configuration.\n\nThis may seems like a lot to implement in what is supposed to be a “thin” layer; however, if you decided\n\nto use an IDL and/or a Hypermedia type, as explained in Chapters 4 and 5, you’ll probably be able to get a lot of this “for free” by using tools available for users of your IDL/Hypermedia type of choice. For example, in the sample project of this book, we decided to follow JSON API, and can use a wide array of libraries (http://jsonapi.org/implementations/) to help us building client libraries.\n\nWe’ll go into more detail on some of the features we expect to see in most client libraries; however, we’ll\n\nfirst take a quick detour to talk about the type of code you should never have in a client library.\n\nWhat Not to Do Here are a few suggestions, based on our experience, about things to absolutely avoid when designing client libraries in a microservices environment.\n\nDon’t wait eternally In a microservices environment, a web page could make a large number of service requests for rendering each page, which will mean that slow response times from a single service could turn the user experience into something abysmal. It’s imperative that, when you consume your own APIs, you define timeouts that, if reached, will not make your site unusable.\n\nThe one thing that will always happen in a microservices environment is seeing service requests time out. In a distributed application, there are a multitude of the reason for seeing such timeouts, be they general networking snafus, network congestion or just general latency increases along the full trajectory of a service request.\n\nDon’t make the clients smart We recommend you never put any sort of business logic in a client library, be it some constant of default values, or any logic that may seem straightforward to code as you’re working on your gem. For example, in our book store example, we will need to know whether or not a specific book is available for purchase at a specific moment in time. Following the initial, very simple requirements for out hypothetical bookstore logic, a book would be considered available if we have more than 0 copies in stock, which seems fairly obvious, however also naive.\n\n76\n\nChapter 6 ■ Consuming Your apis\n\nIf we decide to implement an available? method in our Ruby library, it would look like:\n\ndef available? stock>= 1 end\n\nSomeone, even yourself, could try to convince you that having that logic live in the client library is not harmful, and even more efficient than requiring the service to vend yet another attribute. Don’t be fooled by that influence because every piece of business logic starts simple and gets more complex with time. Keeping the theme of our hypothetical future evolution, your book store may start selling books that can be considered as “always available” because they can be printed on-demand; so, you decide to make tweaks to your “in-library” code.\n\ndef available? type == 'on_demand' || stock >= 1 end\n\nThat wasn’t that bad; however, you now need to release a new version of your library, and get all\n\nconsumers of that data to migrate to it, which may not be that simple, and will cause, even if for a small amount of time, to have different clients using different versions of the library that have a different implementation of that method in production.\n\nIn another unsurprising turn of events, you find out that the bookstore has a new supplier that promises\n\nsame day delivery of books to your warehouse; so, the business wants to not lose any sales of books that belong to that supplier; so, you need to tweak your implementation, you decide to add a new attribute to Book resource called inventory_management, which, when set to allow means that a book can be sold even when it is not in stock. Off you go to update your client library, which now reads:\n\ndef available? inventory_management == 'allow' || type == 'on_demand' || stock >= 1 end\n\nThe code is starting to look like spaghetti, we’ve seen how ugly it gets, don’t do that. Once again, we will have the issue of possibly having clients using different versions of the library at some point in time. Also, in an ideal world, you’d never have to expose the inventory_management to certain clients, it’s an implementation detail that should be kept inside the domain of the books-service; however, it had to be exposed to allow the logic in the library to be updated. We’ve been showing you the available? method implemented in Ruby, but, if your application happens to run in an environment where multiple languages are used, then, you would also have to implement that code in each one of those languages; so, trust us, business logic should never live in a client library. The general rule should be to keep your client libraries as dumb as possible, and move the business logic into service endpoints instead.\n\nService Orchestration We can also offer an important positive recommendation: Try to understand how you can orchestrate the information retrieved, so you can optimize the workflow and cut down on load time.\n\nIn some cases, when working on user-facing applications, you may notice a specific pattern of service\n\naccess that could be optimized. For example, suppose you know that on a book detail page, you need to access detailed book information from books-service, as well as a large amount of information from a set of other services, such as images, reviews, or categories. You may be tempted to add a class into your client library that could load data from those services in parallel, and return it in a faster fashion.\n\n77",
      "page_number": 85
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 95-102)",
      "start_page": 95,
      "end_page": 102,
      "detection_method": "topic_boundary",
      "content": "Chapter 6 ■ Consuming Your apis\n\nThat type of connected service call is what is called service orchestration, and it is very helpful for increasing\n\nthe efficiency from an end-user point of view. We’ll discuss in Chapter 7 the way backend-for-frontend (BFF) services have started to be used for exactly that reason, and we recommend you take that approach—add orchestration to a BFF, and have a thin client library for that service, which will give you all the advantages of optimizing your workflow to improve performance, and will keep logic away from your client libraries.\n\nCaching in the Client In order to increase speed, caches are often used in distributed systems, and a layer that sometimes becomes responsible for caching is the client library. It seems like a decent choice for caching because, by handling caching in the client library, it makes caching transparent to its clients, which can be seen as an advantage, reducing system complexity.\n\nIn quite a few cases, service clients can often tolerate data that is slightly stale. A good way to exploit\n\nthis fact is to cache service responses within the client application (which can of course be either a consumer-facing front-end application or some other service that depends on data from your service). If, for example, the client can tolerate service data being 5 minutes old, then it will incur the interapplication communication costs only once every 5 minutes. The general rule here is that the fastest service calls are the ones your applications never need to make.\n\nBuild Caching into Your Client Library Often the team that builds a service will also be developing the libraries (such as Ruby client gems) to access that service. Some of these client libraries are built to offer the option to have a cache store object injected into them via a configuration option. A cache store injected that way can be used to store all response objects returned by the service. A prudent approach when allowing cache store injection in your client library is to require as little as feasibly possible about the cache object that is injected. That strategy provides the client applications with more flexibility in choosing which cache store to use.\n\nCaching should be entirely a client application’s responsibility. Some clients might not want to cache at all, so make sure to provide the ability to disable caching in your client. It is also the client application’s responsibility to set cache expiration policies, cache size, and cache store back-end. As an example, Rails client applications could simply use Rails.cache; some others might prefer a Dalli::Client instance (backed by Memcached), an ActiveSupport::Cache::MemoryStore (backed by in-process RAM), or even something that is entirely “hand-rolled” by your clients, as long as it conforms to the protocol your gem code expects of the cache store.\n\nWe’ll go into more detail about (mostly service-side) caching in Chapter 7, and just recommend that\n\nclient libraries offset caching to standards—HTTP headers—or have a pluggable caching mechanism.\n\nError Handling In a distributed environment, properly handling errors is extremely important because errors will happen due to the simple fact that there is a network between two systems making a call.\n\nIn systems that do not properly handle errors, one service could bring down an entire application when issues arise. We’ve seen many times a not-so-important service A start to return errors that are not properly handled by its clients, and thus generate a series of “cascading failures” that can potentially bring down other services or entire applications.\n\n78\n\nChapter 6 ■ Consuming Your apis\n\nTimeouts The main cause of failures in a microservices architecture will probably be timeouts, due to the network between client and server. In order to handle those types of errors, you should have timeouts—ideally with exponential back-offs—on connections and requests to all services you connect to. When services take too long to respond, a client can’t possibly know if the delay is caused by expensive computation, or because the connection has failed. In distributed systems, you should always assume that a slow synchronous response is a failure; and having timeout values set very low means that your client code will not wait too long for a service response, and can handle that condition faster.\n\nWe recommend you set aggressive timeouts that will keep your systems honest. If you have synchronous endpoints that are sometimes very slow to respond, you have to make sure you fix all performance bottlenecks in those services, instead of increasing the timeout on the client. We’ve seen clients that have very long timeouts; anything above 1 second should be unacceptable, which means that the timeout is not protecting the client from a slow service, because it’s still waiting too long for a response, and probably making the a user-facing application looks sluggish and incorrect—because of the service failure.\n\nCircuit Breakers A well-known pattern for handling service failures is the circuit breaker, also known as the feature flag. The circuit breaker design pattern is used to detect and isolate repeating failures, preventing system-wide failures caused by an issue with a specific endpoint/service. Circuit-breaker libraries usually allow their users to set a group of settings that characterize the circuit to be defined. Some of the properties that are usually defined in a circuit are\n\n\n\nTimeout: As discussed in the previous section\n\nRetries: The number of times a service call should be retried, in case of errors\n\n\n\nThreshold: The number of errors that will cause the circuit to open\n\nWindow: The period of time to keep the circuit open before a new connection is attempted\n\nIn our sample bookstore application, we display a classic shopping cart icon in the top right of the user interface, and display the number of items currently in the cart, alongside the actual cost of the items in the cart. That interaction point is a good use case for a circuit breaker, and we have a circuit breaker in place. In that specific case, we should have a very slow timeout setting, we don’t really need retries in case of failures, and we have code in place to render an error scenario that doesn’t look like a failure. With the circuit breaker in place, our code properly handles failures, and it stops sending requests to the service in failure state, until it (hopefully) recovers and starts responding appropriately.\n\nclass BooksService include CircuitBreaker\n\ndef find_book(id) ApiClient::Books::Book.find(id).first end circuit_method :find_book\n\ncircuit_handler do |handler| handler.logger = Logger.new(STDOUT) handler.failure_threshold = 5\n\n79\n\nChapter 6 ■ Consuming Your apis\n\nhandler.failure_timeout = 2 handler.invocation_timeout = 10 end end\n\nWe recommend you get in the habit of always having a circuit breaker setup in place. Your application\n\nwill be able to withstand failures in downstream dependencies, while at the same time, it will not overwhelm those services that are under some sort of failure scenario.\n\nSo far, we’ve only touched on handling the errors that happen in a downstream dependency; it’s also extremely important to have some information on how to actually fix those errors. The first place anyone will look when errors start to happen is in service logs; we discuss generic logging in detail in Chapter 13, but the message to convey is that your client libraries are a great location to add logging about service calls. You should log most details your client library uses when requesting data from a service.\n\nIdeally, you should use an error-tracking system, whether built in-house or commercial, such as Airbrake (https://airbrake.io/) or bugsnag (https://bugsnag.com/). Using such tools has some advantages:\n\n\n\nYou can have a central location where your team can see what errors are happening throughout all your services. When a user-facing application starts to experience an increase in errors, the first challenge in a microservices environment is to find out the origin of the error; so having some visibility into error rates of all services is very useful, in stressful situations.\n\n\n\nYou can group similar errors. Some systems can start to generate a really large number of errors; in an ideal world, someone should be notified about that one error with some sort of indication that it is happening quite often. We’ve been in situations where services would generate tens of thousands of error emails that ended up being a single failure, but that would cause email servers to be overwhelmed, delaying the delivery of other, potentially new errors, that could be happening.\n\n\n\nYou can link alarming to escalation and severity policies. The main feature of an error-handling system is to notify the owning team that something is wrong. Depending on how teams are set up in your organization, the error-handling tool should notify one person, and have an escalation policy to alert other individuals if no action is taken. It’s also important to be able to assign a severity to errors, so that errors with a higher impact appear quickly and are given priority over other types of errors.\n\nPerformance Performance is a critical characteristic in a microservices architecture, and we discuss it in detail in Chapter 7. From the point of view of a client library, the message we want to convey is that the performance of a service A can differ from the perceived performance from a client that calls client A. When a client calls a service, network latency, response deserialization, circuit checks, and potentially other tasks will translate into the perceived performance of each service call.\n\nWe have the following performance-related recommendations for client libraries:\n\nHave metrics in place to measure service response time from a client perspective.\n\nOnly request what you need; avoid requesting 100 attributes when all you need is 2.\n\nCheck how much time is spent in each part of a call, as seen by the client.\n\n80\n\nChapter 6 ■ Consuming Your apis\n\nUpdating Client Libraries Once your client library is used in the wild, it will start to evolve, and you should have an upgrade policy for those libraries, similar to how you support only a small number of versions of the service itself. Make sure that you have rules in place that make it easy for users of your client libraries to upgrade, and be as, up-to-date as possible most of the time.\n\nYou should follow a versioning policy for all your client libraries, to better communicate what type of changes your libraries are undergoing as you release new versions. We recommend you follow “Semantic Versioning” (http://semver.org/), as it’s a well-defined policy that is both easy to follow and provides clear guidelines to how the version number should change. In Chapter 4, we talked about versioning the API endpoints in the service; we believe you should never tie the version of the service to the version of the API, as those can evolve in different ways, and having strict rules will not help you move forward quickly.\n\nBriefly, given a version number major.minor.patch, increment them as follows:\n\n\n\nThe major version when you make incompatible API changes\n\n\n\nThe minor version when you add backward-compatible functionality\n\n\n\nThe patch version when you make backward-compatible bug fixes\n\nAlso make sure you have a test suite for your client library; it should most likely be slim. You just have to make sure that you can catch obvious errors before releasing a new version of the library. We cover testing in detail in Chapter 9.\n\nService Discovery One of the things you will have to add to your client libraries is a way of specifying where the service endpoint for each service is to be found. This seems like a very simple configuration parameter, and can even be defined in the client library itself, as all services should follow a naming convention based on the service name and environment in which the service is running. An example of endpoints for the books service could be the following:\n\nDevelopment: books.development.bookstore.net\n\n\n\nStaging: books.staging.bookstore.net\n\n\n\nProduction: books.prod.bookstore.com\n\nHaving endpoints defined in their libraries is a good practice, as it makes it easy for developers to start\n\nusing the library. If you decide to distribute your service endpoints in your libraries, make sure you allow the configuration to accept an override, in case developers want or need to hit a version of the service in a nonstandard host or port.\n\nOne approach we’ve seen and recommend at the moment is to move the knowledge of where services\n\nare to be found to service discovery tools. We’ve successfully used Consul (https://www.consul.io/), which is described as a tool that “makes it simple for services to register themselves and to discover other services via a DNS or HTTP interface. Register external services such as SaaS providers as well.” In our sample project, we make use of Consul, and have our client libraries ask it for service endpoints, which means that you can rely on Consul knowing the correct endpoints to connect to in each of the environments where your application runs.\n\n81\n\nChapter 6 ■ Consuming Your apis\n\nmodule ServiceDiscovery class << self\n\ndef url_for(service_name) client = Consul::Client.v1.http service_details = client.get(\"/catalog/service/#{service_name}\").first # TODO raise if there is no service data endpoint_parts = [] if service_details[\"ServiceAddress\"].empty? endpoint_parts << \"http://localhost\" else service_details[\"ServiceAddress\"] end endpoint_parts << service_details[\"ServicePort\"] URI.parse(endpoint_parts.join(\":\")).to_s end\n\nend end\n\nWe made the complete source of the Ruby service discovery library available at https://github.com/\n\nmicroservices-from-day-one/service_discovery.\n\nSummary This chapter focused on all aspects of consuming APIs. We defined client libraries and discussed their features and responsibilities. We talked about many concerns usually covered in client library implementations, such as…\n\nIn the next chapter will focus on what can be done to optimize the APIs on the service side. We expect\n\nto build APIs that respond quickly and efficiently, and will share some of the techniques we’ve seen that help you keep the promise of fast APIs.\n\n82\n\nCHAPTER 7\n\nOptimizing Your APIs\n\nThis chapter establishes guidelines and rules of thumb for designing and implementing APIs with a special focus on performance and maintainability. The majority of the techniques described here can be used to address and—if applied at design time—avoid performance degradation, while others can help to optimize overall service-related implementation and support costs.\n\nMeasuring Performance The great American engineer and statistician W. Edwards Deming has been credited with the quote:\n\nIn God we trust; all others must bring data.\n\nIn our experience, this rule translates very well into the realm of optimizing service APIs. Too often have\n\nwe seen engineers spending significant time fixing perceived performance issues based on mere hunches rather than concrete data, often adding significant complexity to the code without noticeable improvement. When you are trying to improve system characteristics without introducing metrics, and methods to\n\nmeasure the system in terms of these metrics, you are shooting in the dark more often than not.\n\nFinding the root cause for any long-running action often requires several iterations of defining metrics, adding probes, measuring, and finally evaluating results, drilling down further and further into the issue at hand.\n\nHere are some typical questions that we find ourselves asking on a regular basis when researching API\n\nperformance issues:\n\nDatabase related inquiries:Is the bulk of the response time spent inside the database query?\n\n\n\nIf so, are there N+1 query issues, accessing the same database table over and over?1\n\n\n\nIf the number of queries seems optimal, which particular query is run and takes the bulk of the time?\n\nHow big is the result set of the query, and do all the returned rows need to be processed in application code?\n\n\n\nCan I analyze the query's EXPLAIN plan to see if table indexes are optimally used?\n\n1Note that repeatedly running inexpensive queries can at times be justified if the equivalent SQL query involving a joining of tables turns out to be more expensive. As always, make sure you measure the impact of both options before deciding which approach to take.\n\n83\n\nChapter 7 ■ Optimizing YOur apis\n\n\n\nService dependency-related issues:\n\n\n\nIs the bulk of the response time spent making dependent service calls, and do the downstream dependencies meet their advertised Service Level Agreements (SLAs)?2\n\n\n\nIf so, can some of the calls be backgrounded, made concurrently, or replaced by calling more efficient APIs on the dependencies?\n\n\n\nAre all external calls in circuit breakers with the appropriate settings (like timeouts and maximum number of retry attempts)?\n\n\n\nIs the data returned by the external dependencies optimal (that is, minimal in size and complete in content)?\n\nMemory allocations and CPU cycles:\n\nHow memory-intensive is your API endpoint, and does it spend a significant amount of time allocating, deallocating, or swapping memory to and from disk?\n\n\n\nIs there resource contention with other applications or processes on a co- hosted environment?\n\nHow many CPU cycles are spent in each section of your code (as researched and visualized by tools like gperftools?3\n\n\n\nBusiness logic profiling:\n\n\n\nCan you add special probes to determine how much time is spent inside the business logic of your application?\n\n\n\nAre you using the most efficient libraries available to deserialize and reserialize your objects? Can some of these serialization steps be avoided altogether?\n\nDo you know the algorithmic complexity4 of your calculations? Are you using the most efficient algorithms and data structures to implement your logic?\n\nWould this API be faster if it was implemented in a different programming language or toolset? (The beauty of an SOA approach is that experimentation around this question can be done in isolation and fairly cost-effectively. A prototypical implementation could be done in, for example, Ruby, and—if found too slow—switched to Go without impact if the API definitions can be kept unaffected.)\n\nA few words of caution: while gathering performance data about your service is important, please note that often such data collecting does not come for free. Therefore, investigating the performance of your metrics-collecting tool, for example of your logging framework,5 is always advisable in order to decide how much data should be selected. Your choice of tool to collect data about your applications should be influenced by the efficiency and performance impact it has. You might, for example, decide to rely less on\n\n2Often, such SLAs are only loosely expressed, and—unless the service dependencies are all internal to your company— unenforceable. If you cannot coerce the team owning the dependency to meet their SLAs, then moving to alternative sources, or caching results (discussed later) might be your only options. 3Main gperftools repository: https://github.com/gperftools/gperftools 4“Analysis of algorithms,” Wikipedia. https://en.wikipedia.org/wiki/Analysis_of_algorithms 5Sarah Ervin, “The Real Cost of Logging”, 17 April 2014. https://dzone.com/articles/real-cost-logging\n\n84\n\nChapter 7 ■ Optimizing YOur apis\n\ntraditional logging frameworks, and more on UDP-based (and hence connectionless and faster, but also less reliable) data collection tools like Etsy's StatsD.6 Another thing to consider and watch is how to effectively archive and purge all the data you have collected as part of your performance-measurement efforts. Letting log files eternally accumulate on your application servers is a sure-fire way to cause system downtime. Many log archiving and purging tools exist, which can be integrated with larger data warehouse systems, such as those based on Hadoop (http://hadoop.apache.org/) clusters.7\n\nChapter 13 will have more in-depth information about service application monitoring.\n\nService Access Pattern-Based Improvements\n\nThe very first approach we will explore among the ways to improve performance and design of a microservices architecture is to study the granularity and coherence of the system's APIs.\n\nAs laid out in greater detail in Chapter 3, sometimes service APIs are best aggregated or decomposed based on client access patterns and data joining concerns. If you find that your APIs perform sluggishly, it might be because their functionalities combine write-intensive data (like a bookstore user's reading list) with read-intensive data (like the user's profile data). Separating by such access pattern lines will enable you to introduce more tailored caching strategies, and even separate specialized storage systems with different read or write characteristics optimized for the given access pattern. Similarly, your performance analysis might show that much time is spent joining data retrieved via multiple service calls. This behavior is similar to SQL joins involving multiple tables, and aggregating such often-joined data will improve your API’s performance. In this case, a new service that co-locates the joined data—whether by truly combining and replacing the original downstream services into a service that stores the data authoritatively in a new data store, or simply by caching such aggregated data based on the more granular data from the downstream services—can greatly improve API response times.\n\nAnother consideration might be to optimize the size of the service response to the need of your service's\n\nclients. In situations where a human end-user is waiting for a UI to be rendered based on data vended by your service API, it makes great sense to limit the size of the result set of an API call; the service should enable (or even better, enforce) the UI rendering application to request larger result sets in manageable, coherent chunks (also known as pages). We will focus more on this point later in this chapter.\n\nFor other client use cases, bulk APIs might actually be desirable and efficient. Consider, for example,\n\nthe circumstance of a nightly, offline batch process that runs to aggregate data from several source services. In our social bookstore example, this could be the promotions-service. As mentioned in Chapter 3, this service runs periodically as a background process, sending out emails and push notifications promoting products to users. This service will need to operate on very large sets of users (maybe all female users, as provided by bulk data from users-service APIs), and it will need to mesh the user data up with bulk recommendation data from the recommendations-service. Making such bulk APIs sufficient and maintainable on the service side might require you to add additional pieces of infrastructure. For example, if you notice that the response times of your other client-facing APIs are affected by database connection contention with the queries executed for the bulk APIs, then you could consider adding a replicated DB read-slave8 to service the bulk API queries. If you notice that connection requests to your web server are starting to queue up significantly because of the long-running requests to the bulk APIs, you could consider routing these requests to a dedicated pool of servers that is not part of the general web-application server pool. Try to avoid mixing long-running requests (such as for entire sections of a UI) with requests for data that can be returned much more quickly (like service requests that return within 10 milliseconds).\n\n6Ian Malpass, “Measure Anything, Measure Everything”, 15 February 2011. https://codeascraft.com/2011/02/15/ measure-anything-measure-everything/ 7Peter Dikant, “Storing log messages in Hadoop”, 15 April 2010. http://blog.mgm-tp.com/2010/04/ hadoop-log-management-part2/ 8“Master/slave (technology)”, Wikopedia, https://en.wikipedia.org/wiki/Master/slave_(technology)\n\n85",
      "page_number": 95
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 103-112)",
      "start_page": 103,
      "end_page": 112,
      "detection_method": "topic_boundary",
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nAnother fairly recent development pattern in the microservices community is to provide a very specialized façade service with APIs highly tailored for particular front-end applications. Some companies even go so far as to add one back-end API service per user experience, as Sam Newman lays out in his article on this “Backend for Frontend” (BFF) pattern.9 Companies like REA and SoundCloud appear to have separate BFF services for their iOS app, another one for their Android app, and a third one to feed UX-specific JSON data to their desktop web application, fronting the rest of their SOA infrastructure and providing the client UIs with data in exactly the format that they need to render the desired experience.\n\nThe Blessing and the Curse of Caching\n\nWhen asked to optimize the performance aspects of their web applications, for many developers the first (and often, only) tool of choice is caching.\n\nA loose definition of this approach is to temporarily hold recently retrieved or computed information\n\nin a data store (for example, in the server's RAM, in a distributed object cache like Memcached,10 or in a key-value store like Redis11) in the hope that future access to this information from that temporary data store (cache store, or simply cache) will be faster than reretrieving or recalculating it. Updating of the information held in the cache with refreshed information is usually triggered by a preconfigured timer expiring, but it can also be manually or programmatically triggered.\n\nCaching in the API Consumer The fastest API is call is one that isn't made in the first place. As mentioned briefly earlier, the API consumer should consider caching if it can live with information that is slightly out of date. The rest of this section will mainly focus on caching approaches in the information providing service. Please see Chapter 6 for details on how to implement techniques for client-side caching.\n\nCaching in the API Provider The advantages of caching seem rather obvious and convincing at first. Computing resources—client- and service-side—can be saved, and network calls can be avoided, resulting in faster information load times for the clients and more responsive websites and more enjoyable, and hence more revenue-generating,12 end- user experiences.\n\nAs is often the case with optimization approaches, caching has its advantages but is not a silver bullet.\n\nWhile caching might work very well for relatively static content and for APIs where clients have a certain tolerance for stale data, it is often untenable for information that is very dynamic, very large, or where the clients have very low tolerance for data staleness. Consider the case of the inventory count of a given book in our bookstore example; this count will change very rapidly for popular books, while it might change much more slowly for others, so finding the right time-to-live for a cache for this information is very hard. Additionally, basing any sort of “sold out” decision on a cached count will be very unreliable.\n\nAdditionally, adding caching and with it additional pieces of infrastructure (like Memcached, for example) adds cost and general complexity to an application. The process of procuring, configuring, integrating, and maintaining such solutions (and the related cache writing and retrieval code) is often nontrivial.\n\n9Sam Newman, “Pattern: Backends For Frontends”, 18 November 2015; http://samnewman.io/patterns/architec- tural/bff/ 10https://memcached.org/ 11http://redis.io/ 12Sean Work, “How Loading Time Affects Your Bottom Line”, April 2011; https://blog.kissmetrics.com/ loading-time/\n\n86\n\nChapter 7 ■ Optimizing YOur apis\n\nAlso, adding caching might actually turn out to be harmful to performance. It is not uncommon for\n\nengineers to discover that refreshing the information they were caching overall took less time than storing it to, and fetching it from, the cache storage they introduced. This even sometimes applies when the information authoritatively lives in a relational database that needs to be queried, especially if the cache store is distributed and hence also only reachable over a network boundary.\n\nIn such cases, the developers fell prey to the cardinal sin of “early optimization,” adding complexity without reaping any benefits (or worse, actually degrading performance). Other dangers of caching are the unintended layering of several caches: services might cache information retrieved from downstream services, not knowing that the downstream service already introduces a caching layer around the very same data. As a result, the worst-case staleness of the data exposed to the clients of the first service is the accumulated time-to-live of all downstream caches.\n\nFinally, there are often unintended interactions between the deployment of new code and the expiry of\n\ncache stores in an application. If the information stored in the cache stores has interactions with the newly deployed code, the deploying engineer will only truly be able to determine compatibility between the new code and the cached information once the cache store expiry has taken effect (which could sometimes be hours or days after deployment of the code regression).\n\nCaching API-provided data also doesn't always have to happen inside the providing service itself or downstream from it (as seen from the client) in a key-value store or Memcached server. A caching proxy that is inserted between the client of an API and the API provider might be a useful tool, which we will explore in the next section.\n\nMake Your Service Work Less Retrieving information from a data store and subsequently serializing it to vend it to clients takes valuable time in your API’s request/response cycle. In this section we describe two ways to reduce, or even entirely avoid, incurring this time.\n\nUse the Standard The HTTP standard comes with a set of built-in caching support methods, collectively known as HTTP conditional GET. These cache support functionalities are all implemented using a set of HTTP request and response headers.13 We will not explain the full set of all the HTTP headers, but here are a few to be noted.\n\nIf-Modified-Since and Last-Modified\n\nThe If-Modified-Since request header and the Last-Modified response header work as a pair, as depicted in Figure 7-1.\n\nFigure 7-1. Last-Modified and If-Modified-Since headers\n\n13“List of HTTP header fields”, Wikipedia; http://en.wikipedia.org/wiki/List_of_HTTP_header_fields\n\n87\n\nChapter 7 ■ Optimizing YOur apis\n\nIn this example, the client (left) makes its first request for the book with ID 1 as JSON, not sending any\n\ncaching-related request headers. The service responds with the serialized book data, and it includes a Last- Modified response header with the last modification date and time for the book resource. The client now can store that date/time, until it is time to make the next request for the same book's data. In step 3, when the client sends a request for the same book, it can add the date/time retrieved in step 2 as an If-Modified- Since request header. If nothing has changed since the requested modification time, then the server can return a simple 304 Not Modified response.\n\nEven though no direct data caching is involved for the second request/response cycle, there are several\n\nadvantages to this scenario. First, the data transferred over the network is relatively small, as only headers are transferred, but no JSON data about the book.\n\nAs a corollary, the server will also not spend time serializing the book object retrieved from storage to\n\nJSON; the service will only need to calculate the modification time stamp and compare it to the time sent in the If-Modified-Since request header.\n\nOn the client side, efficiency is gained because the 304 status code of the response tells it that it can\n\nsimply reuse the data retrieved in the previous service response's body; no renewed deserialization of JSON-encoded data into programming language objects is needed.\n\nHowever, note that here, as well as with the ETag and If-None-Match headers discussed next, you will\n\nstill incur the costs of making network calls and retrieving the result object from the data store.\n\nETag and If-None-Match\n\nAnother such pair of request and response headers is If-None-Match and ETag. Figure 7-2 illustrates a similar example scenario as in the previous section, but adapted to this header pair.\n\nFigure 7-2. ETag and If-None-Match headers\n\nIn this example, the client again first makes a request without sending any related headers. The server\n\nresponds with the data and a response header with a unique Entity Tag (also called ETag) for the data, calculated server-side. Any string that uniquely identifies the content of the resource will do, and many implementations simply calculate an MD5 hash14 of the content to serve as this tag.\n\nIn step 3, when the client next requests the same book, it now adds the ETag it has last received (and\n\nsubsequently stored) into the If-None-Match header of the request it sends to the service. The server again calculates the entity's unique hash and, in this case, finds that the requested entity's tag has not changed when compared to the tag sent by the client.\n\nAs a result, the service can skip the book object JSON serialization, and can simply respond with an\n\nempty-body 304 Not Modified response. The client can now also react to this 304 response appropriately by reusing the previously retrieved book object. In short, the performance and efficiency advantages encountered in the early If-Modified-Since / Last-Modified remain the same for ETag.\n\n14“MD5”, Wikipedia; https://en.wikipedia.org/wiki/MD5\n\n88\n\nChapter 7 ■ Optimizing YOur apis\n\nThe Cache-Control Header\n\nThe HTTP Cache-Control response header can be used to help the service control all caching entities along the request/response chain to the end-clients. This includes intermediate (and potentially caching) proxies, browsers (which are built to adhere to this header), and other consuming clients (like language or platform code libraries).\n\nWe will not exhaustively explain all the cache directives that can be included (and combined) in the\n\nCache-Control header, but here are some important ones we advise knowing and using in an SOA:\n\n\n\nprivate: Browsers and client-side caches can freely store the response marked with this directive; shared (intermediate) caches and proxies are not allowed to cache the content.\n\n\n\nno-cache: Sending this directive in the Cache-Control header forces all caches (intermediate and end-client based) to resubmit the request to origin server before releasing a cached copy; this is often useful for API endpoints that require user authentication and authorization.\n\n\n\npublic: This directive marks responses as cacheable (as private is the default).\n\n\n\nmax-age=[secs]: This directive indicates the maximum time in seconds the result should be cached and considered fresh, relative to the time of the request. The max- age directive is usually combined with the private or public directive.\n\nThe advantages of using cache control headers in your microservices infrastructure are manifold for performance optimization. It is an approach to caching “done right,” as it lays the responsibility of cache expiry on the service, so there is a central place to expire it. It also helps to enable caching proxies (discussed shortly) to function as a transparent part of the tech infrastructure (via the public directive), thereby relieving the service itself of any additional unnecessary load. Finally, it enables clients to entirely prevent making potentially expensive network calls by letting them know how long the previously retrieved response should be used unchanged.\n\nFramework Support for HTTP Conditional GET As mentioned in Chapter 6, many programming languages provide client-side libraries that can process, and appropriately react to, the HTTP headers for conditional GET functionality. An example is the faraday- http-cache Ruby gem (https://github.com/plataformatec/faraday-http-cache), which works together with the lower-level HTTP communication gem faraday. Apple's iOS operation system and URL Loading System15 come with built-in support for iOS mobile client apps that use foundation classes like NSURLConnection or NSURLCache. Similarly, the popular OkHttp library for Android (https://github.com/ square/okhttp) also supports the cache control headers for storing previous results, and it reacts to ETag headers appropriately.\n\nOn the service side, there are also a large number of frameworks and libraries that support generating\n\nthe necessary request headers. As an example, see Figure 7-3, which shows a Rails code snippet found in our bookstore sample application's users-service.\n\n15Mac Developer Library, “About the URL Loading System”, https://developer.apple.com/library/ mac/#documentation/cocoa/conceptual/urlloadingsystem/urlloadingsystem.html\n\n89\n\nChapter 7 ■ Optimizing YOur apis\n\nFigure 7-3. Users#show action with HTTP cache control\n\nThis example shows the entire Ruby code necessary to implement the API to data about a given bookstore user account, based on the user's UUID. The important parts in relation to the topic of HTTP conditional GET are the stale? and expires_in helper methods that are built into the Ruby on Rails framework.\n\nCalling the stale? method here with the :etag and the :last_modified option results in Rails\n\ncalculating a hash value for the passed user object (as retrieved from the database), and a date of last modification of the user, based on the date its database row was last updated. Both of these data points are then automatically included in the API's response headers (as an ETag and a Last-Modified date, respectively). Additionally, Rails checks these two data points against the If-None-Match and If-Modified- Since headers from the incoming client request. If these request headers do not match the :etag and :last_modified options provided to the stale? helper, the user resource is considered stale, and the user JSON representation is generated by serializing the user object hash. Otherwise, the user resource is seen as fresh and the JSON will not need to be regenerated; the service API can simply reply with a bodiless 304 Not Modified response.\n\nThe expires_in helper call, which is executed only if the user resource is considered stale, sets the Cache-Control response header's max-age directive to 300 seconds (5 minutes), and indicates via the public option that intermediate proxies can cache the user resource.\n\nThe following cURL commands show this API’s conditional HTTP GET behavior:\n\ntim$ curl -v \\ > http://localhost:5002/users/3a07468a-f97d-4242-9057-8437385efb29 * Trying ::1... * Connected to localhost (::1) port 5002 (#0) > GET /users/3a07468a-f97d-4242-9057-8437385efb29 HTTP/1.1 > Host: localhost:5002 > User-Agent: curl/7.43.0 > Accept: */*\n\n90\n\nChapter 7 ■ Optimizing YOur apis\n\n> < HTTP/1.1 200 OK < X-Frame-Options: SAMEORIGIN < X-XSS-Protection: 1; mode=block < X-Content-Type-Options: nosniff < ETag: \"616210ace8e9681c485ad8c71efcc5b2\" < Last-Modified: Sun, 28 Feb 2016 06:37:23 GMT < Date: Mon, 04 Apr 2016 02:48:24 GMT < Content-Type: application/json; charset=utf-8 < Cache-Control: max-age=300, public < X-Request-Id: 028cb9d3-1965-4f5a-be28-0db8a06c3086 < X-Runtime: 0.128530 < Transfer-Encoding: chunked < * Connection #0 to host localhost left intact {\"user_id\":\"3a07468a-f97d-4242-9057-8437385efb29\",\"name\":\"Tom Copeland\",\"email\":\"tom@books. com\"} tim$ curl -v \\ > -H 'If-None-Match: 616210ace8e9681c485ad8c71efcc5b2' \\ > http://localhost:5002/users/3a07468a-f97d-4242-9057-8437385efb29 * Trying ::1... * Connected to localhost (::1) port 5002 (#0) > GET /users/3a07468a-f97d-4242-9057-8437385efb29 HTTP/1.1 > Host: localhost:5002 > User-Agent: curl/7.43.0 > Accept: */* > If-None-Match: 616210ace8e9681c485ad8c71efcc5b2 > < HTTP/1.1 304 Not Modified < X-Frame-Options: SAMEORIGIN < X-XSS-Protection: 1; mode=block < X-Content-Type-Options: nosniff < ETag: \"616210ace8e9681c485ad8c71efcc5b2\" < Last-Modified: Sun, 28 Feb 2016 06:37:23 GMT < Cache-Control: max-age=0, private, must-revalidate < X-Request-Id: 1bde6ef0-dc77-4647-b7b8-0cfc1a73dde4 < X-Runtime: 0.003823 < * Connection #0 to host localhost left intact\n\nAs can be seen, the first cURL request for a user (in which we send no additional headers) returns the HTTP/1.1 200 OK response with the serialized user object in the response body. But also pay attention to the relevant headers returned by the users-service:\n\n< ETag: \"616210ace8e9681c485ad8c71efcc5b2\" < Last-Modified: Sun, 28 Feb 2016 06:37:23 GMT < ... < ... < Cache-Control: max-age=300, public\n\nThe Rails helper-generated ETag, Last-Modified, and Cache-Control headers are present as expected.\n\n91\n\nChapter 7 ■ Optimizing YOur apis\n\nIn the second cURL request, we include an If-None-Match request header whose value matches the previous response's ETag header value. This time, the Rails stale? helper can match the values, and the user resource is considered fresh. As a result, the service responds with a simple, empty-body HTTP/1.1 304 Not Modified, thereby saving service-side serialization and client-side deserialization of the user object.\n\nCaching Proxies Even if service clients do not send or honor HTTP conditional GET headers, reverse proxies added between your clients and your service will be able to do so. For its most-trafficked internal API services, LivingSocial relies heavily on Varnish (https://www.varnish-cache.org/), a reverse proxy that has excellent performance and scaling characteristics. Using Varnish, we saw some endpoints speed up by a factor of 50. It is effectively just constrained by the speed of the network. Figure 7-4 shows a diagram of inserting Varnish into the request path.\n\nFigure 7-4. Varnish inserted into the request/response path\n\nUnder normal configuration settings, Varnish respects Cache-Control headers, caching content only\n\nwhen the service tells it to do so (via the public directive). Varnish can be configured to insert the If- Modified-Since and/or If-None-Match request headers if the service APIs' clients do not send them by themselves, thereby alleviating load on the service, but still be able to serve the content as requested to the clients.\n\nVarnish comes with a grace mode, in which it uses objects in its cache even if their TimeToLive has expired. This means it can deliver old content while concurrently fetching the latest content from the back- end server.\n\nVarnish is also flexible enough to function as a “hold the fort” cache: if the service fronted by Varnish\n\nis down, the proxy can return to the clients the last “good” response it received (that is, the last one carrying HTTP status code 200) from the backend service. This helps overall system uptime, as Varnish can continue to serve stale data while no healthy back-end service is available.\n\nIt can be administered to cache based on a full URI, including or excluding headers. Three tips for\n\nconfiguring your reverse proxy for maximum efficiency and correctness:\n\n\n\nTry making all query parameters sorted, so that any reverse proxy can yield a higher cache hit rate.\n\nMake sure all parameters that do not directly affect the API's JSON response bodies (for example, a client device platform) are sent in request headers, and instruct Varnish to ignore these headers for its cache key.\n\n\n\nExclude API endpoints that require authentication and authorization from being cached in the proxy.\n\n92\n\nChapter 7 ■ Optimizing YOur apis\n\nSize Matters One of the most common reasons we have seen for sluggish APIs is that no consideration is given to the size of the results returned by a single call to the endpoint. Large results, or sets of results, do not scale, as they hurt performance in at least three key areas:\n\n\n\nIn the service: Large amounts of memory might need to be allocated and deallocated to generate the object from which the results are generated. Once the result objects are created, time is spent serializing them into a wire format (JSON, XML, or the like).\n\n\n\nIn transit: A large number of consecutive packages need to be transferred via TCP over the network. This can be especially harmful in an infrastructure with low bandwidth, low reliability, or high latency, for example when cellular networks are involved.\n\n\n\nIn the client: Clients need to spend a significant amount of time waiting for the arrival of the complete data set. Even if the data connection is broken, a larger timeout needs to be set on circuit breakers to detect failure conditions correctly, so resources will be tied up for a longer time. Once the data has successfully been transferred to the client, significant time is spent deserializing large amounts of serialized data into objects, for which memory again needs to be allocated.\n\nThe rest of this section discusses ways to help you reduce the size of your services' API responses.\n\nResult Representations Not every client needs all the information the service can expose about a given resource or set of resources. Some clients might just need two or three attributes out of potentially tens, or even hundreds, of existing properties to implement their logic.\n\nA good way to provide flexibility with regard to the amount of information returned about an API resource is to honor requests for different representations of an entity. Try to make it easy for clients to define (and iterate over) the best representation for their use case.\n\nConsider a client calling an API in our books-service that returns information about a (set of) book(s).\n\nAll the information about a book that is available to the books-service includes such properties as a full description, a synopsis, and maybe a list of all customer reviews. While it might be entirely appropriate for some clients to retrieve all of this information (maybe in order to display it to the web site user on a detail page), there are many use cases in which the vast majority of this information remains unused (for example, when there is just a small “product tile” about the book to be displayed in an up-sell scenario on another book's detail page).\n\nAnother good candidate for information that might be worth removing from a resource's representation\n\nis any detail information about secondary resources. In the previous example, it might often just suffice to include a book review “star count,” review title, and the review resource ID on an API response for the book resource representation vended to the book's detail page. The full information (if needed) about a particular book review can later be requested from a reviews-service.\n\nImplementing a given set of well-known, predefined representations is often done entirely inside the service that is the system of record for the resource to be retrieved in different representations. Taking the example of our books-service, it gets to define which properties of a book any given named representation includes; clients just request the resource by specifying the name of the representation in which they would like to receive it.\n\n93\n\nChapter 7 ■ Optimizing YOur apis\n\nThe communication of the requested format is most commonly either sent in an Accept header:\n\nGET /books/841a1420-fabe-11e5-9d14-0002a5d5c51b HTTP/1.1 Accept: application/vnd.bookstore.full+json\n\nOr it is sent as an additional representation query parameter, as shown here:\n\nGET /books/841a1420-fabe-11e5-9d14-0002a5d5c51b?representation=full HTTP/1.1\n\nThe service then often stores an internal mapping between the requested representation name (here full), and a presenter class (in this case FullBookPresenter) that has the serialization logic for the resource. The advantage of this service-side control of the properties of a representation comes to light when a presentation requires change. All updates will immediately be made available to all clients that use this representation, and the code change was done in a single place.\n\nThe flip side of the previous point is that in a purely service-controlled representation definition scheme, changes requested by (internal or external) client teams often require cross-team communication and coordination, as the service is most often owned by a different set of engineers than the client applications.\n\nThis is what the partial resources pattern discussed next is trying to address, and we prefer that pattern\n\nbecause it leads to better decoupling and separation of concern in a microservices architecture.\n\nPartial Resources In contrast to the service-controlled resource representation approach, this technique for reducing response sizes is driven by the client.\n\nWhen applying the partial resources pattern, the client specifies the exact subset of resource attributes\n\nof a full set of attributes for a resource it requires as part of the request to the service.\n\nAs an example of how this can be implemented, let's consider the Google Calendar service API.16 To request a partial resource from most Google exposed APIs, the clients can add a fields query parameter. The value of that parameter can then be configured to specify a custom set of the fields the client wants returned.\n\nLet's assume we have the following full representation of a book resource, requested via GET http://\n\nlocalhost:5000/books/582cbc4f-6e04-4d77-a129-616efe01251c:\n\n{ \"book_id\": \"582cbc4f-6e04-4d77-a129-616efe01251c\", \"title\": \"The Story of Diva and Flea\", \"slug\": \"the-story-of-diva-and-flea\", \"description\": \"Diva, a small yet brave dog, and Flea, a curious streetwise cat, develop an unexpected friendship in this unforgettable tale of discovery.\",\n\n\"isbn\": \"978-1484722848\", \"price\": 9.87, \"full_price\": { \"currency\": \"USD\", \"value\": 987, \"exponent\": 2 },\n\n16Google Calendar API documentation, “Partial response”, https://developers.google.com/google-apps/ calendar/performance#partial-response\n\n94\n\nChapter 7 ■ Optimizing YOur apis\n\n\"weight\": null, \"height\": null, \"width\": null, \"depth\": null }\n\nRequesting a partial response for the book resource that will only include the book's UUID, its title, and\n\nthe ISBN number will look like this:\n\nGET http://localhost:5000/books/582cbc4f-6e04-4d77-a129-616efe01251c?fields=book_ id,title,isbn\n\nThe resulting JSON body of the service response should then be as follows:\n\n{ \"book_id\": \"582cbc4f-6e04-4d77-a129-616efe01251c\", \"title\": \"The Story of Diva and Flea\", \"isbn\": \"978-1484722848\" }\n\nThis is a very simplistic example of what is possible with Google's partial response approach. The supported syntax of the fields parameter to specify the elements to include in the response JSON is loosely based on XPath (https://www.w3.org/TR/xpath/).\n\nThis “tell, don't ask” approach to representation definition is very convenient if the clients and the\n\nservice are not under the control of the same team or even company. As a disadvantage, it will be harder to deprecate or change certain fields of a given resource if there is no control over which properties are requested and exposed to client applications.\n\nHTTP Compression Another way of condensing the response size without removing attributes from resources is to condense the actual response.\n\nHTTP compression17 is the standard supported way to losslessly reduce the response body size. The two\n\nmost common compression schemes are\n\nGZip (http://en.wikipedia.org/wiki/Gzip)\n\nDeflate (http://en.wikipedia.org/wiki/DEFLATE)\n\nCompressed HTTP resources are requested by using the Accept-Encoding request header (for example,\n\nAccept-Encoding: gzip, deflate). The service will then indicate in the Content-Encoding response header which type of encoding (if any) has been applied to the response body (Content-Encoding: gzip). Anyone should consider leveraging GZip to compress JSON Responses. Many implementations of this\n\nscheme we have seen have reached a 60–80% reduction in response payload size.\n\nImplementations of these compression schemes are widely available in many languages and\n\nframeworks. JVM-based languages can use java.util.zip.GZIPOutputStream,18 while Ruby applications can use the Rack framework middleware Rack::Deflater19 for both Deflate and GZip-based compression.\n\n17“HTTP compression”, Wikipedia; http://en.wikipedia.org/wiki/HTTP_compression 18Java Platform Standard Edition 8 Documentation, “Class GZIPOutputStream.” https://docs.oracle.com/ javase/8/docs/api/java/util/zip/GZIPOutputStream.html 19RubyDoc, “Class: Rack::Deflater”; http://www.rubydoc.info/github/rack/rack/master/Rack/Deflater\n\n95",
      "page_number": 103
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 113-120)",
      "start_page": 113,
      "end_page": 120,
      "detection_method": "topic_boundary",
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nA more centralized place to handle HTTP compression outside the application server is at the web server level. Examples here are mod_deflate for the Apache web server and ngx_http_gzip_module for NGINX. We recommend compressing inside the web server if only for efficiency.\n\nSemantic Paging Much as there are service clients that will not need all the information a service can expose about a given entity, clients will very often not need all objects in a single response of API endpoints that vend lists of resources (such as search endpoints). Requesting a chunked (often called paged) set of the full result list is often a good way to reduce the size of such API responses.\n\nIn this scheme, the service regulates how many entities for a given resource list can be included in the\n\nresponse. While the client might request the entire list of entities, the service will not allow such a potentially huge response to be returned. If the API were to return all instances of a given resource (for example, all books in the store), then it could easily overrun the memory and processing capacity of the consumer. Instead, the service returns a subset of all requested entities (a page), and a set of links that indicate how to request the previous and/or next page of the result set.\n\nThere are quite a few ways to implement a URI and parameter scheme, but we suggest using pagination\n\nparameters called offset and limit at a resource list's URI to enable paging. An example URL for our books-service to retrieve 25 books, starting at a book with ID 582cbc4f-6e04-4d77-a129-616efe01251c would be the following request:\n\nGET http://localhost:5000/books?offset=582cbc4f-6e04-4d77-a129-616efe01251c&limit=25\n\nThe limit parameter is fairly self-explanatory; it simply signifies the maximum size of the page, so that\n\nin this example no more than 25 books ever get returned.\n\nThe offset parameter has semantic meaning, though. It is meant to contain the value of a sortable property of the resource (such as a UUID, ID, or the creation or update date) based on the most optimal way to access the resource's data. This semantic offset approach gives the engineer who creates the API a chance to choose an optimal way to do an offset into a result set.\n\nGood APIs do not require the consumer to know much about the details of this semantic offset, or\n\neven an ideal page size limit value. Service APIs should lead consumers through the consecutive API invocations to help them along while returning the full result set page by page. This means leading the consumer forward and backward through the full set of results, rather than making them guess or keep track of where to go next.\n\nIn order to achieve this guidance, each API result set should include hyperlinks that provide URIs to the\n\nnext and previous pages (if available). This is one of a set of principles that are collectively called Hypertext as the Engine of Application State (or HATEOAS).20\n\nIn a very simply implementation of a HATEOAS approach, based on the draft IETF standard for HAL (JSON Hypermedia API Language),21 here is how an example response to the earlier request to our books- service might look:\n\nGET /books?offset=582cbc4f-6e04-4d77-a129-616efe01251c&limit=25 { \"_links\": { \"self\": { \"href\": \"/books?offset=582cbc4f-6e04-4d77-a129-616efe01251c&limit=25\" },\n\n20“What is HATEOAS and why is it important for my REST API?” The REST CookBook. http://restcookbook. com/Basics/hateoas/ 21M. Kelly, “JSON Hypertext Application Language”, 3 October 2013; https://tools.ietf.org/html/ draft-kelly-json-hal-06\n\n96\n\nChapter 7 ■ Optimizing YOur apis\n\n\"next\": { \"href\": \"/books?offset=e52cbf4f-6eeb-2067-b249-2345ecd02587&limit=25\" }, \"prev\": { \"href\": \"/books?offset=1345bc34-4ab2-23f3-02da-235ee01234fa&limit=25\" } }, \"items\": [ { \"book_id\": \"582cbc4f-6e04-4d77-a129-616efe01251c\", \"title\": \"The Story of Diva and Flea\", \"slug\": \"the-story-of-diva-and-flea\", ... } ... ] }\n\nIn this example, the “_links” property provides links to help the API client navigate through the result\n\nset:\n\n\n\n\"self\" is the URI identifying the current result set.\n\n\n\n\"next\" is the URI for the next set of results with the same page size.\n\n\n\n\"prev\" is the URI for the previous result set with the same page size.\n\nA Word of Caution\n\nFront-end applications (like web applications) often need to render end-user–exposed views with paging links with data from the API result's _links section.\n\nFor example, when a web application displays a user view containing paginated lists of (for example, book) data, a next link must be embedded into the view, so that the client web app knows where to fetch the next page of API data. Developers should avoid trying to construct new URLs to the API, but rather depend on the next and previous links returned by the API.\n\nIn an attempt to prevent Denial of Service attacks to the internal backing API service, it is recommended\n\nto encrypt the pagination URIs embedded in client views, so that it will be harder for attackers to make arbitrary calls to the (internal) back-end services.\n\nGiven the example results in the previous section, a client web app might generate links in its book list\n\nview that are similar to the following Ruby On Rails/ERB template example:\n\n<a href=\"/results?page=<%= encryptor.encrypt_and_sign(\"/books?offset=582cbc4f-6e04-4d77- a129-616efe01251c&limit=25\") %>\">Next Page</a>\n\nThe code in the end-user–facing web application that renders the /results view can use the value of the page parameter to fetch the next page of data from the backing books-service API, while at the same time shielding books-service from direct exposure to malicious requests.\n\n97\n\nChapter 7 ■ Optimizing YOur apis\n\nOptimizing for Efficiency of Development and Support When designing distributed systems you will encounter many situations in which there is no clear “silver bullet” solution to a problem. The same holds true for the trade-off between incurring development and maintenance costs by duplicating code in a client application, and increasing complexity by introducing an additional dependency in your service' clients.\n\nIn our experience, engineers in companies that are new to implementing an SOA side with the “copy and paste” (that is, code duplication) alternative as their default choice. To clarify our use of “copying and pasting,” we even consider extracting common functionality into a library that is shared among client applications a duplication of code (albeit slightly better than literally copying code from one repository to another).\n\nWhile there might be good reasons to take such an approach, it is often taken without much further analysis or foresight, motivated rather by a focus on the extreme short term, based on the perception that this strategy will unblock a task with the least effort.\n\nBecause it’s our experience that the balance is heavily skewed in favor of code duplication in the real\n\nworld, we have focused this section on listing a number of indications and scenarios where we think that a centralized implementation of a feature should be strongly considered:\n\nDistributed ownership: If not all client code is not owned and maintained by the same team, then the cost incurred in making changes (such as bug fixes or enhancements) is augmented by overhead for coordinating the owning teams, and to line up their respective timelines.\n\nRisky upgrades: Assess the risk for upgrading and deploying the client(s), and compare it to the same assessment for making a change centrally in the service. We have often found that a central change (with the same option to also revert the change centrally) appears to have less risk involved.\n\nAvailability of sane defaults: One of the main risks in centralizing functionality is that a single point of failure is introduced in the dependent clients. This risk can be mitigated by building default values or functionality into the clients to fall back to in case the service API fails. Those defaults could even be delivered as part of a service access library, or a client could be programmed to store the last known good service result, and choose to fall back to it.\n\n\n\nPublic client apps or libraries: If the client applications are released externally, for example, as native applications on mobile or desktop platforms, or as code libraries, you will often find yourself in a situation where you will need to support them quasi-eternally. Update cycles to such public clients are usually much longer than for internal apps. Placing functionality in a central service API to be accessed by thin client-side access code will enable quicker maintenance and extensibility.\n\nNeed for immediacy of change: We have often encountered situations where there is a need for a change to take effect immediately in all clients. This point is related to the previous use case, but it’s even applicable to internal clients. One example could be if an item (such as a book) is available for purchase via client applications. If such a decision was made purely client-side (for example, by calculating an inventory count based on sales numbers, or based on a cut-off date and time), then accounting for more dynamic changes in business logic would be very hard or impossible to achieve. What if you encounter all of a sudden that a book has been declared out of print by its publisher, or that your inventory is otherwise defective? If your make the clients rely on the service to answer the question of book availability, you can much more quickly adapt your business logic across all channels.\n\n98\n\nChapter 7 ■ Optimizing YOur apis\n\n\n\nExperimentation: Often you don't know ahead of time what the best (most revenue- driving, most convenient, most visually appealing, and so on) behavior to implement in your clients will be. Building several options and feature toggles into your clients and collecting data about such experiments in the service is often the best way to address this. Once you have identified a clear winner among the client-side options, you can direct the clients from the service-side experimentation logic to henceforth only use the optimal option.\n\n\n\nSorting and filtering of lists: Imagine you initially planned to implement the book list view in your bookstore clients' search result display logic solely on sales numbers (or another criterion). But then your store strikes a deal with a publisher, who will pay you money to always make their books appear near the top of the search results. Implementing sorting on the service side enables this change to be immediate. Additionally, this helps to reduce payload sizes of results, and enables paging schemes, as the clients do not need to retrieve, and hold in memory, the entire list to sort appropriately.\n\nComplex logic: Functionalities that are nontrivial, or expensive to implement, are better maintained in a centralized place. Often clients don't all conform to the same programming language or framework, so such logic would need to be implemented, tested, and maintained in several different versions (for example, in JavaScript for single-page apps, in ObjectiveC or Swift for iOS apps, and in Java for Android). Because of the complexity, errors would be more likely, and with the larger number of implementations, they would be more expensive to fix. For memory-, CPU-, or I/O-intensive logic, this might become especially prohibitive if the client is particularly constrained (such as bandwidth- or memory-constrained mobile devices).22\n\nTalk Less, Listen More Splitting up your business domain into a large number of small, focused systems of record can potentially have a negative impact on performance: clients that need to mesh up information from different systems of record will often need to make several orchestrated requests to downstream service dependencies. If the requests are made in a traditional, synchronous matter (for example, using an ordinary RESTful scenario), then each of these requests to dependencies has the potential to fail, or to be slow and generally impede the responsiveness of the client performing the aggregation (no matter if the client itself is another service or the edge of the call chain).\n\nAn alternative to the described synchronous dependency calling approach is the “publish and subscribe\" pattern. This is, in rough and general terms, an asynchronous approach that relies on clients registering with an information-providing service, and subsequently waiting for the service to notify the clients whenever the information subscribed to is updated.\n\nWhile there are certainly other ways to implement this pattern, most mid- to large-scale companies\n\nintroduce Enterprise Service Bus (ESB) solutions to handle reliable, monitored, and persisted message registration, marshalling, and distribution. We will not explain particular ESB solutions or patterns in detail, as that would go beyond the scope of this book. We can recommend Hohpe and Woolf's excellent book Enterprise Integration Patterns23 to readers interested in further information.\n\n22This should not be taken as an invitation to neglect resource efficiency when implementing functionality in a service! You don't want to have long-running service code make clients wait and tie up resources, which might cause client-side circuit breakers to trip, or worse make them entirely unresponsive. 23Gregor Hohpe, Bobby Woolf: Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions. Addison-Wesley Professional 2003.\n\n99\n\nChapter 7 ■ Optimizing YOur apis\n\nA popular message bus that we have used successfully for more than a year at LivingSocial is Apache\n\nKafka (https://kafka.apache.org/). Originally developed at LinkedIn, Kafka is a distributed publish- and-subscribe messaging system with a special design focus on scalability, performance, and message persistence. In Kafka, the basic point of interaction is a topic, which can be thought of as a log where messages are constantly stored in an append-only fashion. Messages written to a topic can be identified by their positive integer offset, which indicates a message's location in the storing topic. A topic has a (configurable) rolling window for retaining messages that defaults to seven days. Producers generate messages to be published to a topic, while consumers read and consume messages from a topic. Every consumer registers itself as part of a consumer group, which tracks up to which offset messages have been processed. Consumer groups also ensure that messages are only processed once and round-robin- distributed among multiple consumer processes in the consumer group. To guarantee strong durability and fault-tolerance, Kafka brokers are distributed among several servers in a cluster, and they replicate topic messages among each other. Kafka's design allows for guaranteeing that messages are reliably delivered to consumers in the order they were consumed, albeit in very rare cases more than once. While we will not further explain the concepts behind Kafka; this Apache project comes with an extensive amount of excellent documentation.\n\nIn LivingSocial's case, the engineering best practice is to design a system-of-record service such that\n\nit publishes messages about any change event to entity-type–specific topic. This enables current and future client applications that need information about the published entities to consume some or all of the information in these change events.\n\nAs a result, when synchronous requests are received by these upstream client applications, they are no\n\nlonger required to make secondary synchronous requests to the system of record to check for information about entities for which a change event topic exists; they can instead subscribe to the entity's topic(s) and asynchronously retrieve, store, and finally use the information broadcast therein.\n\n■ Note For the scenario just described, it is not advisable for client applications to simply store full copies of all entities owned by downstream services that they depend on, as that would effectively recreate the system of record's entire data store in each and every client. the goal for the dependent client should instead be to pick out only the limited amount of information about downstream entities that is needed for the aggregating client to perform its own purpose.\n\nA LivingSocial-internal example for such an approach is catalog-service, which was first mentioned in Chapter 3 to illustrate the “aggregation to aid system performance” application-partitioning pattern. The service vends data about all disparate types of products in a uniform and searchable manner. In order to do so, it consumes Kafka topic streams published by the various systems of record for each of the product types, plus additional Kafka topics about data regarding product classifications, and so on. The topic consumers for catalog-service live in an entirely separate demon process than the service's APIs that provide the catalog information to catalog-service's clients. The topic consumers consume, aggregate, and store the topic information into the catalog-service database, while the API process uses read-only access to query the database; no secondary service requests are needed when clients call the catalog-service APIs.\n\nFinding a message-based approach in our fictitious bookstore example laid out in Chapter 3 is similarly\n\neasy. The bestsellers-service, purchase-service, and refund-service can communicate in an entirely decoupled manner. When a purchase is made, a record with the number of copies for each book sold is created in purchase-service, which then turns around and creates a message on a purchase-creation related topic, and the message includes the IDs and sales count for each sold book. Similarly, creation of a refund-service-owned refund record will trigger the publication of a message to a refund-creation topic, and this message also includes information about the number of copies for books that are returned and refunded. The aggregating bestsellers-service consumes both the purchase-creation and the\n\n100\n\nChapter 7 ■ Optimizing YOur apis\n\nrefund-creation topics, and it keeps a count for copies sold and refunded for each of the books listed in these messages, in order to calculate the data it proffers to its own clients (that is, lists of books in various categories, ordered by net sales). No synchronous requests to either the purchase-service or the refund- service need to be made as part of the request cycle to any bestsellers-service API, as the information to calculate the bestseller lists has been precollected via asynchronous topic consumption and stored locally in the bestseller database.\n\nOne of the changes that reliance on a messaging system like Kafka brings along is that your data will only be eventually consistent across all your services that consume the update messages. One such example could be that a user updated her credit card data as part of the purchase flow, but the user’s account detail page still needs some time to catch up on consuming the data change, and hence is still showing the previous details.\n\nIf this behavior seems unacceptable, then a synchronous call to the true data source needs to be made, or workarounds need to be implemented; for example, a secondary (AJAX) request could be made after the account details page is initially rendered, or WebSockets could be used to update the credit card information element upon receipt of the Kafka update message.\n\nSummary In this chapter you learned about ways to reduce our APIs’ consumption of resources (both computer and human). We first discussed the importance of measuring performance before trying to improve it. We suggested some aspects to measure: database queries, service dependencies, memory and CPU consumption, as well as logic probes.\n\nWe next discussed analyzing how service APIs are accessed by their clients and then optimized\n\naccordingly for maximum client performance. Making sure that the information vended is grouped based on the underlying entity's write. Similarly it was explained that data that is often joined together should best live inside a single service, so that cross-service joins can be avoided. Adding separate bulk APIs for the retrieval of very large data sets and using very focused BFF façade services were also explained and proposed as effective improvements.\n\nThe advantages and the potential pitfalls of for caching information in the API-providing service were noted, before schemes to employ the benefits of HTTP conditional GET and its ETag, If-Modified-Since, and Cache-Control headers (including service-side implementation approaches in Ruby on Rails) were discussed. The related option of inserting a caching proxy between the client and the service was also laid out, listing Varnish as a viable option for this.\n\nNext we showed the importance of controlling the size of API responses, along with several ways to do that, including specialized representations, vending resources partially, HTTP compression and semantic paging as ways to help issue responses of optimal size.\n\nWe then addressed the inefficiencies involved in developing and maintaining duplicated code in the clients, and gave a list of eight indicators to consider, each of which might justify centralizing functionality inside a service if these benefits outweigh the cost of introducing an additional service dependency.\n\nFinally, we explained ESBs in general, providing specific detail by using Apache Kafka. We described a real-world use case of employing a message-based approach to improve API performance by obviating the need for secondary service calls. We also described how Kafka could be of use to vending bestseller lists in our bookstore example.\n\nIn the following chapter we will look at a number of ways to handle the nontrivial task of managing\n\nthe development environment in microservices architectures more efficiently, thereby making it less cumbersome and hence more productive to implement.\n\n101\n\nPART III\n\nDevelopment and Deployment\n\nCHAPTER 8\n\nDevelopment Environment and Workflow\n\nSoftware engineering in a microservices environment requires a specific mindset, as well as tools to make development and deployment simple. Any friction caused by a high number of deployment units will put a damper on overall productivity.\n\nIn this chapter we will explain the advantages of a list of measures any organization should take to make\n\ndeveloping in a microservices environment enjoyable and efficient.\n\nThe Premise Your company has decided to wholeheartedly embrace microservices. Given the size and complexity of your business application, and the requirement to be quick and nimble, you have to implement a design with a large number of RESTful services, front-end applications, and backing data stores. Even for smaller applications, having 25–30 individual deployment units is more usual than not; LivingSocial's production environment is made up of hundreds of interconnected deployment units.\n\nYou are a software developer who is tasked with making changes to one of the customer-facing web applications, which also requires enhancements in one of the company's service APIs that provide all the data to the web-applications to be rendered to the end user. Both the web application and the service you need to extend have additional, direct and indirect, service and data store dependencies.\n\nHow do you set up your development environment to make sure you are correctly implementing the\n\nfeature you are tasked to develop?\n\nHow do you run your test suite? How do you pass this feature on to your QA team, and how do they go about testing your changes for\n\nregressions and new features?\n\nAny engineering organization should strive to answer these and other important questions in order to adopt microservices successfully. The following list of attributes of the development environment they should aspire to implement stems from our experience in the matter.\n\nThe Development Environment Wish List The following 12 subsections list circumstances and properties of the software development process that should be present in companies that want to be successful in operating in a microservices environment.\n\n105",
      "page_number": 113
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 121-128)",
      "start_page": 121,
      "end_page": 128,
      "detection_method": "topic_boundary",
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nEase of Development Tool Setup To increase productivity from day one, organizations should avoid requiring their engineers to perform time-consuming and tedious busy work, simply to get their development machine ready. The set of tools needed to be productive as a software engineer in your organization should be easily and quickly installed, and preconfigured, on every new developer’s machine.\n\nNote that these setup steps should be convenient and useful, but they should not be entirely opaque to the developer. Instructions on what is installed and how to customize it should be available to someone joining your team on day one.\n\nEase of Application Setup Additionally, any of the many code repositories for the services or other applications that make up your microservices architecture should provide an easy and preferably uniform way to install (and uninstall) its own specific requirements to develop in, run, and test the application locally on the engineer's computer. This way, your engineers aren't bogged down in trying to understand how any given application they\n\nneed to change is set up on their development machine.\n\nAutomation of Application Tests Automated test suites exercising as many as possible of the five microservice testing layers laid out in the next chapter should exist, and instructions (preferably uniform across projects of the same framework) or— even better—commonly adopted conventions for how to run them need to be included.\n\nThis decreases the risk of introducing regressions in the service behavior, and it also gently nudges developers in the direction of adding test coverage for new functionality by making this step easy. While we focus in much greater detail on the advantages of microservices testing in the next chapter, the main take-away is that automated test suites reduce the chance for costly bugs to make it into the production environment.\n\nAvailability of Sample Data When running any application during local development, sample data to exercise both “happy paths” and exceptional behavior should be available for manual testing. Additionally, it needs to be easy for a developer to extend the sample data sets with new data that exercises the logic that was altered or extended by the code changes under development.\n\nThe arguments for this point are similar to those for test automation (that is, reducing the likelihood of defects), plus the added efficiency during the actual feature development, when developers tend to explore their code changes by manually exercising new and existing code paths.\n\nUp-to-Date Dependencies It is burdensome to all your developers when they have the responsibility of keeping up to date all the service dependencies for the application(s) in which they develop. First, keeping other applications' code and data fresh is a—sometimes rapidly—moving target when there are multiple engineering teams developing in parallel. Second, updating all the applications on the developers' computers is potentially time-consuming and distracts the engineer from the coding task at hand.\n\nAddressing the freshness of application dependencies adds to your developers' velocity, while at the same time helping with the accuracy and reliability of the test scenarios you exercise during development.\n\n106\n\nChapter 8 ■ Development environment anD WorkfloW\n\nFast Developer Machines A related point is that a microservices company should aim to avoid obliging developers to run dependent services (let alone the entire SOA system) on their development machine. This will invariably waste resources on the developers' machines, thereby slowing down their computers and by that token the entire software development process.\n\nNote that using resources more efficiently on a developer’s workstation does not mean we are\n\nadvocating making developers use outdated or subpar computing hardware. Quite to the contrary: the cost difference between providing a developer with a slow versus a fast laptop computer is usually not more than $1,000, which averages over a year of work at around $4 per working day. That is a very modest expense, with great return on investment considering the ensuing gain in developer happiness, efficiency, and productivity.\n\nConcurrency of Development Successful software development organizations want to foster developer collaboration. While some of this can be done via pair programming on the same task in a single service's code base (side-by-side in the same physical location, or remotely using tools like Screenhero1), developers should also be able to collaborate while working in tandem on code bases for separate services.\n\nCollaborative Development Revisiting the premise in the opening section, two or more developers can team up, so that one of them makes changes in the front-end web application while the other developer simultaneously extends the backing service's API to provide the web app with the functionality. Both developers should immediately be able to see changes and receive feedback via their respective development branches of the web app and the service communicating.\n\nThis increases productivity for the developers involved, as they will not need to spend time on elaborate\n\ncode branch synchronization procedures to experience the results of each other's most recent work.\n\nPrivacy of Development Developers of software that lives at the edges of your company's microservices architecture (that is, engineers coding native applications on mobile devices and desktop computers, or web-applications serving HTML and JavaScript to consumer browsers) should be able to test their software without releasing any changes to publicly exposed resources. This means that they should be able to run all their edge systems on local development devices. Additionally, they also need to be able to test their software against unreleased code branches of services they depend upon.\n\nThat way, they will not be obliged to sit idle while waiting for code of service dependencies to be released to production before they can develop features in their own applications. Also, they will be able to provide feedback to the service-providing APIs prior to their public launch, thereby helping to prevent launching (and having to maintain and support) service endpoints that need to be revised soon after launch—it is often very hard, or even impossible, to recall or change APIs once publicly released.\n\n1A collaboration tool that enables remote pair programming. https://screenhero.com/\n\n107\n\nChapter 8 ■ Development environment anD WorkfloW\n\nImmediacy of Change Feedback Once a service-side change or extension has been implemented by a developer, it should get exposure to, and exercise from, all the potential client applications. This should best happen in as many real-world usage scenarios as possible. This service-side change should be visible as immediately after completion as possible, and it should even be visible during the ongoing development and QA cycles of the dependent client applications, and definitely before the changed code is released to a production environment.\n\nThis approach helps finding any breakages or adverse effects of the changed code as early as possible,\n\nand it can be seen as an extension of (and sometimes as a replacement for) component-level testing.2\n\nEase of Service Generation When generating new applications in your microservices environment, developers should not be left guessing about which libraries to use, or what design guidelines, coding style, test tools, and documentation approach to follow.\n\nThe advantages of such out-of-the-box consistency among the services in an organization are\n\nmanifold. For one, developers save time when setting up new services includes sane defaults. Also, support and maintenance costs are decreased when only a smaller set of library, storage engine, and similar dependencies need to be supported. Additionally, it helps synergy and reduces risk for a company if developers can more easily switch between code bases to which they contribute. Making sure that there is a level of uniformity and familiarity among the applications reduces the switching overhead for developers and hence reduces cost.\n\nAccessibility of Documentation Being able to learn about a service's purpose and capabilities is crucial in an environment where potentially hundreds of such microservices and surrounding applications might exist. Developers (and computers) should be able to discover services they need, learn about their APIs and specifications, as well as explore the services' functionalities prior to using them in any code of their own.\n\nThis goal aids in reducing duplication of effort, as chances of reimplementing a functionality that an existing service already provides decrease when a developer can easily find out about what is already there. Additionally, being able to explore, and therefore ultimately better understand, service dependencies that your code will need to rely on helps to produce quality code with less time spent on fixing bugs that are due to incorrect assumptions.\n\nMaking the Wishes Come True Now that we have clarified the software development conditions that need to exist in an organization that wants to successfully adopt microservices, let's look at some processes and tools that can help in implementing these conditions.\n\nAutomating Developer Machine Set-Up In order to make setting up the development tools for any developer easier, a very effective measure is to generate, and keep up to date, scripts to set up development machines. These automated scripts should include the main tools that are common across your engineering organization (such as version and source control tools, programming languages, IDEs or text editors, and communication tools like instant messaging clients or Slack).\n\n2See Chapter 9 for an explanation of component tests.\n\n108\n\nChapter 8 ■ Development environment anD WorkfloW\n\nThere are a large number of configuration management software systems (commercial as well as OSS)\n\nthat can help in automatically setting up, and keeping up to date, all the computers in your organization. Explaining the differences between all the available solutions is a rather fast-moving target and certainly outside the scope of this book. At the time of writing, some of the most popular open-source systems are Ansible, Puppet, CFEngine, and Chef, all of which support Unix-like and Microsoft Windows platforms. Wikipedia has a very thoroughly researched article3 that compares the various OSS solutions and their feature sets.\n\nThe LivingSocial engineering team has automation in place which can install and configure software on the developers' company-provided Apple MacBook Pro laptops. This script will take a computer from a freshly installed OSX image to having the development environment needed to be productive at the company set up on the laptop in less than an hour.\n\nThe project is called sudo-make-me-a-laptop, and it consists of a set of scripts that automatically install git, the Java SDK, rbenv, homebrew, gcc, kafka, emacs, nginx, mysql, Memcached and Redis, as well as some essential internal services.\n\nThe set of scripts in use at LivingSocial is based on Pivotal Labs' Pivotal Sprout toolset,4 and it uses Soloist (https://github.com/mkocher/soloist) and Librarian-Chef (https://github.com/ applicationsonline/librarian-chef) to run a subset of the recipes in Pivotal Sprout's cookbooks, which is the name for a group of scripts used by Chef (https://github.com/chef/chef). A more detailed discussion of the background of the Pivotal Sprout project can be found in a presentation by Pivotal Labs' Brian Cunnie and Abhi Hiremagalur, hosted at http://sprout-talk.cfapps.io/.\n\nAutomated Application Set-Up Similar to providing scripts that install and update the basic tools that any developer needs to contribute on a daily basis, the specific project code under development should also be installed in an automated fashion.\n\nThe responsibility for encoding the specialized knowledge about set-up, build, execution and testing\n\nsteps should lie with the team that owns the specific service or application under development.\n\nConsistently Named Scripts\n\nAn approach to automating application set-up that we have seen work well is to include shell scripts that programmatically set up the specific application and all its required parts as part of the application’s source code repository. This approach should best be paired with organization-wide conventions defining where to find the scripts. For example, there could be a convention to have a bin directory as part of all code repositories, which should include two scripts: install.sh to check for, and potentially set up, all dependencies required for an application to run on a developer's computer, plus an uninstall.sh script to remove all components set up by the install script.\n\nRegardless of having a convention about set-up and tear-down scripts, your team should adopt a convention about shipping each code repository with a file that contains human-readable instructions for setting any given application up for development purposes. Our team adopted a common template for a Readme file included in all GitHub repositories, and this template includes a section entitled “Setting Up Your Environment.” That section defaults to simply listing that a developer should run the well-known scripts (for example, bin/install.sh mentioned earlier), but it should include exact descriptions for any additional steps that cannot be automated (for example, if certain permissions need to be granted via an approval process that involves human intervention).\n\n3See “Comparison of open-source configuration management software,” Wikipedia; https://en.wikipedia.org/ wiki/Comparison_of_open-source_configuration_management_software 4See https://github.com/pivotal-sprout for an overview of Pivotal Sprout tools and recipes.\n\n109\n\nChapter 8 ■ Development environment anD WorkfloW\n\nDocker and Docker Compose\n\nThere are many ways to implement the actual dependency declarations and installation automation process. We have seen configuration management tools like Chef, mentioned in the previous section on developer machine set-up, being also used for describing and setting up dependencies of individual code repositories.\n\nAt LivingSocial, the engineering team is in the process of employing container technology via the use\n\nof the Docker tool chain (https://www.docker.com). The Docker open-source project aims to automate installation and deployment of applications inside LXC containers.5 It provides a layer of abstraction and automation on top of LXC containers, and it allows multiple applications to run independently within a single Linux host.\n\nIn order to run Docker on our development Apple computers, we are also using Docker Machine\n\n(https://docs.docker.com/machine/), which enables us to install and run Docker on a Mac.\n\nWe also use the Docker Compose tool (https://docs.docker.com/compose/) to define, combine, and\n\nrun multiple Docker containers to run the dependencies of our application under development.\n\nNote that for now, we are installing Docker Machine and Docker Compose in a separate step for newer service projects. As these tools become more widely used, they should be moved into the general developer machine set-up scripts.\n\nAs an example, let's walk through the set-up in one of the services we use in our day jobs. The service allows for fast geolocation-based searching of offer data, based on data aggregated from\n\nthe systems of record for the products on offer. We chose to implement it as a light wrapper service that fronts an instance of Elasticsearch, or ES (https://www.elastic.co/products/elasticsearch). The service populates its ES instance with offer data retrieved by subscribing to Kafka topics about product data updates. The Docker Compose configuration file used to declare ES and Kafka as dependencies is shown in\n\nListing 8-1.\n\nListing 8-1. docker-compose.yml file for the search service\n\nversion: '2' services: messaging: image: \"spotify/kafka\" ports: - \"9092:9092\" - \"2181:2181\" environment: - \"ADVERTISED_PORT=9092\" env_file: - ./config/docker/kafka.env\n\nes: image: \"elasticsearch\" ports: - \"9200:9200\" - \"9300:9300\" volumes: - /mnt/sda1/var/elasticsearch/data:/usr/share/elasticsearch/data\n\n5Linux containers, a way to virtualize multiple, isolated Linux systems on top of a single Linux host machine. See https://en.wikipedia.org/wiki/LXC\n\n110\n\nChapter 8 ■ Development environment anD WorkfloW\n\nAfter declaring the docker-compose file format version, the services section in the file lists the search\n\nservice's dependencies that will be run as Docker containers. In this case, those prerequisite applications are labeled messaging for Kafka messaging and es for the Elasticsearch. Both container images for ES and Kafka will be retrieved from the official Docker Hub container repository (https://docs.docker.com/docker- hub/).\n\nThe ports section defines that host ports for zookeeper and brokers (for Kafka), as well as for the native\n\ntransport and RESTful API of ES are forwarded from the host machine to the containers' equivalent ports. The environment and env_file sections help you declare Unix shell environment variables, which are\n\nresolved to their values on the machine on which Docker Compose is running.\n\nFinally, the volumes section allows for mounting paths or named volumes from the container into the\n\nhost's filesystem, so they can be accessed on the host.\n\nDocker Compose defined containers are then installed (on first use) and launched based via the docker-compose up command, which is based on the docker-compose.yml listed earlier. Listing 8-2 shows bin/start_docker.sh, a short bash script that makes Docker use the default machine, adds the Kafka host into the container's environment variable, and finally starts all the containers via docker-compose up.\n\nListing 8-2. bin/start_docker.sh script to start the docker containers\n\n#!/usr/bin/env bash\n\neval $(docker-machine env default) mkdir -p config/docker echo \"ADVERTISED_HOST=`docker-machine ip default`\" > config/docker/kafka.env docker-compose up $1\n\nUsing Consistent Start-Up Scripts\n\nAnother convention adopted at LivingSocial engineering to make running applications on developer machines homogenous is to use the Foreman Ruby gem (https://github.com/ddollar/foreman).\n\nForeman helps with defining a set of processes simultaneously. The list of processes to run by Foreman\n\nis managed via a file called a Procfile. Its aim is to allow you to run your application directly by executing a single command, like so:\n\nforeman start\n\nThe nice thing about this abstract approach is that one can easily combine Foreman and Docker\n\ncontainers. See Listing 8-3 for an example.\n\nListing 8-3. Procfile for the search service, executing bin/start_docker.sh\n\ndocker: bin/start_docker.sh service: bundle exec rails s -p3052\n\nThe Procfile has entries for two processes to start. The service entry starts our Rails-based search\n\nservice mentioned earlier, while the docker entry starts the Docker Compose script.\n\nMaking Testing Efficient with Common Tools Much as it does with setting up the prerequisites of an application for development efforts, the responsibility of making testing as easy and efficient as possible lies with the development team owning the service.\n\n111\n\nChapter 8 ■ Development environment anD WorkfloW\n\nConsistent Process and Documentation\n\nA first step for this could be to make sure there is uniformity in running an applications test suite. A well- structured template for a Readme file for all your code repositories featuring a “Running the Tests” section will go a long way. In addition to tips about how to trigger a run of the automated test suite, such a section can (and should) include detailed instructions for how to run any additional smoke tests that— for whatever reason—cannot be, or simply are not, automated. Our team implemented a similar approach by including a checklist of manual verification steps to perform as part of a change request. The checklist, in the form of a to-do list, is included in the GitHub pull request template6 for our repositories.\n\nSuch a Readme-based documentation template can supplement, but will never fully replace, the approach of using a script with a well-known name and location in the code repository as the agreed-upon convention to run all tests.\n\nLivingSocial engineering therefore is in the process of introducing a shell script—uniformly named run_tests.sh, and located in any project's bin/ directory—that every developer (and ancillary automated tool) can expect to exist in a repository.\n\nAnointing a Test Framework\n\nAnother great way for an engineering organization to help make the testing process more efficient is to settle on a very small set of test frameworks, ideally one test framework per language in use. This reduces the cognitive load for every developer new to a project.\n\nAs an example, all of LivingSocial's Clojure developers use Stuart Sierra's clojure.test test framework\n\nfor unit tests (https://clojure.github.io/clojure/clojure.test-api.html). This was not just a subconscious choice or coincidence. The decision to use this framework was deliberate and preceded by thorough discussions and comparisons with other frameworks (like Brian Marick's Midje, https://github. com/marick/Midje). On the other hand, for Ruby applications in the same engineering organization, such a discussion had not happened for a long time, leading to a very wide range of different testing frameworks in the wild. This has been recognized as a deficiency and a cause for much loss of efficiency, which is why new Ruby applications have recently been unified to adopt the RSpec, Cucumber, and Capybara frameworks.7\n\nIf an organization sees a reason to replace one recommended test framework with a different one, then\n\nwe recommend slowly updating any existing test suites using the deprecated frameworks in an “as you go” manner. In our experience, justifying the (often large) effort to switch all old-style tests to the new framework in a single project is very hard. A more practical approach would be to write all additional tests using the new framework, while only moving the existing test case to the new framework whenever it, or a small set of tests that include the given test case, needs modifying.\n\nImplementing Test Code Guidelines\n\nBut just picking a (set of) supported framework(s) does not entirely suffice to help reduce maintenance effort and to flatten the learning curve for test-related code. Widely published style guides should exist not just for coding, but also for tests. Software consultancy Thoughtbot (https://thoughtbot.com) has published a succinct yet effective style guide for their developers to follow when writing tests using RSpec and Cucumber. Figure 8-1 shows the succinct but effective guide at the time of writing.8\n\n6For a description of how to set up Pull Request templates in your repository, see https://help.github.com/ articles/creating-a-pull-request-template-for-your-repository/. 7See http://rspec.info/, https://cucumber.io, and http://jnicklas.github.io/capybara/; more details on how to use these frameworks will be presented in Chapter 9. 8See https://github.com/thoughtbot/guides/tree/master/style/testing for the full test style guide at Thoughtbot.\n\n112\n\nFigure 8-1. Test-related section of Thoughtbot's coding style guide\n\nChapter 8 ■ Development environment anD WorkfloW\n\n113",
      "page_number": 121
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 129-139)",
      "start_page": 129,
      "end_page": 139,
      "detection_method": "topic_boundary",
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nIntroducing Concurrency of Test Runs and Development\n\nAnother valuable tool to support developer efficiency during the development and test phase is Guard (https://github.com/guard/guard). While Guard is a generalized automation tool that triggers customizable actions whenever observed files or directories are modified, it becomes particularly useful when integrated with the development and test cycle.\n\nMany Guard plugins for test frameworks exist,9 and we use guard-rspec (https://github.com/guard/ guard-rspec) to support automatically running tests on Ruby-based projects. The guard-rspec gem allows you to specify rules that determine the correct subset of all tests to run, based on which file containing business logic changed.\n\nGuard can be configured based on a file called a Guardfile, in which you can specify files and directories to watch, and which action to take based on changes to the watched entities. Actions and watch definitions can be expressed in a DSL. Listing 8-4 shows an excerpt from a sample Guardfile from guard-rspec's documentation,10 which is focused on auto-executing the RSpec specs for a prototypical Ruby on Rails application.\n\nListing 8-4. Guardfile for a typical Rails application\n\nguard :rspec, cmd: 'bundle exec rspec' do watch('spec/spec_helper.rb') { \"spec\" } watch('config/routes.rb') { \"spec/routing\" } watch('app/controllers/application_controller.rb') { \"spec/controllers\" } watch(%r{^spec/.+_spec\\.rb$}) watch(%r{^app/(.+)\\.rb$}) { |m| \"spec/#{m[1]}_spec.rb\" } watch(%r{^app/(.*)(\\.erb|\\.haml|\\.slim)$}) { |m| \"spec/#{m[1]}#{m[2]}_spec.rb\" } watch(%r{^lib/(.+)\\.rb$}) { |m| \"spec/lib/#{m[1]}_spec.rb\" } watch(%r{^app/controllers/(.+)_(controller)\\.rb$}) { |m| [\"spec/routing/#{m[1]}_routing_ spec.rb\", \"spec/#{m[2]}s/#{m[1]}_#{m[2]}_spec.rb\", \"spec/acceptance/#{m[1]}_spec.rb\"] } end\n\nThe guard command block specifies that RSpec should be run (via cmd: 'bundle exec rspec'). The\n\nwatch statements define directories or filenames (directly, or via regular expression matching), and the blocks following the watch expressions list all the test files for which to re-execute the specified cmd if a particular watched (set of) file(s) has changed.\n\nUsing Guard or other automated test execution frameworks, like Autotest (https://github.com/ seattlerb/zentest), provides developers immediate feedback about the effect of their changes in a potentially unfamiliar code base. Forgetting to run, or deliberately choosing not to run, the application's test suite at the end of the contribution cycle will no longer be an option. And because test runs are incremental and only focused on the files currently under change, the cost and impact on productivity of running tests is drastically reduced.\n\nIntegrating Static Analysis Tools into the Test Process\n\nThe introduction of static analysis tools can also help with teaching (and enforcing) coding guidelines and testing etiquette. The Code Climate platform (https://codeclimate.com/) is a commercial tool that runs a configurable set of analysis steps on every code commit, and then generates reports on adherence to coding standards, code complexity, code duplication, or percentage of test coverage. Figure 8-2 shows the summary page of a LivingSocial-internal service repository monitored by an on-premise installed version of Code Climate.\n\n9For a full list of plugins, see https://github.com/guard/guard/wiki/Guard-Plugins 10See the “Typical Rails app\" section at https://github.com/guard/guard-rspec#typical-rails-app\n\n114\n\nChapter 8 ■ Development environment anD WorkfloW\n\nFigure 8-2. Code Climate report for an application called catalog-service\n\nA simpler and, for commercial use,11 less costly approach to encouraging attention to test coverage in the Ruby environment is SimpleCov (https://github.com/colszowka/simplecov), a code coverage analysis tool that makes use of Ruby's built-in coverage-tracking mechanism to collect data about code coverage. SimpleCov lets the developer easily process, format, and display the code coverage data collection results in just a few lines of Ruby code.\n\nIn order to guide developers into adhering to safety and maintainability standards for a given code base, the development team should explore whether a service's test suite can be made to fail whenever code coverage falls below an acceptable threshold value. When using the RSpec test framework and SimpleCov in the Ruby world, you can configure RSpec to fail the run if SimpleCov reports a decline in coverage percentage.\n\nA Spec helper file that implements this for whenever coverage falls below 85% is shown in Listing 8-5.\n\n11Code Climate analysis can be added for free to any open-source project.\n\n115\n\nChapter 8 ■ Development environment anD WorkfloW\n\nListing 8-5. spec/spec_helper.rb that fails test suite runs below 85% coverage\n\nrequire \"simplecov\"\n\nSimpleCov.start\n\nRSpec.configure do |config| config.after(:suite) do simplecov = RSpec.describe('Test coverage') example = simplecov.example('must exceed 85 percent'){ expect( SimpleCov.result.covered_percent ).to be > 85 } simplecov.run\n\nif example.execution_result.status != :passed RSpec.configuration.reporter.example_failed(example) end end end\n\nContinuous Integration to Improve Code Reviews\n\nIf you want to use a less forceful approach than failing the entire test suite, but still want to keep track of declining code coverage numbers, you can instead (or additionally) try and integrate coverage checks into your Continuous Integration (CI) build system.\n\nWhen using the Jenkins project's CI system (https://jenkins.io/), it can easily be extended with plugins to set up a workflow that processes SimpleCov-generated code coverage data. The DocLinks plugin (https://wiki.jenkins-ci.org/display/JENKINS/DocLinks+plugin) can be used to display formatted coverage reports, as shown in Figure 8-3.\n\nFigure 8-3. Jenkins shows SimpleCov reports via the DocLinks plugin.\n\n116\n\nChapter 8 ■ Development environment anD WorkfloW\n\nThe RubyMetrics plugin (https://wiki.jenkins-ci.org/display/JENKINS/RubyMetrics+plugin) for Jenkins can be configured to adapt the CI build symbol and status based on the SimpleCov coverage percentage (see Figure 8-4).\n\nFigure 8-4. Jenkins RubyMetrics plugin configuration for build health assessment\n\nConfiguring your RSpec test suite is easy, and Listing 8-6 shows how it is done. It presents a Spec helper\n\nfile that implements this for whenever coverage falls below 85%.\n\nListing 8-6. spec/spec_helper.rb that generates reports processable by the DocLinks and RubyMetrics plugins\n\nrequire \"simplecov\" require \"simplecov-rcov\"\n\nSimpleCov.formatters = [ SimpleCov::Formatter::RcovFormatter, SimpleCov::Formatter::HTMLFormatter ]\n\nSimpleCov.start\n\nAnother great use case for Jenkins that makes testing newly developed code more efficient is to integrate it directly with the code version control system. Many engineering organizations set up Jenkins to detect new branches or pull requests for a given GitHub repository, and they execute the test suite automatically. This helps the development team owning the service to review any change requests within a shorter feedback loop.\n\nShipping Sample Data with the Code During the development cycle, engineers usually do not want to rely only on test-driven development (TDD) and automated tests to implement their features. A seasoned developer will also try to start the service they are altering locally, to see the changes made in effect immediately, and as perceived by clients. This also helps in efforts to use exploratory testing12 to verify new features or other code behavior changes.\n\n12See the next chapter for further notes on exploratory testing.\n\n117\n\nChapter 8 ■ Development environment anD WorkfloW\n\nTwo Types of Data Sources\n\nLet us first define the two main categories of data sources for which we will need to provide sample data when making changes to a service: a data store in which the service application keeps the data it owns itself, as well as another service application that provides data to the service under development.\n\nThe rest of this section will be structured around several different ways that we have seen successful microservices applications set up such development-time sample data, starting with sample data for entities owned by the service itself, and then addressing sample data and logic provided by a service dependency.\n\nProviding Samples for Data Owned\n\nHere are some ways in which samples of service-owned data can be procured for local development.\n\nSeeding Scripts\n\nProbably the most common set-up to run a service application in which you have made changes and now want to explore locally on your development machine is to also run the application's data store on the development machine.\n\nIn this situation, running automated scripts to populate—or reset and clear out—the service-owned\n\ndata store is often a viable option to provide sample data.\n\nWe do not recommend any particular automated approach to performing such seeding tasks, as many\n\ndevelopment frameworks allow for easy implementation of adding data to a store. As always, we recommend using a standardized approach across your organization applications to foster familiarity by consistency.\n\nIn Ruby on Rails, for example, developers often use the rake tool to run this automation. For example, the built-in rake db:seed reads and executes the Ruby code contained in the db/seeds.rb file in a Ruby on Rails project.\n\nAnother, very similar, option is to populate the development instance of your service with the data\n\nsets that are used for your automated unit tests. In Rails, this data often lives in so-called fixture files, and Rails provides you with the db:fixtures:load rake task to use these fixtures and move them into the development environment.\n\nProduction Snapshots\n\nAnother approach we have seen used in cases that require more exhaustive, or more authentic and “production-like,” data is to work on snapshots of the production data store.\n\nOne way to implement this scenario is to take periodic snapshots of the data store in the production\n\nenvironment (for example, dumping the tables of a SQL database to a file).\n\nThese data dumps are then stored (and optionally compressed) on an internal file server to which all developers have access. To use the dumps locally, the service code base could again run automated bootstrapping scripts to download, decompress, and then install the dumps into the locally running data store.\n\n■ Note if you take this approach, make sure to scrub the production data of any sensitive information, such as personally identifiable information about your customers, passwords, financial information, and the like. You should anonymize, randomize, remove, or otherwise obfuscate sensitive data.\n\n118\n\nChapter 8 ■ Development environment anD WorkfloW\n\nConnecting to a Shared Data Store\n\nSome microservices companies follow a pattern in which they provide a sandbox environment where all the services are instantiated and running, outside and in addition to their production environment.\n\nWhile we will explain more about the advantages of such a set-up later in this chapter, it is relevant to the sample data discussion as well, as it allows the service-owned data to live outside the developer's machine. Instead, the service instance under local development will in this scenario be configured to connect to the data store(s) that serve(s) the instance(s) of the same service in the sandbox environment.\n\nThis way, not every developer is obligated to install and update the sample data on their local machine,\n\nas the sample data can be centralized in the sandbox data stores.\n\nA downside to this approach is certainly that it will be impractical to use for development of changes in the service that require a breaking change to the schema of the sandbox data store, as that would effectively incapacitate the sandbox instances of that same service. Also, it needs to be clear that the mutable state of the shared data store that is in use in the sandbox environment can change at any time in unpredictable ways. For example, purchases made or users created in a sandbox for our bookstore example cannot easily be coordinated across developers concurrently using the sandbox.\n\nProviding Samples for Downstream Services\n\nNext we will show to address dependencies on sample information for services upon which the application under development depends.\n\nProviding Client-Side Samples\n\nWhen a service has other services as downstream dependencies, then usually these service dependencies run remotely and outside of the dependent service's process boundaries and are accessed over a network boundary.\n\nHowever, the development scenario discussed here removes cross-process boundary invocations and\n\ninstead replaces them with in-memory calls. As a consequence, this means that all code that is reaching out to other services in a production scenario needs to be intercepted and rejiggered to respond with sample data that is generated inside the service process that is under test.\n\nOne option to implement this would be to first record live sample communications between the service under test and all its dependencies in production, and then change the service’s development configuration to replay the recorded conversations appropriately when the service under development is run locally. Even though there are options available in almost all programming languages and frameworks to\n\naddress this record-and-replay approach, it is cumbersome to set up and expensive to maintain for any environment with more than a handful of service dependencies. See the next chapter’s section “Out-Of- Process Component Tests” for more examples of such frameworks.\n\nA slightly lower-effort (because more centralized) way of generating all service responses client-side is to have the teams that maintain downstream services ship libraries that can simulate sample data and behavior of their service. This technique is explained at length in the “Pluggable Back-Ends” section of the next chapter. While the focus in that chapter is on using this feature in an automated test suite, it can certainly and easily be adapted to be used in a general development environment.\n\nThe least costly manner in which to stub service dependencies in-process is to auto-generate the\n\nclient-side, in-process service stubs. This can be done either by promoting stubs generated by contract- testing frameworks13 to be used in the development stage with the service responses or by auto-generating a pluggable back-end library for each service based on the service's API specifications document. Support for the latter approach is unfortunately very service-framework specific and not a widely available option.\n\n13Chapter 9 offers insights into frameworks for this purpose, such as pacto, in the sections on contract testing.\n\n119\n\nChapter 8 ■ Development environment anD WorkfloW\n\nMocking Services with Data\n\nA twist on the just-described approach of providing mock data for a service dependency is not to alter the service under development to use in-process/in-memory doubles for their service dependencies. Instead, you provide mocks for the downstream services run remotely and out-of-process, just as the actual downstream dependencies do in a production environment.\n\nThe level of automation involved in generating these service doubles is again the deciding factor in assessing the technique’s usefulness at scale. As a rule of thumb, if your organization employs more than a handful of interdependent services, hand-coding mocks for each one of them is going to be a productivity killer.\n\nThankfully, most service definition frameworks come with options to generate such mock services\n\nin an automated way, based on a computer-readable document that describes the APIs. The next chapter describes some of these tools in the \"Contract Testing\" section. Other (commercial and free) frameworks exist, such as RepreZen (http://reprezen.com/swagger-tools) and vREST (https://vrest.io/) for the Swagger tool chain.\n\nConnecting to a Shared Environment\n\nAs mentioned in the section \"Connecting to a Shared Data Store,\" many organizations using microservices run an additional shared sandbox environment for all their services.\n\nThis approach can be used to provide desired realistic sample data without having to run the\n\ndownstream services locally. The service under test will be configured to rely on the sandbox instances of its dependencies. This service configuration can of course take place outside the service under development itself; for example, in a DNS, or a Consul (https://www.consul.io/) install, which resolves the locations of the dependencies.\n\nIn this scenario it is most advisable to have an organization-wide development policy in place which states that outages in the sandbox environment should be treated with an urgency that is only trumped by tackling issues currently surfaced in the production environment. If your organization cannot adequately implement such fast-response support for the shared environment, developers will be left only with the option of resorting to running service dependencies locally on their workstations. This has the disadvantages mentioned earlier, like slowing down the developers’ machines, and the use of potentially out-of-date dependency code, but it is still better than having no end-to-end testing facilities prior to promoting code to production.\n\nMaintaining a Development Sandbox As alluded to in two of the previous sections, a key approach for making development in a microservices environment more effective and fun is to provide a shared sandbox environment.\n\nThis can be seen as a switched, self-contained network fabric whose topology is such that only the participating service nodes interconnect. In this scenario, the fabric of services is solely meant to enable an improved software development and testing process, in order to shorten release cycles while at the same time decreasing defects shipped to production.\n\nFigure 8-5 shows a set-up of three separate network fabrics (sandbox, staging, and production) into\n\nwhich the entire microservices application is deployed.\n\n120\n\nChapter 8 ■ Development environment anD WorkfloW\n\nFigure 8-5. Interconnected services in the sandbox fabric\n\nSoftware developers who need to make changes to a service will alter and run just the particular service under change on their developer workstation. If the service in which they develop, and which they hence need to run on their workstation, depends on other services or data stores, then the locally running service instance will be configured to use the service and data store dependencies deployed to the sandbox environment.\n\nIntegrating a sandbox environment into the development process can address many items on our\n\ndeveloper wish list. Let's look at each of these items in a little more detail.\n\n■ Note the use of a dedicated sandbox environment is our own recommendation based on our years of experience in developing in microservices architectures, but the approach introduced here is by no means uncontested. as our former colleague evan phoenix, Ceo of vektra innovation (http://vektra.com/), poignantly describes in a post on his personal blog:14\n\nThe industry needs a common set of practices for how to develop microservices. […] Microservice development is still in its infancy. Teams are slowly figuring out solutions to their specific problems and largely hacking something together.\n\n14Evan Phoenix, \"Microservice Development - Who Runs What Where\", April 17, 2016. http://phx.io/posts/ microservice-dev/\n\n121\n\nChapter 8 ■ Development environment anD WorkfloW\n\nProviding Up-to-Date Dependencies\n\nIn a scenario where developers can rely on the development sandbox to run all services and systems that the code base they are currently altering is running, they are relieved of many cumbersome and time- consuming chores.\n\nThey will no longer need to make sure they have the latest version of their service's dependencies\n\nrunning locally on their own workstation. They simply have to make sure that their local setup is communicating with the service and data dependencies in the sandbox fabric.\n\nNote that this has an impact on the discipline of your development process. The responsibility for keeping each of the microservices in the organization fresh in the sandbox resides with the teams that own them. This means that there needs to be internal monitoring and support dedicated to making sure that the systems in the sandbox are running during business hours.\n\nAs an example, Amazon's development organization ensured this by making sure that owner teams of sandbox services were easily identifiable, and it was acceptable to page the owning team's engineer on-call when outages were perceived.\n\nSaving Resources on Developer Workstations\n\nThe point of reducing load and resource consumption on the developer workstations is to increase developer velocity by not wasting their time waiting for unresponsive applications.\n\nRunning the majority of the systems and services making up your microservices architecture on dedicated computing resources definitely helps to offload developer stations: no memory, CPU processing power, or disk space needs to be dedicated to install and run the service dependencies locally.\n\nFostering Concurrent and Collaborative Development\n\nIn our many years of experience we have learned that there is no better way to increase the productivity of your development team than to encourage forming close-knit working relationships. Creating an environment in which solving problems and developing features is fun, and which leverages the combined skills of everyone on the team, is key to the successful growth of your engineering organization. Software development organizations are most successful where collaboration is engrained in the culture. A development sandbox helps greatly with these efforts.\n\nImagine a situation in which a new feature is to be added to our sample bookstore application, to enable customers to save favorite searches, and to be notified by a tool of their choice (for example, via email, mobile push notifications, or pop-up messages on the site). This feature requires changes in various pages in the bookstore front-end, the mobile applications, and a fair number of back-end services (new functionalities to associate users and searches, to periodically re-execute the searches offline, to notify the users of new results, and so on.).\n\nTypically, these changes are most efficiently performed when expert developers focus on the various parts: front-end specialists take care of the HTML, JavaScript, and CSS changes in the desktop and mobile web applications, iOS and Android developers change the native apps, and back-end developers from each of the teams that own the backing services are adding or changing service APIs.\n\nIn this very common situation, having an environment where developers from all these teams can collaborate by pushing their latest code branches to the application instances running in the sandbox fabric has a number of advantages. For one, the velocity of development is increased by having subject matter experts focus on making the changes concurrently. Additionally, communication and collaboration between the participating developers is encouraged very early on in the feature development cycle, so that less time is wasted on hunting down missed or unclear requirements, or potential bugs.\n\n122\n\nChapter 8 ■ Development environment anD WorkfloW\n\n■ Note seen organizations encourage collaborating developers to use a single workstation to develop the changes in all affected systems. in this scenario, the participating engineers would all be given access to a shared workstation, which they would use to make the changes to the two or three services involved. they would then run these development branches of these services simultaneously on the workstation, and connect them with each other, while still utilizing the sandbox environment for all other dependencies. if the resources needed to change and run the applications under development locally do not noticeably slow down the workstation, then this route can cut down feature development time even further, as it obviates the need for deployment of the development branches to the sandbox instances.\n\nin situations where only a few applications need changing to implement a feature, we have\n\nShortening the Feedback Loop for Changes\n\nAnother advantage of using a sandbox environment stems from the fact that all developers will use the services deployed there. It means that all engineers will exercise the services deployed there day in and day out, as the code running on their own developer workstations depends on the proper functioning of the services in the sandbox.\n\nThis simple fact greatly increases the chances of finding bugs and regressions very early on, even during\n\nthe development cycle of a new feature and before any other quality assurance measures. Additionally, for companies whose release process includes quality assurance on an end-to-end basis, the QA team can use the sandbox environment to perform such application-wide testing well ahead of releasing any code changes out into a production environment.\n\nOne caveat of using the sandbox for true QA purposes is that it would be advisable to enforce a brief code-freeze during QA times for the sandbox, so that verification results are not invalidated by any mid-test changes.\n\nIn order to prevent these inaccuracies and the potential slow-down to ongoing development efforts,\n\nmany companies introduce an additional network fabric, often referred to as \"staging\" (see Figure 8-5 earlier). This is an interstitial environment between sandbox and production, which is mostly prepopulated with production-like data sets, so that end-to-end QA efforts can happen in a setting that very closely mirrors actual production scenarios.\n\nUsing a Rewriting Proxy for Edge Device Development As mentioned in the wish list section under \"Privacy of Development,” it is important that applications installed on edge devices (like iOS, Android, or other native OS applications) can be tested against service- side changes prior to releasing these changes to a production environment. This not only aids in security and stability concerns, it also benefits development velocity, as developers will not need to wait for changes to be deployed all the way through the stages out into a development fabric before they can be tested with the native applications.\n\nThe best way that we have seen this situation addressed is via the use of a proxy service. In this\n\napproach, the developer would perform the following steps:\n\n1.\n\nInstall a proxy server in the same network fabric where the service instances under change are running (usually either on the local developer workstation or inside the company's VPN in the sandbox fabric).\n\n2. Change the network settings on the edge devices (for example, iPhones or Android tablets) participating in the development to direct all traffic via the previously set up proxy server.\n\n123\n\nChapter 8 ■ Development environment anD WorkfloW\n\n3. Configure the proxy server with a rewriting rule, so that requests from the edge devices to the production domain of your application are being rerouted to the internal (workstation- or sandbox-hosted) instances of your services.\n\n4. Make sure the edge device is able to join (for example, via Wi-Fi) the network in which the proxy server is exposed, so that subsequently all requests from the device can be routed via the proxy to the respective services under development.\n\nWhile there are many proxy services available that will allow these features to be implemented, we\n\nhave found CharlesProxy (https://www.charlesproxy.com/) to be very feature-rich, yet easy to use, and available for all major desktop operating systems (Mac OSX, Windows, and Linux).\n\nFigure 8-6 shows the CharlesProxy rewrite settings page, configured such that any device sending requests for resources in the books.com domain would instead be serviced by the equivalent hosts in the int.books.dev domain (which you can configure to be the domain for your workstation or sandbox fabric, for example).\n\nFigure 8-6. Example host rewriting settings in CharlesProxy for our fictitious bookstore\n\n124",
      "page_number": 129
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 140-150)",
      "start_page": 140,
      "end_page": 150,
      "detection_method": "topic_boundary",
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nCodifying Internal Standards via Generation Tools Any development organization should make sure to take the guesswork out of setting up new services and applications. Doing so will aid in three areas:\n\nDevelopment speed\n\n\n\nEfficiency in application support and maintenance\n\n\n\nFostering standards and familiarity across applications and frameworks within the development team\n\nOur advice is to develop scripts and tools that can automatically generate service repositories or internal libraries. The code for the service applications so generated should be prepopulated with the approved libraries, configuration files, and preferably even skeleton code and test examples. A newly generated library could simply add certain artifacts (like a change log or Readme file) in consistent places in the directory tree as defined by the organization’s conventions, thereby getting the developers started down the right path.\n\nExamples for such generation tools can be found in many companies that have successfully adopted\n\nmicroservices. Amazon is running an internal web application which sports a web wizard that walks developers through all the steps to generate a service stub conforming to their internal Coral service framework. Developers can choose from options like the programming language to use, and the internal organization and code repositories in which to create the artifacts.\n\nLivingSocial is streamlining the application generation process through a set of templates-based Ruby\n\nscripts. The tool is called ls-forge and is very easy to invoke from the command line:\n\n$ ls-forge rails create ./path/to/new_service\n\nThis command starts an interactive shell-based questionnaire to generate a new Rails API based service. During its course, it asks the user about choices regarding the use of test framework options, selection of approved data store options, the use of crontasks, caching back-ends (like Memcached) and the need for messaging (for example, by including Kafka support), and so on. The generated applications include standard gems (both internal and external) and code stubs that are preconfigured to support test frameworks, circuit breakers, documentation tools, and monitoring and alarming.\n\nThe generated code often does nothing more than expose a \"Hello World!\" API functionality. Still, the trivial example code can be used to showcase an acceptable coding style and other API design guidelines, thereby setting the tone and “soft-enforcing” consistency and best practices. In LivingSocial's case, for example, one feature of the generated application is that APIs are documented via Swagger specifications, and a Swagger UI explorer can be launched locally.\n\nThe main point for such tools is to help consistency between applications, so that developers do not\n\nhave to overcome a high barrier when switching between them during development.\n\nMaking Documentation Search-Efficient With the adoption of a large number of fine-grained service applications also comes the need to make it easy to discover documentation about things like service specifications, characteristics, mission, ownership and development roadmap. Aiding both humans and computers in discovering service attributes can have a great effect on maintainability, stability of your infrastructure, and time-of-production for your business- critical features.\n\nIn our time working with microservices we have not seen many universally accepted solutions to this\n\nchallenge. The only common thread we have been able to determine is that most organizations discover the need to address inconsistencies or lack of documentation rather late in their service adoption process.\n\n125\n\nChapter 8 ■ Development environment anD WorkfloW\n\nIt is fairly easy to start small, though. Amazon, for example, started out with the introduction of nodes describing each service on their company-internal Wiki installation. LivingSocial has adopted Atlassian's Confluence (https://www.atlassian.com/software/confluence) as a company-wide documentation tool, and teams have generated page templates to use for each service they own. These templates include information on team contact details, additional documentation, URL for an API explorer web tool, configured alarms, monitoring dashboard links, production console access, standard operating procedure in case of outages, as well as operational support and SLA agreements.\n\nA really useful addition to service documentation is a web UI that lets you easily explore the APIs exposed by a service. One such example is to describe your APIs using Swagger and then publish the documentation using the Swagger UI explorer.15 To aid documentation and specification even further, it helps to introduce conventions defining where API specifications and the explorer are hosted. As an example, all services using the Swagger toolchain host their API specifications at the /swagger/api path, with the service explorer accessible at /swagger/docs. Additionally, the services' API specifications are pushed to another centralized service, which both other services and humans can use to retrieve a full list of all available services.\n\nThis last approach last can be seen as the beginning of implementing a true service registry that can help in programmatically discovering available services and their capabilities. We will not go into detail about private service discovery tools such as Weaveworks Weave (https://www.weave.works/product/), AWS Route 53 (https://aws.amazon.com/route53/), or HashiCorp's Consul (https://www.consul. io/).16 Nevertheless, such tools play an important role in helping effectively manage both development and production environments, and they become essential when running medium to large-scale microservices architectures.\n\nSummary Any organization that wants to be successful in adopting microservices needs to make sure that a list of processes and tools are in place for engineers to be set up to support the business goals effectively and efficiently.\n\nThe first part of this chapter explains the most essential 12 properties of a successful microservices\n\ndevelopment environment, detailing the rationale behind each of the items on the list.\n\nIn the second part, we describe practices and tools to address each point on the first part’s wish list.\n\nEach of the recommendations we make is backed up by our practical experience with those tools gathered over the course of our careers.\n\nNow that we have explained general development concerns, the next chapter will focus on what is involved in testing microservices applications to verify them in an exhaustive fashion, and what to look out for in particular.\n\n15For more details on the Swagger UI, see the \"The Swagger User Interface\" section in Chapter 5. 16See Chapters 6 and 9 for more information on the Consul tool.\n\n126\n\nCHAPTER 9\n\nTesting with Services\n\nAutomated testing is crucial in any software development project. Manually testing a system built from microservices is tedious and error-prone. In some cases, it is well-nigh impossible to test all the possible failure modes by simply exercising the application through exploratory tests executed by human QA personnel. Automating these tests will save hours of time, and it will prevent those bugs that could cost your business millions in profits.\n\nTesting becomes even more important in a microservices architecture where the components of the\n\nsystem are highly distributed across network boundaries.\n\nThis chapter first discusses the five system layers at which testing can and should be done:\n\nUnit testing\n\n\n\nIntegration testing\n\nContract testing\n\nComponent testing\n\n\n\nEnd-to-end testing\n\nNext, we will show approaches to testing a single microservice component in isolation from other, external microservices with which it interacts. The pros and cons of each approach are discussed, and example code to illustrate implementing them is explained.\n\nWe then move on to showing how tests can support services in guaranteeing that they meet their published API contract. The goal is to shield the service clients from unexpected breakages caused by service-side changes.\n\nLast but not least, we demonstrate how system-level, end-to-end tests can be specified, in order\n\nto exercise the functionalities that make up an application that is composed of multiple interacting microservices.\n\nLayers of Microservice Testing As discussed in Chapter 1, microservice architectures enforce clearer, more pronounced internal boundaries between the components of an entire system than monolithic applications typically do.\n\nThis can be used to the advantage of testing strategies applied to microservices; more options for places\n\nand methods to test are available. Because the entirety of the application is composed of services with clear boundaries, it becomes much more evident which layers can be tested in isolation. This section describes each of the conceptual layers at which tests can be introduced.\n\n127\n\nChapter 9 ■ testing with serviCes\n\nThe Five Layers of Testing An application designed in a microservices architectural style can be viewed at several levels of granularity. Starting from the outside in, end users will interact with the system as a whole. At the next level down, the system internals can be viewed as a series of communications between each of the underlying services. Finally, the services themselves can be described as the code modules that make up each of the services. Testing strategies should aim to cover all these levels or layers, as well as interlayer communication.\n\n■ Note same layer of the microservices application, and hence conceptually belong together.\n\nwe use the terms layer and level interchangeably when referring to a set of tests that exercise the\n\nThe diagram in Figure 9-1 was inspired by one that Toby Clemson of ThoughtWorks used in a\n\npresentation on microservices testing,1 published on Martin Fowler's blog. It illustrates five layers of tests, and where they are applied in a system designed with microservices.\n\nFigure 9-1. Diagram of the five layers of testing applied to microservices\n\nUnit Testing\n\nAs the lowest level of testing, unit tests exercise the smallest piece of testable software in the application to determine whether it behaves as expected. The emphasis here is on trying to identify the truly smallest independent piece of logic to be tested.\n\n1Tobias Clemson, “Testing Strategies in a Microservice Architecture,” November 18, 2014. http://martinfowler. com/articles/microservice-testing/#conclusion-summary\n\n128\n\nChapter 9 ■ testing with serviCes\n\nAn example in the realm of our bookstore could be the functionality of displaying the user’s name\n\ncorrectly. The underlying logic specifies that the first choice is to display a name the user has entered via the bookstore app itself, with a fall-back to a name as vended by a third party (like Facebook), and finally a name deduced from the user’s email address data. The various conditions and fall-back scenarios should be exercised via unit testing.\n\nWhen you find yourself struggling to implement unit tests for a code module, such difficulties often indicate that the design of the module needs improvement: the cause could be that logic needs to be broken down even further, and that responsibilities allotted to the module need to be further separated.\n\nEven at this very low level of testing, the code under test often relies on collaborating with other modules. When running unit tests, the tester has two options for setting up such interactions with these collaborators: either the unit under test is exercised in isolation from all its dependencies, or it is not (meaning that collaboration with external units continues to happen). In his book Working Effectively With Unit Tests (Leanpub 2014), Jay Fields calls these two options for approaching unit testing sociable (where collaborators stay involved), and solitary (where dependencies are not exercised).\n\nBoth solitary and sociable testing are important when verifying microservices architectures. A good\n\nexample of where sociable tests are usually preferred is the domain logic of a service, as these tests usually involve observing changes in state of the unit under test. It is an approach that can best be described as black-box testing that examines the unit under test while interacting with its external interface, and having all collaborating external modules present in their true form adds much to the level of confidence such tests provide.\n\nOn the other hand, when testing “plumbing” code (like gateway code to other services or data stores) or “orchestration” logic (which simply coordinates the message flow with other modules), solitary tests are most often used; the respective external services, data stores, or coordinated modules are replaced by test doubles to facilitate faster, and potentially more reliably repeatable, tests. A word of caution on the solitary testing approach, though: it makes it all too easy to neglect testing mocked responses from dependencies that simulate failure conditions or exceptions. Don't fall prey to the fallacies of distributed computing!\n\nIntegration Testing\n\nCompared to unit tests, integration tests are more coarsely grained: their purpose is to verify that each module of a service correctly interacts with its collaborators. They are meant to detect any defects in how modules should communicate with each other, so that a new feature, or a code change, can be safely deployed to production.\n\nTo achieve that end, they exercise communication paths through the service in order to find incorrect assumptions each module might have about how to interact with its communication partners. Also, make sure not to test just for the “happy paths” of successful communication, but also cover basic failure and exception scenarios of the integration module—an advisable approach, which is not just limited to the realm of integration testing, but which should be heeded for testing at all levels.\n\nWhen testing integration with components that are external services, you should prepopulate external\n\ncomponents with an agreed-upon fixed set of representative data that is guaranteed to be available. This aids repeatability of your tests in an automated integration test suite.\n\nTesting a service's persistence layer at an integration level should include tests that confirm that the schema assumed by the code matches the one actually available in the data store. As is often overlooked, timeouts and network failures can happen for data stores, as well, since they usually exist across network boundaries. Therefore, the service's ability to fail gracefully under such conditions also needs to be verified. An integration test scenario for our bookstore example would be to exercise the communication between the cart-service and the books-service, where the cart-service’s code base would include verifications that it can successfully retrieve and process product data (such as pricing information) provided by books-service. Additionally, the integration tests should assert that the cart-service can still continue to function— albeit most likely in a limited fashion—in case of a books-service outage.\n\n129\n\nChapter 9 ■ testing with serviCes\n\nWhile integration tests should generally not rely on involving test doubles, it can sometimes be difficult to trigger exception behaviors, like timeouts or slow responses, in an external service. This is especially true if the external service is not under the control of the team that is writing the integration tests. In such cases, stubbing failures in predetermined ways can be helpful.\n\nNote also that integration tests can start failing not only when the integration module itself regresses,\n\nbut also if the external component that is being integrated with suddenly becomes unavailable, or breaks its contract. Therefore, our advice is to write a few truly sociable integration tests, but to focus more attention on unit tests, plus contract tests (which are discussed next) on either side of the integration boundary.\n\nContract Testing\n\nWhen implementing and maintaining a service, it is essential that the service’s exposed APIs (collectively also known as the service contract) meet the requirements and expectations of all its consumers. A service contract consists of expectations of input and output data structures, side effects, and nonfunctional points like performance and concurrency characteristics.2 If the microservice is subject to change over time, it is important that the contracts of each of the service consumers continue to be satisfied. Contract testing is meant to verify that a service boundary behaves as advertised.\n\nAs contract tests are aimed at the boundary of a service and its consumers, they are usually bifurcated\n\ninto two parts. The maintainers of each consuming client application write tests suites that verify only those aspects of the producing service that are in use by the client. On the other hand, the maintainers of the service itself run contract tests to determine the impact of their changes on their consumers; these service- side tests are ideally just the bundle of all its consumers' contract test suites. If breaking changes in the service's APIs are to be made, running all the consumers' contract tests will tell the service maintaining team which of its clients would be broken. Then, either all broken clients need to be convinced to move to the new API in lock-step, or versioning of the API is needed, which leaves existing client-exposed API versions untouched.\n\nThe nature of contract test suites also makes them valuable when a new service is being defined. Service\n\nconsumers can help to drive the design of the service's API by building contract tests ahead of time, so that the consumer's requirements can be taken into account in the earliest stages. This pattern of service development is usually called consumer-driven contracts.3\n\nThere are some test tools available for various technology stacks, which are aimed at supporting contract\n\ntesting (for example, Pacto, Pact, and Janus4). Later in this chapter, we will show examples of such tests.\n\nComponent Testing\n\nAt this level of testing in a system that is composed of a set of microservices, the unit under test is one of the services in its entirety. Component tests provide a controlled testing environment for the service, aiming to trigger any applicable error cases in a repeatable manner.\n\nThe scope for these tests is intentionally limited to the single service as a component of the whole system.\n\nThis means that a service is set up to be isolated from any other external components, which are usually replaced with test doubles, usually injected via special start-up configuration of the service under test.\n\nAs a consequence, this level of testing cannot make sure that the microservices all work together as a\n\nwhole to satisfy business requirements; more coarse-grained, end-to-end testing (discussed below) will be necessary for such comprehensive purposes.\n\n2Testing nonfunctional requirements will not be shown in this book. Such specifications are most likely the hardest to verify, and short of a full load-testing environment probably impossible to assert. 3Ian Robinson, “Consumer-Driven Contracts: A Service Evolution Pattern,” June 12, 2006. http://martinfowler.com/ articles/consumerDrivenContracts.html 4Pacto, https://github.com/thoughtworks/pacto; Pact, https://github.com/realestate-com-au/pact; Janus, https://github.com/gga/janus\n\n130\n\nChapter 9 ■ testing with serviCes\n\nOne advantage of component-level tests compared to end-to-end tests is that they provide more accurate test feedback about the service itself; the service's APIs are directly driven through tests from the perspective of a consumer, as opposed to indirectly via the execution of a higher-level interaction of an end- user with the entire system. Component tests also usually run faster than tests that are aimed at the entire system.\n\nWhen implementing component tests, there are principally two options where the tests will be executed: in-process, or out-of-process. The next two sections will discuss these options in more detail.\n\nIn-Process Component Tests\n\nIn-process tests for a service component execute in the same CPU process as the service code under test. Test doubles for outside resources (like external services the component depends upon) live inside the service at the time of executing in-process component tests. Data stores that the service might use for persistence are held in-memory, so that access to them does not cross network or process boundaries. To achieve in-process testability, tests often communicate with the microservice through a special internal interface that allows requests to be dispatched, and responses to be retrieved, without making actual (HTTP) network calls to the service-under-test's API. This way, in-process component tests can get as close as possible to the (out-of-process) alternative of executing real HTTP requests against the service, without incurring the additional overhead of true network interactions.\n\nAs an advantage of this approach, in-process component tests do not touch the network, and thus\n\nexecute much faster and are generally less complex.\n\nOther external services upon which the component under test depends are usually replaced with in- process test doubles. These doubles serve canned responses that cover not just regular service responses, but also failure codes and other communication edge cases.\n\nUsing an in-memory database, like H2 DB, or an in-process implementation of Cassandra or\n\nElasticSearch, improves test performance compared to accessing external DBs, as network calls are unnecessary. The argument for using such in-memory replacements is that any persistence-layer integration tests will provide sufficient coverage for communication with the actual data store in use by the component. A disadvantage is that the microservice being tested has to be altered for testing purposes to be started in a “test” mode, which differs from its usual production mode. Dependency injection frameworks can help with setting services up for tests, but the fact remains that the service is not configured exactly as it would be in a production scenario.\n\nOut-Of-Process Component Tests\n\nOut-of-process tests treat the service component as a black box and verify it via tests that execute in a different process and exercise the service over the network, using its publicly exposed APIs.\n\nTest doubles for external service dependencies are still in use, but they live outside the service. They\n\nexecute in their own process, and they are reached only across a network.\n\nSimilarly, data stores used in these out-of-process component tests are real data stores. They provide\n\nthe service under test with sample data.\n\nOut-of-process testing allows more layers and integration points to be exercised than is possible with the in-process variant; all interactions make use of real network calls, and the deployment unit can remain unchanged. If the microservice under test has complex integration, persistence, or startup logic, then this approach is the only way to test it at a component level.\n\nTests in this category verify that the microservices have the correct network configuration and are capable of handling true network requests. The modules that communicate with other service resources, as well as the persistence-access modules, are tested in full integration with external dependencies, which live in separate processes.\n\n131\n\nChapter 9 ■ testing with serviCes\n\nExternal service dependencies can be stubbed in various ways: the tester can either build a set of actual stub services that expose the same contract or have them dynamically generated. Tools like Moco5 or Mountebank6 can help with this task. To make the data returned by such mock services more realistic, it can again be either manually crafted or prerecorded from interactions with the real service (using tools like VCR7).\n\nThe trade-off to be aware of when choosing this approach is that the complexity of isolating the\n\nmicroservice under test from its dependencies is pushed into the test harness. The harness will be responsible for starting and stopping external stubs, coordination of network ports, and other configuration options. Also, using external stub services and actual out-of-process data stores will most likely result in slower tests than in-process approaches can achieve, because of the additional network communications.\n\nWe will show practical examples of setting up component-level tests for a bookstore-related service\n\nlater in this chapter.\n\nEnd-To-End Testing\n\nEnd-to-end tests aim to exercise the entire application in order to verify that it meets all external requirements and that all goals are met.\n\nCompared to previously discussed component tests, they are concerned not just with a single\n\nmicroservice, but with the entire set of communicating services and technology components making up the system. This is why they are sometimes also referred to as full-stack tests.\n\nThey exercise as much of the fully deployed system as possible, manipulating it through public interfaces, such as GUIs and service APIs. This level of testing allows microservice architectures to evolve over time, as they assert that the system remains intact during large-scale architectural refactorings. When actions like splitting, combining, or decommissioning some of the services that make up the entire system becomes necessary or desirable, they serve as a means of verifying that no regressions are caused to the overall application functionalities and expectations.\n\nOne complication often encountered in end-to-end testing is how to address potential dependencies of the application under test on truly external services, provided by third parties. Similar complications arise if the dependencies are simply flaky or unreliable.\n\nIn such systems it may not be possible to write end-to-end tests that are repeatable and side effect free;\n\ntests can start to fail for reasons outside of the testing team's control. Under these circumstances it can be beneficial to sacrifice a certain level of confidence in the true end-to-end nature of the tests by introducing stubs or other test doubles for the external services. What is lost in complete fidelity is gained in greater stability in the test suite.\n\nFor systems that require direct user manipulation via a GUI, tools such as Selenium WebDriver8 can\n\nhelp to drive the GUI to trigger particular use cases within the system. For headless systems without a GUI, end-to-end tests usually directly manipulate the microservices through their public APIs (for example, via HTTP client libraries, or even just direct cURL command-line tool calls).\n\nAs a general rule, end-to-end test suites carry higher costs than lower-level testing efforts. Given their asynchronous nature, their long run times (as they execute entire application-level usage scenarios), and the fact that there are typically many moving parts involved, each of which can be broken, they are more laborious to implement, and larger efforts are necessary to maintain them.\n\nTo address this problem, it can be prudent to set a fixed limit on the total runtime budgeted for end-to-\n\nend test suites. This introduces a forcing function to purge particularly long-running tests once the budget is exceeded.\n\n5https://github.com/dreamhead/moco 6https://github.com/bbyars/mountebank 7https://github.com/vcr/vcr 8http://docs.seleniumhq.org/projects/webdriver/\n\n132\n\nChapter 9 ■ testing with serviCes\n\nTo determine which tests to keep, it can help to design the test scenarios around the various user roles,\n\nand to formulate stories how these users interact with the system. Another approach would be to order the tests by financial importance. For example, a test that exercises interactions with an external payments gateway might take a quarter of your time budgeted for end-to-end testing, but it is verifying a functionality considered business-critical; it is still considered a worthwhile assertion.\n\nThis helps to focus on the parts of the application that provide the highest value. Coverage for the lesser\n\nconcerns can be addressed by lower-level types of testing.\n\nIn order to make it easier to have the engineering and business organizations of your company\n\ncollaborate on defining user stories, many practitioners use domain-specific languages (DSLs) to implement their test cases. The intent is to make the verification steps human-readable while maintaining the properties of keeping them machine-executable. Popular tools to support test DSLs include Cucumber9 and Gauge.10 We will show examples of end-to-end tests written in Cucumber later in this chapter.\n\nAnother problem to specifically address in end-to-end tests is setting up the entire system with test data\n\nthat makes these tests reliably repeatable.\n\nThe most efficient way to handle this is to apply the same DevOps techniques that one should generally\n\napply to deploying microservices architectures to setting up test beds, as well. Much reliability is gained if pristine test environments are spun up as part of the test initialization process. These environments should already be populated with known test data (provided via DB imports or by populating the services involved via the APIs they expose).\n\nBecause of the effort involved in getting end-to-end testing right, there are many teams that simply skip\n\nthis extra layer of testing. They would rather rely on more stringent production environment monitoring and alarming, paired with a handful of manually executed, exploratory smoke tests once changes have been deployed.\n\nYou can find an example scenario for end-to-end testing in our bookstore application later on in this\n\nchapter.\n\nThe Test Pyramid We are adopting Mike Cohn's metaphor of a test pyramid11 to express the notion that the number of tests written at each testing level should decrease the more coarsely grained the view of the unit under test becomes.\n\nThe diagram in Figure 9-2 shows Toby Clemson's depiction12 of the relative number of tests in the\n\nvarious testing layers just described.\n\n9https://cucumber.io/ 10http://getgauge.io/ 11http://martinfowler.com/bliki/TestPyramid.html 12Tobias Clemson, “Testing Strategies in a Microservice Architecture,” November 18, 2014. http://martinfowler. com/articles/microservice-testing/#conclusion-test-pyramid\n\n133\n\nChapter 9 ■ testing with serviCes\n\nFigure 9-2. The testing pyramid shows the relative number of tests in each layer\n\nAt the very top we find a layer we haven't specifically talked about yet: exploratory tests are executed manually by human testers, and they interact with the application directly through a user interface. These tests can help to educate the testers' efforts of writing and improving automated tests. They can also be used as a sanity-checking tool to make sure a recently released change has in fact taken effect.\n\nAs we move down the pyramid, the scope of each of the tests executed decreases, while the number of\n\ntests in a given layer becomes larger compared to the layers above.\n\nThe metaphor of a pyramid expresses the general advice about where to spend time and effort for writing tests. If the advice is taken, it is easier to arrive at a test suite that minimizes maintenance costs and total runtime while at the same time maximizing utility.\n\nTesting Examples Now that you’ve seen the various layers at which microservices can be tested, the following sections will show examples for approaches to some of the less obvious and commonplace levels of testing. Most of these examples are based on actual test suites we have encountered in our work experience.\n\nIn-Process Stubs for Component and Unit Testing As described in the sidebar, although stubbing external dependencies has its disadvantages, it helps with increasing the speed and isolation of test suites. In order to take advantage of these pros, we have spent some time trying to address the cons.\n\nWHAT IS STUBBING?\n\nFor the remainder of this section, we will be using the terms stub and stubbing generically for any case where you replace a production object for testing purposes. this aligns mostly with gerard Meszaros’ general term test double,13 disregarding the more nuanced differences between the various types of doubles.14\n\n13http://xunitpatterns.com/Test Double.html 14For a more nuanced distinction between types of doubles, see Fowler’s “Marks Aren’t Stubs” article, January 2, 2007. http://martinfowler.com/articles/mocksArentStubs.html\n\n134\n\nChapter 9 ■ testing with serviCes\n\nConcretely, this means that the application under test uses test doubles for every call to an external web services made in the exercised code. this way, no service calls ever leave the process in which the component under test is running. results of the service calls are predefined, or “faked,” so that the rest of the code under test can be exercised based on these assumptions.\n\nthere are a large number of such test double frameworks out there. Just in the ruby ecosystem, projects like Mocha or rspec Mocks15 can help to prevent service calls by stubbing out methods of the service clients’ access objects. Other gems (such as webmock16 and Fakeweb17) tie in at the http request level, letting you define mock responses for all requests to predefined UrL patterns.\n\nthe main advantage of using stubs when testing applications with external dependencies is speed. Calling external dependencies can really slow down a test suite. if no time has to be spent in your tests for making expensive network calls, then you will be much more productive. another advantage is that you can truly test your component in isolation, and dependencies do not need to be available during test runs. additionally, your component’s test runs will be resilient to changes in the dependent services’ data.\n\ntraditional stubbing does come with disadvantages, though. For one, testing never happens on a true “integration” level, as the request round-trip between the application under test and the service will never happen.\n\nanother danger is the potential for the dependent apis to change. if you rely solely on testing the interactions with test doubles, your tests will still pass, even if the api contract or behavior on the service side changes, and you run the risk of not detecting issues that will show up only when deployed to the production environment.\n\nFinally, setting up stubs often introduces distracting boilerplate code into your tests simply to bootstrap your test scenarios.\n\nThe direction we took was to build test doubles right into the service’s client library, and to evolve and maintain them in lock-step with the changes in the service APIs. Here are some key points of the stubbing approach taken. We will summarize with an explanation of the main benefits of this approach.\n\nPluggable Back-Ends\n\nWhen building a client library (or in our case, a Ruby gem), we provide a way to configure (at least) two back-end alternatives: one that makes actual HTTP calls to the respective service in order to assemble the library’s response objects; and a second “fake” back-end, which never makes actual network calls to retrieve response objects, but instead chooses them from a pool of well-known response objects.\n\nFirst, Listing 9-1 shows the common “abstract class” that defines the interface for both the real and the\n\nmock back-end.\n\n15https://github.com/rspec/rspec-mocks 16https://github.com/bblimke/webmock 17https://github.com/chrisk/fakeweb\n\n135",
      "page_number": 140
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 151-160)",
      "start_page": 151,
      "end_page": 160,
      "detection_method": "topic_boundary",
      "content": "Chapter 9 ■ testing with serviCes\n\nListing 9-1. Abstract backend class\n\nmodule BookStore module Books::Backends\n\nclass BackendUnimplementedMethodError < StandardError; end\n\n# This class defines the API that backend implementations need to follow class Backend\n\ndef self.find(id) raise BackendUnimplementedMethodError.new end\n\ndef self.find_by_slug(slug) raise BackendUnimplementedMethodError.new end\n\ndef self.get_all raise BackendUnimplementedMethodError.new end\n\n... end end\n\nNext, Listing 9-2 shows the actual back-end that connects to the service in question. It implements the\n\nmethods defined in BookStore::Books::Backends::BooksService.\n\nListing 9-2. Class for implementing communication with the book-service\n\nmodule BookStore module Books::Backends\n\nclass BooksService < Backend\n\ndef self.api @api ||= Faraday.new(url: ServiceDiscovery.url_for(:books), ...) end\n\ndef self.find(uuid) raise BookStore::Books::BookNotFoundError unless BookStore::Books::ArgumentChecker.valid_uuid?(uuid) response = api.get(URI.escape(\"/books/#{uuid}\")) raise BookStore::Books::BookNotFoundError unless response.status == 200\n\nresponse_to_book_container(parsed_response(response.body)) end\n\n136\n\nChapter 9 ■ testing with serviCes\n\ndef self.find_by_slug(slug) raise BookStore::Books::BookNotFoundError unless BookStore::Books::ArgumentChecker.valid_slug?(slug) response = api.get(URI.escape(\"/books/slug/#{slug}\")) raise BookStore::Books::BookNotFoundError unless response.status == 200\n\nresponse_to_book_container(parsed_response(response.body)) end\n\ndef self.get_all response = api.get(\"/books/\") raise BookStore::Books::BookNotFoundError unless response.status == 200\n\nparsed_response(response.body).map do |book| response_to_book_container(book) end end ... end end\n\nListing 9-3 shows the implementation of the stubbed back-end that serves canned responses to all gem\n\nclients.\n\nListing 9-3. Implementation of a back-end stub\n\nmodule BookStore module Books::Backends\n\nclass Stub < Backend\n\ndef self.find(uuid) raise BookStore::Books::BookNotFoundError unless BookStore::Books::ArgumentChecker.valid_uuid?(uuid) book = @books.detect {|book| book.uuid == uuid} raise BookStore::Books::BookNotFoundError if book.nil?\n\nbook.backend_data end\n\ndef self.find_by_slug(slug) raise BookStore::Books::BookNotFoundError unless BookStore::Books::ArgumentChecker.valid_slug?(slug) book = @books.detect {|book| book.slug == slug} raise BookStore::Books::BookNotFoundError if book.nil?\n\nbook.backend_data end\n\n137\n\nChapter 9 ■ testing with serviCes\n\ndef self.get_all @books.map(&:backend_data) end ... def self.reset_all_data @books = BookStore::Books::TestStubs.all_books ... end end end\n\nFinally, the main accessor class of our gem allows for setting a back-end, and then delegates all API calls\n\nit exposes to the currently set back-end, as shown in Listing 9-4.\n\nListing 9-4. The gem's main access class\n\nmodule BookStore module Books\n\n... class << self attr_writer :backend\n\ndef backend @backend ||= BookStore::Books::Backends::BooksService end\n\ndef stubbed_mode! @backend = BookStore::Books::Backends::Stub @backend.reset_all_data end\n\ndef get(uuid) BookStore::Books::Book.new(backend.find(uuid)) end\n\ndef get_all backend.get_all.map do |book| BookStore::Books::Book.new(book.id, :backend_data => book) end end\n\ndef get_by_slug(slug) books_service_response = backend.find_by_slug(slug) BookStore::Books::Book.new(books_service_response.uuid, :backend_data => books_service_response) end ... end end end\n\n138\n\nChapter 9 ■ testing with serviCes\n\nRealistic Canned Responses\n\nThe well-known response objects served by the stubbed back-end expose the same API as the objects returned from actual network service calls (in fact, they are instances of the same model classes), and they come preloaded inside the stubbed back-end’s registry of responses. Listing 9-5 shows an extract from the code that demonstrates how the test stub objects are exposed.\n\nListing 9-5. Class to provide book test stubs\n\nmodule BookStore module Books class TestStubs\n\ndef self.diva_and_flea Book.new('f81d4fae-7dec-11d0-a765-00a0c91e6bf6', backend_data: BookStore::Books::BookContainer.new( 'f81d4fae-7dec-11d0-a765-00a0c91e6bf6', 'The Story of Diva and Flea', 'the-story-of-diva-and-flea', ...)) end\n\ndef self.winnie_the_pooh Book.new('481dfebc-7fa5-22c0-a765-01b0c83e5ce4', backend_data: BookStore::Books::BookContainer.new( '481dfebc-7fa5-22c0-a765-01b0c83e5ce4', 'Winnie-the-Pooh', 'winnie-the-pooh', ...)) end ... def self.custom_book(attributes={}) book = diva_and_flea.backend_data attributes.each_pair {|k,v| book.send \"#{k}=\", v} Book.new(book.uuid, backend_data: book) end\n\ndef self.all_books [ diva_and_flea, winnie_the_pooh, ...] end ... end end end\n\nRunning Tests in Stubbed Mode\n\nIntroducing the test approach just described into our service access library has some immediate advantages when setting up and customizing test scenarios.\n\n139\n\nChapter 9 ■ testing with serviCes\n\nEase of Test Set-Up\n\nAs part of their test suite set-up, applications under test place the service client library into stubbed mode, thereby configuring it to serve responses entirely out of the stubbed back-end’s registry of preloaded response objects. As can be seen from the preceding code samples, all that these applications need to do is place the line shown in Listing 9-6 into code that is executed as part of their test harness set-up.\n\nListing 9-6. Code to make sure the tests are using stubs\n\n# Applications under test just place this inside a test_helper.rb, or the like BookStore::Books.stubbed_mode!\n\n■ Note of course need to configure the client library to use the actual http service-based back-end instead (which is the default for our client gem anyway).\n\nwhen the client application is running in a production environment, the production-specific setup will\n\nCustomizing the Stubs\n\nTo serve the needs of special-case, nonstandard situations, the client library allows for creating additional stub objects, and for adding them to or removing them from the mock back-end’s business object registry. While the idea here is similar to stubbing response objects corresponding to given request patterns at the transfer protocol layer (as is implemented by some HTTP client libraries, such as the Typhoeus concept of a direct stubbing mechanism18), it differs in that we leverage test doubles at a higher (application logic) level, where the test double behavior is defined in terms of model objects that expose exactly the same interface and behavior as the client gem’s result objects as they are generated in a production environment.\n\nListing 9-7 shows a code example of how our client gem can be convinced to customize the test mocks\n\nstored in the mock back-end.\n\nListing 9-7. Customizing which stubs are used in the tests\n\n# turn on stubbed_mode BookStore::Books.stubbed_mode! # add a couple of books to the stubbed mode's backend data my_test_books = [ BookStore::Books::TestStubs.custom_book( uuid: '6291ba52-7e45-1423-b764-01b0c83e5ce4', title: 'Peter Pan', slug: 'peter-pan'), BookStore::Books::TestStubs.custom_book( uuid: '62bae4a6-54a6-162d-a654-7281fb3a6ce7', title: 'Pete The Cat', slug: 'pete-the-cat')]\n\nBookStore::Books.backend.add_books(*my_test_books)\n\n# ...\n\n18See the “Direct Stubbing” section of the Typhoeus gem’s Readme, https://github.com/typhoeus/ typhoeus#direct-stubbing\n\n140\n\nChapter 9 ■ testing with serviCes\n\n# Remove some of the objects added above from the stub backend's data BookStore::Books.backend.remove_book(*my_test_books)\n\n# ...\n\n# Reset the backend data to the 'well-known' set of books BookStore::Books.backend.reset_all_data\n\nWhy use Pluggable Back-Ends?\n\nThe approach to include several service back-ends and realistic test responses we’ve described allows the mock objects to evolve in lock-step with the client library version. This will increase the client application’s confidence that it is testing against the same API as the actual objects returned by the latest service API version.\n\nAdditionally, none of the usual cumbersome and boilerplate code to create and register stub objects for\n\nthe various tests needs to be written; the stubbed back-end comes preconfigured with a variety of standard responses, which the application code under test will simply use without any additional configuration.\n\nOut-Of-Process Component Testing The sample code that accompanies this book comes with a project called api-testing. In this project, we are testing the books-service in a black-box manner. This means that the test suite executes in a separate process than the books-service itself, and it verifies its proper functioning via calls to the service's externally exposed API.\n\nAs described in Chapter 6, we are using a tool called consul.io (https://www.consul.io/) for independently configuring and discovering the services that make up our entire application in each of our system stages.\n\nSetting Up the Environment\n\nPrior to starting up our component test suite, we need to announce to our local consul.io discovery service that we have a (Rust) service called books running logically at port 5000:\n\necho '{\"service\": {\"name\": \"books\", \"tags\": [\"rust\"], \"port\": 5000}}' | sudo tee /etc/ consul.d/books.json\n\nNext, we start our Consul service:\n\nconsul agent -dev -config-dir=/etc/consul.d\n\nThis command reads all the services configured from the data files in the Consul configuration\n\ndirectory, and also launches a web UI to administer the consul.io service (see Figure 9-3).\n\n141\n\nChapter 9 ■ testing with serviCes\n\nFigure 9-3. Adminstration Web UI for consul.io\n\nLater on, in the test suite, we will show code that simply looks up the currently running books-service\n\ninstance by contacting and searching the local consul.io registry.\n\nNext is starting the actual books-service. This service (implemented in the Rust19 programming language) will be configured at startup to use a test database, which is destroyed at the end of running the full suite. To that end, we will set an environment variable for the database URL, which will be honored by the service code when setting up the DB connection:\n\nDATABASE_URL=\"postgres://tim@localhost/books_componenttests\" cargo run\n\nLet us emphasize this again: the test DB is a real data store, running outside the books-service process.\n\nIt has, however, been preconfigured to include sample data for the component test runs.\n\nWriting Your Test Cases\n\nWhile there are a very large number of testing frameworks for implementing component-level tests, we have decided to demonstrate this level of testing using Cucumber.\n\nCucumber includes a DSL called Gherkin20 for formulating test cases that mostly read like “plain English” (or any of over 60 other languages). The self-described (and might we add, rather lofty) goal Cucumber is trying to achieve is that it \"merges specification and test documentation into one cohesive whole.” By writing system requirements in \"executable specifications,” this tool aims to make collaboration between people who define the business goals and people in engineering positions easier.\n\nCucumber tests revolve around features, written down in the Gherkin language, and stored in .feature files in your test project. Features are there to describe software capabilities and requirements at a high level; they are simply a grouping of related scenarios.\n\n19https://www.rust-lang.org/ 20https://cucumber.io/docs/reference\n\n142\n\nChapter 9 ■ testing with serviCes\n\nYou define a feature in Gherkin by starting a line with the Feature: keyword, followed by a name and an\n\noptional description.\n\nA scenario is a \"concrete example that illustrates a business rule.” Its Gherkin keyword is Scenario: and just like a feature, it can have a name and further descriptive text. After that, a scenario consists of a list of steps, which first describe the outset of the test (starting with a Given keyword), then an event or set of events to occur (via the When keyword), and finally the expected result of the event under the described circumstances (in a Then clause). several Given, When, or Then steps can be logically combined via the And and But keywords.\n\nStep definitions can take arguments, which come in handy when your step definitions don't easily fit\n\ninto a single line. One particularly handy way of defining step arguments is as a Gherkin data table. The contents of such tables can easily be accessed as key-value-pairs in the code behind the step definitions.\n\nLet's look at a very simple example scenario from our books-service component test suite:\n\nFeature: books endpoints As an API user I want to call CRUD endpoints to manipulate books\n\nScenario: POST to add a new book Given I send a POST request to \"/books\" in \"books-service\" | title | price | price | isbn | brand | description | | Pete the Cat | 500 | 650 | 0062303880 | HP | New York Times ...| Then the response status should be \"201\"\n\nThis first feature describes the simple Create-Read-Update-Delete (CRUD) endpoints for the books\n\nresource in our service.\n\nOur scenario describes adding a new book to the service, which can be done via an HTTP POST method\n\ncall as listed in the Given clause. The data table passed to the Given step describes the parameters to send, plus example values for each of the fields. The outcome described and tested for in the Then step is that the service's HTTP response should carry a 201 status, indicating that a new book resource was created.\n\nWhile Gherkin parsers exist in many programming languages, we chose Ruby to implement the steps, as\n\nshown in Listing 9-8.\n\nListing 9-8. Step definitions for the POST scenario\n\n# in `http_steps.rb` ## Uses service as parameter Given(/^I send a POST request to \"([^\"]*)\" in \"([^\"]*)\"$/) do |api_path, service, table| url = File.join(ServiceDiscovery.url_for(service.to_sym), api_path) @response = Faraday.post(url, table.hashes.first) end\n\n# in `json_steps.rb` Then(/^the response status should be \"([^\"]*)\"$/) do |code| assert_equal code.to_i, @response.status end\n\nThe most important line to discuss here is the following:\n\nurl = File.join(ServiceDiscovery.url_for(service.to_sym), \"/books\")\n\n143\n\nChapter 9 ■ testing with serviCes\n\nAs mentioned earlier, we are using a service discovery tool to configure our microservices environment.\n\nThis code looks up the books-service service in the service registry before making test calls to it. This approach enables us to execute these steps unchanged against a version of our services where external dependencies are replaced for test purposes (like connecting to test database content), or against the actual unaltered production service, simply by changing the service registry's entry for the books-service.\n\nTo conclude this out-of-process component tests section, we are showing our test suite's console\n\noutput, as produced by the Ruby cucumber test runner tool, in Listing 9-9.\n\nListing 9-9. Shell output of running the Gerkhin tests\n\napi-testing tim$ bundle exec cucumber Feature: books endpoints As an API user I want to call CRUD endpoints to manipulate books\n\nScenario: POST to add a new book # features/books.feature:5 Given I send a POST request to \"/books\" in \"books\" # features/step_definitions/http_\n\nsteps.rb:2\n\ntitle | price | full_price | isbn | brand | description| | Pete the Cat: Scuba-Cat | 500 | 650 | 0062303880 | HarperCollins | New York Times bestselling author and artist James Dean brings Pete the Cat’s world to life under the sea in this new I Can Read book. | Then the response status should be \"201\" # features/step_definitions/json_\n\nsteps.rb:1\n\nScenario: POST to add a new book without required attributes # features/books.feature:11 Given I send a POST request to \"/books\" in \"books\" # features/step_\n\ndefinitions/http_steps.rb:2\n\n| price | full_price | isbn | brand | description | | 500 | 650 | 0062303880 | HarperCollins | New York Times bestselling\n\nauthor and artist James Dean brings Pete the Cat’s world to life under the sea in this new I Can Read book. |\n\nThen the response status should be \"422\" # features/step_\n\ndefinitions/json_steps.rb:1\n\nAnd the JSON response should be: # features/step_\n\ndefinitions/json_steps.rb:15\n\n\"\"\" { \"missing\": \"title\" } \"\"\"\n\nScenario: GET book details by slug for a book # features/books.\n\nfeature:23\n\nGiven I create a dummy book # features/step_\n\ndefinitions/setup_steps.rb:1\n\nAnd I send a GET request to \"/books/slug/:book_slug\" in \"books\" # features/step_\n\ndefinitions/http_steps.rb:8\n\nThen the response status should be \"200\" # features/step_\n\ndefinitions/json_steps.rb:1\n\n144\n\nChapter 9 ■ testing with serviCes\n\nScenario: GET book details by slug for an invalid book # features/ books.feature:28 Given I send a GET request to \"/books/slug/not-a-real-book\" in \"books\" # features/ step_definitions/http_steps.rb:8 Then the response status should be \"404\" # features/ step_definitions/json_steps.rb:1\n\nScenario: GET book details by book id for a valid book # features/books.feature:32 Given I create a dummy book # features/step_definitions/ setup_steps.rb:1 And I send a GET request to \"/books/:book_id\" in \"books\" # features/step_definitions/ http_steps.rb:8 Then the response status should be \"200\" # features/step_definitions/ json_steps.rb:1\n\nScenario: GET book details by book id for an invalid book # features/books.feature:37 Given I send a GET request to \"/books/d3f764a5-2f09-457e-8a94-0128cd46ad3e\" in \"books\" # features/step_definitions/http_steps.rb:8 Then the response status should be \"404\" # features/step_definitions/json_steps.rb:1\n\nScenario: POST to add a new category # features/books.feature:41 Given I send a POST request to \"/categories\" in \"books\" # features/step_definitions/ http_steps.rb:2 | name | description | | Tech Books | Books for techies | Then the response status should be \"201\" # features/step_definitions/ json_steps.rb:1\n\nScenario: GET list of categories # features/books.feature:47 Given I create a dummy category # features/step_definitions/ setup_steps.rb:17 And I send a GET request to \"/categories\" in \"books\" # features/step_definitions/ http_steps.rb:8 Then the response status should be \"200\" # features/step_definitions/ json_steps.rb:1 And the response should be valid JSON # features/step_definitions/ json_steps.rb:20\n\n8 scenarios (8 passed) 21 steps (21 passed) 0m0.139s\n\nExamples of End-to-End Tests Much as for component tests, as shown in the previous section, Cucumber can also be very useful in end- to-end testing of an application, especially when combined with two other projects: Capybara (http:// jnicklas.github.io/capybara/) and Selenium 2.0 (http://www.seleniumhq.org/projects/webdriver/).\n\n145",
      "page_number": 151
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 161-168)",
      "start_page": 161,
      "end_page": 168,
      "detection_method": "topic_boundary",
      "content": "Chapter 9 ■ testing with serviCes\n\nCapybara is another domain-specific language that aims to help in testing web applications. The DSL\n\nis aimed at simulating how a user would interact with the web application via a web browser. In order to connect to the various browsers installed on the computer executing the end-to-end test suite, it can be configured to make use of the Selenium project, which exposes an API abstraction layer and drivers to connect to the most common browsers, like Google Chrome, Firefox, Apple Safari, and even mobile browsers for Android, or headless browsers that don't require a GUI.\n\nWhen loaded into the same Ruby project, the Cucumber and Capybara DSLs complement each other\n\nquite naturally to create a very powerful toolchain for acceptance testing.\n\nThe sample bookstore project accompanying this book also comes with a very rudimentary Cucumber-\n\nplus-Capybara test suite.\n\nLet's look at the showcase.feature file that describes the very basic use case of navigating to the bookstore homepage, and then locating and following a link to the detail page of a book. On the detail page, we make sure we see the correct ISBN number for the book listed, and we can find a link to add the book to the user's shopping cart:\n\nFeature: Browse books in the store\n\nScenario: Navigate to a book detail page Given I am on the Bookstore homepage When I click on the \"/products/the-story-of-diva-and-flea\" link Then I should see the text \"ISBN : 978-1484722848\" And I should find the \"ADD TO CART\" link\n\nNext, Listing 9-10 shows the Cucumber step definitions that make the feature description text\n\nexecutable.\n\nListing 9-10. Step definitions for the end-to-end tests\n\nGiven(/^I am on the Bookstore homepage$/) do url = File.join(ServiceDiscovery.url_for(:bookstore), \"/\") visit url end\n\nWhen(/^I click on the \"(.*?)\" link$/) do |link_path| find(:xpath, \"//a[@href='#{link_path}']\").click end\n\nThen(/^I should see the text \"(.*?)\"$/) do |expected_text| assert page.has_content?(expected_text) end\n\nAnd(/^I should find the \"(.*?)\" link$/) do |link_text| assert page.find_link(link_text) end\n\nThe Given step navigates to the bookstore's homepage using the Capybara visit command. This\n\ncommand instructs the underlying Selenium webdriver to connect to a browser (by default, Firefox) installed on the test machine, and then navigate to the URL given as an argument.\n\nAs you can see, we again use our service discovery tool's application registry to look up the URL for the bookstore web site. This helps in being able to run the acceptance test suite against several different instances of the whole system without needing to change any code in the test suite.\n\n146\n\nChapter 9 ■ testing with serviCes\n\nNext, the When step uses the Capybara find command to execute an XPath query on the HTML DOM rendered by the browser for the homepage. In this particular case, we verify that a link (or anchor, expressed by the HTML <a> tag) with an href property matching the path quoted in the step's input can be located and clicked.\n\nOnce we get to the book's detail page, the Then step verifies that the book’s correct ISBN number text is\n\ndisplayed. This is done by using Capybara's page object and executing the has_content? method with the step's input.\n\nOnce we verify that this text can be found, the final And step makes sure the page shows a link with the\n\ntext ADD TO CART, which is proof enough for our purpose of verifying that the book can be added to the shopping cart.\n\nContract Testing Examples The following examples are taken from our work experience at LivingSocial's engineering organization. As mentioned in Chapter 4, we use swagger_yard as a tool to generate API specifications in the Swagger interface definition language (which is conforms to the JSON Schema specifications).\n\nBased on those tools, LivingSocial also built a Ruby gem called ls-api_validation. This tool works on\n\ntop of the json-schema21 gem, which helps with validating JSON objects against a JSON Schema definition.\n\nThe use cases of ls-api_validation are mainly the following:\n\nValidation of API requests: The gem can be configured to validate the request parameters and interrupt the service's request cycle and return an error response if the request validation fails (by default via indicating an Unprocessable_Entity22 to the clients).\n\nValidation of API responses: Responses can be sampled and validated (at a configurable percentage of traffic). If response validation fails, log entries are created that explain which properties of the JSON schema API definition are violated.\n\nContract test support: The validations of API request parameters and service responses conforming to the published API contract can be integrated into a service's test suite.\n\nIt is the last use case that we are going to demonstrate in this section. But let's first take a look at some swagger_yard API endpoint and response data type definitions. Here is a snippet of defining documentation that describes an endpoint that allows for searching for offers in a \"geo box,” specified by a set of latitude and longitude coordinates:\n\n# @path [GET] /api/v0/search/geo # # @summary Returns geo search results # @parameter city_id(required) [integer] User's current city ID # @parameter vertical [enum<local,shop,escapes,all>] Vertical filter for ... # @parameter top_left_latitude(required) [double] Top edge of geo box # @parameter top_left_longitude(required) [double] Left edge of geo box # @parameter bottom_right_latitude(required) [double] Bottom edge of geo box # @parameter bottom_right_longitude(required) [double] Right edge of geo box # @parameter expandable [boolean] Optional allow geo box to expand to find ... # @parameter query [string] Optional query text\n\n21https://github.com/ruby-json-schema/json-schema 22http://www.restpatterns.org/HTTP_Status_Codes/422_-_Unprocessable_Entity\n\n147\n\nChapter 9 ■ testing with serviCes\n\n# @parameter category [string] Optional category name # @parameter page [integer] Optional page number # @parameter per_page [integer] Optional results per page # # @response_type [GeoCollection] # # @error_message 422 Unprocessable, if city service reports a city not ... # @error_message 503 Unavailable, if city service or inventory service ...\n\nAs can be seen from these specifications, the API endpoint will return data of the type GeoCollection.\n\nThat data type (also referred to as an API model) is defined next:\n\n# # @model GeoCollection # # @property page(required) [integer] Current page # @property per_page(required) [integer] Count of entries per page # @property total_entries(required) [integer] Count of entries in collection # @property total_pages(required) [integer] Total count of pages # @property collection(required) [array<OfferTile>] Collection items # @property top_left_latitude(required) [double] Top edge of geo box # @property top_left_longitude(required) [double] Left edge of geo box # @property bottom_right_latitude(required) [double] Bottom edge of geo box # @property bottom_right_longitude(required) [double] Right edge of geo box\n\nAs you can see, the specification describes the return type in a reasonably complete manner. While we leave out further details about the exact definition of an OfferTile, note that all attributes of this API model are required, and their data types are also defined (as integer values, double-precision decimal values, and an array of OfferTile JSON objects).\n\nListing 9-11 shows how we use ls-api_validation in our test suite to verify the API's contract.\n\nListing 9-11. Contract tests evaluation schemas and error responses\n\nrequire 'test_helper'\n\nclass V0::SearchControllerTest < ActionController::TestCase include LS::ApiValidation::TestHelpers::Minitest #... context \"GET geo\" do should \"fail request validation for missing geo-box definitions\" do get :geo, city_id: 1 assert_response :unprocessable_entity end\n\nshould \"successfully search offers in geo box\" do VCR.use_cassette('search_geo_city_1') do get :geo, city_id: 1, top_left_latitude: 34.28181943724182, top_left_longitude: -116.533715291111, bottom_right_latitude: 33.03275371591785, bottom_right_longitude: -115.5809313949477,\n\n148\n\nChapter 9 ■ testing with serviCes\n\nexpandable: false, category: \"\" assert_response :success assert_passes_schema_validation request, response.body end end end #... end\n\nThis test code shows an example of validating the API's contract both regarding the incoming\n\nparameters and the data that is returned.\n\nWhen the API is called without the required top_left_latitude, top_left_longitude, bottom_right_\n\nlatitude, and bottom_right_longitude parameters, the validation code intercepts the request cycle and returns an HTTP response with status code 422 (expressed as :unprocessable_entity in the Rails framework).\n\nThe second test case first ensures that the test request is successfully executed, by confirming the response status to be of HTTP status code 200, or—as Rails allows you to refer to it—:success. Note that we are using VCR in this example to increase test speed and reliability by isolating the contract test from direct access to a downstream external service that the service under test depends on.\n\nThe more interesting verification is the assert_passes_schema_validation. This method is provided by the LS::ApiValidation::TestHelpers::Minitest Ruby mixin, and it is passed the request object and the JSON response body generated by the service's API code. The ls-api_validation test helper code now uses information from the request to locate the Swagger JSON API definitions associated with the requested API endpoint (as defined earlier in this chapter). It then takes the JSON response body and simply runs JSON Schema validation code to ensure that the response JSON conforms to the JSON Schema that defines the response type published as part of the API contract; in this case, it compares the response JSON with the definition of the GeoCollection model defined above.\n\nJust to illustrate the usefulness of this approach, here is some sample output generated by validation failures. In this case the latitudes and longitude data returned by the API no longer is encoded as a JSON number, but as a JSON string instead:\n\nThe property '#/top_left_latitude' of type String did not match the following type: number in schema 1191bb4f-6d99-5a3c-8aa8-d4c0cee3908f The property '#/top_left_longitude' of type String did not match the following type: number in schema 1191bb4f-6d99-5a3c-8aa8-d4c0cee3908f The property '#/bottom_right_latitude' of type String did not match the following type: number in schema 1191bb4f-6d99-5a3c-8aa8-d4c0cee3908f The property '#/bottom_right_longitude' of type String did not match the following type: number in schema 1191bb4f-6d99-5a3c-8aa8-d4c0cee3908f\n\nThese failure examples are taken from an actual regression that happened in a service we worked on.\n\nWe had changed the Ruby gem that is used to generate JSON responses, mostly to take advantage of the new library's performance improvements for JSON serialization. Running our contract test suite found this change in the default behavior of JSON serialization for Ruby decimal number objects, and we were able to prevent contract breakages before launching the change.\n\nWhile the LivingSocial engineering department often releases internal tools as open source software, the ls-api_validation gem has not been released publicly at the time of writing. However, there are quite a number of other open source contract testing tools, such as pacto from ThoughtWorks, as well as Realestate. com.au's similarly named pact tool.\n\n149\n\nChapter 9 ■ testing with serviCes\n\nBoth of these tools provide additional functionalities to define the client applications' expectations regarding the API-providing service in the clients' test suite, and record those expectations to a file (a pact). These pact files are then used in the service's test suite as input to the contract test suite. This enables the API provider to verify that the clients that provided such pacts will not suffer from service-side changes.\n\nSummary This chapter first introduced you to the five layers of testing a microservices architecture:\n\nUnit testing\n\n\n\nIntegration testing\n\nContract testing\n\nComponent testing\n\n\n\nEnd-to-end testing\n\nAfter discussing how to best apply them in your projects, we showed an implementation of using\n\npluggable back-ends in service client libraries to aid in making test code less verbose and more realistic when using dependency mocking.\n\nWe then focused on examples for out-of-process component testing, using tools like Gherkin,\n\nCucumber, and Capybara, and we illustrated how these domain-specific languages could be combined to specify end-to-end features of our sample bookstore application.\n\nWe concluded Chapter 9 by showing real-word examples of contract testing and how it is used, and how\n\nit helped us find regressions prior to launching changes to our production environment.\n\nThe next chapter will address deployment-related aspects of introducing a microservices architecture\n\ninto your business.\n\n150\n\nCHAPTER 10\n\nDeploying and Running Microservices\n\nBeing able to deploy a new service easily should be a goal of any company that embraces microservices. While there is much more risk involved in each deployment of a monolithic application, the processes involved with getting it deployed are much simpler than deploying small, but highly connected and interdependent services. In this chapter we will show that not only should new services be built, deployed, and configured in an automated and repeatable way, but that they need to be staged prior to releasing, in support of the overall stability and availability of your business during development efforts.\n\nIn this chapter, we focus on the aspects of building and deploying microservices in a continuous,\n\nautomated, and repeatable way.\n\nWe will also discuss how to utilize preproduction rollout strategies to make sure you can release changes to production with confidence, so that running microservices has a positive impact on the overall availability of your production application.\n\nFinally, we will explain how to design and utilize an infrastructure that is based on multiple stages and\n\ncan support the benefits introduced by adopting microservices.\n\nAutomation Is Key Before we go into any detail about deployment and operational aspects of microservices later in this chapter, let us emphasize something we have learned from our experience.\n\nAny organization that is serious about entering into employing microservices needs to fully embrace\n\nDevOps practices. Without a culture of automation, the productivity of your engineering department is bound to suffer enormously. While your organization might have gotten away with manually executing tests, builds, releases, and monitoring for a single deployment unit, such processes will be ground to a halt by the effort involved in repetitively executing these very automatable tasks for tens—or potentially hundreds—of microservices, each of which should be its own deployment unit.\n\nIn a successful microservices environment, developers build and run their software. Because their time\n\nis very valuable, everything—from provisioning computing resources to building, testing, and deploying artifacts, to monitoring and generating alarming for live services—needs to be self-service and scripted. When dealing with microservices architectures, it is absolutely unacceptable if provisioning and launching a new service takes any longer than an hour.\n\nBefore our mutual employer LivingSocial started adopting services, it started out with one of the largest and most high-traffic monolithic Ruby on Rails applications in existence, serving tens of millions of visitors. When the decision was made to break up the application into smaller parts, one of the main cost factors involved was that provisioning production resources to run a service would sometimes take weeks. Build and deployment tools were not unified or automated, and general service configuration and monitoring approaches were very manual.\n\n151\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nAs a result, during that time quite a few less-than-optimal architectural decisions were made, misplacing functionality into existing applications, simply because that was more expedient and less cumbersome than placing them into a new service with a more focused set of responsibilities.\n\nFocusing on introducing end-to-end automation, from committing to a code repository all the way to launching the changes in the production application, helped LivingSocial's engineering department address the bulk of these issues. Today, software development at LivingSocial is very agile and productive, even when the ratio of services per developer is roughly 2 to 1.\n\nTherefore, it is essential that an organization evaluate the available automation tools (some of which we have mentioned earlier in the book, and to which we will return later in this chapter) and find the ones that are best suited for their engineers, in order to address the added complexity introduced by having many, many deployment units.\n\nContinuously Integrating Microservices One indispensable measure for automating the software development environment for microservice development is to employ Continuous Integration (CI).\n\nA Brief History CI started out as a practice first proposed by Grady Booch in his 1990 book on object-oriented design.1 Back in those days, most software projects would be based on a process where all feature work—done by several different developers—was never integrated into a common code base until the very end of a given development cycle. This “big-bang integration” often led to a problem referred to as \"integration hell,” where a significant amount of time was spent on resolving merge conflicts of largely diverging developer branches before any clean build could happen.\n\nWhen the Extreme Programming movement adopted CI as one of its core practices, it greatly increased\n\nthe frequency with which such integration of developer branches into a code mainline would need to happen: in their view, integration (as well as testing of the integrated code base) would need to happen several times a day. In today’s increased use of agile software development with ever-shorter development and release cycles, the focus has shifted from the branch integration component toward continuously running test suites and building deployment artifacts.\n\nThe first automation support came around in 2001. With the open source CruiseControl (http:// cruisecontrol.sourceforge.net/), ThoughtWorks developed the very first CI server for code integration, artifact building, and subsequent testing.\n\nThe Advantages of Using CI Tools Today, there are a large number of tools available. Popular examples are Jenkins (https://jenkins. io/), Travis CI (https://travis-ci.org), Go from ThoughtWorks (https://www.go.cd/), and JetBrain's TeamCity (https://www.jetbrains.com/teamcity/).\n\nWhile these tools may vary in their detailed feature lists, the principle types of tasks they fulfill are all\n\nvery similar. They all do the following:\n\nMonitor a set of code repository for changes.\n\nCheck out the changed code from the repository.\n\n\n\nBuild artifacts.\n\n1Booch, Grady, Object Oriented Design with Applications, Benjamin Cummings 1990.\n\n152\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\n\n\nVerify that the code in the change set passes the automated test.\n\n\n\nStore the result of the CI run (often of the build artifact) in a uniquely identifiable way for later deployment.\n\nNote that, in our experience, employing continuous integration is worth the investment only if your\n\nengineering team buys into the culture that goes along with it.\n\nThis means that it needs to be engrained in the software development process that automated tests are added for every change. Additionally, changes should be committed to the repository in the smallest possible, testable chunks, in order to avoid the inefficiencies of the big-bang integration approach mentioned earlier. Finally, alerting needs to happen whenever the CI system reports problems or failures, and fixing such issues needs to be addressed by your team as their highest priority.\n\nIf you can implement these conditions in your organization, then the use of CI systems can contribute\n\nsome very valuable benefits to your development process:\n\n\n\nShortened feedback loop: Automatic, change-based execution of builds and tests will surface any bugs or other code quality issues2 early on in the development cycle. Research has confirmed that the cost of fixing a bug is higher the later it is discovered in the development process, so this can definitely be a big financial incentive to adopt CI.\n\n\n\nTraceability: CI tools can help to maintain an audit trail for code that is deployed to your production environment. Storing an identifier for the exact version of the code repository deployed (such as a Git commit SHA-1 identifier), a protocol of the verifications performed, a link to the previous and/or next build artifacts, and so on, will help operational aspects (for example, rollbacks to a known good state) and debugging efforts.\n\nUsing CI for Microservices A finer point about continuous integration and its relationship to microservices architectures is that you should set up your CI systems to deal with each of the microservices on an individual basis.\n\nMany of the advantages of using microservices are rooted in the fact that they enable adopters to make\n\ndecisions on a very fine-grained level. In line with this thinking we recommend having no less than one code repository per service. Such a setup will make things like switching programming language, optimizing around caching and data store technologies, grasping the purpose of the repository, and transferring ownership a lot easier than if several discrete services are rolled into the same code revisioning unit.\n\nAs a corollary, having one microservice per code repository also simplifies setting up dedicated and\n\nindependent CI builds.\n\nThe advantages of configuring your CI to work on each microservice separately are clear. First, CI build times are reduced so that the development feedback loop is tighter and overall productivity is improved. In environments with potentially hundreds of services, where the choice is made to run all of them in a single CI instance, it is possible that full CI runs can take a few hours, even if there is just a one-line change to a single microservice. Instead, setting up a discrete CI cycle for each microservice in this scenario means that the same one-line code change only triggers tasks that can finish in less than a minute.\n\nSecond, smaller build artifacts are produced as the outcome of a CI run. This in turn will lead to smaller\n\npackages to deploy to smaller deployment units per service.\n\nWe will return to the arguments about trying to build and promote the smallest possible deployment\n\nunit after explaining the rationale and benefits of continuous delivery.\n\n2Chapter 8 showed examples of using Jenkins CI for running code coverage and other static analysis tools.\n\n153",
      "page_number": 161
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 169-176)",
      "start_page": 169,
      "end_page": 176,
      "detection_method": "topic_boundary",
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nContinuous Code Delivery The concept of continuous delivery (CD) takes the concepts behind continuous integration and applies them to the entire software release cycle. It is a practice in which the engineering team has visibility into, and automated tool integration for, every step a software deliverable takes, from code check-in all the way through to releasing the software into production.\n\nHow Do You Know You Are Doing CD? In an article on his website,3 Martin Fowler describes four criteria that ThoughtWorks developed to verify that continuous delivery is being practiced in an organization:\n\n\n\nThe software artifact is deployable throughout its lifecycle.\n\n\n\nThe owning team makes keeping the software in a deployable state a priority over feature work.\n\nWhenever the software is changed, feedback regarding the production-readiness of software is automated, fast, and accessible to anyone.\n\n\n\nAny software artifact that has been verified to be working can be released to any stage (development, staging, production, and so on) with the push of a button.\n\nFowler summarizes these points saying that the litmus test is\n\n… that a business sponsor could request that the current development version of the software can be deployed into production at a moment's notice—and nobody would bat an eyelid, let alone panic.\n\nBreaking the Release Process into Stages Full and thorough verification can take a very long time. Before releasing code into production, it usually doesn't suffice to create code artifacts and then run automated unit and integration tests—all of which are relatively fast steps.\n\nIn many cases, you will usually want to deploy the code into a test-bed environment (for example, a staging fabric, as described in Chapter 8), in order to perform more coarse-grained (automated) end-to-end tests, or even have QA professionals expose the build artifacts to (manual) exploratory testing. This could also be a good strategy for verifying that nonfunctional requirementsof your services, like performance, are still met.\n\nImplementing these practices requires that you build a deployment pipeline.4 In such an approach, you\n\nsplit up the entirety of your release process into stages. To achieve fast feedback and best time efficiency, it makes sense to place fast stages (like unit testing) at the beginning of your pipeline, while more time- consuming steps (like manual acceptance testing) are pushed into later stages.\n\nFigure 10-1 shows an example of how a deployment pipeline could be structured.\n\n3Fowler, Martin \"ContinuousDelivery,” May 30 2013; http://martinfowler.com/bliki/ContinuousDelivery.html 4Fowler, Martin \"DeploymentPipeline,” May 30 2013; http://martinfowler.com/bliki/DeploymentPipeline.html\n\n154\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-1. Continuous Delivery pipeline with optional deployment step\n\nIn this pipeline, committing code to the mainline branch of a given repository automatically kicks off\n\nthe build step. If it succeeds, then the unit and integration test suites for the service are executed.\n\nNext, the code under test is installed in a staging environment, where automated end-to-end tests are run before humans manually explore the application. Note that it is best if testers record which tests were performed alongside a particular pipeline run, so that tests can be repeatable and blind spots can be addressed.\n\nHuman sign-off on the exploratory phase triggers building an artifact that can be deployed to the production fabric, and it is checked into a repository. The deployable artifact can be an executable, an image of a virtual machine, or an application container, all based on your company's choice of production infrastructure. More on these choices can be found later in this chapter.\n\nIn some cases, the continuous delivery pipeline ends in an automated step to deploy the latest successfully delivered build artifact to the production fabric, effectively resulting in a continuous release cycle.\n\nIf any steps fail, the pipeline will stop and report failure to the engineering team that owns the code to\n\nbe delivered via this pipeline, and fixing the failure needs to be that team’s very first priority.\n\n155\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nPROS AND CONS OF CONTINUOUS DEPLOYMENT\n\nin a blog post5 on the site for their excellent book Continuous Delivery,6 Jez humble and David Farley point out the differences between continuous delivery and continuous deployment, and which one to employ given the circumstances:\n\nContinuous deployment is the practice of releasing every good build to users—a more accurate name might have been “continuous release.”\n\n…\n\nWhile continuous deployment implies continuous delivery the converse is not true. Continuous delivery is about putting the release schedule in the hands of the business, not in the hands of IT.\n\n…\n\nIn the world of COTS, there are good marketing and support reasons why you'd not want to have more than a few \"released\" versions of your software in play at any given time (although you could still do regular \"developer\" or \"early access\" builds as Eclipse and the Omni Group do). There are probably other good reasons too—the important point is that they must be business reasons.\n\nin short, if you have constraints related to how many times your business can release software, then employ continuous delivery only. otherwise, tweak your delivery pipeline to automatically release your builds as a final step.\n\nif your company decides to implement continuous releasing of software, then make sure that the period of time from code commit to release to production is predictable and on the order of minutes. this way, the developer committing the change can plan to be around to verify and potentially fix the change in production.\n\nCD and Microservices The concepts and practices behind continuous delivery most certainly predate microservices architectures, but they become all the more essential when dealing with potentially hundreds of unique deployment units. But how do we map microservices applications to deployment pipelines? Should there be one single master pipeline that is responsible for monitoring, building, testing, and delivering all services? If not, of how many deployment units should the results of any given delivery pipeline be comprised?\n\nIn our experience, the biggest advantages are reaped if there is a separate pipeline per service. Only by dedicating pipelines to services will we be able to make sure that each of the services that make up the full application can be released (and potentially recalled) individually, and without negatively impacting the rest of the system.\n\nTool Support for Continuous Delivery When investing in starting a culture of CD, you can start fairly small and simple. For example, if you are already practicing continuous integration in your organization, you can often leverage the same toolset to trigger and monitor additional dependent steps that are gated by success of previous stages.\n\n5Jez Humble, \"Continuous Delivery vs Continuous Deployment,\" August 13 2010. http://continuousdelivery. com/2010/08/continuous-delivery-vs-continuous-deployment/ 6Jez Humble, David Farley, Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation, Addison-Wesley Professional 2010 156\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nAs an example, LivingSocial engineering started its own continuous delivery journey by repurposing Jenkins CI to kick off downstream deployment jobs automatically if the build-and-test CI job succeeded. Figure 10-2 shows the example of a Jenkins project configuration that triggers builds on two other downstream Jenkins project.\n\nFigure 10-2. Jenkins CI project configuration that triggers dependent deployments\n\nThe projects triggered sequentially shown here are a deployment to the staging fabric (internally called MYQA) and a subsequent production deployment. Triggering the dependent deployment projects after the CI build hinges on the build status; only if the latest code in the mainline branch could be built and tested successfully, and is hence considered “stable,” will the deployments be started.\n\nNote that LivingSocial also developed a uniform set of tools (on top of the Capistrano deployment tool), command-line scripts collectively called ls-deploy, which can be used to automatically kick off deployments across all projects that can be deployed to all network fabrics. The universal usage of this tool, and the simplifications it provides, are more indications that a DevOps culture is essential to launch and support microservices architectures efficiently.\n\nToday there are quite a few other tools available that specifically focus on supporting continuous\n\ndelivery.\n\nAmazon.com made its internal build pipeline tool available for commercial use as the generally\n\navailable AWS CodePipeline (https://aws.amazon.com/codepipeline/). CodePipeline is a highly configurable and programmable tool that can be used independently of any of the other AWS service offerings, and it integrates with source repository, CI, and deployment solutions like GitHub or Jenkins. AWS CodePipeline workflows consist of sequential, interdependent stages that can communicate via input and output artifacts. Each stage is made up of actions that can be run in parallel or sequentially. CodePipeline provides an API that can mark single actions as success or failure, which serves as an excellent way to integrate custom actions into the flow of the delivery pipeline. CodeDeploy actions can even be directly implemented as serverless AWS Lambda functions (https://aws.amazon.com/lambda/), which can also directly report back results to influence the course of the pipeline.\n\nFigure 10-3 shows the graphical UI for creating and editing CodePipeline flows, in this case a very\n\nsimple test pipeline set up for an experimental service at LivingSocial.\n\n157\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-3. Example of a very simple delivery pipeline in AWS CodePipeline\n\nAnother relatively new arrival, but already a very promising looking CD tool, is Netflix Spinnaker\n\n(http://www.spinnaker.io/). Spinnaker is an open source project that not only allows you to build delivery pipelines, but also aims to introduce a layer of abstraction on top of the infrastructure used to run service software. It aims to support building, testing, and deploying code not only to all three major public cloud platforms (Amazon AWS, Microsoft Azure, and Google's Compute Engine), but also to private cloud installations by supporting a plugin approach for cloud back-ends. Pivotal Cloud Foundry private cloud support is already implemented in this fashion. Figure 10-4 shows the UI to set up Spinnaker pipelines.7\n\n7“Global Continuous Delivery with Spinnaker,” posted by Andy Glover, November 16, 2015; http://techblog. netflix.com/2015/11/global-continuous-delivery-with.html\n\n158\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-4. A Spinnaker pipeline, as shown on the Netflix techblog\n\nMaturing Code Preproduction In an application that is made up of many smaller interdependent service components, it is essential to increase confidence in any code changes in an integration environment.\n\nThere is only so much that testing any service can tell you when you test it in isolation, and it is\n\nunrealistic to assume that stubs or mocks for service dependencies are complete with respect to functional, as well as nonfunctional, edge cases.\n\nThe closer the integration environment is to simulating production-like conditions, the more reliable\n\nand hence valuable the feedback received from prerelease testing becomes.\n\nThe rest of this section describes our experience with the best ways to address these concerns.\n\nStaging as a Preproduction Environment While discussing a suitable development process for a microservices-based application in Chapter 8, we recommended maintaining several network fabrics in which to install instances of all services that make up the application.\n\nWhile the discussions in Chapter 8 focused on the need for a development sandbox environment, this section will consider the use of a staging fabric. Figure 10-5 describes the scenario we recommend implementing.\n\n159\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-5. Use of a staging fabric for preproduction testing\n\nOnce the delivery pipeline has built all the artifacts for a given service, and all tests that can be\n\nperformed in isolation have passed, it is time to deploy the service into a staging environment.\n\nWe will explore the options and our recommendations for generating an entire network fabric later in\n\nthis chapter. For now it is just important to emphasize that the staging setup should aim to mirror the actual production infrastructure as closely as possible.\n\nAt the very minimum, this means that all interdependent microservices (other than the one that is part of the current change set under test) are deployed and running in the staging environment using exactly the same code versions that are live in production.\n\nOn top of that, you should strive to make sure that the rest of the infrastructure can mirror the production setup. Operating systems, databases, proxies, and the like should all run in the same versions as in production. The routing layer configuration (routing tables, ports, use of SSL) and hardware resources (number of CPUs, physical or virtual machines, disk space, and so on) should match what is used to run your actual business.\n\nWe realize that significant costs can be involved in provisioning additional computing resources to fully simulate the production environment; additional hardware and software licenses need to be requisitioned, set up, and so on. It may well be that the costs outweigh the benefits.\n\nThe point is that we strongly advise going through such cost-benefit analysis. Any decision to skip setting up a realistic staging environment should be preceded by a detailed risk analysis, in which cost factors are attached to each risk. For example, how likely is it that a different version of the operating systems in production will cause a 30-minute outage, and how much revenue and/or customer trust would you lose? Once the staging environment is provisioned with the appropriate build artifacts, it needs to be brought\n\ninto a state that has all the appropriate data needed for preproduction testing. This can be best achieved by maintaining set of automated staging setup scripts.\n\nThe purpose of these scripts is to automatically bring the staging fabric into known good states, so that\n\nautomated and manual tests can be performed in a repeatable manner.\n\n160\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nTool Support for Staging Environments Setting up a staging environment that has the same properties as your production environment follows the same rules as all infrastructure-related tasks in a microservices architecture: if you do not want to overwhelm your engineering department with busy work, then it needs to be automated with a set of tools.\n\nThe tools of choice to implement the infrastructure-as-code include Chef, Vagrant, Ansible, and Puppet.\n\nAll of these tools can be used both for managing and provisioning your infrastructure and configuration based on version-controlled configuration files, rather than manually configuring hardware via interactive configuration tools.\n\nThey all can be used irrespective of your company’s choice to run the physical data centers with bare- metals servers or to use a public cloud, and regardless of the choice to utilize virtual machines or application containers or not.\n\nPaaS provider Heroku comes with very good documentation showing how to set up a staging environment,8 and we know from personal experience that using Heroku's cloud platform for staging purposes is very easy and convenient.\n\nAnother toolset we have successfully used to set up staging fabrics is Amazon's Web Services platform,\n\nand we are very impressed with the power of the tool chain surrounding infrastructure automation. AWS provides a dedicated set of services called AWS CloudFormation for storing infrastructure definitions, and to automatically generate environments based on these definitions. Another very useful tool that works together with CloudFormation is the community-supported CloudFormer,9 which can be used to create a CloudFormation-compatible infrastructure definition template for your staging environment from already- deployed production AWS resources.\n\nAnother very promising tool for infrastructure automation is Otto by HashiCorp (https://www.\n\nottoproject.io/). We mention it here even though it is in a very early stage of development, because it promises to greatly simplify the development-to-staging-to-production fabric transitions. While it unfortunately does not yet handle any fabrics other than development on the local workstation and a production deployment, support for more stages is on the project’s roadmap.10\n\nAs far as the staging setup scripts are concerned, their aim is to make sure that the data stores for each\n\nof the services are primed with a set of sample data that enables the end-to-end and exploratory tests to execute both normal and exceptional paths.\n\nSuch scripts are usually very business-logic specific, and are by that token often very close to the particular technology in use. As an example, when SQL-based databases are used to store service state, they often include raw SQL statements stored in seed files, which are imported into the databases for each of the services involved at staging set-up time.\n\nSome companies, including LivingSocial, also provide access to production database dumps (which are scrubbed of sensitive information), which such set-up scripts can use to prepopulate the services' database. While exact repeatability of test situations is lost (because of the dynamic nature of production data), the advantage gained here is that the set-up will more closely match the real-world conditions in which your application needs to function.\n\n8Heroku Dev Center documentation, “Managing Multiple Environments for an App”; https://devcenter.heroku. com/articles/multiple-environments 9AWS Developer Tools: CloudFormer template creation tool; http://aws.amazon.com/ developertools/6460180344805680 10See the related discussion on the project's GitHub issue number 161; https://github.com/hashicorp/otto/ issues/161\n\n161",
      "page_number": 169
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 177-188)",
      "start_page": 177,
      "end_page": 188,
      "detection_method": "topic_boundary",
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nSafer Releases As mentioned in the previous section, setting up a one-to-one production-equivalent staging fabric is sometimes not a cost-effective option for every business.\n\nAlso, while setting up a production-like, preproduction environment is definitely very valuable for detecting issues when integrating a microservices-based application end-to-end, it is certainly no 100% guarantee that the same will be true when exposed to production conditions.\n\nTherefore, it is a good strategy to try to make the process of releasing code changes to production—as well as potentially rolling back defective changes—as safe as possible. The following two release strategies are aimed at supporting the resilience and recovery tenet you’ll see in the section discussing the seven deployment tenets later in this chapter.\n\nUsing microservices, and having properly separated the services into small self-contained units, helps\n\nthe roll-out of code changes incrementally to each single service without blocking other changes to other ones. This is another advantage compared to maintaining a monolithic application, where all changes have to stack up sequentially.\n\nBlue-Green Deployment\n\nLike so many software architectural patterns, blue-green deployment existed for many years as a practice before being named, and it is not entirely certain who first named the technique. In his 2010 article on the subject, Martin Fowler credits Dan North and Jez Humble with coining the term.11\n\nIn its essence, blue-green deployment is a technique that allows for fast cut-over of production traffic to\n\na new code version, but only after a series of smoke tests have been successfully executed.\n\nThis is done by delivering the new code to the production environments (labelled green), to be installed\n\non the service host alongside the service code version currently exposed to live production traffic (which is called blue code).12\n\nAs part of the blue-to-green switch, a set of automated smoke tests are then performed on the green\n\ncode. Once the green code is confirmed to be working as expected, the service host's configuration is changed to direct all incoming requests to the green code.\n\nBlue code now becomes unused, but still installed on the host. This sets you up for rapid rollback in case any problems are detected shortly after the switch-over, so you can easily and quickly switch back to the blue environment.\n\nFigure 10-6 illustrates the process of blue-green deployment.\n\n11Martin Fowler, “BlueGreenDeployment,” 1 March 2010; http://martinfowler.com/bliki/BlueGreenDeployment. html 12Some implementations of blue-green deployments choose to instead install the green code on an entirely separate set of service hosts, and then change over production traffic by changing load-balancer or DNS configuration. The principles of this technique still apply unchanged, though.\n\n162\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-6. Blue and green code are installed side-by-side, and switched upon release\n\nCanary in the Coal Mine\n\nAnother approach for making the release process safer and less impactful that is often used in microservices architectures is referred to as canary release.\n\nCanary releasing—sometimes also referred to as shadow deployment or incremental rollout—reduces\n\nthe risk of releasing new versions of software to production by means of a gradual roll out.\n\nThe diagram in Figure 10-7 describes the technique.\n\nFigure 10-7. Canary releasing in three stages of traffic to the new version\n\n163\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nThe following steps are usually taken during incremental deployments:\n\n\n\nThe new code is deployed alongside the code that is currently used by 100% of the production traffic (step 1 in Figure 10-7).\n\nNext, only a small percentage (such as 10%) of service clients is exposed to the new code version (step 2 in Figure 10-7).\n\nOnce released to 10% of traffic, the engineering team will closely identify and monitor the performance of the change (based, for example, on feature functionality, response times, or service and client-side failure rates).\n\nWhen enough confidence is gained that the new version is functioning as desired, more traffic is diverted from the existing production code version and sent toward the new code. Monitoring continues to ensure that the new service version is behaving as designed.\n\n\n\nFinally, once enough traffic increase iterations have been performed satisfactorily, 100% of client traffic is directed to the new code version, and it becomes available to everybody (step 3 in Figure 10-7).\n\nCanary releasing is very closely related to blue-green deployment in that both versions are deployed to\n\nthe production service hosts simultaneously.\n\nThe main difference is that both versions coexist in production for a much longer time, as the switch-\n\nover from old to new code versions happens in stages (as opposed to the “all or nothing” traffic routing approach employed in the blue-green deployment strategy).\n\nIn a variation of the canary release process, some engineering organizations divert traffic to new code\n\nversions based on criteria other than by percentage or request. One example that we have seen practiced is to identify traffic generated by employee users, and then cherry-pick new or changed functionality only to such internal clients. Exposing new code to such a controlled user group first often also helps with monitoring changes in client-side experience.\n\nAnother twist sometimes applied to this phased rollout is to simply copy a portion of production traffic\n\nand feed it into the new service code version. You can then watch the new version handle the requests without exposing the results to the clients, which will continue to be serviced by service instances running the old production version. While the advantage here lies in additional safety (as initially no clients will receive results generated by the new code), the trade-off is that results are often less meaningful because the impact of the new service code is not verified end-to-end.\n\nNote that neither canary releases nor blue-green deployments by themselves address the problem of mutating shared resources, such as potentially breaking migrations for database tables that might be used by both the old and the new versions of the code. These problems need to be mitigated by other means; for example, via additive-only tables and duplicated data for the new code.\n\nAssigning Resources to Microservices We have tried to demonstrate throughout this book that having a large number of self-contained, small services to make up your application has many upsides.\n\nIn this section, we first recap the gains achieved by adopting microservices. Next we walk through each\n\nof the current technology options for assigning the services to computing resources, and we compare how each one of them supports the microservices architecture advantages previously identified.\n\n164\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nTenets for Microservice Deployments Let's start out by detailing some of the main principles that microservices architectures bring to the business when it comes to deployment and operational aspects:\n\n\n\nYou build it, you run it: Development teams gain autonomy of the entire lifecycle and operational aspects of the software they produce. Developers are empowered to make their own technology choices, as they also are involved in monitoring and maintaining the infrastructure and computing resources on which their software runs.\n\nContinuously and independently deliverable: Code bases for services can be kept small and self-contained. As a result, code artifacts can be built, delivered, and deployed in isolation. Changes in one microservice can trigger a delivery pipeline that is independent from the pipelines associated with other deployment units.\n\n\n\nLocality of technology choice: As the only way to access microservices' capabilities is via well-defined APIs, the technology choices and all dependent resources (like operating systems, data stores, library dependencies, and so on) are local to any given microservice. Therefore, they can be changed or updated fairly inexpensively and without the risk of impacting other parts of the overall application.\n\n\n\nLocality of configuration: Each service has its own set of configuration properties that can be tweaked. This also means that their runtime properties can be independently optimized. Also, when specific security constraints apply to their mission (such as PII or PCI requirements), they can be secured without affecting any interdependencies with other deployment units.\n\n\n\nIndividual scaling: When functionalities need to be scaled out, microservices provide the business with very fine-grained options and hence greater flexibility. Some microservices might profit most from being horizontally scaled, while others should best be scaled vertically. When horizontal scaling is best, the additional resources (such as additional hosts) can be added more cost-effectively, as the additional computing power can be added to smaller parts of the application. Partitioning services based on their expected load is also easier. Your team might own a telemetry service that needs to handle 100,000 requests per minute with very small payloads, while it also owns an internal reporting application with only 10 requests per minute, which trigger longer running data queries. These services will have very different scaling concerns, which means they should live in separate microservices, so that their constraints can be addressed with independent deployment units.\n\n\n\nSelf-contained codebases: Microservices have separated and focused concerns, which make understanding, changing, and verifying their code safer and faster. Code changes can be kept more local and have less potential to affect other parts of your application. Test suites can be focused on a smaller set of scenarios and can hence have very small execution times.\n\nResilience and recovery: Building your application based on services involves more individual service processes. While this means that the chance that any one process can fail increases, this risk is mitigated by the fact that the impact caused by a single process failing is much smaller. Also, the culture of employing circuit breakers and sane fallback values into clients to address dependency outages will become much more engrained in a successful development team. Mean time to recovery is usually also shorter than in a monolithic application, as independent deployment units make the cause of regressions more easily identifiable, and rolling back and launching smaller build artifacts is faster.\n\n165\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nWe will examine various strategies for assigning computing resources to microservices in the remainder\n\nof this section, paying special attention to how each of these strategies can help implement the advantages just described.\n\nOptions for Running Microservices Ever since the invention of computers, much time and effort has been spent in the IT industry make the use of physical computing resources more and more efficient.\n\nOne of the main advances in achieving that has been the development of virtualization, which is mainly\n\naimed at increasing efficiency of a physical server by partitioning it into smaller and smaller units, each of which roughly has the look-and-feel of being an independent host in itself.\n\nThis section will look at each of the major levels of virtualization currently in use. For each level it will explain the pros and cons of installing a single microservice versus multiple microservices in a given virtualization unit for that level, considering whether the microservices tenets discussed above can be maintained.\n\nRunning on Dedicated Hardware\n\nThe largest possible level of virtualization granularity is to not virtualize at all; all services can run directly on physical machines.\n\nThe diagram in Figure 10-8 shows the options.\n\nFigure 10-8. Deploying a single microservice vs. multiple microservices directly on the physical server\n\n166\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nLet's first look at the scenario where each microservice would run on a dedicated physical server. We\n\nknow of no microservices architectures in the wild that really even seriously considered running such a setup. Dedicating a server to each of the often hundreds of microservices in a production environment and duplicating such a configuration in a staging fabric, and potentially even a development sandbox, would be simply prohibitively expensive for most businesses.\n\nEven if money was not an issue given the “you build it, you run it” principle, the development teams would need to provision a physical server each time they decided to start a new service project or needed to scale out the number of service instances. Lead times to procure new hardware for such situations would be too long for any team to avoid idling and would thereby decrease velocity. In most businesses, the development teams' autonomy would also vanish, as other departments are usually involved in granting provisioning requests.\n\nA slightly more reasonable choice is to host more than one service instance on any given physical\n\nserver. In this case, “you build it, you run it” is still affected, as monitoring and support would be complicated by the fact that resource contention (for CPU, memory, connections, storage, and so on) would need to be tracked down and addressed across multiple services. In our experience at LivingSocial we have run into quite a few situations where long-running, resource-intensive tasks for one service caused degradation in another service that was installed on the same physical box, and tracking these issues down takes time-intensive investigations using multiple tools.\n\nRunning more than one service on the same physical machine can also adversely affect the “locality of\n\ntechnology choice” tenet. What do you do if more two services installed on the same, nonvirtualized machine have conflicting requirements for dependencies like libraries or even operating systems or OS versions?\n\nTeam autonomy and the “locality of configuration” benefits are also hard to maintain if services from\n\nmore than one team are deployed to the same physical host. If two services are competing for the same resources, then it will be very hard to configure the partitioning of these resources optimally, and arbitration in such situations will then have to fall to a third team that will be tasked with general infrastructure maintenance.\n\nWhat if there are special requirements about the sensitivity of data held and vended by one service that\n\nis installed alongside services that do not need to follow the same stringency? Another case in point is that the desired “locality of configuration” will be hard to implement with co-hosted services.\n\nResource contention is also the reason why “individual scaling” would become more complicated to implement. How do you best identify the physical servers on which to install additional resources (such as another service instance, or more memory or disk space)?\n\nLikewise, “resilience and recovery” will be negatively impacted if the failure or resource inefficiencies (like memory leaks or CPU stalling) of one service can negatively impact the entire server environment, and therefore the rest of the services installed and running on the server.\n\nRunning Services in an Enterprise Application Server (EAS)\n\nThe next small step toward using the physical servers more resource-efficiently is the use of an EAS technology, like Java Enterprise Edition (JEE)-compatible server products, or Microsoft Internet Information Server (IIS) for .NET technologies.\n\nThe advantage of this virtualization is that an EAS usually gives you nice features like unified service monitoring and alarming, hot service restarts, and clustering. Also, if you run several services that run on top of the Java Virtual Machine (JVM) or the .NET Common Language Runtime (CLR), they help with better use of host resources by requiring only one instance of the respective platform's runtime, which all the service code artifacts (like JEE .WAR files) share.\n\nEmploying an EAS solution where a single service is installed in each EAS incarnation is even less advantageous than running each microservice on a dedicated physical machine. Such an approach would share all the disadvantages of dedicating a server to each service, plus it would incur the costs for the overhead required to run the EAS itself.\n\nFigure 10-9 therefore depicts running more than one service inside an EAS.\n\n167\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-9. Using an Enterprise Application Server to run multiple microservices\n\nEven when you choose to install multiple services inside the same EAS container, the drawbacks are\n\nnearly the same as with running the services directly on the service boxes. While the various EAS suppliers advertise benefits that address some of the monitoring and support concerns,13 this approach introduces new impediments to the “locality of technology choice” tenet: when choosing an EAS, you lock yourself into a particular technology stack for your service implementations, as well (for example, services can be JVM or .NET-based only). Also, while JEE servers usually come with easy ways to redeploy just a single service's code artifact into the EAS container, this is not a given for all EASes per se, so it is well worth your time to research whether your application server allows for this. If not, the “continuously and independently deliverable” benefit will no longer be true.\n\nUsing VMs\n\nIn this approach to using computing resources more efficiently, additional software called a hypervisor is installed in the physical server—either in the server's firmware, or on top of an operating system—to enable virtualization. The hypervisor then allows you to divide the underlying single server machine into a number of separate hosts called virtual machines (VMs).\n\n13Anyone who has attempted to analyze the behavior of Java threads for applications that share the same JVM process can attest that improvements to monitoring are certainly not perceived by every developer.\n\n168\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nEach VM for all intents and purposes looks and feels very much like a separate, physical server: it runs\n\nits own operating system (including its kernel, should it have one) and application process independent from the other VMs installed on top of the hypervisor. They are also separated from any potentially underlying host operating system.\n\nThe hypervisor takes on all VM management tasks, from assigning the physical resources of the host\n\nmachine to the VMs, to controlling and even potentially resizing the VMs. To perform these tasks, the hypervisor itself uses resources on the host machine, though, and the more VMs need managing, the more of the physical server's resources it will needs. This places a natural upper bound on the number of VMs that can be efficiently run on any given server.\n\nFigure 10-10 shows how VMs relate to the physical host machine and the services running inside them.\n\nFigure 10-10. VMs can be configured to run a single or multiple microservices on top of a hypervisor\n\nAs was an option with physical servers directly, and also with employing EAS containers, you can choose to run more than one microservice inside any and all virtual machines controlled by the hypervisor. The problem with that choice is that you will still negatively impact all the tenets mentioned earlier when discussing the disadvantages of running more than one service per hosting unit.\n\nOn the other hand, the use of virtual machines is a great step toward avoiding the downsides involved\n\nwith having to acquire and provision a new physical host for each time you decide to add services. While there is, as mentioned, an upper bound on how many VMs can be managed on a single server, you can get to the state where you host each service independently on a dedicated VM much more cost-effectively.\n\n169\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nOne particular advantage over running directly on a physical machine is good support for the “continuously\n\nand independently deliverable” and “individual scaling” tenets. This is mainly based on the fact that VM images can be automatically built for each service to be deployed in isolation, as part of the service's delivery pipeline. This will also enable automatic scaling mechanisms, as you can start to monitor service loads and respond in a scripted way by deploying a new VM for just the small set of services that experience a higher load.\n\nNote that these advantages cannot be realized when you decide to deploy multiple services into\n\nthe same VM, as you would be obligated to bundle a set of service to always be co-hosted, and thereby sacrificing their independent deliverability and individual scalability.\n\nThere are a large number of choices for software (called “type-2”) and hardware (“type-1”) virtualization.\n\nAWS EC2 deployable Amazon Machine Images (AMIs), VMWare's VMWare Player, Redhat's KVM, and Oracle's VirtualBox are all choices where the hypervisor runs on top of the host OS. Type-1 virtualization options include the Open Source Xen project, VMWare's VMWare ESXisolution, and Microsoft's Hyper-V.\n\nUsing LXC with Docker\n\nVirtualizing at the machine level is not the only option available in order to get more isolated hosting units out of your physical hardware.\n\nAs briefly mentioned in Chapter 8, Linux Containers (LXC) can be used to virtualize multiple isolated\n\nLinux systems on top of a single Linux host machine.\n\nOne of the notable differences between LXCs and VMs is that only one Linux kernel is in use across all virtualization units. VMs host the entire stack of OS, kernel, and applications, while LXCs contain only the higher OS layers and applications. Reusing the same kernel allows LXCs to run without the need for hypervisor software, which VMs need to be managed and to partition host resources; physical resources for LXCs are managed by a single kernel.\n\nAs a result of these differences, LXCs are much speedier to bring into an up-and-running state than VMs, and their smaller footprint also means that a typical host can run a lot more LXCs than VMs before responsiveness is seriously impacted. This means that it will be much more cost-effective to run all your microservices in a separate LXC than it is to run them in separate VMs.\n\nOut of the box, creating, setting up, and installing apps inside LXCs is rather cumbersome. Here is\n\nwhere Docker or proprietary solutions like Heroku's container management come in.\n\nWe introduced Docker in Chapter 8 when we showed how to use interdependent containers for\n\nbuilding a development sandbox. Docker's main mission is to simplify container management and networking setup. It uses a text file called a Dockerfile to describe and install the applications in each container. Docker also comes with the concept of a shared repository for containers (called a Docker registry), so that you can reuse containers that have been contributed by others. Such a reusable container can either come from a public Docker registry like https://hub.docker.com/, or they can be provisioned from a private Docker registry, which you can install for your company-specific applications.\n\nCONTAINERS ON HEROKU'S PLATFORM\n\nthe company heroku offers a set of higher-level services often referred to as Platform as a Service (paas). it implements automated provisioning, deployment, monitoring, scaling, and high availability on top of aWs eC2 instances.\n\nheroku offers service-level options, mostly tiered by the amount of resources they dedicate to a hosting unit (called dyno in heroku's terminology). the largest dynos are called performance and performance-m, and they are hosted on a dedicated large eC2 instance.\n\nall other dyno types run in a multitenancy environment; that is, they run as single process inside an application virtual machine.\n\n170\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\naccording to heroku's own documentation, a dyno is a \"lightweight linux container that runs a single user-specified command.\"14 this means that heroku also uses lXC as their virtualization engine, which—as discussed at length earlier—is the same technology on which Docker is built.\n\nif you are considering the use of a paas provider for your microservices application, then we definitely recommend taking a closer look at heroku. We have deployed several microservices in multiple technologies (ruby on rails, Clojure, and go) into heroku's platform, and we loved the simplicity and convenience of the experience.\n\nTrying to describe all the available tooling on top of Docker containers is like trying to shoot at a very fast-moving target. Much work is being dedicated to managing many distributed Docker instances automatically, and even to create entire cloud platforms that work with Docker containers instead of VMs. We will talk a little more about these later when discussing automated scaling options.\n\nLooking at how to distribute the microservices you run across LXCs/Docker containers, there are even fewer reasonable arguments to co-deploy multiple services into the same container. That is the reason why Figure 10-11 illustrates 9 LXC containers running exactly one microservice inside each and every one of them.\n\nFigure 10-11. Docker LXCs running on top of a single Linux kernel, with no need for a hypervisor\n\n14See the “Dynos and the Dyno Manager” documentation page at https://devcenter.heroku.com/articles/ dynos#dynos\n\n171\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nIsolating each microservice in its own Docker container is much less resource-intensive than even using one per VM. Following such a “one service per container” mantra enables you to implement all the tenets of deploying microservices listed at the beginning of this chapter, and thereby reap all their benefits.\n\nNote also that nothing keeps you from running LXC containers inside VMs, so if you already have a\n\nVM-based infrastructure, you can fairly easily add finer-grained virtualization this way. In fact, AWS offers “Amazon EC2 container service,” which is specifically aimed at helping you manage Docker containers running on EC2 instances that are defined by AMIs, and which can be autoscaled to go on- or offline based on load.\n\nUsing Serverless Architectures\n\nThe newest technology option to arrive on the block for running microservices is something most generally referred to as serverless architecture. It can best be described as an additional layer that aims to alleviate you from even thinking about service hosts, so that service development is uncluttered and can be taken to a higher level of abstraction.\n\nWhen services run serverlessly, there is no longer a need for the developers to operate and maintain the computing resources themselves. The serverless platform where you run your service code will take care of all of these aspects on your behalf. This has the potential to remove a large amount of complexity and operational workload. Automatically scaling up and down, resilience, delivery and deployment, operating system and library updates, setting up monitoring, alarming, and logging capabilities, and so on will now be a responsibility of the platform that provides the serverless computing services.\n\nAUTOSCALING—THE HOLY GRAIL OF INFRASTRUCTURE AUTOMATION\n\nautoscaling is definitely not a “Day one” issue for getting started with microservices architectures. there are quite a few preconditions that need to be met before you can even start thinking about attempting to take advantage of such a feature.\n\nFor one, your organization needs to be set up for speedy and fully automated provisioning of computing resources. this means that your services are either running inside vMs or lXC containers (as provisioning new physical servers—even via automated means—will simply have too much lead time to be useful), or you are running in a public cloud (like aWs or Ms azure) or inside a paas (like heroku).\n\nall major public cloud platforms allow for automated scaling in one way or another. as an example, aWs has long had support for autoscaling (https://aws.amazon.com/autoscaling/). When you run your own cloud via Docker containers, there are solutions based on google's kubernetes (see http:// kubernetes.io/docs/user-guide/horizontal-pod-autoscaling/).\n\nnext, you will do well to research what exactly the autoscaling parameters and triggers are, and how you can tweak them. Most autoscaling mechanisms built into the platforms we have seen are more reactive than proactive: infrastructure monitoring discovers that Cpu load increases, server memory runs low, or that there is a lot more traffic on a particular service, and subsequently spins up new computing power (in the form of a new service instance, for example).\n\n172\n\nChapter 10 ■ Deploying anD running MiCroserviCes\n\nWhile such after-the-fact smarts certainly help deal with acute resource shortages, getting into situations where the client experience degrades can be entirely avoided by some proactive planning. in our experience, you can learn a lot of useful things from watching your site's traffic patterns, which you can apply to “semi-automatic” scaling: if you see that your customers mostly visit your site during the us time zones' lunch hours, then introduce rules that anticipate this behavior and add computing resources ahead of time, so that the site is ready for the rush hour.\n\nAt the time of writing, there are not many providers for serverless platforms. Early providers are Hook.\n\nio (https://hook.io), Webtask.io (https://webtask.io/), and Amazon's AWS Lambda (https://aws. amazon.com/lambda/). Amazon offers another complementary service called AWS API Gateway, which can add a layer in front of Lambda functions to aid security, transform request and response messages, and provide options for throttling and auditing.\n\nMoving to a serverless model for running your services also scores very high in checking which of\n\nthe microservices operational tenets are met. In fact, the only two points we do not see addressed by the currently available options for serverless platforms are “you build it, you run it” and “locality of technology choice.”\n\nIt can certainly be argued that the underlying assumptions for the first of these two tenets are no longer\n\nvalid. The main point of serverless computing is that developers need to be less involved in most of the operational concerns. What remains desirable, though, is that the development team gets notified via alarming if any of the operational metrics of their services start to deviate from normalcy. Amazon's AWS Lambda platform integrates very closely with AWS CloudWatch, so at least this platform has that aspect covered.\n\nWhere current platforms are definitely lacking are the technology options they provide out of the box. Webtask.io requires you to write all code in Node.js using JavaScript. AWS Lambda currently only supports JavaScript, JVM-compatible, and Python code. Hook.io is the platform that has the longest list of supported languages.15 There are also other restrictions on each of the platforms around technology choices; for example, the backing file systems or data store options, and neither of them appears to be set up out-of-the- box to support arbitrary, user-defined replacements.\n\nAs mentioned at the beginning of this subsection, serverless applications are a very new and rapidly evolving technology, with proprietary approaches and platforms, which carries a high potential for provider lock-in. There is not much that has been written about the subject, but we are looking forward to reading Obie Fernandez' upcoming book16 on the subject.\n\nSummary\n\nThis concludes our overview of what we consider the most important aspects of deploying and running microservice-based applications.\n\nOne of the main points to take away from all this is that automating as much as feasibly possible around\n\nbuilding, testing, deploying and releasing is an absolute must if you want to make microservices work for your team.\n\nIntroducing continuous integration for each of your services helps to significantly shorten the feedback loop during your development cycle, and the traceability of your large code artifacts, as long as you heed our advice to place each service into a separate code repository and set up CI for each one of them.\n\nGoing a step further and adopting continuous delivery from code commit to deploying it on the production hosts is also a practice we highly recommend adopting. Building a delivery pipeline per microservice can be done with the support of a number of existing tools.\n\n15See the code examples page for a list of languages in which samples are implemented; https://hook.io/examples 16Obie Fernandez, Serverless: Patterns of Modern Application Design Using Microservices (Amazon Web Services Edition); https://leanpub.com/serverless\n\n173",
      "page_number": 177
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 189-196)",
      "start_page": 189,
      "end_page": 196,
      "detection_method": "topic_boundary",
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nYou can even go so far as to automate the step of releasing new code to customers, but in this case, you have to think about how to do so safely. We explain strategies for implementing safety measures, including the use of a staging environment in which to let your code mature prior to production release, blue-green deployment and canary releasing.\n\nWe also discussed the available options for assigning computing resources to your microservices, based on seven tenets we identified as beneficial for deploying microservices. We explained the different levels of virtualization available today, and how their use can support the advantages gained by employing microservices.\n\nIn the next chapter, we will place our focus on design and implementation aspects of consumer-facing\n\nclient applications backed by a microservices architecture.\n\n174\n\nPART IV\n\nPutting Everything Together\n\nCHAPTER 11\n\nPolyglot Services\n\nAs laid out in Chapters 4 and 5, applications in a service-oriented architecture can profit greatly from using common protocols and data formats (such as JSON over HTTP), and from choosing a single IDL for interapplication communication.\n\nIn this chapter, we lay out why applying uniformity of programming languages, data stores, or other technology components across all the services in your microservices application might not be the best choice. After looking at the various components in which the services in your overall system can change, this chapter discusses the benefits and pitfalls of using differing technologies throughout your service implementations. Next we provide some guidelines for deciding when to become more polyglot in your architecture. Finally, we’ll offer some practical tips for the process of introducing technology options to your microservices application.\n\nDimensions of Change The majority of services in a microservices environment expose many areas that together make up a multidimensional vector for potential improvements.\n\nMany service applications can be rewritten using different programming languages. Even if they choose to keep the language the same across services (or different versions of the same service), the service owners can use a different set of libraries to rely on. Some companies we know, while keeping the core programming language the same, vary application frameworks between different deployment units.\n\nAnother popular change is to examine the use of a different type of data store, such as by switching\n\nsome portion of the service's persistence layer to use a NoSQL solution in place of an RDBMS.\n\nIntroducing a caching layer, for example by adding CDN usage to a publicly exposed service, or by adding Memcached or in-memory caching between the service application code and the data persistence layer, is another adjustment that is often introduced after the initial launch of a microservices application, and it can be applied selectively to a subset of the services an organization has deployed.\n\nAdvantages of Using Different Languages and Technologies Considering the fact that software engineering as a discipline is less than seven decades old, the number of options for technological choices any software developer faces in today's industry landscape is simply staggering.\n\nLet's look at some of the resulting advantages of such advancements in technology diversity, and see\n\nwhy a business would want to operate in a polyglot technology environment.\n\n177\n\nChapter 11 ■ polyglot ServiCeS\n\nSpecialization When looking at the various layers of systems that make up the entirety of a modern online application, you will find a large number of different languages and technologies in use.\n\nLooking at the mobile devices landscape, Apple's iOS and Google's Android deliver very specialized\n\noperating systems, programming languages, and API frameworks, as well as surrounding tools to build native applications. The main goal of this specialization is to enable developers to use the limited resource of the devices optimally.\n\nSpecializing in simple use cases of key-based access, key-value stores can be optimized for much more\n\nefficient access to persisted data than any SQL-based database system.\n\nThe lingua franca of dynamic web-based clients is JavaScript, mostly because all major browsers support it. But still, even within the JavaScript ecosystem, several different frameworks have been able to establish themselves, further specializing for particular use cases. Examples are:\n\nGoogle-sponsored Angular.js (https://angularjs.org), which is particularly popular for building single-page web applications because it offers synchronized two-way data binding, and alleviates much of the hassle of writing code for DOM manipulation.\n\n\n\nFacebook's React.js (https://facebook.github.io/react/), which shines when it comes to highly dynamic web applications, where the concept of working on a virtual DOM results in client-side performance gains.\n\n\n\nYehuda Katz' MVC framework Ember.js (http://emberjs.com), which aims to combine the two-way data-binding and server-side DOM rendering advantages of the previous two frameworks.\n\nThe main theme in all of these specializations is optimizing for the requirements of new use cases.\n\nThe fact that all these technologies exist and continue to thrive is a testament to the fact that they serve a purpose, and any business that wants to build a successful online application is effectively forced into being polyglot already.\n\nBy the same token, and as an extension to this point, applying diversity to the languages and frameworks with which the service applications are being built in the back-end can also help with optimization at this layer.\n\nExperimentation Progress in the computer software and hardware space is still very fast-paced. And even if skeptics are correct that the applicability of Moore's law about transistor density in integrated circuits is about to cease, software system will still see orders of magnitude improvements. Google's company culture asks their employees to apply \"10x thinking\" when thinking about innovation. Their rationale is:\n\n[T]rue innovation happens when you try to improve something by 10 times rather than by 10%.1\n\nBoth the development and the use of such quantum leap advances require a great deal of willingness to experiment. Therefore, curiosity remains a key competitive advantage for any software developer, as well as for the businesses that employ software engineers; in our experience, it is usually the most talented people who will be motivated by opportunities to learn new technologies.\n\n1“Creating a Culture of Innovation: Eight ideas that work at Google”, Google for work, https://apps.google.com/ learn-more/creating_a_culture_of_innovation.html, as of September12, 2016\n\n178\n\nChapter 11 ■ polyglot ServiCeS\n\nThe good news is that implementing a microservices architecture provides a business with many benefits for experimentation. Because the units of deployment are small applications with very limited concerns, experimentation can happen with a very low impact on the overall business.\n\nEvolution Businesses can only achieve longevity when they learn how to adapt to an ever-changing environment, and staying agile by quickly adjusting or reversing course is key. Often the result of such environmental changes is that the business needs shift toward lesser, or sometimes even entirely new, use cases.\n\nAs an example, we have been part of companies where the initial system was not designed to cater to sophisticated, end user triggered product searches, often including search criteria that are dependent on geolocation data gathered about the customer. This can often be best addressed by researching and introducing specialized technology, such as Apache Lucene (https://lucene.apache.org) and Solr (https://lucene.apache.org/solr), or Elasticsearch (https://www.elastic.co/products/ elasticsearch) for the search use cases.\n\nIt is also not uncommon that companies become the victim of their own success, having to serve a much larger number of online visitors than initially envisioned. In such a case, evolving the core services of your infrastructure to a language or framework that makes better use of CPU and memory resources, or that can more easily deal with a higher-level concurrency, can often help in addressing the bottleneck before it turns into a case of subpar user experience or bad press.\n\nNavigating the Pitfalls of Being Polyglot Going from an environment where all your services are using the same technology stack to embracing a more diverse set of tools surely also has some inherent risks that need addressing.\n\nFirst, you will need to think about the development cycle. Your software engineers will need some time\n\nto familiarize themselves with the new language or any other technology that is being investigated. This is not simply restricted to learning the syntax of an unfamiliar language, but also extends to decisions about libraries, frameworks for automated testing or web request handling, and how to automatically install the development environment on an engineer's work computer.\n\nUsually, these bootstrapping decisions are most efficiently made by a very small group of engineers that\n\nare passionate to learn about the technology in question. Their findings can then be spread among the rest of the service-owning team in short “brown bag lunch” tech talk sessions.\n\nSimilar issues can be encountered in the area of deploying new technologies, especially when you have to work in an environment where host machines are shared between services, and the new technology could potentially impact deployment strategies of existing services.\n\nThe cleanest option to address such concerns is generally to avoid co-deploying more than one system, for example, using application containers.2 Otherwise, we recommend pairing up an application developer with an operations engineer to investigate the cleanest way to deploy the new tools alongside existing technology. That way, initial deployment guidelines for the new language stack, or the like, benefit from input of a larger set of engineering functions in the business.\n\nOnce deployment issues are addressed, it is paramount not to forget the requirements for operational\n\nsupport for running new machinery in a production environment. As mentioned for the deployment aspects, having dedicated servers or containers per service will make this task easier, as the newcomer will not be competing for resources with existing services.\n\n2See Chapter 10 for an in-depth discussion of options for deploying and running microservices.\n\n179\n\nChapter 11 ■ polyglot ServiCeS\n\nAnother thing that could help during the deployment and operational support research steps is choosing a technology that is significantly similar to other standards you have already chosen. If, for example, your company already supports JVM applications via running JRuby, and you are trying to introduce a new language that is also JVM-based (such as Clojure), then you might find that you will have to address fewer open issues when introducing this new technology.\n\nIt is very important to involve all parties that have a stake in supporting the new technology in production in the process of deciding how to initially launch the new tech component in production. If your organization is following the “you built it, you run it” setup where application developer teams entirely support the software they own in all production aspects, then fewer departments need to be part of this decision process. But if you are in the fortunate position to work with a technical operations support team, make sure you involve them in these discussions.\n\nConsidering the tail end of a technology's usage life cycle, you should also think about what it will take\n\nto retire any system you introduce. For one, this applies on the one hand to technologies you experiment with and find that they do not add enough value to justify the effort of maintaining them. But more generally this also applies to the technology that is superseded by its successfully tested replacement. Make sure to have follow-through on such technology retirements and replacements, or you will run the risk of having too many technologies to support.3\n\nA Process for Adopting New Technologies When adding new technologies to their stack, companies are well advised to come up with a written policy concerning the selection process. The advantage of this lies in the fact that decision making can be based on a published, step-by-step process with clear milestones and measurable criteria for moving forward.\n\nThis section lists a few questions to answer during the selection, implementation, and production\n\nlaunch steps.\n\nDocument All Findings We have seen all too often that decisions about adopting or rejecting a certain language or technology tool are made without documenting the results on which the rationale for those decisions is based. The drawbacks of such lack of documentation are at least twofold.\n\nFor one, there is a potential for wasted time and effort of a second internal team, which might have started its own investigation process for the same technology, although they could have taken advantage of the results of the previous investigations that were unknown to them.\n\nSecond, the business or technological environment in which the company operates might change\n\nover time, and those circumstances might invalidate, or at least significantly affect, the findings of a prior investigation of a given technology. If there is no detailed documentation about past technology trials, then the company might miss out on the competitive advantage a previously rejected tool might bring them under the new conditions.\n\nStatement of Motivation and Goals When documenting your technology trials, we recommend starting out with a section that describes the motivation for the new language or toolset.\n\nOften such sections will list areas where the current tech stack is known to be lacking, or particular\n\naspects that need to be considered with regard to a new technology (for example, maturity, safety, community and tool support, or performance).\n\n3See notes on a process for evaluating a new technology later in this chapter.\n\n180\n\nChapter 11 ■ polyglot ServiCeS\n\nAt other times the goal of the experiment might not necessarily be directed at a well-established and recognized problem. The motivation might simply be that the general tech community is praising a new tool or language that can solve problems for which there was no prior awareness inside your company, or where improvements can help the company to be more successful.\n\nA reason for investigating a new language might be that the same language could be used in several\n\nlayers of your stack, thereby reducing the general skill set required from your developers. We have seen this argument used when companies decide to introduce Node.js (https://nodejs.org/en/) to write their back-end services in JavaScript when their browser-based logic is already coded in this language. Similarly, we often hear the argument for adopting ClojureScript (http://clojurescript.org/) in the front-end from companies that are invested in Clojure as their service-side language.\n\nAnother very prominent reason for testing a new technology is the attempt to speed up the\n\nperformance of business-critical areas. We know of companies that have discovered that full-text search in their product database became an increasingly important feature for their growing customer base, and investigations needed to be made to replace the initially MySQL-backed search functionality because of performance issues. Similar arguments sometimes lead companies to experiment with migrating a service implementation from one programming language to another one (for example, moving from Ruby to Rust, Go, Elixir, or the like).\n\nIn attempts to avoid being a victim of their own success, many companies realize later on in their existence that the technology they chose does not scale well under load, so they will need to research switching to technologies that behave more resource-efficiently under high load.\n\nDo you see the costs of maintaining a particular technology you use skyrocket? Then it might be time to\n\nresearch more stable or more easily extensible alternatives.\n\nIf your company has a hard time attracting talented engineers or keeping your most valuable tech personnel engaged and excited, then experimenting with new and exciting technologies to counteract attrition or to increase the recruiting success is surely also a reasonable motivation for experimentation.\n\nWhatever turns out to be the reason behind your research, make sure to explicitly state the areas you\n\nwill cover in your experimentation, and list the expected outcomes and metrics associated with judging the success of the experiment.\n\nImplementation Planning Once the groundwork is laid with the description of the goals of the research, the first milestone to pass is to get an official go-ahead from the company's management that the experiment can proceed, and that the metrics and acceptance criteria are sound. We highly recommend getting such buy-in, and officially setting aside time for team members to work on such \"new tech\" projects. In our experience, these projects will not happen unless they appear on a formal roadmap for company work.\n\nNow it is time for planning the execution. It is always a good idea to publicize the plan to test a new\n\ntechnology internally to the broadest possible group of people.\n\nThis can not only help solicit feedback on the strategy and goals, but also aid in finding support and\n\nlike-minded co-workers who are interested in pairing on the initial implementation.\n\nMake sure to measure and record all data that you laid out in your statement of goals as you go about implementing the new technology in your organization. This documentation will help you and a potential gatekeeper decide if the experiment results meet the acceptance criteria to take things to the next step. And even if the outcome of that analysis is such that a go-ahead cannot be recommended, it is very useful to document just this fact and preserve this knowledge within your organization.\n\nIn an ideal scenario, most of this implementation can be performed in a sandbox of staging\n\nenvironment, although we realize that true results—especially where performance is concerned—will only be achieved once the new technology is exposed to production traffic.\n\n181\n\nChapter 11 ■ polyglot ServiCeS\n\nGo-Live Criteria The most important point before adding any new technology to production is to clearly understand the potential impact and failures modes. Hopefully you have had a chance to discover some of these scenarios during a pre-production test in a staging environment.\n\nIf you are replacing existing functionality like for like, such that the new technology has to support all the current use cases, then a great help in verifying that the new service behaves as expected will be a suite of black-box tests4 that you run against both the old and the new system.\n\nTo help mitigate the risk involved in such projects you should seriously consider adopting deployment\n\nstrategies that allow for a fast and easy rollback mechanism (such as blue-green deployments or canary releases, which were explained in the previous chapter).\n\nAspects that you can never neglect when going to production are logging, monitoring, alarming, and data collection. If you cannot analyze a technology's status messaging, then you will never know if it is in failure mode or what particular problems it might encounter in case of an outage. And if your business prides itself on being data driven, then integrating the newcomer into the data warehouse layer must be a first priority.\n\nIt is generally a good guideline to keep the number of languages and other technology options in production as small as possible, but as large as necessary. This reduces maintenance costs and cognitive load for the tech personnel while at the same time providing an environment for staying ahead of the curve and agile.\n\nIt is never a good idea to use a technology that only one or two technical people in the company are sufficiently familiar with to support it in a production setting. This means that you should neither adopt a technology where only a small number of developers understand the system, nor migrate away from a technology after there is too little know-how left to grasp what it takes to deprecate it.\n\nOne way to avoid this is to focus on hiring polyglot and well-rounded engineers who make a habit of\n\nlearning the company's infrastructure. Make sure that a significant amount of time during the on-boarding process for new team members is spent learning the ins and outs of all technologies that are in use by your team.\n\nA Case Study: Moving books-service from Rails to Rust When looking at our example book store business that is running in a microservices environment, we discovered that our most trafficked service is the books-service, which contains all the core product data about the books for sale on the site.\n\nThe service has been implemented using the rapid application development framework Ruby on Rails,\n\nmostly because the developers were most familiar with this technology, and because it is very convenient and easy to implement a JSON API service in it.\n\nDuring day-to-day operations the team owning books-service notices that response times start to become affected during peak business times when the largest number of customers browse the product catalog. While there is no immediate urgency to address the issue—that is, no decrease in purchase rates can be correlated to these times of heavy load—the development team is starting to investigate the root cause of the scaling issue.\n\nAnalysis of the logs and New Relic APM5 data for books-service shows that much time is spent in garbage collection of Ruby objects, and—even though the service was implemented as a Rails API-only6 project—lots of framework code is logged as being busy handling the request and response objects outside the actual application logic.\n\n4Chapter 9 has more details about such tests, and how to craft them. 5This is an application-performance monitoring tool. See https://newrelic.com/application-monitoring. 6A variant of a standard Ruby on Rails application that tailors the middleware components used to the minimum need for API services. See the respective Rails guide at http://edgeguides.rubyonrails.org/api_app.html for more details.\n\n182",
      "page_number": 189
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 197-204)",
      "start_page": 197,
      "end_page": 204,
      "detection_method": "topic_boundary",
      "content": "Chapter 11 ■ polyglot ServiCeS\n\nAlice, one of the senior engineers on the books-service team, suggests starting to investigate alternatives, and the team agrees to take over part of her current responsibilities so that she has time to dedicate to this effort.\n\nThus tasked, she starts out by writing down a problem statement, including all data gathered about the\n\nnature of books-service's perceived scaling issues.\n\nAfter doing some research about more resource-efficient languages and less impactful web frameworks,\n\nshe comes across the Mozilla Research-sponsored language Rust (https://www.rust-lang.org/), which seems very promising for addressing the perceived performance challenges. Additionally, the language seems to raise a lot of interest in the community, and many of the great engineers in her professional network have started to gravitate toward Rust.\n\nAlice formulates a goal statement of the experimentation to include concrete and measurable gains in\n\nperformance and reduction in computing resource usage, compared to the current production-deployed version of books-service. She also includes specifics about the expected advantages of using the Rust language for the experiment, quoting some very rudimentary benchmarks she has gathered from credible sources, as well as her own small experiments. As the company is hiring and therefore in competition with many other employers for experienced, high-quality engineers, she also lists the increase in attractiveness to potential engineering candidates caused by using a cutting-edge technology like Rust. Her metric to test this hypothesis is to compare feedback from the engineering team's combined professional network, so she plans to ask all teams to survey their contacts about an increase in interest in a company that uses Rust in its technology stack.\n\nThe books-service team meets to discuss Alice's proposed experiment based on the statement of motivation and goals, and unanimous agreement is given to go ahead with the next phase of the experiment. This means that time is officially dedicated in the next two team sprints to work on the first stage of the implementation plan. Alice will pair with Peter, another engineer on her team, on this project, simply to make sure that knowledge is not restricted to a single person, and they agree to schedule regular “brownbag lunch” sessions where they share updates with the rest of the team.\n\nAlice and Peter are also asked to work to identify a contact on the company's technical operations (TechOps) team, in order to share knowledge about potential deployment or operational concerns, as well as solicit suggestions they might have for running Rust in the existing infrastructure.\n\nAfter learning the basics of the Rust language and identifying potential API frameworks in the language,\n\nAlice and Peter decide to use Iron (http://ironframework.io/)—a very lightweight and extensible web framework for Rust. They experimented with several other Rust frameworks, but found that they introduced more latency than Iron, and as performance is one of the main objectives of the experiment, they decide to stick with Iron.\n\nWhile reimplementing all the features of the existing Ruby version of books-service in Rust, they can\n\nrely on an existing, out-of-process black-box test suite, written in Ruby using the Cucumber DSL. Once they get to the point of feature equivalence between the Ruby and Rust versions, they can finally measure response times and resource usage for the Rust service under a simulated large load in the company's staging network environment.\n\nInitial results are very promising, and Alice and Peter write up a page on their internal company Wiki which summarizes the results. They present the results first to their own team, which is impressed with the observed 10x response time improvements in the staging environment.\n\nAn official vote is taken and recorded to go ahead and spend time investigating and addressing all the\n\nnecessary steps that are involved with a “go-live” decision in production. These steps include more training on the technologies in use for developers and TechOps engineers, as well as making sure that the team has visibility into the new service's production behavior by integrating it with the logging and alarming infrastructure. Data warehouse integration of the metrics gathered for business intelligence queries also needs to be addressed before the Rust service can go live.\n\nOnce integrating Rust into the company's canary deployment process is addressed by the TechOps department, the team can finally observe the new service under actual production load, and compare it side by side with the existing Ruby version of the service.\n\n183\n\nChapter 11 ■ polyglot ServiCeS\n\nSummary This chapter discussed the implications of being technologically polyglot. After explaining the vectors of change in a microservices architecture where technological diversity can and should be tried, we looked at the opportunities in adopting several competing technologies.\n\nWe identified the main potential advantages to be the benefits of specialization, progress achieved\n\nby experimentation, and remaining agile by setting yourself up to evolve more quickly with your business requirements.\n\nNext we looked at how to navigate around the potential pitfalls of working in a polyglot technology environment, before shifting our focus to describing a viable process for experimentation with, and adoption of, a new technology in a company's tech stack.\n\nWe concluded the chapter with a fictitious case study about replacing a Ruby-on-Rails based microservice in our example bookstore with an equivalent service and APIs implemented in Rust.\n\nThe next chapter will go into much more detail about the implementation of the microservices\n\narchitecture that makes up the sample book store application.\n\n184\n\nCHAPTER 12\n\nSingle-Client Application—Mobile and Web\n\nYour microservice-powered application will ultimately be displayed to actual people using a web browser or mobile devices; we call those customer facing applications. In order to function well, those applications need to make calling each service they depend on a trivial task. We know that many developers have been exposed only to the ideas and concepts behind the microservices architecture, and still have very little actual exposure to how all the pieces fit together in practice.\n\nIn this chapter, we share our book sample project; it has working code that resembles a real-life scenario, so that you can run all the services in this project and see a web application running. Our project implementation is far from complete, as that's not the goal of this book; we only have enough pieces in place to show how everything connects. We will focus on explaining the main building blocks of a few pages of the sample project, and explain the components that make each part of some pages work. We will show code in Ruby and Rust, which are the two languages we are comfortable with and can fairly easily use to showcase interaction between a web application and a few services. Even if you are not familiar with those languages, we recommend you follow along with this chapter as it will put into practice a lot of the concepts we have described in the book so far.\n\nApplication Overview Before we get started, take a look at the architecture of what we are building in this chapter. We'll be implementing much of the application example described throughout the book in the following services:\n\n\n\nshowcase is a client-facing web application, built in Ruby on Rails, that will be receiving requests from customers. The source code is available at https://github. com/microservices-from-day-one/showcase.\n\n\n\nbooks-service is a service responsible for owning all data associated with books, built in Rust. The source code is available at https://github.com/microservices- from-day-one/books.\n\n\n\npurchases-service is a service responsible for owning all data associated with shopping carts and purchases, built in Ruby. The source code is available at https://github.com/microservices-from-day-one/purchases.\n\n\n\nreviews-service is a service responsible for owning all data associated with book reviews, built in Ruby. The source code is available at https://github.com/ microservices-from-day-one/reviews.\n\n185\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nThe diagram in Figure 12-1 makes it easy to visualize that our customers will be using their browsers to point to the web server that is running the showcase app, which will then make requests to books-service, purchases-service and reviews-service in order to render web pages to our users.\n\nFigure 12-1. Architecture of the sample project\n\n■ Note service has a readme file with details on how to set up each application, and we also provide a repository with more documentation at https://github.com/microservices-from-day-one/docs.\n\nYou can—and should—follow along by loading the code available from the Urls shown. each\n\nIt is time to put all that we've talked about in this book together, and demonstrate a working web application in a microservices environment. The sample application for this book is a bookstore, which has a set of fairly standard features that we will describe and dissect in this chapter. We'll talk about each of the application's pages, and will explain all service calls that happen in order to get each page rendered. The sample application is not fully functional, as it would probably require a team of full-time developers to implement and maintain such an application; however, we decided to implement enough functionality that will showcase a lot of the topics covered in this book.\n\nHome Page The home page for the bookstore is a fairly standard e-commerce landing page, as you can see in Figure 12-2.\n\n186\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nFigure 12-2. Home page of our bookstore\n\nBefore we go into the details of a few elements of the home page, we want to bring to your attention the client library we are using to connect to our services. You probably remember from Chapters 5 and 6 that we decided to use the JSON API standard to describe all our responses, which would keep our services interfaces and responses similar across the whole system. We build a client library in Ruby—known as a Ruby gem—to interface with our services. The payoff for using a standard starts to appear, as we were able to build our own library on top of an open source JSON API compliant library called json_api_client (https://github.com/chingor13/json_api_client).\n\n187\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nIn our client library, named api_client and available at https://github.com/microservices-from-\n\nday-one/api-client, you can see that we have defined one module per service, so we'll have a module called ApiClient::Books for books-service, another called ApiClient::Purchases for purchases- service, and so on. Each module is very simple and leverages the json_api_client gem; they also define a Base class, which is responsible for having code that will be shared by all models. The Base class for ApiClient::Books is listed here:\n\nmodule ApiClient module Books\n\nclass Base < JsonApiClient::Resource def self.site @site ||= \"#{ServiceDiscovery.url_for(:books)}/v1\" end end\n\nend end\n\nrequire 'api_client/books/book' require 'api_client/books/category'\n\nThat class is responsible for mainly two things, defining where the endpoint for books-service is to\n\nbe found, as implemented in the site class method, and defining a base resource, which inherits from JsonApiClient::Resource; therefore all the subclasses of this class don't really need to know that detail of the implementation.\n\nWith the Base class in the client library explained, we'll now show how a specific resource is defined. Our example is the ApiClient::Books::Book class, which represents the book model as served by books- service; the class is defined here:\n\nmodule ApiClient module Books\n\nclass Book < Base end\n\nend end\n\nAs you can see, there is really no code associated with this class; we are able to define a standard resource, which has a set of well-known method available that match to a RESTful set of CRUD endpoints— again, one of the advantages of following a standard. So our client library is, as we expected, a very thin layer that allows client applications to reach the service endpoints we wrote.\n\nNow it's time to look at how some of the elements of this page have come together. An element that is a constant feature in most e-commerce applications is our shopping cart widget, which shows us the current value of the cart, as well as the number of elements in it. All details related to purchases are owned by purchases-service, which also owns all cart details. When any user accesses the bookstore, this service tries to find a cart associated with the user, which is saved in the session:\n\ndef current_cart_id if session[:cart_id].nil?\n\n188\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\ncart = ApiClient::Purchases::Cart.create session[:cart_id] = cart.cart_id end session[:cart_id] end helper_method :current_cart_id\n\nAs you can see in this code, the application will load the card_id associated with the user from the session. For new users that do not have an associated cart_id, a new cart will be generated in purchases- service via the ApiClient::Purchases::Cart.create call. Again, we will take a look at the implementation in purchases-service, available at https://github.com/microservices-from-day-one/purchases, which is a Ruby service. In Ruby, we have an open-source gem called jsonapi-resources (https://github.com/ cerebris/jsonapi-resources), which is described as “a resource-focused Rails library for developing JSON API compliant servers.” By following the JSON API standard, and defining a set of resources that define the models to be exposed in the API, this gem makes it very easy to create JSON API-complaint services. In purchases-service, the Cart resource is defined as follows:\n\nmodule V1 class CartResource < JSONAPI::Resource attributes :cart_id, :expires_at, :total, :items_count end end\n\nThe cart’s endpoints are defined in the V1::CartsController class, defined as:\n\nnamespace :v1 do jsonapi_resources :carts end\n\nin routes.rb and implemented as:\n\nmodule V1 class CartsController < JSONAPI::ResourceController end end\n\nThat is all that we need in order to render the shopping cart widget. It currently shows the number of items in the cart and its value as 0; however, that will change when we implement the endpoints and code to add books to the cart.\n\nNow it's time to look into how some of the elements of this page have come together. The main block in\n\nthe home page displays four featured books that are presented to the user. We will start with a very simple implementation, which is a service call that will return the six most recent books in books-service. The code that makes that service call is\n\ndef home_products(limit: 8) @home_products ||= ApiClient::Books::Book.paginate(page: 1, per_page: limit).to_a end\n\nWhen this code is executed, the following request comes to books-service:\n\nGET http://localhost:5000/v1/books?page%5Bnumber%5D=1&page%5Bsize%5D=6`\n\n189\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nIt returns\n\n{ \"data\": [ { \"attributes\": { \"author\": \"Mo Willems\", \"cover_image\": \"https://images-na.ssl-images-amazon.com/images/I/61jurA6wsFL._SX258_ BO1,204,203,200_.jpg\", \"description\": \"Diva, a small yet brave dog, and Flea, a curious streetwise cat, develop an unexpected friendship in this unforgettable tale of discovery.\", \"isbn\": \"978-1484722848\", \"pages\": 80, \"price\": 987, \"slug\": \"the-story-of-diva-and-flea\", \"title\": \"The Story of Diva and Flea\" }, \"id\": \"2497fe7d-4797-4506-a3cd-85e42d90415b\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Charles C. Mann\", \"cover_image\": \"https://upload.wikimedia.org/wikipedia/en/b/b7/1491-cover.jpg\", \"description\": \"In this groundbreaking work of science, history, and archaeology, Charles C. Mann radically alters our understanding of the Americas before the arrival of Columbus in 1492.\", \"isbn\": \"978-1400032051\", \"pages\": 541, \"price\": 1299, \"slug\": \"1491-new-revelations\", \"title\": \"1491: New Revelations of the Americas Before Columbus\" }, \"id\": \"2d831aba-75bb-48b6-bae6-9f1bb613d29b\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Jane Austen\", \"cover_image\": \"https://themodernmanuscript.files.wordpress.com/2013/01/pride-and- prejudice-1946.jpg\", \"description\": \"Pride and Prejudice is a novel of manners by Jane Austen, first published in 1813. The story follows the main character, Elizabeth Bennet, as she deals with issues of manners, upbringing, morality and education.\", \"isbn\": \"978-1484110980\", \"pages\": 279, \"price\": 951, \"slug\": \"pride-and-prejudice\", \"title\": \"Pride and Prejudice\" },\n\n190",
      "page_number": 197
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 205-213)",
      "start_page": 205,
      "end_page": 213,
      "detection_method": "topic_boundary",
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\n\"id\": \"12892302-7ef4-4023-9999-2b3560a2a4d6\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"James Dean \", \"cover_image\": \"https://i.harperapps.com/covers/9780062303899/x300.png\", \"description\": \"Pete the Cat is going scuba diving! Before he hits the water,\n\nCaptain Joe tells him about all the sea creatures he can encounter, and Pete is super excited to see a seahorse.\",\n\n\"isbn\": \"978-0062303882\", \"price\": 319, \"slug\": \"pete-the-cat-scuba-cat\", \"title\": \"Pete the Cat: Scuba-Cat\" }, \"id\": \"8217e21a-97db-4be9-9feb-4cea97a64948\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Eric Litwin\", \"cover_image\": \"https://i.harperapps.com/covers/9780061906220/x300.png\", \"description\": \"Pete the Cat goes walking down the street wearing his brand new\n\nwhite shoes. Along the way, his shoes change from white to red to blue to brown to WET as he steps in piles of strawberries, blueberries, and other big messes.\",\n\n\"isbn\": \"978-0061906237\", \"pages\": 40, \"price\": 1139, \"slug\": \"pete-the-cat-i-love-my-white-shoes\", \"title\": \"Pete the Cat: I Love My White Shoes\" }, \"id\": \"008672a6-c48c-49b2-98e8-b78914f2fce4\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Eric Litwin\", \"cover_image\": \"https://i.harperapps.com/covers/9780062110589/x300.png\", \"description\": \"Pete the Cat is wearing his favorite shirt—the one with the four totally groovy buttons. But when one falls off, does Pete cry? Goodness, no! He just keeps on singing his song—after all, what could be groovier than three groovy buttons?\",\n\n\"isbn\": \"978-0062110589\", \"pages\": 40, \"price\": 1175, \"slug\": \"pete-the-cat-and-his-four-groovy-buttons\", \"title\": \"Pete the Cat and His Four Groovy Buttons\" }, \"id\": \"20ee1ef4-6000-460a-b715-40da50979fc7\", \"type\": \"books\" },\n\n191\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\n{ \"attributes\": { \"author\": \"James Dean\", \"cover_image\": \"https://i.harperapps.com/covers/9780062304186/x300.png\", \"description\": \"Pete the Cat takes on the classic favorite children's song \\\"Five Little Pumpkins\\\" in New York Times bestselling author James Dean's Pete the Cat: Five Little Pumpkins. Join Pete as he rocks out to this cool adaptation of the classic Halloween song!\", \"isbn\": \"978-0062304186\", \"pages\": 32, \"price\": 791, \"slug\": \"pete-the-cat-five-little-pumpkins\", \"title\": \"Pete the Cat: Five Little Pumpkins\" }, \"id\": \"25f2db6f-966a-4c24-9047-1fdfeb43e465\", \"type\": \"books\" } ] } ```\n\nWith those products retrieved, the code in the template that renders the page will pick four random\n\nbooks out of those six, so that users see different books in subsequent requests:\n\n``` <% for product in home_products(limit: 6).shuffle.first(4) do %> <%= render partial: 'home/product', locals: {product: product} %> <% end %>\n\nWe’ve started with a very simple service call; however, as the site develops a wider range of products\n\nand starts to collect user behavior data, this is an area that could benefit from the existence of user recommendations, which could be implemented in a recommendations service, and be based on an algorithm that takes into consideration the user's purchase habits, along with the purchase habits of similar users, to maximize sales opportunities.\n\nFeatured Categories In the second half of the home page, we have the Featured Categories block, which is a section used to feature a couple of categories the business is interested in focusing on. In the showcase application, we make a call to the featured_categories endpoint in books-service, available at /v1/categories/featured, which is responsible for knowing the logic behind deciding how a category gets featured; in the browser, that call looks like Figure 12-3.\n\n192\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nFigure 12-3. Featured categories block in home page\n\ndef featured_categories @featured_categories ||= ApiClient::Books::Category.featured.first(2) end\n\nThe Category resource in the service returns a JSON API-compliant resource, with a response that looks\n\nlike this:\n\n{ \"data\": [ { \"attributes\": { \"description\": \"Something for the little ones.\", \"image\": \"http://sleepingshouldbeeasy.com/wp-content/uploads/2013/01/childrens- books-about-going-to-school-vertical.jpg\", \"name\": \"Childrens Books\", \"slug\": \"children-s-books\" }, \"id\": \"20a05be7-3a9b-4ed0-a36b-ed36b0504557\", \"type\": \"categories\" }, { \"attributes\": { \"description\": \"Learn about your past.\", \"image\": \"http://media1.s-nbcnews.com/ij.aspx?404;http://sys06-media.s-nbcnews. com:80/j/streams/2013/November/131120/2D9747505-Where_Were_You_book_jacket.blocks_desktop_ vertical_tease.jpg\", \"name\": \"History\", \"slug\": \"history\" }, \"id\": \"cc7f1963-23fc-485c-89ec-d1efb25966d1\", \"type\": \"categories\" },\n\n193\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\n{ \"attributes\": { \"description\": \"It is promise and hope. Titillations and excitations.\", \"image\": \"https://s-media-cache-ak0.pinimg.com/564x/29/bd/9e/29bd9eafd49a185874e706 fb4f896ba0.jpg\", \"name\": \"Romance\", \"slug\": \"romance\" }, \"id\": \"c85102c8-3ed2-4af9-be8b-f1008448f6cb\", \"type\": \"categories\" } ] }\n\nIn the service code, the featured endpoint has been implemented as a response that returns categories\n\nin random order, so that all categories are featured equally. Of course, in a production setup, we'd want to implement featured_categories based on business logic, probably based on featuring-best selling categories. In the described architecture, changes to the algorithm of how categories become featured are just an implementation detail of books-service and will have zero impact on the rest of the system, given that the API interface remains unchanged. Here's the current implementation of the featured_categories endpoint, implemented in Rust, and as we discussed in Chapter 11.\n\npub fn featured(req: &mut Request) -> IronResult<Response> { let pool = req.get::<Read<::AppDb>>().unwrap(); let conn = pool.get().unwrap();\n\nlet mut categories: Vec<Category> = Vec::new(); for row in &conn.query(\"SELECT category_id, name, description, slug, image FROM\n\ncategories ORDER BY random()\", &[]).unwrap() {\n\nlet category = Category::from_row(row); categories.push(category); }\n\nlet response = jsonapi::json_collection_wrapped_in(\"data\", categories); Ok(Response::with((status::Ok, Header(jsonapi::jsonapi_content_type()), response))) }\n\nOne last thing to mention about this element is that it is probably the most “cacheable” element of the\n\nhome page, meaning that it's very unlikely we will want featured categories to change frequently throughout the day; thus, we can use client-side caching to cache the featured categories block for a specific period of time (usually to be decided by the business), and not even have to make the service call on most requests, which also makes the application more performant and resilient to service failures. In this case, caching the response for 4 hours seems acceptable; so we could surround the client-side code with these lines:\n\n<% cache 'featured_categories', expires_in: 4.hours do %> <%= render partial: 'home/offers' %> <% end %>\n\n194\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nUsers Should Never See Internal Errors Now that you have seen some code, and we have the home page being rendered by loading data from a few services, we'll look into how we want to treat service errors in this user-facing application. We believe strongly that users should never see server errors; developers and designers should think about the user experience to be presented to the user when there's something wrong with the application. Based on the particular use case, treating errors will take a unique shape, as there are cases that you can easily recover from, and others not so much.\n\nThe rule of thumb is that you need to catch errors that happen when interacting with the server, give your users a fall-back, and try to degrade gracefully in all areas; but degrading means different things based on the situation. In some cases, you can make it look like there is no error at all, and in others, you still have to display a “something went wrong” message, which is still more user-friendly than a 500 server error.\n\nWe will start by looking at the shopping cart widget, which we described a few pages ago, and will see\n\nwhat happens to the application when that service is not available—down. To simulate purchases-service being down, we can just Ctrl-C that process and wait until we are sure it's no longer running. After doing that, just refresh your browser, and you will see that the home page greets you with an ugly ConnectionError error (see Figure 12-4).\n\nFigure 12-4. Error when loading the home page when dependencies are not available\n\n195\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nThe shopping cart widget gives us an insight into what is currently in the cart, and also works as a link to\n\nthe cart page. In this case, we could treat that error to make sure that we still render the widget, without the number of items in the cart nor the current cost. That fall-back seems to be acceptable, since it should only happen during down times of that one service. We've changed the helper method that loads the shopping cart object by adding error-handling code, and we also made sure we logged the error that is happening and left a TODO comment inline to alert us that something is wrong. In Chapter 14, we talk about some options for alerting this type of issue. The code ends by returning a nil object:\n\ndef shopping_cart @shopping_cart ||= ApiClient::Purchases::Cart.find(current_cart_id).first rescue JsonApiClient::Errors::ConnectionError => e Rails.logger.error(\"Error loading shopping_cart: e=#{e.message}\") # TODO: alert that an error is happening nil end\n\nWe can check for a nil object in the template that renders the shopping cart information:\n\n<% if shopping_cart %> <div class=\"total\"> <span class=\"simpleCart_total\"><%= render_price(shopping_cart.total) %></span> (<span id=\"quantity\" class=\"quantity\"><%= shopping_cart.items_count %></span>) </div> <% end %>\n\nFigures 12-5 and 12-6 are screenshots of the shopping cart widget first when it's working as expected\n\nand then under the error case, which may not even be noticeable to the user.\n\nFigure 12-5. Shopping cart widget in functional state\n\nFigure 12-6. Shopping cart widget when purchases-service is down\n\n196\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nWe demonstrated an error case that had a simple solution that didn't impact the page at lot; we could easily degrade gracefully. However, there are cases where that is not possible. For example, if the user were in the book detail page, and the books-service happened to be nonresponsive, you'd have to return a page with an error message that asks the user to “try again in a few minutes.”\n\nCircuit Breakers In most cases, the type of error handling code we’ve just explained is enough to help one application handle failure from a downstream dependency; however, in environments with a very high number of requests, it's possible that having a client-facing application keep making requests to a service when it knows it's down isn't a good solution, for two reasons:\n\n\n\nThe calling application will be slightly delayed, as it may have to wait for its timeout to realize that a dependency is unavailable, which may slow other systems.\n\n\n\nThe struggling server takes longer to respond, or come back to life, if it keeps receiving requests that it cannot handle, compounding the existing problem.\n\nFor that type of situation, the calling application should use the circuit breaker pattern we introduced\n\nin Chapter 6. It is a mechanism that will make the calling application try to request a resource from a service even when it notices that the service has not been responsive. Circuit breakers are usually set up by defining a failure threshold in the calling application that when met will cause the circuit to “open” for a predetermined period of time, causing the calling application to go into error handling mode without even calling the downstream dependency, because it already knows that it has been struggling to properly serve requests. This spares us from the two unwanted scenarios we described earlier and makes a system even more resilient in the face of an abnormal situation.\n\nBook Detail Page The next page we will look at is the book detail page, shown in Figure 12-7, which is probably the most important page in the application; it's where users will make a purchase decision. Here is some of the information a user would expect to see on the book page:\n\n\n\nbibliographic information such as title, author, cover image, description, ISBN, price, page count and publisher name\n\n\n\nreviews of books\n\n\n\nother people's review,\n\n\n\nability to review yourself\n\n\n\nadd to a wish list\n\n197\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nFigure 12-7. Screenshot of book detail page\n\nThe initial set of attributes we need to load are what we could call book metadata, and are owned by books-service; the client helper will load data from that service, by using the book “slug,” which is a unique SEO-friendly string used in the book URL, such as http://localhost:5004/products/pete-the-cat-i- love-my-white-shoes.\n\ndef book_details @book ||= ApiClient::Books::Book.where(slug: params[:id]).first end\n\nThe code above will make this service call:\n\nGET http://localhost:5000/v1/books?filter%5Bslug%5D=pete-the-cat-i-love-my-white-shoes\n\n198\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nThe call in turn returns:\n\n{ \"data\": [ { \"attributes\": { \"author\": \"Eric Litwin\", \"cover_image\": \"https://i.harperapps.com/covers/9780061906220/x300.png\", \"description\": \"Pete the Cat goes walking down the street wearing his brand new white shoes. Along the way, his shoes change from white to red to blue to brown to WET as he steps in piles of strawberries, blueberries, and other big messes.\", \"isbn\": \"978-0061906237\", \"price\": 1139, \"slug\": \"pete-the-cat-i-love-my-white-shoes\", \"title\": \"Pete the Cat: I Love My White Shoes\" }, \"id\": \"008672a6-c48c-49b2-98e8-b78914f2fce4\", \"type\": \"books\" } ] }\n\nAs mentioned previously, our ApiClient library knows the JSON API standard well, and transforms that\n\nresponse into an object we can easily use in the template code, such as this:\n\n<div class=\"single-para \"> <h4><%= book_details.title %></h4> <h5 class=\"item_price\"><%= render_price(book_details.price) %></h5> <p class=\"para\"><%= book_details.description %></p> <div class=\"prdt-info-grid\"> <ul> <li>- ISBN : <%= book_details.isbn %></li> <li>- Author : <%= book_details.author %></li> </ul> </div>\n\nWe hope this is starting to feel easier to follow; we'll follow exactly the same pattern to make service calls\n\nto load review information from reviews-service, which follows the same pattern of loading the book data.\n\nThe last element we'll talk about in the book detail page is the “related books” block, shown in\n\nFigure 12-8, which is a set of links to books that are somehow related to the book featured in this page. Much like the home page's Featured Books block, this is an element that will benefit from having its own endpoint, so that the business can tweak the rules for selecting those elements without that affecting any work on the front-end client application. The first step in implementing this is to define an endpoint in books-service that asks for books related to a specific book. We settle for an endpoint in the following format:\n\nGET /v1/books/2d831aba-75bb-48b6-bae6-9f1bb613d29b/related\n\n199",
      "page_number": 205
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 214-221)",
      "start_page": 214,
      "end_page": 221,
      "detection_method": "topic_boundary",
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nFigure 12-8. The related books block in the book detail page\n\nIn this example, the UUID (2d831aba-75bb-48b6-bae6-9f1bb613d29b) is the ID of the book for which we want to receive “related books.” For the sake of this example, we decided on using a very straightforward implementation that loads five random books that are not the book_id passed in the URL. Here's the implementation in Rust:\n\npub fn related(req: &mut Request) -> IronResult<Response> { let book_id = iron_helpers::extract_param_from_path(req, \"book_id\"); let book_uuid = Uuid::parse_str(&book_id).unwrap();\n\nlet pool = req.get::<Read<::AppDb>>().unwrap(); let conn = pool.get().unwrap();\n\nlet sql = \"SELECT book_id, title, author, description, isbn, price, pages, \\ slug, cover_image FROM books WHERE book_id != $1 ORDER BY random() LIMIT 5\";\n\nlet mut books: Vec<Book> = Vec::new(); for row in &conn.query(&sql, &[&book_uuid]).unwrap() { let book = Book::from_row(row); books.push(book); }\n\nlet response = jsonapi::json_collection_wrapped_in(\"data\", books); Ok(Response::with((status::Ok, Header(jsonapi::jsonapi_content_type()), response))) }\n\nWhen called, the preceding returns the following response:\n\n{ \"data\": [\n\n200\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\n{ \"attributes\": { \"author\": \"James Dean\", \"cover_image\": \"https://i.harperapps.com/covers/9780062304186/x300.png\", \"description\": \"Pete the Cat takes on the classic favorite children's song \\\"Five Little Pumpkins\\\" in New York Times bestselling author James Dean's Pete the Cat: Five Little Pumpkins. Join Pete as he rocks out to this cool adaptation of the classic Halloween song!\", \"isbn\": \"978-0062304186\", \"pages\": 100, \"price\": 791, \"slug\": \"pete-the-cat-five-little-pumpkins\", \"title\": \"Pete the Cat: Five Little Pumpkins\" }, \"id\": \"25f2db6f-966a-4c24-9047-1fdfeb43e465\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Eric Litwin\", \"cover_image\": \"https://i.harperapps.com/covers/9780062110589/x300.png\", \"description\": \"Pete the Cat is wearing his favorite shirt—the one with the four totally groovy buttons. But when one falls off, does Pete cry? Goodness, no! He just keeps on singing his song—after all, what could be groovier than three groovy buttons?\",\n\n\"isbn\": \"978-0062110589\", \"pages\": 100, \"price\": 1175, \"slug\": \"pete-the-cat-and-his-four-groovy-buttons\", \"title\": \"Pete the Cat and His Four Groovy Buttons\" }, \"id\": \"20ee1ef4-6000-460a-b715-40da50979fc7\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Charles C. Mann\", \"cover_image\": \"https://upload.wikimedia.org/wikipedia/en/b/b7/1491-cover.jpg\", \"description\": \"In this groundbreaking work of science, history, and archaeology,\n\nCharles C. Mann radically alters our understanding of the Americas before the arrival of Columbus in 1492.\",\n\n\"isbn\": \"978-1400032051\", \"pages\": 100, \"price\": 1299, \"slug\": \"1491-new-revelations\", \"title\": \"1491: New Revelations of the Americas Before Columbus\" }, \"id\": \"2d831aba-75bb-48b6-bae6-9f1bb613d29b\", \"type\": \"books\"\n\n201\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\n}, { \"attributes\": { \"author\": \"Eric Litwin\", \"cover_image\": \"https://i.harperapps.com/covers/9780061906220/x300.png\", \"description\": \"Pete the Cat goes walking down the street wearing his brand new\n\nwhite shoes. Along the way, his shoes change from white to red to blue to brown to WET as he steps in piles of strawberries, blueberries, and other big messes.\",\n\n\"isbn\": \"978-0061906237\", \"pages\": 100, \"price\": 1139, \"slug\": \"pete-the-cat-i-love-my-white-shoes\", \"title\": \"Pete the Cat: I Love My White Shoes\" }, \"id\": \"008672a6-c48c-49b2-98e8-b78914f2fce4\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Mo Willems\", \"cover_image\": \"https://images-na.ssl-images-amazon.com/images/I/61jurA6wsFL._SX258_\n\nBO1,204,203,200_.jpg\",\n\n\"description\": \"Diva, a small yet brave dog, and Flea, a curious streetwise cat, develop an unexpected friendship in this unforgettable tale of discovery.\",\n\n\"isbn\": \"978-1484722848\", \"pages\": 100, \"price\": 987, \"slug\": \"the-story-of-diva-and-flea\", \"title\": \"The Story of Diva and Flea\" }, \"id\": \"2497fe7d-4797-4506-a3cd-85e42d90415b\", \"type\": \"books\" } ] }\n\nWith everything implemented server-side, we have to make a tweak to the api_client library so that we can make calls to that endpoint. The json_api_client gem makes it ridiculously easy to give ourselves access to that endpoint; we just need to define a custom_endpoint in the Book class:\n\nmodule ApiClient module Books\n\nclass Book < Base custom_endpoint :related, on: :member, request_method: :get end\n\nend end\n\nTurning our attention back to the front-end application, we render those five related books in a separate\n\ntemplate, known as a partial, called related:\n\n202\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\n<%= render \"related\", books: book_details.related %>\n\nThe partial looks like this:\n\n<% for book in books do %> <div class=\"col-md-2 btm-grid\"> <a href=\"<%= book_path(book.slug) %>\"> <%= image_tag book.cover_image, alt: book.title, style: \"width: 120px\" %> <h4><%= book.title %></h4> <span><%= render_price(book.price) %></span> </a> </div> <% end %>\n\nThe Shopping Cart Page We've talked about the shopping cart widget, available in all pages in the application, but now let’s turn our attention to the shopping cart page, shown in Figure 12-9, which is where the user is taken when clicking in the shopping cart widget. As expected, the shopping cart detail page has not only the summary about\n\nFigure 12-9. Shopping cart page\n\n203\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nnumber of items and total price in the cart, but also information about each of the items the user has added to the cart. In our example, we have two books in the shopping cart.\n\nIn purchases-service, shopping carts are modeled as multiple entities. A Cart is the shopping cart\n\nmain model, it has attributes such as items_count, total, user_id, and expires_at, and an Item has attributes such as item_id, quantity, and price. In order to display the shopping cart details, we need to have access to each line item in the cart; this is unlike the shopping cart widget, which only needed basic cart information. We've decided to use a single endpoint to return cart information, but return a richer response. This is what we have referred to as different representations of the same data in responses.\n\nThe following service call:\n\nGET http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2\n\nreturns this:\n\n{ \"data\": { \"id\": \"35c082ba-b974-4d78-9526-4fc6ed7b54a2\", \"type\": \"carts\", \"links\": { \"self\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2\" }, \"attributes\": { \"cart_id\": \"35c082ba-b974-4d78-9526-4fc6ed7b54a2\", \"expires_at\": \"2016-10-23T01:54:29.408Z\", \"total\": 1618, \"items_count\": 2 }, \"relationships\": { \"items\": { \"links\": { \"self\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2/\n\nrelationships/items\",\n\n\"related\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2/\n\nitems\"\n\n} } } } }\n\nFor the cart page, we pass an extra parameter, include=items, to that same API call; it looks like this:\n\nGET localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2?include=items\n\nIt returns\n\n{ \"data\": { \"id\": \"35c082ba-b974-4d78-9526-4fc6ed7b54a2\", \"type\": \"carts\", \"links\": {\n\n204\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\n\"self\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2\" }, \"attributes\": { \"cart_id\": \"35c082ba-b974-4d78-9526-4fc6ed7b54a2\", \"expires_at\": \"2016-10-23T01:54:29.408Z\", \"total\": 1618, \"items_count\": 2 }, \"relationships\": { \"items\": { \"links\": { \"self\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2/\n\nrelationships/items\",\n\n\"related\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2/\n\nitems\"\n\n}, \"data\": [ { \"type\": \"items\", \"id\": \"2d831aba-75bb-48b6-bae6-9f1bb613d29b\" }, { \"type\": \"items\", \"id\": \"8217e21a-97db-4be9-9feb-4cea97a64948\" } ] } } }, \"included\": [ { \"id\": \"2d831aba-75bb-48b6-bae6-9f1bb613d29b\", \"type\": \"items\", \"links\": { \"self\": \"http://localhost:5001/v1/items/2d831aba-75bb-48b6-bae6-9f1bb613d29b\" }, \"attributes\": { \"item_id\": \"2d831aba-75bb-48b6-bae6-9f1bb613d29b\", \"quantity\": 1, \"price\": 1299 } }, { \"id\": \"8217e21a-97db-4be9-9feb-4cea97a64948\", \"type\": \"items\", \"links\": {\n\n205\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\n\"self\": \"http://localhost:5001/v1/items/8217e21a-97db-4be9-9feb-4cea97a64948\" }, \"attributes\": { \"item_id\": \"8217e21a-97db-4be9-9feb-4cea97a64948\", \"quantity\": 1, \"price\": 319 } } ] }\n\nAs you can see, the second response is much richer; it contains information about the two items currently included in the cart, so we'll definitely use that in our cart page. The main advantage of this approach is that we can use the same endpoint in both use cases—cart widget and cart page—but still have a faster response time in the case where we need less data, avoiding sending unnecessary data over the wire and making fewer database calls to provide a response in the service, thus improving overall system performance.\n\nBack to the showcase application, where we define a new helper method to load that information. An interesting feature supported by the JSON API specification is called sparse fieldsets (http:// jsonapi.org/format/#fetching-sparse-fieldsets), which allow a client to request that an endpoint return only specific fields in the response on a per-type basis by including a fields parameter. Sparse fieldsets can be useful in responses that may return a large number of attributes by default, where clients may gain performance by specifying only the attributes they are interested in. For example, if we had code that needed only the current value in dollars of a cart, we could use the client gem to load only that attribute by calling ApiClient::Purchases::Cart.select(:total).find(..), which would make the following service call:\n\nGET http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2?fields[carts]=total\n\nand would generate this response:\n\n{ \"data\": { \"id\": \"35c082ba-b974-4d78-9526-4fc6ed7b54a2\", \"type\": \"carts\", \"links\": { \"self\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2\" }, \"attributes\": { \"total\": 1618 } } }\n\nExercises for You As we mentioned at the beginning of the chapter, we've just scratched the surface of functionality that can be built for our bookstore, and we have some ideas about what could be done next. We'll list a couple of services that you could spend some time implementing, if that sounds like something interesting to you. One\n\n206\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nof the great things about it is that you can use whatever programming language/environment you're most comfortable with.\n\nAn obviously missing piece is a user service, responsible for storing and managing data associated with the users of the bookstore. A user should be able to, among many things, register an account and log in, save books to a wish list, “like” book categories, see his or her own—and others'—wish lists, and follow other users. Based on those quick requirements, some of the endpoints we would want to see implemented are:\n\nUser CRUD\n\n\n\nname, email, password (hash), ID, books_in_wishlist, categories\n\n\n\nAdd liked category: [user_token, category_id] -> [user_id, category_id]\n\n\n\nRemove liked category: [user_token, category_id] -> 204\n\nGet all liked categories: [user_token] -> [category_id1, category_id2, …]\n\n\n\nAdd to wish list: [user_token, book_id] -> [user_id, book_id]\n\n\n\nRemove from wish list: [user_token, book_id] -> 204\n\nGet all books on wish list: [user_token] -> [book_id1, book_id2, …]\n\n\n\nFollow a user: [user_token, user_id] -> [user_id, user_id]\n\nUnfollow a user: [user_token, user_id] -> 204\n\nGet users I followed: [user_token] -> [user1, user2, ...]\n\nGet users that follow me: [user_token] -> [user1, user2, ...]\n\nWe also mentioned that the home page would eventually evolve and benefit a lot from showing books\n\nthat match the logged-in user; a recommendations service would be a very welcome addition to our microservices architecture. That service would probably feed from user-service, since it will have quite a bit of information about users, as well as purchases-service, which has data about what books have been purchased, or are currently in the cart. To make its API very simple, the service could have a single endpoint that, given a user_id, returns a book_id list as recommendations.\n\nService Orchestration You probably have noticed that a lot of the service-related code in the showcase front-end application is actually simple, because it has to make fairly simple service calls in many places; you can imagine that this can get complicated, and some service calls depend on the results of other calls, which can introduce complexities at the front-end layer that may be better suited elsewhere. To help simplify some of that complexity, we recommend using a Back-end for Front-end (BFF) service, briefly introduced in Chapter 7, which would be a service that knows exactly what data the front-end needs to render its pages and would be responsible for fetching that data from many different clients. A practical example of that is the home page, which makes multiple service calls to many services. The code in showcase could be simplified if it could\n\n207",
      "page_number": 214
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 222-229)",
      "start_page": 222,
      "end_page": 229,
      "detection_method": "topic_boundary",
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nFigure 12-10. Architecture of application with showcase-service in place\n\nmake a call to a showcase-service, which would be responsible for orchestrating calls to multiple services, and properly deal with error conditions there. Figure 12-10 diagrams this approach.\n\nWe have described most of the service interactions of this chapter with the web front-end in mind, but most applications have multiple client-facing applications from the start nowadays, which would be mobile iOS and Android applications. In the case of mobile applications, we really recommend the use of a BFF— let's call it mobile-service, because your product and service will evolve; and unlike a web application that has only one version deployed at any time, you will always have a population of users who will not upgrade your application to the latest and will get stuck making calls to APIs that you may want to change, and even deprecate.\n\nPerformance Considerations We have deliberately mentioned performance in this chapter only briefly; we touched on some aspects that will make your application faster, such as sparse fieldsets and using a cache to reduce the number of service calls. We believe that all services should be fairly fast, and that performance will only be a serious problem when you start having slow service responses; so we recommend that you keep an eye on each service’s response times to easily identify which service is slow so that you can fix it. A microservices architecture is not an excuse for having poor performance. We believe that it is actually the opposite; if you have very specific performance requirements, you can build services that will be responsible for reaching\n\n208\n\nChapter 12 ■ Single-Client appliCation—Mobile and Web\n\nthose numbers. We will go into detail about monitoring in the next chapter, so you can find out which dependencies are slow or slower, and how to get alerted.\n\nA very efficient technique for speeding up calls to distributed services is to make parallel service calls whenever possible. For example, we know that on the book page, we'd need to make calls to load the book details, the list of categories, and the shopping cart; we could make those three service calls at the same time, and our application's overall response time would benefit as a whole. Some programming languages make it easy—or hard—to work with threads, so your programming environment of choice may make this trivial—or not.\n\nSummary In this chapter, we dove into our sample project; that is, a bookstore with a client-facing web application that makes requests to a set of four services in order to display book information, and has a simple but working shopping cart. We went into detail showing code in multiple layers to give you a sense of how all the pieces fit together, and left a set of possible enhancements as exercises for the more adventurous. We hope we gave you some extra reasons to choose a standard for your APIs, as we've demonstrated a few nifty features included in the JSON API specification.\n\nIn the next chapter, we will describe the what and how of service monitoring, to help you figure out\n\nwhen your services are misbehaving.\n\n209\n\nCHAPTER 13\n\nMonitoring Your Services\n\nAs you’ve seen throughout this book, embracing a microservices architecture will mean that your production environment has a large number of deployable units, which you will be ultimately responsible for making sure are functioning as expected, based on your requirements. As with any type of software, you will want to know when issues are happening, ideally before they start to affect your company’s business metrics, such as user visits, signups, or sales. In some cases, it could take hours—or days—to find out that one small piece of software is malfunctioning, and you don’t want to discover production issues after your users do.\n\nIt is extremely important that you set up a well-tuned monitoring toolset to alarm you and your team\n\nwhen services are misbehaving. You should spend a significant amount of time thinking about failures modes, and make it easy to integrate new services into your monitoring and alarming system.\n\nMonitor All Services We start this section with a rule: “Every single service in your environment needs to be monitored; no exceptions. Yes, even that one service that was built months ago, and only get 10 requests per day.”\n\nThe only things we can monitor are the ones we measure. In general, this is an area where you should err on the side of collecting more metrics than you are actually using; it’s much better than the alternative.\n\nSome of the tools we have recently used for monitoring are these:\n\n\n\nStatsD: A network daemon that listens and collects statistics and sends aggregates to many pluggable back-end services. It allows for very fast metric collection in a simple way. You can tell StatsD to count or time any metric you’re interested in, and it will save that data for you.\n\nGraphite: Graphite is a visualization tool for statistics configured from various sources. You can create customizable reports and dashboards, and it is one of the preferred front-ends for StatsD. Graphite is where you go to see and analyze—hopefully pretty—graphs, based on the data you push to it. Our former colleague Matt Aimonetti has a very thorough blog post about the subject, titled “Practical Guide to StatsD/Graphite Monitoring” (https://matt.aimonetti.net/ posts/2013/06/26/practical-guide-to-graphite-monitoring/).\n\nNew Relic APM: Application monitoring in one place allows you to see your application performance trends at a glance. You need an application-monitoring platform that is closely tied to your programming environment and can help with finding the root cause of slowdowns/outages. Among many things, tools like New Relic APM help pinpoint parts of the code that are slow, database queries that are not optimized, and endpoints that are returning errors.\n\n211\n\nChapter 13 ■ Monitoring Your ServiCeS\n\n\n\nPagerDuty: PagerDuty’s incident resolution platform sits between your monitoring tools and your on-call responders to make sure your incidents are resolved quickly and efficiently. PagerDuty is the tool that notifies individuals/teams when an error condition has arisen; it allows teams to setup rotations, and notifies its users via a large set of media, such as email, SMS, or phone calls.\n\nSome monitoring tools have a narrow responsibility, and only help with collecting metrics; other tools are more comprehensive, and can also be used for logging and/or alarming, which we will cover later in this chapter.\n\nWhat Do You Need to Monitor? We want services that are available, aka “up and running.” The first thing you should make sure is that your services are indeed reachable from their clients, be they inside or outside your network. A simple way to monitor that a service is available is to hit a health-check endpoint, which is usually responsible for hitting whatever data store it uses and replying with a simple response.\n\nWe want services that are efficient and respond to requests in a correct and timely fashion. In this case,\n\n“correct” means that service responses look like what you have promised to the outside world in all cases. We touched on how to validate that service responses do abide by their contract in Chapter 9. It’s hard to define the exact meaning of “timely fashion,” but it’s imperative that it’s something that will not hurt your end-user experience.\n\nIn a 2006 talk at Stanford University, Greg Linden, from Amazon, said that “every 100 ms delay costs 1% of sales” on the eCommerce juggernaut’s site,1 which can also be read as “you sell more by being fast.” In reality, end-users want an experience that is fast, and a UI that is snappy; it’s known that site speed even affects how Google ranks search results (https://googlewebmastercentral.blogspot.com/2010/04/ using-site-speed-in-web-search-ranking.html), so being performance-oriented in a good thing. As your company grows, we recommend having a small team that specializes in fixing performance issues, so that you end up with fast services and have a set of performance-oriented principles specific to your environment.\n\nFocus on Perceived Performance We recommend special attention to perceived performance in your monitoring. Perceived performance is a measure of how quick a user thinks an action is. The usual example of perceived performance is that a user can think that a web site is fast because of how it renders a page, or how soon he can interact with the site; the real important metric is user perception. Another type of perceived performance metric is how fast clients of your service think you are; a classic example is of one client that claims that a service is responding in 150msec, while the service maintainers claims that the exact request renders a response in 20msec. it’s possible that both are right! By monitoring perceived performance, you can see such discrepancies in performance numbers that can lead you to investigate and find that, for example:\n\n\n\nThere is too much latency in the network call between clients and servers.\n\nOr communication is happening using SSL connections inside a private subnet.\n\nOr the client library used for accessing the service has a performance bottleneck caused by an inefficient JSON/XML parser.\n\n1“Make Data Useful,” Greg Linden, November 29, 2006; Presentation at Stanford University; https://docs.google. com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxnbGluZGVufGd4OjVmZDIzMWMzMGI4MDc3OWM\n\n212\n\nChapter 13 ■ Monitoring Your ServiCeS\n\nThe bottom line is that being able to see that perceived performance is slower than expected will lead you to spend time investigating the reason for such disparity. However, don’t go crazy and make one single data service endpoint extremely fast; it could be that saving 20 msec of response time—which can be an impressive and fun accomplishment—isn’t helping your overall perceived performance if your web site that renders that data is extremely slow because of other web assets, such as JavaScript and CSS So you should know what battles are worth getting into.\n\nAdhere to SLAs Define service-level agreements (SLAs) for every single service endpoint in your environment.\n\nA service-level agreement (SLA) is a part of a standardized service contract where a service is formally defined. Particular aspects of the service – scope, quality, responsibilities – are agreed between the service provider and the service user. 2\n\nIn practical terms, an SLA for a specific endpoint will list some of the following service expectations:\n\n\n\nThroughput for the endpoint\n\nNumber of concurrent clients\n\n\n\nAverage response time\n\n\n\n90th percentile response time\n\n\n\n95th percentile response time\n\nUptime, availability, and any other nonfunctional requirements\n\nAfter you have SLAs in places for all your services, you can start to leverage that data in many ways:\n\n\n\nIf you’re building a new service that will have four downstream dependencies you need to call sequentially, and each of those dependencies has an average response time of 100msec, then it’s unlikely that the new service can deliver requests averaging less than 400msec of response time; you can derive SLAs based on your dependencies.\n\n\n\nIf you’re designing a service that needs to deliver response times much faster than your SLAs currently guarantee, you know that you may need to introduce new components to your infrastructure to hit the performance numbers you are aiming for.\n\nOne advantage of having SLAs is that they will help you set up your alarming because you will have data points for what is acceptable and what isn’t. More importantly, they help to set expectations for the clients of a service, as well as for the service owners’ level of support.\n\nBusiness Metrics Are Important There is a tendency among software professionals to focus on technical metrics, which is not a bad thing; but every organization needs to keep an eye on important business metrics, as it allows the organization to also find out about issues that went otherwise unnoticed.\n\n2“Service-level agreement,” Wikipedia, referenced on August 27, 2016. https://en.wikipedia.org/wiki/ Service-level_agreement\n\n213\n\nChapter 13 ■ Monitoring Your ServiCeS\n\nAn example of an error condition that can happen in any web application is having a login page that\n\nno longer works because of a JavaScript issue, or even a missing Login button. From a systems perspective, all involved systems—web application, authentication and authorization service—could be working as expected, and are looking functional in all their own monitoring graphs; however, there’s a clear issue happening. If you have a business metric called “sign-ins,” you can set that up to alarm after there are no sign-ins for a specified time, alerting you that something is not working as expected even when all services are working properly.\n\nAN IMPORTANT BUSINESS METRIC\n\nthe most famous business metric at amazon is the order rate of the retail site. amazon has good enough tracking about its order rate—orders per second—that it predicts for any point in time that the order rate has to be within a calculated threshold; it will alarm—and wake up—developers in many teams when the order rate drops below the threshold. Whenever that happens, it is expected that specific teams have had their own alarms go off to help pinpoint where in the order processing chain a failure may be happening.\n\nexamples of failures that would make the order rate drop significantly could be issues in the services that authenticate users or in the payment processing service. While the order rate alarms will certainly fire in these situations, it should certainly be the responsibility of the teams owning the primary (authentication or payments processing) services to be alerted of issues in their systems long before sales on the site are affected.\n\nIdentifying Requests Your toolset has to make sure that a globally unique ID identifies each request so that you can track requests as they travel in your infrastructure; we recommend you always use UUIDs. We’ll call those “request IDs” for now; they need to have the following characteristics:\n\nUnique across all applications and services\n\n\n\nEasy to create, forward, and log\n\n\n\nEasy to trace\n\nSome modern web application frameworks already generate unique request IDs per end-user request;\n\nthus, you have to make sure that, once generated, the request ID is forwarded to all underlying services so that one of them can load all the internal requests generated by a specific request. Having such a tool will help you pinpoint issues when nobody is sure what system is failing; you should be able to trace a request through all your system easily. You also need to make sure you can attach that request ID to requests you make to external APIs, so that those are also traceable during debugging.\n\nFor example, a request that goes from our bookstore sample project for order processing will pass through the web application, the books service, and then to purchase/payment services, and will likely be forwarded to one external API for payment processing; there are quite a few players involved in making sure that the purchase experience goes smoothly, and the last thing you want to do is to make guesses when orders are not functional.\n\n214\n\nChapter 13 ■ Monitoring Your ServiCeS\n\nLogging All Requests Having a unique and traceable request ID is only the first half of being able to investigate issues in a distributed environment. You also need to make sure every single request in your environment is saved to a log file, and that each one of those log files is easily accessible. Using the right tools and defining a common log format can help with this effort.\n\nLogging Tools\n\nAs you set up your infrastructure, it’s probably easy to look at raw log files using simple command-line tools like grep; but, as your infrastructure grows, you should use a more sophisticated tool for log inspection. Here are some tools you can use to search your log files:\n\n\n\nSplunk: Splunk captures, indexes, and correlates data (usually logs) in a searchable repository from which it can generate graphs, reports, alerts, dashboards and visualizations.\n\n\n\nLogstash: A modern tool for managing events and logs, with an ElasticSearch back- end which allows a high degree of searching and filtering.\n\nKibana: A modern, beautiful visualization tool that works seamlessly with Logstash and ElasticSearch. It features a no-code setup, real-time analytics, and much more.\n\nAmazon CloudWatch Logs: CloudWatch Logs can be used to monitor your logs for specific phrases, values, or patterns. For example, you could set an alarm on the number of errors that occur in your system logs or view graphs of web request latencies from your application logs.\n\nWe’ve discussed in previous chapters that in a distributed environment, it’s crucial to write code that\n\nproperly deals with downstream dependencies being unavailable or timing out. It’s important that code handling error conditions writes to logs for every single error encountered, whether a timeout or a service error. Once you have log entries that indicate that a service is down, you can use log analysis tools to alarm based on log entries; many tools on the market, including the ones listed above, support that.\n\nDefining a Uniform Log Format\n\nThe data you include in your log files should follow a uniform format to allow easy parsing by your tools. You should also have standards for what you decide to log; ideally, each request should have at least two entries in your log format, the first when a request reaches a service, and another after a request is being processed. You can also log more data, especially when this extra data may become valuable information for debugging purposes.\n\nTo illustrate this further, let’s look at a few log entries that were generated by a single request to our\n\nproduct catalog service:\n\nI, [2015-12-28T01:31:14.322076 #24041] INFO -- : Started request_method=\"GET\" request_url=\"/offers/deal-1542454.json\" request_time=\"2015-12-28 01:31:14 +0000\" request_ip=\"172.17.4.48\" HTTP_CLIENT_IP=\"172.17.4.93\" HTTP_X_FORWARDED_FOR=\"172.17.1.102, 172.17.4.93\" HTTP_USER_AGENT=\"Ruby/ls-catalog/v6.2.1\" HTTP_X_LIVINGSOCIAL_API_CLIENT_ NAME=\"mobile_production\" HTTP_IMPRINTID=\"e9f8bb4a-41e0-4d87-982b-78f1f3eb3668\" HTTP_TRUE_CLIENT_IP=\"172.17.4.93\" HTTP_X_REQUEST_START=\"t=1451266274269000\" representation=\"deal_large\"process_pid=24041 request_id=e9f8bb4a-41e0-4d87-982b- 78f1f3eb3668\n\n215\n\nChapter 13 ■ Monitoring Your ServiCeS\n\nI, [2015-12-28T01:31:14.325896 #24041] INFO -- : method=GET path=/offers/deal-1542454.json format=json controller=offers action=show status=200 duration=5.39 view=0.13 db=0.64 time=\"2015-12-28 01:31:14 +0000\" client_name=mobile_production process_pid=24041 request_id=e9f8bb4a-41e0-4d87-982b-78f1f3eb3668 I, [2015-12-28T01:31:14.326133 #24041] INFO -- : debug_value=\"something important\" process_pid=24041 request_id=e9f8bb4a-41e0-4d87-982b-78f1f3eb3668\n\nFirst, note that all three log entries here share the same process ID (24041), which indicates that they were produced by the same application service process. Next, you can see that the request_id UUID values also match on all entries, which denotes that all three log messages were produced during the processing of the same client request cycle. All three messages also carry increasing microsecond-based timestamps (2015-12-28T01:31:14.322076, and so on), which proves that they were processed in the order they are listed in the log file.\n\nThe main difference between the three messages is the event that generated them: the first message logs the arrival of a request (alongside many interesting properties of the requesting client), and the second message provides info at the end of the request cycle (including response codes and processing time), while the final message simply logs additional debugging information.\n\nNote that all information is given in a key=value format, which makes automated post-processing of\n\nthis information with log analysis tools much simpler.\n\nReading Metrics Now that you are measuring as many metrics as possible, you have to be aware of how you view and interpret those numbers. We’ve all read our share of dubious statistics-based articles in our social network feed; I’m sorry to say that the metrics we collect do sometimes tell a similar story.\n\nAverages Lie\n\nA common mistake that is made is to look at—and rely on—average response times for monitoring overall service performance. Average response time is a “convenience metric” that gauges overall response time as if it were very linear. Averages can convey a wrong one-dimensional number that doesn’t really take into account outliers, and they only paint an accurate picture of overall system health if your response times follow a normal distribution, which is rarely the case with services.\n\nAVERAGES HIDE OUTLIERS\n\nthe main user-facing web application at LivingSocial receives up to 20K requests per minute during peak time, showing average response time for all requests and endpoints usually below 200msec, which seems to be decent performance for most cases. however, when you look at some endpoints, it’s easy to find that endpoints with low usage average terrible response times, sometimes above 2 seconds, just at the web server layer. it’s almost impossible for a terribly implemented endpoint to make a dent in overall average response time in such an environment. Some monitoring tools default to showing overall response time, which doesn’t really tie into any single change that gets deployed, making it really easy to hide endpoints with suboptimal performance.\n\n216",
      "page_number": 222
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 230-237)",
      "start_page": 230,
      "end_page": 237,
      "detection_method": "topic_boundary",
      "content": "Chapter 13 ■ Monitoring Your ServiCeS\n\nAn Endpoint Tells Many Stories\n\nAnother mistake we have seen often is to judge an endpoint’s performance by averaging all requests to that endpoint in one single metric. This particularly applies to endpoints that perform some sort of search, and possibly return a variable number of records, based on an API input. It’s possible that one caller of an endpoint usually requests 100 records to be returned; so, if you have endpoints whose performance can vary greatly based on caller parameters, it may make sense to have metrics based on those parameters.\n\nIn general, developers should default to looking at data for the 95th or 99th percentile, which will give\n\nyou numbers for the worst 5% or 1% of your responses, which is where performance is at its worst and where gains can be made most consistently.\n\nA major advantage of looking at the 95th or 99th percentile is that you will be able to recognize\n\npotential performance problems before they happen system-wide. If you have an issue with a downstream dependency that is causing your average response time to degrade, then it’s usually too late to prevent some sort of service degradation or even an outage; however, if you start getting alerted as soon as a small percentage of requests goes above your SLA threshold, then you’re more likely to solve that issue before it causes serious damage.\n\nLook into Your Slowest Responses\n\nEven when services are functioning well, and have response times you are proud of, you can still learn about how your services work, and find potential performance issues by paying attention to your slowest responses. A very slow response can showcase a lot of potential issues:\n\n\n\nPossible database contention at specific times of the day may cause a slow response due to a slower than usual query.\n\n\n\nExcessive object allocations may make garbage collection kick in at unexpected times.\n\n\n\nThere may be new data access or creation patterns that your service was not designed for (for example, because of a change in business strategy that exposes the service to clients with very different data access patterns).\n\n\n\nThere may be a slow response from a downstream dependency, which could itself need to be fixed. Remember that a service response can’t possibly be faster than its dependencies.\n\nSometimes you can see that a specific use case of your service can’t possibly be efficient within the\n\nconstraints of your current design; it may require extra joins or do a lot of in-memory calculation, among other things. A really slow service response could be a “design smell”; it may show that a specific use case would be better served by a new endpoint, or even a dedicated service altogether.\n\nFIXING PERFORMANCE WITH SLOW REQUESTS\n\nin a well-performing service we run at LivingSocial, we were seeing occasional responses taking above 1 second to respond. that service had one client—out of 20+—that needed a response with hundreds and sometimes thousands of objects in its body. We knew that those requests were supposed to be slow most of the time, and never questioned the slowness. the requests happened in a cron job, so they impacted nobody; but they kept appearing in our “slowest responses” report, so we decided to take a quick look into them one day. to our surprise, 80% of the processing time for those responses was happening building the JSon string to be returned as a response, which was a bit of a surprise\n\n217\n\nChapter 13 ■ Monitoring Your ServiCeS\n\nbecause we were expecting to see most of the time spent in the database. that service used a series of JSon templates based on a templating language we had assumed was fast enough for our use cases; however, that high number made us look into its performance in more detail. after some experimentation, we decided to get rid of that templating language and used a much simpler solution that saved us 15–30% in performance in all requests, even the ones that were already fast enough.\n\nAdjusting Alarms At this point, you have started to collect metrics about your services and hosts, and have defined a set of SLAs that you need to deliver on. It’s time you start to send alarms to your team when something goes wrong. Most modern alarming tools allow their users to pick their preferred method of paging, whether it’s email, SMS, or phone call, so make sure you use a flexible tool. You can issue alarms under many circumstances:\n\nWhenever an error happens (any response with a 500 status)\n\n\n\nResponse time exceeds the SLAs, or even better, response time is getting close to exceeding your SLAs\n\nWhile still meeting its SLAs, a service sees a significant change in response time (for example, more than 1 standard deviation over the past 5 minutes)\n\nOne of the business metrics is beyond its acceptable threshold\n\nAfter you start sending out alarms, someone will have to wake up and fix slow or broken services or\n\nhosts. In a well-run microservices architecture, an alarm will notify a developer—or ops person—who will log into their terminal and start to look into the reported incident. Being notified of a live issue is stressful enough that you want to make sure that:\n\n\n\nYou alarm for a good reason. If you alarm—and wake people up—too often, you will create an army of co-workers who will learn to ignore those alarms.\n\n\n\nThresholds are easily configurable. If you just alarmed someone for a questionable reason, make sure the alarm can be tweaked so that it’s only happening when attention is really required. A team should define threshold for alarming based on the SLAs of the services they support, and tweak those SLAs according to how their service is performing.\n\n\n\nAlarms should help with resolution. As a company grows, the number of services grows with it, and you reach a point where not all developers know all services really well. Add a link to an operations manual that is kept up to date, as part of the alarm text, and to an operations dashboard that helps with investigating root cause of issues, and you will make everyone’s life much easier. An efficient operations manual should include details about the failing service, links to dashboard/graphs that may help with solving the issue, and description of the possible cases that can lead to failure, along with the preferred solution.\n\nSetting up alarming in a distributed environment is an art, and it will take many tries and constant\n\ntweaks to reach a point where a team is confident that a system is always up and performing as expected.\n\n218\n\nChapter 13 ■ Monitoring Your ServiCeS\n\nSummary In this chapter we first looked at tools and guidelines to monitor the important parts of your microservices infrastructure. We clarified that the useful monitoring focuses on performance as it is perceived by the service client, supporting the adherence to published SLAs and measurement of business metrics.\n\nNext, we examined analysis tools and important properties of service logs, explaining the importance of\n\nunified, machine-readable formatting to help monitoring and log-based alarming.\n\nWe gave tips about correctly interpreting the metrics, explaining the problems with relying on averages\n\nand why it is wise to start any investigations by looking at the slowest service responses.\n\nFinally, we laid out guidelines for adjusting the sensitivity for alarms generated from monitoring\n\ninformation.\n\nThe next and final chapter examines tools and processes to help with achieving excellence in\n\nsupporting and maintaining your services through their lifespan.\n\n219\n\nCHAPTER 14\n\nOperational Excellence with Microservices\n\nSuccessfully developed and launched software spends the vast majority of its lifecycle in a state where it needs maintenance and support. In an environment where the number of services can easily outnumber the number of developers in your engineering organization, it is business-critical that operational support runs efficiently and effectively, without impacting the productivity and retention of your engineering staff.\n\nThis chapter will look at the processes and some of the available tools to help with achieving excellence\n\nin supporting your services. We will intersperse the discussion and recommendations with examples to underline the lessons learned from our experience of being on-call for many years.\n\nWhat is Operational Support? Let’s first define what we understand by operational support.\n\nTo us these are all activities that are aimed at supporting the functioning of an already launched service\n\nor feature according to the expectations—explicitly and implicitly stated—of the clients.\n\nImplicitly stated expectations are requirements that are common sense, but were not explicitly stated\n\nin any specifications of the service or feature. If, for example, a newly introduced API functions as designed, but it causes the service to degrade in stability, responsiveness, or correctness of other APIs or features, or it results in scaling issues under increased load, then these issues should be accounted for as operational support.\n\nThe activities included in operational support work can be rather wide-ranging. They include not just root cause analysis for, and fixing of, obvious bugs and errors reported by automated monitoring or client reports, but also general maintenance work related to library or service framework upgrades, adding missing documentation, proactive searching for potential security bugs or edge cases that have not been accounted for, and answering technical questions by the client team.\n\nEmbracing Ownership One of the major advantages that many developers (ourselves included) report getting out of working in a well-functioning microservices environment is that they feel empowered by the level of autonomy given to them. The “you build it, you run it” approach leads to encouraging the team to feel like true owners of their domain, and hence also feel more like they own an actual part in the business as a whole. And along with the feeling of true ownership comes a feeling of responsibility that needs embracing; your new-found autonomy means that the buck stops with you, as well.\n\n221\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nCustomers First The first rule of business you should embrace is that if you don’t add value for your customers, then your business is doomed to failure. The best teams have it engrained in their culture to settle situations of disagreement and conflict asking themselves this question:\n\nWhat is the best thing to do for our customers?\n\nIf you have this mindset, then not only will decisions be easier to make, but you will also elevate the\n\ndiscussion of disagreements to a level where the egos of the people on your team are easier to extract from making business-savvy decisions. The focus will be on designing services for usability and solving real problems and thereby adding maximum business value.\n\nIs the Customer Always Right?\n\nNote that we intentionally did not phrase the question as “What does the customers say they want?” The question we did ask, about what is best for the customer, means that you should make sure you understand what the customer is trying to achieve, balancing the mission and architectural constraints of the services you own against the requests.\n\nAs an example, if a client team asked the books-service team to vend all books sold in the US, sorted\n\nby revenue in the shop, then the owners of the books-service should figure out whether such an API can be reasonably exposed in an efficient manner. At the very least, the service team should try to understand the constraints in which they can operate. Will the API really need to vend all books in the US? What would the client do with this data? Can the information be paged? Is there room for staleness in the revenue numbers by which to sort, or in the list of books? What is the anticipated call volume?\n\nOnly then can the service team make educated decisions about how best to serve the needs of this\n\nparticular client, and also implement such an API without impacting other clients and overall system stability and responsiveness.\n\nOperational Support Benefits Customer Trust\n\nMany companies spent a lot of money on acquiring new customers. But how much budget does your business dedicate to keeping the customers you have happy and engaged?\n\nA big factor in repeat business from your clients is how well you manage to establish and maintain their\n\ntrust. But customer trust is hard to earn, and it is easily lost when you disappoint the clients’ expectations. That is exactly where operational support comes in: it is all about making sure that your service maintains client expectations. Therefore, investing in operational support is an investment in customer trust.\n\nMaking sure all systems are operational is not just contributing in the obvious and direct way, for example, by enabling your customers to make purchases on your site, or to sign up for your services. It is also an investment in keeping current customers’ trust that the site will be up, running, and behaving as they expect when they visit it.\n\nIt is much harder to regain a customer’s trust after it was lost due to a bad experience, and the money\n\nand time you’ve spent on acquiring the client in the first place is wasted. When a judgment call needs to be made between allocating developers to implementing new features and fixing already launched functionality, then a monetary value needs to be attached to the estimated loss of customer trust incurred per day of broken functionality, and it needs to be factored into the prioritization process.\n\nFor that reason, in the majority of successful businesses we have worked for, addressing operational issues always outranked new projects or feature work. When an outage arises and is noticed in a service that has launched, then this underlying issue should best be seen as a use case that was missed during development. It is just seen as though the launched feature is not entirely finished and hence needs fixing first before any new functionality is released.\n\n222\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nNo Code without an Owning Team There should be absolutely no line of code (or whatever unit of software is run in the infrastructure) for which it is unclear who owns the maintenance and support issues.\n\nAnother very important point in this context is that software ownership is never assigned to a single person. Every service is owned by a team of people, and they share all ownership privileges and responsibilities. That does not mean that you cannot have subject matter experts in your team; in fact, you probably want to encourage developers to dive as deeply as they would like into the services they own.\n\nIt just means that shared ownership reduces the risk for the team, and the company as a whole, that all\n\nexpert knowledge about a service in your system would depart with the single owner. Sharing ownership and knowledge in a team also sets up the team to support several services via an on-call rotation, the details of which we will discuss later in this chapter.\n\nAny team can of course own several services, and in a microservices environment where the number\n\nof deployment units is often a multiple of the number of team members this is in fact the only practical approach.1 Therefore make sure that the team knows and documents exactly at each point in time which services, applications, libraries, and/or third-party software packages they own and maintain.\n\nKeeping such complete and up-to-date mappings of software-to-team can be implemented and\n\nenforced in different ways. Your code control system could make sure that code repositories need to be owned by an existing team, and your deployment infrastructure scripts could check that an ownership field is present and validate its value with an up-to-date list of teams. To advertise the team-to-software details, you could introduce a simple, well-known engineering-wide documentation page, or a web UI on top of an ownership database, or even an actual service discovery registry in your microservices infrastructure.\n\nThe most important part is that maintaining such a mapping is universally accepted as part of your\n\nengineering team culture. Such acceptance and visibility needs to go so far that plans for engineering- internal team reorganizations always include a line item that makes sure that, after the reorganization, all software systems have a new owning team. If reorganizations do not include reassigning the services owned by disbanded teams, then you will end up with unowned but live systems, which therefore will become unsupported and hence are an accident waiting to happen.\n\nFinally, avoid set-ups where some software is tagged to be owned by multiple teams. We have never seen such cross-team ownership arrangements work out well, as in practice such shared responsibility leads to confusion and often ultimately leads to no team feeling responsible for services that are officially of shared ownership.\n\nBoosting Team Cohesion Working on operational support for the services they own as a team is essential for embracing true ownership as part of the team’s culture. In a “you build it, you run it” environment, the team soon realizes that the systems they create spend the vast majority of their lifecycle in production and needing operational support; implementing and launching services is only a minute episode in the existence of a successful software application.\n\nOrganizing all the team’s activities and practices around making the software it owns easy and efficient\n\nto maintain becomes an objective that is in the team’s self-interest. This way, the team’s interests are well aligned with the clients’ interests, and—by extension—those of the entire business.\n\n1While it is very hard to give meaningful guidance about optimal team sizes or the best number of services per team developer, in our experience the sweet spot most successful microservices companies choose is between 5 and 8 developers per team. If automation is advanced in an organization, then the ratio of microservices per team member (or developer) does not have a clear upper bound. Running multiple services per developer is definitely possible if development and deployment processes are streamlined.\n\n223\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nAnother team-internal practice that helps team cohesion and the happiness of engaged and curious\n\ndevelopers is to rotate team members through the feature development projects for all the software owned by the team. As mentioned above, there surely are advantages of having subject matter experts for each service, but allowing them to work on other services owned by the team can be a cost-effective risk reduction strategy.\n\nThe team will be able to shake the dangerous reliance on individual “heroes,” thereby mitigating the risk\n\nof time-intensive research and coming up-to-speed of other team members when the hero leaves the team. In short: the real heroes share their special powers to enable others to rely less and less on them.\n\nWhen team members start working on services they previously did not engage with, there is great potential for cross-pollination: they see how things are done in a different code base, and so might recognize patterns that can be applied to the service in which they usually work. Conversely, having a new set of eyes on an existing service might contribute ideas about how to handle tricky problems or inelegant solutions in an improved way.\n\nAn advantage of rotating team members during feature development that applies directly to the team’s operational support efforts is that all team members become more familiar with the entire set of services the team owns. As a consequence, they usually also become more comfortable with being on-call for the full set of systems owned.\n\nOur final advice regarding ownership and team cohesion is to avoid placing developers in a position\n\nwhere they straddle multiple teams at the same time.\n\nThis does not mean that we advise against loaning team members out to other teams; loaning is especially useful when a project in the developer’s home team is blocked by missing functionalities in a service dependency that is owned by a team is not staffed to dedicate time to implementing such feature.\n\nIn our experience it is simply not advisable to expect the developer who is on loan to another team in such situations to still fulfill responsibilities on the home team. When someone is on loan, she will be truly and fully embedded in the team to which she is on loan. This way, the loaned-out developer has enough time to focus on the task and give the members of her temporary team time to gain confidence that the code contributed by the embedded person can be maintained and supported by the receiving team.\n\nWhat It Means to Be On-Call In order for a team to implement operational support effectively and efficiently for the systems it owns, a (set of) developer team member(s) need(s) to be nominated and act as the point person(s) for addressing any and all support issues.\n\nThis role is usually referred to as being on call, and this section will explain the responsibilities and privileges of the person filling it. We will lay out an etiquette and code of conduct to be followed by the on-call person herself, as well as by the rest of the team and the organization in which they work.\n\nWhile many of the points mentioned in the next section are applicable regardless of the architecture that is chosen for the overall business application, we will conclude this section by touching on some points that are particularly important to on-calls supporting microservices architectures.\n\nDefining the Role and Process for Being On-Call The most succinct, to-the-point definition of the role of an on-call developer comes from a presentation our former colleague Jon Daniel delivered at the 2015 DevOpsDays in Pittsburgh:2\n\nThe on-call’s role is to react to failure and prevent catastrophe.\n\n2Jon Daniel, \"Ethical and Sustainable On-Call,” August 13 2015, DevOpsDays Pittsburgh; An overview of Jon's inspirational talk, including slides and a link to a video recording of the talk, can be found on his personal blog at: https://chronicbuildfailure.co/ethical-and-sustainable-on-call-c0075e03a7b\n\n224\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nThis accurately summarizes the priorities of the person in charge of supporting a team’s production- deployed systems. First and foremost, the on-call person needs to make sure that new and current failure situations are addressed as they are ongoing, all with the overarching goal of preventing even greater impact through a potential total outage.\n\nIssue Receipt and Triage\n\nThe ways production issues are reported or noticed can differ greatly. Some are manually generated bug reports from colleagues. Others are created automatically, via integration of your monitoring and alarming systems with your ticketing system. Where such integration is lacking,3 the on-call will have to manually create issues for tracking when aberrations become obvious from monitoring or alarming. Sometimes issues that are reported to the on-call are mere hunches about potentially faulty behavior, or requests for an in-depth analysis of the expected behavior under a given set of circumstances regarding one of the systems owned by the team.\n\nIn all such cases, the first step for an on-call is to make sure a support ticket4 for the issue exists; and if that is not the case, to create a ticket herself. It is essential that the team has a template for trouble tickets, with fields for minimally required information, such as ”expected behavior,” “observed behavior,” ”steps to reproduce the issue,” and so on. This helps to cut down on avoidable back-and-forth communication attempting to clarify information the on-call needs to track down the issue, thereby leading to faster resolution of the problem.\n\nOnce a ticket is created, it needs to be triaged based on the severity of its impact, and prioritized in relation to other unresolved production issues. For that it is important that the team think about generalized rules for severity and priority assessment. We will discuss an example for such rules in the subsection \"Defining and Advertising Team Policy\" later in this chapter.\n\nNote that in organizations where development happens against services that are deployed to a shared\n\nsandbox fabric, it is usually the policy that the team’s on-call is also responsible for the proper functioning of the sandbox instance of the team’s services. While it is of course a serious impediment for the organization if the development efforts for teams that depend on the affected sandbox instances are impeded, the on-call generally applies a lower priority to such issues than is applied to problems surfacing in production.\n\nDeciding What Work to Pick Up\n\nOnce the issues are properly triaged, the on-call will always pick the top priority-item on the list to work on and resolve. If the on-call person ever gets into a situation where all trouble tickets are resolved, then most teams all have a backlog of items that serve operational improvements. Examples of such operational improvement backlogs are tasks to address missing documentation for some of the systems owned by the team, or to add additional metrics to the team’s monitoring dashboard, or to fine-tune alarming rules. It is often also very useful to update the Standard Operating Procedure (SOP) documentation for the team’s services when new approaches to address recurring issues are discovered during the on-call’s current duty. Often, the on-call will also be tasked with writing or following up on post-mortems for outages in which her team has stakes or was directly involved5.\n\nIf the on-call still has spare cycles left to invest, they can often best be used to try to anticipate errors,\n\nhunt for issues that could arise in edge cases, delete unused code, or generally “kick the tires” of the application in use.\n\n3Automatic ticket creation is hard to get right, as often this leads to many duplicated or similar trouble tickets that are rooted in the same underlying issue. An on-call person will then spend a lot of time on de-duplication and other unnecessary ticket management tasks. 4We will discuss properties of useful trouble ticketing system later in this chapter. 5We will talk in more detail in later subsections about dashboards, SOPs, and post-mortems.\n\n225",
      "page_number": 230
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 238-245)",
      "start_page": 238,
      "end_page": 245,
      "detection_method": "topic_boundary",
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nWhatever the on-call person chooses to spend her time with, she should decidedly try to steer clear\n\nof signing up for feature development or necessary technical improvement work that is tracked in the team’s sprint. In our experience, counting on the on-call to contribute to the team’s project work will lead to suffering: the team cannot rely on the on-call person’s availability for working on features, so they might over-promise features to representatives of the business-focused departments, which will lose trust in the team’s ability to deliver on their promises. At the same time, the on-call person—feeling pressured to deliver features—runs the danger of dropping the ball on critical operational issues.\n\nAs a corollary, this means that the number of members in an on-call rotation (usually, the team size) should be large enough to support a dedicated on-call person, which in our experience means that it should include around five to eight developers.\n\nOn-Call People Increase Team Efficiency\n\nIf this approach of dedicating a person to operational issues meets resistance in your organization, then the arguments raised usually amount to not being able to afford a dedicated on-call person, given other business pressures. Our advice in such a situation is to raise awareness of the costs of neglecting operational emergencies. It is not a question if outages will happen, but only when they will hit you, and if you can afford to not be prepared and—as a consequence—lose business or customer trust.\n\nTry to compare the decision to save on operational support to an “unhedged call option” for stocks, rather than using the usual metaphor of “technical debt”6: unhedged calls can cost an unpredictable amount of money should they be exercised, whereas debt is predictable and can be managed.\n\nAnother part of the on-call role is to defend the rest of the team from interruptions. By being dedicated\n\nto all interrupt-driven issues and distractions, the on-call person shields the rest of the team from being disrupted, thereby guarding their focus.\n\nVery conclusive arguments can be made that dedicating a single person to working on all the unpredictable, highly disruptive and stressful tasks increase the team’s overall productivity. Again, disruptions are a fact of life with technology, and even more so in an environment of distributed systems. Acknowledging this reality and preparing for the inevitable outage is a smart decision for every business to make.\n\nThe acting on-call person will act as the single point of contact for all operational and technical inquiry\n\nissues. Any other developer on the team should only be contacted and interrupted by the on-call person, and only if she deems it necessary (in cases where an urgent technical issues requires the knowledge or advice of another team member).\n\nA Responsibility to Rest\n\nIn order to be successful in the role of acting as the team’s firewall, it is the responsibility for the on-call person to be alert and focused while dealing with operational problems. In most around-the-clock businesses, on-call people will need to be able to tend to problems as they arise, any time of the day or night. For that reason, they usually carry a pager or similar device, on which they can be made aware of urgent issues 24 hours a day.\n\nIn order for the on-call person to stay awake at the wheel, she cannot be allowed to be sleep-deprived\n\nand worn out for longer than is required to take care of high-severity operational problems. As a result, it should be expected that the on-call person is entirely autonomous regarding her time management. In short, if the on-call person feels worn out and tired after a night of dealing with outages, then they should\n\n6See Steve Freeman's blog post \"Bad code isn't Technical Debt, it's an unhedged Call Option\" (July 23 2010) for a more detailed explanation of this argument; http://higherorderlogic.com/2010/07/bad-code-isnt-technical-debt- its-an-unhedged-call-option/\n\n226\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nfeel obligated to sleep and relax during times with less trouble. During such times, the on-call should notify the secondary on-call person (whose role and responsibilities we will explain a little later) that he should step in when any urgent issues need addressing.\n\nThe On-Call Person in Relation to the Team As laid out above, on-call duty play an important part in a development team that is also concerned with the operational aspects of the software they deliver. That is why investing thought into how to set up on-call duty for the team is valuable.\n\nWhile being on-call means that you get exposure to a broad range of operational aspects of all services owned by the team, it can also be very distracting and stressful. So, in order to prevent developer burnout, make sure to establish a rotational assignment for on-call duty within your team. We have seen weekly rotations work best in practice, as shorter rotations impact effectiveness on follow-through of ongoing issues, while longer rotations easily can lead to loss of focus and fatigue.\n\nIt is important for team morale to include all team members in the rotation schedule. Make sure to respect the schedule and raise the expectation that it is important to integrate into planning. For example, make sure that, if your time-off overlaps with your on-call rotation slot, you trade slots with another team member before leaving.\n\nNote that we also recommend including new team members and junior engineers into the rotation\n\nschedule as soon as possible. While it is natural that there are justified concerns from both the new or inexperienced team members and the more experienced ones that operational issues may not be addressed appropriately, new team members can always shadow the on-call person, or be a secondary on-call—a concept we explain more later. We have found this to be an excellent way to familiarize new team members quickly in a hands-on setting with the breadth of the systems that the team owns.\n\nAnother good tip we have is to make the on-call rotation overlap for an hour or so. This way, the\n\nprevious on-call has time to walk through any operational issues still in flight, and the next on-call person is sure to hit the ground running.\n\nWhile it is certainly true that the on-call people act as a protector of the team’s productivity by being a single point of contact for all interruptions, they should definitely not hesitate to escalate issues to team members when they feel stuck or in need of assistance. They need to be made comfortable to use their best judgement about when investing more time to understand the issue has diminishing returns given the severity of the issue and the expected contribution from another team member who is considered the expert in the area affected by an issue at hand. Operational issues are another realm where pig-headed attempts at heroism are unwarranted, as they might hurt the business. It is much better to admit defeat early and learn from pairing with the expert on resolving the issue.\n\nAsking team members for input is especially important during the triage phase. You want to make\n\nsure that the impact and severity assessment of an unfamiliar issue is correct. In teams with a dedicated Technical Project Manager (TPM), the on-call person can often direct questions about prioritization of work relating to operational issues to the TPM, in case there are competing high-severity issues to be addressed.\n\nWe have seen that it usually takes at most one full rotation of on-call duty through the team until the\n\nbenefits become very obvious for everyone involved. Especially the team members that are alleviated from distractions by the on-call person come to really appreciate the role she takes on. They soon start to develop the necessary empathy for the person currently on call, and they will even start to excuse grumpiness and irritability caused by constant interruptions and operating in fire-fighting mode. Everyone knows that they will be on-call sooner or later, as well, and that they can expect empathy, as well.\n\n227\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nADVANTAGES OF A SECONDARY ON-CALL\n\nsome teams set up on-call rotations with two developers, one acting as the primary on-call, and another the secondary on-call. while assigning two people to operational emergencies might seem uneconomical to many businesses at first, there are quite a few advantages to take into account when considering the cost-benefit ratio.\n\nFor one, the secondary on-call can often be used as a very effective on-boarding tool for new team members who might still need training. the services that their new team owns are still unfamiliar to them and researching and fixing bugs can often be a very practical and manageable way to get initial exposure.\n\nadditionally, when secondary and primary on-call decide to pair on resolving operational issues, then the benefits are not perceived only by the secondary; many times just having four eyes and two brains dig into an issue leads to faster resolution or at least validation of trains of thought or design decisions.\n\nthe biggest benefit of identifying a secondary on-call person is most likely that he can act as a \"human fail-over\" for the primary. if you can set up your alarming system to escalate to the secondary on-call any critical items not acknowledged by the primary within, let's say, 15 minutes, then your business has implemented better risk mitigation for situations in which the primary on-call is unavailable (for example, when the pager fails to catch the primary on-call's attention).\n\nas a practical tip, teams we were part of in the past have set up their on-call rotation schedules such that the secondary on-calls transition to being primary immediately following their secondary shift. this helps continuity and efficiency for situations in which addressing on-going operational issues span more than one on-call shift.\n\nwe have found having a secondary on-call person especially useful during a team's \"tuning phase,” during which not all services and applications have documented standard Operating procedures. secondary on-calls can use their time taking down, and later publishing, notes about recurring tasks that the primary on-call performs to address operational tasks in the services owned by their team.\n\nsecondary on-calls are also much more likely not to be fully occupied with operational work, so they can dedicate some of their time to feature development and other sprint goals. we still recommend officially not counting on contributions of secondary on-calls; under-committing and then over-delivering is still a better outcome than the alternative.\n\nBeing On-Call in a Microservices Environment While it does not diminish their validity or importance in a microservices setting, any of the previous points about operational support apply more generally than just to microservices. This subsection addresses the finer points that are more directly focused on microservices.\n\nBecause of the extremely distributed nature of a microservices application, it is especially necessary to identify and document the list of all direct service dependencies. Outages of each of these dependencies not only need to be mitigated in the code, but they also need to be tracked, and potentially alarmed on, by teams owning dependent services.\n\nFor that reason, it is very handy for the team’s on-call to have access to the on-call logs and ticket list for all services that the team’s systems depend upon. Additionally, try to set up direct communication channels between team on-calls for related inquiries (for example, support chat rooms for well-known operations).\n\n228\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nIn the case of more widespread and impactful outages, it is advisable to spin up dedicated “war rooms,”\n\nwhich can be used to coordinate efforts to resolve ongoing cross-team issues. Note that such war rooms should not be abused to ask for extensive or repeated status updates; while an issue is still “hot,” no focus should be diverted from resolving it first. Status updates should be performed by the on-call person in charge, recorded alongside the ticket inside the ticketing system, and at a prenegotiated frequency (such as every 15 minutes, or even better, based on the issue’s severity).\n\nAnother great trait for an on-call person to have is to be very suspicious and follow hunches. If the metrics graphs on the team’s services dashboard just simply don’t look right, or if a seemingly transient issue remains unexplained, then an on-call person is more often than not well advised to investigate such oddities; more than once in our careers have we seen “icebergs of problems” hiding beneath seemingly small irregularities, so resist the temptation to sweep issues under the rug.\n\nIn a good microservices architecture, failures are kept from propagating by introducing expectations for acceptable response times, error counts, or other sensible business metrics (like sales per hour, and so on.). A team’s on-call person should be particularly wary of adapting alarm thresholds. Make sure to discuss any adaptations of alarming limits with the rest of the team and other potential stakeholders in your business. If your monitoring detects increased failures in services on which you depend, make sure you pass these issues on to the teams that own the dependencies. Insisting on having SLAs, and on adhering to them, is in everyone’s interest, as ultimately they benefit the consumers of your business’s application.\n\nTracking and Scheduling Operations Support Work Even when microservices teams dedicate a (set of) developer(s) to operations support tasks, it is often unrealistic for all operational tasks to be resolved as part of an on-call duty; keeping the service up and running, or responding in adherence to its SLAs, simply outranks issues like version upgrades of frameworks used in the services.\n\nA Lightweight Process Proposal Whenever an operational issue arises that is estimated to take more time than a given low threshold (such as 15 minutes), a trouble ticket should be created. This guarantees visibility and traceability of operational support efforts. This means that reports can be run to identify the services that generate the most operational workload. Such visibility gives the team great input for prioritization efforts: a maintenance- intensive service can often easily justify refactoring work to fix the underlying issues that cause the frequent production issues.\n\nSometimes, when a team inherits a service that they did not develop themselves (for example, as a result of company department reorganization), they have a hard time answering requests or researching issues that relate to the correctness or expectations concerning the inherited service’s behavior. Such situations should be taken seriously, and often the creation of a “research and document” ticket is appropriate.\n\nWhen the tickets generated outnumber the tickets resolved during any given on-call rotation shift,\n\nthe operational workload is carried forward. In that case, it is important that you have a trouble ticketing system in which to track all backlogged operational tasks. The tickets in the backlog need to be tagged with an assessed impact severity level (based on team or company policy).7 Work on the tickets is generally guided by the SLAs associated with the tickets, but for tickets with similar resolution deadlines and severity, prioritization is handled ad hoc with input from other team members.\n\n7The “Defining and advertising team policy” section later in this chapter lays out points to address in a policy for severity assessment and issue prioritization.\n\n229\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nAn important role in this could fall to a Technical Project Manager (TPM). A TPM is a person who acts\n\nas the interface between the business departments and the engineering team. The TPM is usually a very technically proficient person (such as a former software developer), who has acquired management skills and is also well informed about the business’s current priorities and future direction. Both the technical and business skills are needed to coordinate the development team’s day-to-day work priorities. The TPM should not just be aware of the team’s project work, but should also have enough information to help clarify an on- call person’s questions regarding the prioritization of trouble tickets.\n\nIn order to schedule work effectively in a microservices development team, it will help to have a tool\n\nin which all tasks currently workable by the team are visible. Such an overall team work items board should identify tasks in progress, as well as backlogged, ordered by priority, and it should include both project work and operational tasks. If your team implements agile software development principles, then such an overall work items backlog can be very useful during sprint planning. For example, we have worked in teams that decided to have an “all hands on deck” sprint dedicated to clearing out a backlog of important operational issues, after explaining the impact of addressing long-standing operational problems to the TPM, who then in turn successfully communicated to the business stakeholders that project work would temporarily be put on hold.\n\nUnder ordinary circumstances, though, all work items in the operational backlog should only be handled by the people on call. As mentioned before, this is important for shielding the rest of the team from distractions, so they can be more focused when contributing feature work. While on-calls should the only people allowed to contact the rest of the team for help with operational issues, they should still use this tool wisely to respect the team’s overall productivity.\n\nFeatures to Look for When Picking a Trouble Ticket System When choosing the right toolset for managing operational support it is important to understand the use cases you envision your organization to have. Once you understand your needs, you can research the features of existing trouble ticket systems that cover your use cases directly, or at least allow for customization. Following are the features we find most important when working with trouble ticketing systems in a microservices organization:\n\nCompany-wide use: Make sure the entire company has access to the list of operational issues, past and ongoing, so that research about outages can be performed without unnecessarily contacting and distracting on-calls. On a related note, we recommend not to expose the internal trouble ticket communications directly to external customers, as you do not want to burden the fire-fighting on-call people with public relations concerns, or matters of inadvertently exposing classified information or hints about security vulnerabilities.\n\n\n\nTracking by severity: A view of the list of issues, with the ability to sort and filter by a user-assigned severity, is essential. The on-call person can use this view to guide her decisions about which issue to work on next, while (internal) customers can understand the severity attached to an issue they are following, and how much workload is placed on the team’s on-call.\n\n\n\nTracking by SLA: Trouble ticket systems need a way to attach expected and agreed- upon deadlines for their resolution. It would be great to make these fields mandatory and potentially even prepopulated based on the ticket’s impact severity. This way, the team can communicate a date to the customers which they can use for their own planning purposes. Additionally, on-call people should be able to sort tickets by proximity to their respective deadlines, so that they can make sure to meet the issue’s resolution timeline based on the team’s published operational SLAs.\n\n230\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nHelp in identifying issue owner: In an environment with hundreds of services and multiple teams, it is often not self-evident which team’s on-call person should investigate an issue. The trouble ticket tool should be set up to guide the person filing the ticket in identifying the right team that can resolve the cause of the problem. As a corollary, the ticket system should allow for reassignment of issues, in case it was previously misassigned.\n\n\n\nTicket history and searchability: Trouble tickets should never be deleted, even when they are closed or resolved. This helps in being able to search the ticket database during issue analysis and severity assessment. Also, all changes to a ticket’s properties, as well as comments, status updates, changes in assignee or impact severity, and the like, should be logged in an audit trail entry, alongside a timestamp and handle of the acting user. This will help in later reviewing the exact timeline as part of a potentially triggered post-mortem (discussed later in this section).\n\nRelating tickets: It should be possible to relate tickets to each other. In particular, it should be able to tag a given ticket as being blocked due to another, related ticket. The team’s on-call should be notified once the blocking ticket is resolved, so that she can make a decision about continuing work on the now unblocked task. To take this feature further, all internal customers should be able to subscribe to filterable status changes (like comments or updated severity), so that they don’t need to waste time actively checking status.\n\n\n\nIntegration with on-call scheduling tool: In order to correctly notify the team’s current on-call person of new tickets, and all other ticket-related communications, the trouble ticket system should be able to find out which (set of) developer(s) to notify. If your team runs on-call duty via a rotation schedule, then you want to be sure that changes in the rotation are automatically picked up by the trouble ticket system, to avoid delay and confusion in tending to operational issues. Often, the tool that is used to administer the on-call rotation schedule is also in charge of alerting the on-call of high severity issues, and escalating to a potential secondary on-call person, or the team’s manager. In that case, the trouble ticket system needs to be able to notify the on-call rotation system that new high-severity tickets have been filed, or that they are past their SLA date.\n\n\n\nIntegration with project work tracker: We advise that each team have easy access to, and visibility into, all work that the team needs to handle. Having an integration point that relates trouble tickets to tasks inside the project work tracking tool helps to address these needs.\n\n\n\nIntegration with post-mortems: We will explain the detailed contents and processes behind post-mortems later, but for now let’s focus on the fact that outages of a certain severity trigger deeper research into possible actions to take to prevent similar outages from reoccurring. The action items identified in a post-mortem should manifest themselves in trouble tickets with the appropriate owner, severity, and SLA, and status updates need to be visible in the post-mortem tracking tool.\n\nReporting capabilities: Many teams try to improve on dealing with operational workload, and you cannot improve in areas for which you have no metrics and measurements. Reports about new vs. resolved tickets per month, adherence to SLAs, and the services and features that are the heaviest contributors to operational issues are just some immediately obvious examples of reports your team would want to run. Many successful businesses (including Amazon.com) have very high visibility of trouble ticket reports, and the senior leadership level focuses on keeping operational churn low. 231\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nAnalyzing all available trouble ticket systems is definitely not in the scope of this book, as the range of\n\navailable features in each of them varies significantly. However, Wikipedia has an excellent overview and comparison page for issue tracking systems,8 which not only lists most of systems, but also tries to show an updated feature matrix.\n\nWhile we have worked with a fair number of trouble ticket systems over the course of our work experience, we have not come to find a tool that we could unequivocally recommend. That said, we have worked in organizations that have successfully employed and customized Atlassian’s JIRA and BMC’s Remedy to handle operational issues satisfying the majority of the use cases we’ve discussed.\n\nDashboards to Support Operations Another essential tool for on-call duty is a set of dashboards that graph the behavior of the systems owned by, or of interest to, the on-call’s team.\n\nA service owning team should try to include metrics in dashboards that are based on at least two\n\nsources: data collected on the service-side, and data as seen from the service clients’ perspective.\n\nGood candidates for inclusion in a dashboard that focuses on service-side metrics are, for example:\n\n\n\nEndpoint response times: Add graphs per service API endpoint that track the API’s response time as measured in the service application. Make sure to monitor not just average response times, but also the times in which 95%, 99%, or even 99.9% of all requests are processed.\n\nResponse times per client: Draw the same response times per service API endpoint broken down by the client applications. As before, make sure to visualize not just average values, but also the outliers.\n\nCall volume by endpoint: Seeing the call frequencies for each API endpoint is very useful as a way to detect peak times, and to relate potentially longer response times to a root cause.\n\nCall volume by client: Displaying the API calls on a per-client basis is useful for analyzing sudden drops or surges, as they might indicate a client-side outage or unintended change in behavior. This is also very useful for verifying intended behavior in call patterns, for example, when a migration from a deprecated API to a new service API has been deployed and needs confirmation of having taken effect.\n\n\n\nError responses per code: Seeing the number of error responses returned by a client can give you indications about bad data or other service-side outages.\n\n\n\nError responses per client: If one particular client is affected by most error responses, then it is easier to pinpoint the client in which logic has changed causing bad requests, or requests for data that causes service-side data processing problems.\n\nMessages consumed and published: If your service relies on consuming or publishing messages on an internal message bus, then visualizing metrics about the frequency of messages consumed or published on each of the relevant topics is useful information to observe.\n\n8“Comparison of issue-tracking systems” Wikipedia. https://en.wikipedia.org/wiki/Comparison_of_issue- tracking_systems\n\n232\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nBusiness metrics: Tracking business metrics around purchase rates, number of new customers, number of comments or reviews, and so on gives very expressive feedback and can help uncover problems that remain undetected when simply tracking the proper functioning of the technology involved. They can also help track down the root cause for changes in other metrics (like call volumes or service response times), or provide good indications for when planned outages or feature launches can be least impactful or risky (for example, to launch changes to the comment-service between 2AM and 4AM Eastern time because business metrics show that this is the period when the fewest customers post comments on the site).\n\nFigure 14-1 shows operational metrics for an internal service in LivingSocial’s infrastructure. The\n\nmetrics shown are extracted from service-side logs, aggregated and graphed using Splunk’s operational intelligence and log management tool.\n\nFigure 14-1. Splunk dashboard showing service-side log-based real-time metrics\n\nClient-side metrics that can help track down issues with a service can include:\n\nClient-side error code data: Tracking success and failure rates of requests to the service inside a particular client can be used to alert a team to outages in a dependency, or indicate that a client-side library update did not go as expected.\n\nClient-side performance data: The service performance data that matters the most is how it is perceived inside the client. Clients care deeply about the SLAs of their service dependencies, as their own responsiveness often is directly tied to their dependencies being performant.\n\nObject deserialization costs: Wrapping code that turns the wire format of transmitted data back into programming language objects might indicate that perceived service slowness is in fact more accurately rooted in slow deserialization in the client, which might be fixed by requesting smaller response objects from the service. 233",
      "page_number": 238
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 246-254)",
      "start_page": 246,
      "end_page": 254,
      "detection_method": "topic_boundary",
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\n\n\nTimeouts: While dependency timeouts should not lead to cascading issues, they are a welcome indicator for why the clients’ own functionality might degrade.\n\nCircuit breaker status: Getting an overview of all open circuits for a client can help to identify which circuit settings might be overly stringent, or which dependency teams should be consulted about SLA improvements.\n\nThe graphs shown in Figure 14-2 are part of a dashboard that displays operational data collected in\n\nthe client library of the internal service shown in Figures 14-1. The data was collected using StatsD (https://github.com/etsy/statsd), and the dashboard tool Grafana (http://grafana.org/).\n\nFigure 14-2. Grafana dashboard for client-side metrics based on StatsD held data\n\nOne reason for the usefulness of dashboards is that automated alarms to alert the team of exceptional behaviors are great when they work, but monitoring and alarming automation is never complete. Human intelligence often notices trends earlier and more reliably than algorithms do, and therefore can aid in detecting holes in alarming.\n\nSome teams even go so far as to display the most important dashboards on large screens visible from\n\ntheir work and break room areas. The rationale for featuring these live metrics so prominently is that the cost involved is a good investment in raising awareness of how the system dashboards look under normal circumstances. Humans are incredibly efficient at pattern matching even when subconsciously observing visual clues, and we have seen engineers infer operational aberrations on numerous occasions simply by looking at out-of-the-ordinary patterns in graphs they regularly observe. As mentioned above, being suspicious and not shrugging of irregularities is an essential trait of someone taking on-call duty seriously.\n\nOutage Etiquette No matter how good your development and test processes are, outages will happen. The real question will be how quickly your team will be able to recover from the failure of a launched system. This section covers tuning your process for dealing with outages and working preparedness for these cases into your team’s routines.\n\n234\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nEvery Pager Alarm is a Failure An organization is well advised to consider every occasion when a pager goes off—and a person is potentially woken up—as a failure whose root cause needs fixing. An intrusive alert should not happen unless it is unavoidable because the underlying problem has not been identified and addressed.\n\nIf you start seeing every instance of a pager alerting as an exception to root out, then you do not run the\n\ndanger of simply accepting it as a fact of life as an on-call. You will start to see delaying the true resolution as the only unacceptable course of action. Sweeping the issue under the rug via some rationalization you or someone else comes up with cannot be acceptable.\n\nThe very least you should do is to analyze whether the problem is truly worth alarming someone about,\n\nand you should not take the research lightly. If you and the team determine that the problem was not truly worth waking up someone, then you need to find ways to make the alarming rules smarter to improve on discerning between relatively benign issues and issues that cause a pager to alert.\n\nIf you do not address such “false positives” in your alarming, you will see that fatigue and indifference\n\nto alarms will set in, and truly impactful exceptions run the danger of being ignored. Additionally, having overly sensitive alerting rules that require your person on call to investigate in a very stressful and disruptive setting will burn out your developers, and you will see increased levels of attrition.\n\nIf your company implements a policy that humans can manually trigger pager alerts—for example, when they have reason to believe they noticed a significant outage that has not yet been brought to anyone’s attention—then you need to set clear expectations about which class of issues are “pageable.” Therefore, a team—or even better, company—policy for operational support and SLAs is essential to implement. We will discuss the contents of such a policy later in this chapter. It should be made clear that, for example, requests for information regarding behavior of a system or the existence of, or implementation request for, a given feature are not acceptable reasons to page an engineer.\n\nOn the other hand, engineers need to keep a professional attitude when being paged by a colleague,\n\nirrespective of the time of day or night the pager rings. Ask the reporter for enough details needed to investigate and reproduce the issue. It is very reasonable to require that a ticket exist for any issue that is worth being paged about, so make sure you insist on recording all information as comments in the trouble ticket. If the issue is serious enough, based on criteria also defined in your operational SLA policy, then such information will be essential when you have to create a post-mortem.\n\nPlaybooks for Operational Support Imagine you are the team’s on-call person, and your pager wakes you up at 3:00 AM. You are trying to adjust, wake up and not well enough rested to have all your mental capacity.\n\nWe have seen many teams address this problem by writing down Standard Operating Procedures (SOPs) in the form of step-by-step recipes, useful links, checks to perform or investigations to start. These are meant to be very explicit and easy to follow, so they can be consulted by an on-call person who is new, stressed out, or otherwise not in the best state of mind to analyze the root cause of an outage.\n\nHere are a few things we encourage including in SOP playbooks:\n\n\n\nLinks to operational dashboards: Quick access to an overview page for all operationally relevant metrics is very useful for analyze root causes of issues.\n\n\n\nLinks to access and error logs: Looking at error traces and client access patterns is often invaluable when trying deal with an outage.\n\nContact information for owner team: Links to communication channels (such as a Slack channel) where to discuss outages, contact information for subject matter experts.\n\n235\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nArchitectural overview: Adding overview diagrams for the service’s architecture and its main component and dependencies helps tremendously when attempting to reason about the cause for an outage, or where to best place a patch.\n\n\n\nIf-then scenarios: This section lists known issues or previously experienced outages that could recur, keyed by how such issues usually manifest themselves, plus a checklist for verifying that the issue is encountered, and steps for fixing or mitigating. This is especially useful if it includes listing investigative steps for researching business alarms.\n\n\n\nList of service dependencies: In a massively distributed microservices environment it is unfortunately rather common that an on-call person receives an alarm that is caused by degradation of a service dependency. While circuit breakers should prevent cascading failures, it is still essential that the dependent team know about the situation. Links to trouble ticket boards of all service dependencies and the contact details for all service dependencies’ on-call people should be easily found.\n\nHealth-check URLs: Verifying that an issue is no longer present, or helping to reproduce an exception, include URLs or cURL statements for every feature or API endpoint in your service application.\n\n\n\nProduction server access details: Sometimes problems can only be tracked down or reproduced by having direct access to the production hosts, where data or server condition can be checked or a service console needs to be opened. This also includes access to time-based jobs (such as crontasks) and their logs.\n\n\n\nFAQs: Answering often-asked questions about the way some functionality works can be made much more efficient when listing them in an easy to find section on the SOP page. Also, such a section can list recipes for common administrative tasks, for example, modifying data not exposed by an API, running diagnostics queries on a secondary database, or whitelisting new client applications.\n\nNote that such playbooks are only useful if they are followed often, and maintained and updated when found outdated, incorrect, or lacking. Whenever you find them in such a state, either directly update, or add to them as part of following these instructions, or create a support ticket to take care of it in the near future.\n\nWhat to Do When the Pager Goes Off In a team that takes addressing and fixing operational issues seriously, no two outages are exactly the same, simply because the previous outage was fixed in a way that will not allow it to recur. It is still worth providing some general proven rules and behavioral patterns that are appropriate when being alerted about an urgent production issue.\n\nFirst and foremost, there is no use in falling prey to the temptation of panicking. Being informed by a very loud alert about an impactful outage certainly gets your blood flowing, but that should only go so far as making you aware that your undivided attention is needed to investigate the issue. Dealing with production system failures needs to take the highest precedence for you and the rest of your team, so if you feel like you are stuck or ineffective in fixing the issue, then this is an “all hands on deck” situation, and the entire rest of your team can be pulled in to help.\n\nNext, make sure that you acknowledge the alarm, so and others do not get unnecessarily alerted. Getting alerted over and over is just annoying and distracting, and consequently counterproductive to quick resolution of the problem at hand. In many pager-duty support systems, tickets escalate automatically when not acknowledged, so be respectful of other people on the escalation path.\n\n236\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nOnce you have acknowledged the alert, create a trouble ticket (unless it already exists) and do just as much research as it takes to be reasonably confident you know enough to either fix the root cause, or improve the situation significantly. If you find a quick route to improve conditions, then take it! This often buys you time to work on a more complicated fix in a less impactful setting.\n\nIf you are in the unfortunate position that you get woken up by an alarm, then feel free to patch as little\n\nas necessary to address the immediate problem, but do make sure to work on a more permanent fix once you are fully rested.\n\nWhile you research root cause and potential fixes, take a small amount of time to update the trouble ticket whenever there is news to share. Do not feel obligated to answer questions about status or progress while you work on researching or otherwise addressing an issue. If the inquiries become distracting, inform the person inquiring politely that answering questions distracts you from resolving the issue. Most teams make it a point in their team policy for operation support to mention intervals in which an in-progress trouble ticket of a particular severity is updated. Try your best to adhere to such a published SLA, but favor acting on fixing the issue over updating the ticket.\n\nIf, on the other hand, you need help or information from other on-call people or other team members,\n\ndo not hesitate to reach out to them. Unless they themselves are busy dealing with issues of the same or higher operational severity, they should make every effort to help you.\n\nIt is also important that in your role as on-call person you refrain from passing blame while an issue is being actively worked. The time to assess what went wrong and how it can be prevented from recurring is after the issue is no longer causing production issues. A post-mortem, which is discussed in the next section, is the best place for such analysis.\n\nSupporting Tools There are quite a lot of choices for tool support for the above-mentioned measures to establish a well- functioning operational emergency protocol.\n\nSOP pages and playbooks should first and foremost be easily found. If your company has not already\n\nlaunched an internal tool to enable teams to self-publish documentation, then it is definitely worth evaluating tools like Atlassian’s Confluence (https://www.atlassian.com/software/confluence) or a self-hosted Wiki (see http://www.wikimatrix.org/ for a site that lets you compare features of Wiki implementations).\n\nAs far as incident management and escalation tools go, we have personally had experiences with PagerDuty\n\n(in use at LivingSocial) and a heavily customized version of BMC’s Remedy (in use at Amazon.com). Other alternatives include VictorOps (https://victorops.com/), OpsGenie (https://www.opsgenie.com/), and the open-source tool Openduty (https://github.com/ustream/openduty).\n\nThe main features we have found indispensable for such a tool to be effective are:\n\n\n\nAn ability to acknowledge an incident and hence stop further alerting for it\n\n\n\nBeing able to de-duplicate incidents, so that no more than one alarm is generated for the same issue\n\n\n\nThe ability to integrate with a trouble ticket system for automatic ticket creation and relation\n\n\n\nThe ability to maintain an on-call rotation schedule that allows correctly routing alarms to team members based on the current rotation shift\n\nAnother valuable feature, albeit in the nice-to-have category, is to offer integration points to group\n\ncommunication tools like email, Slack, or Campfire to alert all stakeholders of incident status.\n\n237\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nHow to Learn from Outages:—Post-Mortems The best engineering organizations strive to achieve operational excellence by learning from past outages. A post-mortem can be a powerful instrument to achieve this goal.\n\nPost-mortems—originally used in the context of medical examinations otherwise known as autopsies—\n\ngenerally refer to the process of assessing successes and failures of a task or process after its conclusion. In the realm of on-call duty and operational excellence, their main purpose and value is to help with identifying lessons learned from an incident with a large impact.\n\nWhen Is a Post-Mortem the Right Tool? Good indicators for the usefulness of a post-mortem are that:\n\n\n\nThe issue that occurred, or could have occurred, is severe.\n\n\n\nThe occurrence of the issue appears preventable at first sight.\n\n\n\nThe root cause for the (near-)outage appears to need more research.\n\n\n\nThe issue analysis is worth sharing with a wider audience or preserving for posterity.\n\nWe recommend creating a post-mortem for every high-impact incident of a severity level that is negotiated within the engineering organization. As a rough guideline, you should discuss the value of creating a post-mortem each time your team’s on-call person gets paged, or should have been paged.\n\nAdditionally, it should be acceptable for anyone in the company to call for creation of a post-mortem, if she can argue that there is enough value to the business to outweigh the costs of going through the motions of a full post-mortem.\n\n■ Note try to make sure that post-mortems are not understood as punitive in your organization. it needs to be made clear that the reason for the effort involved is the chance to improve on what you do, and to prevent further operational issues.\n\nStructural Elements of a Post-Mortem While there is no globally accepted table of contents for a post-mortem, we strongly advise that your engineering organization adopt a template that is shared across all teams. The following list outlines such a template, listing the contents we suggest discussing as part of every post-mortem:\n\nAction summary: This lead-in contains a very brief, “executive summary” description of the outage that was dealt with and the actions that were taken to address the issue.\n\nBusiness impact: A very succinct, to-the-point synopsis of the loss to the business is mentioned in here, with calculations that are as accurate as currently possible. The loss could be in various areas, for example, negative revenue impact, exposure of sensitive data, or loss of customer trust.\n\nActors: This section should include all people involved in causing or fixing the issue discussed.\n\n238\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\n\n\nTurn of events: A post-mortem should lay out a detailed and accurate timeline of all relevant events, starting from the initiation of the outage (such as the deployment of unsafe code artifacts), all the way to the (potentially temporary) resolution of the underlying issue. All significant actions and findings should be included and attributed.\n\n\n\nSuccesses: In this part, the author of the post-mortem lists actions and practices that contributed to lessening the impact of the outage.\n\nRoot cause analysis: This section starts out with a more in-depth technical explanation of the cause of the issue and continues by analyzing the actions and circumstances that contributed to causing the outage. We highly recommend using the \"Five Whys\" technique to aid in these efforts (see the accompanying sidebar).\n\n\n\nLessons learned: Areas for improvements are identified and listed in this part, and they should be based on the results of the root cause analysis.\n\nAction items: The points listed in this section are concrete, actionable improvements. They should directly relate to the cause(s) of the outage, and they should explain how they help to fix the identified root of the problem.\n\nTHE FIVE WHYS9\n\nthe Five whys symbolize a strategy for identifying the true cause-and-effect relationships at the heart of a problem, in an attempt to be sure which steps to take to truly fix the issue. at each stage of the ever-deeper drilling chain of questions about causality, the current level asks \"why?\" referring to the cause of the previous step; that is, each stage forms the basis of the next step. the significance of the Five in the technique’s name comes from the experience that—for the majority of problems initially researched—asking “why?” down to the fifth level typically reveals the end of the causality chain.\n\nexample of a Five Whys analysis:\n\nWhat happened? the laptop computer did not start up.\n\n1. Why (did the laptop fail to start up)? the battery is dead.\n\n2. Why (is the battery dead)? the charger for the computer is not functioning.\n\n3. Why (is the charger not working)? the power cable has broken.\n\n4. Why (is the power cable broken)? the power cable was often rolled up, and hence friction caused it to disintegrate.\n\n5. Why (was there a lot of friction)? the charger has a design that exposes sharp edges close to where the cable is meant to be fastened.\n\nFirst used within the toyota Motor Corporation, the Five Whys has been adopted by technology and engineering companies like amazon.com. its power lies in its simplicity and straightforwardness when trying to determine the true causes of problems.\n\n9See “5 Whys,” on Wikipedia, for more details about the origin and history of this technique. https://en.wikipedia. org/wiki/5_Whys\n\n239\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nPost-Mortem Ownership Every post-mortem has exactly one owner. The owner is not necessarily the same person or team who will work on addressing all action items identified as part of the post-mortem. Usually this role is filled by the author of the post-mortem, but often we seen the author’s manager take on ownership of all post-mortems authored by his team.\n\nThe owner of the post-mortem is tasked with helping to identify and track all resulting action items. She\n\ncreates trouble tickets for each of them, each of which has a deadline and a ticket owner (or owning team) associated with it.\n\nThe owner will chase all tickets to completion by the assigned teams, and then she will resolve the post- mortem itself. She will also notify all parties involved of the resolution, and will make sure that the finished document is stored in a searchable repository, so that the knowledge gained is usable in the resolution of future issues.\n\nWhile this process of following up and coordinating post-mortem action item resolutions can be time-\n\nconsuming, it is nevertheless of utmost importance to make the process useful; if follow-through is not taken seriously, then the post-mortem process is mostly ineffective and should most likely be dropped entirely, as it will then only incur costs for very little value in return.\n\nDefining and Advertising Team Policy A lot of what makes an organization achieve efficiency and excellence in operational matters is rooted in the quality of the processes and policies they apply to support the systems they own. This section will lay out which points should be addressed in a policy for operational excellence, and we will give examples of policy points we have seen work well in practice.\n\nImpact Severity and SLAs In order to improve focus and enable the on-call person to make decisions about which operational issue to work on at any given point in time, it helps to categorize trouble tickets by level of severity.\n\nTo do that, define a handful of distinct levels of severity and classify each of them with its level of\n\nimpact. Additionally, assign a priority level to each severity level, so that client expectations can be set about the on-call person’s attention to incidents in each severity category. For clarity, it usually also helps to add examples of outage situations that would fall into the specified severity and impact levels.\n\nFigure 14-3 shows an example of how such incident severity, impact, and priority levels could be\n\ndefined.\n\nFigure 14-3. Definitions for issue impact and severity, with examples and priority levels\n\n240\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nOnce you have defined the severities, your customers need to know what level of service they can expect from your team. As a customer-focused team, you should set expectations for, and facilitate, communication with your customers. SLAs are an essential tool to achieve such customer focus, and they will also let you know when your on-call process is working and, more importantly, when it is not.\n\nAn example for a team SLA, based on incident severities, is shown in Figure 14-4.\n\nFigure 14-4. SLA for first contact, updates, and resolution of incidents by severity level\n\nThe meaning of first contact in the SLA definition is the most important part of the issue lifecycle, next\n\nto resolution. This initial contact informs the client that the issue has been acknowledged. Without such an entry in your SLA, the wait time can be frustrating for your clients. With a defined SLA, it becomes clear when follow-up will be warranted. In addition to reducing client uncertainty, the SLA reduces unnecessary and inefficient back-and-forth, allowing the on-call person more time to focus on issue resolution.\n\nSimilar arguments hold for the update SLAs. Sometimes, resolving a trouble ticket can take the on-call person several days. Updating a ticket that is in progress serves a similar purpose to the first contact, as each update shows the client that their issue has not been abandoned. Updates that simply state that the efforts continue to be blocked still demonstrate strong ownership. By keeping your customers abreast of progress, anxiety and uncertainty is reduced.\n\nAlso advertise which type of incidents you consider worthy of analyzing in a post-mortem. You can use the severity level as an indicator (for example, all incidents of severity 1 and 2 trigger post-mortems), or you could take the stance that they apply mainly to more systemic issues that need to be addressed. As we have shown earlier in this chapter, a post-mortem is an effective tool to address process failures for systemic issues.\n\nPublishing an SLA for issue resolution is the most important step for your clients, as it sets expectations\n\nthey can rely on. If this underlying problem is blocking work on the client side, this expectation can help your customers identify risks in their own roadmaps.\n\nWhile it is first and foremost the team’s responsibility to set the SLAs and severities, the results need to be acceptable to the clients and other stakeholders in your company. Deciding whether an SLA or a severity judgement needs to be adapted is up to negotiations between your team and the internal customers, but it is very important to publish your policy up front.\n\nShare the Steps of Your Process Publishing the details of how your team goes about processing your operational issues helps clients to understand each phase involved in resolving an incident.\n\n241\n\nChapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nA sample list of steps could be as follows:\n\nGeneration: When an operational issue for one of the systems owned by the team arises, a trouble ticket will be created and assigned to the correct on-call person. Make sure to advertise how to find the correct group in the trouble ticket system so that the ticket does not get misassigned and therefore might linger longer than needed.\n\n\n\nTriage: The team’s current on-call person will investigate the issue to a level where he can assess its severity, and an appropriate label (such as SEV-3) will be added to the ticket.\n\n\n\nProcessing: The on-call person is in charge of deciding which issues take priority, and hence will be worked at any given point in time. Decision guidelines for the on-call are based on issue severity and published SLAs, along with feedback from other team members and other engineers. All communication about a ticket’s cause, technical details, and status will be captured in ticket comments. This mainly serves historical and documentation purposes. External inquiries about status should be kept to a minimum in order to avoid distracting the on-call from actually working on the issue’s resolution. In return, the on-call aims to update ongoing issues based on the published SLAs, so that ticket stakeholders can be kept abreast of developments.\n\nResolution: The team will strive to adhere to the published resolution SLAs, and it will alert stakeholders with as much advance notice as possible about any risks that could cause such deadlines to slip. Issues of severity 1 and 2 should automatically result in a post-mortem.\n\nFinally, it is within each team’s ability to customize its on-call policy, but strive to streamline the policies, process and SLAs across the entire company. This will help internal clients to have confidence that their issues will be addressed, that no time is lost trying to contact owners, and they can set their own customers’ expectations correctly based on a generally accepted the process.\n\nSummary This chapter focused on all aspects of what it means to be part of a team that works under the “you build it, you run it” premise. After defining what operational support means, we explained what comes along with the obligations of true ownership for microservices.\n\nNext we looked at the role of being on call for the services owned by your team. We discussed\n\nresponsibilities for the on-call person, in relation to the rest of the team, as well as when serving client teams that rely on the software you own.\n\nWe also gave tips for how to organize and prioritize operational workload compared to the project work\n\nrequests handled in a microservices team, and we discussed important features for tools that are meant to support the team’s efforts to track both types of work.\n\nAfter a brief examination of operational dashboards, we gave an overview of a practical approach to\n\nhandling system outages when being the on-call person in charge.\n\nWe showed the importance of learning from outages, and walked through the process of analyzing such\n\nincidents using post-mortems.\n\nWe concluded this chapter by discussing the topic of defining and sharing a policy of how your team handles operational ticket work, and how to best set client expectations using SLAs and incident severity definitions.\n\n242",
      "page_number": 246
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 255-258)",
      "start_page": 255,
      "end_page": 258,
      "detection_method": "topic_boundary",
      "content": "Index\n\n A Aggregation\n\nbookstore app\n\napplications and aggregated services, 34 decomposing, core services, 33 feature-focused services, 31–32, 35–36 partitioning, 32 supporting platform services, 34–35\n\nbusiness intelligence, 28–29 derived functionality, 28 system performance, 29–30 vs. decomposition, 30\n\nAPI design\n\nAPI optimization caching\n\nadvantages, 86 API-provided data, 87 cache-control header, 89 definition, 86 ETag and If-None-Match headers, 88 HTTP cache control, 89–92 HTTP standard, 87 if-modified-since headers, 88 last-modified headers, 88 proxies, 92 catalog-service, 100 development and support, 98–99 performance issues\n\nAmazon Web Services, 38 app, home page, 49 breaking changes, 45–46 definition, 37 guidance, 41 information hiding, 44–45 nonfunctional requirements\n\navailability, 42 capacity, 43 performance, 43 recoverability, 43 reliability, 43 scalability, 43 service-level agreements (SLAs), 42\n\nbusiness logic profiling, 84 database related inquiries, 83 memory allocations and CPU cycles, 84 service dependency, 84\n\npublish and subscribe pattern, 100 service access pattern, 85 services size\n\nDenial of Service attacks, 97 HTTP compression, 95 partial resources pattern, 94 result representations, 93–94 semantic paging, 96\n\nnumbering scheme, 48 online store\n\nAutomation, 151 Autoscaling, 172 AWS CloudFormation, 161\n\napp, home page, 50 book detail page, 52–54 cart information, 50–51 images, 56–57 reviews, 55–56 user recommendations, 51–52\n\n B Backend for Frontend (BFF) pattern, 86 Blue-green deployment, 162\n\nprinciple, 45 properties, 39–41 requirement setting, 43 version, 47–49\n\n C Cache-control header, 89 Canary release, 163–164\n\n© Cloves Carneiro Jr. and Tim Schmelmer 2016 C. Carneiro Jr. and T. Schmelmer, Microservices From Day One, DOI 10.1007/978-1-4842-1937-9\n\n243\n\nClient libraries caching, 78 error handling\n\ncircuit breaker, 79–80 timeouts, 79\n\nfeatures, 75–76 performance, 80 service discovery, 81 service orchestration, 78 practices to avoid, 76–77 update, 81 ClojureScript, 181 Consumer-facing applications, 19 Continuous delivery (CD), 154 deployment step, 154–155 litmus test, 154 microservices, 156 pros and cons, 156 tool support, 156–159\n\nContinuous integration (CI), 116–117\n\nadvantages, 152–153 microservices, 153 Contract testing, 147–150\n\n D Decomposition\n\nchange feedback, 108, 123 codifying internal standards, 125 collaborative development, 107 concurrent and collaborative development, 123 consistently named scripts, 109 consistent start-up scripts, 111 Continuous Integration (CI), 116–117 data sources, 118 developer machines, 107 docker and docker compose, 110–111 documentation search, 126 downstream services\n\nclient-side samples, 119–120 connecting, shared data store, 120 mock services, 120\n\nguard, 114 privacy, 107 process and documentation, 112 production snapshots, 119 proxy service, 124–125 sample data availability, 106 sandbox fabric, 121 saving resources, 122 scripts, 118 service dependencies, 122 service generation, 108 shared data store, 119 testing\n\nvs. aggregation, 30 bookstore app\n\napplications and aggregated services, 34 core services, 33 feature-focused services, 31–32, 35–36 partitioning, 32 supporting platform services, 34–35\n\ncode guidelines, 113–114 common tools, 112 integrating static\n\nanalysis tools, 115–116\n\ntest framework, 112\n\ntool setup, 106\n\ndata-access pattern, 27 decomposing by maturity, 26 decomposition by context, 27–28 functionalities, 25–26\n\nDeployment and operational aspects\n\nEAS, 167–168 locality of configuration, 167 LXC with Docker, 170–171 microservices architectures, 165 process running options, 166 single microservice vs. multiple microservices, 166\n\nDocumentation search, 126, 180\n\n E, F End-to-end tests, 145–146 Enterprise application server (EAS), 167–168 Enterprise integration patterns, 100 Error handling\n\ncircuit breaker, 79–80 timeouts, 79\n\nETag and If-None-Match headers, 88\n\nvirtual machines (VMs), 168–170\n\nDevelopment environment\n\naccessibility, 108 application dependencies, 106 application test automation, 106 automating application set-up, 109 automating developer machine, 109\n\n G Gerkhin tests, 142–144\n\n H Hypermedia type, 40\n\n244\n\n I, J, K If-Modified-Since header, 87 Interface Definition Languages (IDLs)\n\n O Obidos, 6 Operational support\n\nadvantages, 59 REST, 60–61 RPC, 60\n\ncustomers oriented, 222 dashboards\n\nclient-side metrics, 233–234 sources, 232\n\n L Languages and technologies\n\nFive Whys analysis, 239 mapping, 223 on-call\n\nevolution, 179 implementation, 179 specialization, 178 Last-modified header, 87\n\nadvantages, 228 microservices environment, 228 team efficiency, 226 team relationship, 227\n\n M Message-based approach, 101 Microservices\n\non call, role, 224 pager alerting, 235 pager-duty support systems, 236 playbooks, 235–236 post-mortem\n\nadvantages, 9–10, 12, 18 anti-patterns, 15–16 architecture, 10–11 characteristics, 5 definition, 4–5 disadvantages, monolith, 11 industry adoption\n\nAmazon.com, 6 Google and eBay, 7 Netflix, 7–8 Obidos, 6 SoundCloud, 8–9, 12\n\nprerequisites, 14 “services first” approach, 17 SOA, 15–16\n\nMigrating to Microservices, 15 Monitoring service\n\nownership, 240 structural elements, 238–239 usefulness, 238\n\npriority-item, 225 production issues and triage, 225 service-side metrics, 232–233 shared ownership, 223 supporting tools, 237 team cohesion, 223–224 team policy\n\nimpact severity\n\nand SLAs, 240–241 issue definitions, 240 process steps, 242 tracking and scheduling lightweight process proposal, 229–230\n\nAdjusting Alarms, 218 average response time, 216 business matrics, 213 endpoint’s performance, 217 logging tools, 215 perceived performance, 212 requests identification, 214 SLAs, 213 slow response issue, 217 tools, 211–212 uniform log format, 215–216\n\nMonolith, 13\n\ntrouble ticket systems, 230–231\n\ntrust making, 222\n\nOut-of-process hyperlink, 131–132\n\n P, Q Polyglot service\n\nbootstrapping decisions, 179 caching layer, 177 deployment issues, 179 documentation, 180 implementation planning, 181 rails, 182\n\n N Netflix, 7–8 Node.js, 181\n\nPre-production environment\n\nsafer releases, 162 staging fabric, 160 tool support, 161\n\n245\n\n R RPC vs. REST, 60 Rules of engagement API definition, 21 consistently, 21 consumer-facing applications, 19 existing code bases, 22 interfaces, 20 service access, 20 service creation steps, 20\n\nhosting, API JSON\n\nspecification, 62–65\n\nJSON specifications, 71 resource and paths definitions, 65\n\nUser Interface (UI)\n\nGET /iusers/{id}, 67 users-service, 65–66\n\nyard description\n\nUser API model, 74 UserController, 72–73\n\n S Serverless architecture, 172–173 Service-level agreements (SLAs), 42 Service orchestration, 78 Service-oriented architecture (SOA), 3 Single-client application\n\n T Team Policy\n\nprocess steps, 242\n\nTest-driven development (TDD), 118 Testing\n\narchitecture, 185–186, 208 bookstore\n\nbook detail page, 197–202 circuit breakers, 197 class, 188 featured categories, 192, 194 home page, 186–187 internal error avoidance, 195–196 json_api_client, 187 purchases-service, 189, 191–192 shopping cart widget, 203–204, 206\n\nperformance considerations, 208–209\n\nAdminstration Web UI, 142 book test stubs, 139 contract testing, 147–150 end-to-end tests, 145–146 in-process hyperlink, 131 layers, 128 out-of-process\n\nhyperlink, 131–132, 141 pluggable back-ends, 135–138, 141 stubbing, 134–135 stubs customization, 140 test Set-Up, 140 unit testing, 128–129\n\nSoftware services, 4 SoundCloud, 8–9, 12 Swagger\n\nAPI JSON specification, 61, 62 blocks description, 71–72 code generation\n\nClojure client library, 70–71 Ruby client gem, 68–70\n\n U Unit testing, 128–129\n\n V, W, X, Y, Z Virtual machines (VMs), 168–170\n\n246",
      "page_number": 255
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Microservices from Day One\n\nBuild robust and scalable software from the start — Cloves Carneiro Jr. Tim Schmelmer",
      "content_length": 113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "Microservices from Day One\n\nBuild robust and scalable software from the start\n\nCloves Carneiro Jr.\n\nTim Schmelmer",
      "content_length": 113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Microservices from Day One: Build robust and scalable software from the start\n\nCloves Carneiro Jr. Hollywood, Florida, USA\n\nTim Schmelmer Broomfield, Colorado, USA\n\nISBN-13 (pbk): 978-1-4842-1936-2 DOI 10.1007/978-1-4842-1937-9\n\nISBN-13 (electronic): 978-1-4842-1937-9\n\nLibrary of Congress Control Number: 2016961230\n\nCopyright © 2016 by Cloves Carneiro Jr. and Tim Schmelmer\n\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.\n\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every occurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion and to the benefit of the trademark owner, with no intention of infringement of the trademark.\n\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\n\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.\n\nManaging Director: Welmoed Spahr Acquisitions Editor: Louise Corrigan Development Editor: Chris Nelson Technical Reviewer: Tom Copeland Editorial Board: Steve Anglin, Pramila Balan, Laura Berendson, Aaron Black, Louise Corrigan, Jonathan Gennick, Todd Green, Robert Hutchinson, Celestin Suresh John, Nikhil Karkal, James Markham, Susan McDermott, Matthew Moodie, Natalie Pao, Gwenan Spearing\n\nCoordinating Editor: Nancy Chen Copy Editor: James A. Compton Compositor: SPi Global Indexer: SPi Global Artist: SPi Global, image courtesy of Freepik\n\nDistributed to the book trade worldwide by Springer Science+Business Media New York, 233 Spring Street, 6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders-ny@springer-sbm.com, or visit www.springer.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation.\n\nFor information on translations, please e-mail rights@apress.com, or visit www.apress.com.\n\nApress and friends of ED books may be purchased in bulk for academic, corporate, or promotional use. eBook versions and licenses are also available for most titles. For more information, reference our Special Bulk Sales–eBook Licensing web page at www.apress.com/bulk-sales.\n\nAny source code or other supplementary materials referenced by the author in this text are available to readers at www.apress.com. For detailed information about how to locate your book’s source code, go to www.apress.com/source-code/. Readers can also access source code at SpringerLink in the Supplementary Material section for each chapter.\n\nPrinted on acid-free paper",
      "content_length": 3450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "To my wife Safora, for all her love, and for her support during the hundreds of hours that I spent on this book while she ran the family.\n\n—Tim Schmelmer\n\nFor my family, Jane, Noah, and Sofia.\n\n—Cloves Carneiro Jr.",
      "content_length": 214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Contents at a Glance\n\nAbout the Authors ����������������������������������������������������������������������������������������������������xv\n\nAbout the Technical Reviewer �������������������������������������������������������������������������������xvii\n\nAcknowledgments ��������������������������������������������������������������������������������������������������xix\n\nIntroduction ������������������������������������������������������������������������������������������������������������xxi\n\n■Part I: Service Oriented Architectures ������������������������������������������������ 1\n\n■Chapter 1: Microservices: The What and the Why �������������������������������������������������3\n\n■Chapter 2: Rules of Engagement �������������������������������������������������������������������������19\n\n■Part II: APIs ��������������������������������������������������������������������������������������� 23\n\n■Chapter 3: Partitioning Your Business into Services �������������������������������������������25\n\n■Chapter 4: Designing Your APIs ��������������������������������������������������������������������������37\n\n■Chapter 5: Defining APIs ��������������������������������������������������������������������������������������59\n\n■Chapter 6: Consuming Your APIs �������������������������������������������������������������������������75\n\n■Chapter 7: Optimizing Your APIs �������������������������������������������������������������������������83\n\n■Part III: Development and Deployment ������������������������������������������� 103\n\n■Chapter 8: Development Environment and Workflow ����������������������������������������105\n\n■Chapter 9: Testing with Services �����������������������������������������������������������������������127\n\n■Chapter 10: Deploying and Running Microservices �������������������������������������������151\n\nv",
      "content_length": 1845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "■ Contents at a GlanCe\n\n■Part IV: Putting Everything Together ���������������������������������������������� 175\n\n■Chapter 11: Polyglot Services ���������������������������������������������������������������������������177\n\n■Chapter 12: Single-Client Application—Mobile and Web ����������������������������������185\n\n■Chapter 13: Monitoring Your Services ���������������������������������������������������������������211\n\n■Chapter 14: Operational Excellence with Microservices �����������������������������������221\n\nIndex ���������������������������������������������������������������������������������������������������������������������243\n\nvi",
      "content_length": 649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Contents\n\nAbout the Authors ����������������������������������������������������������������������������������������������������xv\n\nAbout the Technical Reviewer �������������������������������������������������������������������������������xvii\n\nAcknowledgments ��������������������������������������������������������������������������������������������������xix\n\nIntroduction ������������������������������������������������������������������������������������������������������������xxi\n\n■Part I: Service Oriented Architectures ������������������������������������������������ 1\n\n■Chapter 1: Microservices: The What and the Why �������������������������������������������������3\n\nAll Successful Software Undergoes Change ��������������������������������������������������������������������3\n\nRemember Service-Oriented Architectures? ��������������������������������������������������������������������3\n\nWhat Is a Service? ������������������������������������������������������������������������������������������������������������4\n\nWhat Are Microservices? ������������������������������������������������������������������������������������������������������������������������4\n\nMicroservices Are Not “CORBA over HTTP” ��������������������������������������������������������������������������������������������5\n\nIndustry Adoption of Microservices ����������������������������������������������������������������������������������6\n\nWhy Use Microservices? ��������������������������������������������������������������������������������������������������9\n\nAdvantages as Seen by Industry Experts �����������������������������������������������������������������������������������������������9\n\nOur Reasons to Adopt a Microservices Architecture ���������������������������������������������������������������������������12\n\nPotential Pitfalls and How to Avoid Them �����������������������������������������������������������������������13\n\nWhen Should I Start with a Microservices Architecture? �����������������������������������������������17\n\nSummary ������������������������������������������������������������������������������������������������������������������������18\n\n■Chapter 2: Rules of Engagement �������������������������������������������������������������������������19\n\nCustomer-Facing Applications Cannot Directly Touch Any Data Store ���������������������������19\n\nNo Service Accesses Another Service’s Data Store �������������������������������������������������������20\n\nInvest in Making Spinning Up New Services Trivial �������������������������������������������������������20\n\nvii",
      "content_length": 2609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "■ Contents\n\nBuild Services in a Consistent Way ��������������������������������������������������������������������������������21\n\nDefine API Contracts in Code ������������������������������������������������������������������������������������������21\n\nEvery Service Has at Least a Development and a Production Instance �������������������������22\n\nExisting Code Bases �����������������������������������������������������������������������������������������������������������������������������22\n\nSummary ������������������������������������������������������������������������������������������������������������������������22\n\n■Part II: APIs ��������������������������������������������������������������������������������������� 23\n\n■Chapter 3: Partitioning Your Business into Services �������������������������������������������25\n\nGeneral Approaches for Application Decomposition and Aggregation ���������������������������25\n\nApplication Decomposition �������������������������������������������������������������������������������������������������������������������25\n\nApplication Aggregation������������������������������������������������������������������������������������������������������������������������28\n\nWhen to Use Decomposition versus Aggregation? ���������������������������������������������������������30\n\nDecomposing and Aggregating a Sample Application ����������������������������������������������������31\n\nWelcome to Our Bookstore! ������������������������������������������������������������������������������������������������������������������31\n\nPartitioning the Bookstore ��������������������������������������������������������������������������������������������������������������������32\n\nSummary ������������������������������������������������������������������������������������������������������������������������36\n\n■Chapter 4: Designing Your APIs ��������������������������������������������������������������������������37\n\nIntroducing APIs �������������������������������������������������������������������������������������������������������������37\n\nThe Significance of API Design �������������������������������������������������������������������������������������������������������������38\n\nWhy is Designing a Good API Important? ����������������������������������������������������������������������������������������������38\n\nWhat Makes an API Good?����������������������������������������������������������������������������������������������39\n\nGeneral Characteristics ������������������������������������������������������������������������������������������������������������������������39\n\nHypermedia Type ����������������������������������������������������������������������������������������������������������������������������������40\n\nAPI Design Guide ����������������������������������������������������������������������������������������������������������������������������������41\n\nNonfunctional Requirements ����������������������������������������������������������������������������������������������������������������42\n\nImplementing APIs the Right Way ����������������������������������������������������������������������������������43\n\nGather Requirements ����������������������������������������������������������������������������������������������������������������������������43\n\nMaximize Information Hiding ����������������������������������������������������������������������������������������������������������������44\n\nAPIs Change ������������������������������������������������������������������������������������������������������������������������������������������45\n\nviii",
      "content_length": 3663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "■ Contents\n\nDesigning / Splitting Up by Example ������������������������������������������������������������������������������49\n\nThe Bookstore Home Page �������������������������������������������������������������������������������������������������������������������49\n\nThe Book Detail Page����������������������������������������������������������������������������������������������������������������������������52\n\nSummary ������������������������������������������������������������������������������������������������������������������������57\n\n■Chapter 5: Defining APIs ��������������������������������������������������������������������������������������59\n\nInterface Definition Languages (IDLs) ����������������������������������������������������������������������������59\n\nA Matter of Style: RPC versus REST �������������������������������������������������������������������������������60\n\nRPC-Style Interface Definition Languages ��������������������������������������������������������������������������������������������60\n\nRESTful Interface Definition Languages �����������������������������������������������������������������������������������������������60\n\nOur Stance ��������������������������������������������������������������������������������������������������������������������������������������������61\n\nDescribing APIs using Swagger ��������������������������������������������������������������������������������������61\n\nHosting Swagger Specification as JSON Schemas ������������������������������������������������������������������������������62\n\nThe Swagger User Interface �����������������������������������������������������������������������������������������������������������������65\n\nGenerating Code from Swagger Definitions �����������������������������������������������������������������������������������������68\n\nGenerating the Swagger JSON Specifications ��������������������������������������������������������������������������������������71\n\nSummary ������������������������������������������������������������������������������������������������������������������������74\n\n■Chapter 6: Consuming Your APIs �������������������������������������������������������������������������75\n\nWhat Are Client Libraries? ����������������������������������������������������������������������������������������������75\n\nWhat Not to Do ���������������������������������������������������������������������������������������������������������������76\n\nDon’t wait eternally ������������������������������������������������������������������������������������������������������������������������������76\n\nDon’t make the clients smart ���������������������������������������������������������������������������������������������������������������76\n\nService Orchestration �����������������������������������������������������������������������������������������������������77\n\nCaching in the Client ������������������������������������������������������������������������������������������������������78\n\nBuild Caching into Your Client Library ��������������������������������������������������������������������������������������������������78\n\nError Handling ����������������������������������������������������������������������������������������������������������������78\n\nTimeouts �����������������������������������������������������������������������������������������������������������������������������������������������79\n\nCircuit Breakers ������������������������������������������������������������������������������������������������������������������������������������79\n\nPerformance �������������������������������������������������������������������������������������������������������������������80\n\nUpdating Client Libraries ������������������������������������������������������������������������������������������������81\n\nix",
      "content_length": 3880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "■ Contents\n\nService Discovery �����������������������������������������������������������������������������������������������������������81\n\nSummary ������������������������������������������������������������������������������������������������������������������������82\n\n■Chapter 7: Optimizing Your APIs �������������������������������������������������������������������������83\n\nMeasuring Performance �������������������������������������������������������������������������������������������������83\n\nService Access Pattern-Based Improvements ���������������������������������������������������������������85\n\nThe Blessing and the Curse of Caching ��������������������������������������������������������������������������86\n\nCaching in the API Consumer ���������������������������������������������������������������������������������������������������������������86\n\nCaching in the API Provider ������������������������������������������������������������������������������������������������������������������86\n\nMake Your Service Work Less ����������������������������������������������������������������������������������������87\n\nUse the Standard ����������������������������������������������������������������������������������������������������������������������������������87\n\nFramework Support for HTTP Conditional GET �������������������������������������������������������������������������������������89\n\nCaching Proxies ��������������������������������������������������������������������������������������������������������������92\n\nSize Matters �������������������������������������������������������������������������������������������������������������������93\n\nResult Representations�������������������������������������������������������������������������������������������������������������������������93\n\nPartial Resources ����������������������������������������������������������������������������������������������������������������������������������94\n\nHTTP Compression �������������������������������������������������������������������������������������������������������������������������������95\n\nSemantic Paging �����������������������������������������������������������������������������������������������������������������������������������96\n\nOptimizing for Efficiency of Development and Support ��������������������������������������������������98\n\nTalk Less, Listen More ����������������������������������������������������������������������������������������������������99\n\nSummary ����������������������������������������������������������������������������������������������������������������������101\n\n■Part III: Development and Deployment ������������������������������������������� 103\n\n■Chapter 8: Development Environment and Workflow ����������������������������������������105\n\nThe Premise ����������������������������������������������������������������������������������������������������������������105\n\nThe Development Environment Wish List ���������������������������������������������������������������������105\n\nEase of Development Tool Setup ��������������������������������������������������������������������������������������������������������106\n\nEase of Application Setup �������������������������������������������������������������������������������������������������������������������106\n\nAutomation of Application Tests ���������������������������������������������������������������������������������������������������������106\n\nAvailability of Sample Data �����������������������������������������������������������������������������������������������������������������106\n\nUp-to-Date Dependencies ������������������������������������������������������������������������������������������������������������������106\n\nx",
      "content_length": 3788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "■ Contents\n\nFast Developer Machines �������������������������������������������������������������������������������������������������������������������107\n\nConcurrency of Development �������������������������������������������������������������������������������������������������������������107\n\nCollaborative Development �����������������������������������������������������������������������������������������������������������������107\n\nPrivacy of Development ����������������������������������������������������������������������������������������������������������������������107\n\nImmediacy of Change Feedback���������������������������������������������������������������������������������������������������������108\n\nEase of Service Generation�����������������������������������������������������������������������������������������������������������������108\n\nAccessibility of Documentation ����������������������������������������������������������������������������������������������������������108\n\nMaking the Wishes Come True �������������������������������������������������������������������������������������108\n\nAutomating Developer Machine Set-Up ����������������������������������������������������������������������������������������������108\n\nAutomated Application Set-Up ������������������������������������������������������������������������������������������������������������109\n\nMaking Testing Efficient with Common Tools �������������������������������������������������������������������������������������111\n\nShipping Sample Data with the Code �������������������������������������������������������������������������������������������������117\n\nMaintaining a Development Sandbox �������������������������������������������������������������������������������������������������120\n\nUsing a Rewriting Proxy for Edge Device Development ���������������������������������������������������������������������123\n\nCodifying Internal Standards via Generation Tools �����������������������������������������������������������������������������125\n\nMaking Documentation Search-Efficient ��������������������������������������������������������������������������������������������125\n\nSummary ����������������������������������������������������������������������������������������������������������������������126\n\n■Chapter 9: Testing with Services �����������������������������������������������������������������������127\n\nLayers of Microservice Testing �������������������������������������������������������������������������������������127\n\nThe Five Layers of Testing �����������������������������������������������������������������������������������������������������������������128\n\nThe Test Pyramid ��������������������������������������������������������������������������������������������������������������������������������133\n\nTesting Examples ���������������������������������������������������������������������������������������������������������134\n\nIn-Process Stubs for Component and Unit Testing �����������������������������������������������������������������������������134\n\nOut-Of-Process Component Testing����������������������������������������������������������������������������������������������������141\n\nExamples of End-to-End Tests ������������������������������������������������������������������������������������������������������������145\n\nContract Testing Examples �����������������������������������������������������������������������������������������������������������������147\n\nSummary ����������������������������������������������������������������������������������������������������������������������150\n\n■Chapter 10: Deploying and Running Microservices �������������������������������������������151\n\nAutomation Is Key ��������������������������������������������������������������������������������������������������������151\n\nContinuously Integrating Microservices �����������������������������������������������������������������������152\n\nxi",
      "content_length": 4014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "■ Contents\n\nA Brief History �������������������������������������������������������������������������������������������������������������������������������������152\n\nThe Advantages of Using CI Tools �������������������������������������������������������������������������������������������������������152\n\nUsing CI for Microservices ������������������������������������������������������������������������������������������������������������������153\n\nContinuous Code Delivery ��������������������������������������������������������������������������������������������154\n\nHow Do You Know You Are Doing CD? ������������������������������������������������������������������������������������������������154\n\nBreaking the Release Process into Stages �����������������������������������������������������������������������������������������154\n\nCD and Microservices �������������������������������������������������������������������������������������������������������������������������156\n\nTool Support for Continuous Delivery �������������������������������������������������������������������������������������������������156\n\nMaturing Code Preproduction ��������������������������������������������������������������������������������������159\n\nStaging as a Preproduction Environment �������������������������������������������������������������������������������������������159\n\nTool Support for Staging Environments ����������������������������������������������������������������������������������������������161\n\nSafer Releases ������������������������������������������������������������������������������������������������������������������������������������162\n\nAssigning Resources to Microservices ������������������������������������������������������������������������164\n\nTenets for Microservice Deployments ������������������������������������������������������������������������������������������������165\n\nOptions for Running Microservices ����������������������������������������������������������������������������������������������������166\n\nSummary ����������������������������������������������������������������������������������������������������������������������173\n\n■Part IV: Putting Everything Together ���������������������������������������������� 175\n\n■Chapter 11: Polyglot Services ���������������������������������������������������������������������������177\n\nDimensions of Change �������������������������������������������������������������������������������������������������177\n\nAdvantages of Using Different Languages and Technologies ���������������������������������������177\n\nSpecialization �������������������������������������������������������������������������������������������������������������������������������������178\n\nExperimentation ���������������������������������������������������������������������������������������������������������������������������������178\n\nEvolution ���������������������������������������������������������������������������������������������������������������������������������������������179\n\nNavigating the Pitfalls of Being Polyglot ����������������������������������������������������������������������179\n\nA Process for Adopting New Technologies �������������������������������������������������������������������180\n\nDocument All Findings ������������������������������������������������������������������������������������������������������������������������180\n\nStatement of Motivation and Goals�����������������������������������������������������������������������������������������������������180\n\nImplementation Planning ��������������������������������������������������������������������������������������������������������������������181\n\nGo-Live Criteria �����������������������������������������������������������������������������������������������������������������������������������182\n\nxii",
      "content_length": 3895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "■ Contents\n\nA Case Study: Moving books-service from Rails to Rust ����������������������������������������������182\n\nSummary ����������������������������������������������������������������������������������������������������������������������184\n\n■Chapter 12: Single-Client Application—Mobile and Web ����������������������������������185\n\nApplication Overview����������������������������������������������������������������������������������������������������185\n\nHome Page �������������������������������������������������������������������������������������������������������������������186\n\nFeatured Categories ���������������������������������������������������������������������������������������������������������������������������192\n\nUsers Should Never See Internal Errors ���������������������������������������������������������������������������������������������195\n\nCircuit Breakers ����������������������������������������������������������������������������������������������������������������������������������197\n\nBook Detail Page ����������������������������������������������������������������������������������������������������������197\n\nThe Shopping Cart Page �����������������������������������������������������������������������������������������������203\n\nExercises for You ����������������������������������������������������������������������������������������������������������206\n\nService Orchestration ���������������������������������������������������������������������������������������������������207\n\nPerformance Considerations ����������������������������������������������������������������������������������������208\n\nSummary ����������������������������������������������������������������������������������������������������������������������209\n\n■Chapter 13: Monitoring Your Services ���������������������������������������������������������������211\n\nMonitor All Services �����������������������������������������������������������������������������������������������������211\n\nWhat Do You Need to Monitor? �����������������������������������������������������������������������������������������������������������212\n\nFocus on Perceived Performance �������������������������������������������������������������������������������������������������������212\n\nAdhere to SLAs �����������������������������������������������������������������������������������������������������������������������������������213\n\nBusiness Metrics Are Important ���������������������������������������������������������������������������������������������������������213\n\nIdentifying Requests �����������������������������������������������������������������������������������������������������214\n\nLogging All Requests ��������������������������������������������������������������������������������������������������������������������������215\n\nReading Metrics ����������������������������������������������������������������������������������������������������������������������������������216\n\nAdjusting Alarms���������������������������������������������������������������������������������������������������������������������������������218\n\nSummary ����������������������������������������������������������������������������������������������������������������������219\n\n■Chapter 14: Operational Excellence with Microservices �����������������������������������221\n\nWhat is Operational Support? ��������������������������������������������������������������������������������������221\n\nEmbracing Ownership ��������������������������������������������������������������������������������������������������221\n\nxiii",
      "content_length": 3644,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "■ Contents\n\nCustomers First ����������������������������������������������������������������������������������������������������������������������������������222\n\nNo Code without an Owning Team ������������������������������������������������������������������������������������������������������223\n\nBoosting Team Cohesion���������������������������������������������������������������������������������������������������������������������223\n\nWhat It Means to Be On-Call ����������������������������������������������������������������������������������������224\n\nDefining the Role and Process for Being On-Call �������������������������������������������������������������������������������224\n\nThe On-Call Person in Relation to the Team ���������������������������������������������������������������������������������������227\n\nBeing On-Call in a Microservices Environment ����������������������������������������������������������������������������������228\n\nTracking and Scheduling Operations Support Work �����������������������������������������������������229\n\nA Lightweight Process Proposal ���������������������������������������������������������������������������������������������������������229\n\nFeatures to Look for When Picking a Trouble Ticket System ��������������������������������������������������������������230\n\nDashboards to Support Operations ������������������������������������������������������������������������������232\n\nOutage Etiquette �����������������������������������������������������������������������������������������������������������234\n\nEvery Pager Alarm is a Failure ������������������������������������������������������������������������������������������������������������235\n\nPlaybooks for Operational Support �����������������������������������������������������������������������������������������������������235\n\nWhat to Do When the Pager Goes Off �������������������������������������������������������������������������������������������������236\n\nSupporting Tools ���������������������������������������������������������������������������������������������������������������������������������237\n\nHow to Learn from Outages:—Post-Mortems ��������������������������������������������������������������238\n\nWhen Is a Post-Mortem the Right Tool? ���������������������������������������������������������������������������������������������238\n\nStructural Elements of a Post-Mortem �����������������������������������������������������������������������������������������������238\n\nPost-Mortem Ownership ���������������������������������������������������������������������������������������������������������������������240\n\nDefining and Advertising Team Policy ��������������������������������������������������������������������������240\n\nImpact Severity and SLAs �������������������������������������������������������������������������������������������������������������������240\n\nShare the Steps of Your Process ��������������������������������������������������������������������������������������������������������241\n\nSummary ����������������������������������������������������������������������������������������������������������������������242\n\nIndex ���������������������������������������������������������������������������������������������������������������������243\n\nxiv",
      "content_length": 3342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "About the Authors\n\nCloves Carneiro Jr. is a senior software engineer with over 15 years of experience creating software for companies in many fields. In recent years, Cloves was first introduced to the idea of distributed services at Amazon.com, where he was exposed to services with challenging scalability requirements for the Kindle and AWS teams. In 2001, he joined the amazing Engineering team at LivingSocial, and has focused on building microservices as scalable pieces of software to help improve the reliability and availability of the systems at the company. In his free time, Cloves enjoys spending time with his family in sunny South Florida, and is an avid—yet slow—triathlete. Cloves has also previously written for Apress, authoring Beginning Rails, Beginning Rails 3, and Beginning Rails 4.\n\nAfter abandoning his childhood dream of becoming a firefighter, Tim Schmelmer did the next best thing and became a software developer. He has spent the past 19.75 years extinguishing fires while working for technology consulting companies, telecommunications equipment manufacturers, building control systems for research telescopes, and selling things to people via a web browser and mobile apps. Tim found his love for building microservices systems while working at Amazon.com, and he has most recently been working on a team that does surprisingly little firefighting while building and maintaining LivingSocial.com’s core services platform. When he is not hacking away on his keyboard, or trying to be a good husband, father of two girls, and tamer of an overly energetic puppy, Tim loves to run, bike, and hike in the mountains near beautiful Boulder, Colorado.\n\nxv",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "About the Technical Reviewer\n\nTom Copeland is a Rails developer at LivingSocial.com. He has programmed in Ruby since 2004, consulted at InfoEther on Ruby on Rails applications since 2007, and received a “Ruby Hero” award in 2008. Before focusing on Ruby, he wrote two books about Java programming (PMD Applied and Generating Parsers with JavaCC, both from Centennial Books) and was named Oracle’s “Open Source Developer of the Year” in 2003.\n\nxvii",
      "content_length": 447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Acknowledgments\n\nWriting a book is never solely the accomplishment of the authors listed on its cover, and that is particularly true for the one you are holding in your hands right now. We would like to acknowledge that by expressing our gratitude to the people and companies that made this book possible.\n\nFirst of all, we would like to thank the team at Apress Media LLC for helping us through this process.\n\nIt all started with Ben Renow-Clark reaching out to us and planting the idea of writing another book in our heads. We are unsure how many times Nancy Chen, our coordinating editor, who did an amazing job getting all ducks in a row and chasing us for our content when chapter deadlines came approaching fast, must have cursed Ben for signing us on, though. We believe that the only person at Apress who might have had to show even more patience with us than Nancy must have been our development editor Chris Nelson, whom we owe many thanks for countless corrections, suggestions, requests for clarifications, and for pretending not to mind that neither of the authors is a native speaker of the English language.\n\nIf you find that reading this book has some positive bearing on your job in the field of software, then chances are that our technical reviewer Tom Copeland is largely responsible for that. Other than being a Ruby Hero (https://rubyheroes.com/heroes/2008), book author (https://pragprog.com/book/cbdepra/ deploying-rails), as well as generally an exceptional engineer with oodles of experience, Tom gave invaluable feedback and practical tips on how to improve this book so that it matters for practitioners.\n\nThank you also to Toby Clemson at ThoughtWorks for letting us use some of his excellent diagrams illustrating testing strategies in a microservices environment. His excellent article on this topic, published on Martin Fowler’s site, was the inspiration for much of the content in Chapter 9.\n\nCloves and Tim share very formative years of work experience at two corporations that have implemented microservices architectures: Amazon.com and LivingSocial. Both of us consider Amazon. com to be simultaneously the maverick and the gold standard for the architectural style that is now most commonly referred to as microservices. We had the privilege of working with some of the most talented engineers in the world in an environment that formed our understanding of how massively distributed services can be tamed and how they can provide value. If nothing else, then the rules of engagement we formulate in Chapter 2 are a direct result of our time at the world’s number one e-commerce store. We consider ourselves very fortunate to have had the opportunity to contribute to Amazon’s internal tech community.\n\nAlthough both of us overlapped briefly during our time as Amazonians, the first time we met in person\n\nwas when we both started at LivingSocial in late 2011. We have since worked together for the best part of five years, helping in LivingSocial’s move from a monolithic Ruby on Rails application to a microservices setup, breaking some world records on the way (http://www.guinnessworldrecords.com/world-records/most- online-vouchers-sold-in-24-hours). Not only have we learned many lessons during this—sometimes painful—transition, but we also have LivingSocial to thank for being supportive by allowing us to describe some of their systems in this book. Also, we would never have met the amazing Tom Copeland if it had not been for our time on the LivingSocial engineering team.\n\nAnother very gracious contribution to this book was LivingSocial’s sponsorship of Cloves’ and Tim’s attendance at AbrilPro Ruby (http://tropicalrb.com/2014/en/) and Rocky Mountain Ruby (http://rockymtnruby2014.busyconf.com/schedule) in 2014. The materials for, and feedback from, the workshops we held at both conferences laid the foundation for the ideas mentioned in this book. We will\n\nxix",
      "content_length": 3915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "■ aCknowledGments\n\nnever possess the stamina, grit, and organizational talent it takes to arrange internationally renowned software development conferences, so we tip our heads to the organizers of such events.\n\nLast but not least, both Cloves and Tim are family men with multiple children below the age of ten. We cannot imagine how any book project would ever see the light of day without the unwavering support of the authors’ families. Tim would like to thank Safora, his wife and friend of more than 10 years, for keeping the ship that is their family afloat while he was often physically, and even more often mentally, unavailable and preoccupied with finishing this book. Cloves would also like to thank his family, Jane, Noah and Sofia, who were always present and understanding of the effort required to write another book.\n\nxx",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Introduction\n\nAs you have surely deduced from its title, Microservices from Day One touches on many aspects of adopting microservices as the underlying principle of structuring your application.\n\nYour choice to read a book on microservices may have any of several different motivations. If you have heard the buzzwords and general hype surrounding the budding trend in the industry to move toward this architectural style, and now you want to find out more about what really hides behind the curtains, then this book is certainly for you.\n\nIf you are working on a team where the decision has already been made, or if you are exploring the option, to decompose your application in a fine-grained manner with many small, interdependent services to help with ease of scaling and maintainability, and you are looking for practical advice and best practices for how to get started, then this book will also deliver what you need. It is meant to serve as a guide for developers who have a desire to build robust and scalable online applications in smaller, more focused and manageable chunks, but do not know how to get started.\n\nAs the book’s title also indicates, you will get most the leverage out of this publication if you are in a situation where you are at liberty to design your business around microservices from the beginning. Microservices from Day One explains what microservices architecture are about, their advantages, how to avoid stumbling blocks along the way of implementing them, and the reasons why developers should think about using them when starting a new project.\n\nIt is complex, expensive, and at times frustrating to move a large, monolithic system to a microservices-\n\noriented approach when the monolith has reached a sizable code base and traffic load, or has become business-critical in its mission. We know this to be true from multiple years of professional experience dealing with such a metamorphosis. It is not unusual that companies have to declare technological bankruptcy on their existing monolith, and then engage in re-architecture projects that involve 95% of their software development teams simply to break such large systems apart into applications of more manageable size. One of the main arguments of this book is that employing microservices from an early stage of development not only helps you avoid such a scenario, but is also feasible, convenient, and cost-effective if you follow the advice and recipes laid out herein.\n\nBut even though this book does not specifically focus on questions of transitioning from a monolith\n\nto microservices, a reader who is faced with exactly that predicament might still find a large number of the concepts described in this book helpful and relevant.\n\nAnother property of this book is its emphasis on practicality. While some parts certainly explain\n\nconcepts in theory, priority is placed on showing these concepts in practice. This means that we will discuss sample code in the book, most of which is written in the Ruby programming language. This sample code is largely taken from an accompanying set of service and front-end applications that we developed while writing this book, simply to illustrate and put into practice most the theory discussed herein. These services together make up an imaginary bookstore business “_Books.com_”.\n\nFrom a content perspective, the book’s 14 chapters are organized into 4 parts. The first part discusses the basics of microservices architectures. Chapter 1 tries to answer the question\n\nof what microservices are, doing so by digging into the history of this style of building applications and looking at how they are implemented at companies that should count as early adopters. By Chapter 2, we are becoming much more opinionated when we are laying out a set of six fundamental rules that we see as necessary properties for any successful architecture based on microservices.\n\nxxi",
      "content_length": 3908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "■ IntroduCtIon\n\nPart II describes how microservices adoption will allow you to view your business as a set of APIs. Chapter 3 helps with the task of decomposing a problem domain into very basic services and also provides patterns for reaggregating information from these core microservices into secondary, more special- purpose services and applications. We show this using the practical example of our bookstore example, developing decomposition and aggregation patterns as we go.\n\nChapter 4 starts discussing the APIs as the interfaces exposed by the services in your business, with a special focus on explaining what all the properties are that together make up a high-quality design of your APIs. Here again we go into detail by show an example API from our bookstore application, and how it was designed.\n\nChapter 5 is a deep dive into how to use automated tools and IDLs to define, generate, explore and consume the APIs you build inside your business. We will again place high importance on showing real-life examples of how this can be done, using the fictional bookstore we designed for this book.\n\nThe main topics addressed in Chapter 6 are practical tips surrounding the dos and don’ts for designing and building client libraries for the services in a microservices environment. We list the concerns we believe need to be part of any decent client-side code that accesses any of your service resources.\n\nAfter the API basics that were discussed in Chapter 4 through 6, we close out Part II by presenting\n\nan in-depth exploration of API optimization strategies in Chapter 7. In this context, optimization should not be understood as restricted to performance aspects, but also taking into account client convenience, maintainability, extensibility and immediacy of change notification.\n\nIn Part III the focus is on the particularities of setting up development environments and deployment processes in a microservices infrastructure. In Chapter 8 we define a wish list of 12 items to be successful in a microservices environment, and we show which tools can be used to check off every one of them. Chapter 9 identifies and explains five separate layers at which testing can and should be done to make sure your microservices architecture is sound, and we show concrete examples and code for automated test scenarios that exercise one of the services for our fictitious bookstore business. Chapter 10 is dedicated to all aspects of automated delivery, deployment, and releasing of microservice units, and we show practical examples of tools and strategies to achieve continuous integration and delivery in a safe and reversible manner. Along the way, we define seven tenets for deployment and all the currently available options for assigning computing resources to the various services in your infrastructure.\n\nThe fourth and final part of this book serves as a potpourri of practical techniques for making it easier\n\nto run microservices in production. Chapter 11 discusses the benefits and potential stumbling stones of adopting a heterogeneous technology stack using a hands-on scenario, and then Chapter 12 describes in detail one of the consumer-facing applications in our bookstore example. In Chapter 13 we move on to describe which parts of your microservices architecture deserve monitoring and alarming treatment, explaining some of the available tools. The book concludes with Chapter 14 on how to breed excellence in operational support in a self-learning organization that is structured around ownership of microservices.\n\nOur aim was to have this book’s topics be relevant, its structure conducive to your learning, its pace and scope appropriate, and—most of all—its focus on practical examples immediately useful to your career and business.\n\nLast but not least, we hope that you enjoy reading the book as much as we loved writing it, which means\n\nthat you will have a blast!\n\nxxii",
      "content_length": 3907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "PART I\n\nService Oriented Architectures",
      "content_length": 38,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "CHAPTER 1\n\nMicroservices: The What and the Why\n\nThis first part of the book discusses what we mean when we talk about service-oriented architectures in general, and microservices in particular. We will spend some time explaining the benefits, as well as touching on potential pitfalls to be avoided, involved in adopting this architectural style.\n\nIn this chapter, we begin by showing the relevance of taking the microservices approach. After defining\n\nsome general terms like service, service-oriented architecture, and microservices, we list examples of companies that successfully use them in the wild, and we discuss some of the reasons why these companies have adopted this architectural pattern. Next, we will look at some of the stumbling blocks that you encounter when utilizing microservices. We conclude the chapter by discussing the advantages of starting out with a service orientation compared to beginning with a single, monolithic application.\n\nAll Successful Software Undergoes Change If your goal is to have a software application that is successful, there will be a point in the life of your application where it will have to undergo change.\n\nTaking just web applications as an example, one change that comes to mind is that you might need\n\nto add an interface for administrators of your application data. What if you decide to add a mobile version of your site (or native mobile apps) after its initial launch? What will you do when you need to add more business intelligence into the mix and run regular business (or even continuous, real-time) metrics reports? What about the point in time when your customers request public API access to your application?\n\nIn short, it’s not a question of if you will need to invest in changing your application, but when to make\n\nthe change and how expensive it will be. Investing in building your application to be based on microservices from the beginning is your best chance at making changes expedient, safe, and inexpensive.\n\nRemember Service-Oriented Architectures? In order to understand what microservices are all about, it seems necessary to discuss how they relate to their older cousin: an architectural style commonly referred to as service-oriented architecture (SOA).\n\n3",
      "content_length": 2241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nAlthough SOAs were all the rage in the early 2000s, there are a lot of often -conflicting ideas out there\n\naround what an SOA is really all about. Martin Fowler raises this point in a 2005 blog post,1 and his attempt to classify the prevailing opinions can be summarized this way:\n\n\n\nAn SOA is designed to expose software through web services, usually involving a common backbone—usually using HTTP—for an organization’s applications to work with.\n\n\n\nIt is meant to make the monolithic applications disappear, and to replace them with a set of core services that expose business functionality and data. Data and logic provided by the core services will be aggregated in separate user interfaces.\n\n\n\nFor many, an SOA necessarily involves an asynchronous message bus technology, which communicates business events via “documents” (sometimes called messages) between the systems involved.\n\nFor the purpose of this book, let’s define SOA as an architectural principle in which there is a bias to\n\nbuild systems through separately implemented, deployed, evolving, and maintained services.\n\nIn a set-up like that, any client-facing “front-end” applications then solely rely on a set of (back-end)\n\nservices which expose the functionalities that the front-end applications consume in order to do their jobs.\n\nWhat Is a Service? To us, the definition of a software service is very close to the Oxford American Dictionary (nontechnical) definition of any service, in that it is “a system supplying a public need.”\n\nSoftware services should serve the needs of one or more internal or external client applications. The\n\ninterface of such services should therefore be designed from a client perspective, optimizing for maximum usefulness for its API consumer(s).\n\nWhile ideally every service is different as it optimizes for its clients’ use cases, it can generally be said\n\nthat a service consists of a piece of functionality and its associated set of data.\n\nIf the service is also the authoritative and definitive source for the current state of the data it holds and\n\nvends, then we call such a service a system of record. An examples of services where this system-of-record test does not pass could be a service that aggregates data and functionality from other services, which in turn could themselves be the systems of record for this data (for example, in an attempt to improve the usability or performance of the entire system).\n\nWhat Are Microservices? It is hard to determine exactly who first used the term microservice and when. While it seems most likely that although the term first appeared on the scene around 2011, some companies adopted this style much earlier. One example is Amazon, which broke apart its previously monolithic Obidos application into many\n\nsmaller services. In a 2006 interview with Microsoft Fellow Jim Gray for ACM Queue,2 Amazon CTO Werner Vogels describes as a general “service orientation” what comes very close to our definition of microservices:\n\n“For us service orientation means encapsulating the data with the business logic that operates on the data, with the only access through a published service interface. No direct database access is allowed from outside the service, and there’s no data sharing among the services.\"\n\n1Martin Fowler, “ServiceOrientedAmbiguity,” July 1, 2005. http://martinfowler.com/bliki/ ServiceOrientedAmbiguity.html 2Werner Vogels, “A Conversation with Werner Vogels: Learning from the Amazon technology platform,” interview by Jim Gray. ACM Queue, Volume 4, Issue 4. June 30, 2006. https://queue.acm.org/detail.cfm?id=1142065. 4",
      "content_length": 3641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nFormer eBay and Google executive Randy Shoup summarizes3 microservice characteristics as having\n\nthree main features:\n\n\n\nThey have a very focused scope that concentrates on a very few functionalities (at times a single one), expressed by a small and also well-defined API.\n\n\n\nThey are all very modular, and independently deployable, essentially elevating the rules of software encapsulation and modularity from the program to the deployment unit level.\n\n\n\nFinally, microservices have isolated persistence, meaning that they do not share persistent storage, such as a common database.\n\nGiven the similarities to the earlier description of an SOA, how are microservices really different? Is the\n\nterm just a more modern, trendier name for the same service-oriented architecture?\n\nThe main difference we have found is that microservices have the common trait of decomposing a large application into a set of small, focused, versioned, and loosely-coupled services, which are very inexpensive to spin up and maintain.\n\nAn excellent definition of microservices is that of Fowler and Lewis,4 as an architectural style that\n\n\"… is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies.\"\n\nThis approach is a significant departure from old-school SOA implementations, which ended up being much more accurately described as a set of medium-size to large monolithic applications, stitched together via expensive, and often complex, enterprise service bus ( ESB) solutions. This often led to much more tightly coupled systems, where many of the interapplication contracts were not explicit, but rather implicit and hidden inside the ESB “glue layer.”\n\nBy contrast, microservice architectures tend to work with very explicitly published, small and rather simple contracts between the single service applications. Most successful microservice implementations apply Postel’s Law of being “conservative in what you do, be liberal in what you accept from others” to all services that make up the entire system.\n\nMicroservices Are Not “CORBA over HTTP” Another difference between traditional SOAs and microservice architectures is that the latter approach seems to have thus far avoided the pitfalls of advocating distributed objects.\n\nThe idea of distributed objects, which was very popular during the time when SOAs were first developed, is to try to hide the difference between objects that live in the same process as the client code, or remotely in a different process—whether that is running on the same CPU or on an altogether separate machine. Technologies such as CORBA or Microsoft’s DCOM would go to great lengths to hide these differences.\n\n3Randy Shoup, “From the Monolith to Microservices: Lessons from Google and eBay”, April 24th, 2015, Craft Con 2015, http://www.ustream.tv/recorded/61479577 4Martin Fowler and James Lewis, “Microservices: a definition of this new architectural term.” March 25, 2014. http:// martinfowler.com/articles/microservices.html.\n\n5",
      "content_length": 3429,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nThe problem with distributed objects is that they are based on a fallacy that invalidates the entire approach: the boundary between in-process and out-of-process may be hidden to the developer, but it still always exists. The implications of a remote call simply cannot be removed. Out-of-process calls are always slower than in-process calls, and their likelihood and ways of failing need to be addressed by a distributed system’s architecture.\n\nWell-designed microservice contracts tend to reflect this fact by not pretending to be “Remote Method Invocation over HTTP.” Whereas CORBA and similar RMI technologies tried hard to enable client applications to allow for very fine-grained calls, RESTful microservice APIs often favor coarse-grained, or even batch, endpoints to reduce the impact of the unavoidable latency introduced by out-of-process and HTTP calls, or the transfer of documents via lightweight ESBs.\n\nIndustry Adoption of Microservices While more and more companies are starting to migrate their monolith application infrastructure to a set of smaller, more manageable services, or are continuing to do so, some prominent earlier adopters stick out. Most likely the earliest large-scale adoption of what would now be called a microservices architecture\n\nwas started at Amazon.com during the very early part of the third millennium.\n\nIn the 2006 interview cited earlier, Amazon’s CTO Werner Vogels explains their evolution from a\n\nmonolithic web application called Obidos to a large-scale service-oriented architecture. Obidos, which held “all the business logic, all the display logic, and all the functionality” for Amazon’s web shop, had undergone years of extensions and performance tuning, until Amazon came to the conclusion in 2001 that its monolith- based architecture would simply not scale anymore. The same held true not just for their actual web application, but also for their efforts to optimize and scale their backing database resources.\n\nTo quote Vogels,\n\n“the parts that needed to scale independently were tied into sharing resources with other unknown code paths. There was no isolation and, as a result, no clear ownership.\"\n\nVogels goes on to disclose more details about the way and scale at which services are implemented at\n\nAmazon, mentioning that when a consumer hits Amazon.com, more than a hundred services contribute data to the page rendered.\n\nIt is also worth mentioning that Amazon.com does not necessarily expose the same APIs to its external customers as it exposes internally. While most internal services are streamlined to conform to an optimized version of WSDL (called Amazon Service Definition Language, or ASDL), externally exposed Amazon web services usually offer both RESTful, as well as WSDL-based, APIs in order to maximize customer convenience.\n\nAmazon’s process for developing and launching new features relies heavily on the use of small, isolated\n\nservices. Amazon has built an infrastructure in which services are so commoditized that they can be easily spun up, soft-launched, measured, and iterated upon, until the problem domain and the new feature’s business impact are well understood.\n\nAnother key component of Amazon’s service-oriented architecture is the company’s approach to operational support. Vogels describes this as: “you build it, you run it,” in stark contrast to a more traditional approach that erects organizational walls between software development and technical operations departments. This not only exposes developers to the operational aspects of their product, it also incentivizes them to build robust systems.\n\n6",
      "content_length": 3655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nAccording to Randy Shoup, former employee of both Google and eBay, both companies have also shifted significantly toward using microservices in the past years. In his talk at the 2015 CRAFT Conference cited earlier, Randy describes in great detail how quite a few large companies have migrated in three to five steps from a monolithic architecture to a current state that encompasses hundreds to sometimes thousands of microservices. The interplay between these services is not organized in a strict tier-based hierarchy. It is rather best represented in the shape of an ad-hoc direct graph, where the edges of communication evolve over time, based on the entire system’s changing requirements and use cases.\n\nRandy observes that in this graph, companies like Google, eBay, Twitter, Amazon and others who have\n\nbeen successful in adopting microservices have concentrated on standardizing the “edges, rather than the nodes” of their directed services graph. They unified communication (like network protocols, data formats, and specifying APIs in a unified way) and infrastructure support (for source control, configuration management, and monitoring and altering), rather than any particular programming language, application framework, or persistence mechanism. While it is essential that these communication and infrastructure choices are internally supported by excellent, automated tooling, there is usually no top-down mandate to use a particular tool chain. The result of what is actually in use is more like the outcome of a healthy competition for the most useful and expedient results.\n\nAccording to Randy, another essential part of a microservices architecture is that it has no up-front master plan or architecture it aims to implement. In his experience, the architecture is constantly in flux and evolves over time, adapting to the pressures of usefulness: if a service no longer proves to be useful to any client, it is retired due to lack of use.\n\nSimilar to Vogels’ points earlier, Shoup mentions that Google and eBay also place great importance on\n\nservice ownership from cradle to grave. While teams there generally have a high degree of autonomy over their technology choices, they also carry responsibility for the functionality, quality, performance, reliability, and maintainability of what they produce. Their work on a service is not done until the service is unused and fully retired.\n\nAn interesting difference he brings up between Google and eBay compared to Amazon is that the former two tech juggernauts appear to have something like an internal marketplace for services, where clients are charged (mostly in a fictional, internal currency) for their usage of other teams’ service resources. This often serves as an incentive to be more mindful of potential performance issues that a client’s usage can cause, leading to greater cross-company efficiency.\n\nAnother well-known company to bet on microservices early is Netflix. Adrian Cockcroft, in his London QCon talk,5 offers many insights into Netflix’ migration to microservices in the public cloud. Netflix is one of Amazon Web Services’ largest customers, and much of the tooling they developed in-house has been open- sourced under the moniker of NetflixOSS.\n\nMuch as Shoup reports from eBay and Google, Cockcroft describes that a service at Netflix is not done once the code is ready and deployed, but rather when the service is finished being useful, and hence can be retired.\n\nHe also delivers other valuable organizational insights about the Netflix internal structure. For example,\n\nhe explains that at Netflix, decisions are made first about what a particular service architecture should look like, and only then are team structures built out to best match an ownership structure around this architectural layout.\n\nCockcroft calls this approach the “Inverse of Conway’s Law.” He adds that much of why the resulting teams\n\nwork at a very high level of efficiency and creativity stems from the “high trust, low process” environment in which developers can operate. Efficiency there is greatly helped by the fact that all hardware is commoditized to such a degree that Netflix owns no data centers but relies entirely on cloud offerings of Amazon and (more recently) Google. This even holds true for its development-stage environments, so that developers will never have to run more service applications than the one they are currently working on modifying.\n\n5Adrian Cockcroft, “Migrating to Microservices,” QCon London. March 6, 2014. www.infoq.com/presentations/ migration-cloud-native.\n\n7",
      "content_length": 4627,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nThe trust invested in developers also comes with a price: much like Amazon, Netflix lets its software engineers “run what they wrote.” Every developer has production service root-level access privileges, but they also all participate in operational support and carry pagers for being on call, in case any operational issues arise.\n\nCockcroft’s talk also addresses some practical design choices that appear to have crystallized over the\n\ncourse of Netflix’ experience with implementing a microservices architecture. There appears to be no actual limit imposed on the number of services they run. They instead use rule-of-thumb guidelines to decide when to spin up a new service. One such guideline is that more mature code should be kept separate from innovative or unproven features.\n\nAnother rule about service granularity appears to be that each “verb” ( functionality) should be factored\n\ninto a separate service. Cockcroft quotes the example of a service whose sole purpose it is to use a given customer ID and movie ID as inputs to return a list of other movie IDs that should be recommended to the customer based on these inputs.\n\nThe Netflix approach to service versioning is to run several instances of the same service\n\nsimultaneously, but with distinct versions. Instances of older versions will only be shut down once all clients have been convinced that they need to migrate off such an existing, older version.\n\nOther defaults chosen at Netflix are to make service interactions non-blocking, using an Observable Set / Reactive model for communications. Netflix even open-sources its internal tool for this purpose, called RxJava (https://github.com/ReactiveX/RxJava). Use of a message bus at Netflix seems to be restricted to “fire-and-forget” messaging, where no response is needed for the sending client. Cockcroft cites the example of a “delete this from all caches” message that one service can send on a reliable message bus topic, for other applications to consume and act on appropriately.\n\nRegarding decisions about data storage implications of the CAP theorem, the guidelines at Netflix appear to favor “available when partitioned” over consistency, unless having data availability downtime has explicitly been mentioned as an acceptable trade-off in a service’s nonfunctional requirements.\n\nIn a May 2015 blog post,6 Martin Fowler coined the term microservices premium to describe the measures that an organization needs to take to address the added complexity introduced by distributing functionality across multiple services.\n\nNetflix has certainly paid this premium by building, and additionally open-sourcing, many of its\n\ninternal tools for automating and simplifying the additional overhead incurred by employing a large service- oriented architecture. Netflix uses Karyon as a basis for all services to be developed, and Zuul as an edge service component that fronts all of its applications. Edda serves as a change-tracking tool for Netflix’ AWS server configurations, and Eureka is a load-balancing AWS service registry. To further enable reliability and resilience, Netflix also developed Hystrix, a Java library to help placing circuit breakers7 around a service’s points of remote system access. For a full list of Netflix’ open-source tools, see https://netflix.github.io/.\n\nThe engineering team at SoundCloud (https://soundcloud.com/) made a similar move from a monolithic Ruby on Rails application to microservices written in Clojure, Scala, and Ruby. Phil Calçado, formerly director of engineering for SoundCloud’s core team, described their experiences in another August 2015 QCon talk.8\n\nThe first feature that was pulled out of the monolith was search, but even at this very early stage they started wondering whether they were ready for the paradigm shift to microservices, which he calls a “microservices explosion,” where a huge amount of new services were added within a matter of weeks.\n\nOne way they prepared was to hone their provisioning process. SoundCloud runs its own data-centers, but started mimicking the structure of Heroku (https://www.heroku.com/), using the same toolchain that Heroku is built on. As a result of running on a physical data center, SoundCloud encountered issues resulting from the nonoptimal resource sharing of co-hosted applications, paired with “naive scheduling.”\n\n6Martin Fowler, “Microservice Premium.” May 13, 2015. http://martinfowler.com/bliki/MicroservicePremium.html. 7Martin Fowler, “CircuitBreaker” March 6, 2014 . http://martinfowler.com/bliki/CircuitBreaker.html. 8Phil Calçado, “No Free Lunch, Indeed: Three Years of Microservices at SoundCloud,” QCon. August 2015. www.infoq. com/presentations/soundcloud-microservices.\n\n8",
      "content_length": 4764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nOther choices they made during the migration to an SOA were to move applications written in Ruby,\n\nNode.js, and Go over to JVM-based languages like Clojure, Scala, and JRuby.\n\nSoundCloud started employing Tumblr’s Collins tool (https://tumblr.github.io/collins/) for automating their rapid provisioning process, packaging all their microservices into Docker (https://www. docker.com/) containers.\n\nSoundCloud’s telemetry approach was also home-built, and resulted in a tool called Prometheus\n\n(http://prometheus.io/), which had integrated alarm facilities via PagerDuty (https://www. pagerduty.com/). For operational support, SoundCloud made sure that all applications have uniformity of administration endpoint URL schemes, so that on-call engineers have an easy time determining, or modifying, application status.\n\nSoundCloud also made sure to integrate request tracing into their API infrastructure, so that the\n\ncomplete graph of service dependencies can be determined and visualized more easily.\n\nTo automate deployment, SoundCloud adopted a continuous integration approach, combined with continuous deployment directly to Docker containers that are pushed to production servers in SoundCloud’s data centers. SoundCloud’s main challenge was to iteratively adopt an SOA and its necessary preconditions.\n\nWhy Use Microservices? The list of reasons for employing microservices is long, and before summarizing the main advantages from our own viewpoint, we will first summarize the reasons each of the companies and consultants mentioned earlier have given.\n\nAdvantages as Seen by Industry Experts Martin Fowler’s “Microservice Trade-Offs” article9 distils his view of the advantages down to three points:\n\n\n\nStrong Module Boundaries: In his view, designing and implementing any piece of software can benefit from applying a modular structure. Divvying up your business’ IT architecture into microservices reinforces such modular structures, which become increasingly important as a company’s development team grows. Organizations can form tight-knit teams around the ownership structure of a set of small services, which leads to clearer interfaces with stronger boundaries, not just between the teams, but as a consequence also between the software (sub) systems these teams are building. Decoupling subsystems in a monolith is of course also possible, but it is rarely done, and even less often upheld for any length of time, because it is all too easy to violate such module boundaries in a monolith. In Fowler’s words, violating subsystems boundaries “… can be a useful tactical shortcut to getting features built quickly, but done widely they undermine the modular structure and trash the team’s productivity. Putting the modules into separate services makes the boundaries firmer, making it much harder to find these cancerous workarounds.” Another advantageous side effect of erecting service- based boundaries is that, if done right, the data storage and access should also be decentralized and become entirely opaque. All access to data will be at the “service model” layer, where the actual persistence structure is hidden behind the services’ exposed APIs.\n\n9Martin Fowler, “Microservice Trade-Offs,” July 1, 2015. http://martinfowler.com/articles/microservice- trade-offs.html.\n\n9",
      "content_length": 3343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\n\n\nIndependent Deployment: Fowler praises the benefits of a Continuous Delivery approach to software development.10 Where releasing software used to be a “once a quarter,” or—at best—a monthly event, many teams using microservices have trimmed down their release cycles to happen several times per day. The main competitive advantage that results from the ability to deploy at a high frequency comes from the increased speed of responding to the market, or offering a new feature faster than your competitors. Deploying large monoliths makes such a deployment frequency very hard to maintain, as any botched change to such large code bases can mean that the entire deployment will fail. If, on the other hand, microservices are designed to be autonomous, and hence independently deployable, components, then the risk of a complete system failure due to a deployment of any of its (mircroservice-encapsulated) parts is greatly reduced.\n\n\n\nTechnology diversity: In Fowler’s view, the main advantages of this point are not only the more obvious greater ease to “choose an appropriate tool for the job”: it is evident that, in a microservices environment, you can more easily introduce, and experiment with, more diversity in programming languages, frameworks, and data stores. Another often overlooked advantage is the ability to take a more fine-grained, less all-or-nothing approach to upgrading libraries and other common dependencies. As Fowler puts it: “Dealing with library versioning issues is one of those problems that gets exponentially harder as the code base gets bigger.”\n\nWerner Vogels’ arguments for why Amazon switched to what today would be called a microservices\n\narchitecture are as follows:\n\nDevelopers have a deeper feeling of software ownership and control, which leads to more innovation and focus on system stability and performance than was possible when all code lived in a single monolith.\n\n\n\nA fine-grained, service-oriented architecture gives Amazon a “level of isolation” between each of its pieces of software that allows the company to build its components independently, and hence more quickly than before.\n\nNew ideas can more easily be implemented, launched, tried out, and potentially discarded, both at a developer level and in operational support.\n\nWhile the number and (lack of) structure of microservices employed at Amazon may appear chaotic when viewed from close up, they make sense at a larger scale. They are simply an acknowledgement of the fact that large-scale distributed systems possess “many organic and emerging,” and hence nondeterministic, properties. These nondeterministic properties are not seen as problematic at Amazon, but rather as a fact of life that helps their innovation cycles.\n\n\n\nAmazon now considers it a competitive and strategic advantage to be able to build large, complex applications via composition of “primitive” services. Vogels claims that Amazon can scale its “operation independently, maintain unparalleled system availability, and introduce new services quickly without the need for massive reconfiguration.”\n\n10Martin Fowler, “Continuous Delivery,” May 31, 2013. http://martinfowler.com/bliki/ContinuousDelivery.html.\n\n10",
      "content_length": 3246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nUsing a microservices architecture also provides organizational advantages. Teams at Amazon are created to form around a set of services they build and own cradle to grave, making the owners “completely responsible for the service—from scoping out the functionality, to architecting it, to building it, and operating it.” Amazon employs a “you build it, you run it” approach to software development, exposing developers to the needs of production support for all the software they launch, as well as to the customers (be they internal to Amazon or external) that rely on the functionalities provided by the respective service. The resulting customer feedback loop is a key element for improving the service’s “quality of the service.”\n\nRandy Shoup quotes many points that he sees as disadvantages of running a business on top of a monolith:\n\n\n\nAs a company’s software development team grows, coordination of the developers becomes increasingly challenging. Developers will start to step onto each other’s feet, and become much less productive as build-times grow with the size of the application.\n\nWhile it’s not impossible, it is much harder to enforce well-modularized software design in a monolith, where all code eventually has direct access to all resources without having to pass through a layer of abstraction. On the other hand, dividing a system into separate services with well-defined interfaces and resources that are owned exclusively by such services is a very potent facilitator for enforcing modularity.\n\nMonoliths leave very little room for adaptable scaling strategies. If all code lives in a single, large application, then the only way to add performance is via vertically scaling the resources providing the application. While it is again possible, breaking up a single, underlying data store into well-modularized units takes much more discipline in a monolithic system. The database easily becomes a point of strong coupling between the various parts of the application, and tuning a database for all the different use cases quickly becomes impossible.\n\nHaving a single unit of deployment for your system makes every change, however minute, carry an enormous risk for your business, as the entire application can stop functioning because of a single botched code change. Additionally, times for deploying, and potentially reverting, code to the production environment have the potential to grow to a point where they become a burden and productivity impediment for the entire business.\n\n\n\nShoup similarly and inversely lists the advantages of adopting microservices as follows:\n\n\n\nEach single service making up the application is a much smaller piece of the entire system to understand, and to reason about.\n\n\n\nServices become independently deployable, thereby reducing risk and speed of deployments. Additionally, each of the services becomes testable in isolation.\n\n\n\nScaling services individually allows for more options to choose from when the need to performance-tune the application arises.\n\n\n\nLarge-scale organizations like Google and Amazon, which are trying very hard to optimize for development velocity, have many projects moving in parallel, with little to no coordination overhead. Organizing their teams and business capabilities around small services enables this desire to move faster than many of their competitors of a similar scale.\n\n11",
      "content_length": 3427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nAdrian Cockcroft has three main arguments for microservices:\n\n\n\nThe removal of much friction from product development cycles increases the speed of the business as a whole.\n\n\n\nAs a staunch proponent of cloud-based microservices architectures, Cockcroft points out that adding horizontal scaling resources can be completed extremely quickly; when done right, scaling up in the cloud can be done within a matter of minutes.\n\n\n\nSpeed and availability of the overall system are improved, while at the same time cost, size, and risk of software development to the business are reduced. One enabler is the much shortened feedback loop for code that is deployed via small, isolated services, which means the overall rate of change to the system can be increased relatively safely. As Cockcroft puts it, the “Observe, Orient, Develop, Act” loop is shortened.\n\nAccording to Phil Calçado’s blog-post about SoundCloud’s move to microservices, its motivation “had\n\nto do much more with productivity than pure technical matters.”\n\nWhile implementing the “Next SoundCloud” project on top of an existing Ruby on Rails monolith\n\n(lovingly dubbed “the mothership”), they found themselves mostly in scaling issues in which their team processes posed the largest bottleneck to their velocity. Changes to their monolithic code base were so risky, and had caused so many large-scale outages, that they introduced a process of mandatory, intensive code reviews for pull requests. Reviewing such changes took an expert in the existing code base often several hours of their time. In Calçado’s words: “Larger changes required a lot of time to be reviewed and, due to the spaghetti nature of our Rails code, were very risky. People avoided these big Pull Requests like the plague.”\n\nAnd even after such intensive reviews, there remained a significant risk of causing unforeseen negative\n\nside effects. To quote Calçado again: “Turns out that the monolithic code base we had was so massive and so broad no one knew all of it. People had developed their own areas of expertise and custodianship around submodules of the application.”\n\nSoundCloud found that their switch to smaller, independent deployment units, organized as services\n\nwith specific focus on a small set of functionalities, helped them overcome their process scaling issues, as well. They built teams with true system ownership around these smaller services: “These teams were explicitly told that they had full ownership over the modules they owned. This meant that they would be on call for any outages related to those, but would also have the freedom to evolve these in any shape they thought reasonable. If they decided to keep something in the monolith, it would be their call. They were the ones maintaining that code anyway.”\n\nOur Reasons to Adopt a Microservices Architecture Our personal experiences with the advantages of microservices are very close to what many of the experts we’ve seen cite as reasons to employ them.\n\nWhen done right, services have much smaller code bases than monolithic applications. They are therefore easier to maintain, as they are easier to read, reason about, and to independently maintain, upgrade, and evolve.\n\nUsing services can be compared to employing the “single responsibility principle” of object-oriented\n\ndesign on an application design level. Services implement a self-contained, well-defined and documented set of functionalities, which they expose only via versioned APIs.\n\nThey can more easily be designed to be a true system of record for all data they access from a data\n\nstore (such as an underlying database). No other service or application has direct read or write access to the underlying data store. This enforces true decoupling, while at the same time achieving a high level of transparency of the implementation details of data storage from information access.\n\n12",
      "content_length": 3917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nIn most cases, the use of services also makes it easier to scale your application. For one, you can hone the design of your services to be scaled by their traffic pattern: read-intensive API endpoints can be cached independently from write-intensive functionality. You can also more naturally separate cache expiry based on the tolerance for staleness of information based on your individual endpoints, or even by particular clients requesting information (for example, information about the descriptive text on any given inventory item should in most cases have a higher tolerance for staleness than the quantity of the item left to be purchased.)\n\nIn Chapters 6 and 7 of this book, we will show how to implement such caching strategies client-side, as well as taking advantage of the HTTP protocol’s conditional GET facilities service-side. You will see how this can be paired with the use of reverse proxies, precalculating results service-side, as well as employing dedicated database read-only replicas for resource-intensive queries.\n\nPotential Pitfalls and How to Avoid Them It would be irresponsible and almost reckless to pretend that microservices are a silver bullet without any potential challenges or trade-offs. This section will try to summarize the trade-offs and concerns mentioned by the experts we have previously quoted, and it will conclude with our own views on impediments to introducing microservices.\n\nMartin Fowler is the expert who has been most explicit in discussing some of the trade-offsa that come\n\nwith the use of microservices:\n\nDistribution: Distributed systems are harder to program, since remote calls are slow and are always at risk of failure.\n\n\n\nEventual Consistency: Maintaining strong consistency is extremely difficult for a distributed system, which means everyone has to manage eventual consistency.\n\nOperational Complexity: You need a mature operations team to manage lots of services, which are being redeployed regularly.\n\nIn addition to these technological considerations, Fowler and his colleague James Lewis agree that there are also many political factors in a company’s surrounding culture and environment that can affect the chances of a successful microservices adoption. We will later explain our own experience and the cultural landscape that is needed for such an approach to thrive.\n\nPhil Calçado agrees with Fowler and Lewis on many points. In his August 2015 QCon talk, cited in the\n\npreceding section, he emphasizes the need for any organization using microservices to have rapid resource- provisioning, basic monitoring and telemetry, and rapid deployment in place.\n\nParticularly important to him is the monitoring and alarming component. In his words: “Tracking down\n\nissues in a monolith is easier … it’s in some component of the monolith.” When trying to find the root cause of an issue in a distributed environment, it is much harder for any operational support personnel to figure out where in the entire system a problem that has arisen is rooted. Calçado suggests that this be addressed by unifying dashboards for the monitoring data of all services and apps involved, to more easily identify which services were acting up.\n\nAnother hurdle is to aid application developers during their implementation and test efforts.\n\nDevelopment becomes challenging and cumbersome in an environment where the application you work on is never run by itself, because it has dependencies on many other services. SoundCloud decided to address this by having all applications ship as Docker images that could be installed on a single server, or even a developer’s laptop.\n\naMartin Fowler, “Microservice Trade-Offs”, July 1, 2015, http://martinfowler.com/articles/microservice- trade-offs.html.\n\n13",
      "content_length": 3797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nIn his “Microservice Prerequisites” article,11 Fowler also lists ways in which a mature organization can address the potential drawbacks via good planning. He sees three main conditions that need to exist in any successful implementation of a microservices architecture:\n\n\n\nRapid provisioning: while he thinks that companies can gradually approach a full automation of such expedited server provisioning, he is convinced that any organization that is serious about microservices adoption will need to get to a state where new servers can be fired up in a matter of hours. Hosting such servers in a (private or public) cloud environment will make this more natural in his view, but he says that this can also be achieved without a full switch to cloud computing.\n\n\n\nBasic Monitoring: Because fully simulating or testing an environment where often hundreds of independent service components interact is very difficult, he considers it “essential” to have monitoring and alarming in place to quickly be notified about any issues that impact the production environment. Such monitoring should not be restricted to technical problems (like errors or drops in availability or performance), but should extend to business metrics (such as order drops, revenue decrease, or click-through rates).\n\n\n\nRapid application deployment: when the number of services in an organization increases significantly, the need for very short deployment (and rollback) cycles arises, both for the test, as well as for the production stages. This usually involves automated, or even continuous, deployments that can change the code running in any particular stage in less than an hour. Again, Fowler concedes that some initial adoption phases can tolerate manual processes for application deployments, but he also says that full automation should be a near-term goal.\n\nFowler makes the point that especially monitoring and fast deployments go hand in hand:\n\n“This collaboration is needed to ensure that provisioning and deployment can be done rapidly, it's also important to ensure you can react quickly when your monitoring indicates a problem. In particular any incident management needs to involve the development team and operations, both in fixing the immediate problem and the root-cause analysis to ensure the underlying problems are fixed. … There’s also the shift to product centered teams that needs to be started. You’ll need to organize your development environment so developers can easily swap between multiple repositories, libraries, and languages.\"\n\nRandy Shoup’s talk mentions a good list of pitfalls of microservices, and he also adds occasional tips on\n\nhow to avoid them.\n\nMuch like Fowler, he explains that while each small service itself becomes simpler and more intuitive to understand, coordinating the simple parts becomes complex, and understanding the whole distributed system and its information flows becomes more complex than in a monolith. Additionally, the introduction of network latencies between the services needs to be considered, and addressed by the development processes and standards employed in an organization.\n\nA switch to microservices also often results in the active use of hundreds of code repositories. That number is large enough to justify spending time on devising a strategy for managing them in a unified way; for example, to address refactoring concerns across such a large number of repositories.\n\n11Martin Fowler, “Microservice Prerequisites,” August 29, 2014. http://martinfowler.com/bliki/ MicroservicePrerequisites.html.\n\n14",
      "content_length": 3606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nAs the many successful examples of microservices adoption prove, the problems we’ve just seen can certainly be managed, but companies will need more sophisticated tooling to address the challenges than was previously needed in a monolithic environment.\n\nShoup continues by defining what he calls “microservices anti-patterns\":\n\n\n\nThe Mega-Service: This is an instance of failure to adopt the right granularity for a service extraction. An example of this anti-pattern is a service application that hosts too much functionality, or too many areas of responsibility, to maintain the advantage of a code base that can be easily understood. But a service that has too many up- or downstream dependencies can also be seen as an instance of this anti- pattern, as it counteracts the advantage of greater stability and availability of the overall system architecture.\n\n\n\nShared Persistence: Having several services or applications share the same data store “under the hood” breaks the isolation advantages that switching to a loosely coupled collaboration of small services promises to bring. Even if the external interfaces of these coupled services are distinct, tying them back together at the database level leads to near-invisible coupling.\n\n\n\nLeaky Abstraction Service: In a well-designed service, its interface needs to reflect the client’s needs, so the API model exposed should cater to its customers’ needs. Implementations of the leaky abstraction service anti-pattern, on the other hand, expose an API design “from the internals outward,” thereby leaking the underlying persistence model of the API provider’s constraints instead.\n\nAnother concern Shoup raises is that in his experience, companies that have not reached sufficient size will not benefit from a microservices architecture. In his view, there are certain “stable points” in the size of companies as they evolve. The first stable point is when all employees still “fit around a conference table.” The next stable size point is twenty to twenty-five people on the payroll, and the step after that will be when the number of employees reaches around 100 people. Shoup’s advice is to switch to microservices when the second stage (of 20–25 people) is reached.\n\nShoup lays out his rationale for this advice in an interview with InfoQ:\n\n“Google and Amazon are very intentionally trying to optimize for velocity of large-scale organizations, which means lots of things moving in parallel with little coordination. They behave like 1,000 tiny companies rather than one monster company and that is why those companies versus a bunch of even larger organizations, move fast and the other one do not. But in the case of smaller organizations, if you are all one team, do not subdivide.\"\n\nAdrian Cockcroft’s “Migrating to Microservices” QCon talk does not reveal much information about his concerns regarding any potential pitfalls with microservices adoption. The main take-away from his talk appears to be that he whole-heartedly supports commoditization of server resources in the cloud, as opposed to owning and maintaining your own data centers. To summarize his advice in his own catch phrase: “Don’t be a server hugger!”\n\nWerner Vogels’ interview also only touches on a very few points that lay out the challenges of adopting a\n\nlarge-scale SOA:\n\nUnified service access: Vogels makes the case for a single, commonly adopted service access protocol. It is Amazon.com’s experience that such a mechanism is a precondition to easily aggregate services, and to enable monitoring, request tracking, and other more advanced “infrastructure techniques.” Amazon implements this in its home-grown ASDL standard.\n\n15",
      "content_length": 3718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nDeveloper productivity: A fair amount of up-front thought should be invested to ensure efficiency of feature development in a large SOA architecture. Amazon addresses this by making sure that developers only ever need to run locally the code bases for the particular services they are currently trying to modify or extend. All other system dependencies run in a development-stage server fabric, which is also maintained and supported 24/7.\n\nQuality assurance: integration testing in a widely distributed environment becomes a large-scale challenge for an organization’s quality assurance measures. To quote Vogels:\n\n“If you have this decentralized organization where everybody is developing things in parallel, how can you make sure that all the pieces work together as intended, now and in the future? An important part of that is, for example, testing. How do you test in an environment like Amazon? Do we build another Amazon.test somewhere, which has the same number of machines, the same number of data centers, the same number of customers, and the same data sets? These are very hard questions, and we have no simple answers. Testing in a very large-scale distributed setting is a major challenge.\"\n\nOur own experience from working at Amazon.com in the years after Vogels’ interview was that the company has heavily invested in doing just what Vogels alludes to above: production-like server fabrics have been spun up and populated with realistic test data, which a sizeable QA team uses to verify (automatically, as well as manually) larger software changes before they are deployed to the production stage.\n\nOur own views on pitfalls to avoid, and what measures to implement to counteract them, overlap with\n\nmany of the experts quoted in this chapter. Martin Fowler’s and Phil Calçado’s points about the need for monitoring and alarming approaches are particularly necessary, based on our own extensive experience in working in SOA environments.\n\nIt is also important to realize that working in a truly distributed environment introduces a large number of new failure modes, which include but are not limited to handling dependency timeouts or downtime. The need to avoid cascading failures of any system, which can be easily caused by any of the dependencies being unavailable or slow, must be engrained into every developer’s workflow and thinking.\n\nAnother absolutely essential point for us was touched on by Werner Vogels: the need to unburden the individual developer of the complexity and inefficiency involved in running the entire system locally. Most changes are to be made only to a small portion (usually, a single service component) at any given time, so requiring any developer to set up, provision, or run any more than just the component(s) under change means continual grief and friction during the development cycle.\n\nWe do, however, explicitly disagree with Randy Shoup and others in that we do not think that\n\nmicroservices requires a certain level of company or development team size; after all, this book is dedicated to adopting the approach that it is not just possible, but actually preferable, to start out with a microservices architecture.\n\nRandy’s description of “microservices anti-patterns” hits the nail on the head, though. In our own experience, the most harmful of these is the “Shared Persistence” anti-pattern. Applying this defeats the entire purpose of splitting large applications into smaller decoupled parts, precisely because the resulting parts are no longer truly decoupled: they are tied together by having to adhere to the same underlying database schemas and—potentially even worse—shared data access model logic.\n\n16",
      "content_length": 3723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\nThe second most dangerous anti-pattern to employ is Shoup’s “mega-service.” Service repositories of that kind are often born out of an irrational fear (or lazyness) of being potentially “slowed down” by spinning up separate code repositories and deployment units. This approach defeats the fundamental advantages of applying a “single responsibility principle” at a service application level, robbing the implementers of the chance to build a self-contained, easy-to-understand, and inexpensive-to-maintain service code base.\n\nThe rest of this book will present practical tips and examples for handling these measures.\n\nWhen Should I Start with a Microservices Architecture? You should not be surprised to find that we strongly recommend employing microservices from a very early stage in your application: after all, you have bought a book entitled Microservices from Day One.\n\nIf you are looking for more information about why other practitioners advocate the approach of starting\n\na project with a monolith before deciding whether to switch to microservices, we recommend Martin Fowler’s “Monolith First” blog post.12\n\nAlso posted on Fowler’s treasure trove of a site is an article by Stefan Tilkov that takes the exact\n\nopposite stance, titled “Don’t start with a monolith.”13 The following points from this article very accurately summarize our own experiences and the reasons we recommend a “services first” approach:\n\n\n\nTilkov states that the point at which you start building a new system is also exactly the point in time when good design practices demand that you think about “carving it up into pieces,” in order to build them as decoupled from each other as is possible. Postponing such system modularization to any time later in the lifecycle of a system should be strongly discouraged, because “you make it hard—or at least harder—to do the wrong thing: Namely, connecting parts that shouldn’t be connected, and coupling those that need to be connected too tightly.\"\n\n\n\nTilkov’s experience that it is a challenging task , and sometimes well-nigh impossible, to divvy up an existing monolith into coherent modules entirely matches our own experience. When you begin developing in a monolith, each part will access shared domain logic and shared data access model, and assume that database transactions are always a possibility. This leads to very tight coupling inside the application. He mentions that owners of a monolith are “tempted to assume there are a number of nicely separated microservices hiding in your monolith, just waiting to be extracted. In reality, though, it’s extremely hard to avoid creating lots of connections, planned and unplanned. In fact the whole point of the microservices approach is to make it hard to create something like this.”\n\nHe concedes the “monolith-first” camp’s argument that one needs to know the problem domain very well before being able to correctly break it down into subdomains small enough constitute functionalities for microservices. The main reason is the difficulty of making significant changes in the system composition. As he puts it: “Refactoring in the small becomes easier, refactoring in the large becomes much harder.”\n\n\n\nTo him, “the ideal scenario is one where you’re building a second version of an existing system.” This observation also coincides with our own experience: we recommend building a proof-of-concept first to understand the business domain well enough. But, as any proof-of-concept, such a prototype needs to be entirely considered a throw away effort, and not “production-worthy.” The second system can, and should, be built with services in mind.\n\n12Martin Fowler, “MonolithFirst,” June 2, 2015. http://martinfowler.com/bliki/MonolithFirst.html. 13Stefan Tilkov, “Don’t start with a monolith,” June 9, 2015. http://martinfowler.com/articles/dont-start- monolith.html.\n\n17",
      "content_length": 3913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Chapter 1 ■ MiCroserviCes: the What and the Why\n\n\n\nTilkov also argues the counter-intuitive point that developing in more than one code base actually increases efficiency and the speed of tangible results. His point is that microservices are an enabler for parallel development, because they establish “a hard-to-cross boundary between different parts of your system,” and hence are a forcing function for developers to discuss interfaces up-front. The parts that are being developed in parallel can also be delivered independently from each other, so that the feedback loop is shortened.\n\nMoreover, Tilkov states that concentrating on a microservices-based approach, where each subsystem “with its own development, deployment, and delivery cycle, and (the possibility of) its own internal architecture, is a very powerful concept that can help you to deliver a system in the first place.”\n\nThe main point of this book is to show that adopting a microservices-based approach from the\n\nbeginning does not have to be much more expensive than just going the ordinary (monolithic) “one app to rule them all” way.\n\nIn our view, the investment is very comparable to the “write tests or not?” question: investing in repeatable, automated pre-launch testing will pay off almost immediately by sparing companies the cost of addressing bugs found post-release.\n\nSimilarly, starting out with small, more focused services in the early phases of your applications life cycle will prevent you from involving very large parts of your engineering resources in ripping apart and rewriting your platform in a year from now, or so.\n\nAnother advantage to starting out with microservices is that an organization will be forced very early in a system’s lifecycle to address the infrastructure concerns that are prerequisites to succeeding with such an architecture. Making an investment in development, testing, continuous integration and deployment, as well as monitoring and alarming strategies in the early phases will make the total amount of time and resources invested smaller, and the organization will reap the efficiency benefits of these measures much earlier.\n\nSummary In this chapter we laid out what services are and what a service-oriented architecture (SOA) is. We discussed what microservices are, and also what specifically they are not. We looked at four technology industry juggernauts (Amazon, Netflix, Google, and eBay), and discussed their respective reasons for relying on microservices, as well as the advantages such adoption brings in our view.\n\nWe moved on to describing some of the potential pitfalls that result from distributing systems at a large\n\nscale, and suggested remedies, again based on what industry leaders suggest, and what our experience tells us. We closed the chapter with an appeal to start using microservices from the earliest stages of your application development, having mid- to long-term efficiency and effectiveness in mind.\n\nThe next chapter will introduce some ground rules and best practices that are going to make it easier for\n\nyou to get to such an architectural style once you are ready to adopt it.\n\nThen, in part II of this book, we will look at ways and guidelines around decomposing your business application into microservices, and how to design, define, consume, and finally optimize your services’ APIs.\n\nThe third and final part of this book will focus on the development environment, deployment\n\ninfrastructure, testing strategies and monitoring & alarming aspects of running a microservices architecture. All parts will be tied together by a set of example services implementing a fictitious online bookstore.\n\n18",
      "content_length": 3660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "CHAPTER 2\n\nRules of Engagement\n\nIn this chapter, we’ll go over what we call our “Rules of Engagement,” which is a set of principles and best practices we find valuable to follow to ensure an efficient microservices environment. It would be correct to state that this entire book is a collection of what we consider best practices; but the principles we outline in this chapter are a foundation of many of the tools and design ideas we will discuss throughout the book. We have six Rules of Engagement for successfully building applications in a microservices environment. Those rules are based on our years of experience, and are phrased from a developer’s point-of-view.\n\nAs with most rules, there may be cases where you will have a very good reason to move away from them, which is completely acceptable; however, make sure the set of principles that you—and your organization— believe in are easily accessible to your technical team when questions about software architecture arise. We believe that guiding principles like these help your development team spend less time thinking about high- level architecture, and more time writing business-related code and providing value to stakeholders. Our rules of engagement follow in order of importance, with the most important ones coming first.\n\nCustomer-Facing Applications Cannot Directly Touch Any Data Store “Consumer-facing applications” are interfaces used by an organization’s customers. They are usually but not always web sites, administrative interfaces, and native mobile applications; there could even be public- facing APIs that you expose. Consumer-facing applications will be a mere mashup of data retrieved from authoritative systems-of-record, and will never have a database or any other mechanism that persists state. Such systems-of-record will always be service applications with well-defined, versionable interfaces. Apart from the fact that some nasty security issues will be easier to address this way (SQL injection should be a thing of the past), different consumer-facing apps that work on the same information exposed by the owning service will be able to evolve independently, regardless of changes in the underlying data schemas of your data store.\n\nAn important side effect of this rule is that all the features in consumer-facing applications are\n\nimplemented via service calls to microservices.\n\nThis is a rule that if ever broken will take you on a slippery slope you will regret for a really long time. We’ve seen many features being implemented with direct database access “because it’s quicker,” but quickly become a nightmare to evolve because of tight coupling from one client. Do not believe it when someone—possibly you—says that “this will be extracted it into a service later”; you may have good intentions, but shifting business needs may mean that you will not be able to follow up on fixing your application’s design later.\n\n19",
      "content_length": 2922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Chapter 2 ■ rules of engagement\n\nNo Service Accesses Another Service’s Data Store In 2002, Jeff Bezos sent out an email at Amazon saying that “all teams will henceforth expose their data and functionality through service interfaces” and “there will be no other form of inter-process communication allowed,” and that’s how Amazon adopted and built an extremely efficient service-oriented architecture. This principle is mainly about implementation encapsulation; however, we mention data stores in our version because that’s where we’ve quite often seen this rule being broken, causing all sorts of implementation leakage that ends up slowing down service evolution.\n\nSimilarly, all inter-service interactions should happen only through well defined, versioned APIs. While a service has ownership of the data for which it is itself the system-of-record (including direct read and write access), it can only access other information via its dependent authoritative services.\n\nInvest in Making Spinning Up New Services Trivial This point, combined with the preceding rule, is very important; it’s essential to have both your developers and your TechOps teams embrace the “SOA way” inside your company. Driven, smart people tend not to tolerate being seemingly held back, and from a business viewpoint, they are too expensive an asset to be slowed down.\n\nMake everyone happy by having a small accepted list of technologies to support. Invest in creating\n\ntemplates for service code. And, last but not least, introduce rules and guidelines for your interfaces: consider the use of an Interface Definition Language to define APIs, and think about the structure of your RESTful interfaces (what consistent error status codes will you expose, what is in the headers and what is in URI parameters, how will authentication / authorization work? and similar questions).\n\nThese rules stem from more than a decade of combined personal experience (both from the ground up,\n\nas well as via refactoring existing monoliths). With a growing number of deployable units (that is, services); it’s important to be serious about automation (DevOps) —of all aspects of your infrastructure. In a perfect scenario, most code changes would kick off a build that runs a suite of automated tests. When a successful build happens, the change should be deployed to a shared development environment, and then later deployed to production without manual intervention in each step of the way.\n\nWhen a developer—or development team—decides that a new service is required, a lot of steps are\n\nneeded to get that new service live. Some of those steps are:\n\nDefining the language stack to be used based on requirements\n\nCreating a repository\n\nDefining a data-store\n\n\n\nSetting up continuous integration,\n\nDefining deployment\n\n\n\ndeploying To development/staging/production\n\n\n\nSetting up monitoring and logging\n\n... and so on. If your company has an environment where most of those steps are automated, then\n\ncreating one—or multiple—services doesn’t incur any sort of time penalty from the point of view of development, and you help your team keep code that needs to be separated in different services. By the same token, if spinning up a new service requires “asking for permission” and takes a few weeks, then it’s likely that your development team will bunch APIs into existing services to avoid those delays in getting services up and running.\n\n20",
      "content_length": 3411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Chapter 2 ■ rules of engagement\n\nWhen a developer shoehorns new APIs into an existing service just because it’s less friction than\n\nstarting a new service, he or she ends up breaking the single-responsibility concern that should be applied to microservices, which can lead to confusion and disrupt a focused system into becoming a set of seemingly unrelated APIs. The famous “broken windows theory” well fits this situation. It becomes hard to keep proper separation of concern when one API that is a bad fit goes into a service; it will undoubtedly invite similar behavior from other developers.\n\nBuild Services in a Consistent Way All services you build should behave consistently in many aspects. In the perfect scenarios, all your services will:\n\nCommunicate using the same protocol and format—for example, JSON over HTTP\n\nDefine APIs in a consistent manner\n\n\n\nLog request access using a standard log format\n\n\n\nBe monitored with the same tools\n\n\n\n...\n\nThe keyword of this rule is “consistency; surprises are evil and slow everyone down. This rule doesn’t\n\nmean that you put the brakes on innovation and don’t try new languages or libraries. You should always be assessing new technologies and practices, but in an organized way. It’s well known that, in the software development lifecycle, software spends most of its time in the operations and maintenance phase, and you really want to avoid snowflake services that require very specific knowledge for maintenance because some technologies were tested with no process for either applying those technologies broadly or retiring them.\n\nDefine API Contracts in Code An API is generally defined by its inputs and outputs, which should ideally be described using an artifact that computers and people—other developers—can parse. We really mean, at any point in time, the API definition must be clearly defined, and clients of that API can validate and test it unambiguously. In practice, a mechanism known to achieve this is an IDL, Interface Definition Language, a term for a language that allows developers to define the characteristics of an interface—in our case API endpoints. Using IDLs, developers can programmatically define accepted input and output parameters—including data types—for an interface.\n\nAll your services will expose and consume data via APIs; so documenting them is a necessity. At every\n\npoint in time, there should be a clear way of describing your endpoints in detail. You need to find out the expected inputs and outputs of each endpoint, and having those items in code that is exercised often is a way of making sure that you really know what is being exposed. In this book’s sample project, we will be using Swagger, a specification used to define RESTful APIs, and you will see some of the benefits of using an IDL, including the usage of tools built around standards, which help to increase developer productivity.\n\nWe’ll go into a lot of detail about API development in Part II.\n\n21",
      "content_length": 2965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Chapter 2 ■ rules of engagement\n\nEvery Service Has at Least a Development and a Production Instance When developing in SOA, you cannot expect your development and QA teams to run all the various environments for all the services to be deployed, on their local development machines, and on any host for that matter. Not only will this slow down their computers (and hence negatively impact their velocity), but nobody should be expected to update potentially several dozen service applications (and their test data) locally on a regular basis.\n\nNo developer should ever need to run, on their personal development environment, any system other than the one(s) that need modification in the context of their task. Dependent services should have standard, well-known URIs for server instances to which all consuming applications point by default. Ideally, you should use a service-discovery service to take away a lot of the pain involved in knowing how to connect to each service in your entire environment. We will show some techniques to achieve this in Part III.\n\nExisting Code Bases Most of the content of this book is geared toward green-field development, and it may sound too opinionated and hard to apply to existing code bases. Even if you work on an existing code base that doesn’t follow those principles, however, you may still be able to apply them selectively, in order to improve your existing architecture.\n\nThe rule of thumb for applying those principles to a suboptimal code base is to optimize your ROI. For example, if you have a couple of services that share the same database, it will probably be a very large effort to split tables into two separate databases; your existing code could even make it impossible because of joins and reliance on too many tables; so, you’d be better served by properly documenting your existing APIs. On a similar note, if you have a talented DevOps team, then your best improvement may be to invest in making it easy to spin up your next service. We recommend you analyze and use common sense to start making progress toward a more sane microservices environment.\n\nSummary In this chapter, we outlined our six rules of engagement for using microservices:\n\nCustomer-facing applications cannot directly touch any data store.\n\nNo service accesses another service’s data store.\n\n\n\nInvest in making spinning up new services trivial.\n\n\n\nBuild services in a consistent way.\n\nDefine API contracts in code.\n\n\n\nEvery service has at least a development and a production instance.\n\nWe hope all of these Rules of Engagement make sense at this point. They may seem quite opinionated\n\nand complex to follow now. If some of the rules seem blurry to you, that’s almost to be expected at this point. We will refer to them often in the rest of the book, and hope they will make more sense as we go deeper into each subject.\n\nIn Chapter 3, we will dive into how you can decompose your business model into services. We’ll define\n\nsome easy-to-follow rules for defining how to split parts of an application into services, and will use our sample project as an example to put those rules into practice.\n\n22",
      "content_length": 3134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "PART II\n\nAPIs",
      "content_length": 13,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "CHAPTER 3\n\nPartitioning Your Business into Services\n\nThis chapter presents some guidelines for composing a larger business application out of a set of microservices. We start by explaining some general criteria by which to partition a software system into a set of core services. We then explain how these core services can be composed into one or more layers of higher-level services, which eventually are the building blocks for the entire application. Finally, we will apply some of these generic principles to begin building our example bookstore application.\n\nGeneral Approaches for Application Decomposition and Aggregation This section offers some general guidelines for breaking down an entire application into services components. Many of the principles laid out in here are based on the excellent book Service-Oriented Design with Ruby and Rails by Paul Dix (Addison-Wesley Professional 2010), which uses a social feed reader application as an example for demonstrating these principles. In that application, users can subscribe to feeds, comment on articles in the feed, as well as vote them up or down, and follow other users’ feed commenting and voting activities.\n\nApplication Decomposition The first set of methods for breaking apart an application domain into a list of services has in common that they decompose the larger scope of the entire application into small, low-level services.\n\nDecomposing by Functionality\n\nThe first, and possibly most natural, principle to apply when splitting up a problem domain to be addressed with a larger software application is to analyze the functionalities it is meant to provide. Once you have identified the individual pieces of functionality the whole system is to provide, start by grouping them together into bundles that fit together logically.\n\nSome such general candidates for functionalities to factor out into a separate service can be email\n\nprocessing (sending or receiving), application resource monitoring, or connecting to and consuming external data sources (for example, scraping external sites for data, or collecting data via other site’s APIs).\n\n25",
      "content_length": 2123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nDix’s book example of a social feed reader application shows that the logic function of “updating feeds”\n\nsticks out as a separable functionally, as its “only purpose is to download XML files, parse them, and insert [them] … into the data store.” Another example he quotes is the feed and feed entry storage functionality: The users in his sample scenarios share feed and article data, and they subscribe to feeds. But these feeds and article in and of themselves do not exhibit an ownership relationship with the subscribing users. These are good characteristics for factoring the feed functionalities into their own service component.\n\nAs mentioned in Chapter 1, Netflix also takes a functionality-driven approach to divvying up its system into services. In fact, one could argue that Netflix is following this approach quite to the extreme, in that they have a guideline to have a single functionality per service, such as recommending a list of movies based on a given user and her past ratings for different movies.\n\nMore low-level functionalities that could be candidates for splitting out into separate services could be a system’s messaging component, or even a key-value–based data store. Amazon’s Web Services business unit capitalizes on this approach by offering its Simple Storage Service and Simple Queue Service.\n\nGoing the other way, it is also possible to find service candidates at a much higher level. More than just partitioning out data and business logic into a service, we can also isolate UI components rendered in a larger application. Many projects have been successful in splitting out components at the presentation logic layer. Paul Dix gives the example of bundling typical Ruby on Rails framework-based views and controllers into separate services that render consistent HTML (like a “comment service” that renders HTML snippets of a comment section on multifarious pages). To give the look of a single web-application, the controller-and- view based applications can be stitched together via Edge-Side Includes (ESI). The same effect could even be achieved via a fronting Ruby-On-Rails or Rack application instead, where this application’s goal is just to combine the routes of all the backing HTML-component rendering applications into includes as modules.\n\nWhen German etailer OTTO (https://www.otto.de/) re-engineered the second generation of its entire online presence in 2013 and 2014 as a “from-scratch” service-oriented system,1 the engineering department followed an approach of what they coined “vertical decomposition.”\n\nThey split their system according to business subdomains. As part of this process, they divided the online shop into 11 different verticals; for example, their back office, product, and ordering verticals, to name just three. Each of these verticals is serving its own web UI. Other verticals’ web UIs are referenced by explicitly linking to them via HTML links. The goal of integrating the verticals into a coherent, single web application from the end user’s viewpoint is achieved via ESI includes, which a fronting varnish proxy server resolves and pulls from all respective verticals into a single, top-level HTML view.\n\nDecomposing by Maturity\n\nAnother method of identifying parts of a prospective application that are good candidates for successfully combining into a service is to look for parts that have reached a similar level of maturity.\n\nA company’s business organization typically has had several iterations of defining goals and specifications, and these are reflected in the functionalities and behaviors of parts of the application in question. Parts whose functional and nonfunctional requirements are well understood should be bundled together, while things that are less stable and subject to more and faster iterations might move into other separate, versioned services (or not split out into separate services at all, initially).\n\nDix’s example for a core, and fairly static, part of his sample social feed reader application is the storing\n\nand updating the feeds, as it “will exist regardless of other iterations.” On the other hand, the services for users, comments, and votes components will be iterated upon more frequently, so they should not be part of the same service as the feed-handling.\n\n1Guido Steinaker, “On Monoliths and Monoliths and Microservices,” September 30, 2015. http://dev.otto.de/2015/ 09/30/on-monoliths-and-microservices/\n\n26",
      "content_length": 4491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nThe online marketplace LivingSocial has moved to this approach whenever pilot efforts for new business initiatives are being explored. A recent example is the Restaurants Plus initiative (https://www. livingsocial.com/restaurants-plus-rewards/), which was launched early to a set of LivingSocial employees and early-adopters to gather feedback and refine the product before a wider launch. Given the need for speedy and frequent iterations on the nascent business model while at the same time avoiding impact to the existing business, the decision was made to implement all new functionalities in dedicated applications and services.\n\nDecomposing by Data-Access Pattern\n\nThe next method of partitioning an application into service components focuses on data retrieval and persistence efficiency. The advice here is to analyze expected storage access for the various functionalities and classify them into “read intensive,” “write intensive,” and “balanced read and write” buckets.\n\nThe advantage of this approach is that a service’s schemas, access queries, or even the data store engine\n\n(whether RDBMS, key-value store, document-based, or other method) can be optimized for either read- or write-intensive access in isolation. Similarly, different data caching strategies can be applied for each of the services. One might, for example, decide not to introduce the complexity involved with caching and cache invalidation at all if a service deals mainly with write-intensive data.\n\nSometimes, partitioning an application by the data-access pattern leads to different results than trying\n\nto decompose along lines of related functionality. To illustrate this, let’s again consider Dix’s social-feed reader example: user profile data and a user’s reading list are functionally closely related, but the profile data is often read, while the reading list is much more often updated than it is accessed for displaying.\n\nSpecial care needs to be taken with “balanced read and write” data. Dix’s example is the voting data in the feed reader, which is updated about as often as it is read. Modularizing this functionality into a dedicated service enables the service owners to experiment with different optimization strategies to optimize performance for vending voting aggregates at both the user and the feed entry level. Another way to optimize this particular scenario would be to store voting statistics with the users and the feed entries themselves. While this could certainly result in performance improvements, that strategy does not come for free: one will have to live with the implications of only eventually achieving consistency of vote aggregate data duplicated in this way.\n\nDecomposing by Context\n\nSometimes it makes sense to implement multiple services that seem to focus on exactly the same entity. This is appropriate if they represent different distinct meanings in separate bounded contexts, one of the key concepts of an architectural design approach called domain-driven design.\n\nMartin Fowler’s article on the matter of bounded context2 explains that, for example, a customer can be polysemic. A customer can mean two different things in different business contexts; the attributes and business logic associated with a customer will differ greatly for the customer support unit of an organization from the attributes and associations in the sales unit’s business context.\n\nSuch a separation of concerns could go so far that the only shared data between such customer-focused services would be a common, unique customer identifier (best in the form of a universally unique identifier, or UUID). Both such domain-specific customer services could also cross-reference a third, lower-level customer-service for shared data (such as the customer’s address or other contact information).\n\nLivingSocial’s infrastructure serves information about cities in which the company runs voucher-based\n\noffers out of a service called city-service. This core data includes information about the city’s location, its relative size, its name, and so on.\n\n2Martin Fowler, “BoundedContext,” January 15, 2014. http://martinfowler.com/bliki/BoundedContext.html\n\n27",
      "content_length": 4214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nIn addition, there are at least two other services that map such city entities one-to-one to model objects in different contexts: a market-area-service that is used in the context of sending user-subscribed promotional emails, as well as another service that attaches information to cities that is needed only in the context of a new business initiative.\n\nBoth services refer back to the original, core city entities via a shared city ID. In his December 2015 talk at GOTO Berlin, “DDD & Microservices: At Last, Some Boundaries!”3, Eric Evans—the inventor of domain-driven design —confirms that bounded contexts and microservices play well together. In his words, microservices “are a good vehicle for domain-driven design,” because they provide “real isolation for the work within these different teams,” where teams can work in different bounded contexts. In his mind, employing microservices to implement the bounded contexts makes it easier to erect context boundaries and to subsequently enforce them.\n\nApplication Aggregation The opposite of the previously mentioned approach, which arrived at service components to make up an entire application system via decomposition, is to reaggregate smaller, lower-level services into new larger services. The following sections attempt to classify the set of motivations that can make such aggregations desirable.\n\nAggregation for a Derived Functionality\n\nOne motivation for such an approach could be that a desired functionality can best be provided as a mash-up of other existing, more basic core services. An example of such derived functionality could be a recommendation service for an e-commerce website. Such a service could be used to vend a set of personalized product recommendations for a given user. The data upon which the recommendation engine bases its suggestions can come from several, more basic services. One example of a more elementary service that provides input to the recommendation service might be a purchases service, which exposes data about a user’s previous purchases. Another input for data to be mashed up inside the recommendation engine can be a product service, which categorizes products and potentially relates them to complementary products. Combined, those two basic services’ APIs can be combined to recommend other “up-sell” products that are either in the same category as the user’s previous purchases or complement such previous purchase activities.\n\nThis approach is often taken when new features of an application are conceived in the later stages of the application’s lifecycle. Encapsulating such features into their own service enables the business to receive quick feedback, while the engineering team is able to launch the changes in relative isolation.\n\nGoing this route, and again using the previous example of a recommendation service, the business can\n\nmore easily test the effectiveness of new recommendation algorithms, for example, adding in data about purchases of other users that have bought similar products as the user for whom a recommendation is to be made.\n\nAggregation for Business Intelligence\n\nAnother good reason to aggregate a set of services into a secondary layer of services is to provide organizations with tools to gather and evaluate business metrics.\n\nMany companies aggregate data from other core sources in extremely large data stores, such as Apache\n\nHadoop. Systems often called “data warehouses” are built as a service-layer on top of this data, providing “Extract, Transform, and Load” (ETL) functions for use by business analysts.\n\n3A recording of the talk can be found at https://www.youtube.com/watch?v=yPvef9R3k-M\n\n28",
      "content_length": 3706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nA popular use case for such a system is financial reporting and forecasting, and other monitoring of business-related key performance indicators. Another common use case for this type of data analysis and data mining approach is to build fraud detection services on top of the large data sets available in such aggregated data stores. Similarly, the tasks of personalizing functionalities to the granularity of a single consumer of an e-commerce site often rely on post-processing large amounts of other data, from disparate sources.\n\nTypically, most of the business intelligence capabilities relying on such large data are not updated in\n\nreal time. While a service that provides this data could be called as part of an HTTP request cycle from a consumer’s browser, the underlying logic often is based on precalculated data.\n\nAggregation for Client Convenience\n\nAnother common motivation for composing services is to make life for client applications as easy as feasibly possible.\n\nDesigning service APIs with the maximum level of utility to the prospective clients in mind can\n\nsignificantly contribute to an application’s success. Tailoring the service endpoints such that they present a coherent design and only expose data and functionality that a client truly needs is crucial.\n\nHiding implementation details of the underlying data design of the service’s internals helps not only\n\nwith usability, but also with security aspects, especially for publicly accessible APIs.\n\nOften this service aggregation pattern is also applied to provide a more coherent, or generally just more\n\nuseful, presentation layer of data that a client application needs to perform its own duties. One example of this use case could be a “mobile façade service.”\n\nNative mobile applications (such as iOS or Android apps) often have a lifetime that spans multiple\n\nyears, and there usually is no good way to force all client devices to upgrade to newer versions of the application. This means that the service APIs used by such native mobile apps are publicly exposed, and hence need to be virtually frozen once any app version uses them. Unless you want to run the risk of causing breakages in the installed base of already shipped clients, those APIs should not be changed. Newer versions of the mobile apps, which want to take advantage of new features or changes in the APIs that break existing API contracts, will need to use an entirely new version of the previously exposed API. Over time as the number of supported features and mobile app versions grows, so does the number of APIs that need supporting separately and almost eternally.\n\nLivingSocial has such a mobile façade service at the edge between the public network and the application provider’s internal network. One reason for this choice was to shield native clients from the complexities and potential inconsistencies in other downstream services’ API contracts. This way, it is possible to introduce coherence and provide guarantees about data formats: not having to support the many different possible data formats for calendar dates, times, or currency value can greatly reduce the complexity of client applications. This edge service also performs validations of data it vends to native applications (for example, via enforcing that currency data conform to a schema complying with ISO 4217, dates that conform to ISO 8601), so that publicly shipped native applications can be more stable themselves. It also is a central point of dealing with the above-mentioned need for long-term support for publicly exposed APIs.\n\nAggregation to Aid System Performance\n\nA final purpose for service aggregation is to use it to improve response times or availability of the overall application.\n\nSome out-of-the box solutions exist for such efforts, for example caching proxy-services like Varnish (https://www.varnish-cache.org/). Architecturally, Varnish can be introduced as a caching layer to front one or more HTTP-based services, exposing those services’ endpoints directly to clients that otherwise would have directly produced traffic to the back-end services. Varnish is lightning-fast, and it has highly configurable caching and data aggregation capabilities, which include support for ESI and HTTP conditional GET header processing.\n\n29",
      "content_length": 4340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nBesides serving as a proxy that can aggregate and cache service-exposed data for performance improvements, Varnish can also be configured to aid system availability, as it can continue to serve data from its caches even if the back-end services fronted by varnish are currently unavailable.\n\nAnother example of aggregation for performance tuning is the case of services that expose aggregated\n\ndata in different data representations, or are optimized to provide search APIs.\n\nOne example from our work experience at LivingSocial is an internal service for the company’s overall\n\nproduct catalog. While each of the different product types is owned by a dedicated “systems-of-record” service, this catalog-service aggregates data about all product types. This data is stored in a denormalized fashion, and special database indexes are built to allow for very fast search and retrieval of product data by a configurable set of criteria.\n\nYet another service at LivingSocial whose main raison d’être is to improve overall performance is called\n\ninventory-service. It is a service that serves almost the exact same information as catalog-service, but which is specifically optimized for fast geo-location based product searches.\n\nAdditionally, the service provides a way to define special purpose representations of the product data, which can be tailored to the respective client’s needs. This way, the payload received about a set of products can be greatly reduced, so that network transmission time and latency, as well as service-side data serialization and client-side deserialization, can be kept to an absolute minimum.\n\nPaul Dix’s book about service-oriented design also speaks about improving efficiency by aggregating data that is often accessed together to provide information for service clients. He calls this “partitioning by join-frequency,” as this information is often generated by joining data from several database tables.\n\nWhile joining database tables can often be costly, combining data procured by multiple, potentially\n\nsequential, service calls across process and network boundaries can become even more prohibitive to meeting performance requirements. While this realization is surely easy to understand, the problem is much harder to avoid: all nontrivial applications usually expose functionalities that require almost all data to be joined. Dix formulates a few guidelines to impact performance as little as possible:\n\n\n\nAnalyze the functionalities of your application up-front, and consider the expected frequency of joining the various data objects. Looking at the results, you can make decisions about placing data and logic that is most frequently joined inside the same service and data store.\n\n\n\nIf the previous approach impedes the rest of your application design too much, you can denormalize the joined data, and then push it (for example, via a messaging bus) to all the services that want to join against it. This will give you the best performance improvements if the joined data is often accessed. Otherwise, the cost and complexity involved with broadcasting to, and storing it in, all joining services might not be justifiable.\n\nWhen to Use Decomposition versus Aggregation? After spending a significant amount of time trying to research, we have failed to come up with hard-and- fast rules or checklists for which of the above strategies to use under given conditions. Many of these design decisions are made weighing the pros and cons of the given characteristics of each of the approaches laid out above.\n\nThe best general rule of thumb we have been able to identify during a decade building and maintaining\n\nservices is that favoring decomposition techniques over aggregation techniques appears to be a cyclic decision. We have found that decomposition of a domain into small services is often how a business should start out, while aggregating services appears to be more needed and helpful to address the issues that arise later in the lifecycle of an application. Once a business is established and wants to diversify, that situation often triggers another phase of breaking down new ideas into additional small core services, which will then again be composed (often, together with other existing services) to form larger, aggregated services.\n\n30",
      "content_length": 4349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nWhile we realize that these observations and considerations appear rather general, we hope that the\n\nexample business case discussed next might help to provide additional guidance.\n\nDecomposing and Aggregating a Sample Application To illustrate many of the concepts laid out in this book, we have decided to put them into practice in a sample application that is partitioned into several small services.\n\nThe following section first lays out the main features of this application, and then explains decisions about how the application could be structured into a set of microservices, based on the previous section’s guidelines.\n\nWelcome to Our Bookstore! The sample domain chosen to accompany this book is that of an online bookstore. Throughout the rest of this book, we will use this fictitious bookstore to illustrate the concepts laid out in this book. These illustrations include partially implementing some of the services and applications that together could make up the entire system that supports this sample domain, discussing the resulting code samples, and deploying them to a cloud infrastructure.\n\nHere is a list of features that we see as needed for our bookstore business:\n\n\n\nVisitors to the bookstore’s site will be able to view, read reviews about, and search for, books (by category, title, author, or topic), and place them in a personal shopping cart.\n\nOnce registered on the site, such visitors become users, who can also buy existing books for themselves, or as gifts for others, and preorder books soon to be released.\n\nUsers can also sign up for promotional email newsletters and notifications about events regarding order status or activities of other users (see the social component described shortly). Users can also choose to receive personalized recommendations based on previous reading or shopping activity, category or author preferences, and recommendations by users they follow.\n\n\n\nSite-wide and category-specific bestseller lists will be maintained and displayed in the store.\n\n\n\nThe bookstore is aiming to serve all of North America, and therefore not only is obliged to follow Canada’s bilingual regulations, but also Quebec’s strict rules about the use of the French and English languages.\n\n\n\nBooks can exist in different formats: a physical book in paperback or library binding, as well as e-books in PDF, Mobi, or epub formats. Physical books can be bought, using a range of different payment instruments. Physical books are shipped to a set of user-associated addresses, and book returns are permitted within 7 work days after delivery. Electronic books are delivered electronically (via a download link on the user’s account summary page) and cannot be returned once purchased and delivered.\n\n\n\nThe bookstore will also have several social features: users can have reading lists and wish lists for books. They can write comments and reviews of books on the site, up- or down-vote books they have read, and follow other users’ reading lists, commenting, and reviewing activity.\n\n31",
      "content_length": 3076,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\n\n\nA fraud detection system integrated in the online bookstore monitors purchasing, review, and voting activities on the site. The system can flag suspicious activities to site administrators, as well as autonomously take action regarding user, vote and review activation statuses.\n\nWriting reviews about books and voting on them, are ways for a user to receive in- store credits, which can be used to purchase more books on the site.\n\n\n\nAn internal web interface to the bookstore will allow bookstore personnel access to various administrative tasks, based on the fine-grained access control restrictions. Examples of such tasks are adding, removing, or altering data about the books (for example, descriptions, pricing, category, keyword, or author-related data), and administering user-related data (like email and physical addresses, in-store credits, resetting user passwords, changing a user’s registered payment instruments, and toggling the user’s activation status) or curating comments and reviews on the site.\n\n\n\nAdditionally, business intelligence reports can be run on all data entities that are part of the application, for example, about users and their activities, book sales and review performance, current and projected financials.\n\n\n\nLast but not least, most of the consumer-facing functionalities are also exposed in native mobile applications running on iOS and Android devices. Other mobile devices are supported by a mobile web interface.\n\nPartitioning the Bookstore Let us start out with a few words of warning: as should be clear from the first section of this chapter about guidelines, decisions about the exact set of services to make up the entire system are very often design trade- offs to be made by the team(s) involved. There really are no black-or-white answers to this task, or anything like a “correct” partitioning, and even less so one that is optimal under all circumstances now and for the entire lifecycle of the bookstore.\n\nAlso, the bookstore business we’ll describe next has a rather large feature set and complexity; much\n\nmore than anything we are intending to implement in the accompanying example code. Do not be overwhelmed by the number of services proposed in this section. While we advocate having a rather large number of services for any application (very often larger than the number of developers working on them), it should be clear by now that we do not recommend rushing into a microservices architecture approach head-long or naively; adopting strong supporting development, deployment, and monitoring tools prior to engaging in such an architecture remains a necessary prerequisite.\n\nWith that said, we mentioned in the first section of this chapter that decomposing the bookstore\n\nbusiness along lines of functionality is probably the most natural way to start.\n\nDecomposing into Core Services\n\nKey and core functionalities are most obviously the data and logic directly related to the books that are sold on the site, so envisioning the existence of a books-service seems very appropriate. It seems very prudent to spin up an authoritative service for all attributes and direct functionalities directly associated with the products to be sold, commented and voted on, and reviewed on the site. The various versions of a book, in their respective formats (paperback, library binding, epub, and so on) will be called “book variants.”\n\n32",
      "content_length": 3451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nTo help performance, the books-service will vend product information in several different formats, or representations, each of which has a different subset of the entire set of book attributes. As an example, the bestseller list displayed in the web application will request a representation called a tile, which contains only a book’s title, author, a default price, and thumbnail URL for the cover image. In contrast to the page that lists bestsellers, a book’s detail page will request the “full” representation, which also contains the number of up- and down-votes, a full descriptive text, a summary, pricing and other details for all different variants of the book, and more.\n\nIt is important to note that books (much like any other entity in the store) are never truly deleted from the data store. They might be marked as “out of stock” or “out of print” or otherwise unavailable for purchase, but they will always be available in the data store via their direct resource UUID. This ensures that things like a user’s purchase history will be able to refer to them.\n\nAnother key entity is surely the user on the site; so for similar reasons as for books, implementing a\n\nusers-service seems a strategy in line with partitioning the application domain by functionality, as well.\n\nA user service necessarily handles the core data attributes that are associated with a user (for example,\n\nher name, email address, user handle, unique UUID, activation status, and potentially her salted and encrypted password) and the core logic to create, change, or read some or all of these attributes. In addition, it is also often tasked with authenticating a site user. This most likely means that a session cookie containing an encrypted user ID is generated and vended at an authentication endpoint used by browsers. For inter- service authentication, which mostly serves to authenticate the acting user for sessionless API requests, such a users-service often employs OAuth 2.04 tokens.\n\nIn other contexts, it might seem more prudent to separate the authentication functionality out of the core user service into a separate service. Sometimes, such an authentication service even lives outside the actual application domain. OAuth authentication via providers like Google, Facebook, Amazon, LinkedIn, or Twitter, to name but a few, is quite commonplace in today’s online applications.\n\nLooking at the fact that administrative tasks also need to be performed on the site by authenticated\n\npersonnel, a users-service might not just be responsible for consumer customers, but also for handling company employees. Depending on how fine-grained access control will be to various administrative options on the site, a simple, Boolean admin flag to identify administrative personnel might not be sufficient. In this case, many sites apply more or less sophisticated systems for role-based access control (RBAC). Such a service could either be integrated into a users-service, or developed as a dedicated authorization service. Given the scenario for our fictitious bookstore described earlier, we are making the decision to combine\n\nthe core user data and authentication functionalities into a single users-service, while isolating the RBAC functionality into a separate service. The main reason for this decision is the expected access pattern for these separate functionalities. The store will have a consumer-facing set of applications, which will be kept separate from the ones to be used by administrative staff. Both the consumer applications and the administrative interfaces will need to create, alter, and read core user data, as well as authenticate users. RBAC, however, mainly applies in the realm of the administrative context; consumer-facing applications are not envisioned to have much use for fine-grained access control. We only want to prevent consumers from modifying other consumers’ information or viewing other users’ sensitive information.\n\nRelated to users is another entity: addresses. We have decided to also dedicate a separate addresses-\n\nservice to these core data objects. Among the main reasons for this decision are data sensitivity and compliance concerns; billing and shipping addresses, for example, are personally identifiable information (PII), and therefore protected by specific laws in many jurisdictions. Many companies therefore decide to keep such data in separate, specifically protected environments, so that any potential security breaches to less restrictive environments do not lead to loss of personal information.\n\n4Internet Engineering Task Force (IETF), Request For Comment 6749, “The OAuth 2.0 Authorization Framework,” October 2012. https://tools.ietf.org/html/rfc6749\n\n33",
      "content_length": 4769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nApplications and Aggregated Services\n\nThere will be three major applications that are directly at the edges of the bookstore business, catering to external customers, and hence are exposed to the public network:\n\n\n\nThe application that renders HTML for desktop web users (let’s call it desktop-web),\n\nOne application that serves HTML to mobile web clients (called mobile-web)\n\n\n\nA mobile façade service exposing public APIs for access by the iOS and Android mobile applications (named mobile-service)\n\nAdministrative applications for company personal, including the reporting tools for running business\n\nintelligence queries against the data warehouse, will only be exposed inside the company’s firewall.\n\nImportant to note here is that none of these “pixel-rendering” edge applications will have access to any authoritative data store. While they might use some local caching technologies, they will rely solely on data that they retrieve from other non-edge service applications. This holds true even for the mobile API façade service, and—if there will ever be such an extension to the bookstore—for any other publicly facing APIs for external customers’ apps to consume.\n\nThe introduction of the “one-stop shop” mobile-service is done following the previously mentioned\n\npattern to aggregate services further downstream for the convenience of the Android and iOS apps. Localizations are done in this service, and attention is paid that all data is transformed into formats that can be easily and efficiently processed by the native mobile apps. As there is no way to ever truly force mobile devices to upgrade the bookstore apps, this mobile façade will need to implement strict versioning for the exposed APIs, with multi-year support for each of the versions introduced.\n\nFor maximum stability of the native apps, all data formats vended by the service will be specified using\n\nJSON Schema (http://json-schema.org/), and the service’s integration test suite will verify that all the versioned APIs conform to the schema specifications.\n\nAdditionally, mobile-service will serve as an aggregator to help overall system performance, adding HTTP conditional GET and caching headers like ETags and Cache Control max-age.5 This is a very effective and unobtrusive way to help native mobile applications improve performance, as the most used iOS and Android HTTP libraries support honoring and inserting the respective headers without any additional configuration or coding efforts.\n\nSupporting Platform Services\n\nGiven that the bookstore will operate in Canada, translations in at least the Canadian English and Canadian French locales will be required. In order not to duplicate the responsibility for correct localization of any given string across all edge applications mentioned above (for example, desktop-web, mobile-web, and mobile-service), a central translations-service will be developed. Translators and other company personnel will use an internal web UI to define strings via unique keys, and provide translations into all needed locales. The translations-service will store this data and might decide to expose it via RESTful APIs. The main way to communicate these translations internally to the bookstore infrastructure will be by publishing any changes or new translations on a system-wide message bus topic. Because of its message delivery guarantees and replay options, we will opt for using Apache Kafka.\n\nAny (existing and future) edge application that needs to render localized text will consume the translation’s\n\ntopic in a dedicated consumption process, and then update an internal storage engine (for which we will use a simple, file-based Berkeley database) to have the latest translations available inside the edge application. Purchasing books will also be handled by a separate service, called purchase-service, which encapsulates this functionality. It plays together very directly with two other services: cart-service and payments-service.\n\n5See https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html\n\n34",
      "content_length": 4092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nA user can populate her cart with a number of book variants in various quantities. The cart-service will therefore have a dependency on the books-service, as it will reference the book variants by the UUID identifying each book option.\n\nThe payments-service encapsulates the various accepted payment instruments a user can register with her account on the site. The bookstore will initially accept the major credit cards, but encapsulating this functionality into its own service will make it much easier to accommodate foreseeable extensions like the desire to also support PayPal payments, or maybe even in-store credits a user could accrue (for example, in return for reviewing many books). To simplify PCI compliance requirements, the service acts as a gateway an for external charging service (such as Braintree), never actually storing more than an inconspicuous payment instrument foreign key (as opposed to actual credit card numbers, or the like).\n\nThe steps that happen during purchase as part of the checkout process are as follows:\n\n\n\nThe book variants’ prices are retrieved from the books-service, multiplied by the selected quantities, and summed up to retrieve the totals of the cart’s value.\n\n\n\nAn attempt is made to charge the payment instrument chosen for the cart’s value using payments-service.\n\n\n\nThe cart data, cart total, and payment instrument ID are denormalized and then sent as inputs to purchase-service to create a record of the sale.\n\n\n\nA message about such a successful sale is placed on the appropriate Kafka message topic to inform dependent systems (like a service vending best-seller lists based on sales numbers) of this purchase event.\n\nTo allow for customer refunds, we implement a refund-service. It relies on historical purchase data, and the fact that a purchase-service record entirely describes the sale transaction (including the pricing information that was current at the time of purchase, as well as the payment instrument to which to refund the sale total). Refunds are also events that are published to a Kafka topic, so that other systems (for example, a data warehouse used for financial reporting queries, or again the best-seller list service) can update their own internal states accordingly.\n\nThe fraud-service, which detects suspicious or clearly fraudulent consumer behavior, has no directly\n\ncallable endpoints. It autonomously monitors purchasing and other user activities, and subsequently notifies administrative personnel. Depending on the severity of the fraud suspicion and impact, the service can deactivate users, votes, or reviews without prior human approval.\n\nFeature-Focused Services\n\nThe bestsellers-service exposes only a single search endpoint that responds to the HTTP GET method to vend the various bestseller lists based on criteria like a book category, an author, the book format, the language a book was written in, or the year it was published. This service’s backing data store can be optimized based on its purely read-intensive data access pattern to use a key-value store, or a search- optimized back end like Elasticsearch (https://www.elastic.co/products/elasticsearch). Updates to this data store will happen in a separate message-consuming process that listens to the topics published purchase-service and refund-service, while querying other data sources (like books-service) for additional data, like a book option’s title, author, category, format, language, and so on.\n\nThe social features mentioned in the bookstore scenario will also mostly be implemented using\n\ndedicated services. When partitioning these features into service components, we are placing special care on the data-access patterns and data join frequencies, in order to ensure that these features function efficiently at scale.\n\nSeparate services for comments, voting, and reviews functionalities are justified by their separate\n\nfunctionalities and potential maturity and iteration speeds.\n\n35",
      "content_length": 4008,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Chapter 3 ■ partitioning Your Business into serviCes\n\nA decision that is harder to make is this: which service should own the information about the set of users any given customer follows? From a functionality-driven perspective, the most natural place to store and retrieve such “user A follows users B, C, and D” data would be inside the users-service. But, given that we also have a requirement to notify all users of commenting, voting, and review activities by the users they follow, an activity-notifications-service (which will be in charge of notifying all following users) would have to constantly make service calls to the users-service to retrieve the current list of users to notify. Therefore, instead of keeping the “follows” relationship inside the users-service, we decide to keep this information inside the activity-notifications-service data store, so main cross-service data joins can be avoided.\n\nThe recommendations-service vends personalized recommendations and is a good example of a service that exposes a functionality that is mostly a mash-up of other, more basic service information. Recommendations will be based on purchase history data retrieved from the purchase-service application, combined with current cart-service information, and joined against books-service information about categories and authors.\n\nA promotions-service is an example of an entirely out-of-request-cycle service, which exposes no synchronous API endpoints. It operates entirely as a background process that sends promotional emails and push notifications to users based on data from the recommendations-service, as well as a users-service, once every week, or during holidays specials.\n\nSummary This chapter has focused on ways to break down a business domain and its related features into a set of smaller web applications and microservices. We looked at the two general approaches of application decomposition and application aggregation, and identified a set of patterns for them.\n\nWe examined and explained the following decomposition approaches:\n\nDecomposing by functionality\n\nDecomposing by maturity\n\nDecomposing by data access pattern\n\nDecomposing by context\n\nWe discussed the following aggregation patterns and the effects of their properties:\n\n\n\nAggregation for a derived functionality\n\n\n\nAggregation for business intelligence\n\n\n\nAggregation for client convenience\n\n\n\nAggregation to aid system performance\n\nWe concluded the chapter by outlining a fictitious online bookstore business application and its desired\n\nfeatures. We then applied some of the patterns and techniques discussed in this chapter to partition the bookstore into a set of core services, supporting services, and user-facing applications.\n\nChapter 4 will focus on the design aspects involved in exposing specific service APIs, both internal to a company and externally to the public, and we will describe a set of characteristics that we think make up a good API.\n\n36",
      "content_length": 2952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "CHAPTER 4\n\nDesigning Your APIs\n\nIn this chapter we will start explaining in detail how APIs should be designed. We will define what APIs are and will describe a set of characteristics that we think make up a good API, using what you now recognize as our opinionated approach.\n\nWe'll then explain how you should design your own APIs by describing a set of APIs from our bookstore\n\nsample application, listing each use case and describing our thought process when defining inputs and outputs for each API.\n\nIntroducing APIs To answer the question of what we understand by the term API, let’s look at some quotes from standard web resources that we think capture our understanding well and clearly.\n\nAPI (application program interface) is a set of routines, protocols, and tools for building software applications. The API specifies how software components should interact and APIs are used when programming graphical user interface (GUI) components.\n\n—Webopedia1\n\nAn API expresses a software component in terms of its operations, inputs, outputs, and underlying types, defining functionalities that are independent of their respective implementations, which allows definitions and implementations to vary without compromising the interface. A good API makes it easier to develop a program by providing all the building blocks, which are then put together by the programmer.\n\n—Wikipedia2\n\nOne aspect often overlooked when describing any API is its nonfunctional requirements. You will see why it is generally important to understand those requirements early on in the design process, and to include them in each individual API description.\n\nThe use of APIs is not exclusive to microservices or even other web-based applications. Many different\n\nkinds of computer systems employ them to enable software developers to build applications for these systems based on published programming interface specifications. As an example, a programmer who develops apps for Android will use Android APIs to interact with the system to build screens and access phone capabilities and hardware, like the front camera of an Android-based device.\n\n1“API—application program interface”, Webopedia (August 24, 2016). http://www.webopedia.com/TERM/A/API.html 2“Application programming interface”, Wikipedia, referenced on May 5, 2016. https://en.wikipedia.org/wiki/ Application_programming_interface\n\n37",
      "content_length": 2379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Chapter 4 ■ Designing Your apis\n\nAPIs are especially important in the realm of microservices because most communication between applications happens via API calls. Each of your microservices will expose its functionalities via a set of endpoints that have a well-defined set of inputs and outputs, and that perform the task(s) the service is responsible for.\n\nThe Significance of API Design\n\nAPI Design is easy … Good API Design is HARD\n\n—David Carver3\n\nDepending on your line of business, your API may be the actual product you sell and expose to your customer; if so, you have to take as much time as you need before you expose that service to your clients. If possible, come up with a process that helps you start developing and exposing your APIs early to a small subset of your users, so that you start to validate those APIs iteratively.\n\nWhen you decide to use an external API, you acknowledge that it takes a certain amount of time for your development team to learn that API and understand its interaction points. One needs to read documentation and write code, sometimes using trial and error, to take advantage of an API's functionalities. With that in mind, you should strive for providing an API that is well documented, behaves in a consistent manner, and performs well. Creating APIs that are inconsistent across different endpoints or perform poorly will frustrate the developers that have decided to invest in your API, so you really want to get that right the first time.\n\nAt Amazon Web Services, new services are usually extracted from known internal services at Amazon that are already successfully used by a set of developers. Once a service is identified as a possible external product, then getting a first beta of the to-be-public API to a very small set of Amazon customers is the next step in the product development cycle, because that allows some validation of that API. After the service team is happy that internal customers have successfully deployed solutions with the beta API, then a small set of external (to Amazon) customers are selected to be private beta users of the new service, to further validate the service API. Of course, at the scale of AWS, it's probably impossible to fix a bad API after it goes live; so, they have a validation process to make sure that their APIs will be useful in the designed state before they become public.\n\nWhy is Designing a Good API Important?\n\nOnce customers started building their applications and systems using our APIs, changing those APIs becomes impossible, as we would be impacting our customer’s business operations if we would do so. We knew that designing APIs was a very important task as we’d only have one chance to get it right.\n\n—Werner Vogels4\n\nOur goal as API designers is to deliver a product that is easy to learn and use, easy to maintain and\n\nextend, and perform its functions correctly and according to the client’s nonfunctional requirements.\n\n3“API Design is easy … Good API Design is HARD”, David Carver (October 20, 2009). https://intellectual- cramps.wordpress.com/2009/10/20/api-design-is-easy-good-api-design-is-hard/ 4Werner Vogels, “10 Lessons from 10 Years of Amazon Web Services,” March 11, 2016. http://www. allthingsdistributed.com/2016/03/10-lessons-from-10-years-of-aws.html\n\n38",
      "content_length": 3290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Chapter 4 ■ Designing Your apis\n\nJoshua Bloch, formerly the lead designer of many APIs and features of the Java platform at Sun\n\nMicrosystems makes the following key point about placing high importance on API design:\n\nThinking in terms of API Design improves code quality\n\n—Joshua Bloch5\n\nOn the opposite side of the equation, bad API design will cause you a lot of issues that will require you\n\nto make—often big—changes to your API design, which in turn will potentially break the code of your clients who are using the initial poorly defined API. There's nothing worse than asking your clients to rewrite code for no good reason. Another reason to avoid rushing your API designs is that you will probably still have to support the old versions of an API for clients who cannot upgrade to your new version when you want them to, so you may end up having to support both versions for a long time.\n\nWhat Makes an API Good? Now that we understand why API design is very important for microservices, we will talk about a set of properties and best practices that we think make a good API.\n\nGeneral Characteristics Any API designer should pay special attention to the big picture of the interfaces that the system under design exposes. In our experience, the following properties have proven particularly important to the success of APIs. Please note that we see all these characteristics as equally essential, so we do not rank or prioritize any one over others in this list.\n\nClear Naming\n\nEvery API should have an obvious name that makes sense to its users. It's known that naming is one of the hardest problems in software engineering, but it's especially important when you're creating an endpoint that other developers will call to make it very easy to remember. If you manage to give your API a name that developers can guess just based on its functionality, then you've nailed it.\n\nFocus\n\nAPIs should do one thing and do it well; the purpose of each API has to be clear to all its clients. If you make the reason for an API easy to understand, remember, and use, then you achieve this goal.\n\nCompleteness\n\nAn API needs to be able to fulfil its raison d'être; the functionality you advertise has to be properly implemented. This may sounds contrary to the previous characteristic, but is actually complementary.\n\n5Joshua Bloch, “How to Design a Good API and Why it Matters”, JavaPolis 2005, Antwerp, Belgium. https://www. infoq.com/presentations/effective-api-design\n\n39",
      "content_length": 2474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Chapter 4 ■ Designing Your apis\n\nIntuitiveness\n\nThe best APIs are the ones you can call without having to think a lot about them. If an API consumer can guess what your API does by glancing at your documentation, then you've achieved this goal.\n\nConsistency\n\nWithin your company, it's imperative that you have a set of consistent rules around API development. When you adopt consistent naming conventions for how you call your APIs, and how inputs/outputs are expected in the same way across your APIs, using them will become easy and intuitive.\n\nHypermedia Type In the name of consistency, you need a set of rules that any team working on APIs should follow. In our experience, organizations that let each developer and/or development team build APIs as they see fit will end up with a ton of inconsistencies in inputs, outputs, error treatment, and response envelopes. Over time, those inconsistencies will cause headaches and slow down your development effort overall. We've seen cases of APIs that were very inconsistent within the same service. This causes a lot of frustration for API clients who can't reliably reuse code to handle basic things like error-handling, because those endpoints don't follow a standard.\n\nMike Amundsen, author of the 2011 O’Reilly book Building Hypermedia APIs with HTML5 and Node,\n\nmakes an important point in an article called “Hypermedia Types” about the characteristics of hypermedia types and their relationship of the flow of an application using them:\n\nHypermedia Types are MIME media types that contain native hyper-linking semantics that induce application flow. For example, HTML is a hypermedia type; XML is not. 6\n\n—Mike Amundsen\n\nWe think it's vital that your organization picks one hypermedia type option as a standard for API communication, and sticks with it on all your APIs, and it will be the answer to questions that could cause inconsistencies. Some options are:\n\nHAL (https://tools.ietf.org/html/draft-kelly-json-hal-06)\n\n\n\nJSON API (http://jsonapi.org/)\n\n\n\nJSON-LD (http://json-ld.org/)\n\n\n\nSiren (https://github.com/kevinswiber/siren)\n\n\n\nCollections+JSON (http://amundsen.com/media-types/collection/examples/)\n\nEach of these hypermedia types has its rules that describe the exact format that requests and responses\n\nshould conform to, with varying degrees of robustness, verbosity, and strictness. We have decided to use JSON API in this book because it's a complete solution for describing what our APIs are supposed to look like, and it provides the consistency we're looking for. It could be that your preferred programming language has better support for other hypermedia types, and we recommend you do some research to pick the tool that helps you be most efficient.\n\n6“Hypermedia Types”, Mike Amundsen (2010). http://amundsen.com/hypermedia/\n\n40",
      "content_length": 2809,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Chapter 4 ■ Designing Your apis\n\nSome hypermedia types have fairly long and strict specifications, and some leave a lot of room for\n\ndevelopers to maneuver around their definitions. We prefer hypermedia types that leave no room for guessing how APIs should behave and recommend ones that take all of the following into consideration:\n\n\n\nRequest/response envelopes\n\n\n\nAuthentication\n\n\n\nResponse code\n\n\n\nError codes/messages\n\n\n\nPagination parameters\n\n\n\nSorting/ordering responses\n\n\n\nVersioning\n\nWhile we will not discuss the pros and cons of each of the hypermedia type options noted earlier, there\n\nare some very good articles that go over many of the characteristics of each type, and we recommend you experiment to find a specification you're happy with.7 The important part is that you make it obvious what hypermedia type you are using and make sure that it's respected by all teams in your company. By doing that, you won't have surprises when calling services, making your developers happier. As a general rule, we highly recommend making sure to research the tool chain that exists for the hypermedia types you choose with regard to the languages and the rest of your technology stack.\n\nWe realize that choosing a common hypermedia type across team borders is often a challenge, and that\n\nthe preferred choice can and will vary over time. In such scenarios it will often be necessary to introduce breaking changes to existing APIs, and we will present a process to address such evolution later in this chapter.\n\nAPI Design Guide After you have selected a hypermedia type to standardize on, we recommend you create an API Design Guide document (often also referred to as a Style Guide) that describes your organization's best practices regarding API design/development. You should aim for a document that is easy to read, searchable, concise, and explains your practices well. Make sure to update your API Design Guide as you improve your design practices based on what you’ve learned, and provide guidance about prioritizing potentially conflicting goals.\n\nA good API Design Guide should touch on all of these:\n\nHypermedia type to adopt\n\n\n\nRequest formats\n\n\n\nResponse envelopes\n\nConsistent attribute names\n\n\n\nExposed IDs\n\nOther concerns commons across your services\n\n7Chapter 7, which focuses on optimizing APIs, will show the use and virtues of a few hypermedia types in more detail.\n\n41",
      "content_length": 2394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Chapter 4 ■ Designing Your apis\n\nYou've probably noticed that hypermedia types may dictate some of the things that go into your Design\n\nGuide; selecting a complete hypermedia type will make your API Design Guide simpler to write, as a lot of concerns are already covered in the hypermedia type specification.\n\nA great example to follow—and find inspiration in—is Heroku's HTTP API Design Guide (https:// geemus.gitbooks.io/http-api-design/content/en/index.html). It's a very easy to read document that describes Heroku's set of HTTP+JSON API design practices. You will see that it answers quite a few questions about how APIs are expected to behave, and again, removes the need for developers to guess about many aspects of API development, making APIs consistent across the company.\n\nNonfunctional Requirements Often, most of the discussion around APIs navigates toward the inputs, outputs, and side effects of API calls. In our experience, the nonfunctional requirements for API calls—which usually relate to availability, performance, scalability, capacity, recoverability, and reliability—can end up being ignored or not seen as crucial. We, of course, disagree with that emphasis and think it’s essential to be explicit about those requirements when exposing and consuming APIs. We'll discuss some of the most important nonfunctional requirements of APIs in this section.\n\nService-Level Agreements\n\nMake sure that you have service-level agreements—SLAs—as part of your service API contracts. For example, you can have service-level availability agreements that apply to all the endpoints exposed by your service, and can go into detail about specific endpoints that have more fine-grained requirements.\n\nAvailability\n\nAvailability is probably the most important nonfunctional requirement a service owner needs to guarantee, because an unavailable service is not very useful; so your SLA should have an availability clause that explicitly states your service's availability guarantee. The simple fact of thinking about availability will make you think about the various scenarios that can impact availability, and necessary steps to guarantee that your service can be back online in a short time. It may be that, because of their responsibilities, specific services can be unavailable for extended periods of time without affecting a business; if so, make sure that is documented as well. For a public-facing commercial service, a sample availability clause in a SLA could read:\n\nAmazingAPICo will use commercially reasonable efforts to make sure its API is available 99.95% of the time. In the event AmazingAPICo does not meet the goal of 99.95% API availability, you will be eligible to receive a service credit.\n\nFor an internal service within a corporation, the clause could be as simple as this:\n\nServiceA should be available 99.5% of the time. In the event ServiceA is continually unavailable for more than 5 minutes, please page TeamA.\n\nBy being explicit about your nonfunctional requirements, you make sure that your clients are aware of the type of service they will receive, they can plan for fallback measures during outages, and you have measuring sticks for your monitoring and alarming setups. Also, SLAs for internal-only APIs can be used to estimate resources and justify operational expenditures (for example, for fail-over hardware if a given uptime requirement cannot otherwise be met).\n\n42",
      "content_length": 3413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Chapter 4 ■ Designing Your apis\n\nPerformance\n\nPerformance is another extremely important nonfunctional requirement, and it can affect the actual implementation of your service; therefore, you don't want to start thinking about performance only when your service is about to go live, because you may need to make substantial changes if its performance is not acceptable for its clients.\n\nWe go into a lot more detail about performance monitoring and alarming in Chapter 13, but the\n\nimportant aspect here is to make sure performance is not an afterthought. Your SLA should state some metrics that you want to respect during the operation of your service; this could be an average or p908 response time for all endpoints in your service, or an endpoint-specific performance metric for endpoints that have stricter performance requirements.\n\nScalability, Capacity, Recoverability, and Reliability\n\nWe will not go into detail about every aspect of your nonfunctional requirements, as those may vary widely. Our recommendation is to make sure you give them the attention they deserve and spend some time thinking about how to answer most of the following questions:\n\nHow do we scale the service? How do we measure scalability on day 1?\n\nHow many requests can the service support while maintaining its performance? How do we add extra capacity?\n\nHow fast can we recover from known failures?\n\nHow reliable is the service? Do we have specific reliability concerns? Is eventual consistency acceptable?\n\nIdeally, you should have a template SLA that can easily be adopted by your services with little\n\nadaptation, to avoid making “adding an SLA” feel like a complex task that requires weeks of analysis.\n\nImplementing APIs the Right Way Now that you know what an API is, it's time to dive into how APIs should be implemented. We'll outline a fairly simple set of steps to follow when you sit down to think about a problem you're solving by implementing one—or many—endpoints. After we describe those steps, we will move to a set of examples based on our sample project that will illustrate our thought process in building some APIs.\n\nGather Requirements The first thing you need describe when thinking about writing a new API is a use case that describes the functionality required. We use the term use case informally; we really want to come up with a nontechnical description of the business functionality required, ideally with no hints at implementation or technical constraint.\n\nA sample use case can be as simple as this: “As a User, I want to be able to save a file to storage for later\n\nretrieval.” This description of the functionality tells us who the main actor/participant is (User), the action he wants to take (save a file), and the input to the function (the file).\n\n8The p90 value is the number of milliseconds latency in which 90% of the requests reaching the API are serviced. In other words, no more than 10% of all requests take longer for the service to process and respond to.\n\n43",
      "content_length": 2991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Chapter 4 ■ Designing Your apis\n\nThe approach just described is known as a contract-first design style, which means we completely ignored all aspects of implementation and focused on what the API should do at a conceptual level and what inputs and outputs it requires. After you understand the requirements of your API, follow these steps to deliver it:\n\n\n\nPick one use case you understand well.\n\nDefine one API that fulfills an aspect of that use case by describing what it does, its inputs and outputs, and its error cases.\n\n\n\nPublish the API using a known protocol to your team members, and to the team that will consume your APIs. Your design approval process can vary from formal to informal based on the characteristics of the teams involved in the process.\n\nOnce the API documentation is vetted, it is ready to be implemented, and then verified via client integration.\n\nUsing this process, you will have involved multiple teams/individuals in helping you design your API, and you can start implementation knowing that you didn't miss obvious inputs/outputs, speeding up your overall development effort.\n\nMaximize Information Hiding\n\nIn software development, a leaky abstraction is an abstraction that exposes to its users details and limitations of its underlying implementation that should ideally be hidden away.\n\n—Wikipedia9\n\nOne of the main advantages of exposing functionality via APIs is that you provide a public interface to\n\na functionality—described by the API name and its parameters list, and are able to make changes to your implementation without having to disrupt your API’s clients; therefore, APIs are an “information hiding mechanism” for the code you are sharing via APIs. With that in mind, it becomes very important that you only expose enough attributes to make your response fulfil its goal, and nothing more.\n\nSome frameworks make it easy to generate API responses by serializing all attributes of a model in a JSON response by default; this seems to save the developer time deciding what attributes should be exposed, but it turns out to be an anti-pattern. In the long run, those easy-to-generate responses end up restricting the API developer when changes to the API implementation require database schema tweaks, which could break the actual API. Make sure that your APIs expose only attributes that are obviously useful to your clients, and make an effort not to expose artifacts that are too tied to your implementation, which creates what we call “leaky abstractions.” Another drawback is that clients will come to rely on implementation details you didn't need—or mean—to expose; you will have a hard time requesting clients to stop using those attributes, or will have to support them forever.\n\nThe rule of thumb for API design is to “hide” as much information as possible, and only expose data that is needed by clients, by using a vocabulary that is a level above the one you use in your implementation. Using our bookstore, a very simple example of this is a feature of the book detail API called has_errata?, which initially has the following Ruby implementation:\n\ndef has_errata? confirmed_errata.any? end\n\n9“Leaky abstraction,” Wikipedia, referenced on May 2, 2016. https://en.wikipedia.org/wiki/Leaky_abstraction\n\n44",
      "content_length": 3263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Chapter 4 ■ Designing Your apis\n\nIn this case, we could have taken a shortcut and exposed an errata array with all the data we had and let the API users figure out whether a book has any errata, which would have exposed our implementation to outside clients; to avoid this, stick to exposing an attribute called has_errata? which helps clients extract the same information without exposing too much information. If you were to expose your raw data—the errata array— then in the future, when you want your errata entries to have a status that could be one of “submitted,” “verified,” “invalid,” or “corrected,” this would cause issues with clients that rely on any record in errata to mean that a book had errors; so, exposing your errata array broadly is a design mistake that you should avoid.\n\nBy this point, you're starting to understand that consistency is a key aspect of good API design. Here are some aspects of your API where consistency means that there will be no surprises, and the development team will be able to make progress without guessing how each API behaves.\n\nWhen we talk about RESTful APIs, we already picture “JSON over HTTP” described with a verb and a resource. One thing that REST does not describe is how to standardize requests and responses, which will cause developers to be inconsistent across different services and endpoints, causing confusion in day-to- day development when, for example, the set of parameters required to query service A differ from those for service B, or one service returns an array while another returns a hash with an element called data.\n\nAPIs Change The one aspect of API design that is a constant is that APIs will change over time, and often during your development process. Like any other piece of software, APIs are never considered “complete” until the day they are retired.\n\nAcknowledge and Accept Changes\n\nThe Agile Manifesto10—like its practices—is widely accepted in our industry; it has the following principle that also applies well to APIs:\n\nWelcome changing requirements, even late in development. Agile processes harness change for the customer's competitive advantage.\n\nAll code bases go through many evolution steps during development, even when requirements are not a moving target. As soon as one accepts the fact that some code changes will happen over time, and in many layers, then changing an API just becomes part of the job of owning a service. As we mentioned before, APIs describe a contract that has to be followed for data exchange. Changes may sometimes break that contract in incompatible ways. Those are the cases where an API change needs to be thought out extremely well because not only is a service changing, but it's also requiring its clients to make changes.\n\nNonbreaking changes are API changes that, when deployed, will not break the expected behavior for\n\nexisting clients. Nonbreaking changes happen, for example, when one or more data attributes are added to an API response. If your API uses JSON to describe its responses, then adding attributes to clients that are not expecting them will have no effect on how your clients are accessing your API.\n\nBreaking changes are API changes that, when deployed, cause failures for existing clients. Some of the\n\nreasons that can cause breaking changes are:\n\nChange in the endpoint URL, which will cause clients to get 404 errors\n\nChange in the list of required input, which will cause client requests to fail for the lack of that attribute\n\n10“Principles behind the Agile Manifesto,”. http://agilemanifesto.org/principles.html\n\n45",
      "content_length": 3576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Chapter 4 ■ Designing Your apis\n\nChange in the format of the response, which will cause client requests to fail because the previously expected data structure is no longer available\n\nDepending on the size of your development team and the scope of your breaking changes, you can possibly update all clients in a short time, and not cause a lot of disruption; however, by the same token, you could require changes in applications that belong to other busy teams, and it may take months until your new API is used by all its intended audience.\n\nThe Development and Communication Process for Breaking Changes\n\nWhen possible, breaking changes should be avoided because they cause disruption of existing client calls, which may require a nontrivial amount of time to be updated across possibly many clients. When breaking changes cannot be avoided, it's crucial that you have a process to communicate that those changes are coming to your clients. Assuming that your clients can still use the current version of your API, a simple process to track API changes should include these:\n\nDocumentation about the change, explaining exactly what has changed, how important it is that your clients move to the updated API, and when you'd expect all clients to migrate.\n\n\n\nTimelines for when the new version of the API will be available for testing and in other environments, such as staging and production.\n\n\n\nIdeally, some sample code that communicates how the change in the API can be consumed. This is especially valuable in an organization that standardizes in a specific technology, where all API clients will benefit from seeing code changes.\n\nIn small companies, it's possible that developers who are making breaking changes are expected to also\n\nchange the code of the clients that consume those APIs. This is efficient when you have a small team; so, if your environment allows you to do so, it may be part of your process for introducing breaking changes.\n\nAdd a New API\n\nWhen a breaking change alters some of the fundamentals of how your existing API behaves, it may make sense to explain—and expose—your API change as a new API altogether. This can be seen when large companies release new versions of the APIs/libraries that completely break backward-compatibility with the existing version. As software evolves, some large refactoring may be unavoidable, making the new version of the API look like a new API, and you should consider doing that in cases where it’s likely that you will need to make changes in various endpoints because of design decisions taken in early stages of development. Of course, creating a new API—or set of APIs—is a very disrupting type of breaking change, and should be avoided when possible.\n\nVersion Your APIs\n\nAs we’ve said before, change happens! Over time, new requirements for your service will arise. Causing breakages in existing client applications—whether internal or external—can and should be avoided completely. In the vast majority of cases, upgrading your service and all its clients in lock-step is simply not possible.\n\n46",
      "content_length": 3068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Chapter 4 ■ Designing Your apis\n\nThis situation is best addressed by versioning your APIs. Put very simply, you can bump the version\n\nnumber of existing API endpoints that change. If you continue supporting the previous version of such API endpoints ( “API V1”), then current clients will continue to function unchanged, while new clients can choose to call the updated API endpoints at the new version, API V2.\n\nNegotiating the API Version\n\nClients need to communicate to the service which version of an API resource they would like to receive, while the service needs to indicate which version it serves up. There are many options for specifying a version for the API. Let’s look at the most common approaches we have seen used.\n\nUsing HTTP Headers\n\nIn this approach, the negotiations happen based on HTTP headers. The most HTTP standard-compliant way to do this is to use the Accept and Content-Type headers to request, and respond with, a specific version of an API resource.\n\nThe requesting client will specify the acceptable content type and version in an Accept header, for\n\nexample:\n\nAccept: application/vnd.books.com; version=2 GET /books\n\nThe service will then include an appropriate version of the result in its response, indicating the type and\n\nversion in the Content-Type response header, as shown here:\n\nHTTP/1.1 200 Success Date: Wed, 26 Aug 2016 06:25:24 GMT ... Content-Type: application/vnd.books.com; version=2\n\nA variation of this versioning strategy is to accept a nonstandard, application-specific header in place of the Accept header in requests destined for your service. One example for accepting (and including in service responses) is to support a header like, for example, X-BookStore-API-Version:\n\nX-BookStore-API-Version: 2 GET /books\n\nThe service response could then look like this:\n\nHTTP/1.1 200 Success Date: Wed, 26 Aug 2016 06:25:24 GMT ... X-BookStore-API-Version: 2 Content-Type: application/json\n\nUsing HTTP headers for versioning is the way most REST “purists” will probably choose. The argument\n\nfor this is that the version of a resource should not be part of the resource’s URI, as it is in essence still the same resource that is addressed. A disadvantage of this approach is the loss of convenience in quickly testing an API endpoint from a browser, as most browsers need additional plugins or extensions to allow for specifying HTTP request headers.\n\n47",
      "content_length": 2399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Chapter 4 ■ Designing Your apis\n\nUsing the Request Path\n\nSome APIs accept the version specification as part of their endpoint’s URLs, usually as a prefix before specifying the resource in the path. When translated to our bookstore example, the request to the list of all books in version 2 would look like this:\n\nGET /v2/books\n\nWhile it’s not uncontroversial, many API designers usually use this approach because of its simplicity,\n\ncompatibility with browsers, and intuitiveness for handling of versions. If your web application framework’s request routing mechanism supports it, we suggest that you at least consider allowing for specifying the resource version as part of the request path, in addition to relying on HTTP headers as discussed earlier.\n\nUsing a Query Parameter\n\nAnother approach to specify the requested version is to accept it as a query parameter, as shown here:\n\nGET /books?version=2\n\nSome API designers choose this approach, but we do not recommend it. We have found that this strategy seems the least intuitive of the approaches described here. Using a query parameter for the version of a resource muddies the waters around other query parameters that are better placed there, such as representations, search terms, and the like.\n\nDetermining What Numbering Scheme to Use\n\nFor the concrete values that signify a list of versions, most implementations we have observed use either consecutive integers (v1, v2, v3, …), or dates (v20160424, v20150811, …). Another popular software versioning scheme is semantic versioning (http://semver.org/), but we have rarely seen it in use for web- based APIs.\n\nWhile the details can certainly cater to personal preferences, we encourage using something that allows\n\nfor easy and efficient analysis by the server to evaluate which API is the later version. In other words, we discourage using schemes like v_new, v_old, vXmasEdition, or even v08112015 (as using dates in the ISO 8601 format yields larger integers for later date values).\n\nSupporting and Deprecating API Versions\n\nAny published API can potentially live for longer than its designer. This point is especially true when exposing an API publicly, where third-party entities decide how long they will attempt to use it. But even for private, company-internal APIs, time and thought invested in considering support and deprecation policies prior to launch pays off.\n\nA very important step is to make sure that there is a way to identify your clients for any given version of your API. One approach we have taken to help with this is to require an API key parameter (preferably as a special header) for every API call.11 If you do so, make sure to log such client information. You will find numerous opportunities to use it while your service is in production, even if it is just to perform usage pattern analysis.12\n\n11For internal APIs, the security and authentication features surrounding the authenticity of this API key can be quite relaxed; that is, clients can usually be trusted to reliably self-identify. More stringent rules for authentication, authoriza- tion, expiry, and encryption need to apply for externally accessible API keys. 12We will discuss aspects of monitoring your services in more detail in Chapter 13.\n\n48",
      "content_length": 3251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Chapter 4 ■ Designing Your apis\n\nYou should also think about how to contact and notify your clients about any updates to your APIs. Do\n\nyou have email addresses or other reliable contact details for all owners of client applications? Internal to your company you could use a simple mailing list for communicating changes, bug fixes, and so on, that are part of a new API version.\n\nAlso, be sure to publish your API’s deprecation and support policy alongside any API release. That way,\n\nclients can plan ahead, and you are in a better position to tell them to get off a version for which you are about to drop support. Teams that strive to support no more than two versions of the same API, where an outdated version is only available for a small number of weeks after the updated version has been released, can focus more time and resources on development tasks.13\n\nFor internal APIs, be sure to set expectations regarding which team is responsible for upgrading the\n\nclient applications when new (potentially breaking) API changes occur. Is the client application team responsible for acting on it, or will the service-owning team be on the hook?14\n\nDesigning / Splitting Up by Example In this section, we will take a practical approach to API design. We'll talk about two major interaction points of our bookstore project, and will describe some of the features of a couple of its pages, to help us identify the attributes that each page requires to be properly rendered. We'll then group those attributes and talk about the services and endpoints we should hit in each case, and we’ll design a few request/response examples that will hopefully help drive home some of the concepts we touched on in this chapter.\n\nFor all examples in our sample project, we decided to adopt the JSON API hypermedia type, so some of\n\nthe characteristics of our responses and requests are defined by that specification.\n\nThe Bookstore Home Page The bookstore home page has a fairly complex set of features distributed across some of our services (see Figure 4-1). In this section, we will describe those features and talk about the API calls needed to properly render the home page.\n\n13Some companies, like Netflix, on the other hand opt to keep old versions of internal APIs around until no client can be identified as using them. See Chapter 1 for more details on the Netflix approach. 14We discuss this and other topics of operational support in a microservices environment at length in Chapter 14.\n\n49",
      "content_length": 2488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Chapter 4 ■ Designing Your apis\n\nFigure 4-1. The sample application home page\n\nCart Information\n\nThe cart widget, which contains the price and number of elements added to the cart, appears in most pages of the web application. We have decided that carts are saved on a per-user basis, and therefore require the widget’s clients to pass in a UUID representing the current user, who could be logged in or not. In order to retrieve the user_id, we will ask users-service for the ID of the current user, which can be extracted from a secure cookie holding the user's authentication token.\n\nGET /users?token=6BbW0pxO0YENxn38HMUbcQ== Accept: application/vnd.api+json\n\nThe response returns a match that lists some information about the user who matched the token we\n\npassed in as a parameter.\n\n50",
      "content_length": 789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Chapter 4 ■ Designing Your apis\n\n{ \"links\": { \"self\": \"/users/d83d6c58-e05a-43a7-8d58-5995f68e1ba9\" }, \"data\": { \"type\": \"users\", \"id\": \"d83d6c58-e05a-43a7-8d58-5995f68e1ba9\", attributes: { \"name\": \"Cloves Carneiro Jr\", \"email\": \"cloves.carneiro@example.org\", ... \"phone\": \"\", } } }\n\nNow that we have a user ID handy, we can go back to cart-service and request the carts detail information, but we don't need all the details of the cart; we just want a really small subset of its attributes, the current total price, and number of products. The JSON API specification has a feature called sparse fieldsets, which states that an endpoint can return only specific fields in the response on a per-type basis by including a fields[TYPE] parameter:\n\nGET /cart?user_id=d83d6c58-e05a-43a7-8d58-5995f68e1ba9&fields[carts]=total_price,products_count Accept: application/vnd.api+json\n\nThe response returns just the data we requested, increasing efficiency both on the client and on the server.\n\n{ \"links\": { \"self\": \"/carts/178c3c65-16ee-46e8-8931-cdef1fb0568c\" }, \"data\": { \"type\": \"carts\", \"id\": \"178c3c65-16ee-46e8-8931-cdef1fb0568c\", attributes: { \"total_price\": 2259, \"products_count\": 1, } } }\n\nUser Recommendations\n\nIt's common that e-commerce landing pages display a set of products that are recommended to the logged- in user. Our proposed recommendations-service takes as parameters the user_id of the currently logged- in user and the cart_id, so that it can look up recommendations based on the user's previous purchases and current items in cart. Although recommendations-service sounds complex, it knows how to do its job and requires only a simple API call with the parameters we mentioned. We also know that we only want to display four recommendations in our home page, so we pass in page[size] with a value of 4.\n\n51",
      "content_length": 1824,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Chapter 4 ■ Designing Your apis\n\nGET /recommendations?user_id=d83d6c58-e05a-43a7-8d58-5995f68e1ba9&cart_id=178c3c65-16ee- 46e8-8931-cdef1fb0568c&page[size]=4 Accept: application/vnd.api+json\n\n{ \"links\": { \"self\": \"/recommendations?used_id=d83d6c58-e05a-43a7-8d58-5995f68e1ba9&cart_id=178c3c65-\n\n16ee-46e8-8931-cdef1fb0568c&page[size]=4\"\n\n\"next\": \"/recommendations?used_id=d83d6c58-e05a-43a7-8d58-5995f68e1ba9&cart_id=178c3c65-\n\n16ee-46e8-8931-cdef1fb0568c&page[size]=4&page[number]=2\",\n\n}, \"data\": { \"type\": \"books\", \"id\": \"5ea3d5c2-13a7-4282-9d33-693ae38c48e0\", attributes: { \"title\": \"Beginning Rails 4 (Expert's Voice in Web Development)\", }, \"relationships\": [ \"main_author\": { \"links\": {\"self\": \"http://books-service.books.com/authors/ec84275e-0e8a-495c-9ba7-\n\n68e791b0fad7\"\"}\n\n\"data\": {\"type\": \"authors\", \"id\": \"ec84275e-0e8a-495c-9ba7-68e791b0fad7\"} \"attributes\": {\"name\": \"Cloves Carneiro Jr\"} }, \"cover\": { \"links\": {\"self\": \"http://images-service.books.com/images/e54c65b6-2916-4573-a494-\n\n1e71188ae754\"}\n\n\"data\": {\"type\": \"images\", \"id\": \"e54c65b6-2916-4573-a494-1e71188ae754\"} \"attributes\": { \"file_name_url\": \"https://bucket.s3.amazonaws.com/book/5ea3d5c2/c421cdd8-865f-\n\n4bc6-8db5-107ccb1d4459.jpeg\"\n\n} } ] } }\n\nThe response served by recommendations-service provides books resources with a limited set of book attributes, only enough to display basic information about a book, which the user can then click on to get to a book detail page.\n\nThe Book Detail Page Let's now think about the book detail page of our sample application. It has most of the features customers would expect from a bookstore website; some of the obvious pieces of information it needs to render are the actual book details, all images for a book, and book reviews (see Figure 4-2).\n\n52",
      "content_length": 1775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Chapter 4 ■ Designing Your apis\n\nFigure 4-2. The sample application’s book detail page\n\nBook Details\n\nIn the context of the home page, books-service returned a list of books displaying a limited number of attributes for each book, because at that level, there wasn't a requirement for displaying much more than the book title, author, and image. In this case, the book detail page takes the opposite approach; its job is to display as much information as possible about a book to help customers make educated purchase decisions; thus, we can say that we will need all the book data available in that service.\n\nWe've used a feature we call representations to define a well-known set of attributes with a name that conveys the type of data that will be returned. In this case, we will be requesting the “full” representation view of a book detail API endpoint. It will provide the following attributes:\n\n\n\nTitle\n\n\n\nISBN\n\n\n\nAuthor(s)\n\n\n\nExcerpt\n\nDescription\n\n53",
      "content_length": 958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Chapter 4 ■ Designing Your apis\n\n\n\nTable of contents (TOC)\n\n\n\nPage count\n\n\n\nLanguage\n\n\n\nPublisher\n\n\n\nEdition\n\n\n\nRelease date\n\nDimensions\n\nWeight\n\n\n\nPrice\n\nOriginal price (if the current price is promotional)\n\n\n\nFormats (paperback, hardcover, ebook)\n\n\n\nTop category\n\n\n\nSub-category\n\n\n\nTags\n\nThe book details endpoint will be a resource identified by the URI /books/{isbn}, where isbn will be the ISBN that uniquely identifies each book; so, when a client application needs to access the detail endpoint for a book, it will send the following HTTP request:\n\nGET /books/978-1430260349?representation=full Accept: application/vnd.api+json\n\nIt will receive a response that—using the JSON API hypermedia type—looks like this:\n\n{ \"links\": { \"self\": \"http://example.com/books/978-1430260349\" }, \"data\": { \"type\": \"books\", \"id\": \"5ea3d5c2-13a7-4282-9d33-693ae38c48e0\", attributes: { \"title\": \"Beginning Rails 4 (Expert's Voice in Web Development)\", \"page_count\": 232, \"excerpt\": \"\", \"description\": \"\", ... \"tags\": [] },\n\n54",
      "content_length": 1014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Chapter 4 ■ Designing Your apis\n\n\"relationships\": [ \"authors\": [ { \"links\": {\"self\": \"http://example.com/authors/UUID\"} \"data\": {\"type\": \"author\", \"id\": \"UUID\"} \"attributes\": {\"name\": \"Cloves Carneiro Jr\"} } ], ] } }\n\nYou see that the top-level data element is what describes the book—the resource—we are interested in.\n\nIt's a JSON API requirement that responses have a data element. It can include a hash, as in this case, or an array when the response describes multiple resources instead of one, which is the case with the reviews and image requests this page will make.\n\nSome of the resources associated with the book are returned in that response—all under\n\nrelationships, as is the case of the authors array, which returns us enough information to display a list of authors, and links to URLs where we can load more details about those authors when necessary.\n\nReviews\n\nThe same book details page wants to list a small number of highly relevant reviews associated with the book the customer is looking at. For that, it will request that data from the reviews-service by sending an HTTP request to:\n\nGET /reviews/5ea3d5c2-13a7-4282-9d33-693ae38c48e0?page[size]=3 Accept: application/vnd.api+json\n\nYou can see that, in this request, we requested only the first three reviews associated with the book by using the page[size] parameter to limit the number of resources we want to see. We assume that the reviews-service will default to its most important sort order, and hence have not provided an explicit value for the sort parameter.\n\n{ \"links\": { \"self\": \"/reviews/5ea3d5c2-13a7-4282-9d33-693ae38c48e0?page[size]=3\", \"next\": \"/reviews/5ea3d5c2-13a7-4282-9d33-693ae38c48e0?page[size]=3&page[number]=2\", }, \"data\": [ { \"type\": \"reviews\", \"id\": \"UUID\", \"attributes\": { \"title\": \"Everything you need as a Rails programmer\", \"rating\": 5, \"name\": \"Jane Doe\", \"content\": \"Lorem ipsum ...\" } },\n\n55",
      "content_length": 1897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "Chapter 4 ■ Designing Your apis\n\n{ \"type\": \"reviews\", \"id\": \"UUID\", \"attributes\": { \"title\": \"When learning is a pleasure\", \"rating\": 5, \"name\": \"Martin Z\", \"content\": \"Lorem ipsum ...\" } }, ], \"meta\": { \"records\": 22 } }\n\nThat is a response that packs a ton of information, and it displays some of the great features of the JSON\n\nAPI. In this response, we have:\n\n\n\nThe list of reviews we requested, under data\n\n\n\nThe location where to get the next page of reviews, in links#next\n\n\n\nThe total number of review that match our request, in meta#records\n\nFor books that do not have any reviews, the only difference in the response would be an empty data\n\narray:\n\n{ \"links\": { \"self\": \"/reviews/5ea3d5c2-13a7-4282-9d33-693ae38c48e0?page[size]=3\", }, \"data\": [] }\n\nImages\n\nThe same book details page needs to display an image for each available version of the book the customer is looking at. For that, it will request that data from the images-service by sending an HTTP request to:\n\nGET /images/5ea3d5c2-13a7-4282-9d33-693ae38c48e0 Accept: application/vnd.api+json\n\n{ \"links\": { \"self\": \"/images/5ea3d5c2-13a7-4282-9d33-693ae38c48e0\" },\n\n56",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Chapter 4 ■ Designing Your apis\n\n\"data\": [ { \"type\": \"images\", \"id\": \"UUID\", attributes: { \"file_name_url\": \"https://bucket.s3.amazonaws.com/book/5ea3d5c2/a1d4efb5-7908-4683-\n\na1b9-20438c16eedb.jpeg\",\n\n} }, { \"type\": \"images\", \"id\": \"UUID\", attributes: { \"file_name_url\": \"https://bucket.s3.amazonaws.com/book/5ea3d5c2/c421cdd8-865f-4bc6-\n\n8db5-107ccb1d4459.jpeg\",\n\n} } ] }\n\nIn this response, we can see that there are two images associated with the book, which we can display in\n\nthe book detail page.\n\nAs you can see, we were able to load enough data to render all the details needed in the book detail\n\npage by calling three services, each responsible for a type of data associated with the book.\n\nSummary In this chapter, we covered a lot of ground related to designing APIs. We started by stating the importance of good API design, and listed a set of principles we think are important for your API process. We touched on the fact that you also need to think about your API’s nonfunctional requirements and listed a set of properties to consider during your design process. Tips on how to deal with breaking changes and API version control were explained next. We ended the chapter with some hands-on examples of how we defined APIs in our sample book project, describing some of the pages that customers see and endpoints that are hit to render those pages.\n\nIn the next chapter, we will go into more detail about implementing APIs. We will introduce approaches\n\nand tools for describing your services' APIs in a machine-readable format, by introducing the Swagger specification and some of its toolset.\n\n57",
      "content_length": 1613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "CHAPTER 5\n\nDefining APIs\n\nThis chapter will introduce approaches and tools for describing your services' APIs in a machine-readable format. First we will introduce the concepts behind Interface Definition Languages (IDLs) and will briefly introduce a number of such languages. Next, we will introduce the Swagger IDL framework and its associated tool chain in more detail, and we will conclude the chapter by illustrating how to use Swagger to define the APIs of one of the services in our bookstore application.\n\nInterface Definition Languages (IDLs) An IDL is a language for specifying any software component's API in a format that is easy for computers to process. The size of the component described in an IDL can vary widely; some IDLs describe the APIs of a single object, whereas others aim at defining the APIs of an entire application.\n\nThere are many advantages in describing your APIs in such a manner. As shown in the previous chapter, an iterative definition process that involves early drafting, sharing, and soliciting and incorporating feedback helps to increase the chances that your API will be useful to its clients.\n\nAnother advantageous trait of IDLs is that they are usually platform- and language-independent; clients of the APIs do not need to be implemented on top of the same operating system, or even in the same programming language as the service providing the APIs.\n\nThe biggest selling point in our eyes for the use of a machine-readable IDL revolves about the possibilities for developing a supporting chain of tools. Automatic processing of API definitions means that tools for generating client and/or service stub code in many programming languages can be created. Additionally, code for the validation of client requests and service responses based on the IDL specifications can be introduced. Some IDLs also come with tools for generating documentation, or with automatically generated web sites that let you explore the APIs based on their definition.\n\nThe first implementations of Interface Defining Languages date back to the early to mid-1980s,\n\nwhen Sun Microsystems introduced the Remote Procedure Call protocol. Sun’s first version of what is now called Open Network Computing RPC was specified in RFC 1057 and adopted in 1988. The style of defining interprocess communication in ONC RPC was firmly rooted in the Unix operating system and C programming language traditions. To exchange data between clients and services, ONC RPC used the External Data Representation (EDR) serialization protocol. This standardization strategy enabled even this very early IDL to develop a tool chain that included rpcgen for generating C language client libraries and service stubs, which were available across multiple operating systems (Unix derivatives and Microsoft Windows).\n\nThe list of IDLs used in the industry has grown large since these early beginnings. Some IDL frameworks (like Apache Thrift or Google’s Protocol Buffers) introduce special binary formats for transmitting data over the wire, favoring terseness and the resulting speed of transmission and decoding. Other IDLs (for example, JSON-based approaches like Swagger or API Blueprint) favor text-based protocols because of their human-readable format, and despite their implicit inefficiencies the growing availability of mass storage and bandwidth has made such protocols gain large popularity.\n\n59",
      "content_length": 3398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Chapter 5 ■ Defining apis\n\nA Matter of Style: RPC versus REST Most IDLs can be classified into two categories based on which architectural style of interprocess communications they most naturally support: Remote Procedure Calls (RPC) or REpresentational State Transfer (REST).\n\nWe will not go into detail about the specific differences between RPC and REST. For our purposes here, it is sufficient to say that we consider RPC a style that focuses mostly on traditional object-oriented method invocation and message passing. The approach is usually implemented via invoking a method on an object stub that exists in the local process address space of the client. Under the hood, this local invocation triggers a network call to a remote service, in whose process the actual object method is executed instead. The main point is to make the remote aspect of the procedure execution as transparent as feasibly possible.\n\nIn a RESTfully styled API, a set of particular constraints is imposed on the client and the service. Here,\n\nclients can retrieve information about, and perform a very limit set of actions (also known as verbs) on, a set of resources that are exposed by the service’s APIs. Services provide clients with multiple representations of the API-defined resources, and the client can use the action verbs either to retrieve a particular new representation of a resource, or to manipulate the resources. Additionally, communication between clients and the service is stateless. In almost all practical implementations, RESTful APIs are implemented on top of the HTTP protocol, which itself can be seen as the prototypical implementation of the REST architectural style; its set of methods (GET, PUT, POST, PATCH, DELETE, and so on) can be seen as the verbs that are applied to resources, identified via Uniform Resource Identifiers (URIs).\n\nRPC-Style Interface Definition Languages The first IDLs available all followed the RPC style, focusing mostly on allowing for invoking methods on remote objects. In addition to the already-mentioned ONC RPC, another early RPC IDL was the Common Object Request Broker Architecture (CORBA). This architecture, standardized by the Object Management Group (OMG), was a big step forward from ONC RPC for usage of an IDL outside a LAN and in more polyglot environments that demanded language and OS interoperability. Later, technologies like XML-RPC and WSDL, and their close younger cousin JSON-RPC, implemented remote method calling mechanisms on top of the HTTP protocol. Today, technologies like Apache Thrift (originally developed by Facebook), Google's Protocol Buffers,1 and Apache Avro2 have entered the arena of RPC IDLs, and each is used in production by many large companies.\n\nRESTful Interface Definition Languages RESTful IDLs probably started when Sun Microsystems first drafted the Web Application Description Language (WADL) in late 2005. WADL lets you describe REST APIs using an XML-based language, and much like WSDL, which added REST support with its version 2.0, WADL is mainly used in the Java ecosystem, especially in the JAX-RS APIs and its Jersey reference implementation. Another XML-based API definition language that developed fairly recently is RESTful Service Definition Language (RSDL), which is trying to improve on some drawbacks that practitioners found with WADL (mainly the perceived tight coupling between the service and the client caused by insufficient support for hypermedia-driven design).\n\n1https://developers.google.com/protocol-buffers/ 2http://avro.apache.org/\n\n60",
      "content_length": 3554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Chapter 5 ■ Defining apis\n\nOther, more modern and less Java ecosystem-centric IDLs seem to find the use of XML to express\n\nthe API specifications too cumbersome and verbose. One such example is the RESTful API Modeling Language (RAML),3 which expresses API specifications in YAML, and which has an increasingly large set of community-maintained tools surrounding it. API Blueprint4 is a similarly active IDL project, which chose a Markdown dialect as the language in which to write down API descriptions.\n\nAnother notable modern format in which to describe your APIs is JSON, and a significant number\n\nof RESTful IDLs specify service endpoints using it. An important part of such API descriptions is the specification of returned and accepted data types, and JSON Schema5 currently seems to be crystalizing as the common approach for this. The main advantage that JSON Schema appears to be providing is that there is a well-defined, community-supported specification format that has found wide adoption amongst RESTful IDLs.\n\nCloud application platform provider Heroku describes in a 2014 blog post their extensive open- source RESTful IDL tool chain, using JSON Schema as a central technology. Similar to what Heroku’s API Design Guide6 aims at, the JSON-API project tries to codify guidelines about sane choices for API design conventions. The aim is to foster consistency and similarity between APIs described using JSON Schema.\n\nProbably the most mature JSON Schema-compatible IDL framework in existence today is Swagger. Swagger has recently been donated to a consortium called the OpenAPI Initiative7 and has hence been renamed the OpenAPI Specification. Swagger sports an impressive tool chain, some of which we will be exploring in the next section of this chapter.\n\nOur Stance To conclude this section we would like to mention that we have taken sides and decided to design most of our service APIs in a RESTful manner. The REST approach seems more natural to the world of microservices, in which APIs are designed around resources, reachable via the (RESTful) HTTP protocol, and owned by systems of record and authority. This style also appears to make the undeniable fact more obvious that in-process object method invocations are different in many ways from remote, cross-network boundary service calls; external calls are much less reliable, have more failure scenarios, and expose very different performance characteristics, all of which should have an impact on API design.\n\nDescribing APIs using Swagger As explained in the general section on IDLs, the main benefit is that once you have defined your API operations, a computer can automatically process the IDL definitions to produce tooling for you: it can generate documentation from that IDL, it can generate a complete test suite that exhaustively exercises your API endpoints, it can generate load testing scripts for the endpoints, and handle many similar tasks.\n\nAs mentioned in the previous section, Swagger is focused on specifying RESTful web services. The tool chain around Swagger has matured greatly in recent years, and the framework now helps not only with describing, but also with producing, consuming, visualizing, and testing RESTful web services in a multitude of programming languages.\n\nWhile the main focus is surely on allowing computers to understand and discover the service APIs, the\n\ntooling also makes it easier for humans to comprehend and explore the described service.\n\n3http://raml.org/ 4https://apiblueprint.org/ 5http://json-schema.org/ 6https://github.com/interagent/http-api-design 7https://openapis.org/\n\n61",
      "content_length": 3611,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Chapter 5 ■ Defining apis\n\nHosting Swagger Specification as JSON Schemas Let's start by looking at the concrete example of the JSON Schema definitions of one of the sample services' APIs shown in Listing 5-1—the users-service hosts this specification itself at the /swagger/api path.\n\n■ Note show you tools later in this chapter that help you with JsOn schema generation and upkeep.\n\nalthough it is of course possible to produce and maintain these api definitions manually, we will\n\nListing 5-1. Swagger API JSON specification of the users-service\n\ntim$ curl http://localhost:5002/swagger/api | jsonpp { \"swagger\": \"2.0\", \"info\": { \"title\": \"Users-Service\", \"description\": \"Service that owns the concern of user accounts for the bookstore\", \"version\": \"1.0\" }, \"host\": \"localhost:5002\", \"basePath\": \"/\", \"paths\": { \"/users\": { \"post\": { \"tags\": [ \"Users API\" ], \"operationId\": \"Users API-create\", \"parameters\": [ { \"name\": \"email\", \"description\": \"Email address for the new user, needs to be unique.\", \"required\": true, \"in\": \"query\", \"type\": \"string\" }, { \"name\": \"name\", \"description\": \"Full name of the user.\", \"required\": false, \"in\": \"query\", \"type\": \"string\" }, { \"name\": \"password\", \"description\": \"Password for the user.\", \"required\": true,\n\n62",
      "content_length": 1252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "\"in\": \"query\", \"type\": \"string\" } ], \"responses\": { \"default\": { \"description\": \"\", \"schema\": { \"$ref\": \"#/definitions/User\" } }, \"422\": { \"description\": \"User data invalid\" } }, \"description\": \"Creates a new user for the given data\", \"summary\": \"Creates a new user for the given data\", \"x-controller\": \"users\", \"x-action\": \"create\" }, \"get\": { \"tags\": [ \"Users API\" ], \"operationId\": \"Users API-index\", \"parameters\": [], \"responses\": { \"default\": { \"description\": \"\", \"schema\": { \"type\": \"array\", \"items\": { \"$ref\": \"#/definitions/User\" } } } }, \"description\": \"Returns all users' data\", \"summary\": \"Returns all users' data\", \"x-controller\": \"users\", \"x-action\": \"index\" } }, \"/users/{id}\": { \"get\": { \"tags\": [ \"Users API\" ],\n\nChapter 5 ■ Defining apis\n\n63",
      "content_length": 758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Chapter 5 ■ Defining apis\n\n\"operationId\": \"Users API-show\", \"parameters\": [ { \"name\": \"id\", \"description\": \"Scope response to id\", \"required\": true, \"in\": \"path\", \"type\": \"string\" } ], \"responses\": { \"default\": { \"description\": \"\", \"schema\": { \"$ref\": \"#/definitions/User\" } }, \"404\": { \"description\": \"User not found\" } }, \"description\": \"Returns a user's data for a given user by uuid\", \"summary\": \"Returns a user's data for a given user by uuid\", \"x-controller\": \"users\", \"x-action\": \"show\" } } }, \"definitions\": { \"User\": { \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"string\", \"description\": \"UUID for the user's ID\" }, \"name\": { \"type\": \"string\", \"description\": \"the name for user\" }, \"email\": { \"type\": \"string\", \"description\": \"user's email address\" } }, \"required\": [ \"user_id\", \"email\" ] } },\n\n64",
      "content_length": 817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Chapter 5 ■ Defining apis\n\n\"tags\": [ { \"name\": \"Users API\", \"description\": \"This document describes the API for creating and reading user\n\naccounts.\"\n\n} ], \"securityDefinitions\": {} }\n\nAfter some initial declarations about the Swagger version used, and general information about the users\n\nservice, the host and base path for reaching its APIs are declared.\n\nWhat follows is the top-level paths key, where the value is another JSON hash in which each entry represents one of the resources exposed by the users service. This simple service only exposes two resources called /users (for the general list of all accounts for the bookstore application), as well as /users/{id} (which is a parameterized resource that refers to each single user account).\n\nBefore we dig deeper into the resources, let’s look at the next top-level entry, called definitions. The\n\ndefinitions key holds a hash with an entry for each API model’s schema definition. Here, the only API model to describe is the User model. As you can see, this model consists of two required string attributes (email and user_id), as well as an optional name string.\n\nThe Swagger JSON specifications of our users service hash are concluded by the tags entry, which is a\n\nlist of name and description key-value-pairs for the APIs, as well as a securityDefinitions section that can specify authentication and authorization requirements for the APIs.\n\nGetting back to the resource and paths definitions, we can see that they are organized by the HTTP action verbs they support: you can use an HTTP POST request with required input query parameters email and password, as well as an optional name. The results of this user account creation endpoint are captured in the responses section: the operation returns either the newly created User object, or an error response with a 422 status code when the user data passed in was insufficient.\n\nAfter a short description and summary section, you can find x-controller and x-action. These entries are interesting in that they illustrate that the Swagger JSON-Schema is extensible. In this case, the x- prefixed fields are used by a LivingSocial internal tool to better identify the code sections in a Ruby on Rails application that implements the API endpoint to create a new user.\n\nThe next endpoint defined is the parameterless GET /users, which simply returns an array of all bookstore account holders as User API model objects and is tied to the Ruby on Rails users#index action of the users service.\n\nFinally, the JSON schema defines the GET /users/{id} operation, which takes as an input from the URI’s path the user’s UUID as a string, and returns either the identified user’s data as a User object, or an error that indicates that a user by that UUID cannot be found.\n\nThe Swagger User Interface As mentioned earlier, one of the major selling points of Swagger over other RESTful IDL frameworks is the large list of tools surrounding it.\n\nA real gem in this tool chain is the Swagger UI. Swagger UI is a purely HTML/CSS and JavaScript-based tool that helps to visualize the JSON specifications of an API, as well as to interactively explore the described APIs.\n\nOur users service is hosting the Swagger UI at http://localhost:5002/swagger/doc, shown in\n\nFigure 5-1.\n\n65",
      "content_length": 3273,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Chapter 5 ■ Defining apis\n\nFigure 5-1. Swagger UI for users-service\n\nAs you can see in Figure 5-1, the UI provides access to and descriptions of all API endpoints, as well as\n\nAPI models, based entirely on the API specifications hosted locally at http://localhost:5002/swagger/ api.json.\n\nAdditionally, the UI allows for specifying the APIs’ input parameters in a web form, and exposes “Try\n\nit out!” buttons to send requests to the respective endpoint. Each request triggered this way results in displaying the response body, code, and headers in separate sections of the UI. Sample cURL statements and resulting request URLs can also be found on the Swagger UI page.\n\n66",
      "content_length": 672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Chapter 5 ■ Defining apis\n\nThe schemas of API request and response models are shown right above the section where input parameters are described and can be entered before making requests. Boldface type indicates that the named parameter is required, and there are also definitions about the data type and request location (that is, whether in the request body, the URL path, or the URL query) for each parameter.\n\nAt the end of each entry describing an API endpoint is a list of declared response messages, detailing\n\npotential error codes and conditions, models, and headers.\n\nLet’s look at the results from calling the GET /users/{id} API endpoint via the Swagger UI (Figure 5-2).\n\nFigure 5-2. Swagger UI for GET /iusers/{id}\n\n67",
      "content_length": 731,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "Chapter 5 ■ Defining apis\n\nOnce the request triggered by the “Try it out!” button completes, new sections appear. The “Curl”\n\nsection details the exact Unix curl command-line statement equivalent to that made by the Swagger UI.\n\n■ Note it. internal, inside-Vpn-only facing apis should probably at least make sure that an api key is required to authenticate and authorize access. externally-facing apis will need more protection; we recommend Oauth 2.0 via JsOn Web token (JWt). see https://tools.ietf.org/html/rfc7523 for more details.\n\nfor simplicity of illustration, the api exposed here has no authentication or authorization built into\n\nThis new section shows the request URL, a formatted version of the JSON response body, and the response code and headers returned from the service.\n\nSwagger UI is just one of many tools surrounding the framework, and the official Swagger website lists a large number of core, as well as community supported tools. The list is still growing, and it is already too large to explain each one of them in this book. Therefore, we will just focus on two more categories of tools: automatic code generation from Swagger JSON specifications, and generating the JSON schema specification itself based on more human-readable DSLs.\n\nGenerating Code from Swagger Definitions Starting out with code generation, we are using the swagger-codegen tool (https://github.com/swagger- api/swagger-codegen), which is included as a core project in the Swagger toolset. This tool allows automatic generation of client libraries to communicate with APIs based on the service's Swagger API specifications. Another option that the tool provides is to generate an actual service stub to vend the APIs based on a predefined JSON schema specification. Both client and service generation can be configured to generate code in around 35 languages and service frameworks.\n\nLet’s look at an example of how to use this tool to generate a Ruby language gem (Ruby’s equivalent to\n\nother languages’ libraries) for our users service:\n\nswagger-codegen generate -i http://localhost:5002/swagger/api -l ruby -o /tmp/users-client/\n\nThe directory tree of the code generated by this command can be seen in Figure 5-3.\n\n68",
      "content_length": 2218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Chapter 5 ■ Defining apis\n\nFigure 5-3. Directory tree of the Ruby client gem\n\nListing 5-2 demonstrates how this auto-generated client library can be used.\n\nListing 5-2. Using the generated Ruby client gem\n\nusers-client tim$ bundle exec rake console irb -I lib -r swagger_client.rb -r console_defaults.rb 2.3.0 :001 > client = SwaggerClient::UsersAPIApi.new => #<SwaggerClient::UsersAPIApi:0x007fb0c4938b00 @api_client=#<SwaggerClient::ApiClient:0x 007fb0c4938ad8 ...>> 2.3.0 :002 > all_users = client.users_api_index\n\n69",
      "content_length": 520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Chapter 5 ■ Defining apis\n\n=> [#<SwaggerClient::User:0x007fb0c488b298 @user_id=\"3a07468a-f97d-4242-9057-8437385efb29\", @name=\"Tom Copeland\", @email=\"tom@books.com\">, #<SwaggerClient::User:0x007fb0c488ae10 @user_ id=\"943f3586-8f8e-4513-90ff-9da59e86f013\", @email=\"cloves@books,com\">, #<SwaggerClient::User :0x007fb0c488aa00 @user_id=\"b635e0b4-14db-4737-9672-2b5c678156bb\", @email=\"tim@books.com\">] 2.3.0 :003 > tom = client.users_api_show(all_users.first.user_id) => #<SwaggerClient::User:0x007fb0c3a89230 @user_id=\"3a07468a-f97d-4242-9057-8437385efb29\", @name=\"Tom Copeland\", @email=\"tom@books.com\"> 2.3.0 :004 > tom.to_hash => {:user_id=>\"3a07468a-f97d-4242-9057-8437385efb29\", :name=>\"Tom Copeland\", :email=>\"tom@ books.com\"}\n\nIf you’re familiar with idiomatic Ruby code you can see here that using the client gem is rather easy and\n\nwithout surprises. After creating a client object, you call methods on that client that correspond to the API endpoints, accepting as arguments the parameters laid out in the Swagger JSON specification. The API call results also match auto-generated Ruby classes for the API models from the Swagger specification. We made the complete source of the generated Ruby client available at https://github.com/microservices-from- day-one/user-client.\n\nIf your language of choice is Clojure, the only tweak to generating a client library is the language for the\n\nl option of the swagger-codegen command-line tool:\n\nswagger-codegen generate -i http://localhost:5002/swagger/api -l clojure -o /tmp/users- client-clj/\n\nThe code structure generated looks like Figure 5-4.\n\nFigure 5-4. Directory tree of the Clojure client library\n\nUsing the client code from the Clojure REPL is also very idiomatic, as shown in see Listing 5-3.\n\n70",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Chapter 5 ■ Defining apis\n\nListing 5-3. Using the generated Clojure library\n\nuser-client-clj tim$ lein repl nREPL server started on port 51209 REPL-y 0.1.10 Clojure 1.7.0 Exit: Control+D or (exit) or (quit) Commands: (user/help) Docs: (doc function-name-here) (find-doc \"part-of-name-here\") Source: (source function-name-here) (user/sourcery function-name-here) Javadoc: (javadoc java-object-or-class-here) Examples from clojuredocs.org: [clojuredocs or cdoc] (user/clojuredocs name-here) (user/clojuredocs \"ns-here\" \"name-here\") user=> (require '[users-service.api.users-api :as u]) nil user=> (u/users-api-index) ({:user_id \"3a07468a-f97d-4242-9057-8437385efb29\", :name \"Tom Copeland\", :email \"tom@ books.com\"} {:user_id \"943f3586-8f8e-4513-90ff-9da59e86f013\", :name nil, :email \"cloves@ books,com\"} {:user_id \"b635e0b4-14db-4737-9672-2b5c678156bb\", :name nil, :email \"tim@books. com\"}) user=> (u/users-api-show \"3a07468a-f97d-4242-9057-8437385efb29\") {:user_id \"3a07468a-f97d-4242-9057-8437385efb29\", :name \"Tom Copeland\", :email \"tom@books. com\"}\n\nAgain, all API endpoints are accessible via unsurprisingly named functions. The choice of data type\n\nfor the User API model is a simple Clojure map, not a record structure, which is also available in the language. We made the complete source of the generated Clojure client available at https://github.com/ microservices-from-day-one/users-client-clj.\n\nGenerating the Swagger JSON Specifications The last category of tools we discuss aims at making it easier to generate the Swagger JSON specifications for an API from within the project that implements the service.\n\nGiven that our main language of choice for implementing the sample services is Ruby, we are showing\n\ntwo Ruby tools for this task.\n\nSwagger-Blocks\n\nThe first tool we would like to demonstrate is swagger-blocks (https://github.com/fotinakis/swagger- blocks/). Including the swagger-blocks gem into your Ruby on Rails lets you specify your API endpoints using a DSL instead of JSON.\n\nListing 5-4 shows sample documentation that describes the UsersController in our users service, and\n\nwhich generates part of the JSON specification shown earlier in this chapter.\n\nListing 5-4. Swagger-blocks description of /users/{id}\n\nclass UsersController < ApplicationController include Swagger::Blocks\n\n71",
      "content_length": 2311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Chapter 5 ■ Defining apis\n\nswagger_path '/users/{id}' do operation :get do key :description, \"Returns a user's data for a given user by uuid\" key :operationId, 'Users API-show' key :tags, [ 'Users API' ] key :summary, \"Returns a user's data for a given user by uuid\", key :'x-controller', \"users\" key :'x-action', \"show\" parameter do key :name, :id key :in, :path key :description, 'Scope response to id' key :required, true key :type, :string end response 404 do key :description, 'User not found' end response :default do key :description, '' schema do key :'$ref', :User end end end end # Here comes the actual code that implements the `users#show` API def show # ... end\n\n#... end\n\nWhile the swagger-blocks tool has some very nice features and supports the latest Swagger version\n\nv2.0, we chose a different tool to specify our Rails APIs.\n\nOne reason we chose a different tool is that the structure of the swagger-blocks DSL very closely matches the structure of the JSON schema specifications. There doesn’t appear to be a lot of simplification or lowering of the abstraction barrier as a result of choosing this tool.\n\nAdditionally, mixing the specification DSL Ruby code with the Ruby code that implements the actual\n\nAPI endpoints is confusing to read.\n\nSwagger-YARD\n\nThe LivingSocial-maintained open-source tool swagger_yard (https://github.com/livingsocial/ swagger_yard/) approaches the documentation is a different way; the Swagger JSON schema is generated from comments in the code, rather than Ruby code itself. Under the hood, swagger_yard uses the YARD gem that serves as the Ruby community's standard tool for generating code documentation. YARD uses @-prefixed tag definitions to annotate regular Ruby code methods and classes.\n\n72",
      "content_length": 1750,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Chapter 5 ■ Defining apis\n\nSwagger_yard specializes the general YARD tags to provide tags like @resource, @path, @parameter, @response_type, @error_message, @model, and @property that describe the respective related Swagger specification entities.\n\nIn Listing 5-5 you can see the full JSON specification again describing the UsersController, but this\n\ntime expressed using swagger_yard.\n\nListing 5-5. Swagger_yard description of the UserController\n\n# @resource Users API # # This document describes the API for creating, and reading user accounts. # class UsersController < ApplicationController ## # Creates a new user for the given data # # @path [POST] /users # @parameter email(required) [string] Email address for the new user, needs to be unique. # @parameter name [string] Full name of the user. # @parameter password(required) [string] Password for the user. # # @response_type [User] # # @error_message 422 User data invalid def create ... end\n\n## # Returns a user's data for a given user by uuid # # @path [GET] /users/{id} # @response_type [User] # # @error_message 404 User not found # def show ... end\n\n## # Returns all users' data # # @path [GET] /users # @response_type [array<User>] # def index ... end end\n\n73",
      "content_length": 1226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Chapter 5 ■ Defining apis\n\nThe User API model (see Listing 5-6)referenced in the API endpoint definitions just shown is described\n\nas a leading comment associated with another Ruby class (in this case the ActiveRecord model used to persist a user to the database).\n\nListing 5-6. Swagger_yard description of the User API model\n\n# @model User # # @property user_id(required) [string] UUID for the user's ID # @property name [string] the name for user # @property email(required) [string] user's email address # class User < ActiveRecord::Base include ActiveUUID::UUID\n\nhas_secure_password\n\nvalidates :email, presence: true, uniqueness: true validates :password, presence: true\n\n... end\n\nSummary We hope that this chapter helped to illustrate the advantages of defining your APIs in a computer-readable format. Adopting the use of an IDL and tools like Swagger very early on in your development process allows for soliciting feedback during the API design process. This enables a very iterative approach to API development.\n\nIn our daily work, we find ourselves defining API drafts almost every day using swagger_yard before\n\nimplementing a single line of business logic.\n\nWe can then proceed very quickly and share the API drafts via the Swagger UI with the teams that will\n\nbe consuming the APIs. The resulting feedback loop is very tight and fast, and we can iterate on change requests within minutes, speeding up the entire development cycle.\n\nAdditionally, involving the client teams early and defining APIs with their help leads to much more useful and robust APIs, as they are closely focused on the clients’ needs and hence much more likely to contribute to your business goals.\n\nWe will focus on aspects that deserve more consideration when consuming such service APIs in the\n\nnext chapter.\n\n74",
      "content_length": 1800,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "CHAPTER 6\n\nConsuming Your APIs\n\nThe size ofclient-side libraries, and the features included in them, can vary widely. We will discuss the pros and cons of including various responsibilities, like exposing network errors, or making service calls, validating service responses, caching expensive API calls, handling errors, and alarming when things go wrong. Key topics in this chapter include\n\nHow much code should live in a client library?\n\n\n\nLibraries should not have business logic, ever.\n\nHandling errors is a must, and depends on the nature of the endpoint being called.\n\nWhat Are Client Libraries? In a microservices environment, we call client libraries, the application code used to interact with a service. It’s a set of code, that, when added to an application, will give developers access to the APIs of the service it was written for. We believe that client libraries should be a very thin service access layer that implements a convenient set of objects that mimic specific service APIs. For example, a client library for books-service should expose a Book class that has methods that correspond to the API response of the service, returning values of the correct type for each attribute, making it palatable and convenient to interact with the API.\n\nIn general, your client libraries will have a set of plumbing features that should be common to all client libraries, they provide functionality that is not associated with each underlying API, but is required in order to provide all the tools developers need to interact with services. We call the following plumbing features:\n\nNetwork access. The main characteristic of distributed systems is that APIs require a network access layer in order to be accessed. That network access layer is implemented in client libraries; so, that developers don’t need to write network code in all their applications.\n\nObject serialization/deserialization. APIs use a serialization format to retrieve and expose data, which needs to be handled at the client library level. It’s expected that client libraries will abstract the details of the serialization format to its users, making responses look like regular objects, as expected by users of the language in which the library is implemented.\n\n\n\nLow level errors, like timeouts. Client libraries have to handle expected errors, such as access to invalid resources, or network timeouts. Depending on the programming language, client libraries will handle those type of errors, and will raise exceptions, or set specific response codes to service calls, allowing its users to write error handling code according to each failure coming from a service.\n\n75",
      "content_length": 2652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Chapter 6 ■ Consuming Your apis\n\n\n\nExpose application errors. Client libraries can choose to translate some service response into code that is common in programming languages. For example, when a service returns a “404” to a request for a nonexistent resource, the client library may decide to expose that as a ResourceNotFound exception, which may make programming more natural, instead of forcing API users to check response codes.\n\nCaching. Client libraries may have code to cache service responses, thus increasing overall system performance. The caching layer in client libraries can have custom caching code, or it can just be HTTP-compliant to send and honor certain headers, performing tasks such as storing response based on ETags or expiry headers.\n\n\n\nService discovery. A client library needs to know how to connect to the services it exposes, and allow its users to override its default configuration.\n\nThis may seems like a lot to implement in what is supposed to be a “thin” layer; however, if you decided\n\nto use an IDL and/or a Hypermedia type, as explained in Chapters 4 and 5, you’ll probably be able to get a lot of this “for free” by using tools available for users of your IDL/Hypermedia type of choice. For example, in the sample project of this book, we decided to follow JSON API, and can use a wide array of libraries (http://jsonapi.org/implementations/) to help us building client libraries.\n\nWe’ll go into more detail on some of the features we expect to see in most client libraries; however, we’ll\n\nfirst take a quick detour to talk about the type of code you should never have in a client library.\n\nWhat Not to Do Here are a few suggestions, based on our experience, about things to absolutely avoid when designing client libraries in a microservices environment.\n\nDon’t wait eternally In a microservices environment, a web page could make a large number of service requests for rendering each page, which will mean that slow response times from a single service could turn the user experience into something abysmal. It’s imperative that, when you consume your own APIs, you define timeouts that, if reached, will not make your site unusable.\n\nThe one thing that will always happen in a microservices environment is seeing service requests time out. In a distributed application, there are a multitude of the reason for seeing such timeouts, be they general networking snafus, network congestion or just general latency increases along the full trajectory of a service request.\n\nDon’t make the clients smart We recommend you never put any sort of business logic in a client library, be it some constant of default values, or any logic that may seem straightforward to code as you’re working on your gem. For example, in our book store example, we will need to know whether or not a specific book is available for purchase at a specific moment in time. Following the initial, very simple requirements for out hypothetical bookstore logic, a book would be considered available if we have more than 0 copies in stock, which seems fairly obvious, however also naive.\n\n76",
      "content_length": 3098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Chapter 6 ■ Consuming Your apis\n\nIf we decide to implement an available? method in our Ruby library, it would look like:\n\ndef available? stock>= 1 end\n\nSomeone, even yourself, could try to convince you that having that logic live in the client library is not harmful, and even more efficient than requiring the service to vend yet another attribute. Don’t be fooled by that influence because every piece of business logic starts simple and gets more complex with time. Keeping the theme of our hypothetical future evolution, your book store may start selling books that can be considered as “always available” because they can be printed on-demand; so, you decide to make tweaks to your “in-library” code.\n\ndef available? type == 'on_demand' || stock >= 1 end\n\nThat wasn’t that bad; however, you now need to release a new version of your library, and get all\n\nconsumers of that data to migrate to it, which may not be that simple, and will cause, even if for a small amount of time, to have different clients using different versions of the library that have a different implementation of that method in production.\n\nIn another unsurprising turn of events, you find out that the bookstore has a new supplier that promises\n\nsame day delivery of books to your warehouse; so, the business wants to not lose any sales of books that belong to that supplier; so, you need to tweak your implementation, you decide to add a new attribute to Book resource called inventory_management, which, when set to allow means that a book can be sold even when it is not in stock. Off you go to update your client library, which now reads:\n\ndef available? inventory_management == 'allow' || type == 'on_demand' || stock >= 1 end\n\nThe code is starting to look like spaghetti, we’ve seen how ugly it gets, don’t do that. Once again, we will have the issue of possibly having clients using different versions of the library at some point in time. Also, in an ideal world, you’d never have to expose the inventory_management to certain clients, it’s an implementation detail that should be kept inside the domain of the books-service; however, it had to be exposed to allow the logic in the library to be updated. We’ve been showing you the available? method implemented in Ruby, but, if your application happens to run in an environment where multiple languages are used, then, you would also have to implement that code in each one of those languages; so, trust us, business logic should never live in a client library. The general rule should be to keep your client libraries as dumb as possible, and move the business logic into service endpoints instead.\n\nService Orchestration We can also offer an important positive recommendation: Try to understand how you can orchestrate the information retrieved, so you can optimize the workflow and cut down on load time.\n\nIn some cases, when working on user-facing applications, you may notice a specific pattern of service\n\naccess that could be optimized. For example, suppose you know that on a book detail page, you need to access detailed book information from books-service, as well as a large amount of information from a set of other services, such as images, reviews, or categories. You may be tempted to add a class into your client library that could load data from those services in parallel, and return it in a faster fashion.\n\n77",
      "content_length": 3365,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Chapter 6 ■ Consuming Your apis\n\nThat type of connected service call is what is called service orchestration, and it is very helpful for increasing\n\nthe efficiency from an end-user point of view. We’ll discuss in Chapter 7 the way backend-for-frontend (BFF) services have started to be used for exactly that reason, and we recommend you take that approach—add orchestration to a BFF, and have a thin client library for that service, which will give you all the advantages of optimizing your workflow to improve performance, and will keep logic away from your client libraries.\n\nCaching in the Client In order to increase speed, caches are often used in distributed systems, and a layer that sometimes becomes responsible for caching is the client library. It seems like a decent choice for caching because, by handling caching in the client library, it makes caching transparent to its clients, which can be seen as an advantage, reducing system complexity.\n\nIn quite a few cases, service clients can often tolerate data that is slightly stale. A good way to exploit\n\nthis fact is to cache service responses within the client application (which can of course be either a consumer-facing front-end application or some other service that depends on data from your service). If, for example, the client can tolerate service data being 5 minutes old, then it will incur the interapplication communication costs only once every 5 minutes. The general rule here is that the fastest service calls are the ones your applications never need to make.\n\nBuild Caching into Your Client Library Often the team that builds a service will also be developing the libraries (such as Ruby client gems) to access that service. Some of these client libraries are built to offer the option to have a cache store object injected into them via a configuration option. A cache store injected that way can be used to store all response objects returned by the service. A prudent approach when allowing cache store injection in your client library is to require as little as feasibly possible about the cache object that is injected. That strategy provides the client applications with more flexibility in choosing which cache store to use.\n\nCaching should be entirely a client application’s responsibility. Some clients might not want to cache at all, so make sure to provide the ability to disable caching in your client. It is also the client application’s responsibility to set cache expiration policies, cache size, and cache store back-end. As an example, Rails client applications could simply use Rails.cache; some others might prefer a Dalli::Client instance (backed by Memcached), an ActiveSupport::Cache::MemoryStore (backed by in-process RAM), or even something that is entirely “hand-rolled” by your clients, as long as it conforms to the protocol your gem code expects of the cache store.\n\nWe’ll go into more detail about (mostly service-side) caching in Chapter 7, and just recommend that\n\nclient libraries offset caching to standards—HTTP headers—or have a pluggable caching mechanism.\n\nError Handling In a distributed environment, properly handling errors is extremely important because errors will happen due to the simple fact that there is a network between two systems making a call.\n\nIn systems that do not properly handle errors, one service could bring down an entire application when issues arise. We’ve seen many times a not-so-important service A start to return errors that are not properly handled by its clients, and thus generate a series of “cascading failures” that can potentially bring down other services or entire applications.\n\n78",
      "content_length": 3641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Chapter 6 ■ Consuming Your apis\n\nTimeouts The main cause of failures in a microservices architecture will probably be timeouts, due to the network between client and server. In order to handle those types of errors, you should have timeouts—ideally with exponential back-offs—on connections and requests to all services you connect to. When services take too long to respond, a client can’t possibly know if the delay is caused by expensive computation, or because the connection has failed. In distributed systems, you should always assume that a slow synchronous response is a failure; and having timeout values set very low means that your client code will not wait too long for a service response, and can handle that condition faster.\n\nWe recommend you set aggressive timeouts that will keep your systems honest. If you have synchronous endpoints that are sometimes very slow to respond, you have to make sure you fix all performance bottlenecks in those services, instead of increasing the timeout on the client. We’ve seen clients that have very long timeouts; anything above 1 second should be unacceptable, which means that the timeout is not protecting the client from a slow service, because it’s still waiting too long for a response, and probably making the a user-facing application looks sluggish and incorrect—because of the service failure.\n\nCircuit Breakers A well-known pattern for handling service failures is the circuit breaker, also known as the feature flag. The circuit breaker design pattern is used to detect and isolate repeating failures, preventing system-wide failures caused by an issue with a specific endpoint/service. Circuit-breaker libraries usually allow their users to set a group of settings that characterize the circuit to be defined. Some of the properties that are usually defined in a circuit are\n\n\n\nTimeout: As discussed in the previous section\n\nRetries: The number of times a service call should be retried, in case of errors\n\n\n\nThreshold: The number of errors that will cause the circuit to open\n\nWindow: The period of time to keep the circuit open before a new connection is attempted\n\nIn our sample bookstore application, we display a classic shopping cart icon in the top right of the user interface, and display the number of items currently in the cart, alongside the actual cost of the items in the cart. That interaction point is a good use case for a circuit breaker, and we have a circuit breaker in place. In that specific case, we should have a very slow timeout setting, we don’t really need retries in case of failures, and we have code in place to render an error scenario that doesn’t look like a failure. With the circuit breaker in place, our code properly handles failures, and it stops sending requests to the service in failure state, until it (hopefully) recovers and starts responding appropriately.\n\nclass BooksService include CircuitBreaker\n\ndef find_book(id) ApiClient::Books::Book.find(id).first end circuit_method :find_book\n\ncircuit_handler do |handler| handler.logger = Logger.new(STDOUT) handler.failure_threshold = 5\n\n79",
      "content_length": 3099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Chapter 6 ■ Consuming Your apis\n\nhandler.failure_timeout = 2 handler.invocation_timeout = 10 end end\n\nWe recommend you get in the habit of always having a circuit breaker setup in place. Your application\n\nwill be able to withstand failures in downstream dependencies, while at the same time, it will not overwhelm those services that are under some sort of failure scenario.\n\nSo far, we’ve only touched on handling the errors that happen in a downstream dependency; it’s also extremely important to have some information on how to actually fix those errors. The first place anyone will look when errors start to happen is in service logs; we discuss generic logging in detail in Chapter 13, but the message to convey is that your client libraries are a great location to add logging about service calls. You should log most details your client library uses when requesting data from a service.\n\nIdeally, you should use an error-tracking system, whether built in-house or commercial, such as Airbrake (https://airbrake.io/) or bugsnag (https://bugsnag.com/). Using such tools has some advantages:\n\n\n\nYou can have a central location where your team can see what errors are happening throughout all your services. When a user-facing application starts to experience an increase in errors, the first challenge in a microservices environment is to find out the origin of the error; so having some visibility into error rates of all services is very useful, in stressful situations.\n\n\n\nYou can group similar errors. Some systems can start to generate a really large number of errors; in an ideal world, someone should be notified about that one error with some sort of indication that it is happening quite often. We’ve been in situations where services would generate tens of thousands of error emails that ended up being a single failure, but that would cause email servers to be overwhelmed, delaying the delivery of other, potentially new errors, that could be happening.\n\n\n\nYou can link alarming to escalation and severity policies. The main feature of an error-handling system is to notify the owning team that something is wrong. Depending on how teams are set up in your organization, the error-handling tool should notify one person, and have an escalation policy to alert other individuals if no action is taken. It’s also important to be able to assign a severity to errors, so that errors with a higher impact appear quickly and are given priority over other types of errors.\n\nPerformance Performance is a critical characteristic in a microservices architecture, and we discuss it in detail in Chapter 7. From the point of view of a client library, the message we want to convey is that the performance of a service A can differ from the perceived performance from a client that calls client A. When a client calls a service, network latency, response deserialization, circuit checks, and potentially other tasks will translate into the perceived performance of each service call.\n\nWe have the following performance-related recommendations for client libraries:\n\nHave metrics in place to measure service response time from a client perspective.\n\nOnly request what you need; avoid requesting 100 attributes when all you need is 2.\n\nCheck how much time is spent in each part of a call, as seen by the client.\n\n80",
      "content_length": 3316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Chapter 6 ■ Consuming Your apis\n\nUpdating Client Libraries Once your client library is used in the wild, it will start to evolve, and you should have an upgrade policy for those libraries, similar to how you support only a small number of versions of the service itself. Make sure that you have rules in place that make it easy for users of your client libraries to upgrade, and be as, up-to-date as possible most of the time.\n\nYou should follow a versioning policy for all your client libraries, to better communicate what type of changes your libraries are undergoing as you release new versions. We recommend you follow “Semantic Versioning” (http://semver.org/), as it’s a well-defined policy that is both easy to follow and provides clear guidelines to how the version number should change. In Chapter 4, we talked about versioning the API endpoints in the service; we believe you should never tie the version of the service to the version of the API, as those can evolve in different ways, and having strict rules will not help you move forward quickly.\n\nBriefly, given a version number major.minor.patch, increment them as follows:\n\n\n\nThe major version when you make incompatible API changes\n\n\n\nThe minor version when you add backward-compatible functionality\n\n\n\nThe patch version when you make backward-compatible bug fixes\n\nAlso make sure you have a test suite for your client library; it should most likely be slim. You just have to make sure that you can catch obvious errors before releasing a new version of the library. We cover testing in detail in Chapter 9.\n\nService Discovery One of the things you will have to add to your client libraries is a way of specifying where the service endpoint for each service is to be found. This seems like a very simple configuration parameter, and can even be defined in the client library itself, as all services should follow a naming convention based on the service name and environment in which the service is running. An example of endpoints for the books service could be the following:\n\nDevelopment: books.development.bookstore.net\n\n\n\nStaging: books.staging.bookstore.net\n\n\n\nProduction: books.prod.bookstore.com\n\nHaving endpoints defined in their libraries is a good practice, as it makes it easy for developers to start\n\nusing the library. If you decide to distribute your service endpoints in your libraries, make sure you allow the configuration to accept an override, in case developers want or need to hit a version of the service in a nonstandard host or port.\n\nOne approach we’ve seen and recommend at the moment is to move the knowledge of where services\n\nare to be found to service discovery tools. We’ve successfully used Consul (https://www.consul.io/), which is described as a tool that “makes it simple for services to register themselves and to discover other services via a DNS or HTTP interface. Register external services such as SaaS providers as well.” In our sample project, we make use of Consul, and have our client libraries ask it for service endpoints, which means that you can rely on Consul knowing the correct endpoints to connect to in each of the environments where your application runs.\n\n81",
      "content_length": 3181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Chapter 6 ■ Consuming Your apis\n\nmodule ServiceDiscovery class << self\n\ndef url_for(service_name) client = Consul::Client.v1.http service_details = client.get(\"/catalog/service/#{service_name}\").first # TODO raise if there is no service data endpoint_parts = [] if service_details[\"ServiceAddress\"].empty? endpoint_parts << \"http://localhost\" else service_details[\"ServiceAddress\"] end endpoint_parts << service_details[\"ServicePort\"] URI.parse(endpoint_parts.join(\":\")).to_s end\n\nend end\n\nWe made the complete source of the Ruby service discovery library available at https://github.com/\n\nmicroservices-from-day-one/service_discovery.\n\nSummary This chapter focused on all aspects of consuming APIs. We defined client libraries and discussed their features and responsibilities. We talked about many concerns usually covered in client library implementations, such as…\n\nIn the next chapter will focus on what can be done to optimize the APIs on the service side. We expect\n\nto build APIs that respond quickly and efficiently, and will share some of the techniques we’ve seen that help you keep the promise of fast APIs.\n\n82",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "CHAPTER 7\n\nOptimizing Your APIs\n\nThis chapter establishes guidelines and rules of thumb for designing and implementing APIs with a special focus on performance and maintainability. The majority of the techniques described here can be used to address and—if applied at design time—avoid performance degradation, while others can help to optimize overall service-related implementation and support costs.\n\nMeasuring Performance The great American engineer and statistician W. Edwards Deming has been credited with the quote:\n\nIn God we trust; all others must bring data.\n\nIn our experience, this rule translates very well into the realm of optimizing service APIs. Too often have\n\nwe seen engineers spending significant time fixing perceived performance issues based on mere hunches rather than concrete data, often adding significant complexity to the code without noticeable improvement. When you are trying to improve system characteristics without introducing metrics, and methods to\n\nmeasure the system in terms of these metrics, you are shooting in the dark more often than not.\n\nFinding the root cause for any long-running action often requires several iterations of defining metrics, adding probes, measuring, and finally evaluating results, drilling down further and further into the issue at hand.\n\nHere are some typical questions that we find ourselves asking on a regular basis when researching API\n\nperformance issues:\n\nDatabase related inquiries:Is the bulk of the response time spent inside the database query?\n\n\n\nIf so, are there N+1 query issues, accessing the same database table over and over?1\n\n\n\nIf the number of queries seems optimal, which particular query is run and takes the bulk of the time?\n\nHow big is the result set of the query, and do all the returned rows need to be processed in application code?\n\n\n\nCan I analyze the query's EXPLAIN plan to see if table indexes are optimally used?\n\n1Note that repeatedly running inexpensive queries can at times be justified if the equivalent SQL query involving a joining of tables turns out to be more expensive. As always, make sure you measure the impact of both options before deciding which approach to take.\n\n83",
      "content_length": 2185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\n\n\nService dependency-related issues:\n\n\n\nIs the bulk of the response time spent making dependent service calls, and do the downstream dependencies meet their advertised Service Level Agreements (SLAs)?2\n\n\n\nIf so, can some of the calls be backgrounded, made concurrently, or replaced by calling more efficient APIs on the dependencies?\n\n\n\nAre all external calls in circuit breakers with the appropriate settings (like timeouts and maximum number of retry attempts)?\n\n\n\nIs the data returned by the external dependencies optimal (that is, minimal in size and complete in content)?\n\nMemory allocations and CPU cycles:\n\nHow memory-intensive is your API endpoint, and does it spend a significant amount of time allocating, deallocating, or swapping memory to and from disk?\n\n\n\nIs there resource contention with other applications or processes on a co- hosted environment?\n\nHow many CPU cycles are spent in each section of your code (as researched and visualized by tools like gperftools?3\n\n\n\nBusiness logic profiling:\n\n\n\nCan you add special probes to determine how much time is spent inside the business logic of your application?\n\n\n\nAre you using the most efficient libraries available to deserialize and reserialize your objects? Can some of these serialization steps be avoided altogether?\n\nDo you know the algorithmic complexity4 of your calculations? Are you using the most efficient algorithms and data structures to implement your logic?\n\nWould this API be faster if it was implemented in a different programming language or toolset? (The beauty of an SOA approach is that experimentation around this question can be done in isolation and fairly cost-effectively. A prototypical implementation could be done in, for example, Ruby, and—if found too slow—switched to Go without impact if the API definitions can be kept unaffected.)\n\nA few words of caution: while gathering performance data about your service is important, please note that often such data collecting does not come for free. Therefore, investigating the performance of your metrics-collecting tool, for example of your logging framework,5 is always advisable in order to decide how much data should be selected. Your choice of tool to collect data about your applications should be influenced by the efficiency and performance impact it has. You might, for example, decide to rely less on\n\n2Often, such SLAs are only loosely expressed, and—unless the service dependencies are all internal to your company— unenforceable. If you cannot coerce the team owning the dependency to meet their SLAs, then moving to alternative sources, or caching results (discussed later) might be your only options. 3Main gperftools repository: https://github.com/gperftools/gperftools 4“Analysis of algorithms,” Wikipedia. https://en.wikipedia.org/wiki/Analysis_of_algorithms 5Sarah Ervin, “The Real Cost of Logging”, 17 April 2014. https://dzone.com/articles/real-cost-logging\n\n84",
      "content_length": 2959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\ntraditional logging frameworks, and more on UDP-based (and hence connectionless and faster, but also less reliable) data collection tools like Etsy's StatsD.6 Another thing to consider and watch is how to effectively archive and purge all the data you have collected as part of your performance-measurement efforts. Letting log files eternally accumulate on your application servers is a sure-fire way to cause system downtime. Many log archiving and purging tools exist, which can be integrated with larger data warehouse systems, such as those based on Hadoop (http://hadoop.apache.org/) clusters.7\n\nChapter 13 will have more in-depth information about service application monitoring.\n\nService Access Pattern-Based Improvements\n\nThe very first approach we will explore among the ways to improve performance and design of a microservices architecture is to study the granularity and coherence of the system's APIs.\n\nAs laid out in greater detail in Chapter 3, sometimes service APIs are best aggregated or decomposed based on client access patterns and data joining concerns. If you find that your APIs perform sluggishly, it might be because their functionalities combine write-intensive data (like a bookstore user's reading list) with read-intensive data (like the user's profile data). Separating by such access pattern lines will enable you to introduce more tailored caching strategies, and even separate specialized storage systems with different read or write characteristics optimized for the given access pattern. Similarly, your performance analysis might show that much time is spent joining data retrieved via multiple service calls. This behavior is similar to SQL joins involving multiple tables, and aggregating such often-joined data will improve your API’s performance. In this case, a new service that co-locates the joined data—whether by truly combining and replacing the original downstream services into a service that stores the data authoritatively in a new data store, or simply by caching such aggregated data based on the more granular data from the downstream services—can greatly improve API response times.\n\nAnother consideration might be to optimize the size of the service response to the need of your service's\n\nclients. In situations where a human end-user is waiting for a UI to be rendered based on data vended by your service API, it makes great sense to limit the size of the result set of an API call; the service should enable (or even better, enforce) the UI rendering application to request larger result sets in manageable, coherent chunks (also known as pages). We will focus more on this point later in this chapter.\n\nFor other client use cases, bulk APIs might actually be desirable and efficient. Consider, for example,\n\nthe circumstance of a nightly, offline batch process that runs to aggregate data from several source services. In our social bookstore example, this could be the promotions-service. As mentioned in Chapter 3, this service runs periodically as a background process, sending out emails and push notifications promoting products to users. This service will need to operate on very large sets of users (maybe all female users, as provided by bulk data from users-service APIs), and it will need to mesh the user data up with bulk recommendation data from the recommendations-service. Making such bulk APIs sufficient and maintainable on the service side might require you to add additional pieces of infrastructure. For example, if you notice that the response times of your other client-facing APIs are affected by database connection contention with the queries executed for the bulk APIs, then you could consider adding a replicated DB read-slave8 to service the bulk API queries. If you notice that connection requests to your web server are starting to queue up significantly because of the long-running requests to the bulk APIs, you could consider routing these requests to a dedicated pool of servers that is not part of the general web-application server pool. Try to avoid mixing long-running requests (such as for entire sections of a UI) with requests for data that can be returned much more quickly (like service requests that return within 10 milliseconds).\n\n6Ian Malpass, “Measure Anything, Measure Everything”, 15 February 2011. https://codeascraft.com/2011/02/15/ measure-anything-measure-everything/ 7Peter Dikant, “Storing log messages in Hadoop”, 15 April 2010. http://blog.mgm-tp.com/2010/04/ hadoop-log-management-part2/ 8“Master/slave (technology)”, Wikopedia, https://en.wikipedia.org/wiki/Master/slave_(technology)\n\n85",
      "content_length": 4643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nAnother fairly recent development pattern in the microservices community is to provide a very specialized façade service with APIs highly tailored for particular front-end applications. Some companies even go so far as to add one back-end API service per user experience, as Sam Newman lays out in his article on this “Backend for Frontend” (BFF) pattern.9 Companies like REA and SoundCloud appear to have separate BFF services for their iOS app, another one for their Android app, and a third one to feed UX-specific JSON data to their desktop web application, fronting the rest of their SOA infrastructure and providing the client UIs with data in exactly the format that they need to render the desired experience.\n\nThe Blessing and the Curse of Caching\n\nWhen asked to optimize the performance aspects of their web applications, for many developers the first (and often, only) tool of choice is caching.\n\nA loose definition of this approach is to temporarily hold recently retrieved or computed information\n\nin a data store (for example, in the server's RAM, in a distributed object cache like Memcached,10 or in a key-value store like Redis11) in the hope that future access to this information from that temporary data store (cache store, or simply cache) will be faster than reretrieving or recalculating it. Updating of the information held in the cache with refreshed information is usually triggered by a preconfigured timer expiring, but it can also be manually or programmatically triggered.\n\nCaching in the API Consumer The fastest API is call is one that isn't made in the first place. As mentioned briefly earlier, the API consumer should consider caching if it can live with information that is slightly out of date. The rest of this section will mainly focus on caching approaches in the information providing service. Please see Chapter 6 for details on how to implement techniques for client-side caching.\n\nCaching in the API Provider The advantages of caching seem rather obvious and convincing at first. Computing resources—client- and service-side—can be saved, and network calls can be avoided, resulting in faster information load times for the clients and more responsive websites and more enjoyable, and hence more revenue-generating,12 end- user experiences.\n\nAs is often the case with optimization approaches, caching has its advantages but is not a silver bullet.\n\nWhile caching might work very well for relatively static content and for APIs where clients have a certain tolerance for stale data, it is often untenable for information that is very dynamic, very large, or where the clients have very low tolerance for data staleness. Consider the case of the inventory count of a given book in our bookstore example; this count will change very rapidly for popular books, while it might change much more slowly for others, so finding the right time-to-live for a cache for this information is very hard. Additionally, basing any sort of “sold out” decision on a cached count will be very unreliable.\n\nAdditionally, adding caching and with it additional pieces of infrastructure (like Memcached, for example) adds cost and general complexity to an application. The process of procuring, configuring, integrating, and maintaining such solutions (and the related cache writing and retrieval code) is often nontrivial.\n\n9Sam Newman, “Pattern: Backends For Frontends”, 18 November 2015; http://samnewman.io/patterns/architec- tural/bff/ 10https://memcached.org/ 11http://redis.io/ 12Sean Work, “How Loading Time Affects Your Bottom Line”, April 2011; https://blog.kissmetrics.com/ loading-time/\n\n86",
      "content_length": 3656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nAlso, adding caching might actually turn out to be harmful to performance. It is not uncommon for\n\nengineers to discover that refreshing the information they were caching overall took less time than storing it to, and fetching it from, the cache storage they introduced. This even sometimes applies when the information authoritatively lives in a relational database that needs to be queried, especially if the cache store is distributed and hence also only reachable over a network boundary.\n\nIn such cases, the developers fell prey to the cardinal sin of “early optimization,” adding complexity without reaping any benefits (or worse, actually degrading performance). Other dangers of caching are the unintended layering of several caches: services might cache information retrieved from downstream services, not knowing that the downstream service already introduces a caching layer around the very same data. As a result, the worst-case staleness of the data exposed to the clients of the first service is the accumulated time-to-live of all downstream caches.\n\nFinally, there are often unintended interactions between the deployment of new code and the expiry of\n\ncache stores in an application. If the information stored in the cache stores has interactions with the newly deployed code, the deploying engineer will only truly be able to determine compatibility between the new code and the cached information once the cache store expiry has taken effect (which could sometimes be hours or days after deployment of the code regression).\n\nCaching API-provided data also doesn't always have to happen inside the providing service itself or downstream from it (as seen from the client) in a key-value store or Memcached server. A caching proxy that is inserted between the client of an API and the API provider might be a useful tool, which we will explore in the next section.\n\nMake Your Service Work Less Retrieving information from a data store and subsequently serializing it to vend it to clients takes valuable time in your API’s request/response cycle. In this section we describe two ways to reduce, or even entirely avoid, incurring this time.\n\nUse the Standard The HTTP standard comes with a set of built-in caching support methods, collectively known as HTTP conditional GET. These cache support functionalities are all implemented using a set of HTTP request and response headers.13 We will not explain the full set of all the HTTP headers, but here are a few to be noted.\n\nIf-Modified-Since and Last-Modified\n\nThe If-Modified-Since request header and the Last-Modified response header work as a pair, as depicted in Figure 7-1.\n\nFigure 7-1. Last-Modified and If-Modified-Since headers\n\n13“List of HTTP header fields”, Wikipedia; http://en.wikipedia.org/wiki/List_of_HTTP_header_fields\n\n87",
      "content_length": 2838,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nIn this example, the client (left) makes its first request for the book with ID 1 as JSON, not sending any\n\ncaching-related request headers. The service responds with the serialized book data, and it includes a Last- Modified response header with the last modification date and time for the book resource. The client now can store that date/time, until it is time to make the next request for the same book's data. In step 3, when the client sends a request for the same book, it can add the date/time retrieved in step 2 as an If-Modified- Since request header. If nothing has changed since the requested modification time, then the server can return a simple 304 Not Modified response.\n\nEven though no direct data caching is involved for the second request/response cycle, there are several\n\nadvantages to this scenario. First, the data transferred over the network is relatively small, as only headers are transferred, but no JSON data about the book.\n\nAs a corollary, the server will also not spend time serializing the book object retrieved from storage to\n\nJSON; the service will only need to calculate the modification time stamp and compare it to the time sent in the If-Modified-Since request header.\n\nOn the client side, efficiency is gained because the 304 status code of the response tells it that it can\n\nsimply reuse the data retrieved in the previous service response's body; no renewed deserialization of JSON-encoded data into programming language objects is needed.\n\nHowever, note that here, as well as with the ETag and If-None-Match headers discussed next, you will\n\nstill incur the costs of making network calls and retrieving the result object from the data store.\n\nETag and If-None-Match\n\nAnother such pair of request and response headers is If-None-Match and ETag. Figure 7-2 illustrates a similar example scenario as in the previous section, but adapted to this header pair.\n\nFigure 7-2. ETag and If-None-Match headers\n\nIn this example, the client again first makes a request without sending any related headers. The server\n\nresponds with the data and a response header with a unique Entity Tag (also called ETag) for the data, calculated server-side. Any string that uniquely identifies the content of the resource will do, and many implementations simply calculate an MD5 hash14 of the content to serve as this tag.\n\nIn step 3, when the client next requests the same book, it now adds the ETag it has last received (and\n\nsubsequently stored) into the If-None-Match header of the request it sends to the service. The server again calculates the entity's unique hash and, in this case, finds that the requested entity's tag has not changed when compared to the tag sent by the client.\n\nAs a result, the service can skip the book object JSON serialization, and can simply respond with an\n\nempty-body 304 Not Modified response. The client can now also react to this 304 response appropriately by reusing the previously retrieved book object. In short, the performance and efficiency advantages encountered in the early If-Modified-Since / Last-Modified remain the same for ETag.\n\n14“MD5”, Wikipedia; https://en.wikipedia.org/wiki/MD5\n\n88",
      "content_length": 3194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nThe Cache-Control Header\n\nThe HTTP Cache-Control response header can be used to help the service control all caching entities along the request/response chain to the end-clients. This includes intermediate (and potentially caching) proxies, browsers (which are built to adhere to this header), and other consuming clients (like language or platform code libraries).\n\nWe will not exhaustively explain all the cache directives that can be included (and combined) in the\n\nCache-Control header, but here are some important ones we advise knowing and using in an SOA:\n\n\n\nprivate: Browsers and client-side caches can freely store the response marked with this directive; shared (intermediate) caches and proxies are not allowed to cache the content.\n\n\n\nno-cache: Sending this directive in the Cache-Control header forces all caches (intermediate and end-client based) to resubmit the request to origin server before releasing a cached copy; this is often useful for API endpoints that require user authentication and authorization.\n\n\n\npublic: This directive marks responses as cacheable (as private is the default).\n\n\n\nmax-age=[secs]: This directive indicates the maximum time in seconds the result should be cached and considered fresh, relative to the time of the request. The max- age directive is usually combined with the private or public directive.\n\nThe advantages of using cache control headers in your microservices infrastructure are manifold for performance optimization. It is an approach to caching “done right,” as it lays the responsibility of cache expiry on the service, so there is a central place to expire it. It also helps to enable caching proxies (discussed shortly) to function as a transparent part of the tech infrastructure (via the public directive), thereby relieving the service itself of any additional unnecessary load. Finally, it enables clients to entirely prevent making potentially expensive network calls by letting them know how long the previously retrieved response should be used unchanged.\n\nFramework Support for HTTP Conditional GET As mentioned in Chapter 6, many programming languages provide client-side libraries that can process, and appropriately react to, the HTTP headers for conditional GET functionality. An example is the faraday- http-cache Ruby gem (https://github.com/plataformatec/faraday-http-cache), which works together with the lower-level HTTP communication gem faraday. Apple's iOS operation system and URL Loading System15 come with built-in support for iOS mobile client apps that use foundation classes like NSURLConnection or NSURLCache. Similarly, the popular OkHttp library for Android (https://github.com/ square/okhttp) also supports the cache control headers for storing previous results, and it reacts to ETag headers appropriately.\n\nOn the service side, there are also a large number of frameworks and libraries that support generating\n\nthe necessary request headers. As an example, see Figure 7-3, which shows a Rails code snippet found in our bookstore sample application's users-service.\n\n15Mac Developer Library, “About the URL Loading System”, https://developer.apple.com/library/ mac/#documentation/cocoa/conceptual/urlloadingsystem/urlloadingsystem.html\n\n89",
      "content_length": 3268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nFigure 7-3. Users#show action with HTTP cache control\n\nThis example shows the entire Ruby code necessary to implement the API to data about a given bookstore user account, based on the user's UUID. The important parts in relation to the topic of HTTP conditional GET are the stale? and expires_in helper methods that are built into the Ruby on Rails framework.\n\nCalling the stale? method here with the :etag and the :last_modified option results in Rails\n\ncalculating a hash value for the passed user object (as retrieved from the database), and a date of last modification of the user, based on the date its database row was last updated. Both of these data points are then automatically included in the API's response headers (as an ETag and a Last-Modified date, respectively). Additionally, Rails checks these two data points against the If-None-Match and If-Modified- Since headers from the incoming client request. If these request headers do not match the :etag and :last_modified options provided to the stale? helper, the user resource is considered stale, and the user JSON representation is generated by serializing the user object hash. Otherwise, the user resource is seen as fresh and the JSON will not need to be regenerated; the service API can simply reply with a bodiless 304 Not Modified response.\n\nThe expires_in helper call, which is executed only if the user resource is considered stale, sets the Cache-Control response header's max-age directive to 300 seconds (5 minutes), and indicates via the public option that intermediate proxies can cache the user resource.\n\nThe following cURL commands show this API’s conditional HTTP GET behavior:\n\ntim$ curl -v \\ > http://localhost:5002/users/3a07468a-f97d-4242-9057-8437385efb29 * Trying ::1... * Connected to localhost (::1) port 5002 (#0) > GET /users/3a07468a-f97d-4242-9057-8437385efb29 HTTP/1.1 > Host: localhost:5002 > User-Agent: curl/7.43.0 > Accept: */*\n\n90",
      "content_length": 1969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\n> < HTTP/1.1 200 OK < X-Frame-Options: SAMEORIGIN < X-XSS-Protection: 1; mode=block < X-Content-Type-Options: nosniff < ETag: \"616210ace8e9681c485ad8c71efcc5b2\" < Last-Modified: Sun, 28 Feb 2016 06:37:23 GMT < Date: Mon, 04 Apr 2016 02:48:24 GMT < Content-Type: application/json; charset=utf-8 < Cache-Control: max-age=300, public < X-Request-Id: 028cb9d3-1965-4f5a-be28-0db8a06c3086 < X-Runtime: 0.128530 < Transfer-Encoding: chunked < * Connection #0 to host localhost left intact {\"user_id\":\"3a07468a-f97d-4242-9057-8437385efb29\",\"name\":\"Tom Copeland\",\"email\":\"tom@books. com\"} tim$ curl -v \\ > -H 'If-None-Match: 616210ace8e9681c485ad8c71efcc5b2' \\ > http://localhost:5002/users/3a07468a-f97d-4242-9057-8437385efb29 * Trying ::1... * Connected to localhost (::1) port 5002 (#0) > GET /users/3a07468a-f97d-4242-9057-8437385efb29 HTTP/1.1 > Host: localhost:5002 > User-Agent: curl/7.43.0 > Accept: */* > If-None-Match: 616210ace8e9681c485ad8c71efcc5b2 > < HTTP/1.1 304 Not Modified < X-Frame-Options: SAMEORIGIN < X-XSS-Protection: 1; mode=block < X-Content-Type-Options: nosniff < ETag: \"616210ace8e9681c485ad8c71efcc5b2\" < Last-Modified: Sun, 28 Feb 2016 06:37:23 GMT < Cache-Control: max-age=0, private, must-revalidate < X-Request-Id: 1bde6ef0-dc77-4647-b7b8-0cfc1a73dde4 < X-Runtime: 0.003823 < * Connection #0 to host localhost left intact\n\nAs can be seen, the first cURL request for a user (in which we send no additional headers) returns the HTTP/1.1 200 OK response with the serialized user object in the response body. But also pay attention to the relevant headers returned by the users-service:\n\n< ETag: \"616210ace8e9681c485ad8c71efcc5b2\" < Last-Modified: Sun, 28 Feb 2016 06:37:23 GMT < ... < ... < Cache-Control: max-age=300, public\n\nThe Rails helper-generated ETag, Last-Modified, and Cache-Control headers are present as expected.\n\n91",
      "content_length": 1886,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nIn the second cURL request, we include an If-None-Match request header whose value matches the previous response's ETag header value. This time, the Rails stale? helper can match the values, and the user resource is considered fresh. As a result, the service responds with a simple, empty-body HTTP/1.1 304 Not Modified, thereby saving service-side serialization and client-side deserialization of the user object.\n\nCaching Proxies Even if service clients do not send or honor HTTP conditional GET headers, reverse proxies added between your clients and your service will be able to do so. For its most-trafficked internal API services, LivingSocial relies heavily on Varnish (https://www.varnish-cache.org/), a reverse proxy that has excellent performance and scaling characteristics. Using Varnish, we saw some endpoints speed up by a factor of 50. It is effectively just constrained by the speed of the network. Figure 7-4 shows a diagram of inserting Varnish into the request path.\n\nFigure 7-4. Varnish inserted into the request/response path\n\nUnder normal configuration settings, Varnish respects Cache-Control headers, caching content only\n\nwhen the service tells it to do so (via the public directive). Varnish can be configured to insert the If- Modified-Since and/or If-None-Match request headers if the service APIs' clients do not send them by themselves, thereby alleviating load on the service, but still be able to serve the content as requested to the clients.\n\nVarnish comes with a grace mode, in which it uses objects in its cache even if their TimeToLive has expired. This means it can deliver old content while concurrently fetching the latest content from the back- end server.\n\nVarnish is also flexible enough to function as a “hold the fort” cache: if the service fronted by Varnish\n\nis down, the proxy can return to the clients the last “good” response it received (that is, the last one carrying HTTP status code 200) from the backend service. This helps overall system uptime, as Varnish can continue to serve stale data while no healthy back-end service is available.\n\nIt can be administered to cache based on a full URI, including or excluding headers. Three tips for\n\nconfiguring your reverse proxy for maximum efficiency and correctness:\n\n\n\nTry making all query parameters sorted, so that any reverse proxy can yield a higher cache hit rate.\n\nMake sure all parameters that do not directly affect the API's JSON response bodies (for example, a client device platform) are sent in request headers, and instruct Varnish to ignore these headers for its cache key.\n\n\n\nExclude API endpoints that require authentication and authorization from being cached in the proxy.\n\n92",
      "content_length": 2729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nSize Matters One of the most common reasons we have seen for sluggish APIs is that no consideration is given to the size of the results returned by a single call to the endpoint. Large results, or sets of results, do not scale, as they hurt performance in at least three key areas:\n\n\n\nIn the service: Large amounts of memory might need to be allocated and deallocated to generate the object from which the results are generated. Once the result objects are created, time is spent serializing them into a wire format (JSON, XML, or the like).\n\n\n\nIn transit: A large number of consecutive packages need to be transferred via TCP over the network. This can be especially harmful in an infrastructure with low bandwidth, low reliability, or high latency, for example when cellular networks are involved.\n\n\n\nIn the client: Clients need to spend a significant amount of time waiting for the arrival of the complete data set. Even if the data connection is broken, a larger timeout needs to be set on circuit breakers to detect failure conditions correctly, so resources will be tied up for a longer time. Once the data has successfully been transferred to the client, significant time is spent deserializing large amounts of serialized data into objects, for which memory again needs to be allocated.\n\nThe rest of this section discusses ways to help you reduce the size of your services' API responses.\n\nResult Representations Not every client needs all the information the service can expose about a given resource or set of resources. Some clients might just need two or three attributes out of potentially tens, or even hundreds, of existing properties to implement their logic.\n\nA good way to provide flexibility with regard to the amount of information returned about an API resource is to honor requests for different representations of an entity. Try to make it easy for clients to define (and iterate over) the best representation for their use case.\n\nConsider a client calling an API in our books-service that returns information about a (set of) book(s).\n\nAll the information about a book that is available to the books-service includes such properties as a full description, a synopsis, and maybe a list of all customer reviews. While it might be entirely appropriate for some clients to retrieve all of this information (maybe in order to display it to the web site user on a detail page), there are many use cases in which the vast majority of this information remains unused (for example, when there is just a small “product tile” about the book to be displayed in an up-sell scenario on another book's detail page).\n\nAnother good candidate for information that might be worth removing from a resource's representation\n\nis any detail information about secondary resources. In the previous example, it might often just suffice to include a book review “star count,” review title, and the review resource ID on an API response for the book resource representation vended to the book's detail page. The full information (if needed) about a particular book review can later be requested from a reviews-service.\n\nImplementing a given set of well-known, predefined representations is often done entirely inside the service that is the system of record for the resource to be retrieved in different representations. Taking the example of our books-service, it gets to define which properties of a book any given named representation includes; clients just request the resource by specifying the name of the representation in which they would like to receive it.\n\n93",
      "content_length": 3600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nThe communication of the requested format is most commonly either sent in an Accept header:\n\nGET /books/841a1420-fabe-11e5-9d14-0002a5d5c51b HTTP/1.1 Accept: application/vnd.bookstore.full+json\n\nOr it is sent as an additional representation query parameter, as shown here:\n\nGET /books/841a1420-fabe-11e5-9d14-0002a5d5c51b?representation=full HTTP/1.1\n\nThe service then often stores an internal mapping between the requested representation name (here full), and a presenter class (in this case FullBookPresenter) that has the serialization logic for the resource. The advantage of this service-side control of the properties of a representation comes to light when a presentation requires change. All updates will immediately be made available to all clients that use this representation, and the code change was done in a single place.\n\nThe flip side of the previous point is that in a purely service-controlled representation definition scheme, changes requested by (internal or external) client teams often require cross-team communication and coordination, as the service is most often owned by a different set of engineers than the client applications.\n\nThis is what the partial resources pattern discussed next is trying to address, and we prefer that pattern\n\nbecause it leads to better decoupling and separation of concern in a microservices architecture.\n\nPartial Resources In contrast to the service-controlled resource representation approach, this technique for reducing response sizes is driven by the client.\n\nWhen applying the partial resources pattern, the client specifies the exact subset of resource attributes\n\nof a full set of attributes for a resource it requires as part of the request to the service.\n\nAs an example of how this can be implemented, let's consider the Google Calendar service API.16 To request a partial resource from most Google exposed APIs, the clients can add a fields query parameter. The value of that parameter can then be configured to specify a custom set of the fields the client wants returned.\n\nLet's assume we have the following full representation of a book resource, requested via GET http://\n\nlocalhost:5000/books/582cbc4f-6e04-4d77-a129-616efe01251c:\n\n{ \"book_id\": \"582cbc4f-6e04-4d77-a129-616efe01251c\", \"title\": \"The Story of Diva and Flea\", \"slug\": \"the-story-of-diva-and-flea\", \"description\": \"Diva, a small yet brave dog, and Flea, a curious streetwise cat, develop an unexpected friendship in this unforgettable tale of discovery.\",\n\n\"isbn\": \"978-1484722848\", \"price\": 9.87, \"full_price\": { \"currency\": \"USD\", \"value\": 987, \"exponent\": 2 },\n\n16Google Calendar API documentation, “Partial response”, https://developers.google.com/google-apps/ calendar/performance#partial-response\n\n94",
      "content_length": 2778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\n\"weight\": null, \"height\": null, \"width\": null, \"depth\": null }\n\nRequesting a partial response for the book resource that will only include the book's UUID, its title, and\n\nthe ISBN number will look like this:\n\nGET http://localhost:5000/books/582cbc4f-6e04-4d77-a129-616efe01251c?fields=book_ id,title,isbn\n\nThe resulting JSON body of the service response should then be as follows:\n\n{ \"book_id\": \"582cbc4f-6e04-4d77-a129-616efe01251c\", \"title\": \"The Story of Diva and Flea\", \"isbn\": \"978-1484722848\" }\n\nThis is a very simplistic example of what is possible with Google's partial response approach. The supported syntax of the fields parameter to specify the elements to include in the response JSON is loosely based on XPath (https://www.w3.org/TR/xpath/).\n\nThis “tell, don't ask” approach to representation definition is very convenient if the clients and the\n\nservice are not under the control of the same team or even company. As a disadvantage, it will be harder to deprecate or change certain fields of a given resource if there is no control over which properties are requested and exposed to client applications.\n\nHTTP Compression Another way of condensing the response size without removing attributes from resources is to condense the actual response.\n\nHTTP compression17 is the standard supported way to losslessly reduce the response body size. The two\n\nmost common compression schemes are\n\nGZip (http://en.wikipedia.org/wiki/Gzip)\n\nDeflate (http://en.wikipedia.org/wiki/DEFLATE)\n\nCompressed HTTP resources are requested by using the Accept-Encoding request header (for example,\n\nAccept-Encoding: gzip, deflate). The service will then indicate in the Content-Encoding response header which type of encoding (if any) has been applied to the response body (Content-Encoding: gzip). Anyone should consider leveraging GZip to compress JSON Responses. Many implementations of this\n\nscheme we have seen have reached a 60–80% reduction in response payload size.\n\nImplementations of these compression schemes are widely available in many languages and\n\nframeworks. JVM-based languages can use java.util.zip.GZIPOutputStream,18 while Ruby applications can use the Rack framework middleware Rack::Deflater19 for both Deflate and GZip-based compression.\n\n17“HTTP compression”, Wikipedia; http://en.wikipedia.org/wiki/HTTP_compression 18Java Platform Standard Edition 8 Documentation, “Class GZIPOutputStream.” https://docs.oracle.com/ javase/8/docs/api/java/util/zip/GZIPOutputStream.html 19RubyDoc, “Class: Rack::Deflater”; http://www.rubydoc.info/github/rack/rack/master/Rack/Deflater\n\n95",
      "content_length": 2624,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nA more centralized place to handle HTTP compression outside the application server is at the web server level. Examples here are mod_deflate for the Apache web server and ngx_http_gzip_module for NGINX. We recommend compressing inside the web server if only for efficiency.\n\nSemantic Paging Much as there are service clients that will not need all the information a service can expose about a given entity, clients will very often not need all objects in a single response of API endpoints that vend lists of resources (such as search endpoints). Requesting a chunked (often called paged) set of the full result list is often a good way to reduce the size of such API responses.\n\nIn this scheme, the service regulates how many entities for a given resource list can be included in the\n\nresponse. While the client might request the entire list of entities, the service will not allow such a potentially huge response to be returned. If the API were to return all instances of a given resource (for example, all books in the store), then it could easily overrun the memory and processing capacity of the consumer. Instead, the service returns a subset of all requested entities (a page), and a set of links that indicate how to request the previous and/or next page of the result set.\n\nThere are quite a few ways to implement a URI and parameter scheme, but we suggest using pagination\n\nparameters called offset and limit at a resource list's URI to enable paging. An example URL for our books-service to retrieve 25 books, starting at a book with ID 582cbc4f-6e04-4d77-a129-616efe01251c would be the following request:\n\nGET http://localhost:5000/books?offset=582cbc4f-6e04-4d77-a129-616efe01251c&limit=25\n\nThe limit parameter is fairly self-explanatory; it simply signifies the maximum size of the page, so that\n\nin this example no more than 25 books ever get returned.\n\nThe offset parameter has semantic meaning, though. It is meant to contain the value of a sortable property of the resource (such as a UUID, ID, or the creation or update date) based on the most optimal way to access the resource's data. This semantic offset approach gives the engineer who creates the API a chance to choose an optimal way to do an offset into a result set.\n\nGood APIs do not require the consumer to know much about the details of this semantic offset, or\n\neven an ideal page size limit value. Service APIs should lead consumers through the consecutive API invocations to help them along while returning the full result set page by page. This means leading the consumer forward and backward through the full set of results, rather than making them guess or keep track of where to go next.\n\nIn order to achieve this guidance, each API result set should include hyperlinks that provide URIs to the\n\nnext and previous pages (if available). This is one of a set of principles that are collectively called Hypertext as the Engine of Application State (or HATEOAS).20\n\nIn a very simply implementation of a HATEOAS approach, based on the draft IETF standard for HAL (JSON Hypermedia API Language),21 here is how an example response to the earlier request to our books- service might look:\n\nGET /books?offset=582cbc4f-6e04-4d77-a129-616efe01251c&limit=25 { \"_links\": { \"self\": { \"href\": \"/books?offset=582cbc4f-6e04-4d77-a129-616efe01251c&limit=25\" },\n\n20“What is HATEOAS and why is it important for my REST API?” The REST CookBook. http://restcookbook. com/Basics/hateoas/ 21M. Kelly, “JSON Hypertext Application Language”, 3 October 2013; https://tools.ietf.org/html/ draft-kelly-json-hal-06\n\n96",
      "content_length": 3610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\n\"next\": { \"href\": \"/books?offset=e52cbf4f-6eeb-2067-b249-2345ecd02587&limit=25\" }, \"prev\": { \"href\": \"/books?offset=1345bc34-4ab2-23f3-02da-235ee01234fa&limit=25\" } }, \"items\": [ { \"book_id\": \"582cbc4f-6e04-4d77-a129-616efe01251c\", \"title\": \"The Story of Diva and Flea\", \"slug\": \"the-story-of-diva-and-flea\", ... } ... ] }\n\nIn this example, the “_links” property provides links to help the API client navigate through the result\n\nset:\n\n\n\n\"self\" is the URI identifying the current result set.\n\n\n\n\"next\" is the URI for the next set of results with the same page size.\n\n\n\n\"prev\" is the URI for the previous result set with the same page size.\n\nA Word of Caution\n\nFront-end applications (like web applications) often need to render end-user–exposed views with paging links with data from the API result's _links section.\n\nFor example, when a web application displays a user view containing paginated lists of (for example, book) data, a next link must be embedded into the view, so that the client web app knows where to fetch the next page of API data. Developers should avoid trying to construct new URLs to the API, but rather depend on the next and previous links returned by the API.\n\nIn an attempt to prevent Denial of Service attacks to the internal backing API service, it is recommended\n\nto encrypt the pagination URIs embedded in client views, so that it will be harder for attackers to make arbitrary calls to the (internal) back-end services.\n\nGiven the example results in the previous section, a client web app might generate links in its book list\n\nview that are similar to the following Ruby On Rails/ERB template example:\n\n<a href=\"/results?page=<%= encryptor.encrypt_and_sign(\"/books?offset=582cbc4f-6e04-4d77- a129-616efe01251c&limit=25\") %>\">Next Page</a>\n\nThe code in the end-user–facing web application that renders the /results view can use the value of the page parameter to fetch the next page of data from the backing books-service API, while at the same time shielding books-service from direct exposure to malicious requests.\n\n97",
      "content_length": 2086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nOptimizing for Efficiency of Development and Support When designing distributed systems you will encounter many situations in which there is no clear “silver bullet” solution to a problem. The same holds true for the trade-off between incurring development and maintenance costs by duplicating code in a client application, and increasing complexity by introducing an additional dependency in your service' clients.\n\nIn our experience, engineers in companies that are new to implementing an SOA side with the “copy and paste” (that is, code duplication) alternative as their default choice. To clarify our use of “copying and pasting,” we even consider extracting common functionality into a library that is shared among client applications a duplication of code (albeit slightly better than literally copying code from one repository to another).\n\nWhile there might be good reasons to take such an approach, it is often taken without much further analysis or foresight, motivated rather by a focus on the extreme short term, based on the perception that this strategy will unblock a task with the least effort.\n\nBecause it’s our experience that the balance is heavily skewed in favor of code duplication in the real\n\nworld, we have focused this section on listing a number of indications and scenarios where we think that a centralized implementation of a feature should be strongly considered:\n\nDistributed ownership: If not all client code is not owned and maintained by the same team, then the cost incurred in making changes (such as bug fixes or enhancements) is augmented by overhead for coordinating the owning teams, and to line up their respective timelines.\n\nRisky upgrades: Assess the risk for upgrading and deploying the client(s), and compare it to the same assessment for making a change centrally in the service. We have often found that a central change (with the same option to also revert the change centrally) appears to have less risk involved.\n\nAvailability of sane defaults: One of the main risks in centralizing functionality is that a single point of failure is introduced in the dependent clients. This risk can be mitigated by building default values or functionality into the clients to fall back to in case the service API fails. Those defaults could even be delivered as part of a service access library, or a client could be programmed to store the last known good service result, and choose to fall back to it.\n\n\n\nPublic client apps or libraries: If the client applications are released externally, for example, as native applications on mobile or desktop platforms, or as code libraries, you will often find yourself in a situation where you will need to support them quasi-eternally. Update cycles to such public clients are usually much longer than for internal apps. Placing functionality in a central service API to be accessed by thin client-side access code will enable quicker maintenance and extensibility.\n\nNeed for immediacy of change: We have often encountered situations where there is a need for a change to take effect immediately in all clients. This point is related to the previous use case, but it’s even applicable to internal clients. One example could be if an item (such as a book) is available for purchase via client applications. If such a decision was made purely client-side (for example, by calculating an inventory count based on sales numbers, or based on a cut-off date and time), then accounting for more dynamic changes in business logic would be very hard or impossible to achieve. What if you encounter all of a sudden that a book has been declared out of print by its publisher, or that your inventory is otherwise defective? If your make the clients rely on the service to answer the question of book availability, you can much more quickly adapt your business logic across all channels.\n\n98",
      "content_length": 3895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\n\n\nExperimentation: Often you don't know ahead of time what the best (most revenue- driving, most convenient, most visually appealing, and so on) behavior to implement in your clients will be. Building several options and feature toggles into your clients and collecting data about such experiments in the service is often the best way to address this. Once you have identified a clear winner among the client-side options, you can direct the clients from the service-side experimentation logic to henceforth only use the optimal option.\n\n\n\nSorting and filtering of lists: Imagine you initially planned to implement the book list view in your bookstore clients' search result display logic solely on sales numbers (or another criterion). But then your store strikes a deal with a publisher, who will pay you money to always make their books appear near the top of the search results. Implementing sorting on the service side enables this change to be immediate. Additionally, this helps to reduce payload sizes of results, and enables paging schemes, as the clients do not need to retrieve, and hold in memory, the entire list to sort appropriately.\n\nComplex logic: Functionalities that are nontrivial, or expensive to implement, are better maintained in a centralized place. Often clients don't all conform to the same programming language or framework, so such logic would need to be implemented, tested, and maintained in several different versions (for example, in JavaScript for single-page apps, in ObjectiveC or Swift for iOS apps, and in Java for Android). Because of the complexity, errors would be more likely, and with the larger number of implementations, they would be more expensive to fix. For memory-, CPU-, or I/O-intensive logic, this might become especially prohibitive if the client is particularly constrained (such as bandwidth- or memory-constrained mobile devices).22\n\nTalk Less, Listen More Splitting up your business domain into a large number of small, focused systems of record can potentially have a negative impact on performance: clients that need to mesh up information from different systems of record will often need to make several orchestrated requests to downstream service dependencies. If the requests are made in a traditional, synchronous matter (for example, using an ordinary RESTful scenario), then each of these requests to dependencies has the potential to fail, or to be slow and generally impede the responsiveness of the client performing the aggregation (no matter if the client itself is another service or the edge of the call chain).\n\nAn alternative to the described synchronous dependency calling approach is the “publish and subscribe\" pattern. This is, in rough and general terms, an asynchronous approach that relies on clients registering with an information-providing service, and subsequently waiting for the service to notify the clients whenever the information subscribed to is updated.\n\nWhile there are certainly other ways to implement this pattern, most mid- to large-scale companies\n\nintroduce Enterprise Service Bus (ESB) solutions to handle reliable, monitored, and persisted message registration, marshalling, and distribution. We will not explain particular ESB solutions or patterns in detail, as that would go beyond the scope of this book. We can recommend Hohpe and Woolf's excellent book Enterprise Integration Patterns23 to readers interested in further information.\n\n22This should not be taken as an invitation to neglect resource efficiency when implementing functionality in a service! You don't want to have long-running service code make clients wait and tie up resources, which might cause client-side circuit breakers to trip, or worse make them entirely unresponsive. 23Gregor Hohpe, Bobby Woolf: Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions. Addison-Wesley Professional 2003.\n\n99",
      "content_length": 3941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nA popular message bus that we have used successfully for more than a year at LivingSocial is Apache\n\nKafka (https://kafka.apache.org/). Originally developed at LinkedIn, Kafka is a distributed publish- and-subscribe messaging system with a special design focus on scalability, performance, and message persistence. In Kafka, the basic point of interaction is a topic, which can be thought of as a log where messages are constantly stored in an append-only fashion. Messages written to a topic can be identified by their positive integer offset, which indicates a message's location in the storing topic. A topic has a (configurable) rolling window for retaining messages that defaults to seven days. Producers generate messages to be published to a topic, while consumers read and consume messages from a topic. Every consumer registers itself as part of a consumer group, which tracks up to which offset messages have been processed. Consumer groups also ensure that messages are only processed once and round-robin- distributed among multiple consumer processes in the consumer group. To guarantee strong durability and fault-tolerance, Kafka brokers are distributed among several servers in a cluster, and they replicate topic messages among each other. Kafka's design allows for guaranteeing that messages are reliably delivered to consumers in the order they were consumed, albeit in very rare cases more than once. While we will not further explain the concepts behind Kafka; this Apache project comes with an extensive amount of excellent documentation.\n\nIn LivingSocial's case, the engineering best practice is to design a system-of-record service such that\n\nit publishes messages about any change event to entity-type–specific topic. This enables current and future client applications that need information about the published entities to consume some or all of the information in these change events.\n\nAs a result, when synchronous requests are received by these upstream client applications, they are no\n\nlonger required to make secondary synchronous requests to the system of record to check for information about entities for which a change event topic exists; they can instead subscribe to the entity's topic(s) and asynchronously retrieve, store, and finally use the information broadcast therein.\n\n■ Note For the scenario just described, it is not advisable for client applications to simply store full copies of all entities owned by downstream services that they depend on, as that would effectively recreate the system of record's entire data store in each and every client. the goal for the dependent client should instead be to pick out only the limited amount of information about downstream entities that is needed for the aggregating client to perform its own purpose.\n\nA LivingSocial-internal example for such an approach is catalog-service, which was first mentioned in Chapter 3 to illustrate the “aggregation to aid system performance” application-partitioning pattern. The service vends data about all disparate types of products in a uniform and searchable manner. In order to do so, it consumes Kafka topic streams published by the various systems of record for each of the product types, plus additional Kafka topics about data regarding product classifications, and so on. The topic consumers for catalog-service live in an entirely separate demon process than the service's APIs that provide the catalog information to catalog-service's clients. The topic consumers consume, aggregate, and store the topic information into the catalog-service database, while the API process uses read-only access to query the database; no secondary service requests are needed when clients call the catalog-service APIs.\n\nFinding a message-based approach in our fictitious bookstore example laid out in Chapter 3 is similarly\n\neasy. The bestsellers-service, purchase-service, and refund-service can communicate in an entirely decoupled manner. When a purchase is made, a record with the number of copies for each book sold is created in purchase-service, which then turns around and creates a message on a purchase-creation related topic, and the message includes the IDs and sales count for each sold book. Similarly, creation of a refund-service-owned refund record will trigger the publication of a message to a refund-creation topic, and this message also includes information about the number of copies for books that are returned and refunded. The aggregating bestsellers-service consumes both the purchase-creation and the\n\n100",
      "content_length": 4587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Chapter 7 ■ Optimizing YOur apis\n\nrefund-creation topics, and it keeps a count for copies sold and refunded for each of the books listed in these messages, in order to calculate the data it proffers to its own clients (that is, lists of books in various categories, ordered by net sales). No synchronous requests to either the purchase-service or the refund- service need to be made as part of the request cycle to any bestsellers-service API, as the information to calculate the bestseller lists has been precollected via asynchronous topic consumption and stored locally in the bestseller database.\n\nOne of the changes that reliance on a messaging system like Kafka brings along is that your data will only be eventually consistent across all your services that consume the update messages. One such example could be that a user updated her credit card data as part of the purchase flow, but the user’s account detail page still needs some time to catch up on consuming the data change, and hence is still showing the previous details.\n\nIf this behavior seems unacceptable, then a synchronous call to the true data source needs to be made, or workarounds need to be implemented; for example, a secondary (AJAX) request could be made after the account details page is initially rendered, or WebSockets could be used to update the credit card information element upon receipt of the Kafka update message.\n\nSummary In this chapter you learned about ways to reduce our APIs’ consumption of resources (both computer and human). We first discussed the importance of measuring performance before trying to improve it. We suggested some aspects to measure: database queries, service dependencies, memory and CPU consumption, as well as logic probes.\n\nWe next discussed analyzing how service APIs are accessed by their clients and then optimized\n\naccordingly for maximum client performance. Making sure that the information vended is grouped based on the underlying entity's write. Similarly it was explained that data that is often joined together should best live inside a single service, so that cross-service joins can be avoided. Adding separate bulk APIs for the retrieval of very large data sets and using very focused BFF façade services were also explained and proposed as effective improvements.\n\nThe advantages and the potential pitfalls of for caching information in the API-providing service were noted, before schemes to employ the benefits of HTTP conditional GET and its ETag, If-Modified-Since, and Cache-Control headers (including service-side implementation approaches in Ruby on Rails) were discussed. The related option of inserting a caching proxy between the client and the service was also laid out, listing Varnish as a viable option for this.\n\nNext we showed the importance of controlling the size of API responses, along with several ways to do that, including specialized representations, vending resources partially, HTTP compression and semantic paging as ways to help issue responses of optimal size.\n\nWe then addressed the inefficiencies involved in developing and maintaining duplicated code in the clients, and gave a list of eight indicators to consider, each of which might justify centralizing functionality inside a service if these benefits outweigh the cost of introducing an additional service dependency.\n\nFinally, we explained ESBs in general, providing specific detail by using Apache Kafka. We described a real-world use case of employing a message-based approach to improve API performance by obviating the need for secondary service calls. We also described how Kafka could be of use to vending bestseller lists in our bookstore example.\n\nIn the following chapter we will look at a number of ways to handle the nontrivial task of managing\n\nthe development environment in microservices architectures more efficiently, thereby making it less cumbersome and hence more productive to implement.\n\n101",
      "content_length": 3935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "PART III\n\nDevelopment and Deployment",
      "content_length": 36,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "CHAPTER 8\n\nDevelopment Environment and Workflow\n\nSoftware engineering in a microservices environment requires a specific mindset, as well as tools to make development and deployment simple. Any friction caused by a high number of deployment units will put a damper on overall productivity.\n\nIn this chapter we will explain the advantages of a list of measures any organization should take to make\n\ndeveloping in a microservices environment enjoyable and efficient.\n\nThe Premise Your company has decided to wholeheartedly embrace microservices. Given the size and complexity of your business application, and the requirement to be quick and nimble, you have to implement a design with a large number of RESTful services, front-end applications, and backing data stores. Even for smaller applications, having 25–30 individual deployment units is more usual than not; LivingSocial's production environment is made up of hundreds of interconnected deployment units.\n\nYou are a software developer who is tasked with making changes to one of the customer-facing web applications, which also requires enhancements in one of the company's service APIs that provide all the data to the web-applications to be rendered to the end user. Both the web application and the service you need to extend have additional, direct and indirect, service and data store dependencies.\n\nHow do you set up your development environment to make sure you are correctly implementing the\n\nfeature you are tasked to develop?\n\nHow do you run your test suite? How do you pass this feature on to your QA team, and how do they go about testing your changes for\n\nregressions and new features?\n\nAny engineering organization should strive to answer these and other important questions in order to adopt microservices successfully. The following list of attributes of the development environment they should aspire to implement stems from our experience in the matter.\n\nThe Development Environment Wish List The following 12 subsections list circumstances and properties of the software development process that should be present in companies that want to be successful in operating in a microservices environment.\n\n105",
      "content_length": 2179,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nEase of Development Tool Setup To increase productivity from day one, organizations should avoid requiring their engineers to perform time-consuming and tedious busy work, simply to get their development machine ready. The set of tools needed to be productive as a software engineer in your organization should be easily and quickly installed, and preconfigured, on every new developer’s machine.\n\nNote that these setup steps should be convenient and useful, but they should not be entirely opaque to the developer. Instructions on what is installed and how to customize it should be available to someone joining your team on day one.\n\nEase of Application Setup Additionally, any of the many code repositories for the services or other applications that make up your microservices architecture should provide an easy and preferably uniform way to install (and uninstall) its own specific requirements to develop in, run, and test the application locally on the engineer's computer. This way, your engineers aren't bogged down in trying to understand how any given application they\n\nneed to change is set up on their development machine.\n\nAutomation of Application Tests Automated test suites exercising as many as possible of the five microservice testing layers laid out in the next chapter should exist, and instructions (preferably uniform across projects of the same framework) or— even better—commonly adopted conventions for how to run them need to be included.\n\nThis decreases the risk of introducing regressions in the service behavior, and it also gently nudges developers in the direction of adding test coverage for new functionality by making this step easy. While we focus in much greater detail on the advantages of microservices testing in the next chapter, the main take-away is that automated test suites reduce the chance for costly bugs to make it into the production environment.\n\nAvailability of Sample Data When running any application during local development, sample data to exercise both “happy paths” and exceptional behavior should be available for manual testing. Additionally, it needs to be easy for a developer to extend the sample data sets with new data that exercises the logic that was altered or extended by the code changes under development.\n\nThe arguments for this point are similar to those for test automation (that is, reducing the likelihood of defects), plus the added efficiency during the actual feature development, when developers tend to explore their code changes by manually exercising new and existing code paths.\n\nUp-to-Date Dependencies It is burdensome to all your developers when they have the responsibility of keeping up to date all the service dependencies for the application(s) in which they develop. First, keeping other applications' code and data fresh is a—sometimes rapidly—moving target when there are multiple engineering teams developing in parallel. Second, updating all the applications on the developers' computers is potentially time-consuming and distracts the engineer from the coding task at hand.\n\nAddressing the freshness of application dependencies adds to your developers' velocity, while at the same time helping with the accuracy and reliability of the test scenarios you exercise during development.\n\n106",
      "content_length": 3337,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nFast Developer Machines A related point is that a microservices company should aim to avoid obliging developers to run dependent services (let alone the entire SOA system) on their development machine. This will invariably waste resources on the developers' machines, thereby slowing down their computers and by that token the entire software development process.\n\nNote that using resources more efficiently on a developer’s workstation does not mean we are\n\nadvocating making developers use outdated or subpar computing hardware. Quite to the contrary: the cost difference between providing a developer with a slow versus a fast laptop computer is usually not more than $1,000, which averages over a year of work at around $4 per working day. That is a very modest expense, with great return on investment considering the ensuing gain in developer happiness, efficiency, and productivity.\n\nConcurrency of Development Successful software development organizations want to foster developer collaboration. While some of this can be done via pair programming on the same task in a single service's code base (side-by-side in the same physical location, or remotely using tools like Screenhero1), developers should also be able to collaborate while working in tandem on code bases for separate services.\n\nCollaborative Development Revisiting the premise in the opening section, two or more developers can team up, so that one of them makes changes in the front-end web application while the other developer simultaneously extends the backing service's API to provide the web app with the functionality. Both developers should immediately be able to see changes and receive feedback via their respective development branches of the web app and the service communicating.\n\nThis increases productivity for the developers involved, as they will not need to spend time on elaborate\n\ncode branch synchronization procedures to experience the results of each other's most recent work.\n\nPrivacy of Development Developers of software that lives at the edges of your company's microservices architecture (that is, engineers coding native applications on mobile devices and desktop computers, or web-applications serving HTML and JavaScript to consumer browsers) should be able to test their software without releasing any changes to publicly exposed resources. This means that they should be able to run all their edge systems on local development devices. Additionally, they also need to be able to test their software against unreleased code branches of services they depend upon.\n\nThat way, they will not be obliged to sit idle while waiting for code of service dependencies to be released to production before they can develop features in their own applications. Also, they will be able to provide feedback to the service-providing APIs prior to their public launch, thereby helping to prevent launching (and having to maintain and support) service endpoints that need to be revised soon after launch—it is often very hard, or even impossible, to recall or change APIs once publicly released.\n\n1A collaboration tool that enables remote pair programming. https://screenhero.com/\n\n107",
      "content_length": 3221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nImmediacy of Change Feedback Once a service-side change or extension has been implemented by a developer, it should get exposure to, and exercise from, all the potential client applications. This should best happen in as many real-world usage scenarios as possible. This service-side change should be visible as immediately after completion as possible, and it should even be visible during the ongoing development and QA cycles of the dependent client applications, and definitely before the changed code is released to a production environment.\n\nThis approach helps finding any breakages or adverse effects of the changed code as early as possible,\n\nand it can be seen as an extension of (and sometimes as a replacement for) component-level testing.2\n\nEase of Service Generation When generating new applications in your microservices environment, developers should not be left guessing about which libraries to use, or what design guidelines, coding style, test tools, and documentation approach to follow.\n\nThe advantages of such out-of-the-box consistency among the services in an organization are\n\nmanifold. For one, developers save time when setting up new services includes sane defaults. Also, support and maintenance costs are decreased when only a smaller set of library, storage engine, and similar dependencies need to be supported. Additionally, it helps synergy and reduces risk for a company if developers can more easily switch between code bases to which they contribute. Making sure that there is a level of uniformity and familiarity among the applications reduces the switching overhead for developers and hence reduces cost.\n\nAccessibility of Documentation Being able to learn about a service's purpose and capabilities is crucial in an environment where potentially hundreds of such microservices and surrounding applications might exist. Developers (and computers) should be able to discover services they need, learn about their APIs and specifications, as well as explore the services' functionalities prior to using them in any code of their own.\n\nThis goal aids in reducing duplication of effort, as chances of reimplementing a functionality that an existing service already provides decrease when a developer can easily find out about what is already there. Additionally, being able to explore, and therefore ultimately better understand, service dependencies that your code will need to rely on helps to produce quality code with less time spent on fixing bugs that are due to incorrect assumptions.\n\nMaking the Wishes Come True Now that we have clarified the software development conditions that need to exist in an organization that wants to successfully adopt microservices, let's look at some processes and tools that can help in implementing these conditions.\n\nAutomating Developer Machine Set-Up In order to make setting up the development tools for any developer easier, a very effective measure is to generate, and keep up to date, scripts to set up development machines. These automated scripts should include the main tools that are common across your engineering organization (such as version and source control tools, programming languages, IDEs or text editors, and communication tools like instant messaging clients or Slack).\n\n2See Chapter 9 for an explanation of component tests.\n\n108",
      "content_length": 3379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nThere are a large number of configuration management software systems (commercial as well as OSS)\n\nthat can help in automatically setting up, and keeping up to date, all the computers in your organization. Explaining the differences between all the available solutions is a rather fast-moving target and certainly outside the scope of this book. At the time of writing, some of the most popular open-source systems are Ansible, Puppet, CFEngine, and Chef, all of which support Unix-like and Microsoft Windows platforms. Wikipedia has a very thoroughly researched article3 that compares the various OSS solutions and their feature sets.\n\nThe LivingSocial engineering team has automation in place which can install and configure software on the developers' company-provided Apple MacBook Pro laptops. This script will take a computer from a freshly installed OSX image to having the development environment needed to be productive at the company set up on the laptop in less than an hour.\n\nThe project is called sudo-make-me-a-laptop, and it consists of a set of scripts that automatically install git, the Java SDK, rbenv, homebrew, gcc, kafka, emacs, nginx, mysql, Memcached and Redis, as well as some essential internal services.\n\nThe set of scripts in use at LivingSocial is based on Pivotal Labs' Pivotal Sprout toolset,4 and it uses Soloist (https://github.com/mkocher/soloist) and Librarian-Chef (https://github.com/ applicationsonline/librarian-chef) to run a subset of the recipes in Pivotal Sprout's cookbooks, which is the name for a group of scripts used by Chef (https://github.com/chef/chef). A more detailed discussion of the background of the Pivotal Sprout project can be found in a presentation by Pivotal Labs' Brian Cunnie and Abhi Hiremagalur, hosted at http://sprout-talk.cfapps.io/.\n\nAutomated Application Set-Up Similar to providing scripts that install and update the basic tools that any developer needs to contribute on a daily basis, the specific project code under development should also be installed in an automated fashion.\n\nThe responsibility for encoding the specialized knowledge about set-up, build, execution and testing\n\nsteps should lie with the team that owns the specific service or application under development.\n\nConsistently Named Scripts\n\nAn approach to automating application set-up that we have seen work well is to include shell scripts that programmatically set up the specific application and all its required parts as part of the application’s source code repository. This approach should best be paired with organization-wide conventions defining where to find the scripts. For example, there could be a convention to have a bin directory as part of all code repositories, which should include two scripts: install.sh to check for, and potentially set up, all dependencies required for an application to run on a developer's computer, plus an uninstall.sh script to remove all components set up by the install script.\n\nRegardless of having a convention about set-up and tear-down scripts, your team should adopt a convention about shipping each code repository with a file that contains human-readable instructions for setting any given application up for development purposes. Our team adopted a common template for a Readme file included in all GitHub repositories, and this template includes a section entitled “Setting Up Your Environment.” That section defaults to simply listing that a developer should run the well-known scripts (for example, bin/install.sh mentioned earlier), but it should include exact descriptions for any additional steps that cannot be automated (for example, if certain permissions need to be granted via an approval process that involves human intervention).\n\n3See “Comparison of open-source configuration management software,” Wikipedia; https://en.wikipedia.org/ wiki/Comparison_of_open-source_configuration_management_software 4See https://github.com/pivotal-sprout for an overview of Pivotal Sprout tools and recipes.\n\n109",
      "content_length": 4058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nDocker and Docker Compose\n\nThere are many ways to implement the actual dependency declarations and installation automation process. We have seen configuration management tools like Chef, mentioned in the previous section on developer machine set-up, being also used for describing and setting up dependencies of individual code repositories.\n\nAt LivingSocial, the engineering team is in the process of employing container technology via the use\n\nof the Docker tool chain (https://www.docker.com). The Docker open-source project aims to automate installation and deployment of applications inside LXC containers.5 It provides a layer of abstraction and automation on top of LXC containers, and it allows multiple applications to run independently within a single Linux host.\n\nIn order to run Docker on our development Apple computers, we are also using Docker Machine\n\n(https://docs.docker.com/machine/), which enables us to install and run Docker on a Mac.\n\nWe also use the Docker Compose tool (https://docs.docker.com/compose/) to define, combine, and\n\nrun multiple Docker containers to run the dependencies of our application under development.\n\nNote that for now, we are installing Docker Machine and Docker Compose in a separate step for newer service projects. As these tools become more widely used, they should be moved into the general developer machine set-up scripts.\n\nAs an example, let's walk through the set-up in one of the services we use in our day jobs. The service allows for fast geolocation-based searching of offer data, based on data aggregated from\n\nthe systems of record for the products on offer. We chose to implement it as a light wrapper service that fronts an instance of Elasticsearch, or ES (https://www.elastic.co/products/elasticsearch). The service populates its ES instance with offer data retrieved by subscribing to Kafka topics about product data updates. The Docker Compose configuration file used to declare ES and Kafka as dependencies is shown in\n\nListing 8-1.\n\nListing 8-1. docker-compose.yml file for the search service\n\nversion: '2' services: messaging: image: \"spotify/kafka\" ports: - \"9092:9092\" - \"2181:2181\" environment: - \"ADVERTISED_PORT=9092\" env_file: - ./config/docker/kafka.env\n\nes: image: \"elasticsearch\" ports: - \"9200:9200\" - \"9300:9300\" volumes: - /mnt/sda1/var/elasticsearch/data:/usr/share/elasticsearch/data\n\n5Linux containers, a way to virtualize multiple, isolated Linux systems on top of a single Linux host machine. See https://en.wikipedia.org/wiki/LXC\n\n110",
      "content_length": 2574,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nAfter declaring the docker-compose file format version, the services section in the file lists the search\n\nservice's dependencies that will be run as Docker containers. In this case, those prerequisite applications are labeled messaging for Kafka messaging and es for the Elasticsearch. Both container images for ES and Kafka will be retrieved from the official Docker Hub container repository (https://docs.docker.com/docker- hub/).\n\nThe ports section defines that host ports for zookeeper and brokers (for Kafka), as well as for the native\n\ntransport and RESTful API of ES are forwarded from the host machine to the containers' equivalent ports. The environment and env_file sections help you declare Unix shell environment variables, which are\n\nresolved to their values on the machine on which Docker Compose is running.\n\nFinally, the volumes section allows for mounting paths or named volumes from the container into the\n\nhost's filesystem, so they can be accessed on the host.\n\nDocker Compose defined containers are then installed (on first use) and launched based via the docker-compose up command, which is based on the docker-compose.yml listed earlier. Listing 8-2 shows bin/start_docker.sh, a short bash script that makes Docker use the default machine, adds the Kafka host into the container's environment variable, and finally starts all the containers via docker-compose up.\n\nListing 8-2. bin/start_docker.sh script to start the docker containers\n\n#!/usr/bin/env bash\n\neval $(docker-machine env default) mkdir -p config/docker echo \"ADVERTISED_HOST=`docker-machine ip default`\" > config/docker/kafka.env docker-compose up $1\n\nUsing Consistent Start-Up Scripts\n\nAnother convention adopted at LivingSocial engineering to make running applications on developer machines homogenous is to use the Foreman Ruby gem (https://github.com/ddollar/foreman).\n\nForeman helps with defining a set of processes simultaneously. The list of processes to run by Foreman\n\nis managed via a file called a Procfile. Its aim is to allow you to run your application directly by executing a single command, like so:\n\nforeman start\n\nThe nice thing about this abstract approach is that one can easily combine Foreman and Docker\n\ncontainers. See Listing 8-3 for an example.\n\nListing 8-3. Procfile for the search service, executing bin/start_docker.sh\n\ndocker: bin/start_docker.sh service: bundle exec rails s -p3052\n\nThe Procfile has entries for two processes to start. The service entry starts our Rails-based search\n\nservice mentioned earlier, while the docker entry starts the Docker Compose script.\n\nMaking Testing Efficient with Common Tools Much as it does with setting up the prerequisites of an application for development efforts, the responsibility of making testing as easy and efficient as possible lies with the development team owning the service.\n\n111",
      "content_length": 2900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nConsistent Process and Documentation\n\nA first step for this could be to make sure there is uniformity in running an applications test suite. A well- structured template for a Readme file for all your code repositories featuring a “Running the Tests” section will go a long way. In addition to tips about how to trigger a run of the automated test suite, such a section can (and should) include detailed instructions for how to run any additional smoke tests that— for whatever reason—cannot be, or simply are not, automated. Our team implemented a similar approach by including a checklist of manual verification steps to perform as part of a change request. The checklist, in the form of a to-do list, is included in the GitHub pull request template6 for our repositories.\n\nSuch a Readme-based documentation template can supplement, but will never fully replace, the approach of using a script with a well-known name and location in the code repository as the agreed-upon convention to run all tests.\n\nLivingSocial engineering therefore is in the process of introducing a shell script—uniformly named run_tests.sh, and located in any project's bin/ directory—that every developer (and ancillary automated tool) can expect to exist in a repository.\n\nAnointing a Test Framework\n\nAnother great way for an engineering organization to help make the testing process more efficient is to settle on a very small set of test frameworks, ideally one test framework per language in use. This reduces the cognitive load for every developer new to a project.\n\nAs an example, all of LivingSocial's Clojure developers use Stuart Sierra's clojure.test test framework\n\nfor unit tests (https://clojure.github.io/clojure/clojure.test-api.html). This was not just a subconscious choice or coincidence. The decision to use this framework was deliberate and preceded by thorough discussions and comparisons with other frameworks (like Brian Marick's Midje, https://github. com/marick/Midje). On the other hand, for Ruby applications in the same engineering organization, such a discussion had not happened for a long time, leading to a very wide range of different testing frameworks in the wild. This has been recognized as a deficiency and a cause for much loss of efficiency, which is why new Ruby applications have recently been unified to adopt the RSpec, Cucumber, and Capybara frameworks.7\n\nIf an organization sees a reason to replace one recommended test framework with a different one, then\n\nwe recommend slowly updating any existing test suites using the deprecated frameworks in an “as you go” manner. In our experience, justifying the (often large) effort to switch all old-style tests to the new framework in a single project is very hard. A more practical approach would be to write all additional tests using the new framework, while only moving the existing test case to the new framework whenever it, or a small set of tests that include the given test case, needs modifying.\n\nImplementing Test Code Guidelines\n\nBut just picking a (set of) supported framework(s) does not entirely suffice to help reduce maintenance effort and to flatten the learning curve for test-related code. Widely published style guides should exist not just for coding, but also for tests. Software consultancy Thoughtbot (https://thoughtbot.com) has published a succinct yet effective style guide for their developers to follow when writing tests using RSpec and Cucumber. Figure 8-1 shows the succinct but effective guide at the time of writing.8\n\n6For a description of how to set up Pull Request templates in your repository, see https://help.github.com/ articles/creating-a-pull-request-template-for-your-repository/. 7See http://rspec.info/, https://cucumber.io, and http://jnicklas.github.io/capybara/; more details on how to use these frameworks will be presented in Chapter 9. 8See https://github.com/thoughtbot/guides/tree/master/style/testing for the full test style guide at Thoughtbot.\n\n112",
      "content_length": 4022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Figure 8-1. Test-related section of Thoughtbot's coding style guide\n\nChapter 8 ■ Development environment anD WorkfloW\n\n113",
      "content_length": 122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nIntroducing Concurrency of Test Runs and Development\n\nAnother valuable tool to support developer efficiency during the development and test phase is Guard (https://github.com/guard/guard). While Guard is a generalized automation tool that triggers customizable actions whenever observed files or directories are modified, it becomes particularly useful when integrated with the development and test cycle.\n\nMany Guard plugins for test frameworks exist,9 and we use guard-rspec (https://github.com/guard/ guard-rspec) to support automatically running tests on Ruby-based projects. The guard-rspec gem allows you to specify rules that determine the correct subset of all tests to run, based on which file containing business logic changed.\n\nGuard can be configured based on a file called a Guardfile, in which you can specify files and directories to watch, and which action to take based on changes to the watched entities. Actions and watch definitions can be expressed in a DSL. Listing 8-4 shows an excerpt from a sample Guardfile from guard-rspec's documentation,10 which is focused on auto-executing the RSpec specs for a prototypical Ruby on Rails application.\n\nListing 8-4. Guardfile for a typical Rails application\n\nguard :rspec, cmd: 'bundle exec rspec' do watch('spec/spec_helper.rb') { \"spec\" } watch('config/routes.rb') { \"spec/routing\" } watch('app/controllers/application_controller.rb') { \"spec/controllers\" } watch(%r{^spec/.+_spec\\.rb$}) watch(%r{^app/(.+)\\.rb$}) { |m| \"spec/#{m[1]}_spec.rb\" } watch(%r{^app/(.*)(\\.erb|\\.haml|\\.slim)$}) { |m| \"spec/#{m[1]}#{m[2]}_spec.rb\" } watch(%r{^lib/(.+)\\.rb$}) { |m| \"spec/lib/#{m[1]}_spec.rb\" } watch(%r{^app/controllers/(.+)_(controller)\\.rb$}) { |m| [\"spec/routing/#{m[1]}_routing_ spec.rb\", \"spec/#{m[2]}s/#{m[1]}_#{m[2]}_spec.rb\", \"spec/acceptance/#{m[1]}_spec.rb\"] } end\n\nThe guard command block specifies that RSpec should be run (via cmd: 'bundle exec rspec'). The\n\nwatch statements define directories or filenames (directly, or via regular expression matching), and the blocks following the watch expressions list all the test files for which to re-execute the specified cmd if a particular watched (set of) file(s) has changed.\n\nUsing Guard or other automated test execution frameworks, like Autotest (https://github.com/ seattlerb/zentest), provides developers immediate feedback about the effect of their changes in a potentially unfamiliar code base. Forgetting to run, or deliberately choosing not to run, the application's test suite at the end of the contribution cycle will no longer be an option. And because test runs are incremental and only focused on the files currently under change, the cost and impact on productivity of running tests is drastically reduced.\n\nIntegrating Static Analysis Tools into the Test Process\n\nThe introduction of static analysis tools can also help with teaching (and enforcing) coding guidelines and testing etiquette. The Code Climate platform (https://codeclimate.com/) is a commercial tool that runs a configurable set of analysis steps on every code commit, and then generates reports on adherence to coding standards, code complexity, code duplication, or percentage of test coverage. Figure 8-2 shows the summary page of a LivingSocial-internal service repository monitored by an on-premise installed version of Code Climate.\n\n9For a full list of plugins, see https://github.com/guard/guard/wiki/Guard-Plugins 10See the “Typical Rails app\" section at https://github.com/guard/guard-rspec#typical-rails-app\n\n114",
      "content_length": 3573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nFigure 8-2. Code Climate report for an application called catalog-service\n\nA simpler and, for commercial use,11 less costly approach to encouraging attention to test coverage in the Ruby environment is SimpleCov (https://github.com/colszowka/simplecov), a code coverage analysis tool that makes use of Ruby's built-in coverage-tracking mechanism to collect data about code coverage. SimpleCov lets the developer easily process, format, and display the code coverage data collection results in just a few lines of Ruby code.\n\nIn order to guide developers into adhering to safety and maintainability standards for a given code base, the development team should explore whether a service's test suite can be made to fail whenever code coverage falls below an acceptable threshold value. When using the RSpec test framework and SimpleCov in the Ruby world, you can configure RSpec to fail the run if SimpleCov reports a decline in coverage percentage.\n\nA Spec helper file that implements this for whenever coverage falls below 85% is shown in Listing 8-5.\n\n11Code Climate analysis can be added for free to any open-source project.\n\n115",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nListing 8-5. spec/spec_helper.rb that fails test suite runs below 85% coverage\n\nrequire \"simplecov\"\n\nSimpleCov.start\n\nRSpec.configure do |config| config.after(:suite) do simplecov = RSpec.describe('Test coverage') example = simplecov.example('must exceed 85 percent'){ expect( SimpleCov.result.covered_percent ).to be > 85 } simplecov.run\n\nif example.execution_result.status != :passed RSpec.configuration.reporter.example_failed(example) end end end\n\nContinuous Integration to Improve Code Reviews\n\nIf you want to use a less forceful approach than failing the entire test suite, but still want to keep track of declining code coverage numbers, you can instead (or additionally) try and integrate coverage checks into your Continuous Integration (CI) build system.\n\nWhen using the Jenkins project's CI system (https://jenkins.io/), it can easily be extended with plugins to set up a workflow that processes SimpleCov-generated code coverage data. The DocLinks plugin (https://wiki.jenkins-ci.org/display/JENKINS/DocLinks+plugin) can be used to display formatted coverage reports, as shown in Figure 8-3.\n\nFigure 8-3. Jenkins shows SimpleCov reports via the DocLinks plugin.\n\n116",
      "content_length": 1228,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nThe RubyMetrics plugin (https://wiki.jenkins-ci.org/display/JENKINS/RubyMetrics+plugin) for Jenkins can be configured to adapt the CI build symbol and status based on the SimpleCov coverage percentage (see Figure 8-4).\n\nFigure 8-4. Jenkins RubyMetrics plugin configuration for build health assessment\n\nConfiguring your RSpec test suite is easy, and Listing 8-6 shows how it is done. It presents a Spec helper\n\nfile that implements this for whenever coverage falls below 85%.\n\nListing 8-6. spec/spec_helper.rb that generates reports processable by the DocLinks and RubyMetrics plugins\n\nrequire \"simplecov\" require \"simplecov-rcov\"\n\nSimpleCov.formatters = [ SimpleCov::Formatter::RcovFormatter, SimpleCov::Formatter::HTMLFormatter ]\n\nSimpleCov.start\n\nAnother great use case for Jenkins that makes testing newly developed code more efficient is to integrate it directly with the code version control system. Many engineering organizations set up Jenkins to detect new branches or pull requests for a given GitHub repository, and they execute the test suite automatically. This helps the development team owning the service to review any change requests within a shorter feedback loop.\n\nShipping Sample Data with the Code During the development cycle, engineers usually do not want to rely only on test-driven development (TDD) and automated tests to implement their features. A seasoned developer will also try to start the service they are altering locally, to see the changes made in effect immediately, and as perceived by clients. This also helps in efforts to use exploratory testing12 to verify new features or other code behavior changes.\n\n12See the next chapter for further notes on exploratory testing.\n\n117",
      "content_length": 1763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nTwo Types of Data Sources\n\nLet us first define the two main categories of data sources for which we will need to provide sample data when making changes to a service: a data store in which the service application keeps the data it owns itself, as well as another service application that provides data to the service under development.\n\nThe rest of this section will be structured around several different ways that we have seen successful microservices applications set up such development-time sample data, starting with sample data for entities owned by the service itself, and then addressing sample data and logic provided by a service dependency.\n\nProviding Samples for Data Owned\n\nHere are some ways in which samples of service-owned data can be procured for local development.\n\nSeeding Scripts\n\nProbably the most common set-up to run a service application in which you have made changes and now want to explore locally on your development machine is to also run the application's data store on the development machine.\n\nIn this situation, running automated scripts to populate—or reset and clear out—the service-owned\n\ndata store is often a viable option to provide sample data.\n\nWe do not recommend any particular automated approach to performing such seeding tasks, as many\n\ndevelopment frameworks allow for easy implementation of adding data to a store. As always, we recommend using a standardized approach across your organization applications to foster familiarity by consistency.\n\nIn Ruby on Rails, for example, developers often use the rake tool to run this automation. For example, the built-in rake db:seed reads and executes the Ruby code contained in the db/seeds.rb file in a Ruby on Rails project.\n\nAnother, very similar, option is to populate the development instance of your service with the data\n\nsets that are used for your automated unit tests. In Rails, this data often lives in so-called fixture files, and Rails provides you with the db:fixtures:load rake task to use these fixtures and move them into the development environment.\n\nProduction Snapshots\n\nAnother approach we have seen used in cases that require more exhaustive, or more authentic and “production-like,” data is to work on snapshots of the production data store.\n\nOne way to implement this scenario is to take periodic snapshots of the data store in the production\n\nenvironment (for example, dumping the tables of a SQL database to a file).\n\nThese data dumps are then stored (and optionally compressed) on an internal file server to which all developers have access. To use the dumps locally, the service code base could again run automated bootstrapping scripts to download, decompress, and then install the dumps into the locally running data store.\n\n■ Note if you take this approach, make sure to scrub the production data of any sensitive information, such as personally identifiable information about your customers, passwords, financial information, and the like. You should anonymize, randomize, remove, or otherwise obfuscate sensitive data.\n\n118",
      "content_length": 3099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nConnecting to a Shared Data Store\n\nSome microservices companies follow a pattern in which they provide a sandbox environment where all the services are instantiated and running, outside and in addition to their production environment.\n\nWhile we will explain more about the advantages of such a set-up later in this chapter, it is relevant to the sample data discussion as well, as it allows the service-owned data to live outside the developer's machine. Instead, the service instance under local development will in this scenario be configured to connect to the data store(s) that serve(s) the instance(s) of the same service in the sandbox environment.\n\nThis way, not every developer is obligated to install and update the sample data on their local machine,\n\nas the sample data can be centralized in the sandbox data stores.\n\nA downside to this approach is certainly that it will be impractical to use for development of changes in the service that require a breaking change to the schema of the sandbox data store, as that would effectively incapacitate the sandbox instances of that same service. Also, it needs to be clear that the mutable state of the shared data store that is in use in the sandbox environment can change at any time in unpredictable ways. For example, purchases made or users created in a sandbox for our bookstore example cannot easily be coordinated across developers concurrently using the sandbox.\n\nProviding Samples for Downstream Services\n\nNext we will show to address dependencies on sample information for services upon which the application under development depends.\n\nProviding Client-Side Samples\n\nWhen a service has other services as downstream dependencies, then usually these service dependencies run remotely and outside of the dependent service's process boundaries and are accessed over a network boundary.\n\nHowever, the development scenario discussed here removes cross-process boundary invocations and\n\ninstead replaces them with in-memory calls. As a consequence, this means that all code that is reaching out to other services in a production scenario needs to be intercepted and rejiggered to respond with sample data that is generated inside the service process that is under test.\n\nOne option to implement this would be to first record live sample communications between the service under test and all its dependencies in production, and then change the service’s development configuration to replay the recorded conversations appropriately when the service under development is run locally. Even though there are options available in almost all programming languages and frameworks to\n\naddress this record-and-replay approach, it is cumbersome to set up and expensive to maintain for any environment with more than a handful of service dependencies. See the next chapter’s section “Out-Of- Process Component Tests” for more examples of such frameworks.\n\nA slightly lower-effort (because more centralized) way of generating all service responses client-side is to have the teams that maintain downstream services ship libraries that can simulate sample data and behavior of their service. This technique is explained at length in the “Pluggable Back-Ends” section of the next chapter. While the focus in that chapter is on using this feature in an automated test suite, it can certainly and easily be adapted to be used in a general development environment.\n\nThe least costly manner in which to stub service dependencies in-process is to auto-generate the\n\nclient-side, in-process service stubs. This can be done either by promoting stubs generated by contract- testing frameworks13 to be used in the development stage with the service responses or by auto-generating a pluggable back-end library for each service based on the service's API specifications document. Support for the latter approach is unfortunately very service-framework specific and not a widely available option.\n\n13Chapter 9 offers insights into frameworks for this purpose, such as pacto, in the sections on contract testing.\n\n119",
      "content_length": 4101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nMocking Services with Data\n\nA twist on the just-described approach of providing mock data for a service dependency is not to alter the service under development to use in-process/in-memory doubles for their service dependencies. Instead, you provide mocks for the downstream services run remotely and out-of-process, just as the actual downstream dependencies do in a production environment.\n\nThe level of automation involved in generating these service doubles is again the deciding factor in assessing the technique’s usefulness at scale. As a rule of thumb, if your organization employs more than a handful of interdependent services, hand-coding mocks for each one of them is going to be a productivity killer.\n\nThankfully, most service definition frameworks come with options to generate such mock services\n\nin an automated way, based on a computer-readable document that describes the APIs. The next chapter describes some of these tools in the \"Contract Testing\" section. Other (commercial and free) frameworks exist, such as RepreZen (http://reprezen.com/swagger-tools) and vREST (https://vrest.io/) for the Swagger tool chain.\n\nConnecting to a Shared Environment\n\nAs mentioned in the section \"Connecting to a Shared Data Store,\" many organizations using microservices run an additional shared sandbox environment for all their services.\n\nThis approach can be used to provide desired realistic sample data without having to run the\n\ndownstream services locally. The service under test will be configured to rely on the sandbox instances of its dependencies. This service configuration can of course take place outside the service under development itself; for example, in a DNS, or a Consul (https://www.consul.io/) install, which resolves the locations of the dependencies.\n\nIn this scenario it is most advisable to have an organization-wide development policy in place which states that outages in the sandbox environment should be treated with an urgency that is only trumped by tackling issues currently surfaced in the production environment. If your organization cannot adequately implement such fast-response support for the shared environment, developers will be left only with the option of resorting to running service dependencies locally on their workstations. This has the disadvantages mentioned earlier, like slowing down the developers’ machines, and the use of potentially out-of-date dependency code, but it is still better than having no end-to-end testing facilities prior to promoting code to production.\n\nMaintaining a Development Sandbox As alluded to in two of the previous sections, a key approach for making development in a microservices environment more effective and fun is to provide a shared sandbox environment.\n\nThis can be seen as a switched, self-contained network fabric whose topology is such that only the participating service nodes interconnect. In this scenario, the fabric of services is solely meant to enable an improved software development and testing process, in order to shorten release cycles while at the same time decreasing defects shipped to production.\n\nFigure 8-5 shows a set-up of three separate network fabrics (sandbox, staging, and production) into\n\nwhich the entire microservices application is deployed.\n\n120",
      "content_length": 3327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nFigure 8-5. Interconnected services in the sandbox fabric\n\nSoftware developers who need to make changes to a service will alter and run just the particular service under change on their developer workstation. If the service in which they develop, and which they hence need to run on their workstation, depends on other services or data stores, then the locally running service instance will be configured to use the service and data store dependencies deployed to the sandbox environment.\n\nIntegrating a sandbox environment into the development process can address many items on our\n\ndeveloper wish list. Let's look at each of these items in a little more detail.\n\n■ Note the use of a dedicated sandbox environment is our own recommendation based on our years of experience in developing in microservices architectures, but the approach introduced here is by no means uncontested. as our former colleague evan phoenix, Ceo of vektra innovation (http://vektra.com/), poignantly describes in a post on his personal blog:14\n\nThe industry needs a common set of practices for how to develop microservices. […] Microservice development is still in its infancy. Teams are slowly figuring out solutions to their specific problems and largely hacking something together.\n\n14Evan Phoenix, \"Microservice Development - Who Runs What Where\", April 17, 2016. http://phx.io/posts/ microservice-dev/\n\n121",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nProviding Up-to-Date Dependencies\n\nIn a scenario where developers can rely on the development sandbox to run all services and systems that the code base they are currently altering is running, they are relieved of many cumbersome and time- consuming chores.\n\nThey will no longer need to make sure they have the latest version of their service's dependencies\n\nrunning locally on their own workstation. They simply have to make sure that their local setup is communicating with the service and data dependencies in the sandbox fabric.\n\nNote that this has an impact on the discipline of your development process. The responsibility for keeping each of the microservices in the organization fresh in the sandbox resides with the teams that own them. This means that there needs to be internal monitoring and support dedicated to making sure that the systems in the sandbox are running during business hours.\n\nAs an example, Amazon's development organization ensured this by making sure that owner teams of sandbox services were easily identifiable, and it was acceptable to page the owning team's engineer on-call when outages were perceived.\n\nSaving Resources on Developer Workstations\n\nThe point of reducing load and resource consumption on the developer workstations is to increase developer velocity by not wasting their time waiting for unresponsive applications.\n\nRunning the majority of the systems and services making up your microservices architecture on dedicated computing resources definitely helps to offload developer stations: no memory, CPU processing power, or disk space needs to be dedicated to install and run the service dependencies locally.\n\nFostering Concurrent and Collaborative Development\n\nIn our many years of experience we have learned that there is no better way to increase the productivity of your development team than to encourage forming close-knit working relationships. Creating an environment in which solving problems and developing features is fun, and which leverages the combined skills of everyone on the team, is key to the successful growth of your engineering organization. Software development organizations are most successful where collaboration is engrained in the culture. A development sandbox helps greatly with these efforts.\n\nImagine a situation in which a new feature is to be added to our sample bookstore application, to enable customers to save favorite searches, and to be notified by a tool of their choice (for example, via email, mobile push notifications, or pop-up messages on the site). This feature requires changes in various pages in the bookstore front-end, the mobile applications, and a fair number of back-end services (new functionalities to associate users and searches, to periodically re-execute the searches offline, to notify the users of new results, and so on.).\n\nTypically, these changes are most efficiently performed when expert developers focus on the various parts: front-end specialists take care of the HTML, JavaScript, and CSS changes in the desktop and mobile web applications, iOS and Android developers change the native apps, and back-end developers from each of the teams that own the backing services are adding or changing service APIs.\n\nIn this very common situation, having an environment where developers from all these teams can collaborate by pushing their latest code branches to the application instances running in the sandbox fabric has a number of advantages. For one, the velocity of development is increased by having subject matter experts focus on making the changes concurrently. Additionally, communication and collaboration between the participating developers is encouraged very early on in the feature development cycle, so that less time is wasted on hunting down missed or unclear requirements, or potential bugs.\n\n122",
      "content_length": 3882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\n■ Note seen organizations encourage collaborating developers to use a single workstation to develop the changes in all affected systems. in this scenario, the participating engineers would all be given access to a shared workstation, which they would use to make the changes to the two or three services involved. they would then run these development branches of these services simultaneously on the workstation, and connect them with each other, while still utilizing the sandbox environment for all other dependencies. if the resources needed to change and run the applications under development locally do not noticeably slow down the workstation, then this route can cut down feature development time even further, as it obviates the need for deployment of the development branches to the sandbox instances.\n\nin situations where only a few applications need changing to implement a feature, we have\n\nShortening the Feedback Loop for Changes\n\nAnother advantage of using a sandbox environment stems from the fact that all developers will use the services deployed there. It means that all engineers will exercise the services deployed there day in and day out, as the code running on their own developer workstations depends on the proper functioning of the services in the sandbox.\n\nThis simple fact greatly increases the chances of finding bugs and regressions very early on, even during\n\nthe development cycle of a new feature and before any other quality assurance measures. Additionally, for companies whose release process includes quality assurance on an end-to-end basis, the QA team can use the sandbox environment to perform such application-wide testing well ahead of releasing any code changes out into a production environment.\n\nOne caveat of using the sandbox for true QA purposes is that it would be advisable to enforce a brief code-freeze during QA times for the sandbox, so that verification results are not invalidated by any mid-test changes.\n\nIn order to prevent these inaccuracies and the potential slow-down to ongoing development efforts,\n\nmany companies introduce an additional network fabric, often referred to as \"staging\" (see Figure 8-5 earlier). This is an interstitial environment between sandbox and production, which is mostly prepopulated with production-like data sets, so that end-to-end QA efforts can happen in a setting that very closely mirrors actual production scenarios.\n\nUsing a Rewriting Proxy for Edge Device Development As mentioned in the wish list section under \"Privacy of Development,” it is important that applications installed on edge devices (like iOS, Android, or other native OS applications) can be tested against service- side changes prior to releasing these changes to a production environment. This not only aids in security and stability concerns, it also benefits development velocity, as developers will not need to wait for changes to be deployed all the way through the stages out into a development fabric before they can be tested with the native applications.\n\nThe best way that we have seen this situation addressed is via the use of a proxy service. In this\n\napproach, the developer would perform the following steps:\n\n1.\n\nInstall a proxy server in the same network fabric where the service instances under change are running (usually either on the local developer workstation or inside the company's VPN in the sandbox fabric).\n\n2. Change the network settings on the edge devices (for example, iPhones or Android tablets) participating in the development to direct all traffic via the previously set up proxy server.\n\n123",
      "content_length": 3647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\n3. Configure the proxy server with a rewriting rule, so that requests from the edge devices to the production domain of your application are being rerouted to the internal (workstation- or sandbox-hosted) instances of your services.\n\n4. Make sure the edge device is able to join (for example, via Wi-Fi) the network in which the proxy server is exposed, so that subsequently all requests from the device can be routed via the proxy to the respective services under development.\n\nWhile there are many proxy services available that will allow these features to be implemented, we\n\nhave found CharlesProxy (https://www.charlesproxy.com/) to be very feature-rich, yet easy to use, and available for all major desktop operating systems (Mac OSX, Windows, and Linux).\n\nFigure 8-6 shows the CharlesProxy rewrite settings page, configured such that any device sending requests for resources in the books.com domain would instead be serviced by the equivalent hosts in the int.books.dev domain (which you can configure to be the domain for your workstation or sandbox fabric, for example).\n\nFigure 8-6. Example host rewriting settings in CharlesProxy for our fictitious bookstore\n\n124",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nCodifying Internal Standards via Generation Tools Any development organization should make sure to take the guesswork out of setting up new services and applications. Doing so will aid in three areas:\n\nDevelopment speed\n\n\n\nEfficiency in application support and maintenance\n\n\n\nFostering standards and familiarity across applications and frameworks within the development team\n\nOur advice is to develop scripts and tools that can automatically generate service repositories or internal libraries. The code for the service applications so generated should be prepopulated with the approved libraries, configuration files, and preferably even skeleton code and test examples. A newly generated library could simply add certain artifacts (like a change log or Readme file) in consistent places in the directory tree as defined by the organization’s conventions, thereby getting the developers started down the right path.\n\nExamples for such generation tools can be found in many companies that have successfully adopted\n\nmicroservices. Amazon is running an internal web application which sports a web wizard that walks developers through all the steps to generate a service stub conforming to their internal Coral service framework. Developers can choose from options like the programming language to use, and the internal organization and code repositories in which to create the artifacts.\n\nLivingSocial is streamlining the application generation process through a set of templates-based Ruby\n\nscripts. The tool is called ls-forge and is very easy to invoke from the command line:\n\n$ ls-forge rails create ./path/to/new_service\n\nThis command starts an interactive shell-based questionnaire to generate a new Rails API based service. During its course, it asks the user about choices regarding the use of test framework options, selection of approved data store options, the use of crontasks, caching back-ends (like Memcached) and the need for messaging (for example, by including Kafka support), and so on. The generated applications include standard gems (both internal and external) and code stubs that are preconfigured to support test frameworks, circuit breakers, documentation tools, and monitoring and alarming.\n\nThe generated code often does nothing more than expose a \"Hello World!\" API functionality. Still, the trivial example code can be used to showcase an acceptable coding style and other API design guidelines, thereby setting the tone and “soft-enforcing” consistency and best practices. In LivingSocial's case, for example, one feature of the generated application is that APIs are documented via Swagger specifications, and a Swagger UI explorer can be launched locally.\n\nThe main point for such tools is to help consistency between applications, so that developers do not\n\nhave to overcome a high barrier when switching between them during development.\n\nMaking Documentation Search-Efficient With the adoption of a large number of fine-grained service applications also comes the need to make it easy to discover documentation about things like service specifications, characteristics, mission, ownership and development roadmap. Aiding both humans and computers in discovering service attributes can have a great effect on maintainability, stability of your infrastructure, and time-of-production for your business- critical features.\n\nIn our time working with microservices we have not seen many universally accepted solutions to this\n\nchallenge. The only common thread we have been able to determine is that most organizations discover the need to address inconsistencies or lack of documentation rather late in their service adoption process.\n\n125",
      "content_length": 3719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "Chapter 8 ■ Development environment anD WorkfloW\n\nIt is fairly easy to start small, though. Amazon, for example, started out with the introduction of nodes describing each service on their company-internal Wiki installation. LivingSocial has adopted Atlassian's Confluence (https://www.atlassian.com/software/confluence) as a company-wide documentation tool, and teams have generated page templates to use for each service they own. These templates include information on team contact details, additional documentation, URL for an API explorer web tool, configured alarms, monitoring dashboard links, production console access, standard operating procedure in case of outages, as well as operational support and SLA agreements.\n\nA really useful addition to service documentation is a web UI that lets you easily explore the APIs exposed by a service. One such example is to describe your APIs using Swagger and then publish the documentation using the Swagger UI explorer.15 To aid documentation and specification even further, it helps to introduce conventions defining where API specifications and the explorer are hosted. As an example, all services using the Swagger toolchain host their API specifications at the /swagger/api path, with the service explorer accessible at /swagger/docs. Additionally, the services' API specifications are pushed to another centralized service, which both other services and humans can use to retrieve a full list of all available services.\n\nThis last approach last can be seen as the beginning of implementing a true service registry that can help in programmatically discovering available services and their capabilities. We will not go into detail about private service discovery tools such as Weaveworks Weave (https://www.weave.works/product/), AWS Route 53 (https://aws.amazon.com/route53/), or HashiCorp's Consul (https://www.consul. io/).16 Nevertheless, such tools play an important role in helping effectively manage both development and production environments, and they become essential when running medium to large-scale microservices architectures.\n\nSummary Any organization that wants to be successful in adopting microservices needs to make sure that a list of processes and tools are in place for engineers to be set up to support the business goals effectively and efficiently.\n\nThe first part of this chapter explains the most essential 12 properties of a successful microservices\n\ndevelopment environment, detailing the rationale behind each of the items on the list.\n\nIn the second part, we describe practices and tools to address each point on the first part’s wish list.\n\nEach of the recommendations we make is backed up by our practical experience with those tools gathered over the course of our careers.\n\nNow that we have explained general development concerns, the next chapter will focus on what is involved in testing microservices applications to verify them in an exhaustive fashion, and what to look out for in particular.\n\n15For more details on the Swagger UI, see the \"The Swagger User Interface\" section in Chapter 5. 16See Chapters 6 and 9 for more information on the Consul tool.\n\n126",
      "content_length": 3158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "CHAPTER 9\n\nTesting with Services\n\nAutomated testing is crucial in any software development project. Manually testing a system built from microservices is tedious and error-prone. In some cases, it is well-nigh impossible to test all the possible failure modes by simply exercising the application through exploratory tests executed by human QA personnel. Automating these tests will save hours of time, and it will prevent those bugs that could cost your business millions in profits.\n\nTesting becomes even more important in a microservices architecture where the components of the\n\nsystem are highly distributed across network boundaries.\n\nThis chapter first discusses the five system layers at which testing can and should be done:\n\nUnit testing\n\n\n\nIntegration testing\n\nContract testing\n\nComponent testing\n\n\n\nEnd-to-end testing\n\nNext, we will show approaches to testing a single microservice component in isolation from other, external microservices with which it interacts. The pros and cons of each approach are discussed, and example code to illustrate implementing them is explained.\n\nWe then move on to showing how tests can support services in guaranteeing that they meet their published API contract. The goal is to shield the service clients from unexpected breakages caused by service-side changes.\n\nLast but not least, we demonstrate how system-level, end-to-end tests can be specified, in order\n\nto exercise the functionalities that make up an application that is composed of multiple interacting microservices.\n\nLayers of Microservice Testing As discussed in Chapter 1, microservice architectures enforce clearer, more pronounced internal boundaries between the components of an entire system than monolithic applications typically do.\n\nThis can be used to the advantage of testing strategies applied to microservices; more options for places\n\nand methods to test are available. Because the entirety of the application is composed of services with clear boundaries, it becomes much more evident which layers can be tested in isolation. This section describes each of the conceptual layers at which tests can be introduced.\n\n127",
      "content_length": 2141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Chapter 9 ■ testing with serviCes\n\nThe Five Layers of Testing An application designed in a microservices architectural style can be viewed at several levels of granularity. Starting from the outside in, end users will interact with the system as a whole. At the next level down, the system internals can be viewed as a series of communications between each of the underlying services. Finally, the services themselves can be described as the code modules that make up each of the services. Testing strategies should aim to cover all these levels or layers, as well as interlayer communication.\n\n■ Note same layer of the microservices application, and hence conceptually belong together.\n\nwe use the terms layer and level interchangeably when referring to a set of tests that exercise the\n\nThe diagram in Figure 9-1 was inspired by one that Toby Clemson of ThoughtWorks used in a\n\npresentation on microservices testing,1 published on Martin Fowler's blog. It illustrates five layers of tests, and where they are applied in a system designed with microservices.\n\nFigure 9-1. Diagram of the five layers of testing applied to microservices\n\nUnit Testing\n\nAs the lowest level of testing, unit tests exercise the smallest piece of testable software in the application to determine whether it behaves as expected. The emphasis here is on trying to identify the truly smallest independent piece of logic to be tested.\n\n1Tobias Clemson, “Testing Strategies in a Microservice Architecture,” November 18, 2014. http://martinfowler. com/articles/microservice-testing/#conclusion-summary\n\n128",
      "content_length": 1579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Chapter 9 ■ testing with serviCes\n\nAn example in the realm of our bookstore could be the functionality of displaying the user’s name\n\ncorrectly. The underlying logic specifies that the first choice is to display a name the user has entered via the bookstore app itself, with a fall-back to a name as vended by a third party (like Facebook), and finally a name deduced from the user’s email address data. The various conditions and fall-back scenarios should be exercised via unit testing.\n\nWhen you find yourself struggling to implement unit tests for a code module, such difficulties often indicate that the design of the module needs improvement: the cause could be that logic needs to be broken down even further, and that responsibilities allotted to the module need to be further separated.\n\nEven at this very low level of testing, the code under test often relies on collaborating with other modules. When running unit tests, the tester has two options for setting up such interactions with these collaborators: either the unit under test is exercised in isolation from all its dependencies, or it is not (meaning that collaboration with external units continues to happen). In his book Working Effectively With Unit Tests (Leanpub 2014), Jay Fields calls these two options for approaching unit testing sociable (where collaborators stay involved), and solitary (where dependencies are not exercised).\n\nBoth solitary and sociable testing are important when verifying microservices architectures. A good\n\nexample of where sociable tests are usually preferred is the domain logic of a service, as these tests usually involve observing changes in state of the unit under test. It is an approach that can best be described as black-box testing that examines the unit under test while interacting with its external interface, and having all collaborating external modules present in their true form adds much to the level of confidence such tests provide.\n\nOn the other hand, when testing “plumbing” code (like gateway code to other services or data stores) or “orchestration” logic (which simply coordinates the message flow with other modules), solitary tests are most often used; the respective external services, data stores, or coordinated modules are replaced by test doubles to facilitate faster, and potentially more reliably repeatable, tests. A word of caution on the solitary testing approach, though: it makes it all too easy to neglect testing mocked responses from dependencies that simulate failure conditions or exceptions. Don't fall prey to the fallacies of distributed computing!\n\nIntegration Testing\n\nCompared to unit tests, integration tests are more coarsely grained: their purpose is to verify that each module of a service correctly interacts with its collaborators. They are meant to detect any defects in how modules should communicate with each other, so that a new feature, or a code change, can be safely deployed to production.\n\nTo achieve that end, they exercise communication paths through the service in order to find incorrect assumptions each module might have about how to interact with its communication partners. Also, make sure not to test just for the “happy paths” of successful communication, but also cover basic failure and exception scenarios of the integration module—an advisable approach, which is not just limited to the realm of integration testing, but which should be heeded for testing at all levels.\n\nWhen testing integration with components that are external services, you should prepopulate external\n\ncomponents with an agreed-upon fixed set of representative data that is guaranteed to be available. This aids repeatability of your tests in an automated integration test suite.\n\nTesting a service's persistence layer at an integration level should include tests that confirm that the schema assumed by the code matches the one actually available in the data store. As is often overlooked, timeouts and network failures can happen for data stores, as well, since they usually exist across network boundaries. Therefore, the service's ability to fail gracefully under such conditions also needs to be verified. An integration test scenario for our bookstore example would be to exercise the communication between the cart-service and the books-service, where the cart-service’s code base would include verifications that it can successfully retrieve and process product data (such as pricing information) provided by books-service. Additionally, the integration tests should assert that the cart-service can still continue to function— albeit most likely in a limited fashion—in case of a books-service outage.\n\n129",
      "content_length": 4671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Chapter 9 ■ testing with serviCes\n\nWhile integration tests should generally not rely on involving test doubles, it can sometimes be difficult to trigger exception behaviors, like timeouts or slow responses, in an external service. This is especially true if the external service is not under the control of the team that is writing the integration tests. In such cases, stubbing failures in predetermined ways can be helpful.\n\nNote also that integration tests can start failing not only when the integration module itself regresses,\n\nbut also if the external component that is being integrated with suddenly becomes unavailable, or breaks its contract. Therefore, our advice is to write a few truly sociable integration tests, but to focus more attention on unit tests, plus contract tests (which are discussed next) on either side of the integration boundary.\n\nContract Testing\n\nWhen implementing and maintaining a service, it is essential that the service’s exposed APIs (collectively also known as the service contract) meet the requirements and expectations of all its consumers. A service contract consists of expectations of input and output data structures, side effects, and nonfunctional points like performance and concurrency characteristics.2 If the microservice is subject to change over time, it is important that the contracts of each of the service consumers continue to be satisfied. Contract testing is meant to verify that a service boundary behaves as advertised.\n\nAs contract tests are aimed at the boundary of a service and its consumers, they are usually bifurcated\n\ninto two parts. The maintainers of each consuming client application write tests suites that verify only those aspects of the producing service that are in use by the client. On the other hand, the maintainers of the service itself run contract tests to determine the impact of their changes on their consumers; these service- side tests are ideally just the bundle of all its consumers' contract test suites. If breaking changes in the service's APIs are to be made, running all the consumers' contract tests will tell the service maintaining team which of its clients would be broken. Then, either all broken clients need to be convinced to move to the new API in lock-step, or versioning of the API is needed, which leaves existing client-exposed API versions untouched.\n\nThe nature of contract test suites also makes them valuable when a new service is being defined. Service\n\nconsumers can help to drive the design of the service's API by building contract tests ahead of time, so that the consumer's requirements can be taken into account in the earliest stages. This pattern of service development is usually called consumer-driven contracts.3\n\nThere are some test tools available for various technology stacks, which are aimed at supporting contract\n\ntesting (for example, Pacto, Pact, and Janus4). Later in this chapter, we will show examples of such tests.\n\nComponent Testing\n\nAt this level of testing in a system that is composed of a set of microservices, the unit under test is one of the services in its entirety. Component tests provide a controlled testing environment for the service, aiming to trigger any applicable error cases in a repeatable manner.\n\nThe scope for these tests is intentionally limited to the single service as a component of the whole system.\n\nThis means that a service is set up to be isolated from any other external components, which are usually replaced with test doubles, usually injected via special start-up configuration of the service under test.\n\nAs a consequence, this level of testing cannot make sure that the microservices all work together as a\n\nwhole to satisfy business requirements; more coarse-grained, end-to-end testing (discussed below) will be necessary for such comprehensive purposes.\n\n2Testing nonfunctional requirements will not be shown in this book. Such specifications are most likely the hardest to verify, and short of a full load-testing environment probably impossible to assert. 3Ian Robinson, “Consumer-Driven Contracts: A Service Evolution Pattern,” June 12, 2006. http://martinfowler.com/ articles/consumerDrivenContracts.html 4Pacto, https://github.com/thoughtworks/pacto; Pact, https://github.com/realestate-com-au/pact; Janus, https://github.com/gga/janus\n\n130",
      "content_length": 4329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "Chapter 9 ■ testing with serviCes\n\nOne advantage of component-level tests compared to end-to-end tests is that they provide more accurate test feedback about the service itself; the service's APIs are directly driven through tests from the perspective of a consumer, as opposed to indirectly via the execution of a higher-level interaction of an end- user with the entire system. Component tests also usually run faster than tests that are aimed at the entire system.\n\nWhen implementing component tests, there are principally two options where the tests will be executed: in-process, or out-of-process. The next two sections will discuss these options in more detail.\n\nIn-Process Component Tests\n\nIn-process tests for a service component execute in the same CPU process as the service code under test. Test doubles for outside resources (like external services the component depends upon) live inside the service at the time of executing in-process component tests. Data stores that the service might use for persistence are held in-memory, so that access to them does not cross network or process boundaries. To achieve in-process testability, tests often communicate with the microservice through a special internal interface that allows requests to be dispatched, and responses to be retrieved, without making actual (HTTP) network calls to the service-under-test's API. This way, in-process component tests can get as close as possible to the (out-of-process) alternative of executing real HTTP requests against the service, without incurring the additional overhead of true network interactions.\n\nAs an advantage of this approach, in-process component tests do not touch the network, and thus\n\nexecute much faster and are generally less complex.\n\nOther external services upon which the component under test depends are usually replaced with in- process test doubles. These doubles serve canned responses that cover not just regular service responses, but also failure codes and other communication edge cases.\n\nUsing an in-memory database, like H2 DB, or an in-process implementation of Cassandra or\n\nElasticSearch, improves test performance compared to accessing external DBs, as network calls are unnecessary. The argument for using such in-memory replacements is that any persistence-layer integration tests will provide sufficient coverage for communication with the actual data store in use by the component. A disadvantage is that the microservice being tested has to be altered for testing purposes to be started in a “test” mode, which differs from its usual production mode. Dependency injection frameworks can help with setting services up for tests, but the fact remains that the service is not configured exactly as it would be in a production scenario.\n\nOut-Of-Process Component Tests\n\nOut-of-process tests treat the service component as a black box and verify it via tests that execute in a different process and exercise the service over the network, using its publicly exposed APIs.\n\nTest doubles for external service dependencies are still in use, but they live outside the service. They\n\nexecute in their own process, and they are reached only across a network.\n\nSimilarly, data stores used in these out-of-process component tests are real data stores. They provide\n\nthe service under test with sample data.\n\nOut-of-process testing allows more layers and integration points to be exercised than is possible with the in-process variant; all interactions make use of real network calls, and the deployment unit can remain unchanged. If the microservice under test has complex integration, persistence, or startup logic, then this approach is the only way to test it at a component level.\n\nTests in this category verify that the microservices have the correct network configuration and are capable of handling true network requests. The modules that communicate with other service resources, as well as the persistence-access modules, are tested in full integration with external dependencies, which live in separate processes.\n\n131",
      "content_length": 4052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Chapter 9 ■ testing with serviCes\n\nExternal service dependencies can be stubbed in various ways: the tester can either build a set of actual stub services that expose the same contract or have them dynamically generated. Tools like Moco5 or Mountebank6 can help with this task. To make the data returned by such mock services more realistic, it can again be either manually crafted or prerecorded from interactions with the real service (using tools like VCR7).\n\nThe trade-off to be aware of when choosing this approach is that the complexity of isolating the\n\nmicroservice under test from its dependencies is pushed into the test harness. The harness will be responsible for starting and stopping external stubs, coordination of network ports, and other configuration options. Also, using external stub services and actual out-of-process data stores will most likely result in slower tests than in-process approaches can achieve, because of the additional network communications.\n\nWe will show practical examples of setting up component-level tests for a bookstore-related service\n\nlater in this chapter.\n\nEnd-To-End Testing\n\nEnd-to-end tests aim to exercise the entire application in order to verify that it meets all external requirements and that all goals are met.\n\nCompared to previously discussed component tests, they are concerned not just with a single\n\nmicroservice, but with the entire set of communicating services and technology components making up the system. This is why they are sometimes also referred to as full-stack tests.\n\nThey exercise as much of the fully deployed system as possible, manipulating it through public interfaces, such as GUIs and service APIs. This level of testing allows microservice architectures to evolve over time, as they assert that the system remains intact during large-scale architectural refactorings. When actions like splitting, combining, or decommissioning some of the services that make up the entire system becomes necessary or desirable, they serve as a means of verifying that no regressions are caused to the overall application functionalities and expectations.\n\nOne complication often encountered in end-to-end testing is how to address potential dependencies of the application under test on truly external services, provided by third parties. Similar complications arise if the dependencies are simply flaky or unreliable.\n\nIn such systems it may not be possible to write end-to-end tests that are repeatable and side effect free;\n\ntests can start to fail for reasons outside of the testing team's control. Under these circumstances it can be beneficial to sacrifice a certain level of confidence in the true end-to-end nature of the tests by introducing stubs or other test doubles for the external services. What is lost in complete fidelity is gained in greater stability in the test suite.\n\nFor systems that require direct user manipulation via a GUI, tools such as Selenium WebDriver8 can\n\nhelp to drive the GUI to trigger particular use cases within the system. For headless systems without a GUI, end-to-end tests usually directly manipulate the microservices through their public APIs (for example, via HTTP client libraries, or even just direct cURL command-line tool calls).\n\nAs a general rule, end-to-end test suites carry higher costs than lower-level testing efforts. Given their asynchronous nature, their long run times (as they execute entire application-level usage scenarios), and the fact that there are typically many moving parts involved, each of which can be broken, they are more laborious to implement, and larger efforts are necessary to maintain them.\n\nTo address this problem, it can be prudent to set a fixed limit on the total runtime budgeted for end-to-\n\nend test suites. This introduces a forcing function to purge particularly long-running tests once the budget is exceeded.\n\n5https://github.com/dreamhead/moco 6https://github.com/bbyars/mountebank 7https://github.com/vcr/vcr 8http://docs.seleniumhq.org/projects/webdriver/\n\n132",
      "content_length": 4028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Chapter 9 ■ testing with serviCes\n\nTo determine which tests to keep, it can help to design the test scenarios around the various user roles,\n\nand to formulate stories how these users interact with the system. Another approach would be to order the tests by financial importance. For example, a test that exercises interactions with an external payments gateway might take a quarter of your time budgeted for end-to-end testing, but it is verifying a functionality considered business-critical; it is still considered a worthwhile assertion.\n\nThis helps to focus on the parts of the application that provide the highest value. Coverage for the lesser\n\nconcerns can be addressed by lower-level types of testing.\n\nIn order to make it easier to have the engineering and business organizations of your company\n\ncollaborate on defining user stories, many practitioners use domain-specific languages (DSLs) to implement their test cases. The intent is to make the verification steps human-readable while maintaining the properties of keeping them machine-executable. Popular tools to support test DSLs include Cucumber9 and Gauge.10 We will show examples of end-to-end tests written in Cucumber later in this chapter.\n\nAnother problem to specifically address in end-to-end tests is setting up the entire system with test data\n\nthat makes these tests reliably repeatable.\n\nThe most efficient way to handle this is to apply the same DevOps techniques that one should generally\n\napply to deploying microservices architectures to setting up test beds, as well. Much reliability is gained if pristine test environments are spun up as part of the test initialization process. These environments should already be populated with known test data (provided via DB imports or by populating the services involved via the APIs they expose).\n\nBecause of the effort involved in getting end-to-end testing right, there are many teams that simply skip\n\nthis extra layer of testing. They would rather rely on more stringent production environment monitoring and alarming, paired with a handful of manually executed, exploratory smoke tests once changes have been deployed.\n\nYou can find an example scenario for end-to-end testing in our bookstore application later on in this\n\nchapter.\n\nThe Test Pyramid We are adopting Mike Cohn's metaphor of a test pyramid11 to express the notion that the number of tests written at each testing level should decrease the more coarsely grained the view of the unit under test becomes.\n\nThe diagram in Figure 9-2 shows Toby Clemson's depiction12 of the relative number of tests in the\n\nvarious testing layers just described.\n\n9https://cucumber.io/ 10http://getgauge.io/ 11http://martinfowler.com/bliki/TestPyramid.html 12Tobias Clemson, “Testing Strategies in a Microservice Architecture,” November 18, 2014. http://martinfowler. com/articles/microservice-testing/#conclusion-test-pyramid\n\n133",
      "content_length": 2904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Chapter 9 ■ testing with serviCes\n\nFigure 9-2. The testing pyramid shows the relative number of tests in each layer\n\nAt the very top we find a layer we haven't specifically talked about yet: exploratory tests are executed manually by human testers, and they interact with the application directly through a user interface. These tests can help to educate the testers' efforts of writing and improving automated tests. They can also be used as a sanity-checking tool to make sure a recently released change has in fact taken effect.\n\nAs we move down the pyramid, the scope of each of the tests executed decreases, while the number of\n\ntests in a given layer becomes larger compared to the layers above.\n\nThe metaphor of a pyramid expresses the general advice about where to spend time and effort for writing tests. If the advice is taken, it is easier to arrive at a test suite that minimizes maintenance costs and total runtime while at the same time maximizing utility.\n\nTesting Examples Now that you’ve seen the various layers at which microservices can be tested, the following sections will show examples for approaches to some of the less obvious and commonplace levels of testing. Most of these examples are based on actual test suites we have encountered in our work experience.\n\nIn-Process Stubs for Component and Unit Testing As described in the sidebar, although stubbing external dependencies has its disadvantages, it helps with increasing the speed and isolation of test suites. In order to take advantage of these pros, we have spent some time trying to address the cons.\n\nWHAT IS STUBBING?\n\nFor the remainder of this section, we will be using the terms stub and stubbing generically for any case where you replace a production object for testing purposes. this aligns mostly with gerard Meszaros’ general term test double,13 disregarding the more nuanced differences between the various types of doubles.14\n\n13http://xunitpatterns.com/Test Double.html 14For a more nuanced distinction between types of doubles, see Fowler’s “Marks Aren’t Stubs” article, January 2, 2007. http://martinfowler.com/articles/mocksArentStubs.html\n\n134",
      "content_length": 2144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Chapter 9 ■ testing with serviCes\n\nConcretely, this means that the application under test uses test doubles for every call to an external web services made in the exercised code. this way, no service calls ever leave the process in which the component under test is running. results of the service calls are predefined, or “faked,” so that the rest of the code under test can be exercised based on these assumptions.\n\nthere are a large number of such test double frameworks out there. Just in the ruby ecosystem, projects like Mocha or rspec Mocks15 can help to prevent service calls by stubbing out methods of the service clients’ access objects. Other gems (such as webmock16 and Fakeweb17) tie in at the http request level, letting you define mock responses for all requests to predefined UrL patterns.\n\nthe main advantage of using stubs when testing applications with external dependencies is speed. Calling external dependencies can really slow down a test suite. if no time has to be spent in your tests for making expensive network calls, then you will be much more productive. another advantage is that you can truly test your component in isolation, and dependencies do not need to be available during test runs. additionally, your component’s test runs will be resilient to changes in the dependent services’ data.\n\ntraditional stubbing does come with disadvantages, though. For one, testing never happens on a true “integration” level, as the request round-trip between the application under test and the service will never happen.\n\nanother danger is the potential for the dependent apis to change. if you rely solely on testing the interactions with test doubles, your tests will still pass, even if the api contract or behavior on the service side changes, and you run the risk of not detecting issues that will show up only when deployed to the production environment.\n\nFinally, setting up stubs often introduces distracting boilerplate code into your tests simply to bootstrap your test scenarios.\n\nThe direction we took was to build test doubles right into the service’s client library, and to evolve and maintain them in lock-step with the changes in the service APIs. Here are some key points of the stubbing approach taken. We will summarize with an explanation of the main benefits of this approach.\n\nPluggable Back-Ends\n\nWhen building a client library (or in our case, a Ruby gem), we provide a way to configure (at least) two back-end alternatives: one that makes actual HTTP calls to the respective service in order to assemble the library’s response objects; and a second “fake” back-end, which never makes actual network calls to retrieve response objects, but instead chooses them from a pool of well-known response objects.\n\nFirst, Listing 9-1 shows the common “abstract class” that defines the interface for both the real and the\n\nmock back-end.\n\n15https://github.com/rspec/rspec-mocks 16https://github.com/bblimke/webmock 17https://github.com/chrisk/fakeweb\n\n135",
      "content_length": 2991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Chapter 9 ■ testing with serviCes\n\nListing 9-1. Abstract backend class\n\nmodule BookStore module Books::Backends\n\nclass BackendUnimplementedMethodError < StandardError; end\n\n# This class defines the API that backend implementations need to follow class Backend\n\ndef self.find(id) raise BackendUnimplementedMethodError.new end\n\ndef self.find_by_slug(slug) raise BackendUnimplementedMethodError.new end\n\ndef self.get_all raise BackendUnimplementedMethodError.new end\n\n... end end\n\nNext, Listing 9-2 shows the actual back-end that connects to the service in question. It implements the\n\nmethods defined in BookStore::Books::Backends::BooksService.\n\nListing 9-2. Class for implementing communication with the book-service\n\nmodule BookStore module Books::Backends\n\nclass BooksService < Backend\n\ndef self.api @api ||= Faraday.new(url: ServiceDiscovery.url_for(:books), ...) end\n\ndef self.find(uuid) raise BookStore::Books::BookNotFoundError unless BookStore::Books::ArgumentChecker.valid_uuid?(uuid) response = api.get(URI.escape(\"/books/#{uuid}\")) raise BookStore::Books::BookNotFoundError unless response.status == 200\n\nresponse_to_book_container(parsed_response(response.body)) end\n\n136",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Chapter 9 ■ testing with serviCes\n\ndef self.find_by_slug(slug) raise BookStore::Books::BookNotFoundError unless BookStore::Books::ArgumentChecker.valid_slug?(slug) response = api.get(URI.escape(\"/books/slug/#{slug}\")) raise BookStore::Books::BookNotFoundError unless response.status == 200\n\nresponse_to_book_container(parsed_response(response.body)) end\n\ndef self.get_all response = api.get(\"/books/\") raise BookStore::Books::BookNotFoundError unless response.status == 200\n\nparsed_response(response.body).map do |book| response_to_book_container(book) end end ... end end\n\nListing 9-3 shows the implementation of the stubbed back-end that serves canned responses to all gem\n\nclients.\n\nListing 9-3. Implementation of a back-end stub\n\nmodule BookStore module Books::Backends\n\nclass Stub < Backend\n\ndef self.find(uuid) raise BookStore::Books::BookNotFoundError unless BookStore::Books::ArgumentChecker.valid_uuid?(uuid) book = @books.detect {|book| book.uuid == uuid} raise BookStore::Books::BookNotFoundError if book.nil?\n\nbook.backend_data end\n\ndef self.find_by_slug(slug) raise BookStore::Books::BookNotFoundError unless BookStore::Books::ArgumentChecker.valid_slug?(slug) book = @books.detect {|book| book.slug == slug} raise BookStore::Books::BookNotFoundError if book.nil?\n\nbook.backend_data end\n\n137",
      "content_length": 1304,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Chapter 9 ■ testing with serviCes\n\ndef self.get_all @books.map(&:backend_data) end ... def self.reset_all_data @books = BookStore::Books::TestStubs.all_books ... end end end\n\nFinally, the main accessor class of our gem allows for setting a back-end, and then delegates all API calls\n\nit exposes to the currently set back-end, as shown in Listing 9-4.\n\nListing 9-4. The gem's main access class\n\nmodule BookStore module Books\n\n... class << self attr_writer :backend\n\ndef backend @backend ||= BookStore::Books::Backends::BooksService end\n\ndef stubbed_mode! @backend = BookStore::Books::Backends::Stub @backend.reset_all_data end\n\ndef get(uuid) BookStore::Books::Book.new(backend.find(uuid)) end\n\ndef get_all backend.get_all.map do |book| BookStore::Books::Book.new(book.id, :backend_data => book) end end\n\ndef get_by_slug(slug) books_service_response = backend.find_by_slug(slug) BookStore::Books::Book.new(books_service_response.uuid, :backend_data => books_service_response) end ... end end end\n\n138",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Chapter 9 ■ testing with serviCes\n\nRealistic Canned Responses\n\nThe well-known response objects served by the stubbed back-end expose the same API as the objects returned from actual network service calls (in fact, they are instances of the same model classes), and they come preloaded inside the stubbed back-end’s registry of responses. Listing 9-5 shows an extract from the code that demonstrates how the test stub objects are exposed.\n\nListing 9-5. Class to provide book test stubs\n\nmodule BookStore module Books class TestStubs\n\ndef self.diva_and_flea Book.new('f81d4fae-7dec-11d0-a765-00a0c91e6bf6', backend_data: BookStore::Books::BookContainer.new( 'f81d4fae-7dec-11d0-a765-00a0c91e6bf6', 'The Story of Diva and Flea', 'the-story-of-diva-and-flea', ...)) end\n\ndef self.winnie_the_pooh Book.new('481dfebc-7fa5-22c0-a765-01b0c83e5ce4', backend_data: BookStore::Books::BookContainer.new( '481dfebc-7fa5-22c0-a765-01b0c83e5ce4', 'Winnie-the-Pooh', 'winnie-the-pooh', ...)) end ... def self.custom_book(attributes={}) book = diva_and_flea.backend_data attributes.each_pair {|k,v| book.send \"#{k}=\", v} Book.new(book.uuid, backend_data: book) end\n\ndef self.all_books [ diva_and_flea, winnie_the_pooh, ...] end ... end end end\n\nRunning Tests in Stubbed Mode\n\nIntroducing the test approach just described into our service access library has some immediate advantages when setting up and customizing test scenarios.\n\n139",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Chapter 9 ■ testing with serviCes\n\nEase of Test Set-Up\n\nAs part of their test suite set-up, applications under test place the service client library into stubbed mode, thereby configuring it to serve responses entirely out of the stubbed back-end’s registry of preloaded response objects. As can be seen from the preceding code samples, all that these applications need to do is place the line shown in Listing 9-6 into code that is executed as part of their test harness set-up.\n\nListing 9-6. Code to make sure the tests are using stubs\n\n# Applications under test just place this inside a test_helper.rb, or the like BookStore::Books.stubbed_mode!\n\n■ Note of course need to configure the client library to use the actual http service-based back-end instead (which is the default for our client gem anyway).\n\nwhen the client application is running in a production environment, the production-specific setup will\n\nCustomizing the Stubs\n\nTo serve the needs of special-case, nonstandard situations, the client library allows for creating additional stub objects, and for adding them to or removing them from the mock back-end’s business object registry. While the idea here is similar to stubbing response objects corresponding to given request patterns at the transfer protocol layer (as is implemented by some HTTP client libraries, such as the Typhoeus concept of a direct stubbing mechanism18), it differs in that we leverage test doubles at a higher (application logic) level, where the test double behavior is defined in terms of model objects that expose exactly the same interface and behavior as the client gem’s result objects as they are generated in a production environment.\n\nListing 9-7 shows a code example of how our client gem can be convinced to customize the test mocks\n\nstored in the mock back-end.\n\nListing 9-7. Customizing which stubs are used in the tests\n\n# turn on stubbed_mode BookStore::Books.stubbed_mode! # add a couple of books to the stubbed mode's backend data my_test_books = [ BookStore::Books::TestStubs.custom_book( uuid: '6291ba52-7e45-1423-b764-01b0c83e5ce4', title: 'Peter Pan', slug: 'peter-pan'), BookStore::Books::TestStubs.custom_book( uuid: '62bae4a6-54a6-162d-a654-7281fb3a6ce7', title: 'Pete The Cat', slug: 'pete-the-cat')]\n\nBookStore::Books.backend.add_books(*my_test_books)\n\n# ...\n\n18See the “Direct Stubbing” section of the Typhoeus gem’s Readme, https://github.com/typhoeus/ typhoeus#direct-stubbing\n\n140",
      "content_length": 2452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Chapter 9 ■ testing with serviCes\n\n# Remove some of the objects added above from the stub backend's data BookStore::Books.backend.remove_book(*my_test_books)\n\n# ...\n\n# Reset the backend data to the 'well-known' set of books BookStore::Books.backend.reset_all_data\n\nWhy use Pluggable Back-Ends?\n\nThe approach to include several service back-ends and realistic test responses we’ve described allows the mock objects to evolve in lock-step with the client library version. This will increase the client application’s confidence that it is testing against the same API as the actual objects returned by the latest service API version.\n\nAdditionally, none of the usual cumbersome and boilerplate code to create and register stub objects for\n\nthe various tests needs to be written; the stubbed back-end comes preconfigured with a variety of standard responses, which the application code under test will simply use without any additional configuration.\n\nOut-Of-Process Component Testing The sample code that accompanies this book comes with a project called api-testing. In this project, we are testing the books-service in a black-box manner. This means that the test suite executes in a separate process than the books-service itself, and it verifies its proper functioning via calls to the service's externally exposed API.\n\nAs described in Chapter 6, we are using a tool called consul.io (https://www.consul.io/) for independently configuring and discovering the services that make up our entire application in each of our system stages.\n\nSetting Up the Environment\n\nPrior to starting up our component test suite, we need to announce to our local consul.io discovery service that we have a (Rust) service called books running logically at port 5000:\n\necho '{\"service\": {\"name\": \"books\", \"tags\": [\"rust\"], \"port\": 5000}}' | sudo tee /etc/ consul.d/books.json\n\nNext, we start our Consul service:\n\nconsul agent -dev -config-dir=/etc/consul.d\n\nThis command reads all the services configured from the data files in the Consul configuration\n\ndirectory, and also launches a web UI to administer the consul.io service (see Figure 9-3).\n\n141",
      "content_length": 2130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Chapter 9 ■ testing with serviCes\n\nFigure 9-3. Adminstration Web UI for consul.io\n\nLater on, in the test suite, we will show code that simply looks up the currently running books-service\n\ninstance by contacting and searching the local consul.io registry.\n\nNext is starting the actual books-service. This service (implemented in the Rust19 programming language) will be configured at startup to use a test database, which is destroyed at the end of running the full suite. To that end, we will set an environment variable for the database URL, which will be honored by the service code when setting up the DB connection:\n\nDATABASE_URL=\"postgres://tim@localhost/books_componenttests\" cargo run\n\nLet us emphasize this again: the test DB is a real data store, running outside the books-service process.\n\nIt has, however, been preconfigured to include sample data for the component test runs.\n\nWriting Your Test Cases\n\nWhile there are a very large number of testing frameworks for implementing component-level tests, we have decided to demonstrate this level of testing using Cucumber.\n\nCucumber includes a DSL called Gherkin20 for formulating test cases that mostly read like “plain English” (or any of over 60 other languages). The self-described (and might we add, rather lofty) goal Cucumber is trying to achieve is that it \"merges specification and test documentation into one cohesive whole.” By writing system requirements in \"executable specifications,” this tool aims to make collaboration between people who define the business goals and people in engineering positions easier.\n\nCucumber tests revolve around features, written down in the Gherkin language, and stored in .feature files in your test project. Features are there to describe software capabilities and requirements at a high level; they are simply a grouping of related scenarios.\n\n19https://www.rust-lang.org/ 20https://cucumber.io/docs/reference\n\n142",
      "content_length": 1920,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "Chapter 9 ■ testing with serviCes\n\nYou define a feature in Gherkin by starting a line with the Feature: keyword, followed by a name and an\n\noptional description.\n\nA scenario is a \"concrete example that illustrates a business rule.” Its Gherkin keyword is Scenario: and just like a feature, it can have a name and further descriptive text. After that, a scenario consists of a list of steps, which first describe the outset of the test (starting with a Given keyword), then an event or set of events to occur (via the When keyword), and finally the expected result of the event under the described circumstances (in a Then clause). several Given, When, or Then steps can be logically combined via the And and But keywords.\n\nStep definitions can take arguments, which come in handy when your step definitions don't easily fit\n\ninto a single line. One particularly handy way of defining step arguments is as a Gherkin data table. The contents of such tables can easily be accessed as key-value-pairs in the code behind the step definitions.\n\nLet's look at a very simple example scenario from our books-service component test suite:\n\nFeature: books endpoints As an API user I want to call CRUD endpoints to manipulate books\n\nScenario: POST to add a new book Given I send a POST request to \"/books\" in \"books-service\" | title | price | price | isbn | brand | description | | Pete the Cat | 500 | 650 | 0062303880 | HP | New York Times ...| Then the response status should be \"201\"\n\nThis first feature describes the simple Create-Read-Update-Delete (CRUD) endpoints for the books\n\nresource in our service.\n\nOur scenario describes adding a new book to the service, which can be done via an HTTP POST method\n\ncall as listed in the Given clause. The data table passed to the Given step describes the parameters to send, plus example values for each of the fields. The outcome described and tested for in the Then step is that the service's HTTP response should carry a 201 status, indicating that a new book resource was created.\n\nWhile Gherkin parsers exist in many programming languages, we chose Ruby to implement the steps, as\n\nshown in Listing 9-8.\n\nListing 9-8. Step definitions for the POST scenario\n\n# in `http_steps.rb` ## Uses service as parameter Given(/^I send a POST request to \"([^\"]*)\" in \"([^\"]*)\"$/) do |api_path, service, table| url = File.join(ServiceDiscovery.url_for(service.to_sym), api_path) @response = Faraday.post(url, table.hashes.first) end\n\n# in `json_steps.rb` Then(/^the response status should be \"([^\"]*)\"$/) do |code| assert_equal code.to_i, @response.status end\n\nThe most important line to discuss here is the following:\n\nurl = File.join(ServiceDiscovery.url_for(service.to_sym), \"/books\")\n\n143",
      "content_length": 2719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Chapter 9 ■ testing with serviCes\n\nAs mentioned earlier, we are using a service discovery tool to configure our microservices environment.\n\nThis code looks up the books-service service in the service registry before making test calls to it. This approach enables us to execute these steps unchanged against a version of our services where external dependencies are replaced for test purposes (like connecting to test database content), or against the actual unaltered production service, simply by changing the service registry's entry for the books-service.\n\nTo conclude this out-of-process component tests section, we are showing our test suite's console\n\noutput, as produced by the Ruby cucumber test runner tool, in Listing 9-9.\n\nListing 9-9. Shell output of running the Gerkhin tests\n\napi-testing tim$ bundle exec cucumber Feature: books endpoints As an API user I want to call CRUD endpoints to manipulate books\n\nScenario: POST to add a new book # features/books.feature:5 Given I send a POST request to \"/books\" in \"books\" # features/step_definitions/http_\n\nsteps.rb:2\n\ntitle | price | full_price | isbn | brand | description| | Pete the Cat: Scuba-Cat | 500 | 650 | 0062303880 | HarperCollins | New York Times bestselling author and artist James Dean brings Pete the Cat’s world to life under the sea in this new I Can Read book. | Then the response status should be \"201\" # features/step_definitions/json_\n\nsteps.rb:1\n\nScenario: POST to add a new book without required attributes # features/books.feature:11 Given I send a POST request to \"/books\" in \"books\" # features/step_\n\ndefinitions/http_steps.rb:2\n\n| price | full_price | isbn | brand | description | | 500 | 650 | 0062303880 | HarperCollins | New York Times bestselling\n\nauthor and artist James Dean brings Pete the Cat’s world to life under the sea in this new I Can Read book. |\n\nThen the response status should be \"422\" # features/step_\n\ndefinitions/json_steps.rb:1\n\nAnd the JSON response should be: # features/step_\n\ndefinitions/json_steps.rb:15\n\n\"\"\" { \"missing\": \"title\" } \"\"\"\n\nScenario: GET book details by slug for a book # features/books.\n\nfeature:23\n\nGiven I create a dummy book # features/step_\n\ndefinitions/setup_steps.rb:1\n\nAnd I send a GET request to \"/books/slug/:book_slug\" in \"books\" # features/step_\n\ndefinitions/http_steps.rb:8\n\nThen the response status should be \"200\" # features/step_\n\ndefinitions/json_steps.rb:1\n\n144",
      "content_length": 2405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Chapter 9 ■ testing with serviCes\n\nScenario: GET book details by slug for an invalid book # features/ books.feature:28 Given I send a GET request to \"/books/slug/not-a-real-book\" in \"books\" # features/ step_definitions/http_steps.rb:8 Then the response status should be \"404\" # features/ step_definitions/json_steps.rb:1\n\nScenario: GET book details by book id for a valid book # features/books.feature:32 Given I create a dummy book # features/step_definitions/ setup_steps.rb:1 And I send a GET request to \"/books/:book_id\" in \"books\" # features/step_definitions/ http_steps.rb:8 Then the response status should be \"200\" # features/step_definitions/ json_steps.rb:1\n\nScenario: GET book details by book id for an invalid book # features/books.feature:37 Given I send a GET request to \"/books/d3f764a5-2f09-457e-8a94-0128cd46ad3e\" in \"books\" # features/step_definitions/http_steps.rb:8 Then the response status should be \"404\" # features/step_definitions/json_steps.rb:1\n\nScenario: POST to add a new category # features/books.feature:41 Given I send a POST request to \"/categories\" in \"books\" # features/step_definitions/ http_steps.rb:2 | name | description | | Tech Books | Books for techies | Then the response status should be \"201\" # features/step_definitions/ json_steps.rb:1\n\nScenario: GET list of categories # features/books.feature:47 Given I create a dummy category # features/step_definitions/ setup_steps.rb:17 And I send a GET request to \"/categories\" in \"books\" # features/step_definitions/ http_steps.rb:8 Then the response status should be \"200\" # features/step_definitions/ json_steps.rb:1 And the response should be valid JSON # features/step_definitions/ json_steps.rb:20\n\n8 scenarios (8 passed) 21 steps (21 passed) 0m0.139s\n\nExamples of End-to-End Tests Much as for component tests, as shown in the previous section, Cucumber can also be very useful in end- to-end testing of an application, especially when combined with two other projects: Capybara (http:// jnicklas.github.io/capybara/) and Selenium 2.0 (http://www.seleniumhq.org/projects/webdriver/).\n\n145",
      "content_length": 2080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "Chapter 9 ■ testing with serviCes\n\nCapybara is another domain-specific language that aims to help in testing web applications. The DSL\n\nis aimed at simulating how a user would interact with the web application via a web browser. In order to connect to the various browsers installed on the computer executing the end-to-end test suite, it can be configured to make use of the Selenium project, which exposes an API abstraction layer and drivers to connect to the most common browsers, like Google Chrome, Firefox, Apple Safari, and even mobile browsers for Android, or headless browsers that don't require a GUI.\n\nWhen loaded into the same Ruby project, the Cucumber and Capybara DSLs complement each other\n\nquite naturally to create a very powerful toolchain for acceptance testing.\n\nThe sample bookstore project accompanying this book also comes with a very rudimentary Cucumber-\n\nplus-Capybara test suite.\n\nLet's look at the showcase.feature file that describes the very basic use case of navigating to the bookstore homepage, and then locating and following a link to the detail page of a book. On the detail page, we make sure we see the correct ISBN number for the book listed, and we can find a link to add the book to the user's shopping cart:\n\nFeature: Browse books in the store\n\nScenario: Navigate to a book detail page Given I am on the Bookstore homepage When I click on the \"/products/the-story-of-diva-and-flea\" link Then I should see the text \"ISBN : 978-1484722848\" And I should find the \"ADD TO CART\" link\n\nNext, Listing 9-10 shows the Cucumber step definitions that make the feature description text\n\nexecutable.\n\nListing 9-10. Step definitions for the end-to-end tests\n\nGiven(/^I am on the Bookstore homepage$/) do url = File.join(ServiceDiscovery.url_for(:bookstore), \"/\") visit url end\n\nWhen(/^I click on the \"(.*?)\" link$/) do |link_path| find(:xpath, \"//a[@href='#{link_path}']\").click end\n\nThen(/^I should see the text \"(.*?)\"$/) do |expected_text| assert page.has_content?(expected_text) end\n\nAnd(/^I should find the \"(.*?)\" link$/) do |link_text| assert page.find_link(link_text) end\n\nThe Given step navigates to the bookstore's homepage using the Capybara visit command. This\n\ncommand instructs the underlying Selenium webdriver to connect to a browser (by default, Firefox) installed on the test machine, and then navigate to the URL given as an argument.\n\nAs you can see, we again use our service discovery tool's application registry to look up the URL for the bookstore web site. This helps in being able to run the acceptance test suite against several different instances of the whole system without needing to change any code in the test suite.\n\n146",
      "content_length": 2683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Chapter 9 ■ testing with serviCes\n\nNext, the When step uses the Capybara find command to execute an XPath query on the HTML DOM rendered by the browser for the homepage. In this particular case, we verify that a link (or anchor, expressed by the HTML <a> tag) with an href property matching the path quoted in the step's input can be located and clicked.\n\nOnce we get to the book's detail page, the Then step verifies that the book’s correct ISBN number text is\n\ndisplayed. This is done by using Capybara's page object and executing the has_content? method with the step's input.\n\nOnce we verify that this text can be found, the final And step makes sure the page shows a link with the\n\ntext ADD TO CART, which is proof enough for our purpose of verifying that the book can be added to the shopping cart.\n\nContract Testing Examples The following examples are taken from our work experience at LivingSocial's engineering organization. As mentioned in Chapter 4, we use swagger_yard as a tool to generate API specifications in the Swagger interface definition language (which is conforms to the JSON Schema specifications).\n\nBased on those tools, LivingSocial also built a Ruby gem called ls-api_validation. This tool works on\n\ntop of the json-schema21 gem, which helps with validating JSON objects against a JSON Schema definition.\n\nThe use cases of ls-api_validation are mainly the following:\n\nValidation of API requests: The gem can be configured to validate the request parameters and interrupt the service's request cycle and return an error response if the request validation fails (by default via indicating an Unprocessable_Entity22 to the clients).\n\nValidation of API responses: Responses can be sampled and validated (at a configurable percentage of traffic). If response validation fails, log entries are created that explain which properties of the JSON schema API definition are violated.\n\nContract test support: The validations of API request parameters and service responses conforming to the published API contract can be integrated into a service's test suite.\n\nIt is the last use case that we are going to demonstrate in this section. But let's first take a look at some swagger_yard API endpoint and response data type definitions. Here is a snippet of defining documentation that describes an endpoint that allows for searching for offers in a \"geo box,” specified by a set of latitude and longitude coordinates:\n\n# @path [GET] /api/v0/search/geo # # @summary Returns geo search results # @parameter city_id(required) [integer] User's current city ID # @parameter vertical [enum<local,shop,escapes,all>] Vertical filter for ... # @parameter top_left_latitude(required) [double] Top edge of geo box # @parameter top_left_longitude(required) [double] Left edge of geo box # @parameter bottom_right_latitude(required) [double] Bottom edge of geo box # @parameter bottom_right_longitude(required) [double] Right edge of geo box # @parameter expandable [boolean] Optional allow geo box to expand to find ... # @parameter query [string] Optional query text\n\n21https://github.com/ruby-json-schema/json-schema 22http://www.restpatterns.org/HTTP_Status_Codes/422_-_Unprocessable_Entity\n\n147",
      "content_length": 3199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Chapter 9 ■ testing with serviCes\n\n# @parameter category [string] Optional category name # @parameter page [integer] Optional page number # @parameter per_page [integer] Optional results per page # # @response_type [GeoCollection] # # @error_message 422 Unprocessable, if city service reports a city not ... # @error_message 503 Unavailable, if city service or inventory service ...\n\nAs can be seen from these specifications, the API endpoint will return data of the type GeoCollection.\n\nThat data type (also referred to as an API model) is defined next:\n\n# # @model GeoCollection # # @property page(required) [integer] Current page # @property per_page(required) [integer] Count of entries per page # @property total_entries(required) [integer] Count of entries in collection # @property total_pages(required) [integer] Total count of pages # @property collection(required) [array<OfferTile>] Collection items # @property top_left_latitude(required) [double] Top edge of geo box # @property top_left_longitude(required) [double] Left edge of geo box # @property bottom_right_latitude(required) [double] Bottom edge of geo box # @property bottom_right_longitude(required) [double] Right edge of geo box\n\nAs you can see, the specification describes the return type in a reasonably complete manner. While we leave out further details about the exact definition of an OfferTile, note that all attributes of this API model are required, and their data types are also defined (as integer values, double-precision decimal values, and an array of OfferTile JSON objects).\n\nListing 9-11 shows how we use ls-api_validation in our test suite to verify the API's contract.\n\nListing 9-11. Contract tests evaluation schemas and error responses\n\nrequire 'test_helper'\n\nclass V0::SearchControllerTest < ActionController::TestCase include LS::ApiValidation::TestHelpers::Minitest #... context \"GET geo\" do should \"fail request validation for missing geo-box definitions\" do get :geo, city_id: 1 assert_response :unprocessable_entity end\n\nshould \"successfully search offers in geo box\" do VCR.use_cassette('search_geo_city_1') do get :geo, city_id: 1, top_left_latitude: 34.28181943724182, top_left_longitude: -116.533715291111, bottom_right_latitude: 33.03275371591785, bottom_right_longitude: -115.5809313949477,\n\n148",
      "content_length": 2302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Chapter 9 ■ testing with serviCes\n\nexpandable: false, category: \"\" assert_response :success assert_passes_schema_validation request, response.body end end end #... end\n\nThis test code shows an example of validating the API's contract both regarding the incoming\n\nparameters and the data that is returned.\n\nWhen the API is called without the required top_left_latitude, top_left_longitude, bottom_right_\n\nlatitude, and bottom_right_longitude parameters, the validation code intercepts the request cycle and returns an HTTP response with status code 422 (expressed as :unprocessable_entity in the Rails framework).\n\nThe second test case first ensures that the test request is successfully executed, by confirming the response status to be of HTTP status code 200, or—as Rails allows you to refer to it—:success. Note that we are using VCR in this example to increase test speed and reliability by isolating the contract test from direct access to a downstream external service that the service under test depends on.\n\nThe more interesting verification is the assert_passes_schema_validation. This method is provided by the LS::ApiValidation::TestHelpers::Minitest Ruby mixin, and it is passed the request object and the JSON response body generated by the service's API code. The ls-api_validation test helper code now uses information from the request to locate the Swagger JSON API definitions associated with the requested API endpoint (as defined earlier in this chapter). It then takes the JSON response body and simply runs JSON Schema validation code to ensure that the response JSON conforms to the JSON Schema that defines the response type published as part of the API contract; in this case, it compares the response JSON with the definition of the GeoCollection model defined above.\n\nJust to illustrate the usefulness of this approach, here is some sample output generated by validation failures. In this case the latitudes and longitude data returned by the API no longer is encoded as a JSON number, but as a JSON string instead:\n\nThe property '#/top_left_latitude' of type String did not match the following type: number in schema 1191bb4f-6d99-5a3c-8aa8-d4c0cee3908f The property '#/top_left_longitude' of type String did not match the following type: number in schema 1191bb4f-6d99-5a3c-8aa8-d4c0cee3908f The property '#/bottom_right_latitude' of type String did not match the following type: number in schema 1191bb4f-6d99-5a3c-8aa8-d4c0cee3908f The property '#/bottom_right_longitude' of type String did not match the following type: number in schema 1191bb4f-6d99-5a3c-8aa8-d4c0cee3908f\n\nThese failure examples are taken from an actual regression that happened in a service we worked on.\n\nWe had changed the Ruby gem that is used to generate JSON responses, mostly to take advantage of the new library's performance improvements for JSON serialization. Running our contract test suite found this change in the default behavior of JSON serialization for Ruby decimal number objects, and we were able to prevent contract breakages before launching the change.\n\nWhile the LivingSocial engineering department often releases internal tools as open source software, the ls-api_validation gem has not been released publicly at the time of writing. However, there are quite a number of other open source contract testing tools, such as pacto from ThoughtWorks, as well as Realestate. com.au's similarly named pact tool.\n\n149",
      "content_length": 3434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "Chapter 9 ■ testing with serviCes\n\nBoth of these tools provide additional functionalities to define the client applications' expectations regarding the API-providing service in the clients' test suite, and record those expectations to a file (a pact). These pact files are then used in the service's test suite as input to the contract test suite. This enables the API provider to verify that the clients that provided such pacts will not suffer from service-side changes.\n\nSummary This chapter first introduced you to the five layers of testing a microservices architecture:\n\nUnit testing\n\n\n\nIntegration testing\n\nContract testing\n\nComponent testing\n\n\n\nEnd-to-end testing\n\nAfter discussing how to best apply them in your projects, we showed an implementation of using\n\npluggable back-ends in service client libraries to aid in making test code less verbose and more realistic when using dependency mocking.\n\nWe then focused on examples for out-of-process component testing, using tools like Gherkin,\n\nCucumber, and Capybara, and we illustrated how these domain-specific languages could be combined to specify end-to-end features of our sample bookstore application.\n\nWe concluded Chapter 9 by showing real-word examples of contract testing and how it is used, and how\n\nit helped us find regressions prior to launching changes to our production environment.\n\nThe next chapter will address deployment-related aspects of introducing a microservices architecture\n\ninto your business.\n\n150",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "CHAPTER 10\n\nDeploying and Running Microservices\n\nBeing able to deploy a new service easily should be a goal of any company that embraces microservices. While there is much more risk involved in each deployment of a monolithic application, the processes involved with getting it deployed are much simpler than deploying small, but highly connected and interdependent services. In this chapter we will show that not only should new services be built, deployed, and configured in an automated and repeatable way, but that they need to be staged prior to releasing, in support of the overall stability and availability of your business during development efforts.\n\nIn this chapter, we focus on the aspects of building and deploying microservices in a continuous,\n\nautomated, and repeatable way.\n\nWe will also discuss how to utilize preproduction rollout strategies to make sure you can release changes to production with confidence, so that running microservices has a positive impact on the overall availability of your production application.\n\nFinally, we will explain how to design and utilize an infrastructure that is based on multiple stages and\n\ncan support the benefits introduced by adopting microservices.\n\nAutomation Is Key Before we go into any detail about deployment and operational aspects of microservices later in this chapter, let us emphasize something we have learned from our experience.\n\nAny organization that is serious about entering into employing microservices needs to fully embrace\n\nDevOps practices. Without a culture of automation, the productivity of your engineering department is bound to suffer enormously. While your organization might have gotten away with manually executing tests, builds, releases, and monitoring for a single deployment unit, such processes will be ground to a halt by the effort involved in repetitively executing these very automatable tasks for tens—or potentially hundreds—of microservices, each of which should be its own deployment unit.\n\nIn a successful microservices environment, developers build and run their software. Because their time\n\nis very valuable, everything—from provisioning computing resources to building, testing, and deploying artifacts, to monitoring and generating alarming for live services—needs to be self-service and scripted. When dealing with microservices architectures, it is absolutely unacceptable if provisioning and launching a new service takes any longer than an hour.\n\nBefore our mutual employer LivingSocial started adopting services, it started out with one of the largest and most high-traffic monolithic Ruby on Rails applications in existence, serving tens of millions of visitors. When the decision was made to break up the application into smaller parts, one of the main cost factors involved was that provisioning production resources to run a service would sometimes take weeks. Build and deployment tools were not unified or automated, and general service configuration and monitoring approaches were very manual.\n\n151",
      "content_length": 3022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nAs a result, during that time quite a few less-than-optimal architectural decisions were made, misplacing functionality into existing applications, simply because that was more expedient and less cumbersome than placing them into a new service with a more focused set of responsibilities.\n\nFocusing on introducing end-to-end automation, from committing to a code repository all the way to launching the changes in the production application, helped LivingSocial's engineering department address the bulk of these issues. Today, software development at LivingSocial is very agile and productive, even when the ratio of services per developer is roughly 2 to 1.\n\nTherefore, it is essential that an organization evaluate the available automation tools (some of which we have mentioned earlier in the book, and to which we will return later in this chapter) and find the ones that are best suited for their engineers, in order to address the added complexity introduced by having many, many deployment units.\n\nContinuously Integrating Microservices One indispensable measure for automating the software development environment for microservice development is to employ Continuous Integration (CI).\n\nA Brief History CI started out as a practice first proposed by Grady Booch in his 1990 book on object-oriented design.1 Back in those days, most software projects would be based on a process where all feature work—done by several different developers—was never integrated into a common code base until the very end of a given development cycle. This “big-bang integration” often led to a problem referred to as \"integration hell,” where a significant amount of time was spent on resolving merge conflicts of largely diverging developer branches before any clean build could happen.\n\nWhen the Extreme Programming movement adopted CI as one of its core practices, it greatly increased\n\nthe frequency with which such integration of developer branches into a code mainline would need to happen: in their view, integration (as well as testing of the integrated code base) would need to happen several times a day. In today’s increased use of agile software development with ever-shorter development and release cycles, the focus has shifted from the branch integration component toward continuously running test suites and building deployment artifacts.\n\nThe first automation support came around in 2001. With the open source CruiseControl (http:// cruisecontrol.sourceforge.net/), ThoughtWorks developed the very first CI server for code integration, artifact building, and subsequent testing.\n\nThe Advantages of Using CI Tools Today, there are a large number of tools available. Popular examples are Jenkins (https://jenkins. io/), Travis CI (https://travis-ci.org), Go from ThoughtWorks (https://www.go.cd/), and JetBrain's TeamCity (https://www.jetbrains.com/teamcity/).\n\nWhile these tools may vary in their detailed feature lists, the principle types of tasks they fulfill are all\n\nvery similar. They all do the following:\n\nMonitor a set of code repository for changes.\n\nCheck out the changed code from the repository.\n\n\n\nBuild artifacts.\n\n1Booch, Grady, Object Oriented Design with Applications, Benjamin Cummings 1990.\n\n152",
      "content_length": 3270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\n\n\nVerify that the code in the change set passes the automated test.\n\n\n\nStore the result of the CI run (often of the build artifact) in a uniquely identifiable way for later deployment.\n\nNote that, in our experience, employing continuous integration is worth the investment only if your\n\nengineering team buys into the culture that goes along with it.\n\nThis means that it needs to be engrained in the software development process that automated tests are added for every change. Additionally, changes should be committed to the repository in the smallest possible, testable chunks, in order to avoid the inefficiencies of the big-bang integration approach mentioned earlier. Finally, alerting needs to happen whenever the CI system reports problems or failures, and fixing such issues needs to be addressed by your team as their highest priority.\n\nIf you can implement these conditions in your organization, then the use of CI systems can contribute\n\nsome very valuable benefits to your development process:\n\n\n\nShortened feedback loop: Automatic, change-based execution of builds and tests will surface any bugs or other code quality issues2 early on in the development cycle. Research has confirmed that the cost of fixing a bug is higher the later it is discovered in the development process, so this can definitely be a big financial incentive to adopt CI.\n\n\n\nTraceability: CI tools can help to maintain an audit trail for code that is deployed to your production environment. Storing an identifier for the exact version of the code repository deployed (such as a Git commit SHA-1 identifier), a protocol of the verifications performed, a link to the previous and/or next build artifacts, and so on, will help operational aspects (for example, rollbacks to a known good state) and debugging efforts.\n\nUsing CI for Microservices A finer point about continuous integration and its relationship to microservices architectures is that you should set up your CI systems to deal with each of the microservices on an individual basis.\n\nMany of the advantages of using microservices are rooted in the fact that they enable adopters to make\n\ndecisions on a very fine-grained level. In line with this thinking we recommend having no less than one code repository per service. Such a setup will make things like switching programming language, optimizing around caching and data store technologies, grasping the purpose of the repository, and transferring ownership a lot easier than if several discrete services are rolled into the same code revisioning unit.\n\nAs a corollary, having one microservice per code repository also simplifies setting up dedicated and\n\nindependent CI builds.\n\nThe advantages of configuring your CI to work on each microservice separately are clear. First, CI build times are reduced so that the development feedback loop is tighter and overall productivity is improved. In environments with potentially hundreds of services, where the choice is made to run all of them in a single CI instance, it is possible that full CI runs can take a few hours, even if there is just a one-line change to a single microservice. Instead, setting up a discrete CI cycle for each microservice in this scenario means that the same one-line code change only triggers tasks that can finish in less than a minute.\n\nSecond, smaller build artifacts are produced as the outcome of a CI run. This in turn will lead to smaller\n\npackages to deploy to smaller deployment units per service.\n\nWe will return to the arguments about trying to build and promote the smallest possible deployment\n\nunit after explaining the rationale and benefits of continuous delivery.\n\n2Chapter 8 showed examples of using Jenkins CI for running code coverage and other static analysis tools.\n\n153",
      "content_length": 3817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nContinuous Code Delivery The concept of continuous delivery (CD) takes the concepts behind continuous integration and applies them to the entire software release cycle. It is a practice in which the engineering team has visibility into, and automated tool integration for, every step a software deliverable takes, from code check-in all the way through to releasing the software into production.\n\nHow Do You Know You Are Doing CD? In an article on his website,3 Martin Fowler describes four criteria that ThoughtWorks developed to verify that continuous delivery is being practiced in an organization:\n\n\n\nThe software artifact is deployable throughout its lifecycle.\n\n\n\nThe owning team makes keeping the software in a deployable state a priority over feature work.\n\nWhenever the software is changed, feedback regarding the production-readiness of software is automated, fast, and accessible to anyone.\n\n\n\nAny software artifact that has been verified to be working can be released to any stage (development, staging, production, and so on) with the push of a button.\n\nFowler summarizes these points saying that the litmus test is\n\n… that a business sponsor could request that the current development version of the software can be deployed into production at a moment's notice—and nobody would bat an eyelid, let alone panic.\n\nBreaking the Release Process into Stages Full and thorough verification can take a very long time. Before releasing code into production, it usually doesn't suffice to create code artifacts and then run automated unit and integration tests—all of which are relatively fast steps.\n\nIn many cases, you will usually want to deploy the code into a test-bed environment (for example, a staging fabric, as described in Chapter 8), in order to perform more coarse-grained (automated) end-to-end tests, or even have QA professionals expose the build artifacts to (manual) exploratory testing. This could also be a good strategy for verifying that nonfunctional requirementsof your services, like performance, are still met.\n\nImplementing these practices requires that you build a deployment pipeline.4 In such an approach, you\n\nsplit up the entirety of your release process into stages. To achieve fast feedback and best time efficiency, it makes sense to place fast stages (like unit testing) at the beginning of your pipeline, while more time- consuming steps (like manual acceptance testing) are pushed into later stages.\n\nFigure 10-1 shows an example of how a deployment pipeline could be structured.\n\n3Fowler, Martin \"ContinuousDelivery,” May 30 2013; http://martinfowler.com/bliki/ContinuousDelivery.html 4Fowler, Martin \"DeploymentPipeline,” May 30 2013; http://martinfowler.com/bliki/DeploymentPipeline.html\n\n154",
      "content_length": 2788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-1. Continuous Delivery pipeline with optional deployment step\n\nIn this pipeline, committing code to the mainline branch of a given repository automatically kicks off\n\nthe build step. If it succeeds, then the unit and integration test suites for the service are executed.\n\nNext, the code under test is installed in a staging environment, where automated end-to-end tests are run before humans manually explore the application. Note that it is best if testers record which tests were performed alongside a particular pipeline run, so that tests can be repeatable and blind spots can be addressed.\n\nHuman sign-off on the exploratory phase triggers building an artifact that can be deployed to the production fabric, and it is checked into a repository. The deployable artifact can be an executable, an image of a virtual machine, or an application container, all based on your company's choice of production infrastructure. More on these choices can be found later in this chapter.\n\nIn some cases, the continuous delivery pipeline ends in an automated step to deploy the latest successfully delivered build artifact to the production fabric, effectively resulting in a continuous release cycle.\n\nIf any steps fail, the pipeline will stop and report failure to the engineering team that owns the code to\n\nbe delivered via this pipeline, and fixing the failure needs to be that team’s very first priority.\n\n155",
      "content_length": 1465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nPROS AND CONS OF CONTINUOUS DEPLOYMENT\n\nin a blog post5 on the site for their excellent book Continuous Delivery,6 Jez humble and David Farley point out the differences between continuous delivery and continuous deployment, and which one to employ given the circumstances:\n\nContinuous deployment is the practice of releasing every good build to users—a more accurate name might have been “continuous release.”\n\n…\n\nWhile continuous deployment implies continuous delivery the converse is not true. Continuous delivery is about putting the release schedule in the hands of the business, not in the hands of IT.\n\n…\n\nIn the world of COTS, there are good marketing and support reasons why you'd not want to have more than a few \"released\" versions of your software in play at any given time (although you could still do regular \"developer\" or \"early access\" builds as Eclipse and the Omni Group do). There are probably other good reasons too—the important point is that they must be business reasons.\n\nin short, if you have constraints related to how many times your business can release software, then employ continuous delivery only. otherwise, tweak your delivery pipeline to automatically release your builds as a final step.\n\nif your company decides to implement continuous releasing of software, then make sure that the period of time from code commit to release to production is predictable and on the order of minutes. this way, the developer committing the change can plan to be around to verify and potentially fix the change in production.\n\nCD and Microservices The concepts and practices behind continuous delivery most certainly predate microservices architectures, but they become all the more essential when dealing with potentially hundreds of unique deployment units. But how do we map microservices applications to deployment pipelines? Should there be one single master pipeline that is responsible for monitoring, building, testing, and delivering all services? If not, of how many deployment units should the results of any given delivery pipeline be comprised?\n\nIn our experience, the biggest advantages are reaped if there is a separate pipeline per service. Only by dedicating pipelines to services will we be able to make sure that each of the services that make up the full application can be released (and potentially recalled) individually, and without negatively impacting the rest of the system.\n\nTool Support for Continuous Delivery When investing in starting a culture of CD, you can start fairly small and simple. For example, if you are already practicing continuous integration in your organization, you can often leverage the same toolset to trigger and monitor additional dependent steps that are gated by success of previous stages.\n\n5Jez Humble, \"Continuous Delivery vs Continuous Deployment,\" August 13 2010. http://continuousdelivery. com/2010/08/continuous-delivery-vs-continuous-deployment/ 6Jez Humble, David Farley, Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation, Addison-Wesley Professional 2010 156",
      "content_length": 3137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nAs an example, LivingSocial engineering started its own continuous delivery journey by repurposing Jenkins CI to kick off downstream deployment jobs automatically if the build-and-test CI job succeeded. Figure 10-2 shows the example of a Jenkins project configuration that triggers builds on two other downstream Jenkins project.\n\nFigure 10-2. Jenkins CI project configuration that triggers dependent deployments\n\nThe projects triggered sequentially shown here are a deployment to the staging fabric (internally called MYQA) and a subsequent production deployment. Triggering the dependent deployment projects after the CI build hinges on the build status; only if the latest code in the mainline branch could be built and tested successfully, and is hence considered “stable,” will the deployments be started.\n\nNote that LivingSocial also developed a uniform set of tools (on top of the Capistrano deployment tool), command-line scripts collectively called ls-deploy, which can be used to automatically kick off deployments across all projects that can be deployed to all network fabrics. The universal usage of this tool, and the simplifications it provides, are more indications that a DevOps culture is essential to launch and support microservices architectures efficiently.\n\nToday there are quite a few other tools available that specifically focus on supporting continuous\n\ndelivery.\n\nAmazon.com made its internal build pipeline tool available for commercial use as the generally\n\navailable AWS CodePipeline (https://aws.amazon.com/codepipeline/). CodePipeline is a highly configurable and programmable tool that can be used independently of any of the other AWS service offerings, and it integrates with source repository, CI, and deployment solutions like GitHub or Jenkins. AWS CodePipeline workflows consist of sequential, interdependent stages that can communicate via input and output artifacts. Each stage is made up of actions that can be run in parallel or sequentially. CodePipeline provides an API that can mark single actions as success or failure, which serves as an excellent way to integrate custom actions into the flow of the delivery pipeline. CodeDeploy actions can even be directly implemented as serverless AWS Lambda functions (https://aws.amazon.com/lambda/), which can also directly report back results to influence the course of the pipeline.\n\nFigure 10-3 shows the graphical UI for creating and editing CodePipeline flows, in this case a very\n\nsimple test pipeline set up for an experimental service at LivingSocial.\n\n157",
      "content_length": 2604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-3. Example of a very simple delivery pipeline in AWS CodePipeline\n\nAnother relatively new arrival, but already a very promising looking CD tool, is Netflix Spinnaker\n\n(http://www.spinnaker.io/). Spinnaker is an open source project that not only allows you to build delivery pipelines, but also aims to introduce a layer of abstraction on top of the infrastructure used to run service software. It aims to support building, testing, and deploying code not only to all three major public cloud platforms (Amazon AWS, Microsoft Azure, and Google's Compute Engine), but also to private cloud installations by supporting a plugin approach for cloud back-ends. Pivotal Cloud Foundry private cloud support is already implemented in this fashion. Figure 10-4 shows the UI to set up Spinnaker pipelines.7\n\n7“Global Continuous Delivery with Spinnaker,” posted by Andy Glover, November 16, 2015; http://techblog. netflix.com/2015/11/global-continuous-delivery-with.html\n\n158",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-4. A Spinnaker pipeline, as shown on the Netflix techblog\n\nMaturing Code Preproduction In an application that is made up of many smaller interdependent service components, it is essential to increase confidence in any code changes in an integration environment.\n\nThere is only so much that testing any service can tell you when you test it in isolation, and it is\n\nunrealistic to assume that stubs or mocks for service dependencies are complete with respect to functional, as well as nonfunctional, edge cases.\n\nThe closer the integration environment is to simulating production-like conditions, the more reliable\n\nand hence valuable the feedback received from prerelease testing becomes.\n\nThe rest of this section describes our experience with the best ways to address these concerns.\n\nStaging as a Preproduction Environment While discussing a suitable development process for a microservices-based application in Chapter 8, we recommended maintaining several network fabrics in which to install instances of all services that make up the application.\n\nWhile the discussions in Chapter 8 focused on the need for a development sandbox environment, this section will consider the use of a staging fabric. Figure 10-5 describes the scenario we recommend implementing.\n\n159",
      "content_length": 1330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-5. Use of a staging fabric for preproduction testing\n\nOnce the delivery pipeline has built all the artifacts for a given service, and all tests that can be\n\nperformed in isolation have passed, it is time to deploy the service into a staging environment.\n\nWe will explore the options and our recommendations for generating an entire network fabric later in\n\nthis chapter. For now it is just important to emphasize that the staging setup should aim to mirror the actual production infrastructure as closely as possible.\n\nAt the very minimum, this means that all interdependent microservices (other than the one that is part of the current change set under test) are deployed and running in the staging environment using exactly the same code versions that are live in production.\n\nOn top of that, you should strive to make sure that the rest of the infrastructure can mirror the production setup. Operating systems, databases, proxies, and the like should all run in the same versions as in production. The routing layer configuration (routing tables, ports, use of SSL) and hardware resources (number of CPUs, physical or virtual machines, disk space, and so on) should match what is used to run your actual business.\n\nWe realize that significant costs can be involved in provisioning additional computing resources to fully simulate the production environment; additional hardware and software licenses need to be requisitioned, set up, and so on. It may well be that the costs outweigh the benefits.\n\nThe point is that we strongly advise going through such cost-benefit analysis. Any decision to skip setting up a realistic staging environment should be preceded by a detailed risk analysis, in which cost factors are attached to each risk. For example, how likely is it that a different version of the operating systems in production will cause a 30-minute outage, and how much revenue and/or customer trust would you lose? Once the staging environment is provisioned with the appropriate build artifacts, it needs to be brought\n\ninto a state that has all the appropriate data needed for preproduction testing. This can be best achieved by maintaining set of automated staging setup scripts.\n\nThe purpose of these scripts is to automatically bring the staging fabric into known good states, so that\n\nautomated and manual tests can be performed in a repeatable manner.\n\n160",
      "content_length": 2434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nTool Support for Staging Environments Setting up a staging environment that has the same properties as your production environment follows the same rules as all infrastructure-related tasks in a microservices architecture: if you do not want to overwhelm your engineering department with busy work, then it needs to be automated with a set of tools.\n\nThe tools of choice to implement the infrastructure-as-code include Chef, Vagrant, Ansible, and Puppet.\n\nAll of these tools can be used both for managing and provisioning your infrastructure and configuration based on version-controlled configuration files, rather than manually configuring hardware via interactive configuration tools.\n\nThey all can be used irrespective of your company’s choice to run the physical data centers with bare- metals servers or to use a public cloud, and regardless of the choice to utilize virtual machines or application containers or not.\n\nPaaS provider Heroku comes with very good documentation showing how to set up a staging environment,8 and we know from personal experience that using Heroku's cloud platform for staging purposes is very easy and convenient.\n\nAnother toolset we have successfully used to set up staging fabrics is Amazon's Web Services platform,\n\nand we are very impressed with the power of the tool chain surrounding infrastructure automation. AWS provides a dedicated set of services called AWS CloudFormation for storing infrastructure definitions, and to automatically generate environments based on these definitions. Another very useful tool that works together with CloudFormation is the community-supported CloudFormer,9 which can be used to create a CloudFormation-compatible infrastructure definition template for your staging environment from already- deployed production AWS resources.\n\nAnother very promising tool for infrastructure automation is Otto by HashiCorp (https://www.\n\nottoproject.io/). We mention it here even though it is in a very early stage of development, because it promises to greatly simplify the development-to-staging-to-production fabric transitions. While it unfortunately does not yet handle any fabrics other than development on the local workstation and a production deployment, support for more stages is on the project’s roadmap.10\n\nAs far as the staging setup scripts are concerned, their aim is to make sure that the data stores for each\n\nof the services are primed with a set of sample data that enables the end-to-end and exploratory tests to execute both normal and exceptional paths.\n\nSuch scripts are usually very business-logic specific, and are by that token often very close to the particular technology in use. As an example, when SQL-based databases are used to store service state, they often include raw SQL statements stored in seed files, which are imported into the databases for each of the services involved at staging set-up time.\n\nSome companies, including LivingSocial, also provide access to production database dumps (which are scrubbed of sensitive information), which such set-up scripts can use to prepopulate the services' database. While exact repeatability of test situations is lost (because of the dynamic nature of production data), the advantage gained here is that the set-up will more closely match the real-world conditions in which your application needs to function.\n\n8Heroku Dev Center documentation, “Managing Multiple Environments for an App”; https://devcenter.heroku. com/articles/multiple-environments 9AWS Developer Tools: CloudFormer template creation tool; http://aws.amazon.com/ developertools/6460180344805680 10See the related discussion on the project's GitHub issue number 161; https://github.com/hashicorp/otto/ issues/161\n\n161",
      "content_length": 3780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nSafer Releases As mentioned in the previous section, setting up a one-to-one production-equivalent staging fabric is sometimes not a cost-effective option for every business.\n\nAlso, while setting up a production-like, preproduction environment is definitely very valuable for detecting issues when integrating a microservices-based application end-to-end, it is certainly no 100% guarantee that the same will be true when exposed to production conditions.\n\nTherefore, it is a good strategy to try to make the process of releasing code changes to production—as well as potentially rolling back defective changes—as safe as possible. The following two release strategies are aimed at supporting the resilience and recovery tenet you’ll see in the section discussing the seven deployment tenets later in this chapter.\n\nUsing microservices, and having properly separated the services into small self-contained units, helps\n\nthe roll-out of code changes incrementally to each single service without blocking other changes to other ones. This is another advantage compared to maintaining a monolithic application, where all changes have to stack up sequentially.\n\nBlue-Green Deployment\n\nLike so many software architectural patterns, blue-green deployment existed for many years as a practice before being named, and it is not entirely certain who first named the technique. In his 2010 article on the subject, Martin Fowler credits Dan North and Jez Humble with coining the term.11\n\nIn its essence, blue-green deployment is a technique that allows for fast cut-over of production traffic to\n\na new code version, but only after a series of smoke tests have been successfully executed.\n\nThis is done by delivering the new code to the production environments (labelled green), to be installed\n\non the service host alongside the service code version currently exposed to live production traffic (which is called blue code).12\n\nAs part of the blue-to-green switch, a set of automated smoke tests are then performed on the green\n\ncode. Once the green code is confirmed to be working as expected, the service host's configuration is changed to direct all incoming requests to the green code.\n\nBlue code now becomes unused, but still installed on the host. This sets you up for rapid rollback in case any problems are detected shortly after the switch-over, so you can easily and quickly switch back to the blue environment.\n\nFigure 10-6 illustrates the process of blue-green deployment.\n\n11Martin Fowler, “BlueGreenDeployment,” 1 March 2010; http://martinfowler.com/bliki/BlueGreenDeployment. html 12Some implementations of blue-green deployments choose to instead install the green code on an entirely separate set of service hosts, and then change over production traffic by changing load-balancer or DNS configuration. The principles of this technique still apply unchanged, though.\n\n162",
      "content_length": 2927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-6. Blue and green code are installed side-by-side, and switched upon release\n\nCanary in the Coal Mine\n\nAnother approach for making the release process safer and less impactful that is often used in microservices architectures is referred to as canary release.\n\nCanary releasing—sometimes also referred to as shadow deployment or incremental rollout—reduces\n\nthe risk of releasing new versions of software to production by means of a gradual roll out.\n\nThe diagram in Figure 10-7 describes the technique.\n\nFigure 10-7. Canary releasing in three stages of traffic to the new version\n\n163",
      "content_length": 645,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nThe following steps are usually taken during incremental deployments:\n\n\n\nThe new code is deployed alongside the code that is currently used by 100% of the production traffic (step 1 in Figure 10-7).\n\nNext, only a small percentage (such as 10%) of service clients is exposed to the new code version (step 2 in Figure 10-7).\n\nOnce released to 10% of traffic, the engineering team will closely identify and monitor the performance of the change (based, for example, on feature functionality, response times, or service and client-side failure rates).\n\nWhen enough confidence is gained that the new version is functioning as desired, more traffic is diverted from the existing production code version and sent toward the new code. Monitoring continues to ensure that the new service version is behaving as designed.\n\n\n\nFinally, once enough traffic increase iterations have been performed satisfactorily, 100% of client traffic is directed to the new code version, and it becomes available to everybody (step 3 in Figure 10-7).\n\nCanary releasing is very closely related to blue-green deployment in that both versions are deployed to\n\nthe production service hosts simultaneously.\n\nThe main difference is that both versions coexist in production for a much longer time, as the switch-\n\nover from old to new code versions happens in stages (as opposed to the “all or nothing” traffic routing approach employed in the blue-green deployment strategy).\n\nIn a variation of the canary release process, some engineering organizations divert traffic to new code\n\nversions based on criteria other than by percentage or request. One example that we have seen practiced is to identify traffic generated by employee users, and then cherry-pick new or changed functionality only to such internal clients. Exposing new code to such a controlled user group first often also helps with monitoring changes in client-side experience.\n\nAnother twist sometimes applied to this phased rollout is to simply copy a portion of production traffic\n\nand feed it into the new service code version. You can then watch the new version handle the requests without exposing the results to the clients, which will continue to be serviced by service instances running the old production version. While the advantage here lies in additional safety (as initially no clients will receive results generated by the new code), the trade-off is that results are often less meaningful because the impact of the new service code is not verified end-to-end.\n\nNote that neither canary releases nor blue-green deployments by themselves address the problem of mutating shared resources, such as potentially breaking migrations for database tables that might be used by both the old and the new versions of the code. These problems need to be mitigated by other means; for example, via additive-only tables and duplicated data for the new code.\n\nAssigning Resources to Microservices We have tried to demonstrate throughout this book that having a large number of self-contained, small services to make up your application has many upsides.\n\nIn this section, we first recap the gains achieved by adopting microservices. Next we walk through each\n\nof the current technology options for assigning the services to computing resources, and we compare how each one of them supports the microservices architecture advantages previously identified.\n\n164",
      "content_length": 3440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nTenets for Microservice Deployments Let's start out by detailing some of the main principles that microservices architectures bring to the business when it comes to deployment and operational aspects:\n\n\n\nYou build it, you run it: Development teams gain autonomy of the entire lifecycle and operational aspects of the software they produce. Developers are empowered to make their own technology choices, as they also are involved in monitoring and maintaining the infrastructure and computing resources on which their software runs.\n\nContinuously and independently deliverable: Code bases for services can be kept small and self-contained. As a result, code artifacts can be built, delivered, and deployed in isolation. Changes in one microservice can trigger a delivery pipeline that is independent from the pipelines associated with other deployment units.\n\n\n\nLocality of technology choice: As the only way to access microservices' capabilities is via well-defined APIs, the technology choices and all dependent resources (like operating systems, data stores, library dependencies, and so on) are local to any given microservice. Therefore, they can be changed or updated fairly inexpensively and without the risk of impacting other parts of the overall application.\n\n\n\nLocality of configuration: Each service has its own set of configuration properties that can be tweaked. This also means that their runtime properties can be independently optimized. Also, when specific security constraints apply to their mission (such as PII or PCI requirements), they can be secured without affecting any interdependencies with other deployment units.\n\n\n\nIndividual scaling: When functionalities need to be scaled out, microservices provide the business with very fine-grained options and hence greater flexibility. Some microservices might profit most from being horizontally scaled, while others should best be scaled vertically. When horizontal scaling is best, the additional resources (such as additional hosts) can be added more cost-effectively, as the additional computing power can be added to smaller parts of the application. Partitioning services based on their expected load is also easier. Your team might own a telemetry service that needs to handle 100,000 requests per minute with very small payloads, while it also owns an internal reporting application with only 10 requests per minute, which trigger longer running data queries. These services will have very different scaling concerns, which means they should live in separate microservices, so that their constraints can be addressed with independent deployment units.\n\n\n\nSelf-contained codebases: Microservices have separated and focused concerns, which make understanding, changing, and verifying their code safer and faster. Code changes can be kept more local and have less potential to affect other parts of your application. Test suites can be focused on a smaller set of scenarios and can hence have very small execution times.\n\nResilience and recovery: Building your application based on services involves more individual service processes. While this means that the chance that any one process can fail increases, this risk is mitigated by the fact that the impact caused by a single process failing is much smaller. Also, the culture of employing circuit breakers and sane fallback values into clients to address dependency outages will become much more engrained in a successful development team. Mean time to recovery is usually also shorter than in a monolithic application, as independent deployment units make the cause of regressions more easily identifiable, and rolling back and launching smaller build artifacts is faster.\n\n165",
      "content_length": 3758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nWe will examine various strategies for assigning computing resources to microservices in the remainder\n\nof this section, paying special attention to how each of these strategies can help implement the advantages just described.\n\nOptions for Running Microservices Ever since the invention of computers, much time and effort has been spent in the IT industry make the use of physical computing resources more and more efficient.\n\nOne of the main advances in achieving that has been the development of virtualization, which is mainly\n\naimed at increasing efficiency of a physical server by partitioning it into smaller and smaller units, each of which roughly has the look-and-feel of being an independent host in itself.\n\nThis section will look at each of the major levels of virtualization currently in use. For each level it will explain the pros and cons of installing a single microservice versus multiple microservices in a given virtualization unit for that level, considering whether the microservices tenets discussed above can be maintained.\n\nRunning on Dedicated Hardware\n\nThe largest possible level of virtualization granularity is to not virtualize at all; all services can run directly on physical machines.\n\nThe diagram in Figure 10-8 shows the options.\n\nFigure 10-8. Deploying a single microservice vs. multiple microservices directly on the physical server\n\n166",
      "content_length": 1425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nLet's first look at the scenario where each microservice would run on a dedicated physical server. We\n\nknow of no microservices architectures in the wild that really even seriously considered running such a setup. Dedicating a server to each of the often hundreds of microservices in a production environment and duplicating such a configuration in a staging fabric, and potentially even a development sandbox, would be simply prohibitively expensive for most businesses.\n\nEven if money was not an issue given the “you build it, you run it” principle, the development teams would need to provision a physical server each time they decided to start a new service project or needed to scale out the number of service instances. Lead times to procure new hardware for such situations would be too long for any team to avoid idling and would thereby decrease velocity. In most businesses, the development teams' autonomy would also vanish, as other departments are usually involved in granting provisioning requests.\n\nA slightly more reasonable choice is to host more than one service instance on any given physical\n\nserver. In this case, “you build it, you run it” is still affected, as monitoring and support would be complicated by the fact that resource contention (for CPU, memory, connections, storage, and so on) would need to be tracked down and addressed across multiple services. In our experience at LivingSocial we have run into quite a few situations where long-running, resource-intensive tasks for one service caused degradation in another service that was installed on the same physical box, and tracking these issues down takes time-intensive investigations using multiple tools.\n\nRunning more than one service on the same physical machine can also adversely affect the “locality of\n\ntechnology choice” tenet. What do you do if more two services installed on the same, nonvirtualized machine have conflicting requirements for dependencies like libraries or even operating systems or OS versions?\n\nTeam autonomy and the “locality of configuration” benefits are also hard to maintain if services from\n\nmore than one team are deployed to the same physical host. If two services are competing for the same resources, then it will be very hard to configure the partitioning of these resources optimally, and arbitration in such situations will then have to fall to a third team that will be tasked with general infrastructure maintenance.\n\nWhat if there are special requirements about the sensitivity of data held and vended by one service that\n\nis installed alongside services that do not need to follow the same stringency? Another case in point is that the desired “locality of configuration” will be hard to implement with co-hosted services.\n\nResource contention is also the reason why “individual scaling” would become more complicated to implement. How do you best identify the physical servers on which to install additional resources (such as another service instance, or more memory or disk space)?\n\nLikewise, “resilience and recovery” will be negatively impacted if the failure or resource inefficiencies (like memory leaks or CPU stalling) of one service can negatively impact the entire server environment, and therefore the rest of the services installed and running on the server.\n\nRunning Services in an Enterprise Application Server (EAS)\n\nThe next small step toward using the physical servers more resource-efficiently is the use of an EAS technology, like Java Enterprise Edition (JEE)-compatible server products, or Microsoft Internet Information Server (IIS) for .NET technologies.\n\nThe advantage of this virtualization is that an EAS usually gives you nice features like unified service monitoring and alarming, hot service restarts, and clustering. Also, if you run several services that run on top of the Java Virtual Machine (JVM) or the .NET Common Language Runtime (CLR), they help with better use of host resources by requiring only one instance of the respective platform's runtime, which all the service code artifacts (like JEE .WAR files) share.\n\nEmploying an EAS solution where a single service is installed in each EAS incarnation is even less advantageous than running each microservice on a dedicated physical machine. Such an approach would share all the disadvantages of dedicating a server to each service, plus it would incur the costs for the overhead required to run the EAS itself.\n\nFigure 10-9 therefore depicts running more than one service inside an EAS.\n\n167",
      "content_length": 4563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nFigure 10-9. Using an Enterprise Application Server to run multiple microservices\n\nEven when you choose to install multiple services inside the same EAS container, the drawbacks are\n\nnearly the same as with running the services directly on the service boxes. While the various EAS suppliers advertise benefits that address some of the monitoring and support concerns,13 this approach introduces new impediments to the “locality of technology choice” tenet: when choosing an EAS, you lock yourself into a particular technology stack for your service implementations, as well (for example, services can be JVM or .NET-based only). Also, while JEE servers usually come with easy ways to redeploy just a single service's code artifact into the EAS container, this is not a given for all EASes per se, so it is well worth your time to research whether your application server allows for this. If not, the “continuously and independently deliverable” benefit will no longer be true.\n\nUsing VMs\n\nIn this approach to using computing resources more efficiently, additional software called a hypervisor is installed in the physical server—either in the server's firmware, or on top of an operating system—to enable virtualization. The hypervisor then allows you to divide the underlying single server machine into a number of separate hosts called virtual machines (VMs).\n\n13Anyone who has attempted to analyze the behavior of Java threads for applications that share the same JVM process can attest that improvements to monitoring are certainly not perceived by every developer.\n\n168",
      "content_length": 1624,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nEach VM for all intents and purposes looks and feels very much like a separate, physical server: it runs\n\nits own operating system (including its kernel, should it have one) and application process independent from the other VMs installed on top of the hypervisor. They are also separated from any potentially underlying host operating system.\n\nThe hypervisor takes on all VM management tasks, from assigning the physical resources of the host\n\nmachine to the VMs, to controlling and even potentially resizing the VMs. To perform these tasks, the hypervisor itself uses resources on the host machine, though, and the more VMs need managing, the more of the physical server's resources it will needs. This places a natural upper bound on the number of VMs that can be efficiently run on any given server.\n\nFigure 10-10 shows how VMs relate to the physical host machine and the services running inside them.\n\nFigure 10-10. VMs can be configured to run a single or multiple microservices on top of a hypervisor\n\nAs was an option with physical servers directly, and also with employing EAS containers, you can choose to run more than one microservice inside any and all virtual machines controlled by the hypervisor. The problem with that choice is that you will still negatively impact all the tenets mentioned earlier when discussing the disadvantages of running more than one service per hosting unit.\n\nOn the other hand, the use of virtual machines is a great step toward avoiding the downsides involved\n\nwith having to acquire and provision a new physical host for each time you decide to add services. While there is, as mentioned, an upper bound on how many VMs can be managed on a single server, you can get to the state where you host each service independently on a dedicated VM much more cost-effectively.\n\n169",
      "content_length": 1867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nOne particular advantage over running directly on a physical machine is good support for the “continuously\n\nand independently deliverable” and “individual scaling” tenets. This is mainly based on the fact that VM images can be automatically built for each service to be deployed in isolation, as part of the service's delivery pipeline. This will also enable automatic scaling mechanisms, as you can start to monitor service loads and respond in a scripted way by deploying a new VM for just the small set of services that experience a higher load.\n\nNote that these advantages cannot be realized when you decide to deploy multiple services into\n\nthe same VM, as you would be obligated to bundle a set of service to always be co-hosted, and thereby sacrificing their independent deliverability and individual scalability.\n\nThere are a large number of choices for software (called “type-2”) and hardware (“type-1”) virtualization.\n\nAWS EC2 deployable Amazon Machine Images (AMIs), VMWare's VMWare Player, Redhat's KVM, and Oracle's VirtualBox are all choices where the hypervisor runs on top of the host OS. Type-1 virtualization options include the Open Source Xen project, VMWare's VMWare ESXisolution, and Microsoft's Hyper-V.\n\nUsing LXC with Docker\n\nVirtualizing at the machine level is not the only option available in order to get more isolated hosting units out of your physical hardware.\n\nAs briefly mentioned in Chapter 8, Linux Containers (LXC) can be used to virtualize multiple isolated\n\nLinux systems on top of a single Linux host machine.\n\nOne of the notable differences between LXCs and VMs is that only one Linux kernel is in use across all virtualization units. VMs host the entire stack of OS, kernel, and applications, while LXCs contain only the higher OS layers and applications. Reusing the same kernel allows LXCs to run without the need for hypervisor software, which VMs need to be managed and to partition host resources; physical resources for LXCs are managed by a single kernel.\n\nAs a result of these differences, LXCs are much speedier to bring into an up-and-running state than VMs, and their smaller footprint also means that a typical host can run a lot more LXCs than VMs before responsiveness is seriously impacted. This means that it will be much more cost-effective to run all your microservices in a separate LXC than it is to run them in separate VMs.\n\nOut of the box, creating, setting up, and installing apps inside LXCs is rather cumbersome. Here is\n\nwhere Docker or proprietary solutions like Heroku's container management come in.\n\nWe introduced Docker in Chapter 8 when we showed how to use interdependent containers for\n\nbuilding a development sandbox. Docker's main mission is to simplify container management and networking setup. It uses a text file called a Dockerfile to describe and install the applications in each container. Docker also comes with the concept of a shared repository for containers (called a Docker registry), so that you can reuse containers that have been contributed by others. Such a reusable container can either come from a public Docker registry like https://hub.docker.com/, or they can be provisioned from a private Docker registry, which you can install for your company-specific applications.\n\nCONTAINERS ON HEROKU'S PLATFORM\n\nthe company heroku offers a set of higher-level services often referred to as Platform as a Service (paas). it implements automated provisioning, deployment, monitoring, scaling, and high availability on top of aWs eC2 instances.\n\nheroku offers service-level options, mostly tiered by the amount of resources they dedicate to a hosting unit (called dyno in heroku's terminology). the largest dynos are called performance and performance-m, and they are hosted on a dedicated large eC2 instance.\n\nall other dyno types run in a multitenancy environment; that is, they run as single process inside an application virtual machine.\n\n170",
      "content_length": 3987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\naccording to heroku's own documentation, a dyno is a \"lightweight linux container that runs a single user-specified command.\"14 this means that heroku also uses lXC as their virtualization engine, which—as discussed at length earlier—is the same technology on which Docker is built.\n\nif you are considering the use of a paas provider for your microservices application, then we definitely recommend taking a closer look at heroku. We have deployed several microservices in multiple technologies (ruby on rails, Clojure, and go) into heroku's platform, and we loved the simplicity and convenience of the experience.\n\nTrying to describe all the available tooling on top of Docker containers is like trying to shoot at a very fast-moving target. Much work is being dedicated to managing many distributed Docker instances automatically, and even to create entire cloud platforms that work with Docker containers instead of VMs. We will talk a little more about these later when discussing automated scaling options.\n\nLooking at how to distribute the microservices you run across LXCs/Docker containers, there are even fewer reasonable arguments to co-deploy multiple services into the same container. That is the reason why Figure 10-11 illustrates 9 LXC containers running exactly one microservice inside each and every one of them.\n\nFigure 10-11. Docker LXCs running on top of a single Linux kernel, with no need for a hypervisor\n\n14See the “Dynos and the Dyno Manager” documentation page at https://devcenter.heroku.com/articles/ dynos#dynos\n\n171",
      "content_length": 1595,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nIsolating each microservice in its own Docker container is much less resource-intensive than even using one per VM. Following such a “one service per container” mantra enables you to implement all the tenets of deploying microservices listed at the beginning of this chapter, and thereby reap all their benefits.\n\nNote also that nothing keeps you from running LXC containers inside VMs, so if you already have a\n\nVM-based infrastructure, you can fairly easily add finer-grained virtualization this way. In fact, AWS offers “Amazon EC2 container service,” which is specifically aimed at helping you manage Docker containers running on EC2 instances that are defined by AMIs, and which can be autoscaled to go on- or offline based on load.\n\nUsing Serverless Architectures\n\nThe newest technology option to arrive on the block for running microservices is something most generally referred to as serverless architecture. It can best be described as an additional layer that aims to alleviate you from even thinking about service hosts, so that service development is uncluttered and can be taken to a higher level of abstraction.\n\nWhen services run serverlessly, there is no longer a need for the developers to operate and maintain the computing resources themselves. The serverless platform where you run your service code will take care of all of these aspects on your behalf. This has the potential to remove a large amount of complexity and operational workload. Automatically scaling up and down, resilience, delivery and deployment, operating system and library updates, setting up monitoring, alarming, and logging capabilities, and so on will now be a responsibility of the platform that provides the serverless computing services.\n\nAUTOSCALING—THE HOLY GRAIL OF INFRASTRUCTURE AUTOMATION\n\nautoscaling is definitely not a “Day one” issue for getting started with microservices architectures. there are quite a few preconditions that need to be met before you can even start thinking about attempting to take advantage of such a feature.\n\nFor one, your organization needs to be set up for speedy and fully automated provisioning of computing resources. this means that your services are either running inside vMs or lXC containers (as provisioning new physical servers—even via automated means—will simply have too much lead time to be useful), or you are running in a public cloud (like aWs or Ms azure) or inside a paas (like heroku).\n\nall major public cloud platforms allow for automated scaling in one way or another. as an example, aWs has long had support for autoscaling (https://aws.amazon.com/autoscaling/). When you run your own cloud via Docker containers, there are solutions based on google's kubernetes (see http:// kubernetes.io/docs/user-guide/horizontal-pod-autoscaling/).\n\nnext, you will do well to research what exactly the autoscaling parameters and triggers are, and how you can tweak them. Most autoscaling mechanisms built into the platforms we have seen are more reactive than proactive: infrastructure monitoring discovers that Cpu load increases, server memory runs low, or that there is a lot more traffic on a particular service, and subsequently spins up new computing power (in the form of a new service instance, for example).\n\n172",
      "content_length": 3315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nWhile such after-the-fact smarts certainly help deal with acute resource shortages, getting into situations where the client experience degrades can be entirely avoided by some proactive planning. in our experience, you can learn a lot of useful things from watching your site's traffic patterns, which you can apply to “semi-automatic” scaling: if you see that your customers mostly visit your site during the us time zones' lunch hours, then introduce rules that anticipate this behavior and add computing resources ahead of time, so that the site is ready for the rush hour.\n\nAt the time of writing, there are not many providers for serverless platforms. Early providers are Hook.\n\nio (https://hook.io), Webtask.io (https://webtask.io/), and Amazon's AWS Lambda (https://aws. amazon.com/lambda/). Amazon offers another complementary service called AWS API Gateway, which can add a layer in front of Lambda functions to aid security, transform request and response messages, and provide options for throttling and auditing.\n\nMoving to a serverless model for running your services also scores very high in checking which of\n\nthe microservices operational tenets are met. In fact, the only two points we do not see addressed by the currently available options for serverless platforms are “you build it, you run it” and “locality of technology choice.”\n\nIt can certainly be argued that the underlying assumptions for the first of these two tenets are no longer\n\nvalid. The main point of serverless computing is that developers need to be less involved in most of the operational concerns. What remains desirable, though, is that the development team gets notified via alarming if any of the operational metrics of their services start to deviate from normalcy. Amazon's AWS Lambda platform integrates very closely with AWS CloudWatch, so at least this platform has that aspect covered.\n\nWhere current platforms are definitely lacking are the technology options they provide out of the box. Webtask.io requires you to write all code in Node.js using JavaScript. AWS Lambda currently only supports JavaScript, JVM-compatible, and Python code. Hook.io is the platform that has the longest list of supported languages.15 There are also other restrictions on each of the platforms around technology choices; for example, the backing file systems or data store options, and neither of them appears to be set up out-of-the- box to support arbitrary, user-defined replacements.\n\nAs mentioned at the beginning of this subsection, serverless applications are a very new and rapidly evolving technology, with proprietary approaches and platforms, which carries a high potential for provider lock-in. There is not much that has been written about the subject, but we are looking forward to reading Obie Fernandez' upcoming book16 on the subject.\n\nSummary\n\nThis concludes our overview of what we consider the most important aspects of deploying and running microservice-based applications.\n\nOne of the main points to take away from all this is that automating as much as feasibly possible around\n\nbuilding, testing, deploying and releasing is an absolute must if you want to make microservices work for your team.\n\nIntroducing continuous integration for each of your services helps to significantly shorten the feedback loop during your development cycle, and the traceability of your large code artifacts, as long as you heed our advice to place each service into a separate code repository and set up CI for each one of them.\n\nGoing a step further and adopting continuous delivery from code commit to deploying it on the production hosts is also a practice we highly recommend adopting. Building a delivery pipeline per microservice can be done with the support of a number of existing tools.\n\n15See the code examples page for a list of languages in which samples are implemented; https://hook.io/examples 16Obie Fernandez, Serverless: Patterns of Modern Application Design Using Microservices (Amazon Web Services Edition); https://leanpub.com/serverless\n\n173",
      "content_length": 4099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Chapter 10 ■ Deploying anD running MiCroserviCes\n\nYou can even go so far as to automate the step of releasing new code to customers, but in this case, you have to think about how to do so safely. We explain strategies for implementing safety measures, including the use of a staging environment in which to let your code mature prior to production release, blue-green deployment and canary releasing.\n\nWe also discussed the available options for assigning computing resources to your microservices, based on seven tenets we identified as beneficial for deploying microservices. We explained the different levels of virtualization available today, and how their use can support the advantages gained by employing microservices.\n\nIn the next chapter, we will place our focus on design and implementation aspects of consumer-facing\n\nclient applications backed by a microservices architecture.\n\n174",
      "content_length": 894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "PART IV\n\nPutting Everything Together",
      "content_length": 36,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "CHAPTER 11\n\nPolyglot Services\n\nAs laid out in Chapters 4 and 5, applications in a service-oriented architecture can profit greatly from using common protocols and data formats (such as JSON over HTTP), and from choosing a single IDL for interapplication communication.\n\nIn this chapter, we lay out why applying uniformity of programming languages, data stores, or other technology components across all the services in your microservices application might not be the best choice. After looking at the various components in which the services in your overall system can change, this chapter discusses the benefits and pitfalls of using differing technologies throughout your service implementations. Next we provide some guidelines for deciding when to become more polyglot in your architecture. Finally, we’ll offer some practical tips for the process of introducing technology options to your microservices application.\n\nDimensions of Change The majority of services in a microservices environment expose many areas that together make up a multidimensional vector for potential improvements.\n\nMany service applications can be rewritten using different programming languages. Even if they choose to keep the language the same across services (or different versions of the same service), the service owners can use a different set of libraries to rely on. Some companies we know, while keeping the core programming language the same, vary application frameworks between different deployment units.\n\nAnother popular change is to examine the use of a different type of data store, such as by switching\n\nsome portion of the service's persistence layer to use a NoSQL solution in place of an RDBMS.\n\nIntroducing a caching layer, for example by adding CDN usage to a publicly exposed service, or by adding Memcached or in-memory caching between the service application code and the data persistence layer, is another adjustment that is often introduced after the initial launch of a microservices application, and it can be applied selectively to a subset of the services an organization has deployed.\n\nAdvantages of Using Different Languages and Technologies Considering the fact that software engineering as a discipline is less than seven decades old, the number of options for technological choices any software developer faces in today's industry landscape is simply staggering.\n\nLet's look at some of the resulting advantages of such advancements in technology diversity, and see\n\nwhy a business would want to operate in a polyglot technology environment.\n\n177",
      "content_length": 2560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Chapter 11 ■ polyglot ServiCeS\n\nSpecialization When looking at the various layers of systems that make up the entirety of a modern online application, you will find a large number of different languages and technologies in use.\n\nLooking at the mobile devices landscape, Apple's iOS and Google's Android deliver very specialized\n\noperating systems, programming languages, and API frameworks, as well as surrounding tools to build native applications. The main goal of this specialization is to enable developers to use the limited resource of the devices optimally.\n\nSpecializing in simple use cases of key-based access, key-value stores can be optimized for much more\n\nefficient access to persisted data than any SQL-based database system.\n\nThe lingua franca of dynamic web-based clients is JavaScript, mostly because all major browsers support it. But still, even within the JavaScript ecosystem, several different frameworks have been able to establish themselves, further specializing for particular use cases. Examples are:\n\nGoogle-sponsored Angular.js (https://angularjs.org), which is particularly popular for building single-page web applications because it offers synchronized two-way data binding, and alleviates much of the hassle of writing code for DOM manipulation.\n\n\n\nFacebook's React.js (https://facebook.github.io/react/), which shines when it comes to highly dynamic web applications, where the concept of working on a virtual DOM results in client-side performance gains.\n\n\n\nYehuda Katz' MVC framework Ember.js (http://emberjs.com), which aims to combine the two-way data-binding and server-side DOM rendering advantages of the previous two frameworks.\n\nThe main theme in all of these specializations is optimizing for the requirements of new use cases.\n\nThe fact that all these technologies exist and continue to thrive is a testament to the fact that they serve a purpose, and any business that wants to build a successful online application is effectively forced into being polyglot already.\n\nBy the same token, and as an extension to this point, applying diversity to the languages and frameworks with which the service applications are being built in the back-end can also help with optimization at this layer.\n\nExperimentation Progress in the computer software and hardware space is still very fast-paced. And even if skeptics are correct that the applicability of Moore's law about transistor density in integrated circuits is about to cease, software system will still see orders of magnitude improvements. Google's company culture asks their employees to apply \"10x thinking\" when thinking about innovation. Their rationale is:\n\n[T]rue innovation happens when you try to improve something by 10 times rather than by 10%.1\n\nBoth the development and the use of such quantum leap advances require a great deal of willingness to experiment. Therefore, curiosity remains a key competitive advantage for any software developer, as well as for the businesses that employ software engineers; in our experience, it is usually the most talented people who will be motivated by opportunities to learn new technologies.\n\n1“Creating a Culture of Innovation: Eight ideas that work at Google”, Google for work, https://apps.google.com/ learn-more/creating_a_culture_of_innovation.html, as of September12, 2016\n\n178",
      "content_length": 3326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Chapter 11 ■ polyglot ServiCeS\n\nThe good news is that implementing a microservices architecture provides a business with many benefits for experimentation. Because the units of deployment are small applications with very limited concerns, experimentation can happen with a very low impact on the overall business.\n\nEvolution Businesses can only achieve longevity when they learn how to adapt to an ever-changing environment, and staying agile by quickly adjusting or reversing course is key. Often the result of such environmental changes is that the business needs shift toward lesser, or sometimes even entirely new, use cases.\n\nAs an example, we have been part of companies where the initial system was not designed to cater to sophisticated, end user triggered product searches, often including search criteria that are dependent on geolocation data gathered about the customer. This can often be best addressed by researching and introducing specialized technology, such as Apache Lucene (https://lucene.apache.org) and Solr (https://lucene.apache.org/solr), or Elasticsearch (https://www.elastic.co/products/ elasticsearch) for the search use cases.\n\nIt is also not uncommon that companies become the victim of their own success, having to serve a much larger number of online visitors than initially envisioned. In such a case, evolving the core services of your infrastructure to a language or framework that makes better use of CPU and memory resources, or that can more easily deal with a higher-level concurrency, can often help in addressing the bottleneck before it turns into a case of subpar user experience or bad press.\n\nNavigating the Pitfalls of Being Polyglot Going from an environment where all your services are using the same technology stack to embracing a more diverse set of tools surely also has some inherent risks that need addressing.\n\nFirst, you will need to think about the development cycle. Your software engineers will need some time\n\nto familiarize themselves with the new language or any other technology that is being investigated. This is not simply restricted to learning the syntax of an unfamiliar language, but also extends to decisions about libraries, frameworks for automated testing or web request handling, and how to automatically install the development environment on an engineer's work computer.\n\nUsually, these bootstrapping decisions are most efficiently made by a very small group of engineers that\n\nare passionate to learn about the technology in question. Their findings can then be spread among the rest of the service-owning team in short “brown bag lunch” tech talk sessions.\n\nSimilar issues can be encountered in the area of deploying new technologies, especially when you have to work in an environment where host machines are shared between services, and the new technology could potentially impact deployment strategies of existing services.\n\nThe cleanest option to address such concerns is generally to avoid co-deploying more than one system, for example, using application containers.2 Otherwise, we recommend pairing up an application developer with an operations engineer to investigate the cleanest way to deploy the new tools alongside existing technology. That way, initial deployment guidelines for the new language stack, or the like, benefit from input of a larger set of engineering functions in the business.\n\nOnce deployment issues are addressed, it is paramount not to forget the requirements for operational\n\nsupport for running new machinery in a production environment. As mentioned for the deployment aspects, having dedicated servers or containers per service will make this task easier, as the newcomer will not be competing for resources with existing services.\n\n2See Chapter 10 for an in-depth discussion of options for deploying and running microservices.\n\n179",
      "content_length": 3848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "Chapter 11 ■ polyglot ServiCeS\n\nAnother thing that could help during the deployment and operational support research steps is choosing a technology that is significantly similar to other standards you have already chosen. If, for example, your company already supports JVM applications via running JRuby, and you are trying to introduce a new language that is also JVM-based (such as Clojure), then you might find that you will have to address fewer open issues when introducing this new technology.\n\nIt is very important to involve all parties that have a stake in supporting the new technology in production in the process of deciding how to initially launch the new tech component in production. If your organization is following the “you built it, you run it” setup where application developer teams entirely support the software they own in all production aspects, then fewer departments need to be part of this decision process. But if you are in the fortunate position to work with a technical operations support team, make sure you involve them in these discussions.\n\nConsidering the tail end of a technology's usage life cycle, you should also think about what it will take\n\nto retire any system you introduce. For one, this applies on the one hand to technologies you experiment with and find that they do not add enough value to justify the effort of maintaining them. But more generally this also applies to the technology that is superseded by its successfully tested replacement. Make sure to have follow-through on such technology retirements and replacements, or you will run the risk of having too many technologies to support.3\n\nA Process for Adopting New Technologies When adding new technologies to their stack, companies are well advised to come up with a written policy concerning the selection process. The advantage of this lies in the fact that decision making can be based on a published, step-by-step process with clear milestones and measurable criteria for moving forward.\n\nThis section lists a few questions to answer during the selection, implementation, and production\n\nlaunch steps.\n\nDocument All Findings We have seen all too often that decisions about adopting or rejecting a certain language or technology tool are made without documenting the results on which the rationale for those decisions is based. The drawbacks of such lack of documentation are at least twofold.\n\nFor one, there is a potential for wasted time and effort of a second internal team, which might have started its own investigation process for the same technology, although they could have taken advantage of the results of the previous investigations that were unknown to them.\n\nSecond, the business or technological environment in which the company operates might change\n\nover time, and those circumstances might invalidate, or at least significantly affect, the findings of a prior investigation of a given technology. If there is no detailed documentation about past technology trials, then the company might miss out on the competitive advantage a previously rejected tool might bring them under the new conditions.\n\nStatement of Motivation and Goals When documenting your technology trials, we recommend starting out with a section that describes the motivation for the new language or toolset.\n\nOften such sections will list areas where the current tech stack is known to be lacking, or particular\n\naspects that need to be considered with regard to a new technology (for example, maturity, safety, community and tool support, or performance).\n\n3See notes on a process for evaluating a new technology later in this chapter.\n\n180",
      "content_length": 3641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "Chapter 11 ■ polyglot ServiCeS\n\nAt other times the goal of the experiment might not necessarily be directed at a well-established and recognized problem. The motivation might simply be that the general tech community is praising a new tool or language that can solve problems for which there was no prior awareness inside your company, or where improvements can help the company to be more successful.\n\nA reason for investigating a new language might be that the same language could be used in several\n\nlayers of your stack, thereby reducing the general skill set required from your developers. We have seen this argument used when companies decide to introduce Node.js (https://nodejs.org/en/) to write their back-end services in JavaScript when their browser-based logic is already coded in this language. Similarly, we often hear the argument for adopting ClojureScript (http://clojurescript.org/) in the front-end from companies that are invested in Clojure as their service-side language.\n\nAnother very prominent reason for testing a new technology is the attempt to speed up the\n\nperformance of business-critical areas. We know of companies that have discovered that full-text search in their product database became an increasingly important feature for their growing customer base, and investigations needed to be made to replace the initially MySQL-backed search functionality because of performance issues. Similar arguments sometimes lead companies to experiment with migrating a service implementation from one programming language to another one (for example, moving from Ruby to Rust, Go, Elixir, or the like).\n\nIn attempts to avoid being a victim of their own success, many companies realize later on in their existence that the technology they chose does not scale well under load, so they will need to research switching to technologies that behave more resource-efficiently under high load.\n\nDo you see the costs of maintaining a particular technology you use skyrocket? Then it might be time to\n\nresearch more stable or more easily extensible alternatives.\n\nIf your company has a hard time attracting talented engineers or keeping your most valuable tech personnel engaged and excited, then experimenting with new and exciting technologies to counteract attrition or to increase the recruiting success is surely also a reasonable motivation for experimentation.\n\nWhatever turns out to be the reason behind your research, make sure to explicitly state the areas you\n\nwill cover in your experimentation, and list the expected outcomes and metrics associated with judging the success of the experiment.\n\nImplementation Planning Once the groundwork is laid with the description of the goals of the research, the first milestone to pass is to get an official go-ahead from the company's management that the experiment can proceed, and that the metrics and acceptance criteria are sound. We highly recommend getting such buy-in, and officially setting aside time for team members to work on such \"new tech\" projects. In our experience, these projects will not happen unless they appear on a formal roadmap for company work.\n\nNow it is time for planning the execution. It is always a good idea to publicize the plan to test a new\n\ntechnology internally to the broadest possible group of people.\n\nThis can not only help solicit feedback on the strategy and goals, but also aid in finding support and\n\nlike-minded co-workers who are interested in pairing on the initial implementation.\n\nMake sure to measure and record all data that you laid out in your statement of goals as you go about implementing the new technology in your organization. This documentation will help you and a potential gatekeeper decide if the experiment results meet the acceptance criteria to take things to the next step. And even if the outcome of that analysis is such that a go-ahead cannot be recommended, it is very useful to document just this fact and preserve this knowledge within your organization.\n\nIn an ideal scenario, most of this implementation can be performed in a sandbox of staging\n\nenvironment, although we realize that true results—especially where performance is concerned—will only be achieved once the new technology is exposed to production traffic.\n\n181",
      "content_length": 4265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Chapter 11 ■ polyglot ServiCeS\n\nGo-Live Criteria The most important point before adding any new technology to production is to clearly understand the potential impact and failures modes. Hopefully you have had a chance to discover some of these scenarios during a pre-production test in a staging environment.\n\nIf you are replacing existing functionality like for like, such that the new technology has to support all the current use cases, then a great help in verifying that the new service behaves as expected will be a suite of black-box tests4 that you run against both the old and the new system.\n\nTo help mitigate the risk involved in such projects you should seriously consider adopting deployment\n\nstrategies that allow for a fast and easy rollback mechanism (such as blue-green deployments or canary releases, which were explained in the previous chapter).\n\nAspects that you can never neglect when going to production are logging, monitoring, alarming, and data collection. If you cannot analyze a technology's status messaging, then you will never know if it is in failure mode or what particular problems it might encounter in case of an outage. And if your business prides itself on being data driven, then integrating the newcomer into the data warehouse layer must be a first priority.\n\nIt is generally a good guideline to keep the number of languages and other technology options in production as small as possible, but as large as necessary. This reduces maintenance costs and cognitive load for the tech personnel while at the same time providing an environment for staying ahead of the curve and agile.\n\nIt is never a good idea to use a technology that only one or two technical people in the company are sufficiently familiar with to support it in a production setting. This means that you should neither adopt a technology where only a small number of developers understand the system, nor migrate away from a technology after there is too little know-how left to grasp what it takes to deprecate it.\n\nOne way to avoid this is to focus on hiring polyglot and well-rounded engineers who make a habit of\n\nlearning the company's infrastructure. Make sure that a significant amount of time during the on-boarding process for new team members is spent learning the ins and outs of all technologies that are in use by your team.\n\nA Case Study: Moving books-service from Rails to Rust When looking at our example book store business that is running in a microservices environment, we discovered that our most trafficked service is the books-service, which contains all the core product data about the books for sale on the site.\n\nThe service has been implemented using the rapid application development framework Ruby on Rails,\n\nmostly because the developers were most familiar with this technology, and because it is very convenient and easy to implement a JSON API service in it.\n\nDuring day-to-day operations the team owning books-service notices that response times start to become affected during peak business times when the largest number of customers browse the product catalog. While there is no immediate urgency to address the issue—that is, no decrease in purchase rates can be correlated to these times of heavy load—the development team is starting to investigate the root cause of the scaling issue.\n\nAnalysis of the logs and New Relic APM5 data for books-service shows that much time is spent in garbage collection of Ruby objects, and—even though the service was implemented as a Rails API-only6 project—lots of framework code is logged as being busy handling the request and response objects outside the actual application logic.\n\n4Chapter 9 has more details about such tests, and how to craft them. 5This is an application-performance monitoring tool. See https://newrelic.com/application-monitoring. 6A variant of a standard Ruby on Rails application that tailors the middleware components used to the minimum need for API services. See the respective Rails guide at http://edgeguides.rubyonrails.org/api_app.html for more details.\n\n182",
      "content_length": 4070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Chapter 11 ■ polyglot ServiCeS\n\nAlice, one of the senior engineers on the books-service team, suggests starting to investigate alternatives, and the team agrees to take over part of her current responsibilities so that she has time to dedicate to this effort.\n\nThus tasked, she starts out by writing down a problem statement, including all data gathered about the\n\nnature of books-service's perceived scaling issues.\n\nAfter doing some research about more resource-efficient languages and less impactful web frameworks,\n\nshe comes across the Mozilla Research-sponsored language Rust (https://www.rust-lang.org/), which seems very promising for addressing the perceived performance challenges. Additionally, the language seems to raise a lot of interest in the community, and many of the great engineers in her professional network have started to gravitate toward Rust.\n\nAlice formulates a goal statement of the experimentation to include concrete and measurable gains in\n\nperformance and reduction in computing resource usage, compared to the current production-deployed version of books-service. She also includes specifics about the expected advantages of using the Rust language for the experiment, quoting some very rudimentary benchmarks she has gathered from credible sources, as well as her own small experiments. As the company is hiring and therefore in competition with many other employers for experienced, high-quality engineers, she also lists the increase in attractiveness to potential engineering candidates caused by using a cutting-edge technology like Rust. Her metric to test this hypothesis is to compare feedback from the engineering team's combined professional network, so she plans to ask all teams to survey their contacts about an increase in interest in a company that uses Rust in its technology stack.\n\nThe books-service team meets to discuss Alice's proposed experiment based on the statement of motivation and goals, and unanimous agreement is given to go ahead with the next phase of the experiment. This means that time is officially dedicated in the next two team sprints to work on the first stage of the implementation plan. Alice will pair with Peter, another engineer on her team, on this project, simply to make sure that knowledge is not restricted to a single person, and they agree to schedule regular “brownbag lunch” sessions where they share updates with the rest of the team.\n\nAlice and Peter are also asked to work to identify a contact on the company's technical operations (TechOps) team, in order to share knowledge about potential deployment or operational concerns, as well as solicit suggestions they might have for running Rust in the existing infrastructure.\n\nAfter learning the basics of the Rust language and identifying potential API frameworks in the language,\n\nAlice and Peter decide to use Iron (http://ironframework.io/)—a very lightweight and extensible web framework for Rust. They experimented with several other Rust frameworks, but found that they introduced more latency than Iron, and as performance is one of the main objectives of the experiment, they decide to stick with Iron.\n\nWhile reimplementing all the features of the existing Ruby version of books-service in Rust, they can\n\nrely on an existing, out-of-process black-box test suite, written in Ruby using the Cucumber DSL. Once they get to the point of feature equivalence between the Ruby and Rust versions, they can finally measure response times and resource usage for the Rust service under a simulated large load in the company's staging network environment.\n\nInitial results are very promising, and Alice and Peter write up a page on their internal company Wiki which summarizes the results. They present the results first to their own team, which is impressed with the observed 10x response time improvements in the staging environment.\n\nAn official vote is taken and recorded to go ahead and spend time investigating and addressing all the\n\nnecessary steps that are involved with a “go-live” decision in production. These steps include more training on the technologies in use for developers and TechOps engineers, as well as making sure that the team has visibility into the new service's production behavior by integrating it with the logging and alarming infrastructure. Data warehouse integration of the metrics gathered for business intelligence queries also needs to be addressed before the Rust service can go live.\n\nOnce integrating Rust into the company's canary deployment process is addressed by the TechOps department, the team can finally observe the new service under actual production load, and compare it side by side with the existing Ruby version of the service.\n\n183",
      "content_length": 4723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "Chapter 11 ■ polyglot ServiCeS\n\nSummary This chapter discussed the implications of being technologically polyglot. After explaining the vectors of change in a microservices architecture where technological diversity can and should be tried, we looked at the opportunities in adopting several competing technologies.\n\nWe identified the main potential advantages to be the benefits of specialization, progress achieved\n\nby experimentation, and remaining agile by setting yourself up to evolve more quickly with your business requirements.\n\nNext we looked at how to navigate around the potential pitfalls of working in a polyglot technology environment, before shifting our focus to describing a viable process for experimentation with, and adoption of, a new technology in a company's tech stack.\n\nWe concluded the chapter with a fictitious case study about replacing a Ruby-on-Rails based microservice in our example bookstore with an equivalent service and APIs implemented in Rust.\n\nThe next chapter will go into much more detail about the implementation of the microservices\n\narchitecture that makes up the sample book store application.\n\n184",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "CHAPTER 12\n\nSingle-Client Application—Mobile and Web\n\nYour microservice-powered application will ultimately be displayed to actual people using a web browser or mobile devices; we call those customer facing applications. In order to function well, those applications need to make calling each service they depend on a trivial task. We know that many developers have been exposed only to the ideas and concepts behind the microservices architecture, and still have very little actual exposure to how all the pieces fit together in practice.\n\nIn this chapter, we share our book sample project; it has working code that resembles a real-life scenario, so that you can run all the services in this project and see a web application running. Our project implementation is far from complete, as that's not the goal of this book; we only have enough pieces in place to show how everything connects. We will focus on explaining the main building blocks of a few pages of the sample project, and explain the components that make each part of some pages work. We will show code in Ruby and Rust, which are the two languages we are comfortable with and can fairly easily use to showcase interaction between a web application and a few services. Even if you are not familiar with those languages, we recommend you follow along with this chapter as it will put into practice a lot of the concepts we have described in the book so far.\n\nApplication Overview Before we get started, take a look at the architecture of what we are building in this chapter. We'll be implementing much of the application example described throughout the book in the following services:\n\n\n\nshowcase is a client-facing web application, built in Ruby on Rails, that will be receiving requests from customers. The source code is available at https://github. com/microservices-from-day-one/showcase.\n\n\n\nbooks-service is a service responsible for owning all data associated with books, built in Rust. The source code is available at https://github.com/microservices- from-day-one/books.\n\n\n\npurchases-service is a service responsible for owning all data associated with shopping carts and purchases, built in Ruby. The source code is available at https://github.com/microservices-from-day-one/purchases.\n\n\n\nreviews-service is a service responsible for owning all data associated with book reviews, built in Ruby. The source code is available at https://github.com/ microservices-from-day-one/reviews.\n\n185",
      "content_length": 2463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nThe diagram in Figure 12-1 makes it easy to visualize that our customers will be using their browsers to point to the web server that is running the showcase app, which will then make requests to books-service, purchases-service and reviews-service in order to render web pages to our users.\n\nFigure 12-1. Architecture of the sample project\n\n■ Note service has a readme file with details on how to set up each application, and we also provide a repository with more documentation at https://github.com/microservices-from-day-one/docs.\n\nYou can—and should—follow along by loading the code available from the Urls shown. each\n\nIt is time to put all that we've talked about in this book together, and demonstrate a working web application in a microservices environment. The sample application for this book is a bookstore, which has a set of fairly standard features that we will describe and dissect in this chapter. We'll talk about each of the application's pages, and will explain all service calls that happen in order to get each page rendered. The sample application is not fully functional, as it would probably require a team of full-time developers to implement and maintain such an application; however, we decided to implement enough functionality that will showcase a lot of the topics covered in this book.\n\nHome Page The home page for the bookstore is a fairly standard e-commerce landing page, as you can see in Figure 12-2.\n\n186",
      "content_length": 1498,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nFigure 12-2. Home page of our bookstore\n\nBefore we go into the details of a few elements of the home page, we want to bring to your attention the client library we are using to connect to our services. You probably remember from Chapters 5 and 6 that we decided to use the JSON API standard to describe all our responses, which would keep our services interfaces and responses similar across the whole system. We build a client library in Ruby—known as a Ruby gem—to interface with our services. The payoff for using a standard starts to appear, as we were able to build our own library on top of an open source JSON API compliant library called json_api_client (https://github.com/chingor13/json_api_client).\n\n187",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nIn our client library, named api_client and available at https://github.com/microservices-from-\n\nday-one/api-client, you can see that we have defined one module per service, so we'll have a module called ApiClient::Books for books-service, another called ApiClient::Purchases for purchases- service, and so on. Each module is very simple and leverages the json_api_client gem; they also define a Base class, which is responsible for having code that will be shared by all models. The Base class for ApiClient::Books is listed here:\n\nmodule ApiClient module Books\n\nclass Base < JsonApiClient::Resource def self.site @site ||= \"#{ServiceDiscovery.url_for(:books)}/v1\" end end\n\nend end\n\nrequire 'api_client/books/book' require 'api_client/books/category'\n\nThat class is responsible for mainly two things, defining where the endpoint for books-service is to\n\nbe found, as implemented in the site class method, and defining a base resource, which inherits from JsonApiClient::Resource; therefore all the subclasses of this class don't really need to know that detail of the implementation.\n\nWith the Base class in the client library explained, we'll now show how a specific resource is defined. Our example is the ApiClient::Books::Book class, which represents the book model as served by books- service; the class is defined here:\n\nmodule ApiClient module Books\n\nclass Book < Base end\n\nend end\n\nAs you can see, there is really no code associated with this class; we are able to define a standard resource, which has a set of well-known method available that match to a RESTful set of CRUD endpoints— again, one of the advantages of following a standard. So our client library is, as we expected, a very thin layer that allows client applications to reach the service endpoints we wrote.\n\nNow it's time to look at how some of the elements of this page have come together. An element that is a constant feature in most e-commerce applications is our shopping cart widget, which shows us the current value of the cart, as well as the number of elements in it. All details related to purchases are owned by purchases-service, which also owns all cart details. When any user accesses the bookstore, this service tries to find a cart associated with the user, which is saved in the session:\n\ndef current_cart_id if session[:cart_id].nil?\n\n188",
      "content_length": 2387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\ncart = ApiClient::Purchases::Cart.create session[:cart_id] = cart.cart_id end session[:cart_id] end helper_method :current_cart_id\n\nAs you can see in this code, the application will load the card_id associated with the user from the session. For new users that do not have an associated cart_id, a new cart will be generated in purchases- service via the ApiClient::Purchases::Cart.create call. Again, we will take a look at the implementation in purchases-service, available at https://github.com/microservices-from-day-one/purchases, which is a Ruby service. In Ruby, we have an open-source gem called jsonapi-resources (https://github.com/ cerebris/jsonapi-resources), which is described as “a resource-focused Rails library for developing JSON API compliant servers.” By following the JSON API standard, and defining a set of resources that define the models to be exposed in the API, this gem makes it very easy to create JSON API-complaint services. In purchases-service, the Cart resource is defined as follows:\n\nmodule V1 class CartResource < JSONAPI::Resource attributes :cart_id, :expires_at, :total, :items_count end end\n\nThe cart’s endpoints are defined in the V1::CartsController class, defined as:\n\nnamespace :v1 do jsonapi_resources :carts end\n\nin routes.rb and implemented as:\n\nmodule V1 class CartsController < JSONAPI::ResourceController end end\n\nThat is all that we need in order to render the shopping cart widget. It currently shows the number of items in the cart and its value as 0; however, that will change when we implement the endpoints and code to add books to the cart.\n\nNow it's time to look into how some of the elements of this page have come together. The main block in\n\nthe home page displays four featured books that are presented to the user. We will start with a very simple implementation, which is a service call that will return the six most recent books in books-service. The code that makes that service call is\n\ndef home_products(limit: 8) @home_products ||= ApiClient::Books::Book.paginate(page: 1, per_page: limit).to_a end\n\nWhen this code is executed, the following request comes to books-service:\n\nGET http://localhost:5000/v1/books?page%5Bnumber%5D=1&page%5Bsize%5D=6`\n\n189",
      "content_length": 2276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nIt returns\n\n{ \"data\": [ { \"attributes\": { \"author\": \"Mo Willems\", \"cover_image\": \"https://images-na.ssl-images-amazon.com/images/I/61jurA6wsFL._SX258_ BO1,204,203,200_.jpg\", \"description\": \"Diva, a small yet brave dog, and Flea, a curious streetwise cat, develop an unexpected friendship in this unforgettable tale of discovery.\", \"isbn\": \"978-1484722848\", \"pages\": 80, \"price\": 987, \"slug\": \"the-story-of-diva-and-flea\", \"title\": \"The Story of Diva and Flea\" }, \"id\": \"2497fe7d-4797-4506-a3cd-85e42d90415b\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Charles C. Mann\", \"cover_image\": \"https://upload.wikimedia.org/wikipedia/en/b/b7/1491-cover.jpg\", \"description\": \"In this groundbreaking work of science, history, and archaeology, Charles C. Mann radically alters our understanding of the Americas before the arrival of Columbus in 1492.\", \"isbn\": \"978-1400032051\", \"pages\": 541, \"price\": 1299, \"slug\": \"1491-new-revelations\", \"title\": \"1491: New Revelations of the Americas Before Columbus\" }, \"id\": \"2d831aba-75bb-48b6-bae6-9f1bb613d29b\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Jane Austen\", \"cover_image\": \"https://themodernmanuscript.files.wordpress.com/2013/01/pride-and- prejudice-1946.jpg\", \"description\": \"Pride and Prejudice is a novel of manners by Jane Austen, first published in 1813. The story follows the main character, Elizabeth Bennet, as she deals with issues of manners, upbringing, morality and education.\", \"isbn\": \"978-1484110980\", \"pages\": 279, \"price\": 951, \"slug\": \"pride-and-prejudice\", \"title\": \"Pride and Prejudice\" },\n\n190",
      "content_length": 1621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\n\"id\": \"12892302-7ef4-4023-9999-2b3560a2a4d6\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"James Dean \", \"cover_image\": \"https://i.harperapps.com/covers/9780062303899/x300.png\", \"description\": \"Pete the Cat is going scuba diving! Before he hits the water,\n\nCaptain Joe tells him about all the sea creatures he can encounter, and Pete is super excited to see a seahorse.\",\n\n\"isbn\": \"978-0062303882\", \"price\": 319, \"slug\": \"pete-the-cat-scuba-cat\", \"title\": \"Pete the Cat: Scuba-Cat\" }, \"id\": \"8217e21a-97db-4be9-9feb-4cea97a64948\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Eric Litwin\", \"cover_image\": \"https://i.harperapps.com/covers/9780061906220/x300.png\", \"description\": \"Pete the Cat goes walking down the street wearing his brand new\n\nwhite shoes. Along the way, his shoes change from white to red to blue to brown to WET as he steps in piles of strawberries, blueberries, and other big messes.\",\n\n\"isbn\": \"978-0061906237\", \"pages\": 40, \"price\": 1139, \"slug\": \"pete-the-cat-i-love-my-white-shoes\", \"title\": \"Pete the Cat: I Love My White Shoes\" }, \"id\": \"008672a6-c48c-49b2-98e8-b78914f2fce4\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Eric Litwin\", \"cover_image\": \"https://i.harperapps.com/covers/9780062110589/x300.png\", \"description\": \"Pete the Cat is wearing his favorite shirt—the one with the four totally groovy buttons. But when one falls off, does Pete cry? Goodness, no! He just keeps on singing his song—after all, what could be groovier than three groovy buttons?\",\n\n\"isbn\": \"978-0062110589\", \"pages\": 40, \"price\": 1175, \"slug\": \"pete-the-cat-and-his-four-groovy-buttons\", \"title\": \"Pete the Cat and His Four Groovy Buttons\" }, \"id\": \"20ee1ef4-6000-460a-b715-40da50979fc7\", \"type\": \"books\" },\n\n191",
      "content_length": 1785,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\n{ \"attributes\": { \"author\": \"James Dean\", \"cover_image\": \"https://i.harperapps.com/covers/9780062304186/x300.png\", \"description\": \"Pete the Cat takes on the classic favorite children's song \\\"Five Little Pumpkins\\\" in New York Times bestselling author James Dean's Pete the Cat: Five Little Pumpkins. Join Pete as he rocks out to this cool adaptation of the classic Halloween song!\", \"isbn\": \"978-0062304186\", \"pages\": 32, \"price\": 791, \"slug\": \"pete-the-cat-five-little-pumpkins\", \"title\": \"Pete the Cat: Five Little Pumpkins\" }, \"id\": \"25f2db6f-966a-4c24-9047-1fdfeb43e465\", \"type\": \"books\" } ] } ```\n\nWith those products retrieved, the code in the template that renders the page will pick four random\n\nbooks out of those six, so that users see different books in subsequent requests:\n\n``` <% for product in home_products(limit: 6).shuffle.first(4) do %> <%= render partial: 'home/product', locals: {product: product} %> <% end %>\n\nWe’ve started with a very simple service call; however, as the site develops a wider range of products\n\nand starts to collect user behavior data, this is an area that could benefit from the existence of user recommendations, which could be implemented in a recommendations service, and be based on an algorithm that takes into consideration the user's purchase habits, along with the purchase habits of similar users, to maximize sales opportunities.\n\nFeatured Categories In the second half of the home page, we have the Featured Categories block, which is a section used to feature a couple of categories the business is interested in focusing on. In the showcase application, we make a call to the featured_categories endpoint in books-service, available at /v1/categories/featured, which is responsible for knowing the logic behind deciding how a category gets featured; in the browser, that call looks like Figure 12-3.\n\n192",
      "content_length": 1917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nFigure 12-3. Featured categories block in home page\n\ndef featured_categories @featured_categories ||= ApiClient::Books::Category.featured.first(2) end\n\nThe Category resource in the service returns a JSON API-compliant resource, with a response that looks\n\nlike this:\n\n{ \"data\": [ { \"attributes\": { \"description\": \"Something for the little ones.\", \"image\": \"http://sleepingshouldbeeasy.com/wp-content/uploads/2013/01/childrens- books-about-going-to-school-vertical.jpg\", \"name\": \"Childrens Books\", \"slug\": \"children-s-books\" }, \"id\": \"20a05be7-3a9b-4ed0-a36b-ed36b0504557\", \"type\": \"categories\" }, { \"attributes\": { \"description\": \"Learn about your past.\", \"image\": \"http://media1.s-nbcnews.com/ij.aspx?404;http://sys06-media.s-nbcnews. com:80/j/streams/2013/November/131120/2D9747505-Where_Were_You_book_jacket.blocks_desktop_ vertical_tease.jpg\", \"name\": \"History\", \"slug\": \"history\" }, \"id\": \"cc7f1963-23fc-485c-89ec-d1efb25966d1\", \"type\": \"categories\" },\n\n193",
      "content_length": 1017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\n{ \"attributes\": { \"description\": \"It is promise and hope. Titillations and excitations.\", \"image\": \"https://s-media-cache-ak0.pinimg.com/564x/29/bd/9e/29bd9eafd49a185874e706 fb4f896ba0.jpg\", \"name\": \"Romance\", \"slug\": \"romance\" }, \"id\": \"c85102c8-3ed2-4af9-be8b-f1008448f6cb\", \"type\": \"categories\" } ] }\n\nIn the service code, the featured endpoint has been implemented as a response that returns categories\n\nin random order, so that all categories are featured equally. Of course, in a production setup, we'd want to implement featured_categories based on business logic, probably based on featuring-best selling categories. In the described architecture, changes to the algorithm of how categories become featured are just an implementation detail of books-service and will have zero impact on the rest of the system, given that the API interface remains unchanged. Here's the current implementation of the featured_categories endpoint, implemented in Rust, and as we discussed in Chapter 11.\n\npub fn featured(req: &mut Request) -> IronResult<Response> { let pool = req.get::<Read<::AppDb>>().unwrap(); let conn = pool.get().unwrap();\n\nlet mut categories: Vec<Category> = Vec::new(); for row in &conn.query(\"SELECT category_id, name, description, slug, image FROM\n\ncategories ORDER BY random()\", &[]).unwrap() {\n\nlet category = Category::from_row(row); categories.push(category); }\n\nlet response = jsonapi::json_collection_wrapped_in(\"data\", categories); Ok(Response::with((status::Ok, Header(jsonapi::jsonapi_content_type()), response))) }\n\nOne last thing to mention about this element is that it is probably the most “cacheable” element of the\n\nhome page, meaning that it's very unlikely we will want featured categories to change frequently throughout the day; thus, we can use client-side caching to cache the featured categories block for a specific period of time (usually to be decided by the business), and not even have to make the service call on most requests, which also makes the application more performant and resilient to service failures. In this case, caching the response for 4 hours seems acceptable; so we could surround the client-side code with these lines:\n\n<% cache 'featured_categories', expires_in: 4.hours do %> <%= render partial: 'home/offers' %> <% end %>\n\n194",
      "content_length": 2347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nUsers Should Never See Internal Errors Now that you have seen some code, and we have the home page being rendered by loading data from a few services, we'll look into how we want to treat service errors in this user-facing application. We believe strongly that users should never see server errors; developers and designers should think about the user experience to be presented to the user when there's something wrong with the application. Based on the particular use case, treating errors will take a unique shape, as there are cases that you can easily recover from, and others not so much.\n\nThe rule of thumb is that you need to catch errors that happen when interacting with the server, give your users a fall-back, and try to degrade gracefully in all areas; but degrading means different things based on the situation. In some cases, you can make it look like there is no error at all, and in others, you still have to display a “something went wrong” message, which is still more user-friendly than a 500 server error.\n\nWe will start by looking at the shopping cart widget, which we described a few pages ago, and will see\n\nwhat happens to the application when that service is not available—down. To simulate purchases-service being down, we can just Ctrl-C that process and wait until we are sure it's no longer running. After doing that, just refresh your browser, and you will see that the home page greets you with an ugly ConnectionError error (see Figure 12-4).\n\nFigure 12-4. Error when loading the home page when dependencies are not available\n\n195",
      "content_length": 1619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nThe shopping cart widget gives us an insight into what is currently in the cart, and also works as a link to\n\nthe cart page. In this case, we could treat that error to make sure that we still render the widget, without the number of items in the cart nor the current cost. That fall-back seems to be acceptable, since it should only happen during down times of that one service. We've changed the helper method that loads the shopping cart object by adding error-handling code, and we also made sure we logged the error that is happening and left a TODO comment inline to alert us that something is wrong. In Chapter 14, we talk about some options for alerting this type of issue. The code ends by returning a nil object:\n\ndef shopping_cart @shopping_cart ||= ApiClient::Purchases::Cart.find(current_cart_id).first rescue JsonApiClient::Errors::ConnectionError => e Rails.logger.error(\"Error loading shopping_cart: e=#{e.message}\") # TODO: alert that an error is happening nil end\n\nWe can check for a nil object in the template that renders the shopping cart information:\n\n<% if shopping_cart %> <div class=\"total\"> <span class=\"simpleCart_total\"><%= render_price(shopping_cart.total) %></span> (<span id=\"quantity\" class=\"quantity\"><%= shopping_cart.items_count %></span>) </div> <% end %>\n\nFigures 12-5 and 12-6 are screenshots of the shopping cart widget first when it's working as expected\n\nand then under the error case, which may not even be noticeable to the user.\n\nFigure 12-5. Shopping cart widget in functional state\n\nFigure 12-6. Shopping cart widget when purchases-service is down\n\n196",
      "content_length": 1652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nWe demonstrated an error case that had a simple solution that didn't impact the page at lot; we could easily degrade gracefully. However, there are cases where that is not possible. For example, if the user were in the book detail page, and the books-service happened to be nonresponsive, you'd have to return a page with an error message that asks the user to “try again in a few minutes.”\n\nCircuit Breakers In most cases, the type of error handling code we’ve just explained is enough to help one application handle failure from a downstream dependency; however, in environments with a very high number of requests, it's possible that having a client-facing application keep making requests to a service when it knows it's down isn't a good solution, for two reasons:\n\n\n\nThe calling application will be slightly delayed, as it may have to wait for its timeout to realize that a dependency is unavailable, which may slow other systems.\n\n\n\nThe struggling server takes longer to respond, or come back to life, if it keeps receiving requests that it cannot handle, compounding the existing problem.\n\nFor that type of situation, the calling application should use the circuit breaker pattern we introduced\n\nin Chapter 6. It is a mechanism that will make the calling application try to request a resource from a service even when it notices that the service has not been responsive. Circuit breakers are usually set up by defining a failure threshold in the calling application that when met will cause the circuit to “open” for a predetermined period of time, causing the calling application to go into error handling mode without even calling the downstream dependency, because it already knows that it has been struggling to properly serve requests. This spares us from the two unwanted scenarios we described earlier and makes a system even more resilient in the face of an abnormal situation.\n\nBook Detail Page The next page we will look at is the book detail page, shown in Figure 12-7, which is probably the most important page in the application; it's where users will make a purchase decision. Here is some of the information a user would expect to see on the book page:\n\n\n\nbibliographic information such as title, author, cover image, description, ISBN, price, page count and publisher name\n\n\n\nreviews of books\n\n\n\nother people's review,\n\n\n\nability to review yourself\n\n\n\nadd to a wish list\n\n197",
      "content_length": 2454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nFigure 12-7. Screenshot of book detail page\n\nThe initial set of attributes we need to load are what we could call book metadata, and are owned by books-service; the client helper will load data from that service, by using the book “slug,” which is a unique SEO-friendly string used in the book URL, such as http://localhost:5004/products/pete-the-cat-i- love-my-white-shoes.\n\ndef book_details @book ||= ApiClient::Books::Book.where(slug: params[:id]).first end\n\nThe code above will make this service call:\n\nGET http://localhost:5000/v1/books?filter%5Bslug%5D=pete-the-cat-i-love-my-white-shoes\n\n198",
      "content_length": 653,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nThe call in turn returns:\n\n{ \"data\": [ { \"attributes\": { \"author\": \"Eric Litwin\", \"cover_image\": \"https://i.harperapps.com/covers/9780061906220/x300.png\", \"description\": \"Pete the Cat goes walking down the street wearing his brand new white shoes. Along the way, his shoes change from white to red to blue to brown to WET as he steps in piles of strawberries, blueberries, and other big messes.\", \"isbn\": \"978-0061906237\", \"price\": 1139, \"slug\": \"pete-the-cat-i-love-my-white-shoes\", \"title\": \"Pete the Cat: I Love My White Shoes\" }, \"id\": \"008672a6-c48c-49b2-98e8-b78914f2fce4\", \"type\": \"books\" } ] }\n\nAs mentioned previously, our ApiClient library knows the JSON API standard well, and transforms that\n\nresponse into an object we can easily use in the template code, such as this:\n\n<div class=\"single-para \"> <h4><%= book_details.title %></h4> <h5 class=\"item_price\"><%= render_price(book_details.price) %></h5> <p class=\"para\"><%= book_details.description %></p> <div class=\"prdt-info-grid\"> <ul> <li>- ISBN : <%= book_details.isbn %></li> <li>- Author : <%= book_details.author %></li> </ul> </div>\n\nWe hope this is starting to feel easier to follow; we'll follow exactly the same pattern to make service calls\n\nto load review information from reviews-service, which follows the same pattern of loading the book data.\n\nThe last element we'll talk about in the book detail page is the “related books” block, shown in\n\nFigure 12-8, which is a set of links to books that are somehow related to the book featured in this page. Much like the home page's Featured Books block, this is an element that will benefit from having its own endpoint, so that the business can tweak the rules for selecting those elements without that affecting any work on the front-end client application. The first step in implementing this is to define an endpoint in books-service that asks for books related to a specific book. We settle for an endpoint in the following format:\n\nGET /v1/books/2d831aba-75bb-48b6-bae6-9f1bb613d29b/related\n\n199",
      "content_length": 2077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nFigure 12-8. The related books block in the book detail page\n\nIn this example, the UUID (2d831aba-75bb-48b6-bae6-9f1bb613d29b) is the ID of the book for which we want to receive “related books.” For the sake of this example, we decided on using a very straightforward implementation that loads five random books that are not the book_id passed in the URL. Here's the implementation in Rust:\n\npub fn related(req: &mut Request) -> IronResult<Response> { let book_id = iron_helpers::extract_param_from_path(req, \"book_id\"); let book_uuid = Uuid::parse_str(&book_id).unwrap();\n\nlet pool = req.get::<Read<::AppDb>>().unwrap(); let conn = pool.get().unwrap();\n\nlet sql = \"SELECT book_id, title, author, description, isbn, price, pages, \\ slug, cover_image FROM books WHERE book_id != $1 ORDER BY random() LIMIT 5\";\n\nlet mut books: Vec<Book> = Vec::new(); for row in &conn.query(&sql, &[&book_uuid]).unwrap() { let book = Book::from_row(row); books.push(book); }\n\nlet response = jsonapi::json_collection_wrapped_in(\"data\", books); Ok(Response::with((status::Ok, Header(jsonapi::jsonapi_content_type()), response))) }\n\nWhen called, the preceding returns the following response:\n\n{ \"data\": [\n\n200",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\n{ \"attributes\": { \"author\": \"James Dean\", \"cover_image\": \"https://i.harperapps.com/covers/9780062304186/x300.png\", \"description\": \"Pete the Cat takes on the classic favorite children's song \\\"Five Little Pumpkins\\\" in New York Times bestselling author James Dean's Pete the Cat: Five Little Pumpkins. Join Pete as he rocks out to this cool adaptation of the classic Halloween song!\", \"isbn\": \"978-0062304186\", \"pages\": 100, \"price\": 791, \"slug\": \"pete-the-cat-five-little-pumpkins\", \"title\": \"Pete the Cat: Five Little Pumpkins\" }, \"id\": \"25f2db6f-966a-4c24-9047-1fdfeb43e465\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Eric Litwin\", \"cover_image\": \"https://i.harperapps.com/covers/9780062110589/x300.png\", \"description\": \"Pete the Cat is wearing his favorite shirt—the one with the four totally groovy buttons. But when one falls off, does Pete cry? Goodness, no! He just keeps on singing his song—after all, what could be groovier than three groovy buttons?\",\n\n\"isbn\": \"978-0062110589\", \"pages\": 100, \"price\": 1175, \"slug\": \"pete-the-cat-and-his-four-groovy-buttons\", \"title\": \"Pete the Cat and His Four Groovy Buttons\" }, \"id\": \"20ee1ef4-6000-460a-b715-40da50979fc7\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Charles C. Mann\", \"cover_image\": \"https://upload.wikimedia.org/wikipedia/en/b/b7/1491-cover.jpg\", \"description\": \"In this groundbreaking work of science, history, and archaeology,\n\nCharles C. Mann radically alters our understanding of the Americas before the arrival of Columbus in 1492.\",\n\n\"isbn\": \"978-1400032051\", \"pages\": 100, \"price\": 1299, \"slug\": \"1491-new-revelations\", \"title\": \"1491: New Revelations of the Americas Before Columbus\" }, \"id\": \"2d831aba-75bb-48b6-bae6-9f1bb613d29b\", \"type\": \"books\"\n\n201",
      "content_length": 1792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\n}, { \"attributes\": { \"author\": \"Eric Litwin\", \"cover_image\": \"https://i.harperapps.com/covers/9780061906220/x300.png\", \"description\": \"Pete the Cat goes walking down the street wearing his brand new\n\nwhite shoes. Along the way, his shoes change from white to red to blue to brown to WET as he steps in piles of strawberries, blueberries, and other big messes.\",\n\n\"isbn\": \"978-0061906237\", \"pages\": 100, \"price\": 1139, \"slug\": \"pete-the-cat-i-love-my-white-shoes\", \"title\": \"Pete the Cat: I Love My White Shoes\" }, \"id\": \"008672a6-c48c-49b2-98e8-b78914f2fce4\", \"type\": \"books\" }, { \"attributes\": { \"author\": \"Mo Willems\", \"cover_image\": \"https://images-na.ssl-images-amazon.com/images/I/61jurA6wsFL._SX258_\n\nBO1,204,203,200_.jpg\",\n\n\"description\": \"Diva, a small yet brave dog, and Flea, a curious streetwise cat, develop an unexpected friendship in this unforgettable tale of discovery.\",\n\n\"isbn\": \"978-1484722848\", \"pages\": 100, \"price\": 987, \"slug\": \"the-story-of-diva-and-flea\", \"title\": \"The Story of Diva and Flea\" }, \"id\": \"2497fe7d-4797-4506-a3cd-85e42d90415b\", \"type\": \"books\" } ] }\n\nWith everything implemented server-side, we have to make a tweak to the api_client library so that we can make calls to that endpoint. The json_api_client gem makes it ridiculously easy to give ourselves access to that endpoint; we just need to define a custom_endpoint in the Book class:\n\nmodule ApiClient module Books\n\nclass Book < Base custom_endpoint :related, on: :member, request_method: :get end\n\nend end\n\nTurning our attention back to the front-end application, we render those five related books in a separate\n\ntemplate, known as a partial, called related:\n\n202",
      "content_length": 1716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\n<%= render \"related\", books: book_details.related %>\n\nThe partial looks like this:\n\n<% for book in books do %> <div class=\"col-md-2 btm-grid\"> <a href=\"<%= book_path(book.slug) %>\"> <%= image_tag book.cover_image, alt: book.title, style: \"width: 120px\" %> <h4><%= book.title %></h4> <span><%= render_price(book.price) %></span> </a> </div> <% end %>\n\nThe Shopping Cart Page We've talked about the shopping cart widget, available in all pages in the application, but now let’s turn our attention to the shopping cart page, shown in Figure 12-9, which is where the user is taken when clicking in the shopping cart widget. As expected, the shopping cart detail page has not only the summary about\n\nFigure 12-9. Shopping cart page\n\n203",
      "content_length": 786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nnumber of items and total price in the cart, but also information about each of the items the user has added to the cart. In our example, we have two books in the shopping cart.\n\nIn purchases-service, shopping carts are modeled as multiple entities. A Cart is the shopping cart\n\nmain model, it has attributes such as items_count, total, user_id, and expires_at, and an Item has attributes such as item_id, quantity, and price. In order to display the shopping cart details, we need to have access to each line item in the cart; this is unlike the shopping cart widget, which only needed basic cart information. We've decided to use a single endpoint to return cart information, but return a richer response. This is what we have referred to as different representations of the same data in responses.\n\nThe following service call:\n\nGET http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2\n\nreturns this:\n\n{ \"data\": { \"id\": \"35c082ba-b974-4d78-9526-4fc6ed7b54a2\", \"type\": \"carts\", \"links\": { \"self\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2\" }, \"attributes\": { \"cart_id\": \"35c082ba-b974-4d78-9526-4fc6ed7b54a2\", \"expires_at\": \"2016-10-23T01:54:29.408Z\", \"total\": 1618, \"items_count\": 2 }, \"relationships\": { \"items\": { \"links\": { \"self\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2/\n\nrelationships/items\",\n\n\"related\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2/\n\nitems\"\n\n} } } } }\n\nFor the cart page, we pass an extra parameter, include=items, to that same API call; it looks like this:\n\nGET localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2?include=items\n\nIt returns\n\n{ \"data\": { \"id\": \"35c082ba-b974-4d78-9526-4fc6ed7b54a2\", \"type\": \"carts\", \"links\": {\n\n204",
      "content_length": 1817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\n\"self\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2\" }, \"attributes\": { \"cart_id\": \"35c082ba-b974-4d78-9526-4fc6ed7b54a2\", \"expires_at\": \"2016-10-23T01:54:29.408Z\", \"total\": 1618, \"items_count\": 2 }, \"relationships\": { \"items\": { \"links\": { \"self\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2/\n\nrelationships/items\",\n\n\"related\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2/\n\nitems\"\n\n}, \"data\": [ { \"type\": \"items\", \"id\": \"2d831aba-75bb-48b6-bae6-9f1bb613d29b\" }, { \"type\": \"items\", \"id\": \"8217e21a-97db-4be9-9feb-4cea97a64948\" } ] } } }, \"included\": [ { \"id\": \"2d831aba-75bb-48b6-bae6-9f1bb613d29b\", \"type\": \"items\", \"links\": { \"self\": \"http://localhost:5001/v1/items/2d831aba-75bb-48b6-bae6-9f1bb613d29b\" }, \"attributes\": { \"item_id\": \"2d831aba-75bb-48b6-bae6-9f1bb613d29b\", \"quantity\": 1, \"price\": 1299 } }, { \"id\": \"8217e21a-97db-4be9-9feb-4cea97a64948\", \"type\": \"items\", \"links\": {\n\n205",
      "content_length": 1020,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\n\"self\": \"http://localhost:5001/v1/items/8217e21a-97db-4be9-9feb-4cea97a64948\" }, \"attributes\": { \"item_id\": \"8217e21a-97db-4be9-9feb-4cea97a64948\", \"quantity\": 1, \"price\": 319 } } ] }\n\nAs you can see, the second response is much richer; it contains information about the two items currently included in the cart, so we'll definitely use that in our cart page. The main advantage of this approach is that we can use the same endpoint in both use cases—cart widget and cart page—but still have a faster response time in the case where we need less data, avoiding sending unnecessary data over the wire and making fewer database calls to provide a response in the service, thus improving overall system performance.\n\nBack to the showcase application, where we define a new helper method to load that information. An interesting feature supported by the JSON API specification is called sparse fieldsets (http:// jsonapi.org/format/#fetching-sparse-fieldsets), which allow a client to request that an endpoint return only specific fields in the response on a per-type basis by including a fields parameter. Sparse fieldsets can be useful in responses that may return a large number of attributes by default, where clients may gain performance by specifying only the attributes they are interested in. For example, if we had code that needed only the current value in dollars of a cart, we could use the client gem to load only that attribute by calling ApiClient::Purchases::Cart.select(:total).find(..), which would make the following service call:\n\nGET http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2?fields[carts]=total\n\nand would generate this response:\n\n{ \"data\": { \"id\": \"35c082ba-b974-4d78-9526-4fc6ed7b54a2\", \"type\": \"carts\", \"links\": { \"self\": \"http://localhost:5001/v1/carts/35c082ba-b974-4d78-9526-4fc6ed7b54a2\" }, \"attributes\": { \"total\": 1618 } } }\n\nExercises for You As we mentioned at the beginning of the chapter, we've just scratched the surface of functionality that can be built for our bookstore, and we have some ideas about what could be done next. We'll list a couple of services that you could spend some time implementing, if that sounds like something interesting to you. One\n\n206",
      "content_length": 2277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nof the great things about it is that you can use whatever programming language/environment you're most comfortable with.\n\nAn obviously missing piece is a user service, responsible for storing and managing data associated with the users of the bookstore. A user should be able to, among many things, register an account and log in, save books to a wish list, “like” book categories, see his or her own—and others'—wish lists, and follow other users. Based on those quick requirements, some of the endpoints we would want to see implemented are:\n\nUser CRUD\n\n\n\nname, email, password (hash), ID, books_in_wishlist, categories\n\n\n\nAdd liked category: [user_token, category_id] -> [user_id, category_id]\n\n\n\nRemove liked category: [user_token, category_id] -> 204\n\nGet all liked categories: [user_token] -> [category_id1, category_id2, …]\n\n\n\nAdd to wish list: [user_token, book_id] -> [user_id, book_id]\n\n\n\nRemove from wish list: [user_token, book_id] -> 204\n\nGet all books on wish list: [user_token] -> [book_id1, book_id2, …]\n\n\n\nFollow a user: [user_token, user_id] -> [user_id, user_id]\n\nUnfollow a user: [user_token, user_id] -> 204\n\nGet users I followed: [user_token] -> [user1, user2, ...]\n\nGet users that follow me: [user_token] -> [user1, user2, ...]\n\nWe also mentioned that the home page would eventually evolve and benefit a lot from showing books\n\nthat match the logged-in user; a recommendations service would be a very welcome addition to our microservices architecture. That service would probably feed from user-service, since it will have quite a bit of information about users, as well as purchases-service, which has data about what books have been purchased, or are currently in the cart. To make its API very simple, the service could have a single endpoint that, given a user_id, returns a book_id list as recommendations.\n\nService Orchestration You probably have noticed that a lot of the service-related code in the showcase front-end application is actually simple, because it has to make fairly simple service calls in many places; you can imagine that this can get complicated, and some service calls depend on the results of other calls, which can introduce complexities at the front-end layer that may be better suited elsewhere. To help simplify some of that complexity, we recommend using a Back-end for Front-end (BFF) service, briefly introduced in Chapter 7, which would be a service that knows exactly what data the front-end needs to render its pages and would be responsible for fetching that data from many different clients. A practical example of that is the home page, which makes multiple service calls to many services. The code in showcase could be simplified if it could\n\n207",
      "content_length": 2766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nFigure 12-10. Architecture of application with showcase-service in place\n\nmake a call to a showcase-service, which would be responsible for orchestrating calls to multiple services, and properly deal with error conditions there. Figure 12-10 diagrams this approach.\n\nWe have described most of the service interactions of this chapter with the web front-end in mind, but most applications have multiple client-facing applications from the start nowadays, which would be mobile iOS and Android applications. In the case of mobile applications, we really recommend the use of a BFF— let's call it mobile-service, because your product and service will evolve; and unlike a web application that has only one version deployed at any time, you will always have a population of users who will not upgrade your application to the latest and will get stuck making calls to APIs that you may want to change, and even deprecate.\n\nPerformance Considerations We have deliberately mentioned performance in this chapter only briefly; we touched on some aspects that will make your application faster, such as sparse fieldsets and using a cache to reduce the number of service calls. We believe that all services should be fairly fast, and that performance will only be a serious problem when you start having slow service responses; so we recommend that you keep an eye on each service’s response times to easily identify which service is slow so that you can fix it. A microservices architecture is not an excuse for having poor performance. We believe that it is actually the opposite; if you have very specific performance requirements, you can build services that will be responsible for reaching\n\n208",
      "content_length": 1744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "Chapter 12 ■ Single-Client appliCation—Mobile and Web\n\nthose numbers. We will go into detail about monitoring in the next chapter, so you can find out which dependencies are slow or slower, and how to get alerted.\n\nA very efficient technique for speeding up calls to distributed services is to make parallel service calls whenever possible. For example, we know that on the book page, we'd need to make calls to load the book details, the list of categories, and the shopping cart; we could make those three service calls at the same time, and our application's overall response time would benefit as a whole. Some programming languages make it easy—or hard—to work with threads, so your programming environment of choice may make this trivial—or not.\n\nSummary In this chapter, we dove into our sample project; that is, a bookstore with a client-facing web application that makes requests to a set of four services in order to display book information, and has a simple but working shopping cart. We went into detail showing code in multiple layers to give you a sense of how all the pieces fit together, and left a set of possible enhancements as exercises for the more adventurous. We hope we gave you some extra reasons to choose a standard for your APIs, as we've demonstrated a few nifty features included in the JSON API specification.\n\nIn the next chapter, we will describe the what and how of service monitoring, to help you figure out\n\nwhen your services are misbehaving.\n\n209",
      "content_length": 1485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "CHAPTER 13\n\nMonitoring Your Services\n\nAs you’ve seen throughout this book, embracing a microservices architecture will mean that your production environment has a large number of deployable units, which you will be ultimately responsible for making sure are functioning as expected, based on your requirements. As with any type of software, you will want to know when issues are happening, ideally before they start to affect your company’s business metrics, such as user visits, signups, or sales. In some cases, it could take hours—or days—to find out that one small piece of software is malfunctioning, and you don’t want to discover production issues after your users do.\n\nIt is extremely important that you set up a well-tuned monitoring toolset to alarm you and your team\n\nwhen services are misbehaving. You should spend a significant amount of time thinking about failures modes, and make it easy to integrate new services into your monitoring and alarming system.\n\nMonitor All Services We start this section with a rule: “Every single service in your environment needs to be monitored; no exceptions. Yes, even that one service that was built months ago, and only get 10 requests per day.”\n\nThe only things we can monitor are the ones we measure. In general, this is an area where you should err on the side of collecting more metrics than you are actually using; it’s much better than the alternative.\n\nSome of the tools we have recently used for monitoring are these:\n\n\n\nStatsD: A network daemon that listens and collects statistics and sends aggregates to many pluggable back-end services. It allows for very fast metric collection in a simple way. You can tell StatsD to count or time any metric you’re interested in, and it will save that data for you.\n\nGraphite: Graphite is a visualization tool for statistics configured from various sources. You can create customizable reports and dashboards, and it is one of the preferred front-ends for StatsD. Graphite is where you go to see and analyze—hopefully pretty—graphs, based on the data you push to it. Our former colleague Matt Aimonetti has a very thorough blog post about the subject, titled “Practical Guide to StatsD/Graphite Monitoring” (https://matt.aimonetti.net/ posts/2013/06/26/practical-guide-to-graphite-monitoring/).\n\nNew Relic APM: Application monitoring in one place allows you to see your application performance trends at a glance. You need an application-monitoring platform that is closely tied to your programming environment and can help with finding the root cause of slowdowns/outages. Among many things, tools like New Relic APM help pinpoint parts of the code that are slow, database queries that are not optimized, and endpoints that are returning errors.\n\n211",
      "content_length": 2751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Chapter 13 ■ Monitoring Your ServiCeS\n\n\n\nPagerDuty: PagerDuty’s incident resolution platform sits between your monitoring tools and your on-call responders to make sure your incidents are resolved quickly and efficiently. PagerDuty is the tool that notifies individuals/teams when an error condition has arisen; it allows teams to setup rotations, and notifies its users via a large set of media, such as email, SMS, or phone calls.\n\nSome monitoring tools have a narrow responsibility, and only help with collecting metrics; other tools are more comprehensive, and can also be used for logging and/or alarming, which we will cover later in this chapter.\n\nWhat Do You Need to Monitor? We want services that are available, aka “up and running.” The first thing you should make sure is that your services are indeed reachable from their clients, be they inside or outside your network. A simple way to monitor that a service is available is to hit a health-check endpoint, which is usually responsible for hitting whatever data store it uses and replying with a simple response.\n\nWe want services that are efficient and respond to requests in a correct and timely fashion. In this case,\n\n“correct” means that service responses look like what you have promised to the outside world in all cases. We touched on how to validate that service responses do abide by their contract in Chapter 9. It’s hard to define the exact meaning of “timely fashion,” but it’s imperative that it’s something that will not hurt your end-user experience.\n\nIn a 2006 talk at Stanford University, Greg Linden, from Amazon, said that “every 100 ms delay costs 1% of sales” on the eCommerce juggernaut’s site,1 which can also be read as “you sell more by being fast.” In reality, end-users want an experience that is fast, and a UI that is snappy; it’s known that site speed even affects how Google ranks search results (https://googlewebmastercentral.blogspot.com/2010/04/ using-site-speed-in-web-search-ranking.html), so being performance-oriented in a good thing. As your company grows, we recommend having a small team that specializes in fixing performance issues, so that you end up with fast services and have a set of performance-oriented principles specific to your environment.\n\nFocus on Perceived Performance We recommend special attention to perceived performance in your monitoring. Perceived performance is a measure of how quick a user thinks an action is. The usual example of perceived performance is that a user can think that a web site is fast because of how it renders a page, or how soon he can interact with the site; the real important metric is user perception. Another type of perceived performance metric is how fast clients of your service think you are; a classic example is of one client that claims that a service is responding in 150msec, while the service maintainers claims that the exact request renders a response in 20msec. it’s possible that both are right! By monitoring perceived performance, you can see such discrepancies in performance numbers that can lead you to investigate and find that, for example:\n\n\n\nThere is too much latency in the network call between clients and servers.\n\nOr communication is happening using SSL connections inside a private subnet.\n\nOr the client library used for accessing the service has a performance bottleneck caused by an inefficient JSON/XML parser.\n\n1“Make Data Useful,” Greg Linden, November 29, 2006; Presentation at Stanford University; https://docs.google. com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxnbGluZGVufGd4OjVmZDIzMWMzMGI4MDc3OWM\n\n212",
      "content_length": 3603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Chapter 13 ■ Monitoring Your ServiCeS\n\nThe bottom line is that being able to see that perceived performance is slower than expected will lead you to spend time investigating the reason for such disparity. However, don’t go crazy and make one single data service endpoint extremely fast; it could be that saving 20 msec of response time—which can be an impressive and fun accomplishment—isn’t helping your overall perceived performance if your web site that renders that data is extremely slow because of other web assets, such as JavaScript and CSS So you should know what battles are worth getting into.\n\nAdhere to SLAs Define service-level agreements (SLAs) for every single service endpoint in your environment.\n\nA service-level agreement (SLA) is a part of a standardized service contract where a service is formally defined. Particular aspects of the service – scope, quality, responsibilities – are agreed between the service provider and the service user. 2\n\nIn practical terms, an SLA for a specific endpoint will list some of the following service expectations:\n\n\n\nThroughput for the endpoint\n\nNumber of concurrent clients\n\n\n\nAverage response time\n\n\n\n90th percentile response time\n\n\n\n95th percentile response time\n\nUptime, availability, and any other nonfunctional requirements\n\nAfter you have SLAs in places for all your services, you can start to leverage that data in many ways:\n\n\n\nIf you’re building a new service that will have four downstream dependencies you need to call sequentially, and each of those dependencies has an average response time of 100msec, then it’s unlikely that the new service can deliver requests averaging less than 400msec of response time; you can derive SLAs based on your dependencies.\n\n\n\nIf you’re designing a service that needs to deliver response times much faster than your SLAs currently guarantee, you know that you may need to introduce new components to your infrastructure to hit the performance numbers you are aiming for.\n\nOne advantage of having SLAs is that they will help you set up your alarming because you will have data points for what is acceptable and what isn’t. More importantly, they help to set expectations for the clients of a service, as well as for the service owners’ level of support.\n\nBusiness Metrics Are Important There is a tendency among software professionals to focus on technical metrics, which is not a bad thing; but every organization needs to keep an eye on important business metrics, as it allows the organization to also find out about issues that went otherwise unnoticed.\n\n2“Service-level agreement,” Wikipedia, referenced on August 27, 2016. https://en.wikipedia.org/wiki/ Service-level_agreement\n\n213",
      "content_length": 2692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "Chapter 13 ■ Monitoring Your ServiCeS\n\nAn example of an error condition that can happen in any web application is having a login page that\n\nno longer works because of a JavaScript issue, or even a missing Login button. From a systems perspective, all involved systems—web application, authentication and authorization service—could be working as expected, and are looking functional in all their own monitoring graphs; however, there’s a clear issue happening. If you have a business metric called “sign-ins,” you can set that up to alarm after there are no sign-ins for a specified time, alerting you that something is not working as expected even when all services are working properly.\n\nAN IMPORTANT BUSINESS METRIC\n\nthe most famous business metric at amazon is the order rate of the retail site. amazon has good enough tracking about its order rate—orders per second—that it predicts for any point in time that the order rate has to be within a calculated threshold; it will alarm—and wake up—developers in many teams when the order rate drops below the threshold. Whenever that happens, it is expected that specific teams have had their own alarms go off to help pinpoint where in the order processing chain a failure may be happening.\n\nexamples of failures that would make the order rate drop significantly could be issues in the services that authenticate users or in the payment processing service. While the order rate alarms will certainly fire in these situations, it should certainly be the responsibility of the teams owning the primary (authentication or payments processing) services to be alerted of issues in their systems long before sales on the site are affected.\n\nIdentifying Requests Your toolset has to make sure that a globally unique ID identifies each request so that you can track requests as they travel in your infrastructure; we recommend you always use UUIDs. We’ll call those “request IDs” for now; they need to have the following characteristics:\n\nUnique across all applications and services\n\n\n\nEasy to create, forward, and log\n\n\n\nEasy to trace\n\nSome modern web application frameworks already generate unique request IDs per end-user request;\n\nthus, you have to make sure that, once generated, the request ID is forwarded to all underlying services so that one of them can load all the internal requests generated by a specific request. Having such a tool will help you pinpoint issues when nobody is sure what system is failing; you should be able to trace a request through all your system easily. You also need to make sure you can attach that request ID to requests you make to external APIs, so that those are also traceable during debugging.\n\nFor example, a request that goes from our bookstore sample project for order processing will pass through the web application, the books service, and then to purchase/payment services, and will likely be forwarded to one external API for payment processing; there are quite a few players involved in making sure that the purchase experience goes smoothly, and the last thing you want to do is to make guesses when orders are not functional.\n\n214",
      "content_length": 3127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Chapter 13 ■ Monitoring Your ServiCeS\n\nLogging All Requests Having a unique and traceable request ID is only the first half of being able to investigate issues in a distributed environment. You also need to make sure every single request in your environment is saved to a log file, and that each one of those log files is easily accessible. Using the right tools and defining a common log format can help with this effort.\n\nLogging Tools\n\nAs you set up your infrastructure, it’s probably easy to look at raw log files using simple command-line tools like grep; but, as your infrastructure grows, you should use a more sophisticated tool for log inspection. Here are some tools you can use to search your log files:\n\n\n\nSplunk: Splunk captures, indexes, and correlates data (usually logs) in a searchable repository from which it can generate graphs, reports, alerts, dashboards and visualizations.\n\n\n\nLogstash: A modern tool for managing events and logs, with an ElasticSearch back- end which allows a high degree of searching and filtering.\n\nKibana: A modern, beautiful visualization tool that works seamlessly with Logstash and ElasticSearch. It features a no-code setup, real-time analytics, and much more.\n\nAmazon CloudWatch Logs: CloudWatch Logs can be used to monitor your logs for specific phrases, values, or patterns. For example, you could set an alarm on the number of errors that occur in your system logs or view graphs of web request latencies from your application logs.\n\nWe’ve discussed in previous chapters that in a distributed environment, it’s crucial to write code that\n\nproperly deals with downstream dependencies being unavailable or timing out. It’s important that code handling error conditions writes to logs for every single error encountered, whether a timeout or a service error. Once you have log entries that indicate that a service is down, you can use log analysis tools to alarm based on log entries; many tools on the market, including the ones listed above, support that.\n\nDefining a Uniform Log Format\n\nThe data you include in your log files should follow a uniform format to allow easy parsing by your tools. You should also have standards for what you decide to log; ideally, each request should have at least two entries in your log format, the first when a request reaches a service, and another after a request is being processed. You can also log more data, especially when this extra data may become valuable information for debugging purposes.\n\nTo illustrate this further, let’s look at a few log entries that were generated by a single request to our\n\nproduct catalog service:\n\nI, [2015-12-28T01:31:14.322076 #24041] INFO -- : Started request_method=\"GET\" request_url=\"/offers/deal-1542454.json\" request_time=\"2015-12-28 01:31:14 +0000\" request_ip=\"172.17.4.48\" HTTP_CLIENT_IP=\"172.17.4.93\" HTTP_X_FORWARDED_FOR=\"172.17.1.102, 172.17.4.93\" HTTP_USER_AGENT=\"Ruby/ls-catalog/v6.2.1\" HTTP_X_LIVINGSOCIAL_API_CLIENT_ NAME=\"mobile_production\" HTTP_IMPRINTID=\"e9f8bb4a-41e0-4d87-982b-78f1f3eb3668\" HTTP_TRUE_CLIENT_IP=\"172.17.4.93\" HTTP_X_REQUEST_START=\"t=1451266274269000\" representation=\"deal_large\"process_pid=24041 request_id=e9f8bb4a-41e0-4d87-982b- 78f1f3eb3668\n\n215",
      "content_length": 3211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "Chapter 13 ■ Monitoring Your ServiCeS\n\nI, [2015-12-28T01:31:14.325896 #24041] INFO -- : method=GET path=/offers/deal-1542454.json format=json controller=offers action=show status=200 duration=5.39 view=0.13 db=0.64 time=\"2015-12-28 01:31:14 +0000\" client_name=mobile_production process_pid=24041 request_id=e9f8bb4a-41e0-4d87-982b-78f1f3eb3668 I, [2015-12-28T01:31:14.326133 #24041] INFO -- : debug_value=\"something important\" process_pid=24041 request_id=e9f8bb4a-41e0-4d87-982b-78f1f3eb3668\n\nFirst, note that all three log entries here share the same process ID (24041), which indicates that they were produced by the same application service process. Next, you can see that the request_id UUID values also match on all entries, which denotes that all three log messages were produced during the processing of the same client request cycle. All three messages also carry increasing microsecond-based timestamps (2015-12-28T01:31:14.322076, and so on), which proves that they were processed in the order they are listed in the log file.\n\nThe main difference between the three messages is the event that generated them: the first message logs the arrival of a request (alongside many interesting properties of the requesting client), and the second message provides info at the end of the request cycle (including response codes and processing time), while the final message simply logs additional debugging information.\n\nNote that all information is given in a key=value format, which makes automated post-processing of\n\nthis information with log analysis tools much simpler.\n\nReading Metrics Now that you are measuring as many metrics as possible, you have to be aware of how you view and interpret those numbers. We’ve all read our share of dubious statistics-based articles in our social network feed; I’m sorry to say that the metrics we collect do sometimes tell a similar story.\n\nAverages Lie\n\nA common mistake that is made is to look at—and rely on—average response times for monitoring overall service performance. Average response time is a “convenience metric” that gauges overall response time as if it were very linear. Averages can convey a wrong one-dimensional number that doesn’t really take into account outliers, and they only paint an accurate picture of overall system health if your response times follow a normal distribution, which is rarely the case with services.\n\nAVERAGES HIDE OUTLIERS\n\nthe main user-facing web application at LivingSocial receives up to 20K requests per minute during peak time, showing average response time for all requests and endpoints usually below 200msec, which seems to be decent performance for most cases. however, when you look at some endpoints, it’s easy to find that endpoints with low usage average terrible response times, sometimes above 2 seconds, just at the web server layer. it’s almost impossible for a terribly implemented endpoint to make a dent in overall average response time in such an environment. Some monitoring tools default to showing overall response time, which doesn’t really tie into any single change that gets deployed, making it really easy to hide endpoints with suboptimal performance.\n\n216",
      "content_length": 3178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Chapter 13 ■ Monitoring Your ServiCeS\n\nAn Endpoint Tells Many Stories\n\nAnother mistake we have seen often is to judge an endpoint’s performance by averaging all requests to that endpoint in one single metric. This particularly applies to endpoints that perform some sort of search, and possibly return a variable number of records, based on an API input. It’s possible that one caller of an endpoint usually requests 100 records to be returned; so, if you have endpoints whose performance can vary greatly based on caller parameters, it may make sense to have metrics based on those parameters.\n\nIn general, developers should default to looking at data for the 95th or 99th percentile, which will give\n\nyou numbers for the worst 5% or 1% of your responses, which is where performance is at its worst and where gains can be made most consistently.\n\nA major advantage of looking at the 95th or 99th percentile is that you will be able to recognize\n\npotential performance problems before they happen system-wide. If you have an issue with a downstream dependency that is causing your average response time to degrade, then it’s usually too late to prevent some sort of service degradation or even an outage; however, if you start getting alerted as soon as a small percentage of requests goes above your SLA threshold, then you’re more likely to solve that issue before it causes serious damage.\n\nLook into Your Slowest Responses\n\nEven when services are functioning well, and have response times you are proud of, you can still learn about how your services work, and find potential performance issues by paying attention to your slowest responses. A very slow response can showcase a lot of potential issues:\n\n\n\nPossible database contention at specific times of the day may cause a slow response due to a slower than usual query.\n\n\n\nExcessive object allocations may make garbage collection kick in at unexpected times.\n\n\n\nThere may be new data access or creation patterns that your service was not designed for (for example, because of a change in business strategy that exposes the service to clients with very different data access patterns).\n\n\n\nThere may be a slow response from a downstream dependency, which could itself need to be fixed. Remember that a service response can’t possibly be faster than its dependencies.\n\nSometimes you can see that a specific use case of your service can’t possibly be efficient within the\n\nconstraints of your current design; it may require extra joins or do a lot of in-memory calculation, among other things. A really slow service response could be a “design smell”; it may show that a specific use case would be better served by a new endpoint, or even a dedicated service altogether.\n\nFIXING PERFORMANCE WITH SLOW REQUESTS\n\nin a well-performing service we run at LivingSocial, we were seeing occasional responses taking above 1 second to respond. that service had one client—out of 20+—that needed a response with hundreds and sometimes thousands of objects in its body. We knew that those requests were supposed to be slow most of the time, and never questioned the slowness. the requests happened in a cron job, so they impacted nobody; but they kept appearing in our “slowest responses” report, so we decided to take a quick look into them one day. to our surprise, 80% of the processing time for those responses was happening building the JSon string to be returned as a response, which was a bit of a surprise\n\n217",
      "content_length": 3460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Chapter 13 ■ Monitoring Your ServiCeS\n\nbecause we were expecting to see most of the time spent in the database. that service used a series of JSon templates based on a templating language we had assumed was fast enough for our use cases; however, that high number made us look into its performance in more detail. after some experimentation, we decided to get rid of that templating language and used a much simpler solution that saved us 15–30% in performance in all requests, even the ones that were already fast enough.\n\nAdjusting Alarms At this point, you have started to collect metrics about your services and hosts, and have defined a set of SLAs that you need to deliver on. It’s time you start to send alarms to your team when something goes wrong. Most modern alarming tools allow their users to pick their preferred method of paging, whether it’s email, SMS, or phone call, so make sure you use a flexible tool. You can issue alarms under many circumstances:\n\nWhenever an error happens (any response with a 500 status)\n\n\n\nResponse time exceeds the SLAs, or even better, response time is getting close to exceeding your SLAs\n\nWhile still meeting its SLAs, a service sees a significant change in response time (for example, more than 1 standard deviation over the past 5 minutes)\n\nOne of the business metrics is beyond its acceptable threshold\n\nAfter you start sending out alarms, someone will have to wake up and fix slow or broken services or\n\nhosts. In a well-run microservices architecture, an alarm will notify a developer—or ops person—who will log into their terminal and start to look into the reported incident. Being notified of a live issue is stressful enough that you want to make sure that:\n\n\n\nYou alarm for a good reason. If you alarm—and wake people up—too often, you will create an army of co-workers who will learn to ignore those alarms.\n\n\n\nThresholds are easily configurable. If you just alarmed someone for a questionable reason, make sure the alarm can be tweaked so that it’s only happening when attention is really required. A team should define threshold for alarming based on the SLAs of the services they support, and tweak those SLAs according to how their service is performing.\n\n\n\nAlarms should help with resolution. As a company grows, the number of services grows with it, and you reach a point where not all developers know all services really well. Add a link to an operations manual that is kept up to date, as part of the alarm text, and to an operations dashboard that helps with investigating root cause of issues, and you will make everyone’s life much easier. An efficient operations manual should include details about the failing service, links to dashboard/graphs that may help with solving the issue, and description of the possible cases that can lead to failure, along with the preferred solution.\n\nSetting up alarming in a distributed environment is an art, and it will take many tries and constant\n\ntweaks to reach a point where a team is confident that a system is always up and performing as expected.\n\n218",
      "content_length": 3065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Chapter 13 ■ Monitoring Your ServiCeS\n\nSummary In this chapter we first looked at tools and guidelines to monitor the important parts of your microservices infrastructure. We clarified that the useful monitoring focuses on performance as it is perceived by the service client, supporting the adherence to published SLAs and measurement of business metrics.\n\nNext, we examined analysis tools and important properties of service logs, explaining the importance of\n\nunified, machine-readable formatting to help monitoring and log-based alarming.\n\nWe gave tips about correctly interpreting the metrics, explaining the problems with relying on averages\n\nand why it is wise to start any investigations by looking at the slowest service responses.\n\nFinally, we laid out guidelines for adjusting the sensitivity for alarms generated from monitoring\n\ninformation.\n\nThe next and final chapter examines tools and processes to help with achieving excellence in\n\nsupporting and maintaining your services through their lifespan.\n\n219",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "CHAPTER 14\n\nOperational Excellence with Microservices\n\nSuccessfully developed and launched software spends the vast majority of its lifecycle in a state where it needs maintenance and support. In an environment where the number of services can easily outnumber the number of developers in your engineering organization, it is business-critical that operational support runs efficiently and effectively, without impacting the productivity and retention of your engineering staff.\n\nThis chapter will look at the processes and some of the available tools to help with achieving excellence\n\nin supporting your services. We will intersperse the discussion and recommendations with examples to underline the lessons learned from our experience of being on-call for many years.\n\nWhat is Operational Support? Let’s first define what we understand by operational support.\n\nTo us these are all activities that are aimed at supporting the functioning of an already launched service\n\nor feature according to the expectations—explicitly and implicitly stated—of the clients.\n\nImplicitly stated expectations are requirements that are common sense, but were not explicitly stated\n\nin any specifications of the service or feature. If, for example, a newly introduced API functions as designed, but it causes the service to degrade in stability, responsiveness, or correctness of other APIs or features, or it results in scaling issues under increased load, then these issues should be accounted for as operational support.\n\nThe activities included in operational support work can be rather wide-ranging. They include not just root cause analysis for, and fixing of, obvious bugs and errors reported by automated monitoring or client reports, but also general maintenance work related to library or service framework upgrades, adding missing documentation, proactive searching for potential security bugs or edge cases that have not been accounted for, and answering technical questions by the client team.\n\nEmbracing Ownership One of the major advantages that many developers (ourselves included) report getting out of working in a well-functioning microservices environment is that they feel empowered by the level of autonomy given to them. The “you build it, you run it” approach leads to encouraging the team to feel like true owners of their domain, and hence also feel more like they own an actual part in the business as a whole. And along with the feeling of true ownership comes a feeling of responsibility that needs embracing; your new-found autonomy means that the buck stops with you, as well.\n\n221",
      "content_length": 2595,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nCustomers First The first rule of business you should embrace is that if you don’t add value for your customers, then your business is doomed to failure. The best teams have it engrained in their culture to settle situations of disagreement and conflict asking themselves this question:\n\nWhat is the best thing to do for our customers?\n\nIf you have this mindset, then not only will decisions be easier to make, but you will also elevate the\n\ndiscussion of disagreements to a level where the egos of the people on your team are easier to extract from making business-savvy decisions. The focus will be on designing services for usability and solving real problems and thereby adding maximum business value.\n\nIs the Customer Always Right?\n\nNote that we intentionally did not phrase the question as “What does the customers say they want?” The question we did ask, about what is best for the customer, means that you should make sure you understand what the customer is trying to achieve, balancing the mission and architectural constraints of the services you own against the requests.\n\nAs an example, if a client team asked the books-service team to vend all books sold in the US, sorted\n\nby revenue in the shop, then the owners of the books-service should figure out whether such an API can be reasonably exposed in an efficient manner. At the very least, the service team should try to understand the constraints in which they can operate. Will the API really need to vend all books in the US? What would the client do with this data? Can the information be paged? Is there room for staleness in the revenue numbers by which to sort, or in the list of books? What is the anticipated call volume?\n\nOnly then can the service team make educated decisions about how best to serve the needs of this\n\nparticular client, and also implement such an API without impacting other clients and overall system stability and responsiveness.\n\nOperational Support Benefits Customer Trust\n\nMany companies spent a lot of money on acquiring new customers. But how much budget does your business dedicate to keeping the customers you have happy and engaged?\n\nA big factor in repeat business from your clients is how well you manage to establish and maintain their\n\ntrust. But customer trust is hard to earn, and it is easily lost when you disappoint the clients’ expectations. That is exactly where operational support comes in: it is all about making sure that your service maintains client expectations. Therefore, investing in operational support is an investment in customer trust.\n\nMaking sure all systems are operational is not just contributing in the obvious and direct way, for example, by enabling your customers to make purchases on your site, or to sign up for your services. It is also an investment in keeping current customers’ trust that the site will be up, running, and behaving as they expect when they visit it.\n\nIt is much harder to regain a customer’s trust after it was lost due to a bad experience, and the money\n\nand time you’ve spent on acquiring the client in the first place is wasted. When a judgment call needs to be made between allocating developers to implementing new features and fixing already launched functionality, then a monetary value needs to be attached to the estimated loss of customer trust incurred per day of broken functionality, and it needs to be factored into the prioritization process.\n\nFor that reason, in the majority of successful businesses we have worked for, addressing operational issues always outranked new projects or feature work. When an outage arises and is noticed in a service that has launched, then this underlying issue should best be seen as a use case that was missed during development. It is just seen as though the launched feature is not entirely finished and hence needs fixing first before any new functionality is released.\n\n222",
      "content_length": 3945,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nNo Code without an Owning Team There should be absolutely no line of code (or whatever unit of software is run in the infrastructure) for which it is unclear who owns the maintenance and support issues.\n\nAnother very important point in this context is that software ownership is never assigned to a single person. Every service is owned by a team of people, and they share all ownership privileges and responsibilities. That does not mean that you cannot have subject matter experts in your team; in fact, you probably want to encourage developers to dive as deeply as they would like into the services they own.\n\nIt just means that shared ownership reduces the risk for the team, and the company as a whole, that all\n\nexpert knowledge about a service in your system would depart with the single owner. Sharing ownership and knowledge in a team also sets up the team to support several services via an on-call rotation, the details of which we will discuss later in this chapter.\n\nAny team can of course own several services, and in a microservices environment where the number\n\nof deployment units is often a multiple of the number of team members this is in fact the only practical approach.1 Therefore make sure that the team knows and documents exactly at each point in time which services, applications, libraries, and/or third-party software packages they own and maintain.\n\nKeeping such complete and up-to-date mappings of software-to-team can be implemented and\n\nenforced in different ways. Your code control system could make sure that code repositories need to be owned by an existing team, and your deployment infrastructure scripts could check that an ownership field is present and validate its value with an up-to-date list of teams. To advertise the team-to-software details, you could introduce a simple, well-known engineering-wide documentation page, or a web UI on top of an ownership database, or even an actual service discovery registry in your microservices infrastructure.\n\nThe most important part is that maintaining such a mapping is universally accepted as part of your\n\nengineering team culture. Such acceptance and visibility needs to go so far that plans for engineering- internal team reorganizations always include a line item that makes sure that, after the reorganization, all software systems have a new owning team. If reorganizations do not include reassigning the services owned by disbanded teams, then you will end up with unowned but live systems, which therefore will become unsupported and hence are an accident waiting to happen.\n\nFinally, avoid set-ups where some software is tagged to be owned by multiple teams. We have never seen such cross-team ownership arrangements work out well, as in practice such shared responsibility leads to confusion and often ultimately leads to no team feeling responsible for services that are officially of shared ownership.\n\nBoosting Team Cohesion Working on operational support for the services they own as a team is essential for embracing true ownership as part of the team’s culture. In a “you build it, you run it” environment, the team soon realizes that the systems they create spend the vast majority of their lifecycle in production and needing operational support; implementing and launching services is only a minute episode in the existence of a successful software application.\n\nOrganizing all the team’s activities and practices around making the software it owns easy and efficient\n\nto maintain becomes an objective that is in the team’s self-interest. This way, the team’s interests are well aligned with the clients’ interests, and—by extension—those of the entire business.\n\n1While it is very hard to give meaningful guidance about optimal team sizes or the best number of services per team developer, in our experience the sweet spot most successful microservices companies choose is between 5 and 8 developers per team. If automation is advanced in an organization, then the ratio of microservices per team member (or developer) does not have a clear upper bound. Running multiple services per developer is definitely possible if development and deployment processes are streamlined.\n\n223",
      "content_length": 4245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nAnother team-internal practice that helps team cohesion and the happiness of engaged and curious\n\ndevelopers is to rotate team members through the feature development projects for all the software owned by the team. As mentioned above, there surely are advantages of having subject matter experts for each service, but allowing them to work on other services owned by the team can be a cost-effective risk reduction strategy.\n\nThe team will be able to shake the dangerous reliance on individual “heroes,” thereby mitigating the risk\n\nof time-intensive research and coming up-to-speed of other team members when the hero leaves the team. In short: the real heroes share their special powers to enable others to rely less and less on them.\n\nWhen team members start working on services they previously did not engage with, there is great potential for cross-pollination: they see how things are done in a different code base, and so might recognize patterns that can be applied to the service in which they usually work. Conversely, having a new set of eyes on an existing service might contribute ideas about how to handle tricky problems or inelegant solutions in an improved way.\n\nAn advantage of rotating team members during feature development that applies directly to the team’s operational support efforts is that all team members become more familiar with the entire set of services the team owns. As a consequence, they usually also become more comfortable with being on-call for the full set of systems owned.\n\nOur final advice regarding ownership and team cohesion is to avoid placing developers in a position\n\nwhere they straddle multiple teams at the same time.\n\nThis does not mean that we advise against loaning team members out to other teams; loaning is especially useful when a project in the developer’s home team is blocked by missing functionalities in a service dependency that is owned by a team is not staffed to dedicate time to implementing such feature.\n\nIn our experience it is simply not advisable to expect the developer who is on loan to another team in such situations to still fulfill responsibilities on the home team. When someone is on loan, she will be truly and fully embedded in the team to which she is on loan. This way, the loaned-out developer has enough time to focus on the task and give the members of her temporary team time to gain confidence that the code contributed by the embedded person can be maintained and supported by the receiving team.\n\nWhat It Means to Be On-Call In order for a team to implement operational support effectively and efficiently for the systems it owns, a (set of) developer team member(s) need(s) to be nominated and act as the point person(s) for addressing any and all support issues.\n\nThis role is usually referred to as being on call, and this section will explain the responsibilities and privileges of the person filling it. We will lay out an etiquette and code of conduct to be followed by the on-call person herself, as well as by the rest of the team and the organization in which they work.\n\nWhile many of the points mentioned in the next section are applicable regardless of the architecture that is chosen for the overall business application, we will conclude this section by touching on some points that are particularly important to on-calls supporting microservices architectures.\n\nDefining the Role and Process for Being On-Call The most succinct, to-the-point definition of the role of an on-call developer comes from a presentation our former colleague Jon Daniel delivered at the 2015 DevOpsDays in Pittsburgh:2\n\nThe on-call’s role is to react to failure and prevent catastrophe.\n\n2Jon Daniel, \"Ethical and Sustainable On-Call,” August 13 2015, DevOpsDays Pittsburgh; An overview of Jon's inspirational talk, including slides and a link to a video recording of the talk, can be found on his personal blog at: https://chronicbuildfailure.co/ethical-and-sustainable-on-call-c0075e03a7b\n\n224",
      "content_length": 4038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nThis accurately summarizes the priorities of the person in charge of supporting a team’s production- deployed systems. First and foremost, the on-call person needs to make sure that new and current failure situations are addressed as they are ongoing, all with the overarching goal of preventing even greater impact through a potential total outage.\n\nIssue Receipt and Triage\n\nThe ways production issues are reported or noticed can differ greatly. Some are manually generated bug reports from colleagues. Others are created automatically, via integration of your monitoring and alarming systems with your ticketing system. Where such integration is lacking,3 the on-call will have to manually create issues for tracking when aberrations become obvious from monitoring or alarming. Sometimes issues that are reported to the on-call are mere hunches about potentially faulty behavior, or requests for an in-depth analysis of the expected behavior under a given set of circumstances regarding one of the systems owned by the team.\n\nIn all such cases, the first step for an on-call is to make sure a support ticket4 for the issue exists; and if that is not the case, to create a ticket herself. It is essential that the team has a template for trouble tickets, with fields for minimally required information, such as ”expected behavior,” “observed behavior,” ”steps to reproduce the issue,” and so on. This helps to cut down on avoidable back-and-forth communication attempting to clarify information the on-call needs to track down the issue, thereby leading to faster resolution of the problem.\n\nOnce a ticket is created, it needs to be triaged based on the severity of its impact, and prioritized in relation to other unresolved production issues. For that it is important that the team think about generalized rules for severity and priority assessment. We will discuss an example for such rules in the subsection \"Defining and Advertising Team Policy\" later in this chapter.\n\nNote that in organizations where development happens against services that are deployed to a shared\n\nsandbox fabric, it is usually the policy that the team’s on-call is also responsible for the proper functioning of the sandbox instance of the team’s services. While it is of course a serious impediment for the organization if the development efforts for teams that depend on the affected sandbox instances are impeded, the on-call generally applies a lower priority to such issues than is applied to problems surfacing in production.\n\nDeciding What Work to Pick Up\n\nOnce the issues are properly triaged, the on-call will always pick the top priority-item on the list to work on and resolve. If the on-call person ever gets into a situation where all trouble tickets are resolved, then most teams all have a backlog of items that serve operational improvements. Examples of such operational improvement backlogs are tasks to address missing documentation for some of the systems owned by the team, or to add additional metrics to the team’s monitoring dashboard, or to fine-tune alarming rules. It is often also very useful to update the Standard Operating Procedure (SOP) documentation for the team’s services when new approaches to address recurring issues are discovered during the on-call’s current duty. Often, the on-call will also be tasked with writing or following up on post-mortems for outages in which her team has stakes or was directly involved5.\n\nIf the on-call still has spare cycles left to invest, they can often best be used to try to anticipate errors,\n\nhunt for issues that could arise in edge cases, delete unused code, or generally “kick the tires” of the application in use.\n\n3Automatic ticket creation is hard to get right, as often this leads to many duplicated or similar trouble tickets that are rooted in the same underlying issue. An on-call person will then spend a lot of time on de-duplication and other unnecessary ticket management tasks. 4We will discuss properties of useful trouble ticketing system later in this chapter. 5We will talk in more detail in later subsections about dashboards, SOPs, and post-mortems.\n\n225",
      "content_length": 4190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nWhatever the on-call person chooses to spend her time with, she should decidedly try to steer clear\n\nof signing up for feature development or necessary technical improvement work that is tracked in the team’s sprint. In our experience, counting on the on-call to contribute to the team’s project work will lead to suffering: the team cannot rely on the on-call person’s availability for working on features, so they might over-promise features to representatives of the business-focused departments, which will lose trust in the team’s ability to deliver on their promises. At the same time, the on-call person—feeling pressured to deliver features—runs the danger of dropping the ball on critical operational issues.\n\nAs a corollary, this means that the number of members in an on-call rotation (usually, the team size) should be large enough to support a dedicated on-call person, which in our experience means that it should include around five to eight developers.\n\nOn-Call People Increase Team Efficiency\n\nIf this approach of dedicating a person to operational issues meets resistance in your organization, then the arguments raised usually amount to not being able to afford a dedicated on-call person, given other business pressures. Our advice in such a situation is to raise awareness of the costs of neglecting operational emergencies. It is not a question if outages will happen, but only when they will hit you, and if you can afford to not be prepared and—as a consequence—lose business or customer trust.\n\nTry to compare the decision to save on operational support to an “unhedged call option” for stocks, rather than using the usual metaphor of “technical debt”6: unhedged calls can cost an unpredictable amount of money should they be exercised, whereas debt is predictable and can be managed.\n\nAnother part of the on-call role is to defend the rest of the team from interruptions. By being dedicated\n\nto all interrupt-driven issues and distractions, the on-call person shields the rest of the team from being disrupted, thereby guarding their focus.\n\nVery conclusive arguments can be made that dedicating a single person to working on all the unpredictable, highly disruptive and stressful tasks increase the team’s overall productivity. Again, disruptions are a fact of life with technology, and even more so in an environment of distributed systems. Acknowledging this reality and preparing for the inevitable outage is a smart decision for every business to make.\n\nThe acting on-call person will act as the single point of contact for all operational and technical inquiry\n\nissues. Any other developer on the team should only be contacted and interrupted by the on-call person, and only if she deems it necessary (in cases where an urgent technical issues requires the knowledge or advice of another team member).\n\nA Responsibility to Rest\n\nIn order to be successful in the role of acting as the team’s firewall, it is the responsibility for the on-call person to be alert and focused while dealing with operational problems. In most around-the-clock businesses, on-call people will need to be able to tend to problems as they arise, any time of the day or night. For that reason, they usually carry a pager or similar device, on which they can be made aware of urgent issues 24 hours a day.\n\nIn order for the on-call person to stay awake at the wheel, she cannot be allowed to be sleep-deprived\n\nand worn out for longer than is required to take care of high-severity operational problems. As a result, it should be expected that the on-call person is entirely autonomous regarding her time management. In short, if the on-call person feels worn out and tired after a night of dealing with outages, then they should\n\n6See Steve Freeman's blog post \"Bad code isn't Technical Debt, it's an unhedged Call Option\" (July 23 2010) for a more detailed explanation of this argument; http://higherorderlogic.com/2010/07/bad-code-isnt-technical-debt- its-an-unhedged-call-option/\n\n226",
      "content_length": 4050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nfeel obligated to sleep and relax during times with less trouble. During such times, the on-call should notify the secondary on-call person (whose role and responsibilities we will explain a little later) that he should step in when any urgent issues need addressing.\n\nThe On-Call Person in Relation to the Team As laid out above, on-call duty play an important part in a development team that is also concerned with the operational aspects of the software they deliver. That is why investing thought into how to set up on-call duty for the team is valuable.\n\nWhile being on-call means that you get exposure to a broad range of operational aspects of all services owned by the team, it can also be very distracting and stressful. So, in order to prevent developer burnout, make sure to establish a rotational assignment for on-call duty within your team. We have seen weekly rotations work best in practice, as shorter rotations impact effectiveness on follow-through of ongoing issues, while longer rotations easily can lead to loss of focus and fatigue.\n\nIt is important for team morale to include all team members in the rotation schedule. Make sure to respect the schedule and raise the expectation that it is important to integrate into planning. For example, make sure that, if your time-off overlaps with your on-call rotation slot, you trade slots with another team member before leaving.\n\nNote that we also recommend including new team members and junior engineers into the rotation\n\nschedule as soon as possible. While it is natural that there are justified concerns from both the new or inexperienced team members and the more experienced ones that operational issues may not be addressed appropriately, new team members can always shadow the on-call person, or be a secondary on-call—a concept we explain more later. We have found this to be an excellent way to familiarize new team members quickly in a hands-on setting with the breadth of the systems that the team owns.\n\nAnother good tip we have is to make the on-call rotation overlap for an hour or so. This way, the\n\nprevious on-call has time to walk through any operational issues still in flight, and the next on-call person is sure to hit the ground running.\n\nWhile it is certainly true that the on-call people act as a protector of the team’s productivity by being a single point of contact for all interruptions, they should definitely not hesitate to escalate issues to team members when they feel stuck or in need of assistance. They need to be made comfortable to use their best judgement about when investing more time to understand the issue has diminishing returns given the severity of the issue and the expected contribution from another team member who is considered the expert in the area affected by an issue at hand. Operational issues are another realm where pig-headed attempts at heroism are unwarranted, as they might hurt the business. It is much better to admit defeat early and learn from pairing with the expert on resolving the issue.\n\nAsking team members for input is especially important during the triage phase. You want to make\n\nsure that the impact and severity assessment of an unfamiliar issue is correct. In teams with a dedicated Technical Project Manager (TPM), the on-call person can often direct questions about prioritization of work relating to operational issues to the TPM, in case there are competing high-severity issues to be addressed.\n\nWe have seen that it usually takes at most one full rotation of on-call duty through the team until the\n\nbenefits become very obvious for everyone involved. Especially the team members that are alleviated from distractions by the on-call person come to really appreciate the role she takes on. They soon start to develop the necessary empathy for the person currently on call, and they will even start to excuse grumpiness and irritability caused by constant interruptions and operating in fire-fighting mode. Everyone knows that they will be on-call sooner or later, as well, and that they can expect empathy, as well.\n\n227",
      "content_length": 4131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nADVANTAGES OF A SECONDARY ON-CALL\n\nsome teams set up on-call rotations with two developers, one acting as the primary on-call, and another the secondary on-call. while assigning two people to operational emergencies might seem uneconomical to many businesses at first, there are quite a few advantages to take into account when considering the cost-benefit ratio.\n\nFor one, the secondary on-call can often be used as a very effective on-boarding tool for new team members who might still need training. the services that their new team owns are still unfamiliar to them and researching and fixing bugs can often be a very practical and manageable way to get initial exposure.\n\nadditionally, when secondary and primary on-call decide to pair on resolving operational issues, then the benefits are not perceived only by the secondary; many times just having four eyes and two brains dig into an issue leads to faster resolution or at least validation of trains of thought or design decisions.\n\nthe biggest benefit of identifying a secondary on-call person is most likely that he can act as a \"human fail-over\" for the primary. if you can set up your alarming system to escalate to the secondary on-call any critical items not acknowledged by the primary within, let's say, 15 minutes, then your business has implemented better risk mitigation for situations in which the primary on-call is unavailable (for example, when the pager fails to catch the primary on-call's attention).\n\nas a practical tip, teams we were part of in the past have set up their on-call rotation schedules such that the secondary on-calls transition to being primary immediately following their secondary shift. this helps continuity and efficiency for situations in which addressing on-going operational issues span more than one on-call shift.\n\nwe have found having a secondary on-call person especially useful during a team's \"tuning phase,” during which not all services and applications have documented standard Operating procedures. secondary on-calls can use their time taking down, and later publishing, notes about recurring tasks that the primary on-call performs to address operational tasks in the services owned by their team.\n\nsecondary on-calls are also much more likely not to be fully occupied with operational work, so they can dedicate some of their time to feature development and other sprint goals. we still recommend officially not counting on contributions of secondary on-calls; under-committing and then over-delivering is still a better outcome than the alternative.\n\nBeing On-Call in a Microservices Environment While it does not diminish their validity or importance in a microservices setting, any of the previous points about operational support apply more generally than just to microservices. This subsection addresses the finer points that are more directly focused on microservices.\n\nBecause of the extremely distributed nature of a microservices application, it is especially necessary to identify and document the list of all direct service dependencies. Outages of each of these dependencies not only need to be mitigated in the code, but they also need to be tracked, and potentially alarmed on, by teams owning dependent services.\n\nFor that reason, it is very handy for the team’s on-call to have access to the on-call logs and ticket list for all services that the team’s systems depend upon. Additionally, try to set up direct communication channels between team on-calls for related inquiries (for example, support chat rooms for well-known operations).\n\n228",
      "content_length": 3629,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nIn the case of more widespread and impactful outages, it is advisable to spin up dedicated “war rooms,”\n\nwhich can be used to coordinate efforts to resolve ongoing cross-team issues. Note that such war rooms should not be abused to ask for extensive or repeated status updates; while an issue is still “hot,” no focus should be diverted from resolving it first. Status updates should be performed by the on-call person in charge, recorded alongside the ticket inside the ticketing system, and at a prenegotiated frequency (such as every 15 minutes, or even better, based on the issue’s severity).\n\nAnother great trait for an on-call person to have is to be very suspicious and follow hunches. If the metrics graphs on the team’s services dashboard just simply don’t look right, or if a seemingly transient issue remains unexplained, then an on-call person is more often than not well advised to investigate such oddities; more than once in our careers have we seen “icebergs of problems” hiding beneath seemingly small irregularities, so resist the temptation to sweep issues under the rug.\n\nIn a good microservices architecture, failures are kept from propagating by introducing expectations for acceptable response times, error counts, or other sensible business metrics (like sales per hour, and so on.). A team’s on-call person should be particularly wary of adapting alarm thresholds. Make sure to discuss any adaptations of alarming limits with the rest of the team and other potential stakeholders in your business. If your monitoring detects increased failures in services on which you depend, make sure you pass these issues on to the teams that own the dependencies. Insisting on having SLAs, and on adhering to them, is in everyone’s interest, as ultimately they benefit the consumers of your business’s application.\n\nTracking and Scheduling Operations Support Work Even when microservices teams dedicate a (set of) developer(s) to operations support tasks, it is often unrealistic for all operational tasks to be resolved as part of an on-call duty; keeping the service up and running, or responding in adherence to its SLAs, simply outranks issues like version upgrades of frameworks used in the services.\n\nA Lightweight Process Proposal Whenever an operational issue arises that is estimated to take more time than a given low threshold (such as 15 minutes), a trouble ticket should be created. This guarantees visibility and traceability of operational support efforts. This means that reports can be run to identify the services that generate the most operational workload. Such visibility gives the team great input for prioritization efforts: a maintenance- intensive service can often easily justify refactoring work to fix the underlying issues that cause the frequent production issues.\n\nSometimes, when a team inherits a service that they did not develop themselves (for example, as a result of company department reorganization), they have a hard time answering requests or researching issues that relate to the correctness or expectations concerning the inherited service’s behavior. Such situations should be taken seriously, and often the creation of a “research and document” ticket is appropriate.\n\nWhen the tickets generated outnumber the tickets resolved during any given on-call rotation shift,\n\nthe operational workload is carried forward. In that case, it is important that you have a trouble ticketing system in which to track all backlogged operational tasks. The tickets in the backlog need to be tagged with an assessed impact severity level (based on team or company policy).7 Work on the tickets is generally guided by the SLAs associated with the tickets, but for tickets with similar resolution deadlines and severity, prioritization is handled ad hoc with input from other team members.\n\n7The “Defining and advertising team policy” section later in this chapter lays out points to address in a policy for severity assessment and issue prioritization.\n\n229",
      "content_length": 4053,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nAn important role in this could fall to a Technical Project Manager (TPM). A TPM is a person who acts\n\nas the interface between the business departments and the engineering team. The TPM is usually a very technically proficient person (such as a former software developer), who has acquired management skills and is also well informed about the business’s current priorities and future direction. Both the technical and business skills are needed to coordinate the development team’s day-to-day work priorities. The TPM should not just be aware of the team’s project work, but should also have enough information to help clarify an on- call person’s questions regarding the prioritization of trouble tickets.\n\nIn order to schedule work effectively in a microservices development team, it will help to have a tool\n\nin which all tasks currently workable by the team are visible. Such an overall team work items board should identify tasks in progress, as well as backlogged, ordered by priority, and it should include both project work and operational tasks. If your team implements agile software development principles, then such an overall work items backlog can be very useful during sprint planning. For example, we have worked in teams that decided to have an “all hands on deck” sprint dedicated to clearing out a backlog of important operational issues, after explaining the impact of addressing long-standing operational problems to the TPM, who then in turn successfully communicated to the business stakeholders that project work would temporarily be put on hold.\n\nUnder ordinary circumstances, though, all work items in the operational backlog should only be handled by the people on call. As mentioned before, this is important for shielding the rest of the team from distractions, so they can be more focused when contributing feature work. While on-calls should the only people allowed to contact the rest of the team for help with operational issues, they should still use this tool wisely to respect the team’s overall productivity.\n\nFeatures to Look for When Picking a Trouble Ticket System When choosing the right toolset for managing operational support it is important to understand the use cases you envision your organization to have. Once you understand your needs, you can research the features of existing trouble ticket systems that cover your use cases directly, or at least allow for customization. Following are the features we find most important when working with trouble ticketing systems in a microservices organization:\n\nCompany-wide use: Make sure the entire company has access to the list of operational issues, past and ongoing, so that research about outages can be performed without unnecessarily contacting and distracting on-calls. On a related note, we recommend not to expose the internal trouble ticket communications directly to external customers, as you do not want to burden the fire-fighting on-call people with public relations concerns, or matters of inadvertently exposing classified information or hints about security vulnerabilities.\n\n\n\nTracking by severity: A view of the list of issues, with the ability to sort and filter by a user-assigned severity, is essential. The on-call person can use this view to guide her decisions about which issue to work on next, while (internal) customers can understand the severity attached to an issue they are following, and how much workload is placed on the team’s on-call.\n\n\n\nTracking by SLA: Trouble ticket systems need a way to attach expected and agreed- upon deadlines for their resolution. It would be great to make these fields mandatory and potentially even prepopulated based on the ticket’s impact severity. This way, the team can communicate a date to the customers which they can use for their own planning purposes. Additionally, on-call people should be able to sort tickets by proximity to their respective deadlines, so that they can make sure to meet the issue’s resolution timeline based on the team’s published operational SLAs.\n\n230",
      "content_length": 4102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nHelp in identifying issue owner: In an environment with hundreds of services and multiple teams, it is often not self-evident which team’s on-call person should investigate an issue. The trouble ticket tool should be set up to guide the person filing the ticket in identifying the right team that can resolve the cause of the problem. As a corollary, the ticket system should allow for reassignment of issues, in case it was previously misassigned.\n\n\n\nTicket history and searchability: Trouble tickets should never be deleted, even when they are closed or resolved. This helps in being able to search the ticket database during issue analysis and severity assessment. Also, all changes to a ticket’s properties, as well as comments, status updates, changes in assignee or impact severity, and the like, should be logged in an audit trail entry, alongside a timestamp and handle of the acting user. This will help in later reviewing the exact timeline as part of a potentially triggered post-mortem (discussed later in this section).\n\nRelating tickets: It should be possible to relate tickets to each other. In particular, it should be able to tag a given ticket as being blocked due to another, related ticket. The team’s on-call should be notified once the blocking ticket is resolved, so that she can make a decision about continuing work on the now unblocked task. To take this feature further, all internal customers should be able to subscribe to filterable status changes (like comments or updated severity), so that they don’t need to waste time actively checking status.\n\n\n\nIntegration with on-call scheduling tool: In order to correctly notify the team’s current on-call person of new tickets, and all other ticket-related communications, the trouble ticket system should be able to find out which (set of) developer(s) to notify. If your team runs on-call duty via a rotation schedule, then you want to be sure that changes in the rotation are automatically picked up by the trouble ticket system, to avoid delay and confusion in tending to operational issues. Often, the tool that is used to administer the on-call rotation schedule is also in charge of alerting the on-call of high severity issues, and escalating to a potential secondary on-call person, or the team’s manager. In that case, the trouble ticket system needs to be able to notify the on-call rotation system that new high-severity tickets have been filed, or that they are past their SLA date.\n\n\n\nIntegration with project work tracker: We advise that each team have easy access to, and visibility into, all work that the team needs to handle. Having an integration point that relates trouble tickets to tasks inside the project work tracking tool helps to address these needs.\n\n\n\nIntegration with post-mortems: We will explain the detailed contents and processes behind post-mortems later, but for now let’s focus on the fact that outages of a certain severity trigger deeper research into possible actions to take to prevent similar outages from reoccurring. The action items identified in a post-mortem should manifest themselves in trouble tickets with the appropriate owner, severity, and SLA, and status updates need to be visible in the post-mortem tracking tool.\n\nReporting capabilities: Many teams try to improve on dealing with operational workload, and you cannot improve in areas for which you have no metrics and measurements. Reports about new vs. resolved tickets per month, adherence to SLAs, and the services and features that are the heaviest contributors to operational issues are just some immediately obvious examples of reports your team would want to run. Many successful businesses (including Amazon.com) have very high visibility of trouble ticket reports, and the senior leadership level focuses on keeping operational churn low. 231",
      "content_length": 3891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nAnalyzing all available trouble ticket systems is definitely not in the scope of this book, as the range of\n\navailable features in each of them varies significantly. However, Wikipedia has an excellent overview and comparison page for issue tracking systems,8 which not only lists most of systems, but also tries to show an updated feature matrix.\n\nWhile we have worked with a fair number of trouble ticket systems over the course of our work experience, we have not come to find a tool that we could unequivocally recommend. That said, we have worked in organizations that have successfully employed and customized Atlassian’s JIRA and BMC’s Remedy to handle operational issues satisfying the majority of the use cases we’ve discussed.\n\nDashboards to Support Operations Another essential tool for on-call duty is a set of dashboards that graph the behavior of the systems owned by, or of interest to, the on-call’s team.\n\nA service owning team should try to include metrics in dashboards that are based on at least two\n\nsources: data collected on the service-side, and data as seen from the service clients’ perspective.\n\nGood candidates for inclusion in a dashboard that focuses on service-side metrics are, for example:\n\n\n\nEndpoint response times: Add graphs per service API endpoint that track the API’s response time as measured in the service application. Make sure to monitor not just average response times, but also the times in which 95%, 99%, or even 99.9% of all requests are processed.\n\nResponse times per client: Draw the same response times per service API endpoint broken down by the client applications. As before, make sure to visualize not just average values, but also the outliers.\n\nCall volume by endpoint: Seeing the call frequencies for each API endpoint is very useful as a way to detect peak times, and to relate potentially longer response times to a root cause.\n\nCall volume by client: Displaying the API calls on a per-client basis is useful for analyzing sudden drops or surges, as they might indicate a client-side outage or unintended change in behavior. This is also very useful for verifying intended behavior in call patterns, for example, when a migration from a deprecated API to a new service API has been deployed and needs confirmation of having taken effect.\n\n\n\nError responses per code: Seeing the number of error responses returned by a client can give you indications about bad data or other service-side outages.\n\n\n\nError responses per client: If one particular client is affected by most error responses, then it is easier to pinpoint the client in which logic has changed causing bad requests, or requests for data that causes service-side data processing problems.\n\nMessages consumed and published: If your service relies on consuming or publishing messages on an internal message bus, then visualizing metrics about the frequency of messages consumed or published on each of the relevant topics is useful information to observe.\n\n8“Comparison of issue-tracking systems” Wikipedia. https://en.wikipedia.org/wiki/Comparison_of_issue- tracking_systems\n\n232",
      "content_length": 3158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nBusiness metrics: Tracking business metrics around purchase rates, number of new customers, number of comments or reviews, and so on gives very expressive feedback and can help uncover problems that remain undetected when simply tracking the proper functioning of the technology involved. They can also help track down the root cause for changes in other metrics (like call volumes or service response times), or provide good indications for when planned outages or feature launches can be least impactful or risky (for example, to launch changes to the comment-service between 2AM and 4AM Eastern time because business metrics show that this is the period when the fewest customers post comments on the site).\n\nFigure 14-1 shows operational metrics for an internal service in LivingSocial’s infrastructure. The\n\nmetrics shown are extracted from service-side logs, aggregated and graphed using Splunk’s operational intelligence and log management tool.\n\nFigure 14-1. Splunk dashboard showing service-side log-based real-time metrics\n\nClient-side metrics that can help track down issues with a service can include:\n\nClient-side error code data: Tracking success and failure rates of requests to the service inside a particular client can be used to alert a team to outages in a dependency, or indicate that a client-side library update did not go as expected.\n\nClient-side performance data: The service performance data that matters the most is how it is perceived inside the client. Clients care deeply about the SLAs of their service dependencies, as their own responsiveness often is directly tied to their dependencies being performant.\n\nObject deserialization costs: Wrapping code that turns the wire format of transmitted data back into programming language objects might indicate that perceived service slowness is in fact more accurately rooted in slow deserialization in the client, which might be fixed by requesting smaller response objects from the service. 233",
      "content_length": 2028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\n\n\nTimeouts: While dependency timeouts should not lead to cascading issues, they are a welcome indicator for why the clients’ own functionality might degrade.\n\nCircuit breaker status: Getting an overview of all open circuits for a client can help to identify which circuit settings might be overly stringent, or which dependency teams should be consulted about SLA improvements.\n\nThe graphs shown in Figure 14-2 are part of a dashboard that displays operational data collected in\n\nthe client library of the internal service shown in Figures 14-1. The data was collected using StatsD (https://github.com/etsy/statsd), and the dashboard tool Grafana (http://grafana.org/).\n\nFigure 14-2. Grafana dashboard for client-side metrics based on StatsD held data\n\nOne reason for the usefulness of dashboards is that automated alarms to alert the team of exceptional behaviors are great when they work, but monitoring and alarming automation is never complete. Human intelligence often notices trends earlier and more reliably than algorithms do, and therefore can aid in detecting holes in alarming.\n\nSome teams even go so far as to display the most important dashboards on large screens visible from\n\ntheir work and break room areas. The rationale for featuring these live metrics so prominently is that the cost involved is a good investment in raising awareness of how the system dashboards look under normal circumstances. Humans are incredibly efficient at pattern matching even when subconsciously observing visual clues, and we have seen engineers infer operational aberrations on numerous occasions simply by looking at out-of-the-ordinary patterns in graphs they regularly observe. As mentioned above, being suspicious and not shrugging of irregularities is an essential trait of someone taking on-call duty seriously.\n\nOutage Etiquette No matter how good your development and test processes are, outages will happen. The real question will be how quickly your team will be able to recover from the failure of a launched system. This section covers tuning your process for dealing with outages and working preparedness for these cases into your team’s routines.\n\n234",
      "content_length": 2220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nEvery Pager Alarm is a Failure An organization is well advised to consider every occasion when a pager goes off—and a person is potentially woken up—as a failure whose root cause needs fixing. An intrusive alert should not happen unless it is unavoidable because the underlying problem has not been identified and addressed.\n\nIf you start seeing every instance of a pager alerting as an exception to root out, then you do not run the\n\ndanger of simply accepting it as a fact of life as an on-call. You will start to see delaying the true resolution as the only unacceptable course of action. Sweeping the issue under the rug via some rationalization you or someone else comes up with cannot be acceptable.\n\nThe very least you should do is to analyze whether the problem is truly worth alarming someone about,\n\nand you should not take the research lightly. If you and the team determine that the problem was not truly worth waking up someone, then you need to find ways to make the alarming rules smarter to improve on discerning between relatively benign issues and issues that cause a pager to alert.\n\nIf you do not address such “false positives” in your alarming, you will see that fatigue and indifference\n\nto alarms will set in, and truly impactful exceptions run the danger of being ignored. Additionally, having overly sensitive alerting rules that require your person on call to investigate in a very stressful and disruptive setting will burn out your developers, and you will see increased levels of attrition.\n\nIf your company implements a policy that humans can manually trigger pager alerts—for example, when they have reason to believe they noticed a significant outage that has not yet been brought to anyone’s attention—then you need to set clear expectations about which class of issues are “pageable.” Therefore, a team—or even better, company—policy for operational support and SLAs is essential to implement. We will discuss the contents of such a policy later in this chapter. It should be made clear that, for example, requests for information regarding behavior of a system or the existence of, or implementation request for, a given feature are not acceptable reasons to page an engineer.\n\nOn the other hand, engineers need to keep a professional attitude when being paged by a colleague,\n\nirrespective of the time of day or night the pager rings. Ask the reporter for enough details needed to investigate and reproduce the issue. It is very reasonable to require that a ticket exist for any issue that is worth being paged about, so make sure you insist on recording all information as comments in the trouble ticket. If the issue is serious enough, based on criteria also defined in your operational SLA policy, then such information will be essential when you have to create a post-mortem.\n\nPlaybooks for Operational Support Imagine you are the team’s on-call person, and your pager wakes you up at 3:00 AM. You are trying to adjust, wake up and not well enough rested to have all your mental capacity.\n\nWe have seen many teams address this problem by writing down Standard Operating Procedures (SOPs) in the form of step-by-step recipes, useful links, checks to perform or investigations to start. These are meant to be very explicit and easy to follow, so they can be consulted by an on-call person who is new, stressed out, or otherwise not in the best state of mind to analyze the root cause of an outage.\n\nHere are a few things we encourage including in SOP playbooks:\n\n\n\nLinks to operational dashboards: Quick access to an overview page for all operationally relevant metrics is very useful for analyze root causes of issues.\n\n\n\nLinks to access and error logs: Looking at error traces and client access patterns is often invaluable when trying deal with an outage.\n\nContact information for owner team: Links to communication channels (such as a Slack channel) where to discuss outages, contact information for subject matter experts.\n\n235",
      "content_length": 4026,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nArchitectural overview: Adding overview diagrams for the service’s architecture and its main component and dependencies helps tremendously when attempting to reason about the cause for an outage, or where to best place a patch.\n\n\n\nIf-then scenarios: This section lists known issues or previously experienced outages that could recur, keyed by how such issues usually manifest themselves, plus a checklist for verifying that the issue is encountered, and steps for fixing or mitigating. This is especially useful if it includes listing investigative steps for researching business alarms.\n\n\n\nList of service dependencies: In a massively distributed microservices environment it is unfortunately rather common that an on-call person receives an alarm that is caused by degradation of a service dependency. While circuit breakers should prevent cascading failures, it is still essential that the dependent team know about the situation. Links to trouble ticket boards of all service dependencies and the contact details for all service dependencies’ on-call people should be easily found.\n\nHealth-check URLs: Verifying that an issue is no longer present, or helping to reproduce an exception, include URLs or cURL statements for every feature or API endpoint in your service application.\n\n\n\nProduction server access details: Sometimes problems can only be tracked down or reproduced by having direct access to the production hosts, where data or server condition can be checked or a service console needs to be opened. This also includes access to time-based jobs (such as crontasks) and their logs.\n\n\n\nFAQs: Answering often-asked questions about the way some functionality works can be made much more efficient when listing them in an easy to find section on the SOP page. Also, such a section can list recipes for common administrative tasks, for example, modifying data not exposed by an API, running diagnostics queries on a secondary database, or whitelisting new client applications.\n\nNote that such playbooks are only useful if they are followed often, and maintained and updated when found outdated, incorrect, or lacking. Whenever you find them in such a state, either directly update, or add to them as part of following these instructions, or create a support ticket to take care of it in the near future.\n\nWhat to Do When the Pager Goes Off In a team that takes addressing and fixing operational issues seriously, no two outages are exactly the same, simply because the previous outage was fixed in a way that will not allow it to recur. It is still worth providing some general proven rules and behavioral patterns that are appropriate when being alerted about an urgent production issue.\n\nFirst and foremost, there is no use in falling prey to the temptation of panicking. Being informed by a very loud alert about an impactful outage certainly gets your blood flowing, but that should only go so far as making you aware that your undivided attention is needed to investigate the issue. Dealing with production system failures needs to take the highest precedence for you and the rest of your team, so if you feel like you are stuck or ineffective in fixing the issue, then this is an “all hands on deck” situation, and the entire rest of your team can be pulled in to help.\n\nNext, make sure that you acknowledge the alarm, so and others do not get unnecessarily alerted. Getting alerted over and over is just annoying and distracting, and consequently counterproductive to quick resolution of the problem at hand. In many pager-duty support systems, tickets escalate automatically when not acknowledged, so be respectful of other people on the escalation path.\n\n236",
      "content_length": 3733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nOnce you have acknowledged the alert, create a trouble ticket (unless it already exists) and do just as much research as it takes to be reasonably confident you know enough to either fix the root cause, or improve the situation significantly. If you find a quick route to improve conditions, then take it! This often buys you time to work on a more complicated fix in a less impactful setting.\n\nIf you are in the unfortunate position that you get woken up by an alarm, then feel free to patch as little\n\nas necessary to address the immediate problem, but do make sure to work on a more permanent fix once you are fully rested.\n\nWhile you research root cause and potential fixes, take a small amount of time to update the trouble ticket whenever there is news to share. Do not feel obligated to answer questions about status or progress while you work on researching or otherwise addressing an issue. If the inquiries become distracting, inform the person inquiring politely that answering questions distracts you from resolving the issue. Most teams make it a point in their team policy for operation support to mention intervals in which an in-progress trouble ticket of a particular severity is updated. Try your best to adhere to such a published SLA, but favor acting on fixing the issue over updating the ticket.\n\nIf, on the other hand, you need help or information from other on-call people or other team members,\n\ndo not hesitate to reach out to them. Unless they themselves are busy dealing with issues of the same or higher operational severity, they should make every effort to help you.\n\nIt is also important that in your role as on-call person you refrain from passing blame while an issue is being actively worked. The time to assess what went wrong and how it can be prevented from recurring is after the issue is no longer causing production issues. A post-mortem, which is discussed in the next section, is the best place for such analysis.\n\nSupporting Tools There are quite a lot of choices for tool support for the above-mentioned measures to establish a well- functioning operational emergency protocol.\n\nSOP pages and playbooks should first and foremost be easily found. If your company has not already\n\nlaunched an internal tool to enable teams to self-publish documentation, then it is definitely worth evaluating tools like Atlassian’s Confluence (https://www.atlassian.com/software/confluence) or a self-hosted Wiki (see http://www.wikimatrix.org/ for a site that lets you compare features of Wiki implementations).\n\nAs far as incident management and escalation tools go, we have personally had experiences with PagerDuty\n\n(in use at LivingSocial) and a heavily customized version of BMC’s Remedy (in use at Amazon.com). Other alternatives include VictorOps (https://victorops.com/), OpsGenie (https://www.opsgenie.com/), and the open-source tool Openduty (https://github.com/ustream/openduty).\n\nThe main features we have found indispensable for such a tool to be effective are:\n\n\n\nAn ability to acknowledge an incident and hence stop further alerting for it\n\n\n\nBeing able to de-duplicate incidents, so that no more than one alarm is generated for the same issue\n\n\n\nThe ability to integrate with a trouble ticket system for automatic ticket creation and relation\n\n\n\nThe ability to maintain an on-call rotation schedule that allows correctly routing alarms to team members based on the current rotation shift\n\nAnother valuable feature, albeit in the nice-to-have category, is to offer integration points to group\n\ncommunication tools like email, Slack, or Campfire to alert all stakeholders of incident status.\n\n237",
      "content_length": 3694,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nHow to Learn from Outages:—Post-Mortems The best engineering organizations strive to achieve operational excellence by learning from past outages. A post-mortem can be a powerful instrument to achieve this goal.\n\nPost-mortems—originally used in the context of medical examinations otherwise known as autopsies—\n\ngenerally refer to the process of assessing successes and failures of a task or process after its conclusion. In the realm of on-call duty and operational excellence, their main purpose and value is to help with identifying lessons learned from an incident with a large impact.\n\nWhen Is a Post-Mortem the Right Tool? Good indicators for the usefulness of a post-mortem are that:\n\n\n\nThe issue that occurred, or could have occurred, is severe.\n\n\n\nThe occurrence of the issue appears preventable at first sight.\n\n\n\nThe root cause for the (near-)outage appears to need more research.\n\n\n\nThe issue analysis is worth sharing with a wider audience or preserving for posterity.\n\nWe recommend creating a post-mortem for every high-impact incident of a severity level that is negotiated within the engineering organization. As a rough guideline, you should discuss the value of creating a post-mortem each time your team’s on-call person gets paged, or should have been paged.\n\nAdditionally, it should be acceptable for anyone in the company to call for creation of a post-mortem, if she can argue that there is enough value to the business to outweigh the costs of going through the motions of a full post-mortem.\n\n■ Note try to make sure that post-mortems are not understood as punitive in your organization. it needs to be made clear that the reason for the effort involved is the chance to improve on what you do, and to prevent further operational issues.\n\nStructural Elements of a Post-Mortem While there is no globally accepted table of contents for a post-mortem, we strongly advise that your engineering organization adopt a template that is shared across all teams. The following list outlines such a template, listing the contents we suggest discussing as part of every post-mortem:\n\nAction summary: This lead-in contains a very brief, “executive summary” description of the outage that was dealt with and the actions that were taken to address the issue.\n\nBusiness impact: A very succinct, to-the-point synopsis of the loss to the business is mentioned in here, with calculations that are as accurate as currently possible. The loss could be in various areas, for example, negative revenue impact, exposure of sensitive data, or loss of customer trust.\n\nActors: This section should include all people involved in causing or fixing the issue discussed.\n\n238",
      "content_length": 2726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\n\n\nTurn of events: A post-mortem should lay out a detailed and accurate timeline of all relevant events, starting from the initiation of the outage (such as the deployment of unsafe code artifacts), all the way to the (potentially temporary) resolution of the underlying issue. All significant actions and findings should be included and attributed.\n\n\n\nSuccesses: In this part, the author of the post-mortem lists actions and practices that contributed to lessening the impact of the outage.\n\nRoot cause analysis: This section starts out with a more in-depth technical explanation of the cause of the issue and continues by analyzing the actions and circumstances that contributed to causing the outage. We highly recommend using the \"Five Whys\" technique to aid in these efforts (see the accompanying sidebar).\n\n\n\nLessons learned: Areas for improvements are identified and listed in this part, and they should be based on the results of the root cause analysis.\n\nAction items: The points listed in this section are concrete, actionable improvements. They should directly relate to the cause(s) of the outage, and they should explain how they help to fix the identified root of the problem.\n\nTHE FIVE WHYS9\n\nthe Five whys symbolize a strategy for identifying the true cause-and-effect relationships at the heart of a problem, in an attempt to be sure which steps to take to truly fix the issue. at each stage of the ever-deeper drilling chain of questions about causality, the current level asks \"why?\" referring to the cause of the previous step; that is, each stage forms the basis of the next step. the significance of the Five in the technique’s name comes from the experience that—for the majority of problems initially researched—asking “why?” down to the fifth level typically reveals the end of the causality chain.\n\nexample of a Five Whys analysis:\n\nWhat happened? the laptop computer did not start up.\n\n1. Why (did the laptop fail to start up)? the battery is dead.\n\n2. Why (is the battery dead)? the charger for the computer is not functioning.\n\n3. Why (is the charger not working)? the power cable has broken.\n\n4. Why (is the power cable broken)? the power cable was often rolled up, and hence friction caused it to disintegrate.\n\n5. Why (was there a lot of friction)? the charger has a design that exposes sharp edges close to where the cable is meant to be fastened.\n\nFirst used within the toyota Motor Corporation, the Five Whys has been adopted by technology and engineering companies like amazon.com. its power lies in its simplicity and straightforwardness when trying to determine the true causes of problems.\n\n9See “5 Whys,” on Wikipedia, for more details about the origin and history of this technique. https://en.wikipedia. org/wiki/5_Whys\n\n239",
      "content_length": 2821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nPost-Mortem Ownership Every post-mortem has exactly one owner. The owner is not necessarily the same person or team who will work on addressing all action items identified as part of the post-mortem. Usually this role is filled by the author of the post-mortem, but often we seen the author’s manager take on ownership of all post-mortems authored by his team.\n\nThe owner of the post-mortem is tasked with helping to identify and track all resulting action items. She\n\ncreates trouble tickets for each of them, each of which has a deadline and a ticket owner (or owning team) associated with it.\n\nThe owner will chase all tickets to completion by the assigned teams, and then she will resolve the post- mortem itself. She will also notify all parties involved of the resolution, and will make sure that the finished document is stored in a searchable repository, so that the knowledge gained is usable in the resolution of future issues.\n\nWhile this process of following up and coordinating post-mortem action item resolutions can be time-\n\nconsuming, it is nevertheless of utmost importance to make the process useful; if follow-through is not taken seriously, then the post-mortem process is mostly ineffective and should most likely be dropped entirely, as it will then only incur costs for very little value in return.\n\nDefining and Advertising Team Policy A lot of what makes an organization achieve efficiency and excellence in operational matters is rooted in the quality of the processes and policies they apply to support the systems they own. This section will lay out which points should be addressed in a policy for operational excellence, and we will give examples of policy points we have seen work well in practice.\n\nImpact Severity and SLAs In order to improve focus and enable the on-call person to make decisions about which operational issue to work on at any given point in time, it helps to categorize trouble tickets by level of severity.\n\nTo do that, define a handful of distinct levels of severity and classify each of them with its level of\n\nimpact. Additionally, assign a priority level to each severity level, so that client expectations can be set about the on-call person’s attention to incidents in each severity category. For clarity, it usually also helps to add examples of outage situations that would fall into the specified severity and impact levels.\n\nFigure 14-3 shows an example of how such incident severity, impact, and priority levels could be\n\ndefined.\n\nFigure 14-3. Definitions for issue impact and severity, with examples and priority levels\n\n240",
      "content_length": 2647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nOnce you have defined the severities, your customers need to know what level of service they can expect from your team. As a customer-focused team, you should set expectations for, and facilitate, communication with your customers. SLAs are an essential tool to achieve such customer focus, and they will also let you know when your on-call process is working and, more importantly, when it is not.\n\nAn example for a team SLA, based on incident severities, is shown in Figure 14-4.\n\nFigure 14-4. SLA for first contact, updates, and resolution of incidents by severity level\n\nThe meaning of first contact in the SLA definition is the most important part of the issue lifecycle, next\n\nto resolution. This initial contact informs the client that the issue has been acknowledged. Without such an entry in your SLA, the wait time can be frustrating for your clients. With a defined SLA, it becomes clear when follow-up will be warranted. In addition to reducing client uncertainty, the SLA reduces unnecessary and inefficient back-and-forth, allowing the on-call person more time to focus on issue resolution.\n\nSimilar arguments hold for the update SLAs. Sometimes, resolving a trouble ticket can take the on-call person several days. Updating a ticket that is in progress serves a similar purpose to the first contact, as each update shows the client that their issue has not been abandoned. Updates that simply state that the efforts continue to be blocked still demonstrate strong ownership. By keeping your customers abreast of progress, anxiety and uncertainty is reduced.\n\nAlso advertise which type of incidents you consider worthy of analyzing in a post-mortem. You can use the severity level as an indicator (for example, all incidents of severity 1 and 2 trigger post-mortems), or you could take the stance that they apply mainly to more systemic issues that need to be addressed. As we have shown earlier in this chapter, a post-mortem is an effective tool to address process failures for systemic issues.\n\nPublishing an SLA for issue resolution is the most important step for your clients, as it sets expectations\n\nthey can rely on. If this underlying problem is blocking work on the client side, this expectation can help your customers identify risks in their own roadmaps.\n\nWhile it is first and foremost the team’s responsibility to set the SLAs and severities, the results need to be acceptable to the clients and other stakeholders in your company. Deciding whether an SLA or a severity judgement needs to be adapted is up to negotiations between your team and the internal customers, but it is very important to publish your policy up front.\n\nShare the Steps of Your Process Publishing the details of how your team goes about processing your operational issues helps clients to understand each phase involved in resolving an incident.\n\n241",
      "content_length": 2908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "Chapter 14 ■ OperatiOnal exCellenCe with MiCrOserviCes\n\nA sample list of steps could be as follows:\n\nGeneration: When an operational issue for one of the systems owned by the team arises, a trouble ticket will be created and assigned to the correct on-call person. Make sure to advertise how to find the correct group in the trouble ticket system so that the ticket does not get misassigned and therefore might linger longer than needed.\n\n\n\nTriage: The team’s current on-call person will investigate the issue to a level where he can assess its severity, and an appropriate label (such as SEV-3) will be added to the ticket.\n\n\n\nProcessing: The on-call person is in charge of deciding which issues take priority, and hence will be worked at any given point in time. Decision guidelines for the on-call are based on issue severity and published SLAs, along with feedback from other team members and other engineers. All communication about a ticket’s cause, technical details, and status will be captured in ticket comments. This mainly serves historical and documentation purposes. External inquiries about status should be kept to a minimum in order to avoid distracting the on-call from actually working on the issue’s resolution. In return, the on-call aims to update ongoing issues based on the published SLAs, so that ticket stakeholders can be kept abreast of developments.\n\nResolution: The team will strive to adhere to the published resolution SLAs, and it will alert stakeholders with as much advance notice as possible about any risks that could cause such deadlines to slip. Issues of severity 1 and 2 should automatically result in a post-mortem.\n\nFinally, it is within each team’s ability to customize its on-call policy, but strive to streamline the policies, process and SLAs across the entire company. This will help internal clients to have confidence that their issues will be addressed, that no time is lost trying to contact owners, and they can set their own customers’ expectations correctly based on a generally accepted the process.\n\nSummary This chapter focused on all aspects of what it means to be part of a team that works under the “you build it, you run it” premise. After defining what operational support means, we explained what comes along with the obligations of true ownership for microservices.\n\nNext we looked at the role of being on call for the services owned by your team. We discussed\n\nresponsibilities for the on-call person, in relation to the rest of the team, as well as when serving client teams that rely on the software you own.\n\nWe also gave tips for how to organize and prioritize operational workload compared to the project work\n\nrequests handled in a microservices team, and we discussed important features for tools that are meant to support the team’s efforts to track both types of work.\n\nAfter a brief examination of operational dashboards, we gave an overview of a practical approach to\n\nhandling system outages when being the on-call person in charge.\n\nWe showed the importance of learning from outages, and walked through the process of analyzing such\n\nincidents using post-mortems.\n\nWe concluded this chapter by discussing the topic of defining and sharing a policy of how your team handles operational ticket work, and how to best set client expectations using SLAs and incident severity definitions.\n\n242",
      "content_length": 3367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Index\n\n A Aggregation\n\nbookstore app\n\napplications and aggregated services, 34 decomposing, core services, 33 feature-focused services, 31–32, 35–36 partitioning, 32 supporting platform services, 34–35\n\nbusiness intelligence, 28–29 derived functionality, 28 system performance, 29–30 vs. decomposition, 30\n\nAPI design\n\nAPI optimization caching\n\nadvantages, 86 API-provided data, 87 cache-control header, 89 definition, 86 ETag and If-None-Match headers, 88 HTTP cache control, 89–92 HTTP standard, 87 if-modified-since headers, 88 last-modified headers, 88 proxies, 92 catalog-service, 100 development and support, 98–99 performance issues\n\nAmazon Web Services, 38 app, home page, 49 breaking changes, 45–46 definition, 37 guidance, 41 information hiding, 44–45 nonfunctional requirements\n\navailability, 42 capacity, 43 performance, 43 recoverability, 43 reliability, 43 scalability, 43 service-level agreements (SLAs), 42\n\nbusiness logic profiling, 84 database related inquiries, 83 memory allocations and CPU cycles, 84 service dependency, 84\n\npublish and subscribe pattern, 100 service access pattern, 85 services size\n\nDenial of Service attacks, 97 HTTP compression, 95 partial resources pattern, 94 result representations, 93–94 semantic paging, 96\n\nnumbering scheme, 48 online store\n\nAutomation, 151 Autoscaling, 172 AWS CloudFormation, 161\n\napp, home page, 50 book detail page, 52–54 cart information, 50–51 images, 56–57 reviews, 55–56 user recommendations, 51–52\n\n B Backend for Frontend (BFF) pattern, 86 Blue-green deployment, 162\n\nprinciple, 45 properties, 39–41 requirement setting, 43 version, 47–49\n\n C Cache-control header, 89 Canary release, 163–164\n\n© Cloves Carneiro Jr. and Tim Schmelmer 2016 C. Carneiro Jr. and T. Schmelmer, Microservices From Day One, DOI 10.1007/978-1-4842-1937-9\n\n243",
      "content_length": 1836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "Client libraries caching, 78 error handling\n\ncircuit breaker, 79–80 timeouts, 79\n\nfeatures, 75–76 performance, 80 service discovery, 81 service orchestration, 78 practices to avoid, 76–77 update, 81 ClojureScript, 181 Consumer-facing applications, 19 Continuous delivery (CD), 154 deployment step, 154–155 litmus test, 154 microservices, 156 pros and cons, 156 tool support, 156–159\n\nContinuous integration (CI), 116–117\n\nadvantages, 152–153 microservices, 153 Contract testing, 147–150\n\n D Decomposition\n\nchange feedback, 108, 123 codifying internal standards, 125 collaborative development, 107 concurrent and collaborative development, 123 consistently named scripts, 109 consistent start-up scripts, 111 Continuous Integration (CI), 116–117 data sources, 118 developer machines, 107 docker and docker compose, 110–111 documentation search, 126 downstream services\n\nclient-side samples, 119–120 connecting, shared data store, 120 mock services, 120\n\nguard, 114 privacy, 107 process and documentation, 112 production snapshots, 119 proxy service, 124–125 sample data availability, 106 sandbox fabric, 121 saving resources, 122 scripts, 118 service dependencies, 122 service generation, 108 shared data store, 119 testing\n\nvs. aggregation, 30 bookstore app\n\napplications and aggregated services, 34 core services, 33 feature-focused services, 31–32, 35–36 partitioning, 32 supporting platform services, 34–35\n\ncode guidelines, 113–114 common tools, 112 integrating static\n\nanalysis tools, 115–116\n\ntest framework, 112\n\ntool setup, 106\n\ndata-access pattern, 27 decomposing by maturity, 26 decomposition by context, 27–28 functionalities, 25–26\n\nDeployment and operational aspects\n\nEAS, 167–168 locality of configuration, 167 LXC with Docker, 170–171 microservices architectures, 165 process running options, 166 single microservice vs. multiple microservices, 166\n\nDocumentation search, 126, 180\n\n E, F End-to-end tests, 145–146 Enterprise application server (EAS), 167–168 Enterprise integration patterns, 100 Error handling\n\ncircuit breaker, 79–80 timeouts, 79\n\nETag and If-None-Match headers, 88\n\nvirtual machines (VMs), 168–170\n\nDevelopment environment\n\naccessibility, 108 application dependencies, 106 application test automation, 106 automating application set-up, 109 automating developer machine, 109\n\n G Gerkhin tests, 142–144\n\n H Hypermedia type, 40\n\n244",
      "content_length": 2400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": " I, J, K If-Modified-Since header, 87 Interface Definition Languages (IDLs)\n\n O Obidos, 6 Operational support\n\nadvantages, 59 REST, 60–61 RPC, 60\n\ncustomers oriented, 222 dashboards\n\nclient-side metrics, 233–234 sources, 232\n\n L Languages and technologies\n\nFive Whys analysis, 239 mapping, 223 on-call\n\nevolution, 179 implementation, 179 specialization, 178 Last-modified header, 87\n\nadvantages, 228 microservices environment, 228 team efficiency, 226 team relationship, 227\n\n M Message-based approach, 101 Microservices\n\non call, role, 224 pager alerting, 235 pager-duty support systems, 236 playbooks, 235–236 post-mortem\n\nadvantages, 9–10, 12, 18 anti-patterns, 15–16 architecture, 10–11 characteristics, 5 definition, 4–5 disadvantages, monolith, 11 industry adoption\n\nAmazon.com, 6 Google and eBay, 7 Netflix, 7–8 Obidos, 6 SoundCloud, 8–9, 12\n\nprerequisites, 14 “services first” approach, 17 SOA, 15–16\n\nMigrating to Microservices, 15 Monitoring service\n\nownership, 240 structural elements, 238–239 usefulness, 238\n\npriority-item, 225 production issues and triage, 225 service-side metrics, 232–233 shared ownership, 223 supporting tools, 237 team cohesion, 223–224 team policy\n\nimpact severity\n\nand SLAs, 240–241 issue definitions, 240 process steps, 242 tracking and scheduling lightweight process proposal, 229–230\n\nAdjusting Alarms, 218 average response time, 216 business matrics, 213 endpoint’s performance, 217 logging tools, 215 perceived performance, 212 requests identification, 214 SLAs, 213 slow response issue, 217 tools, 211–212 uniform log format, 215–216\n\nMonolith, 13\n\ntrouble ticket systems, 230–231\n\ntrust making, 222\n\nOut-of-process hyperlink, 131–132\n\n P, Q Polyglot service\n\nbootstrapping decisions, 179 caching layer, 177 deployment issues, 179 documentation, 180 implementation planning, 181 rails, 182\n\n N Netflix, 7–8 Node.js, 181\n\nPre-production environment\n\nsafer releases, 162 staging fabric, 160 tool support, 161\n\n245",
      "content_length": 2008,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": " R RPC vs. REST, 60 Rules of engagement API definition, 21 consistently, 21 consumer-facing applications, 19 existing code bases, 22 interfaces, 20 service access, 20 service creation steps, 20\n\nhosting, API JSON\n\nspecification, 62–65\n\nJSON specifications, 71 resource and paths definitions, 65\n\nUser Interface (UI)\n\nGET /iusers/{id}, 67 users-service, 65–66\n\nyard description\n\nUser API model, 74 UserController, 72–73\n\n S Serverless architecture, 172–173 Service-level agreements (SLAs), 42 Service orchestration, 78 Service-oriented architecture (SOA), 3 Single-client application\n\n T Team Policy\n\nprocess steps, 242\n\nTest-driven development (TDD), 118 Testing\n\narchitecture, 185–186, 208 bookstore\n\nbook detail page, 197–202 circuit breakers, 197 class, 188 featured categories, 192, 194 home page, 186–187 internal error avoidance, 195–196 json_api_client, 187 purchases-service, 189, 191–192 shopping cart widget, 203–204, 206\n\nperformance considerations, 208–209\n\nAdminstration Web UI, 142 book test stubs, 139 contract testing, 147–150 end-to-end tests, 145–146 in-process hyperlink, 131 layers, 128 out-of-process\n\nhyperlink, 131–132, 141 pluggable back-ends, 135–138, 141 stubbing, 134–135 stubs customization, 140 test Set-Up, 140 unit testing, 128–129\n\nSoftware services, 4 SoundCloud, 8–9, 12 Swagger\n\nAPI JSON specification, 61, 62 blocks description, 71–72 code generation\n\nClojure client library, 70–71 Ruby client gem, 68–70\n\n U Unit testing, 128–129\n\n V, W, X, Y, Z Virtual machines (VMs), 168–170\n\n246",
      "content_length": 1564,
      "extraction_method": "Unstructured"
    }
  ]
}