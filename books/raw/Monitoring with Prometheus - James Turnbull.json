{
  "metadata": {
    "title": "Monitoring with Prometheus - James Turnbull",
    "author": "James Turnbull",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 394,
    "conversion_date": "2025-12-19T17:40:30.666431",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Monitoring with Prometheus - James Turnbull.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Introduction",
      "start_page": 18,
      "end_page": 58,
      "detection_method": "regex_chapter",
      "content": "Chapter 1\n\nIntroduction\n\nThis book is an introduction to Prometheus, an open-source monitoring system. Prometheus provides real-time collection of time series data from your applica- tions backed by a powerful rules engine to help identify the information you In the next chapter we’ll introduce you to need to monitor your environment. Prometheus and its architecture and components. We’ll use Prometheus in the book to take you through building a monitoring environment, with a focus on monitoring dynamic cloud, Kubernetes, and container environments. We’ll also look at instrumenting applications and using that data for alerting and visualiza- tion.\n\nThis is also a book about monitoring in general—so, before we introduce you to Prometheus, we’re going to take you through some monitoring basics. We’ll go through what monitoring is, some approaches to monitoring, and we’ll explain some terms and concepts that we’ll rely on later in this book.\n\nWhat is monitoring?\n\nFrom a technology perspective, monitoring is the tools and processes by which you measure and manage your technology systems. But monitoring is much more\n\n6\n\nChapter 1: Introduction\n\nthan that. Monitoring provides the translation to business value from metrics generated by your systems and applications. Your monitoring system translates those metrics into a measure of user experience. That measure provides feedback to the business to help ensure it’s delivering what customers want. The measure also provides feedback to technology, as we’ll define below, to indicate what isn’t working and what’s delivering an insufficient quality of service.\n\nA monitoring system has two customers:\n\nTechnology • The business\n\nTechnology as a customer\n\nThe first customer of your monitoring system is Technology. That’s you, your team, and the other folks who manage and maintain your technology environment (you might also be called Engineering or Operations or DevOps or Site Reliability Engineering). You rely on monitoring to let you know the state of your technol- ogy environment. You also use monitoring quite heavily to detect, diagnose, and help resolve faults and other issues in your technology environment, preferably before it impacts your users. Monitoring contributes much of the data that in- forms your critical product and technology decisions, and measures the success of those projects. It’s a foundation of your product management life cycle and your relationship with your internal customers, and it helps demonstrate that the business’s money is being well spent. Without monitoring you’re winging it at best—and at worst being negligent.\n\n NOTE There’s a great diagram from Google’s SRE book that shows how\n\nmonitoring is the foundation of the hierarchy of building and managing applica- tions.\n\nVersion: v1.0.0 (427b8e9)\n\n7\n\nChapter 1: Introduction\n\nThe business as a customer\n\nThe business is the second customer of your monitoring. Your monitoring exists to support the business—and to make sure it continues to do business. Monitoring provides the reporting that allows the business to make good product and tech- nology investments. Monitoring also helps the business measure the value that technology delivers.\n\nMonitoring fundamentals\n\nMonitoring should be a core tool for managing infrastructure and your business. Monitoring should also be mandatory, built, and deployed with your applications. Without it you will not be able to understand the state of your world, readily diagnose problems, capacity plan, or provide information to your organization about performance, costs, or status.\n\nAn excellent exposition of this foundation is the Google service hierarchy chart we mentioned earlier.1\n\n1Site Reliability Engineering, edited by Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall\n\nRichard Murphy (O’Reilly). Copyright 2016 Google, Inc., 978-1-491-92912-4.\n\nVersion: v1.0.0 (427b8e9)\n\n8\n\nChapter 1: Introduction\n\nFigure 1.1: Service hierarchy\n\nBut monitoring can be hard to implement well, and it can very easily be bad if you’re monitoring the wrong things or in the wrong way. There are some key monitoring anti-patterns and mitigation:\n\nMonitoring as afterthought\n\nIn any good application development methodology, it’s a good idea to identify what you want to build before you build it. Sadly, there’s a common anti-pattern\n\nVersion: v1.0.0 (427b8e9)\n\n9\n\nChapter 1: Introduction\n\nof considering monitoring, and other operational functions like security, as value- add components of your application rather than core features. Monitoring, like security, is a core feature of your applications. If you’re building a specification or user stories for your application, include metrics and monitoring for each com- ponent of your application. Don’t wait until the end of a project or just before deployment. I guarantee you’ll miss something that needs to be monitored.\n\n TIP See the discussion about automation and self-service below for ideas on\n\nhow to make this process easier.\n\nMonitoring by rote\n\nMany environments create cargo cult monitoring checks for all your applications. A team reuses the checks they have built in the past rather than evolving those checks for a new system or application. A common example is to monitor CPU, memory, and disk on every host, but not the key services that indicate the appli- cation that runs on the host is functional. If an application can go down without you noticing, even with monitoring in place, then you need to reconsider what you are monitoring.\n\nA good approach to your monitoring is to design a top-down monitoring plan based on value. Identify the parts of the application that deliver value and monitor those first, working your way down the stack.\n\nVersion: v1.0.0 (427b8e9)\n\n10\n\nChapter 1: Introduction\n\nFigure 1.2: Monitoring design\n\nStart with business logic and business outputs, move down to application logic, and finally into infrastructure. This doesn’t mean you shouldn’t collect infrastruc- ture or operating system metrics—they provide value in diagnostics and capacity planning—but you’re unlikely to need them to report the value of your applica- tions.\n\n NOTE If you can’t start with business metrics, then start monitoring close\n\nVersion: v1.0.0 (427b8e9)\n\n11\n\nChapter 1: Introduction\n\nto the user. They are the ultimate customer and their experience is what drives your business. Understanding what their experience is and detecting when they have issues is valuable in its own right.\n\nNot monitoring for correctness\n\nAnother common variant of this anti-pattern is to monitor the status of services on a host but not the correctness. For example, you may monitor if a web appli- cation is running by checking for an HTTP 200 response code. This tells you the application is responding to connections, but not if it’s returning the correct data in response to those requests.\n\nA better approach is monitoring for the correctness of a service first—for example, monitor the content or rates of a business transaction rather than the uptime of the web server it runs on. This allows you to get the value of both: if the content of a service isn’t correct because it is misconfigured, buggy, or broken you’ll see that. If the content isn’t correct because underlying web service goes down, you’ll also know that.\n\nMonitoring statically\n\nA further check anti-pattern is the use of static thresholds—for example, alerting if CPU usage on a host exceeds 80 percent. Checks are often inflexible Boolean logic or arbitrary static in time thresholds. They generally rely on a specific result or range being matched. The checks don’t consider the dynamism of most complex systems. A match or a breach in a threshold may be important or could have been triggered by an exceptional event—or could even be a natural consequence of growth.\n\nArbitrary static thresholds are almost always wrong. Baron Schwartz, CEO of\n\nVersion: v1.0.0 (427b8e9)\n\n12\n\nChapter 1: Introduction\n\ndatabase performance analysis vendor VividCortex, put it well:\n\nThey’re worse than a broken clock, which is at least right twice a day. A threshold is wrong for any given system, because all systems are slightly different, and it’s wrong for any given moment during the day, because systems experience constantly changing load and other cir- cumstances.\n\nTo monitor well we need to look at windows of data, not static points in time, and we need to use smarter techniques to calculate values and thresholds.\n\nNot monitoring frequently enough\n\nIn many monitoring tools, scaling is a challenge or the default check period is set to a high value—for example, only checking an application once every five to 15 minutes. This often results in missing critical events that occur between your checks. You should monitor your applications frequently enough to:\n\nIdentify faults or anomalies. • Meet human response time expectations—you want to find the fault before your users report the fault.\n\nProvide data at sufficient granularity for identifying performance issues and\n\ntrends.\n\nAlways remember to store sufficient historical data to identify performance issues and trends. In many cases this might only need to be days or weeks of data—but it’s impossible to identify a trend or reoccurring problem if you have thrown away the data that shows it.\n\nVersion: v1.0.0 (427b8e9)\n\n13\n\nChapter 1: Introduction\n\nNo automation or self-service\n\nA frequent reason monitoring is poor or not implemented correctly is that it can be hard to implement. If you make it hard for application developers to instrument their applications, collect the data, or visualize its results, they won’t do it. If your monitoring infrastructure is manual or overly complex then fault and issues will result in monitoring gaps, failures, and the potential for you to spend more time fixing and maintaining your monitoring than actually monitoring.\n\nMonitoring implementations and deployments should be automated wherever pos- sible:\n\nDeployments should be managed by configuration management. • Configuration of hosts and services should be via discovery or self-service submission, so new applications can be automatically monitored rather than needing someone to add them.\n\nAdding instrumentation should be simple and based on a pluggable utility pattern, and developers should be able to include a library or the like rather than having to configure it themselves.\n\nData and visualization should be self-service. Everyone who needs to see the outputs of monitoring should be able to query and visualize those outputs. (This is not to say that you shouldn’t build dashboards for people, but rather that if they want more they shouldn’t have to ask you for it.)\n\nGood monitoring summary\n\nGood monitoring should provide:\n\nThe state of the world, from the top (the business) down. • Assistance in fault diagnostics. • A source of information for infrastructure, application development, and business folks.\n\nVersion: v1.0.0 (427b8e9)\n\n14\n\nChapter 1: Introduction\n\nAnd it should be:\n\nBuilt into design and the life cycle of application development and deploy-\n\nment.\n\nAutomated and provided as self-service, where possible.\n\n NOTE This definition of “good” monitoring heavily overlaps with an emerg-\n\ning term: observability. You can read more about this in Cindy Sridharan’s excel- lent blog post on the differences between the two.\n\nLet’s now look at the actual mechanics of monitoring.\n\nMonitoring mechanics\n\nThere are a variety of ways you can monitor. Indeed, you could argue that every- thing from unit testing to checklists are a form of monitoring.\n\n NOTE Lindsay Holmwood has a useful presentation on test-driven monitor-\n\ning that talks about the connection between testing and monitoring. Additionally, Cindy Sridharan’s post on testing microservices draws some interesting parallels between testing and monitoring.\n\nTraditionally, though, the definition of monitoring focuses on checking and mea- suring the state of an application.\n\nVersion: v1.0.0 (427b8e9)\n\n15\n\nChapter 1: Introduction\n\nProbing and introspection\n\nThere are two major approaches to monitoring applications: probing and intro- spection.2 Probing monitoring probes the outside of an application. You query the external characteristics of an application: does it respond to a poll on an open port and return the correct data or response code? An example of probing moni- toring is performing an ICMP check and confirming you have received a response. Nagios is an example of a monitoring system that is largely based around probe monitoring.\n\nIntrospection monitoring looks at what’s inside the application. The application is instrumented and returns measurements of its state, the state of internal com- ponents, or the performance of transactions or events. This is data that shows exactly how your application is functioning, rather than just its availability or the behavior of its surface area. Introspection monitoring either emits events, logs, and metrics to a monitoring tool or exposes this information on a status or health endpoint of some kind, which can then be collected by a monitoring tool.\n\nThe introspection approach provides an idea of the actual running state of ap- plications. It allows you to communicate a much richer, more contextual set of information about the state of your application than probing monitoring does. It also provides a better approach to exposing the information both you and the business require to monitor your application.\n\nThis is not to say that probing monitoring has no place. It is often useful to know the state of external aspects of an application, especially if the application is pro- vided by a third party and if you don’t have insight into its internal operations. It is often also useful to view your application from outside to understand certain types of networking, security, or availability issues. It’s generally recommended to have probing for your safety net, a catchall that something is wrong, but to use introspection to drive reporting and diagnostics. We’ll see some probe monitoring in Chapter 10.\n\n2Some folks call probing and introspection, black-box and white-box monitoring respectively.\n\nVersion: v1.0.0 (427b8e9)\n\n16\n\nChapter 1: Introduction\n\nPull versus push\n\nThere are two approaches to how monitoring checks are executed that are worth briefly discussing. These are the pull versus push approaches.\n\nPull-based systems scrape or check a remote application—for example, an end- point containing metrics or, as from our probing example, a check using ICMP. In push-based systems, applications emit events that are received by the monitoring system.\n\nBoth approaches have pros and cons. There’s considerable debate in monitoring circles about those pros and cons, but for the purposes of many users, the debate is largely moot. Prometheus is primarily a pull-based system, but it also supports re- ceiving events pushed into a gateway. We’ll show you how to use both approaches in this book.\n\nTypes of monitoring data\n\nMonitoring tools can collect a variety of different types of data. That data primar- ily takes two forms:\n\nMetrics — Most modern monitoring tools rely most heavily on metrics to help us understand what’s going on in our environments. Metrics are stored as time series data that record the state of measures of your applications. We’ll see more about this shortly.\n\nLogs — Logs are (usually textual) events emitted from an application. While they’re helpful for letting you know what’s happening, they’re often most useful for fault diagnosis and investigation. We won’t look at logs much in this book, but there are plenty of tools available, like the ELK stack, for collecting and managing log events.\n\nVersion: v1.0.0 (427b8e9)\n\n17\n\nChapter 1: Introduction\n\n TIP I’ve written a book about the ELK stack that might interest you.\n\nAs Prometheus is primarily focused on collecting time series data, let’s take a deeper look at metrics.\n\nMetrics\n\nMetrics always appear to be the most straightforward part of any monitoring archi- tecture. As a result, we sometimes don’t invest quite enough time in understanding what we’re collecting, why we’re collecting it, and what we’re doing with those metrics.\n\nIn a lot of monitoring frameworks, the focus is on fault detection: detecting if a specific system event or state has occurred (this is very much the Nagios style of operation—more on this below). When we receive a notification about a specific system event, usually we go look at whatever metrics we’re collecting, if any, to find out what exactly has happened and why. In this world, metrics are seen as a by-product of, or a supplement to, our fault detection.\n\n TIP See the discussion later in this chapter about notification design for\n\nfurther reasons why this is a challenging problem.\n\nPrometheus changes this idea of “metrics as supplement.” Metrics are the most im- portant part of your monitoring workflow. Prometheus turns the fault–detection— centric model on its head. Metrics provide the state and availability of your envi- ronment and its performance.\n\nVersion: v1.0.0 (427b8e9)\n\n18\n\nChapter 1: Introduction\n\n NOTE This book generally avoids duplicating Boolean status checks when\n\na metric can provide information on both state and performance.\n\nHarnessed correctly, metrics provide a dynamic, real-time picture of the state of your infrastructure that will help you manage and make good decisions about your environment. Additionally, through anomaly detection and pattern analysis, metrics have the potential to identify faults or issues before they occur or before the specific system event that indicates an outage is generated.\n\nSo what’s a metric?\n\nAs metrics and measurement are so critical to our monitoring framework, we’re going to help you understand what metrics are and how to work with them. This is intended to be a simplified background that will allow you to understand what different types of metrics, data, and visualizations will contribute to our monitor- ing framework.\n\nMetrics are measures of properties of components of software or hardware. To make a metric useful we keep track of its state, generally recording data points over time. Those data points are called observations. An observation consists of the value, a timestamp, and sometimes a series of properties that describe the observation, such as a source or tags. A collection of observations is called a time series.\n\nA classic example of time series data we might collect is website visits, or hits. We periodically collect observations about our website hits, recording the number of hits and the times of the observations. We might also collect properties such as the source of a hit, which server was hit, or a variety of other information.\n\nWe generally collect observations at a fixed-time interval—we call this the gran- ularity or resolution. This could range from one second to five minutes to 60\n\nVersion: v1.0.0 (427b8e9)\n\n19\n\nChapter 1: Introduction\n\nminutes or more. Choosing the right granularity at which to record a metric is critical. Choose too coarse a granularity and you can easily miss the detail. For example, sampling CPU or memory usage at five-minute intervals is highly un- likely to identify anomalies in your data. Alternatively, choosing fine granularity can result in the need to store and interpret large amounts of data.\n\nTime series data is a chronologically ordered list of these observations. Time series metrics are often visualized, sometimes with a mathematical function applied, as a two-dimensional plot with data values on the y-axis and time on the x-axis. Often you’ll see multiple data values plotted on the y-axis—for example, the CPU usage values from multiple hosts or successful and unsuccessful transactions.\n\nFigure 1.3: A sample plot\n\nThese plots can be incredibly useful. They provide us with a visual representation of critical data that is (relatively) easy to interpret, certainly with more facility than perusing the same data in the form of a list of values. They also present us with a historical view of whatever we’re monitoring: they show us what has changed and when. We can use both of these capabilities to understand what’s happening in our environment and when it happened.\n\nVersion: v1.0.0 (427b8e9)\n\n20\n\nChapter 1: Introduction\n\nTypes of metrics\n\nThere are a variety of different types of metrics you’ll see in the wild.\n\nGauges\n\nThe first type of metric we’ll look at is a gauge. Gauges are numbers that are expected to go up or down. A gauge is essentially a snapshot of a specific mea- surement. The classic metrics of CPU, memory, and disk usage are usually articu- lated as gauges. For business metrics, a gauge might be the number of customers present on a site.\n\nFigure 1.4: A sample gauge\n\nCounters\n\nThe second type of metric we’ll see frequently is a counter. Counters are numbers that increase over time and never decrease. Although they never decrease, coun- ters can sometimes reset to zero and start incrementing again. Good examples of application and infrastructure counters are system uptime, the number of bytes sent and received by a device, or the number of logins. Examples of business coun- ters might be the number of sales in a month or the number of orders received by an application.\n\nVersion: v1.0.0 (427b8e9)\n\n21\n\nChapter 1: Introduction\n\nFigure 1.5: A sample counter\n\nIn this figure we have a counter incrementing over a period.\n\nA useful thing about counters is that they let you calculate rates of change. Each observed value is a moment in time: t. You can subtract the value at t from the value at t+1 to get the rate of change between the two values. A lot of useful information can be understood by understanding the rate of change between two values. For example, the number of logins is marginally interesting, but create a rate from it and you can see the number of logins per second, which should help identify periods of site popularity.\n\nHistograms\n\nA histogram is a metric that samples observations. This is a frequency distribution of a dataset. You group data together—a process called “binning”—and present the groups in a such a way that their relative sizes are visualized. Each observation is counted and placed into buckets. This results in multiple metrics: one for each bucket, plus metrics for the sum and count of all values.\n\nA common visualization of a frequency distribution histogram looks like a bar graph.\n\nVersion: v1.0.0 (427b8e9)\n\n22\n\nChapter 1: Introduction\n\nFigure 1.6: A histogram example\n\nHere we see a sample histogram for the frequency distribution of heights. On the y-axis we have the frequency and on the x-axis we have the distribution of heights. We see that for the height 160–165 cm tall there is a distribution of two.\n\n NOTE There’s another metric type, called a summary, which is similar\n\nto a histogram, but it also calculates percentiles. You can read more about im- plementation details and some caveats of histogram and summaries specific to Prometheus.\n\nHistograms can be powerful representations of your time series data and especially useful for visualizing data such as application latencies.\n\nMetric summaries\n\nOften the value of a single metric isn’t useful to us. Instead, visualization of a met- ric requires applying mathematical transformations to it. For example, we might apply statistical functions to our metric or to groups of metrics. Some common functions we might apply include:\n\nVersion: v1.0.0 (427b8e9)\n\n23\n\nChapter 1: Introduction\n\nCount or n — Counts the number of observations in a specific time interval.\n\nSum — To sum is to add together values from all observations in a specific\n\ntime interval.\n\nAverage — Provides the mean of all values in a specific time interval.\n\nMedian — The median is the dead center of our values: exactly 50 percent\n\nof values are below it, and 50 percent are above it.\n\nPercentiles — Measures the values below which a given percentage of ob-\n\nservations in a group of observations fall.\n\nStandard deviation — Shows standard deviation from the mean in the distri- bution of our metrics. This measures the variation in a data set. A standard deviation of 0 means the distribution is equal to the mean of the data. Higher deviations mean the data is spread out over a range of values.\n\nRates of change — Rates of change representations show the degree of\n\nchange between data in a time series.\n\n TIP This is a brief introduction to these summary methods. We’ll use some\n\nof them in more detail later in the book.\n\nMetric aggregation\n\nIn addition to summaries of specific metrics, you often want to show aggregated views of metrics from multiple sources, such as the disk space usage of all your application servers. The most typical example of this results in multiple metrics being displayed on a single plot. This is useful in identifying broad trends over\n\nVersion: v1.0.0 (427b8e9)\n\n24\n\nChapter 1: Introduction\n\nyour environment. For example, an intermittent fault in a load balancer might result in web traffic dropping off for multiple servers. This is often easier to see in aggregate than by reviewing each individual metric.\n\nFigure 1.7: An aggregated collection of metrics\n\nIn this plot we see disk usage from numerous hosts over a 30-day period. It gives us a quick way to ascertain the current state (and rate of change) of a group of hosts.\n\nUltimately you’ll find that a combination of single and aggregate metrics provide the most representative view of the health of your environment: the former to drill down into specific issues, and the latter to see the high-level state.\n\nLet’s take a deeper dive into types of metric summaries: the whys, why nots, and hows of using averages, the median, standard deviation, percentiles, and other statistical choices.\n\n NOTE This is a high-level overview of some statistical techniques rather\n\nthan a deep dive into the topic. Exploration of some topics may appear overly simplistic to folks with strong statistics or mathematics backgrounds.\n\nAverages\n\nAverages are the de facto metric analysis method. Indeed, pretty much everyone who has ever monitored or analyzed a website or application has used averages.\n\nVersion: v1.0.0 (427b8e9)\n\n25\n\nChapter 1: Introduction\n\nIn the web operations world, for example, many companies live and die by the average response time of their site or API.\n\nAverages are attractive because they are easy to calculate. Let’s say we have a list of seven time series values: 12, 22, 15, 3, 7, 94, and 39. To calculate the average we sum the list of values and divide the total by the number of values in the list.\n\n(12 + 22 + 15 + 3 + 7 + 94 + 39) / 7 = 27.428571428571\n\nWe first sum the seven values to get the total of 192. We then divide the sum by the number of values, here 7, to return the average: 27.428571428571. Seems pretty simple, huh? The devil, as they say, is in the details.\n\nAverages assume there is a normal event or that your data is a normal (or Gaus- sian) distribution—for example, in our average response time, it’s assumed that all events run at equal speed or that response time distribution is roughly bell curved. But this is rarely the case with applications. In fact, there’s an old statistics joke about a statistician who jumps in a lake with an average depth of only 10 inches and nearly drowns…\n\nVersion: v1.0.0 (427b8e9)\n\n26\n\nChapter 1: Introduction\n\nFigure 1.8: The flaw of averages - copyright Jeff Danzinger\n\nWhy did he nearly drown? The lake contained large areas of shallow water and some areas of deep water. Because there were larger areas of shallow water, the average depth was lower overall. In the monitoring world the same principal lots of low values in our average distort or hide high values and vice applies: versa. These hidden outliers can mean that while we think most of our users are experiencing a quality service, there may be a significant number that are not.\n\nLet’s look at an example using response times and requests for a website.\n\nVersion: v1.0.0 (427b8e9)\n\n27\n\nChapter 1: Introduction\n\nFigure 1.9: Response time average\n\nHere we have a plot showing response time for a number of requests. Calculating the average response time would give us 4.1 seconds. The vast majority of our users would experience a (potentially) healthy 4.1 second response time. But many of our users are experiencing response times of up to 12 seconds, perhaps considerably less acceptable.\n\nLet’s look at another example with a wider distribution of values.\n\nVersion: v1.0.0 (427b8e9)\n\n28\n\nChapter 1: Introduction\n\nFigure 1.10: Response time average Mk II\n\nHere our average would be a less stellar 6.8 seconds. But worse, this average is considerably better than the response time received by the majority of our users with a heavy distribution of request times around 9, 10, and 11 seconds long. If we were relying on the average alone, we’d probably think our application was performing a lot better than it is.\n\nMedian\n\nAt this point you might be wondering about using the median. The median is the dead center of our values: exactly 50 percent of values are below it, and 50 percent are above it. If there’s an odd number of values, then the median will be the value in the middle. For the first data set we looked at—12, 22, 15, 3, 7, 94, and 39—the median is 15. If there were an even number of values, the median\n\nVersion: v1.0.0 (427b8e9)\n\n29\n\nChapter 1: Introduction\n\nwould be the mean of the two values in the middle. So if we were to remove 39 from our data set to make it even, the median would become 13.5.\n\nLet’s apply this to our two plots.\n\nFigure 1.11: Response time average and median\n\nWe see in our first example figure that the median is 3, which provides an even rosier picture of our data.\n\nIn the second example the median is 8, a bit better but close enough to the average to render it ineffective.\n\nVersion: v1.0.0 (427b8e9)\n\n30\n\nChapter 1: Introduction\n\nFigure 1.12: Response time average and median Mk II\n\nYou can probably already see that the problem again here is that, like the mean, the median works best when the data is on a bell curve… And in the real world that’s not realistic.\n\nAnother commonly used technique to identify performance issues is to calculate the standard deviation of a metric from the mean.\n\nStandard deviation\n\nAs we learned earlier in the chapter, standard deviation measures the variation or spread in a data set. A standard deviation of 0 means most of the data is close to the mean. Higher deviations mean the data is more distributed. Standard deviations are represented by positive or negative numbers suffixed with the sigma symbol—for example, 1 sigma is one standard deviation from the mean.\n\nVersion: v1.0.0 (427b8e9)\n\n31\n\nChapter 1: Introduction\n\nLike the mean and the median, however, standard deviation works best when the In a normal distribution there’s a simple way of data is a normal distribution. articulating the distribution: the empirical rule, also known as the 68–95–99.7 rule or three-sigma rule. Within the rule, one standard deviation or 1 to -1 will represent 68.27 percent of all transactions on either side of the mean, two standard deviations or 2 to -2 would be 95.45 percent, and three standard deviations will represent 99.73 percent of all transactions.\n\nFigure 1.13: The empirical rule\n\nMany monitoring approaches take advantage of the empirical rule and trigger on transactions or events that are more than two standard deviations from the mean, potentially catching performance outliers. In instances like our two previous ex- amples, however, the standard deviation isn’t overly helpful either. And without\n\nVersion: v1.0.0 (427b8e9)\n\n32\n\nChapter 1: Introduction\n\na normal distribution of data, the resulting standard deviation can be highly mis- leading.\n\nThus far, our methods for identifying anomalous data in our metrics haven’t been overly promising. But all is not lost! Our next method, percentiles, offer a little more hope.\n\nPercentiles\n\nPercentiles measure the values below which a given percentage of observations in a group of observations fall. Essentially they look at the distribution of values across your data set. For example, the median we looked at above is the 50th percentile (or p50). In the median, 50 percent of values fall below and 50 per- cent above. For metrics, percentiles make a lot of sense because they make the distribution of values easy to grasp. For example, the 99th-percentile value of 10 milliseconds for a transaction is easy to interpret: 99 percent of transactions were completed in 10 milliseconds or less, and 1 percent of transactions took more than 10 milliseconds.\n\n TIP Percentiles are a type of quantile.\n\nPercentiles are ideal for identifying outliers. If a great experience on your site is a response time of less than 10 milliseconds then 99 percent of your users are having a great experience—but 1 percent of them are not. Once you’re aware of this, you can focus on addressing the performance issue that’s causing a problem for that 1 percent.\n\nLet’s apply this to our previous request and response time graphs and see what appears. We’ll apply two percentiles, the 75th and 99th percentiles, to our first example data set.\n\nVersion: v1.0.0 (427b8e9)\n\n33\n\nChapter 1: Introduction\n\nFigure 1.14: Response time average, median, and percentiles\n\nWe see that the 75th percentile is 5.5 seconds. That indicates that 75 percent com- pleted in 5.5 seconds, and 25 percent were slower than that. Still pretty much in line with the earlier analysis we’ve examined for the data set. The 99th percentile, on the other hand, shows 10.74 seconds. This means 99 percent of users had re- quest times of less than 10.74 seconds, and 1 percent had more than 10.74 seconds. This gives us a real picture of how our application is performing. We can also use the distribution between p75 and p99. If we’re comfortable with 99 percent of users getting 10.74 second response times or better and 1 percent being slower than that, then we don’t need to consider any further tuning. Alternatively, if we want a uniform response, or if we want to lower that 10.74 seconds across our distribution, we’ve now identified a pool of transactions we can trace, profile, and improve. As we adjust the performance, we’ll also be able to see the p99 response time improve.\n\nVersion: v1.0.0 (427b8e9)\n\n34\n\nChapter 1: Introduction\n\nThe second data set is even more clear.\n\nFigure 1.15: Response time average, median, and percentiles Mk II\n\nThe 75th percentile is 10 seconds and the 99th percentile is 12 seconds. Here the 99th percentile provides a clear picture of the broader distribution of our transactions. This is a far more accurate reflection of the outlying transactions from our site. We now know that—as opposed to what the mean response times would imply—not all users are enjoying an adequate experience. We can use this data to identify elements of our application we can potentially improve.\n\nPercentiles, however, aren’t perfect all the time. We recommend graphing several combinations of metrics to get a clear picture of the data. For example, when measuring latency it’s often a good idea to display a graph that shows:\n\nThe 50th percentile, or median. • The 99th percentile.\n\nVersion: v1.0.0 (427b8e9)\n\n35\n\nChapter 1: Introduction\n\nThe max value.\n\nThe addition of the max value helps visualize the upward bounds of the metric you are measuring. It’s again not perfect though—a high max value can dwarf other values in a graph.\n\nWe’re going to apply percentiles and other calculations later in the book as we start to build checks and collect metrics.\n\nMonitoring methodologies\n\nWe’ll also make use of a combination of several monitoring methodologies on top of our metrics and metric aggregations to help focus our monitoring. We’re going to combine elements of two monitoring methodologies:\n\nBrendan Gregg’s USE or Utilization Saturation and Errors Method, which\n\nfocuses on host-level monitoring.\n\nGoogle’s Four Golden Signals, which focus on application-level monitoring.\n\nMonitoring methodologies provide guidelines that allow you to narrow down and focus on specific metrics in the sea of time series you collect. When com- bined, these two frameworks—one focused on host-level performance, the other on application-level performance—represent a reasonably holistic view of your environment that should assist you in tackling any issues.\n\nThe USE Method\n\nThe USE, or Utilization Saturation and Errors, Method was developed by Brendan Gregg, a kernel and performance engineer at Netflix. The methodology proposes creating a checklist for server analysis that allows the fast identification of issues.\n\nVersion: v1.0.0 (427b8e9)\n\n36\n\nChapter 1: Introduction\n\nYou work down the checklist to identify common performance issues, making use of data collected from your environment.\n\nFor every resource, check\n\nThe USE Method can be summarized as: utilization, saturation, and errors. The method is most effective for the monitoring of resources that suffer performance issues under high utilization or saturation. Let’s quickly define each term to help understand this.\n\nA resource - A component of a system. In Gregg’s definition of the model it’s traditionally a physical server component like CPUs, disks, etc., but many folks also include software resources in the definition.\n\nUtilization - The average time the resource is busy doing work. It’s usually\n\nexpressed as a percentage over time.\n\nSaturation - The measure of queued work for a resource, work it can’t pro-\n\ncess yet. This is usually expressed as queue length.\n\nErrors - The scalar count of error events for a resource.\n\nWe combine these definitions to create a checklist of the resources and an ap- proach to monitor each element of the methodology: utilization, saturation, or errors. How might this work? Well, let’s say we have a serious performance issue, and we want to dive into some diagnosis. We refer to our checklist and check each element for each monitored component. In our example, we’ll start with CPU:\n\nCPU utilization as a percentage over time. • CPU saturation as the number of processes awaiting CPU time. • Errors, generally less important for the CPU resource.\n\nAnd then, perhaps, memory:\n\nMemory utilization as a percentage over time. • Memory saturation measured via monitoring swapping. • Errors, generally less important here but also can be captured.\n\nVersion: v1.0.0 (427b8e9)\n\n37\n\nChapter 1: Introduction\n\nAnd so on through other components on the system until we’ve identified the bottleneck or signal that points us to the issue.\n\nWe’ll see more of this in Chapter 4 when we look at monitoring some system-level metrics.\n\n TIP You can find an example checklist for a Linux system here.\n\nThe Google Four Golden Signals\n\nThe Google Four Golden Signals come out of the Google SRE book. They take a similar approach to the USE Method, specifying a series of general metric types to monitor. Rather than being system-level-focused time series, the metric types in this methodology are more application or user-facing:\n\nLatency - The time taken to service a request, distinguishing between the latency of successful and failed requests. A failed request, for example, might return with very low latency skewing your results.\n\nTraffic - The demand on your system—for example, HTTP requests per\n\nsecond or transactions for a database system.\n\nErrors - The rate that requests fail, whether explicit failures like HTTP 500 errors, implicit failures like wrong or invalid content being returned, or policy-based failures—for instance if you’ve mandated that failures over 30ms should be considered errors.\n\nSaturation - The “fullness” of your application or the resources that are constraining it—for example, memory or IO. This also includes impending saturation, such as a rapidly filling disk.\n\nVersion: v1.0.0 (427b8e9)\n\n38\n\nChapter 1: Introduction\n\nUsing the golden signals is easy. Select high-level metrics that match each signal and build alerts for them. If one of those signals becomes an issue then an alert will be generated and you can diagnose or resolve the issue.\n\nWe’ll see golden signals again in Chapter 7 and 8 when we look at monitoring some applications.\n\n TIP There’s a related framework called RED—or Rate, Errors, and Duration,\n\ndeveloped by the team at Weaveworks, that might also interest you.\n\nContextual, useful alerts and notifications\n\nAlerts and notifications are the primary output from monitoring tools. So what’s the difference between an alert and a notification? An alert is raised when some- thing happens—for example, when a threshold is reached. This, however, doesn’t mean anyone’s been told about the event. That’s where notifications come in. A notification takes the alert and tells someone or something about it: an email is sent, an SMS is triggered, a ticket is opened, or the like. It may seem like this should be a really simple domain, but it contains a lot of complexity and is often poorly implemented and managed.\n\nTo build a good notification system you need to consider the basics of:\n\nWhat problems to notify on. • Who to tell about a problem. • How to tell them. • How often to tell them. • When to stop telling them, do something else, or escalate to someone else.\n\nVersion: v1.0.0 (427b8e9)\n\n39\n\nChapter 1: Introduction\n\nIf you get it wrong and generate too many notifications then people will be unable to take action on them all and may even mute them. We all have war stories of mailbox folders full of thousands of notification emails from monitoring systems.3 Sometimes so many notifications are generated that you suffer from alert fatigue and ignore them (or worse, conduct notification management via Select All -> Delete). Consequently, you’re likely to miss actual critical notifications when they are sent.\n\nMost importantly, you need to work out what to tell whoever is receiving the notifications. Notifications are usually the sole signal that you receive to tell you that something is amiss or requires your attention. They need to be concise, articulate, accurate, digestible, and actionable. Designing your notifications to actually be useful is critical. Let’s make a brief digression and see why this matters. We’ll look at a typical Nagios notification for disk space.\n\nListing 1.1: Sample Nagios notification\n\nPROBLEM Host: server.example.com Service: Disk Space\n\nState is now: WARNING for 0d 0h 2m 4s (was: WARNING) after 3/3 checks\n\nNotification sent at: Thu Aug 7th 03:36:42 UTC 2015 ( notification number 1)\n\nAdditional info: DISK WARNING - free space: /data 678912 MB (9% inode=99%)\n\nImagine you’ve just received this notification at 3:36 a.m. What does it tell you? That we have a host with a disk space warning. And that the /data volume is 91 percent full. At first glance this seems useful, but in reality it’s not all that practical. First, is this a sudden increase, or has this grown gradually? And what’s the rate\n\n3Or cron.\n\nVersion: v1.0.0 (427b8e9)\n\n40\n\nChapter 1: Introduction\n\nof expansion? (Consider that 9 percent disk space free on a 1 GB partition is quite different from 9 percent disk space free on a 1 TB disk.) Can you ignore or mute this notification or do you need to act now? Without the additional context your ability to take action on the notification is limited, and you need to invest considerably more time to gather context.\n\nIn our framework we’re going to focus on:\n\nMaking notifications actionable, clear, and articulate. Just the use of notifi- cations written by humans rather than by computers can make a significant difference in the clarity and utility of those notifications.\n\nAdding context to notifications. We’re going to send notifications that con-\n\ntain additional information about the component we’re notifying on.\n\nOnly sending those notifications that make sense.\n\n TIP The simplest advice we can give here is to remember/ notifications are\n\nread by humans, not computers. Design them accordingly.\n\nVisualization\n\nVisualizing data is both an incredibly powerful analytic and interpretive technique and an amazing learning tool. Metrics and their visualizations are often tricky to interpret. Humans tend towards apophenia—the perception of meaningful pat- terns within random data—when viewing visualizations. This often leads to mak- ing sudden leaps from correlation to causation, and can be further exacerbated by the granularity and resolution of our available data, how we choose to represent it, and the scale on which we represent it.\n\nVersion: v1.0.0 (427b8e9)\n\n41\n\nChapter 1: Introduction\n\nOur ideal visualizations will clearly show the data, with an emphasis on high- lighting substance over visuals. In this book we’re not going to look at a lot of visualizations but where we have, we’ve tried to build visuals that subscribe to these broad rules:\n\nClearly show the data. • Induce the viewer to think about the substance, not the visuals. • Avoid distorting the data. • Make large data sets coherent. • Allow changing perspectives of granularity without impacting comprehen- sion.\n\nWe’ve drawn most of these ideas from Edward Tufte’s The Visual Display of Quan- titative Information and thoroughly recommend reading it to help you build good visualizations.\n\nThere’s also a great post from the Datadog team on visualizing time series data that is worth reading.\n\nBut didn’t you write that other book?\n\nAs many folks know, I am one of the maintainers of Riemann, an event stream processor focused on monitoring distributed systems. I wrote a book about mon- itoring in which I used Riemann as a centerpiece to explore new monitoring pat- terns and approaches. In the book, I described an architecture of introspection monitoring (with some selective probing monitoring).\n\nIn the book I also focused on push-based monitoring over pull-based monitoring. There are lots of reasons I favor the push model versus the pull model but, as we mentioned earlier, for many folks the distinction is arbitrary. Indeed, many of the concerns of either approach don’t impact implementation due to issues like scale. Other concerns, like many arguments over implementation or tool choice,\n\nVersion: v1.0.0 (427b8e9)\n\n42\n\nChapter 1: Introduction\n\ndon’t change the potential success of the implementation. I’m a strong exponent of using tools that work for you, rather than unreviewed adoption of the latest trend or dogmatism.\n\nIt’s this lack of distinction for folks, and a desire not to be dogmatic about my beliefs, that has encouraged me to write another book, this one about one of the leading pull-based monitoring tools: Prometheus. In the Art of Monitoring I wrote:\n\nPerhaps a better way of looking at these tool choices is that they are merely ways to articulate the change in monitoring approach that is proposed in this book. They are the trees in the woods. If you find other tools that work better for you and achieve the same results then we’d love to hear from you. Write a blog post, give a talk, or share your configuration.\n\nHence, you’ll see much of the methodology of The Art of Monitoring reflected in this book—indeed, much of this chapter is a distillation of some of the book’s elements. We’re taking the core motivation of that book—a better way to monitor applications—and applying it with an alternative tool, and a different architecture and approach.\n\nWhat’s in the book?\n\nThis book covers an introduction to a good approach to monitoring, and then uses Prometheus to instantiate that monitoring approach. By the end of the book you should have a readily extensible and scalable monitoring platform.\n\nThe book assumes you want to build, rather than buy a monitoring platform. There are a lot of off-the-shelf Software-as-a-Service (SaaS) and cloud-based mon- itoring solutions that might work for you. There’s even some hosted Prometheus options. For many folks, this is a better solution when starting out with moni- toring rather than investing in building their own. It’s our view that ultimately\n\nVersion: v1.0.0 (427b8e9)\n\n43\n\nChapter 1: Introduction\n\nmost folks, as their environment and requirements grow, will discover that these platforms don’t quite suit their needs, and will build (some) monitoring in house. But whether that’s the case for you is something you’ll need to determine yourself.\n\nIn this book, we’re going to introduce you to the Prometheus monitoring platform piece by piece, starting with monitoring node and container metrics, service dis- covery, alerting, and then instrumenting and monitoring applications. The book will try to cover a representative sample of technologies you’re likely to manage yourself but that can be adapted to a wide variety of other environments and stacks.\n\nThe book’s chapters are:\n\nChapter 1: This introduction. • Chapter 2: Introducing Prometheus. • Chapter 3: Installing Prometheus. • Chapter 4: Monitoring nodes and containers. • Chapter 5: Service discovery. • Chapter 6: Alerting and AlertManager. • Chapter 7: Scaling. • Chapter 8: Instrumenting an application. • Chapter 9: Logging as instrumentation. • Chapter 10: Probing. • Chapter 11: Pushgateway. • Chapter 12: Monitoring a stack - Kubernetes. • Chapter 13: Monitoring a stack - Application.\n\nSummary\n\nIn this chapter we introduced you to modern monitoring approaches. We laid out the details of several types of monitoring implementations. We discussed what makes good and bad monitoring, and how to avoid poor monitoring outcomes.\n\nVersion: v1.0.0 (427b8e9)\n\n44\n\nChapter 1: Introduction\n\nWe also introduced the details of time series data and metrics to you. We broke down the types of data that can be delivered as metrics. And we demonstrated some common mathematical functions applied to metrics to manipulate and ag- gregate them.\n\nIn the next chapter, we’re going to introduce you to Prometheus and give some insight into its architecture and components.\n\nVersion: v1.0.0 (427b8e9)\n\n45\n\nChapter 2\n\nIntroduction to Prometheus\n\nIn this chapter we’re going to introduce you to Prometheus, its origins, and give you an overview of:\n\nWhere Prometheus came from and why. • Prometheus architecture and design. • The Prometheus data model. • The Prometheus ecosystem.\n\nThis should give you an introduction and understanding of what Prometheus is and where it fits into the monitoring universe.\n\n NOTE This book focuses on Prometheus version 2.0 and later. Much of\n\nthe book’s information will not work for earlier releases.\n\n46",
      "page_number": 18
    },
    {
      "number": 2,
      "title": "Introduction to Prometheus",
      "start_page": 59,
      "end_page": 72,
      "detection_method": "regex_chapter",
      "content": "Chapter 2: Introduction to Prometheus\n\nThe Prometheus backstory\n\nOnce upon a time there was a company in Mountain View, California, called Google. They ran a swathe of products, most famously an advertising erm, search engine platform. To run these diverse products they built a platform called Borg. The Borg system is “a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines.”1 The open-source container manager Kubernetes owes much of its heritage to Borg. Shortly after Borg was deployed at Google, folks realized that this complexity required a similarly capable monitoring system. Google built that system and called it Borgmon. Borgmon is a real-time– focused time series monitoring system that uses that data to identify issues and alert on them.\n\n NOTE Neither Borg nor Borgmon have ever been open sourced. It’s only\n\nrecent that one can learn about how they work. You can read a bit more about it in the Practical Alerting chapter of the SRE book.\n\nIt was originally devel- Prometheus owes its inspiration to Google’s Borgmon. oped by Matt T. Proud, an ex-Google SRE, as a research project. After Proud joined SoundCloud, he teamed up with another engineer, Julius Volz, to develop Prometheus in earnest. Other developers joined the effort, and it continued de- velopment internally at SoundCloud, culminating in a public release in January 2015.\n\nLike Borgmon, Prometheus was primarily designed to provide near real-time in- trospection monitoring of dynamic cloud- and container-based microservices, ser- vices, and applications. SoundCloud was an earlier adopter of these architec-\n\n1Abhishek Verma et al, Large-scale cluster management at Google with Borg, EuroSys, 2015.\n\nVersion: v1.0.0 (427b8e9)\n\n47\n\nChapter 2: Introduction to Prometheus\n\ntural patterns, and Prometheus was built to respond to those needs. These days, Prometheus is used by a wide range of companies, generally for similar monitoring needs, but also for monitoring of more traditional architectures.\n\nPrometheus is focused on what’s happening right now, rather than tracking data over weeks or months. This is based on the premise that the majority of monitor- ing queries and alerts are generated from recent, usually less than day-old, data. Facebook validated this in a paper on Gorilla, its internal time series database. Facebook discovered that 85 percent of queries were for data less than 26 hours old. Prometheus assumes that the problems you may be trying to fix are likely recent, hence the most useful data is the most recent data. This is reflected in the powerful query language available and the typically limited retention period for monitoring data.\n\nPrometheus is written in Go, open source, and licensed under the Apache 2.0 license. It is incubated under the Cloud Native Computing Foundation.\n\nPrometheus architecture\n\nPrometheus works by scraping or pulling time series data exposed from appli- cations. The time series data is exposed by the applications themselves often via client libraries or via proxies called exporters, as HTTP endpoints. Exporters and client libraries exist for many languages, frameworks, and open-source for web servers like Apache and databases like applications—for example, MySQL.\n\nPrometheus also has a push gateway you can use to receive small volumes of data—for example, data from targets that can’t be pulled, like transient jobs or targets behind firewalls.\n\nVersion: v1.0.0 (427b8e9)\n\n48\n\nChapter 2: Introduction to Prometheus\n\nFigure 2.1: Prometheus architecture\n\nMetric collection\n\nPrometheus calls the source of metrics it can scrape endpoints. An endpoint usu- ally corresponds to a single process, host, service, or application. To scrape an endpoint, Prometheus defines configuration called a target. This is the informa- tion required to perform the scrape—for example, how to connect to it, what metadata to apply, any authentication required to connect, or other information that defines how the scrape will occur. Groups of targets are called jobs. Jobs are usually groups of targets with the same role—for example, a cluster of Apache servers behind a load balancer. That is, they’re effectively a group of like pro-\n\nVersion: v1.0.0 (427b8e9)\n\n49\n\nChapter 2: Introduction to Prometheus\n\ncesses.\n\nThe resulting time series data is collected and stored locally on the Prometheus server. It can also be sent from the server to external storage or to another time series database.\n\nService discovery\n\nDiscovery of resources to be monitored can be handled in a variety of ways in- cluding:\n\nA user-provided static list of resources. • File-based discover—for example, using a configuration management tool to generate a list of resources that are automatically updated in Prometheus. • Automated discovery—for example, querying a data store like Consul, run- ning instances in Amazon or Google, or using DNS SRV records to generate a list of resources.\n\n TIP We’ll see how to use a variety of service discovery approaches in Chapter\n\n5.\n\nAggregation and alerting\n\nThe server can also query and aggregate the time series data, and can create rules to record commonly used queries and aggregations. This allows you to create new time series from existing time series—for example, calculating rates and ratios or producing aggregations like sums. This saves you having to recreate common ag- gregations, say ones you use for debugging, and the precomputation is potentially more performant than running the query each time it is required.\n\nVersion: v1.0.0 (427b8e9)\n\n50\n\nChapter 2: Introduction to Prometheus\n\nPrometheus can also define rules for alerting. These are criteria—for example, a resource time series starting to show escalated CPU usage—that can be configured to trigger an alert when the criteria are met. The Prometheus server doesn’t come with an inbuilt alerting tool. Instead, alerts are pushed from the Prometheus server to a separate server called Alertmanager. Alertmanager can manage, consolidate, and distribute alerts to a variety of destinations—for example, it can trigger an email when an alert is raised, but prevent duplicates.\n\n TIP We’ll see a lot more about Alertmanager in Chapter 6.\n\nQuerying data\n\nThe Prometheus server also comes with an inbuilt querying language, PromQL; an expression browser; and a graphing interface you can use to explore the data on your server.\n\nVersion: v1.0.0 (427b8e9)\n\n51\n\nChapter 2: Introduction to Prometheus\n\nFigure 2.2: Prometheus expression browser\n\nAutonomy\n\nEach Prometheus server is designed to be as autonomous as possible. It is designed to scale to millions of time series from many thousands of hosts. Its data storage format is designed to keep disk use down and provide fast retrieval of time series during queries and aggregations.\n\n TIP A good helping of memory (Prometheus does a lot in memory) and SSD\n\ndisks are recommended for Prometheus servers, for speed and reliability. You are using SSDs, right?\n\nVersion: v1.0.0 (427b8e9)\n\n52\n\nChapter 2: Introduction to Prometheus\n\nRedundancy and high availability\n\nRedundancy and high availability center on alerting resilience rather than data durability. The Prometheus team recommends deploying Prometheus servers to specific purposes and teams rather than to a single monolithic Prometheus server. If you do want to deploy in an HA configuration, two or more identically con- figured Prometheus servers collect the time series data, and any alerts generated are handled by a highly available Alertmanager configuration that deduplicates alerts.\n\nFigure 2.3: Redundant Prometheus architecture\n\nVersion: v1.0.0 (427b8e9)\n\n53\n\nChapter 2: Introduction to Prometheus\n\n TIP We’ll see how to implement this configuration in Chapter 7.\n\nVisualization\n\nVisualization is provided via an inbuilt expression browser and integration with the open-source dashboard Grafana. Other dashboards are also supported.\n\n TIP We’ll get to know this integration in Chapter 4.\n\nThe Prometheus data model\n\nAs we’ve seen, Prometheus collects time series data. To handle this data it has a multi-dimensional time series data model. The time series data model combines time series names and key/value pairs called labels; these labels provide the di- mensions. Each time series is uniquely identified by the combination of time series name and any assigned labels.\n\nMetric names\n\nThe time series name usually describes the general nature of the time series data being collected—for example, website_visits_total as the total number of web- site visits.\n\nThe name can contain ASCII letters, digits, underscores, and colons.\n\nVersion: v1.0.0 (427b8e9)\n\n54\n\nChapter 2: Introduction to Prometheus\n\nLabels\n\nLabels enable the Prometheus dimensional data model. They add context to a specific time series. For example, our total_website_visits time series could have labels that identify the name of the website, IP of the requester, or other dimensions that specifically identify that time series and connect it to its source. Prometheus can query on these dimensions to select one time series, groups of time series, or all relevant time series.\n\nLabels come in two broad types: instrumentation labels and target labels. Instru- mentation labels come from the resource being monitored—for example, for a HTTP-related time series, a label might show the specific HTTP verb used. These labels are added to the time series before they are scraped, such as by a client or exporter. Target labels relate more to your architecture—they might iden- tify the data center where the time series originated. Target labels are added by Prometheus during and after the scrape.\n\nA time series is identified by both its name and labels (although technically the name itself is also a label called __name__). If you add or change a label on a time series, Prometheus treats this as a new time series.\n\n TIP You can generally think of labels as tags, albeit in key/value form and\n\nwhere a new tag creates a new time series.\n\nLabel names can contain ASCII letters, digits, and underscores.\n\n TIP Label names prefixed with __ are reserved for internal Prometheus use.\n\nVersion: v1.0.0 (427b8e9)\n\n55\n\nChapter 2: Introduction to Prometheus\n\nSamples\n\nThe actual value of the time series is called a sample. It consists of:\n\nA float64 value. • A millisecond-precision timestamp.\n\nNotation\n\nCombining these elements we can see how Prometheus represents a time series as notation.\n\nListing 2.1: Time series notation\n\n<time series name>{<label name>=<label value>, ...}\n\nFor example, our total_website_visits time series, with attached labels, might look like:\n\nListing 2.2: Example time series\n\ntotal_website_visits{site=\"MegaApp\", location=\"NJ\", instance=\" webserver\",job=\"web\"}\n\nThe time series name is represented first, with a map of key/value pair labels attached. All time series generally have an instance label, which identifies the source host or application, and a job label, which contains the name of the job that scraped the specific time series.\n\nVersion: v1.0.0 (427b8e9)\n\n56\n\nChapter 2: Introduction to Prometheus\n\n NOTE This is roughly the same notation that OpenTSDB uses, which in\n\nturn was influenced by Borgmon.\n\nMetrics retention\n\nPrometheus is designed for short-term monitoring and alerting needs. By default, it keeps 15 days of time series locally in its database. If you want to retain data for longer, the recommended approach is to send the required data to remote, third-party platforms. Prometheus has the ability to write to external data stores, which we’ll see in Chapter 7.\n\nSecurity model\n\nPrometheus can be configured and deployed in a wide variety of ways. It makes two broad assumptions about trust:\n\nThat untrusted users will be able to access the Prometheus server’s HTTP\n\nAPI and hence all the data in the database.\n\nThat only trusted users will have access to the command line, configuration files, rule files, and runtime configuration of Prometheus and its compo- nents.\n\n TIP Since Prometheus 2.0, some administrative elements of the HTTP API\n\nare disabled by default.\n\nVersion: v1.0.0 (427b8e9)\n\n57\n\nChapter 2: Introduction to Prometheus\n\nAs such, Prometheus and its components do not provide any server-side authen- tication, authorization, or encryption. If you are working in a more secure envi- ronment you’ll need to implement additional controls yourself—for instance by front-ending the Prometheus server with a reverse proxy or by proxying your ex- porters. Because of the huge potential variations in configuration, this book does not document how to do this.\n\nPrometheus ecosystem\n\nThe Prometheus ecosystem has a mix of components provided by the Prometheus project itself and a rich collection of open-source integrations and tools. The heart of the ecosystem is the Prometheus server that we’ll see in more detail in the next chapter. Also available is Alertmanager, which provides an alerting manager and engine for Prometheus.\n\nThe Prometheus project also includes a collection of exporters, used to instru- ment applications and services and to expose relevant metrics on an endpoint for scraping. Common tools—like web servers, databases, and the like—are sup- ported by core exporters. Many other exporters are available open source from the Prometheus community.\n\nPrometheus also published a collection of client libraries, used for instrumenting applications and services written in a number of languages. These include com- mon choices like Python, Ruby, Go, and Java. Additional client libraries are also available from the open-source community.\n\nUseful Prometheus links\n\nThe Prometheus home page. • The Prometheus documentation. • Prometheus organization on GitHub.\n\nVersion: v1.0.0 (427b8e9)\n\n58\n\nChapter 2: Introduction to Prometheus\n\nPrometheus source code GitHub. • Prometheus and time series at scale presentation by Jamie Wilkinson. • Grafana.\n\nSummary\n\nIn this chapter we’ve been introduced to Prometheus. We also walked through the Prometheus architecture, data model, and other aspects of the ecosystem.\n\nIn the next chapter, we’ll install Prometheus, configure it, and collect our first metrics.\n\nVersion: v1.0.0 (427b8e9)\n\n59\n\nChapter 3\n\nInstallation and Getting Started\n\nIn the last chapter we got an overview of Prometheus. In this chapter, we’ll take you through the process of installing Prometheus on a variety of platforms. This chapter doesn’t provide instructions for the full list of supported platforms, but a representative sampling to get you started. We’ll look at installing Prometheus on:\n\nLinux. • Microsoft Windows. • Mac OS X.\n\nThe lessons here for installing Prometheus can be extended to other supported platforms.\n\n NOTE We’ve written the examples in this book assuming Prometheus is\n\nrunning on a Linux distribution. The examples should also work for Mac OS X but might need tweaking for Microsoft Windows.\n\n60",
      "page_number": 59
    },
    {
      "number": 3,
      "title": "Installation and Getting Started",
      "start_page": 73,
      "end_page": 98,
      "detection_method": "regex_chapter",
      "content": "Chapter 3: Installation and Getting Started\n\nWe’ll also explore the basics of Prometheus configuration and scrape our first tar- get: the Prometheus server itself. We’ll then use the metrics scraped to walk through the basics of the inbuilt expression browser and see how to use the Prometheus query language, PromQL, to glean interesting information from our metrics. This will give you a base Prometheus server that we’ll build on in subse- quent chapters.\n\nInstalling Prometheus\n\nPrometheus is shipped as a single binary file. The Prometheus download page con- tains tarballs containing the binaries for specific platforms. Currently Prometheus is supported on:\n\nLinux: 32-bit, 64-bit, and ARM. • Max OS X: 32-bit and 64-bit. • FreeBSD: 32-bit, 64-bit, and ARM. • OpenBSD: 32-bit, 64-bit, and ARM. • NetBSD: 32-bit, 64-bit, and ARM. • Microsoft Windows: 32-bit and 64-bit. • DragonFly: 64-bit.\n\nOlder versions of Prometheus are available from the GitHub Releases page.\n\n NOTE At the time of writing, Prometheus was at version 2.3.0.\n\nTo get started, we’re going to show you how to manually install Prometheus in the next few sections. At the end of this section we’ll also provide some links to configuration management modules for installing Prometheus. If you’re deploying\n\nVersion: v1.0.0 (427b8e9)\n\n61\n\nChapter 3: Installation and Getting Started\n\nPrometheus into production or at scale you should always choose configuration management as the installation approach.\n\nInstalling Prometheus on Linux\n\nTo install Prometheus on a 64-bit Linux host, we first download the binary file. We can use wget or curl to get the file from the download site.\n\nListing 3.1: Download the Prometheus tarball\n\n$ cd /tmp $ wget https://github.com/prometheus/prometheus/releases/ download/v2.3.0/prometheus-2.3.0.linux-amd64.tar.gz\n\nNow let’s unpack the prometheus binary from the tarball and move it somewhere useful. We’ll also install promtool, which is a linter for Prometheus configuration.\n\nListing 3.2: Unpack the prometheus binary\n\n$ tar -xzf prometheus-2.3.0.linux-amd64.tar.gz $ sudo cp prometheus-2.3.0.linux-amd64/prometheus /usr/local/bin/\n\n$ sudo cp prometheus-2.3.0.linux-amd64/promtool /usr/local/bin/\n\nWe can test if Prometheus is installed and in our path by checking its version using the --version flag.\n\nVersion: v1.0.0 (427b8e9)\n\n62\n\nChapter 3: Installation and Getting Started\n\nListing 3.3: Checking the Prometheus version on Linux\n\n$ prometheus --version prometheus, version 2.3.0 (branch: HEAD, revision: 3569 eef8b1bc062bb5df43181b938277818f365b) root@bd4857492255 20171006-22:16:15 go1.9.1\n\nbuild user: build date: go version:\n\nNow that we have Prometheus installed, you can skip down to looking at its con- figuration, or you can continue to see how we install it on other platforms.\n\nInstalling Prometheus on Microsoft Windows\n\nTo install Prometheus on Microsoft Windows we need to download the prometheus .exe executable and put it in a directory. Let’s create a directory for the executable using Powershell.\n\nListing 3.4: Creating a directory on Windows\n\nC:\\> MKDIR prometheus C:\\> CD prometheus\n\nNow download Prometheus from the GitHub site:\n\nListing 3.5: Prometheus Windows download\n\nhttps://github.com/prometheus/prometheus/releases/download/v 2.3.0/prometheus-2.3.0.windows-amd64.tar.gz\n\nVersion: v1.0.0 (427b8e9)\n\n63\n\nChapter 3: Installation and Getting Started\n\nUnzip the executable using a tool like 7-Zip and put the contents of the unzipped directory into the C:\\prometheus directory.\n\nFinally, add the C:\\prometheus directory to the path. This will allow Windows to find the executable. To do this, run this command inside Powershell.\n\nListing 3.6: Setting the Windows path\n\n$env:Path += \";C:\\prometheus\"\n\nYou should now be able to run the prometheus.exe executable.\n\nListing 3.7: Checking the Prometheus version on Windows\n\nC:\\> prometheus.exe --version prometheus, version 2.3.0 (branch: HEAD, revision: 3569 eef8b1bc062bb5df43181b938277818f365b) root@bd4857492255 20171006-22:16:15 go1.9.1\n\nbuild user: build date: go version:\n\nYou can use something like nssm, the Non-Sucking Service Manager, if you want to run the Prometheus server as a service.\n\nAlternative Microsoft Windows installation\n\nYou can also use a package manager to install Prometheus on Windows. The Chocolatey package manager has a Prometheus package available. You can use these instructions to install Chocolatey and then use the choco binary to install Prometheus.\n\nVersion: v1.0.0 (427b8e9)\n\n64\n\nChapter 3: Installation and Getting Started\n\nListing 3.8: Installing Prometheus via Chocolatey\n\nC:\\> choco install prometheus\n\nAlternative Mac OS X installation\n\nIn addition to being available as a binary for Mac OS X, Prometheus is also avail- able from Homebrew. If you use Homebrew to provision your Mac OS X hosts then you can install Prometheus via the brew command.\n\nListing 3.9: Installing Prometheus via Homebrew\n\n$ brew install prometheus\n\nHomebrew will install the prometheus binary into the /usr/local/bin directory. We can test that it is operating via the prometheus --version command.\n\nListing 3.10: Checking the Prometheus version on Mac OS X\n\n$ prometheus --version prometheus, version 2.3.0 (branch: HEAD, revision: 3569 eef8b1bc062bb5df43181b938277818f365b) root@bd4857492255 20171006-22:16:15 go1.9.1\n\nbuild user: build date: go version:\n\nVersion: v1.0.0 (427b8e9)\n\n65\n\nChapter 3: Installation and Getting Started\n\nStacks\n\nIn addition to installing Prometheus standalone, there are several prebuilt stacks available. These combine Prometheus with other tools—the Grafana console, for instance.\n\nA Prometheus, Node Exporter, and Grafana docker-compose stack. • Another Docker Compose single-node stack with Prometheus, Alertmanager, Node Exporter, and Grafana.\n\nA Docker Swarm stack for Prometheus.\n\nInstalling via configuration management\n\nThere are also configuration management resources available for installing Prometheus. Here are some examples for a variety of configuration management tools:\n\nA Puppet module for Prometheus. • A Chef cookbook for Prometheus. • An Ansible role for Prometheus. • A SaltStack formula for Prometheus.\n\n TIP Remember that configuration management is the recommended ap-\n\nproach for installing and managing Prometheus!\n\nVersion: v1.0.0 (427b8e9)\n\n66\n\nChapter 3: Installation and Getting Started\n\nDeploying via Kubernetes\n\nLast, there are many ways to deploy Prometheus on Kubernetes. The best way for you to deploy likely depends greatly on your environment. You can build your own deployments and expose Prometheus via a service, use one of a number of bundled configurations, or you can use the Prometheus Operator from CoreOS.\n\nConfiguring Prometheus\n\nNow that we have Prometheus installed let’s look at its configuration. Prometheus is configured via YAML configuration files. When we run the prometheus bi- nary (or prometheus.exe executable on Windows), we specify a configuration file. Prometheus ships with a default configuration file: prometheus.yml. The file is in the directory we’ve just unpacked. Let’s take a peek at it.\n\n TIP YAML configuration is fiddly and can be a real pain. You can validate\n\nYAML online at YAML Lint or from the command line with a tool like this.\n\nVersion: v1.0.0 (427b8e9)\n\n67\n\nChapter 3: Installation and Getting Started\n\nListing 3.11: The default Prometheus configuration file\n\nglobal:\n\n15s scrape_interval: evaluation_interval: 15s\n\nalerting:\n\nalertmanagers: - static_configs:\n\ntargets:\n\n# - alertmanager:9093\n\nrule_files:\n\n# - \"first_rules.yml\" # - \"second_rules.yml\"\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\n NOTE We’ve removed some comments from the file for brevity’s sake. The\n\ndefault file changes from time to time, so yours might not look exactly like this one.\n\nOur default configuration file has four YAML blocks defined: global, alerting, rule_files, and scrape_configs.\n\nLet’s look at each block.\n\nVersion: v1.0.0 (427b8e9)\n\n68\n\nChapter 3: Installation and Getting Started\n\nGlobal\n\nThe first block, global, contains global settings for controlling the Prometheus server’s behavior.\n\nThe first setting, the scrape_interval parameter, specifies the interval between scrapes of any application or service—in our case, 15 seconds. This value will be the resolution of your time series, the period in time that each data point in the series covers.\n\nIt is possible to override this global scrape interval when collecting metrics from specific places. Do not do this. Keep a single scrape interval globally across your server. This ensures that all your time series data has the same resolution and can be combined and calculated together. If you override the global scrape inter- val, you risk having incoherent results from trying to compare data collected at different intervals.\n\n WARNING Only configure scrape intervals globally and keep resolution\n\nconsistent!\n\nThe evaluation_interval tells Prometheus how often to evaluate its rules. Rules come in two major flavors: recording rules and alerting rules:\n\nRecording rules - Allow you to precompute frequent and expensive expres-\n\nsions and to save their result as derived time series data.\n\nAlerting rules - Allow you to define alert conditions.\n\nWith this parameter, Prometheus will (re-)evaluate these rules every 15 seconds. We’ll see more about rules in subsequent chapters.\n\nVersion: v1.0.0 (427b8e9)\n\n69\n\nChapter 3: Installation and Getting Started\n\n NOTE You can find the full Prometheus configuration reference in the\n\ndocumentation.\n\nAlerting\n\nThe second block, alerting, configures Prometheus’ alerting. As we mentioned in the last chapter, alerting is provided by a standalone tool called Alertmanager. Alertmanager is an independent alert management tool that can be clustered.\n\nListing 3.12: Alertmanager configuration\n\nalerting:\n\nalertmanagers: - static_configs:\n\ntargets:\n\n# - alertmanager:9093\n\nIn our default configuration, the alerting block contains the alerting configura- tion for our server. The alertmanagers block lists each Alertmanager used by this Prometheus server. The static_configs block indicates we’re going to specify any Alertmanagers manually, which we have done in the targets array.\n\n TIP Prometheus also supports service discovery for Alertmanagers—for ex-\n\nample, rather than specifying each Alertmanager individually, you could query an external source like a Consul server to return a list of available Alertmanagers. We’ll see more about this in Chapters 5 and 6.\n\nVersion: v1.0.0 (427b8e9)\n\n70\n\nChapter 3: Installation and Getting Started\n\nIn our case we don’t have an Alertmanager defined; instead we have a commented- out example at alertmanager:9093. We can leave this commented out because you don’t specifically need an Alertmanager defined to run Prometheus. We’ll add an Alertmanager and configure it in Chapter 6.\n\n TIP We’ll see more about alerting in Chapter 6 and clustering alerting in\n\nChapter 7.\n\nRule files\n\nThe third block, rule_files, specifies a list of files that can contain recording or alerting rules. We’ll make some use of these in the next chapter.\n\nScrape configuration\n\nThe last block, scrape_configs, specifies all of the targets that Prometheus will scrape.\n\nAs we discovered in the last chapter, Prometheus calls the source of metrics it can scrape endpoints. To scrape an endpoint, Prometheus defines configuration called a target. This is the information required to perform the scrape—for example, what labels to apply, any authentication required to connect, or other information that defines how the scrape will occur. Groups of targets are called jobs. Inside jobs, each target has a label called instance that uniquely identifies it.\n\nVersion: v1.0.0 (427b8e9)\n\n71\n\nChapter 3: Installation and Getting Started\n\nListing 3.13: The default Prometheus scrape configuration\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\nOur default configuration has one job defined called prometheus. Inside this job we have a static_config block, which lists the targets this job will scrape. The static_config block indicates that we’re going to individually list the targets we want to scrape, rather than use any automated service discovery method. You can think about static configuration as manual or human service discovery.\n\n TIP We’re going to look at methods to automatically discover targets to be\n\nscraped in Chapter 5.\n\nThe default prometheus job has one target: It scrapes localhost on port 9090, which returns the server’s own health metrics. Prometheus assumes that metrics will be returned on the path /metrics, so it appends this to the target and scrapes the address http://localhost:9090/ metrics.\n\nthe Prometheus server itself.\n\n TIP You can override the default metrics path.\n\nVersion: v1.0.0 (427b8e9)\n\n72\n\nChapter 3: Installation and Getting Started\n\nStarting the server\n\nLet’s start the server and see what happens. First, though, let’s move our configu- ration file somewhere more suitable.\n\nListing 3.14: Moving the configuration file\n\n$ sudo mkdir -p /etc/prometheus $ sudo cp prometheus.yml /etc/prometheus/\n\nHere we’ve created a directory, /etc/prometheus, to hold our configuration file, and we’ve moved our new file into this directory.\n\nListing 3.15: Starting the Prometheus server\n\n$ prometheus --config.file \"/etc/prometheus/prometheus.yml\" level=info ts=2017-10-23T14:03:02.274562Z caller=main.go:216 msg =\"Starting prometheus\"...\n\nWe run the binary and specify our configuration file in the --config.file com- mand line flag. Our Prometheus server is now running and scraping the instances of the prometheus job and returning the results.\n\nIf something doesn’t work, you can validate your configuration with promtool, a linter that ships with Prometheus.\n\nListing 3.16: Validating your configuration with promtool\n\n$ promtool check config prometheus.yml Checking prometheus.yml\n\nSUCCESS: 0 rule files found\n\nVersion: v1.0.0 (427b8e9)\n\n73\n\nChapter 3: Installation and Getting Started\n\nRunning Prometheus via Docker\n\nIt’s also easy to run Prometheus in Docker. There’s a Docker image provided by the Prometheus team available on the Docker Hub. You can execute it with the docker command.\n\nListing 3.17: Running Prometheus with Docker\n\n$ docker run -p 9090:9090 prom/prometheus\n\nThis will run a Prometheus server locally, with port 9090 bound to port 9090 inside the Docker container. You can then browse to that port on your local host to see your Prometheus server. The server is launched with a default configuration, and you will need to provide custom configuration and data storage. You can take a number of approaches here—for example, you could mount a configuration file into the container.\n\nListing 3.18: Mounting a configuration file into the Docker container\n\n$ docker run -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/ prometheus.yml prom/prometheus\n\nThis would bind mount the file /tmp/prometheus.yml into the container as the Prometheus server’s configuration file.\n\n TIP You can find more information on running Prometheus with Docker in\n\nthe documentation.\n\nVersion: v1.0.0 (427b8e9)\n\n74\n\nChapter 3: Installation and Getting Started\n\nFirst metrics\n\nNow that the server is running, let’s take a look at the endpoint we are scraping and see some raw Prometheus metrics. To do this, let’s browse to the URL http ://localhost:9090/metrics and see what gets returned.\n\n NOTE In all our examples we assume you’re browsing on the server running Prometheus, hence localhost.\n\nListing 3.19: Some sample raw metrics\n\n# HELP go_gc_duration_seconds A summary of the GC invocation durations. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 1.6166e−05 go_gc_duration_seconds{quantile=\"0.25\"} 3.8655e−05 go_gc_duration_seconds{quantile=\"0.5\"} 5.3416e−05 . . .\n\nHere we can see our first Prometheus metrics. These look much like the data model we saw in the last chapter.\n\nListing 3.20: A raw metric\n\ngo_gc_duration_seconds{quantile=\"0.5\"} 1.6166e−05\n\nThe name of our metric is go_gc_duration_seconds. We can see one label on the metric, quantile=\"0.5\", indicating this is measuring the 50th percentile, and the value of the metric.\n\nVersion: v1.0.0 (427b8e9)\n\n75\n\nChapter 3: Installation and Getting Started\n\nPrometheus expression browser\n\nIt is not user friendly to view our metrics this way, though, so let’s make use of Prometheus’ inbuilt expression browser. It’s available on the Prometheus server by browsing to http://localhost:9090/graph.\n\n TIP The Prometheus Expression browser and web interface have other use-\n\nful information, like the status of targets and the rules and configuration of the Prometheus server. Make sure you check out all the interface menu items.\n\nFigure 3.1: Prometheus expression browser\n\nLet’s find the go_gc_duration_seconds metric using the expression browser. To do this, we can either open the dropdown list of available metrics or we can type the metric name into the query box. We then click the Execute button to display\n\nVersion: v1.0.0 (427b8e9)\n\n76\n\nChapter 3: Installation and Getting Started\n\nall the metrics with this name.\n\nFigure 3.2: List of metrics\n\nWe can see a list of metrics here, each decorated with one or more labels. Let’s find the 50th percentile in the list.\n\nListing 3.21: Go garbage collection 50th percentile\n\ngo_gc_duration_seconds{instance=\"localhost:9090\",job=\"prometheus \",quantile=\"0.5\"}\n\nWe can see that two new labels have been added to our metrics. This has been done automatically by Prometheus during the scrape process. The first new label, instance, is the target from which we scraped the metrics. The second label, job, is the name of the job that scraped the metrics. Labels provide dimensions to our metrics. They allow us to query or work with multiple or specific metrics—for example, Go garbage collection metrics for multiple targets.\n\n TIP We’ll see a lot more about labels in the next chapter and later in the\n\nbook.\n\nVersion: v1.0.0 (427b8e9)\n\n77\n\nChapter 3: Installation and Getting Started\n\nPrometheus has a highly flexible expression language called PromQL built into the server, allowing you to query and aggregate metrics. We can use this query language in the query input box at the top of the interface.\n\nFigure 3.3: Querying quantiles\n\nHere we’ve queried all metrics with a label of quantile=\"0.5\" and it has returned a possible 86 metrics. This set is one of the four data types that expressions in the PromQL querying language can return. This type is called an instant vector: a set of time series containing a single sample for each time series, all sharing the same timestamp. We can also return instant vectors for metrics by querying a name and a label. Let’s go back to our go_gc_duration_seconds but this time the 75th percentile. Specify:\n\ngo_gc_duration_seconds{quantile=\"0.75\"}\n\nIn the input box and click Execute to search. It should return an instant vector that matches the query. We can also negate or match a label using a regular expression.\n\ngo_gc_duration_seconds{quantile!=\"0.75\"}\n\nThis will return an instant vector of all the metrics with a quantile label not equal to 0.75.\n\nVersion: v1.0.0 (427b8e9)\n\n78\n\nChapter 3: Installation and Getting Started\n\n TIP If we’re used to tools like Graphite, querying labels is like parsing dotted-\n\nstring named metrics. There’s a blog post that provides a side-by-side comparison of how Graphite, InfluxDB, and Prometheus handle a variety of queries.\n\nLet’s look at another metric, this one called prometheus_build_info, that contains information about the Prometheus server’s build. Put prometheus_build_info into the expression browser’s query box and click Execute to return the metric. You’ll see an entry like so:\n\nListing 3.22: The prometheus_build_info metric\n\nprometheus_build_info{branch=\"HEAD\",goversion=\"go1.9.1\",instance =\"localhost:9090\",job=\"prometheus\",revision=\"5 ab8834befbd92241a88976c790ace7543edcd59\",version=\"2.3.0\"}\n\nYou can see the metric is heavily decorated with labels and has a value of 1. This is a common pattern for passing information to the Prometheus server using a metric. It uses a metric with a perpetual value of 1, and with the relevant information you might want attached via labels. We’ll see more of these types of informational metrics later in the book.\n\nTime series aggregation\n\nThe interface can also do complex aggregation of metrics. Let’s choose another metric, http_requests_total, which is the total HTTP requests made by various handlers in the Prometheus server. Query for that now by specifying its name and clicking Execute.\n\nVersion: v1.0.0 (427b8e9)\n\n79\n\nChapter 3: Installation and Getting Started\n\nFigure 3.4: Querying total HTTP requests\n\nWe have a list of HTTP request metrics. But what we really want is the total HTTP requests per job. To do this, we need to create a new metric via a query. Prometheus’ querying language, PromQL, has a large collection of expressions and functions that can help us do this.\n\nLet’s start by summing the HTTP requests by job. Add the following to the query box and click Execute.\n\nsum(http_requests_total)\n\nThis new query uses the sum() operator on the http_requests_total metric. It adds up all of the requests but doesn’t break it down by job. To do that we need to aggregate over a specific label dimension. PromQL has a clause called by that will allow us to aggregate by a specific dimension. Add the following to the query box and then click Execute.\n\nsum(http_requests_total) by (job)\n\n TIP PromQL also has a clause called without that aggregates without a spe-\n\ncific dimension.\n\nVersion: v1.0.0 (427b8e9)\n\n80\n\nChapter 3: Installation and Getting Started\n\nYou should see something like the following output:\n\nFigure 3.5: Calculating total HTTP requests by job\n\nNow click the Graph tab to see this metric represented as a plot.\n\n TIP The folks at Robust Perception have a great blog post on common query-\n\ning patterns.\n\nThe new output is still not quite useful—let’s convert it into a rate. Update our query to:\n\nsum(rate(http_requests_total[5m])) by (job)\n\nHere we’ve added a new function: rate(). We’ve inserted it inside our sum func- tion.\n\nrate(http_requests_total[5m])\n\nThe rate() function calculates the per-second average rate of increase of the time series in a range. The rate function should only be used with counters. It is quite clever and automatically adjusts for breaks, like a counter being reset when the resource is restarted, and extrapolates to take care of gaps in the time series, such as a missed scrap. The rate() function is best used for slower-moving counters or for alerting purposes.\n\nVersion: v1.0.0 (427b8e9)\n\n81\n\nChapter 3: Installation and Getting Started\n\n TIP There’s also an irate() function to calculate the instant rate of increase\n\nfor faster-moving timers.\n\nHere we’re calculating the rate over a five-minute range vector. Range vectors are a second PromQL data type containing a set of time series with a range of data points over time for each time series. Range vectors allow us to display the time series for that period. The duration of the range is enclosed in [] and has an integer value followed by a unit abbreviation:\n\ns for seconds. • m for minutes. • h for hours. • d for days. • w for weeks. • y for years.\n\nSo here [5m] is a five-minute range.\n\n TIP The other two PromQL data types are Scalars, numeric floating-point values, and Strings, which is a string value and is currently unused.\n\nLet’s Execute that query and see the resulting range vector of time series.\n\nVersion: v1.0.0 (427b8e9)\n\n82\n\nChapter 3: Installation and Getting Started\n\nFigure 3.6: Our rate query\n\nCool! We’ve now got a new metric that is actually useful for tracking or graphing.\n\n TIP If you want help constructing PromQL queries, there’s a query editor\n\ncalled Promeditor available that you can run locally with Prometheus.\n\nNow that we’ve walked through the basics of Prometheus operation, let’s look at some of the requirements for running a Prometheus server.\n\nCapacity planning\n\nPrometheus performance is hard to estimate because it depends greatly on your configuration, the volume of time series you collect, and the complexity of any rules on the server. There are two capacity concerns: memory and disk.\n\n TIP We’ll look at Prometheus scaling concepts in Chapter 7.\n\nVersion: v1.0.0 (427b8e9)\n\n83\n\nChapter 3: Installation and Getting Started\n\nMemory\n\nPrometheus does a lot in memory. It consumes process memory for each time series collected and for querying, recording rules, and the like. There’s not a lot of data on capacity planning for Prometheus, especially since 2.0 was released, but a good, rough, rule of thumb is to multiply the number of samples being collected per second by the size of the samples. We can see the rate of sample collection using this query.\n\nrate(prometheus_tsdb_head_samples_appended_total[1m])\n\nThis will show you the per-second rate of samples being added to the database over the last minute.\n\nIf you want to know the number of metrics you’re collecting you can use:\n\nsum(count by (__name__)({__name__=\\~\"\\.\\+\"}))\n\nThis uses the sum aggregation to add up a count of all metrics that match, using the =~ operator, the regular expression of .+, or all metrics.\n\nEach sample is generally one to two bytes in size. Let’s err on the side of caution and use two bytes. Assuming we’re collecting 100,000 samples per second for 12 hours, we can work out memory usage like so:\n\n100,000 * 2 bytes * 43200 seconds\n\nOr roughly 8.64 GB of RAM.\n\nYou’ll also need to factor in memory use for querying and recording rules. This is very rough and dependent on a lot of other variables. I recommend playing things by ear with regard to memory usage. You can see the memory usage of the Prometheus process by checking the process_resident_memory_bytes metric.\n\nVersion: v1.0.0 (427b8e9)\n\n84\n\nChapter 3: Installation and Getting Started\n\nDisk\n\nDisk usage is bound by the volume of time series stored and the retention of those time series. By default, metrics are stored for 15 days in the local time series database. The location of the database and the retention period are controlled by command line options.\n\nThe --storage.tsdb.path option, which has a default directory of data lo- cated in the directory from which you are running Prometheus, controls your time series database location.\n\nThe --storage.tsdb.retention controls retention of time series. The de-\n\nfault is 15d representing 15 days.\n\n TIP The best disk for time series databases is SSD. You should use SSDs.\n\nFor our 100,000 samples per second example, we know each sample collected in a time series occupies about one to two bytes on disk. Assuming two bytes per sample, then a time series retained for 15 days would mean needing about 259 GB of disk.\n\n TIP There’s more information on Prometheus disk usage in the Storage doc-\n\numentation.\n\nVersion: v1.0.0 (427b8e9)\n\n85\n\nChapter 3: Installation and Getting Started\n\nSummary\n\nIn this chapter we installed Prometheus and configured its basic operation. We also scraped our first target, the Prometheus server itself. We made use of the met- rics collected by the scrape to see how the inbuilt expression browser works, in- cluding graphing our metrics and deriving new metrics using Prometheus’s query language, PromQL.\n\nIn the next chapter we’ll use Prometheus to collect some host metrics, including collecting from Docker containers. We’ll also see a lot more about scraping, jobs, and labels, and we’ll have our first introduction to recording rules.\n\nVersion: v1.0.0 (427b8e9)\n\n86",
      "page_number": 73
    },
    {
      "number": 4,
      "title": "Monitoring Nodes and Containers",
      "start_page": 99,
      "end_page": 162,
      "detection_method": "regex_chapter",
      "content": "Chapter 4\n\nMonitoring Nodes and Containers\n\nIn the last chapter we installed Prometheus and did some basic configuration. We also scraped some time series data from the Prometheus server itself. In this chapter, we’re going to look at using Prometheus to monitor the metrics of both hosts and containers. We’re going to demonstrate this on a cluster of three Ubuntu hosts running the Docker daemon.\n\nFirst, we’ll install exporters on each host, configure exporting of node and Docker metrics, and configure Prometheus to scrape them.\n\nNext, we’ll look at monitoring some basic host resources, including:\n\n1. CPU. 2. Memory. 3. Disk. 4. Availability.\n\nTo determine what to monitor, we’ll revisit the USE Method monitoring method- ology to help assist in identifying the right metrics. We’ll also look at how we might use Prometheus to detect the state of services and the availability of hosts.\n\nThen we’ll make use of the collected metrics to build some aggregated metrics and save them as recording rules.\n\n87\n\nChapter 4: Monitoring Nodes and Containers\n\nLast, we’ll very briefly introduce Grafana to do basic visualizations of some of the data we’re collecting.\n\nThese are probably the most standard tasks for which monitoring tools are de- ployed, and they provide a solid foundation for learning more about Prometheus. This base set of data will allow us to identify host performance issues or will pro- vide sufficient supplemental data for the fault diagnosis of application issues.\n\nMonitoring nodes\n\nPrometheus uses tools called exporters to expose metrics on hosts and applica- tions. There are a number of exporters available for a variety of purposes. Right now we’re going to focus on one specific exporter: the Node Exporter. The Node Exporter is written in Go and has a library of collectors for various host metrics including CPU, memory, and disk. It also has a textfile collector that allows you to export static metrics, which is useful for sending information about the node, as we’ll see shortly, or metrics exported from batch jobs.\n\n NOTE We’ll use the term “node” at times to refer to hosts.\n\nLet’s start by downloading and installing the Node Exporter on a Linux host. We’re going to choose one of our Docker daemon hosts.\n\n TIP If you don’t want to use one of the Prometheus exporters there are a\n\nswath of host-monitoring clients that support Prometheus. For example, collectd can also write Prometheus metrics.\n\nVersion: v1.0.0 (427b8e9)\n\n88\n\nChapter 4: Monitoring Nodes and Containers\n\nInstalling the Node Exporter\n\nThe Node Exporter is available as a tarball and for a limited number of platforms via packages. The tarball of the Node Exporter is available, with a number of other exporters, from the Prometheus website.\n\nLet’s download and extract the Node Exporter for Linux and move the binary into our path.\n\nListing 4.1: Downloading the Node Exporter\n\nwget https://github.com/prometheus/node_exporter/releases/download/v 0.16.0/node_exporter-0.16.0.linux-amd64.tar.gz $ tar -xzf node_exporter-* $ sudo cp node_exporter-*/node_exporter /usr/local/bin/\n\n NOTE At the time of writing the Node Exporter was at version 0.16.0. You\n\nshould download the latest version.\n\nThe Node Exporter is also available as a CentOS and Fedora package via a COPR build.\n\n NOTE Using configuration management is the best way to run and install\n\nany Prometheus exporters. This is an easy way to control configuration, and to provide automation and service management.\n\nLet’s test that the node_exporter binary is working.\n\nVersion: v1.0.0 (427b8e9)\n\n89\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.2: Testing the Node Exporter binary\n\n$ node_exporter --version node_exporter, version 0.16.0 (branch: HEAD, revision: 6 e2053c557f96efb63aef3691f15335a70baaffd) . . .\n\nConfiguring the Node Exporter\n\nThe node_exporter binary is configured via flags. You can see a full list of flags by running the binary with the --help flag.\n\nListing 4.3: Running the help for Node Exporter\n\n$ node_exporter --help\n\nYou’ll see a list of available flags. The node_exporter exporter runs, by default, on port 9100 and exposes metrics on the /metrics path. You can control the interface and port via the --web.listen-address and --web.telemetry-path flags like so:\n\nListing 4.4: Controlling the port and path\n\n$ node_exporter --web.listen-address=\":9600\" --web.telemetry- path=\"/node_metrics\"\n\nThis will bind the node_exporter to port 9600 and return metrics on the /node- metrics path.\n\nThese flags also control which collectors are enabled. By default, many of the collectors are enabled. Collectors either have a disabled or enabled status, and\n\nVersion: v1.0.0 (427b8e9)\n\n90\n\nChapter 4: Monitoring Nodes and Containers\n\nthe status can be flipped by specifying the relevant flag with a no- prefix. For ex- ample, the arp collector, which exposes statistics from /proc/net/arp, is enabled by default. It is controlled by the --collector.arp flag. To disable this collector we’d run:\n\nListing 4.5: Disabling the arp collector\n\n$ node_exporter --no-collector.arp\n\nConfiguring the Textfile collector\n\nWe also want to configure one specific collector, the textfile collector, that we’re going to use later in this chapter. The textfile collector is very useful because it allows us to expose custom metrics. These custom metrics might be the result of tasks like batch or cron jobs, which can’t be scraped; they might come from sources that don’t have an exporter; or they might even be static metrics which provide context for the host.\n\nThe collector works by scanning files in a specified directory, extracting any strings that are formatted as Prometheus metrics, and exposing them to be scraped.\n\nLet’s set the collector up now, starting with creating a directory to hold our the metric definition files.\n\nListing 4.6: Creating a textfile directory\n\n$ mkdir -p /var/lib/node_exporter/textfile_collector\n\nNow let’s create a new metric in this directory. Metrics are defined in files ending in .prom inside the directory we’ve just created. Metrics are defined using the Prometheus text exposition format.\n\nVersion: v1.0.0 (427b8e9)\n\n91\n\nChapter 4: Monitoring Nodes and Containers\n\n NOTE The text exposition format allows us to specify all the metric types\n\nthat Prometheus supports: counters, gauges, timers, etc.\n\nLet’s use this format to create a metric that will contain some metadata about this host.\n\nmetadata{role=\"docker_server\",datacenter=\"NJ\"} 1\n\nWe can see we have a metric name, metadata, and two labels. One label is called role to define a role for this node. In this case this label has a value of docker_server. We also have a label called datacenter to define the geograph- ical location of the host. Finally, the metric has a static value of 1 because it’s providing context rather than recording a counter, gauge, or timer.\n\nLet’s add this metric to a file called metadata.prom in our textfile_collector directory.\n\nListing 4.7: A metadata metric\n\n$ echo 'metadata{role=\"docker_server\",datacenter=\"NJ\"} 1' | sudo\n\ntee /var/lib/node_exporter/textfile_collector/metadata.prom\n\nHere we’ve piped our metric into a file called metadata.prom.\n\n TIP In the real world, you’d populate this file using your configuration man- agement tool. For example, when a new host is provisioned, a metadata metric could be created from a template. This could allow you to automatically classify your hosts and services.\n\nVersion: v1.0.0 (427b8e9)\n\n92\n\nChapter 4: Monitoring Nodes and Containers\n\nTo enable the textfile collector we don’t need to set a flag—it’s loaded by default—but we do need to specify our textfile_exporter directory so the Node Exporter knows where to find our custom metrics. To do this, we specify the --collector.textfile.directory flag.\n\nEnabling the systemd collector\n\nLet’s also turn on an extra collector, systemd, which records services and system status from systemd. This collector gathers a lot of metrics, but we don’t want to collect the status of everything systemd is managing, just some key services. To keep things clean, we can whitelist specific services. We’re only going to collect metrics for:\n\ndocker.service • ssh.service • rsyslog.service\n\nWhich are the Docker daemon, the SSH daemon, and the RSyslog daemon. We do this using the --collector.systemd.unit-whitelist flag, which takes a regular expression matching systemd units.\n\nRunning the Node Exporter\n\nFinally, we can launch node_exporter on one of our Docker nodes like so:\n\nListing 4.8: Starting Node Exporter with the textfile collector and systemd\n\n$ node_exporter --collector.textfile.directory /var/lib/ node_exporter/textfile_collector --collector.systemd --collector. systemd.unit-whitelist=(docker|ssh|rsyslog).service\n\nVersion: v1.0.0 (427b8e9)\n\n93\n\nChapter 4: Monitoring Nodes and Containers\n\nWe’ve specified the directory for the textfile collector to find our metrics, en- abled the systemd collector, and used a regular expression whitelist to match the three services for which we want to collect metrics.\n\nNow that the Node Exporter is running on one of our Docker daemon nodes, let’s add it to the others. We have three nodes, and we’ve identically configured two of them. The name and IP address of each node is:\n\nDocker1 - 138.197.26.39 • Docker2 - 138.197.30.147 • Docker3 - 138.197.30.163\n\nNow let’s see how to scrape the time series data that we’ve just exported.\n\nScraping the Node Exporter\n\nBack on our Prometheus server, let’s configure a new job to scrape the data ex- ported by the Node Exporter. Let’s examine the scrape_configs block from our current prometheus.yml file and our existing scrape configuration.\n\nListing 4.9: The current Prometheus scrape configuration\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\nTo get this new data, we need to add another job to this configuration. We’re going to call our new job node. We’re also going to continue to add individual targets using static_configs, rather than by using any kind of service discovery. (We’ll see more about service discovery in the next chapter.) Let’s add that new job now.\n\nVersion: v1.0.0 (427b8e9)\n\n94\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.10: Adding the node job\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\njob_name: 'node' static_configs:\n\ntargets: ['138.197.26.39:9100', '138.197.30.147:9100', '\n\n138.197.30.163:9100']\n\nYou can see that we’ve added the new job called node. It contains a static_configs block with a list of our three Docker hosts listed via their IP addresses and the relevant port, 9100. Prometheus assumes the Node Exporter has the default path, /metrics, and scrapes a target of:\n\n138.197.26.39:9100/metrics\n\nIf we now SIGHUP or restart the Prometheus server, our configuration will be reloaded and the server will start scraping. We’ll see the time series data start flowing into the Prometheus server shortly.\n\nFiltering collectors on the server\n\nThe Node Exporter can return a lot of metrics though, and perhaps you don’t want to collect them all. In addition to controlling which collectors the Node Exporter runs locally via local configuration, Prometheus also has a way we can limit the collectors actually scraped from the server side. This is especially useful when you don’t control the configuration of the host you’re scraping.\n\nPrometheus achieves this by adding a list of the specific collectors to scrape to our job configuration.\n\nVersion: v1.0.0 (427b8e9)\n\n95\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.11: Filtering collectors\n\nscrape_configs: . . .\n\njob_name: 'node' static_configs:\n\ntargets: ['138.197.26.39:9100', '138.197.30.147:9100', '\n\n138.197.30.163:9100']\n\nparams:\n\ncollect[]: - cpu - meminfo - diskstats - netdev - netstat - filefd - filesystem - xfs - systemd\n\nHere we’ve limited the metrics being scraped to this list of collectors, specified using the collect[] list inside the params block. These are then passed to the scrape request as URL parameters. You can test this using the curl command on a Node Exporter instance.\n\nListing 4.12: Testing collect params\n\n$ curl -g -X GET http://138.197.26.39:9100/metrics?collect[]=cpu\n\nThis would return the base Node Exporter metrics, like the Go metrics we saw for the Prometheus server, and the metrics generated by the CPU collector. All other metrics will be disregarded.\n\nFor now though, on our Prometheus server, we’re going to collect everything.\n\nVersion: v1.0.0 (427b8e9)\n\n96\n\nChapter 4: Monitoring Nodes and Containers\n\nNow that we have our node metrics, let’s instrument our Docker daemons too.\n\nMonitoring Docker\n\nThere are several ways to monitor Docker with Prometheus, including several cus- tom exporters. However these exporters have generally been deprecated in favor of the recommended approach: Google’s cAdvisor tool. cAdvisor runs as a Docker container on your Docker daemon. A single cAdvisor container returns metrics for your Docker daemon and all running containers. It has native Prometheus support to export metrics, as well as support for a variety of other storage destinations like InfluxDB, Elasticsearch, and Kafka.\n\n NOTE We’re going to assume that you have installed and are running\n\nDocker daemons, and that you understand the basics of how Docker works. you’re new to Docker, I have a book on it that might interest you.\n\nRunning cAdvisor\n\nAs cAdvisor is just another container on our Docker host, we can launch it with the docker run command. Let’s run a cAdvisor container on our Docker1 host.\n\nVersion: v1.0.0 (427b8e9)\n\nIf\n\n97\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.13: Running the caAdvisor container\n\n$ docker run \\\n\n--volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:rw \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/dev/disk/:/dev/disk:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ google/cadvisor:latest\n\nLet’s break this docker run command down a little. First, we mount a few directo- ries inside the container. The directories are broken into two types. The first are read-only mounts from which cAdvisor will gather data—for example, mounting the /sys directory like so:\n\n--volume=/sys:/sys:ro\n\n TIP The ro indicates read-only.\n\nThe second type, which contains one mount, is a read-write mount of the Docker socket, usually located in the /var/run directory. We also publish port 8080 from inside the container to 8080 on the host. You could override this with any port that suited you. We run the container with the --detach flag to daemonize it and name it cadvisor. Last, we use the google/cadvisor image with the latest tag.\n\nIf we now run docker ps, we can see our running cAdvisor container.\n\nVersion: v1.0.0 (427b8e9)\n\n98\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.14: The cAdvisor container\n\nCONTAINER ID IMAGE\n\nCOMMAND\n\nCREATED\n\nSTATUS\n\nPORTS\n\nNAMES\n\n6fca3002e351 google/cadvisor \"/usr/bin/...\" 1 hours ago Up 1 hours 0.0.0.0:8080->8080/tcp cadvisor\n\ncAdvisor should start monitoring immediately. We can browse to port 8080 on the host to see cAdvisor’s web interface and confirm that it is operational.\n\nFigure 4.1: cAdvisor web interface\n\nIf we browse to the path /metrics on port 8080, we’ll see the built-in Prometheus metrics being exposed.\n\nVersion: v1.0.0 (427b8e9)\n\n99\n\nChapter 4: Monitoring Nodes and Containers\n\nFigure 4.2: cAdvisor Prometheus metrics\n\nWe’ll also install cAdvisor on our other two Docker daemons as well.\n\nScraping cAdvisor\n\nWith cAdvisor running on our Docker daemons, we need to tell Prometheus about it. To do this, we’re going to add a third job to our configuration. Let’s edit prometheus.yml on our Prometheus server.\n\nWe’re again going to add individual targets using static_configs, rather than by using any kind of service discovery.\n\nVersion: v1.0.0 (427b8e9)\n\n100\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.15: Adding the Docker job\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\njob_name: 'node' static_configs:\n\ntargets: ['138.197.26.39:9100', '138.197.30.147:9100', '\n\n138.197.30.163:9100']\n\njob_name: 'docker' static_configs:\n\ntargets: ['138.197.26.39:8080', '138.197.30.147:8080', '\n\n138.197.30.163:8080']\n\nYou can see we’ve added the new job called docker. It contains a static_configs block with a list of our three Docker daemon servers with their IP addresses and the relevant port, 8080. Again we assume the default /metrics path. If we again SIGHUP or restart the Prometheus server, then our configuration will be reloaded, it will start scraping, and the new time series will appear.\n\nI think it’s important, though, before we continue, that we understand how a scrape works and a bit about the lifecycle of labels. We’ll use our cAdvisor metrics to explore this lifecycle.\n\nScrape lifecycle\n\nLet’s look at the lifecycle of a scrape itself, and into the lifecycle of labels. Every scrape_interval period, in our case 15 seconds, Prometheus will check for jobs to be executed. Inside those jobs it’ll generate a list of targets: the service discovery process. In the cases we’ve seen so far we’ve got manually specified, statically configured hosts. There are other service discovery mechanisms, like loading targets from a file or querying an API.\n\nVersion: v1.0.0 (427b8e9)\n\n101\n\nChapter 4: Monitoring Nodes and Containers\n\n TIP We’ll learn more about service discovery in Chapter 5.\n\nService discovery returns a list of targets with a set of labels attached called meta- data. These labels are prefixed with __meta_. Each service discovery mechanism has different metadata—for example, the AWS EC2 discovery mechanism returns the availability zone of instances in a label called __meta_ec2_availability_zone .\n\nService discovery also sets additional labels based on the configuration of the target. These configuration labels are prefixed and suffixed with __. They include the __scheme__, __address__, and __metrics_path__ labels. These contain the scheme, http or https, of the target; the address of the target; and the specific path to the metrics respectively.\n\nEach label usually has a default—for example, __metrics_path__ would default to /metrics, and __scheme__ to http. Additionally, if any URL parameters are present in the path then they’re set into labels prefixed with __param_*.\n\nThe configuration labels are also reused during the lifecycle of the scrape to pop- ulate other labels. For example, the default contents of the instance label on our metrics is the contents of the __address__ label.\n\n NOTE So, wait—why haven’t we seen any of those __ prefixed and suffixed\n\nlabels? That’s because some are removed later in the lifecycle, and all of them are specifically excluded from display on the Web UI.\n\nThis list of targets and labels are then returned to Prometheus. Some of those labels can be overridden in configuration—for example, the metrics path via the metrics_path parameter, and the scheme to be used via the scheme parameter.\n\nVersion: v1.0.0 (427b8e9)\n\n102\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.16: Overriding the discovered labels\n\nscrape_configs:\n\njob_name: 'node' scheme: https metrics_path: /moremetrics static_configs:\n\ntargets: ['138.197.26.39:9100', '138.197.30.147:9100', '\n\n138.197.30.163:9100']\n\nHere we’re overriding the scheme to https and the metric’s path to /moremetrics.\n\nPrometheus then offers an opportunity to relabel your targets and to potentially make use of some metadata your service discovery has added. You can also filter targets to drop or keep specific items.\n\nAfter this, the actual scrape takes place, and the metrics are returned. When the metrics are being scraped you are offered a final opportunity to relabel and filter them before they are saved to the server.\n\nPhew. That’s complicated. Let’s see a simplified image of that lifecycle:\n\nVersion: v1.0.0 (427b8e9)\n\n103\n\nChapter 4: Monitoring Nodes and Containers\n\nFigure 4.3: Scrape lifecycle\n\nYou can see we’ve introduced a bunch of concepts, including two blocks where Prometheus relabels metrics. This is a good time to talk a bit more about labels, relabelling, and taxonomies. Let’s take a little interlude.\n\nLabels\n\nWe learned in Chapter 2 that labels provide the dimensions of our time series. They can define what the target is and add context to the time series. But most importantly, combined with the metric name, they make up the identity of your time series. They represent the identity of your time series—if they change, so does the identity of the time series.\n\nChanging a label or adding a new label creates a new time series.\n\nVersion: v1.0.0 (427b8e9)\n\n104\n\nChapter 4: Monitoring Nodes and Containers\n\nThis means that labels should be used judiciously and remain as constant as possi- ble. Failure to adhere to this can spawn new time series, creating a dynamic data environment that makes your monitoring data sources harder to track. Imagine you have a time series that you’re using to track the state of a service. You have an alert configured for that time series that relies on the labels of the metric to determine the right criteria. By changing or adding a label, that alert definition is rendered invalid. The same applies to historical time series data: By changing or adding a label we lose track of the previous time series, breaking graphs and expressions, and causing general mayhem.\n\n TIP What happens to the old time series if it’s not being written anymore? If\n\na scrape no longer returns data for a time series that was previously present, that series will be marked as stale. The same applies for any targets that are removed: All of their time series will be marked as stale. Stale data is not returned in graphs.\n\nLabel taxonomies\n\nSo when should we add labels and what labels should we add? Well, like all good monitoring architectures, it’s worth building a taxonomy. Labels, like most monitoring taxonomies, are probably best when broadly hierarchical. A good way of thinking about a taxonomy is in terms of topological and schematic labels.\n\nThe topological labels slice the service components by their physical or logical makeup, e.g., the datacenter label we saw above. We already get two topological labels for free with every metric: job and instance. The job label is set from the job name in the scrape configuration. We tend to use job to describe the type of thing we’re monitoring. In the case of our Node Exporter job we called it node\n\nVersion: v1.0.0 (427b8e9)\n\n105\n\nChapter 4: Monitoring Nodes and Containers\n\n. This will label all the Node Exporter metrics with a job label of node. The instance label identifies the target. It’s usually the IP address and port of the target, and it’s usually sourced from the __address__ label.\n\nSchematic labels are things like url, error_code, or user which let you match time series at the same level in the topology together—for example, to create ratios of one against the other.\n\nIf you need to add additional labels consider a hierarchy something like this:\n\nFigure 4.4: Sample label taxonomy\n\nA little later in this chapter, we’ll look at metrics like the metadata metric we created earlier with the Textfile collector that can be decorated with contextual information.\n\nWe can also create and manipulate existing labels to help us better manage our time series data.\n\nVersion: v1.0.0 (427b8e9)\n\n106\n\nChapter 4: Monitoring Nodes and Containers\n\nRelabelling\n\nGiven the desire to judiciously use labels, why would we relabel things? In a word: control. In a centralized, complex monitoring environment you sometimes don’t control all the resources you are monitoring and the monitoring data they expose. Relabelling allows you to control, manage, and potentially standardize metrics in your environment. Some of the most common use cases are:\n\nDropping unnecessary metrics. • Dropping sensitive or unwanted labels from the metrics. • Adding, editing, or amending the label value or label format of the metrics.\n\nRemember there are two phases at which we can relabel. The first phase is re- labelling targets that have come from service discovery. This is most useful for applying information from metadata labels from service discovery into labels on your metrics. This is done in a relabel_configs block inside a job. We’ll see more of that in the next chapter.\n\nThe second phase is after the scrape but before the metric is saved in the storage system. This allows us to determine what metrics we save, what we drop, and what those metrics will look like. This is done in the metric_relabel_configs block in our job.\n\n TIP The easiest way to remember the two phases are: relabel_configs hap- pens before the scrape and metric_relabel_configs happens after the scrape.\n\nLet’s take a look at some relabelling of our cAdvisor metrics. cAdvisor collects a lot of data. Not all of it is always useful. So let’s see how we might drop some of these metrics before they hit our storage and take up unnecessary space.\n\nVersion: v1.0.0 (427b8e9)\n\n107\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.17: Dropping metrics with relabelling\n\njob_name: 'docker' static_configs:\n\ntargets: ['138.197.26.39:8080', '138.197.30.147:8080', '\n\n138.197.30.163:8080']\n\nmetric_relabel_configs:\n\nsource_labels: [__name__]\n\nregex: '(container_tasks_state|\n\ncontainer_memory_failures_total)'\n\naction: drop\n\nHere we have our docker job. After our static_configs block we’ve added a new block: metric_relabel_configs. Inside the block we specify a series of rela- belling actions.\n\nDropping metrics\n\nLet’s look at our first action. We select the metrics we want to take action on using the source_labels parameter. This takes an array of label names. In our case we’re using the __name__ label. The __name__ label is a reserved label used for the name of a metric. So our source label for our docker job in this case would be the concatenated names of all the metrics scraped from cAdvisor.\n\nMultiple labels are concatenated together using a separator, by default ;. The separator can be overridden using the separator parameter.\n\nVersion: v1.0.0 (427b8e9)\n\n108\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.18: Specifying a new separator\n\n. . . metric_relabel_configs:\n\nsource_labels: [__name__]\n\nseparator: ',' regex: '(container_tasks_state|\n\ncontainer_memory_failures_total)'\n\naction: drop\n\nHere our __name__ label values would be separated with a ,.\n\nNext, we specify a regular expression to search our concatenated metric names and match specific names. The regular expression uses the RE2 expression syntax, which is what the Go regular expression’s library RegExp uses.\n\n TIP Suck at regular expressions? You’re not alone. There are some good\n\nexpression testers available online.\n\nOur regular expression, contained in the regex parameter, is:\n\n(container_tasks_state|container_memory_failures_total)\n\nWhich will match and capture two metrics:\n\ncontainer_tasks_state • container_memory_failures_total\n\nIf we had specified multiple source labels we would specify each regular expres- sion using the separator, for example:\n\nregex1;regex2;regex3\n\nVersion: v1.0.0 (427b8e9)\n\n109\n\nChapter 4: Monitoring Nodes and Containers\n\nWe then perform an action, specified in the action parameter. In this case, both of these metrics contain a significant number of time series—of potentially limited usefulness—so we’re taking the drop action. This will drop the metrics before stor- age. Other actions include keep, which keeps the metrics that match the regular expression and drops all others.\n\nReplacing label values\n\nWe can also replace a label’s value with a new value. Let’s take an example. Many cAdvisor metrics have an id label that contains the name of the running process. If that process is a container we’ll see something like:\n\nid=\"/docker/6fca3002e3513d23ed7e435ca064f557ed1d4226ef788e771b8f933a49d55804\n\n\"\n\nThis is a bit unwieldy. So we’d like to take the container ID:\n\n6fca3002e3513d23ed7e435ca064f557ed1d4226ef788e771b8f933a49d55804\n\nAnd put it into a new label: container_id. Using relabelling we can do this like so:\n\nListing 4.19: Replacing a label\n\nmetric_relabel_configs: - source_labels: [id]\n\nregex: '/docker/([a-z0-9]+);' replacement: '$1' target_label: container_id\n\n TIP Relabelling is applied sequentially, using top-down ordering in the con-\n\nfiguration file.\n\nVersion: v1.0.0 (427b8e9)\n\n110\n\nChapter 4: Monitoring Nodes and Containers\n\nOur source label is id. We then specify a regex to match and capture the container ID. The replacement field holds the new value, in this case our capture group $1. We then specify the destination for the captured information, here container_id, in the target_label parameter.\n\nYou’ll notice we didn’t specify an action for this relabel. This is because the default action is replace. If you don’t specify an action, Prometheus will assume you want to perform a replacement.\n\nPrometheus also has a parameter, honor_labels, that controls the conflict be- havior if you try to overwrite and attach a label that already exists. Let’s say your scraped data already has a label called job. Using the default behavior, in which honor_labels is set to false, Prometheus will rename the existing label by prefixing it with exported_. So our job label would become exported_job. If honor_labels is set to true then Prometheus will keep the label on the scraped data and ignore any relabelling on the server.\n\nDropping labels\n\nIn our last example, we’re going to drop a label. This is often useful for hiding sensitive information or simplifying a time series. In this (somewhat contrived) example we’re going to remove the kernelVersion label, hiding the kernel version of our Docker hosts.\n\nListing 4.20: Dropping a label\n\nmetric_relabel_configs:\n\nregex: 'kernelVersion' action: labeldrop\n\nFor dropping a label, we specify a regex watching our label and then the labeldrop action. This will remove all labels that match the regular expression.\n\nVersion: v1.0.0 (427b8e9)\n\n111\n\nChapter 4: Monitoring Nodes and Containers\n\nThis action also has an inverse action, labelkeep, which will keep all labels that match the regular expression and drop all others.\n\n WARNING Remember that labels are uniqueness constraints for time\n\nseries. If you drop a label and that results in duplicate time series, you will have issues!\n\nUsefully, you can see the state of labels prior to relabelling in the Prometheus web interface. We can see this in the list of targets at http://localhost:9090/ targets. Hover your mouse over the instance label in the Labels box to see a list of the labels as they were before relabelling.\n\nFigure 4.5: Labels prior to relabelling\n\nNow let’s take a closer look at our new metrics.\n\nThe Node Exporter and cAdvisor metrics\n\nWe’re now collecting seven individual sets of metrics from four unique hosts:\n\nVersion: v1.0.0 (427b8e9)\n\n112\n\nChapter 4: Monitoring Nodes and Containers\n\nThe Prometheus server’s own metrics. • Node Exporter metrics from three hosts. • cAdvisor metrics from three hosts.\n\n TIP You can see the status of each target being scraped by looking at the Prometheus web interface. Browse to http://localhost:9090/targets to see a list of what Prometheus is scraping and the status of each.\n\nLet’s skip over the Prometheus server’s own metrics and focus on the Node Ex- porter and cAdvisor metrics. Let’s use some of these metrics to explore the capa- bilities of Prometheus and ensure our hosts are properly monitored.\n\nThe trinity and the USE method\n\nWe’re going to make use of one of the monitoring frameworks we introduced at the start of the book: the USE Method. You’ll remember this method suggests collecting and focusing on utilization, saturation, and error metrics to assist with performance diagnostics. We’re going to apply this method, broadly, to one of the common monitoring patterns—CPU, memory, and disk—to see how we can make use of our Node Exporter metrics and how PromQL can be used.\n\n TIP Remember, these host metrics are useful mostly as broad signals of\n\nperformance trouble on your hosts. We’re using them to learn more about working with metrics. Most of the time, though, we’re going to focus on application metrics, which are better indicators of poor user experience.\n\nVersion: v1.0.0 (427b8e9)\n\n113\n\nChapter 4: Monitoring Nodes and Containers\n\nLet’s start by looking at CPU metrics.\n\nCPU Utilization\n\nTo get the U-for-utilization in USE, we’re going to use a metric the Node Exporter collects named node_cpu. This is the utilization of the CPUs on our host, broken down by mode and presented in seconds used. Let’s query for that metric now from the Prometheus web interface. Navigate to http://localhost:9090/graph, select node_cpu from the metric dropdown and click Execute.\n\nFigure 4.6: node_cpu metrics\n\nYou should see a list of metrics, much like so:\n\nnode_cpu{cpu=\"cpu0\",instance=\"138.197.26.39:9100\",job=\"node\",mode=\"\n\nuser\"}\n\nThe node_cpu metric has a number of labels including the instance and job labels, which identify what host it came from and what job scraped the metric, respec- tively.\n\n NOTE The instance label is generally made up of the address of the host\n\nand the port that was scraped.\n\nVersion: v1.0.0 (427b8e9)\n\n114\n\nChapter 4: Monitoring Nodes and Containers\n\nWe also have two labels specific to CPUs: the cpu the metric was collected from— for example, cpu0—and mode for the CPU mode being measured—for example, user, system, idle, etc. Drawn from /proc/stat, the data are counters that tell us how many seconds each CPU spent in each mode.\n\nThis list of metrics isn’t overly useful as is. For any performance analysis, we’ll need to make use of PromQL to turn these into useful metrics. What we’d really like here is to get the percentage CPU used on each instance—but to get there we’ll need to work with our metrics a little. Let’s step towards this outcome by looking at a sequence of PromQL calculations.\n\nWe start with calculating the per-second rate for each CPU mode. PromQL has a function called irate that calculates the per-second instant rate of increase of a time series in a range vector. Let’s use the irate function over our node_cpu metric. Enter this into the query box:\n\nirate(node_cpu{job=\"node\"}[5m])\n\nAnd click Execute. This wraps the node_cpu metric in the irate function and queries a five-minute range. It’ll return the list of per-cpu, per-mode metrics from the node job, now represented as per-second rates in a five-minute range. But this still isn’t overly helpful—we need to aggregate our metrics across CPUs and modes too.\n\nTo do this, we can use the avg or average operator and the by clause we saw in Chapter 3.\n\navg(irate(node_cpu{job=\"node\"}[5m])) by (instance)\n\nNow we’ve wrapped our irate function inside an avg aggregation and added a by clause that aggregates by the instance label. This will produce three new metrics that average CPU usage by host using the values from all CPUs and all modes.\n\nIt still includes idle usage, and it isn’t But this metric is still not quite right. represented in a useful form like a percentage. Let’s constrain our calculation by\n\nVersion: v1.0.0 (427b8e9)\n\n115\n\nChapter 4: Monitoring Nodes and Containers\n\nquerying only the per-instance idle usage and, as it’s already a ratio, multiplying it by 100 to convert it into a percentage.\n\navg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m])) by (instance) * 100\n\nHere we’ve added the mode label with a value of idle to our irate query. This only queries the idle data. We’ve averaged the result by instance and multiplied it by 100. Now we have the average percentage of idle usage in a five-minute range on each host. We can turn this into the percentage used by subtracting this value from 100, like so:\n\n100 - avg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m])) by (instance)\n\n100\n\nAnd now we have three metrics, one for each host, showing the average percentage CPU used in a five-minute window.\n\nFigure 4.7: Per-host average percentage CPU usage metrics\n\nNow, if we click the Graph tab, we can also see these represented as a plot.\n\nVersion: v1.0.0 (427b8e9)\n\n116\n\nChapter 4: Monitoring Nodes and Containers\n\nFigure 4.8: Per-host percentage CPU plot\n\nCPU Saturation\n\nOne of the ways to get the saturation of CPU on a host is to track the load average, essentially the average run queue length over a time period, taking into consider- ation the number of CPUs on the host. An average less than the number of CPUs is generally normal; averages over that number for prolonged periods indicate the CPU is saturated.\n\nTo see the host’s load average, we can use the node_load* metrics for these. They show load average over one minute, five minutes, and 15 minutes. We’re going to use the one-minute load average: node_load1.\n\nLet’s take a quick look at this metric. Select node_load1 from the metric dropdown and click Execute. A list of the nodes being monitored with the current one-minute load average will be listed.\n\nWe also need to calculate the number of CPUs on our hosts. We can do this using the count aggregation like so:\n\ncount by (instance)(node_cpu{mode=\"idle\"})\n\nVersion: v1.0.0 (427b8e9)\n\n117\n\nChapter 4: Monitoring Nodes and Containers\n\nHere we’re counting the number of occurrences of the node_cpu time series with a mode of idle. We’re then using the by clause to remove all labels except instance from the result vector, giving us a list of hosts with the number of CPUs in each.\n\nFigure 4.9: Number of CPUs in each host\n\nWe can see our three nodes have two CPUs apiece.\n\n TIP Since we’re also collecting Docker metrics we could use one of cAdvisor’s metrics here too, machine_cpu_cores, as a shortcut.\n\nWe can then combine this count with the node_load1 metric like so:\n\nnode_load1 > on (instance) 2 * count by (instance)(node_cpu{mode=\"idle\n\n\"})\n\nHere we’re showing if the one-minute load average is two times more than the CPU count on the host. This is not necessarily an issue, but we’ll see in Chapter 6 how to turn it into an alert that should tell you when there is an issue.\n\nNow let’s see if we can’t do something similar with our memory metrics.\n\n TIP We’re going to skip the E-for-error in USE for CPU errors because it’s\n\nunlikely there will be anything useful in any data we could collect.\n\nVersion: v1.0.0 (427b8e9)\n\n118\n\nChapter 4: Monitoring Nodes and Containers\n\nMemory utilization\n\nLet’s look at the utilization of memory on a host. The Node Exporter’s memory metrics are broken down by type and usage of memory. You’ll find them in the list of metrics prefixed with node_memory.\n\nFigure 4.10: The node_memory_MemTotal\n\nWe’re going to focus on a subset of the node_memory metrics to provide our uti- lization metric:\n\nnode_memory_MemTotal - The total memory on the host. • node_memory_MemFree - The free memory on the host. • node_memory_Buffers - The memory in buffer cache. • node_memory_Cached - The memory in the page cache.\n\nAll of these metrics are represented in bytes.\n\nWe’re going to use this combination of metrics to calculate the percentage of memory used on each host. To do this, we’re going to add the values of the node_memory_MemFree, node_memory_Cached, and node_memory_Buffers metrics. This represents the free memory on our host. We’re then going to calculate the\n\nVersion: v1.0.0 (427b8e9)\n\n119\n\nChapter 4: Monitoring Nodes and Containers\n\npercentage of free memory using this value and the node_memory_MemTotal metric. We’re going to use this query to do that:\n\n(node_memory_MemTotal - (node_memory_MemFree + node_memory_Cached +\n\nnode_memory_Buffers)) / node_memory_MemTotal * 100\n\nHere we’ve added together our three memory metrics, subtracted them from the total, divided by the total, and then multiplied by 100 to convert it into a per- centage. This will produce three metrics showing percentage memory used per host.\n\nFigure 4.11: Per-host percentage memory usage\n\nHere we don’t need to use the by clause to preserve distinct dimensions because the metrics have the same dimensional labels; each metric will have the query applied to it in turn.\n\nMemory saturation\n\nWe can also monitor our memory saturation by checking on the rate of paging in and out of memory. We can use data gathered from /proc/vmstat on paging exposed in two Node Exporter metrics:\n\nnode_vmstat_pswpin - Number of kilobytes the system has paged in from\n\ndisk per second.\n\nVersion: v1.0.0 (427b8e9)\n\n120\n\nChapter 4: Monitoring Nodes and Containers\n\nnode_vmstat_pswpout - Number of kilobytes the system has paged out to\n\ndisk per second.\n\nBoth are totals in kilobytes since last boot.\n\nTo get our saturation metric, we generate a one-minute rate for each metric, add the two rates, and then multiply them by 1024 to get bytes. Let’s create a query to do this now.\n\nListing 4.21: Memory saturation query\n\n1024 * sum by (instance) (\n\n(rate(node_vmstat_pgpgin[1m]) + rate(node_vmstat_pgpgout[1m]))\n\n)\n\nWe can then graph or alert on this to identify hosts with misbehaving applications.\n\nDisk usage\n\nFor disks we’re only going to measure disk usage rather than utilization, saturation, or errors. This is because it’s the most useful data in most cases for visualization and alerting. The Node Exporter’s disk usage metrics are in the list of metrics prefixed with node_filesystem.\n\nVersion: v1.0.0 (427b8e9)\n\n121\n\nChapter 4: Monitoring Nodes and Containers\n\nFigure 4.12: Disk metrics\n\nHere, for example, the node_filesystem_size metric shows the size of each file system mount being monitored. We can use a similar query to our memory metrics to produce a percentage figure of disk space used on our hosts.\n\n(node_filesystem_size{mountpoint=\"/\"} - node_filesystem_free{mountpoint\n\n=\"/\"}) / node_filesystem_size{mountpoint=\"/\"} * 100\n\nUnlike the memory metrics, though, we have filesystem metrics per mount point on each host. So we’ve added the mountpoint label, specifically the / filesystem mount. This will return a disk usage metric for that filesystem on each host being monitored.\n\nFigure 4.13: Per-host disk space metrics\n\nIf we wanted or needed to, we could add additional queries for specific mount points to the configuration now. To monitor a mount point called /data we would use:\n\n(node_filesystem_size{mountpoint=\"/data\"} - node_filesystem_free{\n\nmountpoint=\"/data\"}) / node_filesystem_size{mountpoint=\"/data\"} * 100\n\nOr we could use a regular expression to match more than one mountpoint.\n\n(node_filesystem_size{mountpoint=~\"/|/run\"} - node_filesystem_free{\n\nmountpoint=~\"/|/run\"}) / node_filesystem_size{mountpoint=~\"/|/run\"} *\n\n100\n\nVersion: v1.0.0 (427b8e9)\n\n122\n\nChapter 4: Monitoring Nodes and Containers\n\n TIP You cannot use a regular expression that matches an empty string.\n\nYou can see that we’ve updated our mountpoint label to change the operator from = to =~ which tells Prometheus that the right-hand-side value will be a regular expression. We’ve then matched both the /run and / root filesystems.\n\n TIP There’s also a operator for regular expressions that do not match.\n\nThis is still a fairly old-school measure of disk usage. It tells us a current percentage usage of the filesystem. In many cases, this data is useless. An 80 percent full 1 GB filesystem might not be a concern at all if it’s growing at 1 percent a year. A 10 percent full 1 TB filesystem might be at serious risk of filling up if it’s growing at 10 percent every 10 minutes. With disk space, we really need to understand the trend and direction of a metric. The question we usually want answered is: “Given the usage of the disk now, combined with its growth, in what time frame will we run out of disk space?”\n\nPrometheus actually has a mechanism, a function called predict_linear, by which we can construct a query to answer this exact question. Let’s look at an example:\n\npredict_linear(node_filesystem_free{mountpoint=\"/\"}[1h], 4*3600) < 0\n\nHere we’re grabbing the root filesystem, node_filesystem_free{mountpoint=\"/\"} . We could select all the filesystems by specifying the job name or selectively using a regular expression, as we did earlier in this section.\n\npredict_linear(node_filesystem_free{job=\"node\"}[1h], 4*3600) < 0\n\nWe have selected a one-hour time window, [1h]. We’ve also placed this time\n\nVersion: v1.0.0 (427b8e9)\n\n123\n\nChapter 4: Monitoring Nodes and Containers\n\nseries snapshot inside the predict_linear function. The function uses simple linear regression to determine when a filesystem will run out of space based on previous growth. The function takes a range vector, our one-hour window, and the point in the future for which to predict the value, measured in seconds. Hence, four times 3600 (the number of seconds in an hour), or four hours. The < 0 filters for values less than 0, i.e., the filesystem running out of space.\n\nSo, if, based on the last hour’s worth of growth history, the filesystem is going to run out of space in the next four hours, the query will return a negative number, which we can then use to trigger an alert. We’ll see how this alert would work in Chapter 6.\n\nService status\n\nNow let’s look at the data from the systemd collector. Remember this shows us the state of services and various other systemd configuration on our hosts. The state of the services is exposed in the node_systemd_unit_state metric. There’s a metric for each service and service state you’re collecting. In our case we’re only gathering metrics for the Docker, SSH, and RSyslog daemons.\n\nListing 4.22: The node_systemd_unit_state metrics\n\nnode_systemd_unit_state{name=\"docker.service\",state=\"activating\" } 0 node_systemd_unit_state{name=\"docker.service\",state=\"active\"} 1 node_systemd_unit_state{name=\"docker.service\",state=\" deactivating\"} 0 node_systemd_unit_state{name=\"docker.service\",state=\"failed\"} 0 node_systemd_unit_state{name=\"docker.service\",state=\"inactive\"} 0 . . .\n\nWe can query a segment of this data via the Expression Browser and look specifi-\n\nVersion: v1.0.0 (427b8e9)\n\n124\n\nChapter 4: Monitoring Nodes and Containers\n\ncally for this docker service. To do this, we query using the name label.\n\nnode_systemd_unit_state{name=\"docker.service\"}\n\nFigure 4.14: The systemd time series data\n\nThis query produces a metric for each combination of potential service and state: failed, inactive, active, etc. The metric that represents the current state of each service is set to 1. We could narrow this down further by adding the state label to our search and only returning the active state.\n\nnode_systemd_unit_state{name=\"docker.service\",state=\"active\"}\n\nAlternatively, we could search for all of the metrics with the value 1, which would return the state of the current service.\n\nnode_systemd_unit_state{name=\"docker.service\"} == 1\n\nHere we’ve seen a new query, one that uses a comparison binary operator: ==. This will retrieve all metrics with a value equal to 1 with a name label of docker. service.\n\nVersion: v1.0.0 (427b8e9)\n\n125\n\nChapter 4: Monitoring Nodes and Containers\n\nFigure 4.15: The active services\n\nWe’re going to make use of the systemd metrics to monitor the availability of ser- vices on our host—for example, our Docker daemon—and alert on this in Chapter 6.\n\nAvailability and the up metric\n\nWorth mentioning is another useful metric for monitoring the state of specific nodes: the up metric. For each instance scrape, Prometheus stores a sample in the following time series:\n\nListing 4.23: The up metric\n\nup{job=\"<job−name>\", instance=\"<instance−id>\"}\n\nThe metric is set to 1 if the instance is healthy—i.e., the scrape successfully returned—or to 0 if the scrape failed. The metric is labelled with the job name and the instance of the time series.\n\n TIP Prometheus also populates some other instrumentation metrics, includ- ing scrape_duration_seconds, the duration of the scrape, and scrape_samples_- scraped, the number of samples that the target exposed.\n\nVersion: v1.0.0 (427b8e9)\n\n126\n\nChapter 4: Monitoring Nodes and Containers\n\nWe can query all the up metrics for our hosts.\n\nFigure 4.16: The up metrics\n\nIn addition, many exporters have specific metrics designed to identify the last successful scrape of a service. The cAdvisor metrics include container_last_seen, for example, which provides a list of containers and the last time they were active. The MySQL Exporter returns a metric, mysql_up, that is set to 1 if a successful SELECT query works on a database server.\n\nWe’ll see in Chapter 6 how we can use the up metrics to help us do availability monitoring.\n\n NOTE You cannot relabel autopopulated metrics like up because they are\n\ngenerated after the relabelling phase.\n\nVersion: v1.0.0 (427b8e9)\n\n127\n\nChapter 4: Monitoring Nodes and Containers\n\nThe metadata metric\n\nLast, let’s look at the metric we created, metadata, using the Node Exporter’s Textfile collector.\n\nmetadata{role=\"docker_server\",datacenter=\"NJ\"} 1\n\nThis metric provides context for the resource: its role, docker_server, and the location of the host, datacenter. This data is useful in its own right, but why create a separate metric rather than just add these as labels to all of our metrics from this host? Well, we already know that labels provide the dimensions of our time series and, combined with the metric name, they make up the identity of our time series. We’ve also already been warned that:\n\nChanging a label or adding a new label creates a new time series.\n\nThis means that labels should be used judiciously and should remain as constant as possible. So, instead of decorating every time series with the set of complete labels, we instead create a time series that we can use to query specific types or classes of resources.\n\nLet’s see how we could make use of the labels on this metric. Suppose we want to select metrics only from a specific data center or set of data centers. We can quickly find all hosts in, say, a non-New Jersey (NJ) data center by querying like so:\n\nmetadata{datacenter != \"NJ\"}\n\nYou can see that we’ve queried the metadata metric and specified the datacenter with an operator, != or not equal to, to return any metadata metric from a non- New Jersey data center.\n\n TIP Prometheus has a full set of arithmetic and comparison binary operators\n\nyou can use.\n\nVersion: v1.0.0 (427b8e9)\n\n128\n\nChapter 4: Monitoring Nodes and Containers\n\nVector matches\n\nWe can also use our metadata metric to make vector matches. Vector matches can use any of the PromQL binary operators. Vector matches attempt to find a matching element in the right-hand-side vector for each entry on the left-hand side.\n\nThere are two kinds of vector matches: One-to-one and many-to-one (or one-to- many).\n\nOne-to-one matches\n\nOne-to-one matches find a unique pair of entries from each side of the operation. Two entries match if they have the exact same set of labels and values. You can modify the set of labels considered by using the ignoring modifier, which ignores specific labels, or by using the on modifier, which reduces the set of considered labels to a list. Let’s see an example.\n\nListing 4.24: A one-to-one vector match\n\nnode_systemd_unit_state{name=\"docker.service\"} == 1\n\nand on (instance, job)\n\nmetadata{datacenter=\"SF\"}\n\nThis queries any node_systemd_unit_state metrics with the name label of docker .service and a value of 1. We then use the on modifier to reduce the consid- ered label set to the instance and job labels of the metadata metric, where the datacenter label has a value of SF.\n\nIn our case, this will return a single metric:\n\nnode_systemd_unit_state{instance=\"138.197.30.147:9100\",job=\"node\",name\n\n=\"docker.service\",state=\"active\"}\n\nVersion: v1.0.0 (427b8e9)\n\n129\n\nChapter 4: Monitoring Nodes and Containers\n\nIf we were to change the datacenter label in our query to NJ, we’d return two metrics: one for each of the two Docker servers in the NJ data center.\n\nMany-to-one and one-to-many matches\n\nMany-to-one and one-to-many matches are where each vector element on the “one” side can match with multiple elements on the “many” side. These matches are explicitly specified using the group_left or group_right modifiers, where left or right determines which vector has the higher cardinality. The Prometheus doc- umentation contains some examples of this kind of match, but they are generally not required. In most cases one-to-one matches will suffice.\n\nMetadata-style metrics\n\nMany existing exporters use this “metadata” pattern to provide information about extra state—for example, cAdvisor has the cadvisor_version metric that provides information about the local Docker daemon and related configuration.\n\nFigure 4.17: The cadvisor_version metric\n\nThis type of metric allows you to use vector matches to list all metrics that match some contextual criteria: a location, a version, etc.\n\nSo now that we’ve seen how to use some of the metrics, how do we persist the queries we’ve seen?\n\nVersion: v1.0.0 (427b8e9)\n\n130\n\nChapter 4: Monitoring Nodes and Containers\n\nQuery permanence\n\nUntil now, we’ve just run queries in the Expression Browser. Whilst viewing the output of that query is interesting, the result is stuck on the Prometheus server and is transitory. There are three ways we can make our queries more permanent:\n\nRecording rules - Create new metrics from queries. • Alerting rules - Generate alerts from queries. • Visualization - Visualize queries using a dashboard like Grafana.\n\nThe queries we’ve looked at can be used interchangeably in all three of these mechanisms because all of these mechanisms can understand and execute PromQL queries.\n\nIn this chapter we’re going to make use of some recording rules to create new metrics from our queries and configure Grafana as a dashboard to visualize metrics. In Chapter 6 we’ll make use of alerting rules to generate alerts.\n\nRecording rules\n\nWe talked about recording rules and their close cousin, alerting rules in Chapter 2.\n\nRecording rules are a way to compute new time series, particularly aggregated time series, from incoming time series. We might do this to:\n\nProduce aggregates across multiple time series. • Precompute expensive queries. • Produce a time series that we could use to generate an alert.\n\nLet’s write some rules.\n\nVersion: v1.0.0 (427b8e9)\n\n131\n\nChapter 4: Monitoring Nodes and Containers\n\nConfiguring recording rules\n\nRecording rules are stored on the Prometheus server, in files that are loaded by the Prometheus server. Rules are calculated automatically, with a frequency controlled by the evaluation_interval parameter in the global block of the prometheus.yml configuration file we saw in Chapter 3.\n\nListing 4.25: The evaluation_interval parameter\n\nglobal:\n\nscrape_interval: 15s evaluation_interval: 15s\n\n. . .\n\nRule files are specified in our Prometheus configuration inside the rules_files block.\n\nLet’s create a sub-directory called rules in the same directory as our prometheus. yml file, to hold our recording rules. We’ll also create a file called node_rules.yml for our node metrics. Prometheus rules are written, like Prometheus configuration, in YAML.\n\n WARNING YAML rules were updated in Prometheus 2.0. Earlier re-\n\nleases used a different structure. Your older rule files will not work in Prometheus 2.0 or later. You can use the promtool to upgrade older rules files. There’s a good blog post on the upgrading process here.\n\nVersion: v1.0.0 (427b8e9)\n\n132\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.26: Creating a recorded rules file\n\n$ mkdir -p rules $ cd rules $ touch node_rules.yml\n\nLet’s add that file to our Prometheus configuration in the rule_files block in the prometheus.yml file.\n\nListing 4.27: Adding the rules file\n\nrule_files:\n\n\"rules/node_rules.yml\"\n\nNow let’s populate this file with some rules.\n\nAdding recording rules\n\nLet’s convert our CPU, memory, and disk calculations into recording rules. We have a lot of hosts to be monitored, so we’re going to precompute all the trinity queries. That way we’ll also have the calculations as metrics that we can alert on or visualize via a dashboard like Grafana.\n\nLet’s start with our CPU calculation.\n\nVersion: v1.0.0 (427b8e9)\n\n133\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.28: A recording rule\n\ngroups: - name: node_rules\n\nrules: - record: instance:node_cpu:avg_rate5m\n\nexpr: 100 - avg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m]))\n\nby (instance) * 100\n\nRecording rules are defined in rule groups; here ours is named node_rules. Rule group names must be unique in a server. Rules within a group are run sequentially at a regular interval. By default, this is the global evaluation_interval, but it can be overridden in the rule group using the interval clause.\n\nThe sequential nature of rule execution in groups means that you can use rules you create in subsequent rules. This allows you to create a metric from a rule and then reuse that metric in a later rule. This is only true within rule groups though—rule groups are run concurrently, so it’s not safe to use rules across groups.\n\n TIP This also means you can use recording rules as parameters, for example\n\nyou might want to create a rule with a threshold in it. You can then set the threshold once in the rule and re-use it multiple times. If you need to change the threshold you just need to change it that one place.\n\nVersion: v1.0.0 (427b8e9)\n\n134\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.29: A recording group interval\n\ngroups: - name: node_rules interval: 10s rules:\n\nThis would update the rule group to be run every 10 seconds rather than the globally defined 15 seconds.\n\nNext, we have a YAML block called rules, which contains this group’s recording rules. Each rule contains a record, which tells Prometheus what to name the new time series. You should name rules so you can identify quickly what they represent. The general recommended format is:\n\nlevel:metric:operations\n\nWhere level represents the aggregation level and labels of the rule output. Met- ric is the metric name and should be unchanged other than stripping _total off counters when using the rate() or irate() functions. This makes it easier to find the new metric. Finally, operations is a list of operations that were applied to the metric, the newest operation first.\n\nSo our CPU query would be named:\n\ninstance:node_cpu:avg_rate5m\n\n TIP There are some useful best practices on naming in the Prometheus doc-\n\numentation.\n\nWe then specify an expr field to hold the query that should generate the new time series.\n\nVersion: v1.0.0 (427b8e9)\n\n135\n\nChapter 4: Monitoring Nodes and Containers\n\nWe could also add a labels block to add new labels to the new time series. Time series created from rules inherit the relevant labels of the time series used to create them, but you can also add or overwrite labels. For example:\n\nListing 4.30: A recording rule\n\ngroups: - name: node_rules\n\nrules: - record: instance:node_cpu:avg_rate5m\n\nexpr: 100 - avg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m]))\n\nby (instance) * 100\n\nlabels:\n\nmetric_type: aggregation\n\nLet’s create rules for some of our other trinity queries, and add them, too.\n\nListing 4.31: A recording rule\n\ngroups: - name: node_rules\n\nrules: - record: instance:node_cpu:avg_rate5m\n\nexpr: 100 - avg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m]))\n\nby (instance) * 100\n\nrecord: instance:node_memory_usage:percentage expr: (node_memory_MemTotal - (node_memory_MemFree +\n\nnode_memory_Cached + node_memory_Buffers)) / node_memory_MemTotal * 100\n\nrecord: instance:root:node_filesystem_usage:percentage expr: (node_filesystem_size{mountpoint=\"/\"} -\n\nnode_filesystem_free{mountpoint=\"/\"}) / node_filesystem_size{ mountpoint=\"/\"} * 100\n\nVersion: v1.0.0 (427b8e9)\n\n136\n\nChapter 4: Monitoring Nodes and Containers\n\n TIP The configuration files and code for this book are located on GitHub.\n\nWe now need to restart or SIGHUP the Prometheus server to activate the new rules. This will create a new time series for each rule. You should be able to find the new time series on the server in a few moments.\n\n TIP The rule files can be reloaded at runtime by sending SIGHUP to the\n\nPrometheus process (or by restarting on Microsoft Windows). The reload will only work if the rules file is well formatted. The Prometheus server ships with a utility called promtool that can lint rule files.\n\nIf we now search for one of the new time series, instance:node_cpu:avg_rate5m for example, we should see:\n\nFigure 4.18: The node_cpu recorded rule\n\nLast, let’s quickly look at how we might visualize the metrics we’ve just created.\n\n TIP You can see the current rules defined on your server in the /rules path\n\nVersion: v1.0.0 (427b8e9)\n\n137\n\nChapter 4: Monitoring Nodes and Containers\n\nof the Web UI. This includes useful information, like the execution time of each rule, that can help you debug expensive rules that might need optimization.\n\nVisualization\n\nAs we’ve seen, Prometheus has an inbuilt dashboard and graphing interface. It’s fairly simple and generally best for reviewing metrics and presenting solitary graphs. To add a more fully featured visualization interface to Prometheus, the platform integrates with the open-source dashboard Grafana. Grafana is a dash- board fed via data sources. It supports a variety of formats including Graphite, Elasticsearch, and Prometheus.\n\nIt’s important to note that Prometheus isn’t generally used for long-term data retention—the default is 15 days worth of time series. This means that Prometheus is focused on more immediate monitoring concerns than, perhaps, other systems where visualization and dashboards are more important. The judicious use of the Expression Browser, graphing inside the Prometheus UI, and building appropriate alerts are often more practical uses of Prometheus’ time series data than building extensive dashboards.\n\nWith that said, in this last section we’re going to quickly install Grafana and con- nect Prometheus to it.\n\nInstalling Grafana\n\nInstalling Grafana depends on the platform you’re installing on. Grafana supports running on Linux, Microsoft Windows, and Mac OS X. Let’s look at installation on each platform.\n\nVersion: v1.0.0 (427b8e9)\n\n138\n\nChapter 4: Monitoring Nodes and Containers\n\nInstalling Grafana on Ubuntu\n\nFor Ubuntu and Debian systems, we can add the Grafana package repository. We first need to add the PackageCloud public key, like so:\n\nListing 4.32: Getting the PackageCloud public key on Ubuntu\n\n$ curl https://packagecloud.io/gpg.key | sudo apt-key add -\n\nWe add the following Apt configuration so we can find the Grafana repository:\n\nListing 4.33: Adding the Grafana packages\n\n$ echo \"deb https://packagecloud.io/grafana/stable/debian/ stretch main\" | sudo tee -a /etc/apt/sources.list.d/grafana.list\n\nThen we update Apt and install the grafana package with the apt-get command.\n\nListing 4.34: Updating Apt and installing the Grafana package\n\n$ sudo apt-get update $ sudo apt-get install grafana\n\nOn Red Hat\n\nTo install Grafana on Red Hat systems, we first need to add the Elastic.co public key, like so:\n\nVersion: v1.0.0 (427b8e9)\n\n139\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.35: Getting the Grafana public key on Red Hat\n\n$ sudo rpm --import https://packagecloud.io/gpg.key\n\nThen we add the following to our /etc/yum.repos.d/ directory in a file called grafana.repo:\n\nListing 4.36: The Grafana Yum configuration\n\n[grafana] name=grafana baseurl=https://packagecloud.io/grafana/stable/el/7/$basearch repo_gpgcheck=1 enabled=1 gpgcheck=1 gpgkey=https://packagecloud.io/gpg.key https://grafanarel.s3. amazonaws.com/RPM-GPG-KEY-grafana sslverify=1 sslcacert=/etc/pki/tls/certs/ca-bundle.crt\n\nNow we install Grafana using the yum or dnf commands.\n\nListing 4.37: Installing Grafana on Red Hat\n\n$ sudo yum install grafana\n\nInstalling Grafana on Microsoft Windows\n\nTo install Grafana on Microsoft Windows, we need to download and put it in a directory. Let’s create a directory for the executable using Powershell.\n\nVersion: v1.0.0 (427b8e9)\n\n140\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.38: Creating a Grafana directory on Windows\n\nC:\\> MKDIR grafana C:\\> CD grafana\n\nNow download Grafana from the Grafana site.\n\nListing 4.39: Grafana Windows download\n\nhttps://s3-us-west-2.amazonaws.com/grafana-releases/release/ grafana-5.1.3.windows-x64.zip\n\nThe zip file contains a folder with the current Grafana version. Unzip the file using a tool like 7-Zip, and put the contents of the unzipped directory into the C:\\grafana directory. Finally, add the C:\\grafana directory to the path. This will allow Windows to find the executable. To do this, run this command inside Powershell.\n\nListing 4.40: Setting the Windows path for Grafana\n\n$env:Path += \";C:\\grafana\"\n\nWe then need to make some quick configuration changes to adjust the default port. The default Grafana port is 3000, a port which requires extra permissions on Microsoft Windows. We want to change it to port 8080 to make Grafana easier to use.\n\nGo into the c:\\grafana\\conf\\ directory and copy the sample.ini file to custom. ini. Edit the custom.ini file and uncomment the http_port configuration option. It’ll be prefixed with the ; character, which is the comment character in ini files.\n\nVersion: v1.0.0 (427b8e9)\n\n141\n\nChapter 4: Monitoring Nodes and Containers\n\nChange the port number to 8080. That port will not require extra Microsoft Win- dows privileges.\n\nInstalling Grafana on Mac OS X\n\nGrafana is also available from Homebrew. If you use Homebrew to provision your Mac OS X hosts then you can install Grafana via the brew command.\n\nListing 4.41: Installing Grafana via Homebrew\n\n$ brew install grafana\n\nInstalling Grafana via configuration management or a stack\n\nThere are a variety of options for installing Grafana via configuration manage- ment. Many of the stacks and configuration management modules we saw in Chapter 3 also support Grafana installations.\n\nChef cookbooks for Grafana at https://supermarket.chef.io/cookbooks/ grafana.\n\nPuppet modules for Grafana at https://forge.puppetlabs.com/modules? utf-8=%E2%9C%93&sort=rank&q=grafana.\n\nAnsible roles for Grafana at https://galaxy.ansible.com/list#/roles/ 3563.\n\nDocker images Grafana at https://hub.docker.com/search/?q=grafana.\n\nStarting and configuring Grafana\n\nThere are two places where we can configure Grafana: a local configuration file and the Grafana web interface. The local configuration file, which is primarily for\n\nVersion: v1.0.0 (427b8e9)\n\n142\n\nChapter 4: Monitoring Nodes and Containers\n\nconfiguring server-level settings like authentication and networking, is available at either:\n\n/etc/grafana/grafana.ini on Linux. • /usr/local/etc/grafana/grafana.ini on OS X. • c:\\grafana\\conf\\custom.ini on Microsoft Windows.\n\nThe Grafana web interface is used to configure the source of our data and our graphs, views, and dashboards. Our configuration in this chapter is going to be via the web interface.\n\nTo access the web interface we need to start the Grafana web service, so let’s do that first. On Linux we’d use the service.\n\nListing 4.42: Starting the Grafana Server on Linux\n\n$ sudo service grafana-server start\n\nThis will work on both Ubuntu and Red Hat.\n\nOn OS X, if we want to start Grafana at boot, we need to run:\n\nListing 4.43: Starting Grafana at boot on OSX\n\n$ brew services start grafana\n\nOr to start it ad hoc on OS X, run:\n\nVersion: v1.0.0 (427b8e9)\n\n143\n\nChapter 4: Monitoring Nodes and Containers\n\nListing 4.44: Starting Grafana server on OS X\n\n$ grafana-server --config=/usr/local/etc/grafana/grafana.ini -- homepath /usr/local/share/grafana cfg:default.paths.logs=/usr/ local/var/log/grafana cfg:default.paths.data=/usr/local/var/lib/ grafana cfg:default.paths.plugins=/usr/local/var/lib/grafana/ plugins\n\nOn Microsoft Windows we would run the grafana-server.exe executable and use something like NSSM if we want to run it as a service.\n\nConfiguring the Grafana web interface\n\nGrafana is a Go-based web service that runs on port 3000 (or 8080 on Microsoft Windows, as we configured it) by default. Once it’s running you can browse to it using your web browser—for example, if it’s running on the local host: http:// localhost:3000.\n\nVersion: v1.0.0 (427b8e9)\n\n144\n\nChapter 4: Monitoring Nodes and Containers\n\nFigure 4.19: The Grafana console login\n\nYou’ll see a login screen initially. The default username and password are admin and admin. You can control this by updating the [security] section of the Grafana configuration file.\n\nYou can configure user authentication, including integration with Google authen- tication, GitHub authentication, or local user authentication. The Grafana config- uration documentation includes sections on user management and authentication. For our purposes, we’re going to assume the console is inside our environment and stick with local authentication.\n\nLog in to the console by using the admin / admin username and password pair and clicking the Log in button. You should see the Grafana default console view.\n\nVersion: v1.0.0 (427b8e9)\n\n145\n\nChapter 4: Monitoring Nodes and Containers\n\nFigure 4.20: The Grafana console\n\nIt contains a Getting Started workflow. We first want to connect Grafana to our Prometheus data. Click on Add data source in the Getting Started workflow. You’ll see a new definition for a data source.\n\nTo add a new data source we need to specify a few details. First, we need to name our data source. We’re going to call ours Prometheus. Next, we need to check the Default checkbox to tell Grafana to search for data in this source by default. We also need to ensure the data source Type is set to Prometheus.\n\nWe also need to specify the HTTP settings for our data source. This is the URL of the Prometheus server we wish to query. Here, let’s assume we’re running Grafana on the same host as Prometheus—for our local server, it’s http://localhost :9090. If you’re running Prometheus elsewhere you’ll need to specify the URL to Prometheus and to ensure connectivity is available between the Grafana host and the Prometheus server.\n\n NOTE The Prometheus server needs to be running for Grafana to retrieve\n\ndata.\n\nWe also need to set the Access option to proxy. Surprisingly this doesn’t configure\n\nVersion: v1.0.0 (427b8e9)\n\n146\n\nChapter 4: Monitoring Nodes and Containers\n\nan HTTP proxy for our connection, but it tells Grafana to use its own web service to proxy connections to Prometheus. The other option, direct, makes direct con- nections from the web browser. The proxy setting is much more practical, as the Grafana service takes care of connectivity.\n\nFigure 4.21: Adding a Grafana data source for Prometheus\n\nVersion: v1.0.0 (427b8e9)\n\n147\n\nChapter 4: Monitoring Nodes and Containers\n\nTo add our new data source, click the Add button. This will save it. On the screen we can now see our data source displayed. If it saved with a banner saying Data\n\nsource is working then it is working!\n\nClick the Grafana logo and then click on Dashboards -> Home to return to the main console view.\n\nFigure 4.22: Adding a Grafana data source for Prometheus\n\nVersion: v1.0.0 (427b8e9)\n\n148\n\nChapter 4: Monitoring Nodes and Containers\n\nFirst dashboard\n\nNow that you’re back on the Getting Started workflow, click on the New dashboard button to create a new dashboard.\n\nYou can then see the first dashboard here:\n\nFigure 4.23: The Node dashboard\n\nThe process of creating graphs and dashboards is reasonably complex and beyond the scope of this book. But there are a large number of resources and examples that can help:\n\nGrafana Getting Started • Grafana Tutorials and screencasts • Grafana Prometheus documentation • Grafana Prebuilt Dashboard collection\n\nMany projects also include prebuilt Grafana dashboards for their specific needs— for example, monitoring MySQL or Redis.\n\nYou can then add graphs for some of the other metrics we’ve explored in this chapter. We’ve included the JSON for our complete dashboard in the code for the book that you can import and play with.\n\nVersion: v1.0.0 (427b8e9)\n\n149\n\nChapter 4: Monitoring Nodes and Containers\n\nSummary\n\nIn this chapter we used our first exporters and scraped node and container metrics.\n\nWe’ve started to explore the PromQL query language, how to make use of it to aggregate those metrics and report on the state of some of our node resources, and we’ve delved into the USE Method to find some key metrics to monitor. We also learned how to save those queries as recording rules.\n\nFrom here we can extend the use of those exporters to our whole fleet of hosts. This presents a challenge, though: how does Prometheus know about new hosts? Do we continue to manually add IP addresses to our scrape configuration? We can quickly see that this will not scale. Thankfully, Prometheus has a solution: service discovery. In the next chapter we’ll explore how Prometheus can discover your hosts and services.\n\nVersion: v1.0.0 (427b8e9)\n\n150",
      "page_number": 99
    },
    {
      "number": 5,
      "title": "Service Discovery",
      "start_page": 163,
      "end_page": 181,
      "detection_method": "regex_chapter",
      "content": "Chapter 5\n\nService Discovery\n\nIn the last chapter we installed exporters and scraped node and container metrics. For each target we specified, we manually listed their IP address and port in the scrape configuration. This approach is fine for a few hosts but not for a larger fleet, especially not a dynamic fleet using containers and cloud-based instances, where the instances can change, appear, and disappear.\n\nPrometheus solves this issue by using service discovery: automated mechanisms to detect, classify, and identify new and changed targets. Service discovery can work via a variety of mechanisms:\n\nReceiving lists of targets from files populated via configuration management\n\ntools.\n\nQuerying an API, such as the Amazon AWS API, for lists of targets. • Using DNS records to return lists of targets.\n\nIn this chapter, we’re going to use service discovery to learn how to discover our hosts and services and expose them to Prometheus. We’ll see a variety of discovery mechanisms including file-based, API-driven, and DNS-powered.\n\n151\n\nChapter 5: Service Discovery\n\nScrape lifecycle and static configuration redux\n\nTo understand how service discovery works we need to harken back to our scrape lifecycle. When Prometheus runs a job, the very first step initiated is service discovery. This populates the list of targets and metadata labels that the job will scrape.\n\nFigure 5.1: Scrape lifecycle\n\nIn our existing configuration, our static_configs block:\n\nservice discovery mechanism is\n\nthe\n\nVersion: v1.0.0 (427b8e9)\n\n152\n\nChapter 5: Service Discovery\n\nListing 5.1: Our static service discovery\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\njob_name: 'node' static_configs:\n\ntargets: ['138.197.26.39:9100', '138.197.30.147:9100', '\n\n138.197.30.163:9100']\n\nThe list of targets and any associated labels are manual service discovery. It’s pretty easy to see that maintaining a long list of hosts in a variety of jobs isn’t going to be a human-scalable task (nor is HUP’ing the Prometheus server for each change overly elegant). This is especially true with the dynamic nature of most environments and the scale of hosts, applications, and services that you’re likely to want to monitor.\n\nThis is where more sophisticated service discovery comes into its own. So what alternatives do we have? We’re going to explore several service discovery meth- ods:\n\nFile-based. • Cloud-based. • DNS-based.\n\nWe’ll start with file-based discovery.\n\n NOTE Jobs can use one more than type of service discovery. We can source\n\ntargets from multiple service discovery techniques by specifying them in a job.\n\nVersion: v1.0.0 (427b8e9)\n\n153\n\nChapter 5: Service Discovery\n\nFile-based discovery\n\nFile-based discovery is only a small step more advanced than static configurations, but it’s great for provisioning by configuration management tools. With file-based discovery Prometheus consumes targets specified in files. The files are usually generated by another system—such as a configuration management system like Puppet, Ansible, or Chef—or queried from another source, like a CMDB. Periodi- cally a script or query runs or is triggered to (re)populate these files. Prometheus then reloads targets from these files on a specified schedule.\n\nThe files can be in YAML or JSON format and contain lists of targets defined much like we’d define them in a static configuration. Let’s start by moving our existing jobs to file-based discovery.\n\nListing 5.2: File-based discovery\n\njob_name: node file_sd_configs:\n\nfiles:\n\ntargets/nodes/*.json refresh_interval: 5m\n\njob_name: docker file_sd_configs:\n\nfiles:\n\ntargets/docker/*.json refresh_interval: 5m\n\n. . .\n\nWe’ve replaced the static_configs blocks in our prometheus.yml file with file_sd_configs blocks. Inside these blocks we’ve specified a list of files, contained in the files array. We’ve specified our files for each job under a parent directory, targets, and created a sub-directory for each job. You can create whatever structure works for you.\n\nVersion: v1.0.0 (427b8e9)\n\n154\n\nChapter 5: Service Discovery\n\nWe’ve then specified the files using a glob: *.json. This will load targets from all files ending in .json in this directory, whenever those files change. I’ve chosen JSON for our files because it’s a popular format that’s easy to write using a variety of languages and integrations.\n\nEvery time the job runs or these files change, Prometheus will reload the files’ contents. As a safeguard, we’ve also specified the refresh_interval option. This option will load the targets in the listed files at the end of each interval—for us this is five minutes.\n\n TIP There’s also a metric called prometheus_sd_file_mtime_seconds that\n\nwill tell you when your file discovery files were last updated. You could monitor this metric to identify any staleness issues.\n\nLet’s quickly create this directory structure.\n\nListing 5.3: Creating the target directory structure\n\n$ cd /etc/prometheus $ mkdir -p targets/{nodes,docker}\n\nLet’s move our nodes and Docker daemons to new JSON files. We’ll create two files to hold the targets.\n\nListing 5.4: Creating JSON files to hold our targets\n\n$ touch targets/nodes/nodes.json $ touch targets/docker/daemons.json\n\nVersion: v1.0.0 (427b8e9)\n\n155\n\nChapter 5: Service Discovery\n\nAnd now populate them with our existing targets.\n\nListing 5.5: The nodes.json file\n\n[{\n\n\"targets\": [\n\n\"138.197.26.39:9100\", \"138.197.30.147:9100\", \"138.197.30.163:9100\"\n\n]\n\n}]\n\nAnd the daemons.json file.\n\nListing 5.6: The daemons.json file\n\n[{\n\n\"targets\": [\n\n\"138.197.26.39:8080\", \"138.197.30.147:8080\", \"138.197.30.163:8080\"\n\n]\n\n}]\n\nWe can also articulate the same list of targets we’ve created in JSON in YAML.\n\nListing 5.7: The daemons file in YAML\n\ntargets:\n\n\"138.197.26.39:8080\" - \"138.197.30.147:8080\" - \"138.197.30.163:8080\"\n\nThis moves our existing static configuration into our files. We could add labels to\n\nVersion: v1.0.0 (427b8e9)\n\n156\n\nChapter 5: Service Discovery\n\nthese targets, too.\n\nListing 5.8: Adding labels\n\n[{\n\n\"targets\": [\n\n\"138.197.26.39:8080\", \"138.197.30.147:8080\", \"138.197.30.163:8080\"\n\n], \"labels\": {\n\n\"datacenter\": \"nj\"\n\n}\n\n}]\n\nHere we’ve added the label datacenter with a value of nj to the Docker daemon targets. File-based discovery automatically adds one metadata label during the relabelling phase to every target: __meta_filepath. This contains the path and filename of the file containing the target.\n\n NOTE You can see a full list of the service discovery targets and their meta labels on the Web UI at https://localhost:9090/service-discovery.\n\nWriting files for file discovery\n\nSince writing files out to JSON is fairly specific to the source of the targets, we’re not going to cover any specifics, but we’ll provide a high-level overview of some approaches.\n\nFirst, if your configuration management tool can emit a list of the nodes it is man- aging or configuring, that’s an ideal starting point. Several of the configuration\n\nVersion: v1.0.0 (427b8e9)\n\n157\n\nChapter 5: Service Discovery\n\nmanagement modules we introduced in Chapter 3 have such templates.\n\nIf those tools include a centralized configuration store or configuration manage- ment database (CMDB) of some kind, this can be a potential source for the target data. For example, if you are using PuppetDB, there’s a file-based discovery script you can use to extract your nodes from the database.\n\nAlternatively, if you’re going to write your own, there’s a few simple rules to follow:\n\nMake your file discovery configurable—don’t hardcode options. Preferably, ensure that your file discovery will also work automatically with its default configuration. For instance, ensure the default configuration options assume the default installation state of the source.\n\nDon’t expose secrets like API keys or passwords in configuration. Instead,\n\nrely on secret stores or the environment.\n\nOperations on the files to which you output your targets should be atomic.\n\nHere are some file discovery scripts and tools that might provide examples you can crib from:\n\nAmazon ECS. • An API-driven file discovery script that Wikimedia uses with its CMBD. • Docker Swarm.\n\n TIP There’s also a list of file-based discovery integrations in the Prometheus\n\ndocumentation.\n\nVersion: v1.0.0 (427b8e9)\n\n158\n\nChapter 5: Service Discovery\n\nInbuilt service discovery plugins\n\nSome tools and platforms are supported by native service discovery integrations. These ship with Prometheus. These service discovery plugins use those tools and platform’s existing data stores or APIs to return lists of targets.\n\nThe currently available native service discovery plugins include platforms like:\n\nAmazon EC2 • Azure • Consul • Google Compute Cloud • Kubernetes\n\n TIP We’ll see the Kubernetes service discovery in Chapter 7 when we instru-\n\nment an application running on Kubernetes.\n\nLet’s take a look at the Amazon EC2 service discovery plugin.\n\nAmazon EC2 service discovery plugin\n\nThe Amazon EC2 service discovery plugin uses the Amazon Web Services EC2 API to retrieve a list of EC2 instances to use as Prometheus targets. In order to use the discovery plugin you’ll need to have an Amazon account and credentials. We’re going to assume you already have an Amazon account, but if you haven’t already got an AWS account, you can create one at the AWS Console.\n\nThen follow the Getting Started process.\n\nVersion: v1.0.0 (427b8e9)\n\n159\n\nChapter 5: Service Discovery\n\nAs part of the Getting Started process you’ll receive an access key ID and a secret ac- cess key. If you have an Amazon Web Services (AWS) account you should already have a pair of these.\n\nLet’s add a new job to our Prometheus configuration to retrieve our EC2 instances.\n\nListing 5.9: An EC2 discovery job\n\njob_name: amazon_instances ec2_sd_configs:\n\nregion: us-east-1\n\naccess_key: AKIAIOSFODNN7EXAMPLE secret_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\nWe’ve specified a new amazon_instances job. Our service discovery is provided via the ec2_sd_configs block. Inside this block we’ve specified three parame- ters: region for the AWS region, and access_key and secret_key for our Amazon credentials.\n\nIf you don’t want to specify your keys in the file (and, remember, you shouldn’t hardcode your secrets in configuration), Prometheus supports Amazon’s local CLI configuration approaches. If you don’t specify keys, Prometheus will look for the appropriate environment variables, or for AWS credentials in the user running Prometheus’ home directory.\n\nAlternatively, you can specify a role ARN to use IAM roles.\n\nPrometheus also supports profiles if you have multiple AWS accounts specified on the host.\n\nVersion: v1.0.0 (427b8e9)\n\n160\n\nChapter 5: Service Discovery\n\nListing 5.10: An EC2 discovery job with a profile\n\njob_name: amazon_instances ec2_sd_configs:\n\nregion: us-east-1 profile: work\n\nHere Prometheus will use the work profile when discovering instances.\n\nThe discovery\n\nThe EC2 discovery plugin will return all running instances in that region. By default, it’ll return targets with the private IP address of the instance, with a default port of 80, and with a metrics path of /metrics. So, if you have an EC2 instance with the private IP address of 10.2.1.1, it will return a scrape target address of http://10.2.1.1:80/metrics. We can override the default port with the port parameter.\n\nListing 5.11: An EC2 discovery job with a port\n\njob_name: amazon_instances ec2_sd_configs:\n\nregion: us-east-1 port: 9100\n\nThis will override the default port of 80 with a port of 9100.\n\nOften, though, we want to override more than just the port. If this isn’t where you have metrics exposed, we can adjust this prior to the scrape by relabelling. This relabelling takes place in the first relabel window, prior to the scrape, and uses the relabel_configs block.\n\nVersion: v1.0.0 (427b8e9)\n\n161\n\nChapter 5: Service Discovery\n\nLet’s assume each of our EC2 instances has the Node Exporter configured, and we want to scrape the public IP address—not the private IP address—and relabel the targets accordingly.\n\nListing 5.12: Relabelling an EC2 discovery job\n\njob_name: amazon_instances ec2_sd_configs:\n\nregion: us-east-1\n\naccess_key: AKIAIOSFODNN7EXAMPLE secret_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\nrelabel_configs: - source_labels: [__meta_ec2_public_ip]\n\nregex: target_label: __address__ replacement: '$1:9100'\n\n'(.*)'\n\n TIP Remember, a job can have more than one type of service discovery\n\npresent. For example, we could discover some targets via file-based service dis- covery and others from Amazon, we could statically specify some targets, and so on. Despite this, any relabelling will be applied to all targets. If you’re using ser- vice discovery metadata labels they won’t be available for all targets.\n\nThe configuration syntax and structure for relabelling is identical between the relabel_configs and metric_relabel_configs blocks. The only difference is when they take place: relabel_configs is after service discovery and before the scrape, and metrics_relabel_configs is after the scrape.\n\nHere we’ve specified one of the metadata labels collected by the EC2 service dis- covery plugin, __meta_ec2_public_ip, as the value for our source_labels. We specified (.*) as our regular expression. We technically don’t need to specify this\n\nVersion: v1.0.0 (427b8e9)\n\n162\n\nChapter 5: Service Discovery\n\nas this is the default value of the regex parameter. But we’ve included it to make it clear what’s happening. This expression captures the entire contents of the source label.\n\nWe then specify the target for our replacement in the target_label parameter. As we need to update the IP address we’re writing into the __address__ label Fi- nally, the replacement parameter contains the regular expression capture from the regex and suffixes it with the Node Exporter default port: 9100. If the public IP address of our instance was 34.201.102.225, then the instance would be rela- belled as a target to 34.201.102.225:9100 and the default scheme, http, and the metrics path, /metrics, would be added. The final target would be scraped at http://34.201.102.225:9100/metrics.\n\n TIP The __meta labels are dropped after the first relabelling phase, relabel_- configs.\n\nOther metadata collected by the EC2 discovery plugin includes:\n\n__meta_ec2_availability_zone - The availability zone of the instance. • __meta_ec2_instance_id - The EC2 instance ID. • __meta_ec2_instance_state - The state of the EC2 instance. • __meta_ec2_instance_type - The type of the EC2 instance. • __meta_ec2_private_ip - The private IP address of the EC2 instance, if avail- able.\n\n__meta_ec2_public_dns_name - The public DNS name of the instance, if\n\navailable.\n\n__meta_ec2_subnet_id - A comma-separated list of subnet IDs in which the\n\ninstance is running, if available.\n\n__meta_ec2_tag_<tagkey> - Each tag value of the instance. (One label per\n\ntag.)\n\nVersion: v1.0.0 (427b8e9)\n\n163\n\nChapter 5: Service Discovery\n\n__meta_ec2_vpc_id - The ID of the VPC in which the instance is running, if\n\navailable.\n\n TIP The full list of metadata is available in the Prometheus configuration\n\ndocumentation.\n\nThe __meta_ec2_tag_<tagkey> metadata label for EC2 tags also allows us to use relabelling to better name our targets. Rather than using the IP address for the instance label, effectively the public name of the target, we can make use of the tag values. Let’s say we had an EC2 tag called Name that contained the hostname (or a friendly name of some sort) of the instance. We could use relabelling to make use of that tag value.\n\nListing 5.13: Relabelling the instance name in a EC2 discovery job\n\njob_name: amazon_instances ec2_sd_configs:\n\nregion: us-east-1\n\nrelabel_configs: - source_labels: [__meta_ec2_public_ip]\n\nregex: target_label: __address__ replacement: '$1:9100'\n\n'(.*)'\n\nsource_labels: [__meta_ec2_tag_Name] target_label: instance\n\nYou can see that we’ve added a second relabel that uses the __meta_ec2_tag_Name label, which contains the value of the Name tag as the source label, and writes it into the instance label. Assuming the Name tag, for instance 10.2.1.1, contained bastion, then our instance label would be relabelled from:\n\nVersion: v1.0.0 (427b8e9)\n\n164\n\nChapter 5: Service Discovery\n\nnode_cpu{cpu=\"cpu0\",instance=\"10.2.1.1\",job=\"nodes\",mode=\"system\"}\n\nto:\n\nnode_cpu{cpu=\"cpu0\",instance=\"bastion\",job=\"nodes\",mode=\"system\"}\n\nMaking it easier to parse metrics from that target.\n\nDNS service discovery\n\nIf file discovery doesn’t work for you, or your source or service doesn’t support any of the existing service discovery tools, then DNS discovery may be an option. DNS discovery allows you to specify a list of DNS entries and then query records in those entries to discover a list of targets. It relies on querying A, AAAA, or SRV DNS records.\n\n TIP The DNS records will be resolved by the DNS servers that are defined locally on the Prometheus server—for example, /etc/resolv.conf on Linux.\n\nLet’s look at a new job that uses DNS service discovery.\n\nListing 5.14: DNS service discovery job\n\njob_name: webapp dns_sd_configs:\n\nnames: [ '_prometheus._tcp.example.com' ]\n\nWe’ve defined a new job called webapp and specified a dns_sd_configs block. Inside that block we’ve specified the names parameter which contains an array of the DNS entries we’re going to query.\n\nVersion: v1.0.0 (427b8e9)\n\n165\n\nChapter 5: Service Discovery\n\nBy default, Prometheus’s DNS service discovery assumes you’re querying SRV or Service records. Service records are a way to define services in your DNS con- figuration. A service generally consists of one or more target host and port com- binations upon which your service runs. The format of a DNS SRV entry looks like:\n\nListing 5.15: A SRV record\n\n_service._proto.name. TTL IN SRV priority weight port target.\n\nWhere _service is the name of the service being queried, _proto is the protocol of the service, usually TCP or UDP. We specify the name of the entry, ending in a dot. We then have the TTL, or time to live, of the record. IN is the standard DNS class (it’s always IN). And we specify a priority of the target host: lower values are higher priority. The weight controls preferences for targets with the same priority; higher values are preferred. Last, we specify the port the service runs on and the host name of the host providing the service, ending in a dot.\n\nSo, for Prometheus, we might define records like:\n\nListing 5.16: Example SRV records\n\n_prometheus._tcp.example.com. 300 IN SRV 10 1 9100 webapp1. example.com. _prometheus._tcp.example.com. 300 IN SRV 10 1 9100 webapp2. example.com. _prometheus._tcp.example.com. 300 IN SRV 10 1 9100 webapp3. example.com.\n\n NOTE There is a whole RFC for DNS service discovery: RFC6763.\n\nPrometheus’s DNS discovery does not support it.\n\nVersion: v1.0.0 (427b8e9)\n\n166\n\nChapter 5: Service Discovery\n\nWhen Prometheus queries for targets it will look up the DNS server for the example. com domain. It will then search for a SRV record called _prometheus._tcp.example .com in that domain and return the service records in that entry. We only have the three records in that entry, so we’d see three targets returned.\n\nListing 5.17: The DNS targets from the SRV\n\nwebapp1.example.com:9100 webapp2.example.com:9100 webapp3.example.com:9100\n\nWe can also query individual A or AAAA records using DNS service discovery. To do so we need to explicitly specify the query type and a port for our scrape. We need to specify the port because the A and AAAA records only return the host, not the host and port combination of the SRV record.\n\nListing 5.18: DNS A record service discovery job\n\njob_name: webapp dns_sd_configs:\n\nnames: [ 'example.com' ] type: A port: 9100\n\nThis will only return any A records at the root of the example.com domain. If we wanted to return records from a specific DNS entry, we’d use this:\n\nVersion: v1.0.0 (427b8e9)\n\n167\n\nChapter 5: Service Discovery\n\nListing 5.19: DNS subdomain A record service discovery job\n\njob_name: webapp dns_sd_configs:\n\nnames: [ 'web.example.com' ] type: A port: 9100\n\nHere we’re pulling A records that resolve for web.example.com and suffixing them with the 9100 port.\n\n TIP There’s only one metadata label available from DNS service discovery: __meta_dns_name. This is set to the specific record that generated the target.\n\nSummary\n\nIn this chapter we learned about service discovery. We’ve seen several mecha- nisms for discovering targets for Prometheus to scrape, including:\n\nFile-based discovery, populated by external data sources. • Platform-based service discovery, using the APIs and data of platforms like AWS, Kubernetes, or Google Cloud.\n\nDNS-based using SRV, A, or AAAA records.\n\nBetween these three discovery approaches, you should have sufficient means to identify the resources you wish to monitor.\n\nWe also learned a bit more about relabelling, looking at the pre-scrape relabelling phase and seeing how to use metadata to add more context to our metrics.\n\nVersion: v1.0.0 (427b8e9)\n\n168\n\nChapter 5: Service Discovery\n\nNow that we’ve got metrics coming into Prometheus, let’s tell folks about them. In the next chapter, we’re going to look at alerting.\n\nVersion: v1.0.0 (427b8e9)\n\n169",
      "page_number": 163
    },
    {
      "number": 6,
      "title": "Alerting and Alertmanager",
      "start_page": 182,
      "end_page": 228,
      "detection_method": "regex_chapter",
      "content": "Chapter 6\n\nAlerting and Alertmanager\n\nI think we ought to take the men out of the loop.\n\n— War Games, 1983\n\nIn the last few chapters we’ve installed, configured, and done some basic mon- itoring with Prometheus. Now we need to understand how to generate useful alerts from our monitoring data. Prometheus is a compartmentalized platform, and the collection and storage of metrics is separate from alerting. Alerting is provided by a tool called Alertmanager, a standalone piece of your monitoring environment. Alerting rules are defined on your Prometheus server. These rules can trigger events that are then propagated to Alertmanager. Alertmanager then decides what to do with the respective alerts, handling issues like duplication, and determines what mechanism to use when sending the alert on: realtime messages, email, or via tools like PagerDuty and VictorOps.\n\nIn this chapter, we’re going to discuss what makes good alerting, install and con- figure Alertmanager and look at how to use it to route notifications and manage maintenance. We’ll then define our alerting rules on our Prometheus server, using metrics we’ve collected thus far in the book, and then trigger some alerts.\n\nFirst let’s talk a bit about good alerting.\n\n170\n\nChapter 6: Alerting and Alertmanager\n\nAlerting\n\nAlerting provides us with indication that some state in our environment has changed, usually for the worse. The key to a good alert is to send it for the right reason, at the right time, with the right tempo, and to put useful information in it.\n\nThe most common anti-pattern seen in alerting approaches is sending too many alerts. Too many alerts is the monitoring equivalent of “the boy who cried wolf”. Recipients will become numb to alerts and tune them out. Crucial alerts are often buried in floods of unimportant updates.\n\nThe reasons you’re usually sending too many alerts can include:\n\nAn alert is not actionable, it’s informational. You should turn all of these alerts off or turn them into counters that count the rate rather than alert on the symptom.\n\nA failed host or service upstream triggers alerts for everything downstream of it. You should ensure your alerting system identifies and suppresses these duplicate, adjacent alerts.\n\nYou’re alerting for causes and not symptoms. Symptoms are signs your ap- plication has stopped working, they are the manifestation of issues that may have many causes. High latency of an API or website is a symptom. That symptom could be caused by any number of issues: high database usage, memory issues, disk performance, etc. Alerting on symptoms identifies real problems. Alerting on causes alone, for example high database usage, could identify an issue but most likely will not. High database usage might be per- fectly normal for this application and may result in no performance issues for an end user or the application. Alerting on it is meaningless as its an internal state. This alert is likely to result in engineers missing more criti- cal issues because they have become numb to the volume of non-actionable, 171\n\nYou’re alerting for causes and not symptoms. Symptoms are signs your ap- plication has stopped working, they are the manifestation of issues that may have many causes. High latency of an API or website is a symptom. That symptom could be caused by any number of issues: high database usage, memory issues, disk performance, etc. Alerting on symptoms identifies real problems. Alerting on causes alone, for example high database usage, could identify an issue but most likely will not. High database usage might be per- fectly normal for this application and may result in no performance issues for an end user or the application. Alerting on it is meaningless as its an internal state. This alert is likely to result in engineers missing more criti- cal issues because they have become numb to the volume of non-actionable, 171\n\nChapter 6: Alerting and Alertmanager\n\ncause-based alerts. You should focus on symptom-based alerts and rely on your metrics or other diagnostic data to identify causes.\n\nThe second most common anti-pattern is misclassification of alerts. Sometimes this also means a crucial alert is buried in other alerts. But other times the alert is sent to the wrong place or with the incorrect urgency.\n\nThe third most common anti-pattern is sending alerts that are not useful, especially when the recipient is often a tired, freshly woken engineer on her third or fourth on-call notification for the night. Here’s an example of a stock Nagios alert:\n\nListing 6.1: Stock Nagios alert\n\nPROBLEM Host: server.example.com Service: Disk Space\n\nState is now: WARNING for 0d 0h 2m 4s (was: WARNING) after 3/3 checks\n\nNotification sent at: Thu Aug 7th 03:36:42 UTC 2015 ( notification number 1)\n\nAdditional info: DISK WARNING - free space: /data 678912 MB (9% inode=99%)\n\nThis notification appears informative but it isn’t really. Is this a sudden increase? Or has this grown gradually? What’s the rate of expansion? For example, as we noted in the introduction, 9 percent disk space free on a 1 GB partition is different from 9 percent disk free on a 1 TB disk. Can we ignore or mute this notification or do we need to act now?\n\nGood alerting has some key characteristics:\n\n1. An appropriate volume of alerts that focus on symptoms not causes - Noisy It’s alerting results in alert fatigue and, ultimately, alerts being ignored.\n\nVersion: v1.0.0 (427b8e9)\n\n172\n\nChapter 6: Alerting and Alertmanager\n\neasier to fix under-alerting than over-alerting.\n\n2. The right alert priority should be set. If the alert is urgent then it should be routed quickly and simply to the party responsible for responding. If the alert isn’t urgent, we should send it with an appropriate tempo, to be responded to when required.\n\n3. Alerts should include appropriate context to make them immediately useful.\n\n TIP There’s a great chapter on alerting in the SRE book.\n\nNow, let’s look a little more closely at the Alertmanager.\n\nHow the Alertmanager works\n\nThe Alertmanager handles alerts sent from a client, generally a Prometheus server. (It can also receive alerts from other tools, but this is beyond the scope of this book.) Alertmanager handles deduplicating, grouping, and routing alerts to re- ceivers like email, SMS, or SaaS services like PagerDuty. You can also manage maintenance using Alertmanager.\n\nVersion: v1.0.0 (427b8e9)\n\n173\n\nChapter 6: Alerting and Alertmanager\n\nFigure 6.1: Alertmanager architecture\n\nOn our Prometheus server we’ll be writing alerting rules. These rules will use the metrics we’re collecting and trigger on thresholds or criteria we’ve specified. We’ll also see how we might add some context to the alerts. When the threshold or criteria is met, an alert will be generated and pushed to Alertmanager. The alerts are received on an HTTP endpoint on the Alertmanager. One or many Prometheus servers can direct alerts to a single Alertmanager, or you can create a highly avail- able cluster of Alertmanagers, as we’ll see later in Chapter 7.\n\nAfter being received, alerts are processed by the Alertmanager and routed accord- ing to their labels. If their path determines it, they are sent by the Alertmanager to external destinations like email, SMS, or chat.\n\nVersion: v1.0.0 (427b8e9)\n\n174\n\nChapter 6: Alerting and Alertmanager\n\nLet’s continue with installing Alertmanager.\n\nInstalling Alertmanager\n\nAlertmanager is a standalone Go binary. The Prometheus.io download page con- tains files with the binaries for specific platforms. Currently Alertmanager is sup- ported on:\n\nLinux: 32-bit, 64-bit, and ARM. • Max OS X: 32-bit and 64-bit. • FreeBSD: 32-bit, 64-bit, and ARM. • OpenBSD: 32-bit, 64-bit, and ARM. • NetBSD: 32-bit, 64-bit, and ARM. • Microsoft Windows: 32-bit and 64-bit. • DragonFly: 64-bit.\n\nOlder versions of Alertmanager are available from the GitHub Releases page.\n\n NOTE At the time of writing, Alertmanager was at version 0.15.0-rc.2.\n\nInstalling Alertmanager on Linux\n\nTo install Alertmanager on a 64-bit Linux host, we can download the zipped tarball. We can use wget or curl to get the file from the download site.\n\nVersion: v1.0.0 (427b8e9)\n\n175\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.2: Download the Alertmanager tarball\n\n$ cd /tmp $ wget https://github.com/prometheus/alertmanager/releases/download/v 0.15.0-rc.2/alertmanager-0.15.0-rc.2.linux-amd64.tar.gz\n\nNow let’s unpack the alertmanager binary from the tarball, copy it somewhere useful, and change its ownership to the root user.\n\nListing 6.3: Unpack the alertmanager binary\n\n$ tar -xzf alertmanager-0.15.0-rc.2.linux-amd64.tar.gz $ sudo cp alertmanager-0.15.0-rc.2.linux-amd64/alertmanager /usr/ local/bin/\n\nLet’s also copy the amtool binary into our path. The amtool binary is used to help manage the Alertmanager and schedule maintenance windows from the command line.\n\nListing 6.4: Moving the amtool binary\n\n$ sudo cp alertmanager-0.15.0-rc.2.linux-amd64/amtool /usr/local/ bin\n\nWe can now test if Alertmanager is installed and in our path by checking its ver- sion.\n\nVersion: v1.0.0 (427b8e9)\n\n176\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.5: Checking the Alertmanager version on Linux\n\n$ alertmanager --version alertmanager, version 0.15.0-rc.2 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\n TIP This same approach will work on Mac OS X with the Darwin version of\n\nthe Alertmanager binary.\n\nInstalling Alertmanager on Microsoft Windows\n\nTo install Alertmanager on Microsoft Windows, we need to download the alertmanager.exe executable and put it in a directory. Let’s create a directory for the executable using Powershell.\n\nListing 6.6: Creating a directory on Windows\n\nC:\\> MKDIR alertmanager C:\\> CD alertmanager\n\nNow download the alertmanager.exe executable from GitHub into the C:\\ alertmanager directory:\n\nVersion: v1.0.0 (427b8e9)\n\n177\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.7: Alertmanager Windows download\n\nhttps://github.com/prometheus/alertmanager/releases/download/v 0.15.0-rc.2/alertmanager-0.15.0-rc.2.windows-amd64.tar.gz\n\nUnzip the executable using a tool like 7-Zip into the C:\\alertmanager directory. Finally, add the C:\\alertmanager directory to the path. This will allow Windows to find the executable. To do this, run this command inside Powershell.\n\nListing 6.8: Setting the Windows path\n\n$env:Path += \";C:\\alertmanager\"\n\nYou should now be able to run the alertmanager.exe executable.\n\nListing 6.9: Checking the Alertmanager version on Windows\n\nC:\\> alertmanager.exe --version alertmanager, version 0.15.0-rc.2 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\nStacks\n\nThe stacks we saw in Chapter 3 also include Alertmanager installations.\n\nA Prometheus, Node Exporter, and Grafana docker-compose stack.\n\nVersion: v1.0.0 (427b8e9)\n\n178\n\nChapter 6: Alerting and Alertmanager\n\nAnother Docker Compose single node stack with Prometheus, Alertmanager,\n\nNode Exporter, and Grafana.\n\nA Docker Swarm stack for Prometheus.\n\nInstalling via configuration management\n\nSome of the configuration management modules we saw in Chapter 3 can also install Alertmanager: You could review their capabilities to identify which install and configure Alertmanager.\n\n TIP Remember configuration management is the recommended approach for\n\ninstalling and managing Prometheus and its components!\n\nConfiguring the Alertmanager\n\nLike Prometheus, Alertmanager is configured with a YAML-based configuration file. Let’s create a new file and populate it.\n\nListing 6.10: Creating the alertmanager.yml file\n\n$ sudo mkdir -p /etc/alertmanager/ $ sudo touch /etc/alertmanager/alertmanager.yml\n\nNow let’s add some configuration to the file. Our basic configuration will send any alerts received out via email. We’ll build on this configuration as the chapter unfolds.\n\nVersion: v1.0.0 (427b8e9)\n\n179\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.11: A simple alertmanager.yml configuration file\n\nglobal:\n\nsmtp_smarthost: 'localhost:25' smtp_from: 'alertmanager@example.com' smtp_require_tls: false\n\ntemplates: - '/etc/alertmanager/template/*.tmpl'\n\nroute:\n\nreceiver: email\n\nreceivers: - name: 'email'\n\nemail_configs: - to: 'alerts@example.com'\n\nThis configuration file contains a basic setup that processes alerts and sends them via email to one address. Let’s look at each block in turn.\n\nThe first block, global, contains global configuration for the Alertmanager. These options set defaults for all the other blocks, and are valid in those blocks as over- rides. In our case, we’re just configuring some email/SMTP settings: the email server to use for sending emails, the source/from address of those emails, and we’re disabling the requirement for automatically using TLS.\n\n TIP This assumes you have a SMTP server running on the localhost on port 25.\n\nThe templates block contains a list of directories that hold alert templates. As\n\nVersion: v1.0.0 (427b8e9)\n\n180\n\nChapter 6: Alerting and Alertmanager\n\nAlertmanager can send to a variety of destinations, you often need to be able to customize what an alert looks like and the data it contains. Let’s just create this directory for the moment.\n\nListing 6.12: Creating the templates directory\n\n$ sudo mkdir -p /etc/alertmanager/template\n\nWe’ll see more about templates later.\n\nNext, we have the route block. Routes tell Alertmanager what to do with specific incoming alerts. Alerts are matched against rules and actions taken. You can think about routing like a tree with branches. Every alert enters at the root of the tree—the base route or node. Each route, except the base node, has matching criteria which should match all alerts. You can then define child routes or nodes— the branches of the tree that take specific action or interest in specific alerts. For example, all the alerts from a specific cluster might be processed by a specific child route.\n\nVersion: v1.0.0 (427b8e9)\n\n181\n\nChapter 6: Alerting and Alertmanager\n\nFigure 6.2: Alertmanager routing\n\nIn our current configuration, we have only defined the base route, the node at the root of the tree. Later in this chapter, we’ll take advantage of routes to ensure our alerts have the right volume, frequency, and destinations.\n\nWe’ve also only defined one parameter: receiver. This is the default destination for our alerts, in our case email. We’ll define that receiver next.\n\nThe last block, receivers, specifies alert destinations. You can send alerts via email, to services like PagerDuty and VictorOps, and to chat tools like Slack and HipChat. We only have one destination defined: an email address.\n\nEach receiver has a name and associated configuration. Here we’ve named our receiver email. We then provide configuration for the specific types of receivers.\n\nVersion: v1.0.0 (427b8e9)\n\n182\n\nChapter 6: Alerting and Alertmanager\n\nFor our email alerts, we use the email_configs block to specify email options, like the to address to receive alerts. We could also specify SMTP settings, which would override the settings in global, and additional items to be added, like mail headers.\n\n TIP One of the built-in receivers is called the webhook receiver. You can use\n\nthis receiver to send alerts to other destinations that do not have specific receivers in Alertmanager.\n\nNow that we have configured Alertmanager, let’s launch it.\n\nRunning Alertmanager\n\nAlertmanager runs as a web service, by default on port 9093. It is started by running the alertmanager binary on Linux and OS X, or the alertmanager.exe executable on Windows, specifying the configuration file we’ve just created. Let’s start Alertmanager now.\n\nListing 6.13: Starting Alertmanager\n\n$ alertmanager --config.file alertmanager.yml\n\nWe’ve specified our alertmanager.yml configuration file with the --config.file flag. Alertmanager has a web interface at:\n\nhttp://localhost:9093/\n\nVersion: v1.0.0 (427b8e9)\n\n183\n\nChapter 6: Alerting and Alertmanager\n\nFigure 6.3: Alertmanager web interface\n\nYou can use this interface to view current alerts and manage maintenance window alert suppression, named “silences” in Prometheus terminology.\n\n TIP There’s also a command line tool amtool, that ships with Alertmanager\n\nthat allows you to query alerts, manage silences and work with an Alertmanager server.\n\nNow’s lets configure Prometheus to find our Alertmanager.\n\nConfiguring Prometheus for Alertmanager\n\nLet’s quickly detour back to our Prometheus configuration to tell it about our new Alertmanager. In Chapter 3 we saw the default Alertmanager configuration in the prometheus.yml configuration file. The Alertmanager configuration is contained in the alerting block. Let’s have a look at the default block.\n\nVersion: v1.0.0 (427b8e9)\n\n184\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.14: The alerting block\n\nalerting:\n\nalertmanagers: - static_configs:\n\ntargets:\n\nalertmanager:9093\n\nThe alerting block contains configuration that allows Prometheus to identify one or more Alertmanagers. To do this, Prometheus reuses the same discovery mechanisms it uses to find targets to scrape. In the default configuration this is static_configs. Like a monitoring job this specifies a list of targets, here in the form of a host name, alertmanager, and a port, 9093—the Alertmanager default port. This listing assumes your Prometheus server can resolve the alertmanager hostname to an IP address and that the Alertmanager is running on port 9093 on that host.\n\n TIP You’ll also be able to see any configured Alertmanagers in the Prometheus\n\nweb interface on the status page: http://localhost:9090/status.\n\nAlertmanager service discovery\n\nAs we have access to service discovery mechanisms, we could also use one of those to identify one or more Alertmanagers. Let’s add a DNS SRV record that allows Prometheus to discover our Alertmanagers.\n\nLet’s create that record now.\n\nVersion: v1.0.0 (427b8e9)\n\n185\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.15: The Alertmanager SRV record\n\n_alertmanager._tcp.example.com. 300 IN SRV 10 1 9093 alertmanager1.example.com.\n\nHere we’ve specified a TCP service called _alertmanager in the form of a SRV record. Our record returns the hostname alertmanager1.example.com and port number 9093 where Prometheus will expect to find an Alertmanager running. Let’s configure the Prometheus server to search there.\n\nListing 6.16: Discovering the Alertmanager\n\nalerting:\n\nalertmanagers: - dns_sd_configs:\n\nnames: [ '_alertmanager._tcp.example.com' ]\n\nHere Prometheus will query the _alertmanager._tcp.example.com SRV record to return our Alertmanager’s hostname. We can do the same with other service discovery mechanisms to identify Alertmanagers to Prometheus.\n\n TIP You’ll need to reload or restart Prometheus to enable the Alertmanager\n\nconfiguration.\n\nVersion: v1.0.0 (427b8e9)\n\n186\n\nChapter 6: Alerting and Alertmanager\n\nMonitoring Alertmanager\n\nLike Prometheus, Alertmanager exposes metrics about itself. Prometheus job for monitoring our Alertmanager.\n\nLet’s create a\n\nListing 6.17: The Alertmanager Prometheus job\n\njob_name: 'alertmanager' static_configs:\n\ntargets: ['localhost:9093']\n\nThis will collect metrics from http://localhost:9093/metrics and scrape a series of time series prefixed with alertmanager_. These include counts of alerts by state, and counts of successful and failed notifications by receiver—for example, all failed notifications to the email receiver. It also contains cluster status metrics that we can make use of when we look at clustering Alertmanagers in Chapter 7.\n\nAdding alerting rules\n\nNow that we’ve got Alertmanager set up, let’s add our first alerting rules. We’re going to create alerts from the node queries we developed in Chapter 4 as well as some basic availability alerting using the up metric.\n\nLike recording rules, alerting rules are defined as YAML statements in rules files loaded in the Prometheus server configuration. Let’s create a new file, node_alerts.yml, in our rules directory to hold our node alerting rules.\n\n TIP You can comingle recording rules and alerting rules in the same file, but\n\nI like to keep them in separate files for clarity.\n\nVersion: v1.0.0 (427b8e9)\n\n187\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.18: Creating an alerting rules file\n\n$ cd rules $ touch node_alerts.yml\n\nRather than add this file to the rule_files block in our prometheus.yml config- uration file, let’s use globbing to load all files that end in either _rules.yml or _alerts.yml in that directory.\n\nListing 6.19: Adding globbing rule_files block\n\nrule_files:\n\n\"rules/*_rules.yml\" - \"rules/*_alerts.yml\"\n\nYou can see that we’ve added configuration that will load all files with the right naming convention. We’d need to restart the Prometheus server to load this new alerting rules file.\n\nAdding our first alerting rule\n\nLet’s add our first rule: a CPU alerting rule. We’re going to create an alert that will trigger if the CPU query we created, the average node CPU five-minute rate, is over 80 percent for at least 60 minutes.\n\nLet’s see that rule now.\n\nVersion: v1.0.0 (427b8e9)\n\n188\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.20: Our first alerting rule\n\ngroups: - name: node_alerts\n\nrules: - alert: HighNodeCPU\n\nexpr: instance:node_cpu:avg_rate5m > 80 for: 60m labels:\n\nseverity: warning\n\nannotations:\n\nsummary: High Node CPU for 1 hour console: You might want to check the Node Dashboard at\n\nhttp://grafana.example.com/dashboard/db/node-dashboard\n\nLike our recording rules, alerting rules are grouped together. We’ve specified a group name: node_alerts. The rules in this group are contained in the rules block. Each has a name, specified in the alert clause. Ours is called HighNodeCPU. In each alert group, the alert name needs to be unique.\n\nWe also have the test or expression that will trigger the alert. This is specified in the expr clause. Our test expression uses the instance:node_cpu:avg_rate5m metric we created in Chapter 4 using a recording rule.\n\ninstance:node_cpu:avg_rate5m > 80\n\nWe append a simple check—is the metric greater than 80, or 80 percent?\n\nThe next clause, for, controls the length of time the test expression must be true for before the alert is fired. In our case, the instance:node_cpu:avg_rate5m needs to be greater than 80 percent for 60 minutes before the alert is fired. This limits the potential of the alert being a false positive or a transitory state.\n\nLast, we can decorate our alert with labels and annotations. All the current labels on time series in the alert rule are carried over to the alert. The labels clause allows us to specify additional labels to be attached to the alert; here we’ve added\n\nVersion: v1.0.0 (427b8e9)\n\n189\n\nChapter 6: Alerting and Alertmanager\n\na severity label with a value of warning. We’ll see how we can use this label shortly.\n\nThe labels on the alert, combined with the name of the alert, represent the identity of the alert. This is the same premise as time series, where the metric name and labels represent the identity of a time series.\n\nThe annotations clause allows us to specify informational labels like a description, a link to a run book, or instructions on how to handle this alert. We’ve added a label called summary that describes the alert. We’ve also added an annotation called console that points the recipient to a Grafana dashboard for node-based metrics. This is an excellent example of providing context with an annotation.\n\nNow we need to reload Prometheus to enable our new alerting rule.\n\nOnce Prometheus is restarted, you’ll be able to see your new alert in the Prometheus web interface at http://localhost:9090/alerts.\n\nVersion: v1.0.0 (427b8e9)\n\n190\n\nChapter 6: Alerting and Alertmanager\n\nFigure 6.4: List of Prometheus alerts\n\nThis is both a summary of the alerting rule and, as we’ll see shortly, a way to see the status of each alert.\n\nVersion: v1.0.0 (427b8e9)\n\n191\n\nChapter 6: Alerting and Alertmanager\n\nWhat happens when an alert fires?\n\nSo how does an alert fire? Prometheus evaluates all rules at a regular interval, defined by the evaluation_interval, which we’ve set to 15 seconds. At each evaluation cycle, Prometheus runs the expression defined in each alerting rule and updates the alert state.\n\nAn alert can have three potential states:\n\nInactive - The alert is not active. • Pending - The alert has met the test expression but is still waiting for the duration specified in the for clause to fire.\n\nFiring - The alert has met the test expression and has been Pending for\n\nlonger than the duration of the for clause.\n\nThe Pending to Firing transition ensures an alert is more likely to be valid and not flapping. Alerts without a for clause automatically transition from Inactive to Firing and only take one evaluation cycle to trigger. Alerts with a for clause will transition first to Pending and then to Firing, thus taking at least two evaluation cycles to trigger.\n\nSo far, the lifecycle of our alert is:\n\n1. The CPU of a node constantly changes, and it gets scraped by Prometheus\n\nevery scrape_interval. For us this is every 15 seconds.\n\n2. Alerting rules are then evaluated against the metrics every evaluation_interval\n\n. For us this is 15 seconds again.\n\n3. When the alerting expression is true—for us, CPU is over 80 percent—an alert is created and transitions to the Pending state, honoring the for clause. 4. Over the next evaluation cycles, if the alert test expression continues to be true, then the duration of the for is checked. If that duration is then com- plete, the alert transitions to Firing and a notification is generated and pushed to the Alertmanager. 192\n\n3. When the alerting expression is true—for us, CPU is over 80 percent—an alert is created and transitions to the Pending state, honoring the for clause. 4. Over the next evaluation cycles, if the alert test expression continues to be true, then the duration of the for is checked. If that duration is then com- plete, the alert transitions to Firing and a notification is generated and pushed to the Alertmanager. 192\n\nChapter 6: Alerting and Alertmanager\n\n5. If the alert test expression is no longer true then Prometheus changes the\n\nalerting rule’s state from Pending to Inactive.\n\nThe alert at the Alertmanager\n\nOur alert is now in the Firing state, and a notification has been pushed to the Alertmanager. We can see this alert and its status in the Prometheus web interface at http://localhost:9090/alerts.\n\n NOTE The Alertmanager API receives alerts on the URI /api/v1/alerts.\n\nPrometheus will also create a metric for each alert in the Pending and Firing states. The metric will be called ALERT and will be constructed like this example for our HighNodeCPU alert.\n\nListing 6.21: The ALERT time series\n\nALERTS{alertname=\"HighNodeCPU\",alertstate=\"firing\",severity= warning,instance=\"138.197.26.39:9100\"}\n\nEach alert metric has a fixed value of 1 and exists for the period the alert is in the Pending or Firing states. After that it receives no updates and is eventually expired.\n\nThe notification is sent to the Alertmanager(s) defined in the Prometheus configuration—in our case at the alertmanager host on port 9093. The notifica- tion is pushed to an HTTP endpoint:\n\nhttp://alertmanager:9093/api/v1/alerts\n\nVersion: v1.0.0 (427b8e9)\n\n193\n\nChapter 6: Alerting and Alertmanager\n\nLet’s assume one of our HighNodeCPU alerts has fired. We’ll be able to see that alert in the Alertmanager web console at http://alertmanager:9093/#/alerts.\n\nFigure 6.5: Fired alert in Alertmanager\n\nYou can use this interface to search for, query, and group current alerts, according to their labels.\n\nIn our current Alertmanager configuration, our alert will immediately be routed to our email receiver, and an email like this one below will be generated:\n\nVersion: v1.0.0 (427b8e9)\n\n194\n\nChapter 6: Alerting and Alertmanager\n\nFigure 6.6: HighNodeCPU alert email\n\n TIP We’ll see how to update this template later in the chapter.\n\nThis doesn’t seem very practical if we have many teams, or alerts of different severities. This is where Alertmanager routing is useful.\n\nAdding new alerts and templates\n\nSo that we have more alerts to route, let’s quickly add some other alert rules to the node_alerts.yml alerting rule file.\n\nVersion: v1.0.0 (427b8e9)\n\n195\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.22: Adding more alerting rules\n\ngroups: - name: node_alerts\n\nrules:\n\n. . .\n\nalert: DiskWillFillIn4Hours\n\nexpr: predict_linear(node_filesystem_free{mountpoint=\"/\"}[1h\n\n], 4*3600) < 0 for: 5m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Disk on {{ $labels.instance }} will fill in\n\napproximately 4 hours.\n\nalert: InstanceDown\n\nexpr: up{job=\"node\"} == 0 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Host {{ $labels.instance }} of {{ $labels.job }}\n\nis down!\n\nThe first alert replicates the predict_linear disk prediction we saw in Chapter 4. Here, if the linear regression predicts the disk space of the / root filesystem will be exhausted within four hours, the alert will fire. You’ll also notice that we’ve added some template values to the summary annotation.\n\nTemplates\n\nTemplates are a way of making use of the labels and value of your time series data in your alerts. Templates can be used in annotations and labels. The templates use the standard Go template syntax and expose some variables that contain the labels and value of a time series. The labels are made available in a convenience\n\nVersion: v1.0.0 (427b8e9)\n\n196\n\nChapter 6: Alerting and Alertmanager\n\nvariable, $labels, and the value of the metric in the variable $value.\n\n TIP The $labels and $value variables are more convenient names for the underlying Go variables: .Labels and .Value, respectively.\n\nTo refer to the instance label in our summary annotation we use {{ $labels. instance }}. If we wanted to refer to the value of the time series, we’d use {{ $value }}. Prometheus also provides some functions, which you can see in the template reference. An example of this is the humanize function, which turns a number into a more human-readable form using metric prefixes. For example:\n\nListing 6.23: Humanizing a value\n\n. . . annotations:\n\nsummary: High Node CPU of {{ humanize $value }}% for 1\n\nhour\n\nThis would display the value of the metric as a two-decimal-place percentage, e.g., 88.23%.\n\nPrometheus alerts\n\nWe shouldn’t forget that things can go wrong with our Prometheus server, too. Let’s add a couple of rules to identify issues there and alert on them. We’ll create a new file, prometheus_alerts.yml, in the rules directory to hold these. As this matches our rules glob, it’ll also be loaded by Prometheus.\n\nVersion: v1.0.0 (427b8e9)\n\n197\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.24: Creating the prometheus_alerts.yml file\n\n$ touch rules/prometheus_alerts.yml\n\nAnd let’s populate this file.\n\nListing 6.25: The prometheus_alerts.yml file\n\ngroups: - name: prometheus_alerts\n\nrules: - alert: PrometheusConfigReloadFailed\n\nexpr: prometheus_config_last_reload_successful == 0 for: 10m labels:\n\nseverity: warning\n\nannotations:\n\ndescription: Reloading Prometheus configuration has failed\n\non {{ $labels.instance }}.\n\nalert: PrometheusNotConnectedToAlertmanagers\n\nexpr: prometheus_notifications_alertmanagers_discovered < 1 for: 10m labels:\n\nseverity: warning\n\nannotations:\n\ndescription: Prometheus {{ $labels.instance }} is not\n\nconnected to any Alertmanagers\n\nHere we’ve added two new rules. The first, PrometheusConfigReloadFailed, lets us know if our Prometheus configuration has failed a reload. This lets us know, using the metric prometheus_config_last_reload_successful, if the last reload failed. If the reload did fail, the metric will have a value of 0.\n\nThe second rule makes sure our Prometheus server can discover Alertmanagers. This uses the prometheus_notifications_alertmanagers_discovered metric,\n\nVersion: v1.0.0 (427b8e9)\n\n198\n\nChapter 6: Alerting and Alertmanager\n\nIf it is less than 1, which is a count of Alertmanagers this server has found. Prometheus hasn’t discovered any Alertmanagers and this alert will fire. As there aren’t any Alertmanagers, it will only show up on the Prometheus console on the /alerts page.\n\nAvailability alerts\n\nOur last alerts help us determine the ability of hosts and services. The first of these alerts takes advantage of the systemd metrics we are collecting using the Node Exporter. We’re going to generate an alert if any of the services we’re monitoring on our nodes is no longer active.\n\nListing 6.26: Node service alert\n\nalert: NodeServiceDown\n\nexpr: node_systemd_unit_state{state=\"active\"} != 1 for: 60s labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Service {{ $labels.name }} on {{ $labels.instance }}\n\nis no longer active!\n\ndescription: Werner Heisenberg says - \"OMG Where's my\n\nservice?\"\n\nThis alert will trigger if the node_systemd_unit_state metric with the active label is 0, indicating that a service has failed for at least 60 seconds.\n\nThe next alert uses the up metric we also saw in Chapter 4. This metric is useful for monitoring the availability of a host. It’s not perfect because what we’re really monitoring is the success or failure of a job’s scrape of that target. But it’s useful to know if the instance has stopped responding to scrapes, which potentially indi- cates a larger problem. To do this, the alert detects if the up metric has a value of 0, indicating a failed scrape.\n\nVersion: v1.0.0 (427b8e9)\n\n199\n\nChapter 6: Alerting and Alertmanager\n\nup{job=\"node\"} == 0\n\nWe’ve added a new value to our severity label of critical and added a templated annotation to help indicate which instance and job have failed.\n\nIn many cases, knowing a single instance is down isn’t actually very important. Instead we could also test for a number of failed instances—for example, a per- centage of our instances:\n\navg(up) by (job) <= 0.50\n\nThis test expression works out the average of the up metric, aggregates it by job, and fires if that value is below 50 percent. If 50 percent of the instances in a job fail their scrapes, the alert will fire.\n\nAnother approach might be:\n\nsum by job (up) / count(up) <= 0.8\n\nHere we’re summing the up metric by job, dividing it by the count, and firing if the result is greater than or equal to 0.8 or indicating that 20 percent of instances in a specific job are not up.\n\nWe can make our up alert slightly more robust by identifying when targets dis- appear. If, for example, our target is removed from service discovery, then its metrics will no longer be updated. If all targets disappear from service discovery, no metrics will be recorded—hence our up alert won’t be fired. Prometheus has a function, absent, that detects the presence of missing metrics.\n\nVersion: v1.0.0 (427b8e9)\n\n200\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.27: The up metric missing alert\n\nalert: InstancesGone\n\nexpr: absent(up{job=\"node\"}) for: 10s labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Host {{ $labels.instance }} is no longer\n\nreporting!\n\ndescription: 'Werner Heisenberg says, OMG Where are my\n\ninstances?'\n\nHere our expression uses the absent function to detect if any of the up metrics from the node job disappear, and it fires an alert if they do.\n\n TIP Another approach for availability monitoring is the probing of endpoints\n\nover HTTP, HTTPS, DNS, TCP, and ICMP. We’ll see more of it in Chapter 10.\n\nFinally, we’ll need to restart the Prometheus server to load these new alerts.\n\nRouting\n\nNow that we have a selection of alerts with some varying attributes, we need to route them to various folks. We discovered earlier that routing is a tree. The top, default route is always configured and matches anything that isn’t matched by a child route.\n\nGoing back to our Alertmanager configuration, let’s add some routing configura- tion to our alertmanager.yml file.\n\nVersion: v1.0.0 (427b8e9)\n\n201\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.28: Adding routing configuration\n\nroute:\n\ngroup_by: ['instance'] group_wait: 30s group_interval: 5m repeat_interval: 3h receiver: email routes: - match:\n\nseverity: critical\n\nreceiver: pager\n\nmatch_re:\n\nseverity: ^(warning|critical)$\n\nreceiver: support_team\n\nreceivers: - name: 'email'\n\nemail_configs: - to: 'alerts@example.com'\n\nname: 'support_team'\n\nemail_configs: - to: 'support@example.com'\n\nname: 'pager'\n\nemail_configs: - to: 'alert-pager@example.com'\n\nYou can see that we’ve added some new options to our default route. The first op- tion, group_by, controls how the Alertmanager groups alerts. By default, all alerts are grouped together, but if we specify group_by and any labels, then Alertman- ager will group alerts by those labels. For example, we’ve specified the instance label, which means that all alerts from a specific instance will be grouped together. If you list more than one label, alerts are grouped if every specified label value matches, for example:\n\nVersion: v1.0.0 (427b8e9)\n\n202\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.29: Grouping\n\nroute:\n\ngroup_by: ['service', 'cluster']\n\nHere the values of the service and cluster labels need to match for an alert to be grouped.\n\n NOTE This only works for labels, not annotations.\n\nGrouping also changes the behavior of Alertmanager. If a new alert is raised, Alertmanager will wait for the period specified in our next option, group_wait, to see if other alerts from that group are received, before firing the alert(s). You can think about this like a group alert buffer. In our case, this wait is 30 seconds.\n\nAfter the alert(s) are fired, if new alerts from the next evaluation are received for that grouping, Alertmanager will wait for the period specified in the group_interval option, five minutes for us, before sending the new alerts. This prevents alert floods for groupings of alerts.\n\nWe’ve also specified the repeat_interval. This is a pause that applies not to our groups of alerts, but rather to each single alert, and is the period to wait to resend the same alert. We’ve specified three hours.\n\nRoutes\n\nWe’ve then listed our branched routes. Our first route uses a new receiver we’ve defined, pager. This sends the alerts on this route to a new email address. It finds the specific alerts to be sent using the match option. There are two kind\n\nVersion: v1.0.0 (427b8e9)\n\n203\n\nChapter 6: Alerting and Alertmanager\n\nof matching: label matching and regular expression matching. The match option does simple label matching.\n\nListing 6.30: Label matching\n\nmatch:\n\nseverity: critical\n\nHere we’re matching all severity labels with a value of critical and sending them to the pager receiver.\n\nAs routes are branches, we can also branch the route again if we need. For exam- ple:\n\nListing 6.31: Routing branching\n\nroutes: - match:\n\nseverity: critical\n\nreceiver: pager routes:\n\nmatch:\n\nservice: application1\n\nreceiver: support_team\n\nYou can see our new routes block nested inside our existing route. To trigger this route our alert would first need a severity label of critical and then a service label of application1. If both these criteria matched, then our alert would be routed to the receiver support_team.\n\nWe can nest our routes as far down as we need. By default, any alert that matches a route is handled by that route. We can, however, override that behavior using the continue option. The continue option controls whether an alert will traverse the route and then return to traverse the route tree.\n\nVersion: v1.0.0 (427b8e9)\n\n204\n\nChapter 6: Alerting and Alertmanager\n\n NOTE Alertmanager routes are post-order traversed.\n\nListing 6.32: Routing branching\n\nroutes: - match:\n\nseverity: critical\n\nreceiver: pager continue: true\n\nThe continue option defaults to false, but if set to true the alert will trigger in this route if matched, and continue to the next sibling route. This is sometimes useful for sending alerts to two places, but a better approach is to specify multiple endpoints in your receiver. For example:\n\nListing 6.33: Multiple endpoints in a receiver\n\nreceivers: - name: 'email'\n\nemail_configs: - to: 'alerts@example.com' pagerduty_configs: - service_key: TEAMKEYHERE\n\nThis adds a second pagerduty_configs block that sends to PagerDuty as well as via email. We could specify any of the available receiver destinations—for example, we could send email and a message to a chat service like Slack.\n\n TIP Used to seeing resolution alerts? These are alerts generated when the\n\nVersion: v1.0.0 (427b8e9)\n\n205\n\nChapter 6: Alerting and Alertmanager\n\nalert condition has been resolved. They can be sent with Alertmanager by setting the send_resolved option to true in your receiver configuration. Sending these resolution alerts is often not recommended as it can lead to a cycle of alerting “false alarms” that result in alert fatigue. Think carefully before enabling them.\n\nOur second route uses the match_re option to match a regular expression against a label. The regular expression also uses the severity label.\n\nListing 6.34: A regular expression match\n\nmatch_re:\n\nseverity: ^(informational|warning)$\n\nreceiver: support_team\n\n NOTE Prometheus and Alertmanager regular expressions are fully an-\n\nchored.\n\nIt matches either informational or warning values in the severity label.\n\nOnce you’ve reloaded or restarted Alertmanager to load the new routes, you can try to trigger alerts and see the routing in action.\n\nReceivers and notification templates\n\nNow that we’ve got some basic rules in place, let’s add a non-email receiver. We’re going to add the Slack receiver, which sends messages to Slack instances. Let’s see our new receiver configuration in the alertmanager.yml configuration file.\n\nVersion: v1.0.0 (427b8e9)\n\n206\n\nChapter 6: Alerting and Alertmanager\n\nFirst, we’ll add a Slack configuration to our pager receiver.\n\nListing 6.35: Adding a Slack receiver\n\nreceivers: - name: 'pager'\n\nemail_configs: - to: 'alert-pager@example.com' slack_configs: - api_url: https://hooks.slack.com/services/ABC123/ABC123/\n\nEXAMPLE\n\nchannel: #monitoring\n\nNow, any route that sends alerts to the pager receiver will be sent both to Slack in the #monitoring channel and via email to the alert-pager@example.com email address.\n\nThe generic alert message that Alertmanager sends to Slack is pretty simple. You can see the default template that Alertmanager uses in its source code. This tem- plate contains the defaults for email and other receivers, but we can override these values for many of the receivers. For example, we can add a text line to our Slack alerts.\n\nListing 6.36: Adding a Slack receiver\n\nslack_configs: - api_url: https://hooks.slack.com/services/ABC123/ABC123/\n\nEXAMPLE\n\nchannel: #monitoring text: '{{ .CommonAnnotations.summary }}'\n\nAlertmanager notification customization uses Go templating syntax. The data contained in the alerts is also exposed via variables. We’re using the CommonAnnotations variable, which contains the set of annotations common to\n\nVersion: v1.0.0 (427b8e9)\n\n207\n\nChapter 6: Alerting and Alertmanager\n\na group of alerts. We’re using the summary annotation as the text of the Slack notification.\n\n TIP You can find a full reference to notification template variables in the\n\nAlertmanager documentation.\n\nWe can also use the Go template function to reference external templates, to save on having long, complex strings embedded in our configuration file. We refer- enced the template directory earlier in this chapter—ours is at /etc/alertmanager /templates/. Let’s create a template in this directory.\n\nListing 6.37: Creating a template file\n\n$ touch /etc/alertmanager/templates/slack.tmpl\n\nAnd let’s populate it.\n\nListing 6.38: The slack.tmpl file\n\n{{ define \"slack.example.text\" }}{{ .CommonAnnotations.summary }}{{ end}}\n\nHere we’ve defined a new template using the define function and ending with end. We’ve called it slack.example.text and moved the content from text in- side the template. We can now reference that template inside our Alertmanager configuration.\n\nVersion: v1.0.0 (427b8e9)\n\n208\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.39: Adding a Slack receiver\n\nslack_configs: - api_url: https://hooks.slack.com/services/ABC123/ABC123/\n\nEXAMPLE\n\nchannel: #monitoring text: '{{ template \"slack.example.text\" . }}'\n\nWe’ve used the template option to specify the name of our template. The text field will now be populated with our template notification. This is useful for decorating notifications with context.\n\n TIP There are some other examples of notification templates in the Alertman-\n\nager documentation.\n\nSilences and maintenance\n\nOften we need to let our alerting system know that we’ve taken something out of service for maintenance and that we don’t want alerts triggered. Or we need to mute downstream services and applications when something upstream is bro- ken. Prometheus calls this muting of alerts a “silence.” Silences can be set for specific periods—for example, an hour—or over a set window—for example, un- til midnight today. This is the silence’s expiry time or expiration date. If required, we can also manually expire a silence early, if, say, our maintenance is complete earlier than planned.\n\nYou can schedule silences using two methods.\n\nVersion: v1.0.0 (427b8e9)\n\n209\n\nChapter 6: Alerting and Alertmanager\n\nVia the Alertmanager web console. • Via the amtool command line tool.\n\nControlling silences via the Alertmanager\n\nThe first method is to use the web interface and click the New Silence button.\n\nFigure 6.7: Scheduling silences\n\nSilences specify a start time, end time, or a duration. The alerts to be silenced are identified by matching alerts using labels, much like alert routing. You can use straight matches—for example, matching every alert that has a label with a specific value—or you can use a regular expression match. You also need to specify an author for the silence and a comment explaining why alerts are being silenced.\n\nVersion: v1.0.0 (427b8e9)\n\n210\n\nChapter 6: Alerting and Alertmanager\n\nFigure 6.8: A new silence\n\nWe click Create to create the new silence (and we can use Preview Alerts to identify if any current alerts will be silenced). Once created we can edit a silence or expire it to remove the silence.\n\nVersion: v1.0.0 (427b8e9)\n\n211\n\nChapter 6: Alerting and Alertmanager\n\nFigure 6.9: Editing or expiring silences\n\nYou can see a list of the currently defined silences in the web interface by clicking on the Silences menu item in the Alertmanager top menu.\n\n NOTE There’s an alternative Alertmanager console called Unsee you might\n\nlike to check out.\n\nVersion: v1.0.0 (427b8e9)\n\n212\n\nChapter 6: Alerting and Alertmanager\n\nControlling silences via amtool\n\nThe second method is using the amtool command line. The amtool binary ships with the Alertmanager installation tarball, and we installed it when we installed Alertmanager earlier in the chapter.\n\nListing 6.40: Using amtool to schedule a silence\n\n$ amtool --alertmanager.url=http://localhost:9093 silence add alertname=InstancesGone service=application1 784ac68d-33ce-4e9b-8b95-431a1e0fc268\n\nThis will add a new silence on the Alertmanager at http://localhost:9093. The silence will match alerts with two labels: alertname, an automatically populated label containing the alert’s name, and service, a label we’ve set.\n\n TIP Silences created with amtool are set to automatically expire after one hour. You can specify longer times or a set window with the --expires and --expire-on flags.\n\nA silence ID will also be returned that you can use to later work with the silence. Here ours is:\n\n784ac68d-33ce-4e9b-8b95-431a1e0fc268\n\nWe can query the list of current silences using the query sub-command.\n\nVersion: v1.0.0 (427b8e9)\n\n213\n\nChapter 6: Alerting and Alertmanager\n\nListing 6.41: Querying the silences\n\n$ amtool --alertmanager.url=http://localhost:9093 silence query\n\nThis will return a list of silences and their configurations. You can expire a specific silence via its ID.\n\nListing 6.42: Expiring the silence\n\n$ amtool --alertmanager.url=http://localhost:9093 silence expire\n\n784ac68d-33ce-4e9b-8b95-431a1e0fc268\n\nThis will expire the silence on the Alertmanager.\n\nRather than having to specify the --alertmanager.url flag every time, you can create a YAML configuration file for some options. The default configuration file paths that amtool will look for are $HOME/.config/amtool/config.yml or /etc/ amtool/config.yml. Let’s see a sample file.\n\nListing 6.43: Sample amtool configuration file\n\nalertmanager.url: \"http://localhost:9093\" author: sre@example.com comment_required: true\n\nYou can see that we’ve added an Alertmanager to work with. We’ve also specified an author. This is the setting for the creator of a silence; it defaults to your local username, unless overridden like this, or on the command line with the -a or - -author flag. The comment_required flag controls whether a silence requires a comment explaining what it does.\n\nVersion: v1.0.0 (427b8e9)\n\n214\n\nChapter 6: Alerting and Alertmanager\n\n NOTE You can specify all amtool flags in the configuration file, but some\n\ndon’t make a lot of sense.\n\nBack to creating silences. You can also use a regular expression as the label value when creating a silence.\n\nListing 6.44: Using amtool to schedule a silence\n\n$ amtool silence add --comment \"App1 maintenance\" alertname=~' Instance.*' service=application1\n\nHere we’ve used =~ to indicate the label match is a regular expression and matched on all alerts with an alertname that starts with Instance. We’ve also used the -- comment flag to add information about our alert.\n\nWe can also control further details of the silence, like so:\n\nListing 6.45: Omitting alertname\n\n$ amtool silence add --author \"James\" --duration \"2h\" alertname= InstancesGone service=application1\n\nHere we’ve overridden the silence’s creator with the --author flag and specified the duration of the silence as two hours, instead of the default one hour.\n\n TIP The amtool also allows us to work with Alertmanager and validate its\n\nconfiguration files, among other useful tasks. You can see the full list of command line flags by running amtool with the --help flag. You can also get help for specific\n\nVersion: v1.0.0 (427b8e9)\n\n215\n\nChapter 6: Alerting and Alertmanager\n\nsub-commands, amtool silence --help. You can generate Bash completions and a man page for amtool using instructions from here.\n\nSummary\n\nIn this chapter, we had a crash course on alerting with Prometheus and Alertman- ager.\n\nWe touched upon what good alerts look like. We installed Alertmanager on a variety of platforms and configured it.\n\nWe saw how to use our time series as a source for alerts, and how to generate those alerts using alerting rules. We saw how to use time series directly or how to build further expressions that analyze time series data to identify alert conditions. We also saw how to add new labels and decorate alerts with additional information and context.\n\nWe also saw how to control alerting using silences to mute alerts during mainte- nance windows or outages.\n\nIn the next chapter we’ll see how to make Prometheus and the Alertmanager more resilient and scalable. We’ll also see how to extend the retention life of your metrics by sending them to remote destinations.\n\nVersion: v1.0.0 (427b8e9)\n\n216",
      "page_number": 182
    },
    {
      "number": 7,
      "title": "Scaling and Reliability",
      "start_page": 229,
      "end_page": 249,
      "detection_method": "regex_chapter",
      "content": "Chapter 7\n\nScaling and Reliability\n\nUp until now we’ve seen Prometheus operate as a single server with a single Alert- manager. This fits many monitoring scenarios, especially at the team level when a team is monitoring their own resources, but it often doesn’t scale to multiple teams. It’s also not very resilient or robust. In these situations, if our Prometheus server or Alertmanager becomes overloaded or fails, our monitoring or alerting will fail, too.\n\nWe’re going to separate these into two concerns:\n\nReliability and fault tolerance. • Scaling.\n\nPrometheus addresses each concern differently, but we’ll see how some archi- tectural choices address both. In this chapter we’ll discuss the philosophy and methodology by which Prometheus approaches each concern and understand how to build more scalable and robust Prometheus implementations.\n\n217\n\nChapter 7: Scaling and Reliability\n\nReliability and fault tolerance\n\nPrometheus’s approach to addressing the issue of fault tolerance is tempered by concern about the operational and technical complexities of achieving a high tol- erance. In many cases, fault tolerance for monitoring services is addressed by making the monitoring service highly available, usually by clustering the imple- mentation. Clustering solutions, however, require relatively complex networking and management of state between nodes in the cluster.\n\nIt’s also important to note, as we mentioned in Chapter 2, that Prometheus is focused on real time monitoring, typically with limited data retention, and con- figuration is assumed to be managed by a configuration management tool. An individual Prometheus server is generally considered disposable from an avail- ability perspective. Prometheus architecture argues that the investment required to achieve that cluster, and consensus of data between nodes of that cluster, is higher than the value of the data itself.\n\nPrometheus doesn’t ignore the need to address fault tolerance though. Indeed, the recommended fault-tolerant solution for Prometheus is to run two identically configured Prometheus servers in parallel, both active at the same time. Duplicate alerts generated by this configuration are handled upstream in Alertmanager using its grouping (and its inhibits capability). Instead of focusing on the fault tolerance of the Prometheus server, the recommended approach is to make the upstream Alertmanagers fault tolerant.\n\nVersion: v1.0.0 (427b8e9)\n\n218\n\nChapter 7: Scaling and Reliability\n\nFigure 7.1: Fault-tolerant architecture\n\nThis is achieved by creating a cluster of Alertmanagers. All Prometheus servers send alerts to all Alertmanagers. As mentioned, the Alertmanagers take care of deduplication and share alert state through the cluster.\n\nThere are obviously downsides to this approach. First, both Prometheus servers will be collecting metrics, doubling any potential load generated by that collection. However, one could argue that the load generated by a scrape is likely low enough that this isn’t an issue. Second, if an individual Prometheus server fails or suffers an outage, you’ll have a gap in data on one server. This means being aware of that gap when querying data on that server. This is also a relatively minor concern given there’s another server to query and that the general focus is on immediate data, not on using Prometheus data for long-term trending analysis.\n\nVersion: v1.0.0 (427b8e9)\n\n219\n\nChapter 7: Scaling and Reliability\n\n TIP There are ways to compensate for this in PromQL. For example, when asking for a single metric value from two sources, you could use the max by of both metrics. Or, when alerting from a single worker shard with possible gaps, you might increase the for clause to ensure you have more than one measure.\n\nDuplicate Prometheus servers\n\nWe’re not going to document the details of building two duplicate Prometheus servers; this should be relatively easy to achieve using your configuration man- agement tool. We recommend replicating the installation steps from Chapter 3 using one of the configuration management solutions documented there.\n\nInstead we’re going to focus on the more complex operation of clustering Alert- managers.\n\nSetting up Alertmanager clustering\n\nAlertmanager contains a cluster capability provided by Hashicorp’s memberlist library. Memberlist is a Go library that manages cluster membership and member- failure detection using a gossip-based protocol, in this case an extension of the SWIM protocol.\n\nTo configure clustering we need to install Alertmanager on more than one host. In our case we’re going to run it on three hosts: am1, am2, and am3. We first install Alertmanager on each host as we did in Chapter 6. We’ll then use the am1 host to initiate the cluster.\n\nVersion: v1.0.0 (427b8e9)\n\n220\n\nChapter 7: Scaling and Reliability\n\nListing 7.1: Starting Alertmanager cluster\n\nam1$ alertmanager --config.file alertmanager.yml --cluster. listen-address 172.19.0.10:8001\n\nWe’ve run the alertmanager binary specifying a configuration file, we can just use the file we created in Chapter 6, and a cluster listen address and port. You should use identical configuration on every node in the cluster. This ensures that alert handling is identical and that your cluster will behave consistently.\n\n WARNING All Alertmanagers should use identical configuration! If it’s\n\nnot identical, it’s not actually highly available.\n\nWe’ve specified the IP address of the am1 host, 172.19.0.10, and a port of 8001. Other nodes in the Alertmanager cluster will use this address to connect to the clus- ter, so that port will need to be open on the network between your Alertmanager cluster nodes.\n\n TIP If you don’t specify the cluster listen address, it’ll default to 0.0.0.0 on port 9094.\n\nWe can then run the Alertmanager on the remaining two hosts, listening on their local IP addresses, and referencing the IP address and port of the cluster node we’ve just created.\n\nVersion: v1.0.0 (427b8e9)\n\n221\n\nChapter 7: Scaling and Reliability\n\nListing 7.2: Starting Alertmanager cluster remaining nodes\n\nam2$ alertmanager --config.file alertmanager.yml --cluster. listen-address 172.19.0.20:8001 --cluster.peer 172.19.0.10:8001 am3$ alertmanager --config.file alertmanager.yml --cluster. listen-address 172.19.0.30:8001 --cluster.peer 172.19.0.10:8001\n\nYou can see that we’ve run the alertmanager binary on the other two Alertman- ager hosts: am2 and am3. We’ve specified a cluster listen address for each using their own IP addresses and the 8001 port. We’ve also specified, using the cluster .peer flag, the IP address and port of the am1 node as a peer so they can join the cluster.\n\nYou won’t see any specific messages indicating the cluster has started (although if you pass the --debug flag you’ll get more informative output) but you can confirm it on one of the Alertmanager’s console status page at /status. Let’s look at am1 at https://172.19.0.10:9093/status.\n\nVersion: v1.0.0 (427b8e9)\n\n222\n\nChapter 7: Scaling and Reliability\n\nFigure 7.2: Alertmanager cluster status\n\nWe can see our am1 Alertmanager can see three nodes in the cluster: itself plus am2 and am3.\n\nYou can test that the cluster is working by scheduling a silence on one Alertman- ager and seeing if it is replicated to the other Alertmanagers. To do this, click the New Silence button on am1 and schedule a silence. Then check the /silences path on am2 and am3. You should see the same silence replicated on all hosts.\n\nNow that our cluster is running, we need to tell Prometheus about all the Alert-\n\nVersion: v1.0.0 (427b8e9)\n\n223\n\nChapter 7: Scaling and Reliability\n\nmanagers.\n\nConfiguring Prometheus for an Alertmanager cluster\n\nFor resilience purposes, we have to specifically identify all Alertmanagers to the Prometheus server. This way, if an Alertmanager goes down, Prometheus can find an alternative to send an alert to. The Alertmanager cluster itself takes care of sharing any received alert with the other active members of the cluster and potentially handles any deduplication. Thus you should not load balance your Alertmanagers—Prometheus handles that for you.\n\nWe could define all of the Alertmanagers to Prometheus using static configuration like so:\n\nListing 7.3: Defining alertmanagers statically\n\nalerting:\n\nalertmanagers: - static_configs:\n\ntargets:\n\nam1:9093 - am2:9093 - am3:9093\n\nWith this configuration the Prometheus server will connect to all three of our Alertmanagers. This assumes that our Prometheus server can resolve DNS entries for each of the Alertmanagers. A smarter approach is to use service discovery to find all of the Alertmanagers. For example, to use DNS-based discovery as we saw in Chapter 6, we can add DNS SRV records for each Alertmanager.\n\nVersion: v1.0.0 (427b8e9)\n\n224\n\nChapter 7: Scaling and Reliability\n\nListing 7.4: The Alertmanager SRV record\n\n_alertmanager._tcp.example.com. 300 IN SRV 10 1 9093 am1.example. com. _alertmanager._tcp.example.com. 300 IN SRV 10 1 9093 am2.example. com. _alertmanager._tcp.example.com. 300 IN SRV 10 1 9093 am3.example. com.\n\nHere we’ve specified a TCP service called _alertmanager in the form of a SRV record. Our record returns three host names—am1, am2, and am3—and port num- ber 9093 where Prometheus can expect to find an Alertmanager running. Let’s configure the Prometheus server to discover them.\n\nListing 7.5: Discovering the Alertmanager\n\nalerting:\n\nalertmanagers: - dns_sd_configs:\n\nnames: [ '_alertmanager._tcp.example.com' ]\n\nHere Prometheus will query the alertmanager.example.com SRV record to return our list of Alertmanagers. We could do the same with other service discovery mechanisms to identify all the Alertmanagers in our cluster to Prometheus.\n\nIf we now restart Prometheus we can see all of our connected Alertmanagers in the Prometheus server’s status page.\n\nVersion: v1.0.0 (427b8e9)\n\n225\n\nChapter 7: Scaling and Reliability\n\nFigure 7.3: Prometheus clustered Alertmanagers\n\nNow when an alert is raised it is sent to all the discovered Alertmanagers. The Alertmanagers receive the alert, handle deduplication, and share state across the cluster.\n\nTogether this provides the upstream fault tolerance that ensures your alerts are delivered.\n\nScaling\n\nIn addition to fault tolerance, we also have options for scaling Prometheus. Most of the options are essentially manual and involve selecting specific workloads to run on specific Prometheus servers.\n\nScaling your Prometheus environment usually takes two forms: functional scaling or horizontal scaling.\n\nVersion: v1.0.0 (427b8e9)\n\n226\n\nChapter 7: Scaling and Reliability\n\nFunctional scaling\n\nFunctional scaling uses shards to split monitoring concerns onto separate Prometheus servers. For example, this could be splitting servers via geography or logical domains.\n\nFigure 7.4: Organizational sharding\n\nOr it could be via specific functions, sending all infrastructure monitoring to one server and all application monitoring to another server.\n\nVersion: v1.0.0 (427b8e9)\n\n227\n\nChapter 7: Scaling and Reliability\n\nFigure 7.5: Functional sharding\n\nIt is a relatively simple process to create otherwise identical Prometheus servers with specific jobs running on each server. It is best done using configuration management tools to ensure creating servers, and the specific jobs that run on them, is an automated process.\n\nFrom here, if you need a holistic view of certain areas or functions, you can po- tentially use federation (more on this shortly) to extract time series to centralized Prometheus servers. Usefully, Grafana supports pulling data from more than one Prometheus server to construct a graph. This allows you to federate data from multiple servers at the visualization level, assuming some consistency in the time series being collected.\n\nVersion: v1.0.0 (427b8e9)\n\n228\n\nChapter 7: Scaling and Reliability\n\nHorizontal shards\n\nAt some point, usually in huge installations, the capacity and complexity of ver- tical sharding will become problematic. This is especially true when individual jobs contain thousands of instances. In that case, you can consider an alternative: horizontal sharding. Horizontal sharding uses a series of worker servers, each of which scrapes a subset of targets. We then aggregate specific time series we’re in- terested in on the worker servers. For example, if we’re monitoring host metrics, we might aggregate a subset of those metrics. A primary server then scrapes each of the worker’s aggregated metrics using Prometheus’s federation API.\n\nFigure 7.6: Horizontal sharding\n\nVersion: v1.0.0 (427b8e9)\n\n229\n\nChapter 7: Scaling and Reliability\n\nOur primary server not only pulls in the aggregated metrics but now also acts as the default source for graphing or exposing metrics to tools like Grafana. You can add tiered layers of workers and primaries if you need to go deeper or scale further. A good example of this is a zone-based primary and workers, perhaps for a failure domain or a logical zone like an Amazon Availability Zone, reporting up to a global primary that treats the zone-based primaries as workers.\n\n TIP If you need to query metrics that are not being aggregated, you will\n\nneed to refer to the specific worker server that is collecting for the specific target or targets you are interested in. You can use the worker label to help you identify the right worker.\n\nIt’s important to note that this sort of scaling does have risks and limitations, perhaps the most obvious being that you need to scrape a subset of metrics from the worker servers rather than a large volume or all of the metrics the workers are collecting. This is a pyramid-like hierarchy rather than a distributed hierarchy. The scraping requests of the primary onto the workers is also load that you will need to consider.\n\nNext, you’re creating a more complex hierarchy of Prometheus servers in your environment. Rather than just the connection between the workers and the tar- gets, you also need to worry about the connection between the primary and the workers. This could reduce the reliability of your solution.\n\nLast, the potential consistency and correctness of your data could be reduced. Your workers are scraping targets according to their intervals, and your primary server is in turn scraping the workers. This introduces a delay in the results reaching the primary server and could potentially skew data or result in an alert being delayed.\n\nA consequences of the latter two issues is that it’s probably not a good idea to centralize alerting on the primary server. Instead, push alerting down onto the\n\nVersion: v1.0.0 (427b8e9)\n\n230\n\nChapter 7: Scaling and Reliability\n\nworker servers where they are more likely to identify issues, like a missing target, or reduce the lag between identifying the alert condition and the alert firing.\n\n NOTE Horizontal sharding is generally a last resort. We’d expect you to\n\nhave tens of thousands of targets or large volumes of time series being scraped per target before you’d need to scale out in this manner.\n\nWith these caveats in mind, let’s look at how we might use this configuration.\n\nCreating shard workers\n\nLet’s create some workers and see how they can scrape the target subsets. We’re going to create workers and number them 0 through 2. We’re going to assume that the primary job our workers will execute is scraping the node_exporter. Ev- ery worker needs to be uniquely identifiable. We’re going to use external labels to do this. External labels are added to every time series or alert that leaves a Prometheus server. External labels are provided via the external_labels config- uration block in our prometheus.yml.\n\nLet’s create the base configuration for our first worker, worker0, now.\n\n TIP As always, use configuration management to do this.\n\nVersion: v1.0.0 (427b8e9)\n\n231\n\nChapter 7: Scaling and Reliability\n\nListing 7.6: The worker0 configuration\n\nglobal:\n\nexternal_labels:\n\nworker: 0\n\nrule_files:\n\n\"rules/node_rules.yml\"\n\nscrape_configs:\n\njob_name: 'node' file_sd_configs:\n\nfiles:\n\ntargets/nodes/*.json refresh_interval: 5m\n\nrelabel_configs: - source_labels: [__address__]\n\nmodulus: target_label: action:\n\n3 __tmp_hash hashmod\n\nsource_labels: [__tmp_hash] ^0$ keep\n\nWe can see our external_labels block contains a label, worker, with a value of 0. We’ll use worker: 1, worker: 2, and so on for our remaining workers. We’ve defined a single job, which uses file-based service discovery, to load a list of targets from any file ending in *.json in the targets/nodes directory. We would use a service discovery tool or a configuration management tool to populate all of our nodes into the JSON file or files.\n\nWe then use relabelling to create a modulus of the source_labels hash. In our case, we’re just creating a modulus of the hash of the concatenated address label. We use a modulus of 3, the number of workers scraping metrics. You’ll need to update this value if you add workers (another good reason to use a configuration management tool that can automatically increment the modulus). The hash is\n\nVersion: v1.0.0 (427b8e9)\n\n232\n\nChapter 7: Scaling and Reliability\n\ncreated using the hashmod action. The result is then stored in a target label called __tmp_hash.\n\nWe then use the keep action to match any time series from any targets that match the modulus. So worker0 would retrieve time series from targets with a modulo of 0, worker1 those targets with a modulo of 1, etc. This evenly distributes targets between the workers. If you need to scale to more targets you can add workers and update the modulus used on the hash.\n\nWe can then aggregate the worker time series we want to federate using rules. Let’s say we’d like to gather the memory, CPU, and disk metrics from the Node Exporter for federation. To aggregate the time series we want we’re going to use the rules we created in Chapter 4—for example, the CPU rule:\n\nListing 7.7: The instance CPU rule\n\ngroups: - name: node_rules\n\nrules: - record: instance:node_cpu:avg_rate5m\n\nexpr: 100 - avg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m]))\n\nby (instance) * 100\n\nThis will create a series of new time series that we’ll then scrape upstream using a primary Prometheus server.\n\nPrimary shard server\n\nLet’s now configure a primary Prometheus server to scrape the workers for the time series. The primary Prometheus server has a job or jobs to scrape workers; each worker is a target in a job. Let’s look at the prometheus.yml configuration for our primary server.\n\nVersion: v1.0.0 (427b8e9)\n\n233\n\nChapter 7: Scaling and Reliability\n\nListing 7.8: The primary configuration\n\n. . .\n\nscrape_configs: - job_name: 'node_workers'\n\nfile_sd_configs:\n\nfiles:\n\n'targets/workers/*.json' refresh_interval: 5m\n\nhonor_labels: true metrics_path: /federate params:\n\n'match[]':\n\n'{__name__=~\"^instance:.*\"}'\n\nOn our primary server, we’ve got a job called node_workers. This job discovers the list of workers using file-based service discovery. Our workers are in workers /targets/workers.json.\n\nListing 7.9: Worker file discovery\n\n[{\n\n\"targets\": [\n\n\"worker0:9090\", \"worker1:9090\", \"worker2:9090\"\n\n]\n\n}]\n\nYou’ll note we’ve enabled the honor_labels flag. This flag controls how Prometheus handles conflicts between labels. By setting it to true we ensure that an upstream primary server doesn’t overwrite labels from downstream workers.\n\nWe’ve overridden the standard metrics path to use the /federate API. The\n\nVersion: v1.0.0 (427b8e9)\n\n234\n\nChapter 7: Scaling and Reliability\n\nfederate API endpoint allows us to query a remote Prometheus server for specific time series, specified by a matching parameter.\n\nListing 7.10: Matching parameter\n\nmetrics_path: /federate\n\nparams:\n\n'match[]':\n\n'{__name__=~\"^instance:.*\"}'\n\nWe use the params option to specify the match[] parameter. The match[] param- eter takes an instant vector selector that has to match the specific time series we want to return. In our case we’re matching against the name of the time series.\n\nListing 7.11: condition]The match[] condition\n\n'{__name__=~\"^instance:.*\"}'\n\n TIP You can specify multiple match[] parameters, and Prometheus will return\n\nthe union of all of the conditions.\n\nThis match is a regular expression match that returns all of the time series that start with instance:. All of the rules we used to aggregate our Node Exporter metrics are prefixed with instance:, so the time series for CPU, memory, and disk will be selected and scraped by the primary server.\n\nWe can see what is going to be selected by the query parameter by using curl or browsing to the /federate path, with an appropriate match[] parameter, on one of the worker servers.\n\nVersion: v1.0.0 (427b8e9)\n\n235\n\nChapter 7: Scaling and Reliability\n\nFigure 7.7: The Federate API\n\nOur query has returned all of the time series starting with instance:.\n\nThe primary server’s node_workers job will scrape these metrics each time it’s run. You can then use the primary server to query and graph the aggregated metrics from all of the targets scraped by the worker servers.\n\nRemote storage\n\nThere’s one last aspect of scaling we should mention: remote storage. Prometheus has the capability to write to (and in some cases read from) remote stores of metrics. The ability to write to remote storage allows you to send metrics from Prometheus, working around its constraints in scalability, to a remote system.\n\nPrometheus has two types of remote storage integration:\n\nIt can write metric samples to a remote destination. • It can read metric samples to a remote destination.\n\nVersion: v1.0.0 (427b8e9)\n\n236\n\nChapter 7: Scaling and Reliability\n\nThe remote storage protocol uses a Snappy-compressed protocol buffer encoding over HTTP. It’s configured in Prometheus via the remote_write and remote_read blocks.\n\nCurrently, Prometheus supports a variety of endpoints for writing and reading. You can find a full list in the Prometheus documentation but highlights include Chronix, CrateDB, Graphite, InfluxDB, OpenTSDB, and PostgreSQL.\n\nWe’re not going to cover any of these in any detail, but you should be able to follow the documentation and examples to get started.\n\nThird-party tools\n\nThere’s a small selection of third-party tools that aim to make Prometheus scaling easier. These include:\n\nCortex - A scalable Prometheus-as-a-Service tool. • Thanos - A highly available Prometheus setup with long-term storage capa- bilities.\n\nVulcan - A now discontinued attempt to build a more scalable Prometheus.\n\nMostly useful as a reference.\n\nSummary\n\nIn this chapter we learned how Prometheus handles fault tolerance for monitoring. We saw how to create a cluster of Alertmanagers to ensure that your alerts are sent.\n\nWe also saw how to scale Prometheus monitoring using additional servers or via sharding with federation.\n\nIn the next chapter we’ll look at instrumenting applications for monitoring.\n\nVersion: v1.0.0 (427b8e9)\n\n237",
      "page_number": 229
    },
    {
      "number": 8,
      "title": "Instrumenting Applications",
      "start_page": 250,
      "end_page": 267,
      "detection_method": "regex_chapter",
      "content": "Chapter 8\n\nInstrumenting Applications\n\nIn the last few chapters we’ve seen the mechanics of Prometheus. We’ve collected metrics to process and visualize. We’ve gathered host and container metrics using Prometheus and exporters.\n\nIn this chapter we’re going to extend our monitoring and collection to applica- tions. We’re going to focus on monitoring applications and how to emit metrics by instrumenting code. We’re going to see how to add a Prometheus client to an application, add metrics to the application, and then use a Prometheus job to scrape those metrics.\n\nFirst, though, we’re going to go through some high-level design patterns and prin- ciples you should consider when thinking about application monitoring.\n\nAn application monitoring primer\n\nLet’s look at some basic tenets for application monitoring. First, in any good ap- plication development methodology, it’s a great idea to identify what you want to build before you build it. Monitoring is no different. Sadly there’s a common anti-pattern in application development of considering monitoring and other oper-\n\n238\n\nChapter 8: Instrumenting Applications\n\national functions like security as value-add components of your application rather than core features. Monitoring (and security!) are core functional features of your applications. If you’re building a specification or user stories for your application, include monitoring for each component of your application. Not building metrics or monitoring is a serious business and operational risk resulting in:\n\nAn inability to identify or diagnose faults. • An inability to measure the operational performance of your application. • An inability to measure the business performance and success of an applica- tion or a component, such as tracking sales figures or the value of transac- tions.\n\nAnother common anti-pattern is not instrumenting enough. We’ll always recom- mended that you over-instrument your applications. One will often complain about having too little data, but rarely will one worry about having too much.\n\n NOTE Within constraints of storage capacity, your monitoring stopping\n\nworking because you exceeded that capacity is obviously undesirable. It’s often useful to look at retention time as a primary way to reduce storage without losing useful information.\n\nThird, if you use multiple environments—for example development, testing, stag- ing, and production—ensure that your monitoring configuration provides labels so you know that the data is from a specific environment. This way you can parti- tion your monitoring and metrics. We’ll talk more about this later in the chapter.\n\nVersion: v1.0.0 (427b8e9)\n\n239\n\nChapter 8: Instrumenting Applications\n\nWhere should I instrument?\n\nGood places to start adding instrumentation for your applications are at points of ingress and egress. For example:\n\nMeasure counts and timings of requests and responses, such as to specific web pages or API endpoints. If you’re instrumenting an existing application, make a priority-driven list of specific pages or endpoints, and instrument them in order of importance.\n\nMeasure counts and timings of calls to external services and APIs, such as if your application uses a database, cache, or search service, or if it uses third-party services like a payments gateway.\n\nMeasure counts and timings of job scheduling, execution, and other periodic\n\nevents like cron jobs.\n\nMeasure counts and timings of significant business and functional events,\n\nsuch as users being created, or transactions like payments and sales.\n\nInstrument taxonomies\n\nYou should ensure that metrics are categorized and clearly identified by the appli- cation, method, function, or similar marker so that you can ensure you know what and where a metric is generated. We talked about label taxonomies in Chapter 4.\n\nMetrics\n\nLike much of the rest of our monitoring, metrics are going to be key to our appli- cation monitoring. So what should we monitor in our applications? We want to look at two broad types of metrics—albeit types with considerable overlap:\n\nApplication metrics, which generally measure the state and performance of\n\nyour application code.\n\nVersion: v1.0.0 (427b8e9)\n\n240\n\nChapter 8: Instrumenting Applications\n\nBusiness metrics, which generally measure the value of your application. For example, on an e-commerce site, it might be how many sales you made.\n\nWe’re going to look at examples of both types of metrics in this chapter, with the caveat that Prometheus tends to focus on more immediate metrics. For longer- term business metrics, you may, in many cases, use event-based systems.\n\nApplication metrics\n\nApplication metrics measure the performance and state of your applications. They include characteristics of the end user experience of the application, like latency and response times. Behind this we measure the throughput of the application: requests, request volumes, transactions, and transaction timings.\n\n TIP Good examples of how to measure application performance are the USE\n\nand RED Methods and Google Golden Signals that we mentioned earlier.\n\nWe also look at the functionality and state of the application. A good example of this might be successful and failed logins or errors, crashes, and failures. We could also measure the volume and performance of activities like jobs, emails, or other asynchronous activities.\n\nBusiness metrics\n\nBusiness metrics are the next layer up from our applications metrics. They are usually synonymous with application metrics. If you think about measuring the number of requests made to a specific service as being an application metric, then the business metric usually does something with the content of the request. An\n\nVersion: v1.0.0 (427b8e9)\n\n241\n\nChapter 8: Instrumenting Applications\n\nexample of the application metric might be measuring the latency of a payment transaction; the corresponding business metric might be the value of each payment transaction. Business metrics might include the number of new users/customers, number of sales, sales by value or location, or anything else that helps measure the state of a business.\n\nWhere to put your metrics\n\nOnce we know what we want to monitor and measure, we need to work out where to put our metrics. In almost all cases the best place to put these metrics is inside our code, as close as possible to the action we’re trying to monitor or measure.\n\nWe don’t, however, want to put our metrics configuration inline everywhere that we want to record a metric. Instead we want to create a utility library: a function that allows us to create a variety of metrics from a centralized setup. This is sometimes called the utility pattern: a metrics-utility class that does not require instantiation and only has static methods.\n\nThe utility pattern\n\nA common pattern is to create a utility library or module using one of the available clients. The utility library would expose an API that allows us to create and incre- ment metrics. We can then use this API throughout our code base to instrument the areas of the application we’re interested in.\n\nLet’s take a look at an example of this. We’ve created some Ruby-esque code to demonstrate, and we’ve assumed that we have already created a utility library called Metric.\n\n NOTE We’ll see a functioning example of this pattern later in this chapter.\n\nVersion: v1.0.0 (427b8e9)\n\n242\n\nChapter 8: Instrumenting Applications\n\nListing 8.1: A sample payments method\n\ninclude Metric\n\ndef pay_user(user, amount)\n\npay(user.account, amount) Metric.increment 'payment' Metric.increment \"payment-amount, #{amount.to_i}\" send_payment_notification(user.email)\n\nend\n\ndef send_payment_notification(email)\n\nsend_email(payment, email) Metric.increment 'email-payment'\n\nend\n\nHere we’ve first included our Metric utility library. We can see that we’ve spec- ified both application and business metrics. We’ve first defined a method called pay_user that takes user and amount values as parameters. We’ve then made a payment using our data and incremented two metrics in our first method:\n\nA payment metric — Here we increment the metric each time we make a\n\npayment.\n\nA payment-amount metric — This metric records each payment by amount.\n\nFinally, we’ve sent an email using a second method, send_payment_notification , where we’ve incremented a third metric: email-payment. The email-payment metric counts the number of payment emails sent.\n\nVersion: v1.0.0 (427b8e9)\n\n243\n\nChapter 8: Instrumenting Applications\n\nThe external pattern\n\nWhat if you don’t control the code base, can’t insert monitors or measures inside your code, or perhaps have a legacy application that can’t be changed or updated? Then you need to find the next closest place to your application. The most obvi- ous places are the outputs and external subsystems around your application—for example, a database or cache.\n\nIf your application emits logs, then identify what material the logs contain and see if you can use their contents to measure the behavior of the application. Often you can track the frequency of events by simply recording the counts of specific log entries. If your application records or triggers events in other systems—things like database transactions, job scheduling, emails sent, calls to authentication or authorization systems, caches, or data stores—then you can use the data contained in these events or the counts of specific events to record the performance of your application.\n\nWe’ll talk more about this in Chapter 9.\n\nBuilding metrics into a sample application\n\nNow that we have some background on monitoring applications, let’s look at an example of how we might implement this in the real world. We’re going to build an application that takes advantage of a utility library to send events from the ap- plication. We’ve created a sample Rails application using Rails Composer. We’re going to call it mwp-rails, or Monitoring with Prometheus Rails application. The mwp-rails application allows us to create and delete users and sign in to the ap- plication.\n\n NOTE You can find the mwp-rails application on GitHub.\n\nVersion: v1.0.0 (427b8e9)\n\n244\n\nChapter 8: Instrumenting Applications\n\nTo instrument our application we first need to add support for Prometheus using a Ruby-based client. The prometheus-client gem allows us to create a Prometheus client inside our application.\n\nThere are similar clients for a number of platforms including:\n\nGo • Java/JVM • Python\n\nThere is also a large collection of third-party clients for a variety of frameworks and languages.\n\nAdding the client\n\nLet’s add the prometheus-client gem to our Rails application’s Gemfile.\n\nListing 8.2: The mwp-rails Gemfile\n\nsource 'https://rubygems.org' ruby '2.4.2' gem 'rails', '5.1.5' . . . gem 'prometheus-client' . . .\n\nWe then install the new gem using the bundle command.\n\nVersion: v1.0.0 (427b8e9)\n\n245\n\nChapter 8: Instrumenting Applications\n\nListing 8.3: Install prometheus-client with the bundle command\n\n$ sudo bundle install Fetching gem metadata from https://rubygems.org/... Fetching version metadata from https://rubygems.org/... Fetching dependency metadata from https://rubygems.org/.. . . . Installing prometheus-client 0.7.1 . . .\n\nWe can then test the client using a Rails console. Let’s launch one now using the rails c command.\n\nListing 8.4: Testing the Prometheus client with the Rails console\n\n$ rails c Loading development environment (Rails 4.2.4) [1] pry(main)> prometheus = Prometheus::Client.registry [2] pry(main)> test_counter = prometheus.counter(:test_counter, 'A test counter') => #<Prometheus::Client::Counter:0x00007f9aea051dd8\n\n@base_labels={}, @docstring=\"A test counter\", @mutex=#<Thread::Mutex:0x00007f9aea051d88>, @name=:test_counter, @validator=#<Prometheus::Client::LabelSetValidator:0\n\nx00007f9aea051d10 @validated={}>,\n\n@values={}>\n\n[3] pry(main)> test_counter.increment => 1.0\n\nWe’ve launched a Rails console and created a Prometheus registry using the code:\n\nVersion: v1.0.0 (427b8e9)\n\n246\n\nChapter 8: Instrumenting Applications\n\nListing 8.5: Creating a Prometheus registry\n\nprometheus = Prometheus::Client.registry\n\nThe registry is the core of the Prometheus application instrumentation. Every metric you create needs to be registered first. We’ve created a registry called prometheus. We can now create metrics in this registry.\n\nListing 8.6: Registering a Prometheus metric\n\ntest_counter = prometheus.counter(:test_counter, 'A test counter ')\n\nWe have a new metric called test_counter. It’s created with the counter method on the registry. The metric name needs to be a symbol, :test_counter, and needs a description. Our is: A test counter.\n\nWe can increment our new metric with the increment method.\n\nListing 8.7: Incrementing a metric\n\ntest_counter.increment\n\nNow the value of the test_counter metric will be 1.0, which we can see by query- ing the value of the metric using the get method.\n\nVersion: v1.0.0 (427b8e9)\n\n247\n\nChapter 8: Instrumenting Applications\n\nListing 8.8: Incrementing a metric\n\ntest_counter.get 1.0\n\nWe can register a number of types of metrics, including summaries and histograms.\n\nListing 8.9: The basic Prometheus client_ruby metrics\n\ntest_counter = prometheus.counter(:test_counter, 'A test counter ') test_gauge = prometheus.gauge(:test_gauge, 'A test gauge') test_histogram = prometheus.histogram(:test_histogram, 'A test histogram') test_summary = prometheus.summary(:test_summary, 'A test summary ')\n\nWe can now add the instrumentation to our Rails application.\n\nAdding it to Rails\n\nWe’re not going to manually create a registry and metrics every time we want to log any metrics, so let’s set up some utility code to do this for us. We’re going to create a Metrics module in our lib directory that we’ll use in our Rails application. Let’s do that now.\n\nListing 8.10: Creating a Metrics module\n\n$ touch lib/metrics.rb\n\nAnd let’s populate the file with a module.\n\nVersion: v1.0.0 (427b8e9)\n\n248\n\nChapter 8: Instrumenting Applications\n\nListing 8.11: The Metrics module\n\nmodule Metrics\n\ndef self.counter(name, docstring, base_labels = {})\n\nprovide_metric(name) || registry.counter(name, docstring,\n\nbase_labels)\n\nend\n\ndef self.summary(name, docstring, base_labels = {})\n\nprovide_metric(name) || registry.summary(name, docstring,\n\nbase_labels)\n\nend\n\ndef self.gauge(name, docstring, base_labels = {})\n\nprovide_metric(name) || registry.gauge(name, docstring,\n\nbase_labels)\n\nend\n\ndef self.histogram(name, docstring, base_labels = {}, buckets\n\n= ::Prometheus::Client::Histogram::DEFAULT_BUCKETS)\n\nprovide_metric(name) || registry.histogram(name, docstring,\n\nbase_labels, buckets)\n\nend\n\nprivate\n\ndef self.provide_metric(name)\n\nregistry.get(name)\n\nend\n\ndef self.registry\n\n@registry || ::Prometheus::Client.registry\n\nend\n\nend\n\nOur Metrics module has methods for each metric type. The metric methods check for the presence of an existing metric in the registry (using the get method in provide_metric which retrieves metric names) or creates a new metric.\n\nVersion: v1.0.0 (427b8e9)\n\n249\n\nChapter 8: Instrumenting Applications\n\nWe then need to extend Rails to load our Metrics library. There’s a few ways to do this, but adding an initializer is my favorite.\n\nListing 8.12: Creating an initializer for the metrics library\n\n$ touch config/initializers/lib.rb\n\nAnd then requiring our library.\n\nListing 8.13: The config/initializers/lib.rb file\n\nrequire 'metrics'\n\n TIP We could also extend the autoload_paths configuration option to load everything in lib, but I feel like this gives us less control over what loads.\n\nWe can then add metrics to some of our methods. Let’s start with incrementing a counter when users are deleted.\n\nListing 8.14: Counter for user deletions\n\ndef destroy\n\nuser = User.find(params[:id]) user.destroy Metrics.counter(:users_deleted_counter, \"Deleted users counter\n\n\").increment\n\nredirect_to users_path, :notice => \"User deleted.\"\n\nend\n\nVersion: v1.0.0 (427b8e9)\n\n250\n\nChapter 8: Instrumenting Applications\n\nWe can see the line:\n\nMetrics.counter(:users_deleted_counter, \"Deleted users counter\").\n\nincrement\n\nWe call the counter metric in the Metrics module and pass in a metric name in the form of a symbol, :users_deleted_counter, then a description of the metric, Deleted users counter. We’ve ended the line with the increment method that increments the counter once. This will create a metric called users_deleted_counter.\n\nWe could also add a label or increment by a specific value by using the increment method like so:\n\n.increment({ service: 'foo' }, 2)\n\nThis would increment a counter with a value of 2 and add the label service: foo to the metric. You can specify multiple labels by separating each with commas: { service: 'foo', app: 'bar' }.\n\nWe could also create another counter for created users by adding it to the User model.\n\nListing 8.15: Counter for user creation\n\nclass User < ActiveRecord::Base\n\nenum role: [:user, :vip, :admin] after_initialize :set_default_role, :if => :new_record? after_create do\n\nMetrics.counter(:user_created_counter, \"Users created\n\ncounter\").increment\n\nend . . . end\n\nHere we’ve used an Active Record callback, after_create, to increment a counter, users_created_counter, when a new user is created.\n\nVersion: v1.0.0 (427b8e9)\n\n251\n\nChapter 8: Instrumenting Applications\n\n NOTE You may have a wide variety of applications you want to instrument.\n\nWe’re creating an example application to show you how we might apply some of these principles. While you might not be able to reuse the code, the high-level principles apply to almost every framework and language you’re likely to have running.\n\nWe then need to expose our metrics to be scraped. We’re also going to enable some In our Rack middleware to auto-create some useful metrics on HTTP requests. case we enable the metrics endpoint by adding an exporter (and the middleware collector) to our config.ru file.\n\nListing 8.16: Adding Prometheus to the config.ru file\n\nrequire 'prometheus/middleware/collector' require 'prometheus/middleware/exporter'\n\nuse Prometheus::Middleware::Collector use Prometheus::Middleware::Exporter\n\nHere we’ve required and used two components of the Prometheus client: the mid- dleware exporter and collector. The exporter creates a route, /metrics, containing any metrics specified in Prometheus registries defined by the app. The collector adds some HTTP server metrics to the endpoint that are collected via Rack mid- dleware.\n\nIf we browse to this endpoint, /metrics, we’ll see some of those metrics.\n\nVersion: v1.0.0 (427b8e9)\n\n252\n\nChapter 8: Instrumenting Applications\n\nListing 8.17: The Rails /metrics endpoint\n\n# HELP http_server_requests_total The total number of HTTP requests handled by the Rack application. http_server_requests_total{code=\"200\",method=\"get\",path=\"/\"} 2.0 # HELP http_server_request_duration_seconds The HTTP response duration of the Rack application. http_server_request_duration_seconds_bucket{method=\"get\",path=\"/ \",le=\"0.005\"} 0.0 http_server_request_duration_seconds_bucket{method=\"get\",path=\"/ \",le=\"0.01\"} 0.0 . . .\n\n# HELP users_updated_counter Users updated counter users_updated_counter 1.0\n\nWe can see a selection of the metrics available. Perhaps most interesting is a series of histogram buckets showing the HTTP server request duration, with dimensions for method and path. This histogram is an easy way to measure request latency for specific paths and to identify any badly performing requests.\n\nUsing our metrics\n\nNow our application has metrics being generated and we can make use of them in Prometheus. Let’s create a job to scrape our /metrics endpoint. We’re going to add our Rails servers to our file-based service discovery. We’re going to add three Rails servers by their hostnames.\n\nVersion: v1.0.0 (427b8e9)\n\n253\n\nChapter 8: Instrumenting Applications\n\nListing 8.18: Our Rails servers service discovery\n\n[{\n\n\"targets\": [\"mwp−rails1.example.com\", \"mwp−rails2.example.\n\ncom\", \"mwp−rails3.example.com\"] }]\n\nWe’re then going to create our new job in our prometheus.yml configuration file.\n\nListing 8.19: The rails job\n\n− job_name: rails file_sd_configs:\n\n− files:\n\n− targets/rails/*.json refresh_interval: 5m\n\nIf we reload Prometheus we’ll be able to see our Rails servers as new targets.\n\nFigure 8.1: Rails server targets\n\nAnd see our new metrics in the dashboard.\n\nVersion: v1.0.0 (427b8e9)\n\n254\n\nChapter 8: Instrumenting Applications\n\nFigure 8.2: Rails metrics\n\nNow we can make use of these metrics to monitor our Rails servers.\n\nSummary\n\nIn this chapter we explored ways to monitor and instrument our applications and their workflows, including understanding where to place our application monitor- ing.\n\nWe learned about building our own metrics into our applications and services, and we built a sample Rails application to show how to expose and scrape metrics with Prometheus.\n\nIn the next chapter we will see how to turn external data, specifically log entries, into metric data you can consume with Prometheus.\n\nVersion: v1.0.0 (427b8e9)\n\n255",
      "page_number": 250
    },
    {
      "number": 9,
      "title": "Logging as Instrumentation",
      "start_page": 268,
      "end_page": 285,
      "detection_method": "regex_chapter",
      "content": "Chapter 9\n\nLogging as Instrumentation\n\nIn previous chapters we looked at application, host, and container-based monitor- ing. In this chapter we’re going to look at how we can use our logging data as the source of time series data that can be scraped by Prometheus. While our hosts, services, and applications can generate crucial metrics and events, they also often generate logs that can tell us useful things about their state and status.\n\nThis is especially true if you’re monitoring a legacy application that is not instru- mented or that it’s not feasible to instrument. In this case, sometimes the cost of rewriting, patching, or refactoring that application to expose internal state is not a good engineering investment, or there are technological constraints to instru- mentation. You still need to understand what’s happening inside the application, though—and one of the easiest ways is to adapt log output.\n\n TIP Another potential approach is to look at the contents of the /proc sub-\n\nsystem using the Process exporter.\n\nLog output often contains useful status, timing, and measurement information.\n\n256\n\nChapter 9: Logging as Instrumentation\n\nFor example, using the access log output from a web or application server is a useful way of tracking transaction timings or error volumes. Tools can parse these log entries, create metrics from matched output, and make them available to be scraped by a Prometheus job.\n\nIn this chapter we’re going to look at using log entries to create metrics, and then scrape them with Prometheus.\n\nProcessing logs for metrics\n\nIn order to extract data from our log entries we’re going to make use of a log processing tool. There are several we could use, including the Grok Exporter and a utility from Google called mtail. We’ve chosen to look at mtail because it’s a little more lightweight and somewhat more popular.\n\n TIP Got a Logstash/ELK installation? You can’t currently directly output to\n\nPrometheus but you can use Logstash’s metric filter to create metrics and output them to Alertmanager directly.\n\nIntroducing mtail\n\nThe mtail log processor is written by SRE folks at Google. It’s a Go application licensed with the Apache 2.0 license. The mtail log processor is specifically de- signed for extracting metrics from application logs to be exported into a time series database. It aims to fill the niche we described above: parsing log data from applications that cannot export their own internal state.\n\nThe mtail log processor works by running “programs” that define log matching\n\nVersion: v1.0.0 (427b8e9)\n\n257\n\nChapter 9: Logging as Instrumentation\n\npatterns, and specify the metrics to create from the matches and any actions to It works very well with Prometheus and exposes any created metrics for take. scraping, but can also be configured to send the metrics to tools like collectd, StatsD, or Graphite.\n\nInstalling mtail\n\nThe mtail log processor is shipped as a single binary: mtail. It’s packaged for a variety of operating systems including Linux, OS X, and Microsoft Windows.\n\nLet’s download the binary now.\n\nListing 9.1: Download and install the mtail binary\n\n$ wget https://github.com/google/mtail/releases/download/v3.0.0- rc13/mtail_v3.0.0-rc13_linux_amd64 -O mtail $ chmod 0755 mtail $ sudo cp mtail /usr/local/bin\n\nWe can confirm the mtail binary is working by running it with the --version flag.\n\nListing 9.2: Running the mtail binary\n\n$ mtail --version mtail version v3.0.0-rc13-119-g01c76cd git revision 01 c76cde1ee5399be4d6c62536d338ba3077e0e7 go version go1.8.3\n\nVersion: v1.0.0 (427b8e9)\n\n258\n\nChapter 9: Logging as Instrumentation\n\nUsing mtail\n\nThe mtail binary is configured via the command line. You specify a list of log files to parse, and a directory of programs to run over those files.\n\n TIP You can see a full list of mtails’s command line flags using the --help\n\nflag.\n\nLet’s start by creating a directory to hold our mtail programs. As always, this is usually better done by creating a configuration management module (or a Docker container) rather than being done manually, but we’ll show you the details so you can understand what’s happening.\n\nListing 9.3: Creating an mtail program directory\n\n$ sudo mkdir /etc/mtail\n\nNow let’s create our first mtail program in a file in our new directory. Every mtail program needs to end with the suffix .mtail. Let’s create a new program called line_count.mtail. This is the simplest mtail program: it increments a counter every time it parses a new line.\n\nListing 9.4: Creating the line_count.mtail program\n\n$ sudo touch /etc/mtail/line_count.mtail\n\nAnd let’s populate that file.\n\nVersion: v1.0.0 (427b8e9)\n\n259\n\nChapter 9: Logging as Instrumentation\n\nListing 9.5: The line_count.mtail program\n\ncounter line_count\n\n/$/ {\n\nline_count++\n\n}\n\nWe’ve started our program by defining a counter called line_count. Counter names are prefixed with counter (and, naturally, gauges are prefixed with gauge ). These counters and gauges are exported by mtail to whatever destination you define; in our case, it’ll be an endpoint that can be scraped by Prometheus. You must define any counters or gauges before you can work with them.\n\nNext, we define the guts of our mtail program: the condition we want to match and the action we want to take, with the condition specified first and the action following, wrapped in { }.\n\nYou can specify multiple sets of conditions and actions in a program. You can extend them with conditional logic in the form of an else clause too.\n\n NOTE mtail programs look a lot like awk programs.\n\nThe condition can be a regular expression, matching some specific log entry. In our case, we’ve specified /$/, which matches the end of the line. The mtail processor uses RE2 regular expressions. You can see the full syntax on the RE2 wiki.\n\nWe could also specify a relational expression, much like those in a C if clause, for example:\n\nVersion: v1.0.0 (427b8e9)\n\n260\n\nChapter 9: Logging as Instrumentation\n\nListing 9.6: A relational clause\n\nline_count < 20 {\n\n. . .\n\n}\n\nHere the program would only take the action if the value of the line_count counter was greater than 20.\n\nIn the case of our initial program our action is:\n\nline_count++\n\nWhich uses an operator, ++, to increment the line_counter counter. It’s about the simplest action we can take.\n\nLet’s count some log entries now.\n\n TIP You can find more documentation on the mtail syntax on GitHub and on\n\nthe guide to programming in it.\n\nRunning mtail\n\nTo run mtail we need to specify some programs to run and some log files to parse. Let’s do that now.\n\nListing 9.7: Running mtail\n\n$ sudo mtail --progs /etc/mtail --logs '/var/log/*.log'\n\nVersion: v1.0.0 (427b8e9)\n\n261\n\nChapter 9: Logging as Instrumentation\n\nThis will run mtail with two flags. The first flag, --progs, tells mtail where to find our programs. The second flag, --logs, tells mtail where to find log files to parse. We’re using a glob pattern to match all log files in the /var/log directory. You can specify a comma-separated list of files or specify the --logs flag multiple times. mtail is also conscious of log file truncation so it can handle stopping, restarting, and actions like log rotation.\n\n NOTE The user you’re running mtail as will need permissions to the log\n\nfiles you’re parsing, otherwise mtail will not be able to read the files. You will get a read error in the mtail log output, obtained using the --logtostderr flag, when it can’t read a file.\n\nWhen we run mtail, it’ll launch a web server on port 3903 (you can control the IP address and port using the --address and --port flags). Let’s browse to that web server now.\n\nThe home path shows some diagnostic information, like so:\n\nVersion: v1.0.0 (427b8e9)\n\n262\n\nChapter 9: Logging as Instrumentation\n\nFigure 9.1: mtail diagnostics\n\nYou can see the build of mtail as well as links to the default metric output formats, diagnostic information, and a list of programs loaded, any errors, and log files being tracked. You can see mtail outputs metrics in JSON, varz (an internal Google format for metrics collection), and the format we want: Prometheus. Let’s click on the Prometheus link, which will take us to the /metrics path.\n\n TIP You can also send metrics to tools like StatsD and Graphite.\n\nListing 9.8: The mtail /metrics path\n\n# TYPE line_count counter # line_count defined at line_count.mtail:1:9-18 line_count{prog=\"line_count.mtail\"} 1561\n\nWe can see some familiar output: help text and a Prometheus metric with a single label, prog. This is added automatically to each metric and populated with the name of the program that generated the metric. You can omit this label by setting the --emit_prog_label flag to false.\n\nIn our case, our line_count metric has counted 1561 lines worth of log entries. We could then add a job to scrape this endpoint for our line_counter metric.\n\nThis isn’t an overly useful example though. Let’s look at some more complex programs.\n\nVersion: v1.0.0 (427b8e9)\n\n263\n\nChapter 9: Logging as Instrumentation\n\nProcessing web server access logs\n\nLet’s use mtail to extract some metrics from an Apache access log, specifically one using the combined log format. To save some time, we can use an example program provided with mtail. We create the program in the /etc/mtail directory and name it apache_combined.mtail. The contents are:\n\nListing 9.9: The apache_combined program\n\n# Parser for the common apache \"NCSA extended/combined\" log format # LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User- agent}i\\\" counter apache_http_requests_total by request_method, http_version, request_status counter apache_http_bytes_total by request_method, http_version,\n\nrequest_status\n\n/^/ + /(?P<hostname>[0-9A-Za-z\\.-]+) / + # %h /(?P<remote_logname>[0-9A-Za-z-]+) / + # %l /(?P<remote_username>[0-9A-Za-z-]+) / + # %u /(?P<timestamp>\\[\\d{2}\\/\\w{3}\\/\\d{4}:\\d{2}:\\d{2}:\\d{2} (\\+|-)\\d {4}\\]) / + # %u /\"(?P<request_method>[A-Z]+) (?P<URI>\\S+) (?P<http_version>HTTP \\/[0-9\\.]+)\" / + # \\\"%r\\\" /(?P<request_status>\\d{3}) / + # %>s /(?P<response_size>\\d+) / + # %b /\"(?P<referer>\\S+)\" / + # \\\"%{Referer}i\\\" /\"(?P<user_agent>[[:print:]]+)\"/ + # \\\"%{User-agent}i\\\" /$/ {\n\napache_http_requests_total[$request_method][$http_version][\n\n$request_status]++\n\napache_http_bytes_total[$request_method][$http_version][\n\n$request_status] += $response_size }\n\nVersion: v1.0.0 (427b8e9)\n\n264\n\nChapter 9: Logging as Instrumentation\n\n TIP You can find the code for this program on GitHub. There’s also a large\n\ncollection of example programs to help you get started on GitHub.\n\nWe can see our program is well commented; comments are specified using the # symbol. It includes an example of the log format at the top of the program and then defines two counters:\n\napache_http_requests_total by request_method, http_version,\n\nrequest_status\n\napache_http_bytes_total by request_method, http_version, request_status\n\nThe by operator specifies additional dimensions to add to the metric. In the first counter, apache_http_requests_total, we’ve added the additional dimensions of request_method, http_version, and request_status, which will be added as labels on the resulting counter.\n\nWe then see a series of regular expressions that match each element of the access log line and are chained together using + operators.\n\n TIP These regular expressions can get quite complex when parsing convo-\n\nluted log lines, so mtail also allows you to reuse regular expressions by defining them as constants.\n\nInside these regular expressions you can see a series of captures like so:\n\n(?P<request_status>\\d{3})\n\nThese are named capture groups. In this example, we’re capturing a named value\n\nVersion: v1.0.0 (427b8e9)\n\n265\n\nChapter 9: Logging as Instrumentation\n\nof request_status. We can then use these captures in our actions.\n\nListing 9.10: The combined access log actions\n\n{\n\napache_http_requests_total[$request_method][$http_version][\n\n$request_status]++\n\napache_http_bytes_total[$request_method][$http_version][\n\n$request_status] += $response_size }\n\nThe action increments the first counter, apache_http_requests_total, adding some of the captures, prefixed with $, to the counter as dimensions. Each di- mension is wrapped in [ ] square brackets.\n\nThe second counter has an additive operation, using the += operator to add each new response size in bytes to the counter.\n\n TIP mtail can record either integer or floating point values for metrics. By\n\ndefault, all metrics are integers, unless the compiler can infer a floating point. Inference is achieved by analyzing expressions, for example identifying that a regular expression is capturing a floating point value.\n\nIf we were to run mtail again, this time loading some Apache (or other web server that used the combined log format), we’d see these new metrics populated.\n\nVersion: v1.0.0 (427b8e9)\n\n266\n\nChapter 9: Logging as Instrumentation\n\nListing 9.11: Running mtail\n\n$ sudo mtail --progs /etc/mtail --logs '/var/log/apache/*.access '\n\nAnd then browse to the /metrics path:\n\nListing 9.12: Apache combined metrics\n\n# TYPE apache_http_requests_total counter # apache_http_requests_total defined at apache_combined.mtail :6:9-34 apache_http_requests_total{http_version=\"HTTP/1.1\", request_method=\"GET\",request_status=\"200\",prog=\"apache_combined. mtail\"} 73 # apache_http_requests_total defined at apache_combined.mtail :6:9-34 apache_http_requests_total{http_version=\"HTTP/1.1\", request_method=\"GET\",request_status=\"304\",prog=\"apache_combined. mtail\"} 3 # TYPE apache_http_bytes_total counter # apache_http_bytes_total defined at apache_combined.mtail:7:9- 31 apache_http_bytes_total{http_version=\"HTTP/1.1\",request_method=\" GET\",request_status=\"200\",prog=\"apache_combined.mtail\"} 2814654 # apache_http_bytes_total defined at apache_combined.mtail:7:9- 31 apache_http_bytes_total{http_version=\"HTTP/1.1\",request_method=\" GET\",request_status=\"304\",prog=\"apache_combined.mtail\"} 0\n\nWe can see a new set of counters, with one counter for each method and HTTP response code dimension.\n\nWe can also do more complex operations, like building histograms.\n\nVersion: v1.0.0 (427b8e9)\n\n267\n\nChapter 9: Logging as Instrumentation\n\nParsing Rails logs into a histogram\n\nTo see a histogram being created, let’s look at some lines from the example Rails mtail program. Rails request logging is useful for measuring performance, but somewhat unfriendly to parse. Let’s see how mtail does it.\n\nVersion: v1.0.0 (427b8e9)\n\n268\n\nChapter 9: Logging as Instrumentation\n\nListing 9.13: The mtail rails program\n\ncounter rails_requests_started_total counter rails_requests_started by verb counter rails_requests_completed_total counter rails_requests_completed by status counter rails_requests_completed_milliseconds_sum by status counter rails_requests_completed_milliseconds_count by status counter rails_requests_completed_milliseconds_bucket by le, status\n\n/^Started (?P<verb>[A-Z]+) .*/ { rails_requests_started_total++ rails_requests_started[$verb]++\n\n}\n\n/^Completed (?P<status>\\d{3}) .+ in (?P<request_milliseconds>\\d+) ms .*$/ {\n\nrails_requests_completed_total++ rails_requests_completed[$status]++\n\nrails_requests_completed_milliseconds_sum[$status] +=\n\n$request_milliseconds\n\nrails_requests_completed_milliseconds_count[$status]++\n\n# 10ms bucket $request_milliseconds <= 10 {\n\nrails_requests_completed_milliseconds_bucket[\"10\"][$status]++\n\n} # 50ms bucket $request_milliseconds <= 50 {\n\nrails_requests_completed_milliseconds_bucket[\"50\"][$status]++\n\n. . .\n\nOur program opens with defining counters for started and completed requests. We then see a condition and action that increments the request started counters, a\n\nVersion: v1.0.0 (427b8e9)\n\n269\n\nChapter 9: Logging as Instrumentation\n\ntotal and a set of counters with dimensions created from the status of the request.\n\nNext, our program calculates completed requests. Here we’re capturing the status code and the request time in milliseconds. We use these to create a sum of request time and a count of requests, both by status.\n\nWe then nest in another set of conditions and actions, this time to create our histogram. We have a series of conditions testing the length in milliseconds of the request:\n\n$request_milliseconds <= 10\n\nIf our request time is less than or equal to 10, then a histogram bucket counter, with the length test attached as a dimension and also the status, is incremented. We can create counters for each bucket we want.\n\nLet’s run our new program over some Rails logs and see what our resulting metrics look like.\n\nListing 9.14: Rails mtail metric output\n\nrails_requests_started_total{prog=\"rails.mtail\"} 44 rails_requests_started{verb=\"POST\",prog=\"rails.mtail\"} 19 rails_requests_started{verb=\"PUT\",prog=\"rails.mtail\"} 18 rails_requests_started{verb=\"GET\",prog=\"rails.mtail\"} 7 rails_requests_completed_total{prog=\"rails.mtail\"} 217 rails_requests_completed{status=\"200\",prog=\"rails.mtail\"} 217 rails_requests_completed_milliseconds_sum{status=\"200\",prog=\" rails.mtail\"} 3555 rails_requests_completed_milliseconds_count{status=\"200\",prog=\" rails.mtail\"} 217 rails_requests_completed_milliseconds_bucket{le=\"10\",status=\"200 \",prog=\"rails.mtail\"} 93 rails_requests_completed_milliseconds_bucket{le=\"50\",status=\"200 \",prog=\"rails.mtail\"} 217\n\n. . .\n\nVersion: v1.0.0 (427b8e9)\n\n270\n\nChapter 9: Logging as Instrumentation\n\n TIP The le is a common abbreviation for “less than or equal to,” indicating\n\nthe content of the specific bucket.\n\nWe can see that we have counters for each request started by total and verb. We can also see our completed total and a total by status code. And we can see our buckets—in our case just the 10 ms and 50 ms buckets.\n\nDeploying mtail\n\nWe’ve now seen two mtail programs. We deploy them in a number of ways. We recommend running an mtail instance per application, adjacent to the application and deployed via configuration management as a dependency. This pattern is often called a sidecar and lends itself well to containerized applications. We’ll see it in Chapter 13, when we look at monitoring applications running on Kubernetes.\n\nWe can also run multiple programs in a single mtail instance, but this has the caveat that mtail will run every program over every log file passed to it, which could have a performance impact on your host.\n\nScraping our mtail endpoint\n\nNow that we’ve got some metrics being exposed, let’s create a Prometheus job to scrape them.\n\nVersion: v1.0.0 (427b8e9)\n\n271\n\nChapter 9: Logging as Instrumentation\n\nListing 9.15: The mtail job\n\n. . .\n\nscrape_configs: - job_name: 'mtail' file_sd_configs:\n\nfiles:\n\n'targets/mtail/*.json' refresh_interval: 5m\n\nOur job uses file-based service discovery to define a couple of targets, a web server and our rails server. Both targets are scraped on port 3903.\n\nListing 9.16: Worker file discovery\n\n[{\n\n\"targets\": [\n\n\"web:3903\", \"rails:3903\"\n\n]\n\n}]\n\nIf we restart Prometheus we’re now collecting the time series generated from our mtail programs on our Prometheus server, and can make use of these metrics.\n\nSummary\n\nIn this chapter we saw how to use log entries to provide metrics for applications we can’t, or can’t afford to, instrument. We did this using the mtail log processor.\n\nNote that we only scratched the surface of the capabilities of mtail’s language for log parsing and processing. You should read the wiki on GitHub and review the\n\nVersion: v1.0.0 (427b8e9)\n\n272\n\nChapter 9: Logging as Instrumentation\n\nexample programs to learn more about the language and how to write your own mtail programs.\n\nIn the next chapter we’ll learn how to do probe monitoring using Prometheus.\n\nVersion: v1.0.0 (427b8e9)\n\n273",
      "page_number": 268
    },
    {
      "number": 10,
      "title": "Probing",
      "start_page": 286,
      "end_page": 301,
      "detection_method": "regex_chapter",
      "content": "Chapter 10\n\nProbing\n\nIn Chapter 1 we discussed that there are two major approaches to monitoring ap- plications: probing and introspection. In this chapter we’re going to explore probe monitoring. Probe monitoring probes the outside of an application. You query the external characteristics of an application: does it respond to a poll on an open port and return the correct data or response code? An example of probe monitor- ing is performing an ICMP ping or echo check and confirming you have received a response. This type of probing is also called blackbox monitoring because we’re treating the application inside as a black box.\n\nWe’ll use probe monitoring to see the state of external aspects of our applications, which is especially useful from outside of our network. We’re going to use an exporter called the blackbox exporter to conduct this monitoring.\n\nProbing architecture\n\nProbing with Prometheus works by running an exporter, the blackbox exporter, that probes remote targets and exposes any time series collected on a local end- point. A Prometheus job then scrapes any metrics from the endpoint.\n\n274\n\nChapter 10: Probing\n\nFigure 10.1: Probing architecture\n\nMonitoring probes have three constraints:\n\nThey need to be in a position to see the resources being probed. • The position of the probe needs to test the right path to the resources. For ex- ample, if you’re testing external access to an application, running the probe behind your firewall won’t validate this access.\n\nThe position of the probing exporter needs to be able to be scraped by your\n\nPrometheus server.\n\nIt’s common to position probes in geographically distributed locations outside of an organization’s network to ensure maximum coverage for the detection of faults and the collection of data about user experience with the application.\n\nVersion: v1.0.0 (427b8e9)\n\n275\n\nChapter 10: Probing\n\nBecause of the complexity of deploying probes externally, and if a wide distribu- tion of probes is needed, it’s common to outsource these probes to a third-party service. There are all sorts of commercial vendors that provide this service, some of which expose metrics on their platforms and others that allow metrics to be exported for use.\n\nFor our purposes, however, we’re going to deploy the blackbox exporter to an external host and use it to monitor the outside of our applications.\n\nThe blackbox exporter\n\nThe blackbox exporter is a single binary Go application licensed under the Apache 2.0 license. The exporter allows probing of endpoints over HTTP, HTTPS, DNS, TCP, and ICMP. Its architecture is a little different from other exporters. Inside the exporter we define a series of modules that perform specific checks—for example, checking a web server is running, or that a DNS record resolves. When the exporter runs, it exposes these modules and an API on a URL. Prometheus passes targets and specific modules to run on those targets as parameters to that URL. The exporter executes the check and returns the resulting metrics to Prometheus.\n\nLet’s see about installing it.\n\nInstalling the exporter\n\nThe Prometheus.io download page contains zip files with the binaries for specific platforms. Currently, the exporter is supported on:\n\nLinux: 32-bit, 64-bit, and ARM. • Max OS X: 32-bit and 64-bit. • FreeBSD: 32-bit, 64-bit, and ARM. • OpenBSD: 32-bit, 64-bit, and ARM.\n\nVersion: v1.0.0 (427b8e9)\n\n276\n\nChapter 10: Probing\n\nNetBSD: 32-bit, 64-bit, and ARM. • Microsoft Windows: 32-bit and 64-bit. • DragonFly: 64-bit.\n\nOlder versions of the exporter are available from the GitHub Releases page.\n\n NOTE At the time of writing, blackbox exporter was at version 0.12.0.\n\nInstalling the exporter on Linux\n\nTo install blackbox exporter on a 64-bit Linux host, we can download the zipped tarball. We can use wget or curl to get the file from the download site.\n\nListing 10.1: Download the blackbox exporter zip file\n\n$ cd /tmp $ wget https://github.com/prometheus/blackbox_exporter/releases/ download/v0.12.0/blackbox_exporter-0.12.0.linux-amd64.tar.gz\n\nNow let’s unpack the blackbox_exporter binary from the tarball and move it somewhere useful.\n\nListing 10.2: Unpack the blackbox_exporter binary\n\n$ tar -xzf blackbox_exporter-0.12.0.linux-amd64.tar.gz $ sudo cp blackbox_exporter-0.12.0.linux-amd64/blackbox_exporter\n\n/usr/local/bin/\n\nVersion: v1.0.0 (427b8e9)\n\n277\n\nChapter 10: Probing\n\nWe can now test if the exporter is installed and in our path by checking its version.\n\nListing 10.3: Checking the blackbox exporter version on Linux\n\n$ blackbox_exporter --version blackbox_exporter, version 0.12.0 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\n TIP This same approach will work on Mac OS X with the Darwin version of\n\nthe blackbox exporter binary.\n\nInstalling the exporter on Microsoft Windows\n\nTo install blackbox exporter on Microsoft Windows, we need to download the blackbox_exporter.exe executable and put it in a directory. Let’s create a direc- tory for the executable using Powershell.\n\nListing 10.4: Creating a directory on Windows\n\nC:\\> MKDIR blackbox_exporter C:\\> CD blackbox_exporter\n\nNow download the blackbox_exporter.exe executable from GitHub into the C:\\ blackbox_exporter directory:\n\nVersion: v1.0.0 (427b8e9)\n\n278\n\nChapter 10: Probing\n\nListing 10.5: Blackbox exporter Windows download\n\nhttps://github.com/prometheus/blackbox_exporter/releases/ download/v0.12.0/blackbox_exporter-0.12.0.windows-amd64.tar.gz\n\nUnzip the executable, using a tool like 7-Zip, into the C:\\blackbox_exporter di- rectory. Finally, add the C:\\blackbox_exporter directory to the path. This will allow Windows to find the executable. To do this, run this command inside Pow- ershell.\n\nListing 10.6: Setting the Windows path\n\n$env:Path += \";C:\\blackbox_exporter\"\n\nYou should now be able to run the blackbox_exporter.exe executable.\n\nListing 10.7: Checking the blackbox exporter version on Windows\n\nC:\\> blackbox_exporter.exe --version blackbox_exporter, version 0.12.0 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\nInstalling via configuration management\n\nSome of the configuration management modules we saw in Chapter 3 can also install the blackbox exporter:\n\nVersion: v1.0.0 (427b8e9)\n\n279\n\nChapter 10: Probing\n\nA Puppet module for Prometheus. • A Chef cookbook for Prometheus. • A blackbox exporter Docker image. • A SaltStack formula.\n\n TIP Remember, configuration management is the recommended approach\n\nfor installing and managing Prometheus and its components!\n\nConfiguring the exporter\n\nThe exporter is configured via a YAML-based configuration file and driven with command line flags. The command line flags specify the location of the configu- ration file, the port to bind to, and logging. We can see the available command line flags by running the blackbox_exporter binary with the -h flag.\n\nLet’s create a configuration file to run the exporter now.\n\nListing 10.8: The prober.yml file\n\n$ sudo mkdir -p /etc/prober $ sudo touch /etc/prober/prober.yml\n\nAnd let’s populate it with some basic configuration. The exporter uses modules to define various checks. Each module has a name, and inside each is a specific prober—for example, an http prober to check HTTP services and web pages, and an icmp prober to check for ICMP connectivity. Prometheus jobs supply targets to each check inside the module, and the exporter returns metrics that are then scraped.\n\nVersion: v1.0.0 (427b8e9)\n\n280\n\nChapter 10: Probing\n\nListing 10.9: The /etc/prober/prober.yml file\n\nmodules:\n\nhttp_2xx_check: prober: http timeout: 5s http:\n\nvalid_status_codes: [] method: GET\n\nicmp_check:\n\nprober: icmp timeout: 5s icmp:\n\npreferred_ip_protocol: \"ip4\"\n\ndns_examplecom_check:\n\nprober: dns dns:\n\npreferred_ip_protocol: \"ip4\" query_name: \"www.example.com\"\n\nWe’ve defined three checks: an HTTP check that ensures that a web server returns a 2XX status code when queried, an ICMP check that pings the target, and a DNS check that makes a DNS query. Let’s look at each in turn.\n\n TIP The exporter example configuration is also useful to help explain how\n\nthe exporter works.\n\nHTTP check\n\nOur HTTP status check uses the http prober. This prober makes HTTP requests using a variety of methods like GET or POST. We specify a timeout of 5s, or five\n\nVersion: v1.0.0 (427b8e9)\n\n281\n\nChapter 10: Probing\n\nseconds, for any requests. We then configure the prober to make a GET request. We leave the valid_status_codes blank; it defaults to any 2XX status code. If we wanted to validate that a different status code was returned, we’d specify the code or codes in an array in this field.\n\nListing 10.10: Valid status codes\n\nvalid_status_codes: [ '200', '304' ]\n\nHere our check will be for the status codes 200 and 304. We could also check valid HTTP versions, if an HTTP connection is SSL, or if the content matches or does not match a regular expression.\n\nICMP check\n\nOur second check pings a target using ICMP. We set the prober to icmp and specify a timeout of five seconds. We configure the icmp prober with the protocol to use, ip4.\n\n TIP The ICMP requires some additional permissions. On Windows it needs Administrator privileges, on Linux the root or CAP_NET_RAW capability, and on BSD or OS X the root user.\n\nDNS check\n\nOur last check uses the dns prober to check if a DNS entry resolves. In this case our target will be the DNS server we want to make the resolution. We specify our\n\nVersion: v1.0.0 (427b8e9)\n\n282\n\nChapter 10: Probing\n\npreferred protocol, again ip4, and we specify a query.\n\nquery_name: \"www.example.com\"\n\nThis will check that DNS for the www.example.com site will resolve. A query type of ANY is made of the target, and a successful DNS probe relies on the status code returned for the query. The default indicator for success is if the NOERROR response code is received. We can configure for other query types using the query_type option and other response codes using the valid_rcodes option.\n\nStarting the exporter\n\nNow that we have our three checks defined, let’s start the exporter. We’re going to run our exporter on an Ubuntu host called prober.example.com running on an AWS EC2 instance. We run the blackbox_exporter binary and pass it the configuration file we’ve just created.\n\nListing 10.11: Starting the exporter\n\n$ sudo blackbox_exporter --config.file=\"/etc/prober/prober.yml\"\n\nThe exporter runs on port 9115, and you can browse to its console page at http ://localhost:9115.\n\nVersion: v1.0.0 (427b8e9)\n\n283\n\nChapter 10: Probing\n\nFigure 10.2: The blackbox exporter console\n\nThe console includes the exporter’s own metrics, available on the http:// localhost:9115/metrics path, to allow us to monitor it too. It also contains a list of recent checks executed, their status, and debug logs showing what happened. These are useful to debug checks if they have failed.\n\nVersion: v1.0.0 (427b8e9)\n\n284\n\nChapter 10: Probing\n\nCreating the Prometheus job\n\nNow we can create some Prometheus jobs to scrape the exporter. As we’ve dis- cussed, the blackbox exporter is a little different in how it operates: the exporter scrapes the targets passed to it using the checks defined on it. We’ll use a separate job for each check.\n\nLet’s create a job called http_probe that will query our http_2xx_check module.\n\nListing 10.12: The http_probes job\n\njob_name: 'http_probe' metrics_path: /probe params: module: [http_2xx_check]\n\nfile_sd_configs:\n\nfiles:\n\n'targets/probes/http_probes.json' refresh_interval: 5m\n\nrelabel_configs:\n\nsource_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__\n\nreplacement: prober.example.com:9115\n\nWe specify the metrics path, /probe, on the exporter. We pass in the module name as a parameter to the scrape. We’re using file-based discovery to list targets for this job.\n\nVersion: v1.0.0 (427b8e9)\n\n285\n\nChapter 10: Probing\n\nListing 10.13: The http_probe targets\n\n[{\n\n\"targets\": [\n\n\"http://www.example.com\", \"https://www.example.com\", \"\"\n\n]\n\n}]\n\nWe’re going to probe one website, www.example.com, using both HTTP and HTTPS.\n\nSo how does Prometheus know how to find the exporter? We use relabel_configs to overwrite the __address__ label of the target to specify the exporter’s hostname. We do three relabels:\n\n1. Our first relabel creates a parameter by writing the __address__ label, the\n\ncurrent target’s address, into the __param_target label.\n\n2. Our second relabel writes that __param_target label as the instance label.\n\n3. Last, we relabel the __address__ label using the host name (and port) of our\n\nexporter, in our case prober.example.com.\n\nThe relabeling results in a URL being constructed for the scrape:\n\nhttp://prober.example.com:9115/probe?target=www.example.com?module=\n\nhttp_2xx_check\n\nWe can browse to this URL to see the metrics that will be returned. Here are the metrics, minus the comments.\n\nVersion: v1.0.0 (427b8e9)\n\n286\n\nChapter 10: Probing\n\nListing 10.14: The http_2xx_check metrics\n\nprobe_dns_lookup_time_seconds 0.404881857 probe_duration_seconds 0.626351441 probe_failed_due_to_regex 0 probe_http_content_length -1 probe_http_duration_seconds{phase=\"connect\"} 0.013192816999999999 probe_http_duration_seconds{phase=\"processing\"} 0.013948647000000002 probe_http_duration_seconds{phase=\"resolve\"} 0.531245733 probe_http_duration_seconds{phase=\"tls\"} 0.073685882 probe_http_duration_seconds{phase=\"transfer\"} 0.000128069 probe_http_redirects 1 probe_http_ssl 1 probe_http_status_code 200 probe_http_version 1.1 probe_ip_protocol 4 probe_ssl_earliest_cert_expiry 1.527696449e+09 probe_success 1\n\nThe key metric here is probe_http_status_code which shows the status code re- turned by the HTTP request. If this is a 2xx status code, then the probe is consid- ered successful and the probe_success metric will be set to 1. The metrics here also supply useful information like the time of the probe and the HTTP version.\n\nThe other jobs operate in a similar manner to our HTTP check. They use the same relabelling rules to find the right target and the exporter’s address.\n\n NOTE You’ll find the source code for this chapter,\n\nincluding the\n\nPrometheus jobs, on GitHub.\n\nThe ICMP job takes host names or IP addresses, performs an ICMP ping, and re-\n\nVersion: v1.0.0 (427b8e9)\n\n287\n\nChapter 10: Probing\n\nturns the results. The targets for the DNS check are the DNS servers whose reso- lutions you wish to test.\n\nIf we now reload or restart Prometheus, we’ll see the metrics from these jobs in the console.\n\nFigure 10.3: The probe metrics in Prometheus\n\nSummary\n\nIn this chapter we used the blackbox exporter to probe some resources. We were introduced to the basics of probing architecture, and how to install, configure some basic probe checks with, and run the exporter. We also saw how to create Prometheus jobs to initiate probes and how to use relabelling rules to scrape the targets via the exporter.\n\nVersion: v1.0.0 (427b8e9)\n\n288\n\nChapter 10: Probing\n\nIn the next chapter we’ll learn how to push metrics to Prometheus, especially for short-running processes like jobs and deployments, using the Pushgateway.\n\nVersion: v1.0.0 (427b8e9)\n\n289",
      "page_number": 286
    },
    {
      "number": 11,
      "title": "Pushing Metrics and the Pushgateway",
      "start_page": 302,
      "end_page": 322,
      "detection_method": "regex_chapter",
      "content": "Chapter 11\n\nPushing Metrics and the Pushgateway\n\nUp until now, we’ve seen the Prometheus server running jobs to scrape metrics from targets: a pull-based architecture. In some cases, though, there isn’t a target from which to scrape metrics. There are a number of reasons why this might be so:\n\nYou can’t reach the target resources because of security or connectivity. This is quite a common scenario when a service or application only allows ingress to specific ports or paths.\n\nThe target resource has too short a lifespan—for example, a container start- ing, executing, and stopping. In this case, a Prometheus job will run and discover the target has completed execution and is no longer available to be scraped.\n\nThe target resource doesn’t have an endpoint, such as a batch job, that can be scraped. It’s unlikely that a batch job will have a running HTTP service that can be scraped, even assuming the job runs long enough to be available to be scraped.\n\n290\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nIn these cases we need some way to deliver or push our time series to the In this chapter we’re going to learn how to handle these Prometheus server. scenarios using the Pushgateway.\n\nThe Pushgateway\n\nThe Pushgateway is a standalone service that receives Prometheus metrics on an HTTP REST API. The Pushgateway sits between an application sending metrics and the Prometheus server. The Pushgateway receives metrics and is then scraped as a target to deliver the metrics to the Prometheus server. You can think about it like a proxy service, or the opposite of the blackbox exporter’s behavior: it’s receiving metrics rather than probing for them.\n\nVersion: v1.0.0 (427b8e9)\n\n291\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nFigure 11.1: The Pushgateway\n\nLike most of the Prometheus ecosystem, the Pushgateway is written in Go, open source, and licensed under Apache 2.0.\n\nWhen not to use the Pushgateway\n\nThe Pushgateway is essentially a workaround for monitoring resources that can’t be scraped by a Prometheus server for the reasons we discussed above. The gate- way isn’t a perfect solution and should only be used as a limited workaround, especially for monitoring otherwise inaccessible resources.\n\nYou also want to avoid making the gateway a single point of failure or a perfor-\n\nVersion: v1.0.0 (427b8e9)\n\n292\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nmance bottleneck. The Pushgateway definitely does not scale in the same way a Prometheus server will scale.\n\n TIP Although you could use a variant of the worker scaling pattern we intro-\n\nduced in Chapter 7 or push metrics to multiple Pushgateways.\n\nThe gateway is also closer to a proxy than a fully featured push monitoring tool, hence, in using it, you lose a number of useful features that the Prometheus server provides. This includes instance state monitoring via up metrics and the expiration of metrics. It is also a static proxy by default, and remembers every metric sent to it, continuing to expose them as long as it is running (and the metrics aren’t persisted) or until they are deleted. This means that metrics for instances that no longer exist may be persisted in the gateway.\n\nlike You should focus the gateway on monitoring short-lifespan resources, jobs, or the short-term monitoring of inaccessible resources. You should install Prometheus servers to monitor inaccessible resources in the longer term.\n\n TIP A useful tool to monitor some of these inaccessible resources is the\n\nPushProx proxy which is designed to allow scrapes through NAT’ed connections.\n\nInstalling the Pushgateway\n\nThe Prometheus.io download page contains zip files with the binaries for specific platforms. Currently Pushgateway is supported on:\n\nLinux: 32-bit, 64-bit, and ARM.\n\nVersion: v1.0.0 (427b8e9)\n\n293\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nMax OS X: 32-bit and 64-bit. • FreeBSD: 32-bit, 64-bit, and ARM. • OpenBSD: 32-bit, 64-bit, and ARM. • NetBSD: 32-bit, 64-bit, and ARM. • Microsoft Windows: 32-bit and 64-bit. • DragonFly: 64-bit.\n\nOlder versions of Pushgateway are available from the GitHub Releases page.\n\n NOTE At the time of writing Pushgateway was at version 0.5.1.\n\nInstalling the Pushgateway on Linux\n\nTo install Pushgateway on a 64-bit Linux host, we can download the zipped tarball. We can use wget or curl to get the file from the download site.\n\nListing 11.1: Download the Pushgateway zip file\n\n$ cd /tmp $ wget https://github.com/prometheus/pushgateway/releases/download/v 0.5.1/pushgateway-0.5.1.linux-amd64.tar.gz\n\nNow let’s unpack the pushgateway binary from the tarball and move it somewhere useful.\n\nVersion: v1.0.0 (427b8e9)\n\n294\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.2: Unpack the pushgateway binary\n\n$ tar -xzf pushgateway-0.5.1.linux-amd64.tar.gz $ sudo cp pushgateway-0.5.1.linux-amd64/pushgateway /usr/local/ bin/\n\nWe can now test if the Pushgateway is installed and in our path by checking its version.\n\nListing 11.3: Checking the Pushgateway version on Linux\n\n$ pushgateway --version pushgateway, version 0.5.1 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\n TIP This same approach will work on Mac OS X with the Darwin version of\n\nthe Pushgateway binary.\n\nInstalling the Pushgateway on Microsoft Windows\n\nTo install Pushgateway on Microsoft Windows, we need to download the pushgateway.exe executable and put it in a directory. Let’s create a directory for the executable using Powershell.\n\nVersion: v1.0.0 (427b8e9)\n\n295\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.4: Creating a directory on Windows\n\nC:\\> MKDIR pushgateway C:\\> CD pushgateway\n\nNow download the pushgateway.exe executable from GitHub into the C:\\ pushgateway directory:\n\nListing 11.5: Pushgateway Windows download\n\nhttps://github.com/prometheus/pushgateway/releases/download/v 0.5.1/pushgateway-0.5.1.windows-amd64.tar.gz\n\nUnzip the executable, using a tool like 7-Zip, into the C:\\pushgateway directory. Finally, add the C:\\pushgateway directory to the path. This will allow Windows to find the executable. To do this, run this command inside Powershell.\n\nListing 11.6: Setting the Windows path\n\n$env:Path += \";C:\\pushgateway\"\n\nYou should now be able to run the pushgateway.exe executable.\n\nVersion: v1.0.0 (427b8e9)\n\n296\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.7: Checking the Pushgateway version on Windows\n\nC:\\> pushgateway.exe --version pushgateway, version 0.5.1 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\nInstalling via configuration management\n\nSome of the configuration management modules we saw in Chapter 3 can also install the Pushgateway:\n\nA Puppet module for Prometheus. • A Chef cookbook for Prometheus.\n\n TIP Remember configuration management is the recommended approach for\n\ninstalling and managing Prometheus and its components!\n\nConfiguring and running the Pushgateway\n\nThe Pushgateway doesn’t need any configuration out of the box, but it can be configured by setting flags on the command line when you run the pushgateway binary. The gateway runs on port 9091, but you can override this port and any interface using the --web.listen-address flag.\n\nVersion: v1.0.0 (427b8e9)\n\n297\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.8: Running the Pushgateway on an interface\n\n$ pushgateway --web.listen-address=\"0.0.0.0:9091\"\n\nThis will bind the Pushgateway on all interfaces. When the gateway is running, you can browse to its dashboard on that address and port.\n\nFigure 11.2: The Pushgateway dashboard\n\nBy default, the gateway stores all of its metrics in memory. This means if the gateway stops or is restarted you’ll lose any metrics in memory. You can persist the metrics to disk by specifying the --persistence.file flag with a path a file.\n\nVersion: v1.0.0 (427b8e9)\n\n298\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.9: Persisting the metrics\n\n$ pushgateway --persistence.file=\"/tmp/pushgateway_persist\"\n\nThe persistence file is written to every five minutes by default, but you can over- ride this with the --persistence.interval flag.\n\nYou can see other available flags by running the binary with the --help flag.\n\nSending metrics to the Pushgateway\n\nOnce the Pushgateway is running you can start to send it metrics. Most Prometheus client libraries support sending metrics to the Pushgateway, in addition to exposing them for scraping. The easiest way to see how the gateway works is to use a command line tool like curl to post metrics. Let’s push a single metric to our running gateway.\n\nListing 11.10: Posting a metric to the gateway\n\n$ echo 'batchjob1_user_counter 2' | curl --data-binary @- http:// localhost:9091/metrics/job/batchjob1\n\nWe push metrics to the path /metrics. The URL is constructed using labels, here /metrics/job/<jobname> where batchjob1 is our job label. A full metrics path with labels looks like:\n\nListing 11.11: The Pushgateway metrics path\n\n/metrics/job/<jobname>{/<label>/<label>}\n\nVersion: v1.0.0 (427b8e9)\n\n299\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nThe <jobname> will be used as the value of the job label, followed by any other specified labels. Labels specified in the path will override any labels specified in the metric itself.\n\nLet’s add an instance label to our metric using the URL path.\n\nListing 11.12: Posting a metric to the gateway\n\n$ echo 'batchjob1_user_counter 2' | curl --data-binary @- http:// localhost:9091/metrics/job/batchjob1/instance/sidekiq_server\n\n WARNING You cannot use / as part of a label value or job name, even\n\nif it is escaped. This is because the decoding sequences makes it impossible to determine what was escaped, see the Go URL documentation.\n\nIn the above example, we’ve echoed the metric batchjob1_user_counter 2 from the job batchjob1 to our gateway. This will create a new metric grouping for the job batchjob1 with an instance label of sidekiq_server. Metric groupings are collections of metrics. You can add and delete metrics within the grouping, or even delete the whole group. Because the gateway is a cache and not an ag- gregator, metric groupings will live on until the gateway is stopped or they are deleted.\n\nThis counter is the most simple metric we can send. We name the counter, batchjob1_user_counter, and give it a value: 2.\n\nWe can add labels to pushed metrics by enclosing them in {}.\n\nVersion: v1.0.0 (427b8e9)\n\n300\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.13: Adding labels to pushed metrics\n\necho 'batchjob1_user_counter{job_id=\"123ABC\"} 2' | curl --data- binary @- http://localhost:9091/metrics/job/batchjob1/instance/ sidekiq_server\n\nCurrently, the metric will be uploaded untyped; the gateway won’t know whether this is a counter, gauge, or any other metric type. You can add a type (and a description) to a metric by passing TYPE and HELP statements in the push.\n\nListing 11.14: Passing types and descriptions\n\n$ cat <<EOF | curl --data-binary @- http://localhost:9091/ metrics/job/batchjob1/instance/sidekiq_server # TYPE batchjob1_user_counter counter # HELP batchjob1_user_counter A metric from BatchJob1. batchjob1_user_counter{job_id=\"123ABC\"} 2 EOF\n\nWe can also add further metrics to our metric group.\n\nListing 11.15: Passing types and descriptions\n\n$ cat <<EOF | curl --data-binary @- http://localhost:9091/ metrics/job/batchjob1/instance/sidekiq_server # TYPE batchjob1_avg_latency gauge # HELP batchjob1_avg_latency Another metric from BatchJob1. batchjob1_avg_latency{job_id=\"123ABC\"} 74.5 # TYPE batchjob1_sales_counter counter # HELP batchjob1_sales_counter A third metric from BatchJob1. batchjob1_sales_counter{job_id=\"123ABC\"} 1 EOF\n\nVersion: v1.0.0 (427b8e9)\n\n301\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nThis would add two metrics, batchjob1_avg_latency and batchjob1_sales_counter , to our batchjob1 metric group.\n\nViewing metrics on the Pushgateway\n\nWe can then see the metrics we’ve pushed to the gateway by using curl on the / metrics path (or by browsing to the Pushgateway dashboard at http://localhost :9091).\n\nVersion: v1.0.0 (427b8e9)\n\n302\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.16: Curling the gateway metrics\n\n$ curl http://localhost:9091/metrics # HELP batchjob1_user_counter A metric from BatchJob1. # TYPE batchjob1_user_counter counter batchjob1_{instance=\"sidekiq_server\",job=\"batchjob1\",job_id=\"123 ABC\"} 2 # HELP batchjob1_avg_latency Another metric from BatchJob1. # TYPE batchjob1_avg_latency gauge batchjob1_avg_latency{instance=\"sidekiq_server\",job=\"batchjob1\", job_id=\"123ABC\"} 74.5 # HELP batchjob1_sales_counter A third metric from BatchJob1. # TYPE batchjob1_sales_counter counter batchjob1_sales_counter{instance=\"sidekiq_server\",job=\"batchjob1 \",job_id=\"123ABC\"} 1\n\n. . .\n\n# HELP push_time_seconds Last Unix time when this group was changed in the Pushgateway. # TYPE push_time_seconds gauge push_time_seconds{instance=\"sidekiq_server\",job=\"batchjob1\"} 1.523303909484092e+09 # HELP pushgateway_build_info A metric with a constant '1' value labeled by version, revision, branch, and goversion from which\n\npushgateway was built. # TYPE pushgateway_build_info gauge pushgateway_build_info{branch=\"master\",goversion=\"go1.10.1\", revision=\"d07ed465fcfcf2be4a2d80026d057fb5944c9283\",version=\" 0.4.0\"} 1\n\n NOTE We’ve skipped a number of health and performance metrics from\n\nthe gateway in our output.\n\nWe can see our batchjob1 metrics. We can see the job label has been set to\n\nVersion: v1.0.0 (427b8e9)\n\n303\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nbatchjob1, and we have an instance label of sidekiq_server.\n\nFor batchjob1_user_counter, we can see that the value of the metric is 2 even though we sent three pushes to the gateway. This is because the gateway is NOT an aggregator, like StatsD or similar tools. Instead, the last push of the metric is exported until it is updated or deleted.\n\nYou’ll also see another metric here: push_time_seconds. This is a per-job metric that indicates the last time a push occurred. You can use this metric to determine when the last push occurred, and to potentially identify missing pushes. It’s only useful if you expect a push to occur within a specific time frame.\n\nDeleting metrics in the Pushgateway\n\nMetrics exist in the gateway until it is restarted (assuming no persistence is set), or until they are deleted. We can delete metrics using the Pushgateway API. Let’s do this now, again using curl, as an example.\n\nListing 11.17: Deleting Pushgateway metrics\n\n$ curl -X DELETE localhost:9091/metrics/job/batchjob1\n\nThis will delete all metrics for the job batchjob1. You can further limit the se- lection by making the path more granular—for example, by deleting only those metrics from a specific instance.\n\nListing 11.18: Deleting a selection of Pushgateway metrics\n\n$ curl -X DELETE localhost:9091/metrics/job/batchjob1/instance/ sidekiq_server\n\nVersion: v1.0.0 (427b8e9)\n\n304\n\nChapter 11: Pushing Metrics and the Pushgateway\n\n TIP You can also delete an entire metrics grouping from the Pushgateway by using the Delete Group button on the dashboard.\n\nSending metrics from a client\n\nObviously curl’ing metrics to the gateway isn’t practical. Instead we’re going to use a Prometheus client to push metrics to the gateway. All of the official Prometheus clients, and many of the unofficial ones, support the push gateway as a target for metrics.\n\nTo demonstrate how to do this, we’ll use the Rails application we demonstrated in Chapter 8. We’re going to create a MetricsPush class in our lib directory to use in our Rails application. Let’s do that now.\n\nListing 11.19: Creating MetricsPush class\n\n$ touch lib/metricspush.rb\n\nAnd let’s populate the file with a class.\n\nVersion: v1.0.0 (427b8e9)\n\n305\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.20: The MetricsPush module\n\nrequire 'prometheus/client' require 'prometheus/client/push'\n\nclass MetricsPush\n\nattr_reader :job, :registry, :pushgateway_url\n\ndef initialize\n\n@job = 'mwp-rails' @pushgateway_url = 'http://localhost:9091'\n\nend\n\ndef registry\n\n@registry ||= Prometheus::Client.registry\n\nend\n\ndef counter(name, desc, labels = {})\n\nregistry.get(name) || registry.counter(name, desc)\n\nend\n\ndef gauge(name, desc, labels = {})\n\nregistry.get(name) || registry.counter(name, desc)\n\nend\n\ndef summary(name, desc, labels = {})\n\nregistry.get(name) || registry.counter(name, desc)\n\nend\n\ndef histogram(name, desc, labels = {}, buckets = Prometheus::\n\nClient::Histogram::DEFAULT_BUCKETS)\n\nregistry.get(name) || registry.counter(name, desc)\n\nend\n\ndef push\n\nPrometheus::Client::Push.new(job, nil, pushgateway_url).add(\n\nregistry)\n\nend\n\nend\n\nVersion: v1.0.0 (427b8e9)\n\n306\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nOur class is very similar to the code we used in Chapter 8. We can create a wide variety of metrics. We’ve also added a method called push that sends the metrics in the registry to a Pushgateway. In our case, we’ve assumed the gateway is running locally on the host.\n\n\\DNnote{In addition to the add method on Prometheus::Client::Push, we also replace and delete methods we could use to replace or delete a have metric on the gateway.\n\nWe can then use our class when we run a job or some other transitory task.\n\nListing 11.21: Pushing a metric\n\nmp = MetricsPush.new mp.counter(:test_counter, \"A test counter for a job\").increment({\n\nservice: 'mwp-rails-job' })\n\nmp.push\n\nWe create an instance of the MetricsPush class and increment a counter called test_counter. We’ve added a label—one called service with a value of mwp- rails-job. We then use the push method to push the metric. If we were to run this snippet of code we could then check for the metric in the gateway, on the http://localhost:9091/metrics path or in the Pushgateway console.\n\nFigure 11.3: The test_counter in the Pushgateway dashboard\n\nVersion: v1.0.0 (427b8e9)\n\n307\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nHere we can see our pushed metric. Its instance label has been automatically populated with the IP address of our Rails server. We can override this during the push if required (especially as a lot of short-lived jobs are more likely associated with a service than a specific host). We see a job label and the service label we added.\n\nNow we’re ready to have the Prometheus server scrape the gateway to acquire our metrics.\n\n## Scraping the Pushgateway\n\nThe Pushgateway is only the interim stop for our metrics. We now need to get them into the Prometheus server. For that we’re going to need a job. Let’s create one now.\n\nListing 11.22: The pushgateway job\n\njob_name: pushgateway honor_labels: true file_sd_configs:\n\nfiles:\n\ntargets/pushgateway/*.json refresh_interval: 5m\n\nWe can see our job is pretty typical and follows the pattern we’ve seen throughout the book, using file-based discovery. The job will load all targets specified in JSON files in the targets/pushgateway directory. We’ve specified a Pushgateway named pg1.example.com in our file-based service discovery configuration.\n\nVersion: v1.0.0 (427b8e9)\n\n308\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.23: Our Pushgateway\n\n[{\n\n\"targets\": [\"pg1.example.com\"]\n\n}]\n\nWe’ve then specified the honor_labels option and set it to true. As we’ve learned, when Prometheus scrapes a target, it will attach the name of the job that did the scraping, here pushgateway, and an instance label populated with the host or IP address of the target. With the Pushgateway, our metrics already have job and instance labels that indicate where our metrics were pushed from. We want to perpetuate this information in Prometheus rather than have it rewritten by the server when it scrapes the gateway.\n\nIf honor_labels is set to true, Prometheus will use the job and instance labels on the Pushgateway. Set to false, it’ll rename those values, prefixing them with \\texttt{exported_} and attaching new values for those labels on the server.\n\nIf we restart Prometheus it’ll start scraping the gateway. Let’s now look for our test_counter metric.\n\nFigure 11.4: The test_counter metric\n\nWe can see it’s been scraped, and the Prometheus server has honored the local labels.\n\nVersion: v1.0.0 (427b8e9)\n\n309\n\nChapter 11: Pushing Metrics and the Pushgateway\n\nSummary\n\nIn this chapter we saw how to use “push” mechanics with Prometheus via the Pushgateway. We articulated the limited circumstances in which it’s an appropri- ate use case. In those circumstances, we showed you how to install and configure the gateway and instrument your applications and jobs to push metrics to the gateway. And finally, we saw how to use a Prometheus job to scrape the gateway and acquire your pushed metrics.\n\nIn the next two chapters we’ll look at monitoring a whole application stack run- ning on top of Kubernetes, first looking at Kubernetes, and then a multi-service application.\n\nVersion: v1.0.0 (427b8e9)\n\n310",
      "page_number": 302
    },
    {
      "number": 12,
      "title": "Monitoring a Stack - Kubernetes",
      "start_page": 323,
      "end_page": 350,
      "detection_method": "regex_chapter",
      "content": "Chapter 12\n\nMonitoring a Stack - Kubernetes\n\nNow that we’ve got a handle on the building blocks of Prometheus, let’s put the pieces together to monitor a modern application stack in the real world. To do this, we’re going to monitor an API service application called Tornado. Tornado is written in Clojure, and runs on the JVM; it has a Redis data store and a MySQL database. We’re going to deploy Tornado into a Kubernetes cluster we’ve built, so we’ll also look at monitoring Kubernetes with Prometheus.\n\nIn this chapter, we’re going to examine the Kubernetes portion of our stack and how to monitor it.\n\nTo make our monitoring simpler we’ve deployed Prometheus onto Kubernetes, too.\n\nOur Kubernetes cluster\n\nOur Kubernetes cluster is named tornado.quicknuke.com. The cluster is running Kubernetes 1.8.7 and is running in AWS. We built the cluster with kops, and you can find the cluster configuration here. It has three masters and six worker nodes. All nodes are divided between three Availability Zones.\n\n311\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nThe cluster was created with the following kops command.\n\nListing 12.1: The cluster kops command\n\n$ kops create cluster \\ --node-count 6 \\ --zones us-east-2a,us-east-2b,us-east-2c \\ --master-zones us-east-2a,us-east-2b,us-east-2c \\ --node-size t2.micro \\ --master-size t2.micro \\ --topology private \\ --networking kopeio-vxlan \\ --api-loadbalancer-type=public \\ --bastion \\ tornado.quicknuke.com\n\n NOTE This chapter assumes you’ve already installed Kubernetes and have\n\nsome understanding of how it works. If you need more information on Kubernetes, the book Kubernetes: Up and Running is recommended reading.\n\nRunning Prometheus on Kubernetes\n\nThere are a variety of ways to deploy Prometheus on Kubernetes. The best way for you likely depends greatly on your environment. As possibilities, you can build your own deployments and expose Prometheus via a service, use one of a number of bundled configurations, or use the Prometheus Operator from CoreOS.\n\nWe’ve chosen to manually create a deployment and a service. We configure the Prometheus server and manage rules using ConfigMaps and mount these as vol- umes in our deployment. We also expose the Prometheus WebUI via an AWS Load\n\nVersion: v1.0.0 (427b8e9)\n\n312\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nBalancer service.\n\nWe’ve also installed a cluster of three Alertmanagers running on the cluster. We’ve enclosed all of this in a namespace called monitoring.\n\nYou can find the configuration for all of this with the book’s code on GitHub. How- ever, we’re not going to go into huge detail about how we deployed Prometheus onto Kubernetes; instead we’ll focus on monitoring Kubernetes and applications running on Kubernetes with Prometheus.\n\n NOTE This decision also reflects the speed at which Kubernetes is evolving:\n\nany deployment documented here is likely to be dated very quickly. This config- uration provided here is not guaranteed to work for later Kubernetes releases.\n\nMonitoring Kubernetes\n\nLet’s start by talking about monitoring Kubernetes itself. While it’s likely to change, it’s more manageable as a topic. Kubernetes is a container orchestrator and scheduler with a lot of moving pieces. We’re going to show you how to mon- itor aspects of Kubernetes with Prometheus jobs, and we’ll match each of these jobs with some recording and alert rules.\n\nThis chapter will be broken into sections dealing with each piece, how the time se- ries are collected, and any rules and alerts we’re going to generate from those time series. We’re not going to provide the definitive monitoring approach, but instead touch on some key highlights, especially where they expand on a Prometheus con- cept worth exploring.\n\nTo identify what we need to monitor we’ll also make use of Prometheus’s built-in service discovery mechanism for Kubernetes.\n\nVersion: v1.0.0 (427b8e9)\n\n313\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nLet’s start with monitoring the nodes upon which Kubernetes is running.\n\nMonitoring our Kubernetes nodes\n\nOur Kubernetes cluster is made up of nine AWS EC2 instances. To monitor them we’re going to use the Node Exporter. There are several ways we can deploy the Node Exporter onto those instances. We can install the Node Exporter onto the base instances when they are provisioned, much as we did in Chapter 4. Or we can install the Node Exporter into a Kubernetes pod on each node. We can take advantage of Kubernetes DaemonSet controller that automatically deploys a pod on every node in the cluster. This approach is useful when you don’t control the base instances—for example, if you’re using a hosted Kubernetes solution.\n\n WARNING There is a major caveat with this approach. The Node Ex- porter accesses a lot of root-level resources, and running it in a Docker container requires mounting those resources into the container and, for the systemd collec- tor, running the container as root. This poses a potential security risk. If that risk isn’t acceptable to you then you should install Node Exporter directly onto the instances.\n\nNode Exporter DaemonSet\n\nA DaemonSet ensures that a pod runs on all nodes, potentially including the mas- ters, using a toleration. It’s ideal for items like monitoring or logging agents. Let’s look at some elements of our DaemonSet.\n\nVersion: v1.0.0 (427b8e9)\n\n314\n\nChapter 12: Monitoring a Stack - Kubernetes\n\n NOTE You can find the full configuration for the Node Exporter on GitHub.\n\nListing 12.2: The Node Exporter DaemonSet tolerations\n\napiVersion: extensions/v1beta1 kind: DaemonSet metadata:\n\nname: node-exporter namespace: monitoring\n\n. . .\n\nspec:\n\ntolerations: - key: node-role.kubernetes.io/master\n\neffect: NoSchedule\n\nhostNetwork: true hostPID: true hostIPC: true securityContext: runAsUser: 0\n\nFirst, you can see we’ve specified a DaemonSet with a name, node-exporter, and that we’re using a toleration to ensure this pod is also scheduled on our Kubernetes masters, not just our normal nodes.\n\nNow here’s the caveat with this approach. We’re running the pod as user 0 or root (this allows access to systemd). We’ve also enabled hostNetwork, hostPID, and hostIPC to specify that the network, process, and IPC namespace of the instance will be available in the container. This is a potential security exposure, and you must definitely consider if you want to take this risk. If this risk isn’t acceptable, baking the Node Exporter into the image of your instances is potentially a better approach.\n\nVersion: v1.0.0 (427b8e9)\n\n315\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nLet’s look at the containers in the pod.\n\nListing 12.3: The Node Exporter DaemonSet containers\n\ncontainers: - image: prom/node-exporter:latest\n\nname: node-exporter volumeMounts:\n\nmountPath: /run/systemd/private name: systemd-socket readOnly: true\n\nargs:\n\n\"--collector.systemd\" - \"--collector.systemd.unit-whitelist=(docker|ssh|\n\nrsyslog|kubelet).service\"\n\nports:\n\ncontainerPort: 9100 hostPort: 9100 name: scrape\n\nHere we’re using the DockerHub image for Node Exporter, prom/node_exporter, and grabbing the latest release. We’re also mounting in a volume for the /run/ systemd/private directory on the instances themselves. This allows the Node Ex- porter to access the systemd state and gather the service state of systemd-managed services on the instance.\n\nWe’ve also specified some arguments for the node_exporter binary. We saw both in Chapter 4: enabling the systemd collector, and specifying a regular expression of the specific services to monitor, rather than all the services on the host.\n\nWe’ve also specified the port we want our metrics exposed on, 9100: the default port.\n\nTo help keep the Node Exporter pods healthy and to enhance their uptime, we’ve also added liveness and readiness probes to our Node Exporter container. Liveness probes detect the status of applications inside containers.\n\nVersion: v1.0.0 (427b8e9)\n\n316\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.4: Node Exporter liveness and readiness probes\n\nlivenessProbe:\n\nhttpGet:\n\npath: /metrics port: 9100\n\ninitialDelaySeconds: 30 timeoutSeconds: 10 periodSeconds: 1\n\nreadinessProbe:\n\nfailureThreshold: 5 httpGet:\n\npath: /metrics port: 9100\n\ninitialDelaySeconds: 10 timeoutSeconds: 10 periodSeconds: 2\n\nIn our case we use an HTTP GET probe to the /metrics path on port 9100 to confirm the Node Exporter is still working. The probe runs every periodSeconds , one second in our case. If the liveness check fails, Kubernetes will restart the container.\n\n NOTE We’ll see these probes in applications we monitor too. They can\n\nassist in managing the health of your applications by reducing possible false positives—such as a service triggering an alert by not being ready while it is starting—while monitoring. These checks can also restart containers that are faulty, potentially fixing issues before they trigger alerts.\n\nReadiness probes confirm the application is functional. Here, that means an HTTP GET can connect to the /metrics path on port 9100 before marking the container\n\nVersion: v1.0.0 (427b8e9)\n\n317\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nas available and delivering traffic to it. The remaining settings control the probe’s behavior: it’ll wait 10 seconds, the initialDelaySeconds setting, before checking the readiness; thereafter it will check every two seconds, the periodSeconds value, for readiness. If the probe times out after 10 seconds, the timeoutSeconds, more than five times, garnered from the failureThreshold setting, then the container will be marked as Unready.\n\n NOTE You can find the full configuration for the Node Exporter on GitHub.\n\nNode Exporter service\n\nWe also need a service to expose the Node Exporter so it can be scraped.\n\nVersion: v1.0.0 (427b8e9)\n\n318\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.5: The Node Exporter service\n\napiVersion: v1 kind: Service metadata:\n\nannotations:\n\nprometheus.io/scrape: 'true'\n\nlabels:\n\napp: node-exporter name: node-exporter\n\nname: node-exporter namespace: monitoring\n\nspec:\n\nclusterIP: None ports: - name: scrape port: 9100 protocol: TCP\n\nselector:\n\napp: node-exporter\n\ntype: ClusterIP\n\nOur service is relatively straightforward. We add an annotation, prometheus.io /scrape: 'true', as metadata on the services. This will tell Prometheus that it should scrape this service. We’ll see how it’s used in the Prometheus job we’ll create to scrape our Node Exporters.\n\nWe also expose port 9100 as a ClusterIP. This means it is only available to the internal cluster network. As Prometheus is on the local Kubernetes cluster it’ll be able to internally scrape the Node Exporter, and there’s no need to expose it externally.\n\n NOTE You can find the complete Node Exporter service on GitHub.\n\nVersion: v1.0.0 (427b8e9)\n\n319\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nDeploying the Node Exporter\n\nLet’s create our DaemonSet and service on our Kubernetes cluster using the kubectl command. We’ll create both inside the monitoring namespace.\n\nListing 12.6: Deploying the Node Exporter daemonset and service\n\n$ kubectl create -f ./node-exporter.yml -n monitoring daemonset \"node-exporter\" created service \"node-exporter\" created\n\nIf you don’t want to keep specifying the -n monitoring namespace you can specify a default using.\n\nListing 12.7: The default namespace\n\n$ kubectl config set-context $(kubectl config current-context) -- namespace=monitoring\n\nWe can now check our pods are running.\n\nVersion: v1.0.0 (427b8e9)\n\n320\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.8: Checking the Node Exporter pods\n\n$ kubectl get pods -n monitoring READY STATUS NAME 1/1 alertmanager-6854b5d59b-jvjcw 1/1 node-exporter-4fx57 1/1 node-exporter-4nzfk 1/1 node-exporter-5n7kl 1/1 node-exporter-f2mvb 1/1 node-exporter-km7sc 1/1 node-exporter-lvrsq 1/1 node-exporter-mvstg 1/1 node-exporter-tj4cs node-exporter-wh56c 1/1 prometheus-core-785bc8584b-7vfr4 1/1\n\nRunning Running Running Running Running Running Running Running Running Running Running\n\nRESTARTS AGE 0 0 0 0 0 0 0 0 0 0 0\n\n7d 5s 5s 5s 5s 5s 5s 5s 5s 5s 8d\n\nWe can see nine pods, one for each instance in the cluster: three masters and six nodes. We can also see our Prometheus server pod, prometheus-core, and our Alertmanager, alertmanager.\n\nWe can check the Node Exporter pods are running correctly by grabbing their logs.\n\nListing 12.9: A Node Exporter pod’s logs\n\n$ kubectl logs node-exporter-4fx57 -n monitoring time=\"2018-01-18T22:46:05Z\" level=info msg=\"Starting node_exporter (version=0.15.2, branch=HEAD, revision=98 bc64930d34878b84a0f87dfe6e1a6da61e532d)\" source=\"node_exporter. go:43\" time=\"2018-01-18T22:46:05Z\" level=info msg=\"Build context (go= go1.9.2, user=root@d5c4792c921f, date=20171205-14:50:53)\" source =\"node_exporter.go:44\"\n\n. . .\n\nWe can see our Node Exporter daemon is running. We can also confirm our service\n\nVersion: v1.0.0 (427b8e9)\n\n321\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nis in place.\n\nListing 12.10: Checking the Node Exporter service\n\n$ kubectl get services -n monitoring NAME node-exporter ClusterIP None\n\nTYPE\n\nCLUSTER-IP EXTERNAL-IP PORT(S)\n\nAGE\n\n<none>\n\n9100/TCP 8s\n\nHere we can see our node-exporter service with a ClusterIP type and with the 9100 port exposed to the internal Kubernetes cluster, ready to be scraped. We’re not scraping it yet, however, because we haven’t added a Prometheus job.\n\nThe Node Exporter job\n\nIn our Prometheus configuration we now want to add a job to scrape our Node Exporter endpoints. We’re going to kill many birds with one stone by defining a job that scrapes all the service endpoints that Kubernetes exposes. We’re going to control which endpoints Prometheus actually scrapes by only scraping those with a specific annotation, prometheus.io/scrape, set to 'true'. We’ll also use the built-in Kubernetes service discovery to find our endpoints and return them as potential targets to Prometheus.\n\n NOTE All of these jobs are derived or based on the amazing example\n\nKubernetes jobs shipped with Prometheus. Thanks to the contributors to that project for developing them.\n\nLet’s look at that job now.\n\nVersion: v1.0.0 (427b8e9)\n\n322\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.11: The Kubernetes service endpoints job\n\njob_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [\n\n__meta_kubernetes_service_annotation_prometheus_io_scrape]\n\naction: keep regex: true\n\nsource_labels: [\n\n__meta_kubernetes_service_annotation_prometheus_io_scheme]\n\naction: replace target_label: __scheme__ regex: (https?) - source_labels: [\n\n__meta_kubernetes_service_annotation_prometheus_io_path]\n\naction: replace target_label: __metrics_path__ regex: (.+)\n\nsource_labels: [__address__,\n\n__meta_kubernetes_service_annotation_prometheus_io_port]\n\naction: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2\n\naction: labelmap\n\nregex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace]\n\naction: replace target_label: kubernetes_namespace\n\nsource_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name\n\nWe’ve called the job kubernetes-service-endpoints. We’ve specified service dis- covery using the kubernetes_sd_discovery mechanism. This is in inbuilt service discovery mechanism, specifically for Kubernetes. It queries the Kubernetes API\n\nVersion: v1.0.0 (427b8e9)\n\n323\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nfor targets that match specific search criteria.\n\nAs our Prometheus server is running inside Kubernetes we’re able to automatically, with minimal configuration, fetch Kubernetes targets that match specific roles. There are roles for nodes, pods, services, and ingresses. Here, specified by the role parameter, we’re asking our service discovery to return all the Kubernetes endpoints. The endpoints role returns targets for all listed endpoints of a service, with one target per port for each endpoint address. If the endpoint is backed by a pod, as our Node Exporter service is, then any additional container ports are also discovered as targets. In our case, we’ve only exposed port 9100.\n\nService discovery also populates a variety of metadata. We use this metadata to relabel and identify each endpoint. Let’s see what our relabelling rules do and explore that metadata.\n\nOur first rule checks the prometheus.io/scrape: 'true' annotation that we set in our Node Exporter service. During the service discovery process the prometheus.io/scrape annotation will be translated to prometheus_io_scrape to create a valid label name. This is because the dot and slash are not le- gal characters in a Prometheus metric label. Since this is an annotation on the Prometheus service process also adds the prefix a Kubernetes service, __meta_kubernetes_service_annotation_ to the label.\n\nOur job only keeps any targets that have the metadata label: __meta_kubernetes_service_annotation_prometheus_io_scrape set to true. All other targets are dropped. This lets you only scrape those endpoints that you want.\n\nThe next three rules check for the presence of more annotations: prometheus. io/scheme, prometheus.io/path, and prometheus.io/port. If these labels are present it’ll use the contents of these annotations as the scheme, path, and port to be scraped. This lets us control, from the service endpoint, what precisely to scrape, allowing our job to be flexible.\n\nOur next rule maps any labels on the service into Prometheus labels of the same name by using the labelmap action. In our case, this consumes the\n\nVersion: v1.0.0 (427b8e9)\n\n324\n\nChapter 12: Monitoring a Stack - Kubernetes\n\n__meta_kubernetes_service_label_app metadata label, which will become a label simply called app. Our next rule copies the __meta_kubernetes_namespace label as kubernetes_namespace and the __meta_kubernetes_service_name metadata label to kubernetes_name.\n\nWe now add our job to the ConfigMap we’re using for our Prometheus server configuration. We then replace our existing configuration.\n\nListing 12.12: Replacing the ConfigMap\n\n$ kubectl replace -f ./prom-config-map-v1.yml -n monitoring\n\nWe generally have to delete our Prometheus pod and allow it to be recreated in order to load our new configuration. Shortly, we should see some new targets on the Prometheus expression browser.\n\nVersion: v1.0.0 (427b8e9)\n\n325\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nFigure 12.1: The Kubernetes endpoint targets\n\nYou can see that we’ve got thirteen targets listed. Nine of them are the Node Exporter endpoints on our instances. The tenth and eleventh are Prometheus and Alertmanager. The Prometheus and Alertmanager targets have been discovered automatically because their interfaces are exposed as a service too.\n\nListing 12.13: The monitoring services\n\n$ kubectl get services -n monitoring NAME S) alertmanager 9093:32288/TCP 27m node-exporter ClusterIP TCP 15h prometheus 9090:30604/TCP 4d\n\nTYPE\n\nCLUSTER-IP\n\nEXTERNAL-IP\n\nAGE\n\nLoadBalancer 100.68.82.44\n\na6f953a641191...\n\nNone\n\n<none>\n\nLoadBalancer 100.68.154.121 a953a66970c13...\n\nPORT(\n\n9100/\n\nThis job is really useful because we only need to define it once and all future Ku- bernetes service endpoints will be automatically discovered and monitored. We’ll see this in action in this and the next chapter.\n\nWe will also see node_ time series start to appear in the expression browser soon after the job is loaded.\n\nNode Explorer rules\n\nWe’re not going to add any new recording or alert rules for our Kubernetes nodes. Rather we’ve added the rules we created in Chapter 4 to the ConfigMap we’re using to populate Prometheus’s rule files. So we’re adding all the CPU, memory, and disk rules we created, and we’re also adding some availability alert rules for our Kubernetes services. Let’s look at those now.\n\nVersion: v1.0.0 (427b8e9)\n\n326\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.14: Kubernetes availability alerting rules\n\nalert: KubernetesServiceDown\n\nexpr: up{job=\"kubernetes-service-endpoints\"} == 0 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Pod {{ $labels.instance }} is down!\n\nalert: KubernetesServicesGone\n\nexpr: absent(up{job=\"kubernetes-service-endpoints\"}) for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: No Kubernetes services are reporting! description: Werner Heisenberg says - OMG Where are my\n\nservicez?\n\nThe first alert triggers when the value of the up metric for the kubernetes-service -endpoints job is 0. This indicates that Prometheus has failed to scrape a service. The second alert caters for a service disappearing and uses the absent function to check for the presence of the up metric.\n\nWe’ve also added alert rules for the services we’re monitoring on individual nodes using the node_systemd_unit_state metric, which tracks the status of systemd services.\n\nVersion: v1.0.0 (427b8e9)\n\n327\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.15: Kubernetes availability alerting rules\n\nalert: CriticalServiceDown\n\nexpr: node_systemd_unit_state{state=\"active\"} != 1 for: 2m labels:\n\nseverity: critical\n\nannotations:\n\nsummary = {{ $labels.instance }}: Service {{ $labels.name }}\n\nfailed to start.\n\ndescription = {{ $labels.instance }} failed to (re)start\n\nservice {{ $labels.name }}.\n\nThis will alert when it detects that any of the services our Node Exporter is monitoring—Docker, Kubelet, RSyslog, and SSH—are in a failed state.\n\nThere are other rules and alerts in the configuration that you can explore and adapt for node monitoring.\n\nNow let’s look at monitoring some Kubernetes components.\n\nKubernetes\n\nThere are a number of ways to monitor Kubernetes itself. These include tools in the open-source Kubernetes ecosystem like Heapster and Kube-state-metrics as well as commercial and SaaS-based tools. In this chapter, we’re going to focus on Kube-state-metrics to do our monitoring.\n\nKube-state-metrics\n\nWe’ll install Kube-state-metrics on our Kubernetes cluster using a deployment and service. The deployment uses the Kube-state-metrics Docker image and runs it on\n\nVersion: v1.0.0 (427b8e9)\n\n328\n\nChapter 12: Monitoring a Stack - Kubernetes\n\none of our nodes. The service exposes the metrics on port 8080. As it’s a service, it allows us to take advantage of our existing Prometheus service job we created in the last section. When we run it, Prometheus will automatically discover the new service endpoint and start scraping the Kube-state-metrics.\n\nOnce we’ve added the service we’ll see a new target in the kubernetes-service- endpoints job in the http://prometheus.quicknuke.com:9090/targets listing.\n\nFigure 12.2: The Kube-state-metrics endpoint target\n\nWith Kube-state-metrics we’re going to focus on the success and failure of the workloads we’re deploying to Kubernetes and the state of our nodes. Let’s look at some alerts for which we can use our Kube-state-metrics time series.\n\n TIP You can see a full list of the metrics that Kube-state-metrics produces in\n\nits documentation.\n\nVersion: v1.0.0 (427b8e9)\n\n329\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.16: Kube-state-metrics deployment generation alert\n\nalert: DeploymentGenerationMismatch\n\nexpr: kube_deployment_status_observed_generation !=\n\nkube_deployment_metadata_generation\n\nfor: 5m labels:\n\nseverity: warning\n\nannotations:\n\ndescription: Observed deployment generation does not match\n\nexpected one for\n\ndeployment {{$labels.namespace}}/{{$labels.deployment}}\n\nsummary: Deployment is outdated\n\nIt compares the running Our first rule detects if a deployment has succeeded. generation of a deployment with the generation in the metadata. If the two are not equal for five minutes then an alert is raised indicating that a deployment has failed.\n\nOur second rule does similar but for deployment replicas.\n\nVersion: v1.0.0 (427b8e9)\n\n330\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.17: Kube-state-metrics Deployment replicas not updated alert\n\nalert: DeploymentReplicasNotUpdated\n\nexpr: ((kube_deployment_status_replicas_updated !=\n\nkube_deployment_spec_replicas)\n\nor (kube_deployment_status_replicas_available !=\n\nkube_deployment_spec_replicas))\n\nunless (kube_deployment_spec_paused == 1)\n\nfor: 5m labels:\n\nseverity: warning\n\nannotations:\n\ndescription: Replicas are not updated and available for\n\ndeployment {{$labels.namespace}}/{{$labels.deployment}}\n\nsummary: Deployment replicas are outdated\n\nHere we perform a more complex expression that confirms that either the updated or available replicas should match the number of replicas in the deployment spec- ification, assuming the deployment isn’t paused.\n\nOur next rule checks for pod restarts.\n\nListing 12.18: Kube-state-metrics pod restarting alert\n\nalert: PodFrequentlyRestarting\n\nexpr: increase(kube_pod_container_status_restarts_total[1h]) >\n\n5\n\nfor: 10m labels:\n\nseverity: warning\n\nannotations:\n\ndescription: Pod {{ $labels.namespace }}/{{ $labels.pod }}\n\nwas restarted {{ $value }}\n\ntimes within the last hour\n\nsummary: Pod is restarting frequently\n\nVersion: v1.0.0 (427b8e9)\n\n331\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nHere we measure the number of pod restarts using the increase function. The increase function measures the rate of increase in a time series in range vector, here one hour. If the rate is over five for 10 minutes then the alert is raised.\n\nThere are a number of other time series we can use to monitor Kubernetes. For example, we could use the kube_node_status_condition to determine the avail- ability of the Kubernetes’ nodes. You’ll find some additional alerts in the alert rules we’re creating for this chapter.\n\nKube API\n\nWe also want to create a job to monitor our Kubernetes API itself. The metrics associated with the API will form the central core of our Kubernetes monitoring, allowing us to monitor latency, error rate, and availability for our cluster. We’re going to monitor the Kubernetes API specifically looking at latency, errors, and availability. Let’s create a job to monitor the API now.\n\nVersion: v1.0.0 (427b8e9)\n\n332\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.19: API server job\n\njob_name: 'kubernetes-apiservers' scheme: https tls_config:\n\nca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.\n\ncrt\n\ninsecure_skip_verify: true\n\nbearer_token_file: /var/run/secrets/kubernetes.io/\n\nserviceaccount/token\n\nkubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_namespace,\n\n__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n\naction: keep regex: default;kubernetes;https\n\nWe’ve called our job kubernetes-apiservers. We use https to scrape the metrics, and to specify the certification authority and a local token file to authenticate to Kubernetes. We again use Kubernetes discovery, this time to return a list of the Kubernetes endpoints. We won’t use all of the endpoints, and our relabelling configuration uses the keep action to only retain services named kubernetes in the default namespace—which will only be our master Kubernetes nodes running the API.\n\nNow that we’re collecting API server metrics, let’s create some recording rules to calculate the latency of the API servers.\n\nVersion: v1.0.0 (427b8e9)\n\n333\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.20: The API server recording rules\n\nrecord: apiserver_latency_seconds:quantile expr: histogram_quantile(0.99, rate(\n\napiserver_request_latencies_bucket[5m])) / 1e+06\n\nlabels:\n\nquantile: \"0.99\"\n\nrecord: apiserver_latency_seconds:quantile expr: histogram_quantile(0.9, rate(\n\napiserver_request_latencies_bucket[5m])) / 1e+06\n\nlabels:\n\nquantile: \"0.9\"\n\nrecord: apiserver_latency_seconds:quantile expr: histogram_quantile(0.5, rate(\n\napiserver_request_latencies_bucket[5m])) / 1e+06\n\nlabels:\n\nquantile: \"0.5\"\n\nWe make use of the apiserver_request_latencies_bucket metric to calculate our latency. This bucket metric, with dimensions for the specific API resource, sub- resource, and verb, measures request latency. We’ve created three rules for the 50th, 90th, and 99th percentiles, setting the quantile to the specific percentile. We’ve used the histogram_quantile function to create the percentiles from the metric buckets. We’ve specified the percentile we’re seeking, 0.99 for example, and then calculated a rate over a five minute vector and divided the result by 1e+06 or 1,000,000 to get microsecond latency.\n\nWe can then make use of the latency histograms our recording rules have created to create alerts. Let’s start with an alert to detect high latency from the API.\n\nVersion: v1.0.0 (427b8e9)\n\n334\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.21: API high latency alert\n\nalert: APIHighLatency\n\nexpr: apiserver_latency_seconds:quantile{quantile=\"0.99\",\n\nsubresource!=\"log\",verb!~\"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$\"} >\n\n4\n\nfor: 10m labels:\n\nseverity: critical\n\nannotations:\n\ndescription: the API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}\n\nOur alert uses the apiserver_latency_seconds:quantile metric we just created. We use labels to select the 99th percentile, any sub-resource that isn’t log, and any verb that isn’t WATCH, WATCHLIST, PROXY, or CONNECT. If the latency of any of the remaining metrics exceeds 4 for 10 minutes then the alert will be raised.\n\nOur next alert detects high levels of error rates in the API server.\n\nListing 12.22: API high error rate alert\n\nalert: APIServerErrorsHigh\n\nexpr: rate(apiserver_request_count{code=~\"^(?:5..)$\"}[5m]) /\n\nrate(apiserver_request_count[5m]) * 100 > 5\n\nfor: 10m labels:\n\nseverity: critical\n\nannotations:\n\ndescription: API server returns errors for {{ $value }}% of\n\nrequests\n\nThis alert calculates the error rate on API requests, using a regular expression to match any errors beginning with 5xx. If the percentage rate over a five minute\n\nVersion: v1.0.0 (427b8e9)\n\n335\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nvector exceeds 5 percent then the alert will be raised.\n\nOur last two alerts monitor the availability of the API server, monitoring the up metrics and the presence or absence of the up metric.\n\nListing 12.23: API servers down or absent\n\nalert: KubernetesAPIServerDown\n\nexpr: up{job=\"kubernetes-apiservers\"} == 0 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Apiserver {{ $labels.instance }} is down!\n\nalert: KubernetesAPIServersGone\n\nexpr: absent(up{job=\"kubernetes-apiservers\"}) for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: No Kubernetes apiservers are reporting! description: Werner Heisenberg says - OMG Where are my\n\napiserverz?\n\nLast, we can monitor the Kubernetes nodes and the Docker daemons and contain- ers running on them.\n\nCAdvisor and Nodes\n\nKubernetes also has CAdvisor and node-specific time series available by default. We can create a job to scrape these time series from the Kubernetes API for each node. We can use these time series, much as we did in Chapter 4, to monitor the nodes, and the Docker daemons and container-level on each node.\n\nLet’s add a job for CAdvisor.\n\nVersion: v1.0.0 (427b8e9)\n\n336\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.24: The CAdvisor job\n\njob_name: 'kubernetes-cadvisor' scheme: https\n\ntls_config:\n\ninsecure_skip_verify: true ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.\n\ncrt\n\nbearer_token_file: /var/run/secrets/kubernetes.io/\n\nserviceaccount/token\n\nkubernetes_sd_configs: - role: node relabel_configs: - action: labelmap\n\nregex: __meta_kubernetes_node_label_(.+)\n\ntarget_label: __address__\n\nreplacement: kubernetes.default.svc:443\n\nsource_labels: [__meta_kubernetes_node_name]\n\nregex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor\n\nWe’ve called our job kubernetes-cadvisor and used service discovery to return a list of the Kubernetes nodes using the node role. We use https to scrape the met- rics, and to specify the certification authority and a local token file to authenticate to Kubernetes.\n\nWe’re then relabelling our time series to create labels from the metadata labels we’ve discovered using labelmap. We replace the __address__ label with the default DNS name of the Kubernetes API server. We then use one of the metadata labels, a label with the name of the node, to create a new __metrics_path__ on the API that passes in the node name to the path.\n\n/api/v1/nodes/${1}/proxy/metrics/cadvisor\n\nThis will scrape the required time series for each node discovered by the job. We also have a job for the nodes themselves in our configuration that exposes some\n\nVersion: v1.0.0 (427b8e9)\n\n337\n\nChapter 12: Monitoring a Stack - Kubernetes\n\nKubernetes node-level metrics.\n\nWe can use these metrics to monitor the performance of the underlying containers, Docker daemons and the Kubernetes-level performance of the nodes.\n\nSummary\n\nIn this chapter we started looking at monitoring a stack, starting with our compute platform: Kubernetes. We installed Prometheus onto Kubernetes to make our monitoring easier. We looked at monitoring Kubernetes nodes and the nodes upon which they are deployed using the Node Exporter.\n\nWe created several Prometheus jobs, including several that use Kubernetes service discovery to automatically discover the nodes, API servers, and services that make up our environment. The service discovery also allows us to configure jobs that automatically begin scraping specific Kubernetes or application services as they appear, using annotations to select the right addresses, ports, and paths.\n\nIn the next chapter we’re going to monitor a multi-service application running on top of our Kubernetes cluster. We’ll look at monitoring some specific services, like MySQL and Redis, as well as our application.\n\nVersion: v1.0.0 (427b8e9)\n\n338",
      "page_number": 323
    },
    {
      "number": 13,
      "title": "Monitoring a Stack - Tornado",
      "start_page": 351,
      "end_page": 394,
      "detection_method": "regex_chapter",
      "content": "Chapter 13\n\nMonitoring a Stack - Tornado\n\nIn the last chapter we saw the basics of monitoring Kubernetes, using Prometheus. In this chapter we’re going to deploy an application, called Tornado, onto our Kubernetes cluster and monitor it. Tornado is a simple REST-ful HTTP API written in Clojure which runs on the JVM, has a Redis data store, and a MySQL database.\n\nWe’ve deployed each component of the application onto our Kubernetes cluster and will look at how we can monitor each component, collecting information on the component and identifying some key alerts. We’ll monitor:\n\nMySQL, • Redis, and • the Tornado API application.\n\nWe’re going to start with monitoring our two data stores. We’re going to use a pattern called sidecar, which we referenced in Chapter 9. Let’s take a quick look at that pattern.\n\n339\n\nChapter 13: Monitoring a Stack - Tornado\n\nSidecar pattern\n\nTo perform much of our monitoring, we’ll rely heavily on an architecture pat- tern called sidecar. The pattern is named sidecar because it resembles a sidecar attached to a motorcycle: the motorcycle is our application, and the sidecar is at- tached to this parent application. The sidecar provides supporting features for the application—for example, an infrastructure sidecar might collect logs or conduct monitoring. The sidecar also shares the same life cycle as the parent application, being created and deleted alongside the parent.\n\n TIP Sidecars are sometimes called sidekicks.\n\nVersion: v1.0.0 (427b8e9)\n\n340\n\nChapter 13: Monitoring a Stack - Tornado\n\nFigure 13.1: The sidecar\n\nIn our case, the sidebars run Prometheus exporters. The exporters query our ap- plications and in turn are queried by Prometheus. This sidecar model works on more than just Kubernetes, too; anywhere you’re deploying containers or services in clusters lends itself to this pattern.\n\nWe’ll run sidecar-monitoring exporters next to our Redis and MySQL installations, starting with our MySQL database.\n\nVersion: v1.0.0 (427b8e9)\n\n341\n\nChapter 13: Monitoring a Stack - Tornado\n\nMySQL\n\nMonitoring MySQL with Prometheus is done using an exporter: the MySQLd Ex- porter. The exporter works by connecting to a MySQL server using provided cre- dentials and querying the state of the server. The queried data is then exposed and can be scraped by the Prometheus server. This means that the exporter needs to have both network access to the MySQL server as well as credentials for authen- tication. In our case, we’re going to run the exporter inside a Docker container deployed to Kubernetes in our sidecar pattern.\n\nHere’s the segment of our MySQL Kubernetes deployment which runs the exporter in our sidecar container.\n\nListing 13.1: The exporter container\n\nimage: prom/mysqld-exporter:latest name: tornado-db-exp args: - --collect.info_schema.innodb_metrics - --collect.info_schema.userstats - --collect.perf_schema.eventsstatements - --collect.perf_schema.indexiowaits - --collect.perf_schema.tableiowaits env: - name: DATA_SOURCE_NAME\n\nvalue: \"tornado-db-exp:anotherstrongpassword@(tornado-db\n\n:3306)/\" ports: - containerPort: 9104\n\nname: tornado-db-exp\n\nYou can see we’re using the prom/mysqld-exporter image with the latest tag. We’ve called the container tornado-db-exp. We’ve specified our connection de- tails using the DATA_SOURCE_NAME environment variable. This connection uses the DSN format to configure connection and credential details for our MySQL server.\n\nVersion: v1.0.0 (427b8e9)\n\n342\n\nChapter 13: Monitoring a Stack - Tornado\n\nThe exporter running inside the container will automatically pick up the connec- tion details from the environmental variable.\n\nYou should create a separate user with a limited set of permissions. To query the required data from the MySQL server, you’ll need to grant your user the PROCESS, REPLICATION CLIENT, and SELECT permissions.\n\nYou can connect to the MySQL container using kubectl’s exec command, like so:\n\nListing 13.2: Connecting to the MySQL container\n\n$ kubectl exec -ti <pod> -- /usr/bin/mysql -p\n\nWe can then run CREATE USER and GRANT statements to assign the required permis- sions.\n\nListing 13.3: Creating a MySQL user\n\nCREATE USER 'tornado-db-exp'@'localhost' IDENTIFIED BY ' anotherstrongpassword'; GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'tornado-db- exp'; GRANT SELECT ON performance_schema.* TO 'tornado-db-exp';\n\nHere we’ve created a user called tornado-db-exp with the required permissions including a SELECT grant to the performance_schema table containing query per- formance data.\n\n TIP If you have a my.cnf file then the exporter can also use credentials hard-\n\ncoded in there.\n\nVersion: v1.0.0 (427b8e9)\n\n343\n\nChapter 13: Monitoring a Stack - Tornado\n\nWe could also configure the exporter using a variety of flags to control its behavior. We’ve enabled some additional collectors:\n\nListing 13.4: Additional MySQL exporter collector\n\n--collect.info_schema.innodb_metrics --collect.info_schema.userstats --collect.perf_schema.eventsstatements --collect.perf_schema.indexiowaits --collect.perf_schema.tableiowaits\n\nThese all collect data from MySQL’s performance schema database, allowing us to track the performance of specific queries and operations.\n\nIn our container deployment, we’ve also exposed port 9104, the default port of the MySQL Exporter, which in turn we’ve exposed in a service.\n\nListing 13.5: The tornado-db service\n\napiVersion: v1 kind: Service metadata:\n\nname: tornado-db annotations:\n\nprometheus.io/scrape: 'true' prometheus.io/port: '9104'\n\nspec:\n\nselector:\n\napp: tornado-db\n\ntype: ClusterIP ports: - port: 3306\n\nname: tornado-db\n\nport: 9104\n\nname: tornado-db-exp\n\nVersion: v1.0.0 (427b8e9)\n\n344\n\nChapter 13: Monitoring a Stack - Tornado\n\nWe’ve used two annotations: prometheus.io/scrape, which tells Prometheus to scrape this service, and prometheus.io/port, which tells Prometheus which port to scrape. We specify this because we want Prometheus to hit the MySQL Ex- porter port at 9104 rather than the MySQL server directly. These annotations are automatically picked up by the kubernetes-service-endpoints job we created in Chapter 12, and parsed by the relabelling configuration in that job, which we can see below:\n\nListing 13.6: The Kubernetes endpoint job relabelling\n\nrelabel_configs:\n\nsource_labels: [\n\n__meta_kubernetes_service_annotation_prometheus_io_scrape]\n\naction: keep regex: true\n\n. . .\n\nsource_labels: [__address__,\n\n__meta_kubernetes_service_annotation_prometheus_io_port]\n\naction: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2\n\n. . .\n\nThe prometheus.io/scrape annotation ensures Prometheus will only keep metrics from service endpoints with the annotation set to true. The prometheus.io/port annotation will be placed into the __address__ label to be scraped by the job. The next service discovery will start collection of the MySQL metrics.\n\nMySQL Prometheus configuration\n\nAs our exporter is being exposed as a service endpoint, we don’t need to configure a specific job to scrape it. We will, however, create some rules for our MysQL time series and add them to our rules ConfigMap. We’re just going to create a\n\nVersion: v1.0.0 (427b8e9)\n\n345\n\nChapter 13: Monitoring a Stack - Tornado\n\nsampling of possible rules, loosely aligned with Google’s Four Golden Signals, to give you an idea of how you might use your MySQL metrics. We’ll focus on:\n\nLatency • Traffic • Errors • Saturation\n\n WARNING Measuring MySQL performance is hard, especially when\n\ntracking signals like latency, and circumstances vary greatly depending on your application and server configuration. These rules give you starting point, not a definitive answer. There are a number of guides online that might prove useful.\n\nFirst, let’s look at some rules, starting with tracking the growth rate of slow queries using the mysql_global_status_slow_queries metric. This counter is incremented when a query exceeds the long_query_time variable, which defaults to 10 seconds.\n\nListing 13.7: MySQL slow query alert\n\nalert: MySQLHighSlowQueryRate\n\nexpr: rate(mysql_global_status_slow_queries[2m]) > 5 labels:\n\nseverity: warning\n\nannotations:\n\nsummary: MySQL Slow query rate is exceeded on {{ $labels.\n\ninstance }} for {{ $labels.kubernetes_name }}\n\nThis will generate an alert if the rate over two minutes exceeds five. We can also create recording rules to track the request rates on our server.\n\nVersion: v1.0.0 (427b8e9)\n\n346\n\nChapter 13: Monitoring a Stack - Tornado\n\nListing 13.8: MySQL request rate records\n\nrecord: mysql:write_requests:rate2m\n\nexpr: sum(rate(mysql_global_status_commands_total{command=~\"\n\ninsert|update|delete\"}[2m])) without (command) - record: mysql:select_requests:rate2m\n\nexpr: sum(rate(mysql_global_status_commands_total{command=\"\n\nselect\"}[2m])) - record: mysql:total_requests:rate2m\n\nexpr: rate(mysql_global_status_commands_total[2m])\n\nrecord: mysql:top5_statements:rate5m\n\nexpr: topk(5, sum by (schema,digest_text) (rate(\n\nmysql_perf_schema_events_statements_total[5m])))\n\nWe use the mysql_global_status_commands_total metric and grab all the write requests for specific commands: insert, update, and delete. We then calculate a rate over two minutes for these requests. We do the same for read requests using the select command, and for total requests. We’re also using the topk aggregation operator to get the most frequently used statements by schema and rate over the last five minutes, which helps us understand what the server is doing.\n\nFigure 13.2: The topk operator over MySQL statements\n\nWe could graph or alert on these as needed. We can also track connection requests and errors.\n\nVersion: v1.0.0 (427b8e9)\n\n347\n\nChapter 13: Monitoring a Stack - Tornado\n\nListing 13.9: Connections and aborted connections\n\nalert: MySQLAbortedConnectionsHigh\n\nexpr: rate(mysql_global_status_aborted_connects[2m]) > 5 labels:\n\nseverity: warning\n\nannotations:\n\nsummary: MySQL Aborted connection rate is exceeded on {{\n\n$labels.instance }} for {{ $labels.kubernetes_name }} - record: mysql:connection:rate2m\n\nexpr: rate(mysql_global_status_connections[2m])\n\nHere we’re alerting if the rate of aborted connections exceeds a threshold, and creating a recording rule to track the rate of connections overall.\n\nLast, we want to know when our MySQL service is unavailable. These alerts use the combination of the state and presence of the exporter-specific up metric: mysql_up. The mysql_up metric does a SELECT 1 on the MySQL server and is set to 1 if the query succeeds. The first alert checks if the value of the mysql_up metric is 0, indicating the query has failed. The second alert checks for the presence of this metric in the event the service disappears and the metric is expired.\n\nVersion: v1.0.0 (427b8e9)\n\n348\n\nChapter 13: Monitoring a Stack - Tornado\n\nListing 13.10: MySQL alerts\n\nalert: TornadoDBServerDown\n\nexpr: mysql_up{kubernetes_name=\"tornado-db\"} == 0 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: MySQL Server {{ $labels.instance }} is down!\n\nalert: TornadoDBServerGone\n\nexpr: absent(mysql_up{kubernetes_name=\"tornado-db\"}) for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: No Tornado DB servers are reporting! description: Werner Heisenberg says - there is no\n\nuncertainty about the Tornado MySQL server being gone.\n\nThese are are some useful starter rules. You can find these rules in the code for this chapter.\n\nRedis\n\nLike MySQL, Prometheus has an exporter for Redis. The Redis Exporter will export most of the items from the INFO command with details of server, client, memory, and CPU usage. There are also metrics for total keys, expiring keys, and the average TTL for keys in each database. You can also export values of keys.\n\nAnd, again like MySQL, we can run the exporter as a sidecar of the Redis container. Here’s a snippet of our Redis Kubernetes deployment.\n\nVersion: v1.0.0 (427b8e9)\n\n349\n\nChapter 13: Monitoring a Stack - Tornado\n\nListing 13.11: Redis service and sidecar\n\napiVersion: apps/v1beta2 kind: Deployment . . .\n\nname: redis-exporter\n\nimage: oliver006/redis_exporter:latest env: - name: REDIS_ADDR\n\nvalue: redis://tornado-redis:6379\n\nname: REDIS_PASSWORD value: tornadoapi ports: - containerPort: 9121\n\nWe’re running a container called redis-exporter from a Docker image, oliver006/redis_exporter. We’ve specified two environments variables: REDIS_ADDR, which specifies the address of the Redis server we want to scrape, and REDIS_PASSWORD, which specifies a password to connect to the server with. We also specify port 9121 to export our metrics on.\n\n TIP There are other environment variables and command line flags you can\n\nset, which you can read about in the documentation.\n\nWe then expose this port via a Kubernetes service.\n\nVersion: v1.0.0 (427b8e9)\n\n350\n\nChapter 13: Monitoring a Stack - Tornado\n\nListing 13.12: The Redis Kubernetes service\n\napiVersion: v1 kind: Service metadata:\n\nname: tornado-redis annotations:\n\nprometheus.io/scrape: 'true' prometheus.io/port: '9121'\n\nspec:\n\nselector:\n\napp: tornado-redis\n\nports: - port: 6379\n\nname: redis\n\nport: 9121\n\nname: redis-exporter\n\nclusterIP: None\n\nYou can see that we’ve exposed port 9121, and specified two annotations—one to tell our Prometheus service endpoint job to scrape this service, and one which port to scrape. The next time Prometheus does service discovery it will detect the updated service and start collecting our Redis metrics.\n\nRedis Prometheus configuration\n\nAs our exporter is being exposed as a service endpoint, we don’t need to configure a specific job to scrape it. We will, however, create some rules for our Redis time series and add them. We’re again going to show you a sampling of rules, for example:\n\nVersion: v1.0.0 (427b8e9)\n\n351\n\nChapter 13: Monitoring a Stack - Tornado\n\nListing 13.13: Redis alerts\n\nalert: TornadoRedisCacheMissesHigh\n\nexpr: redis_keyspace_hits_total / (redis_keyspace_hits_total +\n\nredis_keyspace_misses_total) > 0.8\n\nfor: 10m labels:\n\nseverity: warning\n\nannotations:\n\nsummary: Redis Server {{ $labels.instance }} Cache Misses\n\nare high. - alert: RedisRejectedConnectionsHigh\n\nexpr: avg(redis_rejected_connections_total) by (addr) < 10 for: 10m labels:\n\nseverity: warning\n\nannotations:\n\nsummary: \"Redis instance {{ $labels.addr }} may be hitting\n\nmaxclient limit.\"\n\ndescription: \"The Redis instance at {{ $labels.addr }} had {{\n\n$value }} rejected connections during the last 10m and may be\n\nhitting the maxclient limit.\"\n\nHere we’re measuring if cache misses exceed 0.8 and if the rejected connections average is high.\n\nLast, like our MySQL service, we want to know when our Redis service is unavail- able. These alerts use the combination of the state and presence of the exporter- specific up metric, redis_up. The redis_up metric is set to 1 if the scrape of the Redis server succeeds. The first alert checks if the value of the redis_up metric is 0, indicating the scrape has failed. The second alert checks for the presence of this metric in the event the service disappears and the metric is expired.\n\nVersion: v1.0.0 (427b8e9)\n\n352\n\nChapter 13: Monitoring a Stack - Tornado\n\nListing 13.14: Redis availability alerts\n\nalert: TornadoRedisServerDown\n\nexpr: redis_up{kubernetes_name=\"tornado-redis\"} == 0 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Redis Server {{ $labels.instance }} is down!\n\nalert: TornadoRedisServerGone\n\nexpr: absent(redis_up{kubernetes_name=\"tornado-redis\"}) for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: No Tornado Redis servers are reporting! description: Werner Heisenberg says - there is no\n\nuncertainty about the Tornado Redis server being gone.\n\nNow that we’ve added some monitoring to our MySQL and Redis services, we want to monitor our API service.\n\nTornado\n\nThe Tornado API is a Clojure application that uses Ring and runs on the JVM. It has a single API endpoint that buys and sells items. We’re going to instrument the application in much the same way we saw in Chapter 8 to create metrics that monitor each API action.\n\nAdding the Clojure wrapper\n\nTo instrument our application we’re using the iapetos Clojure wrapper. There are several Clojure wrappers and clients for Prometheus; we chose iapetos because\n\nVersion: v1.0.0 (427b8e9)\n\n353\n\nChapter 13: Monitoring a Stack - Tornado\n\nits up to date and easy to use. To enable the iapetos wrapper we need to add it to the project’s dependencies in the project.clj file.\n\nListing 13.15: Adding the client to the project.clj\n\n(defproject tornado-api-prometheus \"0.1.0-SNAPSHOT\"\n\n:description \"Example Clojure REST service for \" :url \"http://artofmonitoring.com\" :dependencies [[org.clojure/clojure \"1.8.0\"]\n\n. . .\n\n[iapetos \"0.1.8\"] [io.prometheus/simpleclient_hotspot \"0.4.0\"\n\n]]\n\n:plugins [[lein-ring \"0.7.3\"]]\n\n. . .\n\nHere we’ve added the iapetos and the Prometheus simpleclient_hotspot (which we need for exporting some JVM metrics) dependencies.\n\nWe can then require the relevant components of the wrapper in our application’s source code.\n\nListing 13.16: Requiring the wrapper components\n\n(:require [compojure.handler :as handler] . . .\n\n[iapetos.core :as prometheus] [iapetos.collector.ring :as ring] [iapetos.collector.jvm :as jvm]\n\n. . .\n\nWe’ve included the base iapetos wrapper as prometheus and two context-specific components for exporting Ring and JVM metrics respectively.\n\nVersion: v1.0.0 (427b8e9)\n\n354\n\nChapter 13: Monitoring a Stack - Tornado\n\nAdding a registry\n\nLike our Ruby application in Chapter 8, we need to define a registry to hold all of our metrics. Our application is pretty simple so we’re just adding one registry, but you can add more than one or a registry per subsystem, if, for instance, you want the same counter in different subsystems.\n\nListing 13.17: Defining the registry\n\n(defonce registry\n\n(-> (prometheus/collector-registry)\n\n(jvm/initialize) (ring/initialize) (prometheus/register\n\n(prometheus/counter :tornado/item-get) (prometheus/counter :tornado/item-bought) (prometheus/counter :tornado/item-sold) (prometheus/counter :tornado/update-item) (prometheus/gauge\n\n:tornado/up))))\n\nWe’ve created a registry called registry and we’ve initialized the Ring and JVM metrics, which will be automatically collected and exported. We’ve then de- fined five specific metrics, four of them counters and one gauge, all prefixed with tornado. We have one counter for each of the API’s actions and a gauge to act as an up metric for the application. We can also add labels to the metrics we’ve defined.\n\nListing 13.18: Adding labels\n\n(prometheus/counter :tornado/item-bought {:description \"Total items bought\"})\n\nHere we’ve added a description label to the item-bought counter.\n\nVersion: v1.0.0 (427b8e9)\n\n355\n\nChapter 13: Monitoring a Stack - Tornado\n\nAdding metrics\n\nWe can now add function calls to each API method on our application to increment our counters. For example, here’s the function that increments the metric for buying an item:\n\nListing 13.19: Adding metric calls\n\n(defn buy-item [item]\n\n(let [id (uuid)]\n\n(sql/db-do-commands db-config\n\n(let [item (assoc item \"id\" id)]\n\n(sql/insert! db-config :items item) (prometheus/inc (registry :tornado/item-bought)))) (wcar* (car/ping)\n\n(car/set id (item \"title\")))\n\n(get-item id)))\n\nWe’re calling the inc function to increment our item-bought counter when an item is bought. We could also set gauges or other time series including histograms.\n\nWe’ve also added a gauge called tornado_up that will act as the up metric for our application. When the application starts it will automatically set the value of the gauge to 1.\n\nListing 13.20: The tornado_up gauge\n\n(prometheus/set (registry :tornado/up) 1)\n\nVersion: v1.0.0 (427b8e9)\n\n356\n\nChapter 13: Monitoring a Stack - Tornado\n\nExporting the metrics\n\nLast, we want to enable the /metrics page itself, in our case by using the built-in Ring support.\n\nListing 13.21: Starting the export\n\n(def app\n\n(-> (handler/api app-routes)\n\n(middleware/wrap-json-body) (middleware/wrap-json-response) (ring/wrap-metrics registry {:path \"/metrics\"})))\n\nThis will make the metrics we’ve defined, some JVM-centric metrics and some HTTP-specific metrics emitted from Ring, available on the application at the / metrics path.\n\nIf we now browse to this path we’ll see our metrics emitted. Here’s a quick sample.\n\nListing 13.22: Tornado metrics\n\n# HELP http_request_latency_seconds the response latency for HTTP requests. # TYPE http_request_latency_seconds histogram http_request_latency_seconds_bucket{method=\"GET\", status=\"404\", statusClass=\"4XX\", path=\"index.php\", le=\"0.001\",} 2.0 . . . # HELP tornado_item_sold a counter metric. # TYPE tornado_item_sold counter tornado_item_sold 0.0 . . . # HELP jvm_threads_peak Peak thread count of a JVM # TYPE jvm_threads_peak gauge jvm_threads_peak 14.0\n\nVersion: v1.0.0 (427b8e9)\n\n357\n\nChapter 13: Monitoring a Stack - Tornado\n\nTornado Prometheus configuration\n\nLike our other services, our Clojure exporter is being exposed as an endpoint, and we don’t need to configure a specific job to scrape it. We get a wide variety of metrics—metrics from the JVM, HTTP metrics from Ring, and metrics from the application itself. We can now create some alerts and rules to monitor our API.\n\nHere’s a latency recording rule we created using one of the Ring HTTP metrics.\n\nListing 13.23: Ring latency rule\n\nrecord: tornado:request_latency_seconds:avg\n\nexpr: http_request_latency_seconds_sum{status=\"200\"} /\n\nhttp_request_latency_seconds_count{status=\"200\"}\n\nWe’re created a new metric, tornado:request_latency_seconds:avg, The aver- age request latency in seconds for requests which result in a 200 HTTP code.\n\nWe can also take advantage of one of the Ring-related historgrams to alert on high latency.\n\nListing 13.24: Ring high latency alert\n\nalert: TornadoRequestLatencyHigh expr: histogram_quantile(0.9, rate(\n\nhttp_request_latency_seconds_bucket{ kubernetes_name=\"tornado- api\" [5m])) > 0.05\n\nfor: 10m labels:\n\nseverity: warning\n\nannotations:\n\nsummary: API Server {{ $labels.instance }} latency is over\n\n0.05.\n\nHere we’ve used the histogram_quantile function to generate the 90th percentile\n\nVersion: v1.0.0 (427b8e9)\n\n358\n\nChapter 13: Monitoring a Stack - Tornado\n\nof our request latency over 5 minutes. Our alert will be triggered if that exceeds 0.05 for 10 minutes.\n\nWe can also take advantage of the up-style metric we created, tornado_up, to watch for the availability of our API service.\n\nListing 13.25: Monitoring the Tornado API availability\n\nalert: TornadoAPIServerDown\n\nexpr: tornado_up{kubernetes_name=\"tornado-api\"} != 1 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: API Server {{ $labels.instance }} is down!\n\nalert: TornadoAPIServerGone\n\nexpr: absent(tornado_up{kubernetes_name=\"tornado-api\"}) for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: No Tornado API servers are reporting! description: Werner Heisenberg says - there is no\n\nuncertainty about the Tornado API server being gone.\n\nHere we’ll detect if the tornado_up metric has a value other than 0 or if it disap- peared from our metrics.\n\nThis gives you a simple overview of how you might apply what you’ve learned in the book to monitoring an application stack.\n\nSummary\n\nIn this chapter we’ve seen how we’d monitor services and applications running on top of Kubernetes. We used the sidecar pattern to do this, parallel monitoring\n\nVersion: v1.0.0 (427b8e9)\n\n359\n\nChapter 13: Monitoring a Stack - Tornado\n\nrunning next to our services and application, inside the same deployment.\n\nWe also saw another application instrumented, this time a Clojure-based applica- tion using the iapetos wrapper.\n\nYou can easily build upon this basis to monitor more complex applications and services using this simple building-block pattern.\n\nVersion: v1.0.0 (427b8e9)\n\n360\n\nList of Figures\n\n1 License . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 ISBN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n1.1 Service hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Monitoring design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 A sample plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4 A sample gauge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.5 A sample counter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6 A histogram example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.7 An aggregated collection of metrics . . . . . . . . . . . . . . . . . . . . 1.8 The flaw of averages - copyright Jeff Danzinger . . . . . . . . . . . . . 1.9 Response time average . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.10 Response time average Mk II . . . . . . . . . . . . . . . . . . . . . . . 1.11 Response time average and median . . . . . . . . . . . . . . . . . . . 1.12 Response time average and median Mk II . . . . . . . . . . . . . . . . 1.13 The empirical rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.14 Response time average, median, and percentiles . . . . . . . . . . . 1.15 Response time average, median, and percentiles Mk II . . . . . . . .\n\n2.1 Prometheus architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Prometheus expression browser 2.3 Redundant Prometheus architecture . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . . . . . 3.1 Prometheus expression browser 3.2 List of metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n361\n\n5 5\n\n9 11 20 21 22 23 25 27 28 29 30 31 32 34 35\n\n49 52 53\n\n76 77\n\nList of Figures\n\n3.3 Querying quantiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Querying total HTTP requests . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Calculating total HTTP requests by job . . . . . . . . . . . . . . . . . . 3.6 Our rate query . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n78 80 81 83\n\n4.1 cAdvisor web interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.2 cAdvisor Prometheus metrics . . . . . . . . . . . . . . . . . . . . . . . . 100 4.3 Scrape lifecycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 4.4 Sample label taxonomy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 4.5 Labels prior to relabelling . . . . . . . . . . . . . . . . . . . . . . . . . . 112 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 4.6 node_cpu metrics 4.7 Per-host average percentage CPU usage metrics . . . . . . . . . . . . . 116 4.8 Per-host percentage CPU plot . . . . . . . . . . . . . . . . . . . . . . . . 117 4.9 Number of CPUs in each host . . . . . . . . . . . . . . . . . . . . . . . . 118 4.10 The node_memory_MemTotal . . . . . . . . . . . . . . . . . . . . . . . 119 4.11 Per-host percentage memory usage . . . . . . . . . . . . . . . . . . . . 120 4.12 Disk metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 4.13 Per-host disk space metrics . . . . . . . . . . . . . . . . . . . . . . . . 122 4.14 The systemd time series data . . . . . . . . . . . . . . . . . . . . . . . 125 4.15 The active services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 4.16 The up metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 4.17 The cadvisor_version metric . . . . . . . . . . . . . . . . . . . . . . . . 130 4.18 The node_cpu recorded rule . . . . . . . . . . . . . . . . . . . . . . . . 137 4.19 The Grafana console login . . . . . . . . . . . . . . . . . . . . . . . . . 145 4.20 The Grafana console . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 4.21 Adding a Grafana data source for Prometheus . . . . . . . . . . . . . 147 4.22 Adding a Grafana data source for Prometheus . . . . . . . . . . . . . 148 4.23 The Node dashboard . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n\n5.1 Scrape lifecycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n\n6.1 Alertmanager architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 174\n\nVersion: v1.0.0 (427b8e9)\n\n362\n\nList of Figures\n\n6.2 Alertmanager routing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 6.3 Alertmanager web interface . . . . . . . . . . . . . . . . . . . . . . . . . 184 6.4 List of Prometheus alerts . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 6.5 Fired alert in Alertmanager . . . . . . . . . . . . . . . . . . . . . . . . . 194 6.6 HighNodeCPU alert email . . . . . . . . . . . . . . . . . . . . . . . . . . 195 6.7 Scheduling silences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 6.8 A new silence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 6.9 Editing or expiring silences . . . . . . . . . . . . . . . . . . . . . . . . . 212\n\n7.1 Fault-tolerant architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 219 7.2 Alertmanager cluster status . . . . . . . . . . . . . . . . . . . . . . . . . 223 7.3 Prometheus clustered Alertmanagers . . . . . . . . . . . . . . . . . . . 226 7.4 Organizational sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 7.5 Functional sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 7.6 Horizontal sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 7.7 The Federate API\n\n8.1 Rails server targets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254 8.2 Rails metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n\n9.1 mtail diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262\n\n10.1 Probing architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 10.2 The blackbox exporter console . . . . . . . . . . . . . . . . . . . . . . 284 10.3 The probe metrics in Prometheus . . . . . . . . . . . . . . . . . . . . . 288\n\n11.1 The Pushgateway . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292 11.2 The Pushgateway dashboard . . . . . . . . . . . . . . . . . . . . . . . . 298 11.3 The test_counter in the Pushgateway dashboard . . . . . . . . . . . . 307 11.4 The test_counter metric . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\n\n12.1 The Kubernetes endpoint targets . . . . . . . . . . . . . . . . . . . . . 325 12.2 The Kube-state-metrics endpoint target . . . . . . . . . . . . . . . . . 329\n\nVersion: v1.0.0 (427b8e9)\n\n363\n\nList of Figures\n\n13.1 The sidecar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341 13.2 The topk operator over MySQL statements . . . . . . . . . . . . . . . 347\n\nVersion: v1.0.0 (427b8e9)\n\n364\n\nListings\n\n1 Sample code block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1 Sample Nagios notification . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Time series notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Example time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Download the Prometheus tarball 3.2 Unpack the prometheus binary . . . . . . . . . . . . . . . . . . . . . . . 3.3 Checking the Prometheus version on Linux . . . . . . . . . . . . . . . 3.4 Creating a directory on Windows . . . . . . . . . . . . . . . . . . . . . . 3.5 Prometheus Windows download . . . . . . . . . . . . . . . . . . . . . . 3.6 Setting the Windows path . . . . . . . . . . . . . . . . . . . . . . . . . . 3.7 Checking the Prometheus version on Windows . . . . . . . . . . . . . 3.8 Installing Prometheus via Chocolatey . . . . . . . . . . . . . . . . . . . 3.9 Installing Prometheus via Homebrew . . . . . . . . . . . . . . . . . . . 3.10 Checking the Prometheus version on Mac OS X . . . . . . . . . . . . 3.11 The default Prometheus configuration file . . . . . . . . . . . . . . . 3.12 Alertmanager configuration . . . . . . . . . . . . . . . . . . . . . . . . 3.13 The default Prometheus scrape configuration . . . . . . . . . . . . . 3.14 Moving the configuration file . . . . . . . . . . . . . . . . . . . . . . . 3.15 Starting the Prometheus server . . . . . . . . . . . . . . . . . . . . . . 3.16 Validating your configuration with promtool . . . . . . . . . . . . . . 3.17 Running Prometheus with Docker . . . . . . . . . . . . . . . . . . . . 3.18 Mounting a configuration file into the Docker container . . . . . . .\n\n365\n\n3 40 56 56 62 62 63 63 63 64 64 65 65 65 68 70 72 73 73 73 74 74\n\nListings\n\n75 3.19 Some sample raw metrics . . . . . . . . . . . . . . . . . . . . . . . . . 75 3.20 A raw metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 3.21 Go garbage collection 50th percentile . . . . . . . . . . . . . . . . . . 79 3.22 The prometheus_build_info metric . . . . . . . . . . . . . . . . . . . . 89 4.1 Downloading the Node Exporter . . . . . . . . . . . . . . . . . . . . . . 90 4.2 Testing the Node Exporter binary . . . . . . . . . . . . . . . . . . . . . 90 4.3 Running the help for Node Exporter . . . . . . . . . . . . . . . . . . . . 90 4.4 Controlling the port and path . . . . . . . . . . . . . . . . . . . . . . . . 91 4.5 Disabling the arp collector . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4.6 Creating a textfile directory . . . . . . . . . . . . . . . . . . . . . . . . . 92 4.7 A metadata metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 4.8 Starting Node Exporter with the textfile collector and systemd . . . . 94 4.9 The current Prometheus scrape configuration . . . . . . . . . . . . . . 95 4.10 Adding the node job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 4.11 Filtering collectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.12 Testing collect params 98 4.13 Running the caAdvisor container . . . . . . . . . . . . . . . . . . . . . 4.14 The cAdvisor container . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.15 Adding the Docker job . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 4.16 Overriding the discovered labels . . . . . . . . . . . . . . . . . . . . . 103 4.17 Dropping metrics with relabelling . . . . . . . . . . . . . . . . . . . . 108 4.18 Specifying a new separator . . . . . . . . . . . . . . . . . . . . . . . . . 109 4.19 Replacing a label . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 4.20 Dropping a label . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 4.21 Memory saturation query . . . . . . . . . . . . . . . . . . . . . . . . . 121 4.22 The node_systemd_unit_state metrics . . . . . . . . . . . . . . . . . . . 124 4.23 The up metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 4.24 A one-to-one vector match . . . . . . . . . . . . . . . . . . . . . . . . . 129 4.25 The evaluation_interval parameter . . . . . . . . . . . . . . . . . . . . 132 4.26 Creating a recorded rules file . . . . . . . . . . . . . . . . . . . . . . . 133 4.27 Adding the rules file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n\nVersion: v1.0.0 (427b8e9)\n\n366\n\nListings\n\n4.28 A recording rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 4.29 A recording group interval . . . . . . . . . . . . . . . . . . . . . . . . . 135 4.30 A recording rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 4.31 A recording rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 4.32 Getting the PackageCloud public key on Ubuntu . . . . . . . . . . . 139 4.33 Adding the Grafana packages . . . . . . . . . . . . . . . . . . . . . . . 139 4.34 Updating Apt and installing the Grafana package . . . . . . . . . . . 139 4.35 Getting the Grafana public key on Red Hat . . . . . . . . . . . . . . . 140 4.36 The Grafana Yum configuration . . . . . . . . . . . . . . . . . . . . . . 140 4.37 Installing Grafana on Red Hat . . . . . . . . . . . . . . . . . . . . . . . 140 4.38 Creating a Grafana directory on Windows . . . . . . . . . . . . . . . 141 4.39 Grafana Windows download . . . . . . . . . . . . . . . . . . . . . . . . 141 4.40 Setting the Windows path for Grafana . . . . . . . . . . . . . . . . . . 141 4.41 Installing Grafana via Homebrew . . . . . . . . . . . . . . . . . . . . . 142 4.42 Starting the Grafana Server on Linux . . . . . . . . . . . . . . . . . . 143 4.43 Starting Grafana at boot on OSX . . . . . . . . . . . . . . . . . . . . . 143 4.44 Starting Grafana server on OS X . . . . . . . . . . . . . . . . . . . . . 144 5.1 Our static service discovery . . . . . . . . . . . . . . . . . . . . . . . . . 153 5.2 File-based discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 5.3 Creating the target directory structure . . . . . . . . . . . . . . . . . . 155 5.4 Creating JSON files to hold our targets . . . . . . . . . . . . . . . . . . 155 5.5 The nodes.json file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 5.6 The daemons.json file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 5.7 The daemons file in YAML . . . . . . . . . . . . . . . . . . . . . . . . . . 156 5.8 Adding labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 5.9 An EC2 discovery job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 5.10 An EC2 discovery job with a profile . . . . . . . . . . . . . . . . . . . 161 5.11 An EC2 discovery job with a port . . . . . . . . . . . . . . . . . . . . . 161 5.12 Relabelling an EC2 discovery job . . . . . . . . . . . . . . . . . . . . . 162 5.13 Relabelling the instance name in a EC2 discovery job . . . . . . . . 164 5.14 DNS service discovery job . . . . . . . . . . . . . . . . . . . . . . . . . 165\n\nVersion: v1.0.0 (427b8e9)\n\n367\n\nListings\n\n5.15 A SRV record . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 5.16 Example SRV records . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 5.17 The DNS targets from the SRV . . . . . . . . . . . . . . . . . . . . . . 167 5.18 DNS A record service discovery job . . . . . . . . . . . . . . . . . . . 167 5.19 DNS subdomain A record service discovery job . . . . . . . . . . . . 168 6.1 Stock Nagios alert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 6.2 Download the Alertmanager tarball . . . . . . . . . . . . . . . . . . . . 176 6.3 Unpack the alertmanager binary . . . . . . . . . . . . . . . . . . . . . . 176 6.4 Moving the amtool binary . . . . . . . . . . . . . . . . . . . . . . . . . . 176 6.5 Checking the Alertmanager version on Linux . . . . . . . . . . . . . . 177 6.6 Creating a directory on Windows . . . . . . . . . . . . . . . . . . . . . . 177 6.7 Alertmanager Windows download . . . . . . . . . . . . . . . . . . . . . 178 6.8 Setting the Windows path . . . . . . . . . . . . . . . . . . . . . . . . . . 178 6.9 Checking the Alertmanager version on Windows . . . . . . . . . . . . 178 6.10 Creating the alertmanager.yml file . . . . . . . . . . . . . . . . . . . . 179 6.11 A simple alertmanager.yml configuration file . . . . . . . . . . . . . 180 6.12 Creating the templates directory . . . . . . . . . . . . . . . . . . . . . 181 6.13 Starting Alertmanager . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 6.14 The alerting block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 6.15 The Alertmanager SRV record . . . . . . . . . . . . . . . . . . . . . . . 186 6.16 Discovering the Alertmanager . . . . . . . . . . . . . . . . . . . . . . . 186 6.17 The Alertmanager Prometheus job . . . . . . . . . . . . . . . . . . . . 187 6.18 Creating an alerting rules file . . . . . . . . . . . . . . . . . . . . . . . 188 6.19 Adding globbing rule_files block . . . . . . . . . . . . . . . . . . . . . 188 6.20 Our first alerting rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 6.21 The ALERT time series . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 6.22 Adding more alerting rules . . . . . . . . . . . . . . . . . . . . . . . . . 196 6.23 Humanizing a value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 6.24 Creating the prometheus_alerts.yml file . . . . . . . . . . . . . . . . . 198 6.25 The prometheus_alerts.yml file . . . . . . . . . . . . . . . . . . . . . . 198 6.26 Node service alert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n\nVersion: v1.0.0 (427b8e9)\n\n368\n\nListings\n\n6.27 The up metric missing alert . . . . . . . . . . . . . . . . . . . . . . . . 201 6.28 Adding routing configuration . . . . . . . . . . . . . . . . . . . . . . . 202 6.29 Grouping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 6.30 Label matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204 6.31 Routing branching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204 6.32 Routing branching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 6.33 Multiple endpoints in a receiver . . . . . . . . . . . . . . . . . . . . . 205 6.34 A regular expression match . . . . . . . . . . . . . . . . . . . . . . . . 206 6.35 Adding a Slack receiver . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 6.36 Adding a Slack receiver . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 6.37 Creating a template file . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 6.38 The slack.tmpl file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 6.39 Adding a Slack receiver . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 6.40 Using amtool to schedule a silence . . . . . . . . . . . . . . . . . . . . 213 6.41 Querying the silences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214 6.42 Expiring the silence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214 6.43 Sample amtool configuration file . . . . . . . . . . . . . . . . . . . . . 214 6.44 Using amtool to schedule a silence . . . . . . . . . . . . . . . . . . . . 215 6.45 Omitting alertname . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 7.1 Starting Alertmanager cluster . . . . . . . . . . . . . . . . . . . . . . . . 221 7.2 Starting Alertmanager cluster remaining nodes . . . . . . . . . . . . . 222 7.3 Defining alertmanagers statically . . . . . . . . . . . . . . . . . . . . . . 224 7.4 The Alertmanager SRV record . . . . . . . . . . . . . . . . . . . . . . . 225 7.5 Discovering the Alertmanager . . . . . . . . . . . . . . . . . . . . . . . . 225 7.6 The worker0 configuration . . . . . . . . . . . . . . . . . . . . . . . . . 232 7.7 The instance CPU rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233 7.8 The primary configuration . . . . . . . . . . . . . . . . . . . . . . . . . . 234 7.9 Worker file discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 7.10 Matching parameter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 7.11 The match[ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 8.1 A sample payments method . . . . . . . . . . . . . . . . . . . . . . . . . 243\n\nVersion: v1.0.0 (427b8e9)\n\n369\n\nListings\n\n8.2 The mwp-rails Gemfile . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 8.3 Install prometheus-client with the bundle command . . . . . . . . . . 246 8.4 Testing the Prometheus client with the Rails console . . . . . . . . . . 246 8.5 Creating a Prometheus registry . . . . . . . . . . . . . . . . . . . . . . . 247 8.6 Registering a Prometheus metric . . . . . . . . . . . . . . . . . . . . . . 247 8.7 Incrementing a metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 8.8 Incrementing a metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248 . . . . . . . . . . . . . . . . 248 8.9 The basic Prometheus client_ruby metrics 8.10 Creating a Metrics module . . . . . . . . . . . . . . . . . . . . . . . . . 248 8.11 The Metrics module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 8.12 Creating an initializer for the metrics library . . . . . . . . . . . . . . 250 8.13 The config/initializers/lib.rb file . . . . . . . . . . . . . . . . . . . . . 250 8.14 Counter for user deletions . . . . . . . . . . . . . . . . . . . . . . . . . 250 8.15 Counter for user creation . . . . . . . . . . . . . . . . . . . . . . . . . . 251 8.16 Adding Prometheus to the config.ru file . . . . . . . . . . . . . . . . . 252 8.17 The Rails /metrics endpoint . . . . . . . . . . . . . . . . . . . . . . . . 253 8.18 Our Rails servers service discovery . . . . . . . . . . . . . . . . . . . . 254 8.19 The rails job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254 9.1 Download and install the mtail binary . . . . . . . . . . . . . . . . . . 258 9.2 Running the mtail binary . . . . . . . . . . . . . . . . . . . . . . . . . . 258 9.3 Creating an mtail program directory . . . . . . . . . . . . . . . . . . . . 259 9.4 Creating the line_count.mtail program . . . . . . . . . . . . . . . . . . 259 9.5 The line_count.mtail program . . . . . . . . . . . . . . . . . . . . . . . . 260 9.6 A relational clause . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 9.7 Running mtail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 9.8 The mtail /metrics path . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 9.9 The apache_combined program . . . . . . . . . . . . . . . . . . . . . . . 264 9.10 The combined access log actions . . . . . . . . . . . . . . . . . . . . . 266 9.11 Running mtail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 9.12 Apache combined metrics . . . . . . . . . . . . . . . . . . . . . . . . . 267 9.13 The mtail rails program . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\n\nVersion: v1.0.0 (427b8e9)\n\n370\n\nListings\n\n9.14 Rails mtail metric output . . . . . . . . . . . . . . . . . . . . . . . . . . 270 9.15 The mtail job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 9.16 Worker file discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 10.1 Download the blackbox exporter zip file . . . . . . . . . . . . . . . . 277 10.2 Unpack the blackbox_exporter binary . . . . . . . . . . . . . . . . . . 277 10.3 Checking the blackbox exporter version on Linux . . . . . . . . . . . 278 10.4 Creating a directory on Windows . . . . . . . . . . . . . . . . . . . . . 278 10.5 Blackbox exporter Windows download . . . . . . . . . . . . . . . . . 279 10.6 Setting the Windows path . . . . . . . . . . . . . . . . . . . . . . . . . 279 10.7 Checking the blackbox exporter version on Windows . . . . . . . . . 279 10.8 The prober.yml file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280 10.9 The /etc/prober/prober.yml file . . . . . . . . . . . . . . . . . . . . . 281 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282 10.10 Valid status codes 10.11 Starting the exporter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283 10.12 The http_probes job . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 10.13 The http_probe targets . . . . . . . . . . . . . . . . . . . . . . . . . . . 286 10.14 The http_2xx_check metrics . . . . . . . . . . . . . . . . . . . . . . . . 287 11.1 Download the Pushgateway zip file . . . . . . . . . . . . . . . . . . . 294 11.2 Unpack the pushgateway binary . . . . . . . . . . . . . . . . . . . . . 295 11.3 Checking the Pushgateway version on Linux . . . . . . . . . . . . . . 295 11.4 Creating a directory on Windows . . . . . . . . . . . . . . . . . . . . . 296 11.5 Pushgateway Windows download . . . . . . . . . . . . . . . . . . . . 296 11.6 Setting the Windows path . . . . . . . . . . . . . . . . . . . . . . . . . 296 11.7 Checking the Pushgateway version on Windows . . . . . . . . . . . . 297 11.8 Running the Pushgateway on an interface . . . . . . . . . . . . . . . 298 11.9 Persisting the metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 11.10 Posting a metric to the gateway . . . . . . . . . . . . . . . . . . . . . 299 11.11 The Pushgateway metrics path . . . . . . . . . . . . . . . . . . . . . 299 11.12 Posting a metric to the gateway . . . . . . . . . . . . . . . . . . . . . 300 11.13 Adding labels to pushed metrics . . . . . . . . . . . . . . . . . . . . . 301 11.14 Passing types and descriptions . . . . . . . . . . . . . . . . . . . . . . 301\n\nVersion: v1.0.0 (427b8e9)\n\n371\n\nListings\n\n11.15 Passing types and descriptions . . . . . . . . . . . . . . . . . . . . . . 301 11.16 Curling the gateway metrics . . . . . . . . . . . . . . . . . . . . . . . 303 11.17 Deleting Pushgateway metrics . . . . . . . . . . . . . . . . . . . . . . 304 11.18 Deleting a selection of Pushgateway metrics . . . . . . . . . . . . . 304 11.19 Creating MetricsPush class . . . . . . . . . . . . . . . . . . . . . . . . 305 11.20 The MetricsPush module . . . . . . . . . . . . . . . . . . . . . . . . . 306 11.21 Pushing a metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307 11.22 The pushgateway job . . . . . . . . . . . . . . . . . . . . . . . . . . . 308 11.23 Our Pushgateway . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309 12.1 The cluster kops command . . . . . . . . . . . . . . . . . . . . . . . . . 312 12.2 The Node Exporter DaemonSet tolerations . . . . . . . . . . . . . . . 315 12.3 The Node Exporter DaemonSet containers . . . . . . . . . . . . . . . 316 12.4 Node Exporter liveness and readiness probes . . . . . . . . . . . . . . 317 12.5 The Node Exporter service . . . . . . . . . . . . . . . . . . . . . . . . . 319 12.6 Deploying the Node Exporter daemonset and service . . . . . . . . . 320 12.7 The default namespace . . . . . . . . . . . . . . . . . . . . . . . . . . . 320 12.8 Checking the Node Exporter pods . . . . . . . . . . . . . . . . . . . . 321 12.9 A Node Exporter pod’s logs . . . . . . . . . . . . . . . . . . . . . . . . 321 12.10 Checking the Node Exporter service . . . . . . . . . . . . . . . . . . 322 12.11 The Kubernetes service endpoints job . . . . . . . . . . . . . . . . . 323 12.12 Replacing the ConfigMap . . . . . . . . . . . . . . . . . . . . . . . . . 325 12.13 The monitoring services . . . . . . . . . . . . . . . . . . . . . . . . . . 326 12.14 Kubernetes availability alerting rules . . . . . . . . . . . . . . . . . . 327 12.15 Kubernetes availability alerting rules . . . . . . . . . . . . . . . . . . 328 12.16 Kube-state-metrics deployment generation alert . . . . . . . . . . . 330 12.17 Kube-state-metrics Deployment replicas not updated alert . . . . . 331 12.18 Kube-state-metrics pod restarting alert . . . . . . . . . . . . . . . . . 331 12.19 API server job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333 12.20 The API server recording rules . . . . . . . . . . . . . . . . . . . . . . 334 12.21 API high latency alert . . . . . . . . . . . . . . . . . . . . . . . . . . . 335 . . . . . . . . . . . . . . . . . . . . . . . . . 335 12.22 API high error rate alert\n\nVersion: v1.0.0 (427b8e9)\n\n372\n\nListings\n\n12.23 API servers down or absent . . . . . . . . . . . . . . . . . . . . . . . . 336 12.24 The CAdvisor job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337 13.1 The exporter container . . . . . . . . . . . . . . . . . . . . . . . . . . . 342 13.2 Connecting to the MySQL container . . . . . . . . . . . . . . . . . . . 343 13.3 Creating a MySQL user . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 13.4 Additional MySQL exporter collector . . . . . . . . . . . . . . . . . . 344 13.5 The tornado-db service . . . . . . . . . . . . . . . . . . . . . . . . . . . 344 13.6 The Kubernetes endpoint job relabelling . . . . . . . . . . . . . . . . 345 13.7 MySQL slow query alert . . . . . . . . . . . . . . . . . . . . . . . . . . 346 13.8 MySQL request rate records . . . . . . . . . . . . . . . . . . . . . . . . 347 13.9 Connections and aborted connections . . . . . . . . . . . . . . . . . . 348 13.10 MySQL alerts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349 13.11 Redis service and sidecar . . . . . . . . . . . . . . . . . . . . . . . . . 350 13.12 The Redis Kubernetes service . . . . . . . . . . . . . . . . . . . . . . 351 13.13 Redis alerts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352 13.14 Redis availability alerts . . . . . . . . . . . . . . . . . . . . . . . . . . 353 13.15 Adding the client to the project.clj . . . . . . . . . . . . . . . . . . . 354 13.16 Requiring the wrapper components . . . . . . . . . . . . . . . . . . . 354 13.17 Defining the registry . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355 13.18 Adding labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355 13.19 Adding metric calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356 13.20 The tornado_up gauge . . . . . . . . . . . . . . . . . . . . . . . . . . . 356 13.21 Starting the export . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357 13.22 Tornado metrics 13.23 Ring latency rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358 13.24 Ring high latency alert . . . . . . . . . . . . . . . . . . . . . . . . . . 358 13.25 Monitoring the Tornado API availability . . . . . . . . . . . . . . . . 359\n\nVersion: v1.0.0 (427b8e9)\n\n373\n\nIndex\n\n$labels, 197 $value, 197 __ time series names, 55 __address__, 102, 163, 337 __meta_ec2_public_ip, 162 __metrics_path__, 102, 337 __name__, 108 __scheme__, 102\n\nAbsent, 200 action\n\nhashmod, 233 keep, 233 Aggregation, 80 Alert\n\nAnnotations, 197 templates, 197\n\nAlerting, 70, 170\n\nSymptoms versus causes, 172\n\nAlerting rules, 69, 131, 187 Alertmanager, 51, 58, 70, 170\n\namtool, 184 API, 193 Cluster, 218\n\n374\n\nconfiguration, 179 continue, 204 default route, 201 Email, 180 email\n\nemails_configs, 183\n\nglobal, 180 group_by, 202 group_interval, 202 group_wait, 202 grouping, 202 High availability, 218 Installation, 175 Linux, 175 Mac OS X, 177 Windows, 177\n\nInstalling via configuration manage-\n\nment, 179\n\nmatch, 204 match_re, 206 Mesh, 218 Notification template variable refer-\n\nence, 208\n\nreceivers, 182 Resolved alerts, 206 routing, 181, 204 send_resolved, 206 Silences, 210 Supported platforms, 175 templates, 181, 207 version, 176 web hooks, 183 web interface, 184\n\nAlerts, 39 amtool, 184, 213, 216 Annotations, 197 Ansible, 66, 142 Apophenia, 41 Application architecture, 9, 238 Application metrics, 238 Application monitoring, 9, 238 Architecture, 48, 51 Availability monitoring, 126 Average, 24 Averages, 25, 29 AWS, 159, 161, 164\n\nAccess Key ID, 160 Profile, 161 Secret Access Key, 160\n\nBatch jobs, 291 Bell Curve, 26 Binary operators, 128 Black Exporter\n\nVersion: v1.0.0 (427b8e9)\n\nIndex\n\nConfiguration, 280 Blackbox Exporter, 274, 276\n\nInstallation, 276 Linux, 277 Mac OS X, 278 Windows, 278\n\nInstalling via configuration manage-\n\nment, 279\n\nScraping the exporter, 285 Supported platforms, 276 version, 278\n\nBlackbox exporter, 201 DNS prober, 283 HTTP prober, 282 ICMP prober, 282\n\nBlackbox monitoring, 15, 274 Borg, 47 Borgmon, 47 Business metrics, 238 Buy-v-build, 43\n\ncAdvisor, 97 Capacity planning, 83 Chef, 66, 142, 280, 297 Chocolatey, 64 Client libraries, 58, 245 client_ruby, 245 Clustering, 218 CNCF, 48 collectd, 88, 258 Comparison binary operator, 125\n\n375\n\nConfiguration, 67, 73 Configuration Management, 62, 66,\n\n179, 279, 297\n\nConfiguration management, 142 container_last_seen, 127 Count, 24 Counters, 21 CPU, 114, 117\n\nData model, 54 Disabling collectors, 91 Disk, 121 DNS service discovery, 165 dns_sd_configs, 165 Docker, 97, 142, 280\n\nEC2 Service Discovery metadata, 161, 164 Profile, 161 Role ARN, 160\n\nEC2 Service discovery, 159 ec2_sd_config\n\naccess_key, 160 port, 161 profile, 161 region, 160 secret_key, 160\n\nELK, 17, 257 Endpoints, 50, 71 Exporters, 48, 58 Grok, 257 Node, 88\n\nVersion: v1.0.0 (427b8e9)\n\nExpression browser, 51, 76\n\nFault tolerance, 217 Federation, 228, 229 File-based service discovery, 154 file_sd_config, 154 files, 154 refresh_interval, 155 Frequency distribution, 22\n\nGauges, 21 global\n\nevaluation_interval, 69 scrape_interval, 69\n\nGo, 48\n\nclient, 245\n\nGoogle’s Golden Signals, 36 Grafana, 54\n\ninstallation OS X, 142 Windows, 140\n\nGranularity, 13, 20, 69 Graph, 20 Graphite, 258, 263 Grok Exporter, 257\n\nhashmod, 233 High Availability, 217 High availability, 51, 53 Histogram, 22 histogram_quantile, 334 Homebrew, 65, 142\n\nIndex\n\n376\n\nhonor_labels, 309 Host monitoring, 88\n\nICMP, 288 increase, 332 Installation, 61\n\nLinux, 62, 175, 277, 294 Mac OS X, 65, 142 Microsoft Windows, 63, 64, 140,\n\n177, 278 Windows, 63, 140\n\nInstalling mtail, 258 Installing onto Kubernetes, 67, 312 Installing via configuration manage-\n\nment, 66\n\nInstance label, 102, 114, 164 Instances, 50, 71 Instrumentation, 58, 238 Instrumentation labels, 55 Instrumenting applications, 305 Introspection monitoring, 15 irate, 115\n\nJava\n\nclient, 245 Job definition, 72 job_name, 72 Jobs, 50, 71, 94, 291\n\nService discovery, 151\n\nkeep, 233 kops, 312\n\nVersion: v1.0.0 (427b8e9)\n\nKubernetes, 67, 159, 312 Node Exporter, 314 kubernetes_sd_config, 313\n\nLabel\n\nInstance, 114 instance, 102, 164 Labels, 54, 55, 105, 128 __address__, 102, 163 __meta_ec2_public_ip, 162 __meta_filepath, 157 __metrics_path__, 102 __name__, 108 __scheme__, 102 Metadata, 102\n\nLatency, 36 Logging, 17, 256 Logs, 17 Logstash, 257\n\nMaintenance, 209 Mean, 25 Median, 24, 30, 35 Memory, 119 Metric names, 54, 55, 135 metric_relabel_configs, 107\n\naction, 110 regex, 109, 111 replacement, 111 separator, 108 source_labels, 108 target_label, 111\n\nIndex\n\n377\n\nMetrics, 18, 242\n\nlatency, 36\n\nmetrics_relabel_configs, 162 modulus, 232 Monitoring, 6 Monitoring anti-patterns, 9 Monitoring CPU, 114, 117 Monitoring disk, 121 Monitoring jobs, 291 Monitoring Kubernetes, 313 Monitoring memory, 119 Monitoring methodologies, 36\n\nGoogle’s Golden Signals, 36 USE Method, 36\n\nmtail, 257\n\nconfiguration, 259 constants, 265 histogram, 268 installation, 258 programs, 259 running, 262 types, 266\n\nMySQL, 342\n\nNAT, 293 Node Exporter, 88, 314\n\ndisabling collectors, 91 filtering collectors, 95 Textfile collector, 92\n\nNode monitoring, 88 Notification templates, 207\n\nVersion: v1.0.0 (427b8e9)\n\nNotifications, 39\n\nObservability, 15 Observations, 19\n\nPercentiles, 24, 33, 35 Plot, 20 predict_linear, 123 Probe\n\nDNS, 288 ICMP, 288\n\nProbing, 274\n\nArchitecture, 275 Probing monitoring, 15 Promeditor, 83 Prometheus, 6\n\nconfiguration, 67 disk usage, 83 duplicate servers, 220 fault-tolerance, 220 installation Linux, 62 OS X, 65 Windows, 63 memory usage, 83 Web interface, 76\n\nprometheus\n\n–config.file, 73 –version, 62\n\nPrometheus server, 58 prometheus.yml, 67 PromQL, 51, 78, 80\n\nIndex\n\n378\n\nBinary operators, 128 by, 80 count, 117 irate, 82, 115 predict_linear, 123 Range vectors, 82 rate, 81 regular expressions, 122 Scalar, 82 String, 82 Vector matches, 129 without, 80\n\npromtool, 62, 67, 73, 132, 137 Pull-based monitoring, 17 Puppet, 66, 142, 280, 297 Push Gateway, 48 Push-based monitoring, 17 Pushgateway, 291\n\nAggregation, 304 clients, 305 Configuration, 299 Delete metrics, 304 Installation, 293 Linux, 294 Mac OS X, 295 Windows, 295\n\nInstalling via configuration manage-\n\nment, 297\n\npush_time_seconds, 304 Scaling, 293 Scraping the gateway, 308\n\nVersion: v1.0.0 (427b8e9)\n\nSending metrics, 299 Supported platforms, 293 version, 295 Viewing metrics, 302\n\nPushProx, 293 Python\n\nclient, 245\n\nQuantile, 33 Querying labels, 78\n\nRails, 244\n\nmetrics, 244 Prometheus, 244\n\nRange vectors, 82 Rates of change, 24 RE2, 109 Receivers, 207 Recording rules, 69, 131, 132\n\nsequencing, 134\n\nRedis, 349 Regex, 109 regex, 163 RegExp, 109 relabel_configs, 107, 161, 162 Relabelling, 107, 128, 162\n\naction\n\ndrop, 110 keep, 324 labeldrop, 112 labelmap, 325, 337 replace, 324\n\nIndex\n\n379\n\nhonor_labels, 111 ordering, 110 Remote storage, 236 remote_read, 237 remote_write, 237 Resolution, 13, 20, 69 Ruby\n\nclient, 245\n\nRule files, 71 rule_files, 71, 133 Rules, 69\n\nco-mingle, 187\n\nSaltStack, 66, 280 Samples, 19 Scaling, 217 Scrape configuration, 50, 71 Scrape interval, 69 Scrape lifecycle, 101, 152 scrape_configs, 50, 71 Server, 58 Service discovery, 94, 151–153\n\nDNS, 165 EC2, 159 File-based, 154 multiple configurations, 153, 162\n\nService records, 166 Sharding, 228 Sidecar, 271 Sidecar pattern, 340 Silences, 209, 210\n\nVersion: v1.0.0 (427b8e9)\n\nexpiration, 213\n\nSoundCloud, 48 source_labels, 162 SRV records, 166, 167 SSD, 52, 85 Standard Deviation, 24, 33 static_configs, 94 StatsD, 258, 263 Sum, 24 Summary, 23 Supported platforms, 61\n\nTags, 55 Target labels, 55 target_label, 163 Targets, 50, 71 Templates, 197 Text exposition format, 91 Textfile collector, 92 Thanos, 237 Thresholds, 12 Time series, 19 topk, 347\n\nUnsee, 212 Up metric, 126 USE Method, 36, 113 Utility model, 242\n\nVector matches, 129 Visualization, 41, 131\n\nWhitebox monitoring, 15\n\nIndex\n\n380\n\nYAML, 67 YAML validation, 67\n\nVersion: v1.0.0 (427b8e9)\n\nIndex\n\n381\n\nThanks! I hope you enjoyed the book.\n\n© Copyright 2018 - James Turnbull <james@lovedthanlost.net>",
      "page_number": 351
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Version: v1.0.0 (427b8e9)\n\n1",
      "content_length": 28,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "Monitoring With Prometheus\n\nJames Turnbull\n\nJune 12, 2018\n\nVersion: v1.0.0 (427b8e9)\n\nWebsite: Monitoring With Prometheus",
      "content_length": 121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Some rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical or photocopying, recording, or otherwise, for commercial purposes without the prior permission of the publisher.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License. To view a copy of this license, visit here.\n\n© Copyright 2018 - James Turnbull <james@lovedthanlost.net>",
      "content_length": 490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Contents\n\nForeword\n\nWho is this book for? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Credits and Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . Technical Reviewers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Jamie Wilkinson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Paul Gier Editor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Conventions in the book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Code and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Colophon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Errata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Disclaimer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Copyright . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nChapter 1 Introduction\n\nWhat is monitoring? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Technology as a customer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The business as a customer Monitoring fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nMonitoring as afterthought\n\ni\n\nPage\n\n1 1 1 2 2 2 2 3 3 4 4 4 4 5 5\n\n6 6 7 8 8 9",
      "content_length": 1650,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Monitoring by rote . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Not monitoring for correctness . . . . . . . . . . . . . . . . . . . . . . Monitoring statically . . . . . . . . . . . . . . . . . . . . . . . . . . . . Not monitoring frequently enough . . . . . . . . . . . . . . . . . . . . No automation or self-service . . . . . . . . . . . . . . . . . . . . . . . Good monitoring summary . . . . . . . . . . . . . . . . . . . . . . . . Monitoring mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Probing and introspection . . . . . . . . . . . . . . . . . . . . . . . . . Pull versus push . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Types of monitoring data . . . . . . . . . . . . . . . . . . . . . . . . . . Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . So what’s a metric? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Types of metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Metric summaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Metric aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The USE Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The Google Four Golden Signals . . . . . . . . . . . . . . . . . . . . . Contextual, useful alerts and notifications . . . . . . . . . . . . . . . . . . Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . But didn’t you write that other book? . . . . . . . . . . . . . . . . . . . . . What’s in the book? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nMonitoring methodologies\n\nChapter 2 Introduction to Prometheus\n\nThe Prometheus backstory . . . . . . . . . . . . . . . . . . . . . . . . . . . Prometheus architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Metric collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Service discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Aggregation and alerting . . . . . . . . . . . . . . . . . . . . . . . . . . Querying data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nVersion: v1.0.0 (427b8e9)\n\nContents\n\n10 12 12 13 14 14 15 16 17 17 18 19 21 23 24 36 36 38 39 41 42 43 44\n\n46 47 48 49 50 50 51\n\nii",
      "content_length": 2448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Autonomy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Redundancy and high availability . . . . . . . . . . . . . . . . . . . . Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Metric names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Metrics retention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Security model Prometheus ecosystem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Useful Prometheus links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nThe Prometheus data model\n\nChapter 3 Installation and Getting Started\n\nInstalling Prometheus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Installing Prometheus on Linux . . . . . . . . . . . . . . . . . . . . . . Installing Prometheus on Microsoft Windows . . . . . . . . . . . . . Alternative Microsoft Windows installation . . . . . . . . . . . . . . . Alternative Mac OS X installation . . . . . . . . . . . . . . . . . . . . Stacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Installing via configuration management . . . . . . . . . . . . . . . . Deploying via Kubernetes . . . . . . . . . . . . . . . . . . . . . . . . . Configuring Prometheus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Global Alerting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Rule files Scrape configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . Starting the server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Running Prometheus via Docker . . . . . . . . . . . . . . . . . . . . . First metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nVersion: v1.0.0 (427b8e9)\n\nContents\n\n52 53 54 54 54 55 56 56 57 57 58 58 59\n\n60 61 62 63 64 65 66 66 67 67 69 70 71 71 73 74 75\n\niii",
      "content_length": 2464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Contents\n\nPrometheus expression browser . . . . . . . . . . . . . . . . . . . . . . . . Time series aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Capacity planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Disk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n76 79 83 84 85 86\n\nChapter 4 Monitoring Nodes and Containers\n\n87 88 Monitoring nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 Installing the Node Exporter . . . . . . . . . . . . . . . . . . . . . . . . 90 Configuring the Node Exporter . . . . . . . . . . . . . . . . . . . . . . 91 Configuring the Textfile collector . . . . . . . . . . . . . . . . . . . . . 93 Enabling the systemd collector . . . . . . . . . . . . . . . . . . . . . . 93 Running the Node Exporter . . . . . . . . . . . . . . . . . . . . . . . . 94 Scraping the Node Exporter . . . . . . . . . . . . . . . . . . . . . . . . 95 Filtering collectors on the server . . . . . . . . . . . . . . . . . . . . . 97 Monitoring Docker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Running cAdvisor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 Scraping cAdvisor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 Scrape lifecycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 Label taxonomies Relabelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 The Node Exporter and cAdvisor metrics . . . . . . . . . . . . . . . . . . . 112 The trinity and the USE method . . . . . . . . . . . . . . . . . . . . . 113 Service status . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 Availability and the up metric . . . . . . . . . . . . . . . . . . . . . . . 126 The metadata metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 Query permanence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 Recording rules Configuring recording rules . . . . . . . . . . . . . . . . . . . . . . . . 132\n\nVersion: v1.0.0 (427b8e9)\n\niv",
      "content_length": 2471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Contents\n\nAdding recording rules . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 Installing Grafana . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 Starting and configuring Grafana . . . . . . . . . . . . . . . . . . . . . 142 Configuring the Grafana web interface . . . . . . . . . . . . . . . . . 144 First dashboard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n\nChapter 5 Service Discovery\n\n151 Scrape lifecycle and static configuration redux . . . . . . . . . . . . . . . 152 File-based discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 Writing files for file discovery . . . . . . . . . . . . . . . . . . . . . . . 157 Inbuilt service discovery plugins . . . . . . . . . . . . . . . . . . . . . . . . 159 Amazon EC2 service discovery plugin . . . . . . . . . . . . . . . . . . 159 DNS service discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n\nChapter 6 Alerting and Alertmanager\n\n170 Alerting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171 How the Alertmanager works . . . . . . . . . . . . . . . . . . . . . . . . . 173 Installing Alertmanager . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175 Installing Alertmanager on Linux . . . . . . . . . . . . . . . . . . . . . 175 Installing Alertmanager on Microsoft Windows . . . . . . . . . . . . 177 Stacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178 Installing via configuration management . . . . . . . . . . . . . . . . 179 Configuring the Alertmanager . . . . . . . . . . . . . . . . . . . . . . . . . 179 Running Alertmanager . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 Configuring Prometheus for Alertmanager . . . . . . . . . . . . . . . . . . 184 Alertmanager service discovery . . . . . . . . . . . . . . . . . . . . . . 185 . . . . . . . . . . . . . . . . . . . . . . . . . 187 Monitoring Alertmanager Adding alerting rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n\nVersion: v1.0.0 (427b8e9)\n\nv",
      "content_length": 2348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Contents\n\nAdding our first alerting rule . . . . . . . . . . . . . . . . . . . . . . . 188 What happens when an alert fires? . . . . . . . . . . . . . . . . . . . . 192 The alert at the Alertmanager . . . . . . . . . . . . . . . . . . . . . . . 193 Adding new alerts and templates . . . . . . . . . . . . . . . . . . . . . 195 Routing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 Routes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 Receivers and notification templates . . . . . . . . . . . . . . . . . . . . . 206 Silences and maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 Controlling silences via the Alertmanager . . . . . . . . . . . . . . . . 210 Controlling silences via amtool . . . . . . . . . . . . . . . . . . . . . . 213 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n\nChapter 7 Scaling and Reliability\n\n217 Reliability and fault tolerance . . . . . . . . . . . . . . . . . . . . . . . . . 218 Duplicate Prometheus servers . . . . . . . . . . . . . . . . . . . . . . . 220 Setting up Alertmanager clustering . . . . . . . . . . . . . . . . . . . . 220 Configuring Prometheus for an Alertmanager cluster . . . . . . . . . 224 Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 Functional scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 Horizontal shards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 Remote storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 Third-party tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n\nChapter 8 Instrumenting Applications\n\n238 An application monitoring primer . . . . . . . . . . . . . . . . . . . . . . . 238 Where should I instrument? . . . . . . . . . . . . . . . . . . . . . . . . 240 Instrument taxonomies . . . . . . . . . . . . . . . . . . . . . . . . . . . 240 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240 Application metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 Business metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n\nVersion: v1.0.0 (427b8e9)\n\nvi",
      "content_length": 2361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Contents\n\nWhere to put your metrics . . . . . . . . . . . . . . . . . . . . . . . . . 242 The utility pattern . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242 The external pattern . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244 Building metrics into a sample application . . . . . . . . . . . . . . . 244 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\n\nChapter 9 Logging as Instrumentation\n\n256 Processing logs for metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 Introducing mtail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 Installing mtail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259 Using mtail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 Running mtail Processing web server access logs . . . . . . . . . . . . . . . . . . . . . . . 264 Parsing Rails logs into a histogram . . . . . . . . . . . . . . . . . . . . . . 268 Deploying mtail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271 . . . . . . . . . . . . . . . . . . . . . . . . . . 271 Scraping our mtail endpoint Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n\nChapter 10 Probing\n\n274 Probing architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274 The blackbox exporter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276 Installing the exporter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276 Installing the exporter on Linux . . . . . . . . . . . . . . . . . . . . . . 277 Installing the exporter on Microsoft Windows . . . . . . . . . . . . . 278 Installing via configuration management . . . . . . . . . . . . . . . . 279 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280 HTTP check . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281 ICMP check . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282 DNS check . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282 Starting the exporter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283 Creating the Prometheus job . . . . . . . . . . . . . . . . . . . . . . . . . . 285\n\nConfiguring the exporter\n\nVersion: v1.0.0 (427b8e9)\n\nvii",
      "content_length": 2375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Contents\n\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\n\nChapter 11 Pushing Metrics and the Pushgateway\n\n290 The Pushgateway . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 When not to use the Pushgateway . . . . . . . . . . . . . . . . . . . . 292 Installing the Pushgateway . . . . . . . . . . . . . . . . . . . . . . . . 293 Installing the Pushgateway on Linux . . . . . . . . . . . . . . . . . . . 294 Installing the Pushgateway on Microsoft Windows . . . . . . . . . . 295 Installing via configuration management . . . . . . . . . . . . . . . . 297 Configuring and running the Pushgateway . . . . . . . . . . . . . . . 297 Sending metrics to the Pushgateway . . . . . . . . . . . . . . . . . . . 299 Viewing metrics on the Pushgateway . . . . . . . . . . . . . . . . . . 302 Deleting metrics in the Pushgateway . . . . . . . . . . . . . . . . . . . 304 Sending metrics from a client . . . . . . . . . . . . . . . . . . . . . . . 305 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\n\nChapter 12 Monitoring a Stack - Kubernetes\n\n311 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311 Our Kubernetes cluster . . . . . . . . . . . . . . . . . . . . . 312 Running Prometheus on Kubernetes Monitoring Kubernetes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313 Monitoring our Kubernetes nodes . . . . . . . . . . . . . . . . . . . . . . . 314 Node Exporter DaemonSet . . . . . . . . . . . . . . . . . . . . . . . . . 314 Node Exporter service . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318 Deploying the Node Exporter . . . . . . . . . . . . . . . . . . . . . . . 320 The Node Exporter job . . . . . . . . . . . . . . . . . . . . . . . . . . . 322 Node Explorer rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326 Kubernetes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328 Kube-state-metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328 Kube API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332 CAdvisor and Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\n\nVersion: v1.0.0 (427b8e9)\n\nviii",
      "content_length": 2325,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Contents\n\nChapter 13 Monitoring a Stack - Tornado\n\n339 Sidecar pattern . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340 MySQL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342 MySQL Prometheus configuration . . . . . . . . . . . . . . . . . . . . 345 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349 Redis Prometheus configuration . . . . . . . . . . . . . . . . . . . . . 351 Tornado . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353 Adding the Clojure wrapper . . . . . . . . . . . . . . . . . . . . . . . . 353 Adding a registry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355 Adding metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356 Exporting the metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357 Tornado Prometheus configuration . . . . . . . . . . . . . . . . . . . . 358 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\n\nRedis\n\nList of Figures\n\n364\n\nList of Listings\n\n373\n\nIndex\n\n374\n\nVersion: v1.0.0 (427b8e9)\n\nix",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Foreword\n\nWho is this book for?\n\nThis book is a hands-on introduction to monitoring with Prometheus.\n\nMost of the book’s examples are Ubuntu Linux-based, and there is an expectation that the reader has basic Unix/Linux skills and is familiar with the command line, editing files, installing packages, managing services, and basic networking.\n\nFinally, Prometheus is evolving quickly. That means “Here Be Dragons,” and you should take care to confirm what versions you’re using of the tools in this book.\n\nThe book is designed to be used with Prometheus version 2.3.0 and later. This Material will not work with earlier releases.\n\nCredits and Acknowledgments\n\nRuth Brown, who continues to humor these books and my constant tap-tap-\n\ntap of keys late into the night.\n\nSid Orlando, who makes my words good. • Bryan Brazil for his excellent Prometheus blog. He also runs training that you should check out.\n\nDavid Karlsen for his technical review work.\n\n1",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Foreword\n\nTechnical Reviewers\n\nThanks to the folks who helped make this book more accurate and useful!\n\nJamie Wilkinson\n\nJamie is a Site Reliability Engineer in Google’s Storage Infrastructure team. He began in Linux systems administration in 1999, while earning a Bachelor’s in Computer Science, so knows just enough theory of computation to be dangerous in his field. He contributed a chapter on monitoring to the Google SRE Book. Jamie lives with his family in Sydney, Australia.\n\nPaul Gier\n\nAs a curious kid growing up at a time when proprietary software was the rule, Paul was frustrated by a lack of money and licenses. Soon after learning about a new operating system called Linux, Paul was hooked on the ideas of free software— ideas that eventually led him to his current role as a Principal Software Engineer at Red Hat, where he has been happily developing free software for more than 10 years. Paul is excited about new container-based infrastructures and all the solutions and problems they bring. He lives in Austin, Texas, with his wife, three children, two dogs, and one mischievous cat.\n\nEditor\n\nSid Orlando is an editor and writer, among some other things. She’s currently making Increment, Stripe’s software engineering/tech magazine, while drawing lots of friendly monsters and raising a giant army of plants in her NYC apartment.\n\nVersion: v1.0.0 (427b8e9)\n\n2",
      "content_length": 1380,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Foreword\n\nAuthor\n\nJames is an author and engineer. His most recent books are The Packer Book; The Terraform Book; The Art of Monitoring; The Docker Book, about the open-source container virtualization technology; and The Logstash Book, about the popular open-source logging tool. James also authored two books about Puppet, Pro Pup- pet and Pulling Strings with Puppet. He is the author of three other books: Pro Linux System Administration, Pro Nagios 2.0, and Hardening Linux.\n\nHe is currently CTO at Empatico and was formerly CTO at Kickstarter, VP of Services and Support at Docker, VP of Engineering at Venmo, and VP of Technical Operations at Puppet. He likes food, wine, books, photography, and cats. He is not overly keen on long walks on the beach or holding hands.\n\nConventions in the book\n\nThis is an inline code statement.\n\nThis is a code block:\n\nListing 1: Sample code block\n\nThis is a code block\n\nLong code strings are broken. If you see . . . in a code block it indicates that the output has been shortened for brevity’s sake.\n\nVersion: v1.0.0 (427b8e9)\n\n3",
      "content_length": 1071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Foreword\n\nCode and Examples\n\nThe code and example configurations contained in the book are available on GitHub at:\n\nhttps://github.com/turnbullpress/prometheusbook-code\n\nColophon\n\nThis book was written in Markdown with a large dollop of LaTeX. It was then converted to PDF and other formats using PanDoc (with some help from scripts written by the excellent folks who wrote Backbone.js on Rails).\n\nErrata\n\nPlease email any errata you find to james+errata@lovedthanlost.net.\n\nDisclaimer\n\nThis book is presented solely for educational purposes. The author is not offering it as legal, accounting, or other professional services advice. While best efforts have been used in preparing this book, the author makes no representations or warranties of any kind and assumes no liabilities of any kind with respect to the accuracy or completeness of the contents and specifically disclaims any implied warranties of merchantability or fitness of use for a particular purpose. The author shall not be held liable or responsible to any person or entity with respect to any loss or incidental or consequential damages caused, or alleged to have been caused, directly or indirectly, by the information or programs contained herein. Every company is different and the advice and strategies contained herein may\n\nVersion: v1.0.0 (427b8e9)\n\n4",
      "content_length": 1326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Foreword\n\nnot be suitable for your situation. You should seek the services of a competent professional before beginning any infrastructure project.\n\nCopyright\n\nSome rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechan- ical or photocopying, recording, or otherwise, for commercial purposes without the prior permission of the publisher.\n\nFigure 1: License\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial- NoDerivs 3.0 Unported License. To view a copy of this license, visit here.\n\n© Copyright 2018 - James Turnbull & Turnbull Press\n\nFigure 2: ISBN\n\nVersion\n\nThis is version v1.0.0 (427b8e9) of Monitoring with Prometheus.\n\nVersion: v1.0.0 (427b8e9)\n\n5",
      "content_length": 783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Chapter 1\n\nIntroduction\n\nThis book is an introduction to Prometheus, an open-source monitoring system. Prometheus provides real-time collection of time series data from your applica- tions backed by a powerful rules engine to help identify the information you In the next chapter we’ll introduce you to need to monitor your environment. Prometheus and its architecture and components. We’ll use Prometheus in the book to take you through building a monitoring environment, with a focus on monitoring dynamic cloud, Kubernetes, and container environments. We’ll also look at instrumenting applications and using that data for alerting and visualiza- tion.\n\nThis is also a book about monitoring in general—so, before we introduce you to Prometheus, we’re going to take you through some monitoring basics. We’ll go through what monitoring is, some approaches to monitoring, and we’ll explain some terms and concepts that we’ll rely on later in this book.\n\nWhat is monitoring?\n\nFrom a technology perspective, monitoring is the tools and processes by which you measure and manage your technology systems. But monitoring is much more\n\n6",
      "content_length": 1130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Chapter 1: Introduction\n\nthan that. Monitoring provides the translation to business value from metrics generated by your systems and applications. Your monitoring system translates those metrics into a measure of user experience. That measure provides feedback to the business to help ensure it’s delivering what customers want. The measure also provides feedback to technology, as we’ll define below, to indicate what isn’t working and what’s delivering an insufficient quality of service.\n\nA monitoring system has two customers:\n\nTechnology • The business\n\nTechnology as a customer\n\nThe first customer of your monitoring system is Technology. That’s you, your team, and the other folks who manage and maintain your technology environment (you might also be called Engineering or Operations or DevOps or Site Reliability Engineering). You rely on monitoring to let you know the state of your technol- ogy environment. You also use monitoring quite heavily to detect, diagnose, and help resolve faults and other issues in your technology environment, preferably before it impacts your users. Monitoring contributes much of the data that in- forms your critical product and technology decisions, and measures the success of those projects. It’s a foundation of your product management life cycle and your relationship with your internal customers, and it helps demonstrate that the business’s money is being well spent. Without monitoring you’re winging it at best—and at worst being negligent.\n\n NOTE There’s a great diagram from Google’s SRE book that shows how\n\nmonitoring is the foundation of the hierarchy of building and managing applica- tions.\n\nVersion: v1.0.0 (427b8e9)\n\n7",
      "content_length": 1681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Chapter 1: Introduction\n\nThe business as a customer\n\nThe business is the second customer of your monitoring. Your monitoring exists to support the business—and to make sure it continues to do business. Monitoring provides the reporting that allows the business to make good product and tech- nology investments. Monitoring also helps the business measure the value that technology delivers.\n\nMonitoring fundamentals\n\nMonitoring should be a core tool for managing infrastructure and your business. Monitoring should also be mandatory, built, and deployed with your applications. Without it you will not be able to understand the state of your world, readily diagnose problems, capacity plan, or provide information to your organization about performance, costs, or status.\n\nAn excellent exposition of this foundation is the Google service hierarchy chart we mentioned earlier.1\n\n1Site Reliability Engineering, edited by Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall\n\nRichard Murphy (O’Reilly). Copyright 2016 Google, Inc., 978-1-491-92912-4.\n\nVersion: v1.0.0 (427b8e9)\n\n8",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Chapter 1: Introduction\n\nFigure 1.1: Service hierarchy\n\nBut monitoring can be hard to implement well, and it can very easily be bad if you’re monitoring the wrong things or in the wrong way. There are some key monitoring anti-patterns and mitigation:\n\nMonitoring as afterthought\n\nIn any good application development methodology, it’s a good idea to identify what you want to build before you build it. Sadly, there’s a common anti-pattern\n\nVersion: v1.0.0 (427b8e9)\n\n9",
      "content_length": 468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Chapter 1: Introduction\n\nof considering monitoring, and other operational functions like security, as value- add components of your application rather than core features. Monitoring, like security, is a core feature of your applications. If you’re building a specification or user stories for your application, include metrics and monitoring for each com- ponent of your application. Don’t wait until the end of a project or just before deployment. I guarantee you’ll miss something that needs to be monitored.\n\n TIP See the discussion about automation and self-service below for ideas on\n\nhow to make this process easier.\n\nMonitoring by rote\n\nMany environments create cargo cult monitoring checks for all your applications. A team reuses the checks they have built in the past rather than evolving those checks for a new system or application. A common example is to monitor CPU, memory, and disk on every host, but not the key services that indicate the appli- cation that runs on the host is functional. If an application can go down without you noticing, even with monitoring in place, then you need to reconsider what you are monitoring.\n\nA good approach to your monitoring is to design a top-down monitoring plan based on value. Identify the parts of the application that deliver value and monitor those first, working your way down the stack.\n\nVersion: v1.0.0 (427b8e9)\n\n10",
      "content_length": 1381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Chapter 1: Introduction\n\nFigure 1.2: Monitoring design\n\nStart with business logic and business outputs, move down to application logic, and finally into infrastructure. This doesn’t mean you shouldn’t collect infrastruc- ture or operating system metrics—they provide value in diagnostics and capacity planning—but you’re unlikely to need them to report the value of your applica- tions.\n\n NOTE If you can’t start with business metrics, then start monitoring close\n\nVersion: v1.0.0 (427b8e9)\n\n11",
      "content_length": 495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Chapter 1: Introduction\n\nto the user. They are the ultimate customer and their experience is what drives your business. Understanding what their experience is and detecting when they have issues is valuable in its own right.\n\nNot monitoring for correctness\n\nAnother common variant of this anti-pattern is to monitor the status of services on a host but not the correctness. For example, you may monitor if a web appli- cation is running by checking for an HTTP 200 response code. This tells you the application is responding to connections, but not if it’s returning the correct data in response to those requests.\n\nA better approach is monitoring for the correctness of a service first—for example, monitor the content or rates of a business transaction rather than the uptime of the web server it runs on. This allows you to get the value of both: if the content of a service isn’t correct because it is misconfigured, buggy, or broken you’ll see that. If the content isn’t correct because underlying web service goes down, you’ll also know that.\n\nMonitoring statically\n\nA further check anti-pattern is the use of static thresholds—for example, alerting if CPU usage on a host exceeds 80 percent. Checks are often inflexible Boolean logic or arbitrary static in time thresholds. They generally rely on a specific result or range being matched. The checks don’t consider the dynamism of most complex systems. A match or a breach in a threshold may be important or could have been triggered by an exceptional event—or could even be a natural consequence of growth.\n\nArbitrary static thresholds are almost always wrong. Baron Schwartz, CEO of\n\nVersion: v1.0.0 (427b8e9)\n\n12",
      "content_length": 1672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Chapter 1: Introduction\n\ndatabase performance analysis vendor VividCortex, put it well:\n\nThey’re worse than a broken clock, which is at least right twice a day. A threshold is wrong for any given system, because all systems are slightly different, and it’s wrong for any given moment during the day, because systems experience constantly changing load and other cir- cumstances.\n\nTo monitor well we need to look at windows of data, not static points in time, and we need to use smarter techniques to calculate values and thresholds.\n\nNot monitoring frequently enough\n\nIn many monitoring tools, scaling is a challenge or the default check period is set to a high value—for example, only checking an application once every five to 15 minutes. This often results in missing critical events that occur between your checks. You should monitor your applications frequently enough to:\n\nIdentify faults or anomalies. • Meet human response time expectations—you want to find the fault before your users report the fault.\n\nProvide data at sufficient granularity for identifying performance issues and\n\ntrends.\n\nAlways remember to store sufficient historical data to identify performance issues and trends. In many cases this might only need to be days or weeks of data—but it’s impossible to identify a trend or reoccurring problem if you have thrown away the data that shows it.\n\nVersion: v1.0.0 (427b8e9)\n\n13",
      "content_length": 1400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Chapter 1: Introduction\n\nNo automation or self-service\n\nA frequent reason monitoring is poor or not implemented correctly is that it can be hard to implement. If you make it hard for application developers to instrument their applications, collect the data, or visualize its results, they won’t do it. If your monitoring infrastructure is manual or overly complex then fault and issues will result in monitoring gaps, failures, and the potential for you to spend more time fixing and maintaining your monitoring than actually monitoring.\n\nMonitoring implementations and deployments should be automated wherever pos- sible:\n\nDeployments should be managed by configuration management. • Configuration of hosts and services should be via discovery or self-service submission, so new applications can be automatically monitored rather than needing someone to add them.\n\nAdding instrumentation should be simple and based on a pluggable utility pattern, and developers should be able to include a library or the like rather than having to configure it themselves.\n\nData and visualization should be self-service. Everyone who needs to see the outputs of monitoring should be able to query and visualize those outputs. (This is not to say that you shouldn’t build dashboards for people, but rather that if they want more they shouldn’t have to ask you for it.)\n\nGood monitoring summary\n\nGood monitoring should provide:\n\nThe state of the world, from the top (the business) down. • Assistance in fault diagnostics. • A source of information for infrastructure, application development, and business folks.\n\nVersion: v1.0.0 (427b8e9)\n\n14",
      "content_length": 1626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Chapter 1: Introduction\n\nAnd it should be:\n\nBuilt into design and the life cycle of application development and deploy-\n\nment.\n\nAutomated and provided as self-service, where possible.\n\n NOTE This definition of “good” monitoring heavily overlaps with an emerg-\n\ning term: observability. You can read more about this in Cindy Sridharan’s excel- lent blog post on the differences between the two.\n\nLet’s now look at the actual mechanics of monitoring.\n\nMonitoring mechanics\n\nThere are a variety of ways you can monitor. Indeed, you could argue that every- thing from unit testing to checklists are a form of monitoring.\n\n NOTE Lindsay Holmwood has a useful presentation on test-driven monitor-\n\ning that talks about the connection between testing and monitoring. Additionally, Cindy Sridharan’s post on testing microservices draws some interesting parallels between testing and monitoring.\n\nTraditionally, though, the definition of monitoring focuses on checking and mea- suring the state of an application.\n\nVersion: v1.0.0 (427b8e9)\n\n15",
      "content_length": 1037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Chapter 1: Introduction\n\nProbing and introspection\n\nThere are two major approaches to monitoring applications: probing and intro- spection.2 Probing monitoring probes the outside of an application. You query the external characteristics of an application: does it respond to a poll on an open port and return the correct data or response code? An example of probing moni- toring is performing an ICMP check and confirming you have received a response. Nagios is an example of a monitoring system that is largely based around probe monitoring.\n\nIntrospection monitoring looks at what’s inside the application. The application is instrumented and returns measurements of its state, the state of internal com- ponents, or the performance of transactions or events. This is data that shows exactly how your application is functioning, rather than just its availability or the behavior of its surface area. Introspection monitoring either emits events, logs, and metrics to a monitoring tool or exposes this information on a status or health endpoint of some kind, which can then be collected by a monitoring tool.\n\nThe introspection approach provides an idea of the actual running state of ap- plications. It allows you to communicate a much richer, more contextual set of information about the state of your application than probing monitoring does. It also provides a better approach to exposing the information both you and the business require to monitor your application.\n\nThis is not to say that probing monitoring has no place. It is often useful to know the state of external aspects of an application, especially if the application is pro- vided by a third party and if you don’t have insight into its internal operations. It is often also useful to view your application from outside to understand certain types of networking, security, or availability issues. It’s generally recommended to have probing for your safety net, a catchall that something is wrong, but to use introspection to drive reporting and diagnostics. We’ll see some probe monitoring in Chapter 10.\n\n2Some folks call probing and introspection, black-box and white-box monitoring respectively.\n\nVersion: v1.0.0 (427b8e9)\n\n16",
      "content_length": 2199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Chapter 1: Introduction\n\nPull versus push\n\nThere are two approaches to how monitoring checks are executed that are worth briefly discussing. These are the pull versus push approaches.\n\nPull-based systems scrape or check a remote application—for example, an end- point containing metrics or, as from our probing example, a check using ICMP. In push-based systems, applications emit events that are received by the monitoring system.\n\nBoth approaches have pros and cons. There’s considerable debate in monitoring circles about those pros and cons, but for the purposes of many users, the debate is largely moot. Prometheus is primarily a pull-based system, but it also supports re- ceiving events pushed into a gateway. We’ll show you how to use both approaches in this book.\n\nTypes of monitoring data\n\nMonitoring tools can collect a variety of different types of data. That data primar- ily takes two forms:\n\nMetrics — Most modern monitoring tools rely most heavily on metrics to help us understand what’s going on in our environments. Metrics are stored as time series data that record the state of measures of your applications. We’ll see more about this shortly.\n\nLogs — Logs are (usually textual) events emitted from an application. While they’re helpful for letting you know what’s happening, they’re often most useful for fault diagnosis and investigation. We won’t look at logs much in this book, but there are plenty of tools available, like the ELK stack, for collecting and managing log events.\n\nVersion: v1.0.0 (427b8e9)\n\n17",
      "content_length": 1534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Chapter 1: Introduction\n\n TIP I’ve written a book about the ELK stack that might interest you.\n\nAs Prometheus is primarily focused on collecting time series data, let’s take a deeper look at metrics.\n\nMetrics\n\nMetrics always appear to be the most straightforward part of any monitoring archi- tecture. As a result, we sometimes don’t invest quite enough time in understanding what we’re collecting, why we’re collecting it, and what we’re doing with those metrics.\n\nIn a lot of monitoring frameworks, the focus is on fault detection: detecting if a specific system event or state has occurred (this is very much the Nagios style of operation—more on this below). When we receive a notification about a specific system event, usually we go look at whatever metrics we’re collecting, if any, to find out what exactly has happened and why. In this world, metrics are seen as a by-product of, or a supplement to, our fault detection.\n\n TIP See the discussion later in this chapter about notification design for\n\nfurther reasons why this is a challenging problem.\n\nPrometheus changes this idea of “metrics as supplement.” Metrics are the most im- portant part of your monitoring workflow. Prometheus turns the fault–detection— centric model on its head. Metrics provide the state and availability of your envi- ronment and its performance.\n\nVersion: v1.0.0 (427b8e9)\n\n18",
      "content_length": 1367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Chapter 1: Introduction\n\n NOTE This book generally avoids duplicating Boolean status checks when\n\na metric can provide information on both state and performance.\n\nHarnessed correctly, metrics provide a dynamic, real-time picture of the state of your infrastructure that will help you manage and make good decisions about your environment. Additionally, through anomaly detection and pattern analysis, metrics have the potential to identify faults or issues before they occur or before the specific system event that indicates an outage is generated.\n\nSo what’s a metric?\n\nAs metrics and measurement are so critical to our monitoring framework, we’re going to help you understand what metrics are and how to work with them. This is intended to be a simplified background that will allow you to understand what different types of metrics, data, and visualizations will contribute to our monitor- ing framework.\n\nMetrics are measures of properties of components of software or hardware. To make a metric useful we keep track of its state, generally recording data points over time. Those data points are called observations. An observation consists of the value, a timestamp, and sometimes a series of properties that describe the observation, such as a source or tags. A collection of observations is called a time series.\n\nA classic example of time series data we might collect is website visits, or hits. We periodically collect observations about our website hits, recording the number of hits and the times of the observations. We might also collect properties such as the source of a hit, which server was hit, or a variety of other information.\n\nWe generally collect observations at a fixed-time interval—we call this the gran- ularity or resolution. This could range from one second to five minutes to 60\n\nVersion: v1.0.0 (427b8e9)\n\n19",
      "content_length": 1841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Chapter 1: Introduction\n\nminutes or more. Choosing the right granularity at which to record a metric is critical. Choose too coarse a granularity and you can easily miss the detail. For example, sampling CPU or memory usage at five-minute intervals is highly un- likely to identify anomalies in your data. Alternatively, choosing fine granularity can result in the need to store and interpret large amounts of data.\n\nTime series data is a chronologically ordered list of these observations. Time series metrics are often visualized, sometimes with a mathematical function applied, as a two-dimensional plot with data values on the y-axis and time on the x-axis. Often you’ll see multiple data values plotted on the y-axis—for example, the CPU usage values from multiple hosts or successful and unsuccessful transactions.\n\nFigure 1.3: A sample plot\n\nThese plots can be incredibly useful. They provide us with a visual representation of critical data that is (relatively) easy to interpret, certainly with more facility than perusing the same data in the form of a list of values. They also present us with a historical view of whatever we’re monitoring: they show us what has changed and when. We can use both of these capabilities to understand what’s happening in our environment and when it happened.\n\nVersion: v1.0.0 (427b8e9)\n\n20",
      "content_length": 1333,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Chapter 1: Introduction\n\nTypes of metrics\n\nThere are a variety of different types of metrics you’ll see in the wild.\n\nGauges\n\nThe first type of metric we’ll look at is a gauge. Gauges are numbers that are expected to go up or down. A gauge is essentially a snapshot of a specific mea- surement. The classic metrics of CPU, memory, and disk usage are usually articu- lated as gauges. For business metrics, a gauge might be the number of customers present on a site.\n\nFigure 1.4: A sample gauge\n\nCounters\n\nThe second type of metric we’ll see frequently is a counter. Counters are numbers that increase over time and never decrease. Although they never decrease, coun- ters can sometimes reset to zero and start incrementing again. Good examples of application and infrastructure counters are system uptime, the number of bytes sent and received by a device, or the number of logins. Examples of business coun- ters might be the number of sales in a month or the number of orders received by an application.\n\nVersion: v1.0.0 (427b8e9)\n\n21",
      "content_length": 1035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Chapter 1: Introduction\n\nFigure 1.5: A sample counter\n\nIn this figure we have a counter incrementing over a period.\n\nA useful thing about counters is that they let you calculate rates of change. Each observed value is a moment in time: t. You can subtract the value at t from the value at t+1 to get the rate of change between the two values. A lot of useful information can be understood by understanding the rate of change between two values. For example, the number of logins is marginally interesting, but create a rate from it and you can see the number of logins per second, which should help identify periods of site popularity.\n\nHistograms\n\nA histogram is a metric that samples observations. This is a frequency distribution of a dataset. You group data together—a process called “binning”—and present the groups in a such a way that their relative sizes are visualized. Each observation is counted and placed into buckets. This results in multiple metrics: one for each bucket, plus metrics for the sum and count of all values.\n\nA common visualization of a frequency distribution histogram looks like a bar graph.\n\nVersion: v1.0.0 (427b8e9)\n\n22",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Chapter 1: Introduction\n\nFigure 1.6: A histogram example\n\nHere we see a sample histogram for the frequency distribution of heights. On the y-axis we have the frequency and on the x-axis we have the distribution of heights. We see that for the height 160–165 cm tall there is a distribution of two.\n\n NOTE There’s another metric type, called a summary, which is similar\n\nto a histogram, but it also calculates percentiles. You can read more about im- plementation details and some caveats of histogram and summaries specific to Prometheus.\n\nHistograms can be powerful representations of your time series data and especially useful for visualizing data such as application latencies.\n\nMetric summaries\n\nOften the value of a single metric isn’t useful to us. Instead, visualization of a met- ric requires applying mathematical transformations to it. For example, we might apply statistical functions to our metric or to groups of metrics. Some common functions we might apply include:\n\nVersion: v1.0.0 (427b8e9)\n\n23",
      "content_length": 1013,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Chapter 1: Introduction\n\nCount or n — Counts the number of observations in a specific time interval.\n\nSum — To sum is to add together values from all observations in a specific\n\ntime interval.\n\nAverage — Provides the mean of all values in a specific time interval.\n\nMedian — The median is the dead center of our values: exactly 50 percent\n\nof values are below it, and 50 percent are above it.\n\nPercentiles — Measures the values below which a given percentage of ob-\n\nservations in a group of observations fall.\n\nStandard deviation — Shows standard deviation from the mean in the distri- bution of our metrics. This measures the variation in a data set. A standard deviation of 0 means the distribution is equal to the mean of the data. Higher deviations mean the data is spread out over a range of values.\n\nRates of change — Rates of change representations show the degree of\n\nchange between data in a time series.\n\n TIP This is a brief introduction to these summary methods. We’ll use some\n\nof them in more detail later in the book.\n\nMetric aggregation\n\nIn addition to summaries of specific metrics, you often want to show aggregated views of metrics from multiple sources, such as the disk space usage of all your application servers. The most typical example of this results in multiple metrics being displayed on a single plot. This is useful in identifying broad trends over\n\nVersion: v1.0.0 (427b8e9)\n\n24",
      "content_length": 1411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Chapter 1: Introduction\n\nyour environment. For example, an intermittent fault in a load balancer might result in web traffic dropping off for multiple servers. This is often easier to see in aggregate than by reviewing each individual metric.\n\nFigure 1.7: An aggregated collection of metrics\n\nIn this plot we see disk usage from numerous hosts over a 30-day period. It gives us a quick way to ascertain the current state (and rate of change) of a group of hosts.\n\nUltimately you’ll find that a combination of single and aggregate metrics provide the most representative view of the health of your environment: the former to drill down into specific issues, and the latter to see the high-level state.\n\nLet’s take a deeper dive into types of metric summaries: the whys, why nots, and hows of using averages, the median, standard deviation, percentiles, and other statistical choices.\n\n NOTE This is a high-level overview of some statistical techniques rather\n\nthan a deep dive into the topic. Exploration of some topics may appear overly simplistic to folks with strong statistics or mathematics backgrounds.\n\nAverages\n\nAverages are the de facto metric analysis method. Indeed, pretty much everyone who has ever monitored or analyzed a website or application has used averages.\n\nVersion: v1.0.0 (427b8e9)\n\n25",
      "content_length": 1308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Chapter 1: Introduction\n\nIn the web operations world, for example, many companies live and die by the average response time of their site or API.\n\nAverages are attractive because they are easy to calculate. Let’s say we have a list of seven time series values: 12, 22, 15, 3, 7, 94, and 39. To calculate the average we sum the list of values and divide the total by the number of values in the list.\n\n(12 + 22 + 15 + 3 + 7 + 94 + 39) / 7 = 27.428571428571\n\nWe first sum the seven values to get the total of 192. We then divide the sum by the number of values, here 7, to return the average: 27.428571428571. Seems pretty simple, huh? The devil, as they say, is in the details.\n\nAverages assume there is a normal event or that your data is a normal (or Gaus- sian) distribution—for example, in our average response time, it’s assumed that all events run at equal speed or that response time distribution is roughly bell curved. But this is rarely the case with applications. In fact, there’s an old statistics joke about a statistician who jumps in a lake with an average depth of only 10 inches and nearly drowns…\n\nVersion: v1.0.0 (427b8e9)\n\n26",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Chapter 1: Introduction\n\nFigure 1.8: The flaw of averages - copyright Jeff Danzinger\n\nWhy did he nearly drown? The lake contained large areas of shallow water and some areas of deep water. Because there were larger areas of shallow water, the average depth was lower overall. In the monitoring world the same principal lots of low values in our average distort or hide high values and vice applies: versa. These hidden outliers can mean that while we think most of our users are experiencing a quality service, there may be a significant number that are not.\n\nLet’s look at an example using response times and requests for a website.\n\nVersion: v1.0.0 (427b8e9)\n\n27",
      "content_length": 664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Chapter 1: Introduction\n\nFigure 1.9: Response time average\n\nHere we have a plot showing response time for a number of requests. Calculating the average response time would give us 4.1 seconds. The vast majority of our users would experience a (potentially) healthy 4.1 second response time. But many of our users are experiencing response times of up to 12 seconds, perhaps considerably less acceptable.\n\nLet’s look at another example with a wider distribution of values.\n\nVersion: v1.0.0 (427b8e9)\n\n28",
      "content_length": 502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Chapter 1: Introduction\n\nFigure 1.10: Response time average Mk II\n\nHere our average would be a less stellar 6.8 seconds. But worse, this average is considerably better than the response time received by the majority of our users with a heavy distribution of request times around 9, 10, and 11 seconds long. If we were relying on the average alone, we’d probably think our application was performing a lot better than it is.\n\nMedian\n\nAt this point you might be wondering about using the median. The median is the dead center of our values: exactly 50 percent of values are below it, and 50 percent are above it. If there’s an odd number of values, then the median will be the value in the middle. For the first data set we looked at—12, 22, 15, 3, 7, 94, and 39—the median is 15. If there were an even number of values, the median\n\nVersion: v1.0.0 (427b8e9)\n\n29",
      "content_length": 860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Chapter 1: Introduction\n\nwould be the mean of the two values in the middle. So if we were to remove 39 from our data set to make it even, the median would become 13.5.\n\nLet’s apply this to our two plots.\n\nFigure 1.11: Response time average and median\n\nWe see in our first example figure that the median is 3, which provides an even rosier picture of our data.\n\nIn the second example the median is 8, a bit better but close enough to the average to render it ineffective.\n\nVersion: v1.0.0 (427b8e9)\n\n30",
      "content_length": 501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Chapter 1: Introduction\n\nFigure 1.12: Response time average and median Mk II\n\nYou can probably already see that the problem again here is that, like the mean, the median works best when the data is on a bell curve… And in the real world that’s not realistic.\n\nAnother commonly used technique to identify performance issues is to calculate the standard deviation of a metric from the mean.\n\nStandard deviation\n\nAs we learned earlier in the chapter, standard deviation measures the variation or spread in a data set. A standard deviation of 0 means most of the data is close to the mean. Higher deviations mean the data is more distributed. Standard deviations are represented by positive or negative numbers suffixed with the sigma symbol—for example, 1 sigma is one standard deviation from the mean.\n\nVersion: v1.0.0 (427b8e9)\n\n31",
      "content_length": 830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Chapter 1: Introduction\n\nLike the mean and the median, however, standard deviation works best when the In a normal distribution there’s a simple way of data is a normal distribution. articulating the distribution: the empirical rule, also known as the 68–95–99.7 rule or three-sigma rule. Within the rule, one standard deviation or 1 to -1 will represent 68.27 percent of all transactions on either side of the mean, two standard deviations or 2 to -2 would be 95.45 percent, and three standard deviations will represent 99.73 percent of all transactions.\n\nFigure 1.13: The empirical rule\n\nMany monitoring approaches take advantage of the empirical rule and trigger on transactions or events that are more than two standard deviations from the mean, potentially catching performance outliers. In instances like our two previous ex- amples, however, the standard deviation isn’t overly helpful either. And without\n\nVersion: v1.0.0 (427b8e9)\n\n32",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Chapter 1: Introduction\n\na normal distribution of data, the resulting standard deviation can be highly mis- leading.\n\nThus far, our methods for identifying anomalous data in our metrics haven’t been overly promising. But all is not lost! Our next method, percentiles, offer a little more hope.\n\nPercentiles\n\nPercentiles measure the values below which a given percentage of observations in a group of observations fall. Essentially they look at the distribution of values across your data set. For example, the median we looked at above is the 50th percentile (or p50). In the median, 50 percent of values fall below and 50 per- cent above. For metrics, percentiles make a lot of sense because they make the distribution of values easy to grasp. For example, the 99th-percentile value of 10 milliseconds for a transaction is easy to interpret: 99 percent of transactions were completed in 10 milliseconds or less, and 1 percent of transactions took more than 10 milliseconds.\n\n TIP Percentiles are a type of quantile.\n\nPercentiles are ideal for identifying outliers. If a great experience on your site is a response time of less than 10 milliseconds then 99 percent of your users are having a great experience—but 1 percent of them are not. Once you’re aware of this, you can focus on addressing the performance issue that’s causing a problem for that 1 percent.\n\nLet’s apply this to our previous request and response time graphs and see what appears. We’ll apply two percentiles, the 75th and 99th percentiles, to our first example data set.\n\nVersion: v1.0.0 (427b8e9)\n\n33",
      "content_length": 1573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Chapter 1: Introduction\n\nFigure 1.14: Response time average, median, and percentiles\n\nWe see that the 75th percentile is 5.5 seconds. That indicates that 75 percent com- pleted in 5.5 seconds, and 25 percent were slower than that. Still pretty much in line with the earlier analysis we’ve examined for the data set. The 99th percentile, on the other hand, shows 10.74 seconds. This means 99 percent of users had re- quest times of less than 10.74 seconds, and 1 percent had more than 10.74 seconds. This gives us a real picture of how our application is performing. We can also use the distribution between p75 and p99. If we’re comfortable with 99 percent of users getting 10.74 second response times or better and 1 percent being slower than that, then we don’t need to consider any further tuning. Alternatively, if we want a uniform response, or if we want to lower that 10.74 seconds across our distribution, we’ve now identified a pool of transactions we can trace, profile, and improve. As we adjust the performance, we’ll also be able to see the p99 response time improve.\n\nVersion: v1.0.0 (427b8e9)\n\n34",
      "content_length": 1111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Chapter 1: Introduction\n\nThe second data set is even more clear.\n\nFigure 1.15: Response time average, median, and percentiles Mk II\n\nThe 75th percentile is 10 seconds and the 99th percentile is 12 seconds. Here the 99th percentile provides a clear picture of the broader distribution of our transactions. This is a far more accurate reflection of the outlying transactions from our site. We now know that—as opposed to what the mean response times would imply—not all users are enjoying an adequate experience. We can use this data to identify elements of our application we can potentially improve.\n\nPercentiles, however, aren’t perfect all the time. We recommend graphing several combinations of metrics to get a clear picture of the data. For example, when measuring latency it’s often a good idea to display a graph that shows:\n\nThe 50th percentile, or median. • The 99th percentile.\n\nVersion: v1.0.0 (427b8e9)\n\n35",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Chapter 1: Introduction\n\nThe max value.\n\nThe addition of the max value helps visualize the upward bounds of the metric you are measuring. It’s again not perfect though—a high max value can dwarf other values in a graph.\n\nWe’re going to apply percentiles and other calculations later in the book as we start to build checks and collect metrics.\n\nMonitoring methodologies\n\nWe’ll also make use of a combination of several monitoring methodologies on top of our metrics and metric aggregations to help focus our monitoring. We’re going to combine elements of two monitoring methodologies:\n\nBrendan Gregg’s USE or Utilization Saturation and Errors Method, which\n\nfocuses on host-level monitoring.\n\nGoogle’s Four Golden Signals, which focus on application-level monitoring.\n\nMonitoring methodologies provide guidelines that allow you to narrow down and focus on specific metrics in the sea of time series you collect. When com- bined, these two frameworks—one focused on host-level performance, the other on application-level performance—represent a reasonably holistic view of your environment that should assist you in tackling any issues.\n\nThe USE Method\n\nThe USE, or Utilization Saturation and Errors, Method was developed by Brendan Gregg, a kernel and performance engineer at Netflix. The methodology proposes creating a checklist for server analysis that allows the fast identification of issues.\n\nVersion: v1.0.0 (427b8e9)\n\n36",
      "content_length": 1428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "Chapter 1: Introduction\n\nYou work down the checklist to identify common performance issues, making use of data collected from your environment.\n\nFor every resource, check\n\nThe USE Method can be summarized as: utilization, saturation, and errors. The method is most effective for the monitoring of resources that suffer performance issues under high utilization or saturation. Let’s quickly define each term to help understand this.\n\nA resource - A component of a system. In Gregg’s definition of the model it’s traditionally a physical server component like CPUs, disks, etc., but many folks also include software resources in the definition.\n\nUtilization - The average time the resource is busy doing work. It’s usually\n\nexpressed as a percentage over time.\n\nSaturation - The measure of queued work for a resource, work it can’t pro-\n\ncess yet. This is usually expressed as queue length.\n\nErrors - The scalar count of error events for a resource.\n\nWe combine these definitions to create a checklist of the resources and an ap- proach to monitor each element of the methodology: utilization, saturation, or errors. How might this work? Well, let’s say we have a serious performance issue, and we want to dive into some diagnosis. We refer to our checklist and check each element for each monitored component. In our example, we’ll start with CPU:\n\nCPU utilization as a percentage over time. • CPU saturation as the number of processes awaiting CPU time. • Errors, generally less important for the CPU resource.\n\nAnd then, perhaps, memory:\n\nMemory utilization as a percentage over time. • Memory saturation measured via monitoring swapping. • Errors, generally less important here but also can be captured.\n\nVersion: v1.0.0 (427b8e9)\n\n37",
      "content_length": 1736,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Chapter 1: Introduction\n\nAnd so on through other components on the system until we’ve identified the bottleneck or signal that points us to the issue.\n\nWe’ll see more of this in Chapter 4 when we look at monitoring some system-level metrics.\n\n TIP You can find an example checklist for a Linux system here.\n\nThe Google Four Golden Signals\n\nThe Google Four Golden Signals come out of the Google SRE book. They take a similar approach to the USE Method, specifying a series of general metric types to monitor. Rather than being system-level-focused time series, the metric types in this methodology are more application or user-facing:\n\nLatency - The time taken to service a request, distinguishing between the latency of successful and failed requests. A failed request, for example, might return with very low latency skewing your results.\n\nTraffic - The demand on your system—for example, HTTP requests per\n\nsecond or transactions for a database system.\n\nErrors - The rate that requests fail, whether explicit failures like HTTP 500 errors, implicit failures like wrong or invalid content being returned, or policy-based failures—for instance if you’ve mandated that failures over 30ms should be considered errors.\n\nSaturation - The “fullness” of your application or the resources that are constraining it—for example, memory or IO. This also includes impending saturation, such as a rapidly filling disk.\n\nVersion: v1.0.0 (427b8e9)\n\n38",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Chapter 1: Introduction\n\nUsing the golden signals is easy. Select high-level metrics that match each signal and build alerts for them. If one of those signals becomes an issue then an alert will be generated and you can diagnose or resolve the issue.\n\nWe’ll see golden signals again in Chapter 7 and 8 when we look at monitoring some applications.\n\n TIP There’s a related framework called RED—or Rate, Errors, and Duration,\n\ndeveloped by the team at Weaveworks, that might also interest you.\n\nContextual, useful alerts and notifications\n\nAlerts and notifications are the primary output from monitoring tools. So what’s the difference between an alert and a notification? An alert is raised when some- thing happens—for example, when a threshold is reached. This, however, doesn’t mean anyone’s been told about the event. That’s where notifications come in. A notification takes the alert and tells someone or something about it: an email is sent, an SMS is triggered, a ticket is opened, or the like. It may seem like this should be a really simple domain, but it contains a lot of complexity and is often poorly implemented and managed.\n\nTo build a good notification system you need to consider the basics of:\n\nWhat problems to notify on. • Who to tell about a problem. • How to tell them. • How often to tell them. • When to stop telling them, do something else, or escalate to someone else.\n\nVersion: v1.0.0 (427b8e9)\n\n39",
      "content_length": 1425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Chapter 1: Introduction\n\nIf you get it wrong and generate too many notifications then people will be unable to take action on them all and may even mute them. We all have war stories of mailbox folders full of thousands of notification emails from monitoring systems.3 Sometimes so many notifications are generated that you suffer from alert fatigue and ignore them (or worse, conduct notification management via Select All -> Delete). Consequently, you’re likely to miss actual critical notifications when they are sent.\n\nMost importantly, you need to work out what to tell whoever is receiving the notifications. Notifications are usually the sole signal that you receive to tell you that something is amiss or requires your attention. They need to be concise, articulate, accurate, digestible, and actionable. Designing your notifications to actually be useful is critical. Let’s make a brief digression and see why this matters. We’ll look at a typical Nagios notification for disk space.\n\nListing 1.1: Sample Nagios notification\n\nPROBLEM Host: server.example.com Service: Disk Space\n\nState is now: WARNING for 0d 0h 2m 4s (was: WARNING) after 3/3 checks\n\nNotification sent at: Thu Aug 7th 03:36:42 UTC 2015 ( notification number 1)\n\nAdditional info: DISK WARNING - free space: /data 678912 MB (9% inode=99%)\n\nImagine you’ve just received this notification at 3:36 a.m. What does it tell you? That we have a host with a disk space warning. And that the /data volume is 91 percent full. At first glance this seems useful, but in reality it’s not all that practical. First, is this a sudden increase, or has this grown gradually? And what’s the rate\n\n3Or cron.\n\nVersion: v1.0.0 (427b8e9)\n\n40",
      "content_length": 1693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Chapter 1: Introduction\n\nof expansion? (Consider that 9 percent disk space free on a 1 GB partition is quite different from 9 percent disk space free on a 1 TB disk.) Can you ignore or mute this notification or do you need to act now? Without the additional context your ability to take action on the notification is limited, and you need to invest considerably more time to gather context.\n\nIn our framework we’re going to focus on:\n\nMaking notifications actionable, clear, and articulate. Just the use of notifi- cations written by humans rather than by computers can make a significant difference in the clarity and utility of those notifications.\n\nAdding context to notifications. We’re going to send notifications that con-\n\ntain additional information about the component we’re notifying on.\n\nOnly sending those notifications that make sense.\n\n TIP The simplest advice we can give here is to remember/ notifications are\n\nread by humans, not computers. Design them accordingly.\n\nVisualization\n\nVisualizing data is both an incredibly powerful analytic and interpretive technique and an amazing learning tool. Metrics and their visualizations are often tricky to interpret. Humans tend towards apophenia—the perception of meaningful pat- terns within random data—when viewing visualizations. This often leads to mak- ing sudden leaps from correlation to causation, and can be further exacerbated by the granularity and resolution of our available data, how we choose to represent it, and the scale on which we represent it.\n\nVersion: v1.0.0 (427b8e9)\n\n41",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Chapter 1: Introduction\n\nOur ideal visualizations will clearly show the data, with an emphasis on high- lighting substance over visuals. In this book we’re not going to look at a lot of visualizations but where we have, we’ve tried to build visuals that subscribe to these broad rules:\n\nClearly show the data. • Induce the viewer to think about the substance, not the visuals. • Avoid distorting the data. • Make large data sets coherent. • Allow changing perspectives of granularity without impacting comprehen- sion.\n\nWe’ve drawn most of these ideas from Edward Tufte’s The Visual Display of Quan- titative Information and thoroughly recommend reading it to help you build good visualizations.\n\nThere’s also a great post from the Datadog team on visualizing time series data that is worth reading.\n\nBut didn’t you write that other book?\n\nAs many folks know, I am one of the maintainers of Riemann, an event stream processor focused on monitoring distributed systems. I wrote a book about mon- itoring in which I used Riemann as a centerpiece to explore new monitoring pat- terns and approaches. In the book, I described an architecture of introspection monitoring (with some selective probing monitoring).\n\nIn the book I also focused on push-based monitoring over pull-based monitoring. There are lots of reasons I favor the push model versus the pull model but, as we mentioned earlier, for many folks the distinction is arbitrary. Indeed, many of the concerns of either approach don’t impact implementation due to issues like scale. Other concerns, like many arguments over implementation or tool choice,\n\nVersion: v1.0.0 (427b8e9)\n\n42",
      "content_length": 1639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Chapter 1: Introduction\n\ndon’t change the potential success of the implementation. I’m a strong exponent of using tools that work for you, rather than unreviewed adoption of the latest trend or dogmatism.\n\nIt’s this lack of distinction for folks, and a desire not to be dogmatic about my beliefs, that has encouraged me to write another book, this one about one of the leading pull-based monitoring tools: Prometheus. In the Art of Monitoring I wrote:\n\nPerhaps a better way of looking at these tool choices is that they are merely ways to articulate the change in monitoring approach that is proposed in this book. They are the trees in the woods. If you find other tools that work better for you and achieve the same results then we’d love to hear from you. Write a blog post, give a talk, or share your configuration.\n\nHence, you’ll see much of the methodology of The Art of Monitoring reflected in this book—indeed, much of this chapter is a distillation of some of the book’s elements. We’re taking the core motivation of that book—a better way to monitor applications—and applying it with an alternative tool, and a different architecture and approach.\n\nWhat’s in the book?\n\nThis book covers an introduction to a good approach to monitoring, and then uses Prometheus to instantiate that monitoring approach. By the end of the book you should have a readily extensible and scalable monitoring platform.\n\nThe book assumes you want to build, rather than buy a monitoring platform. There are a lot of off-the-shelf Software-as-a-Service (SaaS) and cloud-based mon- itoring solutions that might work for you. There’s even some hosted Prometheus options. For many folks, this is a better solution when starting out with moni- toring rather than investing in building their own. It’s our view that ultimately\n\nVersion: v1.0.0 (427b8e9)\n\n43",
      "content_length": 1837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Chapter 1: Introduction\n\nmost folks, as their environment and requirements grow, will discover that these platforms don’t quite suit their needs, and will build (some) monitoring in house. But whether that’s the case for you is something you’ll need to determine yourself.\n\nIn this book, we’re going to introduce you to the Prometheus monitoring platform piece by piece, starting with monitoring node and container metrics, service dis- covery, alerting, and then instrumenting and monitoring applications. The book will try to cover a representative sample of technologies you’re likely to manage yourself but that can be adapted to a wide variety of other environments and stacks.\n\nThe book’s chapters are:\n\nChapter 1: This introduction. • Chapter 2: Introducing Prometheus. • Chapter 3: Installing Prometheus. • Chapter 4: Monitoring nodes and containers. • Chapter 5: Service discovery. • Chapter 6: Alerting and AlertManager. • Chapter 7: Scaling. • Chapter 8: Instrumenting an application. • Chapter 9: Logging as instrumentation. • Chapter 10: Probing. • Chapter 11: Pushgateway. • Chapter 12: Monitoring a stack - Kubernetes. • Chapter 13: Monitoring a stack - Application.\n\nSummary\n\nIn this chapter we introduced you to modern monitoring approaches. We laid out the details of several types of monitoring implementations. We discussed what makes good and bad monitoring, and how to avoid poor monitoring outcomes.\n\nVersion: v1.0.0 (427b8e9)\n\n44",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Chapter 1: Introduction\n\nWe also introduced the details of time series data and metrics to you. We broke down the types of data that can be delivered as metrics. And we demonstrated some common mathematical functions applied to metrics to manipulate and ag- gregate them.\n\nIn the next chapter, we’re going to introduce you to Prometheus and give some insight into its architecture and components.\n\nVersion: v1.0.0 (427b8e9)\n\n45",
      "content_length": 427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Chapter 2\n\nIntroduction to Prometheus\n\nIn this chapter we’re going to introduce you to Prometheus, its origins, and give you an overview of:\n\nWhere Prometheus came from and why. • Prometheus architecture and design. • The Prometheus data model. • The Prometheus ecosystem.\n\nThis should give you an introduction and understanding of what Prometheus is and where it fits into the monitoring universe.\n\n NOTE This book focuses on Prometheus version 2.0 and later. Much of\n\nthe book’s information will not work for earlier releases.\n\n46",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Chapter 2: Introduction to Prometheus\n\nThe Prometheus backstory\n\nOnce upon a time there was a company in Mountain View, California, called Google. They ran a swathe of products, most famously an advertising erm, search engine platform. To run these diverse products they built a platform called Borg. The Borg system is “a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines.”1 The open-source container manager Kubernetes owes much of its heritage to Borg. Shortly after Borg was deployed at Google, folks realized that this complexity required a similarly capable monitoring system. Google built that system and called it Borgmon. Borgmon is a real-time– focused time series monitoring system that uses that data to identify issues and alert on them.\n\n NOTE Neither Borg nor Borgmon have ever been open sourced. It’s only\n\nrecent that one can learn about how they work. You can read a bit more about it in the Practical Alerting chapter of the SRE book.\n\nIt was originally devel- Prometheus owes its inspiration to Google’s Borgmon. oped by Matt T. Proud, an ex-Google SRE, as a research project. After Proud joined SoundCloud, he teamed up with another engineer, Julius Volz, to develop Prometheus in earnest. Other developers joined the effort, and it continued de- velopment internally at SoundCloud, culminating in a public release in January 2015.\n\nLike Borgmon, Prometheus was primarily designed to provide near real-time in- trospection monitoring of dynamic cloud- and container-based microservices, ser- vices, and applications. SoundCloud was an earlier adopter of these architec-\n\n1Abhishek Verma et al, Large-scale cluster management at Google with Borg, EuroSys, 2015.\n\nVersion: v1.0.0 (427b8e9)\n\n47",
      "content_length": 1841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Chapter 2: Introduction to Prometheus\n\ntural patterns, and Prometheus was built to respond to those needs. These days, Prometheus is used by a wide range of companies, generally for similar monitoring needs, but also for monitoring of more traditional architectures.\n\nPrometheus is focused on what’s happening right now, rather than tracking data over weeks or months. This is based on the premise that the majority of monitor- ing queries and alerts are generated from recent, usually less than day-old, data. Facebook validated this in a paper on Gorilla, its internal time series database. Facebook discovered that 85 percent of queries were for data less than 26 hours old. Prometheus assumes that the problems you may be trying to fix are likely recent, hence the most useful data is the most recent data. This is reflected in the powerful query language available and the typically limited retention period for monitoring data.\n\nPrometheus is written in Go, open source, and licensed under the Apache 2.0 license. It is incubated under the Cloud Native Computing Foundation.\n\nPrometheus architecture\n\nPrometheus works by scraping or pulling time series data exposed from appli- cations. The time series data is exposed by the applications themselves often via client libraries or via proxies called exporters, as HTTP endpoints. Exporters and client libraries exist for many languages, frameworks, and open-source for web servers like Apache and databases like applications—for example, MySQL.\n\nPrometheus also has a push gateway you can use to receive small volumes of data—for example, data from targets that can’t be pulled, like transient jobs or targets behind firewalls.\n\nVersion: v1.0.0 (427b8e9)\n\n48",
      "content_length": 1713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Chapter 2: Introduction to Prometheus\n\nFigure 2.1: Prometheus architecture\n\nMetric collection\n\nPrometheus calls the source of metrics it can scrape endpoints. An endpoint usu- ally corresponds to a single process, host, service, or application. To scrape an endpoint, Prometheus defines configuration called a target. This is the informa- tion required to perform the scrape—for example, how to connect to it, what metadata to apply, any authentication required to connect, or other information that defines how the scrape will occur. Groups of targets are called jobs. Jobs are usually groups of targets with the same role—for example, a cluster of Apache servers behind a load balancer. That is, they’re effectively a group of like pro-\n\nVersion: v1.0.0 (427b8e9)\n\n49",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Chapter 2: Introduction to Prometheus\n\ncesses.\n\nThe resulting time series data is collected and stored locally on the Prometheus server. It can also be sent from the server to external storage or to another time series database.\n\nService discovery\n\nDiscovery of resources to be monitored can be handled in a variety of ways in- cluding:\n\nA user-provided static list of resources. • File-based discover—for example, using a configuration management tool to generate a list of resources that are automatically updated in Prometheus. • Automated discovery—for example, querying a data store like Consul, run- ning instances in Amazon or Google, or using DNS SRV records to generate a list of resources.\n\n TIP We’ll see how to use a variety of service discovery approaches in Chapter\n\n5.\n\nAggregation and alerting\n\nThe server can also query and aggregate the time series data, and can create rules to record commonly used queries and aggregations. This allows you to create new time series from existing time series—for example, calculating rates and ratios or producing aggregations like sums. This saves you having to recreate common ag- gregations, say ones you use for debugging, and the precomputation is potentially more performant than running the query each time it is required.\n\nVersion: v1.0.0 (427b8e9)\n\n50",
      "content_length": 1314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Chapter 2: Introduction to Prometheus\n\nPrometheus can also define rules for alerting. These are criteria—for example, a resource time series starting to show escalated CPU usage—that can be configured to trigger an alert when the criteria are met. The Prometheus server doesn’t come with an inbuilt alerting tool. Instead, alerts are pushed from the Prometheus server to a separate server called Alertmanager. Alertmanager can manage, consolidate, and distribute alerts to a variety of destinations—for example, it can trigger an email when an alert is raised, but prevent duplicates.\n\n TIP We’ll see a lot more about Alertmanager in Chapter 6.\n\nQuerying data\n\nThe Prometheus server also comes with an inbuilt querying language, PromQL; an expression browser; and a graphing interface you can use to explore the data on your server.\n\nVersion: v1.0.0 (427b8e9)\n\n51",
      "content_length": 864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Chapter 2: Introduction to Prometheus\n\nFigure 2.2: Prometheus expression browser\n\nAutonomy\n\nEach Prometheus server is designed to be as autonomous as possible. It is designed to scale to millions of time series from many thousands of hosts. Its data storage format is designed to keep disk use down and provide fast retrieval of time series during queries and aggregations.\n\n TIP A good helping of memory (Prometheus does a lot in memory) and SSD\n\ndisks are recommended for Prometheus servers, for speed and reliability. You are using SSDs, right?\n\nVersion: v1.0.0 (427b8e9)\n\n52",
      "content_length": 579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Chapter 2: Introduction to Prometheus\n\nRedundancy and high availability\n\nRedundancy and high availability center on alerting resilience rather than data durability. The Prometheus team recommends deploying Prometheus servers to specific purposes and teams rather than to a single monolithic Prometheus server. If you do want to deploy in an HA configuration, two or more identically con- figured Prometheus servers collect the time series data, and any alerts generated are handled by a highly available Alertmanager configuration that deduplicates alerts.\n\nFigure 2.3: Redundant Prometheus architecture\n\nVersion: v1.0.0 (427b8e9)\n\n53",
      "content_length": 634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Chapter 2: Introduction to Prometheus\n\n TIP We’ll see how to implement this configuration in Chapter 7.\n\nVisualization\n\nVisualization is provided via an inbuilt expression browser and integration with the open-source dashboard Grafana. Other dashboards are also supported.\n\n TIP We’ll get to know this integration in Chapter 4.\n\nThe Prometheus data model\n\nAs we’ve seen, Prometheus collects time series data. To handle this data it has a multi-dimensional time series data model. The time series data model combines time series names and key/value pairs called labels; these labels provide the di- mensions. Each time series is uniquely identified by the combination of time series name and any assigned labels.\n\nMetric names\n\nThe time series name usually describes the general nature of the time series data being collected—for example, website_visits_total as the total number of web- site visits.\n\nThe name can contain ASCII letters, digits, underscores, and colons.\n\nVersion: v1.0.0 (427b8e9)\n\n54",
      "content_length": 1002,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Chapter 2: Introduction to Prometheus\n\nLabels\n\nLabels enable the Prometheus dimensional data model. They add context to a specific time series. For example, our total_website_visits time series could have labels that identify the name of the website, IP of the requester, or other dimensions that specifically identify that time series and connect it to its source. Prometheus can query on these dimensions to select one time series, groups of time series, or all relevant time series.\n\nLabels come in two broad types: instrumentation labels and target labels. Instru- mentation labels come from the resource being monitored—for example, for a HTTP-related time series, a label might show the specific HTTP verb used. These labels are added to the time series before they are scraped, such as by a client or exporter. Target labels relate more to your architecture—they might iden- tify the data center where the time series originated. Target labels are added by Prometheus during and after the scrape.\n\nA time series is identified by both its name and labels (although technically the name itself is also a label called __name__). If you add or change a label on a time series, Prometheus treats this as a new time series.\n\n TIP You can generally think of labels as tags, albeit in key/value form and\n\nwhere a new tag creates a new time series.\n\nLabel names can contain ASCII letters, digits, and underscores.\n\n TIP Label names prefixed with __ are reserved for internal Prometheus use.\n\nVersion: v1.0.0 (427b8e9)\n\n55",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Chapter 2: Introduction to Prometheus\n\nSamples\n\nThe actual value of the time series is called a sample. It consists of:\n\nA float64 value. • A millisecond-precision timestamp.\n\nNotation\n\nCombining these elements we can see how Prometheus represents a time series as notation.\n\nListing 2.1: Time series notation\n\n<time series name>{<label name>=<label value>, ...}\n\nFor example, our total_website_visits time series, with attached labels, might look like:\n\nListing 2.2: Example time series\n\ntotal_website_visits{site=\"MegaApp\", location=\"NJ\", instance=\" webserver\",job=\"web\"}\n\nThe time series name is represented first, with a map of key/value pair labels attached. All time series generally have an instance label, which identifies the source host or application, and a job label, which contains the name of the job that scraped the specific time series.\n\nVersion: v1.0.0 (427b8e9)\n\n56",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Chapter 2: Introduction to Prometheus\n\n NOTE This is roughly the same notation that OpenTSDB uses, which in\n\nturn was influenced by Borgmon.\n\nMetrics retention\n\nPrometheus is designed for short-term monitoring and alerting needs. By default, it keeps 15 days of time series locally in its database. If you want to retain data for longer, the recommended approach is to send the required data to remote, third-party platforms. Prometheus has the ability to write to external data stores, which we’ll see in Chapter 7.\n\nSecurity model\n\nPrometheus can be configured and deployed in a wide variety of ways. It makes two broad assumptions about trust:\n\nThat untrusted users will be able to access the Prometheus server’s HTTP\n\nAPI and hence all the data in the database.\n\nThat only trusted users will have access to the command line, configuration files, rule files, and runtime configuration of Prometheus and its compo- nents.\n\n TIP Since Prometheus 2.0, some administrative elements of the HTTP API\n\nare disabled by default.\n\nVersion: v1.0.0 (427b8e9)\n\n57",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Chapter 2: Introduction to Prometheus\n\nAs such, Prometheus and its components do not provide any server-side authen- tication, authorization, or encryption. If you are working in a more secure envi- ronment you’ll need to implement additional controls yourself—for instance by front-ending the Prometheus server with a reverse proxy or by proxying your ex- porters. Because of the huge potential variations in configuration, this book does not document how to do this.\n\nPrometheus ecosystem\n\nThe Prometheus ecosystem has a mix of components provided by the Prometheus project itself and a rich collection of open-source integrations and tools. The heart of the ecosystem is the Prometheus server that we’ll see in more detail in the next chapter. Also available is Alertmanager, which provides an alerting manager and engine for Prometheus.\n\nThe Prometheus project also includes a collection of exporters, used to instru- ment applications and services and to expose relevant metrics on an endpoint for scraping. Common tools—like web servers, databases, and the like—are sup- ported by core exporters. Many other exporters are available open source from the Prometheus community.\n\nPrometheus also published a collection of client libraries, used for instrumenting applications and services written in a number of languages. These include com- mon choices like Python, Ruby, Go, and Java. Additional client libraries are also available from the open-source community.\n\nUseful Prometheus links\n\nThe Prometheus home page. • The Prometheus documentation. • Prometheus organization on GitHub.\n\nVersion: v1.0.0 (427b8e9)\n\n58",
      "content_length": 1619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Chapter 2: Introduction to Prometheus\n\nPrometheus source code GitHub. • Prometheus and time series at scale presentation by Jamie Wilkinson. • Grafana.\n\nSummary\n\nIn this chapter we’ve been introduced to Prometheus. We also walked through the Prometheus architecture, data model, and other aspects of the ecosystem.\n\nIn the next chapter, we’ll install Prometheus, configure it, and collect our first metrics.\n\nVersion: v1.0.0 (427b8e9)\n\n59",
      "content_length": 438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Chapter 3\n\nInstallation and Getting Started\n\nIn the last chapter we got an overview of Prometheus. In this chapter, we’ll take you through the process of installing Prometheus on a variety of platforms. This chapter doesn’t provide instructions for the full list of supported platforms, but a representative sampling to get you started. We’ll look at installing Prometheus on:\n\nLinux. • Microsoft Windows. • Mac OS X.\n\nThe lessons here for installing Prometheus can be extended to other supported platforms.\n\n NOTE We’ve written the examples in this book assuming Prometheus is\n\nrunning on a Linux distribution. The examples should also work for Mac OS X but might need tweaking for Microsoft Windows.\n\n60",
      "content_length": 706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Chapter 3: Installation and Getting Started\n\nWe’ll also explore the basics of Prometheus configuration and scrape our first tar- get: the Prometheus server itself. We’ll then use the metrics scraped to walk through the basics of the inbuilt expression browser and see how to use the Prometheus query language, PromQL, to glean interesting information from our metrics. This will give you a base Prometheus server that we’ll build on in subse- quent chapters.\n\nInstalling Prometheus\n\nPrometheus is shipped as a single binary file. The Prometheus download page con- tains tarballs containing the binaries for specific platforms. Currently Prometheus is supported on:\n\nLinux: 32-bit, 64-bit, and ARM. • Max OS X: 32-bit and 64-bit. • FreeBSD: 32-bit, 64-bit, and ARM. • OpenBSD: 32-bit, 64-bit, and ARM. • NetBSD: 32-bit, 64-bit, and ARM. • Microsoft Windows: 32-bit and 64-bit. • DragonFly: 64-bit.\n\nOlder versions of Prometheus are available from the GitHub Releases page.\n\n NOTE At the time of writing, Prometheus was at version 2.3.0.\n\nTo get started, we’re going to show you how to manually install Prometheus in the next few sections. At the end of this section we’ll also provide some links to configuration management modules for installing Prometheus. If you’re deploying\n\nVersion: v1.0.0 (427b8e9)\n\n61",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "Chapter 3: Installation and Getting Started\n\nPrometheus into production or at scale you should always choose configuration management as the installation approach.\n\nInstalling Prometheus on Linux\n\nTo install Prometheus on a 64-bit Linux host, we first download the binary file. We can use wget or curl to get the file from the download site.\n\nListing 3.1: Download the Prometheus tarball\n\n$ cd /tmp $ wget https://github.com/prometheus/prometheus/releases/ download/v2.3.0/prometheus-2.3.0.linux-amd64.tar.gz\n\nNow let’s unpack the prometheus binary from the tarball and move it somewhere useful. We’ll also install promtool, which is a linter for Prometheus configuration.\n\nListing 3.2: Unpack the prometheus binary\n\n$ tar -xzf prometheus-2.3.0.linux-amd64.tar.gz $ sudo cp prometheus-2.3.0.linux-amd64/prometheus /usr/local/bin/\n\n$ sudo cp prometheus-2.3.0.linux-amd64/promtool /usr/local/bin/\n\nWe can test if Prometheus is installed and in our path by checking its version using the --version flag.\n\nVersion: v1.0.0 (427b8e9)\n\n62",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Chapter 3: Installation and Getting Started\n\nListing 3.3: Checking the Prometheus version on Linux\n\n$ prometheus --version prometheus, version 2.3.0 (branch: HEAD, revision: 3569 eef8b1bc062bb5df43181b938277818f365b) root@bd4857492255 20171006-22:16:15 go1.9.1\n\nbuild user: build date: go version:\n\nNow that we have Prometheus installed, you can skip down to looking at its con- figuration, or you can continue to see how we install it on other platforms.\n\nInstalling Prometheus on Microsoft Windows\n\nTo install Prometheus on Microsoft Windows we need to download the prometheus .exe executable and put it in a directory. Let’s create a directory for the executable using Powershell.\n\nListing 3.4: Creating a directory on Windows\n\nC:\\> MKDIR prometheus C:\\> CD prometheus\n\nNow download Prometheus from the GitHub site:\n\nListing 3.5: Prometheus Windows download\n\nhttps://github.com/prometheus/prometheus/releases/download/v 2.3.0/prometheus-2.3.0.windows-amd64.tar.gz\n\nVersion: v1.0.0 (427b8e9)\n\n63",
      "content_length": 997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "Chapter 3: Installation and Getting Started\n\nUnzip the executable using a tool like 7-Zip and put the contents of the unzipped directory into the C:\\prometheus directory.\n\nFinally, add the C:\\prometheus directory to the path. This will allow Windows to find the executable. To do this, run this command inside Powershell.\n\nListing 3.6: Setting the Windows path\n\n$env:Path += \";C:\\prometheus\"\n\nYou should now be able to run the prometheus.exe executable.\n\nListing 3.7: Checking the Prometheus version on Windows\n\nC:\\> prometheus.exe --version prometheus, version 2.3.0 (branch: HEAD, revision: 3569 eef8b1bc062bb5df43181b938277818f365b) root@bd4857492255 20171006-22:16:15 go1.9.1\n\nbuild user: build date: go version:\n\nYou can use something like nssm, the Non-Sucking Service Manager, if you want to run the Prometheus server as a service.\n\nAlternative Microsoft Windows installation\n\nYou can also use a package manager to install Prometheus on Windows. The Chocolatey package manager has a Prometheus package available. You can use these instructions to install Chocolatey and then use the choco binary to install Prometheus.\n\nVersion: v1.0.0 (427b8e9)\n\n64",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Chapter 3: Installation and Getting Started\n\nListing 3.8: Installing Prometheus via Chocolatey\n\nC:\\> choco install prometheus\n\nAlternative Mac OS X installation\n\nIn addition to being available as a binary for Mac OS X, Prometheus is also avail- able from Homebrew. If you use Homebrew to provision your Mac OS X hosts then you can install Prometheus via the brew command.\n\nListing 3.9: Installing Prometheus via Homebrew\n\n$ brew install prometheus\n\nHomebrew will install the prometheus binary into the /usr/local/bin directory. We can test that it is operating via the prometheus --version command.\n\nListing 3.10: Checking the Prometheus version on Mac OS X\n\n$ prometheus --version prometheus, version 2.3.0 (branch: HEAD, revision: 3569 eef8b1bc062bb5df43181b938277818f365b) root@bd4857492255 20171006-22:16:15 go1.9.1\n\nbuild user: build date: go version:\n\nVersion: v1.0.0 (427b8e9)\n\n65",
      "content_length": 887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Chapter 3: Installation and Getting Started\n\nStacks\n\nIn addition to installing Prometheus standalone, there are several prebuilt stacks available. These combine Prometheus with other tools—the Grafana console, for instance.\n\nA Prometheus, Node Exporter, and Grafana docker-compose stack. • Another Docker Compose single-node stack with Prometheus, Alertmanager, Node Exporter, and Grafana.\n\nA Docker Swarm stack for Prometheus.\n\nInstalling via configuration management\n\nThere are also configuration management resources available for installing Prometheus. Here are some examples for a variety of configuration management tools:\n\nA Puppet module for Prometheus. • A Chef cookbook for Prometheus. • An Ansible role for Prometheus. • A SaltStack formula for Prometheus.\n\n TIP Remember that configuration management is the recommended ap-\n\nproach for installing and managing Prometheus!\n\nVersion: v1.0.0 (427b8e9)\n\n66",
      "content_length": 915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Chapter 3: Installation and Getting Started\n\nDeploying via Kubernetes\n\nLast, there are many ways to deploy Prometheus on Kubernetes. The best way for you to deploy likely depends greatly on your environment. You can build your own deployments and expose Prometheus via a service, use one of a number of bundled configurations, or you can use the Prometheus Operator from CoreOS.\n\nConfiguring Prometheus\n\nNow that we have Prometheus installed let’s look at its configuration. Prometheus is configured via YAML configuration files. When we run the prometheus bi- nary (or prometheus.exe executable on Windows), we specify a configuration file. Prometheus ships with a default configuration file: prometheus.yml. The file is in the directory we’ve just unpacked. Let’s take a peek at it.\n\n TIP YAML configuration is fiddly and can be a real pain. You can validate\n\nYAML online at YAML Lint or from the command line with a tool like this.\n\nVersion: v1.0.0 (427b8e9)\n\n67",
      "content_length": 966,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Chapter 3: Installation and Getting Started\n\nListing 3.11: The default Prometheus configuration file\n\nglobal:\n\n15s scrape_interval: evaluation_interval: 15s\n\nalerting:\n\nalertmanagers: - static_configs:\n\ntargets:\n\n# - alertmanager:9093\n\nrule_files:\n\n# - \"first_rules.yml\" # - \"second_rules.yml\"\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\n NOTE We’ve removed some comments from the file for brevity’s sake. The\n\ndefault file changes from time to time, so yours might not look exactly like this one.\n\nOur default configuration file has four YAML blocks defined: global, alerting, rule_files, and scrape_configs.\n\nLet’s look at each block.\n\nVersion: v1.0.0 (427b8e9)\n\n68",
      "content_length": 711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Chapter 3: Installation and Getting Started\n\nGlobal\n\nThe first block, global, contains global settings for controlling the Prometheus server’s behavior.\n\nThe first setting, the scrape_interval parameter, specifies the interval between scrapes of any application or service—in our case, 15 seconds. This value will be the resolution of your time series, the period in time that each data point in the series covers.\n\nIt is possible to override this global scrape interval when collecting metrics from specific places. Do not do this. Keep a single scrape interval globally across your server. This ensures that all your time series data has the same resolution and can be combined and calculated together. If you override the global scrape inter- val, you risk having incoherent results from trying to compare data collected at different intervals.\n\n WARNING Only configure scrape intervals globally and keep resolution\n\nconsistent!\n\nThe evaluation_interval tells Prometheus how often to evaluate its rules. Rules come in two major flavors: recording rules and alerting rules:\n\nRecording rules - Allow you to precompute frequent and expensive expres-\n\nsions and to save their result as derived time series data.\n\nAlerting rules - Allow you to define alert conditions.\n\nWith this parameter, Prometheus will (re-)evaluate these rules every 15 seconds. We’ll see more about rules in subsequent chapters.\n\nVersion: v1.0.0 (427b8e9)\n\n69",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Chapter 3: Installation and Getting Started\n\n NOTE You can find the full Prometheus configuration reference in the\n\ndocumentation.\n\nAlerting\n\nThe second block, alerting, configures Prometheus’ alerting. As we mentioned in the last chapter, alerting is provided by a standalone tool called Alertmanager. Alertmanager is an independent alert management tool that can be clustered.\n\nListing 3.12: Alertmanager configuration\n\nalerting:\n\nalertmanagers: - static_configs:\n\ntargets:\n\n# - alertmanager:9093\n\nIn our default configuration, the alerting block contains the alerting configura- tion for our server. The alertmanagers block lists each Alertmanager used by this Prometheus server. The static_configs block indicates we’re going to specify any Alertmanagers manually, which we have done in the targets array.\n\n TIP Prometheus also supports service discovery for Alertmanagers—for ex-\n\nample, rather than specifying each Alertmanager individually, you could query an external source like a Consul server to return a list of available Alertmanagers. We’ll see more about this in Chapters 5 and 6.\n\nVersion: v1.0.0 (427b8e9)\n\n70",
      "content_length": 1128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Chapter 3: Installation and Getting Started\n\nIn our case we don’t have an Alertmanager defined; instead we have a commented- out example at alertmanager:9093. We can leave this commented out because you don’t specifically need an Alertmanager defined to run Prometheus. We’ll add an Alertmanager and configure it in Chapter 6.\n\n TIP We’ll see more about alerting in Chapter 6 and clustering alerting in\n\nChapter 7.\n\nRule files\n\nThe third block, rule_files, specifies a list of files that can contain recording or alerting rules. We’ll make some use of these in the next chapter.\n\nScrape configuration\n\nThe last block, scrape_configs, specifies all of the targets that Prometheus will scrape.\n\nAs we discovered in the last chapter, Prometheus calls the source of metrics it can scrape endpoints. To scrape an endpoint, Prometheus defines configuration called a target. This is the information required to perform the scrape—for example, what labels to apply, any authentication required to connect, or other information that defines how the scrape will occur. Groups of targets are called jobs. Inside jobs, each target has a label called instance that uniquely identifies it.\n\nVersion: v1.0.0 (427b8e9)\n\n71",
      "content_length": 1207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Chapter 3: Installation and Getting Started\n\nListing 3.13: The default Prometheus scrape configuration\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\nOur default configuration has one job defined called prometheus. Inside this job we have a static_config block, which lists the targets this job will scrape. The static_config block indicates that we’re going to individually list the targets we want to scrape, rather than use any automated service discovery method. You can think about static configuration as manual or human service discovery.\n\n TIP We’re going to look at methods to automatically discover targets to be\n\nscraped in Chapter 5.\n\nThe default prometheus job has one target: It scrapes localhost on port 9090, which returns the server’s own health metrics. Prometheus assumes that metrics will be returned on the path /metrics, so it appends this to the target and scrapes the address http://localhost:9090/ metrics.\n\nthe Prometheus server itself.\n\n TIP You can override the default metrics path.\n\nVersion: v1.0.0 (427b8e9)\n\n72",
      "content_length": 1084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "Chapter 3: Installation and Getting Started\n\nStarting the server\n\nLet’s start the server and see what happens. First, though, let’s move our configu- ration file somewhere more suitable.\n\nListing 3.14: Moving the configuration file\n\n$ sudo mkdir -p /etc/prometheus $ sudo cp prometheus.yml /etc/prometheus/\n\nHere we’ve created a directory, /etc/prometheus, to hold our configuration file, and we’ve moved our new file into this directory.\n\nListing 3.15: Starting the Prometheus server\n\n$ prometheus --config.file \"/etc/prometheus/prometheus.yml\" level=info ts=2017-10-23T14:03:02.274562Z caller=main.go:216 msg =\"Starting prometheus\"...\n\nWe run the binary and specify our configuration file in the --config.file com- mand line flag. Our Prometheus server is now running and scraping the instances of the prometheus job and returning the results.\n\nIf something doesn’t work, you can validate your configuration with promtool, a linter that ships with Prometheus.\n\nListing 3.16: Validating your configuration with promtool\n\n$ promtool check config prometheus.yml Checking prometheus.yml\n\nSUCCESS: 0 rule files found\n\nVersion: v1.0.0 (427b8e9)\n\n73",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Chapter 3: Installation and Getting Started\n\nRunning Prometheus via Docker\n\nIt’s also easy to run Prometheus in Docker. There’s a Docker image provided by the Prometheus team available on the Docker Hub. You can execute it with the docker command.\n\nListing 3.17: Running Prometheus with Docker\n\n$ docker run -p 9090:9090 prom/prometheus\n\nThis will run a Prometheus server locally, with port 9090 bound to port 9090 inside the Docker container. You can then browse to that port on your local host to see your Prometheus server. The server is launched with a default configuration, and you will need to provide custom configuration and data storage. You can take a number of approaches here—for example, you could mount a configuration file into the container.\n\nListing 3.18: Mounting a configuration file into the Docker container\n\n$ docker run -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/ prometheus.yml prom/prometheus\n\nThis would bind mount the file /tmp/prometheus.yml into the container as the Prometheus server’s configuration file.\n\n TIP You can find more information on running Prometheus with Docker in\n\nthe documentation.\n\nVersion: v1.0.0 (427b8e9)\n\n74",
      "content_length": 1170,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Chapter 3: Installation and Getting Started\n\nFirst metrics\n\nNow that the server is running, let’s take a look at the endpoint we are scraping and see some raw Prometheus metrics. To do this, let’s browse to the URL http ://localhost:9090/metrics and see what gets returned.\n\n NOTE In all our examples we assume you’re browsing on the server running Prometheus, hence localhost.\n\nListing 3.19: Some sample raw metrics\n\n# HELP go_gc_duration_seconds A summary of the GC invocation durations. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 1.6166e−05 go_gc_duration_seconds{quantile=\"0.25\"} 3.8655e−05 go_gc_duration_seconds{quantile=\"0.5\"} 5.3416e−05 . . .\n\nHere we can see our first Prometheus metrics. These look much like the data model we saw in the last chapter.\n\nListing 3.20: A raw metric\n\ngo_gc_duration_seconds{quantile=\"0.5\"} 1.6166e−05\n\nThe name of our metric is go_gc_duration_seconds. We can see one label on the metric, quantile=\"0.5\", indicating this is measuring the 50th percentile, and the value of the metric.\n\nVersion: v1.0.0 (427b8e9)\n\n75",
      "content_length": 1086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Chapter 3: Installation and Getting Started\n\nPrometheus expression browser\n\nIt is not user friendly to view our metrics this way, though, so let’s make use of Prometheus’ inbuilt expression browser. It’s available on the Prometheus server by browsing to http://localhost:9090/graph.\n\n TIP The Prometheus Expression browser and web interface have other use-\n\nful information, like the status of targets and the rules and configuration of the Prometheus server. Make sure you check out all the interface menu items.\n\nFigure 3.1: Prometheus expression browser\n\nLet’s find the go_gc_duration_seconds metric using the expression browser. To do this, we can either open the dropdown list of available metrics or we can type the metric name into the query box. We then click the Execute button to display\n\nVersion: v1.0.0 (427b8e9)\n\n76",
      "content_length": 829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Chapter 3: Installation and Getting Started\n\nall the metrics with this name.\n\nFigure 3.2: List of metrics\n\nWe can see a list of metrics here, each decorated with one or more labels. Let’s find the 50th percentile in the list.\n\nListing 3.21: Go garbage collection 50th percentile\n\ngo_gc_duration_seconds{instance=\"localhost:9090\",job=\"prometheus \",quantile=\"0.5\"}\n\nWe can see that two new labels have been added to our metrics. This has been done automatically by Prometheus during the scrape process. The first new label, instance, is the target from which we scraped the metrics. The second label, job, is the name of the job that scraped the metrics. Labels provide dimensions to our metrics. They allow us to query or work with multiple or specific metrics—for example, Go garbage collection metrics for multiple targets.\n\n TIP We’ll see a lot more about labels in the next chapter and later in the\n\nbook.\n\nVersion: v1.0.0 (427b8e9)\n\n77",
      "content_length": 940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Chapter 3: Installation and Getting Started\n\nPrometheus has a highly flexible expression language called PromQL built into the server, allowing you to query and aggregate metrics. We can use this query language in the query input box at the top of the interface.\n\nFigure 3.3: Querying quantiles\n\nHere we’ve queried all metrics with a label of quantile=\"0.5\" and it has returned a possible 86 metrics. This set is one of the four data types that expressions in the PromQL querying language can return. This type is called an instant vector: a set of time series containing a single sample for each time series, all sharing the same timestamp. We can also return instant vectors for metrics by querying a name and a label. Let’s go back to our go_gc_duration_seconds but this time the 75th percentile. Specify:\n\ngo_gc_duration_seconds{quantile=\"0.75\"}\n\nIn the input box and click Execute to search. It should return an instant vector that matches the query. We can also negate or match a label using a regular expression.\n\ngo_gc_duration_seconds{quantile!=\"0.75\"}\n\nThis will return an instant vector of all the metrics with a quantile label not equal to 0.75.\n\nVersion: v1.0.0 (427b8e9)\n\n78",
      "content_length": 1188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Chapter 3: Installation and Getting Started\n\n TIP If we’re used to tools like Graphite, querying labels is like parsing dotted-\n\nstring named metrics. There’s a blog post that provides a side-by-side comparison of how Graphite, InfluxDB, and Prometheus handle a variety of queries.\n\nLet’s look at another metric, this one called prometheus_build_info, that contains information about the Prometheus server’s build. Put prometheus_build_info into the expression browser’s query box and click Execute to return the metric. You’ll see an entry like so:\n\nListing 3.22: The prometheus_build_info metric\n\nprometheus_build_info{branch=\"HEAD\",goversion=\"go1.9.1\",instance =\"localhost:9090\",job=\"prometheus\",revision=\"5 ab8834befbd92241a88976c790ace7543edcd59\",version=\"2.3.0\"}\n\nYou can see the metric is heavily decorated with labels and has a value of 1. This is a common pattern for passing information to the Prometheus server using a metric. It uses a metric with a perpetual value of 1, and with the relevant information you might want attached via labels. We’ll see more of these types of informational metrics later in the book.\n\nTime series aggregation\n\nThe interface can also do complex aggregation of metrics. Let’s choose another metric, http_requests_total, which is the total HTTP requests made by various handlers in the Prometheus server. Query for that now by specifying its name and clicking Execute.\n\nVersion: v1.0.0 (427b8e9)\n\n79",
      "content_length": 1441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Chapter 3: Installation and Getting Started\n\nFigure 3.4: Querying total HTTP requests\n\nWe have a list of HTTP request metrics. But what we really want is the total HTTP requests per job. To do this, we need to create a new metric via a query. Prometheus’ querying language, PromQL, has a large collection of expressions and functions that can help us do this.\n\nLet’s start by summing the HTTP requests by job. Add the following to the query box and click Execute.\n\nsum(http_requests_total)\n\nThis new query uses the sum() operator on the http_requests_total metric. It adds up all of the requests but doesn’t break it down by job. To do that we need to aggregate over a specific label dimension. PromQL has a clause called by that will allow us to aggregate by a specific dimension. Add the following to the query box and then click Execute.\n\nsum(http_requests_total) by (job)\n\n TIP PromQL also has a clause called without that aggregates without a spe-\n\ncific dimension.\n\nVersion: v1.0.0 (427b8e9)\n\n80",
      "content_length": 1002,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Chapter 3: Installation and Getting Started\n\nYou should see something like the following output:\n\nFigure 3.5: Calculating total HTTP requests by job\n\nNow click the Graph tab to see this metric represented as a plot.\n\n TIP The folks at Robust Perception have a great blog post on common query-\n\ning patterns.\n\nThe new output is still not quite useful—let’s convert it into a rate. Update our query to:\n\nsum(rate(http_requests_total[5m])) by (job)\n\nHere we’ve added a new function: rate(). We’ve inserted it inside our sum func- tion.\n\nrate(http_requests_total[5m])\n\nThe rate() function calculates the per-second average rate of increase of the time series in a range. The rate function should only be used with counters. It is quite clever and automatically adjusts for breaks, like a counter being reset when the resource is restarted, and extrapolates to take care of gaps in the time series, such as a missed scrap. The rate() function is best used for slower-moving counters or for alerting purposes.\n\nVersion: v1.0.0 (427b8e9)\n\n81",
      "content_length": 1035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Chapter 3: Installation and Getting Started\n\n TIP There’s also an irate() function to calculate the instant rate of increase\n\nfor faster-moving timers.\n\nHere we’re calculating the rate over a five-minute range vector. Range vectors are a second PromQL data type containing a set of time series with a range of data points over time for each time series. Range vectors allow us to display the time series for that period. The duration of the range is enclosed in [] and has an integer value followed by a unit abbreviation:\n\ns for seconds. • m for minutes. • h for hours. • d for days. • w for weeks. • y for years.\n\nSo here [5m] is a five-minute range.\n\n TIP The other two PromQL data types are Scalars, numeric floating-point values, and Strings, which is a string value and is currently unused.\n\nLet’s Execute that query and see the resulting range vector of time series.\n\nVersion: v1.0.0 (427b8e9)\n\n82",
      "content_length": 906,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Chapter 3: Installation and Getting Started\n\nFigure 3.6: Our rate query\n\nCool! We’ve now got a new metric that is actually useful for tracking or graphing.\n\n TIP If you want help constructing PromQL queries, there’s a query editor\n\ncalled Promeditor available that you can run locally with Prometheus.\n\nNow that we’ve walked through the basics of Prometheus operation, let’s look at some of the requirements for running a Prometheus server.\n\nCapacity planning\n\nPrometheus performance is hard to estimate because it depends greatly on your configuration, the volume of time series you collect, and the complexity of any rules on the server. There are two capacity concerns: memory and disk.\n\n TIP We’ll look at Prometheus scaling concepts in Chapter 7.\n\nVersion: v1.0.0 (427b8e9)\n\n83",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Chapter 3: Installation and Getting Started\n\nMemory\n\nPrometheus does a lot in memory. It consumes process memory for each time series collected and for querying, recording rules, and the like. There’s not a lot of data on capacity planning for Prometheus, especially since 2.0 was released, but a good, rough, rule of thumb is to multiply the number of samples being collected per second by the size of the samples. We can see the rate of sample collection using this query.\n\nrate(prometheus_tsdb_head_samples_appended_total[1m])\n\nThis will show you the per-second rate of samples being added to the database over the last minute.\n\nIf you want to know the number of metrics you’re collecting you can use:\n\nsum(count by (__name__)({__name__=\\~\"\\.\\+\"}))\n\nThis uses the sum aggregation to add up a count of all metrics that match, using the =~ operator, the regular expression of .+, or all metrics.\n\nEach sample is generally one to two bytes in size. Let’s err on the side of caution and use two bytes. Assuming we’re collecting 100,000 samples per second for 12 hours, we can work out memory usage like so:\n\n100,000 * 2 bytes * 43200 seconds\n\nOr roughly 8.64 GB of RAM.\n\nYou’ll also need to factor in memory use for querying and recording rules. This is very rough and dependent on a lot of other variables. I recommend playing things by ear with regard to memory usage. You can see the memory usage of the Prometheus process by checking the process_resident_memory_bytes metric.\n\nVersion: v1.0.0 (427b8e9)\n\n84",
      "content_length": 1509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Chapter 3: Installation and Getting Started\n\nDisk\n\nDisk usage is bound by the volume of time series stored and the retention of those time series. By default, metrics are stored for 15 days in the local time series database. The location of the database and the retention period are controlled by command line options.\n\nThe --storage.tsdb.path option, which has a default directory of data lo- cated in the directory from which you are running Prometheus, controls your time series database location.\n\nThe --storage.tsdb.retention controls retention of time series. The de-\n\nfault is 15d representing 15 days.\n\n TIP The best disk for time series databases is SSD. You should use SSDs.\n\nFor our 100,000 samples per second example, we know each sample collected in a time series occupies about one to two bytes on disk. Assuming two bytes per sample, then a time series retained for 15 days would mean needing about 259 GB of disk.\n\n TIP There’s more information on Prometheus disk usage in the Storage doc-\n\numentation.\n\nVersion: v1.0.0 (427b8e9)\n\n85",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Chapter 3: Installation and Getting Started\n\nSummary\n\nIn this chapter we installed Prometheus and configured its basic operation. We also scraped our first target, the Prometheus server itself. We made use of the met- rics collected by the scrape to see how the inbuilt expression browser works, in- cluding graphing our metrics and deriving new metrics using Prometheus’s query language, PromQL.\n\nIn the next chapter we’ll use Prometheus to collect some host metrics, including collecting from Docker containers. We’ll also see a lot more about scraping, jobs, and labels, and we’ll have our first introduction to recording rules.\n\nVersion: v1.0.0 (427b8e9)\n\n86",
      "content_length": 662,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Chapter 4\n\nMonitoring Nodes and Containers\n\nIn the last chapter we installed Prometheus and did some basic configuration. We also scraped some time series data from the Prometheus server itself. In this chapter, we’re going to look at using Prometheus to monitor the metrics of both hosts and containers. We’re going to demonstrate this on a cluster of three Ubuntu hosts running the Docker daemon.\n\nFirst, we’ll install exporters on each host, configure exporting of node and Docker metrics, and configure Prometheus to scrape them.\n\nNext, we’ll look at monitoring some basic host resources, including:\n\n1. CPU. 2. Memory. 3. Disk. 4. Availability.\n\nTo determine what to monitor, we’ll revisit the USE Method monitoring method- ology to help assist in identifying the right metrics. We’ll also look at how we might use Prometheus to detect the state of services and the availability of hosts.\n\nThen we’ll make use of the collected metrics to build some aggregated metrics and save them as recording rules.\n\n87",
      "content_length": 1010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nLast, we’ll very briefly introduce Grafana to do basic visualizations of some of the data we’re collecting.\n\nThese are probably the most standard tasks for which monitoring tools are de- ployed, and they provide a solid foundation for learning more about Prometheus. This base set of data will allow us to identify host performance issues or will pro- vide sufficient supplemental data for the fault diagnosis of application issues.\n\nMonitoring nodes\n\nPrometheus uses tools called exporters to expose metrics on hosts and applica- tions. There are a number of exporters available for a variety of purposes. Right now we’re going to focus on one specific exporter: the Node Exporter. The Node Exporter is written in Go and has a library of collectors for various host metrics including CPU, memory, and disk. It also has a textfile collector that allows you to export static metrics, which is useful for sending information about the node, as we’ll see shortly, or metrics exported from batch jobs.\n\n NOTE We’ll use the term “node” at times to refer to hosts.\n\nLet’s start by downloading and installing the Node Exporter on a Linux host. We’re going to choose one of our Docker daemon hosts.\n\n TIP If you don’t want to use one of the Prometheus exporters there are a\n\nswath of host-monitoring clients that support Prometheus. For example, collectd can also write Prometheus metrics.\n\nVersion: v1.0.0 (427b8e9)\n\n88",
      "content_length": 1458,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nInstalling the Node Exporter\n\nThe Node Exporter is available as a tarball and for a limited number of platforms via packages. The tarball of the Node Exporter is available, with a number of other exporters, from the Prometheus website.\n\nLet’s download and extract the Node Exporter for Linux and move the binary into our path.\n\nListing 4.1: Downloading the Node Exporter\n\nwget https://github.com/prometheus/node_exporter/releases/download/v 0.16.0/node_exporter-0.16.0.linux-amd64.tar.gz $ tar -xzf node_exporter-* $ sudo cp node_exporter-*/node_exporter /usr/local/bin/\n\n NOTE At the time of writing the Node Exporter was at version 0.16.0. You\n\nshould download the latest version.\n\nThe Node Exporter is also available as a CentOS and Fedora package via a COPR build.\n\n NOTE Using configuration management is the best way to run and install\n\nany Prometheus exporters. This is an easy way to control configuration, and to provide automation and service management.\n\nLet’s test that the node_exporter binary is working.\n\nVersion: v1.0.0 (427b8e9)\n\n89",
      "content_length": 1095,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.2: Testing the Node Exporter binary\n\n$ node_exporter --version node_exporter, version 0.16.0 (branch: HEAD, revision: 6 e2053c557f96efb63aef3691f15335a70baaffd) . . .\n\nConfiguring the Node Exporter\n\nThe node_exporter binary is configured via flags. You can see a full list of flags by running the binary with the --help flag.\n\nListing 4.3: Running the help for Node Exporter\n\n$ node_exporter --help\n\nYou’ll see a list of available flags. The node_exporter exporter runs, by default, on port 9100 and exposes metrics on the /metrics path. You can control the interface and port via the --web.listen-address and --web.telemetry-path flags like so:\n\nListing 4.4: Controlling the port and path\n\n$ node_exporter --web.listen-address=\":9600\" --web.telemetry- path=\"/node_metrics\"\n\nThis will bind the node_exporter to port 9600 and return metrics on the /node- metrics path.\n\nThese flags also control which collectors are enabled. By default, many of the collectors are enabled. Collectors either have a disabled or enabled status, and\n\nVersion: v1.0.0 (427b8e9)\n\n90",
      "content_length": 1113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nthe status can be flipped by specifying the relevant flag with a no- prefix. For ex- ample, the arp collector, which exposes statistics from /proc/net/arp, is enabled by default. It is controlled by the --collector.arp flag. To disable this collector we’d run:\n\nListing 4.5: Disabling the arp collector\n\n$ node_exporter --no-collector.arp\n\nConfiguring the Textfile collector\n\nWe also want to configure one specific collector, the textfile collector, that we’re going to use later in this chapter. The textfile collector is very useful because it allows us to expose custom metrics. These custom metrics might be the result of tasks like batch or cron jobs, which can’t be scraped; they might come from sources that don’t have an exporter; or they might even be static metrics which provide context for the host.\n\nThe collector works by scanning files in a specified directory, extracting any strings that are formatted as Prometheus metrics, and exposing them to be scraped.\n\nLet’s set the collector up now, starting with creating a directory to hold our the metric definition files.\n\nListing 4.6: Creating a textfile directory\n\n$ mkdir -p /var/lib/node_exporter/textfile_collector\n\nNow let’s create a new metric in this directory. Metrics are defined in files ending in .prom inside the directory we’ve just created. Metrics are defined using the Prometheus text exposition format.\n\nVersion: v1.0.0 (427b8e9)\n\n91",
      "content_length": 1457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\n NOTE The text exposition format allows us to specify all the metric types\n\nthat Prometheus supports: counters, gauges, timers, etc.\n\nLet’s use this format to create a metric that will contain some metadata about this host.\n\nmetadata{role=\"docker_server\",datacenter=\"NJ\"} 1\n\nWe can see we have a metric name, metadata, and two labels. One label is called role to define a role for this node. In this case this label has a value of docker_server. We also have a label called datacenter to define the geograph- ical location of the host. Finally, the metric has a static value of 1 because it’s providing context rather than recording a counter, gauge, or timer.\n\nLet’s add this metric to a file called metadata.prom in our textfile_collector directory.\n\nListing 4.7: A metadata metric\n\n$ echo 'metadata{role=\"docker_server\",datacenter=\"NJ\"} 1' | sudo\n\ntee /var/lib/node_exporter/textfile_collector/metadata.prom\n\nHere we’ve piped our metric into a file called metadata.prom.\n\n TIP In the real world, you’d populate this file using your configuration man- agement tool. For example, when a new host is provisioned, a metadata metric could be created from a template. This could allow you to automatically classify your hosts and services.\n\nVersion: v1.0.0 (427b8e9)\n\n92",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nTo enable the textfile collector we don’t need to set a flag—it’s loaded by default—but we do need to specify our textfile_exporter directory so the Node Exporter knows where to find our custom metrics. To do this, we specify the --collector.textfile.directory flag.\n\nEnabling the systemd collector\n\nLet’s also turn on an extra collector, systemd, which records services and system status from systemd. This collector gathers a lot of metrics, but we don’t want to collect the status of everything systemd is managing, just some key services. To keep things clean, we can whitelist specific services. We’re only going to collect metrics for:\n\ndocker.service • ssh.service • rsyslog.service\n\nWhich are the Docker daemon, the SSH daemon, and the RSyslog daemon. We do this using the --collector.systemd.unit-whitelist flag, which takes a regular expression matching systemd units.\n\nRunning the Node Exporter\n\nFinally, we can launch node_exporter on one of our Docker nodes like so:\n\nListing 4.8: Starting Node Exporter with the textfile collector and systemd\n\n$ node_exporter --collector.textfile.directory /var/lib/ node_exporter/textfile_collector --collector.systemd --collector. systemd.unit-whitelist=(docker|ssh|rsyslog).service\n\nVersion: v1.0.0 (427b8e9)\n\n93",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nWe’ve specified the directory for the textfile collector to find our metrics, en- abled the systemd collector, and used a regular expression whitelist to match the three services for which we want to collect metrics.\n\nNow that the Node Exporter is running on one of our Docker daemon nodes, let’s add it to the others. We have three nodes, and we’ve identically configured two of them. The name and IP address of each node is:\n\nDocker1 - 138.197.26.39 • Docker2 - 138.197.30.147 • Docker3 - 138.197.30.163\n\nNow let’s see how to scrape the time series data that we’ve just exported.\n\nScraping the Node Exporter\n\nBack on our Prometheus server, let’s configure a new job to scrape the data ex- ported by the Node Exporter. Let’s examine the scrape_configs block from our current prometheus.yml file and our existing scrape configuration.\n\nListing 4.9: The current Prometheus scrape configuration\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\nTo get this new data, we need to add another job to this configuration. We’re going to call our new job node. We’re also going to continue to add individual targets using static_configs, rather than by using any kind of service discovery. (We’ll see more about service discovery in the next chapter.) Let’s add that new job now.\n\nVersion: v1.0.0 (427b8e9)\n\n94",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.10: Adding the node job\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\njob_name: 'node' static_configs:\n\ntargets: ['138.197.26.39:9100', '138.197.30.147:9100', '\n\n138.197.30.163:9100']\n\nYou can see that we’ve added the new job called node. It contains a static_configs block with a list of our three Docker hosts listed via their IP addresses and the relevant port, 9100. Prometheus assumes the Node Exporter has the default path, /metrics, and scrapes a target of:\n\n138.197.26.39:9100/metrics\n\nIf we now SIGHUP or restart the Prometheus server, our configuration will be reloaded and the server will start scraping. We’ll see the time series data start flowing into the Prometheus server shortly.\n\nFiltering collectors on the server\n\nThe Node Exporter can return a lot of metrics though, and perhaps you don’t want to collect them all. In addition to controlling which collectors the Node Exporter runs locally via local configuration, Prometheus also has a way we can limit the collectors actually scraped from the server side. This is especially useful when you don’t control the configuration of the host you’re scraping.\n\nPrometheus achieves this by adding a list of the specific collectors to scrape to our job configuration.\n\nVersion: v1.0.0 (427b8e9)\n\n95",
      "content_length": 1356,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.11: Filtering collectors\n\nscrape_configs: . . .\n\njob_name: 'node' static_configs:\n\ntargets: ['138.197.26.39:9100', '138.197.30.147:9100', '\n\n138.197.30.163:9100']\n\nparams:\n\ncollect[]: - cpu - meminfo - diskstats - netdev - netstat - filefd - filesystem - xfs - systemd\n\nHere we’ve limited the metrics being scraped to this list of collectors, specified using the collect[] list inside the params block. These are then passed to the scrape request as URL parameters. You can test this using the curl command on a Node Exporter instance.\n\nListing 4.12: Testing collect params\n\n$ curl -g -X GET http://138.197.26.39:9100/metrics?collect[]=cpu\n\nThis would return the base Node Exporter metrics, like the Go metrics we saw for the Prometheus server, and the metrics generated by the CPU collector. All other metrics will be disregarded.\n\nFor now though, on our Prometheus server, we’re going to collect everything.\n\nVersion: v1.0.0 (427b8e9)\n\n96",
      "content_length": 994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nNow that we have our node metrics, let’s instrument our Docker daemons too.\n\nMonitoring Docker\n\nThere are several ways to monitor Docker with Prometheus, including several cus- tom exporters. However these exporters have generally been deprecated in favor of the recommended approach: Google’s cAdvisor tool. cAdvisor runs as a Docker container on your Docker daemon. A single cAdvisor container returns metrics for your Docker daemon and all running containers. It has native Prometheus support to export metrics, as well as support for a variety of other storage destinations like InfluxDB, Elasticsearch, and Kafka.\n\n NOTE We’re going to assume that you have installed and are running\n\nDocker daemons, and that you understand the basics of how Docker works. you’re new to Docker, I have a book on it that might interest you.\n\nRunning cAdvisor\n\nAs cAdvisor is just another container on our Docker host, we can launch it with the docker run command. Let’s run a cAdvisor container on our Docker1 host.\n\nVersion: v1.0.0 (427b8e9)\n\nIf\n\n97",
      "content_length": 1082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.13: Running the caAdvisor container\n\n$ docker run \\\n\n--volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:rw \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/dev/disk/:/dev/disk:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ google/cadvisor:latest\n\nLet’s break this docker run command down a little. First, we mount a few directo- ries inside the container. The directories are broken into two types. The first are read-only mounts from which cAdvisor will gather data—for example, mounting the /sys directory like so:\n\n--volume=/sys:/sys:ro\n\n TIP The ro indicates read-only.\n\nThe second type, which contains one mount, is a read-write mount of the Docker socket, usually located in the /var/run directory. We also publish port 8080 from inside the container to 8080 on the host. You could override this with any port that suited you. We run the container with the --detach flag to daemonize it and name it cadvisor. Last, we use the google/cadvisor image with the latest tag.\n\nIf we now run docker ps, we can see our running cAdvisor container.\n\nVersion: v1.0.0 (427b8e9)\n\n98",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.14: The cAdvisor container\n\nCONTAINER ID IMAGE\n\nCOMMAND\n\nCREATED\n\nSTATUS\n\nPORTS\n\nNAMES\n\n6fca3002e351 google/cadvisor \"/usr/bin/...\" 1 hours ago Up 1 hours 0.0.0.0:8080->8080/tcp cadvisor\n\ncAdvisor should start monitoring immediately. We can browse to port 8080 on the host to see cAdvisor’s web interface and confirm that it is operational.\n\nFigure 4.1: cAdvisor web interface\n\nIf we browse to the path /metrics on port 8080, we’ll see the built-in Prometheus metrics being exposed.\n\nVersion: v1.0.0 (427b8e9)\n\n99",
      "content_length": 567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nFigure 4.2: cAdvisor Prometheus metrics\n\nWe’ll also install cAdvisor on our other two Docker daemons as well.\n\nScraping cAdvisor\n\nWith cAdvisor running on our Docker daemons, we need to tell Prometheus about it. To do this, we’re going to add a third job to our configuration. Let’s edit prometheus.yml on our Prometheus server.\n\nWe’re again going to add individual targets using static_configs, rather than by using any kind of service discovery.\n\nVersion: v1.0.0 (427b8e9)\n\n100",
      "content_length": 523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.15: Adding the Docker job\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\njob_name: 'node' static_configs:\n\ntargets: ['138.197.26.39:9100', '138.197.30.147:9100', '\n\n138.197.30.163:9100']\n\njob_name: 'docker' static_configs:\n\ntargets: ['138.197.26.39:8080', '138.197.30.147:8080', '\n\n138.197.30.163:8080']\n\nYou can see we’ve added the new job called docker. It contains a static_configs block with a list of our three Docker daemon servers with their IP addresses and the relevant port, 8080. Again we assume the default /metrics path. If we again SIGHUP or restart the Prometheus server, then our configuration will be reloaded, it will start scraping, and the new time series will appear.\n\nI think it’s important, though, before we continue, that we understand how a scrape works and a bit about the lifecycle of labels. We’ll use our cAdvisor metrics to explore this lifecycle.\n\nScrape lifecycle\n\nLet’s look at the lifecycle of a scrape itself, and into the lifecycle of labels. Every scrape_interval period, in our case 15 seconds, Prometheus will check for jobs to be executed. Inside those jobs it’ll generate a list of targets: the service discovery process. In the cases we’ve seen so far we’ve got manually specified, statically configured hosts. There are other service discovery mechanisms, like loading targets from a file or querying an API.\n\nVersion: v1.0.0 (427b8e9)\n\n101",
      "content_length": 1478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\n TIP We’ll learn more about service discovery in Chapter 5.\n\nService discovery returns a list of targets with a set of labels attached called meta- data. These labels are prefixed with __meta_. Each service discovery mechanism has different metadata—for example, the AWS EC2 discovery mechanism returns the availability zone of instances in a label called __meta_ec2_availability_zone .\n\nService discovery also sets additional labels based on the configuration of the target. These configuration labels are prefixed and suffixed with __. They include the __scheme__, __address__, and __metrics_path__ labels. These contain the scheme, http or https, of the target; the address of the target; and the specific path to the metrics respectively.\n\nEach label usually has a default—for example, __metrics_path__ would default to /metrics, and __scheme__ to http. Additionally, if any URL parameters are present in the path then they’re set into labels prefixed with __param_*.\n\nThe configuration labels are also reused during the lifecycle of the scrape to pop- ulate other labels. For example, the default contents of the instance label on our metrics is the contents of the __address__ label.\n\n NOTE So, wait—why haven’t we seen any of those __ prefixed and suffixed\n\nlabels? That’s because some are removed later in the lifecycle, and all of them are specifically excluded from display on the Web UI.\n\nThis list of targets and labels are then returned to Prometheus. Some of those labels can be overridden in configuration—for example, the metrics path via the metrics_path parameter, and the scheme to be used via the scheme parameter.\n\nVersion: v1.0.0 (427b8e9)\n\n102",
      "content_length": 1712,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.16: Overriding the discovered labels\n\nscrape_configs:\n\njob_name: 'node' scheme: https metrics_path: /moremetrics static_configs:\n\ntargets: ['138.197.26.39:9100', '138.197.30.147:9100', '\n\n138.197.30.163:9100']\n\nHere we’re overriding the scheme to https and the metric’s path to /moremetrics.\n\nPrometheus then offers an opportunity to relabel your targets and to potentially make use of some metadata your service discovery has added. You can also filter targets to drop or keep specific items.\n\nAfter this, the actual scrape takes place, and the metrics are returned. When the metrics are being scraped you are offered a final opportunity to relabel and filter them before they are saved to the server.\n\nPhew. That’s complicated. Let’s see a simplified image of that lifecycle:\n\nVersion: v1.0.0 (427b8e9)\n\n103",
      "content_length": 863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nFigure 4.3: Scrape lifecycle\n\nYou can see we’ve introduced a bunch of concepts, including two blocks where Prometheus relabels metrics. This is a good time to talk a bit more about labels, relabelling, and taxonomies. Let’s take a little interlude.\n\nLabels\n\nWe learned in Chapter 2 that labels provide the dimensions of our time series. They can define what the target is and add context to the time series. But most importantly, combined with the metric name, they make up the identity of your time series. They represent the identity of your time series—if they change, so does the identity of the time series.\n\nChanging a label or adding a new label creates a new time series.\n\nVersion: v1.0.0 (427b8e9)\n\n104",
      "content_length": 755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nThis means that labels should be used judiciously and remain as constant as possi- ble. Failure to adhere to this can spawn new time series, creating a dynamic data environment that makes your monitoring data sources harder to track. Imagine you have a time series that you’re using to track the state of a service. You have an alert configured for that time series that relies on the labels of the metric to determine the right criteria. By changing or adding a label, that alert definition is rendered invalid. The same applies to historical time series data: By changing or adding a label we lose track of the previous time series, breaking graphs and expressions, and causing general mayhem.\n\n TIP What happens to the old time series if it’s not being written anymore? If\n\na scrape no longer returns data for a time series that was previously present, that series will be marked as stale. The same applies for any targets that are removed: All of their time series will be marked as stale. Stale data is not returned in graphs.\n\nLabel taxonomies\n\nSo when should we add labels and what labels should we add? Well, like all good monitoring architectures, it’s worth building a taxonomy. Labels, like most monitoring taxonomies, are probably best when broadly hierarchical. A good way of thinking about a taxonomy is in terms of topological and schematic labels.\n\nThe topological labels slice the service components by their physical or logical makeup, e.g., the datacenter label we saw above. We already get two topological labels for free with every metric: job and instance. The job label is set from the job name in the scrape configuration. We tend to use job to describe the type of thing we’re monitoring. In the case of our Node Exporter job we called it node\n\nVersion: v1.0.0 (427b8e9)\n\n105",
      "content_length": 1845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\n. This will label all the Node Exporter metrics with a job label of node. The instance label identifies the target. It’s usually the IP address and port of the target, and it’s usually sourced from the __address__ label.\n\nSchematic labels are things like url, error_code, or user which let you match time series at the same level in the topology together—for example, to create ratios of one against the other.\n\nIf you need to add additional labels consider a hierarchy something like this:\n\nFigure 4.4: Sample label taxonomy\n\nA little later in this chapter, we’ll look at metrics like the metadata metric we created earlier with the Textfile collector that can be decorated with contextual information.\n\nWe can also create and manipulate existing labels to help us better manage our time series data.\n\nVersion: v1.0.0 (427b8e9)\n\n106",
      "content_length": 877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nRelabelling\n\nGiven the desire to judiciously use labels, why would we relabel things? In a word: control. In a centralized, complex monitoring environment you sometimes don’t control all the resources you are monitoring and the monitoring data they expose. Relabelling allows you to control, manage, and potentially standardize metrics in your environment. Some of the most common use cases are:\n\nDropping unnecessary metrics. • Dropping sensitive or unwanted labels from the metrics. • Adding, editing, or amending the label value or label format of the metrics.\n\nRemember there are two phases at which we can relabel. The first phase is re- labelling targets that have come from service discovery. This is most useful for applying information from metadata labels from service discovery into labels on your metrics. This is done in a relabel_configs block inside a job. We’ll see more of that in the next chapter.\n\nThe second phase is after the scrape but before the metric is saved in the storage system. This allows us to determine what metrics we save, what we drop, and what those metrics will look like. This is done in the metric_relabel_configs block in our job.\n\n TIP The easiest way to remember the two phases are: relabel_configs hap- pens before the scrape and metric_relabel_configs happens after the scrape.\n\nLet’s take a look at some relabelling of our cAdvisor metrics. cAdvisor collects a lot of data. Not all of it is always useful. So let’s see how we might drop some of these metrics before they hit our storage and take up unnecessary space.\n\nVersion: v1.0.0 (427b8e9)\n\n107",
      "content_length": 1640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.17: Dropping metrics with relabelling\n\njob_name: 'docker' static_configs:\n\ntargets: ['138.197.26.39:8080', '138.197.30.147:8080', '\n\n138.197.30.163:8080']\n\nmetric_relabel_configs:\n\nsource_labels: [__name__]\n\nregex: '(container_tasks_state|\n\ncontainer_memory_failures_total)'\n\naction: drop\n\nHere we have our docker job. After our static_configs block we’ve added a new block: metric_relabel_configs. Inside the block we specify a series of rela- belling actions.\n\nDropping metrics\n\nLet’s look at our first action. We select the metrics we want to take action on using the source_labels parameter. This takes an array of label names. In our case we’re using the __name__ label. The __name__ label is a reserved label used for the name of a metric. So our source label for our docker job in this case would be the concatenated names of all the metrics scraped from cAdvisor.\n\nMultiple labels are concatenated together using a separator, by default ;. The separator can be overridden using the separator parameter.\n\nVersion: v1.0.0 (427b8e9)\n\n108",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.18: Specifying a new separator\n\n. . . metric_relabel_configs:\n\nsource_labels: [__name__]\n\nseparator: ',' regex: '(container_tasks_state|\n\ncontainer_memory_failures_total)'\n\naction: drop\n\nHere our __name__ label values would be separated with a ,.\n\nNext, we specify a regular expression to search our concatenated metric names and match specific names. The regular expression uses the RE2 expression syntax, which is what the Go regular expression’s library RegExp uses.\n\n TIP Suck at regular expressions? You’re not alone. There are some good\n\nexpression testers available online.\n\nOur regular expression, contained in the regex parameter, is:\n\n(container_tasks_state|container_memory_failures_total)\n\nWhich will match and capture two metrics:\n\ncontainer_tasks_state • container_memory_failures_total\n\nIf we had specified multiple source labels we would specify each regular expres- sion using the separator, for example:\n\nregex1;regex2;regex3\n\nVersion: v1.0.0 (427b8e9)\n\n109",
      "content_length": 1030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nWe then perform an action, specified in the action parameter. In this case, both of these metrics contain a significant number of time series—of potentially limited usefulness—so we’re taking the drop action. This will drop the metrics before stor- age. Other actions include keep, which keeps the metrics that match the regular expression and drops all others.\n\nReplacing label values\n\nWe can also replace a label’s value with a new value. Let’s take an example. Many cAdvisor metrics have an id label that contains the name of the running process. If that process is a container we’ll see something like:\n\nid=\"/docker/6fca3002e3513d23ed7e435ca064f557ed1d4226ef788e771b8f933a49d55804\n\n\"\n\nThis is a bit unwieldy. So we’d like to take the container ID:\n\n6fca3002e3513d23ed7e435ca064f557ed1d4226ef788e771b8f933a49d55804\n\nAnd put it into a new label: container_id. Using relabelling we can do this like so:\n\nListing 4.19: Replacing a label\n\nmetric_relabel_configs: - source_labels: [id]\n\nregex: '/docker/([a-z0-9]+);' replacement: '$1' target_label: container_id\n\n TIP Relabelling is applied sequentially, using top-down ordering in the con-\n\nfiguration file.\n\nVersion: v1.0.0 (427b8e9)\n\n110",
      "content_length": 1233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nOur source label is id. We then specify a regex to match and capture the container ID. The replacement field holds the new value, in this case our capture group $1. We then specify the destination for the captured information, here container_id, in the target_label parameter.\n\nYou’ll notice we didn’t specify an action for this relabel. This is because the default action is replace. If you don’t specify an action, Prometheus will assume you want to perform a replacement.\n\nPrometheus also has a parameter, honor_labels, that controls the conflict be- havior if you try to overwrite and attach a label that already exists. Let’s say your scraped data already has a label called job. Using the default behavior, in which honor_labels is set to false, Prometheus will rename the existing label by prefixing it with exported_. So our job label would become exported_job. If honor_labels is set to true then Prometheus will keep the label on the scraped data and ignore any relabelling on the server.\n\nDropping labels\n\nIn our last example, we’re going to drop a label. This is often useful for hiding sensitive information or simplifying a time series. In this (somewhat contrived) example we’re going to remove the kernelVersion label, hiding the kernel version of our Docker hosts.\n\nListing 4.20: Dropping a label\n\nmetric_relabel_configs:\n\nregex: 'kernelVersion' action: labeldrop\n\nFor dropping a label, we specify a regex watching our label and then the labeldrop action. This will remove all labels that match the regular expression.\n\nVersion: v1.0.0 (427b8e9)\n\n111",
      "content_length": 1611,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nThis action also has an inverse action, labelkeep, which will keep all labels that match the regular expression and drop all others.\n\n WARNING Remember that labels are uniqueness constraints for time\n\nseries. If you drop a label and that results in duplicate time series, you will have issues!\n\nUsefully, you can see the state of labels prior to relabelling in the Prometheus web interface. We can see this in the list of targets at http://localhost:9090/ targets. Hover your mouse over the instance label in the Labels box to see a list of the labels as they were before relabelling.\n\nFigure 4.5: Labels prior to relabelling\n\nNow let’s take a closer look at our new metrics.\n\nThe Node Exporter and cAdvisor metrics\n\nWe’re now collecting seven individual sets of metrics from four unique hosts:\n\nVersion: v1.0.0 (427b8e9)\n\n112",
      "content_length": 871,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nThe Prometheus server’s own metrics. • Node Exporter metrics from three hosts. • cAdvisor metrics from three hosts.\n\n TIP You can see the status of each target being scraped by looking at the Prometheus web interface. Browse to http://localhost:9090/targets to see a list of what Prometheus is scraping and the status of each.\n\nLet’s skip over the Prometheus server’s own metrics and focus on the Node Ex- porter and cAdvisor metrics. Let’s use some of these metrics to explore the capa- bilities of Prometheus and ensure our hosts are properly monitored.\n\nThe trinity and the USE method\n\nWe’re going to make use of one of the monitoring frameworks we introduced at the start of the book: the USE Method. You’ll remember this method suggests collecting and focusing on utilization, saturation, and error metrics to assist with performance diagnostics. We’re going to apply this method, broadly, to one of the common monitoring patterns—CPU, memory, and disk—to see how we can make use of our Node Exporter metrics and how PromQL can be used.\n\n TIP Remember, these host metrics are useful mostly as broad signals of\n\nperformance trouble on your hosts. We’re using them to learn more about working with metrics. Most of the time, though, we’re going to focus on application metrics, which are better indicators of poor user experience.\n\nVersion: v1.0.0 (427b8e9)\n\n113",
      "content_length": 1411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nLet’s start by looking at CPU metrics.\n\nCPU Utilization\n\nTo get the U-for-utilization in USE, we’re going to use a metric the Node Exporter collects named node_cpu. This is the utilization of the CPUs on our host, broken down by mode and presented in seconds used. Let’s query for that metric now from the Prometheus web interface. Navigate to http://localhost:9090/graph, select node_cpu from the metric dropdown and click Execute.\n\nFigure 4.6: node_cpu metrics\n\nYou should see a list of metrics, much like so:\n\nnode_cpu{cpu=\"cpu0\",instance=\"138.197.26.39:9100\",job=\"node\",mode=\"\n\nuser\"}\n\nThe node_cpu metric has a number of labels including the instance and job labels, which identify what host it came from and what job scraped the metric, respec- tively.\n\n NOTE The instance label is generally made up of the address of the host\n\nand the port that was scraped.\n\nVersion: v1.0.0 (427b8e9)\n\n114",
      "content_length": 941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nWe also have two labels specific to CPUs: the cpu the metric was collected from— for example, cpu0—and mode for the CPU mode being measured—for example, user, system, idle, etc. Drawn from /proc/stat, the data are counters that tell us how many seconds each CPU spent in each mode.\n\nThis list of metrics isn’t overly useful as is. For any performance analysis, we’ll need to make use of PromQL to turn these into useful metrics. What we’d really like here is to get the percentage CPU used on each instance—but to get there we’ll need to work with our metrics a little. Let’s step towards this outcome by looking at a sequence of PromQL calculations.\n\nWe start with calculating the per-second rate for each CPU mode. PromQL has a function called irate that calculates the per-second instant rate of increase of a time series in a range vector. Let’s use the irate function over our node_cpu metric. Enter this into the query box:\n\nirate(node_cpu{job=\"node\"}[5m])\n\nAnd click Execute. This wraps the node_cpu metric in the irate function and queries a five-minute range. It’ll return the list of per-cpu, per-mode metrics from the node job, now represented as per-second rates in a five-minute range. But this still isn’t overly helpful—we need to aggregate our metrics across CPUs and modes too.\n\nTo do this, we can use the avg or average operator and the by clause we saw in Chapter 3.\n\navg(irate(node_cpu{job=\"node\"}[5m])) by (instance)\n\nNow we’ve wrapped our irate function inside an avg aggregation and added a by clause that aggregates by the instance label. This will produce three new metrics that average CPU usage by host using the values from all CPUs and all modes.\n\nIt still includes idle usage, and it isn’t But this metric is still not quite right. represented in a useful form like a percentage. Let’s constrain our calculation by\n\nVersion: v1.0.0 (427b8e9)\n\n115",
      "content_length": 1920,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nquerying only the per-instance idle usage and, as it’s already a ratio, multiplying it by 100 to convert it into a percentage.\n\navg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m])) by (instance) * 100\n\nHere we’ve added the mode label with a value of idle to our irate query. This only queries the idle data. We’ve averaged the result by instance and multiplied it by 100. Now we have the average percentage of idle usage in a five-minute range on each host. We can turn this into the percentage used by subtracting this value from 100, like so:\n\n100 - avg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m])) by (instance)\n\n100\n\nAnd now we have three metrics, one for each host, showing the average percentage CPU used in a five-minute window.\n\nFigure 4.7: Per-host average percentage CPU usage metrics\n\nNow, if we click the Graph tab, we can also see these represented as a plot.\n\nVersion: v1.0.0 (427b8e9)\n\n116",
      "content_length": 946,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nFigure 4.8: Per-host percentage CPU plot\n\nCPU Saturation\n\nOne of the ways to get the saturation of CPU on a host is to track the load average, essentially the average run queue length over a time period, taking into consider- ation the number of CPUs on the host. An average less than the number of CPUs is generally normal; averages over that number for prolonged periods indicate the CPU is saturated.\n\nTo see the host’s load average, we can use the node_load* metrics for these. They show load average over one minute, five minutes, and 15 minutes. We’re going to use the one-minute load average: node_load1.\n\nLet’s take a quick look at this metric. Select node_load1 from the metric dropdown and click Execute. A list of the nodes being monitored with the current one-minute load average will be listed.\n\nWe also need to calculate the number of CPUs on our hosts. We can do this using the count aggregation like so:\n\ncount by (instance)(node_cpu{mode=\"idle\"})\n\nVersion: v1.0.0 (427b8e9)\n\n117",
      "content_length": 1039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nHere we’re counting the number of occurrences of the node_cpu time series with a mode of idle. We’re then using the by clause to remove all labels except instance from the result vector, giving us a list of hosts with the number of CPUs in each.\n\nFigure 4.9: Number of CPUs in each host\n\nWe can see our three nodes have two CPUs apiece.\n\n TIP Since we’re also collecting Docker metrics we could use one of cAdvisor’s metrics here too, machine_cpu_cores, as a shortcut.\n\nWe can then combine this count with the node_load1 metric like so:\n\nnode_load1 > on (instance) 2 * count by (instance)(node_cpu{mode=\"idle\n\n\"})\n\nHere we’re showing if the one-minute load average is two times more than the CPU count on the host. This is not necessarily an issue, but we’ll see in Chapter 6 how to turn it into an alert that should tell you when there is an issue.\n\nNow let’s see if we can’t do something similar with our memory metrics.\n\n TIP We’re going to skip the E-for-error in USE for CPU errors because it’s\n\nunlikely there will be anything useful in any data we could collect.\n\nVersion: v1.0.0 (427b8e9)\n\n118",
      "content_length": 1147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nMemory utilization\n\nLet’s look at the utilization of memory on a host. The Node Exporter’s memory metrics are broken down by type and usage of memory. You’ll find them in the list of metrics prefixed with node_memory.\n\nFigure 4.10: The node_memory_MemTotal\n\nWe’re going to focus on a subset of the node_memory metrics to provide our uti- lization metric:\n\nnode_memory_MemTotal - The total memory on the host. • node_memory_MemFree - The free memory on the host. • node_memory_Buffers - The memory in buffer cache. • node_memory_Cached - The memory in the page cache.\n\nAll of these metrics are represented in bytes.\n\nWe’re going to use this combination of metrics to calculate the percentage of memory used on each host. To do this, we’re going to add the values of the node_memory_MemFree, node_memory_Cached, and node_memory_Buffers metrics. This represents the free memory on our host. We’re then going to calculate the\n\nVersion: v1.0.0 (427b8e9)\n\n119",
      "content_length": 997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\npercentage of free memory using this value and the node_memory_MemTotal metric. We’re going to use this query to do that:\n\n(node_memory_MemTotal - (node_memory_MemFree + node_memory_Cached +\n\nnode_memory_Buffers)) / node_memory_MemTotal * 100\n\nHere we’ve added together our three memory metrics, subtracted them from the total, divided by the total, and then multiplied by 100 to convert it into a per- centage. This will produce three metrics showing percentage memory used per host.\n\nFigure 4.11: Per-host percentage memory usage\n\nHere we don’t need to use the by clause to preserve distinct dimensions because the metrics have the same dimensional labels; each metric will have the query applied to it in turn.\n\nMemory saturation\n\nWe can also monitor our memory saturation by checking on the rate of paging in and out of memory. We can use data gathered from /proc/vmstat on paging exposed in two Node Exporter metrics:\n\nnode_vmstat_pswpin - Number of kilobytes the system has paged in from\n\ndisk per second.\n\nVersion: v1.0.0 (427b8e9)\n\n120",
      "content_length": 1087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nnode_vmstat_pswpout - Number of kilobytes the system has paged out to\n\ndisk per second.\n\nBoth are totals in kilobytes since last boot.\n\nTo get our saturation metric, we generate a one-minute rate for each metric, add the two rates, and then multiply them by 1024 to get bytes. Let’s create a query to do this now.\n\nListing 4.21: Memory saturation query\n\n1024 * sum by (instance) (\n\n(rate(node_vmstat_pgpgin[1m]) + rate(node_vmstat_pgpgout[1m]))\n\n)\n\nWe can then graph or alert on this to identify hosts with misbehaving applications.\n\nDisk usage\n\nFor disks we’re only going to measure disk usage rather than utilization, saturation, or errors. This is because it’s the most useful data in most cases for visualization and alerting. The Node Exporter’s disk usage metrics are in the list of metrics prefixed with node_filesystem.\n\nVersion: v1.0.0 (427b8e9)\n\n121",
      "content_length": 903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nFigure 4.12: Disk metrics\n\nHere, for example, the node_filesystem_size metric shows the size of each file system mount being monitored. We can use a similar query to our memory metrics to produce a percentage figure of disk space used on our hosts.\n\n(node_filesystem_size{mountpoint=\"/\"} - node_filesystem_free{mountpoint\n\n=\"/\"}) / node_filesystem_size{mountpoint=\"/\"} * 100\n\nUnlike the memory metrics, though, we have filesystem metrics per mount point on each host. So we’ve added the mountpoint label, specifically the / filesystem mount. This will return a disk usage metric for that filesystem on each host being monitored.\n\nFigure 4.13: Per-host disk space metrics\n\nIf we wanted or needed to, we could add additional queries for specific mount points to the configuration now. To monitor a mount point called /data we would use:\n\n(node_filesystem_size{mountpoint=\"/data\"} - node_filesystem_free{\n\nmountpoint=\"/data\"}) / node_filesystem_size{mountpoint=\"/data\"} * 100\n\nOr we could use a regular expression to match more than one mountpoint.\n\n(node_filesystem_size{mountpoint=~\"/|/run\"} - node_filesystem_free{\n\nmountpoint=~\"/|/run\"}) / node_filesystem_size{mountpoint=~\"/|/run\"} *\n\n100\n\nVersion: v1.0.0 (427b8e9)\n\n122",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\n TIP You cannot use a regular expression that matches an empty string.\n\nYou can see that we’ve updated our mountpoint label to change the operator from = to =~ which tells Prometheus that the right-hand-side value will be a regular expression. We’ve then matched both the /run and / root filesystems.\n\n TIP There’s also a operator for regular expressions that do not match.\n\nThis is still a fairly old-school measure of disk usage. It tells us a current percentage usage of the filesystem. In many cases, this data is useless. An 80 percent full 1 GB filesystem might not be a concern at all if it’s growing at 1 percent a year. A 10 percent full 1 TB filesystem might be at serious risk of filling up if it’s growing at 10 percent every 10 minutes. With disk space, we really need to understand the trend and direction of a metric. The question we usually want answered is: “Given the usage of the disk now, combined with its growth, in what time frame will we run out of disk space?”\n\nPrometheus actually has a mechanism, a function called predict_linear, by which we can construct a query to answer this exact question. Let’s look at an example:\n\npredict_linear(node_filesystem_free{mountpoint=\"/\"}[1h], 4*3600) < 0\n\nHere we’re grabbing the root filesystem, node_filesystem_free{mountpoint=\"/\"} . We could select all the filesystems by specifying the job name or selectively using a regular expression, as we did earlier in this section.\n\npredict_linear(node_filesystem_free{job=\"node\"}[1h], 4*3600) < 0\n\nWe have selected a one-hour time window, [1h]. We’ve also placed this time\n\nVersion: v1.0.0 (427b8e9)\n\n123",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nseries snapshot inside the predict_linear function. The function uses simple linear regression to determine when a filesystem will run out of space based on previous growth. The function takes a range vector, our one-hour window, and the point in the future for which to predict the value, measured in seconds. Hence, four times 3600 (the number of seconds in an hour), or four hours. The < 0 filters for values less than 0, i.e., the filesystem running out of space.\n\nSo, if, based on the last hour’s worth of growth history, the filesystem is going to run out of space in the next four hours, the query will return a negative number, which we can then use to trigger an alert. We’ll see how this alert would work in Chapter 6.\n\nService status\n\nNow let’s look at the data from the systemd collector. Remember this shows us the state of services and various other systemd configuration on our hosts. The state of the services is exposed in the node_systemd_unit_state metric. There’s a metric for each service and service state you’re collecting. In our case we’re only gathering metrics for the Docker, SSH, and RSyslog daemons.\n\nListing 4.22: The node_systemd_unit_state metrics\n\nnode_systemd_unit_state{name=\"docker.service\",state=\"activating\" } 0 node_systemd_unit_state{name=\"docker.service\",state=\"active\"} 1 node_systemd_unit_state{name=\"docker.service\",state=\" deactivating\"} 0 node_systemd_unit_state{name=\"docker.service\",state=\"failed\"} 0 node_systemd_unit_state{name=\"docker.service\",state=\"inactive\"} 0 . . .\n\nWe can query a segment of this data via the Expression Browser and look specifi-\n\nVersion: v1.0.0 (427b8e9)\n\n124",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\ncally for this docker service. To do this, we query using the name label.\n\nnode_systemd_unit_state{name=\"docker.service\"}\n\nFigure 4.14: The systemd time series data\n\nThis query produces a metric for each combination of potential service and state: failed, inactive, active, etc. The metric that represents the current state of each service is set to 1. We could narrow this down further by adding the state label to our search and only returning the active state.\n\nnode_systemd_unit_state{name=\"docker.service\",state=\"active\"}\n\nAlternatively, we could search for all of the metrics with the value 1, which would return the state of the current service.\n\nnode_systemd_unit_state{name=\"docker.service\"} == 1\n\nHere we’ve seen a new query, one that uses a comparison binary operator: ==. This will retrieve all metrics with a value equal to 1 with a name label of docker. service.\n\nVersion: v1.0.0 (427b8e9)\n\n125",
      "content_length": 952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nFigure 4.15: The active services\n\nWe’re going to make use of the systemd metrics to monitor the availability of ser- vices on our host—for example, our Docker daemon—and alert on this in Chapter 6.\n\nAvailability and the up metric\n\nWorth mentioning is another useful metric for monitoring the state of specific nodes: the up metric. For each instance scrape, Prometheus stores a sample in the following time series:\n\nListing 4.23: The up metric\n\nup{job=\"<job−name>\", instance=\"<instance−id>\"}\n\nThe metric is set to 1 if the instance is healthy—i.e., the scrape successfully returned—or to 0 if the scrape failed. The metric is labelled with the job name and the instance of the time series.\n\n TIP Prometheus also populates some other instrumentation metrics, includ- ing scrape_duration_seconds, the duration of the scrape, and scrape_samples_- scraped, the number of samples that the target exposed.\n\nVersion: v1.0.0 (427b8e9)\n\n126",
      "content_length": 976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nWe can query all the up metrics for our hosts.\n\nFigure 4.16: The up metrics\n\nIn addition, many exporters have specific metrics designed to identify the last successful scrape of a service. The cAdvisor metrics include container_last_seen, for example, which provides a list of containers and the last time they were active. The MySQL Exporter returns a metric, mysql_up, that is set to 1 if a successful SELECT query works on a database server.\n\nWe’ll see in Chapter 6 how we can use the up metrics to help us do availability monitoring.\n\n NOTE You cannot relabel autopopulated metrics like up because they are\n\ngenerated after the relabelling phase.\n\nVersion: v1.0.0 (427b8e9)\n\n127",
      "content_length": 727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nThe metadata metric\n\nLast, let’s look at the metric we created, metadata, using the Node Exporter’s Textfile collector.\n\nmetadata{role=\"docker_server\",datacenter=\"NJ\"} 1\n\nThis metric provides context for the resource: its role, docker_server, and the location of the host, datacenter. This data is useful in its own right, but why create a separate metric rather than just add these as labels to all of our metrics from this host? Well, we already know that labels provide the dimensions of our time series and, combined with the metric name, they make up the identity of our time series. We’ve also already been warned that:\n\nChanging a label or adding a new label creates a new time series.\n\nThis means that labels should be used judiciously and should remain as constant as possible. So, instead of decorating every time series with the set of complete labels, we instead create a time series that we can use to query specific types or classes of resources.\n\nLet’s see how we could make use of the labels on this metric. Suppose we want to select metrics only from a specific data center or set of data centers. We can quickly find all hosts in, say, a non-New Jersey (NJ) data center by querying like so:\n\nmetadata{datacenter != \"NJ\"}\n\nYou can see that we’ve queried the metadata metric and specified the datacenter with an operator, != or not equal to, to return any metadata metric from a non- New Jersey data center.\n\n TIP Prometheus has a full set of arithmetic and comparison binary operators\n\nyou can use.\n\nVersion: v1.0.0 (427b8e9)\n\n128",
      "content_length": 1592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nVector matches\n\nWe can also use our metadata metric to make vector matches. Vector matches can use any of the PromQL binary operators. Vector matches attempt to find a matching element in the right-hand-side vector for each entry on the left-hand side.\n\nThere are two kinds of vector matches: One-to-one and many-to-one (or one-to- many).\n\nOne-to-one matches\n\nOne-to-one matches find a unique pair of entries from each side of the operation. Two entries match if they have the exact same set of labels and values. You can modify the set of labels considered by using the ignoring modifier, which ignores specific labels, or by using the on modifier, which reduces the set of considered labels to a list. Let’s see an example.\n\nListing 4.24: A one-to-one vector match\n\nnode_systemd_unit_state{name=\"docker.service\"} == 1\n\nand on (instance, job)\n\nmetadata{datacenter=\"SF\"}\n\nThis queries any node_systemd_unit_state metrics with the name label of docker .service and a value of 1. We then use the on modifier to reduce the consid- ered label set to the instance and job labels of the metadata metric, where the datacenter label has a value of SF.\n\nIn our case, this will return a single metric:\n\nnode_systemd_unit_state{instance=\"138.197.30.147:9100\",job=\"node\",name\n\n=\"docker.service\",state=\"active\"}\n\nVersion: v1.0.0 (427b8e9)\n\n129",
      "content_length": 1374,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nIf we were to change the datacenter label in our query to NJ, we’d return two metrics: one for each of the two Docker servers in the NJ data center.\n\nMany-to-one and one-to-many matches\n\nMany-to-one and one-to-many matches are where each vector element on the “one” side can match with multiple elements on the “many” side. These matches are explicitly specified using the group_left or group_right modifiers, where left or right determines which vector has the higher cardinality. The Prometheus doc- umentation contains some examples of this kind of match, but they are generally not required. In most cases one-to-one matches will suffice.\n\nMetadata-style metrics\n\nMany existing exporters use this “metadata” pattern to provide information about extra state—for example, cAdvisor has the cadvisor_version metric that provides information about the local Docker daemon and related configuration.\n\nFigure 4.17: The cadvisor_version metric\n\nThis type of metric allows you to use vector matches to list all metrics that match some contextual criteria: a location, a version, etc.\n\nSo now that we’ve seen how to use some of the metrics, how do we persist the queries we’ve seen?\n\nVersion: v1.0.0 (427b8e9)\n\n130",
      "content_length": 1252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nQuery permanence\n\nUntil now, we’ve just run queries in the Expression Browser. Whilst viewing the output of that query is interesting, the result is stuck on the Prometheus server and is transitory. There are three ways we can make our queries more permanent:\n\nRecording rules - Create new metrics from queries. • Alerting rules - Generate alerts from queries. • Visualization - Visualize queries using a dashboard like Grafana.\n\nThe queries we’ve looked at can be used interchangeably in all three of these mechanisms because all of these mechanisms can understand and execute PromQL queries.\n\nIn this chapter we’re going to make use of some recording rules to create new metrics from our queries and configure Grafana as a dashboard to visualize metrics. In Chapter 6 we’ll make use of alerting rules to generate alerts.\n\nRecording rules\n\nWe talked about recording rules and their close cousin, alerting rules in Chapter 2.\n\nRecording rules are a way to compute new time series, particularly aggregated time series, from incoming time series. We might do this to:\n\nProduce aggregates across multiple time series. • Precompute expensive queries. • Produce a time series that we could use to generate an alert.\n\nLet’s write some rules.\n\nVersion: v1.0.0 (427b8e9)\n\n131",
      "content_length": 1311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nConfiguring recording rules\n\nRecording rules are stored on the Prometheus server, in files that are loaded by the Prometheus server. Rules are calculated automatically, with a frequency controlled by the evaluation_interval parameter in the global block of the prometheus.yml configuration file we saw in Chapter 3.\n\nListing 4.25: The evaluation_interval parameter\n\nglobal:\n\nscrape_interval: 15s evaluation_interval: 15s\n\n. . .\n\nRule files are specified in our Prometheus configuration inside the rules_files block.\n\nLet’s create a sub-directory called rules in the same directory as our prometheus. yml file, to hold our recording rules. We’ll also create a file called node_rules.yml for our node metrics. Prometheus rules are written, like Prometheus configuration, in YAML.\n\n WARNING YAML rules were updated in Prometheus 2.0. Earlier re-\n\nleases used a different structure. Your older rule files will not work in Prometheus 2.0 or later. You can use the promtool to upgrade older rules files. There’s a good blog post on the upgrading process here.\n\nVersion: v1.0.0 (427b8e9)\n\n132",
      "content_length": 1130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.26: Creating a recorded rules file\n\n$ mkdir -p rules $ cd rules $ touch node_rules.yml\n\nLet’s add that file to our Prometheus configuration in the rule_files block in the prometheus.yml file.\n\nListing 4.27: Adding the rules file\n\nrule_files:\n\n\"rules/node_rules.yml\"\n\nNow let’s populate this file with some rules.\n\nAdding recording rules\n\nLet’s convert our CPU, memory, and disk calculations into recording rules. We have a lot of hosts to be monitored, so we’re going to precompute all the trinity queries. That way we’ll also have the calculations as metrics that we can alert on or visualize via a dashboard like Grafana.\n\nLet’s start with our CPU calculation.\n\nVersion: v1.0.0 (427b8e9)\n\n133",
      "content_length": 748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.28: A recording rule\n\ngroups: - name: node_rules\n\nrules: - record: instance:node_cpu:avg_rate5m\n\nexpr: 100 - avg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m]))\n\nby (instance) * 100\n\nRecording rules are defined in rule groups; here ours is named node_rules. Rule group names must be unique in a server. Rules within a group are run sequentially at a regular interval. By default, this is the global evaluation_interval, but it can be overridden in the rule group using the interval clause.\n\nThe sequential nature of rule execution in groups means that you can use rules you create in subsequent rules. This allows you to create a metric from a rule and then reuse that metric in a later rule. This is only true within rule groups though—rule groups are run concurrently, so it’s not safe to use rules across groups.\n\n TIP This also means you can use recording rules as parameters, for example\n\nyou might want to create a rule with a threshold in it. You can then set the threshold once in the rule and re-use it multiple times. If you need to change the threshold you just need to change it that one place.\n\nVersion: v1.0.0 (427b8e9)\n\n134",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.29: A recording group interval\n\ngroups: - name: node_rules interval: 10s rules:\n\nThis would update the rule group to be run every 10 seconds rather than the globally defined 15 seconds.\n\nNext, we have a YAML block called rules, which contains this group’s recording rules. Each rule contains a record, which tells Prometheus what to name the new time series. You should name rules so you can identify quickly what they represent. The general recommended format is:\n\nlevel:metric:operations\n\nWhere level represents the aggregation level and labels of the rule output. Met- ric is the metric name and should be unchanged other than stripping _total off counters when using the rate() or irate() functions. This makes it easier to find the new metric. Finally, operations is a list of operations that were applied to the metric, the newest operation first.\n\nSo our CPU query would be named:\n\ninstance:node_cpu:avg_rate5m\n\n TIP There are some useful best practices on naming in the Prometheus doc-\n\numentation.\n\nWe then specify an expr field to hold the query that should generate the new time series.\n\nVersion: v1.0.0 (427b8e9)\n\n135",
      "content_length": 1184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nWe could also add a labels block to add new labels to the new time series. Time series created from rules inherit the relevant labels of the time series used to create them, but you can also add or overwrite labels. For example:\n\nListing 4.30: A recording rule\n\ngroups: - name: node_rules\n\nrules: - record: instance:node_cpu:avg_rate5m\n\nexpr: 100 - avg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m]))\n\nby (instance) * 100\n\nlabels:\n\nmetric_type: aggregation\n\nLet’s create rules for some of our other trinity queries, and add them, too.\n\nListing 4.31: A recording rule\n\ngroups: - name: node_rules\n\nrules: - record: instance:node_cpu:avg_rate5m\n\nexpr: 100 - avg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m]))\n\nby (instance) * 100\n\nrecord: instance:node_memory_usage:percentage expr: (node_memory_MemTotal - (node_memory_MemFree +\n\nnode_memory_Cached + node_memory_Buffers)) / node_memory_MemTotal * 100\n\nrecord: instance:root:node_filesystem_usage:percentage expr: (node_filesystem_size{mountpoint=\"/\"} -\n\nnode_filesystem_free{mountpoint=\"/\"}) / node_filesystem_size{ mountpoint=\"/\"} * 100\n\nVersion: v1.0.0 (427b8e9)\n\n136",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\n TIP The configuration files and code for this book are located on GitHub.\n\nWe now need to restart or SIGHUP the Prometheus server to activate the new rules. This will create a new time series for each rule. You should be able to find the new time series on the server in a few moments.\n\n TIP The rule files can be reloaded at runtime by sending SIGHUP to the\n\nPrometheus process (or by restarting on Microsoft Windows). The reload will only work if the rules file is well formatted. The Prometheus server ships with a utility called promtool that can lint rule files.\n\nIf we now search for one of the new time series, instance:node_cpu:avg_rate5m for example, we should see:\n\nFigure 4.18: The node_cpu recorded rule\n\nLast, let’s quickly look at how we might visualize the metrics we’ve just created.\n\n TIP You can see the current rules defined on your server in the /rules path\n\nVersion: v1.0.0 (427b8e9)\n\n137",
      "content_length": 957,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nof the Web UI. This includes useful information, like the execution time of each rule, that can help you debug expensive rules that might need optimization.\n\nVisualization\n\nAs we’ve seen, Prometheus has an inbuilt dashboard and graphing interface. It’s fairly simple and generally best for reviewing metrics and presenting solitary graphs. To add a more fully featured visualization interface to Prometheus, the platform integrates with the open-source dashboard Grafana. Grafana is a dash- board fed via data sources. It supports a variety of formats including Graphite, Elasticsearch, and Prometheus.\n\nIt’s important to note that Prometheus isn’t generally used for long-term data retention—the default is 15 days worth of time series. This means that Prometheus is focused on more immediate monitoring concerns than, perhaps, other systems where visualization and dashboards are more important. The judicious use of the Expression Browser, graphing inside the Prometheus UI, and building appropriate alerts are often more practical uses of Prometheus’ time series data than building extensive dashboards.\n\nWith that said, in this last section we’re going to quickly install Grafana and con- nect Prometheus to it.\n\nInstalling Grafana\n\nInstalling Grafana depends on the platform you’re installing on. Grafana supports running on Linux, Microsoft Windows, and Mac OS X. Let’s look at installation on each platform.\n\nVersion: v1.0.0 (427b8e9)\n\n138",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nInstalling Grafana on Ubuntu\n\nFor Ubuntu and Debian systems, we can add the Grafana package repository. We first need to add the PackageCloud public key, like so:\n\nListing 4.32: Getting the PackageCloud public key on Ubuntu\n\n$ curl https://packagecloud.io/gpg.key | sudo apt-key add -\n\nWe add the following Apt configuration so we can find the Grafana repository:\n\nListing 4.33: Adding the Grafana packages\n\n$ echo \"deb https://packagecloud.io/grafana/stable/debian/ stretch main\" | sudo tee -a /etc/apt/sources.list.d/grafana.list\n\nThen we update Apt and install the grafana package with the apt-get command.\n\nListing 4.34: Updating Apt and installing the Grafana package\n\n$ sudo apt-get update $ sudo apt-get install grafana\n\nOn Red Hat\n\nTo install Grafana on Red Hat systems, we first need to add the Elastic.co public key, like so:\n\nVersion: v1.0.0 (427b8e9)\n\n139",
      "content_length": 911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.35: Getting the Grafana public key on Red Hat\n\n$ sudo rpm --import https://packagecloud.io/gpg.key\n\nThen we add the following to our /etc/yum.repos.d/ directory in a file called grafana.repo:\n\nListing 4.36: The Grafana Yum configuration\n\n[grafana] name=grafana baseurl=https://packagecloud.io/grafana/stable/el/7/$basearch repo_gpgcheck=1 enabled=1 gpgcheck=1 gpgkey=https://packagecloud.io/gpg.key https://grafanarel.s3. amazonaws.com/RPM-GPG-KEY-grafana sslverify=1 sslcacert=/etc/pki/tls/certs/ca-bundle.crt\n\nNow we install Grafana using the yum or dnf commands.\n\nListing 4.37: Installing Grafana on Red Hat\n\n$ sudo yum install grafana\n\nInstalling Grafana on Microsoft Windows\n\nTo install Grafana on Microsoft Windows, we need to download and put it in a directory. Let’s create a directory for the executable using Powershell.\n\nVersion: v1.0.0 (427b8e9)\n\n140",
      "content_length": 916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.38: Creating a Grafana directory on Windows\n\nC:\\> MKDIR grafana C:\\> CD grafana\n\nNow download Grafana from the Grafana site.\n\nListing 4.39: Grafana Windows download\n\nhttps://s3-us-west-2.amazonaws.com/grafana-releases/release/ grafana-5.1.3.windows-x64.zip\n\nThe zip file contains a folder with the current Grafana version. Unzip the file using a tool like 7-Zip, and put the contents of the unzipped directory into the C:\\grafana directory. Finally, add the C:\\grafana directory to the path. This will allow Windows to find the executable. To do this, run this command inside Powershell.\n\nListing 4.40: Setting the Windows path for Grafana\n\n$env:Path += \";C:\\grafana\"\n\nWe then need to make some quick configuration changes to adjust the default port. The default Grafana port is 3000, a port which requires extra permissions on Microsoft Windows. We want to change it to port 8080 to make Grafana easier to use.\n\nGo into the c:\\grafana\\conf\\ directory and copy the sample.ini file to custom. ini. Edit the custom.ini file and uncomment the http_port configuration option. It’ll be prefixed with the ; character, which is the comment character in ini files.\n\nVersion: v1.0.0 (427b8e9)\n\n141",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nChange the port number to 8080. That port will not require extra Microsoft Win- dows privileges.\n\nInstalling Grafana on Mac OS X\n\nGrafana is also available from Homebrew. If you use Homebrew to provision your Mac OS X hosts then you can install Grafana via the brew command.\n\nListing 4.41: Installing Grafana via Homebrew\n\n$ brew install grafana\n\nInstalling Grafana via configuration management or a stack\n\nThere are a variety of options for installing Grafana via configuration manage- ment. Many of the stacks and configuration management modules we saw in Chapter 3 also support Grafana installations.\n\nChef cookbooks for Grafana at https://supermarket.chef.io/cookbooks/ grafana.\n\nPuppet modules for Grafana at https://forge.puppetlabs.com/modules? utf-8=%E2%9C%93&sort=rank&q=grafana.\n\nAnsible roles for Grafana at https://galaxy.ansible.com/list#/roles/ 3563.\n\nDocker images Grafana at https://hub.docker.com/search/?q=grafana.\n\nStarting and configuring Grafana\n\nThere are two places where we can configure Grafana: a local configuration file and the Grafana web interface. The local configuration file, which is primarily for\n\nVersion: v1.0.0 (427b8e9)\n\n142",
      "content_length": 1208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nconfiguring server-level settings like authentication and networking, is available at either:\n\n/etc/grafana/grafana.ini on Linux. • /usr/local/etc/grafana/grafana.ini on OS X. • c:\\grafana\\conf\\custom.ini on Microsoft Windows.\n\nThe Grafana web interface is used to configure the source of our data and our graphs, views, and dashboards. Our configuration in this chapter is going to be via the web interface.\n\nTo access the web interface we need to start the Grafana web service, so let’s do that first. On Linux we’d use the service.\n\nListing 4.42: Starting the Grafana Server on Linux\n\n$ sudo service grafana-server start\n\nThis will work on both Ubuntu and Red Hat.\n\nOn OS X, if we want to start Grafana at boot, we need to run:\n\nListing 4.43: Starting Grafana at boot on OSX\n\n$ brew services start grafana\n\nOr to start it ad hoc on OS X, run:\n\nVersion: v1.0.0 (427b8e9)\n\n143",
      "content_length": 921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nListing 4.44: Starting Grafana server on OS X\n\n$ grafana-server --config=/usr/local/etc/grafana/grafana.ini -- homepath /usr/local/share/grafana cfg:default.paths.logs=/usr/ local/var/log/grafana cfg:default.paths.data=/usr/local/var/lib/ grafana cfg:default.paths.plugins=/usr/local/var/lib/grafana/ plugins\n\nOn Microsoft Windows we would run the grafana-server.exe executable and use something like NSSM if we want to run it as a service.\n\nConfiguring the Grafana web interface\n\nGrafana is a Go-based web service that runs on port 3000 (or 8080 on Microsoft Windows, as we configured it) by default. Once it’s running you can browse to it using your web browser—for example, if it’s running on the local host: http:// localhost:3000.\n\nVersion: v1.0.0 (427b8e9)\n\n144",
      "content_length": 811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nFigure 4.19: The Grafana console login\n\nYou’ll see a login screen initially. The default username and password are admin and admin. You can control this by updating the [security] section of the Grafana configuration file.\n\nYou can configure user authentication, including integration with Google authen- tication, GitHub authentication, or local user authentication. The Grafana config- uration documentation includes sections on user management and authentication. For our purposes, we’re going to assume the console is inside our environment and stick with local authentication.\n\nLog in to the console by using the admin / admin username and password pair and clicking the Log in button. You should see the Grafana default console view.\n\nVersion: v1.0.0 (427b8e9)\n\n145",
      "content_length": 815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nFigure 4.20: The Grafana console\n\nIt contains a Getting Started workflow. We first want to connect Grafana to our Prometheus data. Click on Add data source in the Getting Started workflow. You’ll see a new definition for a data source.\n\nTo add a new data source we need to specify a few details. First, we need to name our data source. We’re going to call ours Prometheus. Next, we need to check the Default checkbox to tell Grafana to search for data in this source by default. We also need to ensure the data source Type is set to Prometheus.\n\nWe also need to specify the HTTP settings for our data source. This is the URL of the Prometheus server we wish to query. Here, let’s assume we’re running Grafana on the same host as Prometheus—for our local server, it’s http://localhost :9090. If you’re running Prometheus elsewhere you’ll need to specify the URL to Prometheus and to ensure connectivity is available between the Grafana host and the Prometheus server.\n\n NOTE The Prometheus server needs to be running for Grafana to retrieve\n\ndata.\n\nWe also need to set the Access option to proxy. Surprisingly this doesn’t configure\n\nVersion: v1.0.0 (427b8e9)\n\n146",
      "content_length": 1208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nan HTTP proxy for our connection, but it tells Grafana to use its own web service to proxy connections to Prometheus. The other option, direct, makes direct con- nections from the web browser. The proxy setting is much more practical, as the Grafana service takes care of connectivity.\n\nFigure 4.21: Adding a Grafana data source for Prometheus\n\nVersion: v1.0.0 (427b8e9)\n\n147",
      "content_length": 419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nTo add our new data source, click the Add button. This will save it. On the screen we can now see our data source displayed. If it saved with a banner saying Data\n\nsource is working then it is working!\n\nClick the Grafana logo and then click on Dashboards -> Home to return to the main console view.\n\nFigure 4.22: Adding a Grafana data source for Prometheus\n\nVersion: v1.0.0 (427b8e9)\n\n148",
      "content_length": 432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nFirst dashboard\n\nNow that you’re back on the Getting Started workflow, click on the New dashboard button to create a new dashboard.\n\nYou can then see the first dashboard here:\n\nFigure 4.23: The Node dashboard\n\nThe process of creating graphs and dashboards is reasonably complex and beyond the scope of this book. But there are a large number of resources and examples that can help:\n\nGrafana Getting Started • Grafana Tutorials and screencasts • Grafana Prometheus documentation • Grafana Prebuilt Dashboard collection\n\nMany projects also include prebuilt Grafana dashboards for their specific needs— for example, monitoring MySQL or Redis.\n\nYou can then add graphs for some of the other metrics we’ve explored in this chapter. We’ve included the JSON for our complete dashboard in the code for the book that you can import and play with.\n\nVersion: v1.0.0 (427b8e9)\n\n149",
      "content_length": 914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Chapter 4: Monitoring Nodes and Containers\n\nSummary\n\nIn this chapter we used our first exporters and scraped node and container metrics.\n\nWe’ve started to explore the PromQL query language, how to make use of it to aggregate those metrics and report on the state of some of our node resources, and we’ve delved into the USE Method to find some key metrics to monitor. We also learned how to save those queries as recording rules.\n\nFrom here we can extend the use of those exporters to our whole fleet of hosts. This presents a challenge, though: how does Prometheus know about new hosts? Do we continue to manually add IP addresses to our scrape configuration? We can quickly see that this will not scale. Thankfully, Prometheus has a solution: service discovery. In the next chapter we’ll explore how Prometheus can discover your hosts and services.\n\nVersion: v1.0.0 (427b8e9)\n\n150",
      "content_length": 882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Chapter 5\n\nService Discovery\n\nIn the last chapter we installed exporters and scraped node and container metrics. For each target we specified, we manually listed their IP address and port in the scrape configuration. This approach is fine for a few hosts but not for a larger fleet, especially not a dynamic fleet using containers and cloud-based instances, where the instances can change, appear, and disappear.\n\nPrometheus solves this issue by using service discovery: automated mechanisms to detect, classify, and identify new and changed targets. Service discovery can work via a variety of mechanisms:\n\nReceiving lists of targets from files populated via configuration management\n\ntools.\n\nQuerying an API, such as the Amazon AWS API, for lists of targets. • Using DNS records to return lists of targets.\n\nIn this chapter, we’re going to use service discovery to learn how to discover our hosts and services and expose them to Prometheus. We’ll see a variety of discovery mechanisms including file-based, API-driven, and DNS-powered.\n\n151",
      "content_length": 1042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Chapter 5: Service Discovery\n\nScrape lifecycle and static configuration redux\n\nTo understand how service discovery works we need to harken back to our scrape lifecycle. When Prometheus runs a job, the very first step initiated is service discovery. This populates the list of targets and metadata labels that the job will scrape.\n\nFigure 5.1: Scrape lifecycle\n\nIn our existing configuration, our static_configs block:\n\nservice discovery mechanism is\n\nthe\n\nVersion: v1.0.0 (427b8e9)\n\n152",
      "content_length": 486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "Chapter 5: Service Discovery\n\nListing 5.1: Our static service discovery\n\nscrape_configs:\n\njob_name: 'prometheus' static_configs:\n\ntargets: ['localhost:9090']\n\njob_name: 'node' static_configs:\n\ntargets: ['138.197.26.39:9100', '138.197.30.147:9100', '\n\n138.197.30.163:9100']\n\nThe list of targets and any associated labels are manual service discovery. It’s pretty easy to see that maintaining a long list of hosts in a variety of jobs isn’t going to be a human-scalable task (nor is HUP’ing the Prometheus server for each change overly elegant). This is especially true with the dynamic nature of most environments and the scale of hosts, applications, and services that you’re likely to want to monitor.\n\nThis is where more sophisticated service discovery comes into its own. So what alternatives do we have? We’re going to explore several service discovery meth- ods:\n\nFile-based. • Cloud-based. • DNS-based.\n\nWe’ll start with file-based discovery.\n\n NOTE Jobs can use one more than type of service discovery. We can source\n\ntargets from multiple service discovery techniques by specifying them in a job.\n\nVersion: v1.0.0 (427b8e9)\n\n153",
      "content_length": 1137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Chapter 5: Service Discovery\n\nFile-based discovery\n\nFile-based discovery is only a small step more advanced than static configurations, but it’s great for provisioning by configuration management tools. With file-based discovery Prometheus consumes targets specified in files. The files are usually generated by another system—such as a configuration management system like Puppet, Ansible, or Chef—or queried from another source, like a CMDB. Periodi- cally a script or query runs or is triggered to (re)populate these files. Prometheus then reloads targets from these files on a specified schedule.\n\nThe files can be in YAML or JSON format and contain lists of targets defined much like we’d define them in a static configuration. Let’s start by moving our existing jobs to file-based discovery.\n\nListing 5.2: File-based discovery\n\njob_name: node file_sd_configs:\n\nfiles:\n\ntargets/nodes/*.json refresh_interval: 5m\n\njob_name: docker file_sd_configs:\n\nfiles:\n\ntargets/docker/*.json refresh_interval: 5m\n\n. . .\n\nWe’ve replaced the static_configs blocks in our prometheus.yml file with file_sd_configs blocks. Inside these blocks we’ve specified a list of files, contained in the files array. We’ve specified our files for each job under a parent directory, targets, and created a sub-directory for each job. You can create whatever structure works for you.\n\nVersion: v1.0.0 (427b8e9)\n\n154",
      "content_length": 1388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "Chapter 5: Service Discovery\n\nWe’ve then specified the files using a glob: *.json. This will load targets from all files ending in .json in this directory, whenever those files change. I’ve chosen JSON for our files because it’s a popular format that’s easy to write using a variety of languages and integrations.\n\nEvery time the job runs or these files change, Prometheus will reload the files’ contents. As a safeguard, we’ve also specified the refresh_interval option. This option will load the targets in the listed files at the end of each interval—for us this is five minutes.\n\n TIP There’s also a metric called prometheus_sd_file_mtime_seconds that\n\nwill tell you when your file discovery files were last updated. You could monitor this metric to identify any staleness issues.\n\nLet’s quickly create this directory structure.\n\nListing 5.3: Creating the target directory structure\n\n$ cd /etc/prometheus $ mkdir -p targets/{nodes,docker}\n\nLet’s move our nodes and Docker daemons to new JSON files. We’ll create two files to hold the targets.\n\nListing 5.4: Creating JSON files to hold our targets\n\n$ touch targets/nodes/nodes.json $ touch targets/docker/daemons.json\n\nVersion: v1.0.0 (427b8e9)\n\n155",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Chapter 5: Service Discovery\n\nAnd now populate them with our existing targets.\n\nListing 5.5: The nodes.json file\n\n[{\n\n\"targets\": [\n\n\"138.197.26.39:9100\", \"138.197.30.147:9100\", \"138.197.30.163:9100\"\n\n]\n\n}]\n\nAnd the daemons.json file.\n\nListing 5.6: The daemons.json file\n\n[{\n\n\"targets\": [\n\n\"138.197.26.39:8080\", \"138.197.30.147:8080\", \"138.197.30.163:8080\"\n\n]\n\n}]\n\nWe can also articulate the same list of targets we’ve created in JSON in YAML.\n\nListing 5.7: The daemons file in YAML\n\ntargets:\n\n\"138.197.26.39:8080\" - \"138.197.30.147:8080\" - \"138.197.30.163:8080\"\n\nThis moves our existing static configuration into our files. We could add labels to\n\nVersion: v1.0.0 (427b8e9)\n\n156",
      "content_length": 678,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "Chapter 5: Service Discovery\n\nthese targets, too.\n\nListing 5.8: Adding labels\n\n[{\n\n\"targets\": [\n\n\"138.197.26.39:8080\", \"138.197.30.147:8080\", \"138.197.30.163:8080\"\n\n], \"labels\": {\n\n\"datacenter\": \"nj\"\n\n}\n\n}]\n\nHere we’ve added the label datacenter with a value of nj to the Docker daemon targets. File-based discovery automatically adds one metadata label during the relabelling phase to every target: __meta_filepath. This contains the path and filename of the file containing the target.\n\n NOTE You can see a full list of the service discovery targets and their meta labels on the Web UI at https://localhost:9090/service-discovery.\n\nWriting files for file discovery\n\nSince writing files out to JSON is fairly specific to the source of the targets, we’re not going to cover any specifics, but we’ll provide a high-level overview of some approaches.\n\nFirst, if your configuration management tool can emit a list of the nodes it is man- aging or configuring, that’s an ideal starting point. Several of the configuration\n\nVersion: v1.0.0 (427b8e9)\n\n157",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Chapter 5: Service Discovery\n\nmanagement modules we introduced in Chapter 3 have such templates.\n\nIf those tools include a centralized configuration store or configuration manage- ment database (CMDB) of some kind, this can be a potential source for the target data. For example, if you are using PuppetDB, there’s a file-based discovery script you can use to extract your nodes from the database.\n\nAlternatively, if you’re going to write your own, there’s a few simple rules to follow:\n\nMake your file discovery configurable—don’t hardcode options. Preferably, ensure that your file discovery will also work automatically with its default configuration. For instance, ensure the default configuration options assume the default installation state of the source.\n\nDon’t expose secrets like API keys or passwords in configuration. Instead,\n\nrely on secret stores or the environment.\n\nOperations on the files to which you output your targets should be atomic.\n\nHere are some file discovery scripts and tools that might provide examples you can crib from:\n\nAmazon ECS. • An API-driven file discovery script that Wikimedia uses with its CMBD. • Docker Swarm.\n\n TIP There’s also a list of file-based discovery integrations in the Prometheus\n\ndocumentation.\n\nVersion: v1.0.0 (427b8e9)\n\n158",
      "content_length": 1284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Chapter 5: Service Discovery\n\nInbuilt service discovery plugins\n\nSome tools and platforms are supported by native service discovery integrations. These ship with Prometheus. These service discovery plugins use those tools and platform’s existing data stores or APIs to return lists of targets.\n\nThe currently available native service discovery plugins include platforms like:\n\nAmazon EC2 • Azure • Consul • Google Compute Cloud • Kubernetes\n\n TIP We’ll see the Kubernetes service discovery in Chapter 7 when we instru-\n\nment an application running on Kubernetes.\n\nLet’s take a look at the Amazon EC2 service discovery plugin.\n\nAmazon EC2 service discovery plugin\n\nThe Amazon EC2 service discovery plugin uses the Amazon Web Services EC2 API to retrieve a list of EC2 instances to use as Prometheus targets. In order to use the discovery plugin you’ll need to have an Amazon account and credentials. We’re going to assume you already have an Amazon account, but if you haven’t already got an AWS account, you can create one at the AWS Console.\n\nThen follow the Getting Started process.\n\nVersion: v1.0.0 (427b8e9)\n\n159",
      "content_length": 1117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Chapter 5: Service Discovery\n\nAs part of the Getting Started process you’ll receive an access key ID and a secret ac- cess key. If you have an Amazon Web Services (AWS) account you should already have a pair of these.\n\nLet’s add a new job to our Prometheus configuration to retrieve our EC2 instances.\n\nListing 5.9: An EC2 discovery job\n\njob_name: amazon_instances ec2_sd_configs:\n\nregion: us-east-1\n\naccess_key: AKIAIOSFODNN7EXAMPLE secret_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\nWe’ve specified a new amazon_instances job. Our service discovery is provided via the ec2_sd_configs block. Inside this block we’ve specified three parame- ters: region for the AWS region, and access_key and secret_key for our Amazon credentials.\n\nIf you don’t want to specify your keys in the file (and, remember, you shouldn’t hardcode your secrets in configuration), Prometheus supports Amazon’s local CLI configuration approaches. If you don’t specify keys, Prometheus will look for the appropriate environment variables, or for AWS credentials in the user running Prometheus’ home directory.\n\nAlternatively, you can specify a role ARN to use IAM roles.\n\nPrometheus also supports profiles if you have multiple AWS accounts specified on the host.\n\nVersion: v1.0.0 (427b8e9)\n\n160",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "Chapter 5: Service Discovery\n\nListing 5.10: An EC2 discovery job with a profile\n\njob_name: amazon_instances ec2_sd_configs:\n\nregion: us-east-1 profile: work\n\nHere Prometheus will use the work profile when discovering instances.\n\nThe discovery\n\nThe EC2 discovery plugin will return all running instances in that region. By default, it’ll return targets with the private IP address of the instance, with a default port of 80, and with a metrics path of /metrics. So, if you have an EC2 instance with the private IP address of 10.2.1.1, it will return a scrape target address of http://10.2.1.1:80/metrics. We can override the default port with the port parameter.\n\nListing 5.11: An EC2 discovery job with a port\n\njob_name: amazon_instances ec2_sd_configs:\n\nregion: us-east-1 port: 9100\n\nThis will override the default port of 80 with a port of 9100.\n\nOften, though, we want to override more than just the port. If this isn’t where you have metrics exposed, we can adjust this prior to the scrape by relabelling. This relabelling takes place in the first relabel window, prior to the scrape, and uses the relabel_configs block.\n\nVersion: v1.0.0 (427b8e9)\n\n161",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Chapter 5: Service Discovery\n\nLet’s assume each of our EC2 instances has the Node Exporter configured, and we want to scrape the public IP address—not the private IP address—and relabel the targets accordingly.\n\nListing 5.12: Relabelling an EC2 discovery job\n\njob_name: amazon_instances ec2_sd_configs:\n\nregion: us-east-1\n\naccess_key: AKIAIOSFODNN7EXAMPLE secret_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\nrelabel_configs: - source_labels: [__meta_ec2_public_ip]\n\nregex: target_label: __address__ replacement: '$1:9100'\n\n'(.*)'\n\n TIP Remember, a job can have more than one type of service discovery\n\npresent. For example, we could discover some targets via file-based service dis- covery and others from Amazon, we could statically specify some targets, and so on. Despite this, any relabelling will be applied to all targets. If you’re using ser- vice discovery metadata labels they won’t be available for all targets.\n\nThe configuration syntax and structure for relabelling is identical between the relabel_configs and metric_relabel_configs blocks. The only difference is when they take place: relabel_configs is after service discovery and before the scrape, and metrics_relabel_configs is after the scrape.\n\nHere we’ve specified one of the metadata labels collected by the EC2 service dis- covery plugin, __meta_ec2_public_ip, as the value for our source_labels. We specified (.*) as our regular expression. We technically don’t need to specify this\n\nVersion: v1.0.0 (427b8e9)\n\n162",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Chapter 5: Service Discovery\n\nas this is the default value of the regex parameter. But we’ve included it to make it clear what’s happening. This expression captures the entire contents of the source label.\n\nWe then specify the target for our replacement in the target_label parameter. As we need to update the IP address we’re writing into the __address__ label Fi- nally, the replacement parameter contains the regular expression capture from the regex and suffixes it with the Node Exporter default port: 9100. If the public IP address of our instance was 34.201.102.225, then the instance would be rela- belled as a target to 34.201.102.225:9100 and the default scheme, http, and the metrics path, /metrics, would be added. The final target would be scraped at http://34.201.102.225:9100/metrics.\n\n TIP The __meta labels are dropped after the first relabelling phase, relabel_- configs.\n\nOther metadata collected by the EC2 discovery plugin includes:\n\n__meta_ec2_availability_zone - The availability zone of the instance. • __meta_ec2_instance_id - The EC2 instance ID. • __meta_ec2_instance_state - The state of the EC2 instance. • __meta_ec2_instance_type - The type of the EC2 instance. • __meta_ec2_private_ip - The private IP address of the EC2 instance, if avail- able.\n\n__meta_ec2_public_dns_name - The public DNS name of the instance, if\n\navailable.\n\n__meta_ec2_subnet_id - A comma-separated list of subnet IDs in which the\n\ninstance is running, if available.\n\n__meta_ec2_tag_<tagkey> - Each tag value of the instance. (One label per\n\ntag.)\n\nVersion: v1.0.0 (427b8e9)\n\n163",
      "content_length": 1584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "Chapter 5: Service Discovery\n\n__meta_ec2_vpc_id - The ID of the VPC in which the instance is running, if\n\navailable.\n\n TIP The full list of metadata is available in the Prometheus configuration\n\ndocumentation.\n\nThe __meta_ec2_tag_<tagkey> metadata label for EC2 tags also allows us to use relabelling to better name our targets. Rather than using the IP address for the instance label, effectively the public name of the target, we can make use of the tag values. Let’s say we had an EC2 tag called Name that contained the hostname (or a friendly name of some sort) of the instance. We could use relabelling to make use of that tag value.\n\nListing 5.13: Relabelling the instance name in a EC2 discovery job\n\njob_name: amazon_instances ec2_sd_configs:\n\nregion: us-east-1\n\nrelabel_configs: - source_labels: [__meta_ec2_public_ip]\n\nregex: target_label: __address__ replacement: '$1:9100'\n\n'(.*)'\n\nsource_labels: [__meta_ec2_tag_Name] target_label: instance\n\nYou can see that we’ve added a second relabel that uses the __meta_ec2_tag_Name label, which contains the value of the Name tag as the source label, and writes it into the instance label. Assuming the Name tag, for instance 10.2.1.1, contained bastion, then our instance label would be relabelled from:\n\nVersion: v1.0.0 (427b8e9)\n\n164",
      "content_length": 1290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Chapter 5: Service Discovery\n\nnode_cpu{cpu=\"cpu0\",instance=\"10.2.1.1\",job=\"nodes\",mode=\"system\"}\n\nto:\n\nnode_cpu{cpu=\"cpu0\",instance=\"bastion\",job=\"nodes\",mode=\"system\"}\n\nMaking it easier to parse metrics from that target.\n\nDNS service discovery\n\nIf file discovery doesn’t work for you, or your source or service doesn’t support any of the existing service discovery tools, then DNS discovery may be an option. DNS discovery allows you to specify a list of DNS entries and then query records in those entries to discover a list of targets. It relies on querying A, AAAA, or SRV DNS records.\n\n TIP The DNS records will be resolved by the DNS servers that are defined locally on the Prometheus server—for example, /etc/resolv.conf on Linux.\n\nLet’s look at a new job that uses DNS service discovery.\n\nListing 5.14: DNS service discovery job\n\njob_name: webapp dns_sd_configs:\n\nnames: [ '_prometheus._tcp.example.com' ]\n\nWe’ve defined a new job called webapp and specified a dns_sd_configs block. Inside that block we’ve specified the names parameter which contains an array of the DNS entries we’re going to query.\n\nVersion: v1.0.0 (427b8e9)\n\n165",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Chapter 5: Service Discovery\n\nBy default, Prometheus’s DNS service discovery assumes you’re querying SRV or Service records. Service records are a way to define services in your DNS con- figuration. A service generally consists of one or more target host and port com- binations upon which your service runs. The format of a DNS SRV entry looks like:\n\nListing 5.15: A SRV record\n\n_service._proto.name. TTL IN SRV priority weight port target.\n\nWhere _service is the name of the service being queried, _proto is the protocol of the service, usually TCP or UDP. We specify the name of the entry, ending in a dot. We then have the TTL, or time to live, of the record. IN is the standard DNS class (it’s always IN). And we specify a priority of the target host: lower values are higher priority. The weight controls preferences for targets with the same priority; higher values are preferred. Last, we specify the port the service runs on and the host name of the host providing the service, ending in a dot.\n\nSo, for Prometheus, we might define records like:\n\nListing 5.16: Example SRV records\n\n_prometheus._tcp.example.com. 300 IN SRV 10 1 9100 webapp1. example.com. _prometheus._tcp.example.com. 300 IN SRV 10 1 9100 webapp2. example.com. _prometheus._tcp.example.com. 300 IN SRV 10 1 9100 webapp3. example.com.\n\n NOTE There is a whole RFC for DNS service discovery: RFC6763.\n\nPrometheus’s DNS discovery does not support it.\n\nVersion: v1.0.0 (427b8e9)\n\n166",
      "content_length": 1455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "Chapter 5: Service Discovery\n\nWhen Prometheus queries for targets it will look up the DNS server for the example. com domain. It will then search for a SRV record called _prometheus._tcp.example .com in that domain and return the service records in that entry. We only have the three records in that entry, so we’d see three targets returned.\n\nListing 5.17: The DNS targets from the SRV\n\nwebapp1.example.com:9100 webapp2.example.com:9100 webapp3.example.com:9100\n\nWe can also query individual A or AAAA records using DNS service discovery. To do so we need to explicitly specify the query type and a port for our scrape. We need to specify the port because the A and AAAA records only return the host, not the host and port combination of the SRV record.\n\nListing 5.18: DNS A record service discovery job\n\njob_name: webapp dns_sd_configs:\n\nnames: [ 'example.com' ] type: A port: 9100\n\nThis will only return any A records at the root of the example.com domain. If we wanted to return records from a specific DNS entry, we’d use this:\n\nVersion: v1.0.0 (427b8e9)\n\n167",
      "content_length": 1064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Chapter 5: Service Discovery\n\nListing 5.19: DNS subdomain A record service discovery job\n\njob_name: webapp dns_sd_configs:\n\nnames: [ 'web.example.com' ] type: A port: 9100\n\nHere we’re pulling A records that resolve for web.example.com and suffixing them with the 9100 port.\n\n TIP There’s only one metadata label available from DNS service discovery: __meta_dns_name. This is set to the specific record that generated the target.\n\nSummary\n\nIn this chapter we learned about service discovery. We’ve seen several mecha- nisms for discovering targets for Prometheus to scrape, including:\n\nFile-based discovery, populated by external data sources. • Platform-based service discovery, using the APIs and data of platforms like AWS, Kubernetes, or Google Cloud.\n\nDNS-based using SRV, A, or AAAA records.\n\nBetween these three discovery approaches, you should have sufficient means to identify the resources you wish to monitor.\n\nWe also learned a bit more about relabelling, looking at the pre-scrape relabelling phase and seeing how to use metadata to add more context to our metrics.\n\nVersion: v1.0.0 (427b8e9)\n\n168",
      "content_length": 1110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "Chapter 5: Service Discovery\n\nNow that we’ve got metrics coming into Prometheus, let’s tell folks about them. In the next chapter, we’re going to look at alerting.\n\nVersion: v1.0.0 (427b8e9)\n\n169",
      "content_length": 195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Chapter 6\n\nAlerting and Alertmanager\n\nI think we ought to take the men out of the loop.\n\n— War Games, 1983\n\nIn the last few chapters we’ve installed, configured, and done some basic mon- itoring with Prometheus. Now we need to understand how to generate useful alerts from our monitoring data. Prometheus is a compartmentalized platform, and the collection and storage of metrics is separate from alerting. Alerting is provided by a tool called Alertmanager, a standalone piece of your monitoring environment. Alerting rules are defined on your Prometheus server. These rules can trigger events that are then propagated to Alertmanager. Alertmanager then decides what to do with the respective alerts, handling issues like duplication, and determines what mechanism to use when sending the alert on: realtime messages, email, or via tools like PagerDuty and VictorOps.\n\nIn this chapter, we’re going to discuss what makes good alerting, install and con- figure Alertmanager and look at how to use it to route notifications and manage maintenance. We’ll then define our alerting rules on our Prometheus server, using metrics we’ve collected thus far in the book, and then trigger some alerts.\n\nFirst let’s talk a bit about good alerting.\n\n170",
      "content_length": 1240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Chapter 6: Alerting and Alertmanager\n\nAlerting\n\nAlerting provides us with indication that some state in our environment has changed, usually for the worse. The key to a good alert is to send it for the right reason, at the right time, with the right tempo, and to put useful information in it.\n\nThe most common anti-pattern seen in alerting approaches is sending too many alerts. Too many alerts is the monitoring equivalent of “the boy who cried wolf”. Recipients will become numb to alerts and tune them out. Crucial alerts are often buried in floods of unimportant updates.\n\nThe reasons you’re usually sending too many alerts can include:\n\nAn alert is not actionable, it’s informational. You should turn all of these alerts off or turn them into counters that count the rate rather than alert on the symptom.\n\nA failed host or service upstream triggers alerts for everything downstream of it. You should ensure your alerting system identifies and suppresses these duplicate, adjacent alerts.\n\nYou’re alerting for causes and not symptoms. Symptoms are signs your ap- plication has stopped working, they are the manifestation of issues that may have many causes. High latency of an API or website is a symptom. That symptom could be caused by any number of issues: high database usage, memory issues, disk performance, etc. Alerting on symptoms identifies real problems. Alerting on causes alone, for example high database usage, could identify an issue but most likely will not. High database usage might be per- fectly normal for this application and may result in no performance issues for an end user or the application. Alerting on it is meaningless as its an internal state. This alert is likely to result in engineers missing more criti- cal issues because they have become numb to the volume of non-actionable, 171\n\nYou’re alerting for causes and not symptoms. Symptoms are signs your ap- plication has stopped working, they are the manifestation of issues that may have many causes. High latency of an API or website is a symptom. That symptom could be caused by any number of issues: high database usage, memory issues, disk performance, etc. Alerting on symptoms identifies real problems. Alerting on causes alone, for example high database usage, could identify an issue but most likely will not. High database usage might be per- fectly normal for this application and may result in no performance issues for an end user or the application. Alerting on it is meaningless as its an internal state. This alert is likely to result in engineers missing more criti- cal issues because they have become numb to the volume of non-actionable, 171",
      "content_length": 2652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Chapter 6: Alerting and Alertmanager\n\ncause-based alerts. You should focus on symptom-based alerts and rely on your metrics or other diagnostic data to identify causes.\n\nThe second most common anti-pattern is misclassification of alerts. Sometimes this also means a crucial alert is buried in other alerts. But other times the alert is sent to the wrong place or with the incorrect urgency.\n\nThe third most common anti-pattern is sending alerts that are not useful, especially when the recipient is often a tired, freshly woken engineer on her third or fourth on-call notification for the night. Here’s an example of a stock Nagios alert:\n\nListing 6.1: Stock Nagios alert\n\nPROBLEM Host: server.example.com Service: Disk Space\n\nState is now: WARNING for 0d 0h 2m 4s (was: WARNING) after 3/3 checks\n\nNotification sent at: Thu Aug 7th 03:36:42 UTC 2015 ( notification number 1)\n\nAdditional info: DISK WARNING - free space: /data 678912 MB (9% inode=99%)\n\nThis notification appears informative but it isn’t really. Is this a sudden increase? Or has this grown gradually? What’s the rate of expansion? For example, as we noted in the introduction, 9 percent disk space free on a 1 GB partition is different from 9 percent disk free on a 1 TB disk. Can we ignore or mute this notification or do we need to act now?\n\nGood alerting has some key characteristics:\n\n1. An appropriate volume of alerts that focus on symptoms not causes - Noisy It’s alerting results in alert fatigue and, ultimately, alerts being ignored.\n\nVersion: v1.0.0 (427b8e9)\n\n172",
      "content_length": 1541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Chapter 6: Alerting and Alertmanager\n\neasier to fix under-alerting than over-alerting.\n\n2. The right alert priority should be set. If the alert is urgent then it should be routed quickly and simply to the party responsible for responding. If the alert isn’t urgent, we should send it with an appropriate tempo, to be responded to when required.\n\n3. Alerts should include appropriate context to make them immediately useful.\n\n TIP There’s a great chapter on alerting in the SRE book.\n\nNow, let’s look a little more closely at the Alertmanager.\n\nHow the Alertmanager works\n\nThe Alertmanager handles alerts sent from a client, generally a Prometheus server. (It can also receive alerts from other tools, but this is beyond the scope of this book.) Alertmanager handles deduplicating, grouping, and routing alerts to re- ceivers like email, SMS, or SaaS services like PagerDuty. You can also manage maintenance using Alertmanager.\n\nVersion: v1.0.0 (427b8e9)\n\n173",
      "content_length": 959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "Chapter 6: Alerting and Alertmanager\n\nFigure 6.1: Alertmanager architecture\n\nOn our Prometheus server we’ll be writing alerting rules. These rules will use the metrics we’re collecting and trigger on thresholds or criteria we’ve specified. We’ll also see how we might add some context to the alerts. When the threshold or criteria is met, an alert will be generated and pushed to Alertmanager. The alerts are received on an HTTP endpoint on the Alertmanager. One or many Prometheus servers can direct alerts to a single Alertmanager, or you can create a highly avail- able cluster of Alertmanagers, as we’ll see later in Chapter 7.\n\nAfter being received, alerts are processed by the Alertmanager and routed accord- ing to their labels. If their path determines it, they are sent by the Alertmanager to external destinations like email, SMS, or chat.\n\nVersion: v1.0.0 (427b8e9)\n\n174",
      "content_length": 881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Chapter 6: Alerting and Alertmanager\n\nLet’s continue with installing Alertmanager.\n\nInstalling Alertmanager\n\nAlertmanager is a standalone Go binary. The Prometheus.io download page con- tains files with the binaries for specific platforms. Currently Alertmanager is sup- ported on:\n\nLinux: 32-bit, 64-bit, and ARM. • Max OS X: 32-bit and 64-bit. • FreeBSD: 32-bit, 64-bit, and ARM. • OpenBSD: 32-bit, 64-bit, and ARM. • NetBSD: 32-bit, 64-bit, and ARM. • Microsoft Windows: 32-bit and 64-bit. • DragonFly: 64-bit.\n\nOlder versions of Alertmanager are available from the GitHub Releases page.\n\n NOTE At the time of writing, Alertmanager was at version 0.15.0-rc.2.\n\nInstalling Alertmanager on Linux\n\nTo install Alertmanager on a 64-bit Linux host, we can download the zipped tarball. We can use wget or curl to get the file from the download site.\n\nVersion: v1.0.0 (427b8e9)\n\n175",
      "content_length": 878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.2: Download the Alertmanager tarball\n\n$ cd /tmp $ wget https://github.com/prometheus/alertmanager/releases/download/v 0.15.0-rc.2/alertmanager-0.15.0-rc.2.linux-amd64.tar.gz\n\nNow let’s unpack the alertmanager binary from the tarball, copy it somewhere useful, and change its ownership to the root user.\n\nListing 6.3: Unpack the alertmanager binary\n\n$ tar -xzf alertmanager-0.15.0-rc.2.linux-amd64.tar.gz $ sudo cp alertmanager-0.15.0-rc.2.linux-amd64/alertmanager /usr/ local/bin/\n\nLet’s also copy the amtool binary into our path. The amtool binary is used to help manage the Alertmanager and schedule maintenance windows from the command line.\n\nListing 6.4: Moving the amtool binary\n\n$ sudo cp alertmanager-0.15.0-rc.2.linux-amd64/amtool /usr/local/ bin\n\nWe can now test if Alertmanager is installed and in our path by checking its ver- sion.\n\nVersion: v1.0.0 (427b8e9)\n\n176",
      "content_length": 923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.5: Checking the Alertmanager version on Linux\n\n$ alertmanager --version alertmanager, version 0.15.0-rc.2 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\n TIP This same approach will work on Mac OS X with the Darwin version of\n\nthe Alertmanager binary.\n\nInstalling Alertmanager on Microsoft Windows\n\nTo install Alertmanager on Microsoft Windows, we need to download the alertmanager.exe executable and put it in a directory. Let’s create a directory for the executable using Powershell.\n\nListing 6.6: Creating a directory on Windows\n\nC:\\> MKDIR alertmanager C:\\> CD alertmanager\n\nNow download the alertmanager.exe executable from GitHub into the C:\\ alertmanager directory:\n\nVersion: v1.0.0 (427b8e9)\n\n177",
      "content_length": 857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.7: Alertmanager Windows download\n\nhttps://github.com/prometheus/alertmanager/releases/download/v 0.15.0-rc.2/alertmanager-0.15.0-rc.2.windows-amd64.tar.gz\n\nUnzip the executable using a tool like 7-Zip into the C:\\alertmanager directory. Finally, add the C:\\alertmanager directory to the path. This will allow Windows to find the executable. To do this, run this command inside Powershell.\n\nListing 6.8: Setting the Windows path\n\n$env:Path += \";C:\\alertmanager\"\n\nYou should now be able to run the alertmanager.exe executable.\n\nListing 6.9: Checking the Alertmanager version on Windows\n\nC:\\> alertmanager.exe --version alertmanager, version 0.15.0-rc.2 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\nStacks\n\nThe stacks we saw in Chapter 3 also include Alertmanager installations.\n\nA Prometheus, Node Exporter, and Grafana docker-compose stack.\n\nVersion: v1.0.0 (427b8e9)\n\n178",
      "content_length": 1025,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "Chapter 6: Alerting and Alertmanager\n\nAnother Docker Compose single node stack with Prometheus, Alertmanager,\n\nNode Exporter, and Grafana.\n\nA Docker Swarm stack for Prometheus.\n\nInstalling via configuration management\n\nSome of the configuration management modules we saw in Chapter 3 can also install Alertmanager: You could review their capabilities to identify which install and configure Alertmanager.\n\n TIP Remember configuration management is the recommended approach for\n\ninstalling and managing Prometheus and its components!\n\nConfiguring the Alertmanager\n\nLike Prometheus, Alertmanager is configured with a YAML-based configuration file. Let’s create a new file and populate it.\n\nListing 6.10: Creating the alertmanager.yml file\n\n$ sudo mkdir -p /etc/alertmanager/ $ sudo touch /etc/alertmanager/alertmanager.yml\n\nNow let’s add some configuration to the file. Our basic configuration will send any alerts received out via email. We’ll build on this configuration as the chapter unfolds.\n\nVersion: v1.0.0 (427b8e9)\n\n179",
      "content_length": 1027,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.11: A simple alertmanager.yml configuration file\n\nglobal:\n\nsmtp_smarthost: 'localhost:25' smtp_from: 'alertmanager@example.com' smtp_require_tls: false\n\ntemplates: - '/etc/alertmanager/template/*.tmpl'\n\nroute:\n\nreceiver: email\n\nreceivers: - name: 'email'\n\nemail_configs: - to: 'alerts@example.com'\n\nThis configuration file contains a basic setup that processes alerts and sends them via email to one address. Let’s look at each block in turn.\n\nThe first block, global, contains global configuration for the Alertmanager. These options set defaults for all the other blocks, and are valid in those blocks as over- rides. In our case, we’re just configuring some email/SMTP settings: the email server to use for sending emails, the source/from address of those emails, and we’re disabling the requirement for automatically using TLS.\n\n TIP This assumes you have a SMTP server running on the localhost on port 25.\n\nThe templates block contains a list of directories that hold alert templates. As\n\nVersion: v1.0.0 (427b8e9)\n\n180",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Chapter 6: Alerting and Alertmanager\n\nAlertmanager can send to a variety of destinations, you often need to be able to customize what an alert looks like and the data it contains. Let’s just create this directory for the moment.\n\nListing 6.12: Creating the templates directory\n\n$ sudo mkdir -p /etc/alertmanager/template\n\nWe’ll see more about templates later.\n\nNext, we have the route block. Routes tell Alertmanager what to do with specific incoming alerts. Alerts are matched against rules and actions taken. You can think about routing like a tree with branches. Every alert enters at the root of the tree—the base route or node. Each route, except the base node, has matching criteria which should match all alerts. You can then define child routes or nodes— the branches of the tree that take specific action or interest in specific alerts. For example, all the alerts from a specific cluster might be processed by a specific child route.\n\nVersion: v1.0.0 (427b8e9)\n\n181",
      "content_length": 975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "Chapter 6: Alerting and Alertmanager\n\nFigure 6.2: Alertmanager routing\n\nIn our current configuration, we have only defined the base route, the node at the root of the tree. Later in this chapter, we’ll take advantage of routes to ensure our alerts have the right volume, frequency, and destinations.\n\nWe’ve also only defined one parameter: receiver. This is the default destination for our alerts, in our case email. We’ll define that receiver next.\n\nThe last block, receivers, specifies alert destinations. You can send alerts via email, to services like PagerDuty and VictorOps, and to chat tools like Slack and HipChat. We only have one destination defined: an email address.\n\nEach receiver has a name and associated configuration. Here we’ve named our receiver email. We then provide configuration for the specific types of receivers.\n\nVersion: v1.0.0 (427b8e9)\n\n182",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "Chapter 6: Alerting and Alertmanager\n\nFor our email alerts, we use the email_configs block to specify email options, like the to address to receive alerts. We could also specify SMTP settings, which would override the settings in global, and additional items to be added, like mail headers.\n\n TIP One of the built-in receivers is called the webhook receiver. You can use\n\nthis receiver to send alerts to other destinations that do not have specific receivers in Alertmanager.\n\nNow that we have configured Alertmanager, let’s launch it.\n\nRunning Alertmanager\n\nAlertmanager runs as a web service, by default on port 9093. It is started by running the alertmanager binary on Linux and OS X, or the alertmanager.exe executable on Windows, specifying the configuration file we’ve just created. Let’s start Alertmanager now.\n\nListing 6.13: Starting Alertmanager\n\n$ alertmanager --config.file alertmanager.yml\n\nWe’ve specified our alertmanager.yml configuration file with the --config.file flag. Alertmanager has a web interface at:\n\nhttp://localhost:9093/\n\nVersion: v1.0.0 (427b8e9)\n\n183",
      "content_length": 1082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Chapter 6: Alerting and Alertmanager\n\nFigure 6.3: Alertmanager web interface\n\nYou can use this interface to view current alerts and manage maintenance window alert suppression, named “silences” in Prometheus terminology.\n\n TIP There’s also a command line tool amtool, that ships with Alertmanager\n\nthat allows you to query alerts, manage silences and work with an Alertmanager server.\n\nNow’s lets configure Prometheus to find our Alertmanager.\n\nConfiguring Prometheus for Alertmanager\n\nLet’s quickly detour back to our Prometheus configuration to tell it about our new Alertmanager. In Chapter 3 we saw the default Alertmanager configuration in the prometheus.yml configuration file. The Alertmanager configuration is contained in the alerting block. Let’s have a look at the default block.\n\nVersion: v1.0.0 (427b8e9)\n\n184",
      "content_length": 823,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.14: The alerting block\n\nalerting:\n\nalertmanagers: - static_configs:\n\ntargets:\n\nalertmanager:9093\n\nThe alerting block contains configuration that allows Prometheus to identify one or more Alertmanagers. To do this, Prometheus reuses the same discovery mechanisms it uses to find targets to scrape. In the default configuration this is static_configs. Like a monitoring job this specifies a list of targets, here in the form of a host name, alertmanager, and a port, 9093—the Alertmanager default port. This listing assumes your Prometheus server can resolve the alertmanager hostname to an IP address and that the Alertmanager is running on port 9093 on that host.\n\n TIP You’ll also be able to see any configured Alertmanagers in the Prometheus\n\nweb interface on the status page: http://localhost:9090/status.\n\nAlertmanager service discovery\n\nAs we have access to service discovery mechanisms, we could also use one of those to identify one or more Alertmanagers. Let’s add a DNS SRV record that allows Prometheus to discover our Alertmanagers.\n\nLet’s create that record now.\n\nVersion: v1.0.0 (427b8e9)\n\n185",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.15: The Alertmanager SRV record\n\n_alertmanager._tcp.example.com. 300 IN SRV 10 1 9093 alertmanager1.example.com.\n\nHere we’ve specified a TCP service called _alertmanager in the form of a SRV record. Our record returns the hostname alertmanager1.example.com and port number 9093 where Prometheus will expect to find an Alertmanager running. Let’s configure the Prometheus server to search there.\n\nListing 6.16: Discovering the Alertmanager\n\nalerting:\n\nalertmanagers: - dns_sd_configs:\n\nnames: [ '_alertmanager._tcp.example.com' ]\n\nHere Prometheus will query the _alertmanager._tcp.example.com SRV record to return our Alertmanager’s hostname. We can do the same with other service discovery mechanisms to identify Alertmanagers to Prometheus.\n\n TIP You’ll need to reload or restart Prometheus to enable the Alertmanager\n\nconfiguration.\n\nVersion: v1.0.0 (427b8e9)\n\n186",
      "content_length": 915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Chapter 6: Alerting and Alertmanager\n\nMonitoring Alertmanager\n\nLike Prometheus, Alertmanager exposes metrics about itself. Prometheus job for monitoring our Alertmanager.\n\nLet’s create a\n\nListing 6.17: The Alertmanager Prometheus job\n\njob_name: 'alertmanager' static_configs:\n\ntargets: ['localhost:9093']\n\nThis will collect metrics from http://localhost:9093/metrics and scrape a series of time series prefixed with alertmanager_. These include counts of alerts by state, and counts of successful and failed notifications by receiver—for example, all failed notifications to the email receiver. It also contains cluster status metrics that we can make use of when we look at clustering Alertmanagers in Chapter 7.\n\nAdding alerting rules\n\nNow that we’ve got Alertmanager set up, let’s add our first alerting rules. We’re going to create alerts from the node queries we developed in Chapter 4 as well as some basic availability alerting using the up metric.\n\nLike recording rules, alerting rules are defined as YAML statements in rules files loaded in the Prometheus server configuration. Let’s create a new file, node_alerts.yml, in our rules directory to hold our node alerting rules.\n\n TIP You can comingle recording rules and alerting rules in the same file, but\n\nI like to keep them in separate files for clarity.\n\nVersion: v1.0.0 (427b8e9)\n\n187",
      "content_length": 1349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.18: Creating an alerting rules file\n\n$ cd rules $ touch node_alerts.yml\n\nRather than add this file to the rule_files block in our prometheus.yml config- uration file, let’s use globbing to load all files that end in either _rules.yml or _alerts.yml in that directory.\n\nListing 6.19: Adding globbing rule_files block\n\nrule_files:\n\n\"rules/*_rules.yml\" - \"rules/*_alerts.yml\"\n\nYou can see that we’ve added configuration that will load all files with the right naming convention. We’d need to restart the Prometheus server to load this new alerting rules file.\n\nAdding our first alerting rule\n\nLet’s add our first rule: a CPU alerting rule. We’re going to create an alert that will trigger if the CPU query we created, the average node CPU five-minute rate, is over 80 percent for at least 60 minutes.\n\nLet’s see that rule now.\n\nVersion: v1.0.0 (427b8e9)\n\n188",
      "content_length": 903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.20: Our first alerting rule\n\ngroups: - name: node_alerts\n\nrules: - alert: HighNodeCPU\n\nexpr: instance:node_cpu:avg_rate5m > 80 for: 60m labels:\n\nseverity: warning\n\nannotations:\n\nsummary: High Node CPU for 1 hour console: You might want to check the Node Dashboard at\n\nhttp://grafana.example.com/dashboard/db/node-dashboard\n\nLike our recording rules, alerting rules are grouped together. We’ve specified a group name: node_alerts. The rules in this group are contained in the rules block. Each has a name, specified in the alert clause. Ours is called HighNodeCPU. In each alert group, the alert name needs to be unique.\n\nWe also have the test or expression that will trigger the alert. This is specified in the expr clause. Our test expression uses the instance:node_cpu:avg_rate5m metric we created in Chapter 4 using a recording rule.\n\ninstance:node_cpu:avg_rate5m > 80\n\nWe append a simple check—is the metric greater than 80, or 80 percent?\n\nThe next clause, for, controls the length of time the test expression must be true for before the alert is fired. In our case, the instance:node_cpu:avg_rate5m needs to be greater than 80 percent for 60 minutes before the alert is fired. This limits the potential of the alert being a false positive or a transitory state.\n\nLast, we can decorate our alert with labels and annotations. All the current labels on time series in the alert rule are carried over to the alert. The labels clause allows us to specify additional labels to be attached to the alert; here we’ve added\n\nVersion: v1.0.0 (427b8e9)\n\n189",
      "content_length": 1599,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Chapter 6: Alerting and Alertmanager\n\na severity label with a value of warning. We’ll see how we can use this label shortly.\n\nThe labels on the alert, combined with the name of the alert, represent the identity of the alert. This is the same premise as time series, where the metric name and labels represent the identity of a time series.\n\nThe annotations clause allows us to specify informational labels like a description, a link to a run book, or instructions on how to handle this alert. We’ve added a label called summary that describes the alert. We’ve also added an annotation called console that points the recipient to a Grafana dashboard for node-based metrics. This is an excellent example of providing context with an annotation.\n\nNow we need to reload Prometheus to enable our new alerting rule.\n\nOnce Prometheus is restarted, you’ll be able to see your new alert in the Prometheus web interface at http://localhost:9090/alerts.\n\nVersion: v1.0.0 (427b8e9)\n\n190",
      "content_length": 974,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "Chapter 6: Alerting and Alertmanager\n\nFigure 6.4: List of Prometheus alerts\n\nThis is both a summary of the alerting rule and, as we’ll see shortly, a way to see the status of each alert.\n\nVersion: v1.0.0 (427b8e9)\n\n191",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Chapter 6: Alerting and Alertmanager\n\nWhat happens when an alert fires?\n\nSo how does an alert fire? Prometheus evaluates all rules at a regular interval, defined by the evaluation_interval, which we’ve set to 15 seconds. At each evaluation cycle, Prometheus runs the expression defined in each alerting rule and updates the alert state.\n\nAn alert can have three potential states:\n\nInactive - The alert is not active. • Pending - The alert has met the test expression but is still waiting for the duration specified in the for clause to fire.\n\nFiring - The alert has met the test expression and has been Pending for\n\nlonger than the duration of the for clause.\n\nThe Pending to Firing transition ensures an alert is more likely to be valid and not flapping. Alerts without a for clause automatically transition from Inactive to Firing and only take one evaluation cycle to trigger. Alerts with a for clause will transition first to Pending and then to Firing, thus taking at least two evaluation cycles to trigger.\n\nSo far, the lifecycle of our alert is:\n\n1. The CPU of a node constantly changes, and it gets scraped by Prometheus\n\nevery scrape_interval. For us this is every 15 seconds.\n\n2. Alerting rules are then evaluated against the metrics every evaluation_interval\n\n. For us this is 15 seconds again.\n\n3. When the alerting expression is true—for us, CPU is over 80 percent—an alert is created and transitions to the Pending state, honoring the for clause. 4. Over the next evaluation cycles, if the alert test expression continues to be true, then the duration of the for is checked. If that duration is then com- plete, the alert transitions to Firing and a notification is generated and pushed to the Alertmanager. 192\n\n3. When the alerting expression is true—for us, CPU is over 80 percent—an alert is created and transitions to the Pending state, honoring the for clause. 4. Over the next evaluation cycles, if the alert test expression continues to be true, then the duration of the for is checked. If that duration is then com- plete, the alert transitions to Firing and a notification is generated and pushed to the Alertmanager. 192",
      "content_length": 2145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Chapter 6: Alerting and Alertmanager\n\n5. If the alert test expression is no longer true then Prometheus changes the\n\nalerting rule’s state from Pending to Inactive.\n\nThe alert at the Alertmanager\n\nOur alert is now in the Firing state, and a notification has been pushed to the Alertmanager. We can see this alert and its status in the Prometheus web interface at http://localhost:9090/alerts.\n\n NOTE The Alertmanager API receives alerts on the URI /api/v1/alerts.\n\nPrometheus will also create a metric for each alert in the Pending and Firing states. The metric will be called ALERT and will be constructed like this example for our HighNodeCPU alert.\n\nListing 6.21: The ALERT time series\n\nALERTS{alertname=\"HighNodeCPU\",alertstate=\"firing\",severity= warning,instance=\"138.197.26.39:9100\"}\n\nEach alert metric has a fixed value of 1 and exists for the period the alert is in the Pending or Firing states. After that it receives no updates and is eventually expired.\n\nThe notification is sent to the Alertmanager(s) defined in the Prometheus configuration—in our case at the alertmanager host on port 9093. The notifica- tion is pushed to an HTTP endpoint:\n\nhttp://alertmanager:9093/api/v1/alerts\n\nVersion: v1.0.0 (427b8e9)\n\n193",
      "content_length": 1227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Chapter 6: Alerting and Alertmanager\n\nLet’s assume one of our HighNodeCPU alerts has fired. We’ll be able to see that alert in the Alertmanager web console at http://alertmanager:9093/#/alerts.\n\nFigure 6.5: Fired alert in Alertmanager\n\nYou can use this interface to search for, query, and group current alerts, according to their labels.\n\nIn our current Alertmanager configuration, our alert will immediately be routed to our email receiver, and an email like this one below will be generated:\n\nVersion: v1.0.0 (427b8e9)\n\n194",
      "content_length": 525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "Chapter 6: Alerting and Alertmanager\n\nFigure 6.6: HighNodeCPU alert email\n\n TIP We’ll see how to update this template later in the chapter.\n\nThis doesn’t seem very practical if we have many teams, or alerts of different severities. This is where Alertmanager routing is useful.\n\nAdding new alerts and templates\n\nSo that we have more alerts to route, let’s quickly add some other alert rules to the node_alerts.yml alerting rule file.\n\nVersion: v1.0.0 (427b8e9)\n\n195",
      "content_length": 466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.22: Adding more alerting rules\n\ngroups: - name: node_alerts\n\nrules:\n\n. . .\n\nalert: DiskWillFillIn4Hours\n\nexpr: predict_linear(node_filesystem_free{mountpoint=\"/\"}[1h\n\n], 4*3600) < 0 for: 5m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Disk on {{ $labels.instance }} will fill in\n\napproximately 4 hours.\n\nalert: InstanceDown\n\nexpr: up{job=\"node\"} == 0 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Host {{ $labels.instance }} of {{ $labels.job }}\n\nis down!\n\nThe first alert replicates the predict_linear disk prediction we saw in Chapter 4. Here, if the linear regression predicts the disk space of the / root filesystem will be exhausted within four hours, the alert will fire. You’ll also notice that we’ve added some template values to the summary annotation.\n\nTemplates\n\nTemplates are a way of making use of the labels and value of your time series data in your alerts. Templates can be used in annotations and labels. The templates use the standard Go template syntax and expose some variables that contain the labels and value of a time series. The labels are made available in a convenience\n\nVersion: v1.0.0 (427b8e9)\n\n196",
      "content_length": 1198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Chapter 6: Alerting and Alertmanager\n\nvariable, $labels, and the value of the metric in the variable $value.\n\n TIP The $labels and $value variables are more convenient names for the underlying Go variables: .Labels and .Value, respectively.\n\nTo refer to the instance label in our summary annotation we use {{ $labels. instance }}. If we wanted to refer to the value of the time series, we’d use {{ $value }}. Prometheus also provides some functions, which you can see in the template reference. An example of this is the humanize function, which turns a number into a more human-readable form using metric prefixes. For example:\n\nListing 6.23: Humanizing a value\n\n. . . annotations:\n\nsummary: High Node CPU of {{ humanize $value }}% for 1\n\nhour\n\nThis would display the value of the metric as a two-decimal-place percentage, e.g., 88.23%.\n\nPrometheus alerts\n\nWe shouldn’t forget that things can go wrong with our Prometheus server, too. Let’s add a couple of rules to identify issues there and alert on them. We’ll create a new file, prometheus_alerts.yml, in the rules directory to hold these. As this matches our rules glob, it’ll also be loaded by Prometheus.\n\nVersion: v1.0.0 (427b8e9)\n\n197",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.24: Creating the prometheus_alerts.yml file\n\n$ touch rules/prometheus_alerts.yml\n\nAnd let’s populate this file.\n\nListing 6.25: The prometheus_alerts.yml file\n\ngroups: - name: prometheus_alerts\n\nrules: - alert: PrometheusConfigReloadFailed\n\nexpr: prometheus_config_last_reload_successful == 0 for: 10m labels:\n\nseverity: warning\n\nannotations:\n\ndescription: Reloading Prometheus configuration has failed\n\non {{ $labels.instance }}.\n\nalert: PrometheusNotConnectedToAlertmanagers\n\nexpr: prometheus_notifications_alertmanagers_discovered < 1 for: 10m labels:\n\nseverity: warning\n\nannotations:\n\ndescription: Prometheus {{ $labels.instance }} is not\n\nconnected to any Alertmanagers\n\nHere we’ve added two new rules. The first, PrometheusConfigReloadFailed, lets us know if our Prometheus configuration has failed a reload. This lets us know, using the metric prometheus_config_last_reload_successful, if the last reload failed. If the reload did fail, the metric will have a value of 0.\n\nThe second rule makes sure our Prometheus server can discover Alertmanagers. This uses the prometheus_notifications_alertmanagers_discovered metric,\n\nVersion: v1.0.0 (427b8e9)\n\n198",
      "content_length": 1207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Chapter 6: Alerting and Alertmanager\n\nIf it is less than 1, which is a count of Alertmanagers this server has found. Prometheus hasn’t discovered any Alertmanagers and this alert will fire. As there aren’t any Alertmanagers, it will only show up on the Prometheus console on the /alerts page.\n\nAvailability alerts\n\nOur last alerts help us determine the ability of hosts and services. The first of these alerts takes advantage of the systemd metrics we are collecting using the Node Exporter. We’re going to generate an alert if any of the services we’re monitoring on our nodes is no longer active.\n\nListing 6.26: Node service alert\n\nalert: NodeServiceDown\n\nexpr: node_systemd_unit_state{state=\"active\"} != 1 for: 60s labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Service {{ $labels.name }} on {{ $labels.instance }}\n\nis no longer active!\n\ndescription: Werner Heisenberg says - \"OMG Where's my\n\nservice?\"\n\nThis alert will trigger if the node_systemd_unit_state metric with the active label is 0, indicating that a service has failed for at least 60 seconds.\n\nThe next alert uses the up metric we also saw in Chapter 4. This metric is useful for monitoring the availability of a host. It’s not perfect because what we’re really monitoring is the success or failure of a job’s scrape of that target. But it’s useful to know if the instance has stopped responding to scrapes, which potentially indi- cates a larger problem. To do this, the alert detects if the up metric has a value of 0, indicating a failed scrape.\n\nVersion: v1.0.0 (427b8e9)\n\n199",
      "content_length": 1551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "Chapter 6: Alerting and Alertmanager\n\nup{job=\"node\"} == 0\n\nWe’ve added a new value to our severity label of critical and added a templated annotation to help indicate which instance and job have failed.\n\nIn many cases, knowing a single instance is down isn’t actually very important. Instead we could also test for a number of failed instances—for example, a per- centage of our instances:\n\navg(up) by (job) <= 0.50\n\nThis test expression works out the average of the up metric, aggregates it by job, and fires if that value is below 50 percent. If 50 percent of the instances in a job fail their scrapes, the alert will fire.\n\nAnother approach might be:\n\nsum by job (up) / count(up) <= 0.8\n\nHere we’re summing the up metric by job, dividing it by the count, and firing if the result is greater than or equal to 0.8 or indicating that 20 percent of instances in a specific job are not up.\n\nWe can make our up alert slightly more robust by identifying when targets dis- appear. If, for example, our target is removed from service discovery, then its metrics will no longer be updated. If all targets disappear from service discovery, no metrics will be recorded—hence our up alert won’t be fired. Prometheus has a function, absent, that detects the presence of missing metrics.\n\nVersion: v1.0.0 (427b8e9)\n\n200",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.27: The up metric missing alert\n\nalert: InstancesGone\n\nexpr: absent(up{job=\"node\"}) for: 10s labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Host {{ $labels.instance }} is no longer\n\nreporting!\n\ndescription: 'Werner Heisenberg says, OMG Where are my\n\ninstances?'\n\nHere our expression uses the absent function to detect if any of the up metrics from the node job disappear, and it fires an alert if they do.\n\n TIP Another approach for availability monitoring is the probing of endpoints\n\nover HTTP, HTTPS, DNS, TCP, and ICMP. We’ll see more of it in Chapter 10.\n\nFinally, we’ll need to restart the Prometheus server to load these new alerts.\n\nRouting\n\nNow that we have a selection of alerts with some varying attributes, we need to route them to various folks. We discovered earlier that routing is a tree. The top, default route is always configured and matches anything that isn’t matched by a child route.\n\nGoing back to our Alertmanager configuration, let’s add some routing configura- tion to our alertmanager.yml file.\n\nVersion: v1.0.0 (427b8e9)\n\n201",
      "content_length": 1108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.28: Adding routing configuration\n\nroute:\n\ngroup_by: ['instance'] group_wait: 30s group_interval: 5m repeat_interval: 3h receiver: email routes: - match:\n\nseverity: critical\n\nreceiver: pager\n\nmatch_re:\n\nseverity: ^(warning|critical)$\n\nreceiver: support_team\n\nreceivers: - name: 'email'\n\nemail_configs: - to: 'alerts@example.com'\n\nname: 'support_team'\n\nemail_configs: - to: 'support@example.com'\n\nname: 'pager'\n\nemail_configs: - to: 'alert-pager@example.com'\n\nYou can see that we’ve added some new options to our default route. The first op- tion, group_by, controls how the Alertmanager groups alerts. By default, all alerts are grouped together, but if we specify group_by and any labels, then Alertman- ager will group alerts by those labels. For example, we’ve specified the instance label, which means that all alerts from a specific instance will be grouped together. If you list more than one label, alerts are grouped if every specified label value matches, for example:\n\nVersion: v1.0.0 (427b8e9)\n\n202",
      "content_length": 1056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.29: Grouping\n\nroute:\n\ngroup_by: ['service', 'cluster']\n\nHere the values of the service and cluster labels need to match for an alert to be grouped.\n\n NOTE This only works for labels, not annotations.\n\nGrouping also changes the behavior of Alertmanager. If a new alert is raised, Alertmanager will wait for the period specified in our next option, group_wait, to see if other alerts from that group are received, before firing the alert(s). You can think about this like a group alert buffer. In our case, this wait is 30 seconds.\n\nAfter the alert(s) are fired, if new alerts from the next evaluation are received for that grouping, Alertmanager will wait for the period specified in the group_interval option, five minutes for us, before sending the new alerts. This prevents alert floods for groupings of alerts.\n\nWe’ve also specified the repeat_interval. This is a pause that applies not to our groups of alerts, but rather to each single alert, and is the period to wait to resend the same alert. We’ve specified three hours.\n\nRoutes\n\nWe’ve then listed our branched routes. Our first route uses a new receiver we’ve defined, pager. This sends the alerts on this route to a new email address. It finds the specific alerts to be sent using the match option. There are two kind\n\nVersion: v1.0.0 (427b8e9)\n\n203",
      "content_length": 1358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "Chapter 6: Alerting and Alertmanager\n\nof matching: label matching and regular expression matching. The match option does simple label matching.\n\nListing 6.30: Label matching\n\nmatch:\n\nseverity: critical\n\nHere we’re matching all severity labels with a value of critical and sending them to the pager receiver.\n\nAs routes are branches, we can also branch the route again if we need. For exam- ple:\n\nListing 6.31: Routing branching\n\nroutes: - match:\n\nseverity: critical\n\nreceiver: pager routes:\n\nmatch:\n\nservice: application1\n\nreceiver: support_team\n\nYou can see our new routes block nested inside our existing route. To trigger this route our alert would first need a severity label of critical and then a service label of application1. If both these criteria matched, then our alert would be routed to the receiver support_team.\n\nWe can nest our routes as far down as we need. By default, any alert that matches a route is handled by that route. We can, however, override that behavior using the continue option. The continue option controls whether an alert will traverse the route and then return to traverse the route tree.\n\nVersion: v1.0.0 (427b8e9)\n\n204",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "Chapter 6: Alerting and Alertmanager\n\n NOTE Alertmanager routes are post-order traversed.\n\nListing 6.32: Routing branching\n\nroutes: - match:\n\nseverity: critical\n\nreceiver: pager continue: true\n\nThe continue option defaults to false, but if set to true the alert will trigger in this route if matched, and continue to the next sibling route. This is sometimes useful for sending alerts to two places, but a better approach is to specify multiple endpoints in your receiver. For example:\n\nListing 6.33: Multiple endpoints in a receiver\n\nreceivers: - name: 'email'\n\nemail_configs: - to: 'alerts@example.com' pagerduty_configs: - service_key: TEAMKEYHERE\n\nThis adds a second pagerduty_configs block that sends to PagerDuty as well as via email. We could specify any of the available receiver destinations—for example, we could send email and a message to a chat service like Slack.\n\n TIP Used to seeing resolution alerts? These are alerts generated when the\n\nVersion: v1.0.0 (427b8e9)\n\n205",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Chapter 6: Alerting and Alertmanager\n\nalert condition has been resolved. They can be sent with Alertmanager by setting the send_resolved option to true in your receiver configuration. Sending these resolution alerts is often not recommended as it can lead to a cycle of alerting “false alarms” that result in alert fatigue. Think carefully before enabling them.\n\nOur second route uses the match_re option to match a regular expression against a label. The regular expression also uses the severity label.\n\nListing 6.34: A regular expression match\n\nmatch_re:\n\nseverity: ^(informational|warning)$\n\nreceiver: support_team\n\n NOTE Prometheus and Alertmanager regular expressions are fully an-\n\nchored.\n\nIt matches either informational or warning values in the severity label.\n\nOnce you’ve reloaded or restarted Alertmanager to load the new routes, you can try to trigger alerts and see the routing in action.\n\nReceivers and notification templates\n\nNow that we’ve got some basic rules in place, let’s add a non-email receiver. We’re going to add the Slack receiver, which sends messages to Slack instances. Let’s see our new receiver configuration in the alertmanager.yml configuration file.\n\nVersion: v1.0.0 (427b8e9)\n\n206",
      "content_length": 1218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "Chapter 6: Alerting and Alertmanager\n\nFirst, we’ll add a Slack configuration to our pager receiver.\n\nListing 6.35: Adding a Slack receiver\n\nreceivers: - name: 'pager'\n\nemail_configs: - to: 'alert-pager@example.com' slack_configs: - api_url: https://hooks.slack.com/services/ABC123/ABC123/\n\nEXAMPLE\n\nchannel: #monitoring\n\nNow, any route that sends alerts to the pager receiver will be sent both to Slack in the #monitoring channel and via email to the alert-pager@example.com email address.\n\nThe generic alert message that Alertmanager sends to Slack is pretty simple. You can see the default template that Alertmanager uses in its source code. This tem- plate contains the defaults for email and other receivers, but we can override these values for many of the receivers. For example, we can add a text line to our Slack alerts.\n\nListing 6.36: Adding a Slack receiver\n\nslack_configs: - api_url: https://hooks.slack.com/services/ABC123/ABC123/\n\nEXAMPLE\n\nchannel: #monitoring text: '{{ .CommonAnnotations.summary }}'\n\nAlertmanager notification customization uses Go templating syntax. The data contained in the alerts is also exposed via variables. We’re using the CommonAnnotations variable, which contains the set of annotations common to\n\nVersion: v1.0.0 (427b8e9)\n\n207",
      "content_length": 1271,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Chapter 6: Alerting and Alertmanager\n\na group of alerts. We’re using the summary annotation as the text of the Slack notification.\n\n TIP You can find a full reference to notification template variables in the\n\nAlertmanager documentation.\n\nWe can also use the Go template function to reference external templates, to save on having long, complex strings embedded in our configuration file. We refer- enced the template directory earlier in this chapter—ours is at /etc/alertmanager /templates/. Let’s create a template in this directory.\n\nListing 6.37: Creating a template file\n\n$ touch /etc/alertmanager/templates/slack.tmpl\n\nAnd let’s populate it.\n\nListing 6.38: The slack.tmpl file\n\n{{ define \"slack.example.text\" }}{{ .CommonAnnotations.summary }}{{ end}}\n\nHere we’ve defined a new template using the define function and ending with end. We’ve called it slack.example.text and moved the content from text in- side the template. We can now reference that template inside our Alertmanager configuration.\n\nVersion: v1.0.0 (427b8e9)\n\n208",
      "content_length": 1037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.39: Adding a Slack receiver\n\nslack_configs: - api_url: https://hooks.slack.com/services/ABC123/ABC123/\n\nEXAMPLE\n\nchannel: #monitoring text: '{{ template \"slack.example.text\" . }}'\n\nWe’ve used the template option to specify the name of our template. The text field will now be populated with our template notification. This is useful for decorating notifications with context.\n\n TIP There are some other examples of notification templates in the Alertman-\n\nager documentation.\n\nSilences and maintenance\n\nOften we need to let our alerting system know that we’ve taken something out of service for maintenance and that we don’t want alerts triggered. Or we need to mute downstream services and applications when something upstream is bro- ken. Prometheus calls this muting of alerts a “silence.” Silences can be set for specific periods—for example, an hour—or over a set window—for example, un- til midnight today. This is the silence’s expiry time or expiration date. If required, we can also manually expire a silence early, if, say, our maintenance is complete earlier than planned.\n\nYou can schedule silences using two methods.\n\nVersion: v1.0.0 (427b8e9)\n\n209",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Chapter 6: Alerting and Alertmanager\n\nVia the Alertmanager web console. • Via the amtool command line tool.\n\nControlling silences via the Alertmanager\n\nThe first method is to use the web interface and click the New Silence button.\n\nFigure 6.7: Scheduling silences\n\nSilences specify a start time, end time, or a duration. The alerts to be silenced are identified by matching alerts using labels, much like alert routing. You can use straight matches—for example, matching every alert that has a label with a specific value—or you can use a regular expression match. You also need to specify an author for the silence and a comment explaining why alerts are being silenced.\n\nVersion: v1.0.0 (427b8e9)\n\n210",
      "content_length": 703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "Chapter 6: Alerting and Alertmanager\n\nFigure 6.8: A new silence\n\nWe click Create to create the new silence (and we can use Preview Alerts to identify if any current alerts will be silenced). Once created we can edit a silence or expire it to remove the silence.\n\nVersion: v1.0.0 (427b8e9)\n\n211",
      "content_length": 293,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Chapter 6: Alerting and Alertmanager\n\nFigure 6.9: Editing or expiring silences\n\nYou can see a list of the currently defined silences in the web interface by clicking on the Silences menu item in the Alertmanager top menu.\n\n NOTE There’s an alternative Alertmanager console called Unsee you might\n\nlike to check out.\n\nVersion: v1.0.0 (427b8e9)\n\n212",
      "content_length": 348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Chapter 6: Alerting and Alertmanager\n\nControlling silences via amtool\n\nThe second method is using the amtool command line. The amtool binary ships with the Alertmanager installation tarball, and we installed it when we installed Alertmanager earlier in the chapter.\n\nListing 6.40: Using amtool to schedule a silence\n\n$ amtool --alertmanager.url=http://localhost:9093 silence add alertname=InstancesGone service=application1 784ac68d-33ce-4e9b-8b95-431a1e0fc268\n\nThis will add a new silence on the Alertmanager at http://localhost:9093. The silence will match alerts with two labels: alertname, an automatically populated label containing the alert’s name, and service, a label we’ve set.\n\n TIP Silences created with amtool are set to automatically expire after one hour. You can specify longer times or a set window with the --expires and --expire-on flags.\n\nA silence ID will also be returned that you can use to later work with the silence. Here ours is:\n\n784ac68d-33ce-4e9b-8b95-431a1e0fc268\n\nWe can query the list of current silences using the query sub-command.\n\nVersion: v1.0.0 (427b8e9)\n\n213",
      "content_length": 1099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Chapter 6: Alerting and Alertmanager\n\nListing 6.41: Querying the silences\n\n$ amtool --alertmanager.url=http://localhost:9093 silence query\n\nThis will return a list of silences and their configurations. You can expire a specific silence via its ID.\n\nListing 6.42: Expiring the silence\n\n$ amtool --alertmanager.url=http://localhost:9093 silence expire\n\n784ac68d-33ce-4e9b-8b95-431a1e0fc268\n\nThis will expire the silence on the Alertmanager.\n\nRather than having to specify the --alertmanager.url flag every time, you can create a YAML configuration file for some options. The default configuration file paths that amtool will look for are $HOME/.config/amtool/config.yml or /etc/ amtool/config.yml. Let’s see a sample file.\n\nListing 6.43: Sample amtool configuration file\n\nalertmanager.url: \"http://localhost:9093\" author: sre@example.com comment_required: true\n\nYou can see that we’ve added an Alertmanager to work with. We’ve also specified an author. This is the setting for the creator of a silence; it defaults to your local username, unless overridden like this, or on the command line with the -a or - -author flag. The comment_required flag controls whether a silence requires a comment explaining what it does.\n\nVersion: v1.0.0 (427b8e9)\n\n214",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "Chapter 6: Alerting and Alertmanager\n\n NOTE You can specify all amtool flags in the configuration file, but some\n\ndon’t make a lot of sense.\n\nBack to creating silences. You can also use a regular expression as the label value when creating a silence.\n\nListing 6.44: Using amtool to schedule a silence\n\n$ amtool silence add --comment \"App1 maintenance\" alertname=~' Instance.*' service=application1\n\nHere we’ve used =~ to indicate the label match is a regular expression and matched on all alerts with an alertname that starts with Instance. We’ve also used the -- comment flag to add information about our alert.\n\nWe can also control further details of the silence, like so:\n\nListing 6.45: Omitting alertname\n\n$ amtool silence add --author \"James\" --duration \"2h\" alertname= InstancesGone service=application1\n\nHere we’ve overridden the silence’s creator with the --author flag and specified the duration of the silence as two hours, instead of the default one hour.\n\n TIP The amtool also allows us to work with Alertmanager and validate its\n\nconfiguration files, among other useful tasks. You can see the full list of command line flags by running amtool with the --help flag. You can also get help for specific\n\nVersion: v1.0.0 (427b8e9)\n\n215",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Chapter 6: Alerting and Alertmanager\n\nsub-commands, amtool silence --help. You can generate Bash completions and a man page for amtool using instructions from here.\n\nSummary\n\nIn this chapter, we had a crash course on alerting with Prometheus and Alertman- ager.\n\nWe touched upon what good alerts look like. We installed Alertmanager on a variety of platforms and configured it.\n\nWe saw how to use our time series as a source for alerts, and how to generate those alerts using alerting rules. We saw how to use time series directly or how to build further expressions that analyze time series data to identify alert conditions. We also saw how to add new labels and decorate alerts with additional information and context.\n\nWe also saw how to control alerting using silences to mute alerts during mainte- nance windows or outages.\n\nIn the next chapter we’ll see how to make Prometheus and the Alertmanager more resilient and scalable. We’ll also see how to extend the retention life of your metrics by sending them to remote destinations.\n\nVersion: v1.0.0 (427b8e9)\n\n216",
      "content_length": 1069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "Chapter 7\n\nScaling and Reliability\n\nUp until now we’ve seen Prometheus operate as a single server with a single Alert- manager. This fits many monitoring scenarios, especially at the team level when a team is monitoring their own resources, but it often doesn’t scale to multiple teams. It’s also not very resilient or robust. In these situations, if our Prometheus server or Alertmanager becomes overloaded or fails, our monitoring or alerting will fail, too.\n\nWe’re going to separate these into two concerns:\n\nReliability and fault tolerance. • Scaling.\n\nPrometheus addresses each concern differently, but we’ll see how some archi- tectural choices address both. In this chapter we’ll discuss the philosophy and methodology by which Prometheus approaches each concern and understand how to build more scalable and robust Prometheus implementations.\n\n217",
      "content_length": 855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Chapter 7: Scaling and Reliability\n\nReliability and fault tolerance\n\nPrometheus’s approach to addressing the issue of fault tolerance is tempered by concern about the operational and technical complexities of achieving a high tol- erance. In many cases, fault tolerance for monitoring services is addressed by making the monitoring service highly available, usually by clustering the imple- mentation. Clustering solutions, however, require relatively complex networking and management of state between nodes in the cluster.\n\nIt’s also important to note, as we mentioned in Chapter 2, that Prometheus is focused on real time monitoring, typically with limited data retention, and con- figuration is assumed to be managed by a configuration management tool. An individual Prometheus server is generally considered disposable from an avail- ability perspective. Prometheus architecture argues that the investment required to achieve that cluster, and consensus of data between nodes of that cluster, is higher than the value of the data itself.\n\nPrometheus doesn’t ignore the need to address fault tolerance though. Indeed, the recommended fault-tolerant solution for Prometheus is to run two identically configured Prometheus servers in parallel, both active at the same time. Duplicate alerts generated by this configuration are handled upstream in Alertmanager using its grouping (and its inhibits capability). Instead of focusing on the fault tolerance of the Prometheus server, the recommended approach is to make the upstream Alertmanagers fault tolerant.\n\nVersion: v1.0.0 (427b8e9)\n\n218",
      "content_length": 1591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Chapter 7: Scaling and Reliability\n\nFigure 7.1: Fault-tolerant architecture\n\nThis is achieved by creating a cluster of Alertmanagers. All Prometheus servers send alerts to all Alertmanagers. As mentioned, the Alertmanagers take care of deduplication and share alert state through the cluster.\n\nThere are obviously downsides to this approach. First, both Prometheus servers will be collecting metrics, doubling any potential load generated by that collection. However, one could argue that the load generated by a scrape is likely low enough that this isn’t an issue. Second, if an individual Prometheus server fails or suffers an outage, you’ll have a gap in data on one server. This means being aware of that gap when querying data on that server. This is also a relatively minor concern given there’s another server to query and that the general focus is on immediate data, not on using Prometheus data for long-term trending analysis.\n\nVersion: v1.0.0 (427b8e9)\n\n219",
      "content_length": 969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Chapter 7: Scaling and Reliability\n\n TIP There are ways to compensate for this in PromQL. For example, when asking for a single metric value from two sources, you could use the max by of both metrics. Or, when alerting from a single worker shard with possible gaps, you might increase the for clause to ensure you have more than one measure.\n\nDuplicate Prometheus servers\n\nWe’re not going to document the details of building two duplicate Prometheus servers; this should be relatively easy to achieve using your configuration man- agement tool. We recommend replicating the installation steps from Chapter 3 using one of the configuration management solutions documented there.\n\nInstead we’re going to focus on the more complex operation of clustering Alert- managers.\n\nSetting up Alertmanager clustering\n\nAlertmanager contains a cluster capability provided by Hashicorp’s memberlist library. Memberlist is a Go library that manages cluster membership and member- failure detection using a gossip-based protocol, in this case an extension of the SWIM protocol.\n\nTo configure clustering we need to install Alertmanager on more than one host. In our case we’re going to run it on three hosts: am1, am2, and am3. We first install Alertmanager on each host as we did in Chapter 6. We’ll then use the am1 host to initiate the cluster.\n\nVersion: v1.0.0 (427b8e9)\n\n220",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "Chapter 7: Scaling and Reliability\n\nListing 7.1: Starting Alertmanager cluster\n\nam1$ alertmanager --config.file alertmanager.yml --cluster. listen-address 172.19.0.10:8001\n\nWe’ve run the alertmanager binary specifying a configuration file, we can just use the file we created in Chapter 6, and a cluster listen address and port. You should use identical configuration on every node in the cluster. This ensures that alert handling is identical and that your cluster will behave consistently.\n\n WARNING All Alertmanagers should use identical configuration! If it’s\n\nnot identical, it’s not actually highly available.\n\nWe’ve specified the IP address of the am1 host, 172.19.0.10, and a port of 8001. Other nodes in the Alertmanager cluster will use this address to connect to the clus- ter, so that port will need to be open on the network between your Alertmanager cluster nodes.\n\n TIP If you don’t specify the cluster listen address, it’ll default to 0.0.0.0 on port 9094.\n\nWe can then run the Alertmanager on the remaining two hosts, listening on their local IP addresses, and referencing the IP address and port of the cluster node we’ve just created.\n\nVersion: v1.0.0 (427b8e9)\n\n221",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "Chapter 7: Scaling and Reliability\n\nListing 7.2: Starting Alertmanager cluster remaining nodes\n\nam2$ alertmanager --config.file alertmanager.yml --cluster. listen-address 172.19.0.20:8001 --cluster.peer 172.19.0.10:8001 am3$ alertmanager --config.file alertmanager.yml --cluster. listen-address 172.19.0.30:8001 --cluster.peer 172.19.0.10:8001\n\nYou can see that we’ve run the alertmanager binary on the other two Alertman- ager hosts: am2 and am3. We’ve specified a cluster listen address for each using their own IP addresses and the 8001 port. We’ve also specified, using the cluster .peer flag, the IP address and port of the am1 node as a peer so they can join the cluster.\n\nYou won’t see any specific messages indicating the cluster has started (although if you pass the --debug flag you’ll get more informative output) but you can confirm it on one of the Alertmanager’s console status page at /status. Let’s look at am1 at https://172.19.0.10:9093/status.\n\nVersion: v1.0.0 (427b8e9)\n\n222",
      "content_length": 994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "Chapter 7: Scaling and Reliability\n\nFigure 7.2: Alertmanager cluster status\n\nWe can see our am1 Alertmanager can see three nodes in the cluster: itself plus am2 and am3.\n\nYou can test that the cluster is working by scheduling a silence on one Alertman- ager and seeing if it is replicated to the other Alertmanagers. To do this, click the New Silence button on am1 and schedule a silence. Then check the /silences path on am2 and am3. You should see the same silence replicated on all hosts.\n\nNow that our cluster is running, we need to tell Prometheus about all the Alert-\n\nVersion: v1.0.0 (427b8e9)\n\n223",
      "content_length": 605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Chapter 7: Scaling and Reliability\n\nmanagers.\n\nConfiguring Prometheus for an Alertmanager cluster\n\nFor resilience purposes, we have to specifically identify all Alertmanagers to the Prometheus server. This way, if an Alertmanager goes down, Prometheus can find an alternative to send an alert to. The Alertmanager cluster itself takes care of sharing any received alert with the other active members of the cluster and potentially handles any deduplication. Thus you should not load balance your Alertmanagers—Prometheus handles that for you.\n\nWe could define all of the Alertmanagers to Prometheus using static configuration like so:\n\nListing 7.3: Defining alertmanagers statically\n\nalerting:\n\nalertmanagers: - static_configs:\n\ntargets:\n\nam1:9093 - am2:9093 - am3:9093\n\nWith this configuration the Prometheus server will connect to all three of our Alertmanagers. This assumes that our Prometheus server can resolve DNS entries for each of the Alertmanagers. A smarter approach is to use service discovery to find all of the Alertmanagers. For example, to use DNS-based discovery as we saw in Chapter 6, we can add DNS SRV records for each Alertmanager.\n\nVersion: v1.0.0 (427b8e9)\n\n224",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Chapter 7: Scaling and Reliability\n\nListing 7.4: The Alertmanager SRV record\n\n_alertmanager._tcp.example.com. 300 IN SRV 10 1 9093 am1.example. com. _alertmanager._tcp.example.com. 300 IN SRV 10 1 9093 am2.example. com. _alertmanager._tcp.example.com. 300 IN SRV 10 1 9093 am3.example. com.\n\nHere we’ve specified a TCP service called _alertmanager in the form of a SRV record. Our record returns three host names—am1, am2, and am3—and port num- ber 9093 where Prometheus can expect to find an Alertmanager running. Let’s configure the Prometheus server to discover them.\n\nListing 7.5: Discovering the Alertmanager\n\nalerting:\n\nalertmanagers: - dns_sd_configs:\n\nnames: [ '_alertmanager._tcp.example.com' ]\n\nHere Prometheus will query the alertmanager.example.com SRV record to return our list of Alertmanagers. We could do the same with other service discovery mechanisms to identify all the Alertmanagers in our cluster to Prometheus.\n\nIf we now restart Prometheus we can see all of our connected Alertmanagers in the Prometheus server’s status page.\n\nVersion: v1.0.0 (427b8e9)\n\n225",
      "content_length": 1081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Chapter 7: Scaling and Reliability\n\nFigure 7.3: Prometheus clustered Alertmanagers\n\nNow when an alert is raised it is sent to all the discovered Alertmanagers. The Alertmanagers receive the alert, handle deduplication, and share state across the cluster.\n\nTogether this provides the upstream fault tolerance that ensures your alerts are delivered.\n\nScaling\n\nIn addition to fault tolerance, we also have options for scaling Prometheus. Most of the options are essentially manual and involve selecting specific workloads to run on specific Prometheus servers.\n\nScaling your Prometheus environment usually takes two forms: functional scaling or horizontal scaling.\n\nVersion: v1.0.0 (427b8e9)\n\n226",
      "content_length": 693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "Chapter 7: Scaling and Reliability\n\nFunctional scaling\n\nFunctional scaling uses shards to split monitoring concerns onto separate Prometheus servers. For example, this could be splitting servers via geography or logical domains.\n\nFigure 7.4: Organizational sharding\n\nOr it could be via specific functions, sending all infrastructure monitoring to one server and all application monitoring to another server.\n\nVersion: v1.0.0 (427b8e9)\n\n227",
      "content_length": 439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Chapter 7: Scaling and Reliability\n\nFigure 7.5: Functional sharding\n\nIt is a relatively simple process to create otherwise identical Prometheus servers with specific jobs running on each server. It is best done using configuration management tools to ensure creating servers, and the specific jobs that run on them, is an automated process.\n\nFrom here, if you need a holistic view of certain areas or functions, you can po- tentially use federation (more on this shortly) to extract time series to centralized Prometheus servers. Usefully, Grafana supports pulling data from more than one Prometheus server to construct a graph. This allows you to federate data from multiple servers at the visualization level, assuming some consistency in the time series being collected.\n\nVersion: v1.0.0 (427b8e9)\n\n228",
      "content_length": 805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "Chapter 7: Scaling and Reliability\n\nHorizontal shards\n\nAt some point, usually in huge installations, the capacity and complexity of ver- tical sharding will become problematic. This is especially true when individual jobs contain thousands of instances. In that case, you can consider an alternative: horizontal sharding. Horizontal sharding uses a series of worker servers, each of which scrapes a subset of targets. We then aggregate specific time series we’re in- terested in on the worker servers. For example, if we’re monitoring host metrics, we might aggregate a subset of those metrics. A primary server then scrapes each of the worker’s aggregated metrics using Prometheus’s federation API.\n\nFigure 7.6: Horizontal sharding\n\nVersion: v1.0.0 (427b8e9)\n\n229",
      "content_length": 764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "Chapter 7: Scaling and Reliability\n\nOur primary server not only pulls in the aggregated metrics but now also acts as the default source for graphing or exposing metrics to tools like Grafana. You can add tiered layers of workers and primaries if you need to go deeper or scale further. A good example of this is a zone-based primary and workers, perhaps for a failure domain or a logical zone like an Amazon Availability Zone, reporting up to a global primary that treats the zone-based primaries as workers.\n\n TIP If you need to query metrics that are not being aggregated, you will\n\nneed to refer to the specific worker server that is collecting for the specific target or targets you are interested in. You can use the worker label to help you identify the right worker.\n\nIt’s important to note that this sort of scaling does have risks and limitations, perhaps the most obvious being that you need to scrape a subset of metrics from the worker servers rather than a large volume or all of the metrics the workers are collecting. This is a pyramid-like hierarchy rather than a distributed hierarchy. The scraping requests of the primary onto the workers is also load that you will need to consider.\n\nNext, you’re creating a more complex hierarchy of Prometheus servers in your environment. Rather than just the connection between the workers and the tar- gets, you also need to worry about the connection between the primary and the workers. This could reduce the reliability of your solution.\n\nLast, the potential consistency and correctness of your data could be reduced. Your workers are scraping targets according to their intervals, and your primary server is in turn scraping the workers. This introduces a delay in the results reaching the primary server and could potentially skew data or result in an alert being delayed.\n\nA consequences of the latter two issues is that it’s probably not a good idea to centralize alerting on the primary server. Instead, push alerting down onto the\n\nVersion: v1.0.0 (427b8e9)\n\n230",
      "content_length": 2028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "Chapter 7: Scaling and Reliability\n\nworker servers where they are more likely to identify issues, like a missing target, or reduce the lag between identifying the alert condition and the alert firing.\n\n NOTE Horizontal sharding is generally a last resort. We’d expect you to\n\nhave tens of thousands of targets or large volumes of time series being scraped per target before you’d need to scale out in this manner.\n\nWith these caveats in mind, let’s look at how we might use this configuration.\n\nCreating shard workers\n\nLet’s create some workers and see how they can scrape the target subsets. We’re going to create workers and number them 0 through 2. We’re going to assume that the primary job our workers will execute is scraping the node_exporter. Ev- ery worker needs to be uniquely identifiable. We’re going to use external labels to do this. External labels are added to every time series or alert that leaves a Prometheus server. External labels are provided via the external_labels config- uration block in our prometheus.yml.\n\nLet’s create the base configuration for our first worker, worker0, now.\n\n TIP As always, use configuration management to do this.\n\nVersion: v1.0.0 (427b8e9)\n\n231",
      "content_length": 1199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "Chapter 7: Scaling and Reliability\n\nListing 7.6: The worker0 configuration\n\nglobal:\n\nexternal_labels:\n\nworker: 0\n\nrule_files:\n\n\"rules/node_rules.yml\"\n\nscrape_configs:\n\njob_name: 'node' file_sd_configs:\n\nfiles:\n\ntargets/nodes/*.json refresh_interval: 5m\n\nrelabel_configs: - source_labels: [__address__]\n\nmodulus: target_label: action:\n\n3 __tmp_hash hashmod\n\nsource_labels: [__tmp_hash] ^0$ keep\n\nWe can see our external_labels block contains a label, worker, with a value of 0. We’ll use worker: 1, worker: 2, and so on for our remaining workers. We’ve defined a single job, which uses file-based service discovery, to load a list of targets from any file ending in *.json in the targets/nodes directory. We would use a service discovery tool or a configuration management tool to populate all of our nodes into the JSON file or files.\n\nWe then use relabelling to create a modulus of the source_labels hash. In our case, we’re just creating a modulus of the hash of the concatenated address label. We use a modulus of 3, the number of workers scraping metrics. You’ll need to update this value if you add workers (another good reason to use a configuration management tool that can automatically increment the modulus). The hash is\n\nVersion: v1.0.0 (427b8e9)\n\n232",
      "content_length": 1262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Chapter 7: Scaling and Reliability\n\ncreated using the hashmod action. The result is then stored in a target label called __tmp_hash.\n\nWe then use the keep action to match any time series from any targets that match the modulus. So worker0 would retrieve time series from targets with a modulo of 0, worker1 those targets with a modulo of 1, etc. This evenly distributes targets between the workers. If you need to scale to more targets you can add workers and update the modulus used on the hash.\n\nWe can then aggregate the worker time series we want to federate using rules. Let’s say we’d like to gather the memory, CPU, and disk metrics from the Node Exporter for federation. To aggregate the time series we want we’re going to use the rules we created in Chapter 4—for example, the CPU rule:\n\nListing 7.7: The instance CPU rule\n\ngroups: - name: node_rules\n\nrules: - record: instance:node_cpu:avg_rate5m\n\nexpr: 100 - avg (irate(node_cpu{job=\"node\",mode=\"idle\"}[5m]))\n\nby (instance) * 100\n\nThis will create a series of new time series that we’ll then scrape upstream using a primary Prometheus server.\n\nPrimary shard server\n\nLet’s now configure a primary Prometheus server to scrape the workers for the time series. The primary Prometheus server has a job or jobs to scrape workers; each worker is a target in a job. Let’s look at the prometheus.yml configuration for our primary server.\n\nVersion: v1.0.0 (427b8e9)\n\n233",
      "content_length": 1421,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "Chapter 7: Scaling and Reliability\n\nListing 7.8: The primary configuration\n\n. . .\n\nscrape_configs: - job_name: 'node_workers'\n\nfile_sd_configs:\n\nfiles:\n\n'targets/workers/*.json' refresh_interval: 5m\n\nhonor_labels: true metrics_path: /federate params:\n\n'match[]':\n\n'{__name__=~\"^instance:.*\"}'\n\nOn our primary server, we’ve got a job called node_workers. This job discovers the list of workers using file-based service discovery. Our workers are in workers /targets/workers.json.\n\nListing 7.9: Worker file discovery\n\n[{\n\n\"targets\": [\n\n\"worker0:9090\", \"worker1:9090\", \"worker2:9090\"\n\n]\n\n}]\n\nYou’ll note we’ve enabled the honor_labels flag. This flag controls how Prometheus handles conflicts between labels. By setting it to true we ensure that an upstream primary server doesn’t overwrite labels from downstream workers.\n\nWe’ve overridden the standard metrics path to use the /federate API. The\n\nVersion: v1.0.0 (427b8e9)\n\n234",
      "content_length": 925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "Chapter 7: Scaling and Reliability\n\nfederate API endpoint allows us to query a remote Prometheus server for specific time series, specified by a matching parameter.\n\nListing 7.10: Matching parameter\n\nmetrics_path: /federate\n\nparams:\n\n'match[]':\n\n'{__name__=~\"^instance:.*\"}'\n\nWe use the params option to specify the match[] parameter. The match[] param- eter takes an instant vector selector that has to match the specific time series we want to return. In our case we’re matching against the name of the time series.\n\nListing 7.11: condition]The match[] condition\n\n'{__name__=~\"^instance:.*\"}'\n\n TIP You can specify multiple match[] parameters, and Prometheus will return\n\nthe union of all of the conditions.\n\nThis match is a regular expression match that returns all of the time series that start with instance:. All of the rules we used to aggregate our Node Exporter metrics are prefixed with instance:, so the time series for CPU, memory, and disk will be selected and scraped by the primary server.\n\nWe can see what is going to be selected by the query parameter by using curl or browsing to the /federate path, with an appropriate match[] parameter, on one of the worker servers.\n\nVersion: v1.0.0 (427b8e9)\n\n235",
      "content_length": 1219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "Chapter 7: Scaling and Reliability\n\nFigure 7.7: The Federate API\n\nOur query has returned all of the time series starting with instance:.\n\nThe primary server’s node_workers job will scrape these metrics each time it’s run. You can then use the primary server to query and graph the aggregated metrics from all of the targets scraped by the worker servers.\n\nRemote storage\n\nThere’s one last aspect of scaling we should mention: remote storage. Prometheus has the capability to write to (and in some cases read from) remote stores of metrics. The ability to write to remote storage allows you to send metrics from Prometheus, working around its constraints in scalability, to a remote system.\n\nPrometheus has two types of remote storage integration:\n\nIt can write metric samples to a remote destination. • It can read metric samples to a remote destination.\n\nVersion: v1.0.0 (427b8e9)\n\n236",
      "content_length": 886,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "Chapter 7: Scaling and Reliability\n\nThe remote storage protocol uses a Snappy-compressed protocol buffer encoding over HTTP. It’s configured in Prometheus via the remote_write and remote_read blocks.\n\nCurrently, Prometheus supports a variety of endpoints for writing and reading. You can find a full list in the Prometheus documentation but highlights include Chronix, CrateDB, Graphite, InfluxDB, OpenTSDB, and PostgreSQL.\n\nWe’re not going to cover any of these in any detail, but you should be able to follow the documentation and examples to get started.\n\nThird-party tools\n\nThere’s a small selection of third-party tools that aim to make Prometheus scaling easier. These include:\n\nCortex - A scalable Prometheus-as-a-Service tool. • Thanos - A highly available Prometheus setup with long-term storage capa- bilities.\n\nVulcan - A now discontinued attempt to build a more scalable Prometheus.\n\nMostly useful as a reference.\n\nSummary\n\nIn this chapter we learned how Prometheus handles fault tolerance for monitoring. We saw how to create a cluster of Alertmanagers to ensure that your alerts are sent.\n\nWe also saw how to scale Prometheus monitoring using additional servers or via sharding with federation.\n\nIn the next chapter we’ll look at instrumenting applications for monitoring.\n\nVersion: v1.0.0 (427b8e9)\n\n237",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "Chapter 8\n\nInstrumenting Applications\n\nIn the last few chapters we’ve seen the mechanics of Prometheus. We’ve collected metrics to process and visualize. We’ve gathered host and container metrics using Prometheus and exporters.\n\nIn this chapter we’re going to extend our monitoring and collection to applica- tions. We’re going to focus on monitoring applications and how to emit metrics by instrumenting code. We’re going to see how to add a Prometheus client to an application, add metrics to the application, and then use a Prometheus job to scrape those metrics.\n\nFirst, though, we’re going to go through some high-level design patterns and prin- ciples you should consider when thinking about application monitoring.\n\nAn application monitoring primer\n\nLet’s look at some basic tenets for application monitoring. First, in any good ap- plication development methodology, it’s a great idea to identify what you want to build before you build it. Monitoring is no different. Sadly there’s a common anti-pattern in application development of considering monitoring and other oper-\n\n238",
      "content_length": 1086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Chapter 8: Instrumenting Applications\n\national functions like security as value-add components of your application rather than core features. Monitoring (and security!) are core functional features of your applications. If you’re building a specification or user stories for your application, include monitoring for each component of your application. Not building metrics or monitoring is a serious business and operational risk resulting in:\n\nAn inability to identify or diagnose faults. • An inability to measure the operational performance of your application. • An inability to measure the business performance and success of an applica- tion or a component, such as tracking sales figures or the value of transac- tions.\n\nAnother common anti-pattern is not instrumenting enough. We’ll always recom- mended that you over-instrument your applications. One will often complain about having too little data, but rarely will one worry about having too much.\n\n NOTE Within constraints of storage capacity, your monitoring stopping\n\nworking because you exceeded that capacity is obviously undesirable. It’s often useful to look at retention time as a primary way to reduce storage without losing useful information.\n\nThird, if you use multiple environments—for example development, testing, stag- ing, and production—ensure that your monitoring configuration provides labels so you know that the data is from a specific environment. This way you can parti- tion your monitoring and metrics. We’ll talk more about this later in the chapter.\n\nVersion: v1.0.0 (427b8e9)\n\n239",
      "content_length": 1571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Chapter 8: Instrumenting Applications\n\nWhere should I instrument?\n\nGood places to start adding instrumentation for your applications are at points of ingress and egress. For example:\n\nMeasure counts and timings of requests and responses, such as to specific web pages or API endpoints. If you’re instrumenting an existing application, make a priority-driven list of specific pages or endpoints, and instrument them in order of importance.\n\nMeasure counts and timings of calls to external services and APIs, such as if your application uses a database, cache, or search service, or if it uses third-party services like a payments gateway.\n\nMeasure counts and timings of job scheduling, execution, and other periodic\n\nevents like cron jobs.\n\nMeasure counts and timings of significant business and functional events,\n\nsuch as users being created, or transactions like payments and sales.\n\nInstrument taxonomies\n\nYou should ensure that metrics are categorized and clearly identified by the appli- cation, method, function, or similar marker so that you can ensure you know what and where a metric is generated. We talked about label taxonomies in Chapter 4.\n\nMetrics\n\nLike much of the rest of our monitoring, metrics are going to be key to our appli- cation monitoring. So what should we monitor in our applications? We want to look at two broad types of metrics—albeit types with considerable overlap:\n\nApplication metrics, which generally measure the state and performance of\n\nyour application code.\n\nVersion: v1.0.0 (427b8e9)\n\n240",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "Chapter 8: Instrumenting Applications\n\nBusiness metrics, which generally measure the value of your application. For example, on an e-commerce site, it might be how many sales you made.\n\nWe’re going to look at examples of both types of metrics in this chapter, with the caveat that Prometheus tends to focus on more immediate metrics. For longer- term business metrics, you may, in many cases, use event-based systems.\n\nApplication metrics\n\nApplication metrics measure the performance and state of your applications. They include characteristics of the end user experience of the application, like latency and response times. Behind this we measure the throughput of the application: requests, request volumes, transactions, and transaction timings.\n\n TIP Good examples of how to measure application performance are the USE\n\nand RED Methods and Google Golden Signals that we mentioned earlier.\n\nWe also look at the functionality and state of the application. A good example of this might be successful and failed logins or errors, crashes, and failures. We could also measure the volume and performance of activities like jobs, emails, or other asynchronous activities.\n\nBusiness metrics\n\nBusiness metrics are the next layer up from our applications metrics. They are usually synonymous with application metrics. If you think about measuring the number of requests made to a specific service as being an application metric, then the business metric usually does something with the content of the request. An\n\nVersion: v1.0.0 (427b8e9)\n\n241",
      "content_length": 1539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "Chapter 8: Instrumenting Applications\n\nexample of the application metric might be measuring the latency of a payment transaction; the corresponding business metric might be the value of each payment transaction. Business metrics might include the number of new users/customers, number of sales, sales by value or location, or anything else that helps measure the state of a business.\n\nWhere to put your metrics\n\nOnce we know what we want to monitor and measure, we need to work out where to put our metrics. In almost all cases the best place to put these metrics is inside our code, as close as possible to the action we’re trying to monitor or measure.\n\nWe don’t, however, want to put our metrics configuration inline everywhere that we want to record a metric. Instead we want to create a utility library: a function that allows us to create a variety of metrics from a centralized setup. This is sometimes called the utility pattern: a metrics-utility class that does not require instantiation and only has static methods.\n\nThe utility pattern\n\nA common pattern is to create a utility library or module using one of the available clients. The utility library would expose an API that allows us to create and incre- ment metrics. We can then use this API throughout our code base to instrument the areas of the application we’re interested in.\n\nLet’s take a look at an example of this. We’ve created some Ruby-esque code to demonstrate, and we’ve assumed that we have already created a utility library called Metric.\n\n NOTE We’ll see a functioning example of this pattern later in this chapter.\n\nVersion: v1.0.0 (427b8e9)\n\n242",
      "content_length": 1630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Chapter 8: Instrumenting Applications\n\nListing 8.1: A sample payments method\n\ninclude Metric\n\ndef pay_user(user, amount)\n\npay(user.account, amount) Metric.increment 'payment' Metric.increment \"payment-amount, #{amount.to_i}\" send_payment_notification(user.email)\n\nend\n\ndef send_payment_notification(email)\n\nsend_email(payment, email) Metric.increment 'email-payment'\n\nend\n\nHere we’ve first included our Metric utility library. We can see that we’ve spec- ified both application and business metrics. We’ve first defined a method called pay_user that takes user and amount values as parameters. We’ve then made a payment using our data and incremented two metrics in our first method:\n\nA payment metric — Here we increment the metric each time we make a\n\npayment.\n\nA payment-amount metric — This metric records each payment by amount.\n\nFinally, we’ve sent an email using a second method, send_payment_notification , where we’ve incremented a third metric: email-payment. The email-payment metric counts the number of payment emails sent.\n\nVersion: v1.0.0 (427b8e9)\n\n243",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "Chapter 8: Instrumenting Applications\n\nThe external pattern\n\nWhat if you don’t control the code base, can’t insert monitors or measures inside your code, or perhaps have a legacy application that can’t be changed or updated? Then you need to find the next closest place to your application. The most obvi- ous places are the outputs and external subsystems around your application—for example, a database or cache.\n\nIf your application emits logs, then identify what material the logs contain and see if you can use their contents to measure the behavior of the application. Often you can track the frequency of events by simply recording the counts of specific log entries. If your application records or triggers events in other systems—things like database transactions, job scheduling, emails sent, calls to authentication or authorization systems, caches, or data stores—then you can use the data contained in these events or the counts of specific events to record the performance of your application.\n\nWe’ll talk more about this in Chapter 9.\n\nBuilding metrics into a sample application\n\nNow that we have some background on monitoring applications, let’s look at an example of how we might implement this in the real world. We’re going to build an application that takes advantage of a utility library to send events from the ap- plication. We’ve created a sample Rails application using Rails Composer. We’re going to call it mwp-rails, or Monitoring with Prometheus Rails application. The mwp-rails application allows us to create and delete users and sign in to the ap- plication.\n\n NOTE You can find the mwp-rails application on GitHub.\n\nVersion: v1.0.0 (427b8e9)\n\n244",
      "content_length": 1680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Chapter 8: Instrumenting Applications\n\nTo instrument our application we first need to add support for Prometheus using a Ruby-based client. The prometheus-client gem allows us to create a Prometheus client inside our application.\n\nThere are similar clients for a number of platforms including:\n\nGo • Java/JVM • Python\n\nThere is also a large collection of third-party clients for a variety of frameworks and languages.\n\nAdding the client\n\nLet’s add the prometheus-client gem to our Rails application’s Gemfile.\n\nListing 8.2: The mwp-rails Gemfile\n\nsource 'https://rubygems.org' ruby '2.4.2' gem 'rails', '5.1.5' . . . gem 'prometheus-client' . . .\n\nWe then install the new gem using the bundle command.\n\nVersion: v1.0.0 (427b8e9)\n\n245",
      "content_length": 733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "Chapter 8: Instrumenting Applications\n\nListing 8.3: Install prometheus-client with the bundle command\n\n$ sudo bundle install Fetching gem metadata from https://rubygems.org/... Fetching version metadata from https://rubygems.org/... Fetching dependency metadata from https://rubygems.org/.. . . . Installing prometheus-client 0.7.1 . . .\n\nWe can then test the client using a Rails console. Let’s launch one now using the rails c command.\n\nListing 8.4: Testing the Prometheus client with the Rails console\n\n$ rails c Loading development environment (Rails 4.2.4) [1] pry(main)> prometheus = Prometheus::Client.registry [2] pry(main)> test_counter = prometheus.counter(:test_counter, 'A test counter') => #<Prometheus::Client::Counter:0x00007f9aea051dd8\n\n@base_labels={}, @docstring=\"A test counter\", @mutex=#<Thread::Mutex:0x00007f9aea051d88>, @name=:test_counter, @validator=#<Prometheus::Client::LabelSetValidator:0\n\nx00007f9aea051d10 @validated={}>,\n\n@values={}>\n\n[3] pry(main)> test_counter.increment => 1.0\n\nWe’ve launched a Rails console and created a Prometheus registry using the code:\n\nVersion: v1.0.0 (427b8e9)\n\n246",
      "content_length": 1124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Chapter 8: Instrumenting Applications\n\nListing 8.5: Creating a Prometheus registry\n\nprometheus = Prometheus::Client.registry\n\nThe registry is the core of the Prometheus application instrumentation. Every metric you create needs to be registered first. We’ve created a registry called prometheus. We can now create metrics in this registry.\n\nListing 8.6: Registering a Prometheus metric\n\ntest_counter = prometheus.counter(:test_counter, 'A test counter ')\n\nWe have a new metric called test_counter. It’s created with the counter method on the registry. The metric name needs to be a symbol, :test_counter, and needs a description. Our is: A test counter.\n\nWe can increment our new metric with the increment method.\n\nListing 8.7: Incrementing a metric\n\ntest_counter.increment\n\nNow the value of the test_counter metric will be 1.0, which we can see by query- ing the value of the metric using the get method.\n\nVersion: v1.0.0 (427b8e9)\n\n247",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Chapter 8: Instrumenting Applications\n\nListing 8.8: Incrementing a metric\n\ntest_counter.get 1.0\n\nWe can register a number of types of metrics, including summaries and histograms.\n\nListing 8.9: The basic Prometheus client_ruby metrics\n\ntest_counter = prometheus.counter(:test_counter, 'A test counter ') test_gauge = prometheus.gauge(:test_gauge, 'A test gauge') test_histogram = prometheus.histogram(:test_histogram, 'A test histogram') test_summary = prometheus.summary(:test_summary, 'A test summary ')\n\nWe can now add the instrumentation to our Rails application.\n\nAdding it to Rails\n\nWe’re not going to manually create a registry and metrics every time we want to log any metrics, so let’s set up some utility code to do this for us. We’re going to create a Metrics module in our lib directory that we’ll use in our Rails application. Let’s do that now.\n\nListing 8.10: Creating a Metrics module\n\n$ touch lib/metrics.rb\n\nAnd let’s populate the file with a module.\n\nVersion: v1.0.0 (427b8e9)\n\n248",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "Chapter 8: Instrumenting Applications\n\nListing 8.11: The Metrics module\n\nmodule Metrics\n\ndef self.counter(name, docstring, base_labels = {})\n\nprovide_metric(name) || registry.counter(name, docstring,\n\nbase_labels)\n\nend\n\ndef self.summary(name, docstring, base_labels = {})\n\nprovide_metric(name) || registry.summary(name, docstring,\n\nbase_labels)\n\nend\n\ndef self.gauge(name, docstring, base_labels = {})\n\nprovide_metric(name) || registry.gauge(name, docstring,\n\nbase_labels)\n\nend\n\ndef self.histogram(name, docstring, base_labels = {}, buckets\n\n= ::Prometheus::Client::Histogram::DEFAULT_BUCKETS)\n\nprovide_metric(name) || registry.histogram(name, docstring,\n\nbase_labels, buckets)\n\nend\n\nprivate\n\ndef self.provide_metric(name)\n\nregistry.get(name)\n\nend\n\ndef self.registry\n\n@registry || ::Prometheus::Client.registry\n\nend\n\nend\n\nOur Metrics module has methods for each metric type. The metric methods check for the presence of an existing metric in the registry (using the get method in provide_metric which retrieves metric names) or creates a new metric.\n\nVersion: v1.0.0 (427b8e9)\n\n249",
      "content_length": 1080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Chapter 8: Instrumenting Applications\n\nWe then need to extend Rails to load our Metrics library. There’s a few ways to do this, but adding an initializer is my favorite.\n\nListing 8.12: Creating an initializer for the metrics library\n\n$ touch config/initializers/lib.rb\n\nAnd then requiring our library.\n\nListing 8.13: The config/initializers/lib.rb file\n\nrequire 'metrics'\n\n TIP We could also extend the autoload_paths configuration option to load everything in lib, but I feel like this gives us less control over what loads.\n\nWe can then add metrics to some of our methods. Let’s start with incrementing a counter when users are deleted.\n\nListing 8.14: Counter for user deletions\n\ndef destroy\n\nuser = User.find(params[:id]) user.destroy Metrics.counter(:users_deleted_counter, \"Deleted users counter\n\n\").increment\n\nredirect_to users_path, :notice => \"User deleted.\"\n\nend\n\nVersion: v1.0.0 (427b8e9)\n\n250",
      "content_length": 904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "Chapter 8: Instrumenting Applications\n\nWe can see the line:\n\nMetrics.counter(:users_deleted_counter, \"Deleted users counter\").\n\nincrement\n\nWe call the counter metric in the Metrics module and pass in a metric name in the form of a symbol, :users_deleted_counter, then a description of the metric, Deleted users counter. We’ve ended the line with the increment method that increments the counter once. This will create a metric called users_deleted_counter.\n\nWe could also add a label or increment by a specific value by using the increment method like so:\n\n.increment({ service: 'foo' }, 2)\n\nThis would increment a counter with a value of 2 and add the label service: foo to the metric. You can specify multiple labels by separating each with commas: { service: 'foo', app: 'bar' }.\n\nWe could also create another counter for created users by adding it to the User model.\n\nListing 8.15: Counter for user creation\n\nclass User < ActiveRecord::Base\n\nenum role: [:user, :vip, :admin] after_initialize :set_default_role, :if => :new_record? after_create do\n\nMetrics.counter(:user_created_counter, \"Users created\n\ncounter\").increment\n\nend . . . end\n\nHere we’ve used an Active Record callback, after_create, to increment a counter, users_created_counter, when a new user is created.\n\nVersion: v1.0.0 (427b8e9)\n\n251",
      "content_length": 1306,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Chapter 8: Instrumenting Applications\n\n NOTE You may have a wide variety of applications you want to instrument.\n\nWe’re creating an example application to show you how we might apply some of these principles. While you might not be able to reuse the code, the high-level principles apply to almost every framework and language you’re likely to have running.\n\nWe then need to expose our metrics to be scraped. We’re also going to enable some In our Rack middleware to auto-create some useful metrics on HTTP requests. case we enable the metrics endpoint by adding an exporter (and the middleware collector) to our config.ru file.\n\nListing 8.16: Adding Prometheus to the config.ru file\n\nrequire 'prometheus/middleware/collector' require 'prometheus/middleware/exporter'\n\nuse Prometheus::Middleware::Collector use Prometheus::Middleware::Exporter\n\nHere we’ve required and used two components of the Prometheus client: the mid- dleware exporter and collector. The exporter creates a route, /metrics, containing any metrics specified in Prometheus registries defined by the app. The collector adds some HTTP server metrics to the endpoint that are collected via Rack mid- dleware.\n\nIf we browse to this endpoint, /metrics, we’ll see some of those metrics.\n\nVersion: v1.0.0 (427b8e9)\n\n252",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "Chapter 8: Instrumenting Applications\n\nListing 8.17: The Rails /metrics endpoint\n\n# HELP http_server_requests_total The total number of HTTP requests handled by the Rack application. http_server_requests_total{code=\"200\",method=\"get\",path=\"/\"} 2.0 # HELP http_server_request_duration_seconds The HTTP response duration of the Rack application. http_server_request_duration_seconds_bucket{method=\"get\",path=\"/ \",le=\"0.005\"} 0.0 http_server_request_duration_seconds_bucket{method=\"get\",path=\"/ \",le=\"0.01\"} 0.0 . . .\n\n# HELP users_updated_counter Users updated counter users_updated_counter 1.0\n\nWe can see a selection of the metrics available. Perhaps most interesting is a series of histogram buckets showing the HTTP server request duration, with dimensions for method and path. This histogram is an easy way to measure request latency for specific paths and to identify any badly performing requests.\n\nUsing our metrics\n\nNow our application has metrics being generated and we can make use of them in Prometheus. Let’s create a job to scrape our /metrics endpoint. We’re going to add our Rails servers to our file-based service discovery. We’re going to add three Rails servers by their hostnames.\n\nVersion: v1.0.0 (427b8e9)\n\n253",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Chapter 8: Instrumenting Applications\n\nListing 8.18: Our Rails servers service discovery\n\n[{\n\n\"targets\": [\"mwp−rails1.example.com\", \"mwp−rails2.example.\n\ncom\", \"mwp−rails3.example.com\"] }]\n\nWe’re then going to create our new job in our prometheus.yml configuration file.\n\nListing 8.19: The rails job\n\n− job_name: rails file_sd_configs:\n\n− files:\n\n− targets/rails/*.json refresh_interval: 5m\n\nIf we reload Prometheus we’ll be able to see our Rails servers as new targets.\n\nFigure 8.1: Rails server targets\n\nAnd see our new metrics in the dashboard.\n\nVersion: v1.0.0 (427b8e9)\n\n254",
      "content_length": 579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "Chapter 8: Instrumenting Applications\n\nFigure 8.2: Rails metrics\n\nNow we can make use of these metrics to monitor our Rails servers.\n\nSummary\n\nIn this chapter we explored ways to monitor and instrument our applications and their workflows, including understanding where to place our application monitor- ing.\n\nWe learned about building our own metrics into our applications and services, and we built a sample Rails application to show how to expose and scrape metrics with Prometheus.\n\nIn the next chapter we will see how to turn external data, specifically log entries, into metric data you can consume with Prometheus.\n\nVersion: v1.0.0 (427b8e9)\n\n255",
      "content_length": 653,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "Chapter 9\n\nLogging as Instrumentation\n\nIn previous chapters we looked at application, host, and container-based monitor- ing. In this chapter we’re going to look at how we can use our logging data as the source of time series data that can be scraped by Prometheus. While our hosts, services, and applications can generate crucial metrics and events, they also often generate logs that can tell us useful things about their state and status.\n\nThis is especially true if you’re monitoring a legacy application that is not instru- mented or that it’s not feasible to instrument. In this case, sometimes the cost of rewriting, patching, or refactoring that application to expose internal state is not a good engineering investment, or there are technological constraints to instru- mentation. You still need to understand what’s happening inside the application, though—and one of the easiest ways is to adapt log output.\n\n TIP Another potential approach is to look at the contents of the /proc sub-\n\nsystem using the Process exporter.\n\nLog output often contains useful status, timing, and measurement information.\n\n256",
      "content_length": 1117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "Chapter 9: Logging as Instrumentation\n\nFor example, using the access log output from a web or application server is a useful way of tracking transaction timings or error volumes. Tools can parse these log entries, create metrics from matched output, and make them available to be scraped by a Prometheus job.\n\nIn this chapter we’re going to look at using log entries to create metrics, and then scrape them with Prometheus.\n\nProcessing logs for metrics\n\nIn order to extract data from our log entries we’re going to make use of a log processing tool. There are several we could use, including the Grok Exporter and a utility from Google called mtail. We’ve chosen to look at mtail because it’s a little more lightweight and somewhat more popular.\n\n TIP Got a Logstash/ELK installation? You can’t currently directly output to\n\nPrometheus but you can use Logstash’s metric filter to create metrics and output them to Alertmanager directly.\n\nIntroducing mtail\n\nThe mtail log processor is written by SRE folks at Google. It’s a Go application licensed with the Apache 2.0 license. The mtail log processor is specifically de- signed for extracting metrics from application logs to be exported into a time series database. It aims to fill the niche we described above: parsing log data from applications that cannot export their own internal state.\n\nThe mtail log processor works by running “programs” that define log matching\n\nVersion: v1.0.0 (427b8e9)\n\n257",
      "content_length": 1452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "Chapter 9: Logging as Instrumentation\n\npatterns, and specify the metrics to create from the matches and any actions to It works very well with Prometheus and exposes any created metrics for take. scraping, but can also be configured to send the metrics to tools like collectd, StatsD, or Graphite.\n\nInstalling mtail\n\nThe mtail log processor is shipped as a single binary: mtail. It’s packaged for a variety of operating systems including Linux, OS X, and Microsoft Windows.\n\nLet’s download the binary now.\n\nListing 9.1: Download and install the mtail binary\n\n$ wget https://github.com/google/mtail/releases/download/v3.0.0- rc13/mtail_v3.0.0-rc13_linux_amd64 -O mtail $ chmod 0755 mtail $ sudo cp mtail /usr/local/bin\n\nWe can confirm the mtail binary is working by running it with the --version flag.\n\nListing 9.2: Running the mtail binary\n\n$ mtail --version mtail version v3.0.0-rc13-119-g01c76cd git revision 01 c76cde1ee5399be4d6c62536d338ba3077e0e7 go version go1.8.3\n\nVersion: v1.0.0 (427b8e9)\n\n258",
      "content_length": 1003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "Chapter 9: Logging as Instrumentation\n\nUsing mtail\n\nThe mtail binary is configured via the command line. You specify a list of log files to parse, and a directory of programs to run over those files.\n\n TIP You can see a full list of mtails’s command line flags using the --help\n\nflag.\n\nLet’s start by creating a directory to hold our mtail programs. As always, this is usually better done by creating a configuration management module (or a Docker container) rather than being done manually, but we’ll show you the details so you can understand what’s happening.\n\nListing 9.3: Creating an mtail program directory\n\n$ sudo mkdir /etc/mtail\n\nNow let’s create our first mtail program in a file in our new directory. Every mtail program needs to end with the suffix .mtail. Let’s create a new program called line_count.mtail. This is the simplest mtail program: it increments a counter every time it parses a new line.\n\nListing 9.4: Creating the line_count.mtail program\n\n$ sudo touch /etc/mtail/line_count.mtail\n\nAnd let’s populate that file.\n\nVersion: v1.0.0 (427b8e9)\n\n259",
      "content_length": 1071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "Chapter 9: Logging as Instrumentation\n\nListing 9.5: The line_count.mtail program\n\ncounter line_count\n\n/$/ {\n\nline_count++\n\n}\n\nWe’ve started our program by defining a counter called line_count. Counter names are prefixed with counter (and, naturally, gauges are prefixed with gauge ). These counters and gauges are exported by mtail to whatever destination you define; in our case, it’ll be an endpoint that can be scraped by Prometheus. You must define any counters or gauges before you can work with them.\n\nNext, we define the guts of our mtail program: the condition we want to match and the action we want to take, with the condition specified first and the action following, wrapped in { }.\n\nYou can specify multiple sets of conditions and actions in a program. You can extend them with conditional logic in the form of an else clause too.\n\n NOTE mtail programs look a lot like awk programs.\n\nThe condition can be a regular expression, matching some specific log entry. In our case, we’ve specified /$/, which matches the end of the line. The mtail processor uses RE2 regular expressions. You can see the full syntax on the RE2 wiki.\n\nWe could also specify a relational expression, much like those in a C if clause, for example:\n\nVersion: v1.0.0 (427b8e9)\n\n260",
      "content_length": 1265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Chapter 9: Logging as Instrumentation\n\nListing 9.6: A relational clause\n\nline_count < 20 {\n\n. . .\n\n}\n\nHere the program would only take the action if the value of the line_count counter was greater than 20.\n\nIn the case of our initial program our action is:\n\nline_count++\n\nWhich uses an operator, ++, to increment the line_counter counter. It’s about the simplest action we can take.\n\nLet’s count some log entries now.\n\n TIP You can find more documentation on the mtail syntax on GitHub and on\n\nthe guide to programming in it.\n\nRunning mtail\n\nTo run mtail we need to specify some programs to run and some log files to parse. Let’s do that now.\n\nListing 9.7: Running mtail\n\n$ sudo mtail --progs /etc/mtail --logs '/var/log/*.log'\n\nVersion: v1.0.0 (427b8e9)\n\n261",
      "content_length": 760,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "Chapter 9: Logging as Instrumentation\n\nThis will run mtail with two flags. The first flag, --progs, tells mtail where to find our programs. The second flag, --logs, tells mtail where to find log files to parse. We’re using a glob pattern to match all log files in the /var/log directory. You can specify a comma-separated list of files or specify the --logs flag multiple times. mtail is also conscious of log file truncation so it can handle stopping, restarting, and actions like log rotation.\n\n NOTE The user you’re running mtail as will need permissions to the log\n\nfiles you’re parsing, otherwise mtail will not be able to read the files. You will get a read error in the mtail log output, obtained using the --logtostderr flag, when it can’t read a file.\n\nWhen we run mtail, it’ll launch a web server on port 3903 (you can control the IP address and port using the --address and --port flags). Let’s browse to that web server now.\n\nThe home path shows some diagnostic information, like so:\n\nVersion: v1.0.0 (427b8e9)\n\n262",
      "content_length": 1028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "Chapter 9: Logging as Instrumentation\n\nFigure 9.1: mtail diagnostics\n\nYou can see the build of mtail as well as links to the default metric output formats, diagnostic information, and a list of programs loaded, any errors, and log files being tracked. You can see mtail outputs metrics in JSON, varz (an internal Google format for metrics collection), and the format we want: Prometheus. Let’s click on the Prometheus link, which will take us to the /metrics path.\n\n TIP You can also send metrics to tools like StatsD and Graphite.\n\nListing 9.8: The mtail /metrics path\n\n# TYPE line_count counter # line_count defined at line_count.mtail:1:9-18 line_count{prog=\"line_count.mtail\"} 1561\n\nWe can see some familiar output: help text and a Prometheus metric with a single label, prog. This is added automatically to each metric and populated with the name of the program that generated the metric. You can omit this label by setting the --emit_prog_label flag to false.\n\nIn our case, our line_count metric has counted 1561 lines worth of log entries. We could then add a job to scrape this endpoint for our line_counter metric.\n\nThis isn’t an overly useful example though. Let’s look at some more complex programs.\n\nVersion: v1.0.0 (427b8e9)\n\n263",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "Chapter 9: Logging as Instrumentation\n\nProcessing web server access logs\n\nLet’s use mtail to extract some metrics from an Apache access log, specifically one using the combined log format. To save some time, we can use an example program provided with mtail. We create the program in the /etc/mtail directory and name it apache_combined.mtail. The contents are:\n\nListing 9.9: The apache_combined program\n\n# Parser for the common apache \"NCSA extended/combined\" log format # LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User- agent}i\\\" counter apache_http_requests_total by request_method, http_version, request_status counter apache_http_bytes_total by request_method, http_version,\n\nrequest_status\n\n/^/ + /(?P<hostname>[0-9A-Za-z\\.-]+) / + # %h /(?P<remote_logname>[0-9A-Za-z-]+) / + # %l /(?P<remote_username>[0-9A-Za-z-]+) / + # %u /(?P<timestamp>\\[\\d{2}\\/\\w{3}\\/\\d{4}:\\d{2}:\\d{2}:\\d{2} (\\+|-)\\d {4}\\]) / + # %u /\"(?P<request_method>[A-Z]+) (?P<URI>\\S+) (?P<http_version>HTTP \\/[0-9\\.]+)\" / + # \\\"%r\\\" /(?P<request_status>\\d{3}) / + # %>s /(?P<response_size>\\d+) / + # %b /\"(?P<referer>\\S+)\" / + # \\\"%{Referer}i\\\" /\"(?P<user_agent>[[:print:]]+)\"/ + # \\\"%{User-agent}i\\\" /$/ {\n\napache_http_requests_total[$request_method][$http_version][\n\n$request_status]++\n\napache_http_bytes_total[$request_method][$http_version][\n\n$request_status] += $response_size }\n\nVersion: v1.0.0 (427b8e9)\n\n264",
      "content_length": 1399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "Chapter 9: Logging as Instrumentation\n\n TIP You can find the code for this program on GitHub. There’s also a large\n\ncollection of example programs to help you get started on GitHub.\n\nWe can see our program is well commented; comments are specified using the # symbol. It includes an example of the log format at the top of the program and then defines two counters:\n\napache_http_requests_total by request_method, http_version,\n\nrequest_status\n\napache_http_bytes_total by request_method, http_version, request_status\n\nThe by operator specifies additional dimensions to add to the metric. In the first counter, apache_http_requests_total, we’ve added the additional dimensions of request_method, http_version, and request_status, which will be added as labels on the resulting counter.\n\nWe then see a series of regular expressions that match each element of the access log line and are chained together using + operators.\n\n TIP These regular expressions can get quite complex when parsing convo-\n\nluted log lines, so mtail also allows you to reuse regular expressions by defining them as constants.\n\nInside these regular expressions you can see a series of captures like so:\n\n(?P<request_status>\\d{3})\n\nThese are named capture groups. In this example, we’re capturing a named value\n\nVersion: v1.0.0 (427b8e9)\n\n265",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Chapter 9: Logging as Instrumentation\n\nof request_status. We can then use these captures in our actions.\n\nListing 9.10: The combined access log actions\n\n{\n\napache_http_requests_total[$request_method][$http_version][\n\n$request_status]++\n\napache_http_bytes_total[$request_method][$http_version][\n\n$request_status] += $response_size }\n\nThe action increments the first counter, apache_http_requests_total, adding some of the captures, prefixed with $, to the counter as dimensions. Each di- mension is wrapped in [ ] square brackets.\n\nThe second counter has an additive operation, using the += operator to add each new response size in bytes to the counter.\n\n TIP mtail can record either integer or floating point values for metrics. By\n\ndefault, all metrics are integers, unless the compiler can infer a floating point. Inference is achieved by analyzing expressions, for example identifying that a regular expression is capturing a floating point value.\n\nIf we were to run mtail again, this time loading some Apache (or other web server that used the combined log format), we’d see these new metrics populated.\n\nVersion: v1.0.0 (427b8e9)\n\n266",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "Chapter 9: Logging as Instrumentation\n\nListing 9.11: Running mtail\n\n$ sudo mtail --progs /etc/mtail --logs '/var/log/apache/*.access '\n\nAnd then browse to the /metrics path:\n\nListing 9.12: Apache combined metrics\n\n# TYPE apache_http_requests_total counter # apache_http_requests_total defined at apache_combined.mtail :6:9-34 apache_http_requests_total{http_version=\"HTTP/1.1\", request_method=\"GET\",request_status=\"200\",prog=\"apache_combined. mtail\"} 73 # apache_http_requests_total defined at apache_combined.mtail :6:9-34 apache_http_requests_total{http_version=\"HTTP/1.1\", request_method=\"GET\",request_status=\"304\",prog=\"apache_combined. mtail\"} 3 # TYPE apache_http_bytes_total counter # apache_http_bytes_total defined at apache_combined.mtail:7:9- 31 apache_http_bytes_total{http_version=\"HTTP/1.1\",request_method=\" GET\",request_status=\"200\",prog=\"apache_combined.mtail\"} 2814654 # apache_http_bytes_total defined at apache_combined.mtail:7:9- 31 apache_http_bytes_total{http_version=\"HTTP/1.1\",request_method=\" GET\",request_status=\"304\",prog=\"apache_combined.mtail\"} 0\n\nWe can see a new set of counters, with one counter for each method and HTTP response code dimension.\n\nWe can also do more complex operations, like building histograms.\n\nVersion: v1.0.0 (427b8e9)\n\n267",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "Chapter 9: Logging as Instrumentation\n\nParsing Rails logs into a histogram\n\nTo see a histogram being created, let’s look at some lines from the example Rails mtail program. Rails request logging is useful for measuring performance, but somewhat unfriendly to parse. Let’s see how mtail does it.\n\nVersion: v1.0.0 (427b8e9)\n\n268",
      "content_length": 326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "Chapter 9: Logging as Instrumentation\n\nListing 9.13: The mtail rails program\n\ncounter rails_requests_started_total counter rails_requests_started by verb counter rails_requests_completed_total counter rails_requests_completed by status counter rails_requests_completed_milliseconds_sum by status counter rails_requests_completed_milliseconds_count by status counter rails_requests_completed_milliseconds_bucket by le, status\n\n/^Started (?P<verb>[A-Z]+) .*/ { rails_requests_started_total++ rails_requests_started[$verb]++\n\n}\n\n/^Completed (?P<status>\\d{3}) .+ in (?P<request_milliseconds>\\d+) ms .*$/ {\n\nrails_requests_completed_total++ rails_requests_completed[$status]++\n\nrails_requests_completed_milliseconds_sum[$status] +=\n\n$request_milliseconds\n\nrails_requests_completed_milliseconds_count[$status]++\n\n# 10ms bucket $request_milliseconds <= 10 {\n\nrails_requests_completed_milliseconds_bucket[\"10\"][$status]++\n\n} # 50ms bucket $request_milliseconds <= 50 {\n\nrails_requests_completed_milliseconds_bucket[\"50\"][$status]++\n\n. . .\n\nOur program opens with defining counters for started and completed requests. We then see a condition and action that increments the request started counters, a\n\nVersion: v1.0.0 (427b8e9)\n\n269",
      "content_length": 1223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "Chapter 9: Logging as Instrumentation\n\ntotal and a set of counters with dimensions created from the status of the request.\n\nNext, our program calculates completed requests. Here we’re capturing the status code and the request time in milliseconds. We use these to create a sum of request time and a count of requests, both by status.\n\nWe then nest in another set of conditions and actions, this time to create our histogram. We have a series of conditions testing the length in milliseconds of the request:\n\n$request_milliseconds <= 10\n\nIf our request time is less than or equal to 10, then a histogram bucket counter, with the length test attached as a dimension and also the status, is incremented. We can create counters for each bucket we want.\n\nLet’s run our new program over some Rails logs and see what our resulting metrics look like.\n\nListing 9.14: Rails mtail metric output\n\nrails_requests_started_total{prog=\"rails.mtail\"} 44 rails_requests_started{verb=\"POST\",prog=\"rails.mtail\"} 19 rails_requests_started{verb=\"PUT\",prog=\"rails.mtail\"} 18 rails_requests_started{verb=\"GET\",prog=\"rails.mtail\"} 7 rails_requests_completed_total{prog=\"rails.mtail\"} 217 rails_requests_completed{status=\"200\",prog=\"rails.mtail\"} 217 rails_requests_completed_milliseconds_sum{status=\"200\",prog=\" rails.mtail\"} 3555 rails_requests_completed_milliseconds_count{status=\"200\",prog=\" rails.mtail\"} 217 rails_requests_completed_milliseconds_bucket{le=\"10\",status=\"200 \",prog=\"rails.mtail\"} 93 rails_requests_completed_milliseconds_bucket{le=\"50\",status=\"200 \",prog=\"rails.mtail\"} 217\n\n. . .\n\nVersion: v1.0.0 (427b8e9)\n\n270",
      "content_length": 1607,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "Chapter 9: Logging as Instrumentation\n\n TIP The le is a common abbreviation for “less than or equal to,” indicating\n\nthe content of the specific bucket.\n\nWe can see that we have counters for each request started by total and verb. We can also see our completed total and a total by status code. And we can see our buckets—in our case just the 10 ms and 50 ms buckets.\n\nDeploying mtail\n\nWe’ve now seen two mtail programs. We deploy them in a number of ways. We recommend running an mtail instance per application, adjacent to the application and deployed via configuration management as a dependency. This pattern is often called a sidecar and lends itself well to containerized applications. We’ll see it in Chapter 13, when we look at monitoring applications running on Kubernetes.\n\nWe can also run multiple programs in a single mtail instance, but this has the caveat that mtail will run every program over every log file passed to it, which could have a performance impact on your host.\n\nScraping our mtail endpoint\n\nNow that we’ve got some metrics being exposed, let’s create a Prometheus job to scrape them.\n\nVersion: v1.0.0 (427b8e9)\n\n271",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "Chapter 9: Logging as Instrumentation\n\nListing 9.15: The mtail job\n\n. . .\n\nscrape_configs: - job_name: 'mtail' file_sd_configs:\n\nfiles:\n\n'targets/mtail/*.json' refresh_interval: 5m\n\nOur job uses file-based service discovery to define a couple of targets, a web server and our rails server. Both targets are scraped on port 3903.\n\nListing 9.16: Worker file discovery\n\n[{\n\n\"targets\": [\n\n\"web:3903\", \"rails:3903\"\n\n]\n\n}]\n\nIf we restart Prometheus we’re now collecting the time series generated from our mtail programs on our Prometheus server, and can make use of these metrics.\n\nSummary\n\nIn this chapter we saw how to use log entries to provide metrics for applications we can’t, or can’t afford to, instrument. We did this using the mtail log processor.\n\nNote that we only scratched the surface of the capabilities of mtail’s language for log parsing and processing. You should read the wiki on GitHub and review the\n\nVersion: v1.0.0 (427b8e9)\n\n272",
      "content_length": 946,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "Chapter 9: Logging as Instrumentation\n\nexample programs to learn more about the language and how to write your own mtail programs.\n\nIn the next chapter we’ll learn how to do probe monitoring using Prometheus.\n\nVersion: v1.0.0 (427b8e9)\n\n273",
      "content_length": 240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "Chapter 10\n\nProbing\n\nIn Chapter 1 we discussed that there are two major approaches to monitoring ap- plications: probing and introspection. In this chapter we’re going to explore probe monitoring. Probe monitoring probes the outside of an application. You query the external characteristics of an application: does it respond to a poll on an open port and return the correct data or response code? An example of probe monitor- ing is performing an ICMP ping or echo check and confirming you have received a response. This type of probing is also called blackbox monitoring because we’re treating the application inside as a black box.\n\nWe’ll use probe monitoring to see the state of external aspects of our applications, which is especially useful from outside of our network. We’re going to use an exporter called the blackbox exporter to conduct this monitoring.\n\nProbing architecture\n\nProbing with Prometheus works by running an exporter, the blackbox exporter, that probes remote targets and exposes any time series collected on a local end- point. A Prometheus job then scrapes any metrics from the endpoint.\n\n274",
      "content_length": 1118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Chapter 10: Probing\n\nFigure 10.1: Probing architecture\n\nMonitoring probes have three constraints:\n\nThey need to be in a position to see the resources being probed. • The position of the probe needs to test the right path to the resources. For ex- ample, if you’re testing external access to an application, running the probe behind your firewall won’t validate this access.\n\nThe position of the probing exporter needs to be able to be scraped by your\n\nPrometheus server.\n\nIt’s common to position probes in geographically distributed locations outside of an organization’s network to ensure maximum coverage for the detection of faults and the collection of data about user experience with the application.\n\nVersion: v1.0.0 (427b8e9)\n\n275",
      "content_length": 737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Chapter 10: Probing\n\nBecause of the complexity of deploying probes externally, and if a wide distribu- tion of probes is needed, it’s common to outsource these probes to a third-party service. There are all sorts of commercial vendors that provide this service, some of which expose metrics on their platforms and others that allow metrics to be exported for use.\n\nFor our purposes, however, we’re going to deploy the blackbox exporter to an external host and use it to monitor the outside of our applications.\n\nThe blackbox exporter\n\nThe blackbox exporter is a single binary Go application licensed under the Apache 2.0 license. The exporter allows probing of endpoints over HTTP, HTTPS, DNS, TCP, and ICMP. Its architecture is a little different from other exporters. Inside the exporter we define a series of modules that perform specific checks—for example, checking a web server is running, or that a DNS record resolves. When the exporter runs, it exposes these modules and an API on a URL. Prometheus passes targets and specific modules to run on those targets as parameters to that URL. The exporter executes the check and returns the resulting metrics to Prometheus.\n\nLet’s see about installing it.\n\nInstalling the exporter\n\nThe Prometheus.io download page contains zip files with the binaries for specific platforms. Currently, the exporter is supported on:\n\nLinux: 32-bit, 64-bit, and ARM. • Max OS X: 32-bit and 64-bit. • FreeBSD: 32-bit, 64-bit, and ARM. • OpenBSD: 32-bit, 64-bit, and ARM.\n\nVersion: v1.0.0 (427b8e9)\n\n276",
      "content_length": 1535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "Chapter 10: Probing\n\nNetBSD: 32-bit, 64-bit, and ARM. • Microsoft Windows: 32-bit and 64-bit. • DragonFly: 64-bit.\n\nOlder versions of the exporter are available from the GitHub Releases page.\n\n NOTE At the time of writing, blackbox exporter was at version 0.12.0.\n\nInstalling the exporter on Linux\n\nTo install blackbox exporter on a 64-bit Linux host, we can download the zipped tarball. We can use wget or curl to get the file from the download site.\n\nListing 10.1: Download the blackbox exporter zip file\n\n$ cd /tmp $ wget https://github.com/prometheus/blackbox_exporter/releases/ download/v0.12.0/blackbox_exporter-0.12.0.linux-amd64.tar.gz\n\nNow let’s unpack the blackbox_exporter binary from the tarball and move it somewhere useful.\n\nListing 10.2: Unpack the blackbox_exporter binary\n\n$ tar -xzf blackbox_exporter-0.12.0.linux-amd64.tar.gz $ sudo cp blackbox_exporter-0.12.0.linux-amd64/blackbox_exporter\n\n/usr/local/bin/\n\nVersion: v1.0.0 (427b8e9)\n\n277",
      "content_length": 959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "Chapter 10: Probing\n\nWe can now test if the exporter is installed and in our path by checking its version.\n\nListing 10.3: Checking the blackbox exporter version on Linux\n\n$ blackbox_exporter --version blackbox_exporter, version 0.12.0 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\n TIP This same approach will work on Mac OS X with the Darwin version of\n\nthe blackbox exporter binary.\n\nInstalling the exporter on Microsoft Windows\n\nTo install blackbox exporter on Microsoft Windows, we need to download the blackbox_exporter.exe executable and put it in a directory. Let’s create a direc- tory for the executable using Powershell.\n\nListing 10.4: Creating a directory on Windows\n\nC:\\> MKDIR blackbox_exporter C:\\> CD blackbox_exporter\n\nNow download the blackbox_exporter.exe executable from GitHub into the C:\\ blackbox_exporter directory:\n\nVersion: v1.0.0 (427b8e9)\n\n278",
      "content_length": 976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "Chapter 10: Probing\n\nListing 10.5: Blackbox exporter Windows download\n\nhttps://github.com/prometheus/blackbox_exporter/releases/ download/v0.12.0/blackbox_exporter-0.12.0.windows-amd64.tar.gz\n\nUnzip the executable, using a tool like 7-Zip, into the C:\\blackbox_exporter di- rectory. Finally, add the C:\\blackbox_exporter directory to the path. This will allow Windows to find the executable. To do this, run this command inside Pow- ershell.\n\nListing 10.6: Setting the Windows path\n\n$env:Path += \";C:\\blackbox_exporter\"\n\nYou should now be able to run the blackbox_exporter.exe executable.\n\nListing 10.7: Checking the blackbox exporter version on Windows\n\nC:\\> blackbox_exporter.exe --version blackbox_exporter, version 0.12.0 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\nInstalling via configuration management\n\nSome of the configuration management modules we saw in Chapter 3 can also install the blackbox exporter:\n\nVersion: v1.0.0 (427b8e9)\n\n279",
      "content_length": 1054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Chapter 10: Probing\n\nA Puppet module for Prometheus. • A Chef cookbook for Prometheus. • A blackbox exporter Docker image. • A SaltStack formula.\n\n TIP Remember, configuration management is the recommended approach\n\nfor installing and managing Prometheus and its components!\n\nConfiguring the exporter\n\nThe exporter is configured via a YAML-based configuration file and driven with command line flags. The command line flags specify the location of the configu- ration file, the port to bind to, and logging. We can see the available command line flags by running the blackbox_exporter binary with the -h flag.\n\nLet’s create a configuration file to run the exporter now.\n\nListing 10.8: The prober.yml file\n\n$ sudo mkdir -p /etc/prober $ sudo touch /etc/prober/prober.yml\n\nAnd let’s populate it with some basic configuration. The exporter uses modules to define various checks. Each module has a name, and inside each is a specific prober—for example, an http prober to check HTTP services and web pages, and an icmp prober to check for ICMP connectivity. Prometheus jobs supply targets to each check inside the module, and the exporter returns metrics that are then scraped.\n\nVersion: v1.0.0 (427b8e9)\n\n280",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "Chapter 10: Probing\n\nListing 10.9: The /etc/prober/prober.yml file\n\nmodules:\n\nhttp_2xx_check: prober: http timeout: 5s http:\n\nvalid_status_codes: [] method: GET\n\nicmp_check:\n\nprober: icmp timeout: 5s icmp:\n\npreferred_ip_protocol: \"ip4\"\n\ndns_examplecom_check:\n\nprober: dns dns:\n\npreferred_ip_protocol: \"ip4\" query_name: \"www.example.com\"\n\nWe’ve defined three checks: an HTTP check that ensures that a web server returns a 2XX status code when queried, an ICMP check that pings the target, and a DNS check that makes a DNS query. Let’s look at each in turn.\n\n TIP The exporter example configuration is also useful to help explain how\n\nthe exporter works.\n\nHTTP check\n\nOur HTTP status check uses the http prober. This prober makes HTTP requests using a variety of methods like GET or POST. We specify a timeout of 5s, or five\n\nVersion: v1.0.0 (427b8e9)\n\n281",
      "content_length": 855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "Chapter 10: Probing\n\nseconds, for any requests. We then configure the prober to make a GET request. We leave the valid_status_codes blank; it defaults to any 2XX status code. If we wanted to validate that a different status code was returned, we’d specify the code or codes in an array in this field.\n\nListing 10.10: Valid status codes\n\nvalid_status_codes: [ '200', '304' ]\n\nHere our check will be for the status codes 200 and 304. We could also check valid HTTP versions, if an HTTP connection is SSL, or if the content matches or does not match a regular expression.\n\nICMP check\n\nOur second check pings a target using ICMP. We set the prober to icmp and specify a timeout of five seconds. We configure the icmp prober with the protocol to use, ip4.\n\n TIP The ICMP requires some additional permissions. On Windows it needs Administrator privileges, on Linux the root or CAP_NET_RAW capability, and on BSD or OS X the root user.\n\nDNS check\n\nOur last check uses the dns prober to check if a DNS entry resolves. In this case our target will be the DNS server we want to make the resolution. We specify our\n\nVersion: v1.0.0 (427b8e9)\n\n282",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "Chapter 10: Probing\n\npreferred protocol, again ip4, and we specify a query.\n\nquery_name: \"www.example.com\"\n\nThis will check that DNS for the www.example.com site will resolve. A query type of ANY is made of the target, and a successful DNS probe relies on the status code returned for the query. The default indicator for success is if the NOERROR response code is received. We can configure for other query types using the query_type option and other response codes using the valid_rcodes option.\n\nStarting the exporter\n\nNow that we have our three checks defined, let’s start the exporter. We’re going to run our exporter on an Ubuntu host called prober.example.com running on an AWS EC2 instance. We run the blackbox_exporter binary and pass it the configuration file we’ve just created.\n\nListing 10.11: Starting the exporter\n\n$ sudo blackbox_exporter --config.file=\"/etc/prober/prober.yml\"\n\nThe exporter runs on port 9115, and you can browse to its console page at http ://localhost:9115.\n\nVersion: v1.0.0 (427b8e9)\n\n283",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "Chapter 10: Probing\n\nFigure 10.2: The blackbox exporter console\n\nThe console includes the exporter’s own metrics, available on the http:// localhost:9115/metrics path, to allow us to monitor it too. It also contains a list of recent checks executed, their status, and debug logs showing what happened. These are useful to debug checks if they have failed.\n\nVersion: v1.0.0 (427b8e9)\n\n284",
      "content_length": 387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "Chapter 10: Probing\n\nCreating the Prometheus job\n\nNow we can create some Prometheus jobs to scrape the exporter. As we’ve dis- cussed, the blackbox exporter is a little different in how it operates: the exporter scrapes the targets passed to it using the checks defined on it. We’ll use a separate job for each check.\n\nLet’s create a job called http_probe that will query our http_2xx_check module.\n\nListing 10.12: The http_probes job\n\njob_name: 'http_probe' metrics_path: /probe params: module: [http_2xx_check]\n\nfile_sd_configs:\n\nfiles:\n\n'targets/probes/http_probes.json' refresh_interval: 5m\n\nrelabel_configs:\n\nsource_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__\n\nreplacement: prober.example.com:9115\n\nWe specify the metrics path, /probe, on the exporter. We pass in the module name as a parameter to the scrape. We’re using file-based discovery to list targets for this job.\n\nVersion: v1.0.0 (427b8e9)\n\n285",
      "content_length": 1001,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Chapter 10: Probing\n\nListing 10.13: The http_probe targets\n\n[{\n\n\"targets\": [\n\n\"http://www.example.com\", \"https://www.example.com\", \"\"\n\n]\n\n}]\n\nWe’re going to probe one website, www.example.com, using both HTTP and HTTPS.\n\nSo how does Prometheus know how to find the exporter? We use relabel_configs to overwrite the __address__ label of the target to specify the exporter’s hostname. We do three relabels:\n\n1. Our first relabel creates a parameter by writing the __address__ label, the\n\ncurrent target’s address, into the __param_target label.\n\n2. Our second relabel writes that __param_target label as the instance label.\n\n3. Last, we relabel the __address__ label using the host name (and port) of our\n\nexporter, in our case prober.example.com.\n\nThe relabeling results in a URL being constructed for the scrape:\n\nhttp://prober.example.com:9115/probe?target=www.example.com?module=\n\nhttp_2xx_check\n\nWe can browse to this URL to see the metrics that will be returned. Here are the metrics, minus the comments.\n\nVersion: v1.0.0 (427b8e9)\n\n286",
      "content_length": 1040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "Chapter 10: Probing\n\nListing 10.14: The http_2xx_check metrics\n\nprobe_dns_lookup_time_seconds 0.404881857 probe_duration_seconds 0.626351441 probe_failed_due_to_regex 0 probe_http_content_length -1 probe_http_duration_seconds{phase=\"connect\"} 0.013192816999999999 probe_http_duration_seconds{phase=\"processing\"} 0.013948647000000002 probe_http_duration_seconds{phase=\"resolve\"} 0.531245733 probe_http_duration_seconds{phase=\"tls\"} 0.073685882 probe_http_duration_seconds{phase=\"transfer\"} 0.000128069 probe_http_redirects 1 probe_http_ssl 1 probe_http_status_code 200 probe_http_version 1.1 probe_ip_protocol 4 probe_ssl_earliest_cert_expiry 1.527696449e+09 probe_success 1\n\nThe key metric here is probe_http_status_code which shows the status code re- turned by the HTTP request. If this is a 2xx status code, then the probe is consid- ered successful and the probe_success metric will be set to 1. The metrics here also supply useful information like the time of the probe and the HTTP version.\n\nThe other jobs operate in a similar manner to our HTTP check. They use the same relabelling rules to find the right target and the exporter’s address.\n\n NOTE You’ll find the source code for this chapter,\n\nincluding the\n\nPrometheus jobs, on GitHub.\n\nThe ICMP job takes host names or IP addresses, performs an ICMP ping, and re-\n\nVersion: v1.0.0 (427b8e9)\n\n287",
      "content_length": 1357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "Chapter 10: Probing\n\nturns the results. The targets for the DNS check are the DNS servers whose reso- lutions you wish to test.\n\nIf we now reload or restart Prometheus, we’ll see the metrics from these jobs in the console.\n\nFigure 10.3: The probe metrics in Prometheus\n\nSummary\n\nIn this chapter we used the blackbox exporter to probe some resources. We were introduced to the basics of probing architecture, and how to install, configure some basic probe checks with, and run the exporter. We also saw how to create Prometheus jobs to initiate probes and how to use relabelling rules to scrape the targets via the exporter.\n\nVersion: v1.0.0 (427b8e9)\n\n288",
      "content_length": 655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "Chapter 10: Probing\n\nIn the next chapter we’ll learn how to push metrics to Prometheus, especially for short-running processes like jobs and deployments, using the Pushgateway.\n\nVersion: v1.0.0 (427b8e9)\n\n289",
      "content_length": 208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "Chapter 11\n\nPushing Metrics and the Pushgateway\n\nUp until now, we’ve seen the Prometheus server running jobs to scrape metrics from targets: a pull-based architecture. In some cases, though, there isn’t a target from which to scrape metrics. There are a number of reasons why this might be so:\n\nYou can’t reach the target resources because of security or connectivity. This is quite a common scenario when a service or application only allows ingress to specific ports or paths.\n\nThe target resource has too short a lifespan—for example, a container start- ing, executing, and stopping. In this case, a Prometheus job will run and discover the target has completed execution and is no longer available to be scraped.\n\nThe target resource doesn’t have an endpoint, such as a batch job, that can be scraped. It’s unlikely that a batch job will have a running HTTP service that can be scraped, even assuming the job runs long enough to be available to be scraped.\n\n290",
      "content_length": 965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nIn these cases we need some way to deliver or push our time series to the In this chapter we’re going to learn how to handle these Prometheus server. scenarios using the Pushgateway.\n\nThe Pushgateway\n\nThe Pushgateway is a standalone service that receives Prometheus metrics on an HTTP REST API. The Pushgateway sits between an application sending metrics and the Prometheus server. The Pushgateway receives metrics and is then scraped as a target to deliver the metrics to the Prometheus server. You can think about it like a proxy service, or the opposite of the blackbox exporter’s behavior: it’s receiving metrics rather than probing for them.\n\nVersion: v1.0.0 (427b8e9)\n\n291",
      "content_length": 727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nFigure 11.1: The Pushgateway\n\nLike most of the Prometheus ecosystem, the Pushgateway is written in Go, open source, and licensed under Apache 2.0.\n\nWhen not to use the Pushgateway\n\nThe Pushgateway is essentially a workaround for monitoring resources that can’t be scraped by a Prometheus server for the reasons we discussed above. The gate- way isn’t a perfect solution and should only be used as a limited workaround, especially for monitoring otherwise inaccessible resources.\n\nYou also want to avoid making the gateway a single point of failure or a perfor-\n\nVersion: v1.0.0 (427b8e9)\n\n292",
      "content_length": 641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nmance bottleneck. The Pushgateway definitely does not scale in the same way a Prometheus server will scale.\n\n TIP Although you could use a variant of the worker scaling pattern we intro-\n\nduced in Chapter 7 or push metrics to multiple Pushgateways.\n\nThe gateway is also closer to a proxy than a fully featured push monitoring tool, hence, in using it, you lose a number of useful features that the Prometheus server provides. This includes instance state monitoring via up metrics and the expiration of metrics. It is also a static proxy by default, and remembers every metric sent to it, continuing to expose them as long as it is running (and the metrics aren’t persisted) or until they are deleted. This means that metrics for instances that no longer exist may be persisted in the gateway.\n\nlike You should focus the gateway on monitoring short-lifespan resources, jobs, or the short-term monitoring of inaccessible resources. You should install Prometheus servers to monitor inaccessible resources in the longer term.\n\n TIP A useful tool to monitor some of these inaccessible resources is the\n\nPushProx proxy which is designed to allow scrapes through NAT’ed connections.\n\nInstalling the Pushgateway\n\nThe Prometheus.io download page contains zip files with the binaries for specific platforms. Currently Pushgateway is supported on:\n\nLinux: 32-bit, 64-bit, and ARM.\n\nVersion: v1.0.0 (427b8e9)\n\n293",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nMax OS X: 32-bit and 64-bit. • FreeBSD: 32-bit, 64-bit, and ARM. • OpenBSD: 32-bit, 64-bit, and ARM. • NetBSD: 32-bit, 64-bit, and ARM. • Microsoft Windows: 32-bit and 64-bit. • DragonFly: 64-bit.\n\nOlder versions of Pushgateway are available from the GitHub Releases page.\n\n NOTE At the time of writing Pushgateway was at version 0.5.1.\n\nInstalling the Pushgateway on Linux\n\nTo install Pushgateway on a 64-bit Linux host, we can download the zipped tarball. We can use wget or curl to get the file from the download site.\n\nListing 11.1: Download the Pushgateway zip file\n\n$ cd /tmp $ wget https://github.com/prometheus/pushgateway/releases/download/v 0.5.1/pushgateway-0.5.1.linux-amd64.tar.gz\n\nNow let’s unpack the pushgateway binary from the tarball and move it somewhere useful.\n\nVersion: v1.0.0 (427b8e9)\n\n294",
      "content_length": 863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.2: Unpack the pushgateway binary\n\n$ tar -xzf pushgateway-0.5.1.linux-amd64.tar.gz $ sudo cp pushgateway-0.5.1.linux-amd64/pushgateway /usr/local/ bin/\n\nWe can now test if the Pushgateway is installed and in our path by checking its version.\n\nListing 11.3: Checking the Pushgateway version on Linux\n\n$ pushgateway --version pushgateway, version 0.5.1 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\n TIP This same approach will work on Mac OS X with the Darwin version of\n\nthe Pushgateway binary.\n\nInstalling the Pushgateway on Microsoft Windows\n\nTo install Pushgateway on Microsoft Windows, we need to download the pushgateway.exe executable and put it in a directory. Let’s create a directory for the executable using Powershell.\n\nVersion: v1.0.0 (427b8e9)\n\n295",
      "content_length": 926,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.4: Creating a directory on Windows\n\nC:\\> MKDIR pushgateway C:\\> CD pushgateway\n\nNow download the pushgateway.exe executable from GitHub into the C:\\ pushgateway directory:\n\nListing 11.5: Pushgateway Windows download\n\nhttps://github.com/prometheus/pushgateway/releases/download/v 0.5.1/pushgateway-0.5.1.windows-amd64.tar.gz\n\nUnzip the executable, using a tool like 7-Zip, into the C:\\pushgateway directory. Finally, add the C:\\pushgateway directory to the path. This will allow Windows to find the executable. To do this, run this command inside Powershell.\n\nListing 11.6: Setting the Windows path\n\n$env:Path += \";C:\\pushgateway\"\n\nYou should now be able to run the pushgateway.exe executable.\n\nVersion: v1.0.0 (427b8e9)\n\n296",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.7: Checking the Pushgateway version on Windows\n\nC:\\> pushgateway.exe --version pushgateway, version 0.5.1 (branch: HEAD, revision: 30 dd0426c08b6479d9a26259ea5efd63bc1ee273)\n\nbuild user: build date: go version:\n\nroot@3e103e3fc918 20171116-17:45:26 go1.9.2\n\nInstalling via configuration management\n\nSome of the configuration management modules we saw in Chapter 3 can also install the Pushgateway:\n\nA Puppet module for Prometheus. • A Chef cookbook for Prometheus.\n\n TIP Remember configuration management is the recommended approach for\n\ninstalling and managing Prometheus and its components!\n\nConfiguring and running the Pushgateway\n\nThe Pushgateway doesn’t need any configuration out of the box, but it can be configured by setting flags on the command line when you run the pushgateway binary. The gateway runs on port 9091, but you can override this port and any interface using the --web.listen-address flag.\n\nVersion: v1.0.0 (427b8e9)\n\n297",
      "content_length": 1005,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.8: Running the Pushgateway on an interface\n\n$ pushgateway --web.listen-address=\"0.0.0.0:9091\"\n\nThis will bind the Pushgateway on all interfaces. When the gateway is running, you can browse to its dashboard on that address and port.\n\nFigure 11.2: The Pushgateway dashboard\n\nBy default, the gateway stores all of its metrics in memory. This means if the gateway stops or is restarted you’ll lose any metrics in memory. You can persist the metrics to disk by specifying the --persistence.file flag with a path a file.\n\nVersion: v1.0.0 (427b8e9)\n\n298",
      "content_length": 606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.9: Persisting the metrics\n\n$ pushgateway --persistence.file=\"/tmp/pushgateway_persist\"\n\nThe persistence file is written to every five minutes by default, but you can over- ride this with the --persistence.interval flag.\n\nYou can see other available flags by running the binary with the --help flag.\n\nSending metrics to the Pushgateway\n\nOnce the Pushgateway is running you can start to send it metrics. Most Prometheus client libraries support sending metrics to the Pushgateway, in addition to exposing them for scraping. The easiest way to see how the gateway works is to use a command line tool like curl to post metrics. Let’s push a single metric to our running gateway.\n\nListing 11.10: Posting a metric to the gateway\n\n$ echo 'batchjob1_user_counter 2' | curl --data-binary @- http:// localhost:9091/metrics/job/batchjob1\n\nWe push metrics to the path /metrics. The URL is constructed using labels, here /metrics/job/<jobname> where batchjob1 is our job label. A full metrics path with labels looks like:\n\nListing 11.11: The Pushgateway metrics path\n\n/metrics/job/<jobname>{/<label>/<label>}\n\nVersion: v1.0.0 (427b8e9)\n\n299",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nThe <jobname> will be used as the value of the job label, followed by any other specified labels. Labels specified in the path will override any labels specified in the metric itself.\n\nLet’s add an instance label to our metric using the URL path.\n\nListing 11.12: Posting a metric to the gateway\n\n$ echo 'batchjob1_user_counter 2' | curl --data-binary @- http:// localhost:9091/metrics/job/batchjob1/instance/sidekiq_server\n\n WARNING You cannot use / as part of a label value or job name, even\n\nif it is escaped. This is because the decoding sequences makes it impossible to determine what was escaped, see the Go URL documentation.\n\nIn the above example, we’ve echoed the metric batchjob1_user_counter 2 from the job batchjob1 to our gateway. This will create a new metric grouping for the job batchjob1 with an instance label of sidekiq_server. Metric groupings are collections of metrics. You can add and delete metrics within the grouping, or even delete the whole group. Because the gateway is a cache and not an ag- gregator, metric groupings will live on until the gateway is stopped or they are deleted.\n\nThis counter is the most simple metric we can send. We name the counter, batchjob1_user_counter, and give it a value: 2.\n\nWe can add labels to pushed metrics by enclosing them in {}.\n\nVersion: v1.0.0 (427b8e9)\n\n300",
      "content_length": 1376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.13: Adding labels to pushed metrics\n\necho 'batchjob1_user_counter{job_id=\"123ABC\"} 2' | curl --data- binary @- http://localhost:9091/metrics/job/batchjob1/instance/ sidekiq_server\n\nCurrently, the metric will be uploaded untyped; the gateway won’t know whether this is a counter, gauge, or any other metric type. You can add a type (and a description) to a metric by passing TYPE and HELP statements in the push.\n\nListing 11.14: Passing types and descriptions\n\n$ cat <<EOF | curl --data-binary @- http://localhost:9091/ metrics/job/batchjob1/instance/sidekiq_server # TYPE batchjob1_user_counter counter # HELP batchjob1_user_counter A metric from BatchJob1. batchjob1_user_counter{job_id=\"123ABC\"} 2 EOF\n\nWe can also add further metrics to our metric group.\n\nListing 11.15: Passing types and descriptions\n\n$ cat <<EOF | curl --data-binary @- http://localhost:9091/ metrics/job/batchjob1/instance/sidekiq_server # TYPE batchjob1_avg_latency gauge # HELP batchjob1_avg_latency Another metric from BatchJob1. batchjob1_avg_latency{job_id=\"123ABC\"} 74.5 # TYPE batchjob1_sales_counter counter # HELP batchjob1_sales_counter A third metric from BatchJob1. batchjob1_sales_counter{job_id=\"123ABC\"} 1 EOF\n\nVersion: v1.0.0 (427b8e9)\n\n301",
      "content_length": 1289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nThis would add two metrics, batchjob1_avg_latency and batchjob1_sales_counter , to our batchjob1 metric group.\n\nViewing metrics on the Pushgateway\n\nWe can then see the metrics we’ve pushed to the gateway by using curl on the / metrics path (or by browsing to the Pushgateway dashboard at http://localhost :9091).\n\nVersion: v1.0.0 (427b8e9)\n\n302",
      "content_length": 393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.16: Curling the gateway metrics\n\n$ curl http://localhost:9091/metrics # HELP batchjob1_user_counter A metric from BatchJob1. # TYPE batchjob1_user_counter counter batchjob1_{instance=\"sidekiq_server\",job=\"batchjob1\",job_id=\"123 ABC\"} 2 # HELP batchjob1_avg_latency Another metric from BatchJob1. # TYPE batchjob1_avg_latency gauge batchjob1_avg_latency{instance=\"sidekiq_server\",job=\"batchjob1\", job_id=\"123ABC\"} 74.5 # HELP batchjob1_sales_counter A third metric from BatchJob1. # TYPE batchjob1_sales_counter counter batchjob1_sales_counter{instance=\"sidekiq_server\",job=\"batchjob1 \",job_id=\"123ABC\"} 1\n\n. . .\n\n# HELP push_time_seconds Last Unix time when this group was changed in the Pushgateway. # TYPE push_time_seconds gauge push_time_seconds{instance=\"sidekiq_server\",job=\"batchjob1\"} 1.523303909484092e+09 # HELP pushgateway_build_info A metric with a constant '1' value labeled by version, revision, branch, and goversion from which\n\npushgateway was built. # TYPE pushgateway_build_info gauge pushgateway_build_info{branch=\"master\",goversion=\"go1.10.1\", revision=\"d07ed465fcfcf2be4a2d80026d057fb5944c9283\",version=\" 0.4.0\"} 1\n\n NOTE We’ve skipped a number of health and performance metrics from\n\nthe gateway in our output.\n\nWe can see our batchjob1 metrics. We can see the job label has been set to\n\nVersion: v1.0.0 (427b8e9)\n\n303",
      "content_length": 1401,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nbatchjob1, and we have an instance label of sidekiq_server.\n\nFor batchjob1_user_counter, we can see that the value of the metric is 2 even though we sent three pushes to the gateway. This is because the gateway is NOT an aggregator, like StatsD or similar tools. Instead, the last push of the metric is exported until it is updated or deleted.\n\nYou’ll also see another metric here: push_time_seconds. This is a per-job metric that indicates the last time a push occurred. You can use this metric to determine when the last push occurred, and to potentially identify missing pushes. It’s only useful if you expect a push to occur within a specific time frame.\n\nDeleting metrics in the Pushgateway\n\nMetrics exist in the gateway until it is restarted (assuming no persistence is set), or until they are deleted. We can delete metrics using the Pushgateway API. Let’s do this now, again using curl, as an example.\n\nListing 11.17: Deleting Pushgateway metrics\n\n$ curl -X DELETE localhost:9091/metrics/job/batchjob1\n\nThis will delete all metrics for the job batchjob1. You can further limit the se- lection by making the path more granular—for example, by deleting only those metrics from a specific instance.\n\nListing 11.18: Deleting a selection of Pushgateway metrics\n\n$ curl -X DELETE localhost:9091/metrics/job/batchjob1/instance/ sidekiq_server\n\nVersion: v1.0.0 (427b8e9)\n\n304",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\n TIP You can also delete an entire metrics grouping from the Pushgateway by using the Delete Group button on the dashboard.\n\nSending metrics from a client\n\nObviously curl’ing metrics to the gateway isn’t practical. Instead we’re going to use a Prometheus client to push metrics to the gateway. All of the official Prometheus clients, and many of the unofficial ones, support the push gateway as a target for metrics.\n\nTo demonstrate how to do this, we’ll use the Rails application we demonstrated in Chapter 8. We’re going to create a MetricsPush class in our lib directory to use in our Rails application. Let’s do that now.\n\nListing 11.19: Creating MetricsPush class\n\n$ touch lib/metricspush.rb\n\nAnd let’s populate the file with a class.\n\nVersion: v1.0.0 (427b8e9)\n\n305",
      "content_length": 821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.20: The MetricsPush module\n\nrequire 'prometheus/client' require 'prometheus/client/push'\n\nclass MetricsPush\n\nattr_reader :job, :registry, :pushgateway_url\n\ndef initialize\n\n@job = 'mwp-rails' @pushgateway_url = 'http://localhost:9091'\n\nend\n\ndef registry\n\n@registry ||= Prometheus::Client.registry\n\nend\n\ndef counter(name, desc, labels = {})\n\nregistry.get(name) || registry.counter(name, desc)\n\nend\n\ndef gauge(name, desc, labels = {})\n\nregistry.get(name) || registry.counter(name, desc)\n\nend\n\ndef summary(name, desc, labels = {})\n\nregistry.get(name) || registry.counter(name, desc)\n\nend\n\ndef histogram(name, desc, labels = {}, buckets = Prometheus::\n\nClient::Histogram::DEFAULT_BUCKETS)\n\nregistry.get(name) || registry.counter(name, desc)\n\nend\n\ndef push\n\nPrometheus::Client::Push.new(job, nil, pushgateway_url).add(\n\nregistry)\n\nend\n\nend\n\nVersion: v1.0.0 (427b8e9)\n\n306",
      "content_length": 925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nOur class is very similar to the code we used in Chapter 8. We can create a wide variety of metrics. We’ve also added a method called push that sends the metrics in the registry to a Pushgateway. In our case, we’ve assumed the gateway is running locally on the host.\n\n\\DNnote{In addition to the add method on Prometheus::Client::Push, we also replace and delete methods we could use to replace or delete a have metric on the gateway.\n\nWe can then use our class when we run a job or some other transitory task.\n\nListing 11.21: Pushing a metric\n\nmp = MetricsPush.new mp.counter(:test_counter, \"A test counter for a job\").increment({\n\nservice: 'mwp-rails-job' })\n\nmp.push\n\nWe create an instance of the MetricsPush class and increment a counter called test_counter. We’ve added a label—one called service with a value of mwp- rails-job. We then use the push method to push the metric. If we were to run this snippet of code we could then check for the metric in the gateway, on the http://localhost:9091/metrics path or in the Pushgateway console.\n\nFigure 11.3: The test_counter in the Pushgateway dashboard\n\nVersion: v1.0.0 (427b8e9)\n\n307",
      "content_length": 1184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nHere we can see our pushed metric. Its instance label has been automatically populated with the IP address of our Rails server. We can override this during the push if required (especially as a lot of short-lived jobs are more likely associated with a service than a specific host). We see a job label and the service label we added.\n\nNow we’re ready to have the Prometheus server scrape the gateway to acquire our metrics.\n\n## Scraping the Pushgateway\n\nThe Pushgateway is only the interim stop for our metrics. We now need to get them into the Prometheus server. For that we’re going to need a job. Let’s create one now.\n\nListing 11.22: The pushgateway job\n\njob_name: pushgateway honor_labels: true file_sd_configs:\n\nfiles:\n\ntargets/pushgateway/*.json refresh_interval: 5m\n\nWe can see our job is pretty typical and follows the pattern we’ve seen throughout the book, using file-based discovery. The job will load all targets specified in JSON files in the targets/pushgateway directory. We’ve specified a Pushgateway named pg1.example.com in our file-based service discovery configuration.\n\nVersion: v1.0.0 (427b8e9)\n\n308",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nListing 11.23: Our Pushgateway\n\n[{\n\n\"targets\": [\"pg1.example.com\"]\n\n}]\n\nWe’ve then specified the honor_labels option and set it to true. As we’ve learned, when Prometheus scrapes a target, it will attach the name of the job that did the scraping, here pushgateway, and an instance label populated with the host or IP address of the target. With the Pushgateway, our metrics already have job and instance labels that indicate where our metrics were pushed from. We want to perpetuate this information in Prometheus rather than have it rewritten by the server when it scrapes the gateway.\n\nIf honor_labels is set to true, Prometheus will use the job and instance labels on the Pushgateway. Set to false, it’ll rename those values, prefixing them with \\texttt{exported_} and attaching new values for those labels on the server.\n\nIf we restart Prometheus it’ll start scraping the gateway. Let’s now look for our test_counter metric.\n\nFigure 11.4: The test_counter metric\n\nWe can see it’s been scraped, and the Prometheus server has honored the local labels.\n\nVersion: v1.0.0 (427b8e9)\n\n309",
      "content_length": 1134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "Chapter 11: Pushing Metrics and the Pushgateway\n\nSummary\n\nIn this chapter we saw how to use “push” mechanics with Prometheus via the Pushgateway. We articulated the limited circumstances in which it’s an appropri- ate use case. In those circumstances, we showed you how to install and configure the gateway and instrument your applications and jobs to push metrics to the gateway. And finally, we saw how to use a Prometheus job to scrape the gateway and acquire your pushed metrics.\n\nIn the next two chapters we’ll look at monitoring a whole application stack run- ning on top of Kubernetes, first looking at Kubernetes, and then a multi-service application.\n\nVersion: v1.0.0 (427b8e9)\n\n310",
      "content_length": 691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "Chapter 12\n\nMonitoring a Stack - Kubernetes\n\nNow that we’ve got a handle on the building blocks of Prometheus, let’s put the pieces together to monitor a modern application stack in the real world. To do this, we’re going to monitor an API service application called Tornado. Tornado is written in Clojure, and runs on the JVM; it has a Redis data store and a MySQL database. We’re going to deploy Tornado into a Kubernetes cluster we’ve built, so we’ll also look at monitoring Kubernetes with Prometheus.\n\nIn this chapter, we’re going to examine the Kubernetes portion of our stack and how to monitor it.\n\nTo make our monitoring simpler we’ve deployed Prometheus onto Kubernetes, too.\n\nOur Kubernetes cluster\n\nOur Kubernetes cluster is named tornado.quicknuke.com. The cluster is running Kubernetes 1.8.7 and is running in AWS. We built the cluster with kops, and you can find the cluster configuration here. It has three masters and six worker nodes. All nodes are divided between three Availability Zones.\n\n311",
      "content_length": 1013,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nThe cluster was created with the following kops command.\n\nListing 12.1: The cluster kops command\n\n$ kops create cluster \\ --node-count 6 \\ --zones us-east-2a,us-east-2b,us-east-2c \\ --master-zones us-east-2a,us-east-2b,us-east-2c \\ --node-size t2.micro \\ --master-size t2.micro \\ --topology private \\ --networking kopeio-vxlan \\ --api-loadbalancer-type=public \\ --bastion \\ tornado.quicknuke.com\n\n NOTE This chapter assumes you’ve already installed Kubernetes and have\n\nsome understanding of how it works. If you need more information on Kubernetes, the book Kubernetes: Up and Running is recommended reading.\n\nRunning Prometheus on Kubernetes\n\nThere are a variety of ways to deploy Prometheus on Kubernetes. The best way for you likely depends greatly on your environment. As possibilities, you can build your own deployments and expose Prometheus via a service, use one of a number of bundled configurations, or use the Prometheus Operator from CoreOS.\n\nWe’ve chosen to manually create a deployment and a service. We configure the Prometheus server and manage rules using ConfigMaps and mount these as vol- umes in our deployment. We also expose the Prometheus WebUI via an AWS Load\n\nVersion: v1.0.0 (427b8e9)\n\n312",
      "content_length": 1262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nBalancer service.\n\nWe’ve also installed a cluster of three Alertmanagers running on the cluster. We’ve enclosed all of this in a namespace called monitoring.\n\nYou can find the configuration for all of this with the book’s code on GitHub. How- ever, we’re not going to go into huge detail about how we deployed Prometheus onto Kubernetes; instead we’ll focus on monitoring Kubernetes and applications running on Kubernetes with Prometheus.\n\n NOTE This decision also reflects the speed at which Kubernetes is evolving:\n\nany deployment documented here is likely to be dated very quickly. This config- uration provided here is not guaranteed to work for later Kubernetes releases.\n\nMonitoring Kubernetes\n\nLet’s start by talking about monitoring Kubernetes itself. While it’s likely to change, it’s more manageable as a topic. Kubernetes is a container orchestrator and scheduler with a lot of moving pieces. We’re going to show you how to mon- itor aspects of Kubernetes with Prometheus jobs, and we’ll match each of these jobs with some recording and alert rules.\n\nThis chapter will be broken into sections dealing with each piece, how the time se- ries are collected, and any rules and alerts we’re going to generate from those time series. We’re not going to provide the definitive monitoring approach, but instead touch on some key highlights, especially where they expand on a Prometheus con- cept worth exploring.\n\nTo identify what we need to monitor we’ll also make use of Prometheus’s built-in service discovery mechanism for Kubernetes.\n\nVersion: v1.0.0 (427b8e9)\n\n313",
      "content_length": 1619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nLet’s start with monitoring the nodes upon which Kubernetes is running.\n\nMonitoring our Kubernetes nodes\n\nOur Kubernetes cluster is made up of nine AWS EC2 instances. To monitor them we’re going to use the Node Exporter. There are several ways we can deploy the Node Exporter onto those instances. We can install the Node Exporter onto the base instances when they are provisioned, much as we did in Chapter 4. Or we can install the Node Exporter into a Kubernetes pod on each node. We can take advantage of Kubernetes DaemonSet controller that automatically deploys a pod on every node in the cluster. This approach is useful when you don’t control the base instances—for example, if you’re using a hosted Kubernetes solution.\n\n WARNING There is a major caveat with this approach. The Node Ex- porter accesses a lot of root-level resources, and running it in a Docker container requires mounting those resources into the container and, for the systemd collec- tor, running the container as root. This poses a potential security risk. If that risk isn’t acceptable to you then you should install Node Exporter directly onto the instances.\n\nNode Exporter DaemonSet\n\nA DaemonSet ensures that a pod runs on all nodes, potentially including the mas- ters, using a toleration. It’s ideal for items like monitoring or logging agents. Let’s look at some elements of our DaemonSet.\n\nVersion: v1.0.0 (427b8e9)\n\n314",
      "content_length": 1451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\n NOTE You can find the full configuration for the Node Exporter on GitHub.\n\nListing 12.2: The Node Exporter DaemonSet tolerations\n\napiVersion: extensions/v1beta1 kind: DaemonSet metadata:\n\nname: node-exporter namespace: monitoring\n\n. . .\n\nspec:\n\ntolerations: - key: node-role.kubernetes.io/master\n\neffect: NoSchedule\n\nhostNetwork: true hostPID: true hostIPC: true securityContext: runAsUser: 0\n\nFirst, you can see we’ve specified a DaemonSet with a name, node-exporter, and that we’re using a toleration to ensure this pod is also scheduled on our Kubernetes masters, not just our normal nodes.\n\nNow here’s the caveat with this approach. We’re running the pod as user 0 or root (this allows access to systemd). We’ve also enabled hostNetwork, hostPID, and hostIPC to specify that the network, process, and IPC namespace of the instance will be available in the container. This is a potential security exposure, and you must definitely consider if you want to take this risk. If this risk isn’t acceptable, baking the Node Exporter into the image of your instances is potentially a better approach.\n\nVersion: v1.0.0 (427b8e9)\n\n315",
      "content_length": 1175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nLet’s look at the containers in the pod.\n\nListing 12.3: The Node Exporter DaemonSet containers\n\ncontainers: - image: prom/node-exporter:latest\n\nname: node-exporter volumeMounts:\n\nmountPath: /run/systemd/private name: systemd-socket readOnly: true\n\nargs:\n\n\"--collector.systemd\" - \"--collector.systemd.unit-whitelist=(docker|ssh|\n\nrsyslog|kubelet).service\"\n\nports:\n\ncontainerPort: 9100 hostPort: 9100 name: scrape\n\nHere we’re using the DockerHub image for Node Exporter, prom/node_exporter, and grabbing the latest release. We’re also mounting in a volume for the /run/ systemd/private directory on the instances themselves. This allows the Node Ex- porter to access the systemd state and gather the service state of systemd-managed services on the instance.\n\nWe’ve also specified some arguments for the node_exporter binary. We saw both in Chapter 4: enabling the systemd collector, and specifying a regular expression of the specific services to monitor, rather than all the services on the host.\n\nWe’ve also specified the port we want our metrics exposed on, 9100: the default port.\n\nTo help keep the Node Exporter pods healthy and to enhance their uptime, we’ve also added liveness and readiness probes to our Node Exporter container. Liveness probes detect the status of applications inside containers.\n\nVersion: v1.0.0 (427b8e9)\n\n316",
      "content_length": 1382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.4: Node Exporter liveness and readiness probes\n\nlivenessProbe:\n\nhttpGet:\n\npath: /metrics port: 9100\n\ninitialDelaySeconds: 30 timeoutSeconds: 10 periodSeconds: 1\n\nreadinessProbe:\n\nfailureThreshold: 5 httpGet:\n\npath: /metrics port: 9100\n\ninitialDelaySeconds: 10 timeoutSeconds: 10 periodSeconds: 2\n\nIn our case we use an HTTP GET probe to the /metrics path on port 9100 to confirm the Node Exporter is still working. The probe runs every periodSeconds , one second in our case. If the liveness check fails, Kubernetes will restart the container.\n\n NOTE We’ll see these probes in applications we monitor too. They can\n\nassist in managing the health of your applications by reducing possible false positives—such as a service triggering an alert by not being ready while it is starting—while monitoring. These checks can also restart containers that are faulty, potentially fixing issues before they trigger alerts.\n\nReadiness probes confirm the application is functional. Here, that means an HTTP GET can connect to the /metrics path on port 9100 before marking the container\n\nVersion: v1.0.0 (427b8e9)\n\n317",
      "content_length": 1161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nas available and delivering traffic to it. The remaining settings control the probe’s behavior: it’ll wait 10 seconds, the initialDelaySeconds setting, before checking the readiness; thereafter it will check every two seconds, the periodSeconds value, for readiness. If the probe times out after 10 seconds, the timeoutSeconds, more than five times, garnered from the failureThreshold setting, then the container will be marked as Unready.\n\n NOTE You can find the full configuration for the Node Exporter on GitHub.\n\nNode Exporter service\n\nWe also need a service to expose the Node Exporter so it can be scraped.\n\nVersion: v1.0.0 (427b8e9)\n\n318",
      "content_length": 690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.5: The Node Exporter service\n\napiVersion: v1 kind: Service metadata:\n\nannotations:\n\nprometheus.io/scrape: 'true'\n\nlabels:\n\napp: node-exporter name: node-exporter\n\nname: node-exporter namespace: monitoring\n\nspec:\n\nclusterIP: None ports: - name: scrape port: 9100 protocol: TCP\n\nselector:\n\napp: node-exporter\n\ntype: ClusterIP\n\nOur service is relatively straightforward. We add an annotation, prometheus.io /scrape: 'true', as metadata on the services. This will tell Prometheus that it should scrape this service. We’ll see how it’s used in the Prometheus job we’ll create to scrape our Node Exporters.\n\nWe also expose port 9100 as a ClusterIP. This means it is only available to the internal cluster network. As Prometheus is on the local Kubernetes cluster it’ll be able to internally scrape the Node Exporter, and there’s no need to expose it externally.\n\n NOTE You can find the complete Node Exporter service on GitHub.\n\nVersion: v1.0.0 (427b8e9)\n\n319",
      "content_length": 1010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nDeploying the Node Exporter\n\nLet’s create our DaemonSet and service on our Kubernetes cluster using the kubectl command. We’ll create both inside the monitoring namespace.\n\nListing 12.6: Deploying the Node Exporter daemonset and service\n\n$ kubectl create -f ./node-exporter.yml -n monitoring daemonset \"node-exporter\" created service \"node-exporter\" created\n\nIf you don’t want to keep specifying the -n monitoring namespace you can specify a default using.\n\nListing 12.7: The default namespace\n\n$ kubectl config set-context $(kubectl config current-context) -- namespace=monitoring\n\nWe can now check our pods are running.\n\nVersion: v1.0.0 (427b8e9)\n\n320",
      "content_length": 698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.8: Checking the Node Exporter pods\n\n$ kubectl get pods -n monitoring READY STATUS NAME 1/1 alertmanager-6854b5d59b-jvjcw 1/1 node-exporter-4fx57 1/1 node-exporter-4nzfk 1/1 node-exporter-5n7kl 1/1 node-exporter-f2mvb 1/1 node-exporter-km7sc 1/1 node-exporter-lvrsq 1/1 node-exporter-mvstg 1/1 node-exporter-tj4cs node-exporter-wh56c 1/1 prometheus-core-785bc8584b-7vfr4 1/1\n\nRunning Running Running Running Running Running Running Running Running Running Running\n\nRESTARTS AGE 0 0 0 0 0 0 0 0 0 0 0\n\n7d 5s 5s 5s 5s 5s 5s 5s 5s 5s 8d\n\nWe can see nine pods, one for each instance in the cluster: three masters and six nodes. We can also see our Prometheus server pod, prometheus-core, and our Alertmanager, alertmanager.\n\nWe can check the Node Exporter pods are running correctly by grabbing their logs.\n\nListing 12.9: A Node Exporter pod’s logs\n\n$ kubectl logs node-exporter-4fx57 -n monitoring time=\"2018-01-18T22:46:05Z\" level=info msg=\"Starting node_exporter (version=0.15.2, branch=HEAD, revision=98 bc64930d34878b84a0f87dfe6e1a6da61e532d)\" source=\"node_exporter. go:43\" time=\"2018-01-18T22:46:05Z\" level=info msg=\"Build context (go= go1.9.2, user=root@d5c4792c921f, date=20171205-14:50:53)\" source =\"node_exporter.go:44\"\n\n. . .\n\nWe can see our Node Exporter daemon is running. We can also confirm our service\n\nVersion: v1.0.0 (427b8e9)\n\n321",
      "content_length": 1400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nis in place.\n\nListing 12.10: Checking the Node Exporter service\n\n$ kubectl get services -n monitoring NAME node-exporter ClusterIP None\n\nTYPE\n\nCLUSTER-IP EXTERNAL-IP PORT(S)\n\nAGE\n\n<none>\n\n9100/TCP 8s\n\nHere we can see our node-exporter service with a ClusterIP type and with the 9100 port exposed to the internal Kubernetes cluster, ready to be scraped. We’re not scraping it yet, however, because we haven’t added a Prometheus job.\n\nThe Node Exporter job\n\nIn our Prometheus configuration we now want to add a job to scrape our Node Exporter endpoints. We’re going to kill many birds with one stone by defining a job that scrapes all the service endpoints that Kubernetes exposes. We’re going to control which endpoints Prometheus actually scrapes by only scraping those with a specific annotation, prometheus.io/scrape, set to 'true'. We’ll also use the built-in Kubernetes service discovery to find our endpoints and return them as potential targets to Prometheus.\n\n NOTE All of these jobs are derived or based on the amazing example\n\nKubernetes jobs shipped with Prometheus. Thanks to the contributors to that project for developing them.\n\nLet’s look at that job now.\n\nVersion: v1.0.0 (427b8e9)\n\n322",
      "content_length": 1247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.11: The Kubernetes service endpoints job\n\njob_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [\n\n__meta_kubernetes_service_annotation_prometheus_io_scrape]\n\naction: keep regex: true\n\nsource_labels: [\n\n__meta_kubernetes_service_annotation_prometheus_io_scheme]\n\naction: replace target_label: __scheme__ regex: (https?) - source_labels: [\n\n__meta_kubernetes_service_annotation_prometheus_io_path]\n\naction: replace target_label: __metrics_path__ regex: (.+)\n\nsource_labels: [__address__,\n\n__meta_kubernetes_service_annotation_prometheus_io_port]\n\naction: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2\n\naction: labelmap\n\nregex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace]\n\naction: replace target_label: kubernetes_namespace\n\nsource_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name\n\nWe’ve called the job kubernetes-service-endpoints. We’ve specified service dis- covery using the kubernetes_sd_discovery mechanism. This is in inbuilt service discovery mechanism, specifically for Kubernetes. It queries the Kubernetes API\n\nVersion: v1.0.0 (427b8e9)\n\n323",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nfor targets that match specific search criteria.\n\nAs our Prometheus server is running inside Kubernetes we’re able to automatically, with minimal configuration, fetch Kubernetes targets that match specific roles. There are roles for nodes, pods, services, and ingresses. Here, specified by the role parameter, we’re asking our service discovery to return all the Kubernetes endpoints. The endpoints role returns targets for all listed endpoints of a service, with one target per port for each endpoint address. If the endpoint is backed by a pod, as our Node Exporter service is, then any additional container ports are also discovered as targets. In our case, we’ve only exposed port 9100.\n\nService discovery also populates a variety of metadata. We use this metadata to relabel and identify each endpoint. Let’s see what our relabelling rules do and explore that metadata.\n\nOur first rule checks the prometheus.io/scrape: 'true' annotation that we set in our Node Exporter service. During the service discovery process the prometheus.io/scrape annotation will be translated to prometheus_io_scrape to create a valid label name. This is because the dot and slash are not le- gal characters in a Prometheus metric label. Since this is an annotation on the Prometheus service process also adds the prefix a Kubernetes service, __meta_kubernetes_service_annotation_ to the label.\n\nOur job only keeps any targets that have the metadata label: __meta_kubernetes_service_annotation_prometheus_io_scrape set to true. All other targets are dropped. This lets you only scrape those endpoints that you want.\n\nThe next three rules check for the presence of more annotations: prometheus. io/scheme, prometheus.io/path, and prometheus.io/port. If these labels are present it’ll use the contents of these annotations as the scheme, path, and port to be scraped. This lets us control, from the service endpoint, what precisely to scrape, allowing our job to be flexible.\n\nOur next rule maps any labels on the service into Prometheus labels of the same name by using the labelmap action. In our case, this consumes the\n\nVersion: v1.0.0 (427b8e9)\n\n324",
      "content_length": 2180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\n__meta_kubernetes_service_label_app metadata label, which will become a label simply called app. Our next rule copies the __meta_kubernetes_namespace label as kubernetes_namespace and the __meta_kubernetes_service_name metadata label to kubernetes_name.\n\nWe now add our job to the ConfigMap we’re using for our Prometheus server configuration. We then replace our existing configuration.\n\nListing 12.12: Replacing the ConfigMap\n\n$ kubectl replace -f ./prom-config-map-v1.yml -n monitoring\n\nWe generally have to delete our Prometheus pod and allow it to be recreated in order to load our new configuration. Shortly, we should see some new targets on the Prometheus expression browser.\n\nVersion: v1.0.0 (427b8e9)\n\n325",
      "content_length": 760,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nFigure 12.1: The Kubernetes endpoint targets\n\nYou can see that we’ve got thirteen targets listed. Nine of them are the Node Exporter endpoints on our instances. The tenth and eleventh are Prometheus and Alertmanager. The Prometheus and Alertmanager targets have been discovered automatically because their interfaces are exposed as a service too.\n\nListing 12.13: The monitoring services\n\n$ kubectl get services -n monitoring NAME S) alertmanager 9093:32288/TCP 27m node-exporter ClusterIP TCP 15h prometheus 9090:30604/TCP 4d\n\nTYPE\n\nCLUSTER-IP\n\nEXTERNAL-IP\n\nAGE\n\nLoadBalancer 100.68.82.44\n\na6f953a641191...\n\nNone\n\n<none>\n\nLoadBalancer 100.68.154.121 a953a66970c13...\n\nPORT(\n\n9100/\n\nThis job is really useful because we only need to define it once and all future Ku- bernetes service endpoints will be automatically discovered and monitored. We’ll see this in action in this and the next chapter.\n\nWe will also see node_ time series start to appear in the expression browser soon after the job is loaded.\n\nNode Explorer rules\n\nWe’re not going to add any new recording or alert rules for our Kubernetes nodes. Rather we’ve added the rules we created in Chapter 4 to the ConfigMap we’re using to populate Prometheus’s rule files. So we’re adding all the CPU, memory, and disk rules we created, and we’re also adding some availability alert rules for our Kubernetes services. Let’s look at those now.\n\nVersion: v1.0.0 (427b8e9)\n\n326",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.14: Kubernetes availability alerting rules\n\nalert: KubernetesServiceDown\n\nexpr: up{job=\"kubernetes-service-endpoints\"} == 0 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Pod {{ $labels.instance }} is down!\n\nalert: KubernetesServicesGone\n\nexpr: absent(up{job=\"kubernetes-service-endpoints\"}) for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: No Kubernetes services are reporting! description: Werner Heisenberg says - OMG Where are my\n\nservicez?\n\nThe first alert triggers when the value of the up metric for the kubernetes-service -endpoints job is 0. This indicates that Prometheus has failed to scrape a service. The second alert caters for a service disappearing and uses the absent function to check for the presence of the up metric.\n\nWe’ve also added alert rules for the services we’re monitoring on individual nodes using the node_systemd_unit_state metric, which tracks the status of systemd services.\n\nVersion: v1.0.0 (427b8e9)\n\n327",
      "content_length": 1022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.15: Kubernetes availability alerting rules\n\nalert: CriticalServiceDown\n\nexpr: node_systemd_unit_state{state=\"active\"} != 1 for: 2m labels:\n\nseverity: critical\n\nannotations:\n\nsummary = {{ $labels.instance }}: Service {{ $labels.name }}\n\nfailed to start.\n\ndescription = {{ $labels.instance }} failed to (re)start\n\nservice {{ $labels.name }}.\n\nThis will alert when it detects that any of the services our Node Exporter is monitoring—Docker, Kubelet, RSyslog, and SSH—are in a failed state.\n\nThere are other rules and alerts in the configuration that you can explore and adapt for node monitoring.\n\nNow let’s look at monitoring some Kubernetes components.\n\nKubernetes\n\nThere are a number of ways to monitor Kubernetes itself. These include tools in the open-source Kubernetes ecosystem like Heapster and Kube-state-metrics as well as commercial and SaaS-based tools. In this chapter, we’re going to focus on Kube-state-metrics to do our monitoring.\n\nKube-state-metrics\n\nWe’ll install Kube-state-metrics on our Kubernetes cluster using a deployment and service. The deployment uses the Kube-state-metrics Docker image and runs it on\n\nVersion: v1.0.0 (427b8e9)\n\n328",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\none of our nodes. The service exposes the metrics on port 8080. As it’s a service, it allows us to take advantage of our existing Prometheus service job we created in the last section. When we run it, Prometheus will automatically discover the new service endpoint and start scraping the Kube-state-metrics.\n\nOnce we’ve added the service we’ll see a new target in the kubernetes-service- endpoints job in the http://prometheus.quicknuke.com:9090/targets listing.\n\nFigure 12.2: The Kube-state-metrics endpoint target\n\nWith Kube-state-metrics we’re going to focus on the success and failure of the workloads we’re deploying to Kubernetes and the state of our nodes. Let’s look at some alerts for which we can use our Kube-state-metrics time series.\n\n TIP You can see a full list of the metrics that Kube-state-metrics produces in\n\nits documentation.\n\nVersion: v1.0.0 (427b8e9)\n\n329",
      "content_length": 925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.16: Kube-state-metrics deployment generation alert\n\nalert: DeploymentGenerationMismatch\n\nexpr: kube_deployment_status_observed_generation !=\n\nkube_deployment_metadata_generation\n\nfor: 5m labels:\n\nseverity: warning\n\nannotations:\n\ndescription: Observed deployment generation does not match\n\nexpected one for\n\ndeployment {{$labels.namespace}}/{{$labels.deployment}}\n\nsummary: Deployment is outdated\n\nIt compares the running Our first rule detects if a deployment has succeeded. generation of a deployment with the generation in the metadata. If the two are not equal for five minutes then an alert is raised indicating that a deployment has failed.\n\nOur second rule does similar but for deployment replicas.\n\nVersion: v1.0.0 (427b8e9)\n\n330",
      "content_length": 792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.17: Kube-state-metrics Deployment replicas not updated alert\n\nalert: DeploymentReplicasNotUpdated\n\nexpr: ((kube_deployment_status_replicas_updated !=\n\nkube_deployment_spec_replicas)\n\nor (kube_deployment_status_replicas_available !=\n\nkube_deployment_spec_replicas))\n\nunless (kube_deployment_spec_paused == 1)\n\nfor: 5m labels:\n\nseverity: warning\n\nannotations:\n\ndescription: Replicas are not updated and available for\n\ndeployment {{$labels.namespace}}/{{$labels.deployment}}\n\nsummary: Deployment replicas are outdated\n\nHere we perform a more complex expression that confirms that either the updated or available replicas should match the number of replicas in the deployment spec- ification, assuming the deployment isn’t paused.\n\nOur next rule checks for pod restarts.\n\nListing 12.18: Kube-state-metrics pod restarting alert\n\nalert: PodFrequentlyRestarting\n\nexpr: increase(kube_pod_container_status_restarts_total[1h]) >\n\n5\n\nfor: 10m labels:\n\nseverity: warning\n\nannotations:\n\ndescription: Pod {{ $labels.namespace }}/{{ $labels.pod }}\n\nwas restarted {{ $value }}\n\ntimes within the last hour\n\nsummary: Pod is restarting frequently\n\nVersion: v1.0.0 (427b8e9)\n\n331",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nHere we measure the number of pod restarts using the increase function. The increase function measures the rate of increase in a time series in range vector, here one hour. If the rate is over five for 10 minutes then the alert is raised.\n\nThere are a number of other time series we can use to monitor Kubernetes. For example, we could use the kube_node_status_condition to determine the avail- ability of the Kubernetes’ nodes. You’ll find some additional alerts in the alert rules we’re creating for this chapter.\n\nKube API\n\nWe also want to create a job to monitor our Kubernetes API itself. The metrics associated with the API will form the central core of our Kubernetes monitoring, allowing us to monitor latency, error rate, and availability for our cluster. We’re going to monitor the Kubernetes API specifically looking at latency, errors, and availability. Let’s create a job to monitor the API now.\n\nVersion: v1.0.0 (427b8e9)\n\n332",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.19: API server job\n\njob_name: 'kubernetes-apiservers' scheme: https tls_config:\n\nca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.\n\ncrt\n\ninsecure_skip_verify: true\n\nbearer_token_file: /var/run/secrets/kubernetes.io/\n\nserviceaccount/token\n\nkubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_namespace,\n\n__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n\naction: keep regex: default;kubernetes;https\n\nWe’ve called our job kubernetes-apiservers. We use https to scrape the metrics, and to specify the certification authority and a local token file to authenticate to Kubernetes. We again use Kubernetes discovery, this time to return a list of the Kubernetes endpoints. We won’t use all of the endpoints, and our relabelling configuration uses the keep action to only retain services named kubernetes in the default namespace—which will only be our master Kubernetes nodes running the API.\n\nNow that we’re collecting API server metrics, let’s create some recording rules to calculate the latency of the API servers.\n\nVersion: v1.0.0 (427b8e9)\n\n333",
      "content_length": 1172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.20: The API server recording rules\n\nrecord: apiserver_latency_seconds:quantile expr: histogram_quantile(0.99, rate(\n\napiserver_request_latencies_bucket[5m])) / 1e+06\n\nlabels:\n\nquantile: \"0.99\"\n\nrecord: apiserver_latency_seconds:quantile expr: histogram_quantile(0.9, rate(\n\napiserver_request_latencies_bucket[5m])) / 1e+06\n\nlabels:\n\nquantile: \"0.9\"\n\nrecord: apiserver_latency_seconds:quantile expr: histogram_quantile(0.5, rate(\n\napiserver_request_latencies_bucket[5m])) / 1e+06\n\nlabels:\n\nquantile: \"0.5\"\n\nWe make use of the apiserver_request_latencies_bucket metric to calculate our latency. This bucket metric, with dimensions for the specific API resource, sub- resource, and verb, measures request latency. We’ve created three rules for the 50th, 90th, and 99th percentiles, setting the quantile to the specific percentile. We’ve used the histogram_quantile function to create the percentiles from the metric buckets. We’ve specified the percentile we’re seeking, 0.99 for example, and then calculated a rate over a five minute vector and divided the result by 1e+06 or 1,000,000 to get microsecond latency.\n\nWe can then make use of the latency histograms our recording rules have created to create alerts. Let’s start with an alert to detect high latency from the API.\n\nVersion: v1.0.0 (427b8e9)\n\n334",
      "content_length": 1361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.21: API high latency alert\n\nalert: APIHighLatency\n\nexpr: apiserver_latency_seconds:quantile{quantile=\"0.99\",\n\nsubresource!=\"log\",verb!~\"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$\"} >\n\n4\n\nfor: 10m labels:\n\nseverity: critical\n\nannotations:\n\ndescription: the API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}\n\nOur alert uses the apiserver_latency_seconds:quantile metric we just created. We use labels to select the 99th percentile, any sub-resource that isn’t log, and any verb that isn’t WATCH, WATCHLIST, PROXY, or CONNECT. If the latency of any of the remaining metrics exceeds 4 for 10 minutes then the alert will be raised.\n\nOur next alert detects high levels of error rates in the API server.\n\nListing 12.22: API high error rate alert\n\nalert: APIServerErrorsHigh\n\nexpr: rate(apiserver_request_count{code=~\"^(?:5..)$\"}[5m]) /\n\nrate(apiserver_request_count[5m]) * 100 > 5\n\nfor: 10m labels:\n\nseverity: critical\n\nannotations:\n\ndescription: API server returns errors for {{ $value }}% of\n\nrequests\n\nThis alert calculates the error rate on API requests, using a regular expression to match any errors beginning with 5xx. If the percentage rate over a five minute\n\nVersion: v1.0.0 (427b8e9)\n\n335",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nvector exceeds 5 percent then the alert will be raised.\n\nOur last two alerts monitor the availability of the API server, monitoring the up metrics and the presence or absence of the up metric.\n\nListing 12.23: API servers down or absent\n\nalert: KubernetesAPIServerDown\n\nexpr: up{job=\"kubernetes-apiservers\"} == 0 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Apiserver {{ $labels.instance }} is down!\n\nalert: KubernetesAPIServersGone\n\nexpr: absent(up{job=\"kubernetes-apiservers\"}) for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: No Kubernetes apiservers are reporting! description: Werner Heisenberg says - OMG Where are my\n\napiserverz?\n\nLast, we can monitor the Kubernetes nodes and the Docker daemons and contain- ers running on them.\n\nCAdvisor and Nodes\n\nKubernetes also has CAdvisor and node-specific time series available by default. We can create a job to scrape these time series from the Kubernetes API for each node. We can use these time series, much as we did in Chapter 4, to monitor the nodes, and the Docker daemons and container-level on each node.\n\nLet’s add a job for CAdvisor.\n\nVersion: v1.0.0 (427b8e9)\n\n336",
      "content_length": 1198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nListing 12.24: The CAdvisor job\n\njob_name: 'kubernetes-cadvisor' scheme: https\n\ntls_config:\n\ninsecure_skip_verify: true ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.\n\ncrt\n\nbearer_token_file: /var/run/secrets/kubernetes.io/\n\nserviceaccount/token\n\nkubernetes_sd_configs: - role: node relabel_configs: - action: labelmap\n\nregex: __meta_kubernetes_node_label_(.+)\n\ntarget_label: __address__\n\nreplacement: kubernetes.default.svc:443\n\nsource_labels: [__meta_kubernetes_node_name]\n\nregex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor\n\nWe’ve called our job kubernetes-cadvisor and used service discovery to return a list of the Kubernetes nodes using the node role. We use https to scrape the met- rics, and to specify the certification authority and a local token file to authenticate to Kubernetes.\n\nWe’re then relabelling our time series to create labels from the metadata labels we’ve discovered using labelmap. We replace the __address__ label with the default DNS name of the Kubernetes API server. We then use one of the metadata labels, a label with the name of the node, to create a new __metrics_path__ on the API that passes in the node name to the path.\n\n/api/v1/nodes/${1}/proxy/metrics/cadvisor\n\nThis will scrape the required time series for each node discovered by the job. We also have a job for the nodes themselves in our configuration that exposes some\n\nVersion: v1.0.0 (427b8e9)\n\n337",
      "content_length": 1499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "Chapter 12: Monitoring a Stack - Kubernetes\n\nKubernetes node-level metrics.\n\nWe can use these metrics to monitor the performance of the underlying containers, Docker daemons and the Kubernetes-level performance of the nodes.\n\nSummary\n\nIn this chapter we started looking at monitoring a stack, starting with our compute platform: Kubernetes. We installed Prometheus onto Kubernetes to make our monitoring easier. We looked at monitoring Kubernetes nodes and the nodes upon which they are deployed using the Node Exporter.\n\nWe created several Prometheus jobs, including several that use Kubernetes service discovery to automatically discover the nodes, API servers, and services that make up our environment. The service discovery also allows us to configure jobs that automatically begin scraping specific Kubernetes or application services as they appear, using annotations to select the right addresses, ports, and paths.\n\nIn the next chapter we’re going to monitor a multi-service application running on top of our Kubernetes cluster. We’ll look at monitoring some specific services, like MySQL and Redis, as well as our application.\n\nVersion: v1.0.0 (427b8e9)\n\n338",
      "content_length": 1167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "Chapter 13\n\nMonitoring a Stack - Tornado\n\nIn the last chapter we saw the basics of monitoring Kubernetes, using Prometheus. In this chapter we’re going to deploy an application, called Tornado, onto our Kubernetes cluster and monitor it. Tornado is a simple REST-ful HTTP API written in Clojure which runs on the JVM, has a Redis data store, and a MySQL database.\n\nWe’ve deployed each component of the application onto our Kubernetes cluster and will look at how we can monitor each component, collecting information on the component and identifying some key alerts. We’ll monitor:\n\nMySQL, • Redis, and • the Tornado API application.\n\nWe’re going to start with monitoring our two data stores. We’re going to use a pattern called sidecar, which we referenced in Chapter 9. Let’s take a quick look at that pattern.\n\n339",
      "content_length": 817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nSidecar pattern\n\nTo perform much of our monitoring, we’ll rely heavily on an architecture pat- tern called sidecar. The pattern is named sidecar because it resembles a sidecar attached to a motorcycle: the motorcycle is our application, and the sidecar is at- tached to this parent application. The sidecar provides supporting features for the application—for example, an infrastructure sidecar might collect logs or conduct monitoring. The sidecar also shares the same life cycle as the parent application, being created and deleted alongside the parent.\n\n TIP Sidecars are sometimes called sidekicks.\n\nVersion: v1.0.0 (427b8e9)\n\n340",
      "content_length": 677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nFigure 13.1: The sidecar\n\nIn our case, the sidebars run Prometheus exporters. The exporters query our ap- plications and in turn are queried by Prometheus. This sidecar model works on more than just Kubernetes, too; anywhere you’re deploying containers or services in clusters lends itself to this pattern.\n\nWe’ll run sidecar-monitoring exporters next to our Redis and MySQL installations, starting with our MySQL database.\n\nVersion: v1.0.0 (427b8e9)\n\n341",
      "content_length": 497,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nMySQL\n\nMonitoring MySQL with Prometheus is done using an exporter: the MySQLd Ex- porter. The exporter works by connecting to a MySQL server using provided cre- dentials and querying the state of the server. The queried data is then exposed and can be scraped by the Prometheus server. This means that the exporter needs to have both network access to the MySQL server as well as credentials for authen- tication. In our case, we’re going to run the exporter inside a Docker container deployed to Kubernetes in our sidecar pattern.\n\nHere’s the segment of our MySQL Kubernetes deployment which runs the exporter in our sidecar container.\n\nListing 13.1: The exporter container\n\nimage: prom/mysqld-exporter:latest name: tornado-db-exp args: - --collect.info_schema.innodb_metrics - --collect.info_schema.userstats - --collect.perf_schema.eventsstatements - --collect.perf_schema.indexiowaits - --collect.perf_schema.tableiowaits env: - name: DATA_SOURCE_NAME\n\nvalue: \"tornado-db-exp:anotherstrongpassword@(tornado-db\n\n:3306)/\" ports: - containerPort: 9104\n\nname: tornado-db-exp\n\nYou can see we’re using the prom/mysqld-exporter image with the latest tag. We’ve called the container tornado-db-exp. We’ve specified our connection de- tails using the DATA_SOURCE_NAME environment variable. This connection uses the DSN format to configure connection and credential details for our MySQL server.\n\nVersion: v1.0.0 (427b8e9)\n\n342",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nThe exporter running inside the container will automatically pick up the connec- tion details from the environmental variable.\n\nYou should create a separate user with a limited set of permissions. To query the required data from the MySQL server, you’ll need to grant your user the PROCESS, REPLICATION CLIENT, and SELECT permissions.\n\nYou can connect to the MySQL container using kubectl’s exec command, like so:\n\nListing 13.2: Connecting to the MySQL container\n\n$ kubectl exec -ti <pod> -- /usr/bin/mysql -p\n\nWe can then run CREATE USER and GRANT statements to assign the required permis- sions.\n\nListing 13.3: Creating a MySQL user\n\nCREATE USER 'tornado-db-exp'@'localhost' IDENTIFIED BY ' anotherstrongpassword'; GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'tornado-db- exp'; GRANT SELECT ON performance_schema.* TO 'tornado-db-exp';\n\nHere we’ve created a user called tornado-db-exp with the required permissions including a SELECT grant to the performance_schema table containing query per- formance data.\n\n TIP If you have a my.cnf file then the exporter can also use credentials hard-\n\ncoded in there.\n\nVersion: v1.0.0 (427b8e9)\n\n343",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nWe could also configure the exporter using a variety of flags to control its behavior. We’ve enabled some additional collectors:\n\nListing 13.4: Additional MySQL exporter collector\n\n--collect.info_schema.innodb_metrics --collect.info_schema.userstats --collect.perf_schema.eventsstatements --collect.perf_schema.indexiowaits --collect.perf_schema.tableiowaits\n\nThese all collect data from MySQL’s performance schema database, allowing us to track the performance of specific queries and operations.\n\nIn our container deployment, we’ve also exposed port 9104, the default port of the MySQL Exporter, which in turn we’ve exposed in a service.\n\nListing 13.5: The tornado-db service\n\napiVersion: v1 kind: Service metadata:\n\nname: tornado-db annotations:\n\nprometheus.io/scrape: 'true' prometheus.io/port: '9104'\n\nspec:\n\nselector:\n\napp: tornado-db\n\ntype: ClusterIP ports: - port: 3306\n\nname: tornado-db\n\nport: 9104\n\nname: tornado-db-exp\n\nVersion: v1.0.0 (427b8e9)\n\n344",
      "content_length": 1003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nWe’ve used two annotations: prometheus.io/scrape, which tells Prometheus to scrape this service, and prometheus.io/port, which tells Prometheus which port to scrape. We specify this because we want Prometheus to hit the MySQL Ex- porter port at 9104 rather than the MySQL server directly. These annotations are automatically picked up by the kubernetes-service-endpoints job we created in Chapter 12, and parsed by the relabelling configuration in that job, which we can see below:\n\nListing 13.6: The Kubernetes endpoint job relabelling\n\nrelabel_configs:\n\nsource_labels: [\n\n__meta_kubernetes_service_annotation_prometheus_io_scrape]\n\naction: keep regex: true\n\n. . .\n\nsource_labels: [__address__,\n\n__meta_kubernetes_service_annotation_prometheus_io_port]\n\naction: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2\n\n. . .\n\nThe prometheus.io/scrape annotation ensures Prometheus will only keep metrics from service endpoints with the annotation set to true. The prometheus.io/port annotation will be placed into the __address__ label to be scraped by the job. The next service discovery will start collection of the MySQL metrics.\n\nMySQL Prometheus configuration\n\nAs our exporter is being exposed as a service endpoint, we don’t need to configure a specific job to scrape it. We will, however, create some rules for our MysQL time series and add them to our rules ConfigMap. We’re just going to create a\n\nVersion: v1.0.0 (427b8e9)\n\n345",
      "content_length": 1507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nsampling of possible rules, loosely aligned with Google’s Four Golden Signals, to give you an idea of how you might use your MySQL metrics. We’ll focus on:\n\nLatency • Traffic • Errors • Saturation\n\n WARNING Measuring MySQL performance is hard, especially when\n\ntracking signals like latency, and circumstances vary greatly depending on your application and server configuration. These rules give you starting point, not a definitive answer. There are a number of guides online that might prove useful.\n\nFirst, let’s look at some rules, starting with tracking the growth rate of slow queries using the mysql_global_status_slow_queries metric. This counter is incremented when a query exceeds the long_query_time variable, which defaults to 10 seconds.\n\nListing 13.7: MySQL slow query alert\n\nalert: MySQLHighSlowQueryRate\n\nexpr: rate(mysql_global_status_slow_queries[2m]) > 5 labels:\n\nseverity: warning\n\nannotations:\n\nsummary: MySQL Slow query rate is exceeded on {{ $labels.\n\ninstance }} for {{ $labels.kubernetes_name }}\n\nThis will generate an alert if the rate over two minutes exceeds five. We can also create recording rules to track the request rates on our server.\n\nVersion: v1.0.0 (427b8e9)\n\n346",
      "content_length": 1244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nListing 13.8: MySQL request rate records\n\nrecord: mysql:write_requests:rate2m\n\nexpr: sum(rate(mysql_global_status_commands_total{command=~\"\n\ninsert|update|delete\"}[2m])) without (command) - record: mysql:select_requests:rate2m\n\nexpr: sum(rate(mysql_global_status_commands_total{command=\"\n\nselect\"}[2m])) - record: mysql:total_requests:rate2m\n\nexpr: rate(mysql_global_status_commands_total[2m])\n\nrecord: mysql:top5_statements:rate5m\n\nexpr: topk(5, sum by (schema,digest_text) (rate(\n\nmysql_perf_schema_events_statements_total[5m])))\n\nWe use the mysql_global_status_commands_total metric and grab all the write requests for specific commands: insert, update, and delete. We then calculate a rate over two minutes for these requests. We do the same for read requests using the select command, and for total requests. We’re also using the topk aggregation operator to get the most frequently used statements by schema and rate over the last five minutes, which helps us understand what the server is doing.\n\nFigure 13.2: The topk operator over MySQL statements\n\nWe could graph or alert on these as needed. We can also track connection requests and errors.\n\nVersion: v1.0.0 (427b8e9)\n\n347",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nListing 13.9: Connections and aborted connections\n\nalert: MySQLAbortedConnectionsHigh\n\nexpr: rate(mysql_global_status_aborted_connects[2m]) > 5 labels:\n\nseverity: warning\n\nannotations:\n\nsummary: MySQL Aborted connection rate is exceeded on {{\n\n$labels.instance }} for {{ $labels.kubernetes_name }} - record: mysql:connection:rate2m\n\nexpr: rate(mysql_global_status_connections[2m])\n\nHere we’re alerting if the rate of aborted connections exceeds a threshold, and creating a recording rule to track the rate of connections overall.\n\nLast, we want to know when our MySQL service is unavailable. These alerts use the combination of the state and presence of the exporter-specific up metric: mysql_up. The mysql_up metric does a SELECT 1 on the MySQL server and is set to 1 if the query succeeds. The first alert checks if the value of the mysql_up metric is 0, indicating the query has failed. The second alert checks for the presence of this metric in the event the service disappears and the metric is expired.\n\nVersion: v1.0.0 (427b8e9)\n\n348",
      "content_length": 1082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nListing 13.10: MySQL alerts\n\nalert: TornadoDBServerDown\n\nexpr: mysql_up{kubernetes_name=\"tornado-db\"} == 0 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: MySQL Server {{ $labels.instance }} is down!\n\nalert: TornadoDBServerGone\n\nexpr: absent(mysql_up{kubernetes_name=\"tornado-db\"}) for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: No Tornado DB servers are reporting! description: Werner Heisenberg says - there is no\n\nuncertainty about the Tornado MySQL server being gone.\n\nThese are are some useful starter rules. You can find these rules in the code for this chapter.\n\nRedis\n\nLike MySQL, Prometheus has an exporter for Redis. The Redis Exporter will export most of the items from the INFO command with details of server, client, memory, and CPU usage. There are also metrics for total keys, expiring keys, and the average TTL for keys in each database. You can also export values of keys.\n\nAnd, again like MySQL, we can run the exporter as a sidecar of the Redis container. Here’s a snippet of our Redis Kubernetes deployment.\n\nVersion: v1.0.0 (427b8e9)\n\n349",
      "content_length": 1128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nListing 13.11: Redis service and sidecar\n\napiVersion: apps/v1beta2 kind: Deployment . . .\n\nname: redis-exporter\n\nimage: oliver006/redis_exporter:latest env: - name: REDIS_ADDR\n\nvalue: redis://tornado-redis:6379\n\nname: REDIS_PASSWORD value: tornadoapi ports: - containerPort: 9121\n\nWe’re running a container called redis-exporter from a Docker image, oliver006/redis_exporter. We’ve specified two environments variables: REDIS_ADDR, which specifies the address of the Redis server we want to scrape, and REDIS_PASSWORD, which specifies a password to connect to the server with. We also specify port 9121 to export our metrics on.\n\n TIP There are other environment variables and command line flags you can\n\nset, which you can read about in the documentation.\n\nWe then expose this port via a Kubernetes service.\n\nVersion: v1.0.0 (427b8e9)\n\n350",
      "content_length": 883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nListing 13.12: The Redis Kubernetes service\n\napiVersion: v1 kind: Service metadata:\n\nname: tornado-redis annotations:\n\nprometheus.io/scrape: 'true' prometheus.io/port: '9121'\n\nspec:\n\nselector:\n\napp: tornado-redis\n\nports: - port: 6379\n\nname: redis\n\nport: 9121\n\nname: redis-exporter\n\nclusterIP: None\n\nYou can see that we’ve exposed port 9121, and specified two annotations—one to tell our Prometheus service endpoint job to scrape this service, and one which port to scrape. The next time Prometheus does service discovery it will detect the updated service and start collecting our Redis metrics.\n\nRedis Prometheus configuration\n\nAs our exporter is being exposed as a service endpoint, we don’t need to configure a specific job to scrape it. We will, however, create some rules for our Redis time series and add them. We’re again going to show you a sampling of rules, for example:\n\nVersion: v1.0.0 (427b8e9)\n\n351",
      "content_length": 954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nListing 13.13: Redis alerts\n\nalert: TornadoRedisCacheMissesHigh\n\nexpr: redis_keyspace_hits_total / (redis_keyspace_hits_total +\n\nredis_keyspace_misses_total) > 0.8\n\nfor: 10m labels:\n\nseverity: warning\n\nannotations:\n\nsummary: Redis Server {{ $labels.instance }} Cache Misses\n\nare high. - alert: RedisRejectedConnectionsHigh\n\nexpr: avg(redis_rejected_connections_total) by (addr) < 10 for: 10m labels:\n\nseverity: warning\n\nannotations:\n\nsummary: \"Redis instance {{ $labels.addr }} may be hitting\n\nmaxclient limit.\"\n\ndescription: \"The Redis instance at {{ $labels.addr }} had {{\n\n$value }} rejected connections during the last 10m and may be\n\nhitting the maxclient limit.\"\n\nHere we’re measuring if cache misses exceed 0.8 and if the rejected connections average is high.\n\nLast, like our MySQL service, we want to know when our Redis service is unavail- able. These alerts use the combination of the state and presence of the exporter- specific up metric, redis_up. The redis_up metric is set to 1 if the scrape of the Redis server succeeds. The first alert checks if the value of the redis_up metric is 0, indicating the scrape has failed. The second alert checks for the presence of this metric in the event the service disappears and the metric is expired.\n\nVersion: v1.0.0 (427b8e9)\n\n352",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nListing 13.14: Redis availability alerts\n\nalert: TornadoRedisServerDown\n\nexpr: redis_up{kubernetes_name=\"tornado-redis\"} == 0 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: Redis Server {{ $labels.instance }} is down!\n\nalert: TornadoRedisServerGone\n\nexpr: absent(redis_up{kubernetes_name=\"tornado-redis\"}) for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: No Tornado Redis servers are reporting! description: Werner Heisenberg says - there is no\n\nuncertainty about the Tornado Redis server being gone.\n\nNow that we’ve added some monitoring to our MySQL and Redis services, we want to monitor our API service.\n\nTornado\n\nThe Tornado API is a Clojure application that uses Ring and runs on the JVM. It has a single API endpoint that buys and sells items. We’re going to instrument the application in much the same way we saw in Chapter 8 to create metrics that monitor each API action.\n\nAdding the Clojure wrapper\n\nTo instrument our application we’re using the iapetos Clojure wrapper. There are several Clojure wrappers and clients for Prometheus; we chose iapetos because\n\nVersion: v1.0.0 (427b8e9)\n\n353",
      "content_length": 1169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nits up to date and easy to use. To enable the iapetos wrapper we need to add it to the project’s dependencies in the project.clj file.\n\nListing 13.15: Adding the client to the project.clj\n\n(defproject tornado-api-prometheus \"0.1.0-SNAPSHOT\"\n\n:description \"Example Clojure REST service for \" :url \"http://artofmonitoring.com\" :dependencies [[org.clojure/clojure \"1.8.0\"]\n\n. . .\n\n[iapetos \"0.1.8\"] [io.prometheus/simpleclient_hotspot \"0.4.0\"\n\n]]\n\n:plugins [[lein-ring \"0.7.3\"]]\n\n. . .\n\nHere we’ve added the iapetos and the Prometheus simpleclient_hotspot (which we need for exporting some JVM metrics) dependencies.\n\nWe can then require the relevant components of the wrapper in our application’s source code.\n\nListing 13.16: Requiring the wrapper components\n\n(:require [compojure.handler :as handler] . . .\n\n[iapetos.core :as prometheus] [iapetos.collector.ring :as ring] [iapetos.collector.jvm :as jvm]\n\n. . .\n\nWe’ve included the base iapetos wrapper as prometheus and two context-specific components for exporting Ring and JVM metrics respectively.\n\nVersion: v1.0.0 (427b8e9)\n\n354",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nAdding a registry\n\nLike our Ruby application in Chapter 8, we need to define a registry to hold all of our metrics. Our application is pretty simple so we’re just adding one registry, but you can add more than one or a registry per subsystem, if, for instance, you want the same counter in different subsystems.\n\nListing 13.17: Defining the registry\n\n(defonce registry\n\n(-> (prometheus/collector-registry)\n\n(jvm/initialize) (ring/initialize) (prometheus/register\n\n(prometheus/counter :tornado/item-get) (prometheus/counter :tornado/item-bought) (prometheus/counter :tornado/item-sold) (prometheus/counter :tornado/update-item) (prometheus/gauge\n\n:tornado/up))))\n\nWe’ve created a registry called registry and we’ve initialized the Ring and JVM metrics, which will be automatically collected and exported. We’ve then de- fined five specific metrics, four of them counters and one gauge, all prefixed with tornado. We have one counter for each of the API’s actions and a gauge to act as an up metric for the application. We can also add labels to the metrics we’ve defined.\n\nListing 13.18: Adding labels\n\n(prometheus/counter :tornado/item-bought {:description \"Total items bought\"})\n\nHere we’ve added a description label to the item-bought counter.\n\nVersion: v1.0.0 (427b8e9)\n\n355",
      "content_length": 1319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nAdding metrics\n\nWe can now add function calls to each API method on our application to increment our counters. For example, here’s the function that increments the metric for buying an item:\n\nListing 13.19: Adding metric calls\n\n(defn buy-item [item]\n\n(let [id (uuid)]\n\n(sql/db-do-commands db-config\n\n(let [item (assoc item \"id\" id)]\n\n(sql/insert! db-config :items item) (prometheus/inc (registry :tornado/item-bought)))) (wcar* (car/ping)\n\n(car/set id (item \"title\")))\n\n(get-item id)))\n\nWe’re calling the inc function to increment our item-bought counter when an item is bought. We could also set gauges or other time series including histograms.\n\nWe’ve also added a gauge called tornado_up that will act as the up metric for our application. When the application starts it will automatically set the value of the gauge to 1.\n\nListing 13.20: The tornado_up gauge\n\n(prometheus/set (registry :tornado/up) 1)\n\nVersion: v1.0.0 (427b8e9)\n\n356",
      "content_length": 979,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nExporting the metrics\n\nLast, we want to enable the /metrics page itself, in our case by using the built-in Ring support.\n\nListing 13.21: Starting the export\n\n(def app\n\n(-> (handler/api app-routes)\n\n(middleware/wrap-json-body) (middleware/wrap-json-response) (ring/wrap-metrics registry {:path \"/metrics\"})))\n\nThis will make the metrics we’ve defined, some JVM-centric metrics and some HTTP-specific metrics emitted from Ring, available on the application at the / metrics path.\n\nIf we now browse to this path we’ll see our metrics emitted. Here’s a quick sample.\n\nListing 13.22: Tornado metrics\n\n# HELP http_request_latency_seconds the response latency for HTTP requests. # TYPE http_request_latency_seconds histogram http_request_latency_seconds_bucket{method=\"GET\", status=\"404\", statusClass=\"4XX\", path=\"index.php\", le=\"0.001\",} 2.0 . . . # HELP tornado_item_sold a counter metric. # TYPE tornado_item_sold counter tornado_item_sold 0.0 . . . # HELP jvm_threads_peak Peak thread count of a JVM # TYPE jvm_threads_peak gauge jvm_threads_peak 14.0\n\nVersion: v1.0.0 (427b8e9)\n\n357",
      "content_length": 1122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nTornado Prometheus configuration\n\nLike our other services, our Clojure exporter is being exposed as an endpoint, and we don’t need to configure a specific job to scrape it. We get a wide variety of metrics—metrics from the JVM, HTTP metrics from Ring, and metrics from the application itself. We can now create some alerts and rules to monitor our API.\n\nHere’s a latency recording rule we created using one of the Ring HTTP metrics.\n\nListing 13.23: Ring latency rule\n\nrecord: tornado:request_latency_seconds:avg\n\nexpr: http_request_latency_seconds_sum{status=\"200\"} /\n\nhttp_request_latency_seconds_count{status=\"200\"}\n\nWe’re created a new metric, tornado:request_latency_seconds:avg, The aver- age request latency in seconds for requests which result in a 200 HTTP code.\n\nWe can also take advantage of one of the Ring-related historgrams to alert on high latency.\n\nListing 13.24: Ring high latency alert\n\nalert: TornadoRequestLatencyHigh expr: histogram_quantile(0.9, rate(\n\nhttp_request_latency_seconds_bucket{ kubernetes_name=\"tornado- api\" [5m])) > 0.05\n\nfor: 10m labels:\n\nseverity: warning\n\nannotations:\n\nsummary: API Server {{ $labels.instance }} latency is over\n\n0.05.\n\nHere we’ve used the histogram_quantile function to generate the 90th percentile\n\nVersion: v1.0.0 (427b8e9)\n\n358",
      "content_length": 1329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nof our request latency over 5 minutes. Our alert will be triggered if that exceeds 0.05 for 10 minutes.\n\nWe can also take advantage of the up-style metric we created, tornado_up, to watch for the availability of our API service.\n\nListing 13.25: Monitoring the Tornado API availability\n\nalert: TornadoAPIServerDown\n\nexpr: tornado_up{kubernetes_name=\"tornado-api\"} != 1 for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: API Server {{ $labels.instance }} is down!\n\nalert: TornadoAPIServerGone\n\nexpr: absent(tornado_up{kubernetes_name=\"tornado-api\"}) for: 10m labels:\n\nseverity: critical\n\nannotations:\n\nsummary: No Tornado API servers are reporting! description: Werner Heisenberg says - there is no\n\nuncertainty about the Tornado API server being gone.\n\nHere we’ll detect if the tornado_up metric has a value other than 0 or if it disap- peared from our metrics.\n\nThis gives you a simple overview of how you might apply what you’ve learned in the book to monitoring an application stack.\n\nSummary\n\nIn this chapter we’ve seen how we’d monitor services and applications running on top of Kubernetes. We used the sidecar pattern to do this, parallel monitoring\n\nVersion: v1.0.0 (427b8e9)\n\n359",
      "content_length": 1239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "Chapter 13: Monitoring a Stack - Tornado\n\nrunning next to our services and application, inside the same deployment.\n\nWe also saw another application instrumented, this time a Clojure-based applica- tion using the iapetos wrapper.\n\nYou can easily build upon this basis to monitor more complex applications and services using this simple building-block pattern.\n\nVersion: v1.0.0 (427b8e9)\n\n360",
      "content_length": 391,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "List of Figures\n\n1 License . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 ISBN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n1.1 Service hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Monitoring design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 A sample plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4 A sample gauge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.5 A sample counter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6 A histogram example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.7 An aggregated collection of metrics . . . . . . . . . . . . . . . . . . . . 1.8 The flaw of averages - copyright Jeff Danzinger . . . . . . . . . . . . . 1.9 Response time average . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.10 Response time average Mk II . . . . . . . . . . . . . . . . . . . . . . . 1.11 Response time average and median . . . . . . . . . . . . . . . . . . . 1.12 Response time average and median Mk II . . . . . . . . . . . . . . . . 1.13 The empirical rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.14 Response time average, median, and percentiles . . . . . . . . . . . 1.15 Response time average, median, and percentiles Mk II . . . . . . . .\n\n2.1 Prometheus architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Prometheus expression browser 2.3 Redundant Prometheus architecture . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . . . . . 3.1 Prometheus expression browser 3.2 List of metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n361\n\n5 5\n\n9 11 20 21 22 23 25 27 28 29 30 31 32 34 35\n\n49 52 53\n\n76 77",
      "content_length": 1871,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "List of Figures\n\n3.3 Querying quantiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Querying total HTTP requests . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Calculating total HTTP requests by job . . . . . . . . . . . . . . . . . . 3.6 Our rate query . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n78 80 81 83\n\n4.1 cAdvisor web interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.2 cAdvisor Prometheus metrics . . . . . . . . . . . . . . . . . . . . . . . . 100 4.3 Scrape lifecycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 4.4 Sample label taxonomy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 4.5 Labels prior to relabelling . . . . . . . . . . . . . . . . . . . . . . . . . . 112 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 4.6 node_cpu metrics 4.7 Per-host average percentage CPU usage metrics . . . . . . . . . . . . . 116 4.8 Per-host percentage CPU plot . . . . . . . . . . . . . . . . . . . . . . . . 117 4.9 Number of CPUs in each host . . . . . . . . . . . . . . . . . . . . . . . . 118 4.10 The node_memory_MemTotal . . . . . . . . . . . . . . . . . . . . . . . 119 4.11 Per-host percentage memory usage . . . . . . . . . . . . . . . . . . . . 120 4.12 Disk metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 4.13 Per-host disk space metrics . . . . . . . . . . . . . . . . . . . . . . . . 122 4.14 The systemd time series data . . . . . . . . . . . . . . . . . . . . . . . 125 4.15 The active services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 4.16 The up metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 4.17 The cadvisor_version metric . . . . . . . . . . . . . . . . . . . . . . . . 130 4.18 The node_cpu recorded rule . . . . . . . . . . . . . . . . . . . . . . . . 137 4.19 The Grafana console login . . . . . . . . . . . . . . . . . . . . . . . . . 145 4.20 The Grafana console . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 4.21 Adding a Grafana data source for Prometheus . . . . . . . . . . . . . 147 4.22 Adding a Grafana data source for Prometheus . . . . . . . . . . . . . 148 4.23 The Node dashboard . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n\n5.1 Scrape lifecycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n\n6.1 Alertmanager architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 174\n\nVersion: v1.0.0 (427b8e9)\n\n362",
      "content_length": 2517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "List of Figures\n\n6.2 Alertmanager routing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 6.3 Alertmanager web interface . . . . . . . . . . . . . . . . . . . . . . . . . 184 6.4 List of Prometheus alerts . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 6.5 Fired alert in Alertmanager . . . . . . . . . . . . . . . . . . . . . . . . . 194 6.6 HighNodeCPU alert email . . . . . . . . . . . . . . . . . . . . . . . . . . 195 6.7 Scheduling silences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 6.8 A new silence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 6.9 Editing or expiring silences . . . . . . . . . . . . . . . . . . . . . . . . . 212\n\n7.1 Fault-tolerant architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 219 7.2 Alertmanager cluster status . . . . . . . . . . . . . . . . . . . . . . . . . 223 7.3 Prometheus clustered Alertmanagers . . . . . . . . . . . . . . . . . . . 226 7.4 Organizational sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 7.5 Functional sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 7.6 Horizontal sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 7.7 The Federate API\n\n8.1 Rails server targets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254 8.2 Rails metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n\n9.1 mtail diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262\n\n10.1 Probing architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 10.2 The blackbox exporter console . . . . . . . . . . . . . . . . . . . . . . 284 10.3 The probe metrics in Prometheus . . . . . . . . . . . . . . . . . . . . . 288\n\n11.1 The Pushgateway . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292 11.2 The Pushgateway dashboard . . . . . . . . . . . . . . . . . . . . . . . . 298 11.3 The test_counter in the Pushgateway dashboard . . . . . . . . . . . . 307 11.4 The test_counter metric . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\n\n12.1 The Kubernetes endpoint targets . . . . . . . . . . . . . . . . . . . . . 325 12.2 The Kube-state-metrics endpoint target . . . . . . . . . . . . . . . . . 329\n\nVersion: v1.0.0 (427b8e9)\n\n363",
      "content_length": 2373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "List of Figures\n\n13.1 The sidecar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341 13.2 The topk operator over MySQL statements . . . . . . . . . . . . . . . 347\n\nVersion: v1.0.0 (427b8e9)\n\n364",
      "content_length": 216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "Listings\n\n1 Sample code block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1 Sample Nagios notification . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Time series notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Example time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Download the Prometheus tarball 3.2 Unpack the prometheus binary . . . . . . . . . . . . . . . . . . . . . . . 3.3 Checking the Prometheus version on Linux . . . . . . . . . . . . . . . 3.4 Creating a directory on Windows . . . . . . . . . . . . . . . . . . . . . . 3.5 Prometheus Windows download . . . . . . . . . . . . . . . . . . . . . . 3.6 Setting the Windows path . . . . . . . . . . . . . . . . . . . . . . . . . . 3.7 Checking the Prometheus version on Windows . . . . . . . . . . . . . 3.8 Installing Prometheus via Chocolatey . . . . . . . . . . . . . . . . . . . 3.9 Installing Prometheus via Homebrew . . . . . . . . . . . . . . . . . . . 3.10 Checking the Prometheus version on Mac OS X . . . . . . . . . . . . 3.11 The default Prometheus configuration file . . . . . . . . . . . . . . . 3.12 Alertmanager configuration . . . . . . . . . . . . . . . . . . . . . . . . 3.13 The default Prometheus scrape configuration . . . . . . . . . . . . . 3.14 Moving the configuration file . . . . . . . . . . . . . . . . . . . . . . . 3.15 Starting the Prometheus server . . . . . . . . . . . . . . . . . . . . . . 3.16 Validating your configuration with promtool . . . . . . . . . . . . . . 3.17 Running Prometheus with Docker . . . . . . . . . . . . . . . . . . . . 3.18 Mounting a configuration file into the Docker container . . . . . . .\n\n365\n\n3 40 56 56 62 62 63 63 63 64 64 65 65 65 68 70 72 73 73 73 74 74",
      "content_length": 1804,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "Listings\n\n75 3.19 Some sample raw metrics . . . . . . . . . . . . . . . . . . . . . . . . . 75 3.20 A raw metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 3.21 Go garbage collection 50th percentile . . . . . . . . . . . . . . . . . . 79 3.22 The prometheus_build_info metric . . . . . . . . . . . . . . . . . . . . 89 4.1 Downloading the Node Exporter . . . . . . . . . . . . . . . . . . . . . . 90 4.2 Testing the Node Exporter binary . . . . . . . . . . . . . . . . . . . . . 90 4.3 Running the help for Node Exporter . . . . . . . . . . . . . . . . . . . . 90 4.4 Controlling the port and path . . . . . . . . . . . . . . . . . . . . . . . . 91 4.5 Disabling the arp collector . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4.6 Creating a textfile directory . . . . . . . . . . . . . . . . . . . . . . . . . 92 4.7 A metadata metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 4.8 Starting Node Exporter with the textfile collector and systemd . . . . 94 4.9 The current Prometheus scrape configuration . . . . . . . . . . . . . . 95 4.10 Adding the node job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 4.11 Filtering collectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.12 Testing collect params 98 4.13 Running the caAdvisor container . . . . . . . . . . . . . . . . . . . . . 4.14 The cAdvisor container . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.15 Adding the Docker job . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 4.16 Overriding the discovered labels . . . . . . . . . . . . . . . . . . . . . 103 4.17 Dropping metrics with relabelling . . . . . . . . . . . . . . . . . . . . 108 4.18 Specifying a new separator . . . . . . . . . . . . . . . . . . . . . . . . . 109 4.19 Replacing a label . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 4.20 Dropping a label . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 4.21 Memory saturation query . . . . . . . . . . . . . . . . . . . . . . . . . 121 4.22 The node_systemd_unit_state metrics . . . . . . . . . . . . . . . . . . . 124 4.23 The up metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 4.24 A one-to-one vector match . . . . . . . . . . . . . . . . . . . . . . . . . 129 4.25 The evaluation_interval parameter . . . . . . . . . . . . . . . . . . . . 132 4.26 Creating a recorded rules file . . . . . . . . . . . . . . . . . . . . . . . 133 4.27 Adding the rules file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n\nVersion: v1.0.0 (427b8e9)\n\n366",
      "content_length": 2661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "Listings\n\n4.28 A recording rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 4.29 A recording group interval . . . . . . . . . . . . . . . . . . . . . . . . . 135 4.30 A recording rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 4.31 A recording rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 4.32 Getting the PackageCloud public key on Ubuntu . . . . . . . . . . . 139 4.33 Adding the Grafana packages . . . . . . . . . . . . . . . . . . . . . . . 139 4.34 Updating Apt and installing the Grafana package . . . . . . . . . . . 139 4.35 Getting the Grafana public key on Red Hat . . . . . . . . . . . . . . . 140 4.36 The Grafana Yum configuration . . . . . . . . . . . . . . . . . . . . . . 140 4.37 Installing Grafana on Red Hat . . . . . . . . . . . . . . . . . . . . . . . 140 4.38 Creating a Grafana directory on Windows . . . . . . . . . . . . . . . 141 4.39 Grafana Windows download . . . . . . . . . . . . . . . . . . . . . . . . 141 4.40 Setting the Windows path for Grafana . . . . . . . . . . . . . . . . . . 141 4.41 Installing Grafana via Homebrew . . . . . . . . . . . . . . . . . . . . . 142 4.42 Starting the Grafana Server on Linux . . . . . . . . . . . . . . . . . . 143 4.43 Starting Grafana at boot on OSX . . . . . . . . . . . . . . . . . . . . . 143 4.44 Starting Grafana server on OS X . . . . . . . . . . . . . . . . . . . . . 144 5.1 Our static service discovery . . . . . . . . . . . . . . . . . . . . . . . . . 153 5.2 File-based discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 5.3 Creating the target directory structure . . . . . . . . . . . . . . . . . . 155 5.4 Creating JSON files to hold our targets . . . . . . . . . . . . . . . . . . 155 5.5 The nodes.json file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 5.6 The daemons.json file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 5.7 The daemons file in YAML . . . . . . . . . . . . . . . . . . . . . . . . . . 156 5.8 Adding labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 5.9 An EC2 discovery job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 5.10 An EC2 discovery job with a profile . . . . . . . . . . . . . . . . . . . 161 5.11 An EC2 discovery job with a port . . . . . . . . . . . . . . . . . . . . . 161 5.12 Relabelling an EC2 discovery job . . . . . . . . . . . . . . . . . . . . . 162 5.13 Relabelling the instance name in a EC2 discovery job . . . . . . . . 164 5.14 DNS service discovery job . . . . . . . . . . . . . . . . . . . . . . . . . 165\n\nVersion: v1.0.0 (427b8e9)\n\n367",
      "content_length": 2647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "Listings\n\n5.15 A SRV record . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 5.16 Example SRV records . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 5.17 The DNS targets from the SRV . . . . . . . . . . . . . . . . . . . . . . 167 5.18 DNS A record service discovery job . . . . . . . . . . . . . . . . . . . 167 5.19 DNS subdomain A record service discovery job . . . . . . . . . . . . 168 6.1 Stock Nagios alert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 6.2 Download the Alertmanager tarball . . . . . . . . . . . . . . . . . . . . 176 6.3 Unpack the alertmanager binary . . . . . . . . . . . . . . . . . . . . . . 176 6.4 Moving the amtool binary . . . . . . . . . . . . . . . . . . . . . . . . . . 176 6.5 Checking the Alertmanager version on Linux . . . . . . . . . . . . . . 177 6.6 Creating a directory on Windows . . . . . . . . . . . . . . . . . . . . . . 177 6.7 Alertmanager Windows download . . . . . . . . . . . . . . . . . . . . . 178 6.8 Setting the Windows path . . . . . . . . . . . . . . . . . . . . . . . . . . 178 6.9 Checking the Alertmanager version on Windows . . . . . . . . . . . . 178 6.10 Creating the alertmanager.yml file . . . . . . . . . . . . . . . . . . . . 179 6.11 A simple alertmanager.yml configuration file . . . . . . . . . . . . . 180 6.12 Creating the templates directory . . . . . . . . . . . . . . . . . . . . . 181 6.13 Starting Alertmanager . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 6.14 The alerting block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 6.15 The Alertmanager SRV record . . . . . . . . . . . . . . . . . . . . . . . 186 6.16 Discovering the Alertmanager . . . . . . . . . . . . . . . . . . . . . . . 186 6.17 The Alertmanager Prometheus job . . . . . . . . . . . . . . . . . . . . 187 6.18 Creating an alerting rules file . . . . . . . . . . . . . . . . . . . . . . . 188 6.19 Adding globbing rule_files block . . . . . . . . . . . . . . . . . . . . . 188 6.20 Our first alerting rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 6.21 The ALERT time series . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 6.22 Adding more alerting rules . . . . . . . . . . . . . . . . . . . . . . . . . 196 6.23 Humanizing a value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 6.24 Creating the prometheus_alerts.yml file . . . . . . . . . . . . . . . . . 198 6.25 The prometheus_alerts.yml file . . . . . . . . . . . . . . . . . . . . . . 198 6.26 Node service alert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n\nVersion: v1.0.0 (427b8e9)\n\n368",
      "content_length": 2641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "Listings\n\n6.27 The up metric missing alert . . . . . . . . . . . . . . . . . . . . . . . . 201 6.28 Adding routing configuration . . . . . . . . . . . . . . . . . . . . . . . 202 6.29 Grouping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 6.30 Label matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204 6.31 Routing branching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204 6.32 Routing branching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 6.33 Multiple endpoints in a receiver . . . . . . . . . . . . . . . . . . . . . 205 6.34 A regular expression match . . . . . . . . . . . . . . . . . . . . . . . . 206 6.35 Adding a Slack receiver . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 6.36 Adding a Slack receiver . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 6.37 Creating a template file . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 6.38 The slack.tmpl file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 6.39 Adding a Slack receiver . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 6.40 Using amtool to schedule a silence . . . . . . . . . . . . . . . . . . . . 213 6.41 Querying the silences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214 6.42 Expiring the silence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214 6.43 Sample amtool configuration file . . . . . . . . . . . . . . . . . . . . . 214 6.44 Using amtool to schedule a silence . . . . . . . . . . . . . . . . . . . . 215 6.45 Omitting alertname . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 7.1 Starting Alertmanager cluster . . . . . . . . . . . . . . . . . . . . . . . . 221 7.2 Starting Alertmanager cluster remaining nodes . . . . . . . . . . . . . 222 7.3 Defining alertmanagers statically . . . . . . . . . . . . . . . . . . . . . . 224 7.4 The Alertmanager SRV record . . . . . . . . . . . . . . . . . . . . . . . 225 7.5 Discovering the Alertmanager . . . . . . . . . . . . . . . . . . . . . . . . 225 7.6 The worker0 configuration . . . . . . . . . . . . . . . . . . . . . . . . . 232 7.7 The instance CPU rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233 7.8 The primary configuration . . . . . . . . . . . . . . . . . . . . . . . . . . 234 7.9 Worker file discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 7.10 Matching parameter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 7.11 The match[ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 8.1 A sample payments method . . . . . . . . . . . . . . . . . . . . . . . . . 243\n\nVersion: v1.0.0 (427b8e9)\n\n369",
      "content_length": 2699,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "Listings\n\n8.2 The mwp-rails Gemfile . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 8.3 Install prometheus-client with the bundle command . . . . . . . . . . 246 8.4 Testing the Prometheus client with the Rails console . . . . . . . . . . 246 8.5 Creating a Prometheus registry . . . . . . . . . . . . . . . . . . . . . . . 247 8.6 Registering a Prometheus metric . . . . . . . . . . . . . . . . . . . . . . 247 8.7 Incrementing a metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 8.8 Incrementing a metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248 . . . . . . . . . . . . . . . . 248 8.9 The basic Prometheus client_ruby metrics 8.10 Creating a Metrics module . . . . . . . . . . . . . . . . . . . . . . . . . 248 8.11 The Metrics module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 8.12 Creating an initializer for the metrics library . . . . . . . . . . . . . . 250 8.13 The config/initializers/lib.rb file . . . . . . . . . . . . . . . . . . . . . 250 8.14 Counter for user deletions . . . . . . . . . . . . . . . . . . . . . . . . . 250 8.15 Counter for user creation . . . . . . . . . . . . . . . . . . . . . . . . . . 251 8.16 Adding Prometheus to the config.ru file . . . . . . . . . . . . . . . . . 252 8.17 The Rails /metrics endpoint . . . . . . . . . . . . . . . . . . . . . . . . 253 8.18 Our Rails servers service discovery . . . . . . . . . . . . . . . . . . . . 254 8.19 The rails job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254 9.1 Download and install the mtail binary . . . . . . . . . . . . . . . . . . 258 9.2 Running the mtail binary . . . . . . . . . . . . . . . . . . . . . . . . . . 258 9.3 Creating an mtail program directory . . . . . . . . . . . . . . . . . . . . 259 9.4 Creating the line_count.mtail program . . . . . . . . . . . . . . . . . . 259 9.5 The line_count.mtail program . . . . . . . . . . . . . . . . . . . . . . . . 260 9.6 A relational clause . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 9.7 Running mtail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 9.8 The mtail /metrics path . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 9.9 The apache_combined program . . . . . . . . . . . . . . . . . . . . . . . 264 9.10 The combined access log actions . . . . . . . . . . . . . . . . . . . . . 266 9.11 Running mtail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 9.12 Apache combined metrics . . . . . . . . . . . . . . . . . . . . . . . . . 267 9.13 The mtail rails program . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\n\nVersion: v1.0.0 (427b8e9)\n\n370",
      "content_length": 2670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "Listings\n\n9.14 Rails mtail metric output . . . . . . . . . . . . . . . . . . . . . . . . . . 270 9.15 The mtail job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 9.16 Worker file discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 10.1 Download the blackbox exporter zip file . . . . . . . . . . . . . . . . 277 10.2 Unpack the blackbox_exporter binary . . . . . . . . . . . . . . . . . . 277 10.3 Checking the blackbox exporter version on Linux . . . . . . . . . . . 278 10.4 Creating a directory on Windows . . . . . . . . . . . . . . . . . . . . . 278 10.5 Blackbox exporter Windows download . . . . . . . . . . . . . . . . . 279 10.6 Setting the Windows path . . . . . . . . . . . . . . . . . . . . . . . . . 279 10.7 Checking the blackbox exporter version on Windows . . . . . . . . . 279 10.8 The prober.yml file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280 10.9 The /etc/prober/prober.yml file . . . . . . . . . . . . . . . . . . . . . 281 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282 10.10 Valid status codes 10.11 Starting the exporter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283 10.12 The http_probes job . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 10.13 The http_probe targets . . . . . . . . . . . . . . . . . . . . . . . . . . . 286 10.14 The http_2xx_check metrics . . . . . . . . . . . . . . . . . . . . . . . . 287 11.1 Download the Pushgateway zip file . . . . . . . . . . . . . . . . . . . 294 11.2 Unpack the pushgateway binary . . . . . . . . . . . . . . . . . . . . . 295 11.3 Checking the Pushgateway version on Linux . . . . . . . . . . . . . . 295 11.4 Creating a directory on Windows . . . . . . . . . . . . . . . . . . . . . 296 11.5 Pushgateway Windows download . . . . . . . . . . . . . . . . . . . . 296 11.6 Setting the Windows path . . . . . . . . . . . . . . . . . . . . . . . . . 296 11.7 Checking the Pushgateway version on Windows . . . . . . . . . . . . 297 11.8 Running the Pushgateway on an interface . . . . . . . . . . . . . . . 298 11.9 Persisting the metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 11.10 Posting a metric to the gateway . . . . . . . . . . . . . . . . . . . . . 299 11.11 The Pushgateway metrics path . . . . . . . . . . . . . . . . . . . . . 299 11.12 Posting a metric to the gateway . . . . . . . . . . . . . . . . . . . . . 300 11.13 Adding labels to pushed metrics . . . . . . . . . . . . . . . . . . . . . 301 11.14 Passing types and descriptions . . . . . . . . . . . . . . . . . . . . . . 301\n\nVersion: v1.0.0 (427b8e9)\n\n371",
      "content_length": 2618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "Listings\n\n11.15 Passing types and descriptions . . . . . . . . . . . . . . . . . . . . . . 301 11.16 Curling the gateway metrics . . . . . . . . . . . . . . . . . . . . . . . 303 11.17 Deleting Pushgateway metrics . . . . . . . . . . . . . . . . . . . . . . 304 11.18 Deleting a selection of Pushgateway metrics . . . . . . . . . . . . . 304 11.19 Creating MetricsPush class . . . . . . . . . . . . . . . . . . . . . . . . 305 11.20 The MetricsPush module . . . . . . . . . . . . . . . . . . . . . . . . . 306 11.21 Pushing a metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307 11.22 The pushgateway job . . . . . . . . . . . . . . . . . . . . . . . . . . . 308 11.23 Our Pushgateway . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309 12.1 The cluster kops command . . . . . . . . . . . . . . . . . . . . . . . . . 312 12.2 The Node Exporter DaemonSet tolerations . . . . . . . . . . . . . . . 315 12.3 The Node Exporter DaemonSet containers . . . . . . . . . . . . . . . 316 12.4 Node Exporter liveness and readiness probes . . . . . . . . . . . . . . 317 12.5 The Node Exporter service . . . . . . . . . . . . . . . . . . . . . . . . . 319 12.6 Deploying the Node Exporter daemonset and service . . . . . . . . . 320 12.7 The default namespace . . . . . . . . . . . . . . . . . . . . . . . . . . . 320 12.8 Checking the Node Exporter pods . . . . . . . . . . . . . . . . . . . . 321 12.9 A Node Exporter pod’s logs . . . . . . . . . . . . . . . . . . . . . . . . 321 12.10 Checking the Node Exporter service . . . . . . . . . . . . . . . . . . 322 12.11 The Kubernetes service endpoints job . . . . . . . . . . . . . . . . . 323 12.12 Replacing the ConfigMap . . . . . . . . . . . . . . . . . . . . . . . . . 325 12.13 The monitoring services . . . . . . . . . . . . . . . . . . . . . . . . . . 326 12.14 Kubernetes availability alerting rules . . . . . . . . . . . . . . . . . . 327 12.15 Kubernetes availability alerting rules . . . . . . . . . . . . . . . . . . 328 12.16 Kube-state-metrics deployment generation alert . . . . . . . . . . . 330 12.17 Kube-state-metrics Deployment replicas not updated alert . . . . . 331 12.18 Kube-state-metrics pod restarting alert . . . . . . . . . . . . . . . . . 331 12.19 API server job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333 12.20 The API server recording rules . . . . . . . . . . . . . . . . . . . . . . 334 12.21 API high latency alert . . . . . . . . . . . . . . . . . . . . . . . . . . . 335 . . . . . . . . . . . . . . . . . . . . . . . . . 335 12.22 API high error rate alert\n\nVersion: v1.0.0 (427b8e9)\n\n372",
      "content_length": 2621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "Listings\n\n12.23 API servers down or absent . . . . . . . . . . . . . . . . . . . . . . . . 336 12.24 The CAdvisor job . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337 13.1 The exporter container . . . . . . . . . . . . . . . . . . . . . . . . . . . 342 13.2 Connecting to the MySQL container . . . . . . . . . . . . . . . . . . . 343 13.3 Creating a MySQL user . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 13.4 Additional MySQL exporter collector . . . . . . . . . . . . . . . . . . 344 13.5 The tornado-db service . . . . . . . . . . . . . . . . . . . . . . . . . . . 344 13.6 The Kubernetes endpoint job relabelling . . . . . . . . . . . . . . . . 345 13.7 MySQL slow query alert . . . . . . . . . . . . . . . . . . . . . . . . . . 346 13.8 MySQL request rate records . . . . . . . . . . . . . . . . . . . . . . . . 347 13.9 Connections and aborted connections . . . . . . . . . . . . . . . . . . 348 13.10 MySQL alerts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349 13.11 Redis service and sidecar . . . . . . . . . . . . . . . . . . . . . . . . . 350 13.12 The Redis Kubernetes service . . . . . . . . . . . . . . . . . . . . . . 351 13.13 Redis alerts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352 13.14 Redis availability alerts . . . . . . . . . . . . . . . . . . . . . . . . . . 353 13.15 Adding the client to the project.clj . . . . . . . . . . . . . . . . . . . 354 13.16 Requiring the wrapper components . . . . . . . . . . . . . . . . . . . 354 13.17 Defining the registry . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355 13.18 Adding labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355 13.19 Adding metric calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356 13.20 The tornado_up gauge . . . . . . . . . . . . . . . . . . . . . . . . . . . 356 13.21 Starting the export . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357 13.22 Tornado metrics 13.23 Ring latency rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358 13.24 Ring high latency alert . . . . . . . . . . . . . . . . . . . . . . . . . . 358 13.25 Monitoring the Tornado API availability . . . . . . . . . . . . . . . . 359\n\nVersion: v1.0.0 (427b8e9)\n\n373",
      "content_length": 2338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "Index\n\n$labels, 197 $value, 197 __ time series names, 55 __address__, 102, 163, 337 __meta_ec2_public_ip, 162 __metrics_path__, 102, 337 __name__, 108 __scheme__, 102\n\nAbsent, 200 action\n\nhashmod, 233 keep, 233 Aggregation, 80 Alert\n\nAnnotations, 197 templates, 197\n\nAlerting, 70, 170\n\nSymptoms versus causes, 172\n\nAlerting rules, 69, 131, 187 Alertmanager, 51, 58, 70, 170\n\namtool, 184 API, 193 Cluster, 218\n\n374\n\nconfiguration, 179 continue, 204 default route, 201 Email, 180 email\n\nemails_configs, 183\n\nglobal, 180 group_by, 202 group_interval, 202 group_wait, 202 grouping, 202 High availability, 218 Installation, 175 Linux, 175 Mac OS X, 177 Windows, 177\n\nInstalling via configuration manage-\n\nment, 179\n\nmatch, 204 match_re, 206 Mesh, 218 Notification template variable refer-\n\nence, 208",
      "content_length": 794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "receivers, 182 Resolved alerts, 206 routing, 181, 204 send_resolved, 206 Silences, 210 Supported platforms, 175 templates, 181, 207 version, 176 web hooks, 183 web interface, 184\n\nAlerts, 39 amtool, 184, 213, 216 Annotations, 197 Ansible, 66, 142 Apophenia, 41 Application architecture, 9, 238 Application metrics, 238 Application monitoring, 9, 238 Architecture, 48, 51 Availability monitoring, 126 Average, 24 Averages, 25, 29 AWS, 159, 161, 164\n\nAccess Key ID, 160 Profile, 161 Secret Access Key, 160\n\nBatch jobs, 291 Bell Curve, 26 Binary operators, 128 Black Exporter\n\nVersion: v1.0.0 (427b8e9)\n\nIndex\n\nConfiguration, 280 Blackbox Exporter, 274, 276\n\nInstallation, 276 Linux, 277 Mac OS X, 278 Windows, 278\n\nInstalling via configuration manage-\n\nment, 279\n\nScraping the exporter, 285 Supported platforms, 276 version, 278\n\nBlackbox exporter, 201 DNS prober, 283 HTTP prober, 282 ICMP prober, 282\n\nBlackbox monitoring, 15, 274 Borg, 47 Borgmon, 47 Business metrics, 238 Buy-v-build, 43\n\ncAdvisor, 97 Capacity planning, 83 Chef, 66, 142, 280, 297 Chocolatey, 64 Client libraries, 58, 245 client_ruby, 245 Clustering, 218 CNCF, 48 collectd, 88, 258 Comparison binary operator, 125\n\n375",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "Configuration, 67, 73 Configuration Management, 62, 66,\n\n179, 279, 297\n\nConfiguration management, 142 container_last_seen, 127 Count, 24 Counters, 21 CPU, 114, 117\n\nData model, 54 Disabling collectors, 91 Disk, 121 DNS service discovery, 165 dns_sd_configs, 165 Docker, 97, 142, 280\n\nEC2 Service Discovery metadata, 161, 164 Profile, 161 Role ARN, 160\n\nEC2 Service discovery, 159 ec2_sd_config\n\naccess_key, 160 port, 161 profile, 161 region, 160 secret_key, 160\n\nELK, 17, 257 Endpoints, 50, 71 Exporters, 48, 58 Grok, 257 Node, 88\n\nVersion: v1.0.0 (427b8e9)\n\nExpression browser, 51, 76\n\nFault tolerance, 217 Federation, 228, 229 File-based service discovery, 154 file_sd_config, 154 files, 154 refresh_interval, 155 Frequency distribution, 22\n\nGauges, 21 global\n\nevaluation_interval, 69 scrape_interval, 69\n\nGo, 48\n\nclient, 245\n\nGoogle’s Golden Signals, 36 Grafana, 54\n\ninstallation OS X, 142 Windows, 140\n\nGranularity, 13, 20, 69 Graph, 20 Graphite, 258, 263 Grok Exporter, 257\n\nhashmod, 233 High Availability, 217 High availability, 51, 53 Histogram, 22 histogram_quantile, 334 Homebrew, 65, 142\n\nIndex\n\n376",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "honor_labels, 309 Host monitoring, 88\n\nICMP, 288 increase, 332 Installation, 61\n\nLinux, 62, 175, 277, 294 Mac OS X, 65, 142 Microsoft Windows, 63, 64, 140,\n\n177, 278 Windows, 63, 140\n\nInstalling mtail, 258 Installing onto Kubernetes, 67, 312 Installing via configuration manage-\n\nment, 66\n\nInstance label, 102, 114, 164 Instances, 50, 71 Instrumentation, 58, 238 Instrumentation labels, 55 Instrumenting applications, 305 Introspection monitoring, 15 irate, 115\n\nJava\n\nclient, 245 Job definition, 72 job_name, 72 Jobs, 50, 71, 94, 291\n\nService discovery, 151\n\nkeep, 233 kops, 312\n\nVersion: v1.0.0 (427b8e9)\n\nKubernetes, 67, 159, 312 Node Exporter, 314 kubernetes_sd_config, 313\n\nLabel\n\nInstance, 114 instance, 102, 164 Labels, 54, 55, 105, 128 __address__, 102, 163 __meta_ec2_public_ip, 162 __meta_filepath, 157 __metrics_path__, 102 __name__, 108 __scheme__, 102 Metadata, 102\n\nLatency, 36 Logging, 17, 256 Logs, 17 Logstash, 257\n\nMaintenance, 209 Mean, 25 Median, 24, 30, 35 Memory, 119 Metric names, 54, 55, 135 metric_relabel_configs, 107\n\naction, 110 regex, 109, 111 replacement, 111 separator, 108 source_labels, 108 target_label, 111\n\nIndex\n\n377",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "Metrics, 18, 242\n\nlatency, 36\n\nmetrics_relabel_configs, 162 modulus, 232 Monitoring, 6 Monitoring anti-patterns, 9 Monitoring CPU, 114, 117 Monitoring disk, 121 Monitoring jobs, 291 Monitoring Kubernetes, 313 Monitoring memory, 119 Monitoring methodologies, 36\n\nGoogle’s Golden Signals, 36 USE Method, 36\n\nmtail, 257\n\nconfiguration, 259 constants, 265 histogram, 268 installation, 258 programs, 259 running, 262 types, 266\n\nMySQL, 342\n\nNAT, 293 Node Exporter, 88, 314\n\ndisabling collectors, 91 filtering collectors, 95 Textfile collector, 92\n\nNode monitoring, 88 Notification templates, 207\n\nVersion: v1.0.0 (427b8e9)\n\nNotifications, 39\n\nObservability, 15 Observations, 19\n\nPercentiles, 24, 33, 35 Plot, 20 predict_linear, 123 Probe\n\nDNS, 288 ICMP, 288\n\nProbing, 274\n\nArchitecture, 275 Probing monitoring, 15 Promeditor, 83 Prometheus, 6\n\nconfiguration, 67 disk usage, 83 duplicate servers, 220 fault-tolerance, 220 installation Linux, 62 OS X, 65 Windows, 63 memory usage, 83 Web interface, 76\n\nprometheus\n\n–config.file, 73 –version, 62\n\nPrometheus server, 58 prometheus.yml, 67 PromQL, 51, 78, 80\n\nIndex\n\n378",
      "content_length": 1110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "Binary operators, 128 by, 80 count, 117 irate, 82, 115 predict_linear, 123 Range vectors, 82 rate, 81 regular expressions, 122 Scalar, 82 String, 82 Vector matches, 129 without, 80\n\npromtool, 62, 67, 73, 132, 137 Pull-based monitoring, 17 Puppet, 66, 142, 280, 297 Push Gateway, 48 Push-based monitoring, 17 Pushgateway, 291\n\nAggregation, 304 clients, 305 Configuration, 299 Delete metrics, 304 Installation, 293 Linux, 294 Mac OS X, 295 Windows, 295\n\nInstalling via configuration manage-\n\nment, 297\n\npush_time_seconds, 304 Scaling, 293 Scraping the gateway, 308\n\nVersion: v1.0.0 (427b8e9)\n\nSending metrics, 299 Supported platforms, 293 version, 295 Viewing metrics, 302\n\nPushProx, 293 Python\n\nclient, 245\n\nQuantile, 33 Querying labels, 78\n\nRails, 244\n\nmetrics, 244 Prometheus, 244\n\nRange vectors, 82 Rates of change, 24 RE2, 109 Receivers, 207 Recording rules, 69, 131, 132\n\nsequencing, 134\n\nRedis, 349 Regex, 109 regex, 163 RegExp, 109 relabel_configs, 107, 161, 162 Relabelling, 107, 128, 162\n\naction\n\ndrop, 110 keep, 324 labeldrop, 112 labelmap, 325, 337 replace, 324\n\nIndex\n\n379",
      "content_length": 1083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "honor_labels, 111 ordering, 110 Remote storage, 236 remote_read, 237 remote_write, 237 Resolution, 13, 20, 69 Ruby\n\nclient, 245\n\nRule files, 71 rule_files, 71, 133 Rules, 69\n\nco-mingle, 187\n\nSaltStack, 66, 280 Samples, 19 Scaling, 217 Scrape configuration, 50, 71 Scrape interval, 69 Scrape lifecycle, 101, 152 scrape_configs, 50, 71 Server, 58 Service discovery, 94, 151–153\n\nDNS, 165 EC2, 159 File-based, 154 multiple configurations, 153, 162\n\nService records, 166 Sharding, 228 Sidecar, 271 Sidecar pattern, 340 Silences, 209, 210\n\nVersion: v1.0.0 (427b8e9)\n\nexpiration, 213\n\nSoundCloud, 48 source_labels, 162 SRV records, 166, 167 SSD, 52, 85 Standard Deviation, 24, 33 static_configs, 94 StatsD, 258, 263 Sum, 24 Summary, 23 Supported platforms, 61\n\nTags, 55 Target labels, 55 target_label, 163 Targets, 50, 71 Templates, 197 Text exposition format, 91 Textfile collector, 92 Thanos, 237 Thresholds, 12 Time series, 19 topk, 347\n\nUnsee, 212 Up metric, 126 USE Method, 36, 113 Utility model, 242\n\nVector matches, 129 Visualization, 41, 131\n\nWhitebox monitoring, 15\n\nIndex\n\n380",
      "content_length": 1080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "YAML, 67 YAML validation, 67\n\nVersion: v1.0.0 (427b8e9)\n\nIndex\n\n381",
      "content_length": 67,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "Thanks! I hope you enjoyed the book.\n\n© Copyright 2018 - James Turnbull <james@lovedthanlost.net>",
      "content_length": 97,
      "extraction_method": "Unstructured"
    }
  ]
}