{
  "metadata": {
    "title": "ACM Digital Library Paper 3425174",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 11,
    "conversion_date": "2025-12-25T17:59:46.341660",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "ACM Digital Library Paper 3425174.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": ".\n\nPDF Download 3425174.3425212.pdf 25 December 2025 Total Citations: 21 Total Downloads: 316\n\nLatest updates: hps://dl.acm.org/doi/10.1145/3425174.3425212\n\nPublished: 20 October 2020\n\nRESEARCH-ARTICLE Refactoring Test Smells: A Perspective from Open-Source Developers\n\n.\n\nCitation in BibTeX format\n\n.\n\n.\n\nELVYS SOARES, Federal University of Pernambuco, Recife, PE, Brazil\n\nCurrently researching test smells, their eﬀects on automated tests, and strategies to mitigate or eliminate them. Also, researching mutation analysis, operators in specific contexts, and tool support.\n\nSAST 20: 5th Brazilian Symposium on Systematic and Automated Soware Testing October 20 - 21, 2020 Natal, Brazil\n\n.\n\n.\n\nMÁRCIO RIBEIRO, Federal University of Alagoas, Maceio, AL, Brazil\n\nGUILHERME AMARAL, Federal University of Alagoas, Maceio, AL, Brazil\n\nROHIT GHEYI, Federal University of Campina Grande, Campina Grande, PB, Brazil\n\nLEONARDO FERNANDES, Federal Institute of Alagoas, Maceio, AL, Brazil\n\nALESSANDRO FABRICIO GARCIA, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, RJ, Brazil\n\nView all\n\nOpen Access Support provided by:\n\nFederal Institute of Alagoas\n\nFederal University of Alagoas\n\nFederal University of Campina Grande\n\nFederal University of Pernambuco\n\nPontifical Catholic University of Rio de Janeiro\n\nSAST '20: Proceedings of the 5th Brazilian Symposium on Systematic and Automated Soware Testing (October 2020) hps://doi.org/10.1145/3425174.3425212 ISBN: 9781450387552\n\nRefactoring Test Smells: A Perspective from Open-Source Developers\n\nElvys Soares∗ Federal University of Pernambuco Recife, Brazil eas5@cin.ufpe.br\n\nMárcio Ribeiro Federal University of Alagoas Maceió, Brazil marcio@ic.ufal.br\n\nGuilherme Amaral Federal University of Alagoas Maceió, Brazil gvma@ic.ufal.br\n\nRohit Gheyi Federal University of Campina Grande Campina Grande, Brazil rohit@dsc.ufcg.edu.br\n\nLeo Fernandes Federal Institute of Alagoas Rio Largo, Brazil leonardo.fernandes@ifal.edu.br\n\nAlessandro Garcia Pontifical Catholic University of Rio de Janeiro Rio de Janeiro, Brazil afgarcia@inf.puc-rio.br\n\nBaldoino Fonseca Federal University of Alagoas Maceió, Brazil baldoino@ic.ufal.br\n\nAndré Santos Federal University of Pernambuco Recife, Brazil alms@cin.ufpe.br\n\nABSTRACT Test smells are symptoms in the test code that indicate possible de- sign or implementation problems. Their presence, along with their harmfulness,hasalreadybeendemonstratedbypreviousresearches. However, we do not know to what extent developers acknowledge the presence of test smells and how to refactor existing code to eliminate them in practice. This study aims to assess open-source developers’ awareness about the existence of test smells and their refactoring strategies. We conducted a mixed-method study with two parts: (i) a survey with 73 experienced open-source develop- ers to assess their preference and motivation to choose between 10 different smelly test code samples, found in 272 open-source projects, and their refactored versions; and (ii) the submission of 50 pull requests to assess developers’ acceptance of the proposed refactorings. As a result, most surveyed developers preferred the refactored proposal for 78% of the investigated test smells, and the pull requests had an average acceptance of 75% among respon- dents. Additionally, we were able to provide empiric validation for literature-proposed refactoring strategies. This study demonstrates that although not always using the academic terminology, develop- ers acknowledge both the negative impact of test smells presence and most of the literature’s proposals for their removal.\n\nCCS CONCEPTS • Software and its engineering → Software testing and de- bugging; Empirical software validation.\n\nKEYWORDS Test Smells, Refactoring, Open Source, Validation\n\nACM Reference Format: Elvys Soares, Márcio Ribeiro, Guilherme Amaral, Rohit Gheyi, Leo Fernan- des, Alessandro Garcia, Baldoino Fonseca, and André Santos. 2020. Refac- toring Test Smells: A Perspective from Open-Source Developers. In 5th Brazilian Symposium on Systematic and Automated Software Testing (SAST ’20), October 20–21, 2020, Natal, Brazil. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3425174.3425212\n\n1 INTRODUCTION Automated software testing and the development of test scripts are frequent in the software industry [6]. A wide range of tools and frameworks for these activities, whether in support of creating auto- matic tests or the complete automatic generation of these, is already available [3, 9]. Automated test suites with high internal quality facilitate maintenance activities, such as code understanding and regression testing. However, the test code’s development is tedious, error-prone, and requires a significant initial investment [1].\n\nPoorly designed test code is frequent in practice [1], and the symptoms that indicate possible design problems in the test code are called bad test smells, or just test smells [6, 18]. These smells are not the same as those defined by Fowler [2] for the production code. As they are based on test cases organization, implementation, and interaction with one another, their corresponding refactoring operations generally differ from production code.\n\n∗The author is also affiliated to the Federal Institute of Alagoas, Maceió, Brazil, available at elvys.soares@ifal.edu.br\n\nACMacknowledgesthatthiscontributionwasauthoredorco-authoredbyanemployee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SAST ’20, October 20–21, 2020, Natal, Brazil © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8755-2/20/09...$15.00 https://doi.org/10.1145/3425174.3425212\n\nAs a motivating example, Figure 1 presents a code snippet1 ex- tracted from the core module of the AssertJ project, self-described as “a set of strongly-typed assertions to use for unit testing with any test automation framework that is very close to plain English”. This test intends to verify a specific error and its specific properties. One can note the presence of a try/catch structure in the test and commands on lines 9 and 11 to handle the test outcome manually. According to Peruma et al. [11], manually passing or failing a test\n\n1Original source code available at https://git.io/JJ1gA\n\n50\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\ndepending on whether the production code throws an exception to a custom exception handling code in a test is expendable once the test framework already provides exception handling features to that end.\n\nFigure 1: Example of the Exception Handling test smell\n\nLater, we observed a test code refactoring2 using both AssertJ new features and Java 8 lambda expressions that made the excep- tion test more transparent and eliminated the previous need for manually dealing with errors, and manually passing or failing the test. Figure 2 presents the refactoring performed by the AssertJ team, eliminating the fragility of the original design.\n\nFigure 2: Refactoring the Exception Handling test smell\n\nThe provided example reflects what a test smell means: a symp- tom in the test code that may (or may not) indicate a design problem. The subsequent refactoring action demonstrated awareness of the design problem and implemented a coding strategy to eliminate it. Although numerous previous studies have already demonstrated the presence of test smells in test suites of open-source projects, as well as their negative impact on code maintenance and understand- ing activities [1, 9–11, 19], there is a lack of studies to assess the awareness of open-source developers about the existence of tests smells and their possible solutions.\n\nThus, this study presents two empirical investigations that aim\n\nto answer the following research questions:\n\n2Refactored source code available at https://git.io/JJ1Vn\n\n51\n\nSoares, et al.\n\nRQ1: To what extent test smells are relevant to open- source developers?\n\nRQ2: To what extent open-source developers accept refactorings that remove test smells?\n\nThe first empirical study is a survey with open-source devel- opers to investigate RQ1 by considering questions such as “Are open-source developers aware of test smells and recognize smelly test code?” and “Do they know how to prevent or refactor a test smell?”. This study consisted of showing smelly test code snippets that we gathered from open-source projects, along with an alternate refac- tored version of the same test code. Then we asked developers to choose which version they preferred and justify their answers.\n\nThe second empirical study aimed at answering RQ2 and ques- tions such as “Do they acknowledge the existence of test smells in their projects?” consists of submitting contributions (Pull Requests) with test code refactorings to active open-source projects and collect whether the developers of such projects will accept the contribu- tions, along with considerations to any possible discussion topics thatmayemergefromthecontributions.WehavesentPullRequests to 50 different open-source projects and, among respondents, we achieved a 75% acceptance rate, having most contributions being accepted with very little further discussions.\n\nIn summary, this paper provides the following contributions:\n\nA survey with experienced open-source developers that as- sessed their impressions about test smells and their refactor- ing strategies in Section 3;\n\nAn empirical study based on pull requests to better under- standtheacceptancelevelofopen-sourcedeveloperstorefac- torings of test smells in their projects, along with an empiri- cal validation of 10 literature-proposed refactoring strategies in Section 4;\n\n2 REFACTORING TEST SMELLS We now present a brief explanation of the test smells we used in this study, the strategies to eliminate them and the related code snippets (simplified as toy examples). According to the test smell distribution presented by Peruma et al. [11], these test smells figure between the highest available ones on open-source projects:\n\nFigure 3: Conditional Test Logic\n\nConditional Test Logic: Defined by Meszaros [6], this test smell occurs when the test depends on a condition. It is a dangerous design since a test method may result in a passed status without\n\nRefactoring Test Smells: A Perspective from Open-Source Developers\n\never having asserted a unit result. Palomba et al. [10] propose refac- toring this test smell with an “Extract Method” [2] operation, which we believe to be more suitable in cases of nested loops and decisions. As our example uses a decision structure to verify a precondition, we added an assumption to the precondition, which will make the test to be ignored if not met. Figure 3 presents both the original and the refactored versions.\n\nAssertion Roulette: It is defined by van Deursen et al. [18] as a collection of unexplained assertions in a single test method that makes it difficult to trace which exact assertion had a problem in the event of test failure. This test smell has the original refactoring suggestion of “Add Assertion Explanation”, and an alternative sug- gestion to break up the test into a suite of “Single-Condition Tests” proposed by Meszaros [6]. As a single-condition test is actually a good programming practice that increases code coverage, we opted to use this refactoring strategy. Figure 4 presents the original program with the Assertion Roulette and its refactored version.\n\nFigure 4: Assertion Roulette\n\nException Handling: According to Peruma et al. [11], this smell occurs when the test manually handles both exceptions and test outcome and its frequency rivals with Assertion Roulette on Java projects. Figure 5 presents the first case of this test smell, along with its refactoring, when properties after an exception must be verified. Figure 6 presents the case 2 of the Exception Han- dling test smell, that occurs when an exception message needs to be verified along with an alternate version that uses JUnit frame- work features. Figure 7 presents case 3, along with an alternate refactored option, which happens when some final steps must be executed independently of the test result or the existence of any exceptions. Figure 8 presents case 4, which tests for a specific ex- ception being raised. The JUnit framework recommends the use of @Test(expected=MyException.class) annotation in this case.\n\n52\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\nFigure 5: Case 1 of Exception Handling\n\nFigure 6: Case 2 of Exception Handling\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\nFigure 7: Case 3 of Exception Handling\n\nFigure 8: Case 4 of Exception Handling\n\nSensitive Equality: van Deursen et al. [18] state the agility and ease to write equality checks using the toString() method, being a frequent alternative to calculate a result value and map it to a sequence to compare to a literal string representing the expected value. However, these tests can depend on many irrelevant details, suchas commas,quotes,and spaces.And whenever the toString() method for an object is changed, all related tests will start to fail. The proposed solution to this test smell is to replace the equality checks that use the toString() method with real equality checks\n\n53\n\nSoares, et al.\n\nusing the “Introduce Equality Method” refactoring strategy [18]. Figure 9 presents an example of Sensitive Equality, along with a refactored alternative.\n\nFigure 9: Sensitive Equality\n\nFigure 10: Magic Number Test\n\nMagic Number Test: Proposed by Peruma et al. [11], this smell occurs when the test method contains undocumented numeric\n\nRefactoring Test Smells: A Perspective from Open-Source Developers\n\nliterals with no clear meaning (magic values). To refactor this test smell, the authors state that these numeric values can be replaced with constants or variables, thus providing a descriptive name for the value. This smell is presented in Figure 10.\n\nResource Optimism: According to van Deursen et al. [18], this smell happens when test methods make optimistic assumptions about the existence or the state of external resources like files and databases. Such assumptions may cause non deterministic behavior on the outcome and can be refactored through the “Setup External Resource”, also defined before [18]. Figure 11 presents our example of Resource Optimism and the corresponding refactoring operation.\n\nFigure 11: Resource Optimism\n\nTest Code Duplication: Also proposed van Deursen et al. [18], this test smell normally identifies classes that contain test methods with repeated test code steps. Typically, parts that set up tests fixtures are normally related to this test smell. The authors state that, for cases where the duplicated code is in the same class, this test smell can be eliminated using “Extract Method”, defined by Fowler [2], as it is for normal code duplication. Figure 12 presents our example, as well as its refactored version.\n\nThe next sections present the empirical investigations we per- formed in this study. The replication package containing all the raw data of both investigations can be found at the companion website3.\n\n3 SURVEY WITH DEVELOPERS In this section, we present our first empirical study performed with open-source developers.\n\n3https://bit.ly/2EeL7Io\n\n54\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\nFigure 12: Test Code Duplication\n\n3.1 Planning This survey aimed to analyze implementation preferences concern- ing the presence of test smells and their refactoring identification, from the viewpoint of open-source developers, as stated by RQ1: To what extent test smells are relevant to open-source devel- opers?\n\nThe general idea was to ask developers for their preference between two implementations of equivalent unit tests. One of these implementations was always an original (smelly) code snippet, and the other one was a refactored alternative, following the refactoring principles of Murphy-Hill, Parnin, and Black [7].\n\n3.2 Settings To assemble the survey, we needed a list of test smells from open- source projects. As there is no public list of such smells, we had to search and use a tool to analyze projects from public repositories. Although the most comprehensive survey to the date [3] lists 12 tools that handle test smells, the most recent and complete one is the TSDetect [11] tool proposed by Peruma et al. [11]. It detects 19 different test smells and analyzes JUnit test cases, creating detailed execution logs. Using the GitHub search, we retrieved the projects list by classifying Java projects in descending order of stars and executed the TSDetect [11] in the listed results. According to the tool execution logs, we achieved a test smell distribution result consistent with that reported by Peruma et al. [11] with the first 272 results.\n\n3.3 Design Intending to have as many respondents as possible, we defined the surveytotakeanaverageof10minutesofthedevelopers’time.This way, we searched for suitable test methods that (i) had few lines of code and could be displayed along with the refactored version without the need for scrolling the screen, and (ii) had an identifiable test smell, albeit we did not stress if the code had any problem, nor highlighted it in any way.\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\nThe pilot execution of the survey revealed a significant concern about the displayed code snippets: developers kept trying to un- derstand project specificities, taking much longer than anticipated, and not noticing the problems we wanted them to analyze. To miti- gate this issue, we reduced the code snippets to toy examples, as presented in the code examples from Section 2.\n\nThrough scripts, we retrieved the contributors of open-source Java projects who had public contact info available and contributed to test activities within their projects. Our scripts analyzed about 600 projects, and a total of 2,011 invitation emails were sent.\n\nAlong with questions concerning demographics and the develop- ers’ expertise in software testing, we presented 9 of the test smells examples presented in Section 2, only excluding the Exception Han- dling 4, which is a simple framework usage, as we intended the survey to last the least possible time. The test smells, and their refactoring options were displayed as A/B questions, which were presented in random order to every developer, who could choose from the following list of options: (i) I strongly prefer A, (ii) I prefer A, (iii) Indifferent, (iv) I strongly prefer B, (v) I prefer B, and (vi) I do not know. All questions were mandatory and provided an optional field for the developers to justify their answers.\n\n3.4 Results and Discussion We performed the survey in June 2020, achieving a response rate of 3.63% with the total number of 73 responses. Concerning the demo- graphics, area of work, and expertise questions, we had participants working in 26 countries, 89% of them defined their primary area of work as the industry (over academia), 68.5% declared to have 10+ years of experience with software testing. This number rises to about 87.5% if we consider 5+ years of experience. The average of their daily time spent on testing activities is about 41%. Con- cerning the A/B questions, Table 1 presents the obtained results, in percentage.\n\nTest Smell\n\nOriginal Refactored Other\n\nAssertion Roulette Conditional Test Logic Exception Handling 1 Exception Handling 2 Exception Handling 3 Magic Number Test Resource Optimism Sensitive Equality Test Code Duplication\n\n12.3% 12.3% 24.7% 38.4% 26.0% 31.5% 27.4% 37.0% 31.5%\n\n65.8% 64.4% 68.5% 37.0% 68.5% 54.8% 49.3% 32.9% 49.3%\n\n21.9% 23.3% 6.8% 24.7% 5.5% 13.7% 23.3% 30.1% 19.2%\n\nTable 1: Survey results\n\nIn order to better interpret the obtained results, both numeric and comment analyses are fundamental. While the numeric results indicate whether developers accept a proposed refactoring method, the comment analysis tells us if they are doing so because they correctly recognize the possible flaw in the test code.\n\nThe Assertion Roulette result showed that although the provided refactoringwasnottheoriginallyproposedone,wherevanDeursen et al. [18] suggests commented assertions, most developers prefer\n\n55\n\nSoares, et al.\n\nto separate assertions into single test cases. The provided comments reinforce their choices: “It tests different methods on a class so if one of them fails it is easy to see which one by looking at test results” and “(refactored) is more code, but it will give more fine-grained reporting when there is a failure.”.\n\nThe Conditional Test Logic had results consistent with the Asser- tion Roulette. Although most developers claimed both tests not to be equivalent because the original test would pass without testing all scenarios, they seemed to understand the intent of the condition: “(Original) suggests that b is somehow an assumption that must be satisfied for executing the rest of the test (I would do that differently, though)”.\n\nThe first and third cases of Exception Handling had their refac- tored versions preferred by most developers. The most common argument was using the test framework resources and the improve- ments in code readability and maintainability. Regarding the second case of the Exception Handling test smell, where an exception mes- sage needed to be tested, although comments about the existence of try/catch blocks in test code followed the other Exception Handling cases, the provided refactoring implementation using the Rule an- notation divided opinions. A frequently provided reason is given in the comment “I am not familiar with this ‘@Rule’. The code looks prettier (which in general means more maintainable), but the ordering is a bit counter-intuitive” reinforce our statement.\n\nDevelopers well recognized the Magic Number Test Smell. Some of them argued that the original test code would be acceptable if the test method is unique. Comments like “Using well-named constants enhances readability when comparing with magic numbers usage” demonstrate that developers value code semantics.\n\nAlthough the refactored version of the Resource Optimism test smell was the choice of almost half of the developers, most com- ments concerned “Using a @Before method allows reuse by multi- ple tests.”, which indirectly solves the test smell. Fewer comments stressed the need to have external resources instantiated separately from the test steps, as in “Allows me to distinguish between failures in setting up tests vs. the actual test I want to perform”.\n\nConcerning the Sensitive Equality test smell, as the distribution of choices was slightly balanced between the options, it is possi- ble to find (i) critics to the use of the toString() method: “The ‘toString’ method is not always a reliable indicator of object state.”, (ii) critics to the equality check method: “The equality check method seems to be created for the test: the test should test the real behavior of the object” and (iii) critics to the example design: “Semantics of equalityCheck are not at all obvious”. The comments analysis demon- strates that most developers recognize the used of toString() as problematic, but disagree whether an equality check method is the best refactoring strategy. We also believe reducing a toy example could not evidence the test smell to all developers, causing them difficulties understanding the root problem. Maybe a more suit- able example would make the 30% of developers who opted for “Indifferent” and “ I do not know” to choose more accurate opinions. Almost half of the developers selected the refactored Test Code Duplication. They highlight the need to avoid duplicate test code and good practices as the use of DRY4 principles. The other groups\n\n4The Don’t Repeat Yourself principles were defined by Thomas and Hunt [16]\n\nRefactoring Test Smells: A Perspective from Open-Source Developers\n\nkept elaborating whether the use of an “Extract Method” [2] in such a small example would be over-engineering.\n\nSummary:Evenwhendevelopersdidnotchoosetherefac- tored version, they correctly indicated the code fragility that motivates the test smell definition and sometimes the refactoring fragility. Therefore we conclude that open- source developers acknowledge test smells refactoring strategies, positively answering RQ1.\n\n3.5 Threats to Validity The primary construct validity threat is related to creating toy examples from both original and refactored code snippets, which may have resulted in confusing or over-engineered toy examples that could lead developers to misjudgment. Despite having the survey presentation page with highlights on the simplified, but real open-source examples, and the process of simplification we submitted the real code snippets to, we could find comments to answers considering the toy examples as literal ones. As internal threats, the code examples we collected from open-source projects in the early elaboration stage of the survey may not be the best representatives of the intended test smells and their refactoring strategies, or cover all possible test smells scenarios. The Exception Handling 4, presented in Section 2, which was not part of the survey, although being a relatively simple case, is an excellent example of a lack of complete coverage. As external threats, this study evaluated 7 test smells and their refactoring strategies. While broad surveys like Garousi and Küçük [3] and Peruma [13] have, combined, cataloged, and proposed more than 130 different test smells, generalizing our results to a larger group may not be a valid assumption.\n\n4 SUBMISSION OF PULL REQUESTS This section describes the second empirical investigation performed in this study, where contributions to open-source projects were made to assess developers’ opinions about test smell refactorings.\n\n4.1 Planning Our second empirical study aims to analyze contributions to open source projects regarding the acceptance to test smell refactorings from the viewpoint of the project maintainers, as stated by RQ2: To what extent open-source developers accept refactorings that remove test smells?\n\nWe assume that the acceptance rate of the pull requests might corroborate that test smell refactoring is relevant. In this study, we also focused on the 7 test smells and 10 refactoring strategies presented in Section 2.\n\n4.2 Settings Starting from the TSDetect [11] execution logs acquired in the first study, we manually searched for test smells that fit the generic tem- plates defined by the toy examples. Once a test smell was identified, the same refactoring strategy used in the developers’ survey was applied.\n\n56\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\nAs explained, for each system, TSDetect [11] reports test smells through its execution logs (Step 1). Because we execute TSDe- tect [11] considering the entire project test code, we could find several test smells (even the ones we are not considering in this study). This way, for each project, we randomly selected an iden- tified test smell. The intention was to assess the developers’ ac- ceptance of that specific smell. Besides, to minimize bias on pull request acceptance from the same developer, we decided to submit only one contribution per project. Then, given the selected test smell, we applied a proper refactoring (see Section 2) and submitted a pull request (Step 2). Notice that we neither fix bugs nor submit new test cases. We focused on submitting that one specific test smell refactoring. In addition to sending the pull request, we also submitted a comment justifying the transformation that, unless stated differently by specific projects submission rules, followed the pattern of presenting the definition of the problem, the description of the refactoring strategy (proposed solution), and the before and after (code snippets) changes. When necessary, we discussed with the developers to defend our submission before deciding to accept or reject the pull request (Step 3).\n\n4.3 Results and Discussion Although we have submitted 50 pull requests, we were limited by the template models we defined in the first study. Also, many difficulties were encountered regarding dependencies on projects’ environments, which forced us to abandon submitting some pull re- quests. As we explained, we submitted one pull request per project, which leads us to 50 open-source projects involved in this part of the study. Table 2 presents the pull requests submission distribution by test smell type.\n\nTest Smell Type\n\nSubmitted Accepted Rejected\n\nAssertion Roulette Resource Optimism Conditional Test Logic Exception Handling (all) Magic Number Test Sensitive Equality Test Code Duplication\n\n8 6 9 12 8 2 5\n\n50% 16% 44% 50% 12.5% 50% 20%\n\nN/A 33% N/A 8% 25% 50% N/A\n\nTotal:\n\n50\n\nTable 2: Pull Requests by Test Smell type\n\nUntil submitting this study, we received 18 acceptances, 6 re- jections, and noticed 4 submission errors. The average time the respondent projects took to analyze a pull request was two busi- ness days. We had no projects that contested the submissions and required long, convincing dialogues. However, we had one occur- rence (Conditional Test Logic) that investigated, on project details, therealnecessityoftheconditionaltestlogicandendedupagreeing with the refactoring proposal.\n\nTable 3 presents the pull requests we submitted in this study. It also provides additional project details and summarizes the devel- opers’ comments.",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-11)",
      "start_page": 9,
      "end_page": 11,
      "detection_method": "topic_boundary",
      "content": "SAST ’20, October 20–21, 2020, Natal, Brazil\n\nNo Project Name\n\n1 ffmpeg-cli-wrapper gumtree 2 google-maps-services-java 3 dropwizard 4 docx4j 5 jvm-tools 6 7 lavagna 8 mango 9 mybatis-3 pi4j 10 pippo 11 Fenzo 12 fluent-validator 13 realm-android-adapters 14 15 zxing-android-embedded 16 wiremock 17 18 19 20 21 22 23 24 Apktool rest-client 25 bitcoinj 26 27 RxCache blade 28 size-analyzer 29 30 bt 31 RegexGenerator bugsnag-android 32 bundletool 33 34 cordova-android 35 Conductor 36 37 38 webcam-capture unirest-java 39 testcontainers-java 40 client_java 41 compile-testing 42 scribejava 43 eclipse-collections 44 spark 45 sofa-bolt 46 stetho 47 48 binding-collection-adapter 49 AndroidNetworkTools 50\n\ngerrit-intellij-plugin JSCover pac4j disruptor hutool spring-boot-thin-launcher assertj-core\n\ncommonmark-java tablesaw\n\njava-dns-cache-manipulator\n\nSW Category Test Smell Type\n\nStatus\n\nApp Framework Library Framework Library App App Framework Framework Library Framework Library App Android App Library App Plugin App App Library Library Framework Framework App App Library Library Framework Tool Framework Library Library Tool Framework Framework Library Library Library Library Library App Tool Library Framework Library Framework App Library Library Library\n\nResource Optimism Assertion Roulette Exception Handling Conditional Test Logic Accepted Open Assertion Roulette Accepted Assertion Roulette Accepted Resource Optimism Open Assertion Roulette Submission Error Conditional Test Logic Exception Handling Accepted Conditional Test Logic Accepted Exception Handling Assertion Roulette Exception Handling Assertion Roulette Magic Number Assertion Roulette Exception Handling Exception Handling Magic Number Assertion Roulette Conditional Test Logic Open Resource Optimism Resource Optimism Resource Optimism Sensitive Equality Conditional Test Logic Open Open Magic Number Resource Optimism Submission Error Conditional Test Logic Open Test Code Duplication Open Conditional Test Logic Accepted Conditional Test Logic Accepted Test Code Duplication Open Test Code Duplication Open Test Code Duplication Open Sensitive Equality Magic Number Magic Number Magic Number Exception Handling Exception Handling Exception Handling Conditional Test Logic Open Open Magic Number Open Exception Handling Open Magic Number Exception Handling Accepted Test Code Duplication Accepted Exception Handling\n\nSubmission Error Open Submission Error\n\nOpen Accepted Open Open Open Accepted Accepted Accepted Rejected Accepted\n\nRejected Rejected Open Rejected\n\nAccepted Open Accepted Rejected Accepted Rejected Accepted\n\nOpen\n\nTable 3: Submitted pull requests\n\n57\n\nSoares, et al.\n\nPull Request Comments\n\n“Wrong usage of project classes”\n\n“Wrong pull request format” “Thanks for your contribution”\n\nNo Comments “Thanks for your contribution”\n\n“Wrong usage of project classes” “Thanks for your contribution” “Thanks for your contribution”\n\n“Thanks for your contribution”\n\n“Thanks for your contribution” “Thanks for your contribution” “Thanks for your contribution” “Test is harder to understand” “Thanks for your contribution”\n\n“ Current test behavior preferred” “Current test behavior preferred”\n\n“Wrong Java API usage”\n\n“Wrong pull request format”\n\n“Thanks for your contribution” “Thanks for your contribution”\n\nNo Comments\n\n“Thanks for your contribution” “Test is harder to understand” “Thanks for your contribution” “Discouraged test framework feature” “Unaware of test framework feature” “Rejected test framework feature”\n\nNo Comments No Comments\n\nRefactoring Test Smells: A Perspective from Open-Source Developers\n\nRegarding the pull requests’ comments, it is vital to notice that most of the accepted pull requests were made with a simple “Thanks for your contribution” message or no message at all. From this fact, we understand that the developers accepted both the presence of the test smell and the refactoring strategy.\n\nAs to the rejected pull requests, four different test smells are on the list: Magic Number Test (2), Sensitive Equality (1), Resource Optimism (2) and Exception Handling (1).\n\nIn the rejected Magic Number Tests, the first developer claimed that“Hard-codednumbersaren’tautomaticallybad.” and“Thischange makes the tests harder to understand.”, and closed the pull request after that. Many of the numeric literals inserted directly into the code are used, for example, to retrieve the first position of an array, or to assert the size of a known array. Such numbers are not difficult to read and maintain in the code, and this coding practice seems to be widely used in the analyzed open-source projects. Replacing them with constants or variables may add unnecessary text. The second developer argued that “200 is a well-known status code” and “ I don’t think changing it would make any sense.”. Although we had accepted pull requests to this same case, well-known response codes may be an argument in favor of the magic numbers.\n\nOther rejected pull request was related to the Sensitive Equal- ity test smell. In this case, the developer arguments that “The toString() / fromString() API is actually the recommended API to use if you deal with addresses.”. Indeed, if an API is widely used to retrieve a text value from an object using the toString() method, then defining a new method that always returns the same value as toString() may not be necessary.\n\nTwo rejected pull requests were related to the Resource Opti- mism test smell. Both developers did not want the failing assump- tion of the external resource, which was not the software unit under test, to ignore the test execution. They preferred the original code, which would fail if the resource is not available.\n\nThe last rejection is related to the Exception Throwing test smell. The response given by the developer was “we actually discourage the use of @Test(expected = ...) in general.” and “a test that uses @Test(expected = ...)” will pass if any expression in the test throws an exception of the expected type, even if you only expect the exception from a particular call. It is a good rationale but might apply mostly to integration tests, which are frequently populated with several steps and verifications.\n\nThe four submission errors were due to the lack of complete un- derstanding of the project architecture and the software unit under test, or even the project rules for pull requests, which generated bad submissions.\n\nSummary: Even when a pull request was refused, the discussion with developers showed that they correctly identify the general “issue” we were referring to and were able to discuss why the test smell did not apply to their projects. The 75% acceptance, mostly without discussion, leads us to conclude that open-source developers accept refactorings test smells in their projects, thus positively answering RQ2.\n\n58\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\n4.4 Threats to Validity The construct validity relates to the refactoring templates presented in Section 2, which were not extended to all scenarios within a stud- ied test smell type. A clear example of such an untreated template is the Conditional Test Logic cases that nest their assertions with repetition and decision structures and did not receive pull requests in this study. As internal validity, the submitted pull requests by test smell type may not be enough to generalize results per studied test smell. Concerning external validity, as we worked with almost the same test smells and refactorings we used in the survey with developers, this group of test smells may not be representative of generalizing results to the test smells we did not cover in this study.\n\n5 RELATED WORK Palomba et al. [8] performed a survey on the developer’s perception of bad code smells. They showed production code snippets from three software systems to original and outside (students and indus- try) developers, asking them to indicate if they found any potential design problems and what nature and severity level they would classify the possible problems. As a result, they could divide the investigated code smells into categories related to code complexity and coding good-practices. Our study offers a similar perception by open-source developers, this time concerning test smells.\n\nThe study performed by Bavota et al. [1] demonstrated test smell distributioninsoftwaresystemsandwhethertheirpresenceisharm- ful. As part of the investigation, they performed a controlled experi- ment involving 61 participants among students (freshers, bachelors, and masters) and industrial developers, which were asked to per- form maintenance activities on smelly and refactored test code of two software systems. The study demonstrated the negative impact of test smells in program comprehension during maintenance ac- tivities. Our study uses both open-source developers and projects to survey on test smells perceptions.\n\nTufano et al. [17] investigation aimed at analyzing when test smells occur in source code, what their survivability is, and whether their presence is associated with the presence of design problems in production code (code smells). They collected the developers’ perception of test smells in a study with 19 developers from Apache and Eclipse ecosystems. They demonstrated that (i) test smells have a long life cycle in software systems, and (ii) there are correlations between test and code smells. Our study extends the surveyed public in order to improve the accuracy of developers’ perception. Lambiaseetal.[5]presentedtheDARTS(DetectionAndRefactor- ing of Test Smells) tool. It detects instances of three test smell types (General Fixture, Eager Test, and Lack of Cohesion of Test Methods) at the commit-level and enables their automated refactoring using the refactoring techniques defined by Meszaros [6]. The study pro- poses refactorings based on previously-proposed strategies. Our study validates the same strategies using developers’ opinions.\n\nA work in progress performed by Schvarcbacher et al. [14] aimed to assess the perception of developers on test smells in their code- base and which ones they would consider being more important. They present the developers’ reactions to various instances of test smells pointed out by their detection tool, based on the TSDe- tect [11], in integration with the Better Code Hub (BCH).5 Their\n\n5https://bettercodehub.com/\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\nresults show that developers are only willing to remove a small portion of the found test smells, but did not present them with refactored alternatives to measure their acceptance.\n\nThe study performed by Spadini et al. [15] aimed at investigating the severity rating for four test smells and their perceived impact on test suite maintainability by the developers. They collected test smells from 1,500 open-source projects and used in-company devel- opers’ perceptions to calibrate the established severity thresholds for test smells. As results, they found that current detection rules for certain test smells are considered too strict by the developers and verified that their newly defined severity thresholds are in line with the participants’ perception of how test smells impact the maintainability of a test suite.\n\nAlthough researching refactoring operations, Peruma et al. [12] used a mining tool to detect refactoring operations from an existing dataset of unit test files and smells in 250 open-source Android Apps. Among the results, they could demonstrate that the types of refactorings applied to test and non-test files in Android apps are different and that there exist test smells and refactoring oper- ations that co-occur frequently. More importantly, and following the results of our study, they could verify that developers apply refactorings for reasons other than the correction of smells, making the fixing of smells merely a byproduct.\n\nSilva Junior et al. [4] carried out an expert survey to analyze the usage frequency of a set of test smells, aiming to understand whether test professionals non-intentionally insert test smells. Us- ing an observation by case-control study design, they evaluated the answers of 60 industry developers on 14 test smells. As a result, they found that developers introduce test smells during their daily pro- gramming tasks, even when using their companies’ standardized practices.\n\n6 CONCLUSIONS In this paper, we conducted two empirical studies regarding open- source developers’ awareness about test smells and the refactoring strategies to mitigate them.\n\nIn the first study, we surveyed 73 developers, who analyzed 10 test smells from real open-source projects and their refactored versions. The answers and their justifications led us to conclude that even when developers disagree with the proposed refactorings, they are aware of the threats proposed by the smelly code. This study was also able to provide empirical validation to most of the literature-proposed refactoring strategies we used in the assessed test smells.\n\nInthesecondstudy,wesubmitted50PullRequeststoopensource projects whose tests had at least one of the 7 test smells we inves- tigated in the previous study to verify if the developers accepted our contributions. The 75% acceptance rate among respondents indicates that developers are also prone to accept contributions that solve test smells in their test code.\n\nOur findings corroborate the idea that not every test smell repre- sents a problem to the developers, and their corresponding refactor- ing strategies may not be preferred in most cases, which encourages further study on whether specific test smells are meaningful to de- velopers.\n\n59\n\nSoares, et al.\n\nAs future work, we intend to (i) increase the list of developers’ validated test smells and their refactoring strategies, (ii) investi- gate if new test framework features could be successfully used in such refactorings, and (iii) investigate if any test smell has become obsolete due to new test framework features or properties.\n\nACKNOWLEDGMENTS This work was partially funded by CAPES (117875), CNPq (421306 /2018-1, 309844/2018-5, 426005/2018-0 and 311442/2019-6), and FA- PEAL/FAPESP (60030.0000000462/2020) grants.\n\nREFERENCES [1] Gabriele Bavota, Abdallah Qusef, Rocco Oliveto, Andrea De Lucia, and Dave Binkley. 2015. Are test smells really harmful? An empirical study. Empirical Software Engineering 20, 4 (2015), 1052–1094.\n\n[2] Martin Fowler. 2018. Refactoring: improving the design of existing code. Addison-\n\nWesley Professional.\n\n[3] Vahid Garousi and Barış Küçük. 2018. Smells in software test code: A survey of knowledge in industry and academia. Journal of Systems and Software 138 (2018), 52–81.\n\n[4] Nildo Silva Junior, Larissa Rocha, Luana Almeida Martins, and Ivan Machado. 2020. A survey on test practitioners’ awareness of test smells. arXiv:2003.05613 [cs.SE]\n\n[5] Stefano Lambiase, Andrea Cupito, Fabiano Pecorelli, Andrea De Lucia, and Fabio Palomba. 2018. Just-In-Time Test Smell Detection and Refactoring: The DARTS Project. (2018).\n\n[6] Gerard Meszaros. 2007. xUnit test patterns: Refactoring test code. Pearson Educa-\n\ntion.\n\n[7] Emerson Murphy-Hill, Chris Parnin, and Andrew P Black. 2011. How we refactor, and how we know it. IEEE Transactions on Software Engineering 38, 1 (2011), 5–18.\n\n[8] F. Palomba, G. Bavota, M. D. Penta, R. Oliveto, and A. D. Lucia. 2014. Do They Really Smell Bad? A Study on Developers’ Perception of Bad Code Smells. In 2014 IEEE International Conference on Software Maintenance and Evolution. 101–110. [9] Fabio Palomba, Dario Di Nucci, Annibale Panichella, Rocco Oliveto, and Andrea DeLucia.2016. Onthediffusionoftestsmellsinautomaticallygeneratedtestcode: An empirical study. In 2016 IEEE/ACM 9th International Workshop on Search-Based Software Testing (SBST). IEEE, 5–14.\n\n[10] Fabio Palomba and Andy Zaidman. 2019. The smell of fear: On the relation between test smells and flaky tests. Empirical Software Engineering 24, 5 (2019), 2907–2946.\n\n[11] Anthony Peruma, Khalid Almalki, Christian D Newman, Mohamed Wiem Mkaouer, Ali Ouni, and Fabio Palomba. 2019. On the distribution of test smells in open source Android applications: an exploratory study. In Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering. IBM Corp., 193–202.\n\n[12] Anthony Peruma, Christian D Newman, Mohamed Wiem Mkaouer, Ali Ouni, and Fabio Palomba. 2020. An Exploratory Study on the Refactoring of Unit Test Files in Android Applications. In Conference on Software Engineering Workshops (ICSEW’20).\n\n[13] Anthony Shehan Ayam Peruma. 2018. What the Smell? An Empirical Investigation on the Distribution and Severity of Test Smells in Open Source Android Applications. Ph.D. Dissertation.\n\n[14] Martin Schvarcbacher, Davide Spadini, Magiel Bruntink, and Ana Oprescu. 2019. Investigating developer perception on test smells using better code hub-Work in progress. In 2019 Seminar Series on Advanced Techniques and Tools for Software Evolution, SATTOSE.\n\n[15] Davide Spadini, Martin Schvarcbacher, Ana-Maria Oprescu, Magiel Bruntink, and Alberto Bacchelli. [n.d.]. Investigating Severity Thresholds for Test Smells. ([n.d.]).\n\n[16] David Thomas and Andrew Hunt. 2019. The Pragmatic Programmer: your journey\n\nto mastery. Addison-Wesley Professional.\n\n[17] Michele Tufano, Fabio Palomba, Gabriele Bavota, Massimiliano Di Penta, Rocco Oliveto, Andrea De Lucia, and Denys Poshyvanyk. 2016. An empirical investiga- tionintothenatureoftestsmells.InProceedingsofthe31stIEEE/ACMInternational Conference on Automated Software Engineering. 4–15.\n\n[18] Arie Van Deursen, Leon Moonen, Alex Van Den Bergh, and Gerard Kok. 2001. Refactoring test code. In Proceedings of the 2nd international conference on extreme programming and flexible processes in software engineering (XP). 92–95.\n\n[19] Bart Van Rompaey, Bart Du Bois, Serge Demeyer, and Matthias Rieger. 2007. On the detection of test smells: A metrics-based approach for general fixture and eager test. IEEE Transactions on Software Engineering 33, 12 (2007), 800–817.",
      "page_number": 9
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": ".\n\nPDF Download 3425174.3425212.pdf 25 December 2025 Total Citations: 21 Total Downloads: 316\n\nLatest updates: hps://dl.acm.org/doi/10.1145/3425174.3425212\n\nPublished: 20 October 2020\n\nRESEARCH-ARTICLE Refactoring Test Smells: A Perspective from Open-Source Developers\n\n.\n\nCitation in BibTeX format\n\n.\n\n.\n\nELVYS SOARES, Federal University of Pernambuco, Recife, PE, Brazil\n\nCurrently researching test smells, their eﬀects on automated tests, and strategies to mitigate or eliminate them. Also, researching mutation analysis, operators in specific contexts, and tool support.\n\nSAST 20: 5th Brazilian Symposium on Systematic and Automated Soware Testing October 20 - 21, 2020 Natal, Brazil\n\n.\n\n.\n\nMÁRCIO RIBEIRO, Federal University of Alagoas, Maceio, AL, Brazil\n\nGUILHERME AMARAL, Federal University of Alagoas, Maceio, AL, Brazil\n\nROHIT GHEYI, Federal University of Campina Grande, Campina Grande, PB, Brazil\n\nLEONARDO FERNANDES, Federal Institute of Alagoas, Maceio, AL, Brazil\n\nALESSANDRO FABRICIO GARCIA, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, RJ, Brazil\n\nView all\n\nOpen Access Support provided by:\n\nFederal Institute of Alagoas\n\nFederal University of Alagoas\n\nFederal University of Campina Grande\n\nFederal University of Pernambuco\n\nPontifical Catholic University of Rio de Janeiro\n\nSAST '20: Proceedings of the 5th Brazilian Symposium on Systematic and Automated Soware Testing (October 2020) hps://doi.org/10.1145/3425174.3425212 ISBN: 9781450387552",
      "content_length": 1485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "Refactoring Test Smells: A Perspective from Open-Source Developers\n\nElvys Soares∗ Federal University of Pernambuco Recife, Brazil eas5@cin.ufpe.br\n\nMárcio Ribeiro Federal University of Alagoas Maceió, Brazil marcio@ic.ufal.br\n\nGuilherme Amaral Federal University of Alagoas Maceió, Brazil gvma@ic.ufal.br\n\nRohit Gheyi Federal University of Campina Grande Campina Grande, Brazil rohit@dsc.ufcg.edu.br\n\nLeo Fernandes Federal Institute of Alagoas Rio Largo, Brazil leonardo.fernandes@ifal.edu.br\n\nAlessandro Garcia Pontifical Catholic University of Rio de Janeiro Rio de Janeiro, Brazil afgarcia@inf.puc-rio.br\n\nBaldoino Fonseca Federal University of Alagoas Maceió, Brazil baldoino@ic.ufal.br\n\nAndré Santos Federal University of Pernambuco Recife, Brazil alms@cin.ufpe.br\n\nABSTRACT Test smells are symptoms in the test code that indicate possible de- sign or implementation problems. Their presence, along with their harmfulness,hasalreadybeendemonstratedbypreviousresearches. However, we do not know to what extent developers acknowledge the presence of test smells and how to refactor existing code to eliminate them in practice. This study aims to assess open-source developers’ awareness about the existence of test smells and their refactoring strategies. We conducted a mixed-method study with two parts: (i) a survey with 73 experienced open-source develop- ers to assess their preference and motivation to choose between 10 different smelly test code samples, found in 272 open-source projects, and their refactored versions; and (ii) the submission of 50 pull requests to assess developers’ acceptance of the proposed refactorings. As a result, most surveyed developers preferred the refactored proposal for 78% of the investigated test smells, and the pull requests had an average acceptance of 75% among respon- dents. Additionally, we were able to provide empiric validation for literature-proposed refactoring strategies. This study demonstrates that although not always using the academic terminology, develop- ers acknowledge both the negative impact of test smells presence and most of the literature’s proposals for their removal.\n\nCCS CONCEPTS • Software and its engineering → Software testing and de- bugging; Empirical software validation.\n\nKEYWORDS Test Smells, Refactoring, Open Source, Validation\n\nACM Reference Format: Elvys Soares, Márcio Ribeiro, Guilherme Amaral, Rohit Gheyi, Leo Fernan- des, Alessandro Garcia, Baldoino Fonseca, and André Santos. 2020. Refac- toring Test Smells: A Perspective from Open-Source Developers. In 5th Brazilian Symposium on Systematic and Automated Software Testing (SAST ’20), October 20–21, 2020, Natal, Brazil. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3425174.3425212\n\n1 INTRODUCTION Automated software testing and the development of test scripts are frequent in the software industry [6]. A wide range of tools and frameworks for these activities, whether in support of creating auto- matic tests or the complete automatic generation of these, is already available [3, 9]. Automated test suites with high internal quality facilitate maintenance activities, such as code understanding and regression testing. However, the test code’s development is tedious, error-prone, and requires a significant initial investment [1].\n\nPoorly designed test code is frequent in practice [1], and the symptoms that indicate possible design problems in the test code are called bad test smells, or just test smells [6, 18]. These smells are not the same as those defined by Fowler [2] for the production code. As they are based on test cases organization, implementation, and interaction with one another, their corresponding refactoring operations generally differ from production code.\n\n∗The author is also affiliated to the Federal Institute of Alagoas, Maceió, Brazil, available at elvys.soares@ifal.edu.br\n\nACMacknowledgesthatthiscontributionwasauthoredorco-authoredbyanemployee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SAST ’20, October 20–21, 2020, Natal, Brazil © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8755-2/20/09...$15.00 https://doi.org/10.1145/3425174.3425212\n\nAs a motivating example, Figure 1 presents a code snippet1 ex- tracted from the core module of the AssertJ project, self-described as “a set of strongly-typed assertions to use for unit testing with any test automation framework that is very close to plain English”. This test intends to verify a specific error and its specific properties. One can note the presence of a try/catch structure in the test and commands on lines 9 and 11 to handle the test outcome manually. According to Peruma et al. [11], manually passing or failing a test\n\n1Original source code available at https://git.io/JJ1gA\n\n50",
      "content_length": 4931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "SAST ’20, October 20–21, 2020, Natal, Brazil\n\ndepending on whether the production code throws an exception to a custom exception handling code in a test is expendable once the test framework already provides exception handling features to that end.\n\nFigure 1: Example of the Exception Handling test smell\n\nLater, we observed a test code refactoring2 using both AssertJ new features and Java 8 lambda expressions that made the excep- tion test more transparent and eliminated the previous need for manually dealing with errors, and manually passing or failing the test. Figure 2 presents the refactoring performed by the AssertJ team, eliminating the fragility of the original design.\n\nFigure 2: Refactoring the Exception Handling test smell\n\nThe provided example reflects what a test smell means: a symp- tom in the test code that may (or may not) indicate a design problem. The subsequent refactoring action demonstrated awareness of the design problem and implemented a coding strategy to eliminate it. Although numerous previous studies have already demonstrated the presence of test smells in test suites of open-source projects, as well as their negative impact on code maintenance and understand- ing activities [1, 9–11, 19], there is a lack of studies to assess the awareness of open-source developers about the existence of tests smells and their possible solutions.\n\nThus, this study presents two empirical investigations that aim\n\nto answer the following research questions:\n\n2Refactored source code available at https://git.io/JJ1Vn\n\n51\n\nSoares, et al.\n\nRQ1: To what extent test smells are relevant to open- source developers?\n\nRQ2: To what extent open-source developers accept refactorings that remove test smells?\n\nThe first empirical study is a survey with open-source devel- opers to investigate RQ1 by considering questions such as “Are open-source developers aware of test smells and recognize smelly test code?” and “Do they know how to prevent or refactor a test smell?”. This study consisted of showing smelly test code snippets that we gathered from open-source projects, along with an alternate refac- tored version of the same test code. Then we asked developers to choose which version they preferred and justify their answers.\n\nThe second empirical study aimed at answering RQ2 and ques- tions such as “Do they acknowledge the existence of test smells in their projects?” consists of submitting contributions (Pull Requests) with test code refactorings to active open-source projects and collect whether the developers of such projects will accept the contribu- tions, along with considerations to any possible discussion topics thatmayemergefromthecontributions.WehavesentPullRequests to 50 different open-source projects and, among respondents, we achieved a 75% acceptance rate, having most contributions being accepted with very little further discussions.\n\nIn summary, this paper provides the following contributions:\n\nA survey with experienced open-source developers that as- sessed their impressions about test smells and their refactor- ing strategies in Section 3;\n\nAn empirical study based on pull requests to better under- standtheacceptancelevelofopen-sourcedeveloperstorefac- torings of test smells in their projects, along with an empiri- cal validation of 10 literature-proposed refactoring strategies in Section 4;\n\n2 REFACTORING TEST SMELLS We now present a brief explanation of the test smells we used in this study, the strategies to eliminate them and the related code snippets (simplified as toy examples). According to the test smell distribution presented by Peruma et al. [11], these test smells figure between the highest available ones on open-source projects:\n\nFigure 3: Conditional Test Logic\n\nConditional Test Logic: Defined by Meszaros [6], this test smell occurs when the test depends on a condition. It is a dangerous design since a test method may result in a passed status without",
      "content_length": 3940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Refactoring Test Smells: A Perspective from Open-Source Developers\n\never having asserted a unit result. Palomba et al. [10] propose refac- toring this test smell with an “Extract Method” [2] operation, which we believe to be more suitable in cases of nested loops and decisions. As our example uses a decision structure to verify a precondition, we added an assumption to the precondition, which will make the test to be ignored if not met. Figure 3 presents both the original and the refactored versions.\n\nAssertion Roulette: It is defined by van Deursen et al. [18] as a collection of unexplained assertions in a single test method that makes it difficult to trace which exact assertion had a problem in the event of test failure. This test smell has the original refactoring suggestion of “Add Assertion Explanation”, and an alternative sug- gestion to break up the test into a suite of “Single-Condition Tests” proposed by Meszaros [6]. As a single-condition test is actually a good programming practice that increases code coverage, we opted to use this refactoring strategy. Figure 4 presents the original program with the Assertion Roulette and its refactored version.\n\nFigure 4: Assertion Roulette\n\nException Handling: According to Peruma et al. [11], this smell occurs when the test manually handles both exceptions and test outcome and its frequency rivals with Assertion Roulette on Java projects. Figure 5 presents the first case of this test smell, along with its refactoring, when properties after an exception must be verified. Figure 6 presents the case 2 of the Exception Han- dling test smell, that occurs when an exception message needs to be verified along with an alternate version that uses JUnit frame- work features. Figure 7 presents case 3, along with an alternate refactored option, which happens when some final steps must be executed independently of the test result or the existence of any exceptions. Figure 8 presents case 4, which tests for a specific ex- ception being raised. The JUnit framework recommends the use of @Test(expected=MyException.class) annotation in this case.\n\n52\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\nFigure 5: Case 1 of Exception Handling\n\nFigure 6: Case 2 of Exception Handling",
      "content_length": 2241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "SAST ’20, October 20–21, 2020, Natal, Brazil\n\nFigure 7: Case 3 of Exception Handling\n\nFigure 8: Case 4 of Exception Handling\n\nSensitive Equality: van Deursen et al. [18] state the agility and ease to write equality checks using the toString() method, being a frequent alternative to calculate a result value and map it to a sequence to compare to a literal string representing the expected value. However, these tests can depend on many irrelevant details, suchas commas,quotes,and spaces.And whenever the toString() method for an object is changed, all related tests will start to fail. The proposed solution to this test smell is to replace the equality checks that use the toString() method with real equality checks\n\n53\n\nSoares, et al.\n\nusing the “Introduce Equality Method” refactoring strategy [18]. Figure 9 presents an example of Sensitive Equality, along with a refactored alternative.\n\nFigure 9: Sensitive Equality\n\nFigure 10: Magic Number Test\n\nMagic Number Test: Proposed by Peruma et al. [11], this smell occurs when the test method contains undocumented numeric",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Refactoring Test Smells: A Perspective from Open-Source Developers\n\nliterals with no clear meaning (magic values). To refactor this test smell, the authors state that these numeric values can be replaced with constants or variables, thus providing a descriptive name for the value. This smell is presented in Figure 10.\n\nResource Optimism: According to van Deursen et al. [18], this smell happens when test methods make optimistic assumptions about the existence or the state of external resources like files and databases. Such assumptions may cause non deterministic behavior on the outcome and can be refactored through the “Setup External Resource”, also defined before [18]. Figure 11 presents our example of Resource Optimism and the corresponding refactoring operation.\n\nFigure 11: Resource Optimism\n\nTest Code Duplication: Also proposed van Deursen et al. [18], this test smell normally identifies classes that contain test methods with repeated test code steps. Typically, parts that set up tests fixtures are normally related to this test smell. The authors state that, for cases where the duplicated code is in the same class, this test smell can be eliminated using “Extract Method”, defined by Fowler [2], as it is for normal code duplication. Figure 12 presents our example, as well as its refactored version.\n\nThe next sections present the empirical investigations we per- formed in this study. The replication package containing all the raw data of both investigations can be found at the companion website3.\n\n3 SURVEY WITH DEVELOPERS In this section, we present our first empirical study performed with open-source developers.\n\n3https://bit.ly/2EeL7Io\n\n54\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\nFigure 12: Test Code Duplication\n\n3.1 Planning This survey aimed to analyze implementation preferences concern- ing the presence of test smells and their refactoring identification, from the viewpoint of open-source developers, as stated by RQ1: To what extent test smells are relevant to open-source devel- opers?\n\nThe general idea was to ask developers for their preference between two implementations of equivalent unit tests. One of these implementations was always an original (smelly) code snippet, and the other one was a refactored alternative, following the refactoring principles of Murphy-Hill, Parnin, and Black [7].\n\n3.2 Settings To assemble the survey, we needed a list of test smells from open- source projects. As there is no public list of such smells, we had to search and use a tool to analyze projects from public repositories. Although the most comprehensive survey to the date [3] lists 12 tools that handle test smells, the most recent and complete one is the TSDetect [11] tool proposed by Peruma et al. [11]. It detects 19 different test smells and analyzes JUnit test cases, creating detailed execution logs. Using the GitHub search, we retrieved the projects list by classifying Java projects in descending order of stars and executed the TSDetect [11] in the listed results. According to the tool execution logs, we achieved a test smell distribution result consistent with that reported by Peruma et al. [11] with the first 272 results.\n\n3.3 Design Intending to have as many respondents as possible, we defined the surveytotakeanaverageof10minutesofthedevelopers’time.This way, we searched for suitable test methods that (i) had few lines of code and could be displayed along with the refactored version without the need for scrolling the screen, and (ii) had an identifiable test smell, albeit we did not stress if the code had any problem, nor highlighted it in any way.",
      "content_length": 3621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "SAST ’20, October 20–21, 2020, Natal, Brazil\n\nThe pilot execution of the survey revealed a significant concern about the displayed code snippets: developers kept trying to un- derstand project specificities, taking much longer than anticipated, and not noticing the problems we wanted them to analyze. To miti- gate this issue, we reduced the code snippets to toy examples, as presented in the code examples from Section 2.\n\nThrough scripts, we retrieved the contributors of open-source Java projects who had public contact info available and contributed to test activities within their projects. Our scripts analyzed about 600 projects, and a total of 2,011 invitation emails were sent.\n\nAlong with questions concerning demographics and the develop- ers’ expertise in software testing, we presented 9 of the test smells examples presented in Section 2, only excluding the Exception Han- dling 4, which is a simple framework usage, as we intended the survey to last the least possible time. The test smells, and their refactoring options were displayed as A/B questions, which were presented in random order to every developer, who could choose from the following list of options: (i) I strongly prefer A, (ii) I prefer A, (iii) Indifferent, (iv) I strongly prefer B, (v) I prefer B, and (vi) I do not know. All questions were mandatory and provided an optional field for the developers to justify their answers.\n\n3.4 Results and Discussion We performed the survey in June 2020, achieving a response rate of 3.63% with the total number of 73 responses. Concerning the demo- graphics, area of work, and expertise questions, we had participants working in 26 countries, 89% of them defined their primary area of work as the industry (over academia), 68.5% declared to have 10+ years of experience with software testing. This number rises to about 87.5% if we consider 5+ years of experience. The average of their daily time spent on testing activities is about 41%. Con- cerning the A/B questions, Table 1 presents the obtained results, in percentage.\n\nTest Smell\n\nOriginal Refactored Other\n\nAssertion Roulette Conditional Test Logic Exception Handling 1 Exception Handling 2 Exception Handling 3 Magic Number Test Resource Optimism Sensitive Equality Test Code Duplication\n\n12.3% 12.3% 24.7% 38.4% 26.0% 31.5% 27.4% 37.0% 31.5%\n\n65.8% 64.4% 68.5% 37.0% 68.5% 54.8% 49.3% 32.9% 49.3%\n\n21.9% 23.3% 6.8% 24.7% 5.5% 13.7% 23.3% 30.1% 19.2%\n\nTable 1: Survey results\n\nIn order to better interpret the obtained results, both numeric and comment analyses are fundamental. While the numeric results indicate whether developers accept a proposed refactoring method, the comment analysis tells us if they are doing so because they correctly recognize the possible flaw in the test code.\n\nThe Assertion Roulette result showed that although the provided refactoringwasnottheoriginallyproposedone,wherevanDeursen et al. [18] suggests commented assertions, most developers prefer\n\n55\n\nSoares, et al.\n\nto separate assertions into single test cases. The provided comments reinforce their choices: “It tests different methods on a class so if one of them fails it is easy to see which one by looking at test results” and “(refactored) is more code, but it will give more fine-grained reporting when there is a failure.”.\n\nThe Conditional Test Logic had results consistent with the Asser- tion Roulette. Although most developers claimed both tests not to be equivalent because the original test would pass without testing all scenarios, they seemed to understand the intent of the condition: “(Original) suggests that b is somehow an assumption that must be satisfied for executing the rest of the test (I would do that differently, though)”.\n\nThe first and third cases of Exception Handling had their refac- tored versions preferred by most developers. The most common argument was using the test framework resources and the improve- ments in code readability and maintainability. Regarding the second case of the Exception Handling test smell, where an exception mes- sage needed to be tested, although comments about the existence of try/catch blocks in test code followed the other Exception Handling cases, the provided refactoring implementation using the Rule an- notation divided opinions. A frequently provided reason is given in the comment “I am not familiar with this ‘@Rule’. The code looks prettier (which in general means more maintainable), but the ordering is a bit counter-intuitive” reinforce our statement.\n\nDevelopers well recognized the Magic Number Test Smell. Some of them argued that the original test code would be acceptable if the test method is unique. Comments like “Using well-named constants enhances readability when comparing with magic numbers usage” demonstrate that developers value code semantics.\n\nAlthough the refactored version of the Resource Optimism test smell was the choice of almost half of the developers, most com- ments concerned “Using a @Before method allows reuse by multi- ple tests.”, which indirectly solves the test smell. Fewer comments stressed the need to have external resources instantiated separately from the test steps, as in “Allows me to distinguish between failures in setting up tests vs. the actual test I want to perform”.\n\nConcerning the Sensitive Equality test smell, as the distribution of choices was slightly balanced between the options, it is possi- ble to find (i) critics to the use of the toString() method: “The ‘toString’ method is not always a reliable indicator of object state.”, (ii) critics to the equality check method: “The equality check method seems to be created for the test: the test should test the real behavior of the object” and (iii) critics to the example design: “Semantics of equalityCheck are not at all obvious”. The comments analysis demon- strates that most developers recognize the used of toString() as problematic, but disagree whether an equality check method is the best refactoring strategy. We also believe reducing a toy example could not evidence the test smell to all developers, causing them difficulties understanding the root problem. Maybe a more suit- able example would make the 30% of developers who opted for “Indifferent” and “ I do not know” to choose more accurate opinions. Almost half of the developers selected the refactored Test Code Duplication. They highlight the need to avoid duplicate test code and good practices as the use of DRY4 principles. The other groups\n\n4The Don’t Repeat Yourself principles were defined by Thomas and Hunt [16]",
      "content_length": 6555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Refactoring Test Smells: A Perspective from Open-Source Developers\n\nkept elaborating whether the use of an “Extract Method” [2] in such a small example would be over-engineering.\n\nSummary:Evenwhendevelopersdidnotchoosetherefac- tored version, they correctly indicated the code fragility that motivates the test smell definition and sometimes the refactoring fragility. Therefore we conclude that open- source developers acknowledge test smells refactoring strategies, positively answering RQ1.\n\n3.5 Threats to Validity The primary construct validity threat is related to creating toy examples from both original and refactored code snippets, which may have resulted in confusing or over-engineered toy examples that could lead developers to misjudgment. Despite having the survey presentation page with highlights on the simplified, but real open-source examples, and the process of simplification we submitted the real code snippets to, we could find comments to answers considering the toy examples as literal ones. As internal threats, the code examples we collected from open-source projects in the early elaboration stage of the survey may not be the best representatives of the intended test smells and their refactoring strategies, or cover all possible test smells scenarios. The Exception Handling 4, presented in Section 2, which was not part of the survey, although being a relatively simple case, is an excellent example of a lack of complete coverage. As external threats, this study evaluated 7 test smells and their refactoring strategies. While broad surveys like Garousi and Küçük [3] and Peruma [13] have, combined, cataloged, and proposed more than 130 different test smells, generalizing our results to a larger group may not be a valid assumption.\n\n4 SUBMISSION OF PULL REQUESTS This section describes the second empirical investigation performed in this study, where contributions to open-source projects were made to assess developers’ opinions about test smell refactorings.\n\n4.1 Planning Our second empirical study aims to analyze contributions to open source projects regarding the acceptance to test smell refactorings from the viewpoint of the project maintainers, as stated by RQ2: To what extent open-source developers accept refactorings that remove test smells?\n\nWe assume that the acceptance rate of the pull requests might corroborate that test smell refactoring is relevant. In this study, we also focused on the 7 test smells and 10 refactoring strategies presented in Section 2.\n\n4.2 Settings Starting from the TSDetect [11] execution logs acquired in the first study, we manually searched for test smells that fit the generic tem- plates defined by the toy examples. Once a test smell was identified, the same refactoring strategy used in the developers’ survey was applied.\n\n56\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\nAs explained, for each system, TSDetect [11] reports test smells through its execution logs (Step 1). Because we execute TSDe- tect [11] considering the entire project test code, we could find several test smells (even the ones we are not considering in this study). This way, for each project, we randomly selected an iden- tified test smell. The intention was to assess the developers’ ac- ceptance of that specific smell. Besides, to minimize bias on pull request acceptance from the same developer, we decided to submit only one contribution per project. Then, given the selected test smell, we applied a proper refactoring (see Section 2) and submitted a pull request (Step 2). Notice that we neither fix bugs nor submit new test cases. We focused on submitting that one specific test smell refactoring. In addition to sending the pull request, we also submitted a comment justifying the transformation that, unless stated differently by specific projects submission rules, followed the pattern of presenting the definition of the problem, the description of the refactoring strategy (proposed solution), and the before and after (code snippets) changes. When necessary, we discussed with the developers to defend our submission before deciding to accept or reject the pull request (Step 3).\n\n4.3 Results and Discussion Although we have submitted 50 pull requests, we were limited by the template models we defined in the first study. Also, many difficulties were encountered regarding dependencies on projects’ environments, which forced us to abandon submitting some pull re- quests. As we explained, we submitted one pull request per project, which leads us to 50 open-source projects involved in this part of the study. Table 2 presents the pull requests submission distribution by test smell type.\n\nTest Smell Type\n\nSubmitted Accepted Rejected\n\nAssertion Roulette Resource Optimism Conditional Test Logic Exception Handling (all) Magic Number Test Sensitive Equality Test Code Duplication\n\n8 6 9 12 8 2 5\n\n50% 16% 44% 50% 12.5% 50% 20%\n\nN/A 33% N/A 8% 25% 50% N/A\n\nTotal:\n\n50\n\nTable 2: Pull Requests by Test Smell type\n\nUntil submitting this study, we received 18 acceptances, 6 re- jections, and noticed 4 submission errors. The average time the respondent projects took to analyze a pull request was two busi- ness days. We had no projects that contested the submissions and required long, convincing dialogues. However, we had one occur- rence (Conditional Test Logic) that investigated, on project details, therealnecessityoftheconditionaltestlogicandendedupagreeing with the refactoring proposal.\n\nTable 3 presents the pull requests we submitted in this study. It also provides additional project details and summarizes the devel- opers’ comments.",
      "content_length": 5626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "SAST ’20, October 20–21, 2020, Natal, Brazil\n\nNo Project Name\n\n1 ffmpeg-cli-wrapper gumtree 2 google-maps-services-java 3 dropwizard 4 docx4j 5 jvm-tools 6 7 lavagna 8 mango 9 mybatis-3 pi4j 10 pippo 11 Fenzo 12 fluent-validator 13 realm-android-adapters 14 15 zxing-android-embedded 16 wiremock 17 18 19 20 21 22 23 24 Apktool rest-client 25 bitcoinj 26 27 RxCache blade 28 size-analyzer 29 30 bt 31 RegexGenerator bugsnag-android 32 bundletool 33 34 cordova-android 35 Conductor 36 37 38 webcam-capture unirest-java 39 testcontainers-java 40 client_java 41 compile-testing 42 scribejava 43 eclipse-collections 44 spark 45 sofa-bolt 46 stetho 47 48 binding-collection-adapter 49 AndroidNetworkTools 50\n\ngerrit-intellij-plugin JSCover pac4j disruptor hutool spring-boot-thin-launcher assertj-core\n\ncommonmark-java tablesaw\n\njava-dns-cache-manipulator\n\nSW Category Test Smell Type\n\nStatus\n\nApp Framework Library Framework Library App App Framework Framework Library Framework Library App Android App Library App Plugin App App Library Library Framework Framework App App Library Library Framework Tool Framework Library Library Tool Framework Framework Library Library Library Library Library App Tool Library Framework Library Framework App Library Library Library\n\nResource Optimism Assertion Roulette Exception Handling Conditional Test Logic Accepted Open Assertion Roulette Accepted Assertion Roulette Accepted Resource Optimism Open Assertion Roulette Submission Error Conditional Test Logic Exception Handling Accepted Conditional Test Logic Accepted Exception Handling Assertion Roulette Exception Handling Assertion Roulette Magic Number Assertion Roulette Exception Handling Exception Handling Magic Number Assertion Roulette Conditional Test Logic Open Resource Optimism Resource Optimism Resource Optimism Sensitive Equality Conditional Test Logic Open Open Magic Number Resource Optimism Submission Error Conditional Test Logic Open Test Code Duplication Open Conditional Test Logic Accepted Conditional Test Logic Accepted Test Code Duplication Open Test Code Duplication Open Test Code Duplication Open Sensitive Equality Magic Number Magic Number Magic Number Exception Handling Exception Handling Exception Handling Conditional Test Logic Open Open Magic Number Open Exception Handling Open Magic Number Exception Handling Accepted Test Code Duplication Accepted Exception Handling\n\nSubmission Error Open Submission Error\n\nOpen Accepted Open Open Open Accepted Accepted Accepted Rejected Accepted\n\nRejected Rejected Open Rejected\n\nAccepted Open Accepted Rejected Accepted Rejected Accepted\n\nOpen\n\nTable 3: Submitted pull requests\n\n57\n\nSoares, et al.\n\nPull Request Comments\n\n“Wrong usage of project classes”\n\n“Wrong pull request format” “Thanks for your contribution”\n\nNo Comments “Thanks for your contribution”\n\n“Wrong usage of project classes” “Thanks for your contribution” “Thanks for your contribution”\n\n“Thanks for your contribution”\n\n“Thanks for your contribution” “Thanks for your contribution” “Thanks for your contribution” “Test is harder to understand” “Thanks for your contribution”\n\n“ Current test behavior preferred” “Current test behavior preferred”\n\n“Wrong Java API usage”\n\n“Wrong pull request format”\n\n“Thanks for your contribution” “Thanks for your contribution”\n\nNo Comments\n\n“Thanks for your contribution” “Test is harder to understand” “Thanks for your contribution” “Discouraged test framework feature” “Unaware of test framework feature” “Rejected test framework feature”\n\nNo Comments No Comments",
      "content_length": 3535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Refactoring Test Smells: A Perspective from Open-Source Developers\n\nRegarding the pull requests’ comments, it is vital to notice that most of the accepted pull requests were made with a simple “Thanks for your contribution” message or no message at all. From this fact, we understand that the developers accepted both the presence of the test smell and the refactoring strategy.\n\nAs to the rejected pull requests, four different test smells are on the list: Magic Number Test (2), Sensitive Equality (1), Resource Optimism (2) and Exception Handling (1).\n\nIn the rejected Magic Number Tests, the first developer claimed that“Hard-codednumbersaren’tautomaticallybad.” and“Thischange makes the tests harder to understand.”, and closed the pull request after that. Many of the numeric literals inserted directly into the code are used, for example, to retrieve the first position of an array, or to assert the size of a known array. Such numbers are not difficult to read and maintain in the code, and this coding practice seems to be widely used in the analyzed open-source projects. Replacing them with constants or variables may add unnecessary text. The second developer argued that “200 is a well-known status code” and “ I don’t think changing it would make any sense.”. Although we had accepted pull requests to this same case, well-known response codes may be an argument in favor of the magic numbers.\n\nOther rejected pull request was related to the Sensitive Equal- ity test smell. In this case, the developer arguments that “The toString() / fromString() API is actually the recommended API to use if you deal with addresses.”. Indeed, if an API is widely used to retrieve a text value from an object using the toString() method, then defining a new method that always returns the same value as toString() may not be necessary.\n\nTwo rejected pull requests were related to the Resource Opti- mism test smell. Both developers did not want the failing assump- tion of the external resource, which was not the software unit under test, to ignore the test execution. They preferred the original code, which would fail if the resource is not available.\n\nThe last rejection is related to the Exception Throwing test smell. The response given by the developer was “we actually discourage the use of @Test(expected = ...) in general.” and “a test that uses @Test(expected = ...)” will pass if any expression in the test throws an exception of the expected type, even if you only expect the exception from a particular call. It is a good rationale but might apply mostly to integration tests, which are frequently populated with several steps and verifications.\n\nThe four submission errors were due to the lack of complete un- derstanding of the project architecture and the software unit under test, or even the project rules for pull requests, which generated bad submissions.\n\nSummary: Even when a pull request was refused, the discussion with developers showed that they correctly identify the general “issue” we were referring to and were able to discuss why the test smell did not apply to their projects. The 75% acceptance, mostly without discussion, leads us to conclude that open-source developers accept refactorings test smells in their projects, thus positively answering RQ2.\n\n58\n\nSAST ’20, October 20–21, 2020, Natal, Brazil\n\n4.4 Threats to Validity The construct validity relates to the refactoring templates presented in Section 2, which were not extended to all scenarios within a stud- ied test smell type. A clear example of such an untreated template is the Conditional Test Logic cases that nest their assertions with repetition and decision structures and did not receive pull requests in this study. As internal validity, the submitted pull requests by test smell type may not be enough to generalize results per studied test smell. Concerning external validity, as we worked with almost the same test smells and refactorings we used in the survey with developers, this group of test smells may not be representative of generalizing results to the test smells we did not cover in this study.\n\n5 RELATED WORK Palomba et al. [8] performed a survey on the developer’s perception of bad code smells. They showed production code snippets from three software systems to original and outside (students and indus- try) developers, asking them to indicate if they found any potential design problems and what nature and severity level they would classify the possible problems. As a result, they could divide the investigated code smells into categories related to code complexity and coding good-practices. Our study offers a similar perception by open-source developers, this time concerning test smells.\n\nThe study performed by Bavota et al. [1] demonstrated test smell distributioninsoftwaresystemsandwhethertheirpresenceisharm- ful. As part of the investigation, they performed a controlled experi- ment involving 61 participants among students (freshers, bachelors, and masters) and industrial developers, which were asked to per- form maintenance activities on smelly and refactored test code of two software systems. The study demonstrated the negative impact of test smells in program comprehension during maintenance ac- tivities. Our study uses both open-source developers and projects to survey on test smells perceptions.\n\nTufano et al. [17] investigation aimed at analyzing when test smells occur in source code, what their survivability is, and whether their presence is associated with the presence of design problems in production code (code smells). They collected the developers’ perception of test smells in a study with 19 developers from Apache and Eclipse ecosystems. They demonstrated that (i) test smells have a long life cycle in software systems, and (ii) there are correlations between test and code smells. Our study extends the surveyed public in order to improve the accuracy of developers’ perception. Lambiaseetal.[5]presentedtheDARTS(DetectionAndRefactor- ing of Test Smells) tool. It detects instances of three test smell types (General Fixture, Eager Test, and Lack of Cohesion of Test Methods) at the commit-level and enables their automated refactoring using the refactoring techniques defined by Meszaros [6]. The study pro- poses refactorings based on previously-proposed strategies. Our study validates the same strategies using developers’ opinions.\n\nA work in progress performed by Schvarcbacher et al. [14] aimed to assess the perception of developers on test smells in their code- base and which ones they would consider being more important. They present the developers’ reactions to various instances of test smells pointed out by their detection tool, based on the TSDe- tect [11], in integration with the Better Code Hub (BCH).5 Their\n\n5https://bettercodehub.com/",
      "content_length": 6825,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "SAST ’20, October 20–21, 2020, Natal, Brazil\n\nresults show that developers are only willing to remove a small portion of the found test smells, but did not present them with refactored alternatives to measure their acceptance.\n\nThe study performed by Spadini et al. [15] aimed at investigating the severity rating for four test smells and their perceived impact on test suite maintainability by the developers. They collected test smells from 1,500 open-source projects and used in-company devel- opers’ perceptions to calibrate the established severity thresholds for test smells. As results, they found that current detection rules for certain test smells are considered too strict by the developers and verified that their newly defined severity thresholds are in line with the participants’ perception of how test smells impact the maintainability of a test suite.\n\nAlthough researching refactoring operations, Peruma et al. [12] used a mining tool to detect refactoring operations from an existing dataset of unit test files and smells in 250 open-source Android Apps. Among the results, they could demonstrate that the types of refactorings applied to test and non-test files in Android apps are different and that there exist test smells and refactoring oper- ations that co-occur frequently. More importantly, and following the results of our study, they could verify that developers apply refactorings for reasons other than the correction of smells, making the fixing of smells merely a byproduct.\n\nSilva Junior et al. [4] carried out an expert survey to analyze the usage frequency of a set of test smells, aiming to understand whether test professionals non-intentionally insert test smells. Us- ing an observation by case-control study design, they evaluated the answers of 60 industry developers on 14 test smells. As a result, they found that developers introduce test smells during their daily pro- gramming tasks, even when using their companies’ standardized practices.\n\n6 CONCLUSIONS In this paper, we conducted two empirical studies regarding open- source developers’ awareness about test smells and the refactoring strategies to mitigate them.\n\nIn the first study, we surveyed 73 developers, who analyzed 10 test smells from real open-source projects and their refactored versions. The answers and their justifications led us to conclude that even when developers disagree with the proposed refactorings, they are aware of the threats proposed by the smelly code. This study was also able to provide empirical validation to most of the literature-proposed refactoring strategies we used in the assessed test smells.\n\nInthesecondstudy,wesubmitted50PullRequeststoopensource projects whose tests had at least one of the 7 test smells we inves- tigated in the previous study to verify if the developers accepted our contributions. The 75% acceptance rate among respondents indicates that developers are also prone to accept contributions that solve test smells in their test code.\n\nOur findings corroborate the idea that not every test smell repre- sents a problem to the developers, and their corresponding refactor- ing strategies may not be preferred in most cases, which encourages further study on whether specific test smells are meaningful to de- velopers.\n\n59\n\nSoares, et al.\n\nAs future work, we intend to (i) increase the list of developers’ validated test smells and their refactoring strategies, (ii) investi- gate if new test framework features could be successfully used in such refactorings, and (iii) investigate if any test smell has become obsolete due to new test framework features or properties.\n\nACKNOWLEDGMENTS This work was partially funded by CAPES (117875), CNPq (421306 /2018-1, 309844/2018-5, 426005/2018-0 and 311442/2019-6), and FA- PEAL/FAPESP (60030.0000000462/2020) grants.\n\nREFERENCES [1] Gabriele Bavota, Abdallah Qusef, Rocco Oliveto, Andrea De Lucia, and Dave Binkley. 2015. Are test smells really harmful? An empirical study. Empirical Software Engineering 20, 4 (2015), 1052–1094.\n\n[2] Martin Fowler. 2018. Refactoring: improving the design of existing code. Addison-\n\nWesley Professional.\n\n[3] Vahid Garousi and Barış Küçük. 2018. Smells in software test code: A survey of knowledge in industry and academia. Journal of Systems and Software 138 (2018), 52–81.\n\n[4] Nildo Silva Junior, Larissa Rocha, Luana Almeida Martins, and Ivan Machado. 2020. A survey on test practitioners’ awareness of test smells. arXiv:2003.05613 [cs.SE]\n\n[5] Stefano Lambiase, Andrea Cupito, Fabiano Pecorelli, Andrea De Lucia, and Fabio Palomba. 2018. Just-In-Time Test Smell Detection and Refactoring: The DARTS Project. (2018).\n\n[6] Gerard Meszaros. 2007. xUnit test patterns: Refactoring test code. Pearson Educa-\n\ntion.\n\n[7] Emerson Murphy-Hill, Chris Parnin, and Andrew P Black. 2011. How we refactor, and how we know it. IEEE Transactions on Software Engineering 38, 1 (2011), 5–18.\n\n[8] F. Palomba, G. Bavota, M. D. Penta, R. Oliveto, and A. D. Lucia. 2014. Do They Really Smell Bad? A Study on Developers’ Perception of Bad Code Smells. In 2014 IEEE International Conference on Software Maintenance and Evolution. 101–110. [9] Fabio Palomba, Dario Di Nucci, Annibale Panichella, Rocco Oliveto, and Andrea DeLucia.2016. Onthediffusionoftestsmellsinautomaticallygeneratedtestcode: An empirical study. In 2016 IEEE/ACM 9th International Workshop on Search-Based Software Testing (SBST). IEEE, 5–14.\n\n[10] Fabio Palomba and Andy Zaidman. 2019. The smell of fear: On the relation between test smells and flaky tests. Empirical Software Engineering 24, 5 (2019), 2907–2946.\n\n[11] Anthony Peruma, Khalid Almalki, Christian D Newman, Mohamed Wiem Mkaouer, Ali Ouni, and Fabio Palomba. 2019. On the distribution of test smells in open source Android applications: an exploratory study. In Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering. IBM Corp., 193–202.\n\n[12] Anthony Peruma, Christian D Newman, Mohamed Wiem Mkaouer, Ali Ouni, and Fabio Palomba. 2020. An Exploratory Study on the Refactoring of Unit Test Files in Android Applications. In Conference on Software Engineering Workshops (ICSEW’20).\n\n[13] Anthony Shehan Ayam Peruma. 2018. What the Smell? An Empirical Investigation on the Distribution and Severity of Test Smells in Open Source Android Applications. Ph.D. Dissertation.\n\n[14] Martin Schvarcbacher, Davide Spadini, Magiel Bruntink, and Ana Oprescu. 2019. Investigating developer perception on test smells using better code hub-Work in progress. In 2019 Seminar Series on Advanced Techniques and Tools for Software Evolution, SATTOSE.\n\n[15] Davide Spadini, Martin Schvarcbacher, Ana-Maria Oprescu, Magiel Bruntink, and Alberto Bacchelli. [n.d.]. Investigating Severity Thresholds for Test Smells. ([n.d.]).\n\n[16] David Thomas and Andrew Hunt. 2019. The Pragmatic Programmer: your journey\n\nto mastery. Addison-Wesley Professional.\n\n[17] Michele Tufano, Fabio Palomba, Gabriele Bavota, Massimiliano Di Penta, Rocco Oliveto, Andrea De Lucia, and Denys Poshyvanyk. 2016. An empirical investiga- tionintothenatureoftestsmells.InProceedingsofthe31stIEEE/ACMInternational Conference on Automated Software Engineering. 4–15.\n\n[18] Arie Van Deursen, Leon Moonen, Alex Van Den Bergh, and Gerard Kok. 2001. Refactoring test code. In Proceedings of the 2nd international conference on extreme programming and flexible processes in software engineering (XP). 92–95.\n\n[19] Bart Van Rompaey, Bart Du Bois, Serge Demeyer, and Matthias Rieger. 2007. On the detection of test smells: A metrics-based approach for general fixture and eager test. IEEE Transactions on Software Engineering 33, 12 (2007), 800–817.",
      "content_length": 7694,
      "extraction_method": "Unstructured"
    }
  ]
}