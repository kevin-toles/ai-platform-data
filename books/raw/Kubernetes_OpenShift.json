{
  "metadata": {
    "title": "Kubernetes_OpenShift",
    "author": "David K. Rensin",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 43,
    "conversion_date": "2025-12-19T18:46:57.449937",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Kubernetes_OpenShift.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "Kubernetes\n\nScheduling the Future at Cloud Scale\n\nDavid K. Rensin\n\nCompliments of\n\nKubernetes\n\nScheduling the Future at Cloud Scale\n\nDavid K. Rensin\n\nKubernetes by David Rensin\n\nCopyright © 2015 O’Reilly Media, Inc. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles ( http://safaribooksonline.com ). For more sales department: 800-998-9938 or corporate@oreilly.com .\n\ninformation,\n\ncontact our\n\ncorporate/institutional\n\nEditor: Brian Anderson Production Editor: Matt Hacker Interior Designer: David Futato\n\nCover Designer: Karen Montgomery Illustrator: Rebecca Demarest\n\nJune 2015:\n\nFirst Edition\n\nRevision History for the First Edition 2015-06-19: First Release The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. The cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the author(s) have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author(s) disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is sub‐ ject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-491-93599-6\n\n[LSI]\n\nTable of Contents\n\n1 In The Beginning…. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Introduction 1 Who I Am 2 Who I Think You Are 3 The Problem 3\n\nGo Big or Go Home!. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Introducing Kubernetes—Scaling through Scheduling 5 Applications vs. Services 6 The Master and Its Minions 7 Pods 10 Volumes 12 From Bricks to House 14\n\nOrganize, Grow, and Go. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Better Living through Labels, Annotations, and Selectors 15 Replication Controllers 18 Services 21 Health Checking 27 Moving On 30\n\nHere, There, and Everywhere. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Starting Small with Your Local Machine 32 Bare Metal 33 Virtual Metal (IaaS on a Public Cloud) 33 Other Configurations 34 Fully Managed 35\n\niii\n\niv\n\nA Word about Multi-Cloud Deployments 36 Getting Started with Some Examples 36 Where to Go for More 36\n\n|\n\nTable of Contents\n\nIn The Beginning…\n\nCloud computing has come a long way.\n\nJust a few years ago there was a raging religious debate about whether people and projects would migrate en masse to public cloud infrastructures. Thanks to the success of providers like AWS, Google, and Microsoft, that debate is largely over.\n\nIntroduction In the “early days” (three years ago), managing a web-scale applica‐ tion meant doing a lot of tooling on your own. You had to manage your own VM images, instance fleets, load balancers, and more. It got complicated fast. Then, orchestration tools like Chef, Puppet, Ansible, and Salt caught up to the problem and things got a little bit easier.\n\nA little later (approximately two years ago) people started to really feel the pain of managing their applications at the VM layer. Even under the best circumstances it takes a brand new virtual machine at least a couple of minutes to spin up, get recognized by a load bal‐ ancer, and begin handling traffic. That’s a lot faster than ordering and installing new hardware, but not quite as fast as we expect our systems to respond.\n\nThen came Docker.\n\nJust In Case…\n\nIf you have no idea what containers are or how Docker helped make them popular, you should stop reading this paper right now and go here.\n\n1\n\nSo now the problem of VM spin-up times and image versioning has been seriously mitigated. All should be right with the world, right? Wrong.\n\nContainers are lightweight and awesome, but they aren’t full VMs. That means that they need a lot of orchestration to run efficiently and resiliently. Their execution needs to be scheduled and managed. When they die (and they do), they need to be seamlessly replaced and re-balanced.\n\nThis is a non-trivial problem.\n\nIn this book, I will introduce you to one of the solutions to this chal‐ lenge—Kubernetes. It’s not the only way to skin this cat, but getting a good grasp on what it is and how it works will arm you with the information you need to make good choices later.\n\nWho I Am Full disclosure: I work for Google.\n\nSpecifically, I am the Director of Global Cloud Support and Services. As you might imagine, I very definitely have a bias towards the things my employer uses and/or invented, and it would be pretty silly for me to pretend otherwise.\n\nThat said, I used to work at their biggest competitor—AWS—and before that, I wrote a book for O’Reilly on Cloud Computing, so I do have some perspective.\n\nI’ll do my best to write in an evenhanded way, but it’s unlikely I’ll be able to completely stamp out my biases for the sake of perfectly objective prose. I promise to keep the preachy bits to a minimum and keep the text as non-denominational as I can muster.\n\nIf you’re so inclined, you can see my full bio here.\n\nFinally, you should know that the words you read are completely my own. This paper does not reflect the views of Google, my family, friends, pets, or anyone I now know or might meet in the future. I speak for myself and nobody else. I own these words.\n\nSo that’s me. Let’s chat a little about you…\n\n2\n\n|\n\nIn The Beginning…\n\nWho I Think You Are For you to get the most out of this book, I need you to have accom‐ plished the following basic things:\n\n1. Spun up at least three instances in somebody’s public cloud infrastructure—it doesn’t matter whose. (Bonus points points if you’ve deployed behind a load balancer.)\n\n2. Have read and digested the basics about Docker and containers.\n\n3. Have created at least one local container—just to play with.\n\nIf any of those things are not true, you should probably wait to read this paper until they are. If you don’t, then you risk confusion.\n\nThe Problem Containers are really lightweight. That makes them super flexible and fast. However, they are designed to be short-lived and fragile. I know it seems odd to talk about system components that are designed to not be particularly resilient, but there’s a good reason for it.\n\nInstead of making each small computing component of a system bullet-proof, you can actually make the whole system a lot more sta‐ ble by assuming each compute unit is going to fail and designing your overall process to handle it.\n\nAll the scheduling and orchestration systems gaining mindshare now— Kubernetes or others—are designed first and foremost with this principle in mind. They will kill and re-deploy a container in a cluster if it even thinks about misbehaving!\n\nThis is probably the thing people have the hardest time with when they make the jump from VM-backed instances to containers. You just can’t have the same expectation for isolation or resiliency with a container as you do for a full-fledged virtual machine.\n\nThe comparison I like to make is between a commercial passenger airplane and the Apollo Lunar Module (LM).\n\nAn airplane is meant to fly multiple times a day and ferry hundreds of people long distances. It’s made to withstand big changes in alti‐ tude, the failure of at least one of its engines, and seriously violent\n\nWho I Think You Are\n\n|\n\n3",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary",
      "content": "winds. Discovery Channel documentaries notwithstanding, it takes a lot to make a properly maintained commercial passenger jet fail.\n\nThe LM, on the other hand, was basically made of tin foil and balsa wood. It was optimized for weight and not much else. Little things could (and did during design and construction) easily destroy the thing. That was OK, though. It was meant to operate in a near vac‐ uum and under very specific conditions. It could afford to be light‐ weight and fragile because it only operated under very orchestrated conditions.\n\nAny of this sound familiar?\n\nVMs are a lot like commercial passenger jets. They contain full operating systems—including firewalls and other protective systems —and can be super resilient. Containers, on the other hand, are like the LM. They’re optimized for weight and therefore are a lot less for‐ giving.\n\nIn the real world, individual containers fail a lot more than individ‐ ual virtual machines. To compensate for this, containers have to be run in managed clusters that are heavily scheduled and orchestrated. The environment has to detect a container failure and be prepared to replace it immediately. The environment has to make sure that containers are spread reasonably evenly across physical machines (so as to lessen the effect of a machine failure on the system) and manage overall network and memory resources for the cluster.\n\nIt’s a big job and well beyond the abilities of normal IT orchestration tools like Chef, Puppet, etc….\n\n4\n\n|\n\nIn The Beginning…\n\nGo Big or Go Home!\n\nIf having to manage virtual machines gets cumbersome at scale, it probably won’t come as a surprise to you that it was a problem Goo‐ gle hit pretty early on—nearly ten years ago, in fact. If you’ve ever had to manage more than a few dozen VMs, this will be familiar to you. Now imagine the problems when managing and coordinating millions of VMs.\n\nAt that scale, you start to re-think the problem entirely, and that’s exactly what happened. If your plan for scale was to have a stagger‐ ingly large fleet of identical things that could be interchanged at a moment’s notice, then did it really matter if any one of them failed? Just mark it as bad, clean it up, and replace it.\n\nUsing that lens, the challenge shifts from configuration management to orchestration, scheduling, and isolation. A failure of one comput‐ ing unit cannot take down another (isolation), resources should be reasonably well balanced geographically to distribute load (orches‐ tration), and you need to detect and replace failures near instantane‐ ously (scheduling).\n\nIntroducing Kubernetes—Scaling through Scheduling Pretty early on, engineers working at companies with similar scaling problems started playing around with smaller units of deployment using cgroups and kernel namespaces to create process separation. The net result of these efforts over time became what we commonly refer to as containers.\n\n5\n\nGoogle necessarily had to create a lot of orchestration and schedul‐ ing software to handle isolation, load balancing, and placement. That system is called Borg, and it schedules and launches approxi‐ mately 7,000 containers a second on any given day.\n\nWith the initial release of Docker in March of 2013, Google decided it was finally time to take the most useful (and externalizable) bits of the Borg cluster management system, package them up and publish them via Open Source.\n\nKubernetes was born. (You can browse the source code here.)\n\nApplications vs. Services It is regularly said that in the new world of containers we should be thinking in terms of services (and sometimes micro-services) instead of applications. That sentiment is often confusing to a newcomer, so let me try to ground it a little for you. At first this discussion might seem a little off topic. It isn’t. I promise.\n\nDanger—Religion Ahead!\n\nTo begin with, I need to acknowledge that the line between the two concepts can sometimes get blurry, and people occasionally get religious in the way they argue over it. I’m not trying to pick a fight over philos‐ ophy, but it’s important to give a newcomer some frame of reference. If you happen to be a more experi‐ enced developer and already have well-formed opin‐ ions that differ from mine, please know that I’m not trying to provoke you.\n\nA service is a process that:\n\n1. is designed to do a small number of things (often just one).\n\n2. has no user interface and is invoked solely via some kind of API.\n\nAn application, on the other hand, is pretty much the opposite of that. It has a user interface (even if it’s just a command line) and often performs lots of different tasks. It can also expose an API, but that’s just bonus points in my book.\n\n6\n\n| Go Big or Go Home!\n\nIt has become increasingly common for applications to call several services behind the scenes. The web UI you interact with at https:// www.google.com actually calls several services behind the scenes.\n\nWhere it starts to go off the rails is when people refer to the web page you open in your browser as a web application. That’s not nec‐ essarily wrong so much as it’s just too confusing. Let me try to be more precise.\n\nYour web browser is an application. It has a user interface and does lots of different things. When you tell it to open a web page it con‐ nects to a web server. It then asks the web server to do some stuff via the HTTP protocol.\n\nThe web server has no user interface, only does a limited number of things, and can only be interacted with via an API (HTTP in this example). Therefore, in our discussion, the web server is really a ser‐ vice—not an application.\n\nThis may seem a little too pedantic for this conversation, but it’s actually kind of important. A Kubernetes cluster does not manage a fleet of applications. It manages a cluster of services. You might run an application (often your web browser) that communicates with these services, but the two concepts should not be confused.\n\nA service running in a container managed by Kubernetes is designed to do a very small number of discrete things. As you design your overall system, you should keep that in mind. I’ve seen a lot of well meaning websites fall over because they made their services do too much. That stems from not keeping this distinction in mind when they designed things.\n\nIf your services are small and of limited purpose, then they can more easily be scheduled and re-arranged as your load demands. Otherwise, the dependencies become too much to manage and either your scale or your stability suffers.\n\nThe Master and Its Minions At the end of the day, all cloud infrastructures resolve down to phys‐ ical machines—lots and lots of machines that sit in lots and lots of data centers scattered all around the world. For the sake of explana‐ tion, here’s a simplified (but still useful) view of the basic Kubernetes layout.\n\nThe Master and Its Minions\n\n|\n\n7\n\nBunches of machines sit networked together in lots of data centers. Each of those machines is hosting one or more Docker containers. Those worker machines are called nodes.\n\nNodes used to be called minions and you will some‐ times still see them referred to in this way. I happen to think they should have kept that name because I like whimsical things, but I digress…\n\nOther machines run special coordinating software that schedule containers on the nodes. These machines are called masters. Collec‐ tions of masters and nodes are known as clusters.\n\nFigure 2-1. The Basic Kubernetes Layout\n\nThat’s the simple view. Now let me get a little more specific.\n\nMasters and nodes are defined by which software components they run.\n\nThe Master runs three main items:\n\n1. API Server—nearly all the components on the master and nodes accomplish their respective tasks by making API calls. These are handled by the API Server running on the master.\n\n2. Etcd—Etcd is a service whose job is to keep and replicate the current configuration and run state of the cluster. It is imple‐ mented as a lightweight distributed key-value store and was developed inside the CoreOS project.\n\n3. Scheduler and Controller Manager—These processes schedule containers (actually, pods—but more on them later) onto target\n\n8\n\n| Go Big or Go Home!\n\nnodes. They also make sure that the correct numbers of these things are running at all times.\n\nA node usually runs three important processes:\n\n1. Kubelet—A special background process (daemon that runs on each node whose job is to respond to commands from the mas‐ ter to create, destroy, and monitor the containers on that host.\n\n2. Proxy—This is a simple network proxy that’s used to separate the IP address of a target container from the name of the service it provides. (I’ll cover this in depth a little later.)\n\n3. cAdvisor (optional)—http://bit.ly/1izYGLi[Container Advisor (cAdvisor)] is a special daemon that collects, aggregates, pro‐ cesses, and exports information about running containers. This information includes information about resource isolation, his‐ torical usage, and key network statistics.\n\nThese various parts can be distributed across different machines for scale or all run on the same host for simplicity. The key difference between a master and a node comes down to who’s running which set of processes.\n\nFigure 2-2. The Expanded Kubernetes Layout\n\nIf you’ve read ahead in the Kubernetes documentation, you might be tempted to point out that I glossed over some bits—particularly on the master. You’re right, I did. That was on purpose. Right now, the important thing is to get you up to speed on the basics. I’ll fill in some of the finer details a little later.\n\nThe Master and Its Minions\n\n|\n\n9\n\nAt this point in your reading I am assuming you have some basic familiarity with containers and have created a least one simple one with Docker. If that’s not the case, you should stop here and head over to the main Docker site and run through the basic tutorial.\n\nI have taken great care to keep this text “code free.” As a developer, I love program code, but the purpose of this book is to introduce the concepts and structure of Kubernetes. It’s not meant to be a how-to guide to set‐ ting up a cluster.\n\nFor a good introduction to the kinds of configuration files used for this, you should look here.\n\nThat said, I will very occasionally sprinkle in a few lines of sample configuration to illustrate a point. These will be written in YAML because that’s the for‐ mat Kubernetes expects for its configurations.\n\nPods A pod is a collection of containers and volumes that are bundled and scheduled together because they share a common resource—usually a filesystem or IP address.\n\nFigure 2-3. How Pods Fit in the Picture\n\nKubernetes introduces some simplifications with pods vs. normal Docker. In the standard Docker configuration, each container gets its own IP address. Kubernetes simplifies this scheme by assigning a shared IP address to the pod. The containers in the pod all share the same address and communicate with one another via localhost. In this way, you can think of a pod a little like a VM because it basically emulates a logical host to the containers in it.\n\n10\n\n| Go Big or Go Home!\n\nThis is a very important optimization. Kubernetes schedules and orchestrates things at the pod level, not the container level. That means if you have several containers running in the same pod they have to be managed together. This concept—known as shared fate— is a key underpinning of any clustering system.\n\nAt this point you might be thinking that things would be easier if you just ran processes that need to talk to each other in the same container.\n\nYou can do it, but I really wouldn’t. It’s a bad idea.\n\nIf you do, you undercut a lot of what Kubernetes has to offer. Specif‐ ically:\n\n1. Management Transparency—If you are running more than one process in a container, then you are responsible for moni‐ toring and managing the resources each uses. It is entirely possi‐ ble that one misbehaved process can starve the others within the container, and it will be up to you to detect and fix that. On the other hand, if you separate your logical units of work into sepa‐ rate containers, Kubernetes can manage that for you, which will make things easier to debug and fix.\n\n2. Deployment and Maintenance—Individual containers can be rebuilt and redeployed by you whenever you make a software change. That decoupling of deployment dependencies will make your development and testing faster. It also makes it super easy to rollback in case there’s a problem.\n\n3. Focus—If Kubernetes is handling your process and resource management, then your containers can be lighter. You can focus on your code instead of your overhead.\n\nAnother key concept in any clustering system—including Kuber‐ netes—is lack of durability. Pods are not durable things, and you shouldn’t count on them to be. From time to time (as the overall health of the cluster demands), the master scheduler may choose to evict a pod from its host. That’s a polite way of saying that it will delete the pod and bring up a new copy on another node.\n\nYou are responsible for preserving the state of your application.\n\nThat’s not as hard as it may seem. It just takes a small adjustment to your planning. Instead of storing your state in memory in some\n\nPods\n\n|\n\n11",
      "page_number": 9
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-26)",
      "start_page": 17,
      "end_page": 26,
      "detection_method": "topic_boundary",
      "content": "non-durable way, you should think about using a shared data store like Redis, Memcached, Cassandra, etc.\n\nThat’s the architecture cloud vendors have been preaching for years to people trying to build super-scalable systems—even with more long-lived things like VMs—so this ought not come as a huge sur‐ prise.\n\nThere is some discussion in the Kubernetes community about trying to add migration to the system. In that case, the current running state (including memory) would be saved and moved from one node to another when an eviction occurs. Google introduced some‐ thing similar recently called live migration to its managed VM offer‐ ing (Google Compute Engine), but at the time of this writing, no such mechanism exists in Kubernetes.\n\nSharing and preserving state between the containers in your pod, however, has an even easier solution: volumes.\n\nVolumes Those of you who have played with more than the basics of Docker will already be familiar with Docker volumes. In Docker, a volume is a virtual filesystem that your container can see and use.\n\nAn easy example of when to use a volume is if you are running a web server that has to have ready access to some static content. The easy way to do that is to create a volume for the container and pre- populate it with the needed content. That way, every time a new container is started it has access to a local copy of the content.\n\nSo far, that seems pretty straightforward.\n\nKubernetes also has volumes, but they behave differently. A Kuber‐ netes volume is defined at the pod level—not the container level. This solves a couple of key problems.\n\n1. Durability—Containers die and are reborn all the time. If a vol‐ ume is tied to a container, it will also go away when the con‐ tainer dies. If you’ve been using that space to write temporary files, you’re out of luck. If the volume is bound to the pod, on the other hand, then the data will survive the death and rebirth of any container in that pod. That solves one headache.\n\n12\n\n| Go Big or Go Home!\n\n2. Communication—Since volumes exist at the pod level, any container in the pod can see and use them. That makes moving temporary data between containers super easy.\n\nFigure 2-4. Containers Sharing Storage\n\nBecause they share the same generic name—volume—it’s important to always be clear when discussing storage. Instead of saying “I have a volume that has…,” be sure to say something like “I have a con‐ tainer volume,” or “I have a pod volume.” That will make talking to other people (and getting help) a little easier.\n\nKubernetes currently supports a handful of different pod volume types—with many more in various stages of development in the community. Here are the three most popular types.\n\nEmptyDir The most commonly used type is EmptyDir.\n\nThis type of volume is bound to the pod and is initially always empty when it’s first created. (Hence the name!) Since the volume is bound to the pod, it only exists for the life of the pod. When the pod is evicted, the contents of the volume are lost.\n\nFor the life of the pod, every container in the pod can read and write to this volume—which makes sharing temporary data really easy. As you can imagine, however, it’s important to be diligent and store data that needs to live more permanently some other way.\n\nIn general, this type of storage is known as ephemeral. Storage whose contents survive the life of its host is known as persistent.\n\nVolumes\n\n|\n\n13\n\nNetwork File System (NFS) Recently, Kubernetes added the ability to mount an NFS volume at the pod level. That was a particularly welcome enhancement because it meant that containers could store and retrieve important file- based data—like logs—easily and persistently, since NFS volumes exists beyond the life of the pod.\n\nGCEPersistentDisk (PD) Google Cloud Platform (GCP) has a managed Kubernetes offering named GKE. If you are using Kubernetes via GKE, then you have the option of creating a durable network-attached storage volume called a persistent disk (PD) that can also be mounted as a volume on a pod. You can think of a PD as a managed NFS service. GCP will take care of all the lifecycle and process bits and you just worry about managing your data. They are long-lived and will survive as long as you want them to.\n\nFrom Bricks to House Those are the basic building blocks of your cluster. Now it’s time to talk about how these things assemble to create scale, flexibility, and stability.\n\n14\n\n| Go Big or Go Home!\n\nOrganize, Grow, and Go\n\nOnce you start creating pods, you’ll quickly discover how important it is to organize them. As your clusters grow in size and scope, you’ll need to use this organization to manage things effectively. More than that, however, you will need a way to find pods that have been created for a specific purpose and route requests and data to them. In an environment where things are being created and destroyed with some frequency, that’s harder than you think!\n\nBetter Living through Labels, Annotations, and Selectors Kubernetes provides two basic ways to document your infrastruc‐ ture—labels and annotations.\n\nLabels A label is a key/value pair that you assign to a Kubernetes object (a pod in this case). You can use pretty well any name you like for your label, as long as you follow some basic naming rules. In this case, the label will decorate a pod and will be part of the pod.yaml file you might create to define your pods and containers.\n\nLet’s use an easy example to demonstrate. Suppose you wanted to identify a pod as being part of the front-end tier of your application. You might create a label named tier and assign it a value of frontend —like so:\n\n“labels”: {\n\n“tier”: “frontend”\n\n15\n\n}\n\nThe text “tier” is the key, and the text “frontend” is the value.\n\nKeys are a combination of zero or more prefixes followed by a “/” character followed by a name string. The prefix and slash are optional. Two examples:\n\n“application.game.awesome-game/tier”\n\n“tier”\n\nIn the first case, “application.game.awesome-game” is the prefix, and “tier” is the name. In the second example there is no prefix.\n\nYou have to make sure that your labels conform to the same rules as regular DNS entries—known as DNS Labels.\n\nThe prefix part of the key can be one or more DNS Labels separated by “.” characters. The total length of the prefix (including dots) can‐ not exceed 253 characters.\n\nValues have the same rules but cannot be any longer than 63 charac‐ ters.\n\nNeither keys nor values may contain spaces.\n\nUm…That Seems a Little “In the Weeds”\n\nI’m embarrassed to tell you how many times I’ve tried to figure out why a certain request didn’t get properly routed to the right pod only to discover that my label was too long or had an invalid character. Accordingly, I would be remiss if didn’t at least try to keep you from suffering the same pain!\n\nLabel Selectors Labels are queryable—which makes them especially useful in organ‐ izing things. The mechanism for this query is a label selector.\n\nHeads Up!\n\nYou will live and die by your label selectors. Pay close attention here!\n\n16\n\n| Organize, Grow, and Go\n\nA label selector is a string that identifies which labels you are trying to match.\n\nThere are two kinds of label selectors—equality-based and set-based.\n\nAn equality-based test is just a “IS/IS NOT” test. For example:\n\ntier = frontend\n\nwill return all pods that have a label with the key “tier” and the value “frontend”. On the other hand, if we wanted to get all the pods that were not in the frontend tier, we would say:\n\ntier != frontend\n\nYou can also combine requirements with commas like so:\n\ntier != frontend, game = super-shooter-2\n\nThis would return all pods that were part of the game named “super- shooter-2” but were not in its front end tier.\n\nSet-based tests, on the other hand, are of the “IN/NOT IN” variety. For example:\n\nenvironment in (production, qa) tier notin (frontend, backend) partition\n\nThe first test returns pods that have the “environment” label and a value of either “production” or “qa”. The next test returns all the pods not in the front end or back end tiers. Finally, the third test will return all pods that have the “partition” label—no matter what value it contains.\n\nLike equality-based tests, these can also be combined with commas to perform an AND operation like so:\n\nenvironment in (production, qa), tier notin (frontend, back- end), partition\n\nThis test returns all pods that are in either the production or qa environment, also not in either the front end or back end tiers, and have a partition label of some kind.\n\nAnnotations Annotations are bits of useful information you might want to store about a pod (or cluster, node, etc.) that you will not have to query\n\nBetter Living through Labels, Annotations, and Selectors\n\n|\n\n17\n\nagainst. They are also key/value pairs and have the same rules as labels.\n\nExamples of things you might put there are the pager contact, the build date, or a pointer to more information someplace else—like a URL.\n\nLabels are used to store identifying information about a thing that you might need to query against. Annotations are used to store other arbitrary information that would be handy to have close but won’t need to be filtered or searched.\n\nIt Might Be Boring, but…\n\nI know that labeling and annotating bits of cluster infrastructure is nobody’s idea of a hootenanny. You need to do it, though. Really. Label selectors are the central means of routing and orchestration, so you need to have good labeling hygiene to make things work well.\n\nIf you don’t, your requests will probably never get routed correctly to your pods!\n\nIf you don’t take the time upfront to label and annotate at least the big pieces, you will regret it dearly when it comes time to run your clusters day-to-day. You don’t have to write War and Peace, but you need to write something.\n\nReplication Controllers If you’re building an application that needs to handle a lot of load or have a lot of availability, you clearly are going to want more than one pod running at a time. This is no different than having multiple data centers (back in the days when we all ran our own hardware) or having multiple VMs running behind some kind of load-balancing service. It’s just simple common sense.\n\nMultiple copies of a pod are called replicas. They exist to provide scale and fault-tolerance to your cluster.\n\nThe process that manages these replicas is the replication controller. Specifically, the job of the replication controller is to make sure that the correct number of replicas are running at all times. That’s its prime directive. Anything else it may do is to serve that end.\n\nThis means that the controller will create or destroy replicas as it needs to in order to maintain that objective.\n\n18\n\n| Organize, Grow, and Go\n\nThe way the controller does this is by following a set of rules you define in a pod template. The pod template is a specific definition you provide about the desired state of the cluster. You specify which images will be used to startup each pod, how many replicas there will be, and other state-related things. (The pod.yaml file I refer‐ enced back in the Labels section is an example of this template.)\n\nThe Gestalt of a Replication Controller The whole replication scheme in Kubernetes is designed to be loosely coupled. What I mean by that is that you don’t actually tell a control‐ ler which pods you want it to control. Instead, you define the label selector it will use. Pods that match that query will be managed by the controller.\n\nIn addition, if you kill a replication controller it will not delete the replicas it has under management. (For that, you have to explicitly set the controller’s replicas field to 0.)\n\nThis design has a number of interesting benefits:\n\nFirst, if you want to remove a pod from service for debugging— but not delete it—you just need to change its label so that it no longer matches the label selector used by the controller. That will lower the number of replicas managed by the controller by 1, so it will automatically start a new replica to compensate.\n\nNext, you can change the label selector used by the controller to cause it to assume control over an entirely different fleet in real- time.\n\nFinally, you can kill a replication controller and start a new one in its place and your pods will be none-the-wiser.\n\nNothing Lasts Forever\n\nReplication controllers will die. Count on it. So plan accordingly!\n\nYou can usually count on a replication controller to be more long- lived than any one pod (and certainly more than any one container), but you should not think of them as invincible. They aren’t. A well-\n\nReplication Controllers\n\n|\n\n19\n\ndesigned cluster has at least two controllers running at all times so that they can avoid any single points of failure (SPOF).\n\nThat’s a good thing, because SPOFs are the death of any high- availability system.\n\nScheduling != Scaling Since replication controllers are concerned only with making sure that the correct numbers of pods are running at any one time, they are great for availability. All your replication controllers and clusters would have to fail (or be unreachable owing to network failure) for your service to be unavailable. That sounds pretty good, right?\n\nWhat you may have noticed, however, is that we haven’t talked about how you dynamically size your cluster as your load increases or decreases. With just the components we’ve discussed so far, you will always have a fixed-size cluster. That might be highly available, but it won’t be very cost effective during off-peak hours.\n\nFor that, you will need something sitting in front of your cluster that understands your business logic for scaling—usually a load balancer or traffic shaper. The basic idea is that requests coming from your users will hit a front end load balancer and be directed to your clus‐ ter. As the traffic increases to the load balancer—commonly meas‐ ured in queries per second (QPS)—the balancer will change the repli‐ cas value in the controller and more pods will be created. As the load abates, the reverse will happen.\n\nThere are some finer details I’m glossing over, but I’ll cover them in more depth when we look at services. For those of you who want to dig into some really interesting reading about how a controller decides to evict and replace a pod, I recommend this article.\n\nThe Best Time to Use a Replication Controller Is… Always! (Really).\n\nReplication controllers are always a good idea—even for the sim‐ plest configurations.\n\nEven if you have a situation where you only need to run one con‐ tainer in one pod, it’s still a good idea to use a replication controller because when that pod unexpectedly dies (and it will from time to time) you want a new one to automatically take its place. It’s a pretty\n\n20\n\n| Organize, Grow, and Go\n\nsimple matter to create a controller with a replica count of 1 and then just let it run.\n\nReplication controllers also make zero-downtime rolling updates much easier. Once upon a time, nearly every system went down for at least a few minutes once and a while for “scheduled maintenance.” That’s completely unacceptable now.\n\nWe expect our services to be available 24/7/365. Realistically, every service goes down at least a little bit every year so most really rock solid services aim for “five 9s” of uptime—99.999%. That means a service can only be unavailable .001% of the time—a mere 5.26 minutes a year.\n\nGood luck scheduling your maintenance in that window!\n\nReplication controllers let us do cost-effective rolling updates. We start by bringing up a new controller with 1 updated replica and then removing 1 replica from the old controller. We keep doing this +1/-1 dance until the new controller has the number of replicas we need and the old controller is empty. Then we just delete the old controller.\n\nIf we’re careful, we can make sure that the total number of replicas across both controllers never exceeds the capacity we wanted to pay for. It’s an exceptionally cost-effective and safe way to roll out (and roll back) new code without having any scheduled downtime.\n\nServices Now you have a bunch of pods running your code in a distributed cluster. You have a couple of replication controllers alive to manage things, so life should be good.\n\nWell…Almost…\n\nThe replication controller is only concerned about making sure the right number of replicas is constantly running. Everything else is up to you. Particularly, it doesn’t care if your public-facing application is easily findable by your users. Since it will evict and create pods as it sees fit, there’s no guarantee that the IP addresses of your pods will stay constant—in fact, they almost certainly will not.\n\nThat’s going to break a lot of things.\n\nServices\n\n|\n\n21",
      "page_number": 17
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 27-34)",
      "start_page": 27,
      "end_page": 34,
      "detection_method": "topic_boundary",
      "content": "For example, if you’re application is multi-tiered, then unplanned IP address changes in your backend may make it impossible for your frontend to connect. Similarly, a load balancer sitting in front of your frontend tier won’t know where to route new traffic as your pods die and get new IP addresses.\n\nThe way Kubernetes solves this is through services.\n\nA service is a long-lived, well-known endpoint that points to a set of pods in your cluster. It consists of three things—an external IP address (known as a portal, or sometimes a portal IP), a port, and a label selector.\n\nFigure 3-1. Services Hide Orchestration\n\nThe service is exposed via a small proxy process. When a request comes in for an endpoint you want to expose, the service proxy decides which pod to route it to via a label selector. Just like with a replication controller, the use of a label selector lets us keep fluid which pods will service which request.\n\nSince pods will be created and evicted with unknown frequency, the service proxy acts as a thin lookup service to figure out how to han‐ dle requests. The service proxy is therefore nothing more than a tuple that maps a portal, port, and label selector. It’s a kind of dictio‐ nary for your traffic, not unlike DNS.\n\n22\n\n| Organize, Grow, and Go\n\nThe Life of a Client Request There are enough moving parts to this diagram that now’s a good time to talk about how they work together. Let’s suppose you have a mobile device that is going to connect to some application API run‐ ning in your cluster via REST over HTTPS. Here’s how that goes:\n\n1. The client looks up your endpoint via DNS and attempts a con‐ nection\n\n2. More likely than not, that endpoint is some kind of frontend load balancer. This load balancer figures out which cluster it wants to route the request to and then sends the request along to the portal IP for the requested service.\n\n3. The proxy service uses a label selector to decide which pods are available to send the request to and then forwards the query on to be serviced.\n\nIt’s a pretty straightforward workflow, but its design has some inter‐ esting and useful features.\n\nFirst, there’s no guarantee that the pod that serviced one request will service the next one—even if it’s very close in time or from the same client. The consequence of that is that you have to make sure your pods don’t keep state ephemerally.\n\nSecond, there’s no guarantee that the pod that serviced the request will even exist when the next request comes in. It’s entirely possible that it will be evicted for some reason and replaced by the replica‐ tion controller. That’s completely invisible to your user because when that change happens the evicted pod will no longer match the service label selector and the new one will.\n\nIn practice, this happens in less than a second. I’ve personally meas‐ ured this de-registration / eviction / replacement / registration cycle and found it to take on the order of 300 milliseconds. Compare that to replacing a running VM instance behind a load balancer. That process is almost always on the order of minutes.\n\nLastly, you can tinker with which pods service which requests just by playing with the label selector or changing labels on individual pods. If you’re wondering why you’d want to do that, imagine trying to A/B test a new version of your web service in real-time using sim‐ ple DNS.\n\nServices\n\n|\n\n23\n\nYou also might be wondering how a service proxy decides which pod is going to service the request if more than one matches the label selector. As of this writing, the answer is that it uses simple round- robin routing. There are efforts in progress in the community to have pods expose other run-state information to the service proxy and for the proxy to use that information to make business-based routing decisions, but that’s still a little ways off.\n\nOf course, these advantages don’t just benefit your end clients. Your pods will benefit as well. Suppose you have a frontend pod that needs to connect to a backend pod. Knowing that the IP address of your backend pod can change pretty much anytime, it’s a good idea to have your backend expose itself as a service to which only your frontend can connect.\n\nThe analogy is having frontend VMs connect to backend VMs via DNS instead of fixed IPs.\n\nThat’s the best practice, and you should keep it in mind as we dis‐ cuss some of the fine print around services.\n\nA Few of the Finer Points about Integration with Legacy Stuff Everything you just read is always true if you use the defaults. Like most systems, however, Kubernetes lets you tweak things for your specific edge cases. The most common of these edge cases is when you need your cluster to talk to some legacy backend like an older production database.\n\nTo do that, we have to talk a little bit about how different services find one another—from static IP address maps all the way to fully clustered DNS.\n\nSelector-less Services\n\nIt is possible to have services that do not use label selectors. When you define your service you can just give it a set of static IPs for the backend processes you want it to represent. Of course, that removes one of the key advantages of using services in the first place, so you’re probably wondering why you would ever do such a thing.\n\nSometimes you will have non-Kubernetes backend things you need your pods to know about and connect to. Perhaps you will need your pods to connect to some legacy backend database that is run‐\n\n24\n\n| Organize, Grow, and Go\n\nning in some other infrastructure. In that case you have a choice. You could:\n\n1. Put the IP address (or DNS name) of the legacy backend in each pod, or\n\n2. Create a service that doesn’t route to a Kubernetes pod, but to your other legacy service.\n\nFar and away, (2) is your better choice.\n\n1. It fits seamlessly into your regular architecture—which makes change management easier. If the IP address of the legacy back‐ end changes, you don’t have to re-deploy pods. You just change the service configuration.\n\n2. You can have the frontend tier in one cluster easily point to the backend tier in another cluster just by changing the label selec‐ tor for the service. In certain high-availability (HA) situations, you might need to do this as a fallback until you get things working correctly with your primary backend tier.\n\n3. DNS is slow (minutes), so relying on it will seriously degrade your responsiveness. Lots of software caches DNS entries, so the problem gets even worse.\n\nService Discovery with Environment Variables\n\nWhen a pod wants to consume another service, it needs a way to do a lookup and figure out how to connect.\n\nKubernetes provides two such mechanisms—environment variable and DNS.\n\nWhen a pod exposes a service on a node, Kubernetes creates a set of environment variables on that node to describe the new service. That way, other pods on the same node can consume it easily.\n\nAs you can imagine, managing discovery via environment variables is not super scalable, so Kubernetes gives us a second way to do it: Cluster DNS.\n\nCluster DNS\n\nIn a perfect world, there would be a resilient service that could let any pod discover all the services in the cluster. That way, different\n\nServices\n\n|\n\n25\n\ntiers could talk to each other without having to worry about IP addresses and other fragile schemes.\n\nThat’s where cluster DNS comes in.\n\nYou can configure your cluster to schedule a pod and service that expose DNS. When new pods are created, they are told about this service and will use it for lookups—which is pretty handy.\n\nThese DNS pods contains three special containers:\n\n1. Etcd—Which will store all the actual lookup information\n\n2. SkyDns—A special DNS server written to read from etcd. You can find out more about it here.\n\n3. Kube2sky—A Kubernetes-specific program that watches the master for any changes to the list of services and then publishes the information into etcd. SkyDns will then pick it up.\n\nYou can instructions on how to configure this for yourself here.\n\nExposing Your Services to the World OK!\n\nNow your services can find each other. At some point, however, you will probably want to expose some of the services in your cluster to the rest of the world. For this, you have three basic choices: Direct Access, DIY Load Balancing, and Managed Hosting.\n\nOption #1: Direct Access\n\nThe simplest thing for you to do is to configure your firewall to pass traffic from the outside world to the portal IP of your service. The proxy on that node will then pick which container should service the request.\n\nThe problem, of course, is that this strategy is not particularly fault tolerant. You are limited to just one pod to service the request.\n\nOption #2: DIY Load Balancing\n\nThe next thing you might try is to put a load balancer in front of your cluster and populate it with the portal IPs of your service. That way, you can have multiple pods available to service requests. A common way to do this is to just setup instances of the super popu‐ lar HAProxy software to handle this.\n\n26\n\n| Organize, Grow, and Go\n\nThat’s better, to be sure, but there’s still a fair amount of configura‐ tion and maintenance you will need to do—especially if you want to dynamically size your load balancer fleet under load.\n\nA really good getting-started tutorial on doing this with HAProxy can be found here. If you’re planning on deploying Kubernetes on bare metal (as opposed to in a public cloud) and want to roll your own load balancing, then I would definitely read that doc.\n\nOption #3: Managed Hosting\n\nAll the major cloud providers that support Kubernetes also provide a pretty easy way to scale out your load. When you define your ser‐ vice, you can include a flag named CreateExternalLoadBalancer and set its value to true.\n\nWhen you do this, the cloud provider will automatically add the portal IPs for your service to a fleet of load balancers that it creates on your behalf. The mechanics of this will vary from provider to provider.\n\nYou can find documentation about how to do this on Google’s man‐ aged Kubernetes offering (GKE) here.\n\nHealth Checking Do you write perfect code? Yeah. Me neither.\n\nOne of the great things about Kubernetes is that it will evict degra‐ ded pods and replace them so that it can make sure you always have a system performing reliably at capacity. Sometimes it can do this for you automatically, but sometimes you’ll need to provide some hints.\n\nLow-Level Process Checking You get this for free in Kubernetes. The Kubelet running on each node will talk to the Docker runtime to make sure that the contain‐ ers in your pods are responding. If they aren’t, they will be killed and replaced.\n\nThe problem, of course, is that you have no ability to finesse what it means for a container to be considered healthy. In this case, only a bare minimum of checking is occurring—e.g., whether the container process is still running.\n\nHealth Checking\n\n|\n\n27\n\nThat’s a pretty low bar. Your code could be completely hung and non-responsive and still pass that test. For a reliable production sys‐ tem, we need more.\n\nAutomatic Application Level Checking The next level of sophistication we can employ to test the health of our deployment is automatic health checking. Kubernetes supports some simple probes that it will run on your behalf to determine the health of your pods.\n\nWhen you configure the Kubelet for your nodes, you can ask it to perform one of three types of health checks.\n\nTCP Socket\n\nFor this check you tell the Kubelet which TCP port you want to probe and how long it should take to connect. If the Kubelet cannot open a socket to that port on your pod in the allotted time period, it will restart the pod.\n\nHTTP GET\n\nIf your pod is serving HTTP traffic, a simple health check you can configure is to ask the Kubelet to periodically attempt an HTTP GET from a specific URL. For the pod to register as healthy, that URL fetch must:\n\n1. Return a status code between 200 and 399\n\n2. Return before the timeout interval expires\n\nContainer Exec\n\nFinally, your pod might not already be serving HTTP, and perhaps a simple socket probe is not enough. In that case, you can configure the Kubelet to periodically launch a command line inside the con‐ tainers in your pod. If that command exits with a status code of 0 (the normal “OK” code for a Unix process) then the pod will be marked as healthy.\n\nConfiguring Automatic Health Checks\n\nThe following is a snippet from a pod configuration that enables a simple HTTP health check. The Kubelet will periodically probe the\n\n28\n\n| Organize, Grow, and Go\n\nURL /_status/healthz on port 8080. As long as that fetch returns a code between 200-399, everything will be marked healthy.\n\nlivenessProbe:\n\n# turn on application health checking\n\nenabled: true\n\ntype: http\n\n# length of time to wait for a pod to initialize\n\n# after pod startup, before applying health checking\n\ninitialDelaySeconds: 30\n\n# an http probe\n\nhttpGet:\n\npath: /_status/healthz\n\nport: 8080\n\nHealth check configuration is set in the livenessProbe section.\n\nOne interesting thing to notice is the initialDelaySeconds setting. In this example, the Kubelet will wait 30 seconds after the pod starts before probing for health. This gives your code time to initialize and start your listening threads before the first health check. Otherwise, your pods would never be considered healthy because they would always fail the first check!\n\nManual Application Level Checking As your business logic grows in scope, so will the complexity of what you might consider “healthy” or “unhealthy.” It won’t be long before you won’t be able to simply use the automatic health checks to maintain availability and performance.\n\nFor that, you’re going to want to implement some business rule driven manual health checks.\n\nThe basic idea is this:\n\nHealth Checking\n\n|\n\n29",
      "page_number": 27
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 35-42)",
      "start_page": 35,
      "end_page": 42,
      "detection_method": "topic_boundary",
      "content": "1. You run a special pod in your cluster designed to probe your other pods and take the results they give you and decide if they’re operating correctly.\n\n2. If a pod looks unhealthy, you change one of its labels so that it no longer matches the label selector the replication controller is testing against.\n\n3. The controller will detect that the number of required pods is less than the value it requires and will start a replacement pod.\n\n4. Your health check code can then decide whether or not it wants to delete the malfunctioning pod or simply keep it out of service for further debugging.\n\nIf this seems familiar to you, it’s because this process is very similar to the one I introduced earlier when we discussed rolling updates.\n\nMoving On That covers the what and how parts of the picture. You know what the pieces are and how they fit together. Now it’s time to move on to where they will all run.\n\n30\n\n| Organize, Grow, and Go\n\nHere, There, and Everywhere\n\nSo here we are, 30 pages or so later, and you now have a solid under‐ standing of what Kubernetes is and how it works. By this point in your reading I hope you’ve started to form an opinion about whether or not Kubernetes is a technology that makes sense to you right now.\n\nIn my opinion, it’s clearly the direction the world is heading, but you might think it’s a little too bleeding edge to invest in right this sec‐ ond. That is only the first of two important decisions you have to make.\n\nOnce you’ve decided to keep going, the next question you have to answer is this: do I roll my own or use someone’s managed offering?\n\nYou have three basic choices:\n\n1. Use physical servers you own (or will buy/rent) and install Kubernetes from scratch. Let’s call this option the bare metal option. You can take this route if you have these servers in your office or you rent them in a CoLo. It doesn’t matter. The key thing is that you will be dealing with physical machines.\n\n2. Use virtual machines from a public cloud provider and install Kubernetes on them from scratch. This has the obvious advan‐ tage of not needing to buy physical hardware, but is very differ‐ ent than the bare metal option, because there are important changes to your configuration and operation. Let’s call this the virtual metal option.\n\n3. Use one of the managed offerings from the major cloud provid‐ ers. This route will allow you fewer configuration choices, but\n\n31\n\nwill be a lot easier than rolling your own solution. Let’s call this the fully managed option.\n\nStarting Small with Your Local Machine Sometimes the easiest way to learn something is to install it locally and start poking at it. Installing a full bare metal Kubernetes solu‐ tion is not trivial, but you can start smaller by running all the com‐ ponents on your local machine.\n\nLinux If you’re running Linux locally—or in a VM you can easily access— then it’s pretty easy to get started.\n\n1. Install Docker and make sure it’s in your path. If you already have Docker installed, then make sure it’s at least version 1.3 by running the docker --version command.\n\n2. Install etcd, and make sure it’s in your path.\n\n3. Make sure go is installed and also in your path. Check to make sure your version is also at least 1.3 by running go version.\n\nOnce you’ve completed these steps you should follow along with this getting started guide. It will tell you everything you need to know to get up and running.\n\nWindows/Mac If you’re on Windows or a Mac, on the other hand, the process is a little (but not much) more complicated. There are a few different ways to do it, but the one I’m going to recommend is to use a tool called Vagrant.\n\nVagrant is an application that automatically sets up and manages self-contained runtime environments. It was created so that different software developers could be certain that each of them was running an identical configuration on their local machines.\n\nThe basic idea is that you install a copy of Vagrant and tell it that you want to create a Kubernetes environment. It will run some scripts and set everything up for you. You can try this yourself by following along with the handy setup guide here.\n\n32\n\n| Here, There, and Everywhere\n\nBare Metal After you’ve experimented a little and have gotten the feel for instal‐ ling and configuring Kubernetes on your local machine, you might get the itch to deploy a more realistic configuration on some spare servers you have lying around. (Who among us doesn’t have a few servers sitting in a closet someplace?)\n\nThis setup—a fully bare metal setup—is definitely the most difficult path you can choose, but it does have the advantage of keeping absolutely everything under your control.\n\nThe first question you should ask yourself is do you prefer one Linux distribution over another? Some people are really familiar with Fedora or RHEL, while others are more in the Ubuntu or Debian camps. You don’t need to have a preference—but some people do.\n\nHere are my recommendations for soup-to-nuts getting-started guides for some of the more popular distributions:\n\n1. Fedora, RHEL—There are many such tutorials, but I think the easiest one is here. If you’re looking for something that goes into some of the grittier details, then this might be more to your lik‐ ing.\n\n2. Ubuntu—Another popular choice. I prefer this guide, but a quick Google search shows many others.\n\n3. CentOS—I’ve used this guide and found it to be very helpful.\n\n4. Other—Just because I don’t list a guide for your preferred dis‐ tribution doesn’t mean one doesn’t exist or that the task is undo‐ able. I found a really good getting-started guide that will apply to pretty much any bare metal installation here.\n\nVirtual Metal (IaaS on a Public Cloud) So maybe you don’t have a bunch of spare servers lying around in a closet like I do—or maybe you just don’t want to have to worry about cabling, power, cooling, etc. In that case, it’s a pretty straight‐ forward exercise to build your own Kubernetes cluster from scratch using VMs you spin up on one of the major public clouds.\n\nBare Metal\n\n|\n\n33\n\nThis is a different process than installing on bare metal because your choice of network layout and configura‐ is governed by your choice of provider. tion Whichever bare metal guides you may have read in the previous section will only be mostly helpful in a public cloud.\n\nHere are some quick resources to get you started.\n\n1. AWS—The easiest way is to use this guide, though it also points you to some other resources if you’re looking for a little more configuration control.\n\n2. Azure—Are you a fan of Microsoft Azure? Then this is the guide for you.\n\n3. Google Cloud Platform (GCP)—I’ll bet it won’t surprise you to find out that far and away the most documented way to run Kubernetes in the virtual metal configuration is for GCP. I found hundreds of pages of tips and setup scripts and guides, but the easiest one to start with is this guide.\n\n4. Rackspace—A reliable installation guide for Rackspace has been a bit of a moving target. The most recent guide is here, but things seem to change enough every few months such that it is not always perfectly reliable. You can see a discussion on this topic here. If you’re an experienced Linux administrator then you can probably work around the rough edges reasonably easily. If not, you might want to check back later.\n\nOther Configurations The previous two sections are by no means an exhaustive list of configuration options or getting-started guides. If you’re interested in other possible configurations, then I recommend two things:\n\n1. Start with this list. It’s continuously maintained at the main Kubernetes Github site and contains lots of really useful point‐ ers.\n\n2. Search Google. Really. Things are changing a lot in the Kuber‐ netes space. New guides and scripts are being published nearly every day. A simple Google search every now and again will keep you up to date. If you’re like me and you absolutely want to\n\n34\n\n| Here, There, and Everywhere\n\nknow as soon as something new pops up, then I recommend you set up a Google alert. You can start here.\n\nFully Managed By far, your easiest path into the world of clusters and global scaling will be to use a fully managed service provided by one of the large public cloud providers (AWS, Google, and Microsoft). Strictly speaking, however, only one of them is actually Kubernetes.\n\nLet me explain.\n\nAmazon recently announced a brand new managed offering named Elastic Container Service (ECS). It’s designed to manage Docker containers and shares many of the same organizing principles as Kubernetes. It does not, however, appear to actually use Kubernetes under the hood. AWS doesn’t say what the underlying technology is, but there are enough configuration and deployment differences that it appears they have rolled their own solution. (If you know differ‐ ently, please feel free to email me and I’ll update this text accord‐ ingly.)\n\nIn April of 2015, Microsoft announced Service Fabric for their Azure cloud offering. This new service lets you build microservices using containers and is apparently the same technology that has been powering their underlying cloud offerings for the past five years. Mark Russinovich (Azure’s CTO) gave a helpful overview ses‐ sion of the new service at their annual //Build conference. He was pretty clear that the underlying technology in the new service was not Kubernetes—though Microsoft has contributed knowledge to the project GitHub site on how to configure Kubernetes on Azure VMs.\n\nAs far as I know, the only fully managed Kubernetes service on the market among the large public cloud providers is Google Container Engine (GKE). So if your goal is to use the things I’ve discussed in this paper to build a web-scale service, then GKE is pretty much your only fully managed offering. Additionally, since Kubernetes is an open source project with full source code living on GitHub, you can really dig into the mechanics of how GKE operates by studying the code directly.\n\nFully Managed\n\n|\n\n35\n\nA Word about Multi-Cloud Deployments What if you could create a service that seamlessly spanned your bare metal and several public cloud infrastructures? I think we can agree that would be pretty handy. It certainly would make it hard for your service to go offline under any circumstances short of a large meteor strike or nuclear war.\n\nUnfortunately, that’s still a little bit of a fairy tale in the clustering world. People are thinking hard about the problem, and a few are even taking some tentative steps to create the frameworks necessary to achieve it.\n\nOne such effort is being led by my colleague Quinton Hoole, and it’s called Kubernetes Cluster Federation, though it’s also cheekily some‐ times called Ubernetes. He keeps his current thinking and product design docs on the main Kubernetes GitHub site here, and it’s a pretty interesting read—though it’s still early days.\n\nGetting Started with Some Examples The main Kubernetes GitHub page keeps a running list of example deployments you can try. Two of the more popular ones are the WordPress and Guestbook examples.\n\nThe WordPress example will walk you through how to set up the popular WordPress publishing platform with a MySQL backend whose data will survive the loss of a container or a system reboot. It assumes you are deploying on GKE, though you can pretty easily adapt the example to run on bare/virtual metal.\n\nThe Guestbook example is a little more complicated. It takes you step-by-step through configuring a simple guestbook web applica‐ tion (written in Go) that stores its data in a Redis backend. Although this example has more moving parts, it does have the advantage of being easily followed on a bare/virtual metal setup. It has no depen‐ dencies on GKE and serves as an easy introduction to replication.\n\nWhere to Go for More There are a number of good places you can go on the Web to con‐ tinue your learning about Kubernetes.\n\n36\n\n| Here, There, and Everywhere\n\nThe main Kubernetes homepage is here and has all the official documentation.\n\nThe project GitHub page is here and contains all the source code plus a wealth of other configuration and design documen‐ tation.\n\nIf you’ve decided that you want to use the GKE-managed offer‐ ing, then you’ll want to head over here.\n\nWhen I have thorny questions about a cluster I’m building, I often head to Stack Overflow and grab all the Kubernetes dis‐ cussion here.\n\nYou can also learn a lot by reading bug reports at the official Kubernetes issues tracker.\n\nFinally, if you want to contribute to the Kubernetes project, you will want to start here.\n\nThese are exciting days for cloud computing. Some of the key tech‐ nologies that we will all be using to build and deploy our future applications and services are being created and tested right around us. For those of us old enough to remember it, this feels a lot like the early days of personal computing or perhaps those first few key years of the World Wide Web. This is where the world is going, and those of our peers that are patient enough to tolerate the inevitable fits and starts will be in the best position to benefit.\n\nGood luck, and thanks for reading.\n\nWhere to Go for More\n\n|\n\n37",
      "page_number": 35
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 43-43)",
      "start_page": 43,
      "end_page": 43,
      "detection_method": "topic_boundary",
      "content": "About the Author\n\nDave Rensin, Director of Global Cloud Support and Services at Google, also served as Senior Vice President of Products at Novitas Group, and Principal Solutions Architect at Amazon Web Services. As a technology entrepreneur, he co-founded and sold several busi‐ nesses, including one for more than $1 billion. Dave is the principal inventor on 15 granted U.S. patents.\n\nAcknowledgments\n\nEverytime I finish a book I solemnly swear on a stack of bibles that I’ll never do it again. Writing is hard.\n\nI know. This isn’t Hemingway, but a blank page is a blank page, and it will torture you equally whether you’re writing a poem, a polemic, or a program.\n\nHelping you through all your self-imposed (and mostly ridiculous) angst is an editor—equal parts psychiatrist, tactician, and task mas‐ ter.\n\nI’d like to thank Brian Anderson for both convincing me to do this and for being a fine editor. He cajoled when he had to, reassured when he needed to, and provided constant and solid advice on both clarity and composition.\n\nMy employer—Google—encourages us to write and to generally contribute knowledge to the world. I’ve worked at other places where that was not true, and I really appreciate the difference that makes.\n\nIn addition, I’d like to thank my colleagues Henry Robertson and Daz Wilkins for providing valuable advice on this text as I was writ‐ ing it.\n\nI’d very much like to hear your opinions about this work—good or bad—so please feel free to contribute them liberally via O’Reilly or to me directly at rensin@google.com.\n\nThings are changing a lot in our industry and sometimes it’s hard to know how to make the right decision. I hope this text helps—at least a little.",
      "page_number": 43
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Kubernetes\n\nScheduling the Future at Cloud Scale\n\nDavid K. Rensin\n\nCompliments of",
      "content_length": 81,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "Kubernetes\n\nScheduling the Future at Cloud Scale\n\nDavid K. Rensin",
      "content_length": 65,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Kubernetes by David Rensin\n\nCopyright © 2015 O’Reilly Media, Inc. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles ( http://safaribooksonline.com ). For more sales department: 800-998-9938 or corporate@oreilly.com .\n\ninformation,\n\ncontact our\n\ncorporate/institutional\n\nEditor: Brian Anderson Production Editor: Matt Hacker Interior Designer: David Futato\n\nCover Designer: Karen Montgomery Illustrator: Rebecca Demarest\n\nJune 2015:\n\nFirst Edition\n\nRevision History for the First Edition 2015-06-19: First Release The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. The cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the author(s) have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author(s) disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is sub‐ ject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-491-93599-6\n\n[LSI]",
      "content_length": 1613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Table of Contents\n\n1 In The Beginning…. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Introduction 1 Who I Am 2 Who I Think You Are 3 The Problem 3\n\nGo Big or Go Home!. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Introducing Kubernetes—Scaling through Scheduling 5 Applications vs. Services 6 The Master and Its Minions 7 Pods 10 Volumes 12 From Bricks to House 14\n\nOrganize, Grow, and Go. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Better Living through Labels, Annotations, and Selectors 15 Replication Controllers 18 Services 21 Health Checking 27 Moving On 30\n\nHere, There, and Everywhere. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Starting Small with Your Local Machine 32 Bare Metal 33 Virtual Metal (IaaS on a Public Cloud) 33 Other Configurations 34 Fully Managed 35\n\niii",
      "content_length": 912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "iv\n\nA Word about Multi-Cloud Deployments 36 Getting Started with Some Examples 36 Where to Go for More 36\n\n|\n\nTable of Contents",
      "content_length": 127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "In The Beginning…\n\nCloud computing has come a long way.\n\nJust a few years ago there was a raging religious debate about whether people and projects would migrate en masse to public cloud infrastructures. Thanks to the success of providers like AWS, Google, and Microsoft, that debate is largely over.\n\nIntroduction In the “early days” (three years ago), managing a web-scale applica‐ tion meant doing a lot of tooling on your own. You had to manage your own VM images, instance fleets, load balancers, and more. It got complicated fast. Then, orchestration tools like Chef, Puppet, Ansible, and Salt caught up to the problem and things got a little bit easier.\n\nA little later (approximately two years ago) people started to really feel the pain of managing their applications at the VM layer. Even under the best circumstances it takes a brand new virtual machine at least a couple of minutes to spin up, get recognized by a load bal‐ ancer, and begin handling traffic. That’s a lot faster than ordering and installing new hardware, but not quite as fast as we expect our systems to respond.\n\nThen came Docker.\n\nJust In Case…\n\nIf you have no idea what containers are or how Docker helped make them popular, you should stop reading this paper right now and go here.\n\n1",
      "content_length": 1268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "So now the problem of VM spin-up times and image versioning has been seriously mitigated. All should be right with the world, right? Wrong.\n\nContainers are lightweight and awesome, but they aren’t full VMs. That means that they need a lot of orchestration to run efficiently and resiliently. Their execution needs to be scheduled and managed. When they die (and they do), they need to be seamlessly replaced and re-balanced.\n\nThis is a non-trivial problem.\n\nIn this book, I will introduce you to one of the solutions to this chal‐ lenge—Kubernetes. It’s not the only way to skin this cat, but getting a good grasp on what it is and how it works will arm you with the information you need to make good choices later.\n\nWho I Am Full disclosure: I work for Google.\n\nSpecifically, I am the Director of Global Cloud Support and Services. As you might imagine, I very definitely have a bias towards the things my employer uses and/or invented, and it would be pretty silly for me to pretend otherwise.\n\nThat said, I used to work at their biggest competitor—AWS—and before that, I wrote a book for O’Reilly on Cloud Computing, so I do have some perspective.\n\nI’ll do my best to write in an evenhanded way, but it’s unlikely I’ll be able to completely stamp out my biases for the sake of perfectly objective prose. I promise to keep the preachy bits to a minimum and keep the text as non-denominational as I can muster.\n\nIf you’re so inclined, you can see my full bio here.\n\nFinally, you should know that the words you read are completely my own. This paper does not reflect the views of Google, my family, friends, pets, or anyone I now know or might meet in the future. I speak for myself and nobody else. I own these words.\n\nSo that’s me. Let’s chat a little about you…\n\n2\n\n|\n\nIn The Beginning…",
      "content_length": 1789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Who I Think You Are For you to get the most out of this book, I need you to have accom‐ plished the following basic things:\n\n1. Spun up at least three instances in somebody’s public cloud infrastructure—it doesn’t matter whose. (Bonus points points if you’ve deployed behind a load balancer.)\n\n2. Have read and digested the basics about Docker and containers.\n\n3. Have created at least one local container—just to play with.\n\nIf any of those things are not true, you should probably wait to read this paper until they are. If you don’t, then you risk confusion.\n\nThe Problem Containers are really lightweight. That makes them super flexible and fast. However, they are designed to be short-lived and fragile. I know it seems odd to talk about system components that are designed to not be particularly resilient, but there’s a good reason for it.\n\nInstead of making each small computing component of a system bullet-proof, you can actually make the whole system a lot more sta‐ ble by assuming each compute unit is going to fail and designing your overall process to handle it.\n\nAll the scheduling and orchestration systems gaining mindshare now— Kubernetes or others—are designed first and foremost with this principle in mind. They will kill and re-deploy a container in a cluster if it even thinks about misbehaving!\n\nThis is probably the thing people have the hardest time with when they make the jump from VM-backed instances to containers. You just can’t have the same expectation for isolation or resiliency with a container as you do for a full-fledged virtual machine.\n\nThe comparison I like to make is between a commercial passenger airplane and the Apollo Lunar Module (LM).\n\nAn airplane is meant to fly multiple times a day and ferry hundreds of people long distances. It’s made to withstand big changes in alti‐ tude, the failure of at least one of its engines, and seriously violent\n\nWho I Think You Are\n\n|\n\n3",
      "content_length": 1923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "winds. Discovery Channel documentaries notwithstanding, it takes a lot to make a properly maintained commercial passenger jet fail.\n\nThe LM, on the other hand, was basically made of tin foil and balsa wood. It was optimized for weight and not much else. Little things could (and did during design and construction) easily destroy the thing. That was OK, though. It was meant to operate in a near vac‐ uum and under very specific conditions. It could afford to be light‐ weight and fragile because it only operated under very orchestrated conditions.\n\nAny of this sound familiar?\n\nVMs are a lot like commercial passenger jets. They contain full operating systems—including firewalls and other protective systems —and can be super resilient. Containers, on the other hand, are like the LM. They’re optimized for weight and therefore are a lot less for‐ giving.\n\nIn the real world, individual containers fail a lot more than individ‐ ual virtual machines. To compensate for this, containers have to be run in managed clusters that are heavily scheduled and orchestrated. The environment has to detect a container failure and be prepared to replace it immediately. The environment has to make sure that containers are spread reasonably evenly across physical machines (so as to lessen the effect of a machine failure on the system) and manage overall network and memory resources for the cluster.\n\nIt’s a big job and well beyond the abilities of normal IT orchestration tools like Chef, Puppet, etc….\n\n4\n\n|\n\nIn The Beginning…",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Go Big or Go Home!\n\nIf having to manage virtual machines gets cumbersome at scale, it probably won’t come as a surprise to you that it was a problem Goo‐ gle hit pretty early on—nearly ten years ago, in fact. If you’ve ever had to manage more than a few dozen VMs, this will be familiar to you. Now imagine the problems when managing and coordinating millions of VMs.\n\nAt that scale, you start to re-think the problem entirely, and that’s exactly what happened. If your plan for scale was to have a stagger‐ ingly large fleet of identical things that could be interchanged at a moment’s notice, then did it really matter if any one of them failed? Just mark it as bad, clean it up, and replace it.\n\nUsing that lens, the challenge shifts from configuration management to orchestration, scheduling, and isolation. A failure of one comput‐ ing unit cannot take down another (isolation), resources should be reasonably well balanced geographically to distribute load (orches‐ tration), and you need to detect and replace failures near instantane‐ ously (scheduling).\n\nIntroducing Kubernetes—Scaling through Scheduling Pretty early on, engineers working at companies with similar scaling problems started playing around with smaller units of deployment using cgroups and kernel namespaces to create process separation. The net result of these efforts over time became what we commonly refer to as containers.\n\n5",
      "content_length": 1406,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Google necessarily had to create a lot of orchestration and schedul‐ ing software to handle isolation, load balancing, and placement. That system is called Borg, and it schedules and launches approxi‐ mately 7,000 containers a second on any given day.\n\nWith the initial release of Docker in March of 2013, Google decided it was finally time to take the most useful (and externalizable) bits of the Borg cluster management system, package them up and publish them via Open Source.\n\nKubernetes was born. (You can browse the source code here.)\n\nApplications vs. Services It is regularly said that in the new world of containers we should be thinking in terms of services (and sometimes micro-services) instead of applications. That sentiment is often confusing to a newcomer, so let me try to ground it a little for you. At first this discussion might seem a little off topic. It isn’t. I promise.\n\nDanger—Religion Ahead!\n\nTo begin with, I need to acknowledge that the line between the two concepts can sometimes get blurry, and people occasionally get religious in the way they argue over it. I’m not trying to pick a fight over philos‐ ophy, but it’s important to give a newcomer some frame of reference. If you happen to be a more experi‐ enced developer and already have well-formed opin‐ ions that differ from mine, please know that I’m not trying to provoke you.\n\nA service is a process that:\n\n1. is designed to do a small number of things (often just one).\n\n2. has no user interface and is invoked solely via some kind of API.\n\nAn application, on the other hand, is pretty much the opposite of that. It has a user interface (even if it’s just a command line) and often performs lots of different tasks. It can also expose an API, but that’s just bonus points in my book.\n\n6\n\n| Go Big or Go Home!",
      "content_length": 1799,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "It has become increasingly common for applications to call several services behind the scenes. The web UI you interact with at https:// www.google.com actually calls several services behind the scenes.\n\nWhere it starts to go off the rails is when people refer to the web page you open in your browser as a web application. That’s not nec‐ essarily wrong so much as it’s just too confusing. Let me try to be more precise.\n\nYour web browser is an application. It has a user interface and does lots of different things. When you tell it to open a web page it con‐ nects to a web server. It then asks the web server to do some stuff via the HTTP protocol.\n\nThe web server has no user interface, only does a limited number of things, and can only be interacted with via an API (HTTP in this example). Therefore, in our discussion, the web server is really a ser‐ vice—not an application.\n\nThis may seem a little too pedantic for this conversation, but it’s actually kind of important. A Kubernetes cluster does not manage a fleet of applications. It manages a cluster of services. You might run an application (often your web browser) that communicates with these services, but the two concepts should not be confused.\n\nA service running in a container managed by Kubernetes is designed to do a very small number of discrete things. As you design your overall system, you should keep that in mind. I’ve seen a lot of well meaning websites fall over because they made their services do too much. That stems from not keeping this distinction in mind when they designed things.\n\nIf your services are small and of limited purpose, then they can more easily be scheduled and re-arranged as your load demands. Otherwise, the dependencies become too much to manage and either your scale or your stability suffers.\n\nThe Master and Its Minions At the end of the day, all cloud infrastructures resolve down to phys‐ ical machines—lots and lots of machines that sit in lots and lots of data centers scattered all around the world. For the sake of explana‐ tion, here’s a simplified (but still useful) view of the basic Kubernetes layout.\n\nThe Master and Its Minions\n\n|\n\n7",
      "content_length": 2155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Bunches of machines sit networked together in lots of data centers. Each of those machines is hosting one or more Docker containers. Those worker machines are called nodes.\n\nNodes used to be called minions and you will some‐ times still see them referred to in this way. I happen to think they should have kept that name because I like whimsical things, but I digress…\n\nOther machines run special coordinating software that schedule containers on the nodes. These machines are called masters. Collec‐ tions of masters and nodes are known as clusters.\n\nFigure 2-1. The Basic Kubernetes Layout\n\nThat’s the simple view. Now let me get a little more specific.\n\nMasters and nodes are defined by which software components they run.\n\nThe Master runs three main items:\n\n1. API Server—nearly all the components on the master and nodes accomplish their respective tasks by making API calls. These are handled by the API Server running on the master.\n\n2. Etcd—Etcd is a service whose job is to keep and replicate the current configuration and run state of the cluster. It is imple‐ mented as a lightweight distributed key-value store and was developed inside the CoreOS project.\n\n3. Scheduler and Controller Manager—These processes schedule containers (actually, pods—but more on them later) onto target\n\n8\n\n| Go Big or Go Home!",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "nodes. They also make sure that the correct numbers of these things are running at all times.\n\nA node usually runs three important processes:\n\n1. Kubelet—A special background process (daemon that runs on each node whose job is to respond to commands from the mas‐ ter to create, destroy, and monitor the containers on that host.\n\n2. Proxy—This is a simple network proxy that’s used to separate the IP address of a target container from the name of the service it provides. (I’ll cover this in depth a little later.)\n\n3. cAdvisor (optional)—http://bit.ly/1izYGLi[Container Advisor (cAdvisor)] is a special daemon that collects, aggregates, pro‐ cesses, and exports information about running containers. This information includes information about resource isolation, his‐ torical usage, and key network statistics.\n\nThese various parts can be distributed across different machines for scale or all run on the same host for simplicity. The key difference between a master and a node comes down to who’s running which set of processes.\n\nFigure 2-2. The Expanded Kubernetes Layout\n\nIf you’ve read ahead in the Kubernetes documentation, you might be tempted to point out that I glossed over some bits—particularly on the master. You’re right, I did. That was on purpose. Right now, the important thing is to get you up to speed on the basics. I’ll fill in some of the finer details a little later.\n\nThe Master and Its Minions\n\n|\n\n9",
      "content_length": 1426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "At this point in your reading I am assuming you have some basic familiarity with containers and have created a least one simple one with Docker. If that’s not the case, you should stop here and head over to the main Docker site and run through the basic tutorial.\n\nI have taken great care to keep this text “code free.” As a developer, I love program code, but the purpose of this book is to introduce the concepts and structure of Kubernetes. It’s not meant to be a how-to guide to set‐ ting up a cluster.\n\nFor a good introduction to the kinds of configuration files used for this, you should look here.\n\nThat said, I will very occasionally sprinkle in a few lines of sample configuration to illustrate a point. These will be written in YAML because that’s the for‐ mat Kubernetes expects for its configurations.\n\nPods A pod is a collection of containers and volumes that are bundled and scheduled together because they share a common resource—usually a filesystem or IP address.\n\nFigure 2-3. How Pods Fit in the Picture\n\nKubernetes introduces some simplifications with pods vs. normal Docker. In the standard Docker configuration, each container gets its own IP address. Kubernetes simplifies this scheme by assigning a shared IP address to the pod. The containers in the pod all share the same address and communicate with one another via localhost. In this way, you can think of a pod a little like a VM because it basically emulates a logical host to the containers in it.\n\n10\n\n| Go Big or Go Home!",
      "content_length": 1503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "This is a very important optimization. Kubernetes schedules and orchestrates things at the pod level, not the container level. That means if you have several containers running in the same pod they have to be managed together. This concept—known as shared fate— is a key underpinning of any clustering system.\n\nAt this point you might be thinking that things would be easier if you just ran processes that need to talk to each other in the same container.\n\nYou can do it, but I really wouldn’t. It’s a bad idea.\n\nIf you do, you undercut a lot of what Kubernetes has to offer. Specif‐ ically:\n\n1. Management Transparency—If you are running more than one process in a container, then you are responsible for moni‐ toring and managing the resources each uses. It is entirely possi‐ ble that one misbehaved process can starve the others within the container, and it will be up to you to detect and fix that. On the other hand, if you separate your logical units of work into sepa‐ rate containers, Kubernetes can manage that for you, which will make things easier to debug and fix.\n\n2. Deployment and Maintenance—Individual containers can be rebuilt and redeployed by you whenever you make a software change. That decoupling of deployment dependencies will make your development and testing faster. It also makes it super easy to rollback in case there’s a problem.\n\n3. Focus—If Kubernetes is handling your process and resource management, then your containers can be lighter. You can focus on your code instead of your overhead.\n\nAnother key concept in any clustering system—including Kuber‐ netes—is lack of durability. Pods are not durable things, and you shouldn’t count on them to be. From time to time (as the overall health of the cluster demands), the master scheduler may choose to evict a pod from its host. That’s a polite way of saying that it will delete the pod and bring up a new copy on another node.\n\nYou are responsible for preserving the state of your application.\n\nThat’s not as hard as it may seem. It just takes a small adjustment to your planning. Instead of storing your state in memory in some\n\nPods\n\n|\n\n11",
      "content_length": 2127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "non-durable way, you should think about using a shared data store like Redis, Memcached, Cassandra, etc.\n\nThat’s the architecture cloud vendors have been preaching for years to people trying to build super-scalable systems—even with more long-lived things like VMs—so this ought not come as a huge sur‐ prise.\n\nThere is some discussion in the Kubernetes community about trying to add migration to the system. In that case, the current running state (including memory) would be saved and moved from one node to another when an eviction occurs. Google introduced some‐ thing similar recently called live migration to its managed VM offer‐ ing (Google Compute Engine), but at the time of this writing, no such mechanism exists in Kubernetes.\n\nSharing and preserving state between the containers in your pod, however, has an even easier solution: volumes.\n\nVolumes Those of you who have played with more than the basics of Docker will already be familiar with Docker volumes. In Docker, a volume is a virtual filesystem that your container can see and use.\n\nAn easy example of when to use a volume is if you are running a web server that has to have ready access to some static content. The easy way to do that is to create a volume for the container and pre- populate it with the needed content. That way, every time a new container is started it has access to a local copy of the content.\n\nSo far, that seems pretty straightforward.\n\nKubernetes also has volumes, but they behave differently. A Kuber‐ netes volume is defined at the pod level—not the container level. This solves a couple of key problems.\n\n1. Durability—Containers die and are reborn all the time. If a vol‐ ume is tied to a container, it will also go away when the con‐ tainer dies. If you’ve been using that space to write temporary files, you’re out of luck. If the volume is bound to the pod, on the other hand, then the data will survive the death and rebirth of any container in that pod. That solves one headache.\n\n12\n\n| Go Big or Go Home!",
      "content_length": 2010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "2. Communication—Since volumes exist at the pod level, any container in the pod can see and use them. That makes moving temporary data between containers super easy.\n\nFigure 2-4. Containers Sharing Storage\n\nBecause they share the same generic name—volume—it’s important to always be clear when discussing storage. Instead of saying “I have a volume that has…,” be sure to say something like “I have a con‐ tainer volume,” or “I have a pod volume.” That will make talking to other people (and getting help) a little easier.\n\nKubernetes currently supports a handful of different pod volume types—with many more in various stages of development in the community. Here are the three most popular types.\n\nEmptyDir The most commonly used type is EmptyDir.\n\nThis type of volume is bound to the pod and is initially always empty when it’s first created. (Hence the name!) Since the volume is bound to the pod, it only exists for the life of the pod. When the pod is evicted, the contents of the volume are lost.\n\nFor the life of the pod, every container in the pod can read and write to this volume—which makes sharing temporary data really easy. As you can imagine, however, it’s important to be diligent and store data that needs to live more permanently some other way.\n\nIn general, this type of storage is known as ephemeral. Storage whose contents survive the life of its host is known as persistent.\n\nVolumes\n\n|\n\n13",
      "content_length": 1413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Network File System (NFS) Recently, Kubernetes added the ability to mount an NFS volume at the pod level. That was a particularly welcome enhancement because it meant that containers could store and retrieve important file- based data—like logs—easily and persistently, since NFS volumes exists beyond the life of the pod.\n\nGCEPersistentDisk (PD) Google Cloud Platform (GCP) has a managed Kubernetes offering named GKE. If you are using Kubernetes via GKE, then you have the option of creating a durable network-attached storage volume called a persistent disk (PD) that can also be mounted as a volume on a pod. You can think of a PD as a managed NFS service. GCP will take care of all the lifecycle and process bits and you just worry about managing your data. They are long-lived and will survive as long as you want them to.\n\nFrom Bricks to House Those are the basic building blocks of your cluster. Now it’s time to talk about how these things assemble to create scale, flexibility, and stability.\n\n14\n\n| Go Big or Go Home!",
      "content_length": 1028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Organize, Grow, and Go\n\nOnce you start creating pods, you’ll quickly discover how important it is to organize them. As your clusters grow in size and scope, you’ll need to use this organization to manage things effectively. More than that, however, you will need a way to find pods that have been created for a specific purpose and route requests and data to them. In an environment where things are being created and destroyed with some frequency, that’s harder than you think!\n\nBetter Living through Labels, Annotations, and Selectors Kubernetes provides two basic ways to document your infrastruc‐ ture—labels and annotations.\n\nLabels A label is a key/value pair that you assign to a Kubernetes object (a pod in this case). You can use pretty well any name you like for your label, as long as you follow some basic naming rules. In this case, the label will decorate a pod and will be part of the pod.yaml file you might create to define your pods and containers.\n\nLet’s use an easy example to demonstrate. Suppose you wanted to identify a pod as being part of the front-end tier of your application. You might create a label named tier and assign it a value of frontend —like so:\n\n“labels”: {\n\n“tier”: “frontend”\n\n15",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "}\n\nThe text “tier” is the key, and the text “frontend” is the value.\n\nKeys are a combination of zero or more prefixes followed by a “/” character followed by a name string. The prefix and slash are optional. Two examples:\n\n“application.game.awesome-game/tier”\n\n“tier”\n\nIn the first case, “application.game.awesome-game” is the prefix, and “tier” is the name. In the second example there is no prefix.\n\nYou have to make sure that your labels conform to the same rules as regular DNS entries—known as DNS Labels.\n\nThe prefix part of the key can be one or more DNS Labels separated by “.” characters. The total length of the prefix (including dots) can‐ not exceed 253 characters.\n\nValues have the same rules but cannot be any longer than 63 charac‐ ters.\n\nNeither keys nor values may contain spaces.\n\nUm…That Seems a Little “In the Weeds”\n\nI’m embarrassed to tell you how many times I’ve tried to figure out why a certain request didn’t get properly routed to the right pod only to discover that my label was too long or had an invalid character. Accordingly, I would be remiss if didn’t at least try to keep you from suffering the same pain!\n\nLabel Selectors Labels are queryable—which makes them especially useful in organ‐ izing things. The mechanism for this query is a label selector.\n\nHeads Up!\n\nYou will live and die by your label selectors. Pay close attention here!\n\n16\n\n| Organize, Grow, and Go",
      "content_length": 1402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "A label selector is a string that identifies which labels you are trying to match.\n\nThere are two kinds of label selectors—equality-based and set-based.\n\nAn equality-based test is just a “IS/IS NOT” test. For example:\n\ntier = frontend\n\nwill return all pods that have a label with the key “tier” and the value “frontend”. On the other hand, if we wanted to get all the pods that were not in the frontend tier, we would say:\n\ntier != frontend\n\nYou can also combine requirements with commas like so:\n\ntier != frontend, game = super-shooter-2\n\nThis would return all pods that were part of the game named “super- shooter-2” but were not in its front end tier.\n\nSet-based tests, on the other hand, are of the “IN/NOT IN” variety. For example:\n\nenvironment in (production, qa) tier notin (frontend, backend) partition\n\nThe first test returns pods that have the “environment” label and a value of either “production” or “qa”. The next test returns all the pods not in the front end or back end tiers. Finally, the third test will return all pods that have the “partition” label—no matter what value it contains.\n\nLike equality-based tests, these can also be combined with commas to perform an AND operation like so:\n\nenvironment in (production, qa), tier notin (frontend, back- end), partition\n\nThis test returns all pods that are in either the production or qa environment, also not in either the front end or back end tiers, and have a partition label of some kind.\n\nAnnotations Annotations are bits of useful information you might want to store about a pod (or cluster, node, etc.) that you will not have to query\n\nBetter Living through Labels, Annotations, and Selectors\n\n|\n\n17",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "against. They are also key/value pairs and have the same rules as labels.\n\nExamples of things you might put there are the pager contact, the build date, or a pointer to more information someplace else—like a URL.\n\nLabels are used to store identifying information about a thing that you might need to query against. Annotations are used to store other arbitrary information that would be handy to have close but won’t need to be filtered or searched.\n\nIt Might Be Boring, but…\n\nI know that labeling and annotating bits of cluster infrastructure is nobody’s idea of a hootenanny. You need to do it, though. Really. Label selectors are the central means of routing and orchestration, so you need to have good labeling hygiene to make things work well.\n\nIf you don’t, your requests will probably never get routed correctly to your pods!\n\nIf you don’t take the time upfront to label and annotate at least the big pieces, you will regret it dearly when it comes time to run your clusters day-to-day. You don’t have to write War and Peace, but you need to write something.\n\nReplication Controllers If you’re building an application that needs to handle a lot of load or have a lot of availability, you clearly are going to want more than one pod running at a time. This is no different than having multiple data centers (back in the days when we all ran our own hardware) or having multiple VMs running behind some kind of load-balancing service. It’s just simple common sense.\n\nMultiple copies of a pod are called replicas. They exist to provide scale and fault-tolerance to your cluster.\n\nThe process that manages these replicas is the replication controller. Specifically, the job of the replication controller is to make sure that the correct number of replicas are running at all times. That’s its prime directive. Anything else it may do is to serve that end.\n\nThis means that the controller will create or destroy replicas as it needs to in order to maintain that objective.\n\n18\n\n| Organize, Grow, and Go",
      "content_length": 2004,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "The way the controller does this is by following a set of rules you define in a pod template. The pod template is a specific definition you provide about the desired state of the cluster. You specify which images will be used to startup each pod, how many replicas there will be, and other state-related things. (The pod.yaml file I refer‐ enced back in the Labels section is an example of this template.)\n\nThe Gestalt of a Replication Controller The whole replication scheme in Kubernetes is designed to be loosely coupled. What I mean by that is that you don’t actually tell a control‐ ler which pods you want it to control. Instead, you define the label selector it will use. Pods that match that query will be managed by the controller.\n\nIn addition, if you kill a replication controller it will not delete the replicas it has under management. (For that, you have to explicitly set the controller’s replicas field to 0.)\n\nThis design has a number of interesting benefits:\n\nFirst, if you want to remove a pod from service for debugging— but not delete it—you just need to change its label so that it no longer matches the label selector used by the controller. That will lower the number of replicas managed by the controller by 1, so it will automatically start a new replica to compensate.\n\nNext, you can change the label selector used by the controller to cause it to assume control over an entirely different fleet in real- time.\n\nFinally, you can kill a replication controller and start a new one in its place and your pods will be none-the-wiser.\n\nNothing Lasts Forever\n\nReplication controllers will die. Count on it. So plan accordingly!\n\nYou can usually count on a replication controller to be more long- lived than any one pod (and certainly more than any one container), but you should not think of them as invincible. They aren’t. A well-\n\nReplication Controllers\n\n|\n\n19",
      "content_length": 1885,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "designed cluster has at least two controllers running at all times so that they can avoid any single points of failure (SPOF).\n\nThat’s a good thing, because SPOFs are the death of any high- availability system.\n\nScheduling != Scaling Since replication controllers are concerned only with making sure that the correct numbers of pods are running at any one time, they are great for availability. All your replication controllers and clusters would have to fail (or be unreachable owing to network failure) for your service to be unavailable. That sounds pretty good, right?\n\nWhat you may have noticed, however, is that we haven’t talked about how you dynamically size your cluster as your load increases or decreases. With just the components we’ve discussed so far, you will always have a fixed-size cluster. That might be highly available, but it won’t be very cost effective during off-peak hours.\n\nFor that, you will need something sitting in front of your cluster that understands your business logic for scaling—usually a load balancer or traffic shaper. The basic idea is that requests coming from your users will hit a front end load balancer and be directed to your clus‐ ter. As the traffic increases to the load balancer—commonly meas‐ ured in queries per second (QPS)—the balancer will change the repli‐ cas value in the controller and more pods will be created. As the load abates, the reverse will happen.\n\nThere are some finer details I’m glossing over, but I’ll cover them in more depth when we look at services. For those of you who want to dig into some really interesting reading about how a controller decides to evict and replace a pod, I recommend this article.\n\nThe Best Time to Use a Replication Controller Is… Always! (Really).\n\nReplication controllers are always a good idea—even for the sim‐ plest configurations.\n\nEven if you have a situation where you only need to run one con‐ tainer in one pod, it’s still a good idea to use a replication controller because when that pod unexpectedly dies (and it will from time to time) you want a new one to automatically take its place. It’s a pretty\n\n20\n\n| Organize, Grow, and Go",
      "content_length": 2147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "simple matter to create a controller with a replica count of 1 and then just let it run.\n\nReplication controllers also make zero-downtime rolling updates much easier. Once upon a time, nearly every system went down for at least a few minutes once and a while for “scheduled maintenance.” That’s completely unacceptable now.\n\nWe expect our services to be available 24/7/365. Realistically, every service goes down at least a little bit every year so most really rock solid services aim for “five 9s” of uptime—99.999%. That means a service can only be unavailable .001% of the time—a mere 5.26 minutes a year.\n\nGood luck scheduling your maintenance in that window!\n\nReplication controllers let us do cost-effective rolling updates. We start by bringing up a new controller with 1 updated replica and then removing 1 replica from the old controller. We keep doing this +1/-1 dance until the new controller has the number of replicas we need and the old controller is empty. Then we just delete the old controller.\n\nIf we’re careful, we can make sure that the total number of replicas across both controllers never exceeds the capacity we wanted to pay for. It’s an exceptionally cost-effective and safe way to roll out (and roll back) new code without having any scheduled downtime.\n\nServices Now you have a bunch of pods running your code in a distributed cluster. You have a couple of replication controllers alive to manage things, so life should be good.\n\nWell…Almost…\n\nThe replication controller is only concerned about making sure the right number of replicas is constantly running. Everything else is up to you. Particularly, it doesn’t care if your public-facing application is easily findable by your users. Since it will evict and create pods as it sees fit, there’s no guarantee that the IP addresses of your pods will stay constant—in fact, they almost certainly will not.\n\nThat’s going to break a lot of things.\n\nServices\n\n|\n\n21",
      "content_length": 1939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "For example, if you’re application is multi-tiered, then unplanned IP address changes in your backend may make it impossible for your frontend to connect. Similarly, a load balancer sitting in front of your frontend tier won’t know where to route new traffic as your pods die and get new IP addresses.\n\nThe way Kubernetes solves this is through services.\n\nA service is a long-lived, well-known endpoint that points to a set of pods in your cluster. It consists of three things—an external IP address (known as a portal, or sometimes a portal IP), a port, and a label selector.\n\nFigure 3-1. Services Hide Orchestration\n\nThe service is exposed via a small proxy process. When a request comes in for an endpoint you want to expose, the service proxy decides which pod to route it to via a label selector. Just like with a replication controller, the use of a label selector lets us keep fluid which pods will service which request.\n\nSince pods will be created and evicted with unknown frequency, the service proxy acts as a thin lookup service to figure out how to han‐ dle requests. The service proxy is therefore nothing more than a tuple that maps a portal, port, and label selector. It’s a kind of dictio‐ nary for your traffic, not unlike DNS.\n\n22\n\n| Organize, Grow, and Go",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "The Life of a Client Request There are enough moving parts to this diagram that now’s a good time to talk about how they work together. Let’s suppose you have a mobile device that is going to connect to some application API run‐ ning in your cluster via REST over HTTPS. Here’s how that goes:\n\n1. The client looks up your endpoint via DNS and attempts a con‐ nection\n\n2. More likely than not, that endpoint is some kind of frontend load balancer. This load balancer figures out which cluster it wants to route the request to and then sends the request along to the portal IP for the requested service.\n\n3. The proxy service uses a label selector to decide which pods are available to send the request to and then forwards the query on to be serviced.\n\nIt’s a pretty straightforward workflow, but its design has some inter‐ esting and useful features.\n\nFirst, there’s no guarantee that the pod that serviced one request will service the next one—even if it’s very close in time or from the same client. The consequence of that is that you have to make sure your pods don’t keep state ephemerally.\n\nSecond, there’s no guarantee that the pod that serviced the request will even exist when the next request comes in. It’s entirely possible that it will be evicted for some reason and replaced by the replica‐ tion controller. That’s completely invisible to your user because when that change happens the evicted pod will no longer match the service label selector and the new one will.\n\nIn practice, this happens in less than a second. I’ve personally meas‐ ured this de-registration / eviction / replacement / registration cycle and found it to take on the order of 300 milliseconds. Compare that to replacing a running VM instance behind a load balancer. That process is almost always on the order of minutes.\n\nLastly, you can tinker with which pods service which requests just by playing with the label selector or changing labels on individual pods. If you’re wondering why you’d want to do that, imagine trying to A/B test a new version of your web service in real-time using sim‐ ple DNS.\n\nServices\n\n|\n\n23",
      "content_length": 2107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "You also might be wondering how a service proxy decides which pod is going to service the request if more than one matches the label selector. As of this writing, the answer is that it uses simple round- robin routing. There are efforts in progress in the community to have pods expose other run-state information to the service proxy and for the proxy to use that information to make business-based routing decisions, but that’s still a little ways off.\n\nOf course, these advantages don’t just benefit your end clients. Your pods will benefit as well. Suppose you have a frontend pod that needs to connect to a backend pod. Knowing that the IP address of your backend pod can change pretty much anytime, it’s a good idea to have your backend expose itself as a service to which only your frontend can connect.\n\nThe analogy is having frontend VMs connect to backend VMs via DNS instead of fixed IPs.\n\nThat’s the best practice, and you should keep it in mind as we dis‐ cuss some of the fine print around services.\n\nA Few of the Finer Points about Integration with Legacy Stuff Everything you just read is always true if you use the defaults. Like most systems, however, Kubernetes lets you tweak things for your specific edge cases. The most common of these edge cases is when you need your cluster to talk to some legacy backend like an older production database.\n\nTo do that, we have to talk a little bit about how different services find one another—from static IP address maps all the way to fully clustered DNS.\n\nSelector-less Services\n\nIt is possible to have services that do not use label selectors. When you define your service you can just give it a set of static IPs for the backend processes you want it to represent. Of course, that removes one of the key advantages of using services in the first place, so you’re probably wondering why you would ever do such a thing.\n\nSometimes you will have non-Kubernetes backend things you need your pods to know about and connect to. Perhaps you will need your pods to connect to some legacy backend database that is run‐\n\n24\n\n| Organize, Grow, and Go",
      "content_length": 2103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "ning in some other infrastructure. In that case you have a choice. You could:\n\n1. Put the IP address (or DNS name) of the legacy backend in each pod, or\n\n2. Create a service that doesn’t route to a Kubernetes pod, but to your other legacy service.\n\nFar and away, (2) is your better choice.\n\n1. It fits seamlessly into your regular architecture—which makes change management easier. If the IP address of the legacy back‐ end changes, you don’t have to re-deploy pods. You just change the service configuration.\n\n2. You can have the frontend tier in one cluster easily point to the backend tier in another cluster just by changing the label selec‐ tor for the service. In certain high-availability (HA) situations, you might need to do this as a fallback until you get things working correctly with your primary backend tier.\n\n3. DNS is slow (minutes), so relying on it will seriously degrade your responsiveness. Lots of software caches DNS entries, so the problem gets even worse.\n\nService Discovery with Environment Variables\n\nWhen a pod wants to consume another service, it needs a way to do a lookup and figure out how to connect.\n\nKubernetes provides two such mechanisms—environment variable and DNS.\n\nWhen a pod exposes a service on a node, Kubernetes creates a set of environment variables on that node to describe the new service. That way, other pods on the same node can consume it easily.\n\nAs you can imagine, managing discovery via environment variables is not super scalable, so Kubernetes gives us a second way to do it: Cluster DNS.\n\nCluster DNS\n\nIn a perfect world, there would be a resilient service that could let any pod discover all the services in the cluster. That way, different\n\nServices\n\n|\n\n25",
      "content_length": 1717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "tiers could talk to each other without having to worry about IP addresses and other fragile schemes.\n\nThat’s where cluster DNS comes in.\n\nYou can configure your cluster to schedule a pod and service that expose DNS. When new pods are created, they are told about this service and will use it for lookups—which is pretty handy.\n\nThese DNS pods contains three special containers:\n\n1. Etcd—Which will store all the actual lookup information\n\n2. SkyDns—A special DNS server written to read from etcd. You can find out more about it here.\n\n3. Kube2sky—A Kubernetes-specific program that watches the master for any changes to the list of services and then publishes the information into etcd. SkyDns will then pick it up.\n\nYou can instructions on how to configure this for yourself here.\n\nExposing Your Services to the World OK!\n\nNow your services can find each other. At some point, however, you will probably want to expose some of the services in your cluster to the rest of the world. For this, you have three basic choices: Direct Access, DIY Load Balancing, and Managed Hosting.\n\nOption #1: Direct Access\n\nThe simplest thing for you to do is to configure your firewall to pass traffic from the outside world to the portal IP of your service. The proxy on that node will then pick which container should service the request.\n\nThe problem, of course, is that this strategy is not particularly fault tolerant. You are limited to just one pod to service the request.\n\nOption #2: DIY Load Balancing\n\nThe next thing you might try is to put a load balancer in front of your cluster and populate it with the portal IPs of your service. That way, you can have multiple pods available to service requests. A common way to do this is to just setup instances of the super popu‐ lar HAProxy software to handle this.\n\n26\n\n| Organize, Grow, and Go",
      "content_length": 1832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "That’s better, to be sure, but there’s still a fair amount of configura‐ tion and maintenance you will need to do—especially if you want to dynamically size your load balancer fleet under load.\n\nA really good getting-started tutorial on doing this with HAProxy can be found here. If you’re planning on deploying Kubernetes on bare metal (as opposed to in a public cloud) and want to roll your own load balancing, then I would definitely read that doc.\n\nOption #3: Managed Hosting\n\nAll the major cloud providers that support Kubernetes also provide a pretty easy way to scale out your load. When you define your ser‐ vice, you can include a flag named CreateExternalLoadBalancer and set its value to true.\n\nWhen you do this, the cloud provider will automatically add the portal IPs for your service to a fleet of load balancers that it creates on your behalf. The mechanics of this will vary from provider to provider.\n\nYou can find documentation about how to do this on Google’s man‐ aged Kubernetes offering (GKE) here.\n\nHealth Checking Do you write perfect code? Yeah. Me neither.\n\nOne of the great things about Kubernetes is that it will evict degra‐ ded pods and replace them so that it can make sure you always have a system performing reliably at capacity. Sometimes it can do this for you automatically, but sometimes you’ll need to provide some hints.\n\nLow-Level Process Checking You get this for free in Kubernetes. The Kubelet running on each node will talk to the Docker runtime to make sure that the contain‐ ers in your pods are responding. If they aren’t, they will be killed and replaced.\n\nThe problem, of course, is that you have no ability to finesse what it means for a container to be considered healthy. In this case, only a bare minimum of checking is occurring—e.g., whether the container process is still running.\n\nHealth Checking\n\n|\n\n27",
      "content_length": 1860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "That’s a pretty low bar. Your code could be completely hung and non-responsive and still pass that test. For a reliable production sys‐ tem, we need more.\n\nAutomatic Application Level Checking The next level of sophistication we can employ to test the health of our deployment is automatic health checking. Kubernetes supports some simple probes that it will run on your behalf to determine the health of your pods.\n\nWhen you configure the Kubelet for your nodes, you can ask it to perform one of three types of health checks.\n\nTCP Socket\n\nFor this check you tell the Kubelet which TCP port you want to probe and how long it should take to connect. If the Kubelet cannot open a socket to that port on your pod in the allotted time period, it will restart the pod.\n\nHTTP GET\n\nIf your pod is serving HTTP traffic, a simple health check you can configure is to ask the Kubelet to periodically attempt an HTTP GET from a specific URL. For the pod to register as healthy, that URL fetch must:\n\n1. Return a status code between 200 and 399\n\n2. Return before the timeout interval expires\n\nContainer Exec\n\nFinally, your pod might not already be serving HTTP, and perhaps a simple socket probe is not enough. In that case, you can configure the Kubelet to periodically launch a command line inside the con‐ tainers in your pod. If that command exits with a status code of 0 (the normal “OK” code for a Unix process) then the pod will be marked as healthy.\n\nConfiguring Automatic Health Checks\n\nThe following is a snippet from a pod configuration that enables a simple HTTP health check. The Kubelet will periodically probe the\n\n28\n\n| Organize, Grow, and Go",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "URL /_status/healthz on port 8080. As long as that fetch returns a code between 200-399, everything will be marked healthy.\n\nlivenessProbe:\n\n# turn on application health checking\n\nenabled: true\n\ntype: http\n\n# length of time to wait for a pod to initialize\n\n# after pod startup, before applying health checking\n\ninitialDelaySeconds: 30\n\n# an http probe\n\nhttpGet:\n\npath: /_status/healthz\n\nport: 8080\n\nHealth check configuration is set in the livenessProbe section.\n\nOne interesting thing to notice is the initialDelaySeconds setting. In this example, the Kubelet will wait 30 seconds after the pod starts before probing for health. This gives your code time to initialize and start your listening threads before the first health check. Otherwise, your pods would never be considered healthy because they would always fail the first check!\n\nManual Application Level Checking As your business logic grows in scope, so will the complexity of what you might consider “healthy” or “unhealthy.” It won’t be long before you won’t be able to simply use the automatic health checks to maintain availability and performance.\n\nFor that, you’re going to want to implement some business rule driven manual health checks.\n\nThe basic idea is this:\n\nHealth Checking\n\n|\n\n29",
      "content_length": 1254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "1. You run a special pod in your cluster designed to probe your other pods and take the results they give you and decide if they’re operating correctly.\n\n2. If a pod looks unhealthy, you change one of its labels so that it no longer matches the label selector the replication controller is testing against.\n\n3. The controller will detect that the number of required pods is less than the value it requires and will start a replacement pod.\n\n4. Your health check code can then decide whether or not it wants to delete the malfunctioning pod or simply keep it out of service for further debugging.\n\nIf this seems familiar to you, it’s because this process is very similar to the one I introduced earlier when we discussed rolling updates.\n\nMoving On That covers the what and how parts of the picture. You know what the pieces are and how they fit together. Now it’s time to move on to where they will all run.\n\n30\n\n| Organize, Grow, and Go",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Here, There, and Everywhere\n\nSo here we are, 30 pages or so later, and you now have a solid under‐ standing of what Kubernetes is and how it works. By this point in your reading I hope you’ve started to form an opinion about whether or not Kubernetes is a technology that makes sense to you right now.\n\nIn my opinion, it’s clearly the direction the world is heading, but you might think it’s a little too bleeding edge to invest in right this sec‐ ond. That is only the first of two important decisions you have to make.\n\nOnce you’ve decided to keep going, the next question you have to answer is this: do I roll my own or use someone’s managed offering?\n\nYou have three basic choices:\n\n1. Use physical servers you own (or will buy/rent) and install Kubernetes from scratch. Let’s call this option the bare metal option. You can take this route if you have these servers in your office or you rent them in a CoLo. It doesn’t matter. The key thing is that you will be dealing with physical machines.\n\n2. Use virtual machines from a public cloud provider and install Kubernetes on them from scratch. This has the obvious advan‐ tage of not needing to buy physical hardware, but is very differ‐ ent than the bare metal option, because there are important changes to your configuration and operation. Let’s call this the virtual metal option.\n\n3. Use one of the managed offerings from the major cloud provid‐ ers. This route will allow you fewer configuration choices, but\n\n31",
      "content_length": 1472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "will be a lot easier than rolling your own solution. Let’s call this the fully managed option.\n\nStarting Small with Your Local Machine Sometimes the easiest way to learn something is to install it locally and start poking at it. Installing a full bare metal Kubernetes solu‐ tion is not trivial, but you can start smaller by running all the com‐ ponents on your local machine.\n\nLinux If you’re running Linux locally—or in a VM you can easily access— then it’s pretty easy to get started.\n\n1. Install Docker and make sure it’s in your path. If you already have Docker installed, then make sure it’s at least version 1.3 by running the docker --version command.\n\n2. Install etcd, and make sure it’s in your path.\n\n3. Make sure go is installed and also in your path. Check to make sure your version is also at least 1.3 by running go version.\n\nOnce you’ve completed these steps you should follow along with this getting started guide. It will tell you everything you need to know to get up and running.\n\nWindows/Mac If you’re on Windows or a Mac, on the other hand, the process is a little (but not much) more complicated. There are a few different ways to do it, but the one I’m going to recommend is to use a tool called Vagrant.\n\nVagrant is an application that automatically sets up and manages self-contained runtime environments. It was created so that different software developers could be certain that each of them was running an identical configuration on their local machines.\n\nThe basic idea is that you install a copy of Vagrant and tell it that you want to create a Kubernetes environment. It will run some scripts and set everything up for you. You can try this yourself by following along with the handy setup guide here.\n\n32\n\n| Here, There, and Everywhere",
      "content_length": 1768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Bare Metal After you’ve experimented a little and have gotten the feel for instal‐ ling and configuring Kubernetes on your local machine, you might get the itch to deploy a more realistic configuration on some spare servers you have lying around. (Who among us doesn’t have a few servers sitting in a closet someplace?)\n\nThis setup—a fully bare metal setup—is definitely the most difficult path you can choose, but it does have the advantage of keeping absolutely everything under your control.\n\nThe first question you should ask yourself is do you prefer one Linux distribution over another? Some people are really familiar with Fedora or RHEL, while others are more in the Ubuntu or Debian camps. You don’t need to have a preference—but some people do.\n\nHere are my recommendations for soup-to-nuts getting-started guides for some of the more popular distributions:\n\n1. Fedora, RHEL—There are many such tutorials, but I think the easiest one is here. If you’re looking for something that goes into some of the grittier details, then this might be more to your lik‐ ing.\n\n2. Ubuntu—Another popular choice. I prefer this guide, but a quick Google search shows many others.\n\n3. CentOS—I’ve used this guide and found it to be very helpful.\n\n4. Other—Just because I don’t list a guide for your preferred dis‐ tribution doesn’t mean one doesn’t exist or that the task is undo‐ able. I found a really good getting-started guide that will apply to pretty much any bare metal installation here.\n\nVirtual Metal (IaaS on a Public Cloud) So maybe you don’t have a bunch of spare servers lying around in a closet like I do—or maybe you just don’t want to have to worry about cabling, power, cooling, etc. In that case, it’s a pretty straight‐ forward exercise to build your own Kubernetes cluster from scratch using VMs you spin up on one of the major public clouds.\n\nBare Metal\n\n|\n\n33",
      "content_length": 1874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "This is a different process than installing on bare metal because your choice of network layout and configura‐ is governed by your choice of provider. tion Whichever bare metal guides you may have read in the previous section will only be mostly helpful in a public cloud.\n\nHere are some quick resources to get you started.\n\n1. AWS—The easiest way is to use this guide, though it also points you to some other resources if you’re looking for a little more configuration control.\n\n2. Azure—Are you a fan of Microsoft Azure? Then this is the guide for you.\n\n3. Google Cloud Platform (GCP)—I’ll bet it won’t surprise you to find out that far and away the most documented way to run Kubernetes in the virtual metal configuration is for GCP. I found hundreds of pages of tips and setup scripts and guides, but the easiest one to start with is this guide.\n\n4. Rackspace—A reliable installation guide for Rackspace has been a bit of a moving target. The most recent guide is here, but things seem to change enough every few months such that it is not always perfectly reliable. You can see a discussion on this topic here. If you’re an experienced Linux administrator then you can probably work around the rough edges reasonably easily. If not, you might want to check back later.\n\nOther Configurations The previous two sections are by no means an exhaustive list of configuration options or getting-started guides. If you’re interested in other possible configurations, then I recommend two things:\n\n1. Start with this list. It’s continuously maintained at the main Kubernetes Github site and contains lots of really useful point‐ ers.\n\n2. Search Google. Really. Things are changing a lot in the Kuber‐ netes space. New guides and scripts are being published nearly every day. A simple Google search every now and again will keep you up to date. If you’re like me and you absolutely want to\n\n34\n\n| Here, There, and Everywhere",
      "content_length": 1919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "know as soon as something new pops up, then I recommend you set up a Google alert. You can start here.\n\nFully Managed By far, your easiest path into the world of clusters and global scaling will be to use a fully managed service provided by one of the large public cloud providers (AWS, Google, and Microsoft). Strictly speaking, however, only one of them is actually Kubernetes.\n\nLet me explain.\n\nAmazon recently announced a brand new managed offering named Elastic Container Service (ECS). It’s designed to manage Docker containers and shares many of the same organizing principles as Kubernetes. It does not, however, appear to actually use Kubernetes under the hood. AWS doesn’t say what the underlying technology is, but there are enough configuration and deployment differences that it appears they have rolled their own solution. (If you know differ‐ ently, please feel free to email me and I’ll update this text accord‐ ingly.)\n\nIn April of 2015, Microsoft announced Service Fabric for their Azure cloud offering. This new service lets you build microservices using containers and is apparently the same technology that has been powering their underlying cloud offerings for the past five years. Mark Russinovich (Azure’s CTO) gave a helpful overview ses‐ sion of the new service at their annual //Build conference. He was pretty clear that the underlying technology in the new service was not Kubernetes—though Microsoft has contributed knowledge to the project GitHub site on how to configure Kubernetes on Azure VMs.\n\nAs far as I know, the only fully managed Kubernetes service on the market among the large public cloud providers is Google Container Engine (GKE). So if your goal is to use the things I’ve discussed in this paper to build a web-scale service, then GKE is pretty much your only fully managed offering. Additionally, since Kubernetes is an open source project with full source code living on GitHub, you can really dig into the mechanics of how GKE operates by studying the code directly.\n\nFully Managed\n\n|\n\n35",
      "content_length": 2037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "A Word about Multi-Cloud Deployments What if you could create a service that seamlessly spanned your bare metal and several public cloud infrastructures? I think we can agree that would be pretty handy. It certainly would make it hard for your service to go offline under any circumstances short of a large meteor strike or nuclear war.\n\nUnfortunately, that’s still a little bit of a fairy tale in the clustering world. People are thinking hard about the problem, and a few are even taking some tentative steps to create the frameworks necessary to achieve it.\n\nOne such effort is being led by my colleague Quinton Hoole, and it’s called Kubernetes Cluster Federation, though it’s also cheekily some‐ times called Ubernetes. He keeps his current thinking and product design docs on the main Kubernetes GitHub site here, and it’s a pretty interesting read—though it’s still early days.\n\nGetting Started with Some Examples The main Kubernetes GitHub page keeps a running list of example deployments you can try. Two of the more popular ones are the WordPress and Guestbook examples.\n\nThe WordPress example will walk you through how to set up the popular WordPress publishing platform with a MySQL backend whose data will survive the loss of a container or a system reboot. It assumes you are deploying on GKE, though you can pretty easily adapt the example to run on bare/virtual metal.\n\nThe Guestbook example is a little more complicated. It takes you step-by-step through configuring a simple guestbook web applica‐ tion (written in Go) that stores its data in a Redis backend. Although this example has more moving parts, it does have the advantage of being easily followed on a bare/virtual metal setup. It has no depen‐ dencies on GKE and serves as an easy introduction to replication.\n\nWhere to Go for More There are a number of good places you can go on the Web to con‐ tinue your learning about Kubernetes.\n\n36\n\n| Here, There, and Everywhere",
      "content_length": 1947,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "The main Kubernetes homepage is here and has all the official documentation.\n\nThe project GitHub page is here and contains all the source code plus a wealth of other configuration and design documen‐ tation.\n\nIf you’ve decided that you want to use the GKE-managed offer‐ ing, then you’ll want to head over here.\n\nWhen I have thorny questions about a cluster I’m building, I often head to Stack Overflow and grab all the Kubernetes dis‐ cussion here.\n\nYou can also learn a lot by reading bug reports at the official Kubernetes issues tracker.\n\nFinally, if you want to contribute to the Kubernetes project, you will want to start here.\n\nThese are exciting days for cloud computing. Some of the key tech‐ nologies that we will all be using to build and deploy our future applications and services are being created and tested right around us. For those of us old enough to remember it, this feels a lot like the early days of personal computing or perhaps those first few key years of the World Wide Web. This is where the world is going, and those of our peers that are patient enough to tolerate the inevitable fits and starts will be in the best position to benefit.\n\nGood luck, and thanks for reading.\n\nWhere to Go for More\n\n|\n\n37",
      "content_length": 1231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "About the Author\n\nDave Rensin, Director of Global Cloud Support and Services at Google, also served as Senior Vice President of Products at Novitas Group, and Principal Solutions Architect at Amazon Web Services. As a technology entrepreneur, he co-founded and sold several busi‐ nesses, including one for more than $1 billion. Dave is the principal inventor on 15 granted U.S. patents.\n\nAcknowledgments\n\nEverytime I finish a book I solemnly swear on a stack of bibles that I’ll never do it again. Writing is hard.\n\nI know. This isn’t Hemingway, but a blank page is a blank page, and it will torture you equally whether you’re writing a poem, a polemic, or a program.\n\nHelping you through all your self-imposed (and mostly ridiculous) angst is an editor—equal parts psychiatrist, tactician, and task mas‐ ter.\n\nI’d like to thank Brian Anderson for both convincing me to do this and for being a fine editor. He cajoled when he had to, reassured when he needed to, and provided constant and solid advice on both clarity and composition.\n\nMy employer—Google—encourages us to write and to generally contribute knowledge to the world. I’ve worked at other places where that was not true, and I really appreciate the difference that makes.\n\nIn addition, I’d like to thank my colleagues Henry Robertson and Daz Wilkins for providing valuable advice on this text as I was writ‐ ing it.\n\nI’d very much like to hear your opinions about this work—good or bad—so please feel free to contribute them liberally via O’Reilly or to me directly at rensin@google.com.\n\nThings are changing a lot in our industry and sometimes it’s hard to know how to make the right decision. I hope this text helps—at least a little.",
      "content_length": 1698,
      "extraction_method": "Unstructured"
    }
  ]
}