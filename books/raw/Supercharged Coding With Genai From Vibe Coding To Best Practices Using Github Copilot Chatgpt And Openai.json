{
  "metadata": {
    "title": "Supercharged Coding With Genai From Vibe Coding To Best Practices Using Github Copilot Chatgpt And Openai",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 460,
    "conversion_date": "2025-12-19T17:46:20.632104",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Supercharged Coding With Genai From Vibe Coding To Best Practices Using Github Copilot Chatgpt And Openai.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "From Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI",
      "start_page": 10,
      "end_page": 25,
      "detection_method": "regex_chapter",
      "content": "Table of Contents\n\nChapter 3: A Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nTechnical requirements .................................................................................................... 46\n\nIntroducing GitHub Copilot – our AI pair programmer ..................................................... 46\n\nCode completion design overview • 46\n\nUnderstanding GitHub Copilot’s free account limits, costs, and policies .......................... 48\n\nWho is eligible for a free account? • 48\n\nPricing • 49\n\nCode completion policies • 49\n\nActivating your GitHub Copilot account • 50\n\nSetting up Copilot in PyCharm and VS Code ...................................................................... 51\n\nGitHub Copilot plugin for PyCharm • 52\n\nGitHub Copilot extension for VS Code • 53\n\nThe three interaction modes – chat, completion, and analysis • 54\n\nUsing Copilot chat ............................................................................................................. 54\n\nLab 3.1 – calculating geometric mean with chat completion ............................................. 57\n\nLab 3.1 guided walk-through • 58\n\nLab 3.1 analysis • 59\n\nLab 3.2 – introducing completion and keyboard shortcuts for geometric mean calculations ............................................................................................................. 61\n\nLab 3.2 guided walk-through • 61\n\nLab 3.2 analysis • 63\n\nAnalyzing code with Copilot ............................................................................................. 65\n\nAnalyzing code with Copilot editor window in VS Code • 67\n\nLab 3.3 – working with Copilot in VS Code’s Jupyter Notebook for Code Analysis ............ 69\n\nLab 3.3 guided walk-through • 69\n\nLab 3.3 analysis • 72\n\nxi\n\n45\n\nxii\n\nTable of Contents\n\nSummary .......................................................................................................................... 74\n\nQuiz time .......................................................................................................................... 75\n\nFurther reading ................................................................................................................. 75\n\nChapter 4: Best Practices for Prompting with ChatGPT\n\n77\n\nTechnical requirements .................................................................................................... 78\n\nCan we trust GenAI for coding tasks? ................................................................................ 78\n\nThe three pillars of good outputs • 79\n\nBest practices for crafting prompts for coding tasks ......................................................... 81\n\nStructured • 81\n\nSurrounding information • 82\n\nSingle task per prompt • 82\n\nSpecific instructions • 83\n\nShort prompts • 83\n\nCrafting prompts for ChatGPT .......................................................................................... 84\n\nLab 4.1 – converting PyCharm GUI steps to CLI • 85\n\nLab 4.1 guided walk-through • 87\n\nLab 4.1 analysis • 92\n\nPrompt samples by OpenAI ............................................................................................... 94\n\nLab 4.2 – code debugging • 95\n\nLab 4.2 guided walk-through • 97\n\nLab 4.2 analysis • 99\n\nSummary ......................................................................................................................... 101\n\nQuiz time ......................................................................................................................... 101\n\nFurther reading ............................................................................................................... 102\n\nSubscribe for a free eBook ............................................................................................... 102\n\nTable of Contents\n\nChapter 5: Best Practices for Prompting with OpenAI API and GitHub Copilot\n\nTechnical requirements .................................................................................................. 104\n\nExtracting properties from Python objects for OpenAI API ............................................. 104\n\nCrafting precise prompts for OpenAI API ......................................................................... 107\n\nStructured prompts • 107\n\nSystem prompts with the surrounding context and single task • 108\n\nUser prompts with specific instructions • 109\n\nLab 5.1 – Generating docstrings with OpenAI API ............................................................ 110\n\nSingleton design pattern • 110\n\nGenerating a docstring • 112\n\nLab 5.1 guided walk-through • 113\n\nLab 5.1 analysis • 116\n\nCrafting precise prompts for GitHub Copilot ................................................................... 117\n\nStructuring with a lead-in cue • 117\n\nSurrounding with imports and hashtags • 118\n\nFurther narrowing the single task • 121\n\nSpecific instructions with type hints, docstrings, descriptive names, and unit tests • 123\n\nShort prompts without comment fluff • 124\n\nLab 5.2 – Fixing faulty implementation with GitHub Copilot .......................................... 125\n\nLab 5.2 guided walk-through • 126\n\nLab 5.2 analysis • 134\n\nPrompt 1 – following the structured prompt principle with a lead-in cue • 134\n\nPrompt 2– providing specific and short prompt principles • 134\n\nPrompt 3 – adding surrounding context and focusing on a single task with VS Code • 135\n\nPrompt 4 – applying the five S’s with PyCharm chat • 135\n\nPrompt 5 – narrowing down the single task to fix a method implementation • 135\n\nSummary ......................................................................................................................... 136\n\nQuiz time ......................................................................................................................... 137\n\nFurther reading ............................................................................................................... 138\n\nxiii\n\n103\n\nxiv\n\nTable of Contents\n\nPart 2: Basics to Advanced LLM Prompting for GenAI Coding\n\n139\n\nChapter 6: Behind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\n141\n\nTechnical requirements ................................................................................................... 142\n\nStatistical approach to LLMs ............................................................................................ 142\n\nExtending the model to words • 147\n\nSmall and large language models for words ..................................................................... 149\n\nWord2vec • 149\n\nDeep learning • 150\n\nImproving LLM output performance ............................................................................... 151\n\nPrompt engineering • 151\n\nAdding context • 151\n\nProviding examples • 152\n\nFine-tuning • 152\n\nRetrieval-augmented generation • 153\n\nWhat are the limitations of LLMs? ................................................................................... 153\n\nChatGPT .......................................................................................................................... 154\n\nGitHub Copilot ................................................................................................................. 155\n\nSummary ......................................................................................................................... 156\n\nQuiz time ......................................................................................................................... 156\n\nFurther reading ................................................................................................................ 157\n\nSubscribe for a free eBook ............................................................................................... 158\n\nChapter 7: Reading and Understanding Code Bases with GenAI\n\n159\n\nTechnical requirements .................................................................................................. 160\n\nReading and understanding the code base structure ...................................................... 160\n\nIntroducing the Manhattan distance • 161\n\nManhattan distance for matrices • 162\n\nTable of Contents\n\nUsing LLMs to explain code .............................................................................................. 163\n\nUsing the /explain command with GitHub Copilot • 164\n\nUsing ChatGPT to explain code • 166\n\nUsing OpenAI API to explain code • 168\n\nExplaining non-project files ............................................................................................. 169\n\nExplaining an entire file • 170\n\nExplaining with context • 171\n\nExplaining non-project files with ChatGPT • 172\n\nExplaining non-project files with OpenAI API • 174\n\nCreating calls with example parameters .......................................................................... 175\n\nDebugging with GitHub Copilot • 177\n\nDebugging dependency code • 180\n\nCustomizing call parameters with ChatGPT and OpenAI • 182\n\nDebugging dependency code • 183\n\nPutting it all together • 184\n\nSummary ......................................................................................................................... 185\n\nQuiz time ........................................................................................................................ 186\n\nFurther reading ............................................................................................................... 186\n\nChapter 8: An Introduction to Prompt Engineering\n\nTechnical requirements .................................................................................................. 188\n\nUtilizing prompt engineering for coding ......................................................................... 188\n\nEnhancing prompts with few-shot examples ................................................................. 189\n\nFew-shot learning for code refactoring • 190\n\nLeveraging few-shot prompting with ChatGPT ............................................................... 192\n\nFew-shot structure • 192\n\nImprovement in output with code refactoring • 193\n\nCrafting a style guide for GitHub Copilot ........................................................................ 196\n\nStyle guide file • 198\n\nWorking with few-shot learning for OpenAI API ............................................................ 199\n\nImplementing code with OpenAI API • 200\n\nxv\n\n187\n\nxvi\n\nTable of Contents\n\nIntroducing more prompt engineering techniques ......................................................... 203\n\nBug fix at scale with prompt engineering • 203\n\nIterative prompting • 204\n\nTemplate-based prompting • 205\n\nChain-of-thought prompting • 205\n\nExploring additional methods • 206\n\nSummary ........................................................................................................................ 207\n\nQuiz time ........................................................................................................................ 207\n\nFurther reading .............................................................................................................. 208\n\nSubscribe for a free eBook .............................................................................................. 208\n\nChapter 9: Advanced Prompt Engineering for Coding-Related Tasks\n\n209\n\nTechnical requirements .................................................................................................. 210\n\nExtending the prompt implementation scope ................................................................. 210\n\nExtending the geometric mean scope • 210\n\nEvaluating baseline outputs • 212\n\nGitHub Copilot baseline • 212\n\nChatGPT baseline • 213\n\nOpenAI API baseline • 215\n\nLeveraging CoT reasoning with LLM ............................................................................... 217\n\nCoT in the GPT-o series • 219\n\no-series adoption • 220\n\nUsing CoT for GitHub Copilot • 221\n\nCoT for the geometric mean with ChatGPT • 225\n\nCoT with OpenAI API • 226\n\nUsing prompt chaining for LLMs .................................................................................... 227\n\nChaining with ChatGPT • 228\n\nSelective history with OpenAI API • 231\n\nChaining with Copilot • 236\n\nTable of Contents\n\nSummary ........................................................................................................................ 238\n\nQuiz time ........................................................................................................................ 238\n\nFurther reading ............................................................................................................... 239\n\nChapter 10: Refactoring Code with GenAI\n\nTechnical requirements .................................................................................................. 242\n\nIntroducing code refactoring .......................................................................................... 242\n\nManhattan and Euclidean distance • 243\n\nImplementing the distance function • 244\n\nTypes of refactoring • 245\n\nImproving readability before GenAI • 247\n\nLimitations of code refactoring with IDE tools • 248\n\nRefactoring with chain-of-thought ................................................................................ 250\n\nRefactoring for a better structure • 250\n\nCoT for refactoring with GitHub Copilot • 251\n\nLeveraging CoT for refactoring with ChatGPT and OpenAI API • 254\n\nOpenAI API prompt • 256\n\nPerformance refactoring with GenAI .............................................................................. 256\n\nPerformance refactoring with GitHub Copilot • 256\n\nPerformance refactoring with ChatGPT and OpenAI • 258\n\nVectorizing with OpenAI API • 260\n\nMore advanced performance refactoring • 261\n\nSummary ........................................................................................................................ 262\n\nQuiz time ........................................................................................................................ 262\n\nFurther reading ............................................................................................................... 263\n\nSubscribe for a free eBook ............................................................................................... 264\n\nxvii\n\n241\n\nxviii\n\nTable of Contents\n\nChapter 11: Fine-Tuning Models with OpenAI\n\n265\n\nTechnical requirements .................................................................................................. 266\n\nFine-tuning LLMs toward a specialization ...................................................................... 266\n\nLLMs specialization • 267\n\nFine-tuning an LLM • 267\n\nFavoring fine-tuning over prompt engineering • 268\n\nTask specializing in cleaner code • 268\n\nExperimenting with the OpenAI API Playground ............................................................ 269\n\nComputing quadratic roots with GPT-4o mini • 271\n\nImplementing the quadratic roots calculator • 272\n\nCrafting a JSONL file for fine-tuning ............................................................................... 273\n\nFine-tuning file structure • 274\n\nStrategy • 275\n\nCode-only – five training examples • 276\n\nContrastive code-only – three training examples • 276\n\nCode with no comments – five examples • 277\n\nContrastive no comments – two examples • 278\n\nFine-tuning a model with the OpenAI GUI ..................................................................... 280\n\nCosts of fine-tuning a model • 280\n\nInitiating a fine-tuning job • 281\n\nUsing the fine-tuned model • 284\n\nComparing our results .................................................................................................... 285\n\nFine-tuning in action • 287\n\nSummary ........................................................................................................................ 287\n\nQuiz time ........................................................................................................................ 288\n\nFurther reading ............................................................................................................... 289\n\nTable of Contents\n\nPart 3: From Code to Production with GenAI\n\nChapter 12: Documenting Code with GenAI\n\nTechnical requirements .................................................................................................. 294\n\nIntroducing software documentation ............................................................................. 294\n\nWorking with the /explain GitHub Copilot command • 296\n\nUsing GenAI to write docstrings ..................................................................................... 300\n\nDocstring for a single method • 300\n\nDocstrings for an entire file • 302\n\nFinding outdated docstrings with GitHub Copilot, ChatGPT, and OpenAI API ............... 304\n\nPractical insights ............................................................................................................ 308\n\nSummary ........................................................................................................................ 308\n\nQuiz time ........................................................................................................................ 309\n\nFurther reading ............................................................................................................... 310\n\nSubscribe for a free eBook ............................................................................................... 310\n\nChapter 13: Writing and Maintaining Unit Tests\n\nTechnical requirements ................................................................................................... 312\n\nUnit tests with GenAI ....................................................................................................... 312\n\nChatGPT for testing • 318\n\nData-driven tests ............................................................................................................. 321\n\nTest-driven development ................................................................................................ 322\n\nTDD using GitHub Copilot within VS Code • 325\n\nTDD using GitHub Copilot within PyCharm • 327\n\nTDD using ChatGPT • 327\n\nSample data creation with GenAI .................................................................................... 329\n\nPractical insights ............................................................................................................ 332\n\nSummary ........................................................................................................................ 332\n\nQuiz time ........................................................................................................................ 333\n\nFurther reading ............................................................................................................... 334\n\nxix\n\n291\n\n293\n\n311\n\nxx\n\nTable of Contents\n\nChapter 14: GenAI for Runtime and Memory Management\n\n335\n\nTechnical requirements .................................................................................................. 336\n\nIntroducing time and space analysis ............................................................................... 336\n\nThe runtime of a program • 336\n\nRuntime complexity refresher • 337\n\nThe space consumption of a program • 338\n\nSpace complexity • 338\n\nBalancing efficiency tradeoffs • 339\n\nProfiling runtime and memory consumption with GitHub Copilot ................................ 339\n\nProfiling runtime • 339\n\nProfiling space usage • 342\n\nAnalyzing maximal capacity with ChatGPT .................................................................... 343\n\nProfiling runtime across multiple runs • 343\n\nProfiling memory consumption across multiple runs • 345\n\nDetermining maximal capacity within runtime constraints • 347\n\nDetermining maximal capacity within memory constraints • 349\n\nOptimizing code with chained prompts ........................................................................... 351\n\nRuntime optimization • 352\n\nSpace optimization • 354\n\nFurther runtime and space optimization • 357\n\nSummary ......................................................................................................................... 357\n\nQuiz time ........................................................................................................................ 358\n\nFurther reading ............................................................................................................... 358\n\nSubscribe for a free eBook ............................................................................................... 359\n\nTable of Contents\n\nChapter 15: Going Live with GenAI: Logging, Monitoring, and Errors\n\nTechnical requirements .................................................................................................. 362\n\nIntroducing logging, monitoring, and raising errors ....................................................... 362\n\nThe FizzBuzz use case • 363\n\nLogging • 364\n\nHandling unexpected input • 365\n\nMonitoring • 366\n\nLeveraging GenAI for higher-level coding patterns ......................................................... 368\n\nUsing decorators to separate responsibilities • 368\n\nInverse CoT for decorator implementation • 369\n\nApplying inverse CoT with ChatGPT and OpenAI ........................................................... 375\n\nUsing ChatGPT to monitor decorators • 375\n\nUsing OpenAI API to monitor decorators • 377\n\nUtilizing few-shot learning and fine-tuning as style guides ............................................ 378\n\nFew-shot decorator style for GitHub Copilot • 378\n\nFew-shot learning in ChatGPT • 380\n\nLeveraging fine-tuning for OpenAI API • 383\n\nSummary ........................................................................................................................ 387\n\nQuiz time ........................................................................................................................ 387\n\nFurther reading ............................................................................................................... 388\n\nChapter 16: Architecture, Design, and the Future\n\nThe rapid rise of GenAI .................................................................................................... 390\n\nEconomics of faster software development ...................................................................... 391\n\nWill GenAI be adopted by software engineers? ................................................................ 392\n\nShifting landscape of software developers ...................................................................... 393\n\nGenAI as related to the Software Engineering Body of Knowledge • 394\n\nDemocratization of software engineers • 395\n\nLegacy software and flexible teams • 395\n\nxxi\n\n361\n\n389\n\nxxii\n\nTable of Contents\n\nWill GenAI change programming languages? .................................................................. 396\n\nThe future of GenAI in software engineering .................................................................. 397\n\nVibe coding • 397\n\nShorter term • 398\n\nLonger term • 398\n\nWill AI replace software engineering? • 398\n\nRisk and governance • 399\n\nSummary ....................................................................................................................... 400\n\nFurther reading .............................................................................................................. 400\n\nSubscribe for a free eBook ............................................................................................... 401\n\nAppendix\n\n403\n\nSet up an OpenAI Account ............................................................................................... 403\n\nCreating a virtual environment ...................................................................................... 404\n\nPrompting virtual environment • 406\n\nAdding virtual environment to .gitignore • 409\n\nFurther reading ............................................................................................................... 410\n\nOther Books You May Enjoy\n\n413\n\nIndex\n\n417\n\nPreface\n\nSupercharged Coding with GenAI aims to train software developers to achieve increased productivity throughout the software development life cycle (SDLC). It covers not only the programming aspect but also how to write tests, documentation, and other aspects required for putting software into production using GenAI methods. The book introduces the five S’s framework, a standardized approach for consistently producing high-quality output that many GenAI users face.\n\nIt guides you on how and when to use the three most common GenAI software tools that currently dominate the marketplace: ChatGPT, OpenAI API, and GitHub Copilot. Each of these tools offers a different interface for generating code, each with different strengths and weaknesses. Learn- ing how to effectively use these tools is an empowering skill set in the quickly evolving field of software engineering.\n\nThis book is a hands-on approach, with many labs introduced throughout the different chapters, since GenAI coding tools require practice. The labs provide the necessary practice to challenge the knowledge and explore the key skills introduced. The book also dives deeper into the concepts behind how to use instructions, making sure that you not only learn how to do something but also understand why the GenAI is producing particular outputs.\n\nThe book is structured into three parts:\n\n\n\nPart 1, Foundations for Coding with GenAI, provides a quick start tutorial for the three dif- ferent GenAI tools you can use for code completion and surrounding tasks. We will start with OpenAI API framework to harness the large language models (LLMs) as a software developer. We built a program for code completion so we can better understand the de- sign of GenAI tools. Next, we will get started with GitHub Copilot and ChatGPT using three different interaction modes: chat, completion, and analysis. By then, we will also understand the design differences among the different interaction modes. Finally, we will introduce the five S’s framework, a structured approach to crafting precise prompts that lead to predictable and more desirable outputs.\n\nxxiv\n\n\n\nPart 2, Basics to Advanced LLM Prompting for GenAI Coding, takes the next step toward be- coming supercharged coders. We will dive deeper into the foundations of LLMs. The goal is to gain a better understanding of why these models work so much better than the many tools that came before them. We will then start developing the mindset of a supercharged coder by learning which tasks are native to the models, which require advanced prompting techniques, and which tasks are better handled without the assistance of GenAI altogeth- er. We will also learn about applying advanced prompting techniques to coding-related tasks, how to evaluate the goodness of our output with evaluation techniques, and how to fine-tune a model to specialize it for a specific task.\n\n\n\nPart 3, From Code to Production with GenAI, is dedicated to the advanced SDLC approach, where we will be able to use our newly obtained skillset and mindset to work with GenAI tools. We will talk about logging, monitoring, debugging, unit testing, and documenting our code efficiently and quickly with GenAI tools. We will also apply prompt engineering techniques to both space and memory optimizations. We will close this chapter with talks about design, architecture, and the future.\n\nWho this book is for If you are a Python developer curious about GenAI and are looking to elevate your software engineering productivity, this book will transform your approach to software. We include many structured examples of varying problem complexity that will demonstrate the use of advanced prompting techniques, suitable for early intermediate through advanced developers.\n\nTo get the most out of this book, you should have at least one year of hands-on Python develop- ment experience and be somewhat familiar with the SDLC.\n\nWhat this book covers Chapter 1, From Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI, describes the recent convergence of software development tools and the AI transforma- tion with LLMs. It makes a case that now is the best time to improve skills in GenAI to produce quality code faster.\n\nChapter 2, Your Quickstart Guide to OpenAI API, introduces everything you need to know to begin using OpenAI. By the end of the chapter, you will be able to start using it to write code.\n\nPreface\n\nPreface\n\nChapter 3, A Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook, provides setup instructions for GitHub Copilot, the pair programming GenAI tool. It then shows how to inte- grate GitHub Copilot with the two most popular IDEs. With this background, you will be able to prompt with GenAI.\n\nChapter 4, Best Practices for Prompting with ChatGPT, dives into prompting, which is a core skill for GenAI coding. This chapter introduces the five S’s framework for consistent outputs.\n\nChapter 5, Best Practices for Prompting with OpenAI API and GitHub Copilot, completes Part 1 of the book by extending the prompting methods to functions, classes, and methods. The methods are applied to not just writing code but also fixing bugs and providing documentation.\n\nChapter 6, Behind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work, sheds light on how LLMs work under the hood since such an understanding can help understand the strengths and limitations. The chapter begins with a statistical example as a motivation and continues through general LLMs. The chapter concludes with the extension to code-specific LLMs that are at the heart of GitHub Copilot.\n\nChapter 7, Reading and Understanding Code Bases with GenAI, showcases the effectiveness of the different GenAI tools in working with full code bases. The chapter shows how GenAI can explain how code works and help debug issues using the code and support documentation.\n\nChapter 8, An Introduction to Prompt Engineering, applies this recent field to code development and introduces few-shot learning. Prompt engineering methods are applied through OpenAI API and GitHub Copilot for code refactoring to support a consistent coding style.\n\nChapter 9, Advanced Prompt Engineering for Coding-Related Tasks, extends the few-shot approaches from Chapter 8 into reasoning models. Iterative prompting, chain-of-thought prompting, and chaining can effectively deliver highly predictable code that implements key functionality, in- cluding code and comments.\n\nChapter 10, Refactoring Code with GenAI, revisits and extends the chain-of-thought prompting technique to improve code. The chapter covers examples of refactoring code for improved quality and performance.\n\nChapter 11, Fine-Tuning Models with OpenAI, compares and contrasts the few-shot learning ap- proach with fine-tuning, a standard approach for customizing LLMs. The chapter uses the OpenAI Playground to compare the effectiveness and costs of the two approaches.\n\nxxv\n\nxxvi\n\nChapter 12, Documenting Code with GenAI, begins Part 3, which focuses on SDLC components beyond coding. The chapter is a how-to guide for effectively writing docstrings, the standard documentation for Python. It also provides solutions to the problem of out-of-date comments within the code.\n\nChapter 13, Writing and Maintaining Unit Tests, demonstrates multiple methods for leveraging GenAI to accelerate the creation of tests. The chapter also covers the test-driven development (TDD) process of writing test cases first and automatically generating the functional code to pass the tests.\n\nChapter 14, GenAI for Runtime and Memory Management, delves into performance optimization, which is rarely discussed in GenAI code literature. Its examples show how to assess performance trade-offs and, using a combination of prompting with chaining prompts, how to estimate run- time capacities and optimize code to run at scale.\n\nChapter 15, Going Live with GenAI: Logging, Monitoring, and Errors, provides guidance on the use of GenAI for key late-stage components for putting code into production. The chapter brings together past concepts such as enforcing style, few-shot learning, and fine-tuning as applied to monitoring and logging methods.\n\nChapter 16, Architecture, Design, and the Future, discusses the rise of GenAI, software economics, and the changing landscape of software engineering. It provides a perspective on the future of software engineering from vibe coding to long-term expectations, including risks and governance.\n\nTo get the most out of this book Following along will be easier if you bear the following in mind:\n\n\n\nExamples: Begin with the hands-on examples provided in each chapter to make sure that you can effectively use all the tools, rather than focus on just one\n\n\n\nLabs: Use the labs to test your knowledge of how you approach the challenges, and use the questions at the end of each chapter to make sure you grasp the fundamentals\n\nGenAI approach: Experiment with the different techniques from each chapter on your own code and examples to see how GenAI can change your approach to software engineering\n\n\n\nThink beyond: Reflect on how the practical knowledge of the introduced GenAI tools re- lates to the fundamentals of how LLMs work, and how they can enhance multiple aspects of your organization’s software development practices\n\nPreface",
      "page_number": 10
    },
    {
      "number": 2,
      "title": "into reasoning models. Iterative prompting, chain-of-thought prompting, and chaining can effectively deliver highly predictable code that implements key functionality, in- cluding code and comments",
      "start_page": 26,
      "end_page": 53,
      "detection_method": "regex_chapter",
      "content": "Preface\n\nHere is a list of things you need to have:\n\nSoftware/hardware covered in the book\n\nSystem requirements\n\nPython 3.11 or higher\n\nWindows, macOS, or Linux.\n\nLLM chat and embedding models\n\nWindows, macOS, or Linux.\n\nYou can decide to leverage your LLM of choice. Throughout the book, we will be using a variety of GPT models from ChatGPT, OpenAI API, and GitHub Copilot.\n\nDownload the example code files The code bundle for the book is hosted on GitHub at https://github.com/PacktPublishing/ Supercharged-Coding-with-Gen-AI. We also have other code bundles from our rich catalog of books and videos available at https://github.com/PacktPublishing. Check them out!\n\nDownload the color images We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: https://packt.link/gbp/9781836645290.\n\nConventions used There are a number of text conventions used throughout this book.\n\nCodeInText: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and X handles. For example, “The edits window supports the slash commands such as /fix, /explain, and /test, as well as tailored instructions to modify your code.”\n\nA block of code is set as follows:\n\ndef get_geometric_mean_of_two_numbers(\n\na: float,\n\nb: float,\n\n) -> float:\n\nreturn pow(a * b, 1 / 2)\n\nnum1: float = 5.0\n\nnum2: float = 20.0\n\nprint(get_geometric_mean_of_two_numbers(num1, num2))\n\nxxvii\n\nxxviii\n\nAny command-line input or output is written as follows:\n\n(.venv) $ pip install openai\n\nBold: Indicates a new term, an important word, or words that you see on the screen, for example, in menus or dialog boxes. For example: “In this chapter, we will introduce the practice of prompt engineering, a field of study that started in 2020, which offers advanced techniques for refining prompts to achieve more reliable, predictable, and desirable outcomes in large language model (LLM)-based applications.”\n\nWarnings or important notes appear like this.\n\nTips and tricks appear like this.\n\nDisclaimer on AI usage The authors acknowledge the use of cutting-edge AI, such as ChatGPT, OpenAI API, and GitHub Copilot, with the sole aim of enhancing the language and clarity within the book, thereby ensuring a smooth reading experience for readers. It is important to note that the content itself has been crafted by the authors and edited by a professional publishing team.\n\nGet in touch Feedback from our readers is always welcome!\n\nGeneral feedback: Email feedback@packtpub.com and mention the book’s title in the subject of your message. If you have questions about any aspect of this book, please email us at questions@ packtpub.com.\n\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you could report this to us. Please visit http://www.packtpub.com/submit-errata, click Submit Errata, and fill in the form.\n\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at copyright@packtpub.com with a link to the material.\n\nPreface\n\nPreface\n\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit http://authors.packtpub. com/.\n\nJoin our Discord and Reddit spaces You’re not the only one navigating fragmented tools, constant updates, and unclear best prac- tices. Join a growing community of professionals exchanging insights that don’t make it into documentation.\n\nStay informed with updates, discussions, and behind-the-scenes insights from our authors.\n\nJoin our Discord space at https://packt. link/z8ivB or scan the QR code below:\n\nConnect with peers, share ideas, and discuss real-world GenAI challenges. Follow us on Reddit at https://packt.link/0rExL or scan the QR code below:\n\nShare your thoughts Once you’ve read Supercharged Coding with GenAI, we’d love to hear your thoughts! Please click here to go straight to the Amazon review page for this book and share your feedback.\n\nYour review is important to us and the tech community and will help us make sure we’re deliv- ering excellent quality content.\n\nxxix\n\nxxx\n\nYour Book Comes with Exclusive Perks – Here’s How to Unlock Them\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search this book by name. Ensure it’s the correct edition.\n\nNote: Keep your purchase invoice ready before you start.\n\nEnhanced reading experience with our Next-gen Reader:\n\nMulti-device progress sync: Learn from any device with seamless progress sync.\n\nHighlighting and notetaking: Turn your reading into lasting knowledge.\n\nBookmarking: Revisit your most important learnings anytime.\n\nDark mode: Focus with minimal eye strain by switching to dark or sepia mode.\n\nLearn smarter using our AI assistant (Beta):\n\nSummarize it: Summarize key sections or an entire chapter.\n\nAI code explainers: In the next-gen Packt Reader, click the Explain button above each code block for AI-powered code explanations.\n\nPreface\n\nPreface\n\nNote: The AI assistant is part of next-gen Packt Reader and is still in beta.\n\nLearn anytime, anywhere:\n\nAccess your content offline with DRM-free PDF and ePub versions—compatible with your favorite e-readers.\n\nUnlock Your Book’s Exclusive Benefits Your copy of this book comes with the following exclusive benefits:\n\nNext-gen Packt Reader\n\nAI assistant (beta)\n\nDRM-free PDF/ePub downloads\n\nUse the following guide to unlock them if you haven’t already. The process takes just a few min- utes and needs to be done only once.\n\nHow to unlock these benefits in three easy steps\n\nStep 1 Keep your purchase invoice for this book ready, as you’ll need it in Step 3. If you received a physical invoice, scan it on your phone and have it ready as either a PDF, JPG, or PNG.\n\nFor more help on finding your invoice, visit https://www.packtpub.com/unlock-benefits/help.\n\nNote: Did you buy this book directly from Packt? You don’t need an invoice. After completing Step 2, you can jump straight to your exclusive content.\n\nxxxi\n\nxxxii\n\nStep 2 Scan this QR code or go to packtpub.com/unlock.\n\nOn the page that opens (which will look similar to Figure 0.1 if you’re on desktop), search for this book by name. Make sure you select the correct edition.\n\nFigure 0.1: Packt unlock landing page on desktop\n\nPreface\n\nPreface\n\nStep 3 Sign in to your Packt account or create a new one for free. Once you’re logged in, upload your invoice. It can be in PDF, PNG, or JPG format and must be no larger than 10 MB. Follow the rest of the instructions on the screen to complete the process.\n\nNeed help? If you get stuck and need help, visit https://www.packtpub.com/unlock- benefits/help for a detailed FAQ on how to find your invoices and more. The following QR code will take you to the help page directly:\n\nNote: If you are still facing issues, reach out to customercare@packt.com.\n\nxxxiii\n\nPart 1\n\nFoundations for Coding with GenAI\n\nIn Part 1 of this book, we introduce the fundamentals of GenAI for coding and get you started with both OpenAI API and GitHub Copilot. The part begins with a discussion of how GenAI for coding has recently emerged from the intersection of a long evolution in software development tools and the recent large language models (LLMs) from the AI space. This recent fusion of technologies has completely changed the programming landscape. Now is the perfect time to begin the jour- ney since applying them across software engineering tasks requires both training and practice.\n\nThe remainder of Part 1 provides hands-on guidance to start using OpenAI API and GitHub Copilot. After setting up these tools, the part introduces best practices for prompting.\n\nThis part contains the following chapters:\n\n\n\nChapter 1, From Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nChapter 2, Your Quickstart Guide to OpenAI API\n\nChapter 3, A Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nChapter 4, Best Practices for Prompting with ChatGPT\n\nChapter 5, Best Practices for Prompting with OpenAI API and GitHub Copilot\n\n1\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nIf you are reading this book, you have probably heard some of the excitement, hype, concerns, and reality of Generative Artificial Intelligence (GenAI) for coding. You may have checked out some tutorials online and perhaps even explored using this technology for your own coding.\n\nLearning to apply GenAI to software coding takes both practice and time. While there are many online demonstrations of the capabilities, there has not been a systematic approach for achieving functional, quality code with any consistency. There also aren’t many resources that guide devel- opers to use GenAI beyond simple code completion or perhaps testing. GenAI can be particularly useful in expediting tasks such as standardizing coding style to improve readability, debugging, optimizing performance, and the many other tasks performed by software engineers.\n\nIn this chapter, we will explore the following topics:\n\n\n\nChanging the software engineering field\n\n\n\nIntroducing the rise of large language models\n\n\n\nExploring the software development lifecycle\n\n\n\nEmbracing a GenAI toolkit\n\n4\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\n\n\nIs GenAI worth learning for software engineering?\n\nWhat you will get from this book\n\nChanging the software engineering field Computer programming and software engineering, in general, contribute not only to the tech industry, but to many different sectors of the economy, including commerce, finance, health, transportation, and energy. Software drives the creation of many new products. It increases the productivity of companies through the automation and optimization of processes and enables cost reductions.\n\nAs software continues to deliver economic value, new paradigms and tools for software developers have increased the ability to write quality software at a faster pace. Over the last couple of years, GenAI has become one of these tools.\n\nIn software engineering, GenAI has suddenly advanced to reach an inflection point and is fun- damentally changing the field. This recent technology allows everyone from novices to expert software developers to supercharge their productivity not only in coding but, more generally, the full software development lifecycle (SDLC).\n\nAdvanced technologies, including artificial intelligence, seem to be in the news every day lately. Despite this, many software engineers seem somewhat surprised that AI has progressed to the point that it can support their field and specific software development work. The current state of software engineering tools has resulted from the convergence of two separate trends. First, software development tools are not new but have progressed continuously over many decades. Second, GenAI technology has crossed over from the rapid emergence of large language models (LLMs), which trace back to neural networks and the origins of artificial intelligence.\n\nThe evolution of tools for software development The application of GenAI to software engineering is quite a recent development. Although AI has been discussed for many years as a promising set of tools for enhancing code development, the emergence of GenAI has ushered in a new era of capabilities.\n\nSoftware development has experienced many new tools over the past decades that have trans- formed the field. It is easy to argue that software development is constantly evolving, with new tools that have streamlined the processes and enhanced productivity. This section provides an overview of some major technology revolutions that have aided software developers.\n\nChapter 1\n\nIn the 1970s and 1980s, the Maestro I was developed as the first integrated development envi- ronment (IDE), although it would hardly be recognized as such by today’s standards. Its succes- sors, such as Borland’s Turbo Pascal and Visual Studio, provided an easy integration of coding, file management, debugging, compilation, and execution. Today’s IDEs for Python, such as Visual Studio Code, PyCharm, and Spyder, facilitate global changes to variables, code highlighting, syntax checkers, and access to multiple tools.\n\nVersion control systems were a critical step in software engineering, enabling many developers to work on a single project. With a single code base, different versions of code can be tracked and managed. IBM’s IEBUPDTE in the 1960s was a forerunner of the technology, followed by the Re- vision Control System in 1982 and the Concurrent Versions System (CVS) in 1986. It wasn’t until 2005 that the now ubiquitous Git was developed, which enabled a distributed version control system.\n\nBuild tools and continuous integration and continuous deployment (CI/CD) systems speed the delivery process of software. Build tools such as Jenkins and Maven transform source code into executable code. CI/CD tools are often triggered by the build, but continue further to automate the testing, execute linters or other code tools, and often deploy the updated version to users. The full deployment pipeline frees the developers from the many manual steps and enables both a rapid and consistent way of providing users with the latest functionality.\n\nSignificant research has been poured into software testing. Apart from many specialized tools for different forms of testing, testing frameworks are now a standard part of virtually all software development suites. IDEs already speed up the process of creating skeleton tests from existing code by using method signatures and standard test naming conventions. The unittest frameworks run all the tests and report failures, significantly speeding up the process.\n\nCode analysis and refactoring tools identify issues with code and can improve the overall quality. SonarQube is an example of a code analysis tool that performs static code analysis. It identifies potential problems with code, often referred to as code smell, but can also check for a range of potential issues, such as deviations in code style and poor security handling.\n\nSome more advanced tools have been able to not only recognize coding problems but also fix them. For example, ReSharper actually refactors the code to improve its quality. Such tools save developers time and achieve this result through a combination of pattern matching and AI.\n\n5\n\n6\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nWith continual changes in coding sources and packages, software development always seems to require new packages, platforms, or even languages. As a result, software developers require access to the latest manuals or other documentation. Some refer to searching for code examples in Stack Overflow or Reddit. Innovations in this space included Kite, AI-powered software that provided automated code completion and instant code documentation. Kite proved to reduce keyboard clicks and improved code development speed, gaining a user base of an estimated 500,000 pro- grammers. Unfortunately, the company ceased to exist in 2021 and donated its multi-language code tools to the open source community.\n\nNext, we will introduce the turning point in AI research that has driven significant adoption across a variety of domains, including software engineering.\n\nIntroducing the rise of LLMs Over the past few short years, LLMs have emerged as the dominant AI resource for writing, re- search, and inference. They are currently transforming the tech industry, and their applications have a far-reaching impact across all fields. This section provides a brief overview of their un- precedented ascent.\n\nArtificial Intelligence was formally started in 1956 at a famous Dartmouth College workshop of computer science experts. They coined the term artificial intelligence (AI) and set ambitious goals ranging from automated reasoning to natural language processing (NLP). Although the participants expected a rapid progression to these goals, the compute and technology limitations thwarted their success. A publication in 1969 denounced the key technology and allegedly started the first well-documented AI winter, an extended period of no funding or research.\n\nIn the 1980s, expert systems emerged as a workable solution where rules could be crafted by technologists to reproduce human-like reasoning over limited domains for a specific problem. Despite some early successes with the approach, it proved difficult to craft and manage the or- dering for sets of rules. This hindered its adoption and eventually led to the second AI winter.\n\nMachine learning (ML), a sub-field of AI, emerged as the only viable solution. Unlike the hand-crafted rules of expert systems, ML systems could learn to make predictions or decisions directly from data. Research has led to dozens of techniques within the sub-field, but neural networks have become the dominant approach over the past dozen years. Mildly inspired by biological neurons, neural networks have proven to be a powerful system for learning and mod- eling data. Researchers have shown that neural networks can generalize well and approximate\n\nChapter 1\n\nany function. Deep learning, any neural network with multiple layers of neurons, overcomes the limitations of more traditional machine learning techniques. Specifically, it can continue to learn when provided with ever larger training sets.\n\nNLP is the application of machine learning to human language data. It applies to any texts, such as articles, blogs, emails, or books. The field draws from computer science, AI, and linguistics. Earlier methods drew extensively from statistical methods and later traditional ML techniques. In recent years, deep learning methods have revolutionized the NLP field by introducing lan- guage models (LMs), which predict and generate text based on existing language data. LLMs are expanded versions of LMs, trained on massive datasets and billions of parameters, which are internal weights tuned to reflect the patterns in the training data. We will discuss LLMs exten- sively in later chapters of the book.\n\nOver the past several years, deep learning models have been trained on ever-increasing volumes of text and, with new techniques, can understand how words within each sentence are related to each other. This class of LLMs includes OpenAI’s GPT, Meta’s Llama, Google’s Gemini, Anthrop- ic’s Claude, and newer models continue to be developed. These LLMs were initially designed to accurately predict the next word of a phrase. At scale and with recent technologies, they have enabled natural language generation (NLG) solutions that can write full texts to enable report writing, question-answering, chatbots, and much more.\n\nLLMs are typically trained on large sets of available online text sources, but the same models can also be trained on software code. These LLMs use publicly available code in Python, Java, and other programming languages that are mostly available from GitHub repositories. The result is that the LLMs can predict the next block of code, can generate comments, write tests, and even refactor code. These are all parts of the overall SDLC that we will describe in the next section.\n\nExploring the software development lifecycle To deliver quality software, most software teams progress through a series of stages known as the software development lifecycle (SDLC). As shown in Figure 1.1, these steps are designed to be an efficient approach that minimizes the risk of failure. The process usually begins with the recognition of an unmet business need, and cycles through many stages to meet the need with a software system. Projects progress from analyzing the existing state to gathering requirements, designing the system, implementing and testing the code, delivering the solution, and often maintaining the software.\n\n7\n\n8\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nWhile most people associate software development with coding, actual programming makes up only 25-35% of the overall effort, depending on the type of software and its requirements. The remaining steps are needed to gather requirements, test and document the code, deploy the software, and support its continued functionality, as shown in Figure 1.1.\n\nFigure 1.1: The SDLC – the continual process of developing or improving software systems from requirements through maintenance\n\nThe SDLC process begins with gathering requirements, followed by planning, feasibility, and risk analysis. A successful analysis leads to the creation of a high-level system design, and only after this step does an engineer continue on to software coding. The form will be formally tested before it is deployed, resulting in a live or production system. As the environment or business needs change, support and maintenance are always needed, and that can trigger the next development cycle.\n\nChapter 1\n\nImportant note\n\nWhile the SDLC is an industry-standard approach, individual organizations often introduce variations to tailor it for their software development processes. For in- stance, some organizations may choose to implement tests before writing the code, a practice known as test-driven development (TDD). Others may create a prototype system or introduce a proof of concept (POC) before conducting a feasibility analysis, a step that has become easier to perform with the help of LLMs.\n\nThere are an increasing number of books and videos that describe the use of GenAI for coding, but the technology can supercharge the entire process, not just the actual coding implementation. This book will explore several of these aspects, including testing, documenting, and monitoring software. These are critical for the success of software projects.\n\nNext, we will see how we can embrace a comprehensive GenAI toolkit in our technological stack as software developers.\n\nEmbracing a GenAI toolkit This book focuses on three separate tools for software development: ChatGPT, OpenAI API, and GitHub Copilot. In 2024, these three tools had roughly a $35 million combined market size for software engineering applications. The market is expected to grow 25% per year throughout the rest of the decade, according to a Research and Markets report. The following chapters of the book will provide instructions on how to subscribe to these services and how to get started. These tools provide distinct kinds of functionality, and knowing when to use which tool is part of the learning curve. Later chapters will highlight the features and use cases for each of the tools.\n\nChatGPT OpenAI has been a leader in LLMs since 2015. Led by CEO Sam Altman, the company has produced multiple versions of its Generative Pretrained Transformer (GPT) LLM. While these were well received, the release of ChatGPT in December 2022 transformed the perspectives of AI worldwide.\n\nChatGPT is an AI-driven chatbot, an application that is designed for text conversations using natural language. Its release spurred widespread use, reaching 100 million users the following month. It continues to be one of the most visited websites across the world.\n\n9\n\n10\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nWhile natural language conversations with ChatGPT often succeed in eliciting answers to ques- tions, prompt engineering has proven a more robust technique. It is the art of crafting an instruc- tion to produce a more desirable output. The prompt typically consists of context, instructions, a history of the dialog, and sometimes examples of desired output. This book will provide structured formats that guide the reader to effectively perform prompt engineering for producing code, comments, tests, and other outputs.\n\nOpenAI API ChatGPT is among the most popular tools for interacting with LLMs. However, in many cases, prompt engineering lacks the simple structures found in software, such as loops and conditions. OpenAI provides a developer platform for coding directly against the same OpenAI LLM used by ChatGPT. Through its Application Programming Interface (API), OpenAI enables developers to combine software and prompt engineering. The API also provides specific added functionality that is useful for solving software engineering problems.\n\nGitHub Copilot While GitHub is one of the most popular platforms for sharing code using Git distributed version control, the company released GitHub Copilot in 2021. Originally powered by OpenAI’s LLM, it provides intelligent code completion using GenAI’s programming capability. The functionality has been integrated into many IDEs, including Visual Studio Code and PyCharm—two of the most popular IDEs for Python.\n\nUnlike the other OpenAI models, Copilot functions as a pair programmer. This concept comes from the Extreme Programming (XP) agile methodology, where two developers work together to write code with a single keyboard. Although not yet a fully functioning pair programmer, Copilot can quickly find and display references for code syntax and even provide annotated examples or full code as requested by the user. It interprets the intention from the function and variable names used. Together with the surrounding code as context, it can predict and suggest the next block of code.\n\nNext, we will review recent studies that assess the use of GenAI for software development.\n\nChapter 1\n\nImproving software development with GenAI A number of studies have assessed whether GenAI provides increased productivity in coding tasks. McKinsey reported increases ranging from minimal to 50%, depending on the complexity of the task. For code documentation and generation, the gains were much higher than for difficult tasks. They found it was particularly good for routine tasks and repetitive work, as well as initial dives into new code projects. Refactoring code to make changes and tackling new challenges were also improved through GenAI technology. Perhaps as important, their study showed that users of GenAI for software felt happier, were able to focus more on meaningful work, and achieved flow much more frequently. The study details can be found at https://www.mckinsey.com/capabilities/ mckinsey-digital/our-insights/unleashing-developer-productivity-with-generative-ai.\n\nA similar study by Exadel reported that half of the developers in their study used GitHub Copilot at least 50% of the time. Two-thirds of these developers completed tasks more quickly, saving 10-30% of their development time. Copilot made them more productive and fulfilled. See https://exadel. com/news/measuring-generative-ai-software-development/ for more details on the study.\n\nResearch by Colombatto and Rivadulla (https://aws.amazon.com/blogs/apn/transforming- the-software-development-lifecycle-sdlc-with-generative-ai/) found benefits of applying GenAI across the full SDLC. Examining data from AWS and IBM, they found that the benefits begin in the analysis phase with requirements engineering. Even in this early phase of the SDLC, the researchers observed up to a 60% reduction in time from using GenAI. They found a 30% reduction in development time and a 25% reduction in time for generating unit tests and test plans. Even though less time was spent, the code quality improved by 25%, which contributes to fewer bugs and lower software maintenance costs.\n\nA study conducted by BlueOptima from 2022 to 2024 used code repositories to analyze produc- tivity, quality, and cost across 77,338 developers. In contrast with the other studies that reported significant savings, the findings were much more modest. They found only a 3.99% increase in productivity for those with access to GenAI and a 5.12% decrease for those without. Quality still improved slightly, which is important since it proves that the productivity gains do not compro- mise quality, but the gains were not as significant. However, the study used access to these tools as an input variable without characterizing the training, familiarity, or integration of GenAI into their workflows. In addition, productivity is likely to increase as the predictive accuracy and overall performance of GenAI tools continue to rapidly improve. The details of the study can be found through this link: https://www.blueoptima.com/resource/llm-paper-1/.\n\n11\n\n12\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nNext, we will discuss our perspective on the benefits and downsides of using GenAI in software development.\n\nIs GenAI worth learning for software engineering? We have been using code completion tools for over a decade, but current GenAI tools are different. We have used the full range of tools, such as keyboard shortcuts, Stack Overflow searches for help, code API search tools, and all the latest refactoring tools and templates available in the IDE. All these strategies have helped us be more efficient in our work, but there has always been a lot of mundane, repetitive work that has limited our coding speed and enjoyment.\n\nThe benefits of coding with GenAI GenAI tools have transformed our output. Within three months of using an earlier version of GitHub Copilot, we were writing code 15% faster. Now, after two years, the combination of GitHub Copilot, ChatGPT, and OpenAI API has supercharged our coding output more than anything else that we have used. We complete twice as much work as we did previously with multiple tools. The improvements in productivity were a combination of advances in the tools themselves as well as familiarity with how to use them, both of which are covered throughout this book.\n\nBeyond the productivity in merely writing code, GenAI contributes to other aspects of software development. GenAI can help refactor code automatically, which helps make it more readable and hence maintainable. As shown in later chapters, code can also be improved by selecting better algorithms that execute faster. GenAI can also help write documentation of code and automate the creation of tests. With GitHub Copilot, the pair programming approach to efficient coding includes providing help, which is useful for senior developers, but invaluable to developers learn- ing a new language or framework.\n\nThe downsides of coding with GenAI The technology behind GenAI for software development is still quite new. Early studies from 2022 showed that GitHub Copilot’s accuracy in producing correct code was below 50%. While advances and new versions of the underlying models continue to be released every few months, they are certainly not perfect.\n\nIn fact, GenAI has produced some of the worst fatal development mistakes we have ever seen. To put that in perspective, we have seen a data scientist pushing their entire environment file to the corporate repository, which exposed secret tokens that had to be replaced. One software developer crashed a microservice after renaming a file pandas. One data engineer spent two\n\nChapter 1\n\nweeks learning Cython to handle a Python DataFrame memory issue instead of just switching to Dask or PySpark. GenAI may not only supercharge your strengths, but may also supercharge your weaknesses. After all, it is still a developing technology, but continues to improve arguably faster than anyone expected.\n\nAI coding has made the headlines, but it may not be clear why it would fail. The underlying coding models are trained on available GitHub repository data and other code that is publicly available in various languages. For problems that are widely documented, such as the Fibonacci sequence calculation or the many code snippets used to pass LeetCode interviewing questions, the answers are nearly perfect. For this reason, YouTube is full of videos showing how GitHub Copilot can program a React web page in 3 minutes.\n\nGenAI has far more difficulty solving more obscure coding tasks where there is far less training data. Even if the most famous LeetCode’s Two Sum Problem were changed slightly to include Python Threads, for example, the solution would be unpredictable.\n\nA well-documented problem with LLMs for generating text is that they tend to hallucinate or fabricate information when the answers are not apparent. Significant research is ongoing to counter this poorly understood problem. However, hallucinations and other LLM issues do occur when GenAI is applied to software engineering.\n\nSome developers worry that GenAI coding tools will turn them into less capable developers. They fear that relying on automatic code completion, suggestions, and examples will cause them to lose their programming edge or familiarity with the functions.\n\nRecent research by Michael Gerlick (https://doi.org/10.3390/soc15010006) suggests that AI tools might decrease our critical thinking capability through a process known as cognitive off- loading. However, similar arguments have been made about automated spelling checkers that produce better documents but perhaps reinforce our spelling crimes. It is true that it may take a bit longer to remember the exact syntax of adding tick marks to a matplotlib plot when the internet is down. However, if you can double your output with fewer keystrokes, you can focus on the more important problems that GenAI has yet to solve.\n\nTakeaways Recent blogs describe a new trend called vibe coding, where developers and even non-developers design and build full applications extensively using GenAI over a weekend that would probably take months. It is remarkable that the technology has advanced to the point where rapid proto- typing is effective. However, prototypes are not production code.\n\n13\n\n14\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nIn many tutorials where GenAI fails, the common wisdom is You should verify the output you get, yet none offers a pragmatic way or even a guided mindset of how to effectively evaluate the outputs and improve the code.\n\nIt is considered good practice to apply unit and other testing approaches for all code. However, using GenAI is neither about blindly trusting nor fact-checking everything. GenAI failures do not mean we have to go overboard with fact-checking any piece of code it produces. Similarly, evi- dence of GenAI success does not imply you should push every memory optimization suggestion into production.\n\nLeveraging GenAI is about developing a new set of skills to formalize the inputs and outputs obtained from LLMs. This will enable you to truly supercharge your coding tasks throughout the SDLC. It enables you to own the code whether you wrote it from scratch yourself or utilized LLMs. When you can assess the quality and risk of the output these tools generate, you will be able to transform your approach to software engineering.\n\nSummary This chapter highlighted that GenAI for coding emerged from the combination of software tool advancements with LLMs. This nascent technology applies not only to coding but can enhance many aspects of the SDLC. The combination of ChatGPT, OpenAI API, and GitHub Copilot pro- vides a complementary set of tools that have been shown to not only improve productivity and enhance code quality but can even bring happiness to programmers.\n\nAlthough the technology is new and still evolving, GenAI is already changing the software engi- neering field. This book was developed to provide a structured approach to effectively leverage the tools and achieve the best results across many aspects of the SDLC.\n\nIn the next chapter, we will introduce a quick-start guide to OpenAI API and use the chat service for coding tasks. We will build our own code completion program that takes a function’s signature as input and returns its implementation as output.\n\nChapter 1\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nVS Code Plugin: https://github.com/kiteco/vscode-plugin\n\n\n\nBegum Karaci Deniz, Chandra Gnanasambandam, Martin Harrysson, Alharith Hussin, Shivam Srivastava. Unleashing developer productivity with Generative AI: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ unleashing-developer-productivity-with-generative-ai.\n\n\n\nAlexey Girzhadovich. Scientifically Measuring the True Impact of Generative AI Software Development: https://exadel.com/news/measuring-generative-ai-software- development/.\n\nDiego Colombatto and Jose Manual Pose Rivadulla. Transforming the software development lifecycle (SDLC) with Generative AII: https://aws.amazon.com/blogs/apn/transforming- the-software-development-lifecycle-sdlc-with-generative-ai/\n\n\n\nResearch and Markets Report: Generative Artificial Intelligence (AI) in Coding Market - Forecasts from 2024 to 2029: https://www.researchandmarkets.com/ reports/6014321/generative-artificial-intelligence-ai-in?utm_source=GNE&utm_ medium=PressRelease&utm_code=8xz7cm&utm_campaign=2014387+-+Generative+Arti ficial+Intelligence+(AI)+in+Coding+Market+Research+2024-2029%2c+Profiles+o f+Codecademy%2c+CodiumAI%2c+Google%2c+IBM%2c+Microsoft%2c+NVIDIA%2c+OpenAI %2c+and+Tabnine&utm_exec=chdomspi\n\nMichael Gerlick. AI Tools in Society: Impacts on Cognitive Offloading and the Future of Critical\n\nThinking: https://doi.org/10.3390/soc15010006\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n15\n\n2\n\nYour Quickstart Guide to OpenAI API\n\nGenerative artificial intelligence (GenAI) platforms such as OpenAI are transforming entire industries by empowering everyday users to generate custom answers to questions, reports, and images. You do not have to be an AI expert to leverage OpenAI to supercharge your software de- velopment. This chapter focuses on setting up OpenAI and using its application programming interface (API) calls to generate quality code.\n\nOpenAI offers numerous services such as chat, image generation, and text-to-speech. These are available through a RESTful HTTP request, which is a standard way for applications to commu- nicate over the web, or through a programmatic interface of the openai Python package.\n\nIn our first quickstart guide of the book, we will walk through three interactive labs and three sets of requests, working with the OpenAI Chat service. We will dive into the different models available through OpenAI API, and how to impact the AI-generated output.\n\nWe will obtain our project API keys, analyze the services’ limits, and learn how to compute each request’s costs. The costs are computed from the number of tokens or subword fragments of the model’s input and output.\n\nWe will wrap up this chapter by creating a basic code completion program that can generate a Python code implementation based on a function signature.\n\n18\n\nYour Quickstart Guide to OpenAI API\n\nIn this chapter, we will explore the following topics:\n\n\n\nIntroducing OpenAI API\n\nObtaining your project API keys from OpenAI\n\n\n\nSending your first OpenAI API request using the openai Python package\n\n\n\nLearning how request costs are calculated\n\nUnderstanding rate limits and usage restrictions for free and paid OpenAI accounts\n\n\n\nAnalyzing request parameters\n\nUsing OpenAI API to generate code\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\n\n\nYour personal OpenAI account credentials\n\n\n\nAccess to the book’s GitHub repository, which is available at https://github.com/\n\nPacktPublishing/Supercharged-Coding-with-Gen-AI\n\n\n\nA virtual environment set up in your integrated development environment (IDE), either VS Code or PyCharm\n\nIf you require additional assistance in setting up your OpenAI account or a virtual environment in your IDE, refer to the Appendix for detailed instructions.\n\nIntroducing OpenAI API The OpenAI platform offers a variety of natural language processing (NLP) services, powering many AI applications. At its core, the platform leverages large language models (LLMs) that are trained on a massive amount of data, with a complex architecture and billions of learnable param- eters to produce AI-generated responses. Developers can interact with the platform through its API with text-based requests that are often called prompts. The prompts are processed, enriched, and transformed before being passed into the LLM model. The model’s output is further refined and delivered as a response to the API request.\n\nThe Chat service by OpenAI is designed for conversational interactions with the LLM where the conversation uses three types of prompt roles – user, system, and assistant:\n\nUser prompts represent an end user of the system such as a ChatGPT user, and resemble prompts we feed into ChatGPT, such as Tell me about the Fibonacci sequence.\n\nChapter 2\n\n\n\nAssistant prompts represent the model output through the conversation and resemble answers we will get back from ChatGPT, such as The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones.\n\n\n\nSystem prompts define the guidelines of the assistant behavior through the conversation, such as You are a helpful programming instructor.\n\nLike many other GenAI tools in the industry, the OpenAI API calls are primarily a paid service. It does offer some limited free usage such as three requests per minute to a restricted selection of models, excluding GPT-4o. With just $5 in OpenAI credits, you can fully engage with the examples and labs presented in this book. We will instruct you how to do this right after our initial Lab 2.1.\n\nOpenAI API through a RESTful HTTP request The OpenAI API services are accessed via RESTful HTTP requests, a communication protocol based on the principles of Representational State Transfer (REST) architecture, and using the HyperText Transfer Protocol (HTTP). RESTful HTTP provides a standardized way to send and retrieve data from servers over the internet. A request to OpenAI API includes four key components:\n\n\n\nEndpoint: The URL specifying the service you are accessing, such as https://api.openai. com/v1/chat/completions for the Chat service\n\nHTTP method: The action to perform, such as POST for OpenAI Chat service requests,\n\nwhich sends data to the server\n\nHeaders: Metadata about the request, including your API token, to authenticate and\n\nprovide context for the request\n\n\n\nBody: The data payload containing details such as the LLM to use and the input prompts\n\nHere is an example of a request to the OpenAI API Chat service using the requests package in Python. It demonstrates how to make a POST request to the service, including a header with your API key for authentication and a body containing the data payload, such as the model and a prompt:\n\nimport requests\n\nimport json\n\napi_key = \"your-key-here\"\n\nheaders = {\n\n\"Authorization\": f\"Bearer {api_key}\",\n\n\"Content-Type\": \"application/json\"\n\n}\n\npayload = {\n\n\"model\": \"your model's name here\",\n\n19\n\n20\n\nYour Quickstart Guide to OpenAI API\n\n\"messages\": \"your message here\"}\n\nresponse = requests.post(\"https://api.openai.com/v1/chat/completions\",\n\nheaders=headers,\n\ndata=json.dumps(payload))\n\nchat_response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n\nQuick tip: Enhance your coding experience with the AI Code Explainer and Quick Copy features. Open this book in the next-gen Packt Reader. Click the Copy button\n\n(1) to quickly copy code into your coding environment, or click the Explain button\n\n(2) to get the AI assistant to explain a block of code to you.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR visit packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nNote that this code sample can also be found at the book’s repository path, ch2/code_samples/ openai_request.py.\n\nIn this example, the payload is submitted in JSON format. If the request is successfully accepted and the server responds, the OpenAI API Chat service’s output can be retrieved from response. json()[\"choices\"][0][\"message\"][\"content\"].\n\nChapter 2\n\nFor Python developers, a more efficient approach to accessing the OpenAI Chat service is available. Rather than dealing with the complexities of crafting and sending RESTful HTTP requests, such as constructing the correct endpoint URL, setting authentication tokens in headers, formatting payloads in JSON, and handling potential response errors, we can leverage the dedicated openai package.\n\nOpenAI API Python package installation The openai Python package simplifies the process by abstracting complexities such as URL paths, retries, error handling, and authentication. This allows us to interact directly with Python objects, instead of manually constructing RESTful HTTP requests.\n\nTo install the openai package, run the following command in your terminal within your virtual environment:\n\n(.venv) $ pip install openai\n\nTo verify that the openai package is installed correctly on your virtual environment, run the following command, which specifies the information of the package:\n\n(.venv) $ pip show openai\n\nYour console will output information about the package version, author, and support email.\n\nWith the openai package installed, we can simplify our interactions with the OpenAI platform using a programmatic framework. Instead of making direct RESTful HTTP requests, we will uti- lize Python classes. This results in fewer boilerplates, such as handling authentication and error management, and a more maintainable code.\n\nImportant note\n\nIf you are already working with the book repository, you do not need to install the openai package separately. It is already specified in the requirements.txt file and should be installed in your virtual environment. If you require additional assistance in setting up your virtual environments, refer to the Appendix for detailed instructions.\n\n21\n\n22\n\nYour Quickstart Guide to OpenAI API\n\nObtaining your project API keys from OpenAI All requests to OpenAI API require an authentication token, and we must obtain one prior to submitting any request.\n\nNavigate to https://platform.openai.com and log in with the same credentials as you use for ChatGPT at https://chat.openai.com/.\n\nOnce logged in, click on Dashboard in the upper-right menu, then select API keys from the left menu.\n\nRecently, OpenAI replaced personal API keys with the option to create a project API key. This new key serves two purposes. First, it allows personal use, as required for the work throughout this book. Second, it supports service API keys intended for non-human identities such as microser- vices running in production.\n\nFigure 2.1: Project API keys dashboard\n\nTo generate your first API key, click on Create new secret key under your account icon at the upper left. Choose an API key that is owned by You, as we will use this key rather than assign it to a service account. You can assign a name for future reference, and it should be associated with a project (named default project if you have not changed the project name yet).\n\nChapter 2\n\nFigure 2.2: Creating a new secret key for personal use\n\nOnce you click on Create secret key, the platform will generate a new secret key for you.\n\nReturn to your IDE, either VS Code or PyCharm, and export it as an environment variable from your terminal by running the following:\n\n$ export OPENAI_API_KEY=\"your-key-here\"\n\nAlternatively, for a reproducible environment, you can use the .env file template provided with this book’s repository:\n\nFigure 2.3: The .env file template, placed at the root of the repository\n\n23",
      "page_number": 26
    },
    {
      "number": 3,
      "title": "Assistant prompts represent the model output through the conversation and resemble answers we will get back from ChatGPT, such as The Fibonacci sequence is a series of numbers where each number is the",
      "start_page": 54,
      "end_page": 81,
      "detection_method": "regex_chapter",
      "content": "24\n\nYour Quickstart Guide to OpenAI API\n\nIf you choose to work with .env for your environment variables management, you should verify that your running configurations are set to work with this file. Refer to the Appendix if you need additional assistance.\n\nNext, we will learn how to submit our first request to OpenAI API using the openai Python pack- age, following a template that will be utilized throughout this book.\n\nLab 2.1 – sending your first OpenAI API request using the openai Python package With the installed Python package and the API Keys, we are all set to execute our first OpenAI API request. Note that you do not need OpenAI platform credits to complete this lab, and we will instruct you how to load credits to your account later in this chapter. If you have previously loaded credits to the OpenAI platform, this example should cost no more than a fraction of a cent to execute.\n\nIn this lab, we will start with a simple template for executing OpenAI API requests to the Chat service and printing the response output. We will have three steps:\n\n1. Create an openai.OpenAI class instance.\n\n2. Submit a request to the Chat service with a single user prompt and a model selection such\n\nas GPT-4o-mini or GPT-3.5-turbo.\n\n3. Print the response output.\n\nThese steps are in place in our starter code, and we will walk you through completing it, running it, and analyzing the results we get:\n\nimport openai\n\nfrom openai import OpenAI\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"\", # Your model's name here\n\nmessages=[] # Your messages here\n\n))\n\nprint(\"your print here\")\n\nIn this lab walk-through, we will build on this starter code to complete the model and messages ar- guments, enabling us to receive an AI-generated response to our request from the OpenAI platform.\n\nChapter 2\n\nLab 2.1 guided walk-through Open Lab 2.1 in the book’s repository located at ch2/labs/lab21.py. First, we will instantiate a new instance of the openai.OpenAI class:\n\nclient: OpenAI = OpenAI()\n\nWe do not specify any authentication parameters directly in the code. The OpenAI client automat- ically retrieves the API key from the OPENAI_API_KEY environment variable. While it is possible to pass the API key using the api_key argument during instantiation, we strongly advise against hardcoding credentials in scripts for security reasons.\n\nThe next step is to send a request using the OpenAI client. We will initiate a call to the Chat ser- vice by accessing chat.completion, where we will define the necessary RESTful HTTP request parameters. This includes at least two parts: the model and messages keyword arguments. The model argument represents the name of the LLM you are querying, and the messages argument represents the input prompts sent to the model:\n\ncompletion = client.chat.completions.create(\n\nmodel=\"\", # Your model's name here\n\nmessages=[] # Your messages here\n\n)\n\nIf you have loaded credits onto your OpenAI platform account, enter the model value between the double quotes:\n\ngpt-4o-mini\n\nOtherwise, use the following model instead:\n\ngpt-3.5-turbo\n\nNext, add a single user prompt to the messages argument:\n\nmessages = [{\"role\": \"user\", \"content\": \"What is the FizzBuzz problem?\"}]\n\nThe last step is to print the response output of the Chat service. On the last line, replace the print statement with the following:\n\nprint(completion.choices[0].message.content)\n\n25\n\n26\n\nYour Quickstart Guide to OpenAI API\n\nAt this point, your Lab 2.1 code should look as follows:\n\nimport openai\n\nfrom openai import OpenAI\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[{\"role\": \"user\", \"content\": \"What is the FizzBuzz problem?\"}]\n\n)\n\nprint(completion.choices[0].message.content)\n\nLab 2.1 is now ready to run and will print the Chat completion in response to the following user prompt: What is the FizzBuzz problem?.\n\nNote\n\nWhen working on your lab code, you can always find the full lab solutions in the ch2/solutions folder.\n\nLab 2.1 analysis Run the script and check the output in the terminal if you are using VS Code, or the run console if you are using PyCharm. The output of the program should be the Chat service’s response to the user prompt What is the FizzBuzz problem?, a problem we will revisit in later chapters.\n\nDepending on the model you chose to work with (either GPT-3.5-turbo or the more advanced GPT- 4o-mini), you might get quite a different output from the Chat service. For your reference, here is a response from the GPT-3.5-turbo model, which includes a context and problem description:\n\nFizzBuzz is a common programming problem often used in job interviews to test a candidate's ability to write simple code. The problem is as follows:\n\nWrite a program that prints the numbers from 1 to 100. But for multiples of three, print \"Fizz\" instead of the number, and for the multiples of five, print \"Buzz For numbers which are multiples of both three and five, print \"FizzBuzz\"\n\nChapter 2\n\nHere is a response from the GPT-4o-mini model, which, in addition to the context and descrip- tion, included a detailed explanation of the coding task stages, an example output, and the task’s challenges:\n\nThe FizzBuzz problem is a classic programming task often used in coding interviews and educational settings to test a developer's basic understanding of control structures, loops, and conditional statements. The problem is simple:\n\n1. Write a program that prints the numbers from 1 to a specified number, typically 100.\n\n2. For multiples of three, print \"Fizz\" instead of the number.\n\n3. For multiples of five, print \"Buzz\" instead of the number.\n\n4. For numbers that are multiples of both three and five, print \"FizzBuzz.\" The output would look something like this:\n\n```\n\n1, 2, Fizz, 4, Buzz, Fizz, 7, 8, Fizz, Buzz, 11, Fizz, 13, 14, FizzBuzz, ...\n\n```\n\nThis task helps demonstrate an understanding of loops and conditionals, often revealing both logical reasoning and code clarity.\n\nIn Chapter 6, we will explore the underlying differences between GPT-4o and GPT-3.5 in greater detail.\n\nImportant note\n\nAs discussed in Chapter 1, applications built on top of LLMs rarely output determinis- tic results, and responses for similar prompts may differ due to prompt construction, user customization, and randomness. Moving forward in this book, we will discuss further how to control all three to achieve the desired outputs from the model.\n\nNext, we will explore how to anticipate the costs associated with each API call and understand how tokens, as fragments of words, are used to calculate the request charges.\n\n27\n\n28\n\nYour Quickstart Guide to OpenAI API\n\nLearning how request costs are calculated Since OpenAI API is primarily a paid service, it is essential to understand and anticipate the costs associated with each request.\n\nBefore proceeding with the lab exercises, it is strongly recommended that you add credits to your OpenAI platform account. An initial charge of $5 in credits should be more than enough to complete all the labs in this book. Loading credits will also upgrade your account from Free tier usage (with no prior credits) to a Tier 1 account, significantly increasing your rate and usage limits, as we will discuss later in this chapter.\n\nTo load credits, open the OpenAI Billing Overview page at https://platform.openai.com/ settings/organization/billing/overview and add a payment method with a minimum charge of $5.\n\nFigure 2.4: Remaining credit balance\n\nYou can also check your remaining balance at any time by visiting the same link.\n\nUnderstanding tokens The OpenAI platform calculates request costs based on tokens, which are small segments of text (often subwords, words, or characters) averaging about 0.75 words or 4 characters each. Text is divided into tokens by elements such as punctuation, root words, and word endings. We will discuss tokens in detail in Chapter 6.\n\nChapter 2\n\nEach API request consists of both input and output tokens, and OpenAI charges for both. For example, in Lab 2.1, the message {\"role\": \"user\", \"content\": \"What is the FizzBuzz problem?\"} contains 15 tokens. You can access the exact count from the prompt_token attribute as in the following code:\n\n>> completion.usage.prompt_tokens\n\n>> 15\n\nAlternatively, we can estimate our total input token consumption using tools such as the Ope- nAI Tokenizer, available at https://platform.openai.com/tokenizer. The Tokenizer also col- or-codes the different tokens in the text, identifying words such as role and user as a single token, punctuation marks such as commas and question marks as a single token each, and FizzBuzz as three tokens.\n\nFigure 2.5: OpenAI Tokenizer simulation\n\nOutput tokens are also billed. For example, in Lab 2.1, the output response from the GPT-4o- mini model contains 292 tokens. You can access the completion_tokens attribute by calling the following:\n\n>> completion.usage.completion_tokens\n\n>> 292\n\nThis brings the total billable tokens to 307. You can access the total_tokens attributes by calling the following:\n\n>> completion.usage.total_tokens\n\n>> 307\n\n29\n\n30\n\nYour Quickstart Guide to OpenAI API\n\nHow billing is computed OpenAI charges for both input and output tokens, with costs varying by model. Newer, more advanced models typically have higher costs, while optimized versions, such as models with the -mini or -turbo postfix, are more affordable. You can find the full pricing details at https:// openai.com/api/pricing/.\n\nAs of January 2025, input tokens for the GPT-4o model are priced at $2.50 per million tokens (or $1 for every 400,000 input tokens), while output tokens are priced at $10 per million tokens (or $1 for every 100,000 output tokens).\n\nIn Lab 2.1, we used the more affordable GPT-4o-mini model. Input tokens for this model cost $0.15 per million tokens (or $1 for every 6.6 million input tokens), and output tokens are priced at $0.60 per million tokens (or $1 for every 1.6 million output tokens). These costs are approximately 1/16 of the GPT-4o model’s pricing.\n\nFigure 2.6: Costs of GPT-4o input and output tokens\n\nFor example, we can calculate the cost of running Lab 2.1, which uses 15 input tokens and 292 output tokens. Based on the pricing for the GPT-4o-mini model, the cost is as follows:\n\n>> (15 * 0.15 + 292 * 0.6) / 1000000\n\n>> 0.00018\n\nThis is approximately 1/55 of a cent.\n\nChapter 2\n\nIf we run Lab 2.1 with the larger GPT-4o model instead, the cost would be as follows:\n\n>> (15 * 2.5 + 292 * 10) / 1000000\n\n>> 0.002\n\nThis amounts to 1/5 of a cent.\n\nKeep these cost differences in mind when designing your own applications, especially when de- ciding between using more advanced models and their smaller, more affordable versions. Model prices have historically declined as newer models are introduced, so it is a good idea to periodically review the Pricing page at https://openai.com/api/pricing/.\n\nWith a clear understanding of cost calculations, we can now move on to discussing the limitations and restrictions associated with different tiers of OpenAI accounts.\n\nUnderstanding rate limits and usage restrictions for free and paid OpenAI accounts OpenAI usage is limited by three factors: requests per minute (RPM), requests per day (RPD), and tokens per minute (TPM). The limits of each vary based on both the model and the type of account you have. For Free tier accounts with no credits added, the limit is three RPM, and only certain models are available, excluding GPT-4o. Token limits tend to be more restrictive for advanced models such as GPT-4o to all account tiers, whereas older or mini models, such as GPT-4o-mini, have more relaxed limits.\n\nFor reference, here are the rate limits for Free tier accounts as of January 2025:\n\nFigure 2.7: Rate limits example for Free tier accounts\n\n31\n\n32\n\nYour Quickstart Guide to OpenAI API\n\nAnd here are the rate limits for Tier 1 accounts as of January 2025:\n\nFigure 2.8: Rate limits example for Tier 1 accounts\n\nWe encourage you to learn about your account’s API limits by navigating to the Settings sec- tion in the upper-left corner of your organization: https://platform.openai.com/settings/ organization/limits.\n\nWith Tier 1’s increased RPM and TPM limits in place, we’re now ready to move forward to the next lab, where we will explore additional parameters for OpenAI API.\n\nLab 2.2 – analyzing request parameters In Lab 2.2, we will expand on our basic OpenAI API request to the Chat service by adding addi- tional parameters to customize the model’s response. This time, we will ask the Chat service to explain the Two Sum problem, a topic we will explore further in later chapters.\n\nWe will use a similar starter code from Lab 2.1, which includes setting up an OpenAI client and print statements:\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create())\n\nprint(\"Completion Tokens: \")\n\nprint(\"Output: \")\n\nChapter 2\n\nIn this lab’s guided walk-through, we will add the following payload parameters:\n\nn: Specifies the number of response variations\n\ntemperature: Controls the level of randomness in the model’s output ranging from 0 to 2\n\nmax_tokens: Limits the number of tokens in each response\n\nApplications such as code completion services often offer users multiple suggestions from a lan- guage model, so we may want to retrieve more than one response. Additionally, in some cases, such as generating a creative prompt (for example, Give me a marketing slogan for a book about GenAI for coding), we might prefer a higher degree of randomness in the output that can be obtained by setting the temperature parameter closer to its maximum value of 2.\n\nThe max_tokens parameter limits the number of tokens in the response, though it does not guar- antee the output will be fully complete within that limit. When using max_tokens, OpenAI charges for the specified token limit rather than the actual number of tokens used, so it is best to set it close to the expected length of the response.\n\nLab 2.2 guided walk-through This walk-through consists of two steps: updating the payload with additional parameters and adding a print statement to display the extra responses.\n\nOpen Lab 2.2 in the book repository at ch2/labs/lab22.py and ensure that you have either set the OPENAI_API_KEY environment variable or configured your run settings to point to the .env file.\n\nAs the first step, update the arguments for the .create method on line 7 as follows:\n\n1.\n\nSet the model to gpt-4o-mini:\n\nmodel = \"gpt-4o-mini\"\n\nIf you have not added credits to your OpenAI platform account yet, your access may be limited to certain models, excluding gpt-4o-mini. In that case, use the gpt-3.5-turbo model instead.\n\n2. Set a low temperature argument to reduce the randomness of the output:\n\ntemperature=0.2\n\n3. Set the max_tokens argument to restrict the model’s output to a maximum of 100 tokens:\n\nmax_tokens=100\n\n33\n\n34\n\nYour Quickstart Guide to OpenAI API\n\n4. Set the n argument to instruct the model to return a single response for the request:\n\nn=1\n\n5. Add a system prompt to the messages argument to give the agent the tone of a tech com-\n\npany hiring manager:\n\n{\"role\": \"system\", \"content\": \"You are a hiring manager at a tech company.\"}\n\n6. Add an additional user prompt to the messages argument asking about the Two Sum\n\nproblem:\n\n{\"role\": \"user\", \"content\": \"What is the Two Sum problem?\"}\n\n7. For the second step, update the print statements to display the output token count and\n\nthe model output.\n\n8. Modify the first print statement to indicate the output count:\n\nprint(\"Completion Tokens: \", completion.usage.completion_tokens)\n\n9. To display the output response, update the second print statement as follows:\n\nprint(\"Output: \", completion.choices[0].message.content)\n\nIf you followed all the steps correctly, your code should look like this:\n\nimport openai\n\nfrom openai import OpenAI\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\ntemperature=0.2,\n\nmax_tokens=100,\n\nn=1,\n\nmessages=[{\"role\": \"system\", \"content\": \"You are a hiring manager at a tech company.\"},\n\n{\"role\": \"user\", \"content\": \"What is the Two Sum problem?\"}],\n\n))\n\nChapter 2\n\nprint(\"Completion Tokens: \", completion.usage.completion_tokens)\n\nprint(\"Output: \", completion.choices[0].message.content)\n\nLab 2.2 is now ready to run and will print the output token count along with the response to the user prompt What is the Two Sum problem?\n\nLab 2.2 analysis Run the script and check the output. If you are using VS Code, look in the terminal, and if you are using PyCharm, check the run console. Ensure that the output token count does not exceed 100 and that the Chat service begins explaining the Two Sum problem, a topic we will revisit in future chapters.\n\nKeep in mind that setting a max_tokens limit does not guarantee a complete explanation and may truncate responses.\n\nThe following is a sample output, limited to 100 tokens, obtained from running the lab. The completion was truncated at 100 tokens, equivalent to 84 words:\n\n\"The Two Sum problem is a classic algorithmic problem often encountered in coding interviews and competitive programming. The problem can be stated as follows:\n\nGiven an array of integers `nums` and an integer `target`, you need to determine if there are two distinct indices `i` and `j` in the array such that the sum of the elements at these indices equals the target value. In other words, you need to find two numbers in the array that add up to the specified target.\n\n### Problem Statement\"\n\nAlthough this output is truncated, it may still provide sufficient information to understand the Two Sum problem.\n\nNote\n\nQuestion: Can you compute the cost of the lab22.py request?\n\nAnswer:\n\n>> (28 * 0.15 + 100 * 0.6) / 1000000>> 0.000064\n\nThis is approximately 1/150 of a cent.\n\n35\n\n36\n\nYour Quickstart Guide to OpenAI API\n\nMultiple service responses with Lab 2.2 Reflecting on Lab 2.2, we can now better understand the impact that payload request parameters have on the initial output from the Chat service.\n\nIn that lab, we set the n parameter to 1, which is also the default value. However, in some GenAI applications, it may be beneficial to generate more than one response. For example, in tools such as GitHub Copilot or other code completion services, displaying multiple suggestions can enhance user experience. In such cases, increasing the number of responses with the n argument allows the model to provide multiple outputs. Keep in mind that the billing will include all generated outputs along with the input tokens.\n\nIn Lab 2.2, modify the n argument to request three different responses, and increase the temperature parameter to encourage more randomness and variety in the output:\n\nn=3\n\ntemperature = 2\n\nNow, update the print statements to include the additional outputs from the Chat service:\n\nprint(\"Output 1: \", completion.choices[0].message.content)\n\nprint(\"Output 2: \", completion.choices[1].message.content)\n\nprint(\"Output 3: \", completion.choices[2].message.content)\n\nRun the lab again to view the three different responses from the Chat service explaining the Two Sum problem. Feel free to experiment with the parameters further to see how they affect the output.\n\nNext, we will explore how to utilize OpenAI API for coding-related tasks, such as generating a Python code implementation based on a function’s signature.\n\nLab 2.3 – using OpenAI API to generate code Now that we have learned how to execute OpenAI API calls and configure various parameters, we can use them to create our own basic code completion program. Previously, developers could select models such as code-davinci-002, which were specifically trained for code completion. These models have since been deprecated and now chat services can effectively handle code completion tasks.\n\nChapter 2\n\nRouting chat capabilities to code completion In this lab, we will create a code completion program by following five steps:\n\n1. Define the input to function signatures only.\n\n2. Add system prompts to guide the model in generating the complete code.\n\n3. Wrap the function signature with a specific instruction for the model to complete the code.\n\n4. Get the Chat service response for our request.\n\n5. Extract only the code from the model’s output to present to the end user.\n\nThese steps demonstrate that applications built on top of LLM models require a tailored approach such as using wrapping designs and specific prompts to align the model’s output with the busi- ness problem at hand. We will cover that in depth later in the book.\n\nOur starter code consists of all five steps, which we will complete in the guided walk-through:\n\nimport openai\n\nfrom openai import OpenAI\n\nUSER_PROMPT = \"\"\"\n\nuser prompt here\n\n\"\"\"\n\nSYSTEM_PROMPT = \"system prompt here\"\n\ndef get_code_with_instructions(code: str) -> str:\n\n\"\"\"\n\nAdd a comment to the code for specific code completion instruction\n\n:param code: Python code as string\n\n:return: The code with additional instruction - \"Complete this code\"\n\n\"\"\"\n\nreturn code + \"your wrapping instructions here\"\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create())\n\n37\n\n38\n\nYour Quickstart Guide to OpenAI API\n\nfor i in range(2):\n\noutput = completion.choices[i].message.content\n\nprint(f\"Output {i + 1}:\")\n\ntry:\n\nsuggested_code = output.split(\"```\")[1]\n\nprint(suggested_code)\n\nexcept IndexError:\n\nprint(output)\n\nIn this lab walk-through, we will learn how to complete the system prompt, wrapping instruc- tions for the get_code_with_instructions function, and payload arguments for client.chat. completions.create. Combined with a function signature as the user prompt, these elements will channel OpenAI’s Chat service capabilities toward a targeted code completion program.\n\nLab 2.3 guided walk-through Open Lab 2.3 in the book repository at ch2/labs/lab23.py and ensure that you have either set the OPENAI_API_KEY environment variable or configured your run settings to point to the .env file.\n\nFirst, set the user prompt to be a function signature for printing the Fibonacci sequence:\n\nUSER_PROMPT=\"\"\"\n\ndef print_fibonacci_sequence(n: int) -> None:\n\n\"\"\"\n\nNext, include a system prompt to set guidelines for the assistant to act as an AI pair programmer:\n\nSYSTEM_PROMPT = \"You will be provided with a Python function signature. Your task is to implement the function. Return code only. \"\n\nWe will also wrap the function signature to include a specific instruction for code completion. Update the get_code_with_instructions function to wrap the given code with an extended comment that instructs the model to complete the function signature:\n\nreturn code + \"\\n# Complete this code\"\n\nChapter 2\n\nNext, configure the payload arguments by specifying the appropriate model, the desired number of responses, and the level of randomness:\n\n1.\n\nSet the model to gpt-4o-mini:\n\nmodel = \"gpt-4o-mini\"\n\nIf you have not loaded credits to your OpenAI platform account yet, your access may be limited to certain models, excluding gpt-4o-mini. In that case, use the gpt-3.5-turbo model instead.\n\n2. Set a medium temperature argument to increase the randomness of the outputs:\n\ntemperature=1\n\n3. Set the n argument to instruct the model to return two different coding suggestions:\n\nn=2\n\n4. Finally, set up the request arguments to include the wrapped function signature as the\n\nuser prompt, along with a system prompt:\n\nmodel = \"gpt-4o-mini\"\n\ntemperature=1\n\nn=2\n\nmessages = [\n\n{\"role\": \"system\",\n\n\"content\": SYSTEM_PROMPT},\n\n{\"role\": \"user\",\n\n\"content\": get_code_with_instructions(USER_PROMPT)}\n\n]\n\nIf you followed all the steps correctly, your lab’s code should be implemented as follows:\n\nimport openai\n\nfrom openai import OpenAI\n\nUSER_PROMPT = \"\"\"\n\ndef print_fibonacci_sequence(n: int) -> None:\n\n\"\"\"\n\nSYSTEM_PROMPT = \"You will be provided with a Python function signature.\n\n39\n\n40\n\nYour Quickstart Guide to OpenAI API\n\nYour task is to implement the function. Return code only.\"\n\ndef get_code_with_instructions(code: str) -> str:\n\n\"\"\"\n\nAdd a comment to the code for specific code completion instruction\n\n:param code: Python code as string\n\n:return: The code with additional instruction - \"Complete this code\"\n\n\"\"\"\n\nreturn code + \"\\n# Complete this code\"\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\ntemperature=1,\n\nn=2,\n\nmessages=[\n\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n\n{\"role\": \"user\", \"content\": get_code_with_ instructions(USER_PROMPT)}\n\n],\n\n)\n\n)\n\n...\n\nLab 2.3 is now ready to run and will print two coding suggestions for the Fibonacci sequence.\n\nLab 2.3 analysis In this lab, we wrapped the user prompt and added a supporting system prompt to explicitly define the task for the Chat service: to complete a function that prints the Fibonacci sequence based solely on its signature. The following is a sample output generated when running this lab with two outputs. The first response provides a shorter solution without type checking, printing the Fibonacci numbers one by one. The second response includes input validation, creates the sequence up to the n limit, and then prints the entire sequence:\n\nOutput 1:\n\ndef print_fibonacci_sequence(n: int) -> None:\n\nChapter 2\n\na, b = 0, 1\n\nfor _ in range(n):\n\nprint(a, end=' ')\n\na, b = b, a + b\n\nprint() # for a new line after the sequence\n\nOutput 2:\n\ndef print_fibonacci_sequence(n: int) -> None:\n\nif n <= 0:\n\nprint(\"Please enter a positive integer.\")\n\nreturn\n\nsequence = []\n\na, b = 0, 1\n\ncount = 0\n\nwhile count < n:\n\nsequence.append(a)\n\na, b = b, a + b\n\ncount += 1\n\nprint(sequence)\n\nTo further explore code completion, try adding additional system prompts such as add a docstring or add type hints and observe the results. You can also increase the number of re- turned outputs or modify the instructions in the wrapped user prompt to see how these changes affect the generated code.\n\nImportant note\n\nIn this lab, the system prompt includes specific instructions for the desired output: Return code only. In later chapters, we will explore how to craft more effective instructions to guide the model toward the desired output structure using best practices and prompt engineering techniques.\n\n41\n\n42\n\nYour Quickstart Guide to OpenAI API\n\nSummary In the first quickstart guide, we used the openai Python package to build three different programs based on AI-generated output. We experimented with various models, including GPT-4o and GPT-3.5, explored different request parameters, and combined user prompts with system prompts.\n\nWe also loaded credits to the OpenAI platform, which increased our rate limits and allowed us to use a broader range of models. We learned how request costs are calculated based on the number of input and output tokens.\n\nFinally, we developed a basic code completion program that wrapped a function signature, guiding the model to generate the rest of the function’s implementation.\n\nWith these foundations in place, we can progress to the next chapter, Quickstart Guide to GitHub Copilot, and better understand how Copilot generates coding suggestions and how to tailor them to fit our unique preferences when necessary.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: What is the difference between a system prompt and a user prompt in a Chat service request?\n\nAnswer: User prompts are the inputs provided by the end user, such as the questions or com- mands given to ChatGPT. They represent the direct interaction with the AI-generated content, such as the following:\n\nExplain the Two Sum problem\n\nHere is another example:\n\ndef print_fibonacci_sequence(n): # complete this code\n\nSystem prompts, however, define the guidelines and tone for the Chat interaction. They instruct the assistant on how to behave during the conversation with the end user, as in this example:\n\nYou are a technical interviewer for a software engineering internship\n\nChapter 2\n\nHere is another example:\n\nYou will be provided with a Python function signature. Your task is to implement the function. Return code only.\n\nQuestion 2: What are tokens, and how do they impact the cost of an OpenAI API request?\n\nAnswer: Tokens are small segments of text, typically averaging around 0.75 words or 4 characters each when the text is split into tokens based on elements such as punctuation, root words, and word endings. The cost of an OpenAI API request is calculated based on the number of input tokens, output tokens, and the max_tokens argument, with different models having varying token costs.\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\nOpenAI Platform home page: https://platform.openai.com\n\nOpenAI API official documentation: https://platform.openai.com/docs/overview\n\nOpenAI Pricing: https://openai.com/api/pricing/\n\nOpenAI Rate Limits (General): https://platform.openai.com/docs/guides/rate-\n\nlimits\n\nOpenAI available models: https://platform.openai.com/docs/models/model-\n\nendpoint-compatibility\n\nOpenAI individual rate limits: https://platform.openai.com/settings/organization/\n\nlimits\n\n\n\nThe FizzBuzz wiki page: https://en.wikipedia.org/wiki/Fizz_buzz\n\n\n\nThe Two Sum Leet Code page: https://leetcode.com/problems/two-sum/description/\n\n43\n\n44\n\nYour Quickstart Guide to OpenAI API\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.\n\n3\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nGitHub Copilot, an AI pair programmer, is enabling developers to supercharge their coding abil- ities and write code more efficiently. The application uses an underlying large language model (LLM) designed specifically for code completion tasks, code analysis, and chat for coding-related tasks.\n\nSince we built our own code completion program capable of completing a Python function based on its signature alone in Chapter 2, we can better understand the role of LLMs in Copilot’s imple- mentation, as well as what other preprocessing and postprocessing work is required.\n\nThis chapter is dedicated to working with GitHub Copilot in our integrated development en- vironment (IDE). First, we will set up a GitHub Copilot account and determine who is eligible for a free account. Then, we will connect our account to our IDE (either VS Code or PyCharm) and explore the three interaction modes of the tool: chat, completion, and analysis. We will also work with Copilot in a Jupyter Notebook using VS Code. PyCharm cannot run Jupyter notebooks yet with Copilot.\n\nThrough three interactive labs, we will implement a geometric mean calculator using all three GitHub Copilot interaction modes: chat, completion, and analysis. Additionally, we will exper- iment with keyboard shortcuts for core Copilot functionality to help streamline our workflow and boost productivity.\n\n46\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nIn this chapter, we will cover the following key topics:\n\n\n\nIntroducing GitHub Copilot – our AI pair programmer\n\nUnderstanding GitHub Copilot’s free account limits, costs, and policies\n\n\n\nSetting up Copilot in PyCharm and VS Code\n\nUsing Copilot chat\n\n\n\nIntroducing completion and keyboard shortcuts for geometric mean calculations\n\n\n\nAnalyzing code with Copilot\n\nWorking with Copilot in VS Code’s Jupyter Notebook for code analysis\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub account\n\n\n\nAccess to IDE – either VS Code or PyCharm\n\n\n\nAccess to the book’s repository, which is available at https://github.com/\n\nPacktPublishing/Supercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in your preferred IDE, VS Code or PyCharm\n\nIf you require additional assistance in setting up your OpenAI account or a virtual environment in your IDE, refer to the Appendix for detailed instructions.\n\nIntroducing GitHub Copilot – our AI pair programmer GitHub Copilot is an AI-powered tool that uses LLMs trained on vast and diverse datasets contain- ing a significant amount of open source code from public repositories across various programming languages, including Python. By combining the model’s extensive knowledge with your code’s context, GitHub Copilot provides tailored coding suggestions, as well as chat and code analysis features, to assist in your development process.\n\nCode completion design overview To provide meaningful code completions, Copilot processes the input and output of the LLM, like the code completion program developed in Lab 2.3. For the input, Copilot structures a prompt based on the lines surrounding the cursor, function signatures, and additional context, such as recent edits, Git details, file names and open files. The LLM’s output is further processed, including validation to ensure it compiles successfully.\n\nChapter 3\n\nWe can visualize Copilot’s code completion process as a flowchart, as shown in Figure 3.1.\n\nFigure 3.1: An overview of GitHub Copilot’s suggestion process\n\nIn addition to code completion, GitHub Copilot offers two other modes of interaction: chat (avail- able inline or through the chat window) and code analysis. Both modes are based on the under- lying code and comments but differ in their final output and the postprocessing steps involved. We will discuss these extensively later in this chapter.\n\nImportant note\n\nSome developers may think that GitHub Copilot sends the entire Git repository code they are working on to the LLM. This is very unlikely given two reasons: the cost of input tokens and the relevance of the entire code to the specific code completion task. It is far more likely that only relevant code is preprocessed by Copilot first. Then, only the necessary information including coding style preferences or relevant classes, code, and variables, is added to the prompt in addition to the cursor’s surrounding context.\n\n47\n\n48\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nUnderstanding GitHub Copilot’s free account limits, costs, and policies To enable GitHub Copilot in your IDE, you first need to subscribe to the service through GitHub’s platform. Like many other GenAI services, GitHub Copilot is primarily a paid service, though some exceptions allow free access.\n\nWho is eligible for a free account? There are three ways to qualify for GitHub Copilot at no cost:\n\n\n\nFree account available to anyone, which includes up to 2,000 code completions and 50 chat requests per month.\n\n\n\nFree pro account for students and educators with a valid university email\n\n\n\nFree pro account for maintainers of popular open source repositories\n\nIf you are a student or educator with a university email and the required documentation, you can apply for GitHub’s Student Pack at https://education.github.com/pack.\n\nIf you are a leading maintainer of a popular open source repository, you may have free access to Copilot. GitHub determines eligibility based on internal criteria, without requiring an application process. If eligible, upon clicking your profile icon and selecting Your Copilot, you will be directed to a page titled GitHub Copilot Individual instead of a payment page, as detailed later in this chapter.\n\nSome organizations and enterprises cover the cost of GitHub Copilot access for their users. If you belong to such an organization or enterprise, you will find a request access button next to the organization’s name at https://github.com/settings/copilot. In these cases, you will not be charged for using Copilot, as the organization or enterprise will handle the licensing fees on your behalf.\n\nCompared to individual accounts, GitHub Copilot for business and enterprises offers additional features, such as pull request summaries, a knowledge base, and the ability to fine-tune a custom LLM for enterprise versions. These options come at a higher cost than individual memberships.\n\nChapter 3\n\nPricing For individual users, the service is priced at $10 per month or $100 annually. For Copilot business, the rate is $19 per user per month, while the enterprise version increases to $39 per user per month.\n\nCode completion policies All GitHub Copilot users are governed by specific policies that define the scope of code suggestions, privacy, and access points, including your IDE and github.com.\n\nA notable policy is suggestions matching public code. Since the GitHub Copilot model is primarily trained on open source and publicly available code repositories, some of its suggestions may be subject to copyright protection. To mitigate potential issues, GitHub provides users with an option to exclude coding suggestions that replicate such code.\n\nFigure 3.2: Privacy and ownership options in Copilot\n\n49\n\n50\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nQuick tip: Need to see a high-resolution version of this image? Open this book\n\nin the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nHere, suggestions that match public code are allowed. We also recommend enabling additional underlying models such as Anthropic Claude and Google Gemini alongside to support further experimentation.\n\nFor business or enterprise accounts, the policies are pre-determined by the administrator of the organization or enterprise. Individual account users, however, can set their own policies during account setup or through the Copilot settings page: https://github.com/settings/copilot.\n\nActivating your GitHub Copilot account To participate in the labs and hands-on exercises in this book, you will need either a free, pro, business, or enterprise GitHub Copilot account. If you wish to set up an individual account, go to https://www.github.com, click on your profile icon, and select Your Copilot.\n\nFigure 3.3: Copilot access in the GitHub account\n\nChapter 3\n\nIf you are a leading maintainer of a popular open source repository and qualify for free access to Copilot, you will be directed to a page titled GitHub Copilot Individual page. Otherwise, you will be directed to the payment page, where you can select a free trial option or a paid plan.\n\nSelect the plan that suits you, adjust settings to align with your individual or company policies (e.g., enable or disable public code matches), and enter your payment details.\n\nOnce completed, you will have access to Copilot services at https://github.com/settings/ copilot as shown in Figure 3.4:\n\nFigure 3.4: GitHub Copilot settings header\n\nThis page shows your access level, provides links to resources for setting up Copilot in your IDE, and outlines Copilot’s privacy and ownership policies.\n\nWith our account now activated, we are ready to proceed and connect Copilot to our IDE.\n\nSetting up Copilot in PyCharm and VS Code GitHub Copilot is available via a PyCharm plugin or a VS Code extension. The core features of GitHub Copilot in both IDEs are similar, offering the three interaction modes we will cover in this book: chat, completion, and analysis.\n\nHowever, Copilot is more tightly integrated with VS Code, meaning new features, such as Jupy- ter Notebook support or switching to newer LLMs, are often released in VS Code before they are available in the PyCharm extension.\n\nSince your GitHub Copilot subscription can be used across multiple IDEs, we encourage PyCharm users to try VS Code to explore Jupyter Notebook support. However, the labs in this book are tailored for both PyCharm and VS Code.\n\n51",
      "page_number": 54
    },
    {
      "number": 4,
      "title": "We can visualize Copilot’s code completion process as a flowchart, as shown in Figure 3.1",
      "start_page": 82,
      "end_page": 108,
      "detection_method": "regex_chapter",
      "content": "52\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nGitHub Copilot plugin for PyCharm To install the GitHub Copilot plugin in PyCharm, go to PyCharm | Settings | Plugins, then search GitHub Copilot in the marketplace. After installation, you will be prompted to restart your IDE.\n\nFigure 3.5: GitHub Copilot Plugin in PyCharm\n\nWhen your IDE restarts, you will be prompted to log in to your GitHub account.\n\nTo check whether the service is available, look for the Copilot icon at the bottom of the IDE. Hov- ering over it should display Ready.\n\nFigure 3.6: GitHub Copilot plugin icon health check in PyCharm\n\nThis confirms that the plugin is set up correctly, allowing us to start using Copilot.\n\nChapter 3\n\nGitHub Copilot extension for VS Code To install the Copilot extension, navigate to Code | Extensions, search GitHub Copilot, and in- stall it. When you do this, the GitHub Copilot Chat extension will also be automatically installed.\n\nFigure 3.7: GitHub Copilot and Copilot Chat extension in VS Code\n\nAfter the installation, the GitHub Copilot extension window will open, prompting you to authen- ticate or sign up for a GitHub Copilot account.\n\nTo check whether the extension is working properly after authentication, click on the GitHub Copilot icon in the bottom-right corner to open GitHub Copilot Menu.\n\nFigure 3.8: GitHub Copilot icon in the bottom right window\n\nA drop-down list will appear, and if you are logged into your GitHub account with a stable internet connection, the status should display Ready.\n\n53\n\n54\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nFigure 3.9: Verifying GitHub Copilot extension status in VS Code\n\nThis confirms that the extension is set up correctly, allowing us to start using Copilot.\n\nThe three interaction modes – chat, completion, and analysis Originally designed to provide tailored code completion suggestions, GitHub Copilot now offers three key interaction modes powered by LLMs: chat, completion, and analysis:\n\n1. Chat is accessible via a chat window resembling the ChatGPT interface or through an inline compact view. We will use this mode when we want contextual assistance with coding-related questions.\n\n2. Completion is available automatically in our files. As we type, Copilot will suggest code completions based on the code we write. We will use this mode when implementing new code.\n\n3. Analysis includes features such as explain, fix, and tests, which are available through the chat window, the inline chat, and the edits window in VS Code. We will use this mode when interacting with existing code.\n\nIn the next three labs, we will experiment with each of these interaction modes.\n\nUsing Copilot chat The chat feature complements GitHub Copilot’s code completion capabilities. The chat window can assist with questions related to your code, external topics, terminal commands, and even the VS Code IDE itself.\n\nIn VS Code, you can open the Copilot chat window using Ctrl + Alt + I (or cmd + control + I on Mac). Alternatively, click on the Copilot icon next to the top search bar of the screen.\n\nChapter 3\n\nFigure 3.10: GitHub Copilot chat window in VS Code\n\nIn PyCharm, you can open the Copilot chat window using Ctrl + Shift + C or by selecting the GitHub Copilot Chat tab in the left-hand menu.\n\nFigure 3.11: GitHub Copilot chat window in PyCharm\n\n55\n\n56\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nLet us look at an example. Open a new chat window and enter this prompt:\n\nWhat can you do?\n\nCopilot will respond with examples of use cases across the three interaction modes, such as refactoring existing code (analysis), generating unit tests (completion), providing version con- trol assistance (chat), and more. In addition to the response itself, GitHub Copilot chat will also display the pre-processed context in addition to the prompt itself, and the steps taken to prepare the input for the LLMs such as the following:\n\n\n\nCollecting recent files\n\n\n\nRetrieving Git information\n\n\n\nAnalyzing problems and errors\n\nThis aligns with the GitHub Copilot process flow we discussed earlier in this chapter.\n\nHere is an example of the prompt, What can you do?, output in PyCharm:\n\nI can assist you with various programming tasks, such as:\n\nWriting and debugging code\n\nExplaining code snippets and concepts\n\nProviding code examples and templates\n\nAssisting with version control (e.g., Git)\n\nHelping with configuration and setup of development environments\n\nOffering best practices and optimization tips\n\nIf you have a specific question or need help with a particular piece of code, feel free to ask!\n\nHere is an example of the prompt, What can you do?, output in VS Code:\n\nI can assist with various programming tasks, including:\n\nAnswering general programming questions\n\nExplaining how the code in your active editor works\n\nReviewing the selected code in your active editor\n\nGenerating unit tests for the selected code\n\nProposing fixes for problems in the selected code\n\nScaffolding code for a new file or project in your workspace\n\nCreating a new Jupyter Notebook\n\nFinding relevant code to your query\n\nChapter 3\n\nProposing fixes for test failures\n\nAnswering questions about VS Code\n\nGenerating query parameters for workspace search\n\nRunning commands\n\nExplaining terminal outputs\n\nProviding step-by-step guidance for terminal tasks\n\nHow can I assist you today?\n\nThe PyCharm plugin and the VS Code extension may produce different outputs for the same prompt. This occurs because the GitHub Copilot application handles files in PyCharm and VS Code slightly differently, offers distinct features in each, and incorporates an element of randomness. Keep in mind that, like any GenAI output, suggestions may not always be accurate or complete, as we will observe later in this chapter.\n\nNext, we will explore how the chat window can enhance code completion tasks within our IDE, even when further exploration of a topic is needed (such as an explanation of the geometric mean).\n\nLab 3.1 – calculating geometric mean with chat completion In Lab 3.1, we will explore the geometric mean through the chat interaction mode. The geometric mean is a way of averaging numbers that are multiplicative in nature, such as financial portfolio growth rates, population growth rates, or drug concentrations that vary exponentially over time. Unlike the arithmetic mean, which calculates the sum of a sequence of numbers divided by its length, the geometric mean is the n-th root of the product of the numbers:\n\n𝑛𝑛\n\n\n\nArithematic Mean =\n\n1 𝑛𝑛\n\n∑𝑥𝑥𝑖𝑖 𝑖𝑖𝑖𝑖\n\n𝑛𝑛\n\n𝑖 𝑛𝑛\n\nGeometric Mean = (∏𝑥𝑥𝑖𝑖\n\n)\n\n𝑖𝑖𝑖𝑖\n\nFor example, the geometric mean of 5 and 20 is 10, because of the following:\n\n1 2\n\n1 2\n\n\n\n(20.5)\n\n= (100)\n\n= √100 = 10\n\n57\n\n58\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nSimilarly, the geometric mean of 5, 20, and 10 is also 10, because of the following:\n\n1 3\n\n1 3\n\n3\n\n\n\n(20 ∙ 5 ∙ 10)\n\n= (1000)\n\n= √1000\n\n= 10\n\nIn this lab, we will use the chat window along with code completion. First, we will obtain an ex- planation of the geometric mean using the chat window. Next, we will utilize Copilot to complete an implementation based on a function signature, including the name, parameters, type hints, and return values. Finally, we will print the function’s output by defining sample call parameters.\n\nLab 3.1 guided walk-through Open Lab 3.1 in the book’s repository at ch3/labs/lab31.py and ensure that you have a stable internet connection, and that GitHub Copilot is ready.\n\nThe starter code consists of three parts:\n\n# Part 1: Open the chat window and ask to explain the geometric mean.\n\n# Part 2: Implement the geometric mean function for two floating-point numbers.\n\n# Part 3: Call\n\nPart 1 is for practice only, Part 2 requires specifying the function signature for code completion, and Part 3 involves adding sample parameters and printing their response.\n\nFor Part 1, delete the comment, open the Copilot chat window and enter the following prompt:\n\nExplain the geometric mean.\n\nThis prompt will provide an explanation of the geometric mean. Note that the explanation may be truncated due to the max_tokens limit of Copilot, which we have no control over, or contain raw mathematical formatting, such as LaTeX code instead of compiled equations. Despite this, it should still provide enough information for us to understand the topic.\n\nFor Part 2, delete the comment and type the function signature to calculate the geometric mean of two floating-point numbers:\n\ndef get_geometric_mean_of_two_numbers(\n\na: float,\n\nb: float,\n\n) -> float:\n\nChapter 3\n\nThis function signature, with its name, parameters, type hints, and return type, is considered a complete prompt to Copilot that should yield a desirable implementation.\n\nIn Part 3, delete the comment and add a sample call that tests the implementation. Enter the following prompt:\n\nnum1: float = 5.0\n\nnum2: float = 20.0\n\nprint\n\nThe print statement acts as the cue for Copilot, which will likely suggest a call to the get_ geometric_mean function using num1 and num2 as arguments.\n\nIf you followed all the steps correctly, your code should look like this:\n\ndef get_geometric_mean_of_two_numbers(\n\na: float,\n\nb: float,\n\n) -> float:\n\nreturn pow(a * b, 1 / 2)\n\nnum1: float = 5.0\n\nnum2: float = 20.0\n\nprint(get_geometric_mean_of_two_numbers(num1, num2))\n\nLab 3.1 is now ready to run and will print the geometric mean of 5 and 20, which is as follows:\n\n10.0\n\nLab 3.1 analysis In this lab, we used Copilot chat to learn about the geometric mean. We also leveraged code completion based on either a function signature as a starter code or a print call as a cue.\n\nFor your reference, here is the Copilot chat response to the prompt, Explain the geometric mean. We generated it using the VS Code chat window with the GPT-4o model. The response includes raw LaTeX code and is truncated before the full output:\n\n>> The geometric mean is a measure of central tendency that is particularly useful for sets of numbers whose values are meant to be multiplied together or are exponential in nature. It is defined as the nth root of the product of n numbers.\n\nFormula\n\n59\n\n60\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nFor a set of ( n ) positive numbers ( x_1, x_2, \\ldots, x_n ), the geometric mean ( G ) is given by:\n\n[ G = \\left( \\prod_{i=1}^{n} x_i \\right)^{\\frac{1}{n}} ]\n\nSteps to Calculate\n\nMultiply all the numbers together to get the product.\n\nTake the nth root of the product, where ( n ) is the total number of values.\n\nExample\n\nConsider the numbers 2, 8, and 4:\n\nMultiply the numbers: ( 2 \\times 8 \\times 4 = 64 )\n\nSince there are 3 numbers, take the cube root: ( \\sqrt[3]{64} =\n\nChoosing the underlying models to power the chat is available in the bottom-right corner of the chat window.\n\nFigure 3.12: Model selection for the Copilot chat window\n\nWe found this response lacking due to the truncation of the example, as Copilot imposes a max_ tokens limit on model outputs, and the formatting of mathematics as raw LaTeX code. However, for topics we are already familiar with, the chat window saves considerable time and eliminates the need to use external tools, such as Google or ChatGPT.\n\nNext, we will further explore GitHub Copilot’s code completion capabilities using a variety of keyboard shortcuts to make our experience even smoother.\n\nChapter 3\n\nLab 3.2 – introducing completion and keyboard shortcuts for geometric mean calculations Keyboard shortcuts are vital for enhancing a programmer’s efficiency, especially when working with Copilot, which provides a wide range of options to accept or reject suggestions partially or fully. This lab will focus on implementing the geometric mean using code completion, incorpo- rating keyboard shortcuts to do the following:\n\n\n\nReject a suggestion\n\n\n\nRegenerate a rejected suggestion\n\n\n\nAccept a full suggestion or parts of it\n\n\n\nSwitch between different code options.\n\nIn this lab, we will implement the geometric mean for a sequence of floating-point numbers using a function signature and an instructive prompt to validate the input arguments. We will explore various implementation suggestions, selectively accepting or rejecting parts of them.\n\nImportant note\n\nThis lab includes standard keyboard shortcuts for working with Copilot. If any of these are missing or you would prefer different shortcuts, you can customize them by editing your keymap. In PyCharm, navigate to Settings | Keymap, and in VS Code, go to Settings | Keyboard Shortcuts. For further assistance with keyboard shortcuts, refer to the Appendix.\n\nLab 3.2 guided walk-through Open Lab 3.2 in the book repository at ch3/labs/lab32.py and ensure that you have a stable internet connection, and that GitHub Copilot is ready.\n\nThere is no starter code for this lab. Our task is to begin by typing an instructive function signature to calculate the geometric mean of a sequence of floating-point numbers.\n\nAdd the following code to the file. If you type it rather than copy and paste it, Copilot may offer early completion suggestions. Ignore these until you complete the prompt:\n\ndef get_geometric_mean(*nums):\n\n61\n\n62\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nAt this point, you should already see completion suggestions by Copilot. Use the following in- structions for both PyCharm and VS Code to explore these suggestions:\n\nFigure 3.13: Copilot code completion example in VS Code\n\nTo experiment with Copilot’s keyboard shortcuts, follow these steps. If any of the keyboard short- cuts are different or missing in your interface, make sure to find the alternate shortcuts:\n\n1. Accept the entire code completion by pressing Tab.\n\n2. Then, undo it by Ctrl + Z / cmd + Z.\n\n3.\n\nIn PyCharm, accept a single line from the suggestion by pressing Ctrl + Alt + the right arrow (cmd + control + right if you are a Mac user).\n\n4.\n\nIn VS Code, there is no default keyboard shortcut for this. Instead, hover over the suggested text, click on the ellipsis (...), and select Accept Line.\n\n5. Then, undo it by pressing Ctrl + Z / cmd + Z.\n\n6. Accept a single word from the suggestion by pressing Ctrl + the right arrow (cmd + the\n\nright arrow if you are a Mac user).\n\n7. Then, undo it by pressing Ctrl + Z / cmd + Z.\n\n8. Hide the suggestion by pressing Esc.\n\n9. Retrigger the Copilot suggestion by pressing Alt + \\ (option + \\ if you are a Mac user).\n\n10. Toggle to the next suggestion by pressing Alt + ] (option + ] if you are a Mac user). Go back\n\nto previous suggestions by pressing Alt + [ (option + [ if you are a Mac user).\n\n11.\n\nIn PyCharm, access the code completion pane by right-clicking on Copilot’s suggestions and selecting Copilot: Open Completion.\n\n12. In VS Code, access it by pressing Ctrl + Enter.\n\nChapter 3\n\nNext, we encourage you to modify the prompt by adjusting the function name, arguments, type hints, or return type. Keep practicing with the keyboard shortcuts and observe how the comple- tions evolve with each change.\n\nLab 3.2 analysis In this lab, we practiced keyboard shortcuts in PyCharm and VS Code to streamline pairing with Copilot, making it easier to accept, reject, or switch between completion suggestions smoothly.\n\nFor your reference, a sample completion for the get_geometric_mean implementation in Py- Charm is shown in Figure 3.14.\n\nFigure 3.14: Copilot inline menu in PyCharm\n\nWhen right-clicking on the completion and selecting Copilot: Open Completion, the completion pane displays various suggestions provided by Copilot.\n\n63\n\n64\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nFigure 3.15: Copilot code completion pane in PyCharm\n\nWe can then choose to accept any of these suggested solutions.\n\nSimilarly, VS Code will also display a variety of different completion suggestions.\n\nFigure 3.16: VS Code Copilot code completion pane\n\nChapter 3\n\nHere too, we can then choose to accept any of these suggested solutions.\n\nImportant Note\n\nIn later chapters, we will explore best practices for prompt precision and prompt engineering techniques. These will help us better structure a function’s content by covering aspects like input validation, formatting with type hints, and handling side effects such as monitoring and logging.\n\nNext, we will explore Copilot’s code analysis options, demonstrating how they complement code completion when working with existing code.\n\nAnalyzing code with Copilot The recently introduced code analysis features in both PyCharm and VS Code complement GitHub Copilot’s code completion capabilities. It can be accessed via the Copilot menu or the inline chat window using commands such as /fix /explain and /test\n\nTo use these features, right-click on your code, select Copilot, and you will see the available op- tions. This menu is accessible in both PyCharm and VS Code.\n\nHere is how the menu appears in PyCharm:\n\nFigure 3.17: Copilot code analysis in PyCharm\n\n65\n\n66\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nHere is how the menu appears in VS Code:\n\nFigure 3.18: Copilot code analysis in VS Code\n\nFor quicker access, you can use Copilot inline chat commands. In VS Code, press Ctrl + I (cmd + I for Mac).\n\nFigure 3.19: Copilot editor inline chat in VS Code\n\nIn PyCharm, press Ctrl + Shift + I (cmd + shift + I for older Mac versions of PyCharm).\n\nFigure 3.20: Copilot inline chat in PyCharm\n\nIn the inline chat window, type your code analysis command, such as /fix or /explain.\n\nChapter 3\n\nAnalyzing code with Copilot editor window in VS Code GitHub Copilot has recently introduced a dedicated editor window to enhance editing tasks and streamline working across multiple files simultaneously. To access the window, open the chat window and select Edit, as shown in Figure 3.21:\n\nFigure 3.21: Accessing Copilot’s edits window\n\nIn this editor window, you can add files you want to work on or target specific sections using the annotation #selection, as shown in Figure 3.22:\n\nFigure 3.22: Working with #selection in Copilot edits window\n\n67\n\n68\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nThe edits window supports the slash commands such as /fix, /explain, and /test, as well as tailored instructions to modify your code. For instance, to add input validation for float inputs to the following function:\n\ndef get_geometric_mean_for_three_numbers(a, b, c):\n\nWe can type the prompt:\n\n#selection add input validation for float inputs.\n\nCopilot will generate suggestions to include input validation for float types, as shown in Figure 3.23:\n\nFigure 3.23: Adding input validation with the edits window\n\nWith this prompt, Copilot presents suggestions in a version control format within the working set file, allowing us to approve or revert each change individually. We can also click Accept to apply all changes at once.\n\nIn the next chapters, we will explore best practices for crafting precise prompts to achieve more desirable results with Copilot, including its edits window. Later in the book, we will delve deeper into using the edits window for prompt engineering techniques and refactoring code.\n\nNow that we have explored how to analyze and edit code with Copilot, we can practice addressing and fixing a faulty implementation of the geometric mean, using both VS Code and PyCharm.\n\nChapter 3\n\nLab 3.3 – working with Copilot in VS Code’s Jupyter Notebook for Code Analysis The following lab focuses on code analysis. We will use the /fix command to fix an incorrect implementation of the geometric mean for three floating-point numbers. We will then use the /explain command to get a brief explanation of an implementation for the geometric mean of a sequence of numbers.\n\nThe lab is available in two formats: a Jupyter Notebook or a Python script. If you are using VS Code, choose the Jupyter Notebook version to explore Copilot’s features for notebooks. Since Jupyter Notebook support is not yet available in PyCharm, a Python script version is also provided. In Lab 3.3 guided walk-through, we will cover both formats and use Copilot’s code analysis features to fix and understand code more efficiently.\n\nLab 3.3 guided walk-through Open Lab 3.3 from the book repository: If you are using VS Code, go to ch3/labs/lab33.ipynb, and if you are using PyCharm, go to ch3/labs/lab33.py. Make sure you have a stable internet connection, and that GitHub Copilot is ready.\n\nOur starter code includes two functions that we will apply code analysis to. The first function, get_geometric_mean_for_three_numbers, incorrectly implements the geometric mean for three floating-point numbers, as it takes the square root of their product instead of the cube root.\n\nThe second function, get_geometric_mean, calculates the geometric mean for a sequence of numbers of any length. It first validates that there is at least one item in the sequence. Then, it computes the product of all the numbers using a reduce operation. It also checks that the root results in a real number, which occurs if either the product is positive, or the required root is odd. Finally, after validation, it returns the n-th root of the product for a sequence of length n:\n\nfrom functools import reduce\n\ndef get_geometric_mean_for_three_numbers(a, b, c):\n\nreturn (a*b*c)**(1/2)\n\ndef get_geometric_mean(*nums: float) -> float:\n\n\"\"\"\n\nGet the geometric mean of a sequence of numbers\n\n\"\"\"\n\n69\n\n70\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nif not len(nums):\n\nraise ValueError(\"Cannot calculate the geometric mean of an empty sequence\")\n\nproduct = reduce(lambda a, b: a * b, nums)\n\nif product < 0 and len(nums) % 2 == 0:\n\nraise ValueError(\"Cannot calculate the geometric mean\")\n\nreturn pow(product, 1 / len(nums))\n\nThe calculation in get_geometric_mean_for_three_numbers is incorrect, as it takes the square root of the product of the numbers instead of the cube root.\n\nTo fix this code, choose the fix option or type /fix in the inline chat.\n\nCopilot should suggest replacing the incorrect square root with the cube root:\n\nreturn (a*b*c)**(1/3)\n\nFor your reference, here is a sample output in VS Code with the proposed fix (note that this fix could be implemented in your code using the Accept button):\n\nFigure 3.24: Copilot fix suggestion in VS Code\n\nChapter 3\n\nIn PyCharm, here is a sample output with the proposed fix. To accept it, click the Preview button and then Accept.\n\nFigure 3.25: Copilot fix suggestion in PyCharm\n\nIn the top-left corner, the preview icon is the first on the left. Click it to open the preview mode:\n\nFigure 3.26: The Preview button for accepting Copilot changes in PyCharm\n\nThe preview window displays the original code on the left and the proposed fix on the right, with an Accept button to confirm the changes.\n\n71\n\n72\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nFigure 3.27: Accepting Copilot fix suggestions in PyCharm\n\nAccept this proposed implementation.\n\nNext, move your cursor to the get_geometric_mean function and examine how Copilot explains its four parts: validating the input length, computing the product, verifying that the geometric mean is real, and returning the n-th root of the product.\n\nRight-click on the code, select Copilot | explain, or use the keyboard shortcuts to open the inline chat and type /explain. Copilot should cover all four steps of the function.\n\nLab 3.3 analysis In this lab, we used the code analysis mode to efficiently review existing code. Since a sizable portion of our work as software developers involves maintaining code rather than generating new code, these extended capabilities are invaluable for enhancing our productivity.\n\nChapter 3\n\nFor reference, here is the code explanation we obtained in VS Code, covering the four steps of the function:\n\nThe provided code defines a function get_geometric_mean that calculates the geometric mean of a sequence of numbers. The function accepts a variable number of floating-point arguments (*nums) and returns a floating-point result.\n\nThe function begins by checking if the sequence of numbers is empty using if not len(nums). If the sequence is empty, it raises a ValueError with the message \"Cannot calculate the geometric mean of an empty sequence\". This ensures that the function does not proceed with an invalid input.\n\nNext, the function calculates the product of all the numbers in the sequence using the reduce function combined with a lambda function (lambda a, b: a * b). The reduce function applies the lambda function cumulatively to the items of the sequence, effectively multiplying all the numbers together.\n\nAfter calculating the product, the function checks if the product is negative and if the number of elements in the sequence is even (if product < 0 and len(nums) % 2 == 0). If both conditions are true, it raises a ValueError with the message \"Cannot calculate the geometric mean\". This check is necessary because the geometric mean of a set of numbers that includes an even number of negative values is not defined in the real number system.\n\nFinally, the function returns the geometric mean by taking the nth root of the product, where n is the number of elements in the sequence (pow(product, 1 / len(nums))). The pow function is used to compute the nth root, which is equivalent to raising the product to the power of the reciprocal of the number of elements.\n\nOverall, the function is designed to handle various edge cases and ensure that the geometric mean is calculated correctly for valid inputs.\n\n73\n\n74\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nOverall, Copilot explains all four steps of the function well and addresses its input. However, the interpretation of the real geometric mean output is missing from Copilot’s explanation, as shown in the highlighted part of the output:\n\nAfter calculating the product, the function checks if the product is negative and if the number of elements in the sequence is even...\n\nRather than being a limitation of the tool, this suggests that our implementation could be im- proved. We will explore this further in Chapter 4.\n\nSummary In this chapter, we took our first steps with the GitHub Copilot by setting up subscriptions and enabling Copilot in our IDE. Through three labs, we explored its three interaction modes: chat, completion, and analysis.\n\nAfter implementing a basic code completion program in Chapter 2, we were able to better under- stand how GitHub Copilot is designed. Copilot enhances the code to be completed with contextual information, such as recent edits, coding preferences, and Git logs. This information is processed into user and system prompts, which are then fed to an LLM. The model’s output is further pro- cessed before being presented to the user.\n\nThroughout the three labs, we utilized the three interaction modes to implement and refine several calculators of the geometric mean. We saw how a function signatures can be used as a prompt to Copilot. We also experimented with various completion options using keyboard shortcuts. While mastering these techniques requires some practice, they can significantly boost productivity when working with Copilot.\n\nIn the next chapter, we will discuss the three pillars of good model output: model mastery, eval- uation metrics, and precise prompts. We will also learn about the best practices in making our prompt precise with the five S’s: having a structured prompt, providing surrounding context, focusing on a single task, and having specific and short instructions.\n\nChapter 3\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions.\n\nQuestion 1: Is Copilot a paid service?\n\nAnswer: Copilot is primarily a paid service, with a few exceptions including free limited access. Students, educators, and maintainers of leading open-source repositories are also eligible for a free subscription.\n\nQuestion 2: What are the three interaction modes for GitHub Copilot?\n\nAnswer: Copilot’s original task of code completion is complemented by two additional features: chat, which is available inline or through the chat window, and code analysis, which is accessible via the Copilot right-click menu, using slash commands, such as /fix and /explain, or in the edits window within VS Code.\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\nGitHub Copilot documentation: https://docs.github.com/en/copilot/quickstart\n\n\n\nCopilot subscription plans: https://docs.github.com/en/copilot/about-github-\n\ncopilot/subscription-plans-for-github-copilot\n\n\n\nCopilot setup in your IDE: https://docs.github.com/en/copilot/setting-up-github-\n\ncopilot/setting-up-github-copilot-for-yourself\n\n\n\nCopilot best practices: https://docs.github.com/en/copilot/using-github-copilot/ best-practices-for-using-github-copilot\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n75\n\n4\n\nBest Practices for Prompting with ChatGPT\n\nGenAI applications excel in certain aspects of coding and often achieve high scores in coding assignment tests. Ongoing improvements to these models continue to push the boundaries, as they begin to master a wide range of coding-related tasks and supercharge the way we develop and refine code.\n\nTo achieve good results from GenAI applications, we will focus on three pillars: model mastery of the specific task at hand, evaluation metrics to critically assess the output, and crafting precise prompts to clearly achieve the desired outcome.\n\nThis chapter focuses on best practices for crafting prompts in coding-related tasks, guided by the five S’s: structured prompt, including surrounding context, focusing on a single task, providing specific instructions, and keeping the prompt as short as possible. Later in the book, we will delve into advanced prompt engineering techniques, methods to construct manual and automatic evaluation metrics, and strategies for improving the model’s mastery of specific tasks.\n\nThrough two interactive labs, we will explore how following best practices for crafting prompts for coding-related tasks leads to reusable prompt structures. We will apply these principles to refine a prompt for translating Graphical User Interface (GUI) actions into Command-Line In- terface (CLI) commands. We will also review a sample prompt published by OpenAI and analyze it through the lens of the five S’s for crafting prompts. This analysis will help us understand how these examples yield desirable outcomes and highlight potential improvements, which we will explore further in later chapters of the book.\n\n78\n\nBest Practices for Prompting with ChatGPT\n\nIn this chapter, we will cover the following topics:\n\n\n\nCan we trust GenAI for coding tasks?\n\n\n\nBest practices for crafting prompts for coding tasks\n\n\n\nCrafting prompts for ChatGPT\n\n\n\nPrompt samples by OpenAI\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nOpenAI account with access to ChatGPT\n\n\n\nAccess to the book’s repository available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\nIf you need help creating an OpenAI account, refer to the Appendix for detailed guidance.\n\nCan we trust GenAI for coding tasks? Large language models (LLMs) are continuously improving their capabilities in mathematics and coding, achieving impressive results in both areas. For instance, internal research by OpenAI showed that the GPT-o1 model scored correctly on 11 out of 15 questions in the American Invi- tational Mathematics Examination (AIME). This performance not only exceeds the threshold required to qualify for the USA Mathematical Olympiad (USAMO) or the USA Junior Mathe- matical Olympiad (USAJMO) but also places the model in the top percentile of participants. Sim- ilarly, when evaluated on coding problems from the Codeforces competition website, the internal research showed that the model performed at the 89th percentile, highlighting its proficiency in tackling challenging coding tasks.\n\nYet, the web is filled with screenshots, videos, and examples of highly undesirable outputs from certain prompts. Not long ago, the author encountered a pull request (PR) containing a file named new.env that exposed seven secret keys. A post-mortem meeting dedicated to that issue revealed that the data scientist who committed the file had used the following prompt:\n\nHow can I commit my files to GitHub from the PyCharm terminal?\n\nThe output included the following instruction:\n\ngit add .\n\nChapter 4\n\nThe data scientist, who typically utilized PyCharm’s Git tool window and was used to choosing the files to commit manually, failed to notice that the file named new.env with the secret keys had been included in the commit. This incident highlights that following GenAI-generated in- structions can sometimes lead to risky outcomes.\n\nThis example does not imply that we should distrust all GenAI output or validate every detail. The fault did not occur because of the model’s hindsight about exposing secret keys through a .env file. Instead, it stemmed from a poorly crafted prompt combined with the model’s bias to- ward suggesting CLI commands such as git add ., which frequently appear in online manuals.\n\nWe argue that crafting a better prompt, such as the following, would lead to a more desirable model output:\n\nCONTEXT: You will be provided with PyCharm GUI steps enclosed with {{{ STEPS }}} to execute a process enclosed with {{{ PROCESS }}}.\n\nTASK: Convert the steps to CLI commands.\n\nPROCESS: {{{ Commit and push files to a remote git branch }}}\n\nSTEPS: {{{\n\n1. Review changed files\n\n2. Stage the desired files\n\n3. Add a commit message\n\n4. Commit the files\n\n5. Validate branch name\n\n6. Push the changes to the remote branch\n\n}}}\n\nCLI COMMANDS:\n\nThis prompt, which follows the best practices for crafting effective prompts, is structured, provides surrounding context for the problem, focuses on a single task, is specific, and is short enough. In this chapter, we will explore how to create such prompts and understand why following these practices leads to better GenAI outputs.\n\nThe three pillars of good outputs To achieve good results from GenAI applications, we will focus on the three pillars: model mastery, evaluation metrics, and crafting precise prompts.\n\nModel mastery refers to an LLM’s ability to handle a specific task described in a prompt. For instance, GPT models are extensively trained in Python code and problems such as those found on LeetCode, a platform for coding interview preparation. However, when the task deviates from\n\n79",
      "page_number": 82
    },
    {
      "number": 5,
      "title": "Summary In this chapter, we took our first steps with the GitHub Copilot by setting up subscriptions and enabling Copilot in our IDE. Through three labs, we explored its three interaction modes: chat,",
      "start_page": 109,
      "end_page": 139,
      "detection_method": "regex_chapter",
      "content": "80\n\nBest Practices for Prompting with ChatGPT\n\nmainstream topics, such as integrating Python threading to parallelize code execution across cores and virtual cores, there is significantly less training material available online, leading to outputs that may not match the quality of those for well-covered topics. Keep in mind that for such topics, LLM outputs may be suboptimal. However, we can enhance model mastery using techniques such as fine-tuning to achieve more desirable outputs. This will be covered in greater detail later in the book.\n\nEvaluation metrics refer to the manual and automatic methods we use to assess the quality of GenAI outputs. Effective prompting always involves evaluating the GenAI output, as it is not guaranteed that good results will be achieved on the first attempt. Manual evaluations involve tasks we can personally verify, such as the following:\n\n\n\nReading code in a language we recognize, such as Python\n\n\n\nAsking for CLI commands we can identify as incorrect\n\n\n\nEvaluating visual outputs such as plots to see whether they align with our expectations\n\n\n\nVerifying straightforward outputs, such as ensuring a requested CSS color matches light pink\n\nAutomatic evaluations rely on computational checks, such as:\n\n\n\nCompiling suggested code to ensure it runs without errors.\n\n\n\nRunning unit tests to validate functional correctness.\n\nUsing frameworks such as OpenAI Evals to systematically assess outputs.\n\nThese evaluations help detect faulty outputs and guide iterative improvements toward better suggestions.\n\nOpenAI Evals in brief\n\nEvals is a system designed to assess the performance of LLM applications by using a collection of prompts and expected outputs. These can include prompts and out- puts provided by the system itself or custom examples that we create. Accessible via both Python package installers and a CLI, it compares the application’s outputs against the expected results and assigns a performance score to the full set. In later chapters, we will delve into strategies for improving evaluation metrics and explore how to effectively leverage the OpenAI Evals system for evaluating GenAI outputs.\n\nChapter 4\n\nPrecise prompts refer to how closely the instructions given to the model align with the actual task we want it to perform. When prompts are ambiguous, the model is likely to fill in gaps with assumptions based on its training data, which may not always align with the task at hand.\n\nFor example, the prompt How can I commit my files to GitHub from the PyCharm terminal? is less precise than a prompt providing specific steps performed in the PyCharm GUI and a request to convert these steps to CLI commands.\n\nNext, we will explore best practices for designing prompts for coding tasks and learn how to refine these prompts into precise inputs that produce high-quality outputs.\n\nBest practices for crafting prompts for coding tasks When working with GenAI applications for coding-related tasks, the best practices for crafting precise prompts can be summarized by the five S’s:\n\n\n\nStructured prompts with a clear separation between the instructions and provided data\n\n\n\nSurrounding information that provides context for the problem, such as the code language for the provided code and whether it compiles\n\n\n\nSingle-task focus to ensure each prompt addresses only one objective\n\n\n\nSpecific instructions detailing how the coding task should be performed, rather than using generic requests\n\n\n\nShort and minimal prompts that avoid fluff, redundant, or overly verbose instructions\n\nLet us dive into each of these practices.\n\nStructured Just as well-written, functional Python code separates logic from data, a good prompt should also distinguish instructions from provided data. When including elements such as steps, code snippets, or code drafts, ensure they are separated from the main instructions and aliased ap- propriately for clarity and reusability.\n\nReplace unstructured prompts that mix supporting data with instructions, such as the following:\n\nConvert the COBOL code\n\nIDENTIFICATION DIVISION.\n\nPROGRAM-ID.\n\n...\n\nto Python.\n\n81\n\n82\n\nBest Practices for Prompting with ChatGPT\n\nUse a structured approach that separates the instructions from the data:\n\nCONTEXT: You will be provided with a COBOL code enclosed with {{{ COBOL CODE }}}.\n\nTASK: Your task is to convert it to Python, with the following guidelines:\n\n...\n\nCOBOL CODE: {{{\n\n}}}\n\nPython Code:\n\nThis structured prompt is simple to reuse with other COBOL snippets.\n\nSurrounding information When specifying the surrounding context, include information about the data relevant to the prob- lem. Here are three examples of three different prompt pieces that could be used for different tasks:\n\n\n\nYou are provided with Python code / a function / a function signature.\n\n\n\nYou are provided with a Python function that contains a bug.\n\n\n\nYou are provided with GUI steps.\n\nThese instructions are general and avoid specific details, making them reusable:\n\n\n\nThe first and second examples do not specify any function\n\n\n\nThe third example does not describe specific GUI steps\n\nAs a result, these prompt pieces are reusable across various tasks.\n\nSingle task per prompt Always aim for a single task in your prompts. This approach not only improves your ability to evaluate the model’s output by focusing on a single task but also increases the likelihood that the model has encountered similar tasks during its training, enhancing its mastery of the topic.\n\nFor example, in refactoring tasks, refrain from combining multiple objectives, as follows:\n\nExplain the following function and fix compilation bugs.\n\nWhile LLMs are well trained on both individual tasks, combining them into a single prompt may be less common, reducing the model’s mastery of the topic. Instead, focus on either explaining the function or fixing its compilation bugs.\n\nChapter 4\n\nSpecific instructions As software developers, it is tempting to use keywords such as optimize, improve, refactor, or scale. However, these are broad and ambiguous, often leaving room for interpretation by the model.\n\nTo achieve better results, use specific instructions, such as the following:\n\n\n\nUse list comprehensions instead of for loops.\n\n\n\nUse np.array to vectorize for loop computations.\n\n\n\nConvert hard-coded integers to global constants.\n\n\n\nCache function results using lru_cache.\n\nSpecific instructions eliminate ambiguity, ensuring the model understands exactly what changes or actions are expected, and increase the likelihood of producing the desired outcome.\n\nShort prompts Focus on including only relevant information, leaving fluff out, and avoiding ambiguity in your language. Note that this does not mean that you should avoid valuable and relevant information. With the GPT-o1 model supporting over 65,000 tokens (about 164 pages), it can handle longer prompts if its content is minimal and relevant.\n\nFor example, avoid phrases such as the following:\n\nI think that the following function has a bug [...]\n\nPrefer instructions such as the following:\n\nfind the compilation error in [...]\n\nAdditionally, avoid vague phrases such as the following:\n\nIt looks like this for loop could be converted to a list comprehension.\n\nPrefer instead the following instruction:\n\nConvert the for loop to a list comprehension.\n\nKeeping your prompts minimal and relevant and avoiding fluff and ambiguity allows the model to focus on the specific instructions at hand.\n\nNext, we will see how to adapt these best practices to crafting prompts for ChatGPT.\n\n83\n\n84\n\nBest Practices for Prompting with ChatGPT\n\nCrafting prompts for ChatGPT When crafting prompts for ChatGPT, we can apply the structured approach suggested earlier:\n\nCONTEXT: ...\n\nTASK: ...\n\nSUPPORTING_DATA: {{{\n\n}}}\n\nCOMPLETION:\n\nReplace SUPPORTING_DATA with terms such as CODE, STEPS, OLD_CODE, or any other appropriate label for the provided data. Replace COMPLETION with terms such as SOLUTION, CLI COMMANDS, NEW_CODE, or another label that matches the desired model output.\n\nWhen working with the GPT-4o model with the integrated canvas editor, apply the following structure:\n\nCONTEXT: ...\n\nTASK: ...\n\nSUPPORTING_DATA: {{{ }}}\n\nTo access it, start a new GPT-4o chat and type the following to launch the canvas editor:\n\nOpen Python editor\n\nThen, add your supporting code to the Python editor as you would in any IDE, as shown in Figure 4.1:\n\nFigure 4.1: GPT-4o with canvas editor for Python\n\nChapter 4\n\nChatGPT will edit your code based on your request and provide additional output in the chat window, as shown in Figure 4.2:\n\nFigure 4.2: Sample edited code from canvas editor\n\nLater in this chapter, we will use the canvas editor in ChatGPT to fix errors in a function.\n\nIn the upcoming lab, we will follow these practices to refine a ChatGPT prompt for converting PyCharm GUI steps into Git CLI commands. We will also evaluate how these improvements improve the model output.\n\nLab 4.1 – converting PyCharm GUI steps to CLI When transitioning from GUI steps to working directly with the CLI for Git operations, we can use ChatGPT to construct the steps required for committing and pushing staged files to a remote repository. A critical part of this process involves manually reviewing the staged files to ensure that only the desired ones are committed to our remote branch.\n\nBy applying the best practices for prompt precision, we will refine the following initial prompt into a prompt that produces a more reliable output:\n\nHow can I commit my files to GitHub from the PyCharm terminal?\n\nFor reference, using this initial prompt alone may result in a response such as the one shown in the following figure when entering it into the GitHub Copilot chat window:\n\n85\n\n86\n\nBest Practices for Prompting with ChatGPT\n\nFigure 4.3: Copilot chat suggestion\n\nQuick tip: Need to see a high-resolution version of this image? Open this book\n\nin the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nChapter 4\n\nIn our guided walk-through, we will apply the following best practices to craft a more effective prompt:\n\n\n\nStructured format: Separate the logic of the task from the actual steps to be converted and the desired GUI process\n\n\n\nSurrounding context: Provide the context of the data and structure\n\n\n\nSingle-task focus: Narrow the task to converting GUI steps into CLI commands\n\n\n\nSpecific instructions: Clearly indicate all steps previously executed from the GUI, avoid- ing a generic request\n\n\n\nShort prompt: Eliminate redundant details such as PyCharm terminal\n\nWe will then feed this refined prompt to ChatGPT and observe how applying these best practices enhances the trustworthiness and reliability of the model’s output.\n\nLab 4.1 guided walk-through Navigate to https://chatgpt.com/ and start a new chat with the GPT-o1-mini model. This model has high proficiency in science, technology, engineering, and mathematics (STEM) tasks, par- ticularly coding and mathematics. As of November 2024, it achieves the highest performance among available GPT models on the Codeforces coding exam.\n\nFigure 4.4: ChatGPT model menu\n\n87\n\n88\n\nBest Practices for Prompting with ChatGPT\n\nOur starter information consists of the following:\n\n\n\nScenario: A data scientist aims to work with the terminal instead of the PyCharm GUI to push changes into a desired branch\n\n\n\nInitial prompt:\n\nHow can I commit my files to GitHub from the PyCharm\n\nterminal?\n\nWe will now address each of the five S’s best practices for coding-related prompts following these four guidelines:\n\n\n\nStructure the prompt\n\n\n\nAdd surrounding context\n\n\n\nFocus on a specific task\n\n\n\nInclude specific instructions\n\nThroughout the steps, we will follow the fifth best practice of keeping prompts short, ensuring they remain minimal and free of unnecessary details and fluff.\n\nStep 1 – structure the prompt\n\nA well-structured prompt separates the instructions from the data, ensuring clarity and allowing for simple adjustments. In this case, the instructions outline the task, while the data contains the specific steps required to push the files to the remote branch. For ChatGPT, one strategy is to alias the data by enclosing it within delimiters such as {{{ DATA }}} for easy reference:\n\nInstructions with reference to data enclosed with {{{ DATA }}}\n\n...\n\nDATA: {{{\n\n...\n\n}}}\n\nSolution:\n\nChapter 4\n\nImportant note\n\nWe recommend using {{{ DATA }}} as an alias because the {{{ }}} delimiter is rare in both code and other types of data. However, choosing alternative separators is also acceptable if they do not commonly appear in your code. For example, enclosers such as \"\"\" \"\"\" are used in Python for multi-line strings and could cause confusion for the model if used as delimiters.\n\nStep 2 – add surrounding context\n\nA precise prompt should include information about the surrounding context of the problem and the provided data. When converting GUI commands into terminal commands, a clear and relevant surrounding context might look like this:\n\nCONTEXT: You will be provided with PyCharm GUI steps enclosed with {{{ STEPS }}} to execute a process enclosed with {{{ PROCESS }}}.\n\nBy avoiding references to specific processes, such as committing and pushing files to a remote Git branch, this context remains flexible and can be reused for similar tasks in the future.\n\nAdd this context at the beginning of your ChatGPT prompt to establish the context of the task.\n\nStep 3 – single task\n\nOur single task in this lab is to translate GUI steps into CLI commands. A clear phrasing of this task could be as follows:\n\nTASK: Convert the steps to CLI commands.\n\nThis prompt piece specifies that the GUI step needs to be converted into corresponding CLI com- mands.\n\nAdd this prompt piece directly below the surrounding context information. Then, complete the structured prompt with the following line at the end:\n\nCLI COMMANDS:\n\nThis signals to the model that the output should be the list of CLI commands to execute.\n\n89\n\n90\n\nBest Practices for Prompting with ChatGPT\n\nStep 4 – specific instructions\n\nIn this lab, we aim to provide detailed and accurate instructions reflecting what is typically done when pushing changed files to a remote branch. In PyCharm, the first step of reviewing changed files can be performed by navigating to the Git tool window icon in the left menu:\n\nFigure 4.5: Git tool window\n\nIt is good practice to manually review your staged files in the working directory, ensuring that files such as new.env, which should be ignored by Git, are not included in the commit.\n\nAfter adding a commit message and clicking on Commit and Push, PyCharm pops up the push window, indicating that the selected files are about to be pushed to the remote branch.\n\nChapter 4\n\nFigure 4.6: Pushing a commit to a remote branch from PyCharm\n\nIn this figure, we can see that a single file, the solution to Lab 4.1, is being pushed to a new remote branch called ch4-labs.\n\nDescribing these steps may look like this:\n\nSTEPS: {{{\n\n1. Review changed files\n\n2. Stage the desired files\n\n3. Add a commit message\n\n4. Commit the files\n\n5. Validate branch name\n\n6. Push the changes to the remote branch\n\n}}}\n\n91\n\n92\n\nBest Practices for Prompting with ChatGPT\n\nInclude these steps in the prompt, placing them directly beneath the TASK line.\n\nIf you have followed all the steps correctly, your final prompt should look like this:\n\nCONTEXT: You will be provided with PyCharm GUI steps enclosed with {{{ STEPS }}} to execute a process enclosed with {{{ PROCESS }}}.\n\nTASK: Convert the steps to CLI commands.\n\nPROCESS: {{{ Commit and push files to a remote Git branch }}}\n\nSTEPS: {{{\n\n1. Review changed files\n\n2. Stage the desired files\n\n3. Add a commit message\n\n4. Commit the files\n\n5. Validate branch name\n\n6. Push the changes to the remote branch\n\n}}}\n\nCLI COMMANDS:\n\nYour prompt is now ready to use in ChatGPT. Execute it and refer to the lab analysis for a sample output generated by the model.\n\nLab 4.1 analysis In this lab, we crafted a precise prompt by applying the five S’s best practices. When we ran the prompt in ChatGPT using the GPT-o1-mini model, the output provided a set of CLI commands for committing and pushing the desired files to a remote branch. The generated commands were as follows:\n\nChapter 4\n\nFigure 4.7: ChatGPT’s response to the Lab 4.1 prompt\n\nThis output is more desirable than the initial prompt because it directly references each of the six steps we would perform using the GUI, providing explanations for each corresponding command. Additionally, the output aligns with the prompt’s specificity, offering only the CLI commands with minimal surrounding explanation.\n\nImportant note\n\nRefining prompts using the five S’s best practices may seem like a lot of work initially. However, over time, seasoned prompters develop their own reusable prompt tem- plates, making the process more efficient. Well-structured prompts are reproducible across different tasks and adapting them with varied data can often be done at scale. In the next chapters, we will reuse prompts crafted with these best practices for a variety of tasks.\n\n93\n\n94\n\nBest Practices for Prompting with ChatGPT\n\nHaving introduced and implemented the best practices for crafting precise prompts, we can now observe how applying the five S’s can further enhance outputs, even from prompts already considered strong. Next, we will delve into OpenAI’s examples of good prompts, analyze them through the lens of the five S’s, and refine a prompt further to achieve a more desirable output.\n\nPrompt samples by OpenAI OpenAI occasionally updates a collection of sample prompts tailored to various tasks, including coding, content generation, translations, and grammar corrections. These prompts display the core capabilities where the underlying models exhibit strong mastery. They have been carefully crafted and have demonstrated consistent success across these use cases.\n\nYou can explore the full list of prompts and filter the samples by category at https://platform. openai.com/docs/examples.\n\nFigure 4.8: OpenAI sample prompts\n\nWhen clicking on a prompt sample from this page, we can observe the prompt itself, divided into a system prompt and a user prompt, a sample response, and a code snippet demonstrating how to implement the prompt using OpenAI API.\n\nChapter 4\n\nFigure 4.9: Improve code efficiency prompt example\n\nIn this example, the system prompt contains general instructions describing the task to be per- formed, while the user prompt includes the import statement and a function that could benefit from efficiency improvements.\n\nNext, we will critically analyze a sample prompt to assess how well it aligns with the five S’s.\n\nLab 4.2 – code debugging In this lab, we will examine a Python bug fixer use case through the lens of our best practices.\n\nNavigate to https://platform.openai.com/docs/examples and search for Python bug fixer. This should yield the following prompt example to instruct the model to find a bug in a Python pro- gram that quizzes a user on multiplication problems:\n\n95\n\n96\n\nBest Practices for Prompting with ChatGPT\n\nFigure 4.10: Sample prompt for Python bug fixer\n\nThis prompt, like the code efficiency improvement prompt example by OpenAI, separates the system and user prompts while providing a sample response. Scrolling further down the window reveals the code necessary to implement this prompt with OpenAI API.\n\nIn the guided walk-through, we will identify the errors in the provided code, so we can better evaluate the output of the model. In the analysis, we will analyze the usage of the five S’s and see how we can improve the prompt even further.\n\nChapter 4\n\nLab 4.2 guided walk-through Before we can analyze the output of the model, here are the errors in the user-provided functions:\n\n\n\nCompilation errors:\n\n\n\nTypo in import statement: import Random should be corrected to import random.\n\n\n\nType error: The question variable cannot concatenate integers a and b with strings directly. This needs to be resolved by converting integers to strings.\n\nMissing colon: The if statement is missing a : character at the end, leading to\n\nSyntaxError.\n\n\n\nInvalid assignment: The answer = a*b statement attempts to assign a value within the if condition. Modern Python compilers will raise SyntaxError as this is an invalid syntax.\n\n\n\nInvalid print statement: The string Well done! is not enclosed with quotes and therefore is an invalid syntax.\n\n\n\nLogical errors:\n\n\n\nThe a and b variables are generated once before the loop and remain constant through all 10 iterations. This is not desirable.\n\n\n\nThe output from the user is a string, while the comparison is made against integers. As a result, answer of type str can never be equal to a * b, which is of type int.\n\n\n\nReproducibility issue:\n\n\n\nThe code with random numbers is not reproducible across runs due to the absence of a random seed set. Without this, the output will vary unpredictably between executions.\n\nConsidering the output published on the OpenAI website, here is a summary of identified and fixed code errors and issues:\n\nError type\n\nProblem\n\nError\n\nFound\n\nFixed\n\nCompilation\n\nInvalid case\n\nimport Random\n\nYes\n\nYes\n\nCompilation\n\nConcatenate int and str\n\na + \" x \" + b\n\nYes\n\nYes\n\nCompilation\n\nMissing colon in if statement\n\nif statement\n\n...\n\nNo\n\nYes\n\n97\n\n98\n\nBest Practices for Prompting with ChatGPT\n\nError type\n\nProblem\n\nError\n\nFound\n\nFixed\n\nCompilation\n\nAssignment in if statement\n\nif answer = a * b\n\nYes\n\nYes\n\nCompilation\n\nMissing quotes for str\n\nprint(Well done!)\n\nYes\n\nYes\n\nLogic\n\nRandom numbers generated once\n\na, b outside the for loop\n\nNo\n\nNo\n\nLogic\n\nstr compared to int\n\nanswer = a * b\n\nYes\n\nYes\n\nReproduce\n\nNo seed set\n\nMissing random.seed(...), or equivalent initialization\n\nNo\n\nNo\n\nTable 4.1: OpenAI API’s performance in detecting and fixing errors\n\nThe proposed solution also introduced a new issue related to casting the input to an integer; there is no proper error handling if an input such as hello cannot be cast.\n\nThe following is the full model response, as published on the OpenAI website:\n\nThere are several bugs in the provided code:\n\n1. The import statement is incorrect. Python is case-sensitive, so \"Random\" should be \"random\".\n\n2. Variables `a` and `b` are defined outside the loop, so they will not change for each iteration. They should be inside the loop if you want to generate different questions.\n\n3. In the question string, `a` and `b` are integers, so you need to convert them to strings using the `str()` function to concatenate them.\n\n4. In the if statement, you are using a single equals sign `=` which is an assignment operator. You should use the double equals sign `==` for comparison.\n\n5. The print statement \"Well done!\" is not enclosed in quotes. In Python, strings must be enclosed in quotes.\n\n6. The input from the user is a string, so you need to convert it to an integer using the `int()` function before comparing it with the product of `a` and `b`.\n\nHere is the corrected code:\n\nChapter 4\n\n```python\n\nimport random\n\nfor i in range(10):\n\na = random.randint(1,12)\n\nb = random.randint(1,12)\n\nquestion = \"What is \"+ str(a) +\" x \"+ str(b) +\"? \"\n\nanswer = int(input(question))\n\nif answer == a*b:\n\nprint (\"Well done!\")\n\nelse:\n\nprint(\"No.\")\n\n```\n\nNext, we will analyze the prompt through the lens of the five S’s, evaluate the quality of the out- put based on the errors identified, fixed, and introduced, and propose improvements to achieve a better outcome.\n\nLab 4.2 analysis The prompt introduced in this example mostly aligns with the five S’s guidelines:\n\nGood structure: The instructions are clearly separated from the code, making the prompt\n\nreusable and adaptable for other tasks.\n\n\n\nSufficient surrounding context: The prompt provides context by mentioning Python code as the supporting data.\n\nNo single task: The prompt combines two related tasks, identifying and fixing errors in the code. While these tasks are interconnected, focusing solely on fixing the errors could improve the model’s response and responsibility.\n\n\n\nSpecific: The task specifies that the model should identify and fix the errors in the code. However, the instructions could be improved by explicitly stating the types of errors to address, such as compilation, logical scoping, reproducibility, or error handling.\n\n\n\nShort: The prompt is concise and avoids unnecessary or irrelevant details.\n\nOverall, following most of the guidelines resulted in a decent output. Since we have strong eval- uation capabilities and are aware of all the errors the model should address, we can continue iterating to resolve all remaining issues in the code.\n\n99\n\n100\n\nBest Practices for Prompting with ChatGPT\n\nAlternatively, we could better align the best practices with the following prompt:\n\nCONTEXT: You will be provided with Python code in the Python editor with explanation enclosed with\n\n{{{ CODE_EXPLANATION }}} that may contain errors enclosed with {{{ ERRORS }}}.\n\nTASK: Fix the errors in the code.\n\nCODE_EXPLANATION: {{{\n\nRun 10 random multiplication quizzes and provide feedback about each to the user\n\n}}}\n\nERRORS: {{{\n\n1. Compilation\n\n2. Reproducibility of random number generation\n\n3. Logical scoping of variable assignments\n\n4. Error handling for input casting\n\n}}}\n\nUsing this prompt with GPT-4o with canvas resulted in an output that addressed all the required fixes.\n\nFigure 4.11: Better output\n\nChapter 4\n\nAll compilation, logic, reproducibility, and error handling are addressed in the fixes.\n\nEven better prompts In the next chapters, we will explore advanced prompt engineering techniques such as iterative prompting for creating a feedback loop with LLMs and chain-of-thought prompting for guiding model reasoning. These techniques will enable us to refine prompts that yield decent results for manual use and scale them effectively for broader applications.\n\nSummary In this chapter, we explored how to achieve high-quality GenAI outputs by focusing on the three pillars: model mastery for handling a specific topic, evaluation metrics, and precise prompts. We examined how imprecise prompts can lead to risky outcomes, such as CLI commands exposing secret keys, and demonstrated how following best practices can produce far better and more reliable model outputs.\n\nWe also learned about the five S’s of prompt precision best practices for coding-related tasks: keeping a structured prompt, providing surrounding context, maintaining a single task per prompt to best leverage model mastery and evaluation capabilities, ensuring that prompts are specific rather than general and vague, and keeping the input as short as possible by removing irrelevant information and fluff.\n\nWith two interactive labs, we explored how to implement those best practices in ChatGPT. With an OpenAI example of a good prompt to a bug fix, we saw that even good prompts can be improved to achieve more desirable results.\n\nIn the next chapter, we will explore how to apply the five S’s using OpenAI API and GitHub Copilot. In later chapters, we will delve into advanced prompt engineering techniques to further refine prompts and meet specific engineering needs in the Software Development Life Cycle (SDLC).\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: What are the three factors that determine the quality of output of an LLM?\n\nAnswer: Model mastery, evaluation capabilities, and prompt precision. Model mastery means how much the LLM trained on this specific task, evaluation capabilities means how much we can assess the desirability of the outcome, and prompt precision means to what extent the prompt is crafted to tell the model what exactly we need.\n\n101\n\n102\n\nBest Practices for Prompting with ChatGPT\n\nQuestion 2: What are the best practices for a coding-related task prompt?\n\nAnswer: The best practices are the five S’s, structuring the prompt to accommodate supporting data such as code and steps, including surrounding context on the task, focusing on a single task per prompt, and keeping specific and short instructions.\n\nQuestion 3: Which parts of a prompt can be reproducible?\n\nAnswer: When following the best practices of precise prompts, we can reuse the surrounding con- text and the single task. These can be leveraged to many prompts and reused with a change of the details, such as the function attached, steps to be converted to CLI commands, or improvements required to a given function. This is highly efficient for large-scope refactoring tasks, repetitive tasks, and many more examples we will see in the book.\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\nOpenAI on GPT-o1 performances: https://openai.com/index/openai-o1-mini-\n\nadvancing-cost-efficient-reasoning/\n\n\n\nAmerican Invitational Mathematics Examination: https://en.wikipedia.org/wiki/\n\nAmerican_Invitational_Mathematics_Examination\n\n\n\nCodeforces: https://codeforces.com/\n\n\n\nPrompt best practices in OpenAI: https://help.openai.com/en/articles/6654000-\n\nbest-practices-for-prompt-engineering-with-the-openai-api\n\n\n\nPrompt examples in OpenAI: https://platform.openai.com/docs/examples\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.\n\n5\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nIn the concluding chapter of Part 1, we will explore how to apply best practices for crafting prompts tailored to OpenAI API and GitHub Copilot. These practices are encapsulated in the five S’s: structuring the prompt, providing surrounding context, focusing on a single task, including specific instructions, and keeping prompts short and fluff-free. While the core principles remain consistent, their application varies slightly across different GenAI tools.\n\nWe will delve into working with Python objects such as functions, classes, and methods for cod- ing-related tasks. We will leverage the built-in inspect package to extract key details about objects, such as their source code, docstrings, and filenames. We will also learn how to integrate this information into the system and user prompts offered by OpenAI API, combining context, task focus, and concise, structured instructions.\n\nAdditionally, we will examine how GitHub Copilot inherently provides structure, context, and single-task focus while generating short, specific prompts for us. We will learn techniques to ex- tend Copilot’s capabilities by incorporating additional structure and precision, ensuring minimal fluff and redundant comments.\n\n104\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nThrough two interactive labs, we will implement these practices using a real-world example: a Singleton design pattern to enforce single-instance creation per class. Using OpenAI API, we will craft a docstring for the Singleton __call__ method, which controls the instantiation process of a new class instance. Then, with Copilot, we will generate a unit test to validate the functionality and fix a faulty implementation.\n\nIn this chapter, we will cover the following topics:\n\n\n\nExtracting properties from Python objects for OpenAI API\n\n\n\nCrafting precise prompts for OpenAI API\n\nGenerating docstrings with OpenAI API\n\n\n\nCrafting precise prompts for GitHub Copilot\n\n\n\nFixing faulty implementation with GitHub Copilot\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIntegrated development environment (IDE) – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, which is available at https://github.com/\n\nPacktPublishing/Supercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 2. For instructions on setting up OpenAI API access and token generation, see Chapter 3. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nExtracting properties from Python objects for OpenAI API When using OpenAI for coding tasks, we often include details about Python objects such as variables, functions, and classes in our prompts. This helps with various tasks such as debugging, generating docstrings, creating unit test suites, or improving code. The information provided may include the object name, arguments, return type, filename, docstring, and source code.\n\nChapter 5\n\nThroughout this book, we frequently use the built-in inspect package to extract relevant details from Python objects. Additionally, we leverage some special attributes, identified by a double underscore (dunder), such as obj.__attribute__, which store metadata about the object or class. Here are some more examples:\n\n\n\nobj.__name__ retrieves the name of the object\n\n\n\nobj.__class__.__name__ retrieves the class name of an instance or function or the meta- class name of a class\n\n\n\nobj.__doc__ retrieves the docstring of the object\n\nTo extract the source code and filename of an object, we typically use Python’s built-in inspect library:\n\n\n\ninspect.getsource(obj) retrieves the source code of the object\n\n\n\ninspect.getfile(obj) retrieves the file name of the object\n\nFor example, we can use inspect.getsource(obj) to retrieve the source code for the faulty implementation of the multiplication_quiz function discussed in Lab 4.2 in Chapter 4. This function is intended to execute 10 different multiplication quizzes, as outlined in its docstring, but contains implementation bugs that we need to address:\n\ndef multiplication_quiz() -> None:\n\n\"\"\"\n\nThis function generates a multiplication quiz of ten different random multiplication questions between 1 and 12.\n\nThe user is prompted to answer each question. If the user answers correctly, the function prints \"Well done!\".\n\nIf the user answers incorrectly, the function prints \"No.\".\n\n\"\"\"\n\na = random.randint(1, 12)\n\nb = random.randint(1, 12)\n\nscore = 0\n\nfor _ in range(10):\n\nquestion = \"What is \" + a + \" x \" + b + \"? \"\n\nanswer = input(question)\n\nif answer == a * b:\n\nprint(\"Well done!\")\n\nscore += 1\n\n105\n\n106\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nelse:\n\nprint(\"No.\")\n\nprint(f\"Your score is {score} out of 10.\")\n\nQuick tip: Enhance your coding experience with the AI Code Explainer and Quick Copy features. Open this book in the next-gen Packt Reader. Click the Copy button\n\n(1) to quickly copy code into your coding environment, or click the Explain button\n\n(2) to get the AI assistant to explain a block of code to you.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR visit packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nThe code for the function can be extracted using the getsource function:\n\ninspect.getsource(multiplication_quiz)\n\nThe returned value will be a string containing the function’s code as text, as shown here:\n\n'def multiplication_quiz() -> None:\\n \"\"\"\\n This function generates a multiplication quiz of ten different random multiplication questions between 1 and 12. ... the rest of the code is omitted for brevity ...'\n\nThis format is easily interpreted by GenAI applications and large language models (LLMs), and we will use it to include Python objects and code blocks in our prompts, enclosed within {{{ }}}.\n\nNext, we will explore how these objects are integrated into OpenAI prompts using the five S’s best practices for crafting precise and effective prompts.\n\nChapter 5\n\nCrafting precise prompts for OpenAI API Applying best practices for building a precise prompt with OpenAI API enables us to automate and scale the prompting process while ensuring quality outputs from the underlying LLMs.\n\nWe will revisit the five S’s best practices, beginning with structuring OpenAI API prompts. We will craft system prompts that incorporate the surrounding context and focus on a single task, enabling the development of reproducible and adaptable templates. Additionally, we will explore how incorporating specific and short instructions helps transform diverse inputs into concise user prompts.\n\nStructured prompts To structure prompts for OpenAI API, we use system prompts and user prompts to separate the surrounding context and single task objective from specific details such as steps, source code, and data. System prompts define the context and task, allowing for reuse across varying details such as different source codes. In later chapters of the book, we will also explore how to integrate assistant prompts to achieve even more refined results from our prompts.\n\nIn Python, this prompt structure usually includes a variable called SURROUND to provide the sur- rounding context and a variable called SINGLE_TASK to specify the task for the model. It may be structured as follows:\n\nSURROUND = \" Your surrounding context here \"\n\nSINGLE_TASK = \" Your task here \"\n\ndef get_user_prompt(*args, **kwargs) -> str:\n\nreturn \"\"\"\"\n\n# your code here\n\nCOMPLETION:\n\n\"\"\"\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(...)\n\nmessages = [\n\n107\n\n108\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\n{\"role\": \"system\", \"content\": \"system_prompt\"\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n]\n\nIn this structure, the messages parameter combines both system_prompt and user_prompt to form a precise prompt for OpenAI API. system_prompt integrates SURROUND and SINGLE_TASK, while the get_user_prompt function provides details such as steps, source code, and object in- formation, concluding with a lead-in cue such as COMPLETION to align with SINGLE_TASK, as we will discuss in depth throughout this chapter.\n\nNow that we have examined the structure that combines both system prompts and user prompts, let us dive deeper into each, starting with system prompts.\n\nSystem prompts with the surrounding context and single task In OpenAI API, system prompts are used to define the surrounding context and specify a single task. These prompts are designed to be reproducible, accommodating varying supporting data.\n\nFor instance, a system prompt might state that the model is provided with a code snippet and tasked with identifying the programming language:\n\nSURROUND = \"You will be provided with a code snippet enclosed with {{{ CODE }}}.\"\n\nSINGLE_TASK = \"Your task is to identify the programming language of the provided code snippet.\"\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nThe SURROUND context specifies that the provided data is a code snippet in an unidentified lan- guage, while SINGLE_TASK clarifies that the expected output should identify the programming language of the code.\n\nIn another scenario, the task might involve generating documentation for a Python function where the programming language (Python) and object type (function) are already known. In such cases, this information should be included in the surrounding context to help the model focus on generating the docstring rather than determining the language or object type:\n\nSURROUND = \"You will be provided with a Python function enclosed with {{{ FUNCTION }}}.\"\n\nSINGLE_TASK = \"Your task is to generate a Google Style docstring for it.\"\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nChapter 5\n\nIn this case, SURROUND provides context about the Python function, enabling the model to con- centrate on the task of generating docstrings, as specified in SINGLE_TASK.\n\nUser prompts with specific instructions To create an effective user prompt for OpenAI API call, we typically define a get_user_prompt function that encapsulates the necessary task details along with a lead-in cue to align with the system prompt.\n\nFor example, if the task is to identify the programming language from the source code, the func- tion implementation might look like this:\n\ndef get_user_prompt(code_source: str) -> str:\n\nreturn f\"\"\"\n\nCODE: {{{{{{ {code_source} }}}}}}\n\nPROGRAMMING LANGUAGE:\n\n\"\"\"\n\nIn the preceding implementation, {{ in f-strings are used to represent a literal {. As a result, {{{{{{ is required to represent {{{. Also, the source code is retrieved as a string, requiring no further conversion.\n\nIn a different scenario involving Python objects, such as extracting the docstring for a class method, the source code can be directly obtained from the object. The implementation of get_user_prompt in this case might look like this:\n\ndef get_user_prompt(func: callable) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nDOCSTRING:\n\n\"\"\"\n\nIn this implementation, the inspect.getsource function is used to extract the source code from the function.\n\nNext, we will combine system prompts and user prompts to observe how they work together to produce high-quality outputs.\n\n109\n\n110\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nLab 5.1 – Generating docstrings with OpenAI API In this lab, we will utilize OpenAI API to generate a docstring for the __call__ method in a Singleton design pattern, as outlined in the following steps. The __call__ method defines the instantiation process, which involves creating or managing a new instance of the class.\n\nSingleton design pattern As Python developers, we often use design patterns to define and control the behavior of Python objects, such as functions and classes. One such pattern is the Singleton pattern, which ensures that only one instance of a class is created and used throughout the program’s execution.\n\nRegular classes create a new instance from scratch with each instantiation, using either pro- vided arguments or default parameters. For example, consider the following program with an Environment class that defaults to the name Production:\n\nclass Environment:\n\ndef __init__(self, name: str = \"Production\"):\n\nself.name = name\n\nenv1 = Environment(\"Development\")\n\nenv2 = Environment()\n\nprint(\"env1 name: \", env1.name)\n\nprint(\"env2 name: \", env2.name)\n\nIn this case, env1 and env2 are two separate instances of the Environment class. For instance, the name attribute of env1 is set to Development, while the name of env2 is set to Production based on the provided default argument.\n\nThe output of this program is as follows:\n\nenv1 name: Development\n\nenv2 name: Production\n\nThe Singleton design pattern ensures that only one instance of a class is created and maintained throughout the program’s runtime. It is typically implemented using a metaclass, which acts as a class of classes, defining how other classes are structured and behave.",
      "page_number": 109
    },
    {
      "number": 6,
      "title": "Throughout this book, we frequently use the built-in inspect package to extract relevant details from Python objects. Additionally, we leverage some special attributes, identified by a double undersco",
      "start_page": 140,
      "end_page": 177,
      "detection_method": "regex_chapter",
      "content": "Chapter 5\n\nA typical Singleton implementation maintains a registry dictionary of instantiated classes. For each class defined with metaclass=Singleton, whenever a new instance is requested, the Single- ton checks its registry for an existing instance of that class. If an instance is found, it is returned; otherwise, a new instance is created and added to the registry. Here is an example of such an implementation:\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n\nreturn cls._instances[cls]\n\nIn this implementation, the __call__ method is invoked whenever a class of the Singleton type, referred to as cls (a common name for a class argument), attempts to instantiate a new instance. If an instance of the class already exists, it is stored in the _instances attribute with the class’s hash as the key, and the existing instance is returned. Otherwise, a new instance of cls is created as usual and added to the _instances registry for future use.\n\nTo use classes of the Singleton type, such as Environment, we specify that the metaclass for the class is Singleton, as shown here:\n\nclass Environment(metaclass=Singleton):\n\ndef __init__(self, name: str = \"Production\"):\n\nself.name = name\n\nAssigning the Singleton metaclass ensures that all instances of the class refer to the same object. When two instances are created, one with the Development name and the other without any parameters, both Environment instances will share the name Development:\n\nenv1 = Environment(\"Development\")\n\nenv2 = Environment()\n\nprint(\"env1 name: \", env1.name)\n\nprint(\"env2 name: \", env2.name)\n\n111\n\n112\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nIn this case, env1 and env2 are the same instance and env2.name is also expected to be Development. The program output confirms this, as shown here:\n\nenv1 name: Development\n\nenv2 name: Development\n\nNext, we will see how to leverage OpenAI API to generate a docstring for the Singleton __call__ method.\n\nGenerating a docstring A docstring is a special string literal in Python that provides a clear description of an object’s purpose, behavior, arguments, and return values, typically for functions or classes, and would be followed by the code implementation. For instance, here is a Google-style docstring for a function that calculates the nth Fibonacci number:\n\ndef fibonacci(n):\n\n\"\"\"\n\nCalculate the n-th Fibonacci number.\n\nThe Fibonacci sequence is a series of numbers where each number is the sum\n\nof the two preceding ones, starting from 0 and 1. This implementation uses\n\nan iterative approach for better performance with large values of n.\n\nArgs:\n\nn (int): The position in the Fibonacci sequence (0-indexed).\n\nReturns:\n\nint: The n-th Fibonacci number.\n\nRaises:\n\nValueError: If n is a negative integer.\n\nExamples:\n\n>>> fibonacci(0)\n\n0\n\n>>> fibonacci(1)\n\n1\n\n>>> fibonacci(5)\n\nChapter 5\n\n5\n\n\"\"\"\n\n...\n\nWe will explore how to effectively create and utilize high-quality docstrings for Python objects in greater detail in Chapter 18.\n\nIn this lab, we will focus on crafting a simple Google-style docstring for the __call__ method of the Singleton class, incorporating information about the source code of the Python object.\n\nLab 5.1 guided walk-through Open Lab 5.1 in the book repository at ch5/labs/lab51.py and ensure that you have either set the OPENAI_API_KEY environment variable or configured your run settings to point to the .env file.\n\nThe starter code includes the implementation of the Singleton metaclass, along with placehold- ers for the SURROUND and SINGLE_TASK strings, the get_user_prompt function, and the messages parameter for OpenAI API:\n\nimport inspect\n\nfrom openai import OpenAI\n\nfrom openai.types.chat import ChatCompletion\n\nSURROUND = \"\"\n\nSINGLE_TASK = \"\"\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n\nreturn cls._instances[cls]\n\ndef get_user_prompt(func: callable) -> str:\n\n...\n\n113\n\n114\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[],\n\n)\n\nprint(\"Docstring:\", completion.choices[0].message.content)\n\nIn the guided walk-through, we will complete the SURROUND and SINGLE_TASK strings, the get_ user_prompt function, and the messages parameter.\n\nFirst, we will define the surrounding context and single task for the reusable system prompt. Update the SURROUND and SINGLE_TASK strings as follows:\n\nSURROUND = \"You will be provided with a Python function enclosed with {{{ Function }}}.\"\n\nSINGLE_TASK = \"Your task is to generate Google Style docstring for it.\"\n\nThese strings inform the model that the provided source code is a Python function and specify that the generated docstring should follow Google-style conventions.\n\nNext, we will integrate specific and short instructions into the user prompt. Implement the get_ user_prompt function to encapsulate the source code of a provided function, along with a lead-in cue to generate a Google-style docstring:\n\ndef get_user_prompt(func: callable) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nGOOGLE STYLE DOCSTRING:\n\n\"\"\"\n\nFinally, complete the messages argument for OpenAI API call by including the system prompt with the SURROUND and SINGLE_TASK strings, along with the output of get_user_prompt encapsulating the __call__ method of the Singleton class:\n\nmessages=[\n\n{\"role\": \"system\", \"content\": f\"{SURROUND} {SINGLE_TASK}\"},\n\n{\"role\": \"user\", \"content\": get_user_prompt(Singleton.__call__)},\n\n],\n\nChapter 5\n\nIf you have completed all the steps correctly, your lab code should appear as follows:\n\nimport inspect\n\nfrom openai import OpenAI\n\nfrom openai.types.chat import ChatCompletion\n\nSURROUND = \"You will be provide with a Python function enclosed with {{{ Function }}}.\"\n\nSINGLE_TASK = \"Your task is to generate Google Style docstring for it\"\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n\nreturn cls._instances[cls]\n\ndef get_user_prompt(func: callable) -> str:\n\nreturn f\"\"\"\n\nFunction: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nGOOGLE STYLE DOCSTRING:\n\n\"\"\"\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": f\"{SURROUND} {SINGLE_TASK}\"},\n\n{\"role\": \"user\", \"content\": get_user_prompt(Singleton.__ call__)},\n\n],\n\n)\n\nprint(\"Docstring:\", completion.choices[0].message.content)\n\n115\n\n116\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nYour lab is now ready to execute and generate a Google-style docstring. Stay tuned for the lab analysis, where we will review a sample output from the run.\n\nLab 5.1 analysis In this lab, we demonstrated how applying the five S’s best practices for precise prompts can yield good results with just a few lines of code. Using OpenAI API, we successfully generated a Google-style docstring for the __call__ method of the Singleton class.\n\nFor reference, here is a sample output from the lab, which produced a Google-style docstring that clearly explains the method’s general purpose, its arguments, and its return value, ensuring that the same single instance of the class is returned for every instantiation:\n\nDocstring: ```python\n\ndef __call__(cls, *args, **kwargs):\n\n\"\"\"Overrides the default behavior of instance creation to ensure that only one instance of\n\nthe class is created (Singleton Pattern).\n\nThis method checks if the class has already been instantiated. If not, it creates a new instance\n\nand stores it in the class's `_instances` dictionary. Subsequent calls will return the existing\n\ninstance.\n\nArgs:\n\ncls: The class that is being instantiated.\n\nargs: Variable length argument list for the class constructor.\n\n**kwargs: Arbitrary keyword arguments for the class constructor.\n\nReturns:\n\nThe single instance of the class.\n\n\"\"\"\n\n```\n\nThis docstring accurately describes the _instances attribute of the Singleton metaclass, which stores the single instances of the class, as well as the arguments of the __call__ method and its return value.\n\nChapter 5\n\nHaving explored how to craft prompts for OpenAI API using the five S’s, including system and user prompts that integrate information about Python objects for coding tasks, identifying the programming language of a code snippet, and generating a Google-style docstring, we will now turn to implementing the five S’s best practices for prompt precision when crafting prompts for GitHub Copilot.\n\nCrafting precise prompts for GitHub Copilot When working with GitHub Copilot’s three interaction modes, code completion, chat, and code analysis, many best practices are already built in. Copilot structures the prompt automatically, providing surrounding context for these modes, each with a clearly defined single task, whether it is completing code, engaging in a chat, or analyzing code. The prompts are also designed to be specific and short.\n\nHowever, we can use the five S’s more precisely and achieve even better results. In this section, we will use the Singleton design pattern as an example to explore how Copilot supports code completion, code analysis, and chat.\n\nWith an understanding of this pattern and its applications, let us examine how the five S’s are implemented and can be extended within Copilot’s three interaction modes.\n\nStructuring with a lead-in cue Copilot automatically structures prompts for its three interaction modes while considering sur- rounding context such as the filename, open files, code before and after the cursor, and Git history to generate a prompt on your behalf.\n\nTo improve Copilot’s structured prompts, you can provide lead-in cues such as def func_name, the start of a variable declaration, env1 =, or the beginning of a return statement for an f-string, such as return \"\"\". These lead-ins, like the closing cues used in ChatGPT and OpenAI prompts, help clarify the starting point for Copilot’s generation and ensure it proceeds as intended.\n\nFor example, consider a prompt such as the following:\n\nclass Singleton\n\nThis is more effective than a comment such as the following:\n\n# Implement a Singleton meta-class that ensures only one instance of a class is created.\n\n117\n\n118\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nThe first option provides a clear lead-in cue indicating the class to be implemented, while the latter is likely to result in another comment instead of initiating the actual implementation.\n\nFor reference, here is a sample completion generated by Copilot, which includes an additional comment on the next line:\n\nFigure 5.1: Comment completion with a code comment instead of an implementation\n\nCopilot interprets the request as an opportunity to elaborate further on the Singleton implemen- tation as a comment rather than starting the actual implementation.\n\nHere is a sample completion based on the lead-in cue, class Singleton:\n\nFigure 5.2: Lead-in code for Copilot to implement the Singleton metaclass\n\nIn this case, Copilot correctly inferred that it should generate the metaclass implementation even before the Singleton class name was fully specified.\n\nSurrounding with imports and hashtags GitHub Copilot already supplies the underlying LLM with extensive context across its three in- teraction modes. This includes information such as the filename, analysis of open files for coding preferences and customization, import statements, code before and after the cursor, the program- ming language being used, and preferred styling guidelines.\n\nChapter 5\n\nWhen writing new code, we can enhance the surrounding context by explicitly including cues about the import statements we plan to use. For instance, if we plan to use the built-in @dataclass decorator, which simplifies data-focused classes by automatically generating boilerplate code such as the __init__ method, adding an import statement such as from dataclasses import dataclass helps Copilot understand the decorators we want to work with:\n\nFigure 5.3: Import cues for dataclass decorators\n\nIn this example, Copilot autocompletes the @ cue with the appropriate @dataclass decorator. See the Further reading section for additional resources on data classes.\n\nWhen using Copilot in VS Code, you can enhance the surrounding context in the chat by refer- encing elements such as @workspace or @terminal. These annotations, while beginning with @, do not refer to decorators but rather specify the environment the chat should consider.\n\nAdditionally, specific files or code selections can be referenced using the hashtag, #.\n\n119\n\n120\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nFor example, to request an explanation of the __call__ method in the Singleton metaclass, you can select the method, type #selection in the chat, and then use the /explain command, as shown in Figure 5.4.\n\nFigure 5.4: Using #selection /explain in VS Code/Copilot Chat\n\nQuick tip: Need to see a high-resolution version of this image? Open this book\n\nin the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nThis gives the chat context about the specific code snippet you want explained. For reference, here is the explanation we received for the selected code:\n\nThis code defines a Singleton metaclass in Python. A Singleton is a design pattern that ensures a class has only one instance and provides a global point of access to it.\n\nHere's a breakdown of the code:\n\n1. Class Definition: class Singleton(type):\n\nThis defines a metaclass named Singleton. In Python, a metaclass is a class of a class that defines how a class behaves. A class that uses this\n\nChapter 5\n\nmetaclass will follow the Singleton pattern.\n\n2. Class Attribute: _instances = {}\n\nThis dictionary will store the single instances of the classes that use this metaclass.\n\n3. __call__ Method: def __call__(cls, *args, **kwargs):\n\nThis method is overridden to control the instantiation of the class.\n\ncls refers to the class being instantiated.\n\nargs and **kwargs are used to pass any arguments to the class constructor.\n\n4. Instance Check: if cls not in cls._instances:\n\nThis checks if an instance of the class already exists in the _instances dictionary.\n\n5. Instance Creation: cls._instances[cls] = super(Singleton, cls).__call__ (*args, **kwargs) If the class is not in _instances, it creates a new instance using super() and stores it in the _instances dictionary.\n\n6. Return Instance: return cls._instances[cls]\n\nThis returns the instance of the class, ensuring that only one instance exists.\n\nIn summary, this metaclass ensures that any class using it will only have one instance, implementing the Singleton pattern.\n\nCopilot identified the purpose of the Singleton metaclass and provided a clear explanation of its key components.\n\nNote\n\nAs of January 2025, adding additional surrounding context to Copilot Chat is not yet supported in PyCharm.\n\nFurther narrowing the single task While Copilot is already tailored to the three interaction modes, each aligned with a specific task, there are cases where the task scope can be further refined.\n\nFor example, extracting hardcoded strings with arguments or global variables requires providing more specific guidance instead of relying solely on the straightforward /fix command. Without additional context, the /fix command might generate broader suggestions.\n\n121\n\n122\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nFor reference, here is an example of a /fix suggestion that unnecessarily proposes adding a __post_init__ method to the Environment class (see Figure 5.5).\n\nFigure 5.5: Copilot offers irrelevant fix\n\nAlways aim to include single-task instructions, even when using / commands. For instance, if the goal is to extract hardcoded values, you can specify that the desired fix, extract the hard-coded default values to global constants, alongside the /fix command. Narrowing down the single task in this way helps Copilot better understand your intent and produces more accurate recommendations:\n\nFigure 5.6: Single task narrowed to extracting hardcoded values\n\nIn this example, Copilot effectively understands the task and successfully extracts the hardcoded values, proposing to extract DEFAULT_ENV_NAME and DEFAULT_ENV_VERSION:\n\nChapter 5\n\nFigure 5.7: Successful extraction of hardcoded values\n\nIn the green highlighted text in Figure 5.7, we can see that Copilot suggested extracting the hard- coded default values into global constants named DEFAULT_ENV_NAME and DEFAULT_ENV_VERSION.\n\nThis demonstrates how applying the single task principle and narrowing it further, even when using backslash commands such as \\fix, helps us achieve more effective results when working with Copilot.\n\nSpecific instructions with type hints, docstrings, descriptive names, and unit tests Copilot already preprocesses your files and surrounding context to include specific instructions, ensuring tailored code completions, analyses, and chat responses.\n\nTo follow the specificity practice more precisely in our prompts, we can incorporate type hints, docstrings, descriptive names, and unit tests into our code. These practices not only improve the quality and readability of the code but also help Copilot generate better implementations.\n\nFor instance, if our code involves handling environment versioning as integers, adding clear type hints and a meaningful name enables Copilot to better suggest appropriate default values:\n\nFigure 5.8: Using type hints to specify desired completions\n\n123\n\n124\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nIn Lab 5.2, we will explore how adding a failing unit test can assist Copilot in identifying and addressing issues in our code.\n\nShort prompts without comment fluff When working with Copilot, the goal is to supercharge our work without cluttering the code base we work on. To follow the short principle, we should avoid adding comment fluff, which we define as comments we would not include if not using Copilot and would likely remove after code generation, as they add no value to the implementation.\n\nA better approach is to use meaningful names, type hints, docstrings, and unit tests when gen- erating code, along with a lead-in cue to hint to Copilot that it should start its implementation.\n\nFor example, to instruct Copilot for code completion, it is better to provide a meaningful class signature that aligns with the intended implementation:\n\nclass TestSingleton(TestCase):\n\nThat is better than adding unnecessary comments such as the following:\n\n# Implement a test case to verify the singleton behavior of the Environment class\n\nSimilarly, prefer a docstring instead of comments:\n\ndef recursive_fibonacci(n):\n\n\"\"\"\n\nCalculate the n-th Fibonacci number using a recursive approach.\n\nArgs:\n\nn (int): The position in the Fibonacci sequence (0-indexed).\n\nReturns:\n\nint: The n-th Fibonacci number.\n\nRaises:\n\nValueError: If n is a negative integer.\n\nExamples:\n\n>>> recursive_fibonacci(0)\n\n0\n\nChapter 5\n\n>>> recursive_fibonacci(1)\n\n1\n\n>>> recursive_fibonacci(5)\n\n5\n\n\"\"\"\n\nThat is better than using comments such as the following:\n\n# implement the recursive_fibonacci function here\n\n# The function should calculate the n-th fibonacci number using a recursive approach\n\n# The function should raise a ValueError if n is a negative integer\n\n# the argument is n and the return type is an integer\n\n# Examples:\n\n# recursive_fibonacci(0) => 0\n\n# recursive_fibonacci(1) => 1\n\n# recursive_fibonacci(5) => 5\n\nThe former approach is not only more predictable for Copilot but also helps avoid clutter in the code base if the comment is kept after the class generation, or confusion about the code’s origin if the comment is removed.\n\nAlso, prefer variable names with type hints instead of inline comments:\n\nproduct: float =\n\nUse the preceding code instead of the following:\n\n# initialize a product variable to 1\n\nHere, too, the former approach is more predictable to Copilot and avoids clutter in the code base.\n\nNext, we will explore how to implement the five S’s best practices with Copilot and apply them in the upcoming lab to fix a faulty implementation of a Singleton metaclass.\n\nLab 5.2 – Fixing faulty implementation with GitHub Copilot In this lab, we will address a faulty Singleton implementation by adding a unit test to verify its Singleton behavior, then use Copilot to analyze the error and suggest a fix. This lab is inspired by a bug fix the author encountered earlier in her career, where an Environment class, intended to function as a Singleton, failed to preserve attribute changes across instances.\n\n125\n\n126\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nThe initial faulty implementation of the Singleton metaclass was as follows:\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls.__name__] = super(Singleton, cls).__call__ (*args, **kwargs)\n\nreturn cls._instances[cls.__name__]\n\nThis implementation is close to the correct implementation of a Singleton __call__ function:\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n\nreturn cls._instances[cls]\n\nHowever, the faulty implementation searches for an instantiated class instance using the cls hash key but stores it under the cls.__name__ key. This inconsistency causes a bug that prevents the Singleton from maintaining its intended behavior.\n\nIn this lab, we will address this logical error by first using Copilot to implement a unit test that evaluates the behavior of the metaclass and then asking Copilot to fix the bug.\n\nLab 5.2 guided walk-through Open Lab 5.2 in the book repository at ch5/labs/lab52.py and ensure that you have a stable internet connection and that GitHub Copilot is ready.\n\nThe starter code includes the faulty Singleton implementation along with an import statement for the TestCase class and the main runner from Python’s unittest package:\n\nfrom dataclasses import dataclass\n\nfrom unittest import TestCase, main\n\nclass Singleton(type):\n\n_instances = {}\n\nChapter 5\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls.__name__] = super(Singleton, cls).__call__ (*args, **kwargs)\n\nreturn cls._instances[cls.__name__]\n\n@dataclass\n\nclass Environment(metaclass=Singleton):\n\nname: str = 'Production'\n\nversion: int = 1\n\nif __name__ == \"__main__\":\n\nmain()\n\nIn the next steps, we will create a unit test to debug the faulty behavior of the Singleton metaclass, analyze why it fails, and correct the implementation accordingly.\n\nTo test a metaclass’s behavior, we might want to include a class implementation that utilizes the Singleton and compare different instances of it, as follows:\n\ndef test_something(self):\n\n@dataclass\n\nClass A(metaclass=Singleton):\n\na: int = 0\n\nself.assertEqual(A(2), A())\n\nTo ensure Copilot follows this guideline, we will provide a lead-in cue to start the test case im- plementation by beginning with the class name. Type the following lead-in:\n\nclass TestSingleton\n\nAs you type, Copilot should recognize that you are attempting to test the Singleton metaclass. It may suggest a complete test case, but to tailor it more specifically to our requirements, accept only the first line, which should be as follows:\n\nclass TestSingleton(TestCase):\n\n127\n\n128\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nContinue by adding a descriptive name for the test case:\n\ndef test_singleton_should_return_same_instance(self):\n\nUsing descriptive names for unit tests is good practice, as it clearly conveys the purpose of the test and serves as a documentation tool.\n\nAt this point, Copilot may suggest an implementation involving the Environment class. However, the preferred approach is to test a designated class that uses the Singleton metaclass, as our goal is to validate the metaclass itself rather than the Environment class. Begin the implementation with the following:\n\n@dataclass\n\nclass A(\n\nAt this point, Copilot should detect that we are testing an implementation of the metaclass. By combining the test name and the lead-in cue, it may suggest a unit test such as the following:\n\n@dataclass\n\nclass A(metaclass=Singleton):\n\na: int = 0\n\nself.assertEqual(A(2), A())\n\nThis unit test verifies that an instance of A initialized with a non-default value of 2 should be equal to a new instantiation of the class without additional parameters, ensuring the Singleton behavior is maintained.\n\nWe will delve deeper into writing unit tests with GenAI applications in Chapter 16. For now, accept the suggested test case if it resembles the preceding one or tests similar functionality.\n\nIf you have completed all the steps correctly, your test class should look like this:\n\nclass TestSingleton(TestCase):\n\ndef test_singleton_should_return_same_instance(self):\n\n@dataclass\n\nclass A(metaclass=Singleton):\n\na: int = 0\n\nself.assertEqual(A(2), A())\n\nChapter 5\n\nPlease note that this test will fail because the Singleton implementation has not yet been cor- rected.\n\nNext, execute the lab using the following shell command:\n\npython -m unittest ch5/labs/lab52.py\n\nThis should result in a failing unit test:\n\nFigure 5.9: Failing unit test for Singleton behavior\n\nFailure is expected because the __call__ method of the Singleton is not properly implemented. When a new instance of A is created, its attribute, a, is set to the default value of 0 instead of retaining the value from the previous instantiation of a=2.\n\nIf you are using VS Code, you can get an explanation of the terminal error by typing the following in the Chat window:\n\n@terminal /explain the failing unit test\n\n129\n\n130\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nThis prompt specifies that the context is the terminal, instructing Copilot to explain the test failure as shown in the terminal trace. Here is an example explanation generated by Copilot Chat using the GPT-4o model:\n\nFigure 5.10: VS Code explanation for the failing unit test\n\nNote that the objects in the explanation are clickable for added convenience. Clicking on objects such as A will direct you to their location in the code:\n\nFigure 5.11: Clicking on A would highlight the objects in the file\n\nChapter 5\n\nThe explanation correctly identifies that the incorrect implementation of the Singleton metaclass results in two different instances of the A class, causing the test to fail.\n\nIf you are using PyCharm, copy the failing traceback and paste it into the chat as follows, follow- ing the five S’s:\n\nCONTEXT: You will be provided with a failing Python unit test traceback enclosed with {{{ TRACEBACK }}}.\n\nTASK: Explain why the unit test is failing.\n\nTRACEBACK: {{{\n\npython -m unittest ch5/labs/lab52_singleton.py\n\nF\n\n======================================================================\n\nFAIL: test_singleton_behavior_should_return_same_instance (ch5.labs. lab52_singleton.TestSingleton.test_singleton_behavior_should_return_same_ instance)\n\n----------------------------------------------------------------------\n\nTraceback (most recent call last):\n\nFile \"supercharge/ch5/labs/lab52.py\", line 26, in test_singleton_ behavior_should_return_same_instance\n\nself.assertEqual(A(2), A())\n\nAssertionError: TestS[13 chars]_singleton_behavior_should_return_same_ instance.<locals>.A(a=2) != TestS[13 chars]_singleton_behavior_should_ return_same_instance.<locals>.A(a=0)\n\n----------------------------------------------------------------------\n\nRan 1 test in 0.001s\n\nFAILED (failures=1)\n\n}}}\n\nEXPLANATION:\n\nThis structured prompt, which includes surrounding context, a specific task to explain the failing unit test, the traceback with an enclosed alias, and a lead-in explanation cue, will help us under- stand why our test is failing.\n\n131\n\n132\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nFor reference, here is a sample output from Copilot Chat in PyCharm that correctly identifies the cause of the failing test, specifically, the error in the singleton __call__ method implementation:\n\nFigure 5.12: Copilot Chat explanation for the failing unit test\n\nAlthough this chat does not contain clickable elements, it accurately identifies the problem in the __call__ method that is not implemented correctly.\n\nNow that we have identified the issue with the Singleton metaclass, we can adjust the __call__ method to properly maintain the single class instance design pattern.\n\nChapter 5\n\nTo fix the issue, navigate to the __call__ method, highlight it, and prompt Copilot to fix the cls instantiation, as follows:\n\n/fix cls instantiation\n\nThis prompt will suggest the correct fix for the cls._instances registry. For your reference, here is a sample output we received when using the Copilot extension in VS Code with the GPT-4o model:\n\nFigure 5.13: Suggested fix for the cls instantiation\n\nCopilot provided the correct implementation of the Singleton metaclass using cls._ instances[cls] instead of cls._instances[cls.__name__].\n\nAccept this solution. At this point, your code should appear as follows:\n\nfrom dataclasses import dataclass\n\nfrom unittest import TestCase, main\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n\nreturn cls._instances[cls]\n\n133\n\n134\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nRerun the lab using the following shell command:\n\npython -m unittest ch5/labs/lab52.py\n\nThe unit test should pass, confirming that the singleton is correctly implemented:\n\nFigure 5.14: Test ran with no errors\n\nThe unit test verifies that a new instantiation of the A class retrieves the previously created instance with the argument of a=2, and the main() runner completes without errors.\n\nLab 5.2 analysis In this lab, we debugged the Singleton implementation by creating a unit test that failed. Based on the failure explanation, we used Copilot to fix the error. To accomplish this, we applied the five S’s best practices across five different prompts. Let us examine how each prompt emphasizes different practices of the five S’s, helping us achieve more desirable results.\n\nPrompt 1 – following the structured prompt principle with a lead- in cue To create a test for the Singleton metaclass, we began with a code lead-in cue for Copilot:\n\nclass TestSingleton\n\nWe also followed the short best practice, by avoiding unnecessary comments. This was enough for Copilot to suggest the name class TestSingleton(TestCase) for the test class.\n\nPrompt 2– providing specific and short prompt principles To generate a unit test that utilizes the singleton as a metaclass, we used a descriptive test name, test_singleton_should_return_same_instance, along with a lead-in @dataclass decorator and class implementation:\n\n@dataclass\n\nclass A(metaclass=Singleton):\n\nChapter 5\n\nThis was sufficient for Copilot to understand that the test should validate the correct instantia- tion of the Singleton:\n\nclass TestSingleton(TestCase):\n\ndef test_singleton_should_return_same_instance(self):\n\n@dataclass\n\nclass A(metaclass=Singleton):\n\na: int = 0\n\nself.assertEqual(A(2), A())\n\nThis test case compares two instances of the A class: one created with the default parameter and another with a specified parameter. If the Singleton is implemented correctly, the two instances should be equal.\n\nPrompt 3 – adding surrounding context and focusing on a single task with VS Code After running the failing unit test, we enhanced the prompt with the additional surrounding context and a single task by using the following:\n\n@terminal /explain the failing unit test\n\nThis provided detailed information about the incorrect implementation of the __call__ method.\n\nPrompt 4 – applying the five S’s with PyCharm chat When using PyCharm chat, we applied the same prompt techniques that work with ChatGPT. We used a structured prompt with the surrounding context, focusing on a single task, and created a specific and short prompt that led to an explanation of the traceback for the failing unit test.\n\nPrompt 5 – narrowing down the single task to fix a method implementation Once Copilot identified that the issue was with the __call__ method’s cls instantiation, we provided the following /fix command with a narrowed-down single task:\n\n/fix cls instantiation\n\nThis allowed Copilot to focus on the correct fix required from this method.\n\n135\n\n136\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nIn summary, the five S’s for crafting precise prompts are essential when interacting with any of GitHub Copilot’s three interaction modes. They help refine the structure by incorporating lead-in cues, adding surrounding context to both chats and input statements, and narrowing the focus to the specific task. This approach makes instructions more precise while avoiding unnecessary comments and keeping prompts short and minimal.\n\nIn the upcoming chapters, we will explore advanced prompt engineering techniques to further enhance our ability to achieve even better results from the GenAI applications.\n\nSummary In this chapter, we explored how to apply the five S’s best practices for crafting precise prompts using both OpenAI API and GitHub Copilot. With OpenAI API, we learned how to extract valuable information from Python objects, including attributes such as source code, docstrings, filenames, object types, and more. When working with Copilot, we examined which aspects of the five S’s are already implemented and identified how we can use those more precisely to improve outputs from the GenAI application.\n\nThrough two interactive labs, we utilized the Singleton design pattern, commonly employed by Python developers to enforce a single class instance with a designated metaclass. Using OpenAI API, we generated documentation for the __call__ method of the Singleton. With GitHub Copilot, we constructed a unit test that failed due to an undesirable behavior caused by faulty implemen- tation, analyzed the test failure, and assisted Copilot in fixing the implementation.\n\nWith this, we conclude Part 1 of this book, in which we explored the best practices for crafting precise prompts in ChatGPT, OpenAI API, and GitHub Copilot.\n\nBy now, we have learned how to use OpenAI API, ChatGPT, and GitHub Copilot for a variety of use cases. We examined the costs associated with each tool and observed the OpenAI rate limits. We also explored Copilot’s architecture, built on the foundation of OpenAI API, and gained insights into using ChatGPT’s canvas editor effectively.\n\nWe discussed the three pillars of achieving quality outcomes from a GenAI application: model mastery, evaluation metrics, and precise prompts. We emphasized the role of precise prompts by implementing the five S’s: structured prompts, surrounding context, single-task focus, specific instructions, and short prompts without unnecessary fluff.\n\nChapter 5\n\nNext, we move on to Part 2, where we will delve deeper into understanding the underlying LLMs, analyzing their risks and limitations, and learning how to take ownership of GenAI-generated code, including critically reviewing code created from colleagues’ prompts.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: How can we incorporate information from Python objects when working with OpenAI API?\n\nAnswer: When working with OpenAI API, we often include data about Python objects such as functions, methods, classes, and variables. To extract details such as source code, docstrings, filenames, and object types, we can use special attributes (dunder) methods, or the built-in inspect package.\n\nQuestion 2: Which of the five S’s for precise prompts are implemented in OpenAI API system prompts?\n\nAnswer: OpenAI API system prompts incorporate surrounding context and single-task specifi- cations, separated by a space. These prompts are reusable across different user prompts that are typically handled by a function such as get_user_prompt.\n\nQuestion 3: How can we extend the structured prompt practice with GitHub Copilot?\n\nAnswer: GitHub Copilot handles much of the structuring automatically. Adding a lead-in cue, such as a function name (def func_name), variable type hint (product: float =), or a class name (class ClassName), serves as an effective cue for Copilot to begin the desired implementation.\n\nQuestion 4: Are code comments such as # implement the following function effective when working with Copilot?\n\nAnswer: Such comments are considered fluff and do not enhance the code’s readability or Copi- lot’s output. Always prefer using lead-in cues instead of trivial comments.\n\n137\n\n138\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\n\n\nSingleton design pattern, by Geeks for Geeks: https://www.geeksforgeeks.org/\n\nsingleton-method-python-design-patterns/\n\nMetaclasses, by Real Python: https://realpython.com/python-metaclasses/\n\n\n\nPython dataclass documentation: https://docs.python.org/3/library/dataclasses.\n\nhtml\n\n\n\nThe 3S’s best practices for GitHub Copilot for VS Code: https://techcommunity. microsoft.com/blog/azuredevcommunityblog/single-short-specific---prompting-\n\ngithub-copilot-for-visual-studio/4117226\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\nPart 2\n\nBasics to Advanced LLM Prompting for GenAI Coding\n\nIn Part 2 of this book, we explore how LLMs can be applied to consistently generate quality code. It begins with a high-level guide on how LLMs work and then applies them to different tasks. By the end of this part, you will understand which tools (ChatGPT, OpenAI API, and GitHub Copilot) can be best applied for a given task.\n\nThis part expands into prompt engineering, including advanced topics such as few-shot learning, Chain-of-Thought reasoning, iterative prompting, and chaining to better understand and refactor code. It concludes with an exploration of fine-tuning methods.\n\nThis part contains the following chapters:\n\nChapter 6, Behind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nChapter 7, Reading and Understanding Code Bases with GenAI\n\nChapter 8, An Introduction to Prompt Engineering\n\nChapter 9, Advanced Prompt Engineering for Coding-Related Tasks\n\nChapter 10, Refactoring Code with GenAI\n\nChapter 11, Fine-Tuning Models with OpenAI\n\n6\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nWhile most of the chapters in this book provide hands-on guides to using GenAI models for coding, it is also useful to understand how they work under the hood. ChatGPT, OpenAI API, and GitHub Copilot are based around large language models (LLMs) that have been responsible for the sud- den rise of GenAI not only in the technology space but across industries. By understanding how LLMs work, you will understand when they can be most effective, know their limitations, and appreciate their role in prompting. This will maximize your overall success with GenAI.\n\nThis chapter will introduce the following concepts:\n\n\n\nStatistical approach to finding similar letters\n\n\n\nSmall and large language models for words\n\n\n\nImproving LLM output performance\n\nWhat are the limitations of LLMs?\n\n\n\nChatGPT\n\nGitHub Copilot\n\n142\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nStatistical approach to LLMs Many users of ChatGPT and other LLMs often have an incorrect impression of how LLMs work. They ask questions such as What fraction of the internet do they understand?. To introduce the un- derlying concepts, this section focuses on a simpler problem using just statistics. This will allow us to provide a foundation for discussing how models work before we shift into AI and deep learning. The question we will address is whether we can use probabilities to predict the next letter given a set of preceding letters. If we can, then we should be able to create true English from data without any understanding of linguistic orthography, phonology, morphology, or even general language.\n\nFor this sample problem, we will use a medium-sized dataset called the Reuters-21578 dataset. It is available at the UCI Machine Learning Repository (https://archive.ics.uci.edu/) and consists of 21,578 different news wire articles, which are just text stories across different topics. While the main use of this dataset is to train and assess text classification algorithms, we are using this as a large corpus of text that has over 15 million letters. The rest of this section describes the process of using this corpus to predict the next letter using a statistical approach. A curious reader can experiment further with the provided code available in the GitHub repository for Chapter 6.\n\nOur first analysis step is to look at the frequencies of individual letters. Although the dataset has not only letters but also numbers, punctuation, and other symbols, we will focus on just the letters and the spaces between words. We will also ignore the case and convert everything to lowercase. The frequencies of the individual letters are shown in Figure 6.1. The frequency of\n\nChapter 6\n\nwords typically follows Zipf’s law, but not for letters. However, we do see the expected tapering of frequencies when sorted by letter, although the distribution is not as smooth due to the limited size of the dataset.\n\nFigure 6.1: Letter frequencies for the Reuters-21578 dataset\n\nWith the frequencies or probabilities of the individual letters from Figure 6.1, we can produce words by sampling from this distribution. We can sample proportional to the frequency of the letters in the dataset and try to craft English words. We can do this by generating a random number with a uniform distribution between 0 and 100%. If it’s < 12%, it would be an e. If it’s in the 12 to (12+9)% range, then it would be t. If it’s in the 19 to (19+8.5)% range, it would be a, and so on. The results of this approach are shown in Figure 6.2. Each row represents a different example to show some variation. As you can see, we have not replicated anything that looks vaguely like English:\n\nFigure 6.2: Creating “words” from using just the individual word frequencies\n\nUsing the individual frequencies of words doesn’t use any context, and so it produces words that do not resemble English. Context in this case refers to the order of the letters that come before it. For instance, in the first line, no English words start with wwu or tte or dn. Similarly, the letter q is usually followed by u in English.\n\n143\n\n144\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nTo augment this with the context of preceding letters, we can use conditional probabilities. In statistical terms, p(x|y) is read as the probability of x given y, and it means the probability of x occurring given that y occurs. Mathematically, the formal definition is as follows:\n\n\n\n𝑝𝑝(𝑥𝑥𝑥𝑦𝑦) 𝑝𝑝(𝑥𝑥)\n\n𝑝𝑝(𝑥𝑥|𝑦𝑦) =\n\nIf we think of playing cards, the probability of the queen of diamonds is 1/52, but the probability of the queen of diamonds given it is a queen is 1/4. The probability of a queen of diamonds given it is a diamond is 1/13.\n\nIn our context, we are going to use these conditional probabilities to compute the probability of the next letter. So, given the current letter is a q, the next letter is usually a u. In fact, we can build a table of all the possibilities written as percentages for each combination of letters. Figure 6.3 shows the first letter as the column, and the conditional probability of the next letter as a row. For instance, p(u|q) is 90.3%.\n\nFigure 6.3: Percentage of conditional probabilities for next letters (rows) given letters (columns)\n\nChapter 6\n\nThe sampling is more complicated in the conditional probability case, so let’s use an example. To create text, we might assume that our last letter is a q. To choose the next letter, we randomly sample from the possibilities across all rows of the q column. We then generate a random value from 0 to 100% representing a random probability drawn from a uniform distribution for the next letter. We then consider which bucket the random number fell in. The letter a is 0.7%, b through h are all 0.0%. The letter I has a 1.1% chance. The letter t has a 7.6% chance, and the letter u has the largest probability of 90.3%. While it appears that these numbers do not sum to 100%, this is due to round-off errors only. If we select based on these probabilities, the most likely letter is u. We repeat the process with the u column, where the most likely letters are t, r, n, and s, and the least likely letters are q and u. You are likely wondering which words have these strange letters, but remember that there may be acronyms, email addresses, and foreign names included in the articles.\n\nNow that we can generate words for conditional probabilities, does this approach work? Figure 6.4 shows the results. The word lengths are much closer, and there are a couple of real words, such as it, as, and mag, but otherwise, they do not look like English words:\n\nFigure 6.4: Conditional probabilities for p(next_letter | current letter)\n\nWe can extend the model so that instead of just looking at a letter given its preceding single letter, we can consider a wider context. Specifically, we can consider the next letter given the preceding j letters using the probability p(next_letter | preceding j letters). This makes the table much larger since the number of columns would increase from 26 to combinations of 2 letters, 3 letters, or more. For the next letter, based on all combinations of the 2 previous letters, the table would be 26 by 262 = 676. For the next letter based on the previous 3 letters, the table would be 26 by 263 = 17,576. However, with a longer context, we would expect better results. We will not show these larger tables of percentages or probabilities, but show the results instead.\n\n145\n\n146\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nThe result for the preceding two letters is shown here:\n\nFigure 6.5: Conditional probabilities for p(next_letter | preceding 2 letters)\n\nFor the preceding three letters, the result is as follows:\n\nFigure 6.6: Conditional probabilities for p(next_letter | preceding 3 letters)\n\nFor the preceding four letters, this is the result:\n\nFigure 6.7: Conditional probabilities for p(next_letter | preceding 4 letters)\n\nFor the preceding five letters, the result is as follows:\n\nFigure 6.8: Conditional probabilities for p(next_letter | preceding 5 letters)\n\nChapter 6\n\nFinally, the result for the preceding six letters is the following:\n\nFigure 6.9: Conditional probabilities for p(next_letter | preceding 6 letters)\n\nAs we can see, the results improve substantially as we increase the context window. In fact, most of the output has real English words once we use the four preceding letters. With six preceding letters, the sentence flow starts to make some sense, which may be surprising since we are only looking at single letters.\n\nIt is important to understand what the model is learning. Although the statistics appear to be producing English words and even some meaningful sentences in Figure 6.9, it does not under- stand the concept of an English word or sentence structure. Furthermore, the model is not storing knowledge or facts of the English language or words. It is only storing the probabilities of the next letter from the preceding context. Our model focuses only on letters at this point, and the next section describes the extension to word-level models. If we had training data that was a thousand or a million times larger, it would be merely doing a better job of estimating the same frequencies.\n\nExtending the model to words Our English model, which builds language from letters, is quite limited. To produce meaningful language, we would need a much longer context of preceding words to understand the relation- ships between words spaced throughout a sentence. Furthermore, we don’t think of language in terms of letters—we think in terms of words. With our 26 letters in English and a space, each of the preceding j positions can only have 27 possibilities, which means our model is relatively small.\n\n147\n\n148\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nNote\n\nIf we consider all combinations of the preceding 3 letters, there are 27 x 27 x 27, or about 20,000 possibilities of letters. With 15 million letters in the dataset and 20,000 combinations, we have a 750:1 ratio of training to combinations, which provides excellent training. The average person knows 42,000 words, so if we use all combi- nations of the preceding 3 words, it would be 42,000 x 42,000 x 42,000 ~ 7.4E13, or 74 trillion 3-word combinations possible, which is about 10x more than any PC hard disk can handle today. However, if we have 15 million letters and the average word plus a space character is 6 letters, then there would be 2.5 million words. The ratio of training to combinations drops to a 2.5 million:74 trillion ratio, or 1 in 30 million.\n\nBased on the preceding information, we would need billions of times more data for similar ac- curacy using the statistical model.\n\nAs a model becomes more complex, it requires more training data. We can think of a model as a general function where we try to approximate a y value from different x values. This approxi- mation process determines the best function parameters from a set of training data (x, y) pairs representing the input, x, and the output, y. In the case of a linear model, the model must learn the best slope and y-intercept to represent the data. The training data is collected and designed to be representative of future data so that when the model fits well, it can be useful to predict future y values.\n\nOf course, most models are not simple linear models or functions. There may be multivariate inputs with billions of parameters and large output dimensionality. Learning from such large datasets requires significantly larger training data. Consider fitting a linear or a higher-order linear model to a set of data points, as shown in Figure 6.10:\n\nFigure 6.10: The number of training data points increases as the model becomes more complex\n\nChapter 6\n\nWe can fit a line with just two points, perhaps, but the seventh-order model on the right needs far more data points to accurately fit. AI models often require data for trillions of parameters, which introduces a problem where the model may suffer from insufficient training data available. The accuracy of the model depends on the number of parameters in the model and the size of its training dataset.\n\nTo summarize, the example with letters showed that a statistical model could approximate English language words with a moderate training dataset since the number of combinations is reasonably small. However, we need to store the probabilities of each letter given all preceding letter contexts, which can be quite large. For even the context of just three preceding words, the frequencies are prohibitive to store. In addition to the storage challenge, we would need vast quantities of data to fill a word frequency table to achieve modest prediction accuracy, which is not readily available. The next section uses AI rather than statistics to overcome these challenges for words.\n\nSmall and large language models for words The statistical model clearly worked well for a medium-sized dataset with a small model based on letters. Since grammar and meaning function at the word level, we need to be able to accu- rately predict the next word rather than the next letter. Since the number of possible next words is far larger than the number of next letters, the model will need to be many orders of magnitude larger. If we think about code or even text, there are no datasets that are billions of times larger. AI methods, and specifically LLMs, provide solutions that enable predictions more efficiently than just probabilities. We describe two methods in this section: word2vec and the Transformer deep learning model.\n\nWord2vec The word2vec model is arguably a small language model that uses a simple neural network. In- spired by biology, neural networks are a particular type of machine learning algorithm that learn from data. Although there are a variety of neural network models, this type learns a mathematical function. Each output neuron is a nonlinear function of the weighted sum of the input values. In the training step, the weights are adjusted so that the input values will predict the output values.\n\nAs an example, a neural network could learn to predict whether today is a good day to play tennis based on the inputs of temperature, wind, precipitation, and humidity. The training data would use past decisions to play tennis (output=1) or not play (output=0), and the corresponding weather conditions. With only this data, a neural network could learn to classify future good days for tennis based on the weather. The learned weights for wind and precipitation might be large negative values since high wind speeds and precipitation make tennis unplayable.\n\n149",
      "page_number": 140
    },
    {
      "number": 7,
      "title": "words typically follows Zipf’s law, but not for letters. However, we do see the expected tapering of frequencies when sorted by letter, although the distribution is not as smooth due to the limited si",
      "start_page": 178,
      "end_page": 195,
      "detection_method": "regex_chapter",
      "content": "150\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nDeveloped by Google in 2013, the word2vec algorithm (https://en.wikipedia.org/wiki/ Word2vec) trained a model to predict the target word from the context of two words before and after that word. They also developed the opposite problem of predicting the surrounding context from a single word. Essentially, this is equivalent to the next word prediction using the presence of adjacent words rather than only preceding words. Backed with an efficient training algorithm, the results proved reasonably predictive. Of greater importance, the word2vec weights formed an embedding of the word or numeric vector representation that quantified the semantic and syntax func- tion of words. In other words, the word2vec model learns to associate every trained word with a numeric vector of numbers. These vectors can be compared, which is useful for comparing the similarity of word meanings.\n\nDeep learning Deep learning is a subset of neural networks that involves learning functions of functions, essen- tially. That is, the outputs of one layer form the inputs to a second layer, creating a deep stack of functions. The Universal Approximation Theorem proved that neural networks could learn to approximate any mathematical function, given a two-layer network under certain conditions. Research has found that creating deep neural networks with tens, hundreds, or thousands of layers is an efficient way of achieving accurate results. In each layer, the model adjusts its weights based on the training data to learn functions with the results passed as input to subsequent layers.\n\nIn 2017, a new deep learning architecture called the Transformer was published in a famous paper called Attention is All You Need from Google, the link to which can be found in the Further reading section. Rather than relying on the context of adjacent words, the Transformer architecture ef- ficiently learns the relationship between all words in a sentence. The training process learns the relationship between words using a masking process, through which it hides each word separately and learns to predict it from all the other words in each sentence.\n\nA deep learning network describes an architecture in machine learning with a specific number of inputs, outputs, layers, nodes per layer, inter-node connections, and weights throughout. Like the previously described linear model, the architecture describes the solution and is trained to solve a specific problem. LLMs are the result of training a deep learning network on large volumes of text. They learn to correctly predict the next word much better than statistical and word2vec models. Similar to the statistical next-letter prediction models, they can iteratively predict the next word to generate sentences, paragraphs, and full texts. Researchers have found that by training on ever larger volumes of text (or code), the accuracy continues to improve, so many LLMs have been developed that use increasingly larger network sizes and more training data.\n\nChapter 6\n\nLLMs by themselves offer significant capabilities in next-word prediction, but they are continuing to expand into new areas of intelligence. Many of the breakthroughs are now coming from various techniques that improve the performance of their respective LLMs. The next section describes some steps that users can take with an LLM to improve the outputs of LLMs.\n\nImproving LLM output performance If the LLM does not answer questions well, there are four general courses of action: improving prompt engineering, adding more data through the context window, providing examples (few- shot learning), fine-tuning, and retrieval-augmented generation (RAG). These will be discussed in this section.\n\nPrompt engineering The discipline of prompt engineering has recently emerged as a human interface to the LLM. Us- ers or programs can send commands to, ask questions of, and apply research reasoning through the LLM. The discipline involves applying specific wording to maximize the chance of correct outputs without producing erroneous or biased outputs, including hallucinations. The process of writing prompts is often an iterative process where it may require multiple rounds to achieve the desired output. Most of the prompts discussed in this book focus on a single task with specific and clear wording, which is considered good practice. The five S’s framework outlined in this book provides a useful general-purpose structured approach to writing the prompts. We’ll cover more on prompt engineering techniques in later chapters.\n\nAdding context As discussed in this chapter, LLMs are not databases that store data, nor are they an oracle of all knowledge (yet). They can perform a limited number of tasks and answer a finite number of questions without further enhancements. To enable question-answering, interactive chats, or other tasks such as code generation, the user can send the LLM both the question as well as additional information within a context window.\n\nNote\n\nThe context window is somewhat analogous to a human’s short-term memory in that it has a limited size and may forget older information. However, the context window is generally large enough to include documents, earlier inputs, and returned outputs. A document can be provided as context, and the prompt engineer can ask questions of the document.\n\n151\n\n152\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nDepending on the size of the context window, all or part of earlier outputs from the system and past inputs are automatically included in the next prompts associated with that user, so an entire dialog or thought process is included in every interaction with the LLM. A short context window might only allow a few papers to be uploaded and summarized together, while a longer context window could enable a book to be uploaded with multiple interactive sessions kept as context. This approach has several advantages over an LLM system, which will be discussed next as ways of improving results.\n\nProviding examples The context window allows for documents and other data to be included along with the prompt or question, but it also allows for examples. A prompt may ask the LLM to return a particular nugget of information in a specific format. In many cases, this will be successful, and this is called zero-shot learning. That is, there are no examples of how the LLM should form a response—it just uses its baseline approach. However, the prompts can include one example of an input and an output as part of the prompt. This is called one-shot learning. If multiple examples are provided, this is called few-shot learning. While many examples can be used depending on the context window length, there is a law of diminishing returns. However, it is an effective approach to instruct the LLM to tailor its outputs to meet a specific need. We will discuss few-shot learning in depth in Chapter 8.\n\nFine-tuning Training an LLM from scratch may cost $100 million to learn the trillions of weights that are trained on prose, poetry, drama, literature, and science. Due to the prohibitive cost, it will not be trained often. In fact, it is typically two years out of date, which means it does not have any recent events or information. It also likely doesn’t have personal or corporate documents includ- ed in its training. One solution discussed is to provide these in a context window. As discussed in Chapter 2, for pay-for-service LLMs such as OpenAI, every transaction has two costs: a cost to send information to the LLM and a cost for the output produced, both based on the length of the information. As the amount of input information and the number of calls with the same information increase, it may become a costly solution.\n\nFine-tuning offers an alternative to repeatedly sending a large context to the LLM. It essentially adds a limited supplemental dataset to the LLM training in the form of documents or examples. The training fine-tunes the weights of the neural network by making minor adjustments that are\n\nChapter 6\n\nsufficient to produce improved outputs. Fine-tuning provides a cost-efficient way of improving outputs without repeatedly sending large contexts of data to the LLM. We will dive deeper into fine-tuning in Chapter 11.\n\nRetrieval-augmented generation RAG solves two major problems with the LLMs. First, OpenAI and ChatGPT are not databases of information, so they cannot retrieve information like a web search. Second, their training data is typically two years old and does not include personal or corporate documents. The RAG architecture integrates the LLM with external information from a web search, database, or set of documents. RAG uses the LLM to interpret the input from a user and then retrieves blocks of text from various sources that it finds to be closely related to the input. The LLM then processes the combination of the input and retrieved data to form an answer or response using the stan- dard LLM processing. Essentially, RAG fills the context window with candidate answers relevant to the question based on a search and then instructs the LLM to perform its usual summary or question-answering on that context window. This enables a full system with an LLM to perform web searches on current information or focus on specific documents when generating output.\n\nEach of the approaches described in this section is widely used to improve LLM results. While research continues to advance LLM capabilities, it is important to understand their limitations, which are discussed in the next section.\n\nWhat are the limitations of LLMs? With both the statistical approach for the letters and the LLM approach for words, there is no formal knowledge that many expect, given its ability to answer many questions. There is no extensive knowledge base of topics, common sense, or anything except the word patterns it has learned. What is surprising to researchers is the fact that the learned patterns and their relation- ships somehow enable LLMs to perform new tasks for which they have never been trained. For instance, LLMs have even shown some capability in some foreign languages that are not part of their training set, through a process called zero-shot cross-linguistic transfer. Even if the output has been trained, one may get slightly different results each time from a prompt due to the randomness in the system. Much of the research in the prompt engineering area focuses on how to receive consistent and correct outputs from LLMs by crafting the prompts with specific phrases, commands, and context.\n\n153\n\n154\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nThe deep learning approach will almost always produce syntactically correct and plausible-looking output, but some of the output may be prone to hallucinations. There have been some famous cases where the output was completely incorrect, technically wrong, and sometimes just made up. A professor was accused of assault on a school trip that never happened. Many students have been caught using ChatGPT since none of their references existed. Even a US government health report was caught citing facts of a made-up reference. These incidents can be found in the Further reading section. Hallucinations often occur when there is insufficient training to support a particular output. The statistical methods do not have such an issue since they only sample from the previously seen patterns.\n\nIt is currently nearly impossible to know in which areas exactly the LLM has expertise, and in which areas they have limited training. A few years ago, the training of LLMs was generally re- leased as part of its research publication. In the past couple of years, much of the LLM training has become more secretive, so we do not know exactly which sources were used and cannot exactly predict their domains of capability. However, OpenAI has been trained extensively on both texts and code in multiple languages. OpenAI’s philosophy is to provide safe and beneficial general AI across many domains. ChatGPT is a specialization using an OpenAI LLM and is discussed in the next section.\n\nChatGPT Launched by OpenAI in November 2022, ChatGPT was developed on the GPT-3.5 model. It be- came a focal point of the AI growth since it worked through a conversational interface, unlike the programming interface of other LLMs. The dialog-driven interface requires it to track the history of the conversation, even across sessions. ChatGPT is a specialization of the LLM with improved conversational capability.\n\nOne of the key technology improvements was the use of reinforcement learning from human feedback (RLHF), which improved the dialog capability through training and feedback to give better responses. As discussed earlier, fine-tuning can improve the LLM’s performance for specific tasks. RLHF goes one step further and incorporates feedback from users to adjust the weights. As a result, it delivers results that incorporate human preferences and even reasoning capabilities in terms of responses and answers to questions.\n\nChapter 6\n\nChatGPT continues to be a leader in the growth of AI, in part due to its easy web interface that makes it widely accessible with minimal training. It has led to greater research in natural lan- guage processing (NLP) and multiple use cases. Although it can write code, its capabilities are limited as compared to GitHub Copilot, which extends the GPT model for generative coding, as described in the next section.\n\nGitHub Copilot GitHub Copilot was officially announced as a collaboration between GitHub, Microsoft, and OpenAI, specifically for writing code, in 2021, and opened for technical review. At the time, it started with the GPT-3 instance from OpenAI, and the three companies collaboratively trained it to focus on code development, resulting in the OpenAI Codex. After GPT-4 was released, GitHub Copilot adopted it and released GitHub Copilot X. Its optimization focuses on code completion, fixing bugs, code comments, and tests based on billions of lines of code training beyond the GPT models. The combination of the underlying GPT model with the code-training focus enables the two-way natural language and computer language code writing and explanation.\n\nGitHub Copilot uses a combination of technologies to provide its coding experience. When writing code from a single file, it sends at least part of the current file through the internet to the Copilot server. It will also use the prompt and other open files that are open within their respective VS Code or PyCharm IDE for context. It uses this context to search for similar code using the RAG approach, and then sequentially generates code one token at a time. The amount of code sent to the Copilot server is based on the context length and certainly costs.\n\nIf part of your open code and possibly data is being sent to Copilot, it is natural to be concerned about security. The information is encrypted when sent to and from Copilot. The exchange context resides in memory but is deleted without being stored or used for training. The GitHub Copilot Trust Center page, available at https://copilot.github.trust.page/, provides more infor- mation about their data security, privacy, and related issues. It includes suggestions, feedback, prompts, and user engagement data. None of the code exchanged while using GitHub Copilot is used for training their system.\n\n155\n\n156\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nSummary Statistical and AI approaches can learn patterns from sequential data such as text or code, which can efficiently predict the next symbol. Due to the range of possible words, deep learning methods scale well to efficiently predict the next words much better than statistical techniques and have become the standard. Even so, they require vast training sets of text and code and an expensive computational process to be effective.\n\nWhile LLMs are highly effective, they are not perfect and are limited by their training set of data and perhaps the time it was trained. The output can be limited for various domains and time pe- riods, which can result in hallucinations. While research continues in these areas, some solutions to improve results include effective prompt engineering, providing contextual information such as documents or examples, fine-tuning, and the RAG approach.\n\nOpenAI and ChatGPT are trained on general data and code, and are quite effective at generating code. GitHub Copilot is built upon OpenAI and, with significant additional training on code, is more efficient at generating code, tests, comments, and more.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: How is the deep learning approach for words similar to and different from the sta- tistical approach described for letters in this chapter?\n\nAnswer: Both the deep learning approaches and statistical approaches generate new text or code using a combination of their trained model and randomness. Even though they are both trained on data, they rely on randomness to select from the probable next words, which means their outputs can be different each time. In both cases, if they are trained with insufficient data, they may not produce satisfactory results. Neither has a conceptual understanding of the inputs or outputs, so both are relying on patterns of inputs to predict patterns of outputs.\n\nThe deep learning approach is orders of magnitude more efficient in predicting the range of possible next words or code using complex, less interpretable models. While both could easily predict the next letters with a large training set, the conditional probabilities would not be able to scale to predict next words as the deep learning approach can.\n\nChapter 6\n\nQuestion 2: How is GitHub Copilot different from OpenAI in terms of code development?\n\nAnswer: OpenAI was optimized for general knowledge, and GitHub Copilot was optimized for code generation. GitHub Copilot took the human language capabilities of GPT-4 from OpenAI and further trained it on billions of lines of code, comments, and documentation. In this way, it serves as a pair programmer with the ability to generate code from text, explain code as text, write tests from code, and write comments from code.\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nZipf’s law: https://en.wikipedia.org/wiki/Zipf’s_law\n\nMikolov, Tomas; Chen, Kai; Corrado, Greg; Dean, Jeffrey (16 January 2013). Efficient Estima- tion of Word Representations in Vector Space: https://en.wikipedia.org/wiki/Word2vec\n\n\n\nVaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). Attention is All You Need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (ed.). 31st Conference on Neural Information Processing Systems (NIPS). Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc.: https://arxiv.\n\norg/abs/1706.03762\n\n\n\nPranav Dixit. US law professor claims ChatGPT falsely accused him of sexual assault, says ‘cited article was never written’. Business Today: https://www.businesstoday.in/technology/ news/story/openai-chatgpt-falsely-accuses-us-law-professor-of-sexual-\n\nharassment-376630-2023-04-08\n\n\n\nLoreben Tuquero. RFK Jr.’s health report shows how AI slips fake studies into research: https://\n\nwww.poynter.org/fact-checking/2025/rfk-jr-fake-citations-medical-journals/\n\nGitHub Copilot Trust Center page: https://copilot.github.trust.page/\n\n157\n\n158\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.\n\n7\n\nReading and Understanding Code Bases with GenAI\n\nWriting code is rarely an isolated task. We will typically have to incorporate our Python snippets into a larger code base that includes project files, often structured under a /src folder, along with dependencies listed in a requirements.txt file and other non-project files, such as a Dockerfile that defines container specifications.\n\nIn this chapter, we will work with a code base that computes the Manhattan distance between two matrices represented as pandas DataFrames. With the help of ChatGPT, GitHub Copilot, and OpenAI API, we will explore how various GenAI tools can support us in efficiently understanding both non-project files and the main business logic. We will also examine how generating calls with sample parameters can improve our understanding of more complex code.\n\nHere are the key topics covered in this chapter:\n\n\n\nReading and understanding the code base structure\n\nUsing LLMs to explain code\n\n\n\nInterpreting non-project files\n\n\n\nCreating calls with example parameters\n\n160\n\nReading and Understanding Code Bases with GenAI\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/ Supercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nReading and understanding the code base structure So far in the book, we have mostly explored code snippets that are limited to a single Python file. In this chapter, we will shift our focus and see how GenAI tools can support us in reading and understanding a broader code base.\n\nGetting to know a code base usually involves reading through a variety of files. For instance, when a developer encounters a new project, they might do the following:\n\nExplore entry points such as main.py or app.py\n\nIdentify the main project files, often found in a /src directory\n\nLook for container and dependency management files such as Dockerfile and requirements.txt\n\nExperiment with specific parts of the code\n\nIn this book’s repository, we provide a small code base example located under ch7/. Here is a directory tree view of its contents:\n\nch7/\n\n├── app.py\n\n├── src/\n\n│ └── manhattan.py\n\nChapter 7\n\n├── Dockerfile\n\n├── README.md\n\n├── requirements.txt\n\nIn this tree view, the core business logic that we will explore next is located in src/manhattan. py. In addition, we have the following:\n\napp.py, which serves as the entry point and handles basic Hypertext Transfer Protocol (HTTP) POST requests, routing them to the business logic. As introduced in Chapter 2, HTTP POST is a standard method for sending data to a server.\n\nDockerfile, a text file that outlines the steps and dependencies needed to build a container image for running the application. We will return to this later in the chapter.\n\nrequirements.txt, which lists the application’s dependencies, such as pandas and Flask, as in- troduced in Chapter 2.\n\nREADME.md, a markdown file commonly used to provide documentation about a code base. In our case, it includes examples of how to use the application.\n\nNext, we will introduce the Manhattan distance, which serves as the core business logic of our application.\n\nIntroducing the Manhattan distance In this chapter, our code focuses on calculating the Manhattan distance between two points. This distance is defined as the sum of the absolute differences along each dimension. In two-di- mensional space, we can relate this to classic computer games such as Pacman, where movement is restricted to vertical or horizontal directions, with no diagonal steps allowed. For example, consider Pacman and a ghost positioned in locations (1,1) and (3,2), respectively, in Figure 7.1.\n\nFigure 7.1: Pacman and ghost positions\n\n161\n\n162\n\nReading and Understanding Code Bases with GenAI\n\nIn this figure, the characters can move either horizontally or vertically. The Manhattan distance between them is two units along the x axis and one unit along the y axis, which totals 3:\n\nManhattan\n\nDistance\n\n\n\n=  |3 − 1|  +  |2 − 1| = 3\n\nA simple Python implementation for calculating this distance might look like this:\n\ndef get_manhattan_distance(\n\nx1: Tuple[int, int],\n\nx2: Tuple[int, int],\n\n) -> int:\n\nx_dist: int = abs(x1[0] - x2[0])\n\ny_dist: int = abs(x1[1] - x2[1])\n\ndist: int = x_dist + y_dist\n\nreturn dist\n\nIn this implementation, we use two-dimensional vectors represented as Tuple[int, int] to represent a position on a board. However, the Manhattan distance can be generalized to higher dimensions computed between matrices of floats, as we will see next.\n\nManhattan distance for matrices The Manhattan distance, also referred to as the L1 norm, extends naturally to higher-dimensional data. It is widely used in image analysis when comparing two images and in machine learning research when evaluating prediction errors.\n\nTo compute the Manhattan distance between matrices, we can use pandas DataFrame objects. pandas is a standard library for data manipulation and should be listed in the book’s requirements. txt file and already installed in your virtual environment.\n\nHere is an example of a DataFrame with two rows, A and B, and three columns labeled a, b, and c:\n\nimport pandas as pd\n\ndf1: pd.DataFrame = pd.DataFrame(\n\ndata={\n\n\"A\": [1, 2, 3],\n\n\"B\": [4, 5, 6],\n\n},\n\nindex=[\"a\", \"b\", \"c\"],\n\n)\n\nChapter 7\n\nWe can implement the Manhattan distance using the pandas methods .abs() for absolute values and .sum() for summing along axes:\n\ndef get_manhattan_distance(\n\ndf1: pd.DataFrame,\n\ndf2: pd.DataFrame,\n\n) -> np.float64:\n\nelement_wise_dist: pd.DataFrame = (df1 - df2).abs()\n\ndist: float = element_wise_dist.sum().sum().astype(float)\n\nreturn dist\n\nThe distance calculation uses .sum() twice: first to sum across rows, and then again to sum the resulting column totals. We convert the result to a float to standardize the output, since summing over integer values may return a NumPy int64 data type.\n\nLet’s take these two DataFrames:\n\nA = [[1 3]\n\n[2 4]]\n\nB = [[5 7]\n\n[6 8]]\n\nCalling get_manhattan_distance on these will return the following:\n\n|5-1| + |6-2| + |7-3| + |8-4| = 16\n\nNow that we have a clear understanding of the core business logic and supporting code around the distance computation, we can explore how GitHub Copilot, ChatGPT, and OpenAI API can help us navigate and familiarize ourselves with the code base.\n\nUsing LLMs to explain code In Chapter 3, we noted that GitHub Copilot does not include the entire code base in the prompt. This is due to context window limitations, potential cost, and the fact that much of the content may not be relevant to the specific task.\n\nHowever, this does not prevent GenAI from helping us understand the code bases we are working with. Rather than relying blindly on Large Language Models (LLMs) to identify key components, we will explore how to request explanations for both simple elements and more complex scenarios.\n\n163\n\n164\n\nReading and Understanding Code Bases with GenAI\n\nUsing the /explain command with GitHub Copilot GitHub Copilot includes several slash commands, among them the /explain command, which is available in both the inline chat and the chat window. Unlike code completion tasks, the output of this prompt is explanatory text covering arguments, data transformations, and return values.\n\nThe command can be used alone or with extra instructions. For example, in PyCharm, we can ask Copilot to explain the get_manhattan_distance function by placing the cursor anywhere within the function signature, opening the inline chat window, typing /explain, and hitting Enter, as shown in Figure 7.2:\n\nFigure 7.2: Using GitHub Copilot /explain for methods\n\nAnother available option is to select the contents of the function, open a chat window, and type /explain. Note that in VS Code, you will need to choose @workspace /explain by either typing it directly or selecting the /explain @workspace option in the drop-down menu after typing / explain, as shown in Figure 7.3:\n\nFigure 7.3: Choosing @workspace /explain in VS Code chat\n\nFor either IDE, Copilot is likely to yield a general explanation about the purpose of the function, the data transformation, and the returned value, as shown in Figure 7.4.\n\nChapter 7\n\nFigure 7.4: Sample /explain output in VS Code\n\nIf we wanted a broader analysis of the arguments and their type, for example, we could use this prompt:\n\n/explain argument types\n\nThis should yield an extended explanation about the arguments, as shown in Figure 7.5.\n\nFigure 7.5: Extended /explain argument types with GitHub Copilot\n\n165\n\n166\n\nReading and Understanding Code Bases with GenAI\n\nQuick tip: Need to see a high-resolution version of this image? Open this book\n\nin the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nHere, we see a detailed explanation of the arguments df1 and df2, including a note that df2 is expected to have the same shape as df1 for the function to work properly. This expectation is not mentioned in the plain /explain prompt, but when we ask about a specific aspect of the code, we get a more in-depth explanation.\n\nNext, we will see how we can replicate this outcome through direct prompting with ChatGPT and OpenAI API.\n\nUsing ChatGPT to explain code While Copilot includes a built-in slash command for code explanations, we can apply the five S’s framework introduced in Chapter 4 to achieve similar results with ChatGPT, even when us- ing older or more cost-effective models such as GPT-4o. Unlike GitHub Copilot, which follows predefined instructions about what to explain based on the Python object, ChatGPT allows us to tailor the explanation to our specific needs.\n\nFor instance, the following prompt focuses on describing the function’s purpose, its arguments and types, the data flow, expected output, and possible edge cases:\n\nCONTEXT: you are provided with:\n\n1. A Python function enclosed with {{{ FUNCTION }}}\n\n2. Explanation points enclosed with {{ HEADERS }}}.\n\nTASK: Explain the function using the explanation points.\n\nFUNCTION: {{{ ...omitted for brevity ... }}}\n\nHEADERS: {{{\n\n1. Function's purpose\n\nChapter 7\n\n2. Arguments and their types\n\n3. Step-by-step data flow\n\n4. Output and its types\n\n5. Potential edge cases\n\n}}}\n\nEXPLANATION:\n\nWhen using GPT-4o with this structure, we received an explanation that covered all bullet points with the headers, as partially shown in Figure 7.6:\n\nFigure 7.6: ChatGPT returned extended explanation\n\nFor reference, here is the output generated by ChatGPT under the edge cases header, which adds value by uncovering situations where the function might fail or produce unexpected behavior:\n\n5. Edge cases\n\nShape mismatch: If df1 and df2 do not have the same shape, the subtraction operation will raise a ValueError.\n\nNon-numeric columns: If the DataFrames contain non-numeric data, the subtraction operation will raise a TypeError or produce NaNs that affect the result.\n\nMissing values (NaNs): If either DataFrame contains NaNs, the output\n\n167",
      "page_number": 178
    },
    {
      "number": 8,
      "title": "├── Dockerfile",
      "start_page": 196,
      "end_page": 245,
      "detection_method": "regex_chapter",
      "content": "168\n\nReading and Understanding Code Bases with GenAI\n\ndistance may be NaN unless explicitly handled beforehand.\n\nEmpty DataFrames: If both inputs are empty DataFrames, the result will be 0.0.\n\nAlthough writing a prompt for ChatGPT might take slightly more effort compared to using the /explain command, it can be worthwhile when we need more targeted explanations, such as uncovering edge cases.\n\nNext, we will demonstrate how to achieve this using OpenAI API, which may be more appropri- ate for scenarios where the repository becomes larger and we want to generate explanations for many functions programmatically.\n\nUsing OpenAI API to explain code Using the same prompt structure from ChatGPT, we can make a programmatic call to OpenAI, following the best practices introduced in Chapter 5.\n\nWe will define the same system prompt components that mirror our ChatGPT prompt:\n\nSURROUND = \"\"\"... omitted for brevity ...\"\"\"\n\nSINGLE_TASK = \"... omitted for brevity...\"\n\nHEADERS = \"\"\"...omitted for brevity\"\"\"\n\nWe will add a get_user_prompt function that merges the HEADERS prompt piece with the source code of the function:\n\ndef get_user_prompt(func: callable) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nHEADERS: {{{{{{ {POINTS} }}}}}}\n\nEXPLANATION:\n\n\"\"\"\n\nThen, we can call OpenAI API using the gpt-4o-mini model:\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(get_manhattan_distance)\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nChapter 7\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n],\n\n)\n\nprint(\"Explanation:\", completion.choices[0].message.content)\n\nJust as with ChatGPT, we will receive a detailed explanation of the function, covering all points specified. For reference, here is a sample output from running this script:\n\nFunction's purpose: < ... omitted for brevity ... >\n\nArguments and their types: The function takes in two arguments:\n\ndf1: a pandas DataFrame, which represents the first set of data.\n\ndf2: a pandas DataFrame, which represents the second set of data. Both dataframes are expected to have the same shape and structure for the calculation to be valid. Step-by-step data flow: < ... omitted for brevity ... >\n\nPotential edge cases:\n\nIf df1 and df2 have different shapes, the function may raise a value error due to the inability to perform element-wise operations on differently-sized frames.\n\nIf either of the DataFrames is empty, the function will return a distance of 0.0 as there are no elements to compute differences from.\n\nHandling of NaN (Not a Number) values might need to be considered, as this could affect the calculations when present in either of the DataFrames.\n\nThe output here also provides an overview of the edge cases. In later chapters of the book, we will explore how to make use of this when designing a test suite for functions.\n\nNext, we will explore how to extend the use of the /explain command beyond .py files using various GenAI tools.\n\nExplaining non-project files The /explain command is not limited to .py files or core business logic. For instance, in our Manhattan distance code base, we can use Copilot to interpret an entire non-project file or a specific section of it.\n\n169\n\n170\n\nReading and Understanding Code Bases with GenAI\n\nExplaining an entire file There are times when we may want a summary of a complete non-project file. For example, con- sider the requirements.txt file in our project:\n\npandas==2.2.3\n\nflask==3.1.0\n\nTo get a general explanation, we can highlight the entire content with the cursor, then open the inline chat or chat window and enter /explain. Figure 7.7 shows an example of this using the chat window in VS Code:\n\nFigure 7.7: Chat window explanation of a full file highlighted content\n\nIn this case, Copilot accurately recognized that requirements.txt lists the dependencies needed for the code base to run properly. For common non-project files, we can generally expect reliable explanations since LLMs have encountered many similar examples during training.\n\nChapter 7\n\nNext, we will look at how to ask for a more detailed explanation of a specific line within a non-proj- ect file by providing the right context.\n\nExplaining with context While it is useful to understand a file as a whole, there are often specific lines that may be unclear and require a deeper explanation.\n\nTake the Dockerfile from our Manhattan distance project as an example. Docker is a widely used tool among software developers for packaging applications in a consistent and reproducible way. A key element in this process is the Dockerfile, a plain text file typically placed at the project root that outlines the environment setup. This includes specifying a base image (such as a Python environment), listing dependencies (like the pandas library), and providing commands to build and run the container (such as opening a port to receive HTTP POST requests).\n\nBelow is the Dockerfile we used for our Manhattan distance application:\n\nFROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"python\", \"app.py\"]\n\n171\n\n172\n\nReading and Understanding Code Bases with GenAI\n\nCopilot can provide an explanation for the entire file, but there are times when we want to focus on a specific line. For instance, we may want to understand the meaning of EXPOSE 5000. Although it might seem like this command publishes port 5000 for incoming requests, it serves as docu- mentation only. The application will still need to explicitly expose this port when launching the container, using a shell command such as docker run -p 5000:5000. When we highlight just this line and request an explanation, Copilot incorporates context from nearby lines, the filename and extension, and other signals to generate a relevant and accurate response, as shown in Figure 7.8:\n\nFigure 7.8: Copilot explanation about exposed port from the Dockerfile\n\nCopilot correctly identified that the port specification indicates the port on which the application is listening. For common files such as Dockerfile, the plain /explain command usually provides a sufficient one-line explanation.\n\nNext, we will explore how to get similar explanations in non-project files working with ChatGPT.\n\nExplaining non-project files with ChatGPT When seeking an explanation for a specific line within a non-project file or the entire file itself, the best practice is to provide the full content of the file along with the line in question and the standard filename and extension. In the case of a Dockerfile, there is no extension (such as .txt or .py), so simply specifying Dockerfile is sufficient.\n\nChapter 7\n\nFor instance, if we want to understand the role of an exposed port in a Dockerfile, we should include both the entire file contents and the specific line to get a more accurate explanation, as demonstrated in the following prompt:\n\nCONTEXT: you are provided with:\n\n1. A Dockerfile enclosed with {{{ DOCKERFILE }}}\n\n2. A line from the file enclosed with {{{ LINE }}}.\n\nTASK: Explain the purpose of the line.\n\nDOCKERFILE: {{{ ...omitted for brevity... }}}\n\nLINE: {{{EXPOSE 5000}}}\n\nEXPLANATION:\n\nThis prompt effectively identifies the function of the line in the Dockerfile, as shown in Figure 7.9:\n\nFigure 7.9: Expose 5000 line explanation in ChatGPT\n\nIn general, when using ChatGPT to explain a line of code or a line from a non-project file, it is helpful to include a broader context than just the target line. For example, when explaining a method within a class, include the entire class definition. If a function uses a custom argument type, include the definition of that type as well. This leads to more precise and useful explana- tions. Later in the chapter, we will introduce debugging strategies for complex scenarios where a simple textual explanation does not provide sufficient insight into the purpose of a code block.\n\nNext, we will explore how to use a similar approach when working with OpenAI API.\n\n173\n\n174\n\nReading and Understanding Code Bases with GenAI\n\nExplaining non-project files with OpenAI API Similar to ChatGPT, we can use OpenAI API to explain specific lines from non-project files. Let us revisit the Dockerfile example and apply a comparable system prompt:\n\nSURROUND = \"\"\"you are provided with:\n\n1. A Dockerfile enclosed with {{{ DOCKERFILE }}}\n\n2. A line from the file enclosed with {{{ LINE }}}.\"\"\"\n\nSINGLE_TASK = \"Your task is to explain the purpose of the line.\"\n\nUnlike earlier examples where we passed a function’s source code to OpenAI API, this time we want to send the entire Dockerfile content. We can do this by using Python’s built-in open function inside the get_user_prompt function:\n\ndef get_user_prompt(path: str, line: str) -> str:\n\nwith open(path, 'r') as file:\n\ndockerfile_content = file.read()\n\nreturn f\"\"\"\n\nDOCKERFILE: {{{{{{ {dockerfile_content} }}}}}}\n\nLINE: {{{{{{ {line} }}}}}}\n\nEXPLANATION:\n\n\"\"\"\n\nWe can then call OpenAI API using the following code:\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(\n\n'../../ch7/Dockerfile', 'EXPOSE 5000')\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\nChapter 7\n\n],\n\n)\n\nprint(\"Explanation:\", completion.choices[0].message.content)\n\nFor reference, running this script produced the expected result, identifying the line’s purpose as specifying the port to be exposed. Here is an example output:\n\nExplanation: The line `EXPOSE 5000` in the Dockerfile serves to inform Docker that the application running inside the container will be listening for incoming network connections on port 5000.\n\nWhen a container is created from this Docker image, port 5000 will be exposed for external communications, making it possible for users or other applications to connect to the service provided by the application running in the container. However, it is important to note that this command does not actually publish the port; to make the port accessible from outside the Docker environment, the user must explicitly map it to a port on the host machine when running the container, typically using the `-p` option with the `docker run` command.\n\nIn summary, `EXPOSE 5000` is a documentation feature that indicates the intended service port and enhances the understandability of the Docker container's purpose.\n\nTo conclude, both ChatGPT and OpenAI API can deliver accurate results even when using smaller models such as GPT-4o, if we supply the necessary context, including the filename and extension if it exists, its contents, and the specific line we want explained.\n\nGitHub Copilot is best suited for quick, informal explanations. For more control over the prompt and instructions, ChatGPT is a better fit. When a programmatic solution is required, OpenAI API becomes the preferred choice.\n\nNext, we will explore how to harness the power of GenAI to handle more advanced cases where a simple explanation is not enough, and interaction with the object is required for deeper un- derstanding.\n\nCreating calls with example parameters As our Python objects or non-project files become more complex, a straightforward explanation might not be enough to help us understand the code in front of us. This is especially true when the Python object is part of a class, lacks type hints in its signature, contains no docstring, and strays from best practices.\n\n175\n\n176\n\nReading and Understanding Code Bases with GenAI\n\nTake, for instance, the private method _constructor_from_mgr in the widely used pandas DataFrame class. This method constructs a DataFrame from a set of data blocks and their as- sociated axes. For your reference, here is the source code, which lacks argument type hints, a docstring, or any reference to tests:\n\ndef _constructor_from_mgr(self, mgr, axes) -> DataFrame:\n\ndf = DataFrame._from_mgr(mgr, axes=axes)\n\nif type(self) is DataFrame:\n\n# This would also work `if self._constructor is DataFrame`, but\n\n# this check is slightly faster, benefiting the most-common case.\n\nreturn df\n\nelif type(self).__name__ == \"GeoDataFrame\":\n\n# Shim until geopandas can override their _constructor_from_mgr\n\n# bc they have different behavior for Managers than for DataFrames\n\nreturn self._constructor(mgr)\n\n# We assume that the subclass __init__ knows how to handle a\n\n# pd.DataFrame object.\n\nreturn self._constructor(df)\n\nOne issue here is the naming of the first argument. Although it is called manager, a name typically associated with the _mgr property of a DataFrame, it refers to a block of data, which is essentially the cell content without axis labels. This misleading name, combined with the absence of type hints, a docstring, or proper testing, makes the code block hard to understand for both humans and GenAI applications.\n\nFor reference, here is the explanation output from Copilot when prompting /explain from a PyCharm inline chat:\n\nThe _constructor_from_mgr method is a helper function in the DataFrame class. It is used to create a new DataFrame instance from a given BlockManager (or ArrayManager) and its associated axes.\n\nKey Points:\n\n1. Parameters:\n\nChapter 7\n\nmgr: The BlockManager or ArrayManager that holds the actual data for the DataFrame.\n\naxes: The axes (index and columns) for the new DataFrame.\n\n2. Return Value:\n\nIt returns a new DataFrame object constructed from the provided manager and axes.\n\nExample Snippet:\n\ndef _constructor_from_mgr(self, mgr, axes) -> DataFrame:\n\ndf = DataFrame._from_mgr(mgr, axes=axes)\n\nHere, _from_mgr is a class method that initializes a DataFrame from the given manager and axes. This method ensures that the new DataFrame is properly aligned with the provided data structure.\n\nThis output includes all the elements we are looking for, such as arguments, the return value, and some level of data transformation. However, deciphering the explanation can be just as challenging as deciphering the code itself. We propose a better approach by debugging the code using the generated content.\n\nDebugging with GitHub Copilot Copilot is built for code generation, which makes it useful for interacting with our Python code. If we take the get_manhattan_distance function as an example, prompting Copilot with a partial call to the function can result in a completion that includes example arguments.\n\nWe can do this by writing the function call directly:\n\nget_manhattan_distance(\n\nOr, we can do it by assigning the result to a variable with a matching type hint for the function’s return value:\n\nd: float = get_manhattan_distance(\n\n177\n\n178\n\nReading and Understanding Code Bases with GenAI\n\nIn both cases, Copilot is expected to generate two DataFrame arguments for the function. An example of this kind of completion in PyCharm is shown in Figure 7.10:\n\nFigure 7.10: Sample call completed by GitHub Copilot\n\nKeep in mind that if your IDE automatically inserts a closing bracket, ), after you type the opening one, (, you may need to press Enter to activate Copilot’s suggestion.\n\nThis generated completion can serve as an entry point for interacting with the function through the debugger. In PyCharm, we can initiate debug mode by right-clicking the filename and selecting Debug, as illustrated in Figure 7.11:\n\nFigure 7.11: Debug mode in PyCharm\n\nIn VS Code, debugging is done by clicking the play icon in the top-right corner and selecting Python Debugger: Debug Python File, as shown in Figure 7.12:\n\nChapter 7\n\nFigure 7.12: Running the Debugger in VS Code\n\nRunning the file in debug mode allows us to step through the code line by line, offering a clearer under- standing of its internal workings. For example, when using PyCharm’s debugger, we can inspect the variables within the function and follow the data transformation as it happens, as seen in Figure 7.13:\n\nFigure 7.13: Debug mode in PyCharm showing internal function arguments\n\nThis gives us visibility into the intermediate values within get_manhattan_distance.\n\nThe authors frequently rely on debugging to interact with the code they read and write. This hands-on approach supports faster development of intended functionality and offers deeper insights into complex code logic. This far exceeds reading source code, using GenAI explanations, or consulting docstrings.\n\nNext, we will explore how generating sample calls can help us better understand complex code blocks, such as the _constructor_from_mgr method in the pandas DataFrame class.\n\n179\n\n180\n\nReading and Understanding Code Bases with GenAI\n\nDebugging dependency code Just like with the get_manhattan_distance case, we can use Copilot to help generate sample code for debugging the _constructor_from_mgr method. Since this is a class method, we will split our prompt into two parts. The first prompt generates a sample instance of the class, and the second invokes the method.\n\nWe will begin by asking Copilot to complete a DataFrame instance. A sample prompt may look like this:\n\ncoordinates_a : pd.DataFrame =\n\nThis prompt offers sufficient context for Copilot to suggest an assignment. As shown in Figure 7.14, Copilot begins by proposing an initial completion for a pd.DataFrame.\n\nFigure 7.14: Initial completion in PyCharm\n\nWhen we accept the suggestion, Copilot typically proposes sample data that fits the DataFrame’s name, as illustrated in Figure 7.15:\n\nFigure 7.15: Chained completion in PyCharm\n\nAt this point, we can either trigger additional columns by accepting chained suggestions or type } to flag to Copilot that we do not want any additional columns. We then move on to the method call with the following prompt:\n\ncoordinates_a._constructor_from_mgr(\n\nNote that your IDE may automatically complete the closing bracket, ). In that case, press Enter between the brackets to let Copilot generate a completion suggestion.\n\nChapter 7\n\nFor example, in Figure 7.16, Copilot proposes using the coordinates_a._mgr and coordinates_a. axes properties as the calling parameters.\n\nFigure 7.16: Parameter completion\n\nWith the two proposed parameters, we can debug the method as shown in Figure 7.17 and gain a better understanding of its behavior.\n\nFigure 7.17: Debugging _constructor_from_mgr using Copilot’s suggestion\n\nTo conclude, Copilot can supercharge the process of debugging sections of the code base, allowing us to better understand both simple and complex implementations. We rely heavily on debugging as a daily practice to deepen their understanding of the code they interact with.\n\nNext, we will explore how ChatGPT and OpenAI API offer distinct characteristics that we can utilize.\n\n181\n\n182\n\nReading and Understanding Code Bases with GenAI\n\nCustomizing call parameters with ChatGPT and OpenAI GitHub Copilot is generally better suited for quickly generating debugging calls to trigger Python objects, as it can access relevant context, such as the class code, when we are working with a method. For cases where we want specific characteristics of the sample parameters, prompting ChatGPT and OpenAI may be more suitable.\n\nFor example, to generate suitable arguments for debugging the get_manhattan_distance function, we might want five rows and three columns, and can use the following prompt:\n\nCONTEXT: you are provided with:\n\n1. A Python function enclosed with {{{ FUNCTION }}}\n\n2. Arguments requirements enclosed with {{ REQUIREMENTS }}}.\n\nTASK: Call the function with the data requirements\n\nFUNCTION: {{{ ... omitted for brevity ... }}}\n\nREQUIREMENTS: {{{\n\n1. 5 rows\n\n2. 3 columns\n\n3. floats and integers in cells\n\n}}}\n\nCALL:\n\nFor reference, the output is shown in Figure 7.18:\n\nFigure 7.18: Call parameters for get_manhattan_distance\n\nChapter 7\n\nIn this example, both DataFrames, df1 and df2, have the same shape of five rows and three col- umns, which makes them appropriate arguments for the function call.\n\nThe OpenAI API call is similar, and you can find the full script at ch7/prompts/openai_debug. py for later reference.\n\nTo conclude, we turn to ChatGPT and OpenAI API when our data requirements are beyond trivial for a quick completion that Copilot might otherwise handle well. Later in the book, we will explore how to build on these data generation methods to create a robust test suite.\n\nDebugging dependency code When using ChatGPT or OpenAI API to generate test code for a dependency package, it is import- ant to provide enough context for the model to produce a desirable call. The amount of context required depends on how widely the library is used. For standard packages such as pandas, in- cluding just the relevant method may be enough. For custom or private repository classes, it is usually best to include the entire class definition.\n\nFor example, the DataFrame class in pandas is about 12,000 lines long, which is too much to include in a prompt. However, because pandas is highly popular, we can provide only the _constructor_ from_mgr method and still receive a relevant response.\n\nBelow is a sample prompt used with ChatGPT to generate a call to the _constructor_from_mgr method:\n\nCONTEXT: Given Pandas DataFrame 2.2.3 class, you are provided with:\n\n1. Pandas DataFrame method to be called enclosed with {{{ FUNCTION }}}\n\n2. Arguments requirements enclosed with {{ REQUIREMENTS }}}.\n\nTASK: Generate a method call with sample parameters that meet the data requirements.\n\nFUNCTION: {{{ ... omitted for brevity ... }}}\n\nREQUIREMENTS: {{{\n\n1. 5 rows\n\n2. 3 columns\n\n3. floats and integers in cells\n\n}}}\n\nCALL:\n\n183\n\n184\n\nReading and Understanding Code Bases with GenAI\n\nFigure 7.19 shows an example output from GPT-4o in response to this prompt.\n\nFigure 7.19: Sample test call to _constructor_from_mgr\n\nHere, ChatGPT successfully generated a method call using df._mgr and df.axes, producing a DataFrame with the required dimensions.\n\nSince OpenAI API follows a similar pattern, we have provided the full script in ch7/prompts/ openai_debug_pandas.py for reference.\n\nIn later chapters, we will build on this approach to supercharge the generation of robust test suites.\n\nPutting it all together Now that we have reviewed different approaches to shorter objects, explored tailored\n\nexplanations for Python objects and non-project files, considered programmatic concerns,\n\nand looked at scenarios where a textual explanation is not sufficient, we can summarize\n\neverything in a pros and cons table for each of the tools:\n\nChapter 7\n\nGenAI app\n\nPros\n\nCons\n\nGitHub Copilot\n\nProvides quick responses with relevant context. It supports the use of /explain as is and allows us to add additional requirements.\n\nDoes not support tailored explanations, such as when we need to include edge cases for get_manhattan_distance.\n\nChatGPT\n\nEnables tailored explanations and gives us flexibility to add or remove context.\n\nRequires more effort to craft prompts.\n\nOpenAI API\n\nSuitable for programmatic solutions and automation.\n\nSignificantly more effort compared to GitHub Copilot and ChatGPT.\n\nIn our own work, we typically begin with Copilot when we need an explanation. As our require- ments expand, we move to Copilot’s chat window, ChatGPT, or OpenAI API. When dealing with more complex objects, we always recommend incorporating debugging into the reading process to better understand the code.\n\nSummary In this chapter, we expanded our focus beyond .py files and explored how GenAI can help us un- derstand entire code bases. Using GitHub Copilot, we applied the /explain command in its plain form as well as with more tailored instructions to grasp the role of the get_manhattan_distance function. We also applied this command to understand non-project files such as requirements. txt and Dockerfile.\n\nNext, we used ChatGPT to craft prompts that offered useful explanations about edge cases and argument types. We then moved on to OpenAI API to generate a script that programmatically provided multiple explanations with one call.\n\nWe concluded by generating sample values that are useful for interacting with our code through the debugger. This approach is especially helpful when dealing with more complex implemen- tations.\n\nIn the next chapter, we will dive into prompt engineering techniques to improve the quality of our generated code. Through few-shot learning, also known as in-context learning, we will guide Copilot, ChatGPT, and OpenAI API to follow a specific style guide aligned with our production standards. We will also explore additional techniques to shape prompts for better outcomes.\n\n185\n\n186\n\nReading and Understanding Code Bases with GenAI\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: How can we address explanations of complex Python objects?\n\nAnswer: In cases where the object is relatively simple, we can utilize the GitHub Copilot /explain slash command to help us better grasp the arguments, data transformation, and return values. For more complex objects, we can use debugging by generating calls to the object with GitHub Copilot, ChatGPT, and OpenAI API.\n\nQuestion 2: When asking ChatGPT to explain a single line in a Dockerfile, what context should be added?\n\nAnswer: It is recommended to include the full contents of the Dockerfile along with the filename and extension, and the specific line in question. This gives ChatGPT the context needed for a more accurate explanation.\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\nDockerfile, Wikipedia: https://realpython.com/primer-on-python-decorators/\n\n\n\nPyCharm debugging: https://www.jetbrains.com/help/pycharm/part-1-debugging- python-code.html#step\n\n\n\nVS Code debugging: https://code.visualstudio.com/docs/python/debugging\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n8\n\nAn Introduction to Prompt Engineering\n\nIn this chapter, we will introduce the practice of prompt engineering, a field of study that started in 2020, which offers advanced techniques for refining prompts to achieve more reliable, predict- able, and desirable outcomes in large language model (LLM)-based applications.\n\nWe will focus on an effective prompt engineering technique for code generation with GenAI: few-shot learning, also known as in-context learning. This approach, when used in the context of code-related tasks, guides the model toward producing outputs that align with a specific coding style or objective.\n\nWe will focus on how the few-shot learning technique can enhance the model’s output for more accurate and style-consistent code generation. While there are many different uses of few-shot learning in code-related tasks, the focus on style is a useful example for maintaining consisten- cy in a repository. This includes enforcing type hints, adding docstrings, or following a specific logging convention. By aligning the generated code with the required style guide, we can reduce the need for manual customization and accept the GenAI output as-is.\n\nIn this chapter, we will cover the following topics:\n\nUtilizing prompt engineering for coding\n\n\n\nEnhancing prompts with few-shot examples\n\n\n\nLeveraging few-shot learning with ChatGPT\n\n188\n\nAn Introduction to Prompt Engineering\n\nWorking with few-shot learning for OpenAI API\n\n\n\nCrafting a style guide for GitHub Copilot\n\n\n\nIntroducing more prompt engineering techniques\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, which includes the prompt samples and style guides fea- tured in this chapter, available at https://github.com/PacktPublishing/Supercharged-\n\nCoding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nUtilizing prompt engineering for coding In Chapter 4, we explored the three pillars of achieving quality output: model mastery, evaluation metrics, and precise prompts. We also discussed how following the five S’s best practices for prompts (structured, surrounded, single-tasked, specific, and short) can significantly enhance the quality of model output. Using OpenAI’s example of an effective prompt, we demonstrated how aligning with these principles, such as focusing exclusively on error fixes and providing a clear list of issues to address, could improve results.\n\nAs tasks grow more complex, advanced techniques are essential to guide models toward achieving desired outcomes. LLMs may need additional instructions to adhere to a specific style guide, pass a unit test suite, or fix reproducibility issues.\n\nChapter 8\n\nSince the advent of LLMs in 2020, prompt engineering has developed into a practice that refines and structures prompts to achieve better results and address more complex scenarios. There are now many strategies that guide the model toward more desirable outcomes, including the following:\n\n\n\nStructuring prompts in a way that aligns with LLM capabilities\n\n\n\nProviding additional context to clarify the problem\n\n\n\nIncluding specific guidelines for the desired output\n\nGuiding the model through a reasoning process\n\nDefining constraints and boundaries for the expected output\n\n\n\nProviding external feedback to iteratively refine results\n\nIn coding-related tasks, prompt engineering is valuable for a variety of applications including creating style guides for code refactoring, breaking down complex challenges into manageable steps, and generating production-standard code. We will explore in depth a range of prompt engineering techniques designed to supercharge various stages of the software development life cycle (SDLC) throughout this book.\n\nNext, we will take an in-depth look at the few-shot learning technique, which incorporates spe- cific guidelines for desired outputs, and examine its impact on shaping the coding style of the model’s suggestions. Later in the chapter, we will preview other prompt engineering techniques that will be explored in detail in later chapters.\n\nEnhancing prompts with few-shot examples Few-shot learning, also known as in-context learning, is a prompt engineering technique where we include a few examples of desirable outputs directly within the prompt. This method can be extended to incorporate examples of incorrect output or edge cases, enabling the model to differ- entiate between what constitutes a good result and what does not. By analyzing these patterns, the model can infer how to structure an optimal response for the given task.\n\nTo delve deeper into the research behind this approach, refer to Language Models are Few-Shot Learners by Brown et al. (https://arxiv.org/abs/2005.14165v4).\n\n189\n\n190\n\nAn Introduction to Prompt Engineering\n\nFew-shot learning research demonstrates how, with a few examples (or shots) of input-output pairs, a model can infer patterns from the relationships between these examples and complete tasks in a style consistent with the provided samples. For instance, the following prompt, taken from the study, illustrates the predictability and consistency of an LLM when solving arithmetic problems. Utilizing few-shot prompting increases the predictability of the output, as the model is likely to generate responses that align with the style of the provided few-shot examples as shown here:\n\nInstruction: Solve the following arithmetic problems.\n\nExamples:\n\nProblem: What is 5 + 3?\n\nAnswer: 8\n\nProblem: What is 12 - 4?\n\nAnswer: 8\n\nProblem: What is 7 × 6?\n\nAnswer: 42\n\nProblem: What is 15 ÷ 3?\n\nAnswer:\n\nThe model’s output was as follows:\n\n5\n\nCompared to prompts that only specify the task (Solve the following arithmetic problem), this example-based approach reduces ambiguity. The model aligns its output more closely to the expected structure, providing only the number as the answer, without additional explanation or formatting inconsistencies.\n\nFew-shot prompting is a useful technique for a variety of coding-related tasks, such as code re- factoring, code analysis, and code generation. By including specific examples in the prompt, we can guide the model to produce outputs that align with our desired patterns.\n\nFew-shot learning for code refactoring In a code refactoring scenario, we may want to transform print statements into structured logger messages, a common practice for maintaining production-level software. Replacing the variety of print statements across many files is both error-prone and tedious. However, we can use an LLM with a prompt with clear instructions and a few examples. For example, we can enrich a request to refactor the following print message:\n\nprint('Error! File not found: passwords.txt')\n\nChapter 8\n\nWe can refactor it to a log message, with a few examples:\n\nOld: print('Process started for config.txt')\n\nRefactored: logger.info('Processing started', extra={'stage': 'start', file: 'config.txt'})\n\nOld: print('Warning! Could not load user data from user_info.csv')\n\nRefactored: logger.warning('User data failed to load', extra={'module': 'user_loader', 'status': 'failure', file: 'user_info.csv'})\n\nOld: print('Error! File not found: passwords.txt')\n\nRefactored:\n\nQuick tip: Enhance your coding experience with the AI Code Explainer and Quick Copy features. Open this book in the next-gen Packt Reader. Click the Copy button\n\n(1) to quickly copy code into your coding environment, or click the Explain button\n\n(2) to get the AI assistant to explain a block of code to you.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR visit packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\n191\n\n192\n\nAn Introduction to Prompt Engineering\n\nThe examples in this prompt highlight the distinctions between different logging levels with ex- amples of transforming print messages into info and warning logging messages. The examples also demonstrate static log messages with additional information provided through the extra parameter.\n\nNext, we will see how enhancing our prompt with few-shot examples improves the output we get from ChatGPT.\n\nLeveraging few-shot prompting with ChatGPT When using ChatGPT, if applying the five S’s framework from Chapter 4 does not produce the desired results, we can enhance our prompts with a few-shot approach. In this case, we will still follow the five S’s framework but incorporate indexed variables that represent a series of inputs and outputs for the task.\n\nFew-shot structure A typical approach to implement few-shot prompting in ChatGPT is by extending the five S’s framework to include indexed variables such as {{{ Input_i }}} and {{{ Output_i }}}. This implies that the context references these variables, for example:\n\nCONTEXT: You are provided with:\n\n1. Python snippet enclosed with {{{ OLD }}}\n\n2. Examples enclosed with {{{ OLD_i }}} followed by a desirable output enclosed with {{{ REFACTORED_i }}}\n\nWe then supply the OLD_i and REFACTORED_i examples, followed by {{{ OLD }}}, as such:\n\nOLD_1: {{{ }}}\n\nREFACTORED_1: {{{ }}}\n\nOLD_2: {{{ }}}\n\nREFACTORED_2: {{{ }}}\n\n...\n\nOLD: {{{ }}}\n\nOverall, a ChatGPT prompt that uses few-shot examples can be structured in this way:\n\nCONTEXT: You are provided with:\n\n1. ...\n\n2. Examples enclosed with {{{ Input_i }}} followed by a desirable output\n\nChapter 8\n\nenclosed with {{{ Output_i }}}\n\nTASK: ...\n\nInput_1: {{{ ... }}}\n\nOutput_1: {{{ ... }}}\n\nInput_2: {{{ ... }}}\n\nOutput_2: {{{ ... }}}\n\n...\n\nInput: {{{ }}}\n\nCUE:\n\nThe labels for inputs and outputs should reflect their relationship and remain consistent across examples. For example, when solving arithmetic problems, question and answer are appropriate labels. When transforming print statements to log messages or performing other refactoring tasks, old and refactored make more sense.\n\nImprovement in output with code refactoring Let us examine a case where we want to refactor print statements into structured logging. This in- volves using appropriate logging levels, enforcing static messages, and including extra parameters.\n\nTo observe the default behavior without applying a few-shot approach, we can navigate to https://chatgpt.com/, start a new conversation with the GPT-o3-mini-high model, and prompt it to refactor a print statement into a log message using the following:\n\nCONTEXT: You are provided with a Python snippet enclosed with {{{ OLD }}}.\n\nTASK: Refactor the snippet to a log message.\n\nOLD: {{{ print('Error! File not found: passwords.txt') }}}\n\nREFACTORED:\n\nThe GPT-o3-mini-high model excels at coding tasks and is likely to generate a log message with an error level. However, without explicit instructions on structuring the log message, it may not use a logger instance nor distinguish the static log message from additional details such as the filename, as shown in Figure 8.1:\n\n193\n\n194\n\nAn Introduction to Prompt Engineering\n\nFigure 8.1: Working with Canvas in ChatGPT\n\nWe can improve the model’s output by providing few-shot examples demonstrating how to ex- tract key details from a print statement and use a logger instance. For example, we can refactor a print message to include a filename and verbosity setting:\n\nprint('Process started for config.txt with verbose=True')\n\nWe can refactor it into a structured log message:\n\nlogger.info('Processing started', extra={'verbose': True, file: 'config. txt'})\n\nThis example highlights the extraction of essential details to improve logging clarity. Similarly, we can add another example with print statements containing user and file path information:\n\nprint('Warning! Could not load user U-232 data from user_info.csv')\n\nWe can refactor it into the following structured log message:\n\nlogger.warning('User data failed to load', extra={'user': 'U-232', 'file': 'user_info.csv'})\n\nA complete prompt with these few-shot examples might be structured as follows:\n\nCONTEXT: You are provided with:\n\n1. Python snippet enclosed with {{{ OLD }}}\n\n2. Examples enclosed with {{{ OLD_i }}} followed by a desirable output enclosed with {{{ REFACTORED_i }}}\n\nChapter 8\n\nTASK: Refactor the snippet to a log message.\n\nOLD_1: {{{ print('Process started for config.txt with verbose=True') }}}\n\nREFACTORED_1: {{{ logger.info('Processing started', extra={'verbose': True, file: 'config.txt'}) }}}\n\nOLD_2: {{{ print('Warning! Could not load user U-232 data from user_info. csv') }}}\n\nREFACTORED_2: {{{ logger.warning('User data failed to load', extra={'user': 'U-232', 'file': 'user_info.csv'}) }}}\n\nOLD: {{{ print('Error! File not found: passwords.txt') }}}\n\nREFACTORED:\n\nProviding these two examples as guidance for the model will capture the desired output, as illustrated in Figure 8.2:\n\nFigure 8.2: Improved logging suggestion\n\nThis time, the model successfully extracted the parameters from the static log message and used the logger instance instead of a direct logging message, following the patterns demonstrated in the few-shot examples. We will revisit logging practices in greater detail in Chapter 15, so stay tuned.\n\nNext, we will explore how adopting a Python-formatted style guide with Copilot can go beyond traditional linting and code formatting tools. This few-shot approach is proactive, adaptable, and context-aware, enabling Copilot to predict our intended implementation easily.\n\n195\n\n196\n\nAn Introduction to Prompt Engineering\n\nCrafting a style guide for GitHub Copilot GitHub Copilot functions as a personalized AI assistant, predicting our next coding moves. Be- cause LLMs excel at imitating style, Copilot quickly adapts to any style preferences it observes during a session.\n\nFor example, suppose we have a file named ch8/code_samples/math_calculations.py with two functions (get_area and get_arithmetic_mean), both implemented without type hints and with single-line function signatures:\n\nimport numpy as np\n\ndef get_area(radius):\n\nreturn np.pi * radius ** 2\n\ndef get_arithmetic_mean(x1, x2):\n\nreturn (x1 + x2) / 2\n\nIf we then start typing a new function signature for get_euclidean_distance, Copilot is likely to generate an implementation in the same style, with no type hints and one-line signatures, as shown in Figure 8.3:\n\nFigure 8.3: Copilot preserving our coding style\n\nAlternatively, if our code implements type hints, hanging indents, and docstrings, Copilot will generate suggestions that reflect this updated style. For example, here is an alternate implemen- tation of get_area and get_arithmetic_mean:\n\nimport numpy as np\n\ndef get_area(\n\nradius: float,\n\nChapter 8\n\n) -> float:\n\n\"\"\"\n\nCompute the area of a circle given its radius.\n\n\"\"\"\n\narea: float = np.pi * radius ** 2\n\nreturn area\n\ndef get_arithmetic_mean(\n\nx1: float,\n\nx2: float,\n\n) -> float:\n\n\"\"\"\n\nCompute the arithmetic mean of two numbers.\n\n\"\"\"\n\narithmetic_mean: float = (x1 + x2) / 2\n\nreturn arithmetic_mean\n\nBased on this coding preference, Figure 8.4 shows the suggested implementation of get_euclidean_ distance reflecting the new coding style:\n\nFigure 8.4: Copilot’s adapted coding style\n\n197\n\n198\n\nAn Introduction to Prompt Engineering\n\nAs developers, we would like a more robust method for informing Copilot of our preferred style guidelines without having to repeat them every session. We can achieve this by using a few-shot learning approach: creating a dedicated style guide file, as we will see next.\n\nStyle guide file A simple way to include our preferred style guide in Copilot is to maintain a single file outside the main project folder that we can copy and paste in place when we start a new Copilot session. Copying and pasting this file’s content makes our coding style part of the active session, although we hope that future versions of Copilot will introduce a more convenient method. For example, a sample file located at ch8/code_samples/style_guide.py, shown in Figure 8.5, includes two functions: get_area and get_arithmetic_mean.\n\nFigure 8.5: Style guide\n\nThis file does not include a docstring, as adding one is considered a separate task from imple- menting the function. We will explore this topic in more detail in later chapters.\n\nNext, if we open a new file at ch8/code_samples/distances.py and begin typing the signature of get_euclidean_distance, Copilot is likely to generate an implementation that aligns with that formatting, as shown in Figure 8.6:\n\nChapter 8\n\nFigure 8.6: Preserved coding style\n\nOur coding preferences remain consistent in the new file, showing that Copilot has effectively captured our desired style. In later chapters, we will explore how adding more Python elements, such as decorators, to the style guide can further refine and expand our preferred coding approach.\n\nStyle guide as a next-generation code formatter\n\nIf you are used to code formatters or linters, you will notice that implementing a style guide may remind you of a code formatting initiative, but is more advanced. First, because the style guide approach is proactive rather than reactive, we get suggestions already formatted the way we want. Second, it offers more flexibility: we can adopt the show, don’t tell method by simply demonstrating our preferred style to Copilot. Finally, we can go beyond traditional formatters by including preferences such as type hints, which are not always clear using older methods.\n\nNext, we will explore how to apply few-shot learning with OpenAI, using this technique to re- factor code at scale.\n\nWorking with few-shot learning for OpenAI API For large-scale tasks, such as generating code implementations from function signatures, we can use OpenAI API programmatically and apply few-shot learning to illustrate the desired outcome effectively. To do this, we provide indexed examples of input and output and reference these examples in the system prompt.\n\n199\n\n200\n\nAn Introduction to Prompt Engineering\n\nThe system prompt should reference the indexed input and output examples as follows:\n\nSURROUND = \"\"\"You are provided with:\n\n1. A [...] enclosed with {{{ INPUT }}}.\n\n2. Example [...] enclosed with {{{ INPUT_i }}} followed by the corresponding [...] enclosed with {{{ OUTPUT_i }}}.\"\"\" SINGLE_TASK = \"Your task is to [...].\"\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nNext, we update the get_user_prompt function to include the provided examples, ensuring that the labels align with those in the system prompt:\n\ndef get_user_prompt(some_input: str, few_shots: list) -> str:\n\nprompt = \"\"\n\nfor i, (input_, output_) in enumerate(few_shots):\n\nprompt += f\"\"\"\n\nINPUT_{i + 1}: {{{{{{ {input_} }}}}}}\n\nOUTPUT_{i + 1}: {{{{{{ {output_} }}}}}}\"\"\"\n\nprompt += f\"\"\"\n\nINPUT: {{{{{{ {some_input} }}}}}}\n\nOUTPUT:\n\n\"\"\"\n\nreturn prompt\n\nSimilarly, we replace INPUT_i and OUTPUT_i with task-specific labels, such as FUNCTION_i and CODE_i for code completion tasks.\n\nImplementing code with OpenAI API In Chapter 2, we built a code completion program that generates a Python implementation based on a function signature. For example, when implementing print_fibonacci_sequence, we pro- vided a prompt that requested only the function’s implementation, without additional explana- tions or output samples.\n\nA more effective approach is to include a few examples that demonstrate the desired output format. For instance, we can provide implementations for get_area and get_arithmetic_mean, both based solely on their function signatures:\n\nINPUT_1 = \"\"\"def get_area(radius: float) -> float:\"\"\"\n\nChapter 8\n\nOUTPUT_1 = \"\"\"def get_area(radius: float) -> float:\n\narea: float = np.pi * radius ** 2\n\nreturn area\"\"\"\n\nINPUT_2 = \"\"\"def get_arithmetic_mean(x1: float, x2: float) -> float:\"\"\"\n\nOUTPUT_2 = \"\"\"def get_arithmetic_mean(x1: float, x2: float) -> float:\n\narithmetic_mean: float = (x1 + x2) / 2\n\nreturn arithmetic_mean\"\"\"\n\nFEW_SHOTS = [\n\n(INPUT_1, OUTPUT_1),\n\n(INPUT_2, OUTPUT_2),\n\n]\n\nWith these examples, we can construct a system prompt and user prompt that encourage the model to generate only the function implementation without additional explanations. The system prompt would be structured as follows:\n\nSURROUND = \"\"\"You are provided with:\n\n1. A Python function signature enclosed with {{{ FUNCTION }}}.\n\n2. Example signatures enclosed with {{{ FUNCTION_i }}} followed by the corresponding implementation enclosed with {{{ CODE_i }}}. \"\"\"\n\nSINGLE_TASK = \"Your task is to implement the function.\"\n\nNext, we define a function to generate the user prompt, ensuring consistency with the system prompt’s structure:\n\ndef get_user_prompt(signature: str, few_shots: list) -> str:\n\nprompt = \"\"\n\nfor i, (input_, output_) in enumerate(few_shots):\n\nprompt += f\"\"\"\n\nFUNCTION_{i + 1}: {{{{{{ {input_} }}}}}}\n\nCODE_{i + 1}: {{{{{{ {output_} }}}}}}\"\"\"\n\nprompt += f\"\"\"\n\nFUNCTION: {{{{{{ {signature} }}}}}}\n\n201\n\n202\n\nAn Introduction to Prompt Engineering\n\nCODE:\n\n\"\"\"\n\nreturn prompt\n\nFor the implementation of print_fibonacci_sequence, we define the system and user prompts as follows:\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(\"def print_fibonacci_sequence(n: int) -> None:\", FEW_SHOTS)\n\nFinally, we call the API with the following snippet:\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n],\n\n))\n\nprint(completion.choices[0].message.content)\n\nBy structuring the prompt this way, the model is more likely to follow the expected output format. Running the preceding example, which is available in ch8/code_samples/openai_fibonacci.py, we got the following function implementation:\n\ndef print_fibonacci_sequence(n: int) -> None:\n\na, b = 0, 1\n\nfor _ in range(n):\n\nprint(a, end=' ')\n\na, b = b, a + b\n\nprint() # To add a newline after printing the sequence\n\nThis implementation aligns with our provided examples and does not include surrounding ex- planations.\n\nChapter 8\n\nFine-tuning as a better approach to OpenAI API\n\nWhen scaling projects, fine-tuning offers a more effective way to specialize a model for a specific task, such as generating code in a preferred style. This process involves training a pre-trained model such as GPT-4o-mini on a set of high-quality examples, allowing it to adjust its internal settings to better align with our preferences. We will explore this topic in greater detail in Chapter 11.\n\nHaving explored few-shot learning with ChatGPT, GitHub Copilot, and OpenAI API, we will now introduce additional prompt engineering techniques, which we will examine in greater detail in the upcoming and later chapters of the book.\n\nIntroducing more prompt engineering techniques Variations of few-shot prompting can include edge cases, progressively complex examples, or contrastive examples that highlight both correct and incorrect outcomes. These variations on few-shot prompting enhance a model’s ability to generalize across diverse task requirements. The examples in this chapter on type hinting and style formatting are just the tip of the iceberg of how prompt engineering can be used.\n\nIn other scenarios of the SDLC, we will see how using different prompt engineering techniques can become handy.\n\nBug fix at scale with prompt engineering Refer to the OpenAI example of effective prompts for fixing buggy code, enhanced with the re- finements discussed in Chapter 4:\n\nCONTEXT: You will be provided with Python code in the Python editor with description enclosed with {{{ DESCRIPTION }}}\n\nthat may contain errors enclosed with {{{ ERRORS }}}.\n\nTASK: Fix the errors in the code.\n\nDESCRIPTION: {{{ Execute 10 multiplication quizzes for the user and validate the answer }}}\n\n203\n\n204\n\nAn Introduction to Prompt Engineering\n\nERRORS: {{{\n\n1. Compilation\n\n2. Reproducibility of random number generation\n\n3. Logical scoping of variable assignments\n\n4. Error handling for input casting\n\n}}}\n\nWhile this prompt successfully identified all errors in the code, we cannot guarantee that GenAI applications will consistently capture all errors in different code snippets.\n\nTo improve results, we can explore techniques such as iterative prompting, where the output is verified for compilation and refined through model reiteration if needed. Template-based prompting can enforce a specific format for error descriptions, ensuring consistency and clarity in the output. Additionally, chain-of-thought prompting helps guide the model through a logical reasoning process, improving its ability to understand and fix code issues. Let’s explore these in the next subsections.\n\nIterative prompting Iterative prompting involves engaging the model repeatedly within a feedback loop that refines its output based on evaluations of previous responses. This feedback can be manual, such as asking ChatGPT to revise its response to address unidentified errors, or automated, such as compiling code generated by OpenAI API or running it against a suite of unit tests. The resulting feedback, whether a traceback error message or failed test results, can be reintroduced to the model, con- tinuing the cycle until specific criteria are satisfied.\n\nFor instance, if the goal is to address compilation issues, an iterative mechanism could involve attempting to compile the output and feeding the traceback error messages back to the model for refinement if the code fails to compile:\n\nfor i in range(10):\n\ntry:\n\nexec(user_code)\n\nprint(f\"successfully compiled:\\n {user_code}\")\n\nbreak\n\nexcept SyntaxError as se:\n\nuser_code = get_refined_output(user_code, traceback.format_exc())\n\nChapter 8\n\nHere, we will repeatedly prompt the model with traceback messages from the suggested code until it successfully compiles. Alternatively, we could evaluate the code by running it against a test suite or using other evaluation mechanisms, which we will explore in later chapters of the book.\n\nTemplate-based prompting Template-based prompting involves providing specific guidelines for structuring the desired output. For instance, when fixing code, we may want to understand the type of error being ad- dressed, such as whether it is a compilation error, a logical issue, or something else. In such cases, the output could follow this structure:\n\nThis code requires fix because of { ERROR TYPE}. The fixed code is { CODE }.\n\nTo guide the model toward generating this output, we include hints about the template within the context and provide a structure like this:\n\nCONTEXT: You will be provided with Python code enclosed with {{{ CODE }}} that does not compile, and an error traceback enclosed with {{{ TRACEBACK }}}, along with a template of the output enclosed with {{{ TEMPLATE }}}.\n\nTASK: Fix the errors in the code using the provided template.\n\nTEMPLATE: This code requires a fix because of { ERROR TYPE }. The fixed code is { CODE }.\n\nHere, we guide the model by providing a specific template that includes placeholders for the error type and the fixed code. This approach differs from few-shot prompting in how the output is constructed; rather than offering input-output examples, we define the structure of the output with variables for the model to populate. These two techniques can be used independently or in combination to achieve the desired results.\n\nChain-of-thought prompting Chain-of-thought (CoT) prompting involves guiding the model through a step-by-step reason- ing process toward a final answer. In coding, we apply CoT by referencing functions that are yet to be implemented, serving as a reasoning framework for the GenAI tool, which later fills in the implementation details. For instance, when validating that a code has no bugs, the process may involve the following steps:\n\n1. Validating syntax\n\n2. Checking compilation\n\n3. Ensuring reproducibility across runs\n\n205\n\n206\n\nAn Introduction to Prompt Engineering\n\nUsing CoT, we can explicitly reference functions that handle each of these steps while leaving the actual implementation for GenAI to complete. In code, this might look like this:\n\ndef validate_no_bugs_in_source_code(sc: str) -> None:\n\nvalidate_syntax(sc)\n\nvalidate_compilation(sc)\n\nvalidate_reoroducibility_across_runs(sc)\n\nHere, validate_no_bugs_in_source_code outlines the logical steps needed to verify that the source code is bug-free but does not contain implementation details. GenAI tools are particularly effective at generating low-level implementations, and when typing the first referenced function, validate_syntax, Copilot can generate a suitable implementation, as illustrated in Figure 8.7:\n\nFigure 8.7: GitHub Copilot suggested implementation of validate_syntax\n\nCoT is one of the most effective prompt engineering techniques for coding tasks, and we will explore its applications further in the upcoming chapters.\n\nExploring additional methods This book covers a selection of prompt engineering techniques among the many available. For a broader exploration of prompting strategies, Unlocking the Secrets of Prompt Engineering: Master the art of creative language generation to accelerate your journey from novice to pro by Packt delves into numerous other techniques applicable to diverse use cases.\n\nChapter 8\n\nSummary In this chapter, we introduced few-shot learning as a prompt engineering technique to guide LLMs toward more predictable and desired outcomes. We demonstrated its application in sce- narios such as implementing a specific logging structure and following a particular coding style.\n\nWe explored how to implement few-shot prompting across different GenAI applications. In ChatGPT and OpenAI API, we utilized specific keyword combinations such as question and answer or old and refactored to structure the few-shot examples. With GitHub Copilot, we leveraged a style guide file to influence code completion output.\n\nWe also introduced additional prompt engineering techniques that are valuable for scaling bug fixes. Iterative prompting enables models to refine their output by utilizing feedback from com- pilation checks until the code compiles successfully. Template-based prompting leads the model toward producing outputs with a specific structure. Furthermore, CoT prompting breaks down solutions into clear, step-by-step reasoning processes.\n\nIn the next chapters, we will delve deeper into the mechanics of CoT, and chaining, and see how to apply them with ChatGPT, OpenAI API, and GitHub Copilot.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: When should we use prompt engineering?\n\nAnswer: Prompt engineering should follow best practices to achieve desirable outcomes from LLMs. However, when standard guidelines are insufficient, advanced prompting techniques may be necessary. In this chapter, we saw how few-shot learning helped align outputs with a specific style guide, making results more predictable and desirable.\n\nQuestion 2: Can we use plain English to describe the desired outcome instead of providing few- shot examples in code?\n\nAnswer: While it is theoretically possible to use plain English to describe coding styles and desired outputs, this approach is less effective. Like mathematical equations or music notes, code acts as a concise and precise notation system. LLMs, trained extensively on code, interpret it more accurately than plain-text descriptions.\n\n207\n\n208\n\nAn Introduction to Prompt Engineering\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nPEP-8 Style Guide for Python Code: https://peps.python.org/pep-0008/\n\n\n\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ..., and Amodei, D. (2020). Language Models are Few-Shot Learners. OpenAI. Retrieved from https://arxiv.\n\norg/abs/2005.14165v4\n\n\n\nSahoo, P., Singh, A. K., Saha, S., Jain, V., Mondal, S., and Chadha, A. (2024). A systematic survey of prompt engineering in large language models: Techniques and applications. Indian In- stitute of Technology Patna, Stanford University, and Amazon AI. Retrieved from https:// arxiv.org/abs/2402.07927v1\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.\n\n9\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nSome coding tasks extend beyond simple, single-task initiatives. For these more complex scenarios, advanced prompt engineering techniques such as chain of thought (CoT) and chaining can be highly effective when working with large language models (LLMs). This chapter will explore how to use these techniques when working with OpenAI API, ChatGPT, and GitHub Copilot on challenges that often require more than just the initial prompt to achieve the desired output.\n\nWe will walk through an example of implementing a geometric mean calculator for portfolio returns that handles negative net returns. By applying CoT prompting, we will guide the model with explicit reasoning steps and achieve desirable results, even with a lighter model such as GPT-4o mini. Then, using chaining, we will improve the initial implementation by enhancing its type hints and refining the docstring.\n\nThis chapter covers the following topics:\n\n\n\nExtending the prompt implementation scope\n\n\n\nLeveraging CoT reasoning for code completion\n\nUsing prompt chaining with LLMs\n\n210\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance with setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nExtending the prompt implementation scope Until now, we have mainly worked with short functions spanning only a few lines of code, ap- plying our best practices for prompt precision to generate effective code using GitHub Copilot, OpenAI, and ChatGPT.\n\nHowever, in many real-world scenarios, our tasks will not fit within such a narrow scope. We may need to handle input casting, switch cases, or adapt implementations beyond a raw mathe- matical formula. This chapter explores how we can extend the scope of coding tasks by applying advanced prompt engineering techniques.\n\nExtending the geometric mean scope Let us revisit the geometric mean implementation from Chapter 3. The geometric mean is a method of averaging numbers that have a multiplicative nature, such as financial portfolio growth rates, population growth rates, or drug concentrations that change exponentially over time. The geo- metric mean is defined as the nth root of the product of the inputs, as follows:\n\n𝑛𝑛\n\n𝑖 𝑛𝑛\n\nGeometric Mean = (∏𝑥𝑥𝑖𝑖\n\n𝑖𝑖𝑖𝑖\n\nFor a more detailed explanation of the concept, refer to Chapter 3.\n\n)\n\nChapter 9\n\nHowever, when some of the inputs are negative, such as net portfolio returns, the standard for- mula becomes insufficient. For example, consider IBM’s stock returns from 2000 to 2002, where two years had negative returns:\n\nIBM_YEARLY_RETURNS: Dict[str, float] = {\n\n\"2000\": -0.2084,\n\n\"2001\": 0.4300,\n\n\"2002\": -0.3547\n\n}\n\nIf we were to use the geometric mean formula as is, we would get an average yearly return of 32%:\n\n1 3\n\n1 3\n\n\n\n𝐺𝐺𝐺 𝐺 𝐺(−0.2084𝐺 ⋅ 0.4300𝐺 ⋅ 𝐺 − 0.3547)\n\n𝐺 (0.031)\n\n𝐺 0.32\n\nThis result, however, is incorrect. A $1,000 investment in IBM stock in 2000 would shrink to approximately $730 by the end of 2002, reflecting a total decline of about 27%. Thus, we would expect an average annual return closer to -10%.\n\nOne way to handle negative returns is by using gross returns instead. This involves converting net returns to gross values, computing their product, and extracting the nth root:\n\nfrom typing import Dict\n\nimport numpy as np\n\ndef get_average_return(\n\nnet_returns: Dict[str, float],\n\n) -> float:\n\n# Step 1: Calculate the yearly gross returns\n\ngross_returns: np.ndarray = np.array(list(net_returns.values())) + 1\n\n# Step 2: Calculate the geometric mean of the gross returns\n\npower: float = 1 / len(gross_returns)\n\ngross_average: float = np.prod(gross_returns) ** power\n\n# Step 3: Calculate the net average return\n\nnet_average: float = gross_average - 1\n\nreturn net_average\n\n211\n\n212\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nApplying this function to IBM’s returns produces an average return of -10%:\n\nnet_returns: [-0.2084, 0.43, -0.3547]\n\ngross_returns: [0.7916, 1.43, 0.6453]\n\ngross_average: 0.9006\n\nnet_average: -0.099\n\nThis implementation is more extensive than the functions we have generated so far, so it may require more advanced techniques to get desirable results with minimal iterations.\n\nNext, we will compare baseline responses from GitHub Copilot, ChatGPT, and OpenAI API, ex- amining their limitations and potential pitfalls.\n\nEvaluating baseline outputs In Chapter 6, we discussed how GitHub Copilot, ChatGPT, and OpenAI API each have strengths in different coding-related tasks. Copilot excels at code completion but can fall short when gener- ating larger blocks of starter code. ChatGPT is better suited for producing more extensive initial code, though it often includes longer functions with excessive comments and computations. OpenAI API requires more effort to craft effective prompts but offers greater flexibility through programmatic automation and control over prompt history. For a refresher on these differences, you can revisit Chapter 6.\n\nThese differences become apparent when implementing the geometric mean for portfolio returns over multiple years. Each tool requires different strategies to refine and enhance the quality of the generated output.\n\nGitHub Copilot baseline To apply best practices for prompt precision with Copilot, we can start by providing the function signature for get_geometric_mean as follows:\n\ndef get_geometric_mean(\n\nnet_returns: Dict[str, float],\n\n) -> float:\n\n...\n\nChapter 9\n\nThis gives Copilot a clear starting point with the function name, an argument specifying the use case of net returns, and an expected return type. However, this prompt alone may not be sufficient to guide Copilot in handling negative inputs, as demonstrated in Figure 9.1.\n\nFigure 9.1: Copilot’s completion does not support negative net returns\n\nThis implementation would yield incorrect results for negative inputs and should not be used. Later in the chapter, we will see how CoT can help us get the correct implementation we are after.\n\nChatGPT baseline ChatGPT, on the other hand, presents a different challenge. While newer models, such as o3, excel at deeper reasoning (as discussed later in this chapter) and may produce more accurate imple- mentations, more effort is required to prompt ChatGPT effectively than with Copilot. Additionally, achieving a coding format that aligns with our preferences is more challenging.\n\nTo replicate the GitHub Copilot implementation, we used the following prompt:\n\nCONTEXT: You are provided with a Python function signature enclosed with {{{ FUNCTION }}}.\n\nTASK: implement the function.\n\nFUNCTION: {{{ defget_geometric_mean(\\n\\t\\tnet_returns: Dict[str, float],\\n) -> float: }}}\n\nCODE:\n\n213\n\n214\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nUsing the o3-mini-high model, we generated a functionally correct implementation. Howev- er, the resulting code was cluttered with excessive inline comments and docstrings, making it time-consuming to clean up. Figure 9.2 illustrates this output in ChatGPT.\n\nFigure 9.2: ChatGPT implementation\n\nQuick tip: Need to see a high-resolution version of this image? Open this book\n\nin the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nChapter 9\n\nFor reference, the proposed code by ChatGPT is as follows:\n\nimport math\n\nfrom typing import Dict\n\ndef get_geometric_mean(net_returns: Dict[str, float]) -> float:\n\n\"\"\" [... omitted for brevity ...] \"\"\"\n\nif not net_returns:\n\nreturn 0.0\n\n# Calculate the product of (1 + return) for each net return\n\nproduct = math.prod(1 + r for r in net_returns.values())\n\nn = len(net_returns)\n\n# Compute the nth root of the product and subtract 1 to get the geometric mean\n\ngeometric_mean = product ** (1 / n) - 1\n\nreturn geometric_mean\n\nThis code is functionally correct but includes several undesired elements:\n\nUnnecessary docstring\n\n\n\nInput validation\n\n\n\nExcessive comments\n\n\n\nInline computations\n\nLater in the chapter, we will explore how CoT and chaining requests to the initial implementation can help us achieve the desired output.\n\nOpenAI API baseline We used the o3-mini-2025-01-31 model via OpenAI API and found that its output closely mir- rored what ChatGPT produced. To evaluate this, we ran the script located at ch9/baseline/ baseline_openai.py, using the following system and user prompts:\n\nSURROUND = \"You are provided with a Python function signature enclosed with {{{ FUNCTION }}}.\"\n\nSINGLE_TASK = \"Your task is to implement the function.\"\n\nSRC_CODE = \"\"\"def get_geometric_mean(\\n\\tnet_returns: Dict[str, float],\\n) -> float:\"\"\"\n\n215\n\n216\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\ndef get_user_prompt(src: str) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {src} }}}}}}\n\nCODE:\n\n\"\"\"\n\nWe then called the model using the following code:\n\nif __name__ == '__main__':\n\nclient: OpenAI = OpenAI()\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(SRC_CODE)\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"o3-mini-2025-01-31\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n],\n\n)\n\noutput = completion.choices[0].message.content\n\nprint(output)\n\nThe output generated by OpenAI API was very similar to the one produced by ChatGPT, as seen in the following sample result:\n\ndef get_geometric_mean(net_returns: Dict[str, float]) -> float:\n\n# Return 0.0 if there are no returns (could alternatively raise an exception)\n\nif not net_returns:\n\nreturn 0.0\n\nimport math\n\n# Calculate the product of (1 + return) for each period\n\nproduct = 1.0\n\nn = len(net_returns)\n\nfor r in net_returns.values():\n\nproduct *= (1 + r)\n\nChapter 9\n\n# Compute the geometric mean: nth root of product then subtract 1\n\ngeom_mean = math.pow(product, 1 / n) - 1\n\nreturn geom_mean\n\nIn this implementation, the functionality is correct, but the formatting is less than ideal due to excessive inline comments, unnecessary input validation, and even an import statement placed inside the function. Later in the chapter, we will see how to refine this implementation using CoT and chaining to achieve desirable results even when working with lower-cost models such as GPT-4o mini.\n\nIn summary, as our implementation scope expands beyond simple and short functions, applying more advanced techniques becomes essential to achieving the desired results.\n\nNext, we will see how we can utilize advanced prompt engineering techniques and get the imple- mentation we want faster, and see which application (either Copilot, OpenAI API, or ChatGPT) is most suitable for each technique.\n\nLeveraging CoT reasoning with LLM In Chapter 8, we explored how guided reasoning with CoT can help LLMs produce better code by following a step-by-step reasoning process. This step-by-step thinking encourages models to tackle problems in a structured way, leading to more accurate and desirable outputs.\n\nAs a simple exercise, consider the following sequence:\n\n17, 34, 14, 28, 8, 16\n\nThis sequence follows two rules – first, multiply the previous number by 2 to get the next one; then subtract 20 to get the following number:\n\n34 = 17 x 2\n\n14 = 34 – 20\n\n28 = 14 x 2\n\n8 = 28 – 20\n\n16 = 8 x 2\n\n217",
      "page_number": 196
    },
    {
      "number": 9,
      "title": "However, when some of the inputs are negative, such as net portfolio returns, the standard for- mula becomes insufficient. For example, consider IBM’s stock returns from 2000 to 2002, where two years",
      "start_page": 246,
      "end_page": 277,
      "detection_method": "regex_chapter",
      "content": "218\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nThe next number should be -4, which is 16 - 20. However, prompting Copilot to predict the next number in the sequence is not straightforward, as shown in Figure 9.3.\n\nFigure 9.3: Copilot fails to predict the correct next number\n\nIn this case, Copilot failed to predict the next number accurately, suggesting 4 instead of -4. This demonstrates that the model did not fully grasp the sequence’s logic. To improve the outcome, we can guide the model by prompting it to reason through the relationship between the current number and the previous one before making a final prediction, as shown in Figure 9.4.\n\nFigure 9.4: Reasoning steps provided for the next number\n\nChapter 9\n\nFigure 9.4 shows that, by adding comments in rows 4-8, Copilot predicted the next number in the sequence correctly and also explained its reasoning for the final number, 6. Similarly, as we will explore later in this chapter, incorporating reasoning steps into our coding tasks can help the model produce more desirable outcomes.\n\nCoT in the GPT-o series CoT has proven so effective across a range of domains requiring complex reasoning that it was built into the GPT-o series. This means the model will automatically identify the necessary steps to complete a task before arriving at the final answer.\n\nFor the sequence completion example we explored earlier in the chapter, we could use the GPT-o1 model without additional guidance, as shown in Figure 9.5.\n\nFigure 9.5: Built-in CoT in o1\n\nFrom the ChatGPT output, we can see that the model recognized this as a sequence pattern task, with an internal process for handling reasoning in such cases. The model took 47 seconds to process the task, which included problem analysis, problem recognition, and estimating the next number, as shown in Figure 9.6.\n\n219\n\n220\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nFigure 9.6: Thought process in ChatGPT\n\nHere, we can see that ChatGPT is analyzing the sequence, which is one of the reasoning steps in CoT to get to the final answer.\n\no-series adoption Models from the o series are now available through ChatGPT and OpenAI API. However, they come at a higher cost than their predecessors, as illustrated in Figure 9.7:\n\nFigure 9.7: Costs of o-series models for OpenAI API calls\n\nThe o1 model, for example, is priced at $15 per million input tokens and $60 per million output tokens, making it six times more expensive than GPT-4o and 1,000 times more expensive than GPT-4o mini, as shown in Figure 9.8.\n\nChapter 9\n\nFigure 9.8: GPT-4o pricing for OpenAI API calls\n\nDue to this significant cost difference, o-series models are used less frequently in applications. When selecting the right model for yourself or your company, it is important to weigh the trade-off between cost and output quality. If more affordable models, such as GPT-4o, deliver satisfactory results and meet expectations, it may be prudent to continue using them. However, the o1 and o3 models have been shown to give significantly better outcomes, so it might be worth using them first or as a fallback, even at a higher cost.\n\nUsing CoT for GitHub Copilot The CoT approach is one of our most frequently used strategies when working with GitHub Co- pilot. When applied effectively, it can simplify the development of entire code segments, leading to cleaner and more readable implementations.\n\nThis method involves structuring the reasoning process by referencing functions that have yet to be implemented. For example, when calculating the geometric mean for net returns, we can break it down into three logical steps:\n\n1. Convert input to gross returns.\n\n2. Compute the geometric mean of the gross returns.\n\n3. Convert back to the net returns.\n\n221\n\n222\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nExpressing these steps in code involves defining each operation as a function, as shown in the following code:\n\ndef get_average_return(\n\nnet_returns: Dict[str, float],\n\n) -> float:\n\ngross_returns: np.ndarray = get_gross_returns(net_returns)\n\ngross_average: float = get_geometric_mean(gross_returns)\n\nnet_average: float = get_net_average(gross_average)\n\nreturn net_average\n\nThis function does not include any implementation details and only outlines the functions or steps to be completed. Since the functions are not yet implemented, typing their names in an IDE may look like an error, as shown in Figure 9.9:\n\nFigure 9.9: Error underline in the CoT definition\n\nThis underlying error indicator is expected and resolved as Copilot generates the implementation for the function at hand.\n\nThis reasoning approach facilitates the generation of accurate and predictable code when work- ing with the AI pair programmer. As each function is implemented (such as get_gross_returns, get_geometric_mean, etc.), Copilot learns enough context to write the correct provided param- eters, how the return value is used, and the expected type hints.\n\nAdditionally, we can predict how Copilot will complete the code and in what order. The imple- mentation will follow the sequence of function calls in get_average_return. Indeed, as we press Enter twice and start typing def, Copilot will suggest an implementation for get_gross_returns, as shown in Figure 9.10:\n\nChapter 9\n\nFigure 9.10: Copilot predicts the implementation of the first called function\n\nOnce we accept the implementation and hit Enter twice again, Copilot will suggest the next function in the call sequence, get_geometric_mean, as shown in Figure 9.11:\n\nFigure 9.11: Copilot predicting the next function definition\n\n223\n\n224\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nHere, we did not need to type anything. Just a two-line break, resembling the space between get_average_return and get_gross_returns, was enough to trigger a fully predictable imple- mentation in this file. This demonstrates the optimal use of Copilot, truly leveraging the power of an AI pair programmer. For your reference, here is the full implementation suggested by Copilot:\n\ndef get_gross_returns(\n\nnet_returns: Dict[str, float],\n\n) -> np.ndarray:\n\ngross_returns: np.ndarray = np.array(\n\nlist(net_returns.values())\n\n) + 1\n\nreturn gross_returns\n\ndef get_geometric_mean(\n\ngross_returns: np.ndarray,\n\n) -> float:\n\ngross_average: float = np.prod(gross_returns) ** (\n\n1 / len(gross_returns)\n\n)\n\nreturn gross_average\n\ndef get_net_average(\n\ngross_average: float,\n\n) -> float:\n\nnet_average: float = gross_average - 1\n\nreturn net_average\n\nThis implementation successfully captures the intended logic, and its structuring across different levels of abstraction is superior to what even the most advanced o3 models produce via ChatGPT and OpenAI API. This serves as a strong example of how effective prompting techniques can outperform more powerful and costly models.\n\nEducating your colleagues\n\nNext time you notice a colleague trying to use inline comments in GitHub Copilot such as # step 1: Compute the gross returns, suggest they try using CoT in- stead. This approach will not only be more efficient and a better way to use Copilot, but it will also help the team maintain higher-quality code.\n\nChapter 9\n\nNext, we will explore how to replicate the desired outputs with ChatGPT and OpenAI API, even when using more affordable models such as GPT-4o mini.\n\nCoT for the geometric mean with ChatGPT The same CoT approach can be applied to coding tasks, such as implementing a get_average_ return function that supports negative returns. Rather than supplying the function signature, we will provide a full implementation of get_average_return as we did with Copilot and prompt the model to complete the lower-level details. A prompt for this might look like the following:\n\nCONTEXT: You are provided with a Python function enclosed with {{{ FUNCTION }}} that calls functions that should be completed.\n\nTASK: Implement the missing functions.\n\nFUNCTION: {{{def get_average_return(\\n\\t\\tnet_returns: Dict[str, float],\\n) -> float: \\n\\tgross_returns: np.ndarray = get_gross_ returns(net_returns) \\n\\tgross_average: float = get_geometric_mean(gross_ returns) \\n\\tnet_average: float = get_net_average(gross_average) \\n\\ treturn net_average }}}\n\nCODE:\n\nThis prompt outlines the necessary reasoning steps to generate the correct code. For reference, here is the output produced by ChatGPT using the GPT-4o mini model with the given prompt:\n\ndef get_gross_returns(net_returns: Dict[str, float]) -> np.ndarray:\n\n\"\"\" ... omitted for brevity ... \"\"\"\n\nreturn np.array([1 + net_return for net_return in net_returns. values()])\n\ndef get_geometric_mean(values: np.ndarray) -> float:\n\n\"\"\" ... omitted for brevity ... \"\"\"\n\nreturn np.prod(values) ** (1 / len(values))\n\ndef get_net_average(gross_average: float) -> float:\n\n\"\"\" ... omitted for brevity ... \"\"\"\n\nreturn gross_average - 1\n\ndef get_average_return(net_returns: Dict[str, float]) -> float:\n\ngross_returns: np.ndarray = get_gross_returns(net_returns)\n\ngross_average: float = get_geometric_mean(gross_returns)\n\n225\n\n226\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nnet_average: float = get_net_average(gross_average)\n\nreturn net_average\n\nThis output is more satisfactory than the baseline, even though it was generated with GPT-4o model instead of the more advanced O-series. It no longer includes inline comments or input val- idation, which significantly reduces the amount of manual editing required. Later in the chapter, we will see how chaining can help produce even more polished results that are ready to use as is.\n\nNext, we will apply this approach using OpenAI API.\n\nCoT with OpenAI API We can leverage CoT prompting and provide a full implementation of get_average_return as we did with Copilot and ChatGPT. The system and user prompts may look as follows:\n\nSURROUND = \"You are provided with a Python function enclosed with {{{ FUNCTION }}} that calls functions that should be completed.\"\n\nSINGLE_TASK = \"Your task is to implement the missing functions.\"\n\ndef get_user_prompt(func: callable) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nCODE:\n\n\"\"\"\n\nWe will integrate these prompt pieces into an OpenAI API call, including the system prompt, user prompt, and a GPT-4o mini model, as shown here:\n\nif __name__ == '__main__':\n\nclient: OpenAI = OpenAI()\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(get_average_return)\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n],\n\nChapter 9\n\n)\n\noutput = completion.choices[0].message.content\n\nprint(output)\n\nNote that the code sample can be found in this book’s repository at ch9/cot/cot_openai.py. For reference, here is sample output from the call:\n\nimport numpy as np\n\nfrom typing import Dict\n\ndef get_gross_returns(net_returns: Dict[str, float]) -> np.ndarray:\n\n\"\"\"Convert net returns to gross returns.\"\"\"\n\nreturn np.array([1 + r for r in net_returns.values()])\n\ndef get_geometric_mean(gross_returns: np.ndarray) -> float:\n\n\"\"\"Calculate the geometric mean of a numpy array of gross returns.\"\"\"\n\nreturn np.prod(gross_returns) ** (1 / len(gross_returns))\n\ndef get_net_average(gross_average: float) -> float:\n\n\"\"\"Convert gross average back to net average.\"\"\"\n\nreturn gross_average – 1\n\ndef get_average_return(net_returns: Dict[str, float]) -> float:\n\n... omitted for brevity ...\n\nHere, we obtained a better implementation than the baseline, maintaining the desired level of abstraction and avoiding input validation and inline comments.\n\nWhile leveraging CoT is highly effective when using GitHub Copilot, it is not always sufficient to produce ready-to-use code with ChatGPT or OpenAI API. To address this, we can incorporate chaining through follow-up prompts, as we will demonstrate next.\n\nUsing prompt chaining for LLMs Prompt chaining is another highly effective prompt engineering technique that helps us achieve better results from LLMs. It involves breaking down tasks into smaller, sequential steps that are more efficiently completed individually.\n\n227\n\n228\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nFor instance, when implementing get_average_return, we may want to enhance ChatGPT’s or OpenAI’s initial output by adding type hints and avoiding inline calculations in the return statement. With GitHub Copilot, we might want to construct a barebones implementation first and add a Google Style docstring later.\n\nAlthough we could include all these elements in the initial prompt, it is often more natural and effective to start with an implementation that is functionally correct. From there, we can refine the code step by step through a series of follow-up prompts.\n\nChaining with ChatGPT Applying chaining with ChatGPT is very intuitive given that the UI is already designed for a con- versational style. The chat history helps us reference previous prompt pieces without needing to retype them.\n\nFor instance, say that we wanted to improve the implementation of get_average_return with a Google Style docstring rather than an arbitrary one, and type hints to variables in all functions. Including all these elements in one prompt would defy the single-task best practice and result in suboptimal outputs. Instead, we recommend the following approach:\n\n\n\nFor chained prompts with the same context, focus only on the new task\n\n\n\nFor chained prompts with a different context, such as adding test cases, use a full prompt following the five S’s best practices\n\nFor the task of coding get_average_return, adding type hints for variables, revising the docstring to be in a Google Style format, and adding a test suite can be done with the following strategy:\n\n\n\nPrompt 1: Write the full code prompt using the five S’s for the get_average_return im- plementation with a CoT.\n\n\n\nPrompt 2: Add type hints to all variables.\n\n\n\nPrompt 3: Change the docstring to Google Style.\n\n\n\nPrompt 4: Write unit tests. In this step, we will specify the test cases we want to cover, creating a new context. As a result, we need to provide a complete prompt that follows the five S’s best practices.\n\nChapter 9\n\nFor example, prompt 2, which focuses on the single task of removing comments, is simply as follows:\n\nAdd type hints to all variables.\n\nThis will produce an updated implementation without the inline comments, as shown in Figure 9.12.\n\nFigure 9.12: Added type hints to variables\n\nThe initial implementation has been updated to include type hints for variables across the different functions. For instance, in the first function, get_gross_returns, the gross_returns variable is now correctly annotated with the np.ndarray type and returned in the following line.\n\nIn prompt 3, the focus shifts to improving the docstring. We continue with a single-task prompt:\n\ninclude Google Style docstring.\n\n229\n\n230\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nThis prompt is expected to yield a better implementation, as demonstrated in Figure 9.13.\n\nFigure 9.13: Refined docstring\n\nThe result confirms that we successfully obtained the desired docstring for the get_gross_return function in Google Style format. For reference, here are the docstrings generated by ChatGPT for the get_geometric_mean, get_net_average, and get_average_return functions:\n\ndef get_geometric_mean(gross_returns: np.ndarray) -> float:\n\n\"\"\"Calculates the geometric mean of gross returns.\n\nArgs:\n\ngross_returns (np.ndarray): An array of gross returns.\n\nReturns:\n\nfloat: The geometric mean of the gross returns.\n\n\"\"\"\n\n... omitted for brevity ...\n\ndef get_net_average(gross_average: float) -> float: \"\"\"Converts gross average return to net average return.\n\nArgs:\n\ngross_average (float): The geometric mean of gross returns.\n\nChapter 9\n\nReturns:\n\nfloat: The corresponding net average return.\n\n\"\"\"\n\n... omitted for brevity ...\n\ndef get_average_return(net_returns: Dict[str, float]) -> float: \"\"\"Calculates the geometric mean of net returns from a dictionary of net returns.\n\nArgs:\n\nnet_returns (Dict[str, float]): A dictionary of net returns per period.\n\nReturns:\n\nfloat: The geometric mean of the net returns.\n\n\"\"\"\n\n... omitted for brevity ...\n\nIn later chapters of the book, we will explore a more robust approach to generating docstrings, along with additional chained tasks such as input validation, monitoring, and unit testing.\n\nOne limitation of the ChatGPT interface is the lack of control over how prompts are constructed and sent to the underlying model, especially as the conversation grows longer. In the next section, we will see how to gain more control over the chat history by using OpenAI API.\n\nSelective history with OpenAI API When working with OpenAI API for chained tasks, we can take a selective approach and explicitly choose which parts of the conversation history to include in each prompt.\n\nLet us consider the first three prompts from the previous ChatGPT session:\n\n\n\nPrompt 1: Leverage CoT for the get_average_return implementation\n\n\n\nPrompt 2: Add type hints\n\n\n\nPrompt 3: Refine the docstring\n\nA straightforward way to replicate this workflow with OpenAI API is to continuously append the chained prompt and the corresponding output to our messages list, as shown here:\n\nmessages = [{\"role\": \"system\", \"content\": system_prompt}]\n\nprompt_1 = get_user_prompt(get_average_return)\n\n231\n\n232\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nprompt_2 = \"Add type hints to all variables.\"\n\nprompt_3 = \"include Google Style docstring.\"\n\nfor prompt in [prompt_1, prompt_2, prompt_3]:\n\nmessages.append({\"role\": \"user\", \"content\": prompt})\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=messages,\n\n)\n\noutput: str = completion.choices[0].message.content\n\nmessages.append({\"role\": \"assistant\", \"content\": output})\n\nIn this naive setup, each user prompt and corresponding model response is appended to the con- versation history. By the time we reach prompt_3, we expect the output to closely resemble what ChatGPT would generate. For reference, here is a sample response from the API that successfully produced a correct implementation, complete with Google Style docstrings and properly typed variables:\n\ndef get_gross_returns(net_returns: Dict[str, float]) -> np.ndarray:\n\n\"\"\"Convert net returns to gross returns.\n\nArgs:\n\nnet_returns (Dict[str, float]): A dictionary where keys are identifiers\n\n(such as tickers) and values are net return values (as decimal).\n\nReturns:\n\nnp.ndarray: An array of gross returns corresponding to the net returns.\n\n\"\"\"\n\ngross_returns: np.ndarray = np.array([1 + return_value for return_ value in net_returns.values()])\n\nreturn gross_returns\n\nFor reference, the full script can be found in ch9/chaining/naive_chaining_openai.py.\n\nChapter 9\n\nThe naive approach works reasonably well when there are fewer than 5 follow-up prompts. How- ever, as the chain grows, so does the length of the accumulated prompt, leading to increased token usage. For instance, running the preceding script peaked at 1,253 tokens by prompt_3, as illustrated in Figure 9.14.\n\nFigure 9.14: prompt_3 with 1,253 tokens\n\nAs more prompts are added, the total token count increases, which results in higher usage costs and the risk of distracting the LLM from the specific task at hand.\n\nA more efficient alternative for longer chains is a selective history strategy, where only the relevant parts of the interaction are passed along. Instead of including the entire interaction history in each follow-up request, we can simply pass the code generated in response to the first prompt, along with a new system prompt tailored to the next transformation task.\n\nTo isolate just the code from the assistant’s response, we can use the following utility function:\n\ndef get_refactor_user_prompt(assistant_output: str) -> str:\n\ncode: str = re.sub(\n\nr\"(.*?)```python(.*?)```(.*)\",\n\nr\"\\2\",\n\nassistant_output,\n\nflags=re.DOTALL\n\n).strip()\n\nuser_prompt: str = f\"\"\"\n\nOLD: {{{{{{ {code} }}}}}}\n\nREFACTORED:\n\n\"\"\"\n\nreturn user_prompt\n\n233\n\n234\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nThis extracts the relevant code and formats it into a new user prompt for the next step in the chain.\n\nIn contrast to the static system prompt used in the naive approach, we now update the system prompt with each task:\n\nprompt_1: str = f\"{SURROUND} {SINGLE_TASK}\"\n\nprompt_2: str = \"You are provided with a Python code enclosed in {{{ FUNCTION }}}. Your task is to add type hints to all variables.\"\n\nprompt_3: str = \"You are provided with a Python code enclosed in {{{ FUNCTION }}}. Your task is to include Google Style docstring.\"\n\nWe can then use a prompts dictionary to associate each prompt with the appropriate input-gen- eration function, linking prompt_1 to get_user_prompt for the initial implementation, and the subsequent prompts to get_refactor_user_prompt for the refactoring steps:\n\nprompts: Dict[str, callable] = {\n\nprompt_1: get_user_prompt,\n\nprompt_2: get_refactor_user_prompt,\n\nprompt_3: get_refactor_user_prompt}\n\nThis allows us to generate the user prompt dynamically for each task using the correct function:\n\nfor prompt, func in prompts.items():\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": prompt},\n\n{\"role\": \"user\", \"content\": func(next_input)},\n\n],\n\n)\n\nnext_input: str = completion.choices[0].message.content\n\nNotice how each API call now contains only a system prompt and a single user message. This keeps the interaction focused, passing along only what is necessary rather than the full chain of previous prompts and outputs.\n\nFor reference, here is a sample output from prompt_3 using the selective history strategy:\n\ndef get_gross_returns(net_returns: Dict[str, float]) -> np.ndarray:\n\n\"\"\"\n\nConvert net returns to gross returns.\n\nChapter 9\n\nGross Return is calculated as:\n\nGross Return = 1 + Net Return.\n\nArgs:\n\nnet_returns (Dict[str, float]): A dictionary where keys are identifiers (e.g., asset names) and values are the corresponding net returns.\n\nReturns:\n\nnp.ndarray: An array of gross returns derived from the provided net returns.\n\n\"\"\"\n\ngross_returns: np.ndarray = np.array([1 + net_return for net_return in net_returns.values()])\n\nreturn gross_returns\n\nThis output mirrors the quality of the naive approach, including type hints and a Google Style docstring. The key difference is efficiency: using selective history, we reduced the token count to just 296, which is less than 25% of the naive approach, as shown in Figure 9.15:\n\nFigure 9.15: Selective history reduces input tokens by over 75%\n\nAs chained tasks become more complex, the selective history strategy helps keep prompts short and focused. In later chapters, we will explore additional use cases for chaining across various stages of the Software Development Life Cycle (SDLC).\n\n235\n\n236\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nIn summary, OpenAI API allows both automated execution of chaining and control over prompt history. By including only relevant information, we can reduce costs and help the model concen- trate on the specific task at hand.\n\nIn the final subsection of this chapter, we will look at how chaining can be applied with GitHub Copilot, where we aim for the initial implementations to already follow the desired formatting.\n\nChaining with Copilot When working with Copilot for code completion tasks, we can also leverage chaining by breaking down the task into smaller steps. For tasks that go beyond the barebones implementation, such as adding docstrings, input validations, logs, and tests, it is best to align with a chained approach. For the chained tasks, we can use the chat interaction mode with Copilot. In later chapters of the book, we will see how to leverage code completion for chained tasks as well.\n\nFor example, a chaining flow to extend get_average_return with a docstring and input valida- tion might look as follows:\n\n1.\n\nInitial task: Implement get_average_return with CoT.\n\n2. Chained task 2: Add docstrings to all functions.\n\n3. Chained task 3: Add input validation.\n\nAfter generating the initial implementation of get_average_return, as shown earlier in the chapter, we can proceed to the second chained task, which focuses on adding docstrings to all functions in the file.\n\nIn VS Code, we can use Copilot Edits for this task by opening the chat window and selecting Edit. We will ensure that the working set includes the current file, ch9/chaining/get_average_return. py, and enter the following prompt:\n\nAdd Google Style docstring to all functions.\n\nThis prompt will result in suggestions for adding type hints to the variables, as shown in Figure 9.16.\n\nChapter 9\n\nFigure 9.16: Suggested edits to add a docstring\n\nHere, we can see that Copilot correctly added a docstring to all functions with our preferred Google Style; for example, in line 26, a correct description of get_gross_return was added to the function.\n\nIn PyCharm, open GitHub Copilot Chat by clicking the Copilot icon in the bottom menu and selecting Open GitHub Copilot Chat or using the Ctrl + Shift + C keyboard shortcut. Enter the following prompt:\n\nAdd Google Style docstring to all functions.\n\nCopilot will suggest the necessary edits, adding a docstring to all functions, as seen in Figure 9.17:\n\nFigure 9.17: Suggested edits to add a docstring using PyCharm\n\n237\n\n238\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nIn both VS Code and PyCharm, Copilot suggests the appropriate docstring for each function. Later in this book, we will apply chaining along with other prompt engineering techniques to address SDLC tasks. For example, in Chapter 14, we will explore how to combine CoT and chaining to profile the runtime and space complexity of our code.\n\nSummary In this chapter, we explored how to achieve desirable outcomes from LLMs by effectively applying CoT and chaining for coding tasks with an extended scope.\n\nWith CoT prompting, we saw how introducing reasoning steps into our prompts enables the model to handle more nuanced challenges, such as implementing a geometric mean function that supports negative net returns. We used function names as intermediate reasoning steps, while relying on Copilot, ChatGPT, and OpenAI API to fill in the implementation details.\n\nThrough chaining, we began with an initial implementation that is functionally correct and iteratively improved by adding type hints and refining docstrings. When using OpenAI API, we introduced a selective history approach to make chaining more efficient, which still holds as the chain of tasks gets longer.\n\nIn the next chapter, we will delve deeper into refactoring code with GenAI applications. Later in the book, we will introduce advanced prompt engineering techniques at various stages of the SDLC.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: Is prompt engineering always necessary when working with GenAI?\n\nAnswer: Not always. For tasks with a smaller scope, such as implementing a short function, the five S’s framework is often enough to guide the model toward a good result. Prompt engineering becomes more important when the task grows in complexity, requiring us to either break it into smaller parts or guide the model with reasoning steps to help it succeed.\n\nQuestion 2: How is CoT applied with GitHub Copilot?\n\nAnswer: To apply CoT with Copilot, we specify our reasoning steps in abstracted functions. This approach helps Copilot understand the reasoning behind the desired implementation, allowing it to generate the next lines of code with minimal input, often just by hitting Enter.\n\nChapter 9\n\nQuestion 3: Is chat history irrelevant when using prompt chaining?\n\nAnswer: No. In chaining, we make use of the previously provided context and supporting data, such as code specifications and reasoning steps, and refer to them again, just as we would ref- erence variables in our code. When additional context is needed, such as test cases to include in our test suite, we rebuild the full prompt.\n\nFurther reading To learn more about the topics that were covered in this chapter, refer to the following resources:\n\nGeometric mean in financial applications, Wikipedia: https://en.wikipedia.org/wiki/\n\nGeometric_mean#Financial\n\nOpenAI learning to reason (CoT): https://openai.com/index/learning-to-reason-\n\nwith-llms/\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n239\n\n10\n\nRefactoring Code with GenAI\n\nSoftware developers write in programming languages to instruct a computer to perform specific tasks. The source code in the programming language is translated by a compiler or interpreter into executable computer instructions. One might conclude that programming is only artificial communication with a computer, but it is much more than that.\n\nThe true, implicit goal of a computer program is to communicate with other developers. Once code is written, it is frequently read many times, hours, weeks, or even years later, by the same or other developers. The developers will read the code to understand it, debug it, and integrate it with other systems. The goal must be to communicate with the precision required by software, while maintaining clarity for future developers.\n\nThus, code is written once but read many times. Before GenAI code generation became common, Python practitioners invested significant effort into code quality and style. Today, we can use prompting techniques with GenAI to enhance code quality and style more efficiently.\n\nIn Chapter 7, we introduced techniques for quickly reading and debugging GenAI-generated code. In this chapter, we will focus on using GenAI for refactoring, which means rewriting existing code to improve readability, structure, or performance while preserving its original functionality.\n\nWe will examine a poorly implemented function that computes the distance between two matri- ces. By applying the five S’s framework for prompt precision as introduced in Chapter 4, we will convert a nested loop into a vectorized operation. With chain-of-thought (CoT) prompting, we will restructure the function and delegate implementation details to GitHub Copilot, ChatGPT, and OpenAI API.\n\n242\n\nRefactoring Code with GenAI\n\nTopics covered in this chapter include the following:\n\n\n\nIntroducing code refactoring\n\n\n\nRefactoring for better structure with GenAI\n\n\n\nRefactoring for performance with GenAI\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to Appendix for detailed guidance.\n\nIntroducing code refactoring Code is intended as communication to future software developers. Code that effectively and clearly communicates the purpose and mechanisms is described as clean code.\n\nAs described by Robert C. Martin (better known as Uncle Bob), clean code can be achieved through refactoring. Code refactoring is not merely modifying code, but a disciplined approach to contin- uously improving code. One aspect is removing the indiscriminate code smell that qualitatively characterizes poorly written code (see Further reading). A second aspect is improving the code’s actual design, which should improve its readability and maintainability. This is critical since firms spend an estimated 75% of their software development costs on the evolution and maintenance of software (Sommerville).\n\nChapter 10\n\n243\n\nSoftware developers frequently dedicate time to refactoring, which involves rewriting sections of existing code while keeping the original functionality and purpose intact. The goal is to improve readability, structure, or performance. This is particularly evident in Agile software development, where continuous refactoring is linked to the sustainable evolution of code. The Extreme Pro- gramming (XP) agile methodology explicitly embraces refactoring along with ideals of collective ownership and the use of Test-Driven Development (TDD), as will be discussed in Chapter 13.\n\nIDEs such as PyCharm and VS Code offer built-in tools for tasks such as extracting variables, func- tions, and methods, renaming code elements, and formatting code. Still, they fall short when it comes to more complex scenarios such as extracting a function from non-consecutive lines of code.\n\nNext, we will use an example of a function that computes two types of distances and identify hidden issues caused by poor implementation. Later in the chapter, we will refactor it using GenAI tools.\n\nManhattan and Euclidean distance In Chapter 7, we introduced the Manhattan distance between two matrices, also known as the L1 norm, defined as the sum of absolute differences between corresponding elements. The code example provided used a Flask application that receives matrix data through HTTP requests. If you need a refresher on this topic, refer to Chapter 7.\n\nAnother way to measure the distance between matrices is the Euclidean distance, which is simply the straight-line distance. In two dimensions, the distance between points (x1, y1) and (x2,y2) is calculated as follows:\n\nDistance\n\n\n\n2\n\n2\n\n= √(𝑥𝑥1 − 𝑥𝑥2) When applied to a pair of two-dimensional matrices number of rows and columns), the distance is calculated as follows:\n\n+ (𝑦𝑦1 − 𝑦𝑦2) and 𝑌 𝑋\n\nof the same shape (with the same\n\nDistance\n\n𝑛𝑛\n\n𝑚𝑚\n\n\n\n2\n\n= √ ∑∑(𝑋𝑋𝑖𝑖𝑖𝑖 − 𝑌𝑌𝑖𝑖𝑖𝑖) 𝑖𝑖=1\n\n𝑖𝑖=1\n\nThis measure is also referred to as the L2 norm or Frobenius norm. It is widely used in tasks such as image similarity and machine learning, where it often represents the squared error of a prediction.\n\n244\n\nRefactoring Code with GenAI\n\nImplementing the distance function If we want to implement a function that computes both L1 and L2 norms based on a request, one way to approach this is to build upon our Chapter 7 implementation of the Manhattan distance (L1) and extend it to include the Euclidean distance (L2) as well. A sample implementation, al- though poorly written, is as follows:\n\n@app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\ndata = request.get_json()\n\ndist_type = data.get(\"distance\")\n\nif dist_type == \"L1\":\n\nprint(\"Info: computing L1 distance...\")\n\na = data.get(\"df1\")\n\nb = data.get(\"df2\")\n\ndist = np.sum(np.abs(a - b))\n\nreturn jsonify({\"distance\": dist})\n\nelif dist_type == \"L2\":\n\nprint(\"Info: computing L2 distance...\")\n\na = data.get(\"df1\")\n\nb = data.get(\"df2\")\n\ndist_2 = 0\n\nfor i in range(len(a)):\n\nfor j in range(len(a[i])):\n\ndist_2 += (a[i][j] - b[i][j]) ** 2\n\ndist = np.sqrt(dist_2)\n\nreturn jsonify({\"distance\": dist})\n\nChapter 10\n\nQuick tip: Enhance your coding experience with the AI Code Explainer and Quick Copy features. Open this book in the next-gen Packt Reader. Click the Copy button\n\n(1) to quickly copy code into your coding environment, or click the Explain button\n\n(2) to get the AI assistant to explain a block of code to you.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR visit packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nMaintaining this code may be challenging. For example, reading the parameters a and b is dupli- cated, so if we want to add validations such as checking that both matrices have the same shape, we need to do it twice. Such duplication of code is considered poor programming practice. If we plan to add unit tests, every test needs to mock the request. Adding another distance function would make the function even longer and harder to read.\n\nNext, we will categorize the several types of refactoring and see how they apply to this example.\n\nTypes of refactoring When we refactor code, our goal is to maintain its functionality while improving its quality. There is always a risk that the modifications will break the code or inadvertently change the functionality. To avoid this, unit tests should be in place before making any significant changes to ensure that the code still runs correctly. In addition, refactoring should be performed as a set of incremental changes that do not alter the behavior of the code.\n\n245\n\n246\n\nRefactoring Code with GenAI\n\nThe term refactoring alone is not specific enough to define a single task, according to the five S’s framework introduced in Chapter 4. The changes we aim to make can vary from simple tasks such as renaming a variable to more extensive efforts such as redesigning an entire module.\n\nTo achieve clean code, there are several common refactoring approaches. First, larger and more complex functions should be divided into several that perform a single task. Second, duplicate code should be removed or abstracted by the Do not Repeat Yourself (DRY) principle. Variables and functions should be named for clarity. This principle also applies to arbitrary numeric con- stants and strings, so there should not be hardcoded values (also referred to as magic numbers), but instead have these values assigned to a meaningful constant name.\n\nWe typically divide refactoring tasks into three categories:\n\n\n\nReadability: This includes tasks such as renaming objects, removing redundant imports or excessive comments, introducing constants instead of hardcoded numbers, or adding type hints. For example, within the implementation of calculate_distance, we might change the name of the variable a to df1 to indicate that it represents a matrix in the request. We might also want to add type hints to the request parameters.\n\n\n\nStructure change: This involves organizing code in a better way, such as extracting long code blocks into functions or classes. For example, within the implementation of calculate_distance, we might replace print statements with logger calls, improving how our code handles output. We might also extract the L1 and L2 calculations into separate functions.\n\n\n\nPerformance: This means adjusting the implementation to better handle space or runtime constraints. For example, within the implementation of calculate_distance, instead of using a nested for loop to calculate L2, we might apply vectorization using the NumPy library to improve efficiency.\n\nThere are already good capabilities for refactoring code, particularly for readability, using the built-in tools of our IDEs, as we will see next.\n\nChapter 10\n\nImproving readability before GenAI Modern IDEs such as VS Code and PyCharm offer built-in refactoring tools that help improve code readability. For example, in PyCharm, we can right-click a parameter we would like to rename and select Refactor | Rename to update the variable and all its references within the relevant scope (in this case, the calculate_distance function). This option is illustrated in Figure 10.1.\n\nFigure 10.1: PyCharm built-in refactoring tools for refactoring/renaming\n\nThis figure also highlights other refactoring options such as changing a callable signature, ex- tracting constants or methods, and more. The authors frequently use these features when refining existing code.\n\n247\n\n248\n\nRefactoring Code with GenAI\n\nSimilarly, VS Code allows us to right-click the parameter we want to change and select Rename Symbol to update the variable and all its references within the relevant scope, as shown in Figure 10.2.\n\nFigure 10.2: VS Code Rename Symbol\n\nThe Rename Symbol option appears fourth from the bottom in the figure and performs a similar renaming operation as in PyCharm.\n\nNext, we will explore the limitations of these built-in tools. Later in the chapter, we will use GenAI to address those challenges.\n\nLimitations of code refactoring with IDE tools PyCharm and VS Code offer powerful refactoring features, but these are still limited.\n\nFor example, we may want to extract argument parsing logic for a, b, and dist_type into a sep- arate function, such as the following:\n\ndef parse_request_parameters(r: request) -> Tuple[np.ndarray, np.ndarray, str]:\n\na = np.array(r[\"df1\"])\n\nb = np.array(r[\"df2\"])\n\ndist_type = r[\"distance\"]\n\nreturn a, b, dist_type\n\nChapter 10\n\nThis kind of structural refactoring helps reduce code duplication. However, current IDE tools are unable to extract these three parameters into a single function because they are not written in consecutive lines in the original implementation, as shown in Figure 10.3.\n\nFigure 10.3: Limitation of extracting a code block to a function\n\nQuick tip: Need to see a high-resolution version of this image? Open this book in the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nIn this figure, we cannot extract dist_type, a, and b into a separate function since the lines are not written in a continuous block. As functions become more complex, this approach might no longer be practical.\n\nNext, we will examine how to handle structural refactoring challenges that are beyond the ca- pabilities of IDE tools alone.\n\n249\n\n250\n\nRefactoring Code with GenAI\n\nRefactoring with chain-of-thought In Chapter 9, we introduced chain-of-thought (CoT) as a key prompt engineering technique for working with GenAI to generate code. This method involves writing a high-level function as the prompt, while leaving the implementation details for the GenAI application to complete.\n\nWhen refactoring code, we want to include the old implementation as an additional context for the model. This can help it better understand the intended functionality. For instance, in the case of the calculate_distance function, the previous implementation can clarify which parameters need to be extracted from the JSON request: a, b, and dist_type.\n\nRefactoring for a better structure When reviewing the implementation of the calculate_distance function, we can break it down into three main steps:\n\n1. Extracting the request parameters\n\n2. Deciding whether to compute L1 (Manhattan) or L2 (Euclidean) distance\n\n3. Calculating the distance\n\nA CoT prompt that reflects these steps could look like this:\n\n@app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\na, b, dist_type = parse_request_parameters(request)\n\ndist_func = {\"L1\": get_manhattan_dist, \"L2\": get_euclidean_dist}. get(dist_type)\n\ndist = dist_func(a, b)\n\nreturn jsonify({\"distance\": dist})\n\nIn this high-level implementation, we can observe the three main steps:\n\n1. Call a function called parse_request_parameters that should return the values for a, b,\n\nand dist_type.\n\n2. Map dist_function based on the value of dist_type from the request.\n\n3. Compute the distance using a and b.",
      "page_number": 246
    },
    {
      "number": 10,
      "title": "243",
      "start_page": 278,
      "end_page": 290,
      "detection_method": "regex_chapter",
      "content": "Chapter 10\n\nNote that we have considerable flexibility in how we map dist_type to a function. This could involve more than an inline dictionary, such as using a function or an external dictionary. A less Pythonic but also acceptable approach would be to use a switch-case structure or an if-else block.\n\nWith this function in place, we can now explore how to implement a complete prompt for GitHub Copilot, ChatGPT, and OpenAI API.\n\nCoT for refactoring with GitHub Copilot When using GitHub Copilot, the best practice is to start our prompt with the same function signa- ture as the function we want to refactor. This tells Copilot that we are planning to re-implement the function. Note that Copilot may already suggest an implementation based on the signature alone. However, such an implementation is only as good as asking ChatGPT to refactor a function, since the prompt is not specific enough. For reference, one early completion that Copilot produced from the signature alone is shown in Figure 10.4.\n\nFigure 10.4: Copilot produced a weak refactoring suggestion\n\n251\n\n252\n\nRefactoring Code with GenAI\n\nIn this example, the refactoring involved converting the a and b parameters to NumPy arrays. While this vectorization can be beneficial in some cases, it may be unnecessary in others. To guide Copilot toward a more suitable output, we use the CoT approach by providing the full CoT high-level implementation as shown earlier in the chapter. Like in Chapter 9, Copilot is expected to generate the implementations of the missing functions called by the high-level implementation, as illustrated in Figure 10.5.\n\nFigure 10.5: CoT approach triggers an implementation of missing functions\n\nIn this case, Copilot correctly proposed starting with the parse_request_parameters function. Once accepted, it suggested a correct implementation for parsing a, b, and dist_function, as shown in Figure 10.6.\n\nFigure 10.6: Copilot implementation for parse_request_parameters\n\nChapter 10\n\nHere, Copilot effectively used the context from the previous version of the function. It suggested reading the value of a from the df1 key in the request JSON, b from df2, and dist_type from distance. This highlights how providing relevant context can lead to desirable results.\n\nFor reference, here are the three implementations Copilot generated:\n\ndef parse_request_parameters(request):\n\ndata = request.get_json()\n\na = np.array(data.get(\"df1\"))\n\nb = np.array(data.get(\"df2\"))\n\ndist_type = data.get(\"distance\")\n\nreturn a, b, dist_type\n\ndef get_manhattan_dist(a, b):\n\nprint(\"Info: computing L1 distance...\")\n\nreturn np.sum(np.abs(a - b))\n\ndef get_euclidean_dist(a, b):\n\nprint(\"Info: computing L2 distance...\")\n\ndist_2 = 0\n\nfor i in range(len(a)):\n\nfor j in range(len(a[i])):\n\ndist_2 += (a[i][j] - b[i][j]) ** 2\n\nreturn np.sqrt(dist_2)\n\nThese functions mirror the original logic from calculate_distance while offering improved abstraction and avoiding code duplication. In a later section of the chapter, we will explore how to refactor for performance, including replacing nested loops with vectorized operations.\n\nNext, we will explore how to apply the CoT approach for refactoring with ChatGPT and OpenAI API. This requires a bit more effort to set up effectively.\n\n253\n\n254\n\nRefactoring Code with GenAI\n\nLeveraging CoT for refactoring with ChatGPT and OpenAI API Just like GitHub Copilot, we can also make use of ChatGPT and OpenAI API to refactor our code to have a better structure. To achieve this, we need to provide these tools with both the original implementation for context and the new structure we want to complete.\n\nA structural refactoring prompt for ChatGPT can follow this general template:\n\nCONTEXT: You are provided with\n\n1. Python function enclosed with {{{ fUNCTION }}} with calls to missing implementations.\n\n2. Old implementation enclosed with {{{ OLD }}} for reference.\n\nTASK: implement the missing functions.\n\nFUNCTION: {{{ ... omitted for brevity ... }}}\n\nOLD: {{{ ... omitted for brevity ...}}}\n\nCODE:\n\nNote that in contrast to CoT for new code, refactoring prompts requires giving ChatGPT both the new structure of the function and the reference implementation enclosed in {{{ OLD }}}. This format is like how we prompt Copilot and produces desirable results that preserve functionality, even when using more cost-effective models such as GPT-4o. Figure 10.7 shows the output we received from GPT-4o for this prompt.\n\nChapter 10\n\nFigure 10.7: ChatGPT suggested refactoring\n\nHere is the implementation for the missing functions proposed by ChatGPT:\n\ndef parse_request_parameters(req):\n\ndata = req.get_json()\n\ndist_type = data.get(\"distance\")\n\na = np.array(data.get(\"df1\"))\n\nb = np.array(data.get(\"df2\"))\n\nreturn a, b, dist_type\n\ndef get_manhattan_dist(a, b):\n\nprint(\"Info: computing L1 distance...\")\n\nreturn float(np.sum(np.abs(a - b)))\n\ndef get_euclidean_dist(a, b):\n\nprint(\"Info: computing L2 distance...\")\n\nreturn float(np.sqrt(np.sum((a - b) ** 2)))\n\n255\n\n256\n\nRefactoring Code with GenAI\n\nThis output resembles the structure Copilot suggested, focusing on extracting the distance calcu- lations into smaller functions and removing redundancy. One notable difference is that ChatGPT was more consistent and used NumPy vectorization for both distance functions, and not just the Manhattan distance method.\n\nOpenAI API prompt When we refactor code for improved structure, GitHub Copilot and ChatGPT often prove to be more suitable than OpenAI API. This is because using the API typically requires more manual effort to craft the prompt programmatically. OpenAI API becomes more practical only when we need to scale changes across multiple similar functions, and we find that refactoring the structure with CoT might not be one of these cases.\n\nIn summary, GenAI supports the restructuring of existing code by combining prior implemen- tations with new instructions. We recommend trying this technique with code you encounter in your day-to-day work or even in third-party libraries.\n\nNext, we will briefly introduce refactoring for performance using GenAI, a topic we will cover more extensively in Chapter 14.\n\nPerformance refactoring with GenAI Performance refactoring refers to changes made to the code that preserve its functionality while improving runtime or memory efficiency. One common approach is to use vectorized compu- tations. These can reduce the runtime by benefiting from cache, lower overhead, and parallel computation.\n\nFor instance, in the GitHub Copilot implementation of calculate_distance example, this would mean replacing a nested for loop with a vectorized computation of the Euclidean distance. We will explore further runtime and space complexity in Chapter 14, including when and why to scale system capacity and the trade-offs involved. Until then, let us demonstrate how a simple optimization through vectorization might be applied.\n\nPerformance refactoring with GitHub Copilot As with the CoT approach, we will leverage the context of existing code along with our desired structure. This time, we will specify the library we would like to implement a code block instead of the original implementation.\n\nChapter 10\n\nFor example, consider the function Copilot extracted for us to compute the Euclidean distance:\n\ndef get_euclidean_dist(a, b):\n\nprint(\"Info: computing L2 distance...\")\n\ndist_2 = 0\n\nfor i in range(len(a)):\n\nfor j in range(len(a[i])):\n\ndist_2 += (a[i][j] - b[i][j]) ** 2\n\nreturn np.sqrt(dist_2)\n\nThis function computes the squared distance dist_2 and then returns its square root.\n\nTo vectorize the squared distance computation, we can use NumPy, a widely used Python library that supports optimized operations on arrays and matrices. In our prompt, our goal is to preserve the original logic:\n\n1. Compute the squared distance\n\n2. Compute the distance\n\nTo guide Copilot toward the desired refactoring, we can add a cue using a NumPy function call. The first prompt would be placed near the squared distance step:\n\nDist_2 = np.\n\nThis is shown in Figure 10.8:\n\nFigure 10.8: Prompt with an np. library call as a cue\n\nThen, for the square root step, we would add another cue:\n\ndist = np.\n\n257\n\n258\n\nRefactoring Code with GenAI\n\nThis is sufficient context for Copilot to complete the correct implementation using NumPy, as shown in Figure 10.9:\n\nFigure 10.9: Copilot recognized the correct function from the cue\n\nThese inline completions can be highly effective. However, before applying performance opti- mizations, it is important to determine whether scaling or optimizing code is necessary. We will explore these considerations further in Chapter 14.\n\nNext, we will look at how to achieve the same outcome using ChatGPT and OpenAI.\n\nPerformance refactoring with ChatGPT and OpenAI When using ChatGPT and OpenAI API for performance refactoring tasks such as vectorizing computations, common transformations such as adopting the NumPy package can be achieved by following the five S’s best practices, as introduced in Chapter 4. Like in Chapter 7, where we include the full file content in the prompt along with the specific line we want to explain, here, we also provide the complete function implementation together with the files that need refactoring.\n\nHere is a sample prompt template that can be used in ChatGPT:\n\nCONTEXT: You are provided with:\n\n1. A Python function implementation enclosed with {{{ FUNCTION }}}\n\n2. Lines to be refactored enclosed with {{{ OLD }}}\n\n3. A library to be used in the new code enclosed with {{{ LIBRARY }}}\n\nTASK: Return a new implementation for the old lines using the specified library.\n\nFUNCTION: {{{ ... }}}\n\nOLD: {{{ ... }}}\n\nChapter 10\n\nLIBRARY: {{{ ... }}}\n\nREFACTORED CODE:\n\nNote that in this template, we are not requiring the model to follow the same sequence of steps as in Copilot, such as first computing dist_2 and then applying the square root. This allows ChatGPT to suggest more common implementations that may take a different route. By specifying LIBRARY, we are explicitly guiding the GenAI tools toward an expected solution.\n\nImportant note\n\nIn Chapter 14, we will learn how to replace the library constraint with constraints on runtime and space capacity. If we do not specify either constraints or a library, the model will suggest the most popular implementation, which may not align with our goals. As a result, the response often lacks specific instructions and does not follow the five S’s framework.\n\nFor reference, let us consider a vectorization prompt for the get_euclidean_distance function that is available in this book’s repository at ch10/prompts/performance_refactoring_chatgpt. txt. The response we got from ChatGPT with the GPT-4o economic model is shown in Figure 10.10.\n\nFigure 10.10: ChatGPT proposed vectorized implementation\n\n259\n\n260\n\nRefactoring Code with GenAI\n\nThe implementation shown in the figure is a common implementation for calculating Euclide- an distance. It vectorizes the element-wise distance and then uses the linear algebra module in NumPy to compute the norm. This version is typically easier to read and more flexible than what Copilot suggested.\n\nNote\n\nThis prompt structure can be reused when comparing implementations across mul- tiple libraries offering similar functionality. For instance, when evaluating alter- natives between SciPy and NumPy, this format allows us to generate and compare implementations quickly.\n\nNext, we will look at how to achieve equivalent results using OpenAI API.\n\nVectorizing with OpenAI API Creating a prompt for OpenAI API usually takes more effort than using ChatGPT to get comparable results. This approach is most suitable when we want to produce multiple new implementations at scale, such as converting many for loops to vectorized NumPy expressions.\n\nTo create a system prompt, we can adapt our ChatGPT prompt structure into two components:\n\nSURROUND = \"\"\"You are provided with:\n\n1. A Python function implementation enclosed with {{{ FUNCTION }}}\n\n2. Lines to be refactored enclosed with {{{ OLD }}}\n\n3. A library to be used in the new code enclosed with {{{ LIBRARY }}}.\"\"\"\n\nSINGLE_TASK = \"Your task is to return a new implementation for the old lines using the specified library.\"\n\nLINES = \"\"\"dist_2 = 0\n\nfor i in range(len(a)):\n\nfor j in range(len(a[i])):\n\ndist_2 += (a[i][j] - b[i][j]) ** 2\n\n\"\"\"\n\nWe can define a get_user_prompt function as such:\n\ndef get_user_prompt(func: callable, library: str, lines: str) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nChapter 10\n\nLINES: {{{{{{ {lines} }}}}}}\n\nLIBRARY: {{{{{{ {library} }}}}}}\n\nREFACTORD:\n\n\"\"\"\n\nWe can then use both prompts when calling the service:\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(get_euclidean_distance, \"NumPy\", LINES)\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n],\n\n)\n\nprint(\"Explanation:\", completion.choices[0].message.content)\n\nFor reference, the full script is available in ch10/prompts/performance_refactoring_openai. py. The output we obtained was equivalent to the result from ChatGPT.\n\nTo conclude, GitHub Copilot is the quickest way to get performance refactoring suggestions. ChatGPT is more suitable when we want widely adopted implementations or to quickly assess different libraries’ implementations with a reusable prompt. OpenAI API is helpful in less com- mon cases when we want to generate bulk transformations across many files or a large code base.\n\nMore advanced performance refactoring In Part 3 of the book, Chapter 14 will address runtime and space complexity optimization. We will demonstrate how combining code completion tools with prompt engineering techniques such as chaining can help us find the right balance between space and time complexity.\n\n261\n\n262\n\nRefactoring Code with GenAI\n\nWe will use GitHub Copilot, ChatGPT, and OpenAI to assess the program’s current performance, estimate its runtime and space complexity in terms of Big-O notation, and introduce optimized solutions to extend input capacity with minimal trade-off. Stay tuned for a detailed exploration of these topics.\n\nSummary In this chapter, we explored how GitHub Copilot, ChatGPT, and OpenAI API can assist in rewriting code while preserving its original functionality. These tools can enhance readability, improve structure, and boost performance.\n\nClean code is a shared goal among software developers, especially as projects grow and become complex. Most IDEs already offer strong built-in tools for refactoring, which we frequently use. However, when those tools reach their limits, GenAI can step in. It can support structural refac- toring by using CoT techniques with the original implementation, or it can optimize performance through the five S’s framework.\n\nAlthough refactoring is useful, writing clean and structured code from the start is usually simpler. In the next chapter, we will introduce fine-tuning with OpenAI API as an alternative to few- shot learning. Fine-tuning enables us to adapt a pretrained model to a specific domain, such as generating clean and well-organized code. Unlike prompt engineering, fine-tuning separates the specialization process from the prompt itself, which can be beneficial for larger-scale projects.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: Why do we need GenAI for refactoring tasks if IDEs already provide built-in tools?\n\nAnswer: While built-in tools are powerful, they have limitations. For example, they can only ex- tract code blocks to external functions when the lines are written consecutively in a single block. GenAI allows us to restructure Python objects more flexibly, using the CoT technique with the original implementation as a reference.\n\nQuestion 2: How does the CoT approach differ when refactoring existing code compared to generating new code?\n\nChapter 10\n\nAnswer: When generating new code, we typically describe the high-level function we want GenAI to complete. For refactoring, we also include the original implementation in the prompt. This helps GenAI understand the intended functionality, such as parsing specific JSON request keys or assigning them to specific variable names.\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\nClean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin (aka Uncle Bob):\n\nhttps://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882\n\nCode smell: https://en.wikipedia.org/wiki/Code_smell\n\nRefactoring: Improving the Design of Existing Code, Second Edition: https://www.amazon.com/\n\nRefactoring-Improving-Existing-Addison-Wesley-Signature/dp/0134757599\n\nDesign Patterns: Elements of Reusable Object-Oriented Software: https://www.amazon.com/Design-\n\nPatterns-Elements-Reusable-Object-Oriented/dp/0201633612\n\nMariane Anaya. Clean Code in Python: Develop maintainable and efficient code, Second Edition, 2021:\n\nhttps://www.packtpub.com/en-us/product/clean-code-in-python-9781800560215\n\nKent Beck and Cynthia Andres, Extreme Programming Explained: Embrace Change, 2004: https://\n\nwww.amazon.com/Extreme-Programming-Explained-Embrace-Change/dp/0321278658\n\nIan Sommerville. Software Engineering, 2015: https://www.amazon.com/Software-Engineering-\n\n10th-Ian-Sommerville/dp/0133943038\n\n263",
      "page_number": 278
    },
    {
      "number": 11,
      "title": "Performance refactoring with GenAI Performance refactoring refers to changes made to the code that preserve its functionality while improving runtime or memory efficiency. One common approach is to us",
      "start_page": 291,
      "end_page": 301,
      "detection_method": "regex_chapter",
      "content": "264\n\nRefactoring Code with GenAI\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.\n\n11\n\nFine-Tuning Models with OpenAI\n\nConcluding Part 2, this chapter explores how to train large language models (LLMs) to specialize in completing a specific task. This is achieved by fine-tuning the model’s parameters, known as weights, through the OpenAI platform. This is quite like the prompt engineering strategy few- shot learning, sometimes referred to as in-context learning, which we introduced earlier in the book as a technique for helping the model to specialize in a particular domain. Fine-tuning, however, is best suited for larger-scale projects where we want to separate the specialization aspect from the actual prompt.\n\nWe will delve into a use case where this specialization of the model is required to produce code while avoiding comments clutter. We will examine how outputs from the GPT-4o mini model may be programmatically correct but fail to follow our style guidelines and requirements for concise explanations and code comments. We will construct a fine-tuning JSONL file with high-quality examples that illustrate desired outcomes, initiate a fine-tuning job through the OpenAI website, and compare the improved results with the output of the base model.\n\nThis chapter covers the following topics:\n\n\n\nFine-tuning LLMs toward a specialization\n\n\n\nExperimenting with the OpenAI Playground\n\n\n\nCrafting a JSONL file for fine-tuning\n\n\n\nFine-tuning a model with the OpenAI GUI\n\n\n\nComparing our results\n\n266\n\nFine-Tuning Models with OpenAI\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\n\n\nAn OpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\nFor instructions on setting up OpenAI API access, see Chapter 2. If you need help with creating an OpenAI account, refer to the Appendix for detailed guidance.\n\nFine-tuning LLMs toward a specialization When working on complex coding tasks with LLMs, we may need to employ prompt engineering techniques. In Chapter 8, we saw how including a few examples in the prompt implicitly instructs the model to follow the expected coding style when writing code. This show, don’t tell approach effectively guides the model by enriching our existing prompt, as illustrated in Figure 11.1:\n\nFigure 11.1: Few-shot learning prompt enrichment\n\nBy using this approach, the model adapts to produce outputs that align with the examples pro- vided, effectively specializing in the given task.\n\nChapter 11\n\nLLMs specialization A specialization for LLMs focuses on a specific task or a set of closely related tasks. Here are some examples of specializations you might want your model to develop:\n\nGenerating completion code that follows a specific coding style\n\n\n\nCreating a unit test suite that covers key scenarios such as happy paths, bad inputs, edge cases, and error handling\n\n\n\nAssessing the quality of a function’s docstring\n\n\n\nRefactoring for loops into comprehensions\n\nWe typically identify a specialization as either a single system prompt or a set of closely related system prompts, each incorporating the surrounding context and a single-task focus. These tasks are generally non-trivial for an LLM, such as code completion to reverse a string, requiring additional effort in prompt crafting, such as prompt engineering or fine-tuning.\n\nFor certain tasks, few-shot learning can be particularly effective. For example, if refactoring for loops into comprehensions never involves more than two levels of nesting, providing a few ex- amples in the prompt may be sufficient to achieve reliable results.\n\nHowever, for deeper specializations, such as breaking longer functions into helper functions to improve code quality, we may need to incorporate a larger set of training examples with the desired output, rather than just a few. We might include examples of naming conventions and indentation. Some examples may show the extraction of code blocks into helper functions, and others may show how to eliminate unnecessary explanations that come along with the code. Including all these examples directly in the prompt increases the prompt’s length and, therefore, the cost associated with the input token length. It also makes it harder to maintain.\n\nFine-tuning an LLM Fine-tuning a model is a process that aims to achieve better outputs by using a pre-trained model such as GPT-4o mini and training it toward a specialization using a small set of training exam- ples that depict the intended output. The fine-tuning process adjusts the model’s parameters (or weights, as discussed in Chapter 6) to achieve the desired outcomes shown in the provided examples. This specialization process is shown in Figure 11.2:\n\n267\n\n268\n\nFine-Tuning Models with OpenAI\n\nFigure 11.2: Leveraging fine-tuning for a better output\n\nThe specialization process uses the crafted training examples to fine-tune the LLM, resulting in a new LLM that is ready to be prompted without requiring further examples in the prompts.\n\nFavoring fine-tuning over prompt engineering Based on studies, and confirmed with our experience, fine-tuning requires more training examples than few-shot learning because the examples are retraining the model’s parameters, rather than simply being part of the prompt. In this chapter, we use 15 examples, which is slightly above the OpenAI API’s minimum of 10 for initiating a fine-tuning job. For more complex cases, tens or even hundreds of examples may be necessary, which can be time-consuming to find.\n\nFortunately, for most of our daily coding tasks, we will find that few-shot learning is sufficient where only a few examples are needed. However, for larger-scale projects, or when few-shot learning does not meet our needs, fine-tuning may be a better alternative. Generally, it is good practice to begin with the few-shot technique and turn to fine-tuning only when the results are not satisfactory.\n\nTask specializing in cleaner code In Chapter 7, we noted that LLMs often include explanations and example usage along with the code. The LLMs also add code comments when function implementations become longer to compensate for less readable code. When working with higher-quality code, we may prefer the model to return only the code without extra explanations. Additionally, rather than relying on comments in lengthy functions, we might want the model to break the logic into smaller, more manageable functions.\n\nChapter 11\n\nThis is a perfect use case for fine-tuning since the effort invested in creating a training file of examples is justified as it can be applied to an entire repository using short prompts alone. Later in the book, we will explore how fine-tuning supports advanced refactoring initiatives, such as extracting logging, monitoring, and error-handling responsibilities into separate functions.\n\nNext, we will explore how to experiment with the OpenAI API Playground and observe that the GPT-4o mini model requires improvements in its output implementation. Later in the chapter, we will address these improvements through the fine-tuning process.\n\nExperimenting with the OpenAI API Playground In this section, we will use OpenAI’s graphical user interface (GUI) Playground to quickly view our results for a given preset, which represents a combination of a model and the selected call parameters. This interface is an excellent tool for rapid experimentation and for comparing models before implementing them programmatically in our software.\n\nTo access the Playground, ensure that your OpenAI account has credits applied to unlock tier 1, as explained in Chapter 2. Then, go to https://platform.openai.com/playground/chat. You can also reach it by navigating to the home page, https://platform.openai.com, selecting Play- ground from the top-right menu, and then choosing the Chat option from the left menu. The opening screen of the Playground is shown in Figure 11.3:\n\nFigure 11.3: Opening screen of the Playground interface for OpenAI\n\n269\n\n270\n\nFine-Tuning Models with OpenAI\n\nOn this page, we can craft prompts for OpenAI, as introduced in Chapter 2, and try different pre- sets. This includes the system and user prompts, along with parameters such as temperature and maximum tokens. Additional parameters include the following:\n\n\n\nTop P is an alternative to the temperature, setting a minimal probability threshold that the model’s output must meet. In some cases, higher values can prevent hallucinations.\n\n\n\nThe Store logs checkbox saves the prompt so it can be restored later.\n\nWe can also modify the response format and add functions. These topics will be discussed in detail in later chapters of this book.\n\nNote that the experimentation in the Playground is free for tier 1 or higher accounts, but not all models are available. For instance, as of April 2025, GPT-o1 and GPT-o1-preview models are not available in the reasoning models for experimentation. Figure 11.4 shows the available models.\n\nFigure 11.4: Available reasoning models in the OpenAI API Playground\n\nChapter 11\n\nFor the examples in this chapter, we will use the GPT-4o mini 2024-07-18 model, which is cheaper to use with the API compared to the GPT-o series models.\n\nComputing quadratic roots with GPT-4o mini As a benchmark for the model output, we will evaluate how GPT-4o mini handles implementing functions that extend beyond a few lines of code, such as computing the quadratic roots of the equation . To find the roots of this equation, we compute the two values that satisfy the quadratic formula:\n\n2\n\n𝑎𝑎𝑎𝑎\n\n+ 𝑏𝑏𝑎𝑎 + 𝑐𝑐 𝑐 𝑐\n\n2\n\n\n\n−𝑏𝑏 𝑏 √𝑏𝑏 2𝑎𝑎\n\n− 4𝑎𝑎𝑎𝑎\n\n𝑥𝑥1,2 =\n\nIn our case, we want to return the roots only when they are real, which occurs when the discrim- inant\n\n is larger than or equal to 0.\n\n2\n\nTo obtain a function implementation that returns the real roots only, we define the function signature:\n\n𝑏𝑏\n\n− 4𝑎𝑎𝑎𝑎\n\ndef get_quadratic_roots_only_if_real (a:int, b:int, c:int) -> Tuple[float, float]\n\nWe will use this function signature in our prompt to the model. As usual, we will structure our prompt according to the five S’s framework, as introduced in Chapter 4, where the system prompt provides the surrounding context and specifies the single task to be completed, and the user prompt includes supplementary details, such as the function signature and a lead-in cue.\n\nFor the system prompt, we will add the following instruction:\n\nYou will be provided with a Python function signature enclosed with {{{ FUNCTION }}}. Your task is to implement it.\n\nThis instruction provides the model with sufficient context for the code completion task using a Python function signature.\n\nFor the user prompt, we will ask it to implement the get_quadratic_roots_only_if_real func- tion and include the CODE lead-in cue:\n\nFUNCTION: {{{def get_quadratic_roots_only_if_real (a:int, b:int, c:int) -> Tuple[float, float] }}}\n\nCODE:\n\n271\n\n272\n\nFine-Tuning Models with OpenAI\n\nThe function’s name, get_quadratic_roots_only_if_real, signals the model to return the qua- dratic roots only if they are real.\n\nImplementing the quadratic roots calculator A typical approach to computing quadratic roots and returning them if they are real involves the following steps:\n\n1. Compute the discriminant\n\n.\n\n2 2. Verify that the discriminant is positive so that the roots are real.\n\n𝑏𝑏 3. Return the two roots if real.\n\n− 4𝑎𝑎𝑎𝑎\n\nSince this function requires more than just a few lines of code, the LLM is likely to introduce comments within the code and include surrounding explanations with the implementation.\n\nFor reference, Figure 11.5 shows the explanation provided by the model before the actual code implementation.\n\nFigure 11.5: Base model output with supplementary explanations\n\nChapter 11\n\nThe following is the code output generated by the model:\n\n```python\n\nimport math\n\nfrom typing import Tuple\n\ndef get_quadratic_roots_only_if_real(a: int, b: int, c: int) -> Tuple[float, float]:\n\n# Calculate the discriminant\n\ndiscriminant = b * b - 4 * a * c\n\n# If discriminant is negative, there are no real roots\n\nif discriminant < 0:\n\nraise ValueError(\"The equation does not have real roots.\")\n\n# Calculate the two solutions using the quadratic formula\n\nsqrt_disc = math.sqrt(discriminant)\n\nx1 = (-b + sqrt_disc) / (2 * a)\n\nx2 = (-b - sqrt_disc) / (2 * a)\n\nreturn (x1, x2)\n\nThis implementation covers all three required steps: computing the discriminant, checking for real roots, and returning the two solutions when appropriate. However, the solution includes redundant explanations and a function that is cluttered with comments. Later in the chapter, we will see how fine-tuning can help us achieve improved results by fine-tuning the LLM to return only code for code completion tasks.\n\nNext, we will see how to prepare the training data to fine-tune the GPT-4o mini model. We will later compare this model with those of the default GPT-4o mini model.\n\nCrafting a JSONL file for fine-tuning One of the initial challenges in fine-tuning a model is gathering enough training examples. When fine-tuning a model through OpenAI, a minimum of 10 training examples is required. However, for most tasks that are beyond trivial, tens or even hundreds of examples might be necessary, depending on the task. According to OpenAI’s official documentation, satisfactory results are typically achieved with 50 training examples.\n\n273\n\n274\n\nFine-Tuning Models with OpenAI\n\nIn our case, where we want to fine-tune a model to produce outputs consisting solely of code without comments, 15 examples are sufficient, as we will see later in the chapter. When crafting our training set, we will utilize two techniques: desirable examples that are like few-shot learning and contrastive learning. Contrastive learning builds on the idea of in-context learning by first presenting an incorrect or incomplete output, followed by an additional user-assistant prompt pair that provides the desired output. For more details, see the Further reading section in this chapter.\n\nFine-tuning file structure We will upload a JSONL file as the training data for our fine-tuning process, where each line in the file is a valid JSON object representing one training example. A minimal prompt should include a system prompt and a training example with a user prompt and an assistant prompt. Each as- sistant prompt is attributed with a weight of 0 for undesirable output and 1 for desirable output. In the following desirable example, the assistant prompt has a weight of 1, and line breaks have been added for clarity:\n\n{\"messages\": [\n\n{\"role\": \"system\", \"content\": \"You will be provided with a Python function signature enclosed with {{{ FUNCTION }}}. Your task is to implement it.\"},\n\n{\"role\": \"user\", \"content\": \"FUNCTION: {{{def get_euclidean_distance(x1, y1, x2, y2):}}}\\n CODE: \"},\n\n{\"role\": \"assistant\", \"content\": \"def get_euclidean_distance(x1, y1, x2, y2): \\n return ((x1-x2)**2 + (y1-y2)**2)**0.5\", \"weight\": 1}\n\n]}\n\nThis format may look familiar, as it resembles the messages parameter used in OpenAI API calls. However, in this case, we assign a weight to the assistant message. The preceding example is a training instance included in the fine_tuning.jsonl file for this chapter, located at ch11/fine_ tuning.jsonl, demonstrating the expected implementation of the get_euclidean_distance function.\n\nA minimal training line template should be as follows, containing at least one user prompt and one assistant prompt. Here, too, line spaces are added for clarity:\n\n{\"messages\": [\n\n{\"role\": \"user\", \"content\": \"Your Content Here\"},\n\n{\"role\": \"assistant\", \"content\": \"Your desired output here\", \"weight\": 1}]}",
      "page_number": 291
    },
    {
      "number": 12,
      "title": "LLMs specialization A specialization for LLMs focuses on a specific task or a set of closely related tasks. Here are some examples of specializations you might want your model to develop:",
      "start_page": 302,
      "end_page": 329,
      "detection_method": "regex_chapter",
      "content": "Chapter 11\n\nWe could also incorporate a system prompt if it helps refine the specialization, as we will explore later in the chapter.\n\nWith contrastive learning examples, we will include additional user prompt and assistant prompt pairs. The first pair represents an undesirable output and is assigned a weight of 0, as such:\n\n{\"messages\": [\n\n{\"role\": \"system\", \"content\": \"...omitted for brevity...\"},\n\n{\"role\": \"user\", \"content\": \"FUNCTION: {{{def sum_of_squares(n: int) -> int:}}}\\n CODE: \"},\n\n{\"role\": \"assistant\", \"content\": \"Sure! here is the code:\\n\\n```python\\ ndef sum_of_squares(n: int) -> int:\\n return sum(i * i for i in range(1, n + 1))\\n```\", \"weight\": 0},\n\n{\"role\": \"user\", \"content\": \"Include only code.\"},\n\n{\"role\": \"assistant\", \"content\": \"def sum_of_squares(n: int) -> int:\\n return sum(i * i for i in range(1, n + 1))\", \"weight\": 1}]}\n\nIn this example, we provide two sets of user and assistant messages. The initial assistant response is undesirable because it includes explanatory text alongside the code, so it is assigned a weight of 0. The second user prompt corrects this by explicitly instructing the assistant to provide only the code, and this response is given a weight of 1.\n\nStrategy A practical approach to constructing a fine-tuning training file is to start with the simplest cases of desired outputs and gradually introduce more complex examples to cover the specialization we want the model to develop. While the few-shot examples we saw in Chapter 8 included one or two cases, for fine-tuning, we will use 15 examples, allocating 7-8 examples to each of the following objectives:\n\n\n\nCode-only: Positive and contrastive\n\nNo comments: Positive and contrastive\n\nThese examples will be sufficient to generate code without surrounding explanations and with- out comments within the function. The complete JSONL file is available in this book’s repository at ch11/fine_tuning.jsonl. Next, we will present a single example from each of the preceding objectives.\n\n275\n\n276\n\nFine-Tuning Models with OpenAI\n\nCode-only – five training examples In these cases, we will include simple implementations of functions spanning 1-2 lines. Each example consists of a single user and assistant prompt pair, ensuring that the output consists solely of code. Take the following example:\n\n{\"messages\": [\n\n{\"role\": \"system\", \"content\": \"You will be provided with a Python function signature enclosed with {{{ FUNCTION }}}. Your task is to implement it.\"},\n\n{\"role\": \"user\", \"content\": \"FUNCTION: {{{def get_arithmetic_mean(a, b)}}}\\n CODE: \"},\n\n{\"role\": \"assistant\", \"content\": \"def get_arithmetic_mean(a, b): \\n return (a+b)/2\", \"weight\": 1}]}\n\nIn this example, we provide a desirable implementation of get_arithmetic_mean without any surrounding explanations. Note that the actual JSONL line contains no extra line breaks. This is an example of the training pattern with just a single positive example indicated by the weight 1. Additionally, we will include three more examples for the get_euclidean_distance, reverse_ string, is_even, and is_odd functions.\n\nContrastive code-only – three training examples In addition to providing high-quality examples, we can also include contrastive examples. These scenarios involve an initial assistant response that is undesirable, such as including explanations or comments in the code, followed by a chained request to refine the output into a more desirable version. The chained user prompt should follow the best practices introduced in Chapter 9, such as the following:\n\n\n\nInclude code only\n\n\n\nRemove code comments\n\n\n\nExtract lower-level details to functions\n\nAs introduced in Chapter 9, when crafting chained prompts, we avoid unnecessary phrasing such as “Your answer is wrong!” since it does not follow the short best practice and adds no value. Instead, we should determine whether the assistant’s output is significantly incorrect or if only minor adjustments are needed.\n\nChapter 11\n\nIn the first case, if the output is far off, the user prompt should be refined to produce a more desir- able result, and this example should not be included in our training data. In the second case, if the model’s output only requires minor refinements, we should focus on providing clear guidance on the necessary adjustments, such as instructing the model to return only code or extract low-level details into helper functions. This type of direction is more precise and informative.\n\nFor example, the following is a contrastive code-only training example for an implementation of is_palindrome, where the first assistant prompt includes an explanation along with the code. This output is marked with a weight of 0 to indicate undesirability. It is followed by a chained user prompt instructing the assistant to return only the code, leading to a refined implementation with a weight of 1:\n\n{\"messages\": [\n\n{\"role\": \"system\", \"content\": \"...omitted...\"},\n\n{\"role\": \"user\", \"content\": \"FUNCTION: {{{def is_palindrome(s: str) -> bool:}}}\\nCODE: \"},\n\n{\"role\": \"assistant\", \"content\": \"A palindrome is a word, phrase, or [...] otherwise `False`.\\n\\n```python\\ndef is_palindrome(s: str) -> bool:\\n s = s.lower()\\n return s == s[::-1]\\n```\", \"weight\": 0},\n\n{\"role\": \"user\", \"content\": \"Include code only.\"},\n\n{\"role\": \"assistant\", \"content\": \"def is_palindrome(s: str) -> bool:\\n s = s.lower()\\n return s == s[::-1]\", \"weight\": 1}]}\n\nThe final assistant message consists solely of the implementation code and is assigned a weight of 1 to indicate a desirable output. The fine-tuning file also includes similar contrastive examples for sum_of_squares and flatten_list_of_lists.\n\nCode with no comments – five examples In these examples, we focus on functions that go beyond simple 1-2 line implementations, breaking down more complex tasks into lower-level abstraction functions. This approach enhances read- ability in the main function and eliminates the need for inline comments. For instance, consider the implementation of get_reversed_longer_string, which takes two strings as input and returns the reverse of the longer string. It can be structured as follows:\n\ndef get_reversed_longer_string(s1:str, s2: str) -> str:\n\nlonger_s = get_longer_string(s1, s2)\n\nreversed_longer_s = reverse_string(longer_s)\n\nreturn reversed_longer_s\n\n277\n\n278\n\nFine-Tuning Models with OpenAI\n\ndef get_longer_string(s1, s2):\n\nreturn s1 if len(s1) > len(s2) else s2\n\ndef reverse_string(s):\n\nreturn s[::-1]\n\nWhile more concise implementations exist, they may be less readable. The preceding structure keeps get_reversed_longer_string clear and shifts the implementation details to dedicated lower-level functions.\n\nTo include this approach in our training examples, we will add the following:\n\n{\"messages\": [{\"role\": \"system\", \"content\": \"...omitted for brevity...\"}, {\"role\": \"user\", \"content\": \"FUNCTION: {{{def get_reversed_longer_ string(s1, s2):}}}\\n CODE: \"},\n\n{\"role\": \"assistant\", \"content\": \"def get_reversed_longer_string(s1, s2):\\n longer_s = get_longer_string(s1, s2)\\n reversed_longer_s = reverse_ string(longer_s)\\n return reversed_longer_s\\n\\ndef get_longer_string(s1, s2):\\n return s1 if len(s1) > len(s2) else s2\\n\\ndef reverse_string(s):\\n return s[::-1]\", \"weight\": 1}]}\n\nThis aligns with our preferred coding style of extracting lower-level functionality into separate functions. Similarly, we will include example implementations for the following:\n\n• • •\n\nget_max_multiplication_from_lists\n\nget_maximal_second_element_squared\n\nget_sum_of_absolute_differences\n\nget_highest_even\n\nThese five implementations will demonstrate the separation of abstraction levels in functions, serving as a guideline for the fine-tuning process.\n\nContrastive no comments – two examples To complete our dataset, we will include examples of initial implementations with excessive comments, followed by a refined version where low-level details are extracted into separate functions. For instance, consider a function that, given a list of lists, computes the average of the second elements of each sublist:\n\nget_average_of_second_elements(numbers: List[List[int]])\n\nChapter 11\n\nThis function involves more complex logic than our previous examples, which may lead to an implementation with excessive comments. To steer the model toward a solution that separates levels of abstraction, we first provide an implementation with inline comments, followed by a chained prompt that refines the output. An initial incorrect assistant response might look like this:\n\ndef get_average_of_second_elements(numbers: List[List[int]]) -> float:\n\nif not numbers: # Check if the input list is empty\n\nreturn 0\n\nsecond_elements = [sublist[1] for sublist in numbers] # Extract second elements\n\nreturn sum(second_elements) / len(second_elements) # Compute the average\n\nWhile this implementation is correct, the inline comments add unnecessary fluff. When including this negative example in our dataset, we assign it a weight of 0.\n\nNext, we chain a user prompt requesting a refinement:\n\nExtract low level details to functions\n\nThis results in a cleaner implementation:\n\nimport statistics\n\ndef get_average_of_second_elements(numbers: List[List[int]]) -> float:\n\nsecond_elements = get_second_elements(numbers)\n\nreturn statistics.mean(second_elements)\n\ndef get_second_elements(numbers: List[List[int]]) -> List[int]:\n\nreturn [sublist[1] for sublist in numbers]\n\nBy abstracting the extraction logic into a separate function, get_average_of_second_elements remains more readable. The corresponding example looks like this:\n\n{\"messages\": [{\"role\": \"system\", \"content\": \"...omitted for brevity...\"}, {\"role\": \"user\", \"content\": \"FUNCTION: {{{def get_average_of_second_ elements(numbers: List[List[int]]):}}}\\n CODE: \"}, {\"role\": \"assistant\", \"content\": \"def get_average_of_second_elements(numbers: List[List[int]]) -> float:\\n if not numbers: # Check if the input list is empty\\n return 0\\n \\n second_elements = [sublist[1] for sublist in numbers] # Extract second elements\\n return sum(second_elements) / len(second_elements) # Compute the average\", \"weight\": 0},\n\n{\"role\": \"user\", \"content\": \"Extract low level details to functions\"},\n\n{\"role\": \"assistant\", \"content\": \"import statistics\\n\\n\\ndef get_average_\n\n279\n\n280\n\nFine-Tuning Models with OpenAI\n\nof_second_elements(numbers: List[List[int]]) -> float:\\n second_elements = get_second_elements(numbers)\\n return statistics.mean(second_elements)\\ n\\n\\ndef get_second_elements(numbers: List[List[int]]) -> List[int]:\\n return [sublist[1] for sublist in numbers]\", \"weight\": 1}]}\n\nAdditionally, we will include a similar contrastive example for get_smallest_squared, bringing our total to 15 training examples.\n\nWith the finalized JSONL dataset, we can now proceed with fine-tuning using OpenAI’s GUI.\n\nFine-tuning a model with the OpenAI GUI Fine-tuning an LLM with OpenAI can be done either programmatically via an API call or through OpenAI’s GUI. In this chapter, we will focus on using the GUI to initiate fine-tuning jobs.\n\nCosts of fine-tuning a model Fine-tuning an LLM comes with two types of costs:\n\n\n\nJob cost: As of April 2025, fine-tuning a model costs $3 per 1M tokens. For reference, the JSONL file provided earlier, which contains approximately 12K tokens (2K tokens in the file  6 iterations), would cost around $0.04 to fine-tune. There are no associated storage costs for the model or training files.\n\n×\n\n\n\nApplication: The pricing of using a fine-tuned model depends on the base model used. For instance, the cost for GPT-4o mini-based models is $0.30 per 1M input tokens and $1.20 per 1M output tokens, and much cheaper than the GPT-4o models, as illustrated in Figure 11.6:\n\nFigure 11.6: Fine-tuning costs for GPT-4o models\n\nChapter 11\n\nAs of April 2025, using a fine-tuned GPT-4o mini model costs exactly twice as much as using the model without fine-tuning. However, while the per-token cost is higher, a fine-tuned model can be more efficient, as its specialization allows for more concise inputs and outputs, potentially reducing overall costs.\n\nTo check the latest pricing for current and legacy models, visit OpenAI’s pricing page: https:// openai.com/api/pricing/.\n\nWith these costs in mind, we can now proceed with initiating the fine-tuning process.\n\nInitiating a fine-tuning job We can fine-tune a model using our JSONL file directly through OpenAI’s API GUI. While fine-tun- ing can also be done programmatically, we will focus on the GUI method since it is by far the most common way.\n\nTo initiate the job, go to OpenAI’s platform home page at https://platform.openai.com/, click on Dashboard in the upper-right menu, and then select Fine-tuning from the left-hand menu, as shown in Figure 11.7:\n\nFigure 11.7: Fine-tuning page in the OpenAI API platform\n\n281\n\n282\n\nFine-Tuning Models with OpenAI\n\nTo start a new fine-tuning job, click the Create button in the upper-right corner of the screen. This will open the setup popup, as shown in Figure 11.8:\n\nFigure 11.8: Fine-tuning job popup\n\nWe used the following settings for fine-tuning:\n\nMethod: Supervised mode. Based on our experience, this is the most effective approach\n\nfor coding style specialization.\n\n\n\nBase Model: gpt-4o-mini 2024-07-18 (or the latest available mini model). This model provides sufficient results for our use case and is cost-effective.\n\n\n\nTraining data: The JSONL file introduced earlier in the chapter.\n\n\n\nSuffix (optional): This is appended to the model’s name for identification and does not need to be unique across fine-tuning jobs. We added the clean-code suffix for future reference.\n\nChapter 11\n\nNow, we can start the fine-tuning job by clicking Create.\n\nWith 15 training examples, the process should take about five minutes to complete. Once finished, a success message will appear, as shown in Figure 11.9:\n\nFigure 11.9: Successful completion of fine-tuning job\n\nThe fine-tuning job processed 12,500 tokens, running six iterations (or epochs) of our file, which contains approximately 2,000 tokens. The fine-tuned model can be used in any OpenAI API call or accessed through the Playground, as we will explore later in this chapter.\n\nTo monitor the cost of the fine-tuning job, navigate to the OpenAI usage page at https://platform. openai.com/usage, scroll down to Spend categories, and review the fine-tuning expenses, as shown in Figure 11.10:\n\nFigure 11.10: Incurred fine-tuning costs\n\n283\n\n284\n\nFine-Tuning Models with OpenAI\n\nHere, we can see that fine-tuning the model with 15 training examples and 12K tokens resulted in a total cost of $0.04.\n\nUsing the fine-tuned model The fine-tuned model can be accessed either through the OpenAI Playground or programmati- cally. In Figure 11.9, OpenAI gave a formal name for our fine-tuned model under Output model, in the following format:\n\nft:{base model}:{account name}:{fine-tuning suffix}:{id}\n\nFor example, our model’s name is shown here, but your model will be named differently:\n\nft:gpt-4o-mini-2024-07-18:pazpaz-the-coder:clean-code:B5Ayo5q1\n\nTo use the fine-tuned model in code, replace the standard model reference:\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[{...]))\n\nThe updated call using the fine-tuned model will look as follows:\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"ft:gpt-4o-mini-2024-07-18:pazpaz-the-coder:clean- code:B5Ayo5q1\",\n\nmessages=[{...] ))\n\nThe fine-tuned model is also available in the OpenAI Playground, as shown in Figure 11.11:\n\nFigure 11.11: Playground models, including our fine-tuned model\n\nChapter 11\n\nWithin the Playground interface, we can compare its results with the base model, which we will explore next.\n\nComparing our results Now that we have fine-tuned our custom model, we can evaluate how its outputs compare to those of the base model. To do this, navigate to the OpenAI Playground, https://platform. openai.com/playground/chat, select gpt-4o-mini-2024-07-18 as the base model, and click on Compare, as shown in Figure 11.12:\n\nFigure 11.12: Playground comparison selecting the base model to compare\n\nNext, add the fine-tuned model, as shown in Figure 11.13:\n\nFigure 11.13: Adding a model for comparison\n\nWe will use the same system message from earlier in the chapter that will be applied to both models:\n\nYou will be provided with a Python function signature enclosed with {{{ FUNCTION }}}. Your task is to implement it.\n\n285\n\n286\n\nFine-Tuning Models with OpenAI\n\nNext, we will enter the same user message as earlier in the chapter:\n\nFUNCTION: {{{def get_quadratic_roots_only_if_real(a:int, b:int, c:int) -> Tuple[float, float] }}}\n\nCODE:\n\nOur comparison can be run by clicking Run, generating responses from both models. The base model’s output includes explanations and inline comments, while the fine-tuned model provides a more structured implementation, as shown in Figure 11.14:\n\nFigure 11.14: Comparing the base model (left) and the fine-tuned model (right)\n\nThe fine-tuned model returns a cleaner implementation that abstracts computations into sep- arate functions, keeping the main function concise and readable. For your reference, here is the full implementation proposed:\n\ndef get_quadratic_roots_only_if_real(a: int, b: int, c: int) -> Tuple[float, float]:\n\ndiscriminant = calculate_discriminant(a, b, c)\n\nif discriminant < 0:\n\nraise ValueError(\"Complex roots\")\n\nreturn calculate_quadratic_roots(a, b, discriminant)\n\ndef calculate_discriminant(a: int, b: int, c: int) -> float:\n\nreturn b**2 - 4*a*c\n\ndef calculate_quadratic_roots(a: int, b: int, discriminant: float) -> Tuple[float, float]:\n\nChapter 11\n\nroot1 = (-b + discriminant**0.5) / (2*a)\n\nroot2 = (-b - discriminant**0.5) / (2*a)\n\nreturn root1, root2\n\nThis output aligns with our fine-tuning objective of separating computations into helper functions while keeping the primary function, get_quadratic_roots_only_if_real, clear. Additionally, the base model generates 535 tokens, compared to 179 tokens from the fine-tuned model. Despite the higher per-token cost of the fine-tuned model, it is ultimately more cost-efficient.\n\nTo conclude, we successfully fine-tuned the model to specialize in returning clean, code-only outputs. This targeted specialization was achieved with 15 JSONL examples, using a combination of positive and contrastive learning to guide the model’s behavior.\n\nFine-tuning enables us to adapt a pre-trained LLM for specific tasks, making it especially valu- able for automated and large-scale coding projects. In later chapters, we will explore ways to further improve fine-tuning, such as refining the training data to separate validation logic from core functionality.\n\nFine-tuning in action Currently, we use just two fine-tuned models for coding tasks. The first is for code implementation as we constructed in this chapter, providing an alternative perspective on new implementations. The other is for docstring maintenance, ensuring that a function’s code remains consistent with its docstring. For more narrowly scoped tasks, few-shot learning is usually sufficient and prefer- able since it requires less effort. In general, fine-tuning is only considered when simpler prompt engineering techniques are ineffective or when the project operates at a repository-wide scale, making the investment worthwhile.\n\nWe encourage you to experiment frequently with prompt engineering and fine-tuning. In the next section of the book, we will introduce additional software development life cycle (SDLC) use cases that you can start applying to your code base right away.\n\nSummary In Part 2 of the book, we explored LLMs in greater depth. We explained how they work, what they excel at, and how to leverage prompt engineering techniques to achieve more effective results. We also covered strategies for evaluating their outputs to ensure reliability.\n\n287\n\n288\n\nFine-Tuning Models with OpenAI\n\nThis chapter took the concept of few-shot learning a step further by demonstrating how to fine- tune an LLM to specialize on a given task. Through positive and contrastive training examples, we guided the model to generate function implementations based solely on their signatures, returning clean code without inline comments. This approach can be applied more broadly to tasks such as generating unit test suites, maintaining docstring quality, or refactoring for loops across an entire repository.\n\nWith this deeper understanding of LLMs, prompt engineering, and output evaluation, we now have the essential tools to become supercharged coders. We can determine the best tool for a given task, whether ChatGPT, OpenAI API, or Copilot, and craft prompts that maximize effectiveness using best practices and advanced techniques. Moreover, we now have the skills to assess the quality of the model’s output with confidence.\n\nIn the third part of the book, we will apply this knowledge to advanced SDLC tasks. We will explore how GenAI can assist in areas such as documentation, testing, scaling applications for runtime and memory efficiency, logging, monitoring, and error handling. Some tasks will rely on the five S’s framework for coding-related tasks, while others will require more advanced prompt engineering strategies or fine-tuning to achieve optimal results.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: When is fine-tuning a good approach?\n\nAnswer: Fine-tuning is beneficial for large-scale projects where a repetitive task needs to be performed consistently, such as generating function implementations from signatures while following specific coding standards. Instead of relying on few-shot learning or complex prompt engineering techniques within each request, fine-tuning allows the specialization process to be separate from the prompt, making it easier to maintain and, in some cases, more cost-effective.\n\nQuestion 2: What are the costs involved in fine-tuning?\n\nAnswer: Fine-tuning involves two types of costs:\n\n\n\nTraining cost: A one-time cost incurred when running the fine-tuning job.\n\nUsage cost: Fine-tuned models typically have higher per-token costs for input and output compared to their base models. For example, with GPT-4o mini, using fine-tuned models costs twice as much as those from the base model.\n\nChapter 11\n\nHowever, the true cost of fine-tuning lies in the time and effort required to carefully construct training examples that ensure the model aligns with the desired specialization.\n\nQuestion 3: Why does fine-tuning require more training samples than few-shot learning?\n\nAnswer: The specialization process in fine-tuning is fundamentally different from few-shot learn- ing. In few-shot learning, examples are simply provided within the prompt, guiding the model without altering its internal parameters.\n\nFine-tuning, on the other hand, adjusts the model’s weights to align with the desired behavior. Given that the base model was pre-trained on vast amounts of data, requiring tens or even hun- dreds of fine-tuning examples is relatively modest in comparison to the scale of its original training.\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. OpenAI. Retrieved from https://arxiv.\n\norg/abs/2005.14165v4\n\nDeepchecks on contrastive learning: https://www.deepchecks.com/glossary/\n\ncontrastive-learning/\n\nOpenAI API fine-tuning: https://www.datacamp.com/tutorial/fine-tuning-large-\n\nlanguage-models\n\nOpenAI API request parameters: https://platform.openai.com/docs/api-reference/\n\nresponses/create\n\nData camp fine-tuning tutorial for LLMs: https://www.datacamp.com/tutorial/fine-\n\ntuning-large-language-models\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n289\n\nPart 3\n\nFrom Code to Production with GenAI\n\nIn Part 3 of this book, we branch from simple coding to the full software development life cycle (SDLC). We first focus on writing docstrings, followed by testing, since these are almost universally required for production-ready code. The next advanced topic is scaling applications for systems in terms of runtime and memory. The part will also cover logging, monitoring, and error handling.\n\nThe techniques for completing these SDLC steps build on both the five S’s framework and ad- vanced prompt engineering techniques discussed in Part 2.\n\nThe final chapter examines the current trends, including vibe coding, and suggests what might be next for the field of GenAI.\n\nThis part contains the following chapters:\n\nChapter 12, Documenting Code with GenAI\n\nChapter 13, Writing and Maintaining Unit Tests\n\nChapter 14, GenAI for Runtime and Memory Management\n\nChapter 15, Going Live with GenAI: Logging, Monitoring, and Errors\n\nChapter 16, Architecture, Design, and the Future\n\n12\n\nDocumenting Code with GenAI\n\nIn Part 3 of the book, we will introduce advanced practices from the software development life- cycle (SDLC) when working with ChatGPT, OpenAI API, and GitHub Copilot. Using the five S’s framework introduced in Chapter 4, and prompt engineering techniques introduced in Chapter 8 and Chapter 9, we will see how to productionize our Python applications.\n\nIn this chapter, we will introduce docstrings for Python objects, which already has a built-in slash command (i.e., /doc) with GitHub Copilot. We can use this slash command with the vanilla format or extend it with specific docstring requirements. Docstrings are already a feature of most IDEs as a template structure where arguments will be formatted with a fixed style, but the standard template does not include any descriptive information about the function or its parameters beyond their name. GitHub Copilot will usually produce a full docstring for high-quality code.\n\nThe topics covered in this chapter include the following:\n\n\n\nIntroducing software documentation\n\nUsing GenAI to write docstrings\n\n\n\nFinding outdated docstrings with GitHub Copilot, ChatGPT, and OpenAI API\n\n\n\nPractical insights\n\n294\n\nDocumenting Code with GenAI\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nIntroducing software documentation Software engineers not only write a lot of code, but they also need to read and understand code written by others. The code could be written the same day by other developers or be part of a legacy system maintained over multiple decades. While clean, well-written code is a critical aspect, code documentation is also a fundamental way of communicating with other developers.\n\nSoftware documentation comes in multiple forms that serve different needs. Technical documen- tation includes the comments in source code, API descriptions, database schemas, and software architecture diagrams. User documentation is a guide on how an end user can use software. The software engineering process also adds specific documentation on requirements, specifications, testing plans, CI/CD pipelines, sprint planning, and much more. This chapter focuses on the documentation of the source code.\n\nA general guideline is that the documentation should focus on the “why” rather than the “what.” That is, the code is readable and one can figure out what it does, but the reason for its implemen- tation is what should be documented. The rationale is that well-written code is self-explanatory about what it does, but the reason for the code (the why) can be better understood through documentation. The code documentation can therefore explain the code, improve its readability, and aid later software maintenance.\n\nChapter 12\n\nIn Python, the convention for documentation within the code is the docstring. Docstrings typically occur at the beginning of Python modules, classes, and methods. The purpose is to explain their function and the reason for their inclusion. Python’s docstrings are enclosed in triple quotes (single or double quotes) even if they are just one line. However, for the most common docstrings used to document methods, they should include arguments, return types, and any exceptions raised.\n\nDocstrings serve a few different purposes. First, they provide documentation for a module, class, method, or function for developers. They can be compiled into formal documentation resources. In addition, these forms of documentation are accessible using Python’s help() function. Using docstrings makes the code easier to maintain, which reduces the lifetime cost of code.\n\nThe second purpose of docstrings is based on Miller’s Law, also known as the Magical Number Seven, Plus or Minus Two. Based on George Miller’s 1956 paper (https://psychclassics.yorku. ca/Miller/), it states that humans can retain 5 to 9 pieces of information in their working memory. A software developer trying to understand even a medium-length Python file must understand the interaction of several methods. The docstring and naming conventions facilitate a quick understanding of the abstracted methods.\n\nAlthough docstrings may appear to be standard, there are actually four common formats. The PEP 257 convention is the official Python standard, consisting of a single line with no string formatting that emphasizes readability. The Google style is among the most common and still emphasizes readability. It uses multi-line formatting for the arguments passed in, method returns, and exceptions raised. The NumPy/SciPy style is widely used, but primarily for scientific software. It has similar content to the Google style but uses different formatting. The reStructuredText (reST) or Sphinx style includes more detailed information than the others. This style is used by the Sphinx documentation generator, which produces HTML, PDF, and LaTeX documentation manuals directly from the source code with full hyperlinking and other advanced features. This chapter will focus on both single-line PEP 257 and Google styles due to their widespread use, but other styles are directly supported by requesting specific docstring styles. We have included a few sources in the Further reading section that describe comment styles for Python in more detail.\n\n295\n\n296\n\nDocumenting Code with GenAI\n\nWorking with the /explain GitHub Copilot command As a working example in this chapter, we use Flask routines that support the distance calculation found in Chapter 10. The base code is shown in the GitHub repository and copied for reference here:\n\nfrom flask import Flask, request, jsonify\n\nimport numpy as np\n\napp = Flask(__name__)\n\n@app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\ndata = request.get_json()\n\ndist_type = data.get(\"distance\")\n\na = np.asarray(data.get(\"df1\"))\n\nb = np.asarray(data.get(\"df2\"))\n\nif a.shape != b.shape:\n\nreturn jsonify({\"error\": \"Matrices must have the same shape\"})\n\nif dist_type == \"L1\":\n\ndist = np.sum(np.abs(a - b))\n\nreturn jsonify({\"distance\": dist})\n\nelif dist_type == \"L2\":\n\ndist = np.sqrt(np.sum((a - b) ** 2))\n\nreturn jsonify({\"distance\": dist})\n\nelse:\n\nreturn jsonify({\"error\": \"Invalid distance type\"})\n\nIn VS Code, by simply highlighting the method, right-clicking and selecting Copilot, and selecting Generate Docs as shown in Figure 12.1, a docstring can be automatically written. Note that we have ignored Generate Docstring shown under Copilot. That command is VS Code’s template approach for docstrings, which creates placeholders for the docstring and parameters but does not use any AI to write the comment or description.\n\nChapter 12\n\nFigure 12.1: By highlighting the method and right-clicking, Copilot generates a docstring\n\nThe generated docstring is shown in Figure 12.2. Trying this several times, we obtained several variations on the docstring, some of which had exceedingly long lines or different formats, and some included different information. Regardless, the docstring format has a simple one-line description, but the remainder has a random format that does not follow any of the style guides. Specifically, the “expects…” section of the docstring is unclear.\n\n297\n\n298\n\nDocumenting Code with GenAI\n\nFigure 12.2: GitHub Copilot produces a vague docstring for a poorly constructed function\n\nAnother common case is for GitHub Copilot to produce multiple lines of text as the docstring, excluding the parameters, returns, and so on. Figure 12.2 fails in this respect, with two lines of explanation followed by the expects (This endpoint expects...) and returns sections. While both multi-line and one-line docstrings exist, the preference is for one line rather than an elab- orate description, particularly for low-level methods. If such a method requires multiple lines, it is often considered good practice to refactor the method into multiple simpler methods that each perform a single task.\n\nCopilot’s docstring creation is usually quite good, but can occasionally be incorrect. As in the previous example, you will encounter cases where it does not perform well. Some of these cases are due to the complexity of the underlying code. Rather than fixing the docstring, a preferred solution would be to refactor the code into a simpler form and recreate docstrings. In many cases, the /explain function is quite informative. In Figure 12.3, we apply /explain to the same code by using the GitHub Copilot Editor Inline Chat from within the same function:\n\nChapter 12\n\nFigure 12.3: Using /explain for calculate_distance describes a complex method\n\nA similar functionality occurs within the GitHub Copilot Chat with the /explain calculate_ distance() command, which often returns explanations that are more verbose. Although both provide a clear explanation, their content is technically complex, which is why we recommend refactoring the code as shown:\n\nimport numpy as np\n\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\na, b, dist_type = parse_request_parameters(request)\n\ndist_func = {\"L1\": get_manhattan_dist, \"L2\": get_euclidean_dist}. get(dist_type)\n\ndist = dist_func(a, b)\n\nreturn jsonify({\"distance\": dist})\n\n299\n\n300\n\nDocumenting Code with GenAI\n\ndef parse_request_parameters(request):\n\ndata = request.get_json()\n\na = np.array(data.get(\"df1\",))\n\nb = np.array(data.get(\"df2\",))\n\ndist_type = data.get(\"distance\")\n\nreturn a, b, dist_type\n\ndef get_manhattan_dist(a: np.ndarray, b: np.ndarray) -> float:\n\nreturn np.sum(np.abs(a - b))\n\ndef get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\nreturn np.sum((a - b) ** 2)\n\nThis section has introduced one way of instructing GenAI to write a docstring. The next section provides other approaches.\n\nUsing GenAI to write docstrings This section describes different techniques for creating docstrings for a single method and for a full Python file. Docstrings improve code readability and are often required by organizations as part of their continuous integration/continuous development (CI/CD) pipelines. Using GenAI to create docstrings can increase the speed of the software development process.\n\nDocstring for a single method The simplest approach to generating a docstring in VS Code is simply typing three double quotes “““ or the equivalent in single quotes on a blank line following the method signature, as shown in Figure 12.4, with results shown in the code block that follows, which can be accepted by pressing tab:\n\nFigure 12.4: Initiating the request for a docstring with three single or double quotes\n\ndef get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n'''Calculates the squared Euclidean distance between two arrays.'''\n\nreturn np.sum((a - b) ** 2)\n\nChapter 12\n\nAs we can see, it describes the method but is the simplest form of the PEP 257 style with a single line string and no parameters or return values.\n\nIf we want to have a different docstring style, we can request it using the Copilot Editor Inline Chat (/doc Google style) or any other specified style. Figure 12.5 shows the results for the Google style.\n\nFigure 12.5: Docstring in Google style using the inline /doc Google style\n\nThis approach with the online Editor Inline Chat will not create docstrings for all the methods within a file. Other techniques for working at a file level are discussed next.\n\nSimilarly, with the GitHub Copilot chat, we can highlight a single method, such as get_euclidean_ dist, and request a Google-style docstring using the /doc Google style command. Shown on the left of Figure 12.6, the original (above) and new (below) docstrings are presented with the options to keep or undo the changes.\n\nFigure 12.6: Using GitHub Copilot chat on a single method to change to Google style\n\n301\n\n302\n\nDocumenting Code with GenAI\n\nDocstrings for an entire file GitHub Copilot and ChatGPT can be used to produce docstrings for all methods within a single file at once using the chat mode, which can be quite useful. This may not work with the inline chat, depending on the version you are using of VS Code or PyCharm, so the code may be produced in the chat window and will have to be copied and pasted back into the code base. We find this approach useful at the end of the day, when you are trying to submit code before a deadline. In practice, though, we recommend writing docstrings with the code since the GenAI should be verified, which is best accomplished when writing the code. A later section, Finding outdated doc- strings with GitHub Copilot, ChatGPT, and OpenAI API, will describe strategies for stale comments.\n\nThe first step is asking GitHub Copilot to write the docstrings for your current Python file, as shown in Figure 12.7. We will use the same code as we did earlier, using the Flask-based distance calculation.\n\nFigure 12.7: GitHub Copilot chat request to produce docstrings for the entire file\n\nAfter uploading or pasting the file, the results are shown in the following code block. The results appear in the chat and must be copied from it to the code development window. We have not specified the type of docstring, and it may default to the Google style or the equivalent of /doc Google style. By modifying the prompt to /doc single line style, the code will produce the corresponding single-line docstrings. Both versions are included in the GitHub repository for this book.\n\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\nChapter 12\n\n\"\"\"Calculate distance between two vectors.\"\"\"\n\na, b, dist_type = parse_request_parameters(request)\n\ndist_func = {\"L1\": get_manhattan_dist, \"L2\": get_euclidean_dist}. get(dist_type)\n\ndist = dist_func(a, b)\n\nreturn jsonify({\"distance\": dist})\n\ndef parse_request_parameters(request):\n\n\"\"\"Parse and validate request parameters.\"\"\"\n\ndata = request.get_json()\n\na = np.array(data.get(\"df1\",))\n\nb = np.array(data.get(\"df2\",))\n\ndist_type = data.get(\"distance\")\n\nreturn a, b, dist_type\n\ndef get_manhattan_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n\"\"\"Calculate Manhattan distance.\"\"\"\n\nreturn np.sum(np.abs(a - b))\n\ndef get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n\"\"\"Calculate Euclidean distance.\"\"\"\n\nreturn np.sum((a - b) ** 2)\n\n303\n\n304\n\nDocumenting Code with GenAI\n\nQuick tip: Enhance your coding experience with the AI Code Explainer and Quick Copy features. Open this book in the next-gen Packt Reader. Click the Copy button\n\n(1) to quickly copy code into your coding environment, or click the Explain button\n\n(2) to get the AI assistant to explain a block of code to you.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR visit packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nWith these methods, we can create documentation for a single method or an entire file. Refac- toring code often leads to a problem where the code and the docstrings are out of sync. The next section discusses strategies to overcome this problem.\n\nFinding outdated docstrings with GitHub Copilot, ChatGPT, and OpenAI API A frequent debate among software engineers is whether documentation should be written when writing the code or later, when the code has stabilized. One line of thinking is that the docstring (and test cases) should be written at the same time. The primary argument is that the intention of the code is clearest when the developer is working through its logic and understands its pur- pose. The problem is that, as code is routinely refactored, the docstring comments and test cases must be adapted, making the original versions obsolete. The other line of thinking is that the documentation can and should be written later, once the code settles, to minimize the repeated rewriting of the comments.",
      "page_number": 302
    },
    {
      "number": 13,
      "title": "In Python, the convention for documentation within the code is the docstring. Docstrings typically occur at the beginning of Python modules, classes, and methods. The purpose is to explain their funct",
      "start_page": 330,
      "end_page": 347,
      "detection_method": "regex_chapter",
      "content": "Chapter 12\n\nNew GenAI technology makes the debate irrelevant since it offers two solutions. The first solution is simply to regenerate and update all the comments at the method or file level. The previous section described this strategy. We can use GitHub Copilot or ChatGPT to write the docstrings for the entire file, which will update or redo all the docstrings.\n\nThe second strategy is more specific in that it can detect outdated or stale comments so that these can be manually or automatically adjusted. The advantage of this approach is that individual methods can be updated, leaving previously generated or verified comments unchanged. If we have individually written, verified, or otherwise invested effort in the docstrings, we do not want them to be overwritten.\n\nFor the comments, we have taken the previous example and created two versions of the Euclidean distance by copying one and modifying the code. The new code is shown here. As you can see, the first method returns the Euclidean distance and the second method returns the Euclidean distance squared, yet the docstrings have not been updated and are, in fact, identical due to our copy and paste approach.\n\ndef get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n\"\"\"Return the squared Euclidean (L2) distance between two arrays.\"\"\"\n\nreturn np.sqrt(np.sum((a - b) ** 2))\n\ndef get_euclidean_squared_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n\"\"\"Return the squared Euclidean (L2) distance between two arrays.\"\"\"\n\nreturn np.sum((a - b) ** 2)\n\nUsing GitHub Copilot or ChatGPT, we can simply ask whether any of the docstrings are out of sync, as shown in Figure 12.8. The approach cleverly recognizes the squared root function call in get_euclidean_dist and suggests an update to the docstring:\n\n305\n\n306\n\nDocumenting Code with GenAI\n\nFigure 12.8: Using GitHub Copilot to identify stale docstrings\n\nA better approach is to use OpenAI API to compare the code and docstrings individually across a file. The sample code can be found in the repository for this chapter called openai_docstring_ review.py, which runs on the distance_versions.py file. The resulting output is shown next for each of the methods.\n\nThe output explains the logic and provides an answer as to whether the docstring correctly corre- sponds to the code implementation. It correctly identifies that the get_euclidean_dist method does not match.\n\nFunction: calculate_distance\n\nDocstring: Calculate distance between two arrays from a POST request.\n\nSource: @app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\n<…code deleted for space…>\n\nMatches: The docstring matches the implementation. Both the docstring and the function's purpose convey that the function is designed to calculate the distance between two arrays based on parameters received from a POST request. Therefore, the conclusion is:\n\nMATCHES: Yes\n\nFunction: get_euclidean_dist\n\nDocstring: Return the squared Euclidean (L2) distance between two arrays.\n\nChapter 12\n\nSource: def get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n<…code deleted for space…>\n\nMatches: The provided docstring states that the function returns the \"squared Euclidean (L2) distance between two arrays,\" but the implementation actually returns the Euclidean distance (not squared) by taking the square root of the sum of squared differences. Therefore, the docstring does not accurately describe the implementation.\n\nThus, the conclusion is that the docstring does NOT match the implementation.\n\nFunction: get_euclidean_squared_dist\n\nDocstring: Return the squared Euclidean (L2) distance between two arrays.\n\nSource: def get_euclidean_squared_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n<…code deleted for space…>\n\nMatches: The docstring and the function implementation match. The function `get_euclidean_squared_dist` computes the squared Euclidean distance (also known as the L2 distance) between two numpy arrays `a` and `b`, and the docstring accurately describes this functionality.\n\nTherefore, the answer is:\n\nMATCHES\n\nFunction: get_manhattan_dist\n\nDocstring: Return the Manhattan (L1) distance between two arrays.\n\nSource: def get_manhattan_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n<…code deleted for space…>\n\nMatches: MATCHES: Yes\n\nThe chapter has covered several techniques for creating docstrings. It has also introduced different strategies for finding problems with stale comments. In the next section, we provide recommen- dations based on how we approach writing docstrings with GenAI.\n\n307\n\n308\n\nDocumenting Code with GenAI\n\nPractical insights Given the choices of GitHub Copilot, ChatGPT, and OpenAI API, we find that Copilot is the most convenient for writing docstrings. The convenience of pair programming in the integrated envi- ronment simply makes it easier to request documentation, rather than copying and pasting code to other places. Copilot chat can usually accurately locate stale comments by itself. One issue we have encountered is that the context of sending code to Copilot is not visible. If you copy code and ask if the comments are current, it may miss some methods that were not included in the context. This can be solved by copying and pasting the code in place to renew the context.\n\nWe recommend that the docstrings be created after writing the code for each method, or at least after the code for a method has stabilized, if not after each refactoring. We further suggest check- ing that the docstring accurately reflects the implementation. At the beginning of the chapter, we discussed that documentation should reflect why a particular method was written rather than describing what the code does. The current technology is quite reasonable for the what but ignores the why, which is an opportunity for software engineers to elaborate.\n\nSummary Docstrings are a standard practice for maintaining high-quality code. They enable future readers of the software to understand the code, which is important for debugging, feature enhancements, and more. We recommend docstrings be included as soon as the code has been written. GitHub Copilot offers multiple ways of writing docstrings for a single method, including three single or double quotes, using the Copilot menus on a highlighted method. These approaches will produce single-line docstrings, the Google docstring style, or others when specified as a command.\n\nA challenge in writing software is keeping docstrings up to date with the code. All methods can identify discrepancies between the code and docstrings, which is the preferred method for iden- tifying issues that can be resolved. Alternatively, docstrings can be updated at a method or file level, replacing any existing docstrings.\n\nChapter 12\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: What are three ways of generating docstrings for code?\n\nAnswer: The three ways discussed in this chapter are as follows:\n\n1. GitHub Copilot can recommend code by opening \"\"\" or ''' for a method on the line after\n\nthe method signature\n\n2. GitHub Copilot can be invoked by highlighting a method and using its menu to generate\n\na docstring\n\n3. ChatGPT or Copilot can be used to add docstrings to all methods\n\nQuestion 2: What are two approaches for keeping docstrings consistent with the underlying code?\n\nAnswer: The first approach is simply overwriting any docstrings. This ensures that the GenAI pro- duces a docstring for all methods based on the underlying code. If a docstring were inconsistent, then it would be overwritten. Unfortunately, this overwrites all previously created docstrings, which may be undesirable if you have invested time and effort into writing or checking any of them.\n\nThe second approach uses GitHub Copilot or ChatGPT to ask if there are discrepancies between the code and the docstrings. One can also use OpenAI API to match each code implementation against the docstring and return whether they match on a case-by-case basis as was shown with the openai_docstring_review.py, sample code. Any of the options in the second approach are preferred if docstrings are current and verified.\n\nQuestion 3: Given that GenAI can produce quality docstrings, is it necessary to check the doc- strings, or can they simply be regenerated?\n\nAnswer: While the quality is usually quite high, inaccurate docstrings can confound future de- velopers. It is strongly recommended to use GenAI to save time, but to use human intelligence to verify that they are correct. At the present time, trust but verify is the best approach.\n\n309\n\n310\n\nDocumenting Code with GenAI\n\nFurther reading\n\nMiller’s Law, also known as Magic Number 7 plus or minus 2: https://en.wikipedia.\n\norg/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two\n\n\n\nTypes of docstring formats: https://www.geeksforgeeks.org/python-docstrings/\n\n\n\nFurther formatting of docstrings: https://www.linkedin.com/pulse/python- docstrings-formats-samuel-thomas/\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.\n\n13\n\nWriting and Maintaining Unit Tests\n\nThe task of software engineering can be interpreted as the ability to deliver quality products that contribute to the business value of customers. In this age of technology, customers have high standards and demand excellence.\n\nThe notion of quality has been interpreted in different ways over the years. However, here are two key aspects:\n\n\n\nThat it must meet the user requirements\n\n\n\nIt must be free of defects\n\nThese two requirements are captured by verification and validation processes, which assert that the software achieves the intended goal without any defects. There are many software engineer- ing techniques that aim to achieve these goals, including code reviews, pair programming, and so on, but the most common is testing. Although there are many books that focus on the art and practice of software testing, this chapter focuses on unit testing, which is the most widely used approach. Test-driven development (TDD) is a popular agile approach for software development. Rather than writing the code first and ensuring it functions correctly with unit tests, TDD involves writing the unit test cases first and subsequently writing the minimum code that passes the tests. Proponents of the practice claim superior code quality and an inherently testable design. In this chapter, we will refer to the primary code under development as the implementation code, and the unit tests as the test code.\n\n312\n\nWriting and Maintaining Unit Tests\n\nWith GenAI coding, one can argue that testing is even more critical than conventional program- ming. The generative AI process involves crafting code based on comments and/or method signa- tures from other code, which lacks the precision that a developer would normally apply, given the trade-off for supercharged, faster coding. Unit testing serves as a check that the code functions correctly, as specified by tests. Combined with code inspection while creating the code, it offers a check of correctness.\n\nThis chapter is dedicated to working with both GitHub Copilot and ChatGPT to supercharge our creation of unit tests. In this chapter, we will cover the following topics:\n\nUnit tests with GenAI\n\nData-driven tests\n\n\n\nTest-driven development\n\n\n\nSample data creation with GenAI\n\n\n\nPractical insights\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\n\n\nA GitHub account\n\n\n\nAccess to your preferred IDE – either VS Code or PyCharm\n\n\n\nAccess to the book’s repository available at https://github.com/PacktPublishing/ Supercharged-Coding-with-Gen-AI\n\n\n\nA virtual environment set up in your preferred IDE, VS Code, or PyCharm\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nUnit tests with GenAI GenAI can save considerable time in writing unit tests. As the name suggests, each unit test is designed to test a single unit or aspect of the code. In fact, they are focused on the smallest frag- ments of code that can be separated and tested in isolation. Together, they provide confidence that the code functions correctly at the component level and provide an early signal of issues when the code is refactored. The same developer typically writes the code implementation and unit tests. The unit tests prove that the code functions correctly, as understood, and may also help define the requirements.\n\nChapter 13\n\nBy their nature, unit tests require a deep understanding of the business and software requirements and how each method fulfills those requirements. One must consider the intended functionality as well as the variety of edge cases for each method separately and incorporate those into the test framework. This includes decisions on the test framework, method call, test nomenclature, asser- tion form, and the actual test. IDEs can certainly help with the test framework, but the actual test requires developer time, thought, and perhaps a strategy. These often become hurdles for junior or less motivated developers who perhaps see them as superfluous to the core software coding task. The current GenAI tools provide two-way functionality in writing unit tests for existing code as a common approach or writing code from existing unit tests, which is TDD.\n\nThe GenAI capability is not a substitute for testers or testing, but makes skilled testers much more efficient. The developer can write more tests in less time and focus on the more complex cases. The generic and simpler unit tests can be written by GenAI, usually with high accuracy. However, if the resulting tests are incomplete, one can request or prompt the GenAI to create additional tests to provide better coverage and confidence in the code.\n\nAs example code, we use the concept of creating n-grams for a given text string, which we intro- duced in Chapter 6. N-grams are simply adjacent groups of N letters starting at any and every point within a string without exceeding the left or right side of the string. For example, the 3-grams from the sequence “abcde” are [“abc”, “bcd”, “cde”]. Since we wanted to limit the range of possible characters, we only considered lowercase letters and spaces, but left tabs and line feeds. Uppercase letters were converted to lowercase, and both numbers and symbols were removed. Additionally, since English type uses single spaces, we replaced all adjacent spaces with a single space. This portion of the code is as follows:\n\nimport re\n\ndef lowercase_remove_punct_numbers(text, supercharte=True):\n\nreturn re.sub(r'[^a-z\\s]', '', text.lower())\n\ndef multiple_to_single_spaces(text):\n\nletters_single_spaces = re.sub(r'\\s+', ' ', text)\n\nreturn letters_single_spaces\n\ndef create_ngrams(text, n) -> list:\n\n'''create a list of n-gram tuples from the input text.'''\n\nprocessed_text = lowercase_remove_punct_numbers(text)\n\nsingle_space_processed = multiple_to_single_spaces(processed_text)\n\n313\n\n314\n\nWriting and Maintaining Unit Tests\n\nu = [single_space_processed[i:i+n] for i in range(len(single_space_ processed)-n+1)]\n\nreturn u\n\nif __name__ == \"__main__\":\n\ntext = \"This is a sample text $ABC% for creating n-grams.\"\n\nn = 3\n\nprint(create_ngrams(text, n))\n\nThe output of this function is a list of 3-grams that span the text input string:\n\n['thi', 'his', 'is ', 's i', ' is', 'is ', 's a', ' a ', 'a s', ' sa', 'sam', 'amp', 'mpl', 'ple', 'le ', 'e t', ' te', 'tex', 'ext', 'xt ', 't a', ' ab', 'abc', 'bc ', 'c f', ' fo', 'for', 'or ', 'r c', ' cr', 'cre', 'rea', 'eat', 'ati', 'tin', 'ing', 'ng ', 'g n', ' ng', 'ngr', 'gra', 'ram', 'ams']\n\nIf we start unit testing and have not set up the testing framework, we can use GitHub Copilot Chat with the prompt @workspace /setupTests, which will present an option to use either of the two main frameworks: pytest or unittest, as shown in Figure 13.1. The instructions are useful if you are installing unittest for the first time.\n\nFigure 13.1: @workspace /setupTests provides a choice of the two main test frameworks\n\nTo generate unit tests, first select a single method from the implementation code, such as the lowercase_remove_punct_numbers code. Then, type the @workspace /tests generate unit tests command. Alternatively, explicitly type the name of the target method, such as @workspace\n\nChapter 13\n\n/tests generate unit tests for lowercase_remove_punct_numbers, into the GitHub Copilot Chat. It will first explain the tests that it will generate and then write the corresponding unit test file. The test explanation is shown in Figure 13.2:\n\nFigure 13.2: The test plan from GitHub Copilot is provided along with the code for the method\n\nThe user will be prompted to save the resulting test file. The following code shows an example of what the output unit tests may look like, although it will vary in order and even the number of tests:\n\nimport unittest\n\nfrom src.ngrams import lowercase_remove_punct_numbers\n\n# File: test_ngrams.py\n\nclass TestLowercaseRemovePunctNumbers(unittest.TestCase):\n\ndef test_mixed_input(self):\n\ntext = \"Hello, World! 123\"\n\nexpected = \"hello world\"\n\nself.assertEqual(lowercase_remove_punct_numbers(text), expected)\n\ndef test_lowercase_only(self):\n\ntext = \"this is a test\"\n\nexpected = \"this is a test\"\n\nself.assertEqual(lowercase_remove_punct_numbers(text), expected)\n\n315\n\n316\n\nWriting and Maintaining Unit Tests\n\ndef test_punctuation_and_numbers(self):\n\ntext = \"!@#$%^&*()1234567890\"\n\nexpected = \"\"\n\nself.assertEqual(lowercase_remove_punct_numbers(text), expected)\n\ndef test_empty_string(self):\n\ntext = \"\"\n\nexpected = \"\"\n\nself.assertEqual(lowercase_remove_punct_numbers(text), expected)\n\ndef test_spaces_and_special_characters(self):\n\ntext = \" Hello!! World?? \"\n\nexpected = \" hello world \"\n\nself.assertEqual(lowercase_remove_punct_numbers(text), expected)\n\nif __name__ == \"__main__\":\n\nunittest.main()\n\nThe test code is quite reasonable in the sense that it mostly proves that the regular expressions and lowercase are correct. The tests can be run from the parent level using the python -m unittest test_lowercase_remove_punct_numbers.py command.\n\nCreating tests is an iterative process, and some of the test functions created by GenAI may be un- necessary or incorrect. In other cases, additional tests may be warranted, and the GitHub Copilot Editor Inline Chat is useful for creating those tests from within the test file. As an example, we used /tests with a request to include 10-digit phone numbers to prove they would be correctly handled, which produced inline code in the test file. This process is shown in Figure 13.3:\n\nFigure 13.3: Using Editor Inline Chat within the test code to add a new test\n\nChapter 13\n\nIn some cases, the Editor Inline Chat is more convenient since it will add directly to the code, whereas GitHub Copilot Chat may create a new test file. In either case, a request for a single test, such as a phone number or punctuation, may result in multiple additional tests. The developer can accept or reject these tests individually with the click of a button. Reviewing tests is important since the accuracy of the tests relates to the correctness of the code.\n\nRather than write tests for each method separately, the entire file can be converted into unit tests. Using GitHub Copilot chat and the @workspace /tests create unit tests for ngrams. py command, Copilot will often, but not always, produce a test plan for each of the methods in ngrams.py. Then, it will create an output test file that you must name to save. Using the unittest conventions, each method will have its own class of one or more unit tests within that file. The full set of tests is not shown here but can be found in the test_ngrams.py file in the book’s GitHub repository.\n\nThere are guidelines for adding additional tests to the test file using a comment format. By typing a comment (starting with # in Python), GitHub Copilot reads the comment and suggests code. Although it should be equivalent, we have found this approach to sometimes be less accurate than using the Editor Inline Chat or GitHub Copilot Chat. As shown in Figure 13.4, the expected answer incorrectly includes numbers that should have been removed:\n\nFigure 13.4: This test, generated from the #comment, incorrectly includes numbers\n\nRevising the test with the #test create_ngrams with a 10-digit phone number and make sure the numbers are removed request resulted in fewer numbers included in the test. However, the variability of the unit tests with correct and incorrect responses means that checking them must be part of the verification process.\n\nIf we repeat a similar process of adding a zip code of the form 12345-6789 using the Editor Inline Chat feature, the results are more promising, as shown in Figure 13.5. Not only did it correctly produce the test, but it also added several additional tests. However, as a note of caution, the Inline Chat did not always produce accurate tests and sometimes incorrectly included numbers.\n\n317\n\n318\n\nWriting and Maintaining Unit Tests\n\nFigure 13.5: The tests from Editor Inline Chat produced a correct test and others as well\n\nThis section has described how GitHub Copilot can create unit tests from the implementation code, either for a single method or an entire file. The next section describes how ChatGPT can achieve similar results.\n\nChatGPT for testing ChatGPT can also be used to write unit tests. An example prompt to write Python unit tests is as follows:\n\nYou are a Python testing assistant.\n\nGiven Python code enclosed within {{{ }}}, generate unit tests using the unittest framework. For each function or method in the code:\n\n1.Create a corresponding test method within a unittest.TestCase subclass.\n\n2.Use meaningful test method names that reflect the function being tested.\n\n3.Include appropriate assertions based on the function's logic and expected behavior.\n\n4.Use mock objects or patching where necessary (e.g., for I/O, APIs, or external dependencies).\n\n5.If a function has multiple logical branches or edge cases, include test cases for them.\n\n6.Do not include the original code in the output—only the test code.\n\nChapter 13\n\n7.Import any modules or classes necessary for the tests to run.\n\n8.Format your output as a complete, valid Python test file using the unittest module.\n\n________________________________________\n\nInput:\n\npython\n\n{{{\n\n# Paste Python code here\n\n}}}\n\n________________________________________\n\nOutput:\n\nApplying this template to the ngrams.py code produced seven different tests without further elaboration. The file is listed in test_ngrams_chatgpt.py in the GitHub repository for the book. One of the tests created was incorrect and subsequently failed, as shown in Figure 13.6. The test incorrectly suggests that a $ and s are equivalent, instead of replacing the dollar symbol with a space.\n\nFigure 13.6: ChatGPT produced one incorrect test out of seven for ngrams.py\n\n319\n\n320\n\nWriting and Maintaining Unit Tests\n\nWhen prompted Are there additional tests that should be included?, ChatGPT produced an additional set of tests shown in Figure 13.7, each with code, and offered to convert them into a code library:\n\nFigure 13.7: Additional ChatGPT tests that it suggested when prompted\n\nAs a supplement to the unit tests, additional tests can also be run directly within ChatGPT and included in the overall test suite. The test_ngrams_chatgpt_extra.py file in the GitHub repos- itory shows the tests generated. In addition to running the tests in ChatGPT, this platform also explains why tests fail. Figure 13.8 presents a valuable discussion of the test failure and the han- dling of tabs and newlines, which could be a failed test or an issue with the original code. Such insight into the code will help improve the overall quality. ChatGPT made suggestions to change the tests and rerun them:\n\nFigure 13.8: Explanation of a failed text from ChatGPT\n\nChapter 13\n\nChatGPT is clearly effective at writing unit tests, even if it requires multiple requests. It is also pos- sible to write other kinds of tests, such as data-driven tests, which is the topic of the next section.\n\nData-driven tests The GitHub Copilot or ChatGPT prompt can enable data-driven or random testing that supple- ments the unit test coverage. Using the Generate data driven tests for lowercase_remove_ punct_numbers prompt with Copilot gave the following code:\n\nimport unittest\n\nfrom src.ngrams import lowercase_remove_punct_numbers\n\nclass TestLowercaseRemovePunctNumbers(unittest.TestCase):\n\ndef test_data_driven(self):\n\ntest_cases = [\n\n# (input, expected_output)\n\n(\"Hello, World! 123\", \"hello world \"),\n\n(\"ABCdef\", \"abcdef\"),\n\n(\"1234!@#$\", \"\"),\n\n(\"A1! b2@ C3#\", \"a b c\"),\n\n(\"\", \"\"),\n\n(\" \", \" \"),\n\n(\"Python3.8_is#awesome!\", \"pythonisawesome\"),\n\n(\"No PUNCTUATION\", \"no punctuation\"),\n\n(\"MiXeD CaSe &*^%\", \"mixed case \"),\n\n(\"newline\\nTab\\tSpace \", \"newline\\ttab\\tspace \"),\n\n]\n\nfor input_text, expected in test_cases:\n\nwith self.subTest(input=input_text):\n\nself.assertEqual(lowercase_remove_punct_numbers(input_ text), expected)\n\nif __name__ == \"__main__\":\n\nunittest.main()\n\n321\n\n322\n\nWriting and Maintaining Unit Tests\n\nThis file can be found as test_data_driven_ngrams.py in the book’s GitHub repository. This example and the unit tests in the previous section show how GenAI can create tests from the implementation code. GitHub Copilot and ChatGPT use this code to identify meaningful test cases, including many edge cases. To improve the coverage, additional tests can be written man- ually, semi-manually with GenAI, or by simply requesting additional tests. These unit tests can be extended similarly to other types of testing, such as integration testing, performance testing, and A/B testing.\n\nAs discussed in these two sections, both unit tests and data-driven tests can be written using GenAI from the implementation code. While creating test code from the implementation code works, one should consider writing the tests first rather than the implementation. This approach is called test-driven development and is described in the next section.\n\nTest-driven development Test-driven development (TDD) is one component of the Extreme Programming (XP) agile soft- ware methodology proposed by Kent Beck in the 1990s. Two of his books can be found in the Further reading section. The main concept of TDD is that the unit tests are written first and subsequently fail since there is no code to support them. The software engineer then writes the code until the tests pass. It is widely accepted that this practice improves code design and enhances testability. Empirical studies such as that by Mäkinen & Jürgen Münch (https://link.springer.com/cha pter/10.1007/978-3-319-03602-1_10) generally support the value of TDD. The approach also focuses the software engineer on passing tests rather than adding other features.\n\nWhether to write tests first or last often becomes a religious debate with devout followers on either side. We will not attempt to resolve the issue here. If we believe that GenAI always writes perfect code that exactly and cleanly meets all the requirements, then it does not matter whether the tests or the code are used to write the other. A recent 2024 IEEE study by Jiri et al. (https:// ieeexplore.ieee.org/document/10685204) showed that unit tests written by humans have fewer errors than unit tests by GenAI, but the technology is advancing rapidly and closing this gap. This leaves us with a conundrum that the code may be flawed, and the tests may not be perfect, yet we need both to be accurate. The more common convention of software engineers is to write the code using GenAI and then use GenAI to write the tests, which mirrors their development practice. With TDD, there is a potential to write the perfect tests, perhaps with GenAI, and then use GenAI to prove that the implementation passes the tests and their perceived requirements. In fact, we can go further and use GenAI to iterate and craft the code that meets these requirements.",
      "page_number": 330
    },
    {
      "number": 14,
      "title": "By their nature, unit tests require a deep understanding of the business and software requirements and how each method fulfills those requirements. One must consider the intended functionality as well",
      "start_page": 348,
      "end_page": 397,
      "detection_method": "regex_chapter",
      "content": "Chapter 13\n\nAs an example for this section, we will use the intersection of two rectangles problem, which simply identifies the overlapping area between two rectangles. To simplify the problem, we restrict the rectangle definition to use two points—the x and y coordinates of the lower-left and upper-right corners of the rectangle. In our notation, the rectangle is represented as (xll, yll, xur, yur) as shown in Figure 13.9:\n\nFigure 13.9: Intersecting rectangle problem\n\nRather than immediately start coding the result, we will instead consider the various test cases, which are broken down into the following:\n\n1.\n\nIntersecting rectangles (Figure 13.10):\n\nFigure 13.10: Example test cases for overlapping rectangles\n\n2. Non-intersecting rectangles (Figure 13.11):\n\nFigure 13.11: Example test cases for non-overlapping rectangles\n\n323\n\n324\n\nWriting and Maintaining Unit Tests\n\n3.\n\nInvalid rectangles (Figure 13.12):\n\nFigure 13.12: Example test cases for invalid rectangles that fail our definition\n\nAs part of the TDD process, we begin by coding the unit tests using GenAI. Using descriptive test names facilitates the accurate creation of the test code. A few tests are shown to illustrate the idea in the following code snippet (the full file with 21 unit tests is available in the book’s GitHub repository as test_rectangle_intersection.py):\n\nimport unittest\n\nfrom rectangle_intersection import rect_intersection_area\n\nclass TestRectangleIntersectionArea(unittest.TestCase):\n\ndef test_intersecting_rectangles(self):\n\nrect1 = (0, 0, 4, 4)\n\nrect2 = (2, 2, 6, 6)\n\nself.assertEqual(rect_intersection_area(rect1, rect2), 4)\n\ndef test_intersecting_rectangles_swapped(self):\n\nrect1 = (2, 2, 6, 6)\n\nrect2 = (0, 0, 4, 4)\n\nself.assertEqual(rect_intersection_area(rect1, rect2), 4)\n\ndef test_non_intersecting_rectangles(self):\n\nrect1 = (0, 0, 2, 2)\n\nrect2 = (3, 3, 5, 5)\n\nself.assertEqual(rect_intersection_area(rect1, rect2), 0)\n\nFor the tests to appropriately fail as the next step of the TDD process, we need a stub file, rectangle_ intersection.py, containing rect_intersection_area(rect1, rect), which arbitrarily returns None. An example of this code is shown as follows and is stored as rectangle_intersection.\n\nChapter 13\n\npy in the same folder. These names are based on the import statement at the beginning of the preceding test file code, as shown:\n\ndef rect_intersection_area(rect1, rect):\n\nreturn None\n\nWith the TDD process, we have encoded the requirements of the code as tests. The only missing piece is the actual implementation. Normally, we would iterate between writing the implemen- tation and running the tests to ensure that the code functions as expected. Using GenAI, we can replace this process with a request to GenAI to iteratively write the code until it passes all the tests. We will discuss how this can be achieved using GitHub Copilot in both VS Code and PyCharm IDEs, and then show the TDD approach using ChatGPT.\n\nTDD using GitHub Copilot within VS Code The TDD approach within VS Code is arguably the most straightforward. First, we ensure that both rectangle_intersection.py and test_rectangle_intersection.py are open, then run the latter unit test file. Then, we can use this command in the Copilot chat window:\n\n@terminal /fix rect_intersection_area according to the unit test failure\n\nThis produces an updated set of code in the same chat window that we copied and pasted as a replacement for the rectangle_intersection.py implementation. This is shown as follows:\n\ndef rect_intersection_area(rect1, rect2):\n\n\"\"\"\n\nCalculate the area of intersection between two rectangles.\n\nEach rectangle is represented as a tuple (x1, y1, x2, y2):\n\n(x1, y1): bottom-left corner\n\n(x2, y2): top-right corner\n\n\"\"\"\n\n# Calculate the overlapping region\n\nx_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]))\n\ny_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]))\n\n# If there is an overlap, calculate the area\n\nreturn x_overlap * y_overlap\n\n325\n\n326\n\nWriting and Maintaining Unit Tests\n\nRunning the same unit tests against this new implementation reduced the number of tests that failed from 21 to 9. Most of the failures are caused by the lack of exceptions raised for invalid rectangles. These were included in the tests but ignored by the GenAI.\n\nWe repeated the process with the same /fix command to produce the next iteration of code implementation, but the errors persisted. In some cases, running the process two or three times resolved the errors. In other cases, we needed to manually revise the /fix command with more explicit instructions:\n\n@workspace /fix rectangle_intersection_area according to the unit test failure and be sure to raise ValueError for invalid rectangle dimensions\n\nThe generated implementation passed all the unit tests and is shown as follows:\n\ndef rect_intersection_area(rect1, rect2):\n\n\"\"\"\n\nCalculate the area of intersection between two rectangles.\n\nEach rectangle is represented as a tuple (x1, y1, x2, y2):\n\n(x1, y1): bottom-left corner\n\n(x2, y2): top-right corner\n\n\"\"\"\n\n# Ensure the rectangles are valid\n\nif rect1[0] >= rect1[2] or rect1[1] >= rect1[3]:\n\nraise ValueError(f\"Invalid rectangle dimensions for rect1: {rect1}\")\n\nif rect2[0] >= rect2[2] or rect2[1] >= rect2[3]:\n\nraise ValueError(f\"Invalid rectangle dimensions for rect2: {rect2}\")\n\n# Calculate the overlapping region\n\nx_left = max(rect1[0], rect2[0])\n\ny_bottom = max(rect1[1], rect2[1])\n\nx_right = min(rect1[2], rect2[2])\n\ny_top = min(rect1[3], rect2[3])\n\n# Check if there is an actual overlap\n\nif x_left < x_right and y_bottom < y_top:\n\nreturn (x_right - x_left) * (y_top - y_bottom)\n\nelse:\n\nreturn 0 # No overlap\n\nChapter 13\n\nIn this section, we used a simple one-line command within the VS Code chat to create an im- plementation. It uses the implementation code, unit tests, and the output of the tests directly to write an improved version of the implementation code.\n\nTDD using GitHub Copilot within PyCharm Using the TDD approach within PyCharm is not quite as simple as with VS Code. As of January 2025, GitHub Copilot cannot access PyCharm’s terminal window, which means it cannot access the failed unit tests directly. The output of failed tests informs Copilot what needs to be fixed in the implementation, and so this is a critical component. As a solution, we can paste the error message of the failed messages into the GitHub Copilot chat window:\n\nCONTEXT: You will be given an error message enclosed in {{{ERROR}}}.\n\nTASK: Fix the implementation of rectangle_intersection_area _mean so it passes all tests.\n\nERROR: {{{ ... <omitted for brevity> ... }}}\n\nFIXED CODE:\n\nThis prompt, whether used in an inline chat or a chat window, should generate a fully functional implementation that passes the unit tests.\n\nFor TDD with PyCharm, we need to use a more complete template call within the chat that passes the output of the failed unit tests. With this information, as well as the implementation and test code, a new version of the implementation code aims to solve the issues and pass the unit tests. A similar approach can be used with ChatGPT, which will be described next.\n\nTDD using ChatGPT As part of the TDD process, ChatGPT can also be used to generate improved implementation code based on the combination of the existing implementation code, unit test code, and the errors or results of the unit test execution. We use triple brackets to enclose each of these components:\n\nCONTEXT: You are provided with\n\n1. Python function enclosed with {{{ FUNCTION }}} that has a bug.\n\n2. Test suite code enclosed with {{{ UNIT TESTS }}}.\n\n3. Error message enclosed with {{{ ERROR }}}.\n\nTASK: Fix the function implementation\n\nFUNCTION: {{{def rect_intersection_area(rect1, rect2):\n\nreturn None\n\n327\n\n328\n\nWriting and Maintaining Unit Tests\n\n}}}\n\nUNIT TESTS: {{{ ... omitted for brevity ... }}}\n\nERROR: {{{ ... omitted for brevity ... }}}\n\nCODE:\n\nIn this instance, ChatGPT produced the following output code, which is similar but mildly differ- ent from the GitHub Copilot output. ChatGPT explained the implementation, and all the created tests passed in the first iteration.\n\ndef rect_intersection_area(rect1, rect2):\n\nx1_1, y1_1, x2_1, y2_1 = rect1\n\nx1_2, y1_2, x2_2, y2_2 = rect2\n\n# Validate input rectangles\n\ndef is_invalid_rect(x1, y1, x2, y2):\n\nreturn x1 >= x2 or y1 >= y2\n\nif is_invalid_rect(x1_1, y1_1, x2_1, y2_1) or is_invalid_rect(x1_2, y1_2, x2_2, y2_2):\n\nraise ValueError(\"Invalid rectangle with non-positive width or height\")\n\n# Find intersection bounds\n\ninter_left = max(x1_1, x1_2)\n\ninter_right = min(x2_1, x2_2)\n\ninter_bottom = max(y1_1, y1_2)\n\ninter_top = min(y2_1, y2_2)\n\n# Check if rectangles intersect\n\nif inter_left >= inter_right or inter_bottom >= inter_top:\n\nreturn 0\n\n# Compute and return intersection area\n\nreturn (inter_right - inter_left) * (inter_top - inter_bottom)\n\nChapter 13\n\nIf the tests had not passed, the new code would need to be copied and pasted into the IDE, replacing the previous version. The unit tests would need to be rerun against the new code. The revised im- plementation code and resulting unit test errors must be copied again into the ChatGPT template.\n\nThe advantage of the TDD approach is that it asserts that the implementation is correct based on the fact that the code passes the unit tests. The code produced in this simple example is quite reasonable, but more complex tasks may produce functional code of lower quality. In such cases, the code should be refactored. However, this is much easier with a full set of tests that continue to prove the code is meeting the functional requirements.\n\nWhether TDD is the ideal strategy is still up for debate. Without TDD, the user focuses on the implementation, from which the GenAI can certainly create reasonable tests. As we observed, some of the tests may fail, and some may indicate limitations of the implementation. The software developer must then understand all the tests and provide enough logical coverage to be confident in the implementation. With TDD, the cognitive effort is in assuring the tests are sufficient, from which code can be generated. If the GenAI provides an implementation, the most the developer will have to do is refactor the code.\n\nWhether the code or tests are written first, the tests often require meaningful datasets. The next section describes how GenAI can be used to create realistic data.\n\nSample data creation with GenAI One aspect of testing is the availability of data that is representative of the kind you will encoun- ter in the wild. If you have been involved in creating mock data in years past, you will certainly appreciate GenAI approaches to this problem. The new techniques can create a variety of types of data with as specific a prompt as you need. For example, the following prompt works in either ChatGPT or GitHub Copilot:\n\ncreate a table with 10 rows and the following columns:\n\n5 digit integer id called \"ID\"\n\nfirst name\n\nlast name\n\naddress consisting of a number, street name, and whether it is a st., dr. or ave.\n\ncity\n\ncountry\n\ndisease\n\nage\n\n329\n\n330\n\nWriting and Maintaining Unit Tests\n\nThe resulting table output is shown in Figure 13.13, which includes typical names, address formats, diseases, and so on. This kind of data can be saved to a file and used for testing. Although such data could be generated on the fly, the purpose of unit testing is to create repeatable tests that achieve consistent results. Tests are usually added to a regression test package as part of the CI/ CD methodology and rerun periodically to ensure that past errors do not recur as code versions evolve. Therefore, the data should be stored and reused.\n\nFigure 13.13: Example table of mock data produced by GitHub Copilot\n\nOf course, the data is not limited to addresses but can be more technical, such as the Create a FASTA file with 3 sequences of length 15 to 25 nucleotides prompt, which returns three bioinformatics DNA sequences as shown in Figure 13.14:\n\nFigure 13.14: A FASTA bioinformatics sequence file with three DNA sequences\n\nChapter 13\n\nAs can be seen from the previous output, it suggests saving the file as a .fasta file. In fact, it can create and save data in any format, including tables, JSON, CSV, and so on.\n\nAnother use case that is often convenient when scripting is to create some temporary data for a function, perhaps in a Jupyter notebook. Consider the Euclidean distance method:\n\ndef get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n\"\"\"Return the squared Euclidean (L2) distance between two arrays.\"\"\"\n\nreturn np.sqrt(np.sum((a - b) ** 2))\n\nGitHub Copilot used the Create sample inputs for get_euclidean_distance() prompt to produce a few samples of inputs that can be directly applied, as shown in Figure 13.15:\n\nFigure 13.15: GitHub Copilot creates example inputs for functions for informal testing\n\nIn the TDD process, developers write the unit test code first, and the implementation code is written to pass the unit tests. The unit tests can be written by developers using standard Ge- nAI approaches to verify that the implementation code will meet the requirements. From either VS Code or PyCharm, GitHub Copilot can iteratively create the implementation code from the unit test code alone. ChatGPT offers similar functionality through a prompt template. The next section provides recommendations on how GenAI approaches can best be used to complete the implementation and test code.\n\n331\n\n332\n\nWriting and Maintaining Unit Tests\n\nPractical insights Testing has been a focus of technology from the templates of “vintage AI” to the present time with GenAI. There has always been a tendency to write the code and eventually write the tests to make it acceptable to your organization. These new GenAI tools can definitely increase the velocity of the coding process. The verification process to check the code should not be just a one-time visual inspection but should involve unit tests that can be applied repeatedly to assert the correctness of the code.\n\nThe different GenAI solutions produce slightly different results, but we generally prefer GitHub Copilot for creating tests since the close integration with the code and iterative approach is easier using the single tool within the IDE, whether VS Code or PyCharm. By keeping similar test code open in the browser, accuracy and style improve due to the extra context.\n\nRegardless of the particular GenAI tool used, it is critical to check the implementation and test code. Using GenAI to create the test code from the implementation or vice versa helps minimize errors since the implementation code that passes unit tests achieves at least a first level of re- quirement verification. Failing unit tests provide a chance to rethink code requirements for the implementation and test code.\n\nSummary In this chapter, we explored various aspects of testing using GenAI. Although there are many types of tests and they are the topic of many books, the most common tests are unit tests. In unit testing, each test should test a single point of functionality and should be independent of other tests. Together, a complete set of unit tests proves that the code implementation functions as expected. If the code is refactored, augmented, or integrated in the future, the same unit tests assert the correctness of the functionality.\n\nThe chapter described a few approaches to creating tests from the implementation. GitHub Co- pilot and ChatGPT can both be used to easily write unit tests for a single method or an entire Python file. Both techniques have some variability in terms of the types of tests and coverage that they provide in a first pass. In some cases, the tests may not be correct and may need additional prompting or manual correction. In other scenarios, prompting may be useful to add further tests to improve the coverage of the edge cases. Developers must be cautious that the tests accurately capture the requirements and are sufficient to assert the correctness of the code.\n\nChapter 13\n\nTDD is a methodology from Extreme Programming where unit tests are written first and, by defi- nition, fail. The implementation code is written to pass all the unit tests. GenAI can help suggest the test code, especially when descriptive method names are used. Using a set of tests, either GitHub Copilot or ChatGPT can iteratively generate the implementation code automatically. The process repeats until the unit tests pass. By writing the full set of tests first, the developer conveys the scope of the tests and behavior in various edge cases. The only required interaction with the generated implementation code is possibly refactoring it to improve quality, which is easier to achieve with the unit tests.\n\nTwo additional topics were covered in the chapter, which involved enriching data for tests. GenAI can create more elaborate tests by synthesizing different scenarios. Data-driven testing creates test pairs for inputs and outputs to test multiple scenarios. In addition, the technology can create realistic test sets applicable to a wide range of disciplines.\n\nIn the next chapter, we introduce ways that GenAI can identify capacity limits in terms of execution speed and memory management. We will also show how GenAI can optimize code to improve performance under these two constraints.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: What are the advantages of TDD?\n\nAnswer: TDD, or test-driven development, is a methodology where the tests are written first, and the implementation code follows. The tests cover the range of required functionality and edge cases to ensure they meet expectations. Using TDD ensures that the tests are written and that the code (written by humans or GenAI) functions correctly. Empirical results show that the implementations are generally structured better and are more amenable to tests.\n\nQuestion 2: If software includes both implementation and unit tests, does it need to be manually verified?\n\nAnswer: Without question, a full set of unit tests and correct implementation help identify failures or misinterpretations of the requirements. A failed test is likely due to a flawed test or a flawed implementation and provides an opportunity to review the requirements. While testing helps identify such issues, we have observed many cases where the generated tests lack full coverage and may be incorrect. Similarly, there are numerous examples of incorrect implementations. The authors recommend manual verification in addition to complete unit tests.\n\n333\n\n334\n\nWriting and Maintaining Unit Tests\n\nFurther reading\n\n\n\nKent Beck’s book on extreme programming. 2000. Extreme Programming Explained: Em- brace Change.\n\n\n\nKent Beck’s book on test-driven development. 2002. Test-Driven Development by Example.\n\n\n\nEffects of Test-Driven Development: A Comparative Analysis of Empirical Studies. Simo Mäkinen & Jürgen Münch: https://link.springer.com/chapter/10.1007/978-3-319-03602-1_10\n\n\n\nLeveraging Large Language Models for Python Unit Test. Medlen Jiri, Bari Emese, Patrick Medlen. 2024: https://ieeexplore.ieee.org/document/10685204.\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n14\n\nGenAI for Runtime and Memory Management\n\nIn the age of big data, artificial intelligence, and intensive data processing, an essential char- acteristic of well written Python programs is their ability to efficiently manage both memory usage and runtime performance. As part of the Software Development Life Cycle (SDLC), it is common practice to profile our programs, anticipate performance limitations and strive for efficiency in both aspects.\n\nIn addition to utilizing traditional tools for profiling program performance and addressing inef- ficiencies, we can leverage the model mastery of Large Language Models (LLMs) to code profile snippets, analyze maximal capacities, and optimize code to handle larger workloads.\n\nIn this chapter, we will examine two examples of inefficient implementations: a Fibonacci calcu- lator with significant runtime overhead and a statistical analysis function that demands excessive memory for large matrices. By utilizing prompt precision best practices and chaining prompt engineering technique, we will demonstrate how GenAI applications can help profile performance metrics, estimate maximum capacities, and optimize the code to handle larger inputs effectively.\n\nThe topics covered in this chapter include:\n\n\n\nIntroducing time and space complexity analysis\n\n\n\nProfiling runtime and memory consumption with GitHub Copilot\n\n\n\nAnalyzing maximal capacity with ChatGPT\n\nOptimizing code with chained prompts\n\n336\n\nGenAI for Runtime and Memory Management\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or Pycharm\n\nOpenAI account with access to ChatGPT and OpenAI API\n\n\n\nAccess to the book’s repository available at: https://github.com/PacktPublishing/ Supercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nIntroducing time and space analysis In our current technology cycle, most companies are leveraging big data and artificial intelligence to improve their businesses. For example, videos on the YouTube and TikTok platforms are viewed over a billion times per day which generates recommendations and advertising value. In these cases, even 1% gains in efficiency for storage or computation may save millions of dollars.\n\nAccording to Lambda Labs, training the generative pre-trained transformer model GPT-3 involved about 700 gigabytes of Random Access Memory (RAM) and thousands of powerful Nvidia GPUs operating in parallel, with a cumulative compute time equivalent to 355 years. Even a minor inefficiency in implementing these models could lead to substantial costs or, in the worst case, create a bottleneck that would prevent the model from running effectively.\n\nThe runtime of a program The runtime of a program refers to the time it takes to execute the tasks defined in the source code on a specific hardware configuration with a given input. The runtime duration of a program depends on factors such as the computational complexity (the number of operations required to complete the task) and the hardware such as the central processing unit (CPU).\n\nChapter 14\n\n337\n\nFor example, consider an inefficient implementation of the Fibonacci number computation using recursion:\n\ndef fibonacci_recursive(n):\n\nif n <= 1:\n\nreturn n\n\nreturn fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2)\n\nIn this implementation, each Fibonacci number computation involves two recursive calls: one for the previous number and another for the one before that. Each of these calls triggers two additional recursive calls, and so on, until the base condition n <= 1 is met.\n\nTo illustrate how quickly the runtime of this implementation grows, here are examples of recorded runtimes on a MacBook Pro with an M1 chip using this implementation:\n\nRuntime for fibonacci_recursive(5): 0.000006 seconds\n\nRuntime for fibonacci_recursive(35): 1.7051 seconds\n\nRuntime of fibonacci_recursive(40): 19.5045 seconds\n\nComputing the fibonacci_recursive(100) is estimated to take 15 trillion seconds, or approxi- mately 475,000 years, making it completely impractical. This exponential growth represents one of the least desirable complexities in algorithm design as it remains inefficient regardless of the hardware used, as we will now explore.\n\nRuntime complexity refresher Since computers vary in speed, it is widespread practice to measure runtime complexity in general terms using Big-O notation, denoted as . This notation describes how the runtime scales relatively to the size of the input data.\n\n𝑂𝑂(…)\n\nFor instance:\n\n\n\nFinding the minimum number in an unsorted array of length of\n\n\n\nhas a runtime complexity\n\n𝑛𝑛𝑛\n\n\n\nPrinting all 𝑂𝑂(𝑛𝑛) plexity of\n\ncombinations of elements in an array of length\n\n. (𝑖𝑖𝑖𝑖𝑖𝑖) 2\n\nhas a runtime com-\n\n𝑛𝑛𝑛\n\n𝑂𝑂(𝑛𝑛\n\n)\n\nRetrieving the nth Fibonacci number using fibonacci_recursive has a runtime complexity of\n\n.\n\n𝑛𝑛\n\n𝑂𝑂(2\n\n)\n\n338\n\nGenAI for Runtime and Memory Management\n\nIf you need a further refresher on Big-O notation or are unfamiliar with it, we recommend checking the Further Reading section at the end of this chapter. Big-O notation is an essential concept for developers, both in application design and day-to-day problem-solving.\n\nThe space consumption of a program The RAM consumed by a program during execution reflects its memory space usage. For instance, a MacBook Pro with 16 gigabytes (GB) of RAM can hold about 2 billion float64 values where each float64 occupies 8 bytes.\n\nConsider the following inefficient function get_top_video that reads a comma-separated values (CSV) file into memory as a Pandas DataFrame and returns the video with the highest average watch percentage:\n\ndef get_top_video(path):\n\ninteractions = pd.read_csv(path)\n\navg_ratio = interactions.mean(axis=0, skipna=True)\n\nreturn avg_ratio.idxmax()\n\nThe input CSV file may represent a matrix as follows, where each cell indicates the percentage of a specific video watched by a user:\n\nvideo_1\n\nvideo_2\n\nvideo_3\n\nvideo_4\n\nvideo_5\n\nvideo_6\n\n...\n\nuser_1\n\n0.5\n\n1\n\nuser_2\n\n0.1\n\n0.7\n\n0.9\n\nIf the input dataset contains 50,000 users (rows) and 50,000 videos (columns), this would involve loading 2.5 billion cells into memory. Such a large dataset exceeds the 16 GB RAM limit, leading to an out-of-memory error and causing the function to fail.\n\nSpace complexity To assess memory usage, developers often use space complexity, expressed in Big-O notation, to describe how memory usage scales with the size of the data structure. For example, in get_top_ video, the space complexity is columns, as the function  for a CSV file with loads the entire dataset into memory.\n\nrows and\n\n𝑂𝑂(𝑚𝑚 𝑚 𝑚𝑚)\n\n𝑚𝑚𝑚\n\n𝑛𝑛𝑛\n\nChapter 14\n\nBalancing efficiency tradeoffs When designing programs, it is common to encounter tradeoffs between memory usage and runtime efficiency. For instance, caching is the process of storing information such as a function’s calculated output values in RAM rather than recomputing them. This can significantly improve the speed of repeated calculations but also increases memory consumption. In a program that calls fibonacci_recursive repeatedly with the same input, such as n=35 which previously took 1.7 seconds to run, caching the result could save 1.7 seconds for each subsequent call.\n\nAnother tradeoff is with the get_top_video we read the entire file into memory before starting the calculations. It may be more practical to read smaller batches of data from disk and compute the cumulative top videos for each batch sequentially rather than handling the entire dataset at once. Reading and processing smaller chunks of large CSV files might take more time but helps avoid exceeding RAM limitations.\n\nNow that we grasp the importance of runtime and space efficiency, how to assess their complexity, and the trade-offs involved, we can move forward and explore how GenAI can assist with each step in managing and optimizing program efficiency.\n\nProfiling runtime and memory consumption with GitHub Copilot Profiling runtime and memory usage is a straightforward process. The built-in time module is useful for tracking runtime and the third-party memory_profiler library monitors memory usage. The next two sub-sections describe how GitHub Copilot can assist by either completing our imple- mentation or generating the code from scratch for these two cases. At the end of this section, we will ask ChatGPT to predict runtimes and memory size constraints using these analyses as input.\n\nProfiling runtime Measuring the runtime of a function in Python helps evaluate whether the time taken to complete the tasks in the source code aligns with acceptable thresholds and identifies potential areas for improvement. This can be achieved using a script like the following:\n\nstart = time.process_time()\n\nrecursive_fibonacci(35)\n\nend = time.process_time()\n\nprint(f\"Time taken: {end – start:.3f} seconds\")\n\n339\n\n340\n\nGenAI for Runtime and Memory Management\n\nBy recording the time before and after execution, we can calculate the runtime as the difference between these times to compute the 35th Fibonacci number.\n\nIn Chapter 15, we will explore how to use decorators to log application performance. For now, we will simply use start and end times to measure program runtime.\n\nSince the profiling code is short and straightforward, the authors mostly use GitHub Copilot for such tasks. The following example demonstrates how Copilot can understand the purpose of our file, profile_runtime.py, by applying the five S’s for prompt precision as shown in Figure 14.1:\n\nFigure 14.1: Copilot completion\n\nIn this example, we named the file profile_runtime.py and included the necessary imports for the time package and the fibonacci_recursive function to establish a clear context for Copilot. Based on these, Copilot is likely to identify our goal of profiling the fibonacci_recursive func- tion and suggests a complete implementation for runtime profiling.\n\nAlternatively, we can use chat, ChatGPT or OpenAI API to generate the profiling script from scratch. For example, we could use the following prompt in Copilot chat:\n\nprint the runtime of fibonacci_recursive for n=35\n\nChapter 14\n\nBelow is a sample implementation provided by Copilot in PyCharm:\n\nFigure 14.2: Copilot chat to write the full method from a profiling suggestion\n\nThis code can be used as-is to profile the runtime of the function. Alternatively, we can make slight modifications, such as rounding the runtime value for better readability.\n\nHere is an output from running the suggested profiling script:\n\nResult: 9227465\n\nRuntime: 1.7093181610107422 seconds\n\nWhile a runtime of 1.7 seconds for n=35 might seem reasonable, the runtime for larger inputs, such as 50 or 100, becomes impractical. Later in this chapter, we will explore techniques to identify and address these challenges.\n\n341\n\n342\n\nGenAI for Runtime and Memory Management\n\nProfiling space usage Profiling a program’s memory usage can be achieved with the @profile decorator from the memory_profiler package. This lightweight tool monitors memory usage for every line within a function. The package should be included in the requirements.txt file of this book’s repository, so it should already be accessible in your virtual environment.\n\nFor example, to profile the function get_top_video, which retrieves the video with the highest average watch percentage, we can add the @profile decorator before the function definition. A call to the function with the file interactions_10_000.csv that records 10,000 users’ watch percentages of 10,000 videos will log the memory consumption for each line:\n\nimport pandas as pd\n\nfrom memory_profiler import profile\n\n@profile\n\ndef get_top_video(path):\n\ninteractions = pd.read_csv(path)\n\navg_ratio = interactions.mean(axis=0, skipna=True)\n\nreturn avg_ratio.idxmax()\n\nget_top_video('interactions_10_000.csv')\n\nThe profiling output provides details such as line numbers, total memory usage, incremental memory usage, the number of times each statement was executed, and the content of those executions. For reference, Figure 14.3 shows the output from the code above:\n\nFigure 14.3: Profiler output showing memory consumption by line\n\nChapter 14\n\nThe file interactions_10_000.csv contains 10,000 x 10,000 cells of type float64, requiring 8 bytes each, and the expected memory usage is approximately:\n\nMB\n\n10,000 × 10,000 × 8  ≈ 800\n\nThis aligns closely with the memory consumption recorded at line 7, where loading the CSV file into a Pandas DataFrame consumed 889.7 MB. This includes both the memory required for the cell data and the overhead associated with the Pandas DataFrame object such as column and row names, metadata, and type.\n\nHaving learned how to use GitHub Copilot to simplify the creation of profiling scripts for runtime and space complexity, we can move forward and explore how to use Copilot to profile multiple runs and ChatGPT to estimate the maximum input capacity that our functions can manage.\n\nAnalyzing maximal capacity with ChatGPT Identifying the maximal capacity of a function means determining the largest input beyond which it will fail due to either memory limitations or unacceptable runtime. One strategy for such predictions involves profiling its performance for a few input sizes, identifying the runtime or space growth, and then estimating the maximal capacity the function can handle. GenAI can assist in this two-step process by automating profiling for different inputs and using the results as supplementary data for a prompt to compute the maximal capacity.\n\nProfiling runtime across multiple runs Copilot can assist us in creating a profiling loop for fibonacci_recursive like we did for a single profile. For instance, if we want to profile the runtime of the function for inputs ranging from 10 to 40 in steps of 5, we can start by adding the following import statements to the file:\n\nimport time\n\nfrom fibonacci import fibonacci_recursive\n\nThen use a prompt in Copilot chat like this:\n\nprint the runtime of fibonacci_recursive for n=10:41:5\n\n343\n\n344\n\nGenAI for Runtime and Memory Management\n\nThis prompt, along with the surrounding context of the import statements in the file, is likely to generate a code to profile runtimes for varying arguments:\n\nFigure 14.4: Profiling suggestion for profiling multiple Fibonacci inputs\n\nWe can use the generated code as-is or make minor adjustments, such as rounding the recorded times to four decimal places or changing the range’s stop value to 41. When executed on an M1 MacBook Pro with a 16GHz CPU, the following runtime profiling was recorded:\n\nRuntime for fibonacci_recursive(10): 0.0000 seconds\n\nRuntime for fibonacci_recursive(15): 0.0002 seconds\n\nRuntime for fibonacci_recursive(20): 0.0013 seconds\n\nRuntime for fibonacci_recursive(25): 0.0150 seconds\n\nRuntime for fibonacci_recursive(30): 0.1571 seconds\n\nRuntime for fibonacci_recursive(35): 1.7051 seconds\n\nRuntime for fibonacci_recursive(40): 19.5045 seconds\n\nThis runtime profiling will be used as input for a GenAI application to analyze the function’s complexity and determine the input size capacity.\n\nChapter 14\n\nProfiling memory consumption across multiple runs Profiling the memory consumption across multiple runs of get_top_video function can be done with a straightforward approach:\n\npaths = ['interactions_100.csv',\n\n'interactions_1000.csv',\n\n'interactions_10_000.csv']\n\nfor p in paths:\n\nprint(\"top video: \", get_top_video(p))\n\nIn this example, the files interactions_100.csv, interactions_1000.csv, and interactions_10_000.csv represent datasets of user-video interactions of row and column length corresponding to the numbers in their filenames.\n\nCopilot can assist by completing the for loop, after including the filenames, as shown below:\n\nFigure 14.5: Copilot code completion suggestion\n\nThe generated code can be used as-is or modified slightly, for instance, by enhancing the print statement to include information about the file size.\n\n345\n\n346\n\nGenAI for Runtime and Memory Management\n\nFor reference, running this code produced the following profiling:\n\nFigure 14.6: profiling memory usage output\n\nThe memory usage for reading the CSV files grows as follows:\n\n\n\n0.6 MB for a 100 x 100 matrix\n\n\n\n29.9 MB for a 1,000 x 1,000 matrix\n\n\n\n889 MB for a 10,000 x 10,000 matrix\n\nThis memory consumption data can be included as supplementary information in our prompts included as supplementary information in our prompts to estimate maximum matrix sizes that the function can handle, as we see next in this chapter.\n\nChapter 14\n\nDetermining maximal capacity within runtime constraints The GPT-4o1 model is specifically designed to excel in chain-of-thought reasoning, making it highly effective for determining maximum capacities under runtime constraints. To leverage this capability, we can create precise prompts following the five S’s, including the function’s source code and associated profiling. An ideal structure for these prompts might look as follows:\n\nCONTEXT: You are provided with:\n\n1. Python function enclosed with {{{ FUNCTION }}}\n\n2. Runtime profiling enclosed with {{{ PROFILING }}}.\n\n3. Runtime limit enclosed with {{{ LIMIT }}}\n\nTASK: What is the maximal input the function can run in the time limit?\n\nFUNCTION: {{{ ... }}}\n\nPROFILING: {{{ ... }}}\n\nLIMIT: {{{ ... }}}\n\nMAXIMAL INPUT:\n\nFor example, when using this prompt with a runtime limit of 100 seconds, the GPTo1-mini model in ChatGPT provided the following reasoning steps:\n\n1. Profiling Analysis: Repeatedly reviewed the reported profiling.\n\n2. Observations: Identified the exponential growth in runtime.\n\n3. Calculations: Computed the largest input within the time limit.\n\n4. Conclusion: Determined the maximum input to be 43.\n\nThese reasoning steps result in the final answer, the conclusion step, as illustrated in Figure 14.7:\n\nFigure 14.7: GPT-4o1-mini maximal capacity conclusion\n\n347\n\n348\n\nGenAI for Runtime and Memory Management\n\nThe model predicted a runtime of 78.5 seconds for n=43 and 123 seconds for n=44. When we mea- sured the actual runtime using our script, it was 81.08 seconds and 129.07 seconds respectively, demonstrating a close match and correct identification of the maximal capacity:\n\nElapsed time for n=43: 81.08 seconds\n\nElapsed time for n=44: 129.07 seconds\n\nFor your reference, here is the full prompt used in this example:\n\nCONTEXT: You are provided with:\n\n1. Python function enclosed with {{{ FUNCTION }}}\n\n2. Runtime profiling enclosed with {{{ PROFILING }}}.\n\n3. Runtime limit enclosed with {{{ LIMIT }}}\n\nTASK: What is the maximal input the function can run in the time limit?\n\nFUNCTION: {{{\n\ndef fibonacci_recursive(n):\n\nif n <= 0:\n\nreturn 0\n\nif n == 1:\n\nreturn 1\n\nreturn fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2)\n\n}}}\n\nPROFILING: {{{\n\nRuntime for fibonacci_recursive(10): 0.0000 seconds\n\nRuntime for fibonacci_recursive(15): 0.0002 seconds\n\nRuntime for fibonacci_recursive(20): 0.0013 seconds\n\nRuntime for fibonacci_recursive(25): 0.0150 seconds\n\nRuntime for fibonacci_recursive(30): 0.1571 seconds\n\nRuntime for fibonacci_recursive(35): 1.7051 seconds\n\nRuntime for fibonacci_recursive(40): 19.5045 seconds\n\n}}}\n\nLIMIT: {{{ 100 seconds }}}\n\nMAXIMAL INPUT:\n\nIn the upcoming chapters, we will explore how to intentionally collect data for integration with OpenAI API as part of SDLC automation.\n\nChapter 14\n\nDetermining maximal capacity within memory constraints A similar prompt structure to the one used for identifying maximum capacities under runtime constraints can also be applied to memory constraints:\n\nCONTEXT: You are provided with:\n\n1. Python function enclosed with {{{ FUNCTION }}}\n\n2. RAM usage profiling enclosed with {{{ PROFILING }}}.\n\n3. RAM usage limit enclosed with {{{ LIMIT }}}\n\nTASK: What is the maximal input the function can run in the RAM limit?\n\nFUNCTION: {{{ ... }}}\n\nPROFILING: {{{ ... }}}\n\nLIMIT: {{{ ... }}}\n\nMAXIMAL ROWS / MAXIMAL COLUMNS:\n\nHere, the cue in the prompt is adjusted slightly to MAXIMAL ROWS / MAXIMAL COLUMNS, to empha- size the 2D dimensions of the input.\n\nFor instance, when using this template for the get_top_video function in ChatGPT with the GPT-4o1 model, along with recorded results from sparse matrices of sizes 100 x 100, 1,000 x 1,000, and 10,000 x 10,000, the model generated the following reasoning steps:\n\n1. Profiling Analysis: Included an understanding of the memory usage data and its scaling\n\nbehavior.\n\n2. Conclusion: Estimated the largest input for a memory limit of 5GB.\n\n3. Final Recommendation: Suggested a matrix size of 22,000 x 22,000 as the largest fea-\n\nsible input.\n\nThese reasoning steps result in the final answer, the final recommendation step, as illustrated in Figure 14.8:\n\n349\n\n350\n\nGenAI for Runtime and Memory Management\n\nFigure 14.8: GPT-4o1-mini reasoning for maximal capacity\n\nWhen executing the script with a file containing a 22,000 x 22,000 matrix, the recorded memory usage peak was 4.63 GB, closely aligning with the model’s prediction.\n\nFigure 14.9: Memory usage below 5GB\n\nThis memory usage aligns with ChatGPT’s capacity assessment and is below the required 5GB RAM limit.\n\nFor reference, here is the complete prompt used to generate this output:\n\nCONTEXT: You are provided with:\n\n1. Python function enclosed with {{{ FUNCTION }}}\n\n2. RAM usage profiling enclosed with {{{ PROFILING }}}.\n\n3. RAM usage limit enclosed with {{{ LIMIT }}}\n\nTASK: What is the maximal input the function can run in the RAM limit?\n\nFUNCTION: {{{ def get_top_video(path):\n\ninteractions = pd.read_csv(path)\n\navg_ratio = interactions.mean(axis=0, skipna=True)\n\nChapter 14\n\nreturn avg_ratio.idxmax() }}}\n\nPROFILING: {{{...}}}\n\nLIMIT: {{{ 5 GB }}}\n\nMAXIMAL ROWS / MAXIMAL COLUMNS:\n\nThe profiling information was omitted due to brevity constraints and includes a direct copy-paste of the terminal output, as shown in Figure 14.6. The full prompt can be found at ch14/code_samples/ chatgpt_maximum_input_prompt.txt. This prompt successfully predicted a maximum input size of 22,000 rows and 22,000 columns.\n\nIn both these cases, ChatGPT is certainly extrapolating the input values to predict the runtime or maximum dimensions. It requires sufficient input values to make such a prediction and may not be as accurate as performing your own calculations, but it is a reasonable approximation.\n\nNext in this chapter, we will explore how to use GenAI and increase the capacity of the functions such as algorithms with better runtime complexity and dealing with chunks.\n\nOptimizing code with chained prompts The previous examples depict the simple coding and estimation capability of LLMs. However, LLMs can do much more. They can create code for improved algorithms and even balance specific runtime and memory efficiency goals.\n\nTo effectively increase the input capacities, we will chain our optimization prompt to the results of the maximal capacity prompt. The initial prompt already provides essential details, such as the function, runtime or memory limits, and profiling various inputs. The optimization prompt should also include the following key information:\n\nOptimization goal: Clearly specify the primary goal, either on improving runtime com- plexity, reducing memory usage, or both. Detail the desired approach as much as possible, such as batching, or improving algorithm complexity.\n\n\n\nFunction docstring: Provide a concise description of the function’s purpose, along with any constraints or requirements.\n\n\n\nInput requirements: Define the new input goals under the runtime or memory constraints, such as computing the 1,000,000th Fibonacci number or handling a matrix with 100,000 rows and 100,000 columns.\n\n351\n\n352\n\nGenAI for Runtime and Memory Management\n\nFor more complex scenarios, additional details may be helpful, such as previous optimization attempts, dependencies on external functions or resources, frequency of function calls or con- current usage patterns, and acceptable trade-offs, such as allowing longer runtimes in exchange for reduced memory usage.\n\nThis information will be integrated into a chained prompt, following the five S’s for precision and clarity.\n\nRuntime optimization Let us examine a runtime optimization for the fibonacci_recursive function. Below is an exam- ple of a chained prompt, building on an initial prompt of maximal capacity as presented earlier in the chapter. This prompt requests an improved implementation with better runtime complexity to handle an input of 1,000,000:\n\nCONTEXT: You are provided with the function's docstring enclosed with {{{ DOCSTRING }}} and the required input enclosed with {{{ REQUIRED_INPUT }}}.\n\nTASK: Optimize the function to run in a better time complexity within the runtime limit.\n\nDOCSTRING: {{{ \"\"\"\n\nCalculate the nth Fibonacci number using a recursive approach.\n\nArgs:\n\nn (int): The position in the Fibonacci sequence.\n\nReturns:\n\nint: The nth Fibonacci number.\n\n\"\"\"\n\n}}}\n\nREQUIRED_INPUT = {{{ 1,000,000 }}}\n\nIMPROVED IMPLEMENTATION:\n\nThis prompt includes a docstring describing out specific task of optimizing runtime complexity to achieve the target input of 1,000,000. Providing different constraints, such as a smaller input size, will yield a different output from the model.\n\nA typical approach for Fibonacci calculations has a ed a more efficient implementation for large inputs, achieving a runtime complexity of through the fast-doubling recursive approach:\n\n complexity, however GPT-4o1 suggest- \n\n𝑂𝑂(𝑛𝑛)\n\n𝑂𝑂(log𝑛𝑛)\n\nChapter 14\n\n353\n\nFigure 14.10: Fast-doubling approach for handling large inputs\n\nAnd here is the code proposed by GPT-4o1:\n\ndef fib_pair(n):\n\nif n == 0:\n\nreturn (0, 1)\n\na, b = fib_pair(n // 2)\n\nc = a * ((2 * b) - a)\n\nd = a * a + b * b\n\nif n & 1:\n\nreturn (d, c + d)\n\nelse:\n\nreturn (c, d)\n\nWhile this implementation also uses recursion, it makes a call to reducing the number of function invocations to approximately here are performance comparisons for calculating the 1,000,000th Fibonacci number:\n\n ( without remainder) . To illustrate its efficiency, 𝑛𝑛𝑛 𝑛 𝑛2\n\n𝑛𝑛𝑛 𝑛𝑛⁄\n\nlog(𝑛𝑛)\n\nFast-doubling approach: 0.057893 seconds\n\nIterative (memoization) approach: 9.175633 seconds\n\nAs another benchmark, computing the 10,000,000th Fibonacci number with fibonacci_fast_ doubling took 2.26 seconds, whereas the iterative approach did not complete within a 10-minute time limit.\n\n354\n\nGenAI for Runtime and Memory Management\n\nThis method leverages the mathematical identities:\n\n\n\n𝐹𝐹(2𝑘𝑘)  =  𝐹𝐹(𝑘𝑘)  ⋅ (2𝐹𝐹(𝑘𝑘 𝑘 𝑘) − 𝐹𝐹(𝑘𝑘)) \n\n2\n\n2\n\nWhere\n\n𝐹𝐹(2𝑘𝑘 𝑘 𝑘)  =  𝐹𝐹(𝑘𝑘 𝑘 𝑘)\n\n is the k-th Fibonacci number.\n\n𝑘𝐹𝐹 (𝑘𝑘)\n\nUsing these identities, once  can be de- termined efficiently. This means we do not need to compute all Fibonacci numbers sequentially up to the 1,000,000th number but only focus on its halves, significantly reducing the required computations.\n\n𝐹𝐹(𝑘𝑘)\n\n and\n\n are calculated,\n\n and\n\n𝐹𝐹(𝑘𝑘)\n\n𝐹𝐹(𝑘𝑘 𝑘 𝑘)\n\n𝐹𝐹(2𝑘𝑘)\n\n𝐹𝐹(2𝑘𝑘 𝑘 𝑘)\n\nSpace optimization Like the runtime optimization prompt, we can chain a request to optimize a function to fit within RAM constraints. The prompt structure remains similar, but it describes a memory limit instead of a runtime limit, along with an optimization request to address memory usage.\n\nFor example, consider the following chained prompt requesting an optimization for the get_top_ video function to handle a matrix of 100,000 rows by 100,000 columns within a 5GB RAM limit:\n\nCONTEXT: You are provided with the function's docstring enclosed with {{{ DOCSTRING }}} and the required input enclosed with {{{ REQUIRED_INPUT }}}.\n\nTASK: Optimize the function to run in batches within the RAM limit.\n\nDOCSTRING: {{{ \"\"\"\n\nReads a CSV file containing video interactions, calculates the average watch ratio for each video,\n\nand returns the video with the highest average watch ratio.\n\nArgs:\n\npath (str): The file path to the CSV file containing video interactions.\n\nReturns:\n\nstr: The identifier of the video with the highest average watch ratio.\n\n\"\"\"\n\n}}}\n\nChapter 14\n\nREQUIRED_INPUT = {{{ 100,000 rows x 100,000 columns matrix}}}\n\nIMPROVED IMPLEMENTATION:\n\nThis prompt specifies the size of the input matrix (100,000 rows by 100,000 columns) and builds upon the previously defined RAM limit of 5GB.\n\nFigure 14.11: ChatGPT suggestion for chunk-based processing\n\nBased on this information, ChatGPT suggested processing the CSV file in batches of 1,000 rows, which is an appropriate solution for handling data of this size efficiently.\n\nIn this scenario, specifying the input requirements is vital. For larger matrices, working with a Pandas DataFrame may become impractical, necessitating an alternative solution. Conversely, for matrices with fewer columns, larger column chunks might be sufficient.\n\nFor your reference, here is the implementation for processing the matrix in chunks provided by ChatGPT with the prompt above:\n\ndef get_top_video(path):\n\ncumulative_sum = None\n\ncumulative_count = None\n\nchunksize = 1000\n\nfor chunk in pd.read_csv(path, chunksize=chunksize):\n\nchunk_sum = chunk.sum(skipna=True)\n\nchunk_count = chunk.count()\n\n355\n\n356\n\nGenAI for Runtime and Memory Management\n\nif cumulative_sum is None:\n\ncumulative_sum = chunk_sum\n\ncumulative_count = chunk_count\n\nelse:\n\ncumulative_sum += chunk_sum\n\ncumulative_count += chunk_count\n\naverage_ratio = cumulative_sum / cumulative_count\n\ntop_video = average_ratio.idxmax()\n\nreturn top_video\n\nWhen profiling the implementation with an input of 100,000 rows and 100,000 columns, the chunked implementation concluded with a memory usage of 4,326.1 GB:\n\nFigure 14.12: Improved memory utilization with chunking\n\nIn line 13, we can see that the matrix was processed in 100 chunks, with each chunk containing 1,000 columns.\n\nChapter 14\n\nThe proposed solutions can be further refined to meet both memory and runtime requirements. It is always advisable to test new implementations against both criteria to ensure the solution is satisfactory in terms of both efficiency and performance.\n\nFurther runtime and space optimization The previous optimizations focused on single CPU algorithms. GenAI can go beyond these and recommend additional techniques. For instance, it may be appropriate to use parallelization with threads, leverage graphical processing units (GPUs), or adopt more efficient file formats like sparse matrices or Apache Parquet. In the function get_top_video, processing different videos can be performed in parallel by integrating threading. Further acceleration could be achieved by leveraging GPUs, as each column can be processed independently of others. Alternatively, converting the file format from CSV to a more efficient option could enhance performance. For- mats like compressed sparse row (CSR) or compressed sparse column (CSC) from SciPy are well-suited for sparse data, while columnar storage formats like Apache Parquet are ideal for handling large datasets.\n\nSummary In this chapter, we explored how LLMs can assist in ensuring efficient applications by profiling runtime and memory usage, identifying maximal capacities, and suggesting optimized code to increase those capacities.\n\nUsing the recursive Fibonacci example, we saw how adopting a more efficient algorithm sig- nificantly reduces runtime. With the get_top_video function, we tackled large matrices under RAM constraints, assisting GenAI to optimize memory usage through chunking. GitHub Copilot assisted in profiling runtime and memory consumption and profiling runs across different in- puts. ChatGPT estimated the maximal capacity within runtime and RAM constraints. Leveraging chained prompts, ChatGPT demonstrated the ability to vastly improve the implementation to achieve larger capacities and can do much more.\n\nIn the next chapter, we will further explore how to integrate GenAI into the SDLC, focusing on logging, monitoring applications, and error handling.\n\n357\n\n358\n\nGenAI for Runtime and Memory Management\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: How can Copilot assist in profiling the runtime and memory usage of a program?\n\nAnswer: Profiling runtime and memory usage in Python can be achieved using the built-in time module and the third-party memory-profiler package. Copilot can assist by either completing a starter code or generating that functionality from scratch based on a precise prompt.\n\nQuestion 2: How can ChatGPT help identify maximal inputs under runtime or RAM constraints?\n\nAnswer: By crafting precise prompts that include the function’s source code, runtime or RAM constraints, and profiling results, we can utilize the GPT-4o1 model, designed for chain-of-thought reasoning, to calculate the largest input manageable within those constraints.\n\nQuestion 3: How can chaining prompts help increase program input capacity?\n\nAnswer: After identifying the maximal input a function can handle without becoming a bottle- neck, a chained prompt can include the desired input size and a specific optimization request. GPT-4o1 can leverage this information to propose an optimized solution that follows the runtime and/or memory constraints.\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\nWikipedia, CPU: https://en.wikipedia.org/wiki/Central_processing_unit\n\nGeeks for geeks memory consumption: https://www.geeksforgeeks.org/how-to-get-\n\ncurrent-cpu-and-ram-usage-in-python\n\n\n\nIntroduction to memory profiling in Python: https://www.datacamp.com/tutorial/ memory-profiling-python\n\n\n\nBig-o Python package: https://pypi.org/project/big-O-calculator/\n\n\n\nLambda Labs on GPT-3 training costs: https://lambda.ai/blog/demystifying-gpt-3\n\nChapter 14\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.\n\n359\n\n15\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nLarge Language Models (LLMs) can supercharge our going-live processes, which refers to de- ploying code to production. But if prompted incorrectly, they can also degrade code quality. In this chapter, we will examine how, even when following the five S’s to prompt GenAI for logging, monitoring, and input validation, the resulting code can become cluttered, combining these elements with core functionality and violating the single responsibility principle. For instance, models sometimes suggest using print statements instead of proper logging, handling input validation within functions, or incrementing counters without reporting their values. This vio- lates the single responsibility principle and reduces overall code quality.\n\nTo address this, we will apply the Chain-of-Thought (CoT) prompt engineering technique to GitHub Copilot to generate well-monitored code without interfering with the function’s core functionality, leveraging decorators as a design pattern. We will also leverage few-shot learning to guide it toward following a coding style. While GitHub Copilot is well suited for these tasks, we will explore how similar results can be achieved using CoT with few-shot learning in ChatGPT and CoT with fine-tuning in OpenAI API.\n\nThe key topics covered in this chapter include the following:\n\n\n\nIntroducing logging, monitoring, and raising errors\n\n\n\nLeveraging GenAI for higher-level coding patterns\n\n\n\nApplying inverse CoT with ChatGPT and OpenAI\n\nUtilizing few-shot learning and fine-tuning as style guides\n\n362\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\n\n\nA GitHub Copilot account\n\n\n\nAn integrated development environment (IDE) – either VS Code or PyCharm\n\n\n\nAn OpenAI account with access to ChatGPT and OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/ Supercharged-Coding-with-Gen-AI\n\n\n\nA virtual environment set up in VS Code or PyCharm\n\n\n\nAn OpenAI API token\n\nFor assistance with setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help with creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nIntroducing logging, monitoring, and raising errors When Python software goes live in the production stage and serves clients, it is important to verify that it is consistently working as expected in real-world use cases. If some problems are identified, understanding the behavior of these issues is critical. Thus, logging, monitoring, and raising errors for problematic inputs are an essential aspect of the Software Development Life Cycle (SDLC). They involve tracking key checkpoints within internal processes, which are usually performed by loggers, and these facilitate debugging and other analyses. Recording performance statistics such as counting function calls is often used with profiling to improve performance. Lastly, han- dling unexpected scenarios, such as receiving arguments of an invalid type, is a process to ensure that the code is robust against the wide range of user inputs. These three aspects help maintain software quality, minimize unexpected issues, and simplify debugging when problems occur.\n\nIn this chapter, we will focus on these three fundamental practices since these are typically the first going-live initiatives in any project, and they are likely to be relevant to your software as well. As your strategy expands to additional metrics and error-handling techniques, these standard approaches using GenAI tools can easily be generalized and extended from what we introduce in this chapter.\n\nChapter 15\n\nThe FizzBuzz use case To explore how monitoring efforts can be integrated into our code, let us consider developing a program that generates the FizzBuzz sequence, a coding challenge introduced in Chapter 2. As part of the monitoring efforts for this program, we want to log each function call along with its arguments, maintain a counter of total function calls, and ensure inputs are positive integers less than 500.\n\nAs a starting point, a basic implementation of FizzBuzz can be generated from the function sig- nature:\n\ndef print_fizzbuzz(limit: int) -> None:\n\nFigure 15.1 shows an implementation suggested by GitHub Copilot in PyCharm:\n\nFigure 15.1: Copilot’s initial FizzBuzz implementation\n\nThis implementation serves as a foundation for incorporating going-live features. Before intro- ducing best practices for monitoring with prompt engineering techniques, we will first explore how simply chaining logging, monitoring, and error-raising requests can degrade code quality. Adding GenAI implementation for these tasks results in a longer and more complex function. However, techniques such as CoT and few-shot learning can help add these functionalities yet maintain clean, well-structured code.\n\n363\n\n364\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nLogging Logs are generated text records that serve as a diary of our program’s execution. For example, at the start of the FizzBuzz sequence, we may want to log the call’s arguments. While using a simple print statement is an option, the built-in logging library is a much better choice as it can write to the console but also provides more control over the output and where the logs can be written, such as files and remote servers. It enables the structured formatting of records and log-level management, which can report all outputs for debugging, or a subset of warnings based on importance.\n\nTo use the logging library, we typically define a logger instance with the module’s name:\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nThis creates a logger instance linked to the module name using the __name__ special attribute (also referred to as a magic attribute). This association allows better filtering and customization of log messages in that file.\n\nWhen using GenAI for logging, it is essential to separate the logging task into its own prompt, rather than embedding it within the implementation prompt. This approach aligns with the single-task principle, one of the five S’s best practices introduced earlier in the book.\n\nFor example, when working with Copilot in VS Code’s edits window, we can select the print_ fizzbuzz implementation and use the following prompt:\n\n#selection log the function arguments\n\nFor your reference, here is the suggestion we got when using the 3o-mini model as appears in Figure 15.2:\n\nChapter 15\n\nFigure 15.2: Log suggestion for FizzBuzz\n\nIn PyCharm, we can leverage the inline chat after selecting the function’s code, and prompt the following to yield similar results:\n\nLog the function arguments\n\nWhile the generated log statement correctly logs the function’s argument, it also introduces two lines of unnecessary code clutter within the function, which should remain focused on printing the FizzBuzz sequence. Later in the chapter, we will explore how to maintain cleaner code with prompt engineering techniques.\n\nHandling unexpected input When our program receives unexpected inputs at runtime, such as incorrect argument types (for example, float instead of int) or values that are beyond a manageable range for our program, we want to control the program’s behavior by raising errors rather than failing or producing unpredictable results.\n\nFor instance, in the print_fizzbuzz function, the limit argument is expected to be a positive integer so that we can print numbers, fizz, or buzz from 0 up to that value. An additional con- straint might be that the input should be less than 500 to ensure the output fits on the screen. To enforce both requirements with error handling, we can leverage Copilot.\n\n365\n\n366\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nWhen using Copilot in VS Code’s edit window, we can select the print_fizzbuzz code and use the following prompt:\n\n#selection validate limit type, and that it is in the range [0,500]\n\nIn PyCharm, we can use the inline chat by selecting the print_fizzbuzz function and entering the following prompt:\n\nvalidate limit type, and that it is in the range [0,500]\n\nFor reference, here is the result obtained using Copilot for VS Code with the o3 mini model:\n\nFigure 15.3: VS Code output correctly handles inputs but creates code clutter\n\nThis recommendation verifies both the argument’s type and its range limits, as requested. How- ever, like the logging prompt, it introduces an additional responsibility of validating inputs to the function. Later in this chapter, we will explore how prompt engineering can help generate higher-quality code.\n\nMonitoring Another key aspect of going live is tracking the usage of different functions. By recording metrics, we can collect valuable statistics on performance, call frequency, and execution rates.\n\nA simple starting point is to track the number of function calls in our program. To do this with print_fizzbuzz, we can define a global counter variable, FIZZBUZZ_COUNTER = 0, and instruct Copilot to increment it before or after each successful execution. For example, when working with Copilot in VS Code’s edits window, we can select the print_fizzbuzz implementation and\n\nChapter 15\n\nuse the following prompt:\n\n#selection increment FIZZBUZZ_COUNTER after a successful execution.\n\nSimilarly, in PyCharm, we can select the code and use the inline chat to prompt the following:\n\nIncrement FIZZBUZZ_COUNTER after a successful execution.\n\nFigure 15.4 illustrates a suggested completion for this prompt when using Copilot in VS Code with the o3 mini model:\n\nFigure 15.4: VS Code adds a record usage counter\n\nAt this point, our print_fizzbuzz function integrates multiple monitoring components: logging, handling invalid inputs, and counting calls. While these fulfill our monitoring needs, they lead to a lengthy and less readable function. This violates clean coding principles, particularly the single responsibility principle, which emphasizes that functions should focus on a single task.\n\nCode created by GenAI using simple prompts tends to generate code and add it within existing methods which can lead to cluttered implementations. To maintain clean and structured code, we can leverage CoT prompting. This approach enables us to handle monitoring at a higher ab- straction level, as we will explore next.\n\n367\n\n368\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nLeveraging GenAI for higher-level coding patterns To balance clean code principles, such as the single responsibility principle, with the need for going-live requirements, a Pythonic approach is to use higher-level coding patterns such as dec- orators. Rather than embedding logging statements within a function, we delegate this respon- sibility to a decorator pattern:\n\ndef log_function_args(func: callable):\n\n# logging logic here\n\n@log_function_args\n\ndef print_fizzbuzz(limit: int) -> None:\n\n# main functionality here\n\nHere, log_function_args is responsible for logging function calls, allowing print_fizzbuzz to focus on handling the FizzBuzz sequence. This approach ensures cleaner, more modular code while enabling logging for multiple functions.\n\nGenAI applications may not suggest this pattern unless we actively steer them toward clean coding practices. As introduced in Chapter 9, CoT prompting helps LLMs reason through complex tasks. We can direct the model toward better design choices by using CoT prompting and explicitly writing a decorator call, such as @log_function_args, prior to implementing the actual decorator function. This will result in cleaner and more maintainable implementations.\n\nUsing decorators to separate responsibilities A decorator is a structural design pattern that enables stacking tasks before and after a function call without altering the function’s code. It is a specialized use case of a closure, where a function retains access to its associated arguments. A typical decorator implementation looks like this:\n\ndef sample_decorator(func: callable) -> callable:\n\ndef wrapper(*args, **kwargs):\n\nprint(\"Function is wrapped\")\n\nreturn func(*args, **kwargs)\n\nreturn wrapper\n\nA common usage involves applying the @ annotation, as shown here:\n\n@sample_decorator\n\ndef foo(*num):\n\nreturn len(nums)\n\nChapter 15\n\nHere, the sample_decorator decorator accepts a function object and returns a wrapper function that takes the same arguments as the original function but prints Function is wrapped before execution.\n\nLLMs can certainly include decorators in their generated code as their training data contains many common decorator implementations. For well-known use cases such as timers, loggers, argument validation, and retry mechanisms, the models can generate decent implementations based on the decorator’s signature when prompted to.\n\nNext, we will explore how to apply inverse CoT prompting to generate the desired implementation.\n\nInverse CoT for decorator implementation In Chapter 9, we explored how defining a function that calls low-level implementations provides strong guidance for GitHub Copilot. With inverse CoT, we take a similar approach by referencing functions that have not yet been defined. We use the term inverse because we want the decorators to appear earlier in the file than the function that uses them. To achieve this, we place the cursor higher in the file, allowing Copilot to generate code above an existing implementation.\n\nAs a review, here is an example of CoT prompting used in Chapter 9 to implement a variation of a geometric mean calculator, where Copilot successfully generated the missing low-level functions:\n\nFigure 15.5: CoT-driven implementation of a higher-level function with GitHub Copilot\n\nIn this approach, the function is broken into named functions that are intentionally called undefined. Copilot understands that its role is to generate these necessary implementations. Such an approach enables Copilot to implement the missing functions, producing structured and predictable code.\n\n369\n\n370\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nWe can apply the same technique when implementing going-live functionality, by defining a dec- orator structure and allowing Copilot to generate the implementation. By merely adding calls to the required decorators, we guide Copilot to generate their corresponding implementations. For instance, in our FizzBuzz use case, we can layer three monitoring functionalities onto our function:\n\n1. Validate the input: Define a decorator with parameters to enforce input bounds, that is,\n\n@validate_args_types_and_limits(0, 500).\n\n2.\n\nIncrement a global counter:\n\n3. @increment_counter.\n\n4. Log function calls:\n\n5. @log_function_args.\n\nFor reference, here is how stacking these decorators would appear before Copilot generates their implementations:\n\nFigure 15.6: Inverse CoT approach\n\nHere, as usual with the CoT technique, the IDE will correctly mark the decorators as undefined with the red underlines. After calling the decorators, we move the cursor to line 7 before the call to the first decorator, press Enter three times to create space for their implementations, then return to line 8 and type the following:\n\ndef l\n\nChapter 15\n\nThis prompt is sufficient for Copilot to infer that it should generate the first decorator, interpreting l as a reference to log_function_args. Moving the cursor up and pressing Enter three times is all that is involved in inverse CoT, but it is very effective. As shown in Figure 15.7, this cue enables Copilot to generate a complete implementation of a logging mechanism, including the function’s arguments.\n\nFigure 15.7: Copilot’s decorator implementation\n\nThe suggested implementation of log_function_args correctly logs the function name and arguments as an info-level log statement.\n\nAfter the initial def l prompt, pressing the Enter key twice is enough for Copilot to generate the next decorator, increment_counter, as shown in Figure 15.8:\n\n371\n\n372\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nFigure 15.8: Enter key as a cue for Copilot to implement the next decorator\n\nThe Enter key serves as a sufficient cue for the next implementation. This highlights how Copi- lot can supercharge our workflow. For your reference, here is the suggested implementation of increment_counter by Copilot:\n\ndef increment_counter(func):\n\ndef wrapper(*args, **kwargs):\n\nglobal FIZZBUZZ_COUNTER\n\nFIZZBUZZ_COUNTER += 1\n\nlogger.info(f\"Function {func.__name__} called {FIZZBUZZ_COUNTER} times\")\n\nreturn func(*args, **kwargs)\n\nreturn wrapper\n\nThis decorator correctly increments the previously initialized global variable, FIZZBUZZ_COUNTER, before the function call. If we prefer to increment it after execution, we could have used a different decorator’s call, such as @increment_counter_after_call.\n\nChapter 15\n\nFinally, Copilot’s suggested implementation for argument validation efficiently manages both type-checking and value constraints using a parameterized decorator. This type of decorator introduces an additional closure layer, allowing us to specify arguments, such as the minimum and maximum limits, in this case.\n\nBy calling @validate_args_types_and_limits(0, 500), we provide a clear cue for Copilot to recognize the need for a parameterized decorator with the given constraints. Here is the suggested implementation:\n\ndef validate_args_types_and_limits(min_limit: int, max_limit: int):\n\ndef decorator(func):\n\ndef wrapper(limit: int):\n\nif not isinstance(limit, int):\n\nraise TypeError(f\"Argument 'limit' must be of type int, got {type(limit)}\")\n\nif limit < min_limit or limit > max_limit:\n\nraise ValueError(f\"Argument 'limit' must be between {min_ limit} and {max_limit}, got {limit}\")\n\nreturn func(limit)\n\nreturn wrapper\n\nreturn decorator\n\nHere, the parameterized decorator, validate_args_types_and_limits, ensures that the argument falls within the specified inclusive range before executing the function.\n\nFor reference, here is an example of calling print_fizzbuzz(-15), which should raise a value error due to the negative input:\n\nFile \"supercharge/ch15/application/fizzbuzz_printer.py\", line 32, in wrapper\n\nValueError: Argument 'limit' must be between 0 and 500, got -15\n\nvalidate_args_types_and_limits raised an error due to a bad input, preventing the function from being called with a negative limit. Similarly, if we had called print_fizbuzz(3.14), we would have received a TypeError error, indicating that it handles integers but not floating-point values.\n\n373",
      "page_number": 348
    },
    {
      "number": 15,
      "title": "The FizzBuzz use case To explore how monitoring efforts can be integrated into our code, let us consider developing a program that generates the FizzBuzz sequence, a coding challenge introduced in Cha",
      "start_page": 398,
      "end_page": 425,
      "detection_method": "regex_chapter",
      "content": "374\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nTo see how our log_function_args and increment_counter decorators operate, we will add a basic logging configuration at the beginning of the file, capturing INFO-level and higher logs, in a file named fizzbuzz.log:\n\nlogging.basicConfig(level=logging.INFO,\n\nfilename=\"fizzbuzz.log\")\n\nThis ensures that the log messages are stored in fizzbuzz.log. In most software systems, print statements should not be used and should be replaced by logging that can direct output to the console or a file.\n\nNext, we add two function calls with valid inputs:\n\nprint_fizzbuzz(5)\n\nprint_fizzbuzz(50)\n\nFor reference, the fizzbuzz_printer.py output is shown in Figure 15.9:\n\nFigure 15.9: Console output for the two function calls\n\nThis produces a clean output, containing the FizzBuzz results. Meanwhile, the fizzbuzz.log log file records log messages in the specified format, as illustrated in Figure 15.10:\n\nChapter 15\n\nFigure 15.10: fizzbuzz.log output containing logs and counter messages\n\nThis implementation follows clean code principles, separating core functionality from monitor- ing while leveraging inverse CoT prompting in Copilot to generate neat and maintainable code. However, decorators can be styled in various ways. Later in the chapter, we will explore how few-shot learning can help refine their structure for even better results.\n\nNext, we will apply the inverse CoT technique when prompting ChatGPT and OpenAI API.\n\nApplying inverse CoT with ChatGPT and OpenAI GitHub Copilot is the preferred tool for integrating going-live functionality, as it provides the fastest code completion experience. However, we will also demonstrate how to apply the same CoT prompting technique with ChatGPT and OpenAI API. To do this effectively, we will structure our prompts using the five S’s best practices for prompt precision introduced earlier in this book.\n\nUsing ChatGPT to monitor decorators Similar to our approach with GitHub Copilot, we can leverage ChatGPT to generate the required decorators for monitoring. Since we submit the full code rather than moving the cursor up in the file, this approach aligns with a standard CoT technique. The following is an example of a structured prompt that provides context about the incomplete Python implementation, focusing on a single task, with specific and short instructions, followed by a lead-in cue:\n\nCONTEXT: You are provided with a partial Python script in {{{ FIZZBUZZ_ PRINTER }}}, where some of the code is missing.\n\nTASK: Implement the decorators with the missing implementation in the code.\n\nFIZZBUZZ_PRINTER: {{{\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n375\n\n376\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nFIZZBUZZ_COUNTER = 0\n\n@log_function_args\n\n@increment_counter\n\n@validate_args_types_and_limits(0, 500)\n\ndef print_fizzbuzz(limit: int) -> None: ... full code implementation omitted for brevity ... }}}\n\nCODE:\n\nWhen used in ChatGPT with the o3-mini high model (available to Plus subscribers as of April 2025), this prompt generated a complete script including the full implementation of the missing decorators, as follows:\n\ndef log_function_args(func: callable) -> callable:\n\ndef wrapper(*args, **kwargs):\n\nlogger.info(\n\nf\"Calling function '{func.__name__}' with args: {args} and kwargs: {kwargs}\"\n\nreturn func(*args, **kwargs)\n\nreturn wrapper\n\ndef increment_counter(func: callable) -> callable:\n\ndef wrapper(*args, **kwargs):\n\nglobal FIZZBUZZ_COUNTER\n\nFIZZBUZZ_COUNTER += 1\n\nlogger.info(f\"FIZZBUZZ_COUNTER incremented to {FIZZBUZZ_COUNTER}\")\n\nreturn func(*args, **kwargs)\n\nreturn wrapper\n\ndef validate_args_types_and_limits(min_limit: int, max_limit: int):\n\ndef decorator(func: callable) -> callable:\n\ndef wrapper(*args, **kwargs):\n\n# Use inspect to bind the arguments to the function's signature\n\nsig = inspect.signature(func)\n\nbound_args = sig.bind(*args, **kwargs)\n\nbound_args.apply_defaults()\n\n# Expecting the argument to be named 'limit'\n\nChapter 15\n\nif 'limit' not in bound_args.arguments:\n\nraise ValueError(\"Missing required argument 'limit'.\")\n\nlimit = bound_args.arguments['limit']\n\nif not isinstance(limit, int):\n\nraise TypeError(\"Argument 'limit' must be an integer.\")\n\nif not (min_limit <= limit <= max_limit):\n\nraise ValueError(f\"Argument 'limit' must be between {min_ limit} and {max_limit}.\")\n\nreturn func(*args, **kwargs)\n\nreturn wrapper\n\nreturn decorator\n\nThis implementation retains the same functionality as the decorators generated by Copilot but differs in style. It includes docstrings and the addition of the @functools.wraps(func) decora- tor, which helps preserve the original function’s attributes. Later in the chapter, we will explore how to guide ChatGPT toward a more desirable implementation style using few-shot learning.\n\nUsing OpenAI API to monitor decorators The approach for using OpenAI API closely mirrors the ChatGPT prompt structure. We construct a system prompt that provides context and a single task, as was done for ChatGPT. Additionally, we use a user prompt that includes the existing script along with a cue for code completion.\n\nThe system prompt should look as follows:\n\nSURROUND = \"You are provided with a partial Python script in {{{ FIZZBUZZ_ PRINTER }}}, where some of the code is missing.\"\n\nSINGLE_TASK = \"Implement the decorators with the missing implementation in the code.\"\n\nThese prompt pieces serve the same role as the context and task definitions in the ChatGPT prompt. The user prompt should incorporate the incomplete implementation, for example, by reading the code from a file, as shown here:\n\ndef get_user_prompt(script_path: str) -> str:\n\nwith open(script_path, 'r') as file:\n\nincomplete_code = file.read()\n\nreturn f\"\"\"\n\n377\n\n378\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nFIZZBUZZ_PRINTER: {{{{{{ {incomplete_code} }}}}}}\n\nCODE:\n\n\"\"\"\n\nBy providing the incomplete script alongside the CODE: cue, we can guide OpenAI API toward generating the missing decorator implementations. Calling OpenAI API with these prompts should yield a decorator implementation like the outputs from ChatGPT and GitHub Copilot. The full script is available for your reference at ch15/inverse_cot_openai.py.\n\nNow that we have seen how to use GenAI tools to implement monitoring decorators, the next step is refining their style to better fit our needs. This can be achieved using few-shot learning to guide Copilot and ChatGPT with structured examples or by fine-tuning a model through OpenAI API GUI. We will explore both approaches in the next section.\n\nUtilizing few-shot learning and fine-tuning as style guides While the decorators generated by GenAI applications provide the desired functionality, we can further refine their style using few-shot learning techniques, as introduced in Chapter 8.\n\nFor instance, we might want to include the @functools.wraps decorator to preserve the original function’s attributes and enforce a specific logging style with the extra parameters. Both objec- tives can be achieved by supplying few-shot examples to Copilot and ChatGPT or adding desired prompt outputs to our fine-tuning file to be used by OpenAI API.\n\nFew-shot decorator style for GitHub Copilot As introduced in Chapter 8, enforcing a specific style with GitHub Copilot requires a few exam- ples, which can be provided as a style guide script. This few-shot example achieves the desired implementation.\n\nTo achieve this, we can create a style_guide_decorator.py file, which should be treated like another non-project file and is typically placed outside the src folder in a project repository.\n\nThis style guide should include all the elements we want Copilot to follow, such as log message formatting, type hints, and spacing. These examples guide Copilot and reduce the need for chained prompts or additional edits.\n\nChapter 15\n\nWhen creating a style guide for decorators, we suggest including a time_it decorator to log a function’s runtime. The implementation should demonstrate the use of @functools.wraps to retain the decorated function’s attributes, type hints for arguments and variables, and a hanging indent logging style with extra parameters:\n\nimport logging\n\nimport time\n\nfrom functools import wraps\n\nfrom typing import Any\n\nlogger: logging.Logger = logging.getLogger(__name__)\n\ndef time_it(func: callable) -> callable:\n\n@wraps(func)\n\ndef wrapper(*args, **kwargs):\n\nstart_time: float = time.time()\n\nres: Any = func(*args, **kwargs)\n\nend_time: float = time.time()\n\nlogger.info(\n\n\"Function called.\",\n\nextra={\n\n\"function\": func.__name__,\n\n\"args\": args,\n\n\"kwargs\": kwargs,\n\n\"error\": \"\",\n\n\"timing\": f\"{end_time - start_time} sec\"})\n\nreturn res\n\nreturn wrapper\n\nIn this case, the logger message combines static text with the extra parameter, resulting in a structured log message that is better suited for large-scale logging. For instance, to quickly iden- tify all timing messages where execution exceeded five seconds, we could filter logs based on the timing parameter with a condition such as timing > 5.\n\n379\n\n380\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nTo incorporate the style guide into the session, we open decorators_style_guide.py, then cut and paste the entire content of the file in place. This trick ensures that Copilot recognizes the code in the file as part of the active session and incorporates its style when making suggestions.\n\nNext, we return to print_fizzbuzz.py and repeat the inverse CoT process. This time, Copilot’s suggestions should align with the style guide defined in style_guide_decorator.py. As shown in Figure 15.11, Copilot includes the @functools.wraps decorator in its suggested implementation, while the logging indentation and extra parameters maintain the same formatting as specified in the style guide:\n\nFigure 15.11: Decorator implementation using the style guide in GitHub Copilot\n\nThis implementation demonstrates that Copilot follows our preferred logging style, using a static log message and extra parameters to support a more structured logging format.\n\nNext, we will explore how to apply the style guide when prompting with ChatGPT. We will then show how to incorporate it into a fine-tuning approach when working with OpenAI API.\n\nFew-shot learning in ChatGPT When using ChatGPT, we can apply a few-shot approach to deploying decorators, like that we explored in Chapter 8. For this task, we recommend the o3-mini high model since it is better at complex coding tasks.\n\nChapter 15\n\nWe first enhance the prompt’s surrounding context by including a reference example:\n\nCONTEXT: You are provided with a partial Python script enclosed with {{{FIZZBUZZ_PRINTER}}} where some of the code is missing, and examples of a good implementation enclosed with {{{ EXAMPLES }}}.\n\nTASK: Implement the decorators with the missing implementation in the code while following the style guide.\n\nFor the few-shot example, we will label the input and output code snippets, with INCOMPLETE_CODE used for the input, indicating the missing time_it decorator, and COMPLETE_CODE for the fully implemented script. This distinction clearly highlights the intended transformation:\n\nINCOMPLETE_CODE: {{{\n\nimport logging\n\nlogger: logging.Logger = logging.getLogger(__name__)\n\n@time_it\n\ndef my_func(a: int, b: int) -> int:\n\nreturn a + b\n\n}}}\n\nWith this incomplete code snippet, we expect the model to generate a fully implemented decorator that follows a specific style. To guide the model, we will provide a fully implemented version of the desired code, as shown here:\n\nCOMPLETE_CODE: {{{\n\nimport logging\n\nimport time\n\nfrom functools import wraps\n\nfrom typing import Any\n\nlogger: logging.Logger = logging.getLogger(__name__)\n\ndef time_it(func: callable) -> callable:\n\n@wraps(func)\n\n...omitted for brevity but using \"extra\" parameter...\n\n381\n\n382\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\n@time_it\n\ndef my_func(a: int, b: int) -> int:\n\nreturn a + b\n\n}}}\n\nProviding this example of the desired output helps guide ChatGPT toward generating the expected implementation. The following is the structure of the prompt, with previously introduced parts omitted for brevity:\n\nCONTEXT: You are provided with a partial Python script enclosed with {{{FIZZBUZZ_PRINTER}}} where some of the code is missing, and examples of a good implementation enclosed with {{{ EXAMPLES }}}\n\nTASK: Implement the decorators with the missing implementation in the code while following the style guide.\n\nEXAMPLES:\n\nINCOMPLETE_CODE: {{{...omitted for brevity...}}}\n\nCOMPLETE_CODE: {{{...omitted for brevity...}}}\n\nFIZZBUZZ_PRINTER: {{{...omitted for brevity...}}}\n\nCODE:\n\nThe full prompt is available in the book’s repository at ch15/decorators_few_shot_chatgpt.txt. For reference, the results generated using this prompt closely resemble Copilot’s code completion, as shown in Figure 15.12:\n\nFigure 15.12: Few-shot learning using ChatGPT\n\nChapter 15\n\nIn ChatGPT’s implementation, we can see that the model followed the style guide from our exam- ple, incorporating the extra parameter alongside the static log message and using the @functools. wraps decorator to preserve the function’s attributes. However, this is a relatively lengthy prompt to achieve a code completion that GitHub Copilot can generate much faster. Therefore, we rec- ommend using Copilot whenever possible for a more efficient workflow.\n\nLeveraging fine-tuning for OpenAI API With OpenAI, we can either apply the few-shot technique, as in the ChatGPT input-output exam- ples, or train a fine-tuned model by adding a few training samples in JSONL format, as introduced in Chapter 11. This approach would take even longer than ChatGPT to achieve what GitHub Co- pilot can generate almost instantly. Fine-tuning would be more practical for large-scale projects, such as scanning an entire repository to implement logging, monitoring, and error handling for all files automatically, at scale.\n\nAs we saw in Chapter 11, fine-tuning generally requires more examples than few-shot learning because of the differences in how models handle prompts versus how they update weights during fine-tuning. However, considering our starter fine-tuning file, fine_tuning.jsonl, from Chapter 11, which has already established the preferred logging style, indentation, and spacing, a single decorator may be sufficient to achieve the desired implementation.\n\nFor example, here is a time_it decorator added to ch13/fine_tuning.jsonl, structured with a system and user prompt as used in ChatGPT, along with a desirable assistant response assigned a weight of 1:\n\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are provided with a partial Python script enclosed with {{{ INCOMPLETE_CODE }}} where some of the code is missing. Your task is to implement the decorators with the missing implementation\"}, {\"role\": \"user\", \"content\": \"INCOMPLETE_CODE: {{{import logging\\nimport time\\n\\nlogger: logging.Logger = logging. getLogger(__name__)\\n\\n@time_it\\ndef my_func(a: int, b: int) -> int:\\n return a + b\\n}}}\\n COMPLETE_CODE:\"}, {\"role\": \"assistant\", \"content\": \"...omitted for brevity...\", \"weight\": 1}]}\n\n383\n\n384\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nWe can then upload the fine-tuning file by going to platform.openai.com, selecting Dashboard | Fine-tune, and uploading the JSONL file, as shown in Figure 15.13:\n\nFigure 15.13: Upload the fine-tuned JSONL file\n\nAfter the upload is complete, the fine-tuned model will be trained, which takes around 15 minutes. When done, a Succeeded icon will appear, as shown in Figure 15.14:\n\nChapter 15\n\nFigure 15.14: Successful fine-tuning job indicates the model has finished training\n\nTo compare the benefits of fine-tuning against the base GPT-4o mini model, we can use the Playground page. This allows us to compare the fine-tuned model’s output against the baseline results, highlighting improvements. It is always recommended to validate these enhancements on key prompts once the fine-tuning process is complete.\n\nWe will select gpt-4o-mini-2024-07-18 as the base model on the left and choose the fine-tuned model from the drop-down menu on the right, as shown in Figure 15.15:\n\nFigure 15.15: Base model and fine-tuned model comparison\n\n385\n\n386\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nThis comparison allows us to assess the performance of both models. Next, we add the system instructions on both sides of the screen, mirroring the surrounding context and single-task ap- proach used in ChatGPT:\n\nYou are provided with a partial Python script in {{{FIZZBUZZ_PRINTER}}}, where some of the code is missing. Your task is to implement the decorators with the missing implementation in the code.\n\nThe user message follows a format like our ChatGPT prompt:\n\nFIZZBUZZ_PRINTER: {{{ ...omitted for brevity... }}}\n\nCODE:\n\nAt this stage, our prompt is ready to run, producing more precise results when using the fine- tuned model. Compared to gpt-4o-mini-2024-07-18, the fine-tuned model returns only the code with our preferred logging style, and including the @functools.wraps decorator, as shown in Figure 15.16:\n\nFigure 15.16: Comparison of the fine-tuned model and the base model\n\nIn this figure, we can see that the assistant’s response begins directly with the code, reflecting the influence of the fine-tuned examples. Additionally, the preferred logging style is consistently applied, aligning with the provided sample.\n\nChapter 15\n\nWhile fine-tuning helps maintain the desired structure and style, it requires significantly more effort than using Copilot for going-live tasks related to code completion. For this reason, we rec- ommend Copilot as your primary tool for such tasks. Fine-tuning may be a viable option if you’re building a custom GenAI application for internal use and have dozens of training examples, such as scanning an entire repository for suggestions or developing a GenAI-powered solution to serve other customers. However, Copilot with few-shot learning tends to be a more efficient method of achieving results without having to invest time in finding many use cases.\n\nSummary In this chapter, we explored how going-live tasks, such as adding logs, monitoring usage, and handling bad input, can introduce unnecessary complexity to functions. GenAI does not inherently suggest using decorators to separate these objectives from the core functionality of the program, which can lead to lower-quality code.\n\nThe most effective way to leverage GenAI for these tasks is through CoT prompting with GitHub Copilot. This approach involves first implementing the core function and then specifying addi- tional requirements for decorators to handle logging, metric recording, or argument validation.\n\nTo ensure that the decorator implementation follows our desired style, we can use few-shot learning with Copilot and ChatGPT or fine-tune a model for OpenAI API. This allows us to achieve more desirable results, minimizing the requirements for chained edits to the code.\n\nIn the final chapter of the book, we will summarize the key takeaways from the 15 chapters and explore how to enhance our coding efforts with GenAI tools.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: Do LLMs guarantee higher coding standards?\n\nAnswer: No, not necessarily. Simply asking GenAI applications to add monitoring responsibilities can lead to undesirable suggestions, such as merging monitoring tasks with core functionality instead of properly separating responsibilities.\n\n387\n\n388\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nQuestion 2: What is the inverse CoT for Copilot?\n\nAnswer: It involves defining higher-level coding structures, such as decorators, first and then moving the cursor up to let Copilot complete the implementation details.\n\nQuestion 3: How can we guide GenAI applications to follow a style guide that aligns with our internal standards?\n\nAnswer: We can use a style guide file for Copilot, provide a few examples when prompting ChatGPT, and apply fine-tuning when working with OpenAI API.\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nReal Python on decorators: https://realpython.com/primer-on-python-decorators/\n\n\n\nPython’s logging documentation: https://docs.python.org/3/howto/logging.html\n\nMetrics of code, Wikipedia: http://en.wikipedia.org/wiki/Software_metric\n\n\n\nException handling, Wikipedia: https://en.wikipedia.org/wiki/Exception_handling_ (programming)\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n16\n\nArchitecture, Design, and the Future\n\nIt seems that every discussion about the future of artificial intelligence (AI) bears considering the famous quote of Niels Bohr: “Prediction is very difficult, especially if it’s about the future.” When it comes to AI, though, we already have ideas about what the future will bring. That is, we have read various science fiction books by Asimov and other authors. We have watched The Terminator and other futuristic movies and TV shows. Even cartoons across different generations embody different versions of the future, whether it’s The Jetsons, Futurama, or various anime. We have conceptions of what the future looks like; the uncertainty rests on how long it will take to get there.\n\nWith the increasing investments in AI and their exponential growth, the fields of AI and GenAI continue to evolve and bring new capabilities faster than most practitioners expect, let alone can keep up with. The landscape of software engineering is changing as a result of the new capa- bilities brought about by AI and GenAI for coding. Before we can look forward and estimate the timeframes for advancements in the future, it is useful to look back and understand the major technological advances that have accelerated the transformation over the past 10 years. This chapter briefly reviews these major advances as a conceptual framework for what happens next.\n\nThis book enables readers to embrace these changes and gain the skills to effectively use them for not just coding but a range of software engineering skills. As we all continue to learn, it is useful to understand the impact of these advances on the industry. These include the impact on software engineers and their companies, as well as what the future might bring.\n\n390\n\nArchitecture, Design, and the Future\n\nIn this chapter, we explore the following topics:\n\n\n\nThe rapid rise of GenAI\n\n\n\nThe economics of faster software development\n\n\n\nThe shifting landscape of software developers\n\nWill GenAI change programming languages?\n\n\n\nThe future of GenAI in software engineering\n\nThe rapid rise of GenAI It was only in late 2012 that Thomas Davenport and DJ Patil declared that data science is the “sexiest job of the 21st century” in their Harvard Business Review article (https://hbr.org/2012/10/ data-scientist-the-sexiest-job-of-the-21st-century). Just over a decade later, the pace of transformation in the industry has been nothing short of remarkable.\n\nIn the same period of the last decade, neural networks were finally making their third resurgence after a couple of AI winters. Although neural network research started back in the 1950s, it faced difficulties in reaching its extremely hyped expectations, which resulted in periods of minimal funding and research known as the “AI winters.” By 2012, new neural network architectures had emerged. AlexNet, a type of convolutional neural network (CNN), drew widespread attention from the computer vision community after winning the ImageNet Large Scale Visual Recogni- tion Challenge (ILSVRC) in 2012 by a wide margin. This changed the course of computer vision as all image understanding work quickly shifted to CNNs. The ensuing research over the next few years led to the ability of CNNs to extract information from images as accurately as humans by 2016, according to the ILSVRC competition, although human accuracy levels are still disputed.\n\nWhile computer vision work was advancing through the CNN architecture, the transformer ar- chitecture was introduced through a 2017 paper from Google. This Attention Is All You Need paper (https://arxiv.org/abs/1706.03762) describes an attention mechanism and the transformer architecture. In the years since, this architecture has had a massive impact on neural networks and the field of AI. Although it has improved solutions for many diverse problems within AI, its primary beneficiary has been the subfield of natural language processing (NLP). The work of this paper led to the adoption of large language models (LLMs).\n\nLLMs have had a profound impact on the public perception of AI, though the early generations of this technology were far less impressive. GPT-1 and GPT-2 were released by OpenAI in 2018 and 2019 and served as a step forward in LLMs. These models are essentially prediction models trained on large volumes of text to predict the next word. The transformer architecture learned the\n\nChapter 16\n\nrelationship between the words in a sentence to predict the next word. Empirical research found that the LLMs performed significantly well with ever larger models trained on ever more data. GPT-3 was released in 2020 with 175 billion parameters as compared to the 1.5 billion parameters of GPT-2. GPT-4, released in March 2023, scaled up to an estimated 1.76 trillion parameters, a 10-fold increase over GPT-3.\n\nInstructGPT and ChatGPT were developed as new tools that utilize underlying GPT LLMs. In- structGPT was designed to generate text for specific tasks that follow user instructions. These tasks could be emails, summaries, or even code. InstructGPT included AI predictions based on training data as well as a reinforcement learning technique based on human feedback to adjust the next word predictions based on a human feedback mechanism. In contrast, the release of OpenAI’s ChatGPT in November 2022 was based on GPT-3.5 and was arguably the first time an average user could interact with an LLM without having to program. It focused on conversations with users and general dialogue but also included the reinforcement learning method. These two tools developed many of the LLM methods that led to the prompting techniques field.\n\nThe LLM and ChatGPT methods focus on accurately predicting the next words across languages. The same models have opened other research avenues by training on other types of data. Instead of training on text, researchers in biology have found that the same results can be applied to genomes and protein sequence predictions. Chemists are using the technology to create new drugs and chemical substances with specialized properties. This book, of course, is based on the same idea of applying the same prediction techniques to code by training on public code bases.\n\nA few years ago, AI was rarely discussed in the media. Today, the financial news seems to focus on the recent technology advances and their implications for the markets and the larger economy. The next section discusses some of the economic issues that GenAI coding may offer.\n\nEconomics of faster software development Software development is one of the key enablers of technology and the digital economy. Software impacts many aspects of modern society, from governments to businesses. Most industries rely on software. Software is closely tied to the productivity of workers across the globe, making it a key economic measure.\n\n391\n\n392\n\nArchitecture, Design, and the Future\n\nThe field of software economics researches and estimates the value provided by software against the costs associated with its development. Software platforms and tools are designed to provide incremental business/economic value to the organizations using them. The key decisions of buying commercial off-the-shelf (COTS) software, licensing software, customizing third-party software, or building it in-house are based on the estimated incremental value proposition.\n\nWith GenAI, software engineers can write quality code substantially faster. With the techniques described in this book, the efficiency in writing code should improve and the quality of the pro- duced code should remain quite high. The ability to automate the testing and documentation further reduces the costs of software development. In financial terms, the use of GenAI by the same workforce would result in an increased supply of software. The current costs for the GenAI coding empowerment remain $25/month or under, which is arguably a bargain. The reduced cost will increase the demand for software, creating greater automation and adding value for businesses. There are always more problems to solve, and the lower development cost barrier will alter the economics of software engineering. To reap these benefits, software engineers need to begin using this technology, which is discussed in the next section.\n\nWill GenAI be adopted by software engineers? The improved efficiency, speed, and capability that GenAI brings for software engineers is at least a paradigm shift but may be a revolution in the way that software is being developed. As discussed in this book, the current capabilities of GenAI include many of the tasks that software engineers engage in daily. These include writing code and tests, generating documentation, refactoring, and optimizing code. GenAI aids with the cognitively complex tasks of code debugging down to the less mindful tasks of conforming to code style.\n\nGartner predicts that 50% of software engineers will use AI-powered coding tools by 2027, which is up from the 5% in 2023 when GenAI was at the peak of the hype cycle (https://www.gartner. com/en/newsroom/press-releases/2023-11-28-gartner-hype-cycle-shows-ai-practices- and-platform-engineering-will-reach-mainstream-adoption-in-software-engineering-i- n-two-to-five-years). Over the last two years, GenAI tools have rapidly shifted from marginally useful due to their many errors to widespread tools that software engineers and corporations find valuable. In this brief period, KPMG claims that GenAI has become a software engineer’s most valuable coding partner (https://kpmg.com/us/en/articles/2023/generative-artificial- intelligence.html). Over the next few years, these tools will continue to evolve and produce ever higher quality code, test instances, docstrings, and documentation.\n\nChapter 16\n\nGiven the improved accuracy and overall efficiency that GenAI coding provides, the effect may not be uniform across all ranks of developers. The next section examines how the tools may affect different classes of programmers.\n\nShifting landscape of software developers At the current market pricing, GenAI tools are generally available to all developers. It is interest- ing to speculate how the software engineer landscape may change due to this recent technology. Some have suggested that GenAI tools will level the playing field since any developer will be able to program with increased efficiency in any language. They argue that junior developers may take over more roles and increase the volume of code delivered, and more senior developers will be pushed out of organizations. Others argue the opposite.\n\nThe rise of GenAI tools has also lowered the barrier for software development. Programming in each language has a steep learning curve since it includes syntax, algorithms, libraries, integrated development environments (IDEs), compiler messages, and more. Printing the “hello world” message in any language is usually the first accomplishment. If coding a new language is like reading the front page of a foreign language newspaper, GenAI for coding is a tutor that shows the meaning of all words and explains the grammar upon request. For coding alone, a junior developer may quickly become productive in straightforward coding.\n\nTools such as ChatGPT and GitHub Copilot will empower developers to quickly learn and become productive in new programming languages. There are always new languages and frameworks that all software engineers must continually learn and apply to projects. The implication is that developers can more quickly get up to speed with new languages and frameworks and become productive in far less time. This has implications for not only new languages but also older lan- guages such as COBOL, FORTRAN, and others that are still running in legacy systems, described later in this section.\n\nOur perception is that the landscape will shift toward senior developers, and more junior develop- ers may be displaced by the GenAI tools. If we consider a typical software engineering or computer science undergraduate curriculum, the primary technical courses include calculus, programming, data structures, algorithms, and maybe software engineering. The more advanced courses will include networking, compilers, operating systems, big data, AI, programming language theory, optimizations, cloud computing, and much more.\n\n393\n\n394\n\nArchitecture, Design, and the Future\n\nJunior developers tend to focus more on writing code, tests, and documentation, while the more senior developers focus on the complex interplay between the system requirements, stakeholders, architecture, design, and other aspects that tend to require more experience. The GenAI efficiencies clearly overlap the skillsets of the junior developers, but not the senior developers.\n\nGenAI as related to the Software Engineering Body of Knowledge Another viewpoint is that GenAI performs very well within a limited scope of coding within software engineering. The IEEE Computer Society produces a Software Engineering Body of Knowledge (SWEBOK). The current version, developed in 2024, captures the collective skills, methodologies, knowledge, and so on for the software engineering profession. It has been curated by experts with the goal of reaching a consensus on the core body of knowledge. Table 16.1 lists the 18 knowledge areas (KAs) found in the SWEBOK guide, available at https://www.computer. org/education/bodies-of-knowledge/software-engineering:\n\nSoftware requirements\n\nSoftware architecture\n\nSoftware design\n\nSoftware construction\n\nSoftware testing\n\nSoftware engineering operations\n\nSoftware maintenance\n\nSoftware configuration management\n\nSoftware engineering management\n\nSoftware engineering process\n\nSoftware engineering models and methods\n\nSoftware quality\n\nSoftware security\n\nSoftware engineering professional practice\n\nSoftware engineering economics\n\nComputing foundations\n\nMathematical foundations\n\nEngineering foundations\n\nTable 16.1: Software Engineering Body of Knowledge (SWEBOK) 2024 V4\n\nCompared to the SWEBOK, GenAI enhances a few processes but only a fraction of the overall skillset currently. Specifically, it directly enhances software construction and software testing. Indirectly, it helps support software maintenance and software quality KAs.\n\nChapter 16\n\nDemocratization of software engineers The GenAI tools will help a wider audience than many people expect. We have discussed how junior software engineers and those getting up to speed in a new language can receive help with their programming. Senior developers can leverage their architectural, design, and other software systems experience. There is a wide class of former developers who have software experience but have shifted into other roles in computational biology, finance, business, or management.\n\nFormer programmers who have left the field to take on different responsibilities may return to coding in the near future with the GenAI tools. If they have been out of programming for several years, they would need to learn new languages, cloud computing, new IDEs, and so on. Nor- mally, they would be unlikely to invest considerable time learning this multitude of new skills. However, with GenAI, they are now empowered to write niche applications within their specific roles that use their expertise. The implications are that part of software engineering may shift somewhat from being a domain-independent technical skill to an extra skill of domain experts. That is, rather than hiring expert software engineers who have traditionally applied their techni- cal skills to write code for banking, healthcare, or other industries, these industries may turn to experienced practitioners in their field who can also program. We started to experience some of this with robotic process automation (RPA), where the subject matter experts learned the tools and drove specific workflows. In the last couple of years, we have seen similar domain experts learning prompt engineering to leverage LLMs in their workflow pipelines. We see a similar shift in the democratization of AI, where subject matter experts may be replaced by subject matter experts with AI experience.\n\nLegacy software and flexible teams With the explainability and guidance of GenAI technology, software development will certainly change corporate and industry approaches to software, especially legacy systems. Many compa- nies and industries rely on legacy software systems developed decades ago and continue to run. The systems may be decades old and written in a language that only one or two people in the company have ever learned. The software systems pose a significant risk to organizations. They may run on legacy hardware, but they are still part of existing workflows. Efforts to understand, let alone rewrite, such systems have been cost-prohibitive.\n\nGenAI systems can now be trained on the code bases and both provide support for understanding the code and perhaps enable them to be quickly rewritten. Using similar technology to human language translation, the code can use GenAI techniques to efficiently rewrite the full code base, adhere to the more recent documentation and testing strategies, and eliminate the risk. In terms\n\n395\n\n396\n\nArchitecture, Design, and the Future\n\nof staffing, this would reduce the need to keep developers with specialized language ability or experience with these legacy systems.\n\nAs the barriers with legacy systems decrease, software engineers may become empowered to work on a wider set of projects. While subject matter knowledge in the industry domain is critical, the coding implementation has become more accessible through GenAI.\n\nWith GenAI, coders can more quickly get up to speed with new code bases and become productive, but this has implications. The speed provided by GenAI increases the flexibility of developer teams. Developers can shift more quickly between projects and gear up for new projects. With agile project development practices and changing requirements, this is certainly a positive outcome. However, this flexibility would also presumably lead to increased outsourcing of code development since offshore teams could also quickly ramp up to tackle projects in a similar fashion. This is not a new trend, and many teams still rely on senior developers, designers, and business analysts who interact with customers.\n\nThis section focused on the people involved in software engineering and the impact of GenAI on the field. The next section discusses the potential opportunities for computer languages to change based on GenAI capabilities.\n\nWill GenAI change programming languages? The traditional art and science of writing software involves creating a set of instructions that a computer can follow. This communication currently occurs through a specific programming language. The instructions usually include combinations of data structures that store and manip- ulate data. The instructions may include algorithms that efficiently sort, manipulate, or transform data to add business value. Most programming languages can perform these operations, but their true power is only achieved when using added libraries or packages that enable user interfaces, networking, database interaction, and much more.\n\nOur GenAI coding tools today focus on writing and modifying our code. That is, ChatGPT, OpenAI API, and GitHub Copilot serve as programming assistants that guess the specific programming language code lines from the method signature. While this speeds up our programming at the method level, it requires that the developer have knowledge of how the problem breaks down into multiple methods and the transmission of information between the methods as dictated by their signature.\n\nChapter 16\n\nWe can draw parallels between writing a document using an LLM and writing code using GenAI. ChatGPT and other LLMs are remarkable at rephrasing text for technical audiences, doctors, or children. This is equivalent to refactoring code. To write a letter from scratch, it’s not a single command but a series of prompts that map out the instructions and sequence. Similarly, one does not write the entire code file but must break it down into a series of commands that can each be developed into code. Throughout this book, we have provided many examples that illustrate that GenAI will usually write reasonable functional code. However, by breaking functions into smaller units, the GenAI will produce higher-quality and more readable code that is easier to maintain.\n\nThe next stage of GenAI software development is possibly a more natural language way of com- municating with computers. While computer languages emphasize the precision of commands, they require an unnatural and more formal structure for communicating ideas. This requires a learning curve and arguably slows down development. Natural language could speed up the translation into computer-readable form and enable greater accessibility to software.\n\nThe rise of natural languages for computer instructions will likely require some time and face some hurdles that can be overcome with pseudocode. Three of the formal steps of computer lan- guages are sequences, iterations, and selection (conditions). The interaction of these elements is the formal control flow of computer programs. Natural language can handle the sequences and conditional flows reasonably well, but the notion of iteration loops is arguably not commonly part of languages. Pseudocode represents a somewhat arbitrary syntax but captures the three formal steps well. General pseudocode that can be interpreted by LLMs to produce formal programming languages may further empower a wider set of developers.\n\nThe future of GenAI in software engineering This section first discusses vibe coding and then breaks down the future of software engineering into two different horizons. The shorter term considers the next few years, and the longer term considers what the field might look like in five or more years.\n\nVibe coding Vibe coding is a recent trend where the developer describes what they want to do in natural lan- guage and leaves the technical coding aspects up to the LLM. Using LLMs to write code is certainly a theme of this book, and vibe coding essentially takes this to an extreme. Essentially, vibe coding provides a form of rapid prototyping that is a valuable part of the overall software engineering process, particularly in defining requirements. We believe the software community will come to recognize its value as such. However, for larger projects, code quality is important since most of\n\n397\n\n398\n\nArchitecture, Design, and the Future\n\nthe costs associated with software are in its maintenance, such as adding new features. Many unhappy memes describe the difficulty of “vibe debugging” the relatively poor-quality code produced by vibe coding. we expect a future that lands somewhere between where we are now and vibe coding. We will describe the shorter- and longer-term expectations next.\n\nShorter term Over the last year, the GenAI market has blossomed, and many developers are now working with this technology. It is available as a VSCode extension, Jetbrains products including PyCharm plugin, and other integrated development environments (IDEs) so it’s readily available. In addition, new IDEs with built-in GenAI capabilities have entered the market, including Cursor, Windsurf, and Replit. Other tools, such as AWS CodeWhisperer and Tabnine, offer code suggestions for multiple languages. All of these are offered at a low price, compared to the potential savings of developer time. In the short term, all of these will increase their accuracy and lead to the creation of better software in less time.\n\nLonger term After many have adopted the GenAI coding approaches and can use them effectively, the technol- ogy push will focus on trying to move from formal requirements and specifications to code. As one of the first steps in the software development life cycle, requirements gathering is a formal process to understand the intended goals, the user interfaces, non-functional requirements, and so on. In many organizations, there are more formal diagrams, such as Unified Markup Language (UML), that describe the design. Others include sample user interface designs from Balsamiq and Figma. Efforts are underway to automatically convert these diagrams directly into code.\n\nGenAI has not solved many of the other aspects of software development processes. The planning, designing, higher-level integration, DevOps, deployment, orchestration, and other aspects of software have had minimal impact from GenAI. We are excited about the long-term prospects, but do not have an opinion on when GenAI techniques will make meaningful strides in these areas.\n\nWill AI replace software engineering? The need for software engineering will continue for two reasons. First, we know that GenAI is much more effective at writing parts rather than full end-to-end content. For prose text, we don’t expect GenAI to draft full reports, but we can break it down into sections and paragraphs and use prompt engineering. It’s no surprise that some of the main use cases are web searches and summarization, which have weak parallels in software engineering and improving text, which is essentially refactoring. With software engineering, knowledge of software structure and sup-\n\nChapter 16\n\nporting architectures is needed to break code into modules, files, and succinct methods. The need for human skills for these tasks will likely continue for several years, even though individual methods may be fully crafted using GenAI. Humans will shift to roles in architecture, design, and integration, and will still ultimately be responsible for validating the code.\n\nThe second reason is that GenAI is most effective when working with widely available code bases but often fails without such expansive sets of examples. Most software engineers need to learn a new language every 3–5 years on average. Certainly, some of this is due to switching jobs. However, new languages, frameworks, and new versions of both continue to emerge—none of which begin with sufficiently large code bases for training GenAI models. Domain-specific lan- guages are another good example of limited training data. In addition, the newest technologies in databases, cybersecurity, quantum computing, and so on need software engineers since their respective code bases are still being written.\n\nRisk and governance Humans are currently ultimately responsible for the code that is produced, but the use of GenAI raises a number of challenges. In heavily regulated industries such as transportation, banking, and healthcare, the use of AI will undergo significant scrutiny. The ability to provide explainability and understanding of which programmers, agents, or AI created different sections of code will likely become standard. The risks, legal responsibilities, and ethical considerations will need to address responsibilities and safeguards for software. The ethical, moral, and legal issues around self-driving cars today provide a good example of some of the considerations. MIT’s trolley prob- lem (https://en.wikipedia.org/wiki/Trolley_problem) and the Moral Machine (https:// en.wikipedia.org/wiki/Moral_Machine) provide further examples of the challenges.\n\nAnother challenge is the copyright issue of software. For regular texts, News Corp, the Financial Times, the Associated Press, and other media companies have negotiated agreements to license their text to OpenAI. Microsoft and Google have also negotiated with content providers to scale their training sets without violating copyright laws. For software code, the laws are less clear since it’s a relatively new problem that has not yet been resolved. It is certainly possible that the GenAI could duplicate copyrighted code. Since the accuracy of the GenAI depends in part on the size of the training set, more code licensing may be required to improve performance.\n\nThe education in programming languages and the evaluation of skills must also adapt. Schools and universities have taught programming skills for decades. Just as spelling and grammar checkers are standard practice for writing reports, GenAI is likely to eventually not just be accepted but be a required part of software development. Universities have tended to focus more on the language\n\n399\n\n400\n\nArchitecture, Design, and the Future\n\nnuances and syntax rather than the set of available tools, such as compilers, debuggers, and linters. However, ignoring GenAI will be difficult since it will be used extensively. Job interviewers have often used LeetCode forms of technical questions to weed out candidates, even though many of the questions are more academic than common industry practices. With GenAI, such low-level coding skills have become even less relevant, and we expect these forms of interview questions to evolve as well.\n\nSummary AI and GenAI capabilities have advanced rapidly over the past several years through a number of major technology innovations. With GenAI’s boost in software engineering productivity at a low price point, it will increase the volume of code written and lower the overall costs. This will change the software engineer landscape, perhaps to favor more senior developers with more ex- perience in architecture, design, and SWEBOK. It may also enable specialists with deep domain experience to develop more code. Legacy systems may be rewritten with GenAI support, which will free companies to use software developers across projects in a more agile manner.\n\nThere is a strong movement toward adopting GenAI for coding, and it has already been integrat- ed into many IDEs. In the longer term, we will see how it surpasses the coding tasks and enters more aspects of software engineering. GenAI still requires the communication of commands to computers, and natural language may eventually supplement or replace programming languages, but pseudocode may be the next frontier. The world will still need software engineers for the foreseeable future. There are still many issues to manage, from risk to compliance, but GenAI has arrived and is already supercharging software engineering.\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nThomas Davenport and DJ Patil. Harvard Business Review. https://hbr.org/2012/10/\n\ndata-scientist-the-sexiest-job-of-the-21st-century\n\n\n\nAI winters: https://www.historyofdatascience.com/ai-winter-the-highs-and-lows-\n\nof-artificial-intelligence/\n\n\n\nAlex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton. ImageNet Classification with Deep Convo- lutional Neural Networks. NeurIPS 2012. https://proceedings.neurips.cc/paper_files/\n\npaper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n\nChapter 16\n\n\n\nImageNet Large Scale Visual Recognition Challenge. https://arxiv.org/pdf/1409.0575\n\n\n\nAttention Is All You Need. https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\n\nGartner hype cycle for AI practices. https://www.gartner.com/en/newsroom/press- releases/2023-11-28-gartner-hype-cycle-shows-ai-practices-and-platform- engineering-will-reach-mainstream-adoption-in-software-engineering-in-two- to-five-years\n\n\n\nThe startling power generative AI is bringing to software development. https://kpmg.com/us/\n\nen/articles/2023/generative-artificial-intelligence.html\n\nH. Washizaki, eds., Guide to the Software Engineering Body of Knowledge (SWEBOK Guide), Version 4.0, IEEE Computer Society, 2024. https://www.computer.org/education/ bodies-of-knowledge/software-engineering\n\nMoral Machine. https://en.wikipedia.org/wiki/Moral_Machine\n\n\n\nTrolley problem. https://en.wikipedia.org/wiki/Trolley_problem\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.\n\n401",
      "page_number": 398
    },
    {
      "number": 16,
      "title": "relationship between the words in a sentence to predict the next word. Empirical research found that the LLMs performed significantly well with ever larger models trained on ever more data. GPT-3 was",
      "start_page": 426,
      "end_page": 460,
      "detection_method": "regex_chapter",
      "content": "Appendix\n\nSet up an OpenAI Account Here are the steps to create an OpenAI account:\n\n1. Navigate to https://openai.com/. At the top left of the page, click on either ChatGPT or\n\nAPI Platform to start the account creation process, as shown in Figure 1:\n\nFigure 1: Access to ChatGPT and API Platform from OpenAI API\n\n2. Each of these links will take you to an account creation page, where you can sign up using popular platforms such as Google, Microsoft, or Apple, or register with an email address, as shown in Figure 2:\n\n404\n\nFigure 2: OpenAI platform sign-up page\n\n3. To register with an email address, click on Sign up.\n\nImportant Note\n\nAs of August 2025, you will need to create two separate accounts: one for ChatGPT and one for the OpenAI platform to access the API.\n\nIn the next section, we will go through the setup of a virtual environment.\n\nCreating a virtual environment A virtual environment in Python is an isolated setup that ensures the reproducibility of depen- dencies, which are external libraries our project depends on.\n\nFor example, consider the following code:\n\nimport pandas as pd\n\ndf = pd.read_csv('df.csv')\n\ndf.to_markdown()\n\nAppendix\n\nHere, we depend on Pandas, a third-party and open-source library available at: https://github. com/pandas-dev/pandas. Pandas provides the read_csv function, which returns a DataFrame. Since version 1.0.0, which can only run on Python 3.6.1 or higher, the DataFrame class also includes the to_markdown method. If we use earlier versions of Python or Pandas, this code may not run.\n\nGenerally, developers specify their required dependencies in a file named requirements.txt. In this book’s repository, you will find a specification of the external libraries that we use in requirements.txt, as shown in Figure 3:\n\nFigure 3: Requirements.txt file for this book\n\nIn later chapters of the book, where we dive deeper into the software development lifecycle (SDLC) functions, the requirement files for these projects will include specific versions, as shown in Figure 4:\n\nFigure 4: requirements.txt with versioning from Chapter 10\n\n405\n\n406\n\nJust like in this figure, our production projects also include specific versioning. For the dependency versioning resolution, we leverage dependency and environment management tools such as pipenv that proposes dependency management for specific versioning resolution. Other developers may use Conda, poetry, or other tools. In this book, however, we will use Python’s built-in venv tool for our environment setup.\n\nPrompting virtual environment In this book, we leveraged GenAI for every step of the SDLC, and installing a virtual environment is no exception. This task is a good candidate for using the agent mode within the GitHub Copilot chat window, with a prompt that follows the Five S’s discussed in detail in Chapter 4.\n\nA prompt to install our dependencies should include the following components:\n\n1. Python version: Which version of Python should be used to run this project, in our case,\n\nit is Python 3.11.\n\n2. Environment tool: Either venv or its alternatives.\n\n3. Requirements file: The default is requirements.txt placed in the root of the project.\n\n4. Environment name: A common practice is to name the environment .venv. We can also customize it to suit the project name with a prefix of a period, such as .supercharged.\n\n5. Environment location: The library code is typically installed at the project root.\n\nConsidering these, here is an example of a prompt we can use with GitHub Copilot, either in PyCharm or VS Code:\n\nCONTEXT: You are provided with:\n\n1. Python version enclosed with {{{ VERSION }}}\n\n2. Requirements file path enclosed with {{{ PATH }}}\n\n3. Environment tool enclosed with {{{ TOOL }}}\n\n4. Environment name enclosed with {{{ ENV NAME }}}\n\n5. Location for the environment enclosed with {{{ Location }}}\n\nTASK: Set up a virtual environment using the Python version, requirements file path, using the provided name and location for this project.\n\nVERSION: {{{ Python 3.11 }}\n\nPATH: {{{ requirements.txt }}}\n\nTOOL: {{{ venv }}}\n\nAppendix\n\nENV NAME: {{{ .supercharged }}}\n\nLOCATION: {{{ project root }}}\n\nFor reference, the output of the setup using agent mode with GitHub Copilot and the underlying Claude Sonnet 4.0 specifies the steps for setting up an environment, activating it, and installing its dependencies. Figure 5 demonstrates creating the environment with our desired name and activating it:\n\nFigure 5: Activating the virtual environment\n\nFigure 6 illustrates the installation of dependencies as specified in requirements.txt:\n\nFigure 6: Installation of the dependencies\n\n407\n\n408\n\nOnce installation is complete, Copilot may propose some tests, such as validating the correct versioning of the packages, concluding with a success message as shown in Figure 7:\n\nFigure 7: Successful installation message\n\nAfter the setup is complete, we can see that a .supercharged folder is added to the repository, as shown in Figure 8:\n\nFigure 8: Added files to the repository\n\nAppendix\n\nFor reference, here are the typical three shell commands we can use to manually install the virtual environment:\n\npython3.11 -m venv .supercharged\n\nsource .supercharged/bin/activate\n\npip install -r requirements.txt\n\nNext, we will see how to instruct git to ignore the dependencies files.\n\nAdding virtual environment to .gitignore Since requirements.txt is sufficient for replicating the dependencies on any machine, there is no need to include the actual environment code in the repository. For that reason, it is best practice to ignore the installed dependencies. We can do this by adding the path of the installed requirements to the .gitignore file. We can chain another prompt in agent mode:\n\nAdd the environment path to .gitignore file.\n\nThis prompt successfully adds the path of the virtual environment to .gitignore, as shown in Figure 9.\n\nFigure 9: Adding the path to the .gitignore file\n\nIf you named your environment differently, Copilot would likely add the correct name to the file. If you used .supercharged or .venv, no changes are required because these names are already in our files.\n\nImportant Note\n\nNow that you are a supercharged coder, we encourage you to help your colleagues use these prompts when setting up a new virtual environment. These can be adapt- ed for different environment tools, environment names, and projects, reducing the chances of errors when working with Agent mode\n\n409\n\n410\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nVenv, virtual environment built-in: https://docs.python.org/3/library/venv.html\n\n\n\nPipenv, dependency and environment management tools: https://pipenv.pypa.io/\n\nen/latest/\n\n\n\nPoetry, dependency and environment management tools: https://python-poetry.org/\n\ndocs/dependency-specification/\n\n\n\nConda, dependency and environment management tools: https://anaconda.org/ anaconda/conda\n\nAppendix\n\npacktpub.com\n\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.\n\nWhy subscribe?\n\n\n\nSpend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals\n\n\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\n\n\nFully searchable for easy access to vital information\n\n\n\nCopy and paste, print, and bookmark content\n\nDid you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at packtpub.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at customercare@packtpub. com for more details.\n\nAt www.packtpub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\n\nOther Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other books by Packt:\n\nBuilding Agentic AI Systems\n\nAnjanava Biswas, Wrick Talukdar\n\nISBN: 978-1-80323-875-3\n\nMaster the core principles of GenAI and agentic systems\n\nUnderstand how AI agents operate, reason, and adapt in dynamic environments\n\n\n\nEnable AI agents to analyze their own actions and improvise\n\n\n\nImplement systems where AI agents can leverage external tools and plan complex tasks\n\n\n\nApply methods to enhance transparency, accountability, and reliability in AI\n\n\n\nExplore real-world implementations of AI agents across industries\n\n414\n\nOther Books You May Enjoy\n\nGenerative AI with LangChain - Second Edition\n\nBen Auffarth, Leonid Kuligin\n\nISBN: 978-1-83702-201-4\n\nDesign and implement multi-agent systems using LangGraph\n\n\n\nImplement testing strategies that identify issues before deployment\n\nDeploy observability and monitoring solutions for production environments\n\n\n\nBuild agentic RAG systems with re-ranking capabilities\n\n\n\nArchitect scalable, production-ready AI agents using LangGraph and MCP\n\nWork with the latest LLMs and providers like Google Gemini, Anthropic, Mistral,\n\nDeepSeek, and OpenAI’s o3-mini\n\nDesign secure, compliant AI systems aligned with modern ethical practices\n\nOther Books You May Enjoy\n\nPackt is searching for authors like you If you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general appli- cation, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\n\nShare Your Thoughts Now you’ve finished Supercharged Coding with GenAI, we’d love to hear your thoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon review page for this book and share your feedback or leave a review on the site that you purchased it from.\n\nYour review is important to us and the tech community and will help us make sure we’re deliv- ering excellent quality content.\n\n415\n\n416\n\nOther Books You May Enjoy\n\nJoin our Discord and Reddit spaces You’re not the only one navigating fragmented tools, constant updates, and unclear best prac- tices. Join a growing community of professionals exchanging insights that don’t make it into documentation.\n\nStay informed with updates, discussions, and behind-the-scenes insights from our authors.\n\nJoin our Discord space at https://packt. link/z8ivB or scan the QR code below:\n\nConnect with peers, share ideas, and discuss real-world GenAI challenges. Follow us on Reddit at https://packt.link/0rExL or scan the QR code below:\n\nSymbols\n\n__call__ method 110\n\nA\n\nAI pair programmer 45\n\nAI winter 6\n\nAmerican Invitational Mathematics Examination (AIME) 78\n\napplication programming interface\n\n(API) 10, 17\n\nartificial intelligence (AI) 6, 389\n\nassistant prompts 19, 107\n\nB\n\nBig-O notation 262, 337\n\nC\n\ncalculate_distance function 250\n\ncalls\n\ncall parameters, customizing with ChatGPT\n\nand OpenAI 182, 183 code debugging, with GitHub\n\nCopilot 177-179\n\ncreating, with example parameters 175, 176\n\ncentral processing unit (CPU) 336\n\nIndex\n\nchained prompts, code optimization 351\n\nruntime optimization 352-354 space optimization 354-357\n\nchaining 209\n\nchain-of-thought (CoT) 209, 241, 361\n\nrefactoring 250, 251 refactoring, with ChatGPT 254, 255 refactoring, with GitHub Copilot 251-253 refactoring, with OpenAI API 254, 255\n\nChatGPT 9, 154, 155 chaining 228-231 for testing 318-320 used, for explaining code 166-168 used, for performance refactoring 258, 259 used, for refactoring chain-of-thought\n\n(CoT) 254, 255\n\nused, for searching outdated docstrings 304-307\n\nChatGPT baseline 213, 215\n\nChatGPT maximal capacity\n\nanalyzing 343 determining, within memory constraints 349-351 determining, within runtime constraints 347, 348\n\nprofiling runtime 344 profiling space usage 345, 346\n\n418\n\nClaude 7\n\ncode analysis and refactoring tool 5\n\ncode base structure reading 160, 161\n\ncode completion design\n\noverview 46, 47\n\ncode optimization\n\nwith chained prompts 351\n\ncode readability\n\nimproving 247, 248\n\ncode refactoring 242\n\nlimitations, with IDE tools 248, 249\n\ncode smell 5\n\ncomma-separated values (CSV) 338\n\ncommercial off-the-shelf (COTS) 392\n\ncompressed sparse column (CSC) 357\n\ncompressed sparse row (CSR) 357\n\ncomputer programming 4\n\ncontext window 151\n\nexamples 152\n\ncontinuous integration and continuous\n\ndeployment (CI/CD) systems 5\n\ncontinuous integration/continuous\n\ndevelopment (CI/CD) pipelines 300\n\ncontrastive learning 274\n\nconvolutional neural network (CNN) 390\n\nCopilot\n\nchaining 236, 237\n\nCoT reasoning with LLM\n\nfor GitHub Copilot 221-224 geometric mean with ChatGPT 225, 226 geometric mean with OpenAI API 226, 227 GPT-o series 219 using 217, 219\n\nD\n\ndata-driven tests 321, 322\n\ndecorators 361, 368\n\nusing 368, 369\n\ndeep learning 7, 150\n\ndependency code\n\ndebugging 183, 184\n\ndistance function\n\nimplementing 244, 245\n\nDockerfile 159, 161\n\ndocstrings\n\ngenerating, for Singleton __call__\n\nmethod 112, 113\n\ngenerating, with OpenAI API 110 Google-style docstring 113-116\n\ndocstrings, with GenAI\n\nwriting 300 writing, for entire file 302, 304 writing, for single method 300, 301\n\ndouble underscore (dunder) 105\n\nE\n\nEuclidean distance 243\n\nevaluation metrics 80\n\nexpert systems 6\n\nExtreme Programming (XP) 10, 243, 322\n\nF\n\nfew-shot decorator style\n\nfor GitHub Copilot 378, 380\n\nfew-shot\n\nlearning 152, 189, 190, 265, 274, 361\n\ncode implementation, with OpenAI\n\nAPI 200-203\n\nIndex\n\nIndex\n\nfor code refactoring 190, 192 in ChatGPT 381-383 using, as style guides 378 working with, for OpenAI API 199\n\nfew-shot prompting, with ChatGPT\n\nleveraging 192 output improvement, with code\n\nrefactoring 193-195\n\nstructure 192\n\nfine-tuning 152, 203\n\nused, for OpenAI API 383-387 using, as style guides 378 versus prompt engineering 268\n\nFizzBuzz 363 use case 363\n\nFrobenius norm 243\n\nG\n\nGemini 7\n\nGenAI 390, 391\n\nadopting, by software engineers 392, 393 for coding tasks 78, 79 for unit tests 312-317 programming languages 396, 397 sample data creation 329-331\n\nGenAI for higher-level coding patterns\n\ndecorators, using 368, 369 inverse CoT for decorator\n\nimplementation 369-374\n\nusing 368\n\nGenAI future 397 longer term 398 risk and governance 399 shorter term 398 software engineering, need for 398, 399 vibe coding 397\n\nGenAI toolkit 9 ChatGPT 9 GitHub Copilot 10 OpenAI API 10\n\nGenerative artificial intelligence\n\n(GenAI) 3, 17 benefits, of coding 12 downsides, of coding 12, 13 software development, improving 11\n\nGenerative Pretrained Transformer\n\n(GPT) 7, 9\n\ngeometric mean analysis 59, 60 calculating, with chat completion 57 guided walk-through 58, 59\n\ngeometric mean calculations\n\nanalysis 63-65 completion and keyboard shortcuts 61 guided walk-through 61, 62\n\ngeometric mean scope 210, 211\n\nGitHub Copilot 10, 46, 155, 164, 196\n\naccount, activating 50, 51 chat feature, using 54, 56 code, analyzing 65, 66 code, analyzing with Copilot editor window\n\nin VS Code 67, 68\n\ncode completion design overview 46, 47 code completion policies 49 debugging with 177-179 dependency code, debugging 180, 181 /explain command, using with 164, 165 extension, installing for VS Code 53 faulty implementation, fixing 125 free account limits 48 imports and hashtags 118-121 interaction modes 54 lead-in cue structure 117, 118\n\n419\n\n420\n\nplugin, installing for PyCharm 52 precise prompts, crafting 117 pricing 49 setting up, in PyCharm 51 setting up, in VS Code 51 short prompts, without comment\n\nfluff 124, 125 single task 121-123 style guide 196-199 unit test, evaluating 126-134 used, for profiling runtime 339 used, for profiling space usage 339 used, for refactoring chain-of-thought\n\n(CoT) 251-253\n\nused, for searching outdated docstrings 304-307\n\nwith descriptive names instructions 123 with docstrings instructions 123 with type hints instructions 123 with unit tests instructions 123 working, in VS Code Jupyter Notebook\n\nfor Code Analysis 69\n\nGitHub Copilot baseline 213\n\nGitHub Copilot command working with 296-300\n\nGitHub Copilot, prompt analysis\n\nprompt techniques, applying with\n\nPyCharm chat 135\n\nsingle task, to fix method\n\nimplementation 135, 136 single task, with VS Code 135 specific and short prompt\n\nprinciples 134, 135\n\nstructured prompt principle, with\n\nlead-in cue 134\n\nsurrounding context, with VS Code 135\n\nGoogle style 295\n\nGoogle-style docstring 112-116\n\nanalysis 116, 117\n\nGPT-3.5 model 154\n\nGPT-4o mini 209\n\nused, for computing quadratic roots 271\n\nGPT-o series 219 adoption 220\n\ngraphical processing units (GPUs) 357\n\ngraphical user interface (GUI) 269\n\nH\n\nHypertext Transfer Protocol (HTTP) 19, 161\n\nI\n\nIEBUPDTE 5\n\nImageNet Large Scale Visual Recognition\n\nChallenge (ILSVRC) 390\n\nimplementation code 311\n\nin-context learning 189, 190, 265\n\nintegrated development environment (IDE) 5, 45, 104, 362, 393\n\ninteraction modes\n\nanalysis 54 chat 54 completion 54\n\ninverse CoT for decorator\n\nimplementation 369-374\n\ninverse CoT with ChatGPT\n\napplying 375 decorators, monitoring 375, 377\n\ninverse CoT with OpenAI\n\napplying 375 decorators, monitoring 377, 378\n\nIndex\n\nIndex\n\nJ\n\nJSONL file\n\ncrafting, for fine-tuning 273 file structure, fine-tuning 274, 275 strategy 275\n\nJSONL file, strategy code-only 276 code with no comments 277, 278 contrastive code-only 276, 277 contrastive no comments 278-280\n\nK\n\nKite 6\n\nL\n\nL1 norm 243\n\nL2 norm 243\n\nlanguage models (LMs) 7\n\nlarge language models (LLMs) 4, 18, 45, 78, 106, 149, 209, 265, 335, 361, 390 ChatGPT, for explaining code 166-168 code task, specializing 268 costs 280, 281 /explain command, using with GitHub\n\nCopilot 164, 166 fine-tuned model, using 284 fine-tuning 267, 268 fine-tuning in action 287 fine-tuning job, initiating 281-284 fine-tuning, with OpenAI GUI 280 limitations 153, 154 OpenAI API, for explaining code 168, 169 outputs, comparing 285-287 rise 6, 7 used, for explaining code 163\n\nlinters 5\n\nLlama 7\n\nLLM output performance\n\ncontext window, adding 151, 152 fine-tuning 152 improving 151 prompt engineering 151 retrieval-augmented generation 153\n\nLLMs specialization 267\n\nexamples 267\n\nlogging 362-365\n\nlonger term 398\n\nM\n\nmachine learning (ML) 6\n\nMaestro I 5\n\nmagic attribute 364\n\nManhattan distance 159-162, 243\n\nfor matrices 162, 163\n\nmemory space usage 338\n\nefficiency tradeoffs, balancing 339 space complexity 338\n\nMiller’s Law 295\n\nmodel mastery 79\n\nmonitoring 362, 366, 367\n\nN\n\nnatural language generation (NLG) 7\n\nnatural language processing (NLP) 6, 7, 18, 155, 390\n\nneural networks 6\n\nneurons 7\n\nn-grams 313\n\n421\n\n422\n\nnon-project files 169\n\nentire file, explaining 170 explaining, with ChatGPT 172, 173 explaining, with context 171, 172 explaining, with OpenAI API 174, 175\n\nNumPy/SciPy style 295\n\nO\n\none-shot learning 152\n\nOpenAI 17\n\nbilling, computing 30, 31 project API keys, obtaining 22-24 used, for performance refactoring 258, 259\n\nOpenAI accounts rate limits 31 reference link 403 setting up 403, 404 usage restrictions 31\n\nOpenAI API 10, 18\n\nbody 19 endpoint 19 headers 19 history 231-235 HTTP method 19 precise prompts, crafting 107 prompt 256 routing chat capabilities, to code\n\ncompletion 37-41 structured prompts 107, 108 system prompts, with surrounding context\n\nand single task 108\n\nthrough RESTful HTTP request 19 used, for explaining code 168, 169 used, for extracting Python objects\n\nproperties 104-106\n\nused, for generating docstrings 110\n\nused, for refactoring chain-of-thought\n\n(CoT) 254, 255\n\nused, for searching outdated docstrings 304-307\n\nuser prompts, with specific instructions 109 using, to generate code 36 vectorizing 260, 261\n\nOpenAI API baseline 215-217\n\nOpenAI API Playground\n\nexperimenting with 269-271 quadratic roots, computing\n\nwith GPT-4o mini 271\n\nOpenAI API request cost calculating 28 parameters analyzing 32-36\n\nOpenAI GUI\n\nused, for fine-tuning large language models\n\n(LLMs) 280\n\nopenai Python package\n\ninstalling 21 OpenAI API request, sending 24-27\n\nOpenAI Tokenizer reference link 29\n\noutdated docstrings\n\nsearching, with ChatGPT 304-307 searching, with GitHub Copilot 304-307 searching, with OpenAI API 304-307\n\nP\n\npair programmer 10\n\nparameterized decorator 373\n\npattern matching 5\n\nPEP 257 295\n\nIndex\n\nIndex\n\nperformance refactoring advanced features 261 with ChatGPT 258, 259 with GenAI 256 with GitHub Copilot 256-258 with OpenAI 258, 259\n\nprecise prompts 81\n\ncrafting, for GitHub Copilot 117\n\nproduction system 8\n\nprofiling runtime 339-341 with GitHub Copilot 339\n\nprofiling space usage 342 with GitHub Copilot 339\n\nprogram runtime 336, 337\n\nruntime complexity, measuring 337\n\nprompt chaining for LLM\n\nChatGPT, chaining with 228-231 Copilot, chaining 236, 237 OpenAI API, history 231-235 using 227\n\nprompt engineering 10, 151\n\nfor coding 188, 189 versus fine-tuning 268\n\nprompt engineering, techniques 203, 266\n\nadditional methods 206 bug fix at scale 203, 204 chain-of-thought (CoT) prompting 205, 206 iterative prompting 204, 205 template-based prompting 205\n\nprompt implementation scope 210\n\nprompting techniques 391\n\nprompts 18\n\nassistant prompts 19 system prompts 19 user prompts 18\n\nprompt samples by OpenAI 94, 95\n\nanalysis 99-101 chain-of-thought prompting 101 code debugging 95, 96 guided walk-through 97-99 iterative prompting 101\n\nprompts for ChatGPT\n\nanalysis 92- 94 crafting 84, 85 guided walk-through 87, 88 single task 89 specific instructions 90-92 structuring 88 surrounding context, adding 89\n\nprompts for coding tasks, best practices 81\n\nshort prompts 83 single task per prompt 82 specific instructions 83 structured 81 surrounding information 82\n\nproof of concept (POC) 9\n\npull request (PR) 78\n\nPyCharm 5\n\nused, for setting up GitHub Copilot 51\n\nPyCharm GUI\n\nconverting, to CLI 85, 87\n\nPython objects properties\n\nextracting, for OpenAI API 104-106\n\nPython threading 80\n\nQ\n\nquadratic roots\n\ncalculator, implementing 272, 273 computing, with GPT-4o mini 271\n\n423\n\n424\n\nR\n\nraising errors 362\n\nRandom Access Memory (RAM) 336\n\nReddit 6\n\nrefactoring 241 types 245, 246\n\nrefactoring tasks\n\nperformance 246 readability 246 structure change 246\n\nregression test package 330\n\nreinforcement learning 391\n\nreinforcement learning from human\n\nfeedback (RLHF) 154\n\nRepresentational State Transfer (REST) 19\n\nrequests per day (RPD) 31\n\nrequests per minute (RPM) 31\n\nReSharper 5\n\nRESTful HTTP request 17\n\nreStructuredText (reST) 295\n\nretrieval-augmented generation 153\n\nretrieval-augmented generation (RAG) 151\n\nReuters-21578 dataset 142\n\nRevision Control System 5\n\nrobotic process automation (RPA) 395\n\nS\n\nsingle responsibility principle 361, 367\n\nsingle-task principle 364\n\nSingleton design pattern 104, 110, 111\n\nsmall language model 149\n\nsoftware development 391, 392\n\ntools, evolution 4-6\n\nsoftware development life\n\ncycle (SDLC) 4-8, 235, 287, 293, 335, 362, 405\n\nsoftware development tools 393, 394\n\nlegacy software and flexible teams 395, 396 Software Engineering Body of Knowledge\n\n(SWEBOK) 394\n\nsoftware engineers democratization 395\n\nsoftware documentation 294, 295\n\nsoftware engineering 4\n\nSoftware Engineering Body of Knowledge\n\n(SWEBOK) 394\n\nsoftware testing 5\n\nSonarQube 5\n\nSphinx style 295\n\nSpyder 5\n\nStack Overflow 6\n\nstatistical approach 142-147\n\nstructured prompts 107, 108\n\nsystem prompts 19, 107 with single task 108 with surrounding context 108\n\nT\n\ntest code 311\n\nTest-Driven Development\n\n(TDD) 9, 243, 311, 313, 322-325\n\nwith ChatGPT 327, 329 within PyCharm 327 within VS Code 325, 326\n\ntime and space analysis 336 memory space usage 338 program runtime 336, 337\n\ntokens 17, 28, 29\n\ntokens per minute (TPM) 31\n\nIndex\n\nIndex\n\ntransformer architecture 390\n\nTurbo Pascal 5\n\nU\n\nUCI Machine Learning Repository 142\n\nunexpected input\n\nhandling 365, 366\n\nUnified Markup Language (UML) 398\n\nunittest frameworks 5\n\nunit tests\n\nwith GenAI 312-317\n\nUniversal Approximation Theorem 150\n\nUSA Junior Mathematical Olympiad\n\n(USAJMO) 78\n\nUSA Mathematical Olympiad (USAMO) 78\n\nuser prompts 18, 107\n\nwith specific instructions 109\n\nV\n\nvectorized computations 256\n\nversion control systems 5\n\nvibe coding 13, 397\n\nvirtual environment 404 adding, to .gitignore 409 creating 404, 405 prompting 406-408\n\nVisual Studio 5\n\nVisual Studio Code (VS Code) 5\n\nused, for setting up GitHub Copilot 51\n\nVS Code Jupyter Notebook for Code Analysis\n\nanalysis 72, 74 GitHub Copilot, working with 69 guided walk-through 69-71\n\nW\n\nword2vec model 149, 150\n\nword-level models extension 147-149\n\nZ\n\nzero-shot cross-linguistic transfer 153\n\nzero-shot learning 152\n\nZipf’s law 142\n\n425",
      "page_number": 426
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Supercharged Coding with GenAI\n\nFrom vibe coding to best practices using GitHub Copilot, ChatGPT, and OpenAI\n\nHila Paz Herszfang\n\nPeter V. Henstock",
      "content_length": 147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Supercharged Coding with GenAI\n\nCopyright © 2025 Packt Publishing\n\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.\n\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the authors nor Packt Publishing or its dealers and distributors will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book.\n\nPackt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.\n\nPortfolio Director: Gebin George\n\nRelationship Lead: Sonia Chauhan\n\nProject Manager: Prajakta Naik\n\nContent Engineer: Aditi Chatterjee\n\nTechnical Editor: Irfa Ansari\n\nCopy Editor: Safis Editing\n\nIndexer: Pratik Shirodkar\n\nProofreader: Aditi Chatterjee\n\nProduction Designer: Vijay Kamble\n\nGrowth Lead: Nimisha Dua\n\nFirst published: August 2025\n\nProduction reference: 2220825\n\nPublished by Packt Publishing Ltd.\n\nGrosvenor House\n\n11 St Paul’s Square\n\nBirmingham\n\nB3 1RB, UK.\n\nISBN 978-1-83664-529-0\n\nwww.packtpub.com",
      "content_length": 1493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "To my husband, Dvir, my mother, Yifat, my father, Amos, my brother, Roy, and my dog, Panda— thank you for your support and encouragement throughout this journey.\n\n– Hila\n\nTo my father, with special thanks to my mother, brother, and especially my wife for their support.\n\n– Peter",
      "content_length": 278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Foreword\n\nWhen Hila told me she was working on a book about GenAI-powered software development, I smiled. Of course, she was. We’ve collaborated on papers where AI meets cybersecurity, so I’ve seen her thinking firsthand—rigorous, curious, never satisfied with surface-level insights. If anyone was going to map the future of coding with AI, it was Hila.\n\nThis book is not just another tour of ChatGPT or Copilot. It’s a builder’s manual for the age of AI-augmented engineering. It’s part workflow, part playbook, and part philosophical reflection on what it means to code when the machine is your collaborator. It goes from prompt engineering to system design, from small refactors to architectural guidance, from GitHub Copilot to OpenAI APIs, without losing the plot or pandering to hype. What I appreciate most is how grounded it is. Hila and Peter don’t romanticize GenAI, and they don’t fear it either. They approach it as en- gineers: curious, skeptical, and practical. How do you evaluate GenAI output? How do you keep it reliable? When should you override it, or better yet, teach it? These aren’t abstract questions. They’re daily challenges, and this book meets them with clarity and grit. For those of us who live at the intersection of AI, code, and security, this book feels like home. It speaks to the real problems developers face when integrating these tools into production environments, where correctness matters, hallucinations can be dangerous, and productivity means more than autocomplete. If you’re looking for a book that teaches you how to code faster, sure, you’ll get that. But if you’re looking for a book that teaches you how to think more clearly about coding in a world where machines also write code, then this is your book.\n\nCongratulations, Hila and Peter. You’ve created something timely, honest, and actually useful.\n\nMike Erlihson, PhD\n\nHead of AI, Stealth Cyber Startup",
      "content_length": 1908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Contributors\n\nAbout the authors\n\nHila Paz Herszfang, with seven years of building machine learning (ML) services and leading teams, holds a master’s degree in information management systems and is completing a second master’s in data science, both from Harvard Extension School. She developed a Python for MLOps Udemy course and runs a math and tech TikTok channel boasting 15K followers and 300K+ likes.\n\nPeter V. Henstock is an AI expert with 25+ years of experience at Pfizer, Incyte, and MIT LL. He teaches graduate software engineering and AI/ML courses at Harvard Extension School. He holds a PhD in AI from Purdue and seven Master’s degrees. Recognized as a top AI leader by DKA, Peter guides professionals in AI/ML, software, visualization, and statistics.",
      "content_length": 764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "About the reviewers\n\nMike Erlihson is a seasoned AI professional, leveraging his PhD in mathematics and extensive expertise in deep learning and data science. As a prolific scientific content creator and lecturer, he has reviewed approximately 500 deep learning papers and hosted more than 50 recorded pod- casts in the field, building a substantial following of over 60,000 on LinkedIn. In addition to his professional work, Mike is committed to education and knowledge sharing in the AI community, making complex topics accessible through his various content platforms.\n\nAlice Fridberg is a data science team lead with a master’s in applied statistics from Tel Aviv University. She specializes in innovative ML and deep learning methods for marketing optimi- zation, forecasting, and user modelling. Her work earned her the Top Women in Media & Ad Tech – Data Demystifiers award. Alice in an active public speaker, delivering talks such as A Brief History of Data Science with the Women on Stage community. She also mentors students and early-career professionals through programs with DataHack, Women in Data Science, and Tel Aviv University.",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Subscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below:",
      "content_length": 408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Table of Contents\n\nPreface\n\nxxiii\n\nYour Book Comes with Exclusive Perks – Here’s How to Unlock Them ............................ xxx\n\nPart 1: Foundations for Coding with GenAI\n\n1\n\nChapter 1: From Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\n3\n\nChanging the software engineering field ............................................................................ 4\n\nThe evolution of tools for software development • 4\n\nIntroducing the rise of LLMs ............................................................................................... 6\n\nExploring the software development lifecycle .................................................................... 7\n\nEmbracing a GenAI toolkit .................................................................................................. 9\n\nChatGPT • 9\n\nOpenAI API • 10\n\nGitHub Copilot • 10\n\nImproving software development with GenAI • 11\n\nIs GenAI worth learning for software engineering? ........................................................... 12\n\nThe benefits of coding with GenAI • 12\n\nThe downsides of coding with GenAI • 12\n\nTakeaways • 13",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "x\n\nTable of Contents\n\nSummary .......................................................................................................................... 14\n\nFurther reading .................................................................................................................. 15\n\nChapter 2: Your Quickstart Guide to OpenAI API\n\n17\n\nTechnical requirements .................................................................................................... 18\n\nIntroducing OpenAI API .................................................................................................... 18\n\nOpenAI API through a RESTful HTTP request • 19\n\nOpenAI API Python package installation • 21\n\nObtaining your project API keys from OpenAI • 22\n\nLab 2.1 – sending your first OpenAI API request using the openai Python package ........... 24\n\nLab 2.1 guided walk-through • 25\n\nLab 2.1 analysis • 26\n\nLearning how request costs are calculated ........................................................................ 28\n\nUnderstanding tokens • 28\n\nHow billing is computed • 30\n\nUnderstanding rate limits and usage restrictions for free and paid OpenAI accounts ........ 31\n\nLab 2.2 – analyzing request parameters ............................................................................ 32\n\nLab 2.2 guided walk-through • 33\n\nLab 2.2 analysis • 35\n\nMultiple service responses with Lab 2.2 • 36\n\nLab 2.3 – using OpenAI API to generate code ..................................................................... 36\n\nRouting chat capabilities to code completion • 37\n\nLab 2.3 guided walk-through • 38\n\nLab 2.3 analysis • 40\n\nSummary .......................................................................................................................... 42\n\nQuiz time .......................................................................................................................... 42\n\nFurther reading ................................................................................................................. 43\n\nSubscribe for a free eBook ................................................................................................. 44",
      "content_length": 2150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Table of Contents\n\nChapter 3: A Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nTechnical requirements .................................................................................................... 46\n\nIntroducing GitHub Copilot – our AI pair programmer ..................................................... 46\n\nCode completion design overview • 46\n\nUnderstanding GitHub Copilot’s free account limits, costs, and policies .......................... 48\n\nWho is eligible for a free account? • 48\n\nPricing • 49\n\nCode completion policies • 49\n\nActivating your GitHub Copilot account • 50\n\nSetting up Copilot in PyCharm and VS Code ...................................................................... 51\n\nGitHub Copilot plugin for PyCharm • 52\n\nGitHub Copilot extension for VS Code • 53\n\nThe three interaction modes – chat, completion, and analysis • 54\n\nUsing Copilot chat ............................................................................................................. 54\n\nLab 3.1 – calculating geometric mean with chat completion ............................................. 57\n\nLab 3.1 guided walk-through • 58\n\nLab 3.1 analysis • 59\n\nLab 3.2 – introducing completion and keyboard shortcuts for geometric mean calculations ............................................................................................................. 61\n\nLab 3.2 guided walk-through • 61\n\nLab 3.2 analysis • 63\n\nAnalyzing code with Copilot ............................................................................................. 65\n\nAnalyzing code with Copilot editor window in VS Code • 67\n\nLab 3.3 – working with Copilot in VS Code’s Jupyter Notebook for Code Analysis ............ 69\n\nLab 3.3 guided walk-through • 69\n\nLab 3.3 analysis • 72\n\nxi\n\n45",
      "content_length": 1780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "xii\n\nTable of Contents\n\nSummary .......................................................................................................................... 74\n\nQuiz time .......................................................................................................................... 75\n\nFurther reading ................................................................................................................. 75\n\nChapter 4: Best Practices for Prompting with ChatGPT\n\n77\n\nTechnical requirements .................................................................................................... 78\n\nCan we trust GenAI for coding tasks? ................................................................................ 78\n\nThe three pillars of good outputs • 79\n\nBest practices for crafting prompts for coding tasks ......................................................... 81\n\nStructured • 81\n\nSurrounding information • 82\n\nSingle task per prompt • 82\n\nSpecific instructions • 83\n\nShort prompts • 83\n\nCrafting prompts for ChatGPT .......................................................................................... 84\n\nLab 4.1 – converting PyCharm GUI steps to CLI • 85\n\nLab 4.1 guided walk-through • 87\n\nLab 4.1 analysis • 92\n\nPrompt samples by OpenAI ............................................................................................... 94\n\nLab 4.2 – code debugging • 95\n\nLab 4.2 guided walk-through • 97\n\nLab 4.2 analysis • 99\n\nSummary ......................................................................................................................... 101\n\nQuiz time ......................................................................................................................... 101\n\nFurther reading ............................................................................................................... 102\n\nSubscribe for a free eBook ............................................................................................... 102",
      "content_length": 1994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Table of Contents\n\nChapter 5: Best Practices for Prompting with OpenAI API and GitHub Copilot\n\nTechnical requirements .................................................................................................. 104\n\nExtracting properties from Python objects for OpenAI API ............................................. 104\n\nCrafting precise prompts for OpenAI API ......................................................................... 107\n\nStructured prompts • 107\n\nSystem prompts with the surrounding context and single task • 108\n\nUser prompts with specific instructions • 109\n\nLab 5.1 – Generating docstrings with OpenAI API ............................................................ 110\n\nSingleton design pattern • 110\n\nGenerating a docstring • 112\n\nLab 5.1 guided walk-through • 113\n\nLab 5.1 analysis • 116\n\nCrafting precise prompts for GitHub Copilot ................................................................... 117\n\nStructuring with a lead-in cue • 117\n\nSurrounding with imports and hashtags • 118\n\nFurther narrowing the single task • 121\n\nSpecific instructions with type hints, docstrings, descriptive names, and unit tests • 123\n\nShort prompts without comment fluff • 124\n\nLab 5.2 – Fixing faulty implementation with GitHub Copilot .......................................... 125\n\nLab 5.2 guided walk-through • 126\n\nLab 5.2 analysis • 134\n\nPrompt 1 – following the structured prompt principle with a lead-in cue • 134\n\nPrompt 2– providing specific and short prompt principles • 134\n\nPrompt 3 – adding surrounding context and focusing on a single task with VS Code • 135\n\nPrompt 4 – applying the five S’s with PyCharm chat • 135\n\nPrompt 5 – narrowing down the single task to fix a method implementation • 135\n\nSummary ......................................................................................................................... 136\n\nQuiz time ......................................................................................................................... 137\n\nFurther reading ............................................................................................................... 138\n\nxiii\n\n103",
      "content_length": 2149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "xiv\n\nTable of Contents\n\nPart 2: Basics to Advanced LLM Prompting for GenAI Coding\n\n139\n\nChapter 6: Behind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\n141\n\nTechnical requirements ................................................................................................... 142\n\nStatistical approach to LLMs ............................................................................................ 142\n\nExtending the model to words • 147\n\nSmall and large language models for words ..................................................................... 149\n\nWord2vec • 149\n\nDeep learning • 150\n\nImproving LLM output performance ............................................................................... 151\n\nPrompt engineering • 151\n\nAdding context • 151\n\nProviding examples • 152\n\nFine-tuning • 152\n\nRetrieval-augmented generation • 153\n\nWhat are the limitations of LLMs? ................................................................................... 153\n\nChatGPT .......................................................................................................................... 154\n\nGitHub Copilot ................................................................................................................. 155\n\nSummary ......................................................................................................................... 156\n\nQuiz time ......................................................................................................................... 156\n\nFurther reading ................................................................................................................ 157\n\nSubscribe for a free eBook ............................................................................................... 158\n\nChapter 7: Reading and Understanding Code Bases with GenAI\n\n159\n\nTechnical requirements .................................................................................................. 160\n\nReading and understanding the code base structure ...................................................... 160\n\nIntroducing the Manhattan distance • 161\n\nManhattan distance for matrices • 162",
      "content_length": 2175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Table of Contents\n\nUsing LLMs to explain code .............................................................................................. 163\n\nUsing the /explain command with GitHub Copilot • 164\n\nUsing ChatGPT to explain code • 166\n\nUsing OpenAI API to explain code • 168\n\nExplaining non-project files ............................................................................................. 169\n\nExplaining an entire file • 170\n\nExplaining with context • 171\n\nExplaining non-project files with ChatGPT • 172\n\nExplaining non-project files with OpenAI API • 174\n\nCreating calls with example parameters .......................................................................... 175\n\nDebugging with GitHub Copilot • 177\n\nDebugging dependency code • 180\n\nCustomizing call parameters with ChatGPT and OpenAI • 182\n\nDebugging dependency code • 183\n\nPutting it all together • 184\n\nSummary ......................................................................................................................... 185\n\nQuiz time ........................................................................................................................ 186\n\nFurther reading ............................................................................................................... 186\n\nChapter 8: An Introduction to Prompt Engineering\n\nTechnical requirements .................................................................................................. 188\n\nUtilizing prompt engineering for coding ......................................................................... 188\n\nEnhancing prompts with few-shot examples ................................................................. 189\n\nFew-shot learning for code refactoring • 190\n\nLeveraging few-shot prompting with ChatGPT ............................................................... 192\n\nFew-shot structure • 192\n\nImprovement in output with code refactoring • 193\n\nCrafting a style guide for GitHub Copilot ........................................................................ 196\n\nStyle guide file • 198\n\nWorking with few-shot learning for OpenAI API ............................................................ 199\n\nImplementing code with OpenAI API • 200\n\nxv\n\n187",
      "content_length": 2233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "xvi\n\nTable of Contents\n\nIntroducing more prompt engineering techniques ......................................................... 203\n\nBug fix at scale with prompt engineering • 203\n\nIterative prompting • 204\n\nTemplate-based prompting • 205\n\nChain-of-thought prompting • 205\n\nExploring additional methods • 206\n\nSummary ........................................................................................................................ 207\n\nQuiz time ........................................................................................................................ 207\n\nFurther reading .............................................................................................................. 208\n\nSubscribe for a free eBook .............................................................................................. 208\n\nChapter 9: Advanced Prompt Engineering for Coding-Related Tasks\n\n209\n\nTechnical requirements .................................................................................................. 210\n\nExtending the prompt implementation scope ................................................................. 210\n\nExtending the geometric mean scope • 210\n\nEvaluating baseline outputs • 212\n\nGitHub Copilot baseline • 212\n\nChatGPT baseline • 213\n\nOpenAI API baseline • 215\n\nLeveraging CoT reasoning with LLM ............................................................................... 217\n\nCoT in the GPT-o series • 219\n\no-series adoption • 220\n\nUsing CoT for GitHub Copilot • 221\n\nCoT for the geometric mean with ChatGPT • 225\n\nCoT with OpenAI API • 226\n\nUsing prompt chaining for LLMs .................................................................................... 227\n\nChaining with ChatGPT • 228\n\nSelective history with OpenAI API • 231\n\nChaining with Copilot • 236",
      "content_length": 1812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Table of Contents\n\nSummary ........................................................................................................................ 238\n\nQuiz time ........................................................................................................................ 238\n\nFurther reading ............................................................................................................... 239\n\nChapter 10: Refactoring Code with GenAI\n\nTechnical requirements .................................................................................................. 242\n\nIntroducing code refactoring .......................................................................................... 242\n\nManhattan and Euclidean distance • 243\n\nImplementing the distance function • 244\n\nTypes of refactoring • 245\n\nImproving readability before GenAI • 247\n\nLimitations of code refactoring with IDE tools • 248\n\nRefactoring with chain-of-thought ................................................................................ 250\n\nRefactoring for a better structure • 250\n\nCoT for refactoring with GitHub Copilot • 251\n\nLeveraging CoT for refactoring with ChatGPT and OpenAI API • 254\n\nOpenAI API prompt • 256\n\nPerformance refactoring with GenAI .............................................................................. 256\n\nPerformance refactoring with GitHub Copilot • 256\n\nPerformance refactoring with ChatGPT and OpenAI • 258\n\nVectorizing with OpenAI API • 260\n\nMore advanced performance refactoring • 261\n\nSummary ........................................................................................................................ 262\n\nQuiz time ........................................................................................................................ 262\n\nFurther reading ............................................................................................................... 263\n\nSubscribe for a free eBook ............................................................................................... 264\n\nxvii\n\n241",
      "content_length": 2066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "xviii\n\nTable of Contents\n\nChapter 11: Fine-Tuning Models with OpenAI\n\n265\n\nTechnical requirements .................................................................................................. 266\n\nFine-tuning LLMs toward a specialization ...................................................................... 266\n\nLLMs specialization • 267\n\nFine-tuning an LLM • 267\n\nFavoring fine-tuning over prompt engineering • 268\n\nTask specializing in cleaner code • 268\n\nExperimenting with the OpenAI API Playground ............................................................ 269\n\nComputing quadratic roots with GPT-4o mini • 271\n\nImplementing the quadratic roots calculator • 272\n\nCrafting a JSONL file for fine-tuning ............................................................................... 273\n\nFine-tuning file structure • 274\n\nStrategy • 275\n\nCode-only – five training examples • 276\n\nContrastive code-only – three training examples • 276\n\nCode with no comments – five examples • 277\n\nContrastive no comments – two examples • 278\n\nFine-tuning a model with the OpenAI GUI ..................................................................... 280\n\nCosts of fine-tuning a model • 280\n\nInitiating a fine-tuning job • 281\n\nUsing the fine-tuned model • 284\n\nComparing our results .................................................................................................... 285\n\nFine-tuning in action • 287\n\nSummary ........................................................................................................................ 287\n\nQuiz time ........................................................................................................................ 288\n\nFurther reading ............................................................................................................... 289",
      "content_length": 1817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Table of Contents\n\nPart 3: From Code to Production with GenAI\n\nChapter 12: Documenting Code with GenAI\n\nTechnical requirements .................................................................................................. 294\n\nIntroducing software documentation ............................................................................. 294\n\nWorking with the /explain GitHub Copilot command • 296\n\nUsing GenAI to write docstrings ..................................................................................... 300\n\nDocstring for a single method • 300\n\nDocstrings for an entire file • 302\n\nFinding outdated docstrings with GitHub Copilot, ChatGPT, and OpenAI API ............... 304\n\nPractical insights ............................................................................................................ 308\n\nSummary ........................................................................................................................ 308\n\nQuiz time ........................................................................................................................ 309\n\nFurther reading ............................................................................................................... 310\n\nSubscribe for a free eBook ............................................................................................... 310\n\nChapter 13: Writing and Maintaining Unit Tests\n\nTechnical requirements ................................................................................................... 312\n\nUnit tests with GenAI ....................................................................................................... 312\n\nChatGPT for testing • 318\n\nData-driven tests ............................................................................................................. 321\n\nTest-driven development ................................................................................................ 322\n\nTDD using GitHub Copilot within VS Code • 325\n\nTDD using GitHub Copilot within PyCharm • 327\n\nTDD using ChatGPT • 327\n\nSample data creation with GenAI .................................................................................... 329\n\nPractical insights ............................................................................................................ 332\n\nSummary ........................................................................................................................ 332\n\nQuiz time ........................................................................................................................ 333\n\nFurther reading ............................................................................................................... 334\n\nxix\n\n291\n\n293\n\n311",
      "content_length": 2748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "xx\n\nTable of Contents\n\nChapter 14: GenAI for Runtime and Memory Management\n\n335\n\nTechnical requirements .................................................................................................. 336\n\nIntroducing time and space analysis ............................................................................... 336\n\nThe runtime of a program • 336\n\nRuntime complexity refresher • 337\n\nThe space consumption of a program • 338\n\nSpace complexity • 338\n\nBalancing efficiency tradeoffs • 339\n\nProfiling runtime and memory consumption with GitHub Copilot ................................ 339\n\nProfiling runtime • 339\n\nProfiling space usage • 342\n\nAnalyzing maximal capacity with ChatGPT .................................................................... 343\n\nProfiling runtime across multiple runs • 343\n\nProfiling memory consumption across multiple runs • 345\n\nDetermining maximal capacity within runtime constraints • 347\n\nDetermining maximal capacity within memory constraints • 349\n\nOptimizing code with chained prompts ........................................................................... 351\n\nRuntime optimization • 352\n\nSpace optimization • 354\n\nFurther runtime and space optimization • 357\n\nSummary ......................................................................................................................... 357\n\nQuiz time ........................................................................................................................ 358\n\nFurther reading ............................................................................................................... 358\n\nSubscribe for a free eBook ............................................................................................... 359",
      "content_length": 1744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Table of Contents\n\nChapter 15: Going Live with GenAI: Logging, Monitoring, and Errors\n\nTechnical requirements .................................................................................................. 362\n\nIntroducing logging, monitoring, and raising errors ....................................................... 362\n\nThe FizzBuzz use case • 363\n\nLogging • 364\n\nHandling unexpected input • 365\n\nMonitoring • 366\n\nLeveraging GenAI for higher-level coding patterns ......................................................... 368\n\nUsing decorators to separate responsibilities • 368\n\nInverse CoT for decorator implementation • 369\n\nApplying inverse CoT with ChatGPT and OpenAI ........................................................... 375\n\nUsing ChatGPT to monitor decorators • 375\n\nUsing OpenAI API to monitor decorators • 377\n\nUtilizing few-shot learning and fine-tuning as style guides ............................................ 378\n\nFew-shot decorator style for GitHub Copilot • 378\n\nFew-shot learning in ChatGPT • 380\n\nLeveraging fine-tuning for OpenAI API • 383\n\nSummary ........................................................................................................................ 387\n\nQuiz time ........................................................................................................................ 387\n\nFurther reading ............................................................................................................... 388\n\nChapter 16: Architecture, Design, and the Future\n\nThe rapid rise of GenAI .................................................................................................... 390\n\nEconomics of faster software development ...................................................................... 391\n\nWill GenAI be adopted by software engineers? ................................................................ 392\n\nShifting landscape of software developers ...................................................................... 393\n\nGenAI as related to the Software Engineering Body of Knowledge • 394\n\nDemocratization of software engineers • 395\n\nLegacy software and flexible teams • 395\n\nxxi\n\n361\n\n389",
      "content_length": 2180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "xxii\n\nTable of Contents\n\nWill GenAI change programming languages? .................................................................. 396\n\nThe future of GenAI in software engineering .................................................................. 397\n\nVibe coding • 397\n\nShorter term • 398\n\nLonger term • 398\n\nWill AI replace software engineering? • 398\n\nRisk and governance • 399\n\nSummary ....................................................................................................................... 400\n\nFurther reading .............................................................................................................. 400\n\nSubscribe for a free eBook ............................................................................................... 401\n\nAppendix\n\n403\n\nSet up an OpenAI Account ............................................................................................... 403\n\nCreating a virtual environment ...................................................................................... 404\n\nPrompting virtual environment • 406\n\nAdding virtual environment to .gitignore • 409\n\nFurther reading ............................................................................................................... 410\n\nOther Books You May Enjoy\n\n413\n\nIndex\n\n417",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Preface\n\nSupercharged Coding with GenAI aims to train software developers to achieve increased productivity throughout the software development life cycle (SDLC). It covers not only the programming aspect but also how to write tests, documentation, and other aspects required for putting software into production using GenAI methods. The book introduces the five S’s framework, a standardized approach for consistently producing high-quality output that many GenAI users face.\n\nIt guides you on how and when to use the three most common GenAI software tools that currently dominate the marketplace: ChatGPT, OpenAI API, and GitHub Copilot. Each of these tools offers a different interface for generating code, each with different strengths and weaknesses. Learn- ing how to effectively use these tools is an empowering skill set in the quickly evolving field of software engineering.\n\nThis book is a hands-on approach, with many labs introduced throughout the different chapters, since GenAI coding tools require practice. The labs provide the necessary practice to challenge the knowledge and explore the key skills introduced. The book also dives deeper into the concepts behind how to use instructions, making sure that you not only learn how to do something but also understand why the GenAI is producing particular outputs.\n\nThe book is structured into three parts:\n\n\n\nPart 1, Foundations for Coding with GenAI, provides a quick start tutorial for the three dif- ferent GenAI tools you can use for code completion and surrounding tasks. We will start with OpenAI API framework to harness the large language models (LLMs) as a software developer. We built a program for code completion so we can better understand the de- sign of GenAI tools. Next, we will get started with GitHub Copilot and ChatGPT using three different interaction modes: chat, completion, and analysis. By then, we will also understand the design differences among the different interaction modes. Finally, we will introduce the five S’s framework, a structured approach to crafting precise prompts that lead to predictable and more desirable outputs.",
      "content_length": 2126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "xxiv\n\n\n\nPart 2, Basics to Advanced LLM Prompting for GenAI Coding, takes the next step toward be- coming supercharged coders. We will dive deeper into the foundations of LLMs. The goal is to gain a better understanding of why these models work so much better than the many tools that came before them. We will then start developing the mindset of a supercharged coder by learning which tasks are native to the models, which require advanced prompting techniques, and which tasks are better handled without the assistance of GenAI altogeth- er. We will also learn about applying advanced prompting techniques to coding-related tasks, how to evaluate the goodness of our output with evaluation techniques, and how to fine-tune a model to specialize it for a specific task.\n\n\n\nPart 3, From Code to Production with GenAI, is dedicated to the advanced SDLC approach, where we will be able to use our newly obtained skillset and mindset to work with GenAI tools. We will talk about logging, monitoring, debugging, unit testing, and documenting our code efficiently and quickly with GenAI tools. We will also apply prompt engineering techniques to both space and memory optimizations. We will close this chapter with talks about design, architecture, and the future.\n\nWho this book is for If you are a Python developer curious about GenAI and are looking to elevate your software engineering productivity, this book will transform your approach to software. We include many structured examples of varying problem complexity that will demonstrate the use of advanced prompting techniques, suitable for early intermediate through advanced developers.\n\nTo get the most out of this book, you should have at least one year of hands-on Python develop- ment experience and be somewhat familiar with the SDLC.\n\nWhat this book covers Chapter 1, From Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI, describes the recent convergence of software development tools and the AI transforma- tion with LLMs. It makes a case that now is the best time to improve skills in GenAI to produce quality code faster.\n\nChapter 2, Your Quickstart Guide to OpenAI API, introduces everything you need to know to begin using OpenAI. By the end of the chapter, you will be able to start using it to write code.\n\nPreface",
      "content_length": 2320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Preface\n\nChapter 3, A Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook, provides setup instructions for GitHub Copilot, the pair programming GenAI tool. It then shows how to inte- grate GitHub Copilot with the two most popular IDEs. With this background, you will be able to prompt with GenAI.\n\nChapter 4, Best Practices for Prompting with ChatGPT, dives into prompting, which is a core skill for GenAI coding. This chapter introduces the five S’s framework for consistent outputs.\n\nChapter 5, Best Practices for Prompting with OpenAI API and GitHub Copilot, completes Part 1 of the book by extending the prompting methods to functions, classes, and methods. The methods are applied to not just writing code but also fixing bugs and providing documentation.\n\nChapter 6, Behind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work, sheds light on how LLMs work under the hood since such an understanding can help understand the strengths and limitations. The chapter begins with a statistical example as a motivation and continues through general LLMs. The chapter concludes with the extension to code-specific LLMs that are at the heart of GitHub Copilot.\n\nChapter 7, Reading and Understanding Code Bases with GenAI, showcases the effectiveness of the different GenAI tools in working with full code bases. The chapter shows how GenAI can explain how code works and help debug issues using the code and support documentation.\n\nChapter 8, An Introduction to Prompt Engineering, applies this recent field to code development and introduces few-shot learning. Prompt engineering methods are applied through OpenAI API and GitHub Copilot for code refactoring to support a consistent coding style.\n\nChapter 9, Advanced Prompt Engineering for Coding-Related Tasks, extends the few-shot approaches from Chapter 8 into reasoning models. Iterative prompting, chain-of-thought prompting, and chaining can effectively deliver highly predictable code that implements key functionality, in- cluding code and comments.\n\nChapter 10, Refactoring Code with GenAI, revisits and extends the chain-of-thought prompting technique to improve code. The chapter covers examples of refactoring code for improved quality and performance.\n\nChapter 11, Fine-Tuning Models with OpenAI, compares and contrasts the few-shot learning ap- proach with fine-tuning, a standard approach for customizing LLMs. The chapter uses the OpenAI Playground to compare the effectiveness and costs of the two approaches.\n\nxxv",
      "content_length": 2505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "xxvi\n\nChapter 12, Documenting Code with GenAI, begins Part 3, which focuses on SDLC components beyond coding. The chapter is a how-to guide for effectively writing docstrings, the standard documentation for Python. It also provides solutions to the problem of out-of-date comments within the code.\n\nChapter 13, Writing and Maintaining Unit Tests, demonstrates multiple methods for leveraging GenAI to accelerate the creation of tests. The chapter also covers the test-driven development (TDD) process of writing test cases first and automatically generating the functional code to pass the tests.\n\nChapter 14, GenAI for Runtime and Memory Management, delves into performance optimization, which is rarely discussed in GenAI code literature. Its examples show how to assess performance trade-offs and, using a combination of prompting with chaining prompts, how to estimate run- time capacities and optimize code to run at scale.\n\nChapter 15, Going Live with GenAI: Logging, Monitoring, and Errors, provides guidance on the use of GenAI for key late-stage components for putting code into production. The chapter brings together past concepts such as enforcing style, few-shot learning, and fine-tuning as applied to monitoring and logging methods.\n\nChapter 16, Architecture, Design, and the Future, discusses the rise of GenAI, software economics, and the changing landscape of software engineering. It provides a perspective on the future of software engineering from vibe coding to long-term expectations, including risks and governance.\n\nTo get the most out of this book Following along will be easier if you bear the following in mind:\n\n\n\nExamples: Begin with the hands-on examples provided in each chapter to make sure that you can effectively use all the tools, rather than focus on just one\n\n\n\nLabs: Use the labs to test your knowledge of how you approach the challenges, and use the questions at the end of each chapter to make sure you grasp the fundamentals\n\nGenAI approach: Experiment with the different techniques from each chapter on your own code and examples to see how GenAI can change your approach to software engineering\n\n\n\nThink beyond: Reflect on how the practical knowledge of the introduced GenAI tools re- lates to the fundamentals of how LLMs work, and how they can enhance multiple aspects of your organization’s software development practices\n\nPreface",
      "content_length": 2378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Preface\n\nHere is a list of things you need to have:\n\nSoftware/hardware covered in the book\n\nSystem requirements\n\nPython 3.11 or higher\n\nWindows, macOS, or Linux.\n\nLLM chat and embedding models\n\nWindows, macOS, or Linux.\n\nYou can decide to leverage your LLM of choice. Throughout the book, we will be using a variety of GPT models from ChatGPT, OpenAI API, and GitHub Copilot.\n\nDownload the example code files The code bundle for the book is hosted on GitHub at https://github.com/PacktPublishing/ Supercharged-Coding-with-Gen-AI. We also have other code bundles from our rich catalog of books and videos available at https://github.com/PacktPublishing. Check them out!\n\nDownload the color images We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: https://packt.link/gbp/9781836645290.\n\nConventions used There are a number of text conventions used throughout this book.\n\nCodeInText: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and X handles. For example, “The edits window supports the slash commands such as /fix, /explain, and /test, as well as tailored instructions to modify your code.”\n\nA block of code is set as follows:\n\ndef get_geometric_mean_of_two_numbers(\n\na: float,\n\nb: float,\n\n) -> float:\n\nreturn pow(a * b, 1 / 2)\n\nnum1: float = 5.0\n\nnum2: float = 20.0\n\nprint(get_geometric_mean_of_two_numbers(num1, num2))\n\nxxvii",
      "content_length": 1482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "xxviii\n\nAny command-line input or output is written as follows:\n\n(.venv) $ pip install openai\n\nBold: Indicates a new term, an important word, or words that you see on the screen, for example, in menus or dialog boxes. For example: “In this chapter, we will introduce the practice of prompt engineering, a field of study that started in 2020, which offers advanced techniques for refining prompts to achieve more reliable, predictable, and desirable outcomes in large language model (LLM)-based applications.”\n\nWarnings or important notes appear like this.\n\nTips and tricks appear like this.\n\nDisclaimer on AI usage The authors acknowledge the use of cutting-edge AI, such as ChatGPT, OpenAI API, and GitHub Copilot, with the sole aim of enhancing the language and clarity within the book, thereby ensuring a smooth reading experience for readers. It is important to note that the content itself has been crafted by the authors and edited by a professional publishing team.\n\nGet in touch Feedback from our readers is always welcome!\n\nGeneral feedback: Email feedback@packtpub.com and mention the book’s title in the subject of your message. If you have questions about any aspect of this book, please email us at questions@ packtpub.com.\n\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you could report this to us. Please visit http://www.packtpub.com/submit-errata, click Submit Errata, and fill in the form.\n\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at copyright@packtpub.com with a link to the material.\n\nPreface",
      "content_length": 1787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Preface\n\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit http://authors.packtpub. com/.\n\nJoin our Discord and Reddit spaces You’re not the only one navigating fragmented tools, constant updates, and unclear best prac- tices. Join a growing community of professionals exchanging insights that don’t make it into documentation.\n\nStay informed with updates, discussions, and behind-the-scenes insights from our authors.\n\nJoin our Discord space at https://packt. link/z8ivB or scan the QR code below:\n\nConnect with peers, share ideas, and discuss real-world GenAI challenges. Follow us on Reddit at https://packt.link/0rExL or scan the QR code below:\n\nShare your thoughts Once you’ve read Supercharged Coding with GenAI, we’d love to hear your thoughts! Please click here to go straight to the Amazon review page for this book and share your feedback.\n\nYour review is important to us and the tech community and will help us make sure we’re deliv- ering excellent quality content.\n\nxxix",
      "content_length": 1106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "xxx\n\nYour Book Comes with Exclusive Perks – Here’s How to Unlock Them\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search this book by name. Ensure it’s the correct edition.\n\nNote: Keep your purchase invoice ready before you start.\n\nEnhanced reading experience with our Next-gen Reader:\n\nMulti-device progress sync: Learn from any device with seamless progress sync.\n\nHighlighting and notetaking: Turn your reading into lasting knowledge.\n\nBookmarking: Revisit your most important learnings anytime.\n\nDark mode: Focus with minimal eye strain by switching to dark or sepia mode.\n\nLearn smarter using our AI assistant (Beta):\n\nSummarize it: Summarize key sections or an entire chapter.\n\nAI code explainers: In the next-gen Packt Reader, click the Explain button above each code block for AI-powered code explanations.\n\nPreface",
      "content_length": 876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Preface\n\nNote: The AI assistant is part of next-gen Packt Reader and is still in beta.\n\nLearn anytime, anywhere:\n\nAccess your content offline with DRM-free PDF and ePub versions—compatible with your favorite e-readers.\n\nUnlock Your Book’s Exclusive Benefits Your copy of this book comes with the following exclusive benefits:\n\nNext-gen Packt Reader\n\nAI assistant (beta)\n\nDRM-free PDF/ePub downloads\n\nUse the following guide to unlock them if you haven’t already. The process takes just a few min- utes and needs to be done only once.\n\nHow to unlock these benefits in three easy steps\n\nStep 1 Keep your purchase invoice for this book ready, as you’ll need it in Step 3. If you received a physical invoice, scan it on your phone and have it ready as either a PDF, JPG, or PNG.\n\nFor more help on finding your invoice, visit https://www.packtpub.com/unlock-benefits/help.\n\nNote: Did you buy this book directly from Packt? You don’t need an invoice. After completing Step 2, you can jump straight to your exclusive content.\n\nxxxi",
      "content_length": 1024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "xxxii\n\nStep 2 Scan this QR code or go to packtpub.com/unlock.\n\nOn the page that opens (which will look similar to Figure 0.1 if you’re on desktop), search for this book by name. Make sure you select the correct edition.\n\nFigure 0.1: Packt unlock landing page on desktop\n\nPreface",
      "content_length": 278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Preface\n\nStep 3 Sign in to your Packt account or create a new one for free. Once you’re logged in, upload your invoice. It can be in PDF, PNG, or JPG format and must be no larger than 10 MB. Follow the rest of the instructions on the screen to complete the process.\n\nNeed help? If you get stuck and need help, visit https://www.packtpub.com/unlock- benefits/help for a detailed FAQ on how to find your invoices and more. The following QR code will take you to the help page directly:\n\nNote: If you are still facing issues, reach out to customercare@packt.com.\n\nxxxiii",
      "content_length": 567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Part 1\n\nFoundations for Coding with GenAI\n\nIn Part 1 of this book, we introduce the fundamentals of GenAI for coding and get you started with both OpenAI API and GitHub Copilot. The part begins with a discussion of how GenAI for coding has recently emerged from the intersection of a long evolution in software development tools and the recent large language models (LLMs) from the AI space. This recent fusion of technologies has completely changed the programming landscape. Now is the perfect time to begin the jour- ney since applying them across software engineering tasks requires both training and practice.\n\nThe remainder of Part 1 provides hands-on guidance to start using OpenAI API and GitHub Copilot. After setting up these tools, the part introduces best practices for prompting.\n\nThis part contains the following chapters:\n\n\n\nChapter 1, From Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nChapter 2, Your Quickstart Guide to OpenAI API\n\nChapter 3, A Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nChapter 4, Best Practices for Prompting with ChatGPT\n\nChapter 5, Best Practices for Prompting with OpenAI API and GitHub Copilot",
      "content_length": 1201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "1\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nIf you are reading this book, you have probably heard some of the excitement, hype, concerns, and reality of Generative Artificial Intelligence (GenAI) for coding. You may have checked out some tutorials online and perhaps even explored using this technology for your own coding.\n\nLearning to apply GenAI to software coding takes both practice and time. While there are many online demonstrations of the capabilities, there has not been a systematic approach for achieving functional, quality code with any consistency. There also aren’t many resources that guide devel- opers to use GenAI beyond simple code completion or perhaps testing. GenAI can be particularly useful in expediting tasks such as standardizing coding style to improve readability, debugging, optimizing performance, and the many other tasks performed by software engineers.\n\nIn this chapter, we will explore the following topics:\n\n\n\nChanging the software engineering field\n\n\n\nIntroducing the rise of large language models\n\n\n\nExploring the software development lifecycle\n\n\n\nEmbracing a GenAI toolkit",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "4\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\n\n\nIs GenAI worth learning for software engineering?\n\nWhat you will get from this book\n\nChanging the software engineering field Computer programming and software engineering, in general, contribute not only to the tech industry, but to many different sectors of the economy, including commerce, finance, health, transportation, and energy. Software drives the creation of many new products. It increases the productivity of companies through the automation and optimization of processes and enables cost reductions.\n\nAs software continues to deliver economic value, new paradigms and tools for software developers have increased the ability to write quality software at a faster pace. Over the last couple of years, GenAI has become one of these tools.\n\nIn software engineering, GenAI has suddenly advanced to reach an inflection point and is fun- damentally changing the field. This recent technology allows everyone from novices to expert software developers to supercharge their productivity not only in coding but, more generally, the full software development lifecycle (SDLC).\n\nAdvanced technologies, including artificial intelligence, seem to be in the news every day lately. Despite this, many software engineers seem somewhat surprised that AI has progressed to the point that it can support their field and specific software development work. The current state of software engineering tools has resulted from the convergence of two separate trends. First, software development tools are not new but have progressed continuously over many decades. Second, GenAI technology has crossed over from the rapid emergence of large language models (LLMs), which trace back to neural networks and the origins of artificial intelligence.\n\nThe evolution of tools for software development The application of GenAI to software engineering is quite a recent development. Although AI has been discussed for many years as a promising set of tools for enhancing code development, the emergence of GenAI has ushered in a new era of capabilities.\n\nSoftware development has experienced many new tools over the past decades that have trans- formed the field. It is easy to argue that software development is constantly evolving, with new tools that have streamlined the processes and enhanced productivity. This section provides an overview of some major technology revolutions that have aided software developers.",
      "content_length": 2496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Chapter 1\n\nIn the 1970s and 1980s, the Maestro I was developed as the first integrated development envi- ronment (IDE), although it would hardly be recognized as such by today’s standards. Its succes- sors, such as Borland’s Turbo Pascal and Visual Studio, provided an easy integration of coding, file management, debugging, compilation, and execution. Today’s IDEs for Python, such as Visual Studio Code, PyCharm, and Spyder, facilitate global changes to variables, code highlighting, syntax checkers, and access to multiple tools.\n\nVersion control systems were a critical step in software engineering, enabling many developers to work on a single project. With a single code base, different versions of code can be tracked and managed. IBM’s IEBUPDTE in the 1960s was a forerunner of the technology, followed by the Re- vision Control System in 1982 and the Concurrent Versions System (CVS) in 1986. It wasn’t until 2005 that the now ubiquitous Git was developed, which enabled a distributed version control system.\n\nBuild tools and continuous integration and continuous deployment (CI/CD) systems speed the delivery process of software. Build tools such as Jenkins and Maven transform source code into executable code. CI/CD tools are often triggered by the build, but continue further to automate the testing, execute linters or other code tools, and often deploy the updated version to users. The full deployment pipeline frees the developers from the many manual steps and enables both a rapid and consistent way of providing users with the latest functionality.\n\nSignificant research has been poured into software testing. Apart from many specialized tools for different forms of testing, testing frameworks are now a standard part of virtually all software development suites. IDEs already speed up the process of creating skeleton tests from existing code by using method signatures and standard test naming conventions. The unittest frameworks run all the tests and report failures, significantly speeding up the process.\n\nCode analysis and refactoring tools identify issues with code and can improve the overall quality. SonarQube is an example of a code analysis tool that performs static code analysis. It identifies potential problems with code, often referred to as code smell, but can also check for a range of potential issues, such as deviations in code style and poor security handling.\n\nSome more advanced tools have been able to not only recognize coding problems but also fix them. For example, ReSharper actually refactors the code to improve its quality. Such tools save developers time and achieve this result through a combination of pattern matching and AI.\n\n5",
      "content_length": 2687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "6\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nWith continual changes in coding sources and packages, software development always seems to require new packages, platforms, or even languages. As a result, software developers require access to the latest manuals or other documentation. Some refer to searching for code examples in Stack Overflow or Reddit. Innovations in this space included Kite, AI-powered software that provided automated code completion and instant code documentation. Kite proved to reduce keyboard clicks and improved code development speed, gaining a user base of an estimated 500,000 pro- grammers. Unfortunately, the company ceased to exist in 2021 and donated its multi-language code tools to the open source community.\n\nNext, we will introduce the turning point in AI research that has driven significant adoption across a variety of domains, including software engineering.\n\nIntroducing the rise of LLMs Over the past few short years, LLMs have emerged as the dominant AI resource for writing, re- search, and inference. They are currently transforming the tech industry, and their applications have a far-reaching impact across all fields. This section provides a brief overview of their un- precedented ascent.\n\nArtificial Intelligence was formally started in 1956 at a famous Dartmouth College workshop of computer science experts. They coined the term artificial intelligence (AI) and set ambitious goals ranging from automated reasoning to natural language processing (NLP). Although the participants expected a rapid progression to these goals, the compute and technology limitations thwarted their success. A publication in 1969 denounced the key technology and allegedly started the first well-documented AI winter, an extended period of no funding or research.\n\nIn the 1980s, expert systems emerged as a workable solution where rules could be crafted by technologists to reproduce human-like reasoning over limited domains for a specific problem. Despite some early successes with the approach, it proved difficult to craft and manage the or- dering for sets of rules. This hindered its adoption and eventually led to the second AI winter.\n\nMachine learning (ML), a sub-field of AI, emerged as the only viable solution. Unlike the hand-crafted rules of expert systems, ML systems could learn to make predictions or decisions directly from data. Research has led to dozens of techniques within the sub-field, but neural networks have become the dominant approach over the past dozen years. Mildly inspired by biological neurons, neural networks have proven to be a powerful system for learning and mod- eling data. Researchers have shown that neural networks can generalize well and approximate",
      "content_length": 2778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Chapter 1\n\nany function. Deep learning, any neural network with multiple layers of neurons, overcomes the limitations of more traditional machine learning techniques. Specifically, it can continue to learn when provided with ever larger training sets.\n\nNLP is the application of machine learning to human language data. It applies to any texts, such as articles, blogs, emails, or books. The field draws from computer science, AI, and linguistics. Earlier methods drew extensively from statistical methods and later traditional ML techniques. In recent years, deep learning methods have revolutionized the NLP field by introducing lan- guage models (LMs), which predict and generate text based on existing language data. LLMs are expanded versions of LMs, trained on massive datasets and billions of parameters, which are internal weights tuned to reflect the patterns in the training data. We will discuss LLMs exten- sively in later chapters of the book.\n\nOver the past several years, deep learning models have been trained on ever-increasing volumes of text and, with new techniques, can understand how words within each sentence are related to each other. This class of LLMs includes OpenAI’s GPT, Meta’s Llama, Google’s Gemini, Anthrop- ic’s Claude, and newer models continue to be developed. These LLMs were initially designed to accurately predict the next word of a phrase. At scale and with recent technologies, they have enabled natural language generation (NLG) solutions that can write full texts to enable report writing, question-answering, chatbots, and much more.\n\nLLMs are typically trained on large sets of available online text sources, but the same models can also be trained on software code. These LLMs use publicly available code in Python, Java, and other programming languages that are mostly available from GitHub repositories. The result is that the LLMs can predict the next block of code, can generate comments, write tests, and even refactor code. These are all parts of the overall SDLC that we will describe in the next section.\n\nExploring the software development lifecycle To deliver quality software, most software teams progress through a series of stages known as the software development lifecycle (SDLC). As shown in Figure 1.1, these steps are designed to be an efficient approach that minimizes the risk of failure. The process usually begins with the recognition of an unmet business need, and cycles through many stages to meet the need with a software system. Projects progress from analyzing the existing state to gathering requirements, designing the system, implementing and testing the code, delivering the solution, and often maintaining the software.\n\n7",
      "content_length": 2703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "8\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nWhile most people associate software development with coding, actual programming makes up only 25-35% of the overall effort, depending on the type of software and its requirements. The remaining steps are needed to gather requirements, test and document the code, deploy the software, and support its continued functionality, as shown in Figure 1.1.\n\nFigure 1.1: The SDLC – the continual process of developing or improving software systems from requirements through maintenance\n\nThe SDLC process begins with gathering requirements, followed by planning, feasibility, and risk analysis. A successful analysis leads to the creation of a high-level system design, and only after this step does an engineer continue on to software coding. The form will be formally tested before it is deployed, resulting in a live or production system. As the environment or business needs change, support and maintenance are always needed, and that can trigger the next development cycle.",
      "content_length": 1064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Chapter 1\n\nImportant note\n\nWhile the SDLC is an industry-standard approach, individual organizations often introduce variations to tailor it for their software development processes. For in- stance, some organizations may choose to implement tests before writing the code, a practice known as test-driven development (TDD). Others may create a prototype system or introduce a proof of concept (POC) before conducting a feasibility analysis, a step that has become easier to perform with the help of LLMs.\n\nThere are an increasing number of books and videos that describe the use of GenAI for coding, but the technology can supercharge the entire process, not just the actual coding implementation. This book will explore several of these aspects, including testing, documenting, and monitoring software. These are critical for the success of software projects.\n\nNext, we will see how we can embrace a comprehensive GenAI toolkit in our technological stack as software developers.\n\nEmbracing a GenAI toolkit This book focuses on three separate tools for software development: ChatGPT, OpenAI API, and GitHub Copilot. In 2024, these three tools had roughly a $35 million combined market size for software engineering applications. The market is expected to grow 25% per year throughout the rest of the decade, according to a Research and Markets report. The following chapters of the book will provide instructions on how to subscribe to these services and how to get started. These tools provide distinct kinds of functionality, and knowing when to use which tool is part of the learning curve. Later chapters will highlight the features and use cases for each of the tools.\n\nChatGPT OpenAI has been a leader in LLMs since 2015. Led by CEO Sam Altman, the company has produced multiple versions of its Generative Pretrained Transformer (GPT) LLM. While these were well received, the release of ChatGPT in December 2022 transformed the perspectives of AI worldwide.\n\nChatGPT is an AI-driven chatbot, an application that is designed for text conversations using natural language. Its release spurred widespread use, reaching 100 million users the following month. It continues to be one of the most visited websites across the world.\n\n9",
      "content_length": 2233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "10\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nWhile natural language conversations with ChatGPT often succeed in eliciting answers to ques- tions, prompt engineering has proven a more robust technique. It is the art of crafting an instruc- tion to produce a more desirable output. The prompt typically consists of context, instructions, a history of the dialog, and sometimes examples of desired output. This book will provide structured formats that guide the reader to effectively perform prompt engineering for producing code, comments, tests, and other outputs.\n\nOpenAI API ChatGPT is among the most popular tools for interacting with LLMs. However, in many cases, prompt engineering lacks the simple structures found in software, such as loops and conditions. OpenAI provides a developer platform for coding directly against the same OpenAI LLM used by ChatGPT. Through its Application Programming Interface (API), OpenAI enables developers to combine software and prompt engineering. The API also provides specific added functionality that is useful for solving software engineering problems.\n\nGitHub Copilot While GitHub is one of the most popular platforms for sharing code using Git distributed version control, the company released GitHub Copilot in 2021. Originally powered by OpenAI’s LLM, it provides intelligent code completion using GenAI’s programming capability. The functionality has been integrated into many IDEs, including Visual Studio Code and PyCharm—two of the most popular IDEs for Python.\n\nUnlike the other OpenAI models, Copilot functions as a pair programmer. This concept comes from the Extreme Programming (XP) agile methodology, where two developers work together to write code with a single keyboard. Although not yet a fully functioning pair programmer, Copilot can quickly find and display references for code syntax and even provide annotated examples or full code as requested by the user. It interprets the intention from the function and variable names used. Together with the surrounding code as context, it can predict and suggest the next block of code.\n\nNext, we will review recent studies that assess the use of GenAI for software development.",
      "content_length": 2237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Chapter 1\n\nImproving software development with GenAI A number of studies have assessed whether GenAI provides increased productivity in coding tasks. McKinsey reported increases ranging from minimal to 50%, depending on the complexity of the task. For code documentation and generation, the gains were much higher than for difficult tasks. They found it was particularly good for routine tasks and repetitive work, as well as initial dives into new code projects. Refactoring code to make changes and tackling new challenges were also improved through GenAI technology. Perhaps as important, their study showed that users of GenAI for software felt happier, were able to focus more on meaningful work, and achieved flow much more frequently. The study details can be found at https://www.mckinsey.com/capabilities/ mckinsey-digital/our-insights/unleashing-developer-productivity-with-generative-ai.\n\nA similar study by Exadel reported that half of the developers in their study used GitHub Copilot at least 50% of the time. Two-thirds of these developers completed tasks more quickly, saving 10-30% of their development time. Copilot made them more productive and fulfilled. See https://exadel. com/news/measuring-generative-ai-software-development/ for more details on the study.\n\nResearch by Colombatto and Rivadulla (https://aws.amazon.com/blogs/apn/transforming- the-software-development-lifecycle-sdlc-with-generative-ai/) found benefits of applying GenAI across the full SDLC. Examining data from AWS and IBM, they found that the benefits begin in the analysis phase with requirements engineering. Even in this early phase of the SDLC, the researchers observed up to a 60% reduction in time from using GenAI. They found a 30% reduction in development time and a 25% reduction in time for generating unit tests and test plans. Even though less time was spent, the code quality improved by 25%, which contributes to fewer bugs and lower software maintenance costs.\n\nA study conducted by BlueOptima from 2022 to 2024 used code repositories to analyze produc- tivity, quality, and cost across 77,338 developers. In contrast with the other studies that reported significant savings, the findings were much more modest. They found only a 3.99% increase in productivity for those with access to GenAI and a 5.12% decrease for those without. Quality still improved slightly, which is important since it proves that the productivity gains do not compro- mise quality, but the gains were not as significant. However, the study used access to these tools as an input variable without characterizing the training, familiarity, or integration of GenAI into their workflows. In addition, productivity is likely to increase as the predictive accuracy and overall performance of GenAI tools continue to rapidly improve. The details of the study can be found through this link: https://www.blueoptima.com/resource/llm-paper-1/.\n\n11",
      "content_length": 2920,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "12\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nNext, we will discuss our perspective on the benefits and downsides of using GenAI in software development.\n\nIs GenAI worth learning for software engineering? We have been using code completion tools for over a decade, but current GenAI tools are different. We have used the full range of tools, such as keyboard shortcuts, Stack Overflow searches for help, code API search tools, and all the latest refactoring tools and templates available in the IDE. All these strategies have helped us be more efficient in our work, but there has always been a lot of mundane, repetitive work that has limited our coding speed and enjoyment.\n\nThe benefits of coding with GenAI GenAI tools have transformed our output. Within three months of using an earlier version of GitHub Copilot, we were writing code 15% faster. Now, after two years, the combination of GitHub Copilot, ChatGPT, and OpenAI API has supercharged our coding output more than anything else that we have used. We complete twice as much work as we did previously with multiple tools. The improvements in productivity were a combination of advances in the tools themselves as well as familiarity with how to use them, both of which are covered throughout this book.\n\nBeyond the productivity in merely writing code, GenAI contributes to other aspects of software development. GenAI can help refactor code automatically, which helps make it more readable and hence maintainable. As shown in later chapters, code can also be improved by selecting better algorithms that execute faster. GenAI can also help write documentation of code and automate the creation of tests. With GitHub Copilot, the pair programming approach to efficient coding includes providing help, which is useful for senior developers, but invaluable to developers learn- ing a new language or framework.\n\nThe downsides of coding with GenAI The technology behind GenAI for software development is still quite new. Early studies from 2022 showed that GitHub Copilot’s accuracy in producing correct code was below 50%. While advances and new versions of the underlying models continue to be released every few months, they are certainly not perfect.\n\nIn fact, GenAI has produced some of the worst fatal development mistakes we have ever seen. To put that in perspective, we have seen a data scientist pushing their entire environment file to the corporate repository, which exposed secret tokens that had to be replaced. One software developer crashed a microservice after renaming a file pandas. One data engineer spent two",
      "content_length": 2637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Chapter 1\n\nweeks learning Cython to handle a Python DataFrame memory issue instead of just switching to Dask or PySpark. GenAI may not only supercharge your strengths, but may also supercharge your weaknesses. After all, it is still a developing technology, but continues to improve arguably faster than anyone expected.\n\nAI coding has made the headlines, but it may not be clear why it would fail. The underlying coding models are trained on available GitHub repository data and other code that is publicly available in various languages. For problems that are widely documented, such as the Fibonacci sequence calculation or the many code snippets used to pass LeetCode interviewing questions, the answers are nearly perfect. For this reason, YouTube is full of videos showing how GitHub Copilot can program a React web page in 3 minutes.\n\nGenAI has far more difficulty solving more obscure coding tasks where there is far less training data. Even if the most famous LeetCode’s Two Sum Problem were changed slightly to include Python Threads, for example, the solution would be unpredictable.\n\nA well-documented problem with LLMs for generating text is that they tend to hallucinate or fabricate information when the answers are not apparent. Significant research is ongoing to counter this poorly understood problem. However, hallucinations and other LLM issues do occur when GenAI is applied to software engineering.\n\nSome developers worry that GenAI coding tools will turn them into less capable developers. They fear that relying on automatic code completion, suggestions, and examples will cause them to lose their programming edge or familiarity with the functions.\n\nRecent research by Michael Gerlick (https://doi.org/10.3390/soc15010006) suggests that AI tools might decrease our critical thinking capability through a process known as cognitive off- loading. However, similar arguments have been made about automated spelling checkers that produce better documents but perhaps reinforce our spelling crimes. It is true that it may take a bit longer to remember the exact syntax of adding tick marks to a matplotlib plot when the internet is down. However, if you can double your output with fewer keystrokes, you can focus on the more important problems that GenAI has yet to solve.\n\nTakeaways Recent blogs describe a new trend called vibe coding, where developers and even non-developers design and build full applications extensively using GenAI over a weekend that would probably take months. It is remarkable that the technology has advanced to the point where rapid proto- typing is effective. However, prototypes are not production code.\n\n13",
      "content_length": 2658,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "14\n\nFrom Automation to Full Software Development Life Cycle: The Current Opportunity for GenAI\n\nIn many tutorials where GenAI fails, the common wisdom is You should verify the output you get, yet none offers a pragmatic way or even a guided mindset of how to effectively evaluate the outputs and improve the code.\n\nIt is considered good practice to apply unit and other testing approaches for all code. However, using GenAI is neither about blindly trusting nor fact-checking everything. GenAI failures do not mean we have to go overboard with fact-checking any piece of code it produces. Similarly, evi- dence of GenAI success does not imply you should push every memory optimization suggestion into production.\n\nLeveraging GenAI is about developing a new set of skills to formalize the inputs and outputs obtained from LLMs. This will enable you to truly supercharge your coding tasks throughout the SDLC. It enables you to own the code whether you wrote it from scratch yourself or utilized LLMs. When you can assess the quality and risk of the output these tools generate, you will be able to transform your approach to software engineering.\n\nSummary This chapter highlighted that GenAI for coding emerged from the combination of software tool advancements with LLMs. This nascent technology applies not only to coding but can enhance many aspects of the SDLC. The combination of ChatGPT, OpenAI API, and GitHub Copilot pro- vides a complementary set of tools that have been shown to not only improve productivity and enhance code quality but can even bring happiness to programmers.\n\nAlthough the technology is new and still evolving, GenAI is already changing the software engi- neering field. This book was developed to provide a structured approach to effectively leverage the tools and achieve the best results across many aspects of the SDLC.\n\nIn the next chapter, we will introduce a quick-start guide to OpenAI API and use the chat service for coding tasks. We will build our own code completion program that takes a function’s signature as input and returns its implementation as output.",
      "content_length": 2100,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Chapter 1\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nVS Code Plugin: https://github.com/kiteco/vscode-plugin\n\n\n\nBegum Karaci Deniz, Chandra Gnanasambandam, Martin Harrysson, Alharith Hussin, Shivam Srivastava. Unleashing developer productivity with Generative AI: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ unleashing-developer-productivity-with-generative-ai.\n\n\n\nAlexey Girzhadovich. Scientifically Measuring the True Impact of Generative AI Software Development: https://exadel.com/news/measuring-generative-ai-software- development/.\n\nDiego Colombatto and Jose Manual Pose Rivadulla. Transforming the software development lifecycle (SDLC) with Generative AII: https://aws.amazon.com/blogs/apn/transforming- the-software-development-lifecycle-sdlc-with-generative-ai/\n\n\n\nResearch and Markets Report: Generative Artificial Intelligence (AI) in Coding Market - Forecasts from 2024 to 2029: https://www.researchandmarkets.com/ reports/6014321/generative-artificial-intelligence-ai-in?utm_source=GNE&utm_ medium=PressRelease&utm_code=8xz7cm&utm_campaign=2014387+-+Generative+Arti ficial+Intelligence+(AI)+in+Coding+Market+Research+2024-2029%2c+Profiles+o f+Codecademy%2c+CodiumAI%2c+Google%2c+IBM%2c+Microsoft%2c+NVIDIA%2c+OpenAI %2c+and+Tabnine&utm_exec=chdomspi\n\nMichael Gerlick. AI Tools in Society: Impacts on Cognitive Offloading and the Future of Critical\n\nThinking: https://doi.org/10.3390/soc15010006\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n15",
      "content_length": 1704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "2\n\nYour Quickstart Guide to OpenAI API\n\nGenerative artificial intelligence (GenAI) platforms such as OpenAI are transforming entire industries by empowering everyday users to generate custom answers to questions, reports, and images. You do not have to be an AI expert to leverage OpenAI to supercharge your software de- velopment. This chapter focuses on setting up OpenAI and using its application programming interface (API) calls to generate quality code.\n\nOpenAI offers numerous services such as chat, image generation, and text-to-speech. These are available through a RESTful HTTP request, which is a standard way for applications to commu- nicate over the web, or through a programmatic interface of the openai Python package.\n\nIn our first quickstart guide of the book, we will walk through three interactive labs and three sets of requests, working with the OpenAI Chat service. We will dive into the different models available through OpenAI API, and how to impact the AI-generated output.\n\nWe will obtain our project API keys, analyze the services’ limits, and learn how to compute each request’s costs. The costs are computed from the number of tokens or subword fragments of the model’s input and output.\n\nWe will wrap up this chapter by creating a basic code completion program that can generate a Python code implementation based on a function signature.",
      "content_length": 1370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "18\n\nYour Quickstart Guide to OpenAI API\n\nIn this chapter, we will explore the following topics:\n\n\n\nIntroducing OpenAI API\n\nObtaining your project API keys from OpenAI\n\n\n\nSending your first OpenAI API request using the openai Python package\n\n\n\nLearning how request costs are calculated\n\nUnderstanding rate limits and usage restrictions for free and paid OpenAI accounts\n\n\n\nAnalyzing request parameters\n\nUsing OpenAI API to generate code\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\n\n\nYour personal OpenAI account credentials\n\n\n\nAccess to the book’s GitHub repository, which is available at https://github.com/\n\nPacktPublishing/Supercharged-Coding-with-Gen-AI\n\n\n\nA virtual environment set up in your integrated development environment (IDE), either VS Code or PyCharm\n\nIf you require additional assistance in setting up your OpenAI account or a virtual environment in your IDE, refer to the Appendix for detailed instructions.\n\nIntroducing OpenAI API The OpenAI platform offers a variety of natural language processing (NLP) services, powering many AI applications. At its core, the platform leverages large language models (LLMs) that are trained on a massive amount of data, with a complex architecture and billions of learnable param- eters to produce AI-generated responses. Developers can interact with the platform through its API with text-based requests that are often called prompts. The prompts are processed, enriched, and transformed before being passed into the LLM model. The model’s output is further refined and delivered as a response to the API request.\n\nThe Chat service by OpenAI is designed for conversational interactions with the LLM where the conversation uses three types of prompt roles – user, system, and assistant:\n\nUser prompts represent an end user of the system such as a ChatGPT user, and resemble prompts we feed into ChatGPT, such as Tell me about the Fibonacci sequence.",
      "content_length": 1952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Chapter 2\n\n\n\nAssistant prompts represent the model output through the conversation and resemble answers we will get back from ChatGPT, such as The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones.\n\n\n\nSystem prompts define the guidelines of the assistant behavior through the conversation, such as You are a helpful programming instructor.\n\nLike many other GenAI tools in the industry, the OpenAI API calls are primarily a paid service. It does offer some limited free usage such as three requests per minute to a restricted selection of models, excluding GPT-4o. With just $5 in OpenAI credits, you can fully engage with the examples and labs presented in this book. We will instruct you how to do this right after our initial Lab 2.1.\n\nOpenAI API through a RESTful HTTP request The OpenAI API services are accessed via RESTful HTTP requests, a communication protocol based on the principles of Representational State Transfer (REST) architecture, and using the HyperText Transfer Protocol (HTTP). RESTful HTTP provides a standardized way to send and retrieve data from servers over the internet. A request to OpenAI API includes four key components:\n\n\n\nEndpoint: The URL specifying the service you are accessing, such as https://api.openai. com/v1/chat/completions for the Chat service\n\nHTTP method: The action to perform, such as POST for OpenAI Chat service requests,\n\nwhich sends data to the server\n\nHeaders: Metadata about the request, including your API token, to authenticate and\n\nprovide context for the request\n\n\n\nBody: The data payload containing details such as the LLM to use and the input prompts\n\nHere is an example of a request to the OpenAI API Chat service using the requests package in Python. It demonstrates how to make a POST request to the service, including a header with your API key for authentication and a body containing the data payload, such as the model and a prompt:\n\nimport requests\n\nimport json\n\napi_key = \"your-key-here\"\n\nheaders = {\n\n\"Authorization\": f\"Bearer {api_key}\",\n\n\"Content-Type\": \"application/json\"\n\n}\n\npayload = {\n\n\"model\": \"your model's name here\",\n\n19",
      "content_length": 2147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "20\n\nYour Quickstart Guide to OpenAI API\n\n\"messages\": \"your message here\"}\n\nresponse = requests.post(\"https://api.openai.com/v1/chat/completions\",\n\nheaders=headers,\n\ndata=json.dumps(payload))\n\nchat_response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n\nQuick tip: Enhance your coding experience with the AI Code Explainer and Quick Copy features. Open this book in the next-gen Packt Reader. Click the Copy button\n\n(1) to quickly copy code into your coding environment, or click the Explain button\n\n(2) to get the AI assistant to explain a block of code to you.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR visit packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nNote that this code sample can also be found at the book’s repository path, ch2/code_samples/ openai_request.py.\n\nIn this example, the payload is submitted in JSON format. If the request is successfully accepted and the server responds, the OpenAI API Chat service’s output can be retrieved from response. json()[\"choices\"][0][\"message\"][\"content\"].",
      "content_length": 1165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Chapter 2\n\nFor Python developers, a more efficient approach to accessing the OpenAI Chat service is available. Rather than dealing with the complexities of crafting and sending RESTful HTTP requests, such as constructing the correct endpoint URL, setting authentication tokens in headers, formatting payloads in JSON, and handling potential response errors, we can leverage the dedicated openai package.\n\nOpenAI API Python package installation The openai Python package simplifies the process by abstracting complexities such as URL paths, retries, error handling, and authentication. This allows us to interact directly with Python objects, instead of manually constructing RESTful HTTP requests.\n\nTo install the openai package, run the following command in your terminal within your virtual environment:\n\n(.venv) $ pip install openai\n\nTo verify that the openai package is installed correctly on your virtual environment, run the following command, which specifies the information of the package:\n\n(.venv) $ pip show openai\n\nYour console will output information about the package version, author, and support email.\n\nWith the openai package installed, we can simplify our interactions with the OpenAI platform using a programmatic framework. Instead of making direct RESTful HTTP requests, we will uti- lize Python classes. This results in fewer boilerplates, such as handling authentication and error management, and a more maintainable code.\n\nImportant note\n\nIf you are already working with the book repository, you do not need to install the openai package separately. It is already specified in the requirements.txt file and should be installed in your virtual environment. If you require additional assistance in setting up your virtual environments, refer to the Appendix for detailed instructions.\n\n21",
      "content_length": 1809,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "22\n\nYour Quickstart Guide to OpenAI API\n\nObtaining your project API keys from OpenAI All requests to OpenAI API require an authentication token, and we must obtain one prior to submitting any request.\n\nNavigate to https://platform.openai.com and log in with the same credentials as you use for ChatGPT at https://chat.openai.com/.\n\nOnce logged in, click on Dashboard in the upper-right menu, then select API keys from the left menu.\n\nRecently, OpenAI replaced personal API keys with the option to create a project API key. This new key serves two purposes. First, it allows personal use, as required for the work throughout this book. Second, it supports service API keys intended for non-human identities such as microser- vices running in production.\n\nFigure 2.1: Project API keys dashboard\n\nTo generate your first API key, click on Create new secret key under your account icon at the upper left. Choose an API key that is owned by You, as we will use this key rather than assign it to a service account. You can assign a name for future reference, and it should be associated with a project (named default project if you have not changed the project name yet).",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Chapter 2\n\nFigure 2.2: Creating a new secret key for personal use\n\nOnce you click on Create secret key, the platform will generate a new secret key for you.\n\nReturn to your IDE, either VS Code or PyCharm, and export it as an environment variable from your terminal by running the following:\n\n$ export OPENAI_API_KEY=\"your-key-here\"\n\nAlternatively, for a reproducible environment, you can use the .env file template provided with this book’s repository:\n\nFigure 2.3: The .env file template, placed at the root of the repository\n\n23",
      "content_length": 530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "24\n\nYour Quickstart Guide to OpenAI API\n\nIf you choose to work with .env for your environment variables management, you should verify that your running configurations are set to work with this file. Refer to the Appendix if you need additional assistance.\n\nNext, we will learn how to submit our first request to OpenAI API using the openai Python pack- age, following a template that will be utilized throughout this book.\n\nLab 2.1 – sending your first OpenAI API request using the openai Python package With the installed Python package and the API Keys, we are all set to execute our first OpenAI API request. Note that you do not need OpenAI platform credits to complete this lab, and we will instruct you how to load credits to your account later in this chapter. If you have previously loaded credits to the OpenAI platform, this example should cost no more than a fraction of a cent to execute.\n\nIn this lab, we will start with a simple template for executing OpenAI API requests to the Chat service and printing the response output. We will have three steps:\n\n1. Create an openai.OpenAI class instance.\n\n2. Submit a request to the Chat service with a single user prompt and a model selection such\n\nas GPT-4o-mini or GPT-3.5-turbo.\n\n3. Print the response output.\n\nThese steps are in place in our starter code, and we will walk you through completing it, running it, and analyzing the results we get:\n\nimport openai\n\nfrom openai import OpenAI\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"\", # Your model's name here\n\nmessages=[] # Your messages here\n\n))\n\nprint(\"your print here\")\n\nIn this lab walk-through, we will build on this starter code to complete the model and messages ar- guments, enabling us to receive an AI-generated response to our request from the OpenAI platform.",
      "content_length": 1873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Chapter 2\n\nLab 2.1 guided walk-through Open Lab 2.1 in the book’s repository located at ch2/labs/lab21.py. First, we will instantiate a new instance of the openai.OpenAI class:\n\nclient: OpenAI = OpenAI()\n\nWe do not specify any authentication parameters directly in the code. The OpenAI client automat- ically retrieves the API key from the OPENAI_API_KEY environment variable. While it is possible to pass the API key using the api_key argument during instantiation, we strongly advise against hardcoding credentials in scripts for security reasons.\n\nThe next step is to send a request using the OpenAI client. We will initiate a call to the Chat ser- vice by accessing chat.completion, where we will define the necessary RESTful HTTP request parameters. This includes at least two parts: the model and messages keyword arguments. The model argument represents the name of the LLM you are querying, and the messages argument represents the input prompts sent to the model:\n\ncompletion = client.chat.completions.create(\n\nmodel=\"\", # Your model's name here\n\nmessages=[] # Your messages here\n\n)\n\nIf you have loaded credits onto your OpenAI platform account, enter the model value between the double quotes:\n\ngpt-4o-mini\n\nOtherwise, use the following model instead:\n\ngpt-3.5-turbo\n\nNext, add a single user prompt to the messages argument:\n\nmessages = [{\"role\": \"user\", \"content\": \"What is the FizzBuzz problem?\"}]\n\nThe last step is to print the response output of the Chat service. On the last line, replace the print statement with the following:\n\nprint(completion.choices[0].message.content)\n\n25",
      "content_length": 1593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "26\n\nYour Quickstart Guide to OpenAI API\n\nAt this point, your Lab 2.1 code should look as follows:\n\nimport openai\n\nfrom openai import OpenAI\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[{\"role\": \"user\", \"content\": \"What is the FizzBuzz problem?\"}]\n\n)\n\nprint(completion.choices[0].message.content)\n\nLab 2.1 is now ready to run and will print the Chat completion in response to the following user prompt: What is the FizzBuzz problem?.\n\nNote\n\nWhen working on your lab code, you can always find the full lab solutions in the ch2/solutions folder.\n\nLab 2.1 analysis Run the script and check the output in the terminal if you are using VS Code, or the run console if you are using PyCharm. The output of the program should be the Chat service’s response to the user prompt What is the FizzBuzz problem?, a problem we will revisit in later chapters.\n\nDepending on the model you chose to work with (either GPT-3.5-turbo or the more advanced GPT- 4o-mini), you might get quite a different output from the Chat service. For your reference, here is a response from the GPT-3.5-turbo model, which includes a context and problem description:\n\nFizzBuzz is a common programming problem often used in job interviews to test a candidate's ability to write simple code. The problem is as follows:\n\nWrite a program that prints the numbers from 1 to 100. But for multiples of three, print \"Fizz\" instead of the number, and for the multiples of five, print \"Buzz For numbers which are multiples of both three and five, print \"FizzBuzz\"",
      "content_length": 1626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Chapter 2\n\nHere is a response from the GPT-4o-mini model, which, in addition to the context and descrip- tion, included a detailed explanation of the coding task stages, an example output, and the task’s challenges:\n\nThe FizzBuzz problem is a classic programming task often used in coding interviews and educational settings to test a developer's basic understanding of control structures, loops, and conditional statements. The problem is simple:\n\n1. Write a program that prints the numbers from 1 to a specified number, typically 100.\n\n2. For multiples of three, print \"Fizz\" instead of the number.\n\n3. For multiples of five, print \"Buzz\" instead of the number.\n\n4. For numbers that are multiples of both three and five, print \"FizzBuzz.\" The output would look something like this:\n\n```\n\n1, 2, Fizz, 4, Buzz, Fizz, 7, 8, Fizz, Buzz, 11, Fizz, 13, 14, FizzBuzz, ...\n\n```\n\nThis task helps demonstrate an understanding of loops and conditionals, often revealing both logical reasoning and code clarity.\n\nIn Chapter 6, we will explore the underlying differences between GPT-4o and GPT-3.5 in greater detail.\n\nImportant note\n\nAs discussed in Chapter 1, applications built on top of LLMs rarely output determinis- tic results, and responses for similar prompts may differ due to prompt construction, user customization, and randomness. Moving forward in this book, we will discuss further how to control all three to achieve the desired outputs from the model.\n\nNext, we will explore how to anticipate the costs associated with each API call and understand how tokens, as fragments of words, are used to calculate the request charges.\n\n27",
      "content_length": 1634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "28\n\nYour Quickstart Guide to OpenAI API\n\nLearning how request costs are calculated Since OpenAI API is primarily a paid service, it is essential to understand and anticipate the costs associated with each request.\n\nBefore proceeding with the lab exercises, it is strongly recommended that you add credits to your OpenAI platform account. An initial charge of $5 in credits should be more than enough to complete all the labs in this book. Loading credits will also upgrade your account from Free tier usage (with no prior credits) to a Tier 1 account, significantly increasing your rate and usage limits, as we will discuss later in this chapter.\n\nTo load credits, open the OpenAI Billing Overview page at https://platform.openai.com/ settings/organization/billing/overview and add a payment method with a minimum charge of $5.\n\nFigure 2.4: Remaining credit balance\n\nYou can also check your remaining balance at any time by visiting the same link.\n\nUnderstanding tokens The OpenAI platform calculates request costs based on tokens, which are small segments of text (often subwords, words, or characters) averaging about 0.75 words or 4 characters each. Text is divided into tokens by elements such as punctuation, root words, and word endings. We will discuss tokens in detail in Chapter 6.",
      "content_length": 1290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Chapter 2\n\nEach API request consists of both input and output tokens, and OpenAI charges for both. For example, in Lab 2.1, the message {\"role\": \"user\", \"content\": \"What is the FizzBuzz problem?\"} contains 15 tokens. You can access the exact count from the prompt_token attribute as in the following code:\n\n>> completion.usage.prompt_tokens\n\n>> 15\n\nAlternatively, we can estimate our total input token consumption using tools such as the Ope- nAI Tokenizer, available at https://platform.openai.com/tokenizer. The Tokenizer also col- or-codes the different tokens in the text, identifying words such as role and user as a single token, punctuation marks such as commas and question marks as a single token each, and FizzBuzz as three tokens.\n\nFigure 2.5: OpenAI Tokenizer simulation\n\nOutput tokens are also billed. For example, in Lab 2.1, the output response from the GPT-4o- mini model contains 292 tokens. You can access the completion_tokens attribute by calling the following:\n\n>> completion.usage.completion_tokens\n\n>> 292\n\nThis brings the total billable tokens to 307. You can access the total_tokens attributes by calling the following:\n\n>> completion.usage.total_tokens\n\n>> 307\n\n29",
      "content_length": 1190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "30\n\nYour Quickstart Guide to OpenAI API\n\nHow billing is computed OpenAI charges for both input and output tokens, with costs varying by model. Newer, more advanced models typically have higher costs, while optimized versions, such as models with the -mini or -turbo postfix, are more affordable. You can find the full pricing details at https:// openai.com/api/pricing/.\n\nAs of January 2025, input tokens for the GPT-4o model are priced at $2.50 per million tokens (or $1 for every 400,000 input tokens), while output tokens are priced at $10 per million tokens (or $1 for every 100,000 output tokens).\n\nIn Lab 2.1, we used the more affordable GPT-4o-mini model. Input tokens for this model cost $0.15 per million tokens (or $1 for every 6.6 million input tokens), and output tokens are priced at $0.60 per million tokens (or $1 for every 1.6 million output tokens). These costs are approximately 1/16 of the GPT-4o model’s pricing.\n\nFigure 2.6: Costs of GPT-4o input and output tokens\n\nFor example, we can calculate the cost of running Lab 2.1, which uses 15 input tokens and 292 output tokens. Based on the pricing for the GPT-4o-mini model, the cost is as follows:\n\n>> (15 * 0.15 + 292 * 0.6) / 1000000\n\n>> 0.00018\n\nThis is approximately 1/55 of a cent.",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Chapter 2\n\nIf we run Lab 2.1 with the larger GPT-4o model instead, the cost would be as follows:\n\n>> (15 * 2.5 + 292 * 10) / 1000000\n\n>> 0.002\n\nThis amounts to 1/5 of a cent.\n\nKeep these cost differences in mind when designing your own applications, especially when de- ciding between using more advanced models and their smaller, more affordable versions. Model prices have historically declined as newer models are introduced, so it is a good idea to periodically review the Pricing page at https://openai.com/api/pricing/.\n\nWith a clear understanding of cost calculations, we can now move on to discussing the limitations and restrictions associated with different tiers of OpenAI accounts.\n\nUnderstanding rate limits and usage restrictions for free and paid OpenAI accounts OpenAI usage is limited by three factors: requests per minute (RPM), requests per day (RPD), and tokens per minute (TPM). The limits of each vary based on both the model and the type of account you have. For Free tier accounts with no credits added, the limit is three RPM, and only certain models are available, excluding GPT-4o. Token limits tend to be more restrictive for advanced models such as GPT-4o to all account tiers, whereas older or mini models, such as GPT-4o-mini, have more relaxed limits.\n\nFor reference, here are the rate limits for Free tier accounts as of January 2025:\n\nFigure 2.7: Rate limits example for Free tier accounts\n\n31",
      "content_length": 1427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "32\n\nYour Quickstart Guide to OpenAI API\n\nAnd here are the rate limits for Tier 1 accounts as of January 2025:\n\nFigure 2.8: Rate limits example for Tier 1 accounts\n\nWe encourage you to learn about your account’s API limits by navigating to the Settings sec- tion in the upper-left corner of your organization: https://platform.openai.com/settings/ organization/limits.\n\nWith Tier 1’s increased RPM and TPM limits in place, we’re now ready to move forward to the next lab, where we will explore additional parameters for OpenAI API.\n\nLab 2.2 – analyzing request parameters In Lab 2.2, we will expand on our basic OpenAI API request to the Chat service by adding addi- tional parameters to customize the model’s response. This time, we will ask the Chat service to explain the Two Sum problem, a topic we will explore further in later chapters.\n\nWe will use a similar starter code from Lab 2.1, which includes setting up an OpenAI client and print statements:\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create())\n\nprint(\"Completion Tokens: \")\n\nprint(\"Output: \")",
      "content_length": 1106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Chapter 2\n\nIn this lab’s guided walk-through, we will add the following payload parameters:\n\nn: Specifies the number of response variations\n\ntemperature: Controls the level of randomness in the model’s output ranging from 0 to 2\n\nmax_tokens: Limits the number of tokens in each response\n\nApplications such as code completion services often offer users multiple suggestions from a lan- guage model, so we may want to retrieve more than one response. Additionally, in some cases, such as generating a creative prompt (for example, Give me a marketing slogan for a book about GenAI for coding), we might prefer a higher degree of randomness in the output that can be obtained by setting the temperature parameter closer to its maximum value of 2.\n\nThe max_tokens parameter limits the number of tokens in the response, though it does not guar- antee the output will be fully complete within that limit. When using max_tokens, OpenAI charges for the specified token limit rather than the actual number of tokens used, so it is best to set it close to the expected length of the response.\n\nLab 2.2 guided walk-through This walk-through consists of two steps: updating the payload with additional parameters and adding a print statement to display the extra responses.\n\nOpen Lab 2.2 in the book repository at ch2/labs/lab22.py and ensure that you have either set the OPENAI_API_KEY environment variable or configured your run settings to point to the .env file.\n\nAs the first step, update the arguments for the .create method on line 7 as follows:\n\n1.\n\nSet the model to gpt-4o-mini:\n\nmodel = \"gpt-4o-mini\"\n\nIf you have not added credits to your OpenAI platform account yet, your access may be limited to certain models, excluding gpt-4o-mini. In that case, use the gpt-3.5-turbo model instead.\n\n2. Set a low temperature argument to reduce the randomness of the output:\n\ntemperature=0.2\n\n3. Set the max_tokens argument to restrict the model’s output to a maximum of 100 tokens:\n\nmax_tokens=100\n\n33",
      "content_length": 1989,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "34\n\nYour Quickstart Guide to OpenAI API\n\n4. Set the n argument to instruct the model to return a single response for the request:\n\nn=1\n\n5. Add a system prompt to the messages argument to give the agent the tone of a tech com-\n\npany hiring manager:\n\n{\"role\": \"system\", \"content\": \"You are a hiring manager at a tech company.\"}\n\n6. Add an additional user prompt to the messages argument asking about the Two Sum\n\nproblem:\n\n{\"role\": \"user\", \"content\": \"What is the Two Sum problem?\"}\n\n7. For the second step, update the print statements to display the output token count and\n\nthe model output.\n\n8. Modify the first print statement to indicate the output count:\n\nprint(\"Completion Tokens: \", completion.usage.completion_tokens)\n\n9. To display the output response, update the second print statement as follows:\n\nprint(\"Output: \", completion.choices[0].message.content)\n\nIf you followed all the steps correctly, your code should look like this:\n\nimport openai\n\nfrom openai import OpenAI\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\ntemperature=0.2,\n\nmax_tokens=100,\n\nn=1,\n\nmessages=[{\"role\": \"system\", \"content\": \"You are a hiring manager at a tech company.\"},\n\n{\"role\": \"user\", \"content\": \"What is the Two Sum problem?\"}],\n\n))",
      "content_length": 1326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Chapter 2\n\nprint(\"Completion Tokens: \", completion.usage.completion_tokens)\n\nprint(\"Output: \", completion.choices[0].message.content)\n\nLab 2.2 is now ready to run and will print the output token count along with the response to the user prompt What is the Two Sum problem?\n\nLab 2.2 analysis Run the script and check the output. If you are using VS Code, look in the terminal, and if you are using PyCharm, check the run console. Ensure that the output token count does not exceed 100 and that the Chat service begins explaining the Two Sum problem, a topic we will revisit in future chapters.\n\nKeep in mind that setting a max_tokens limit does not guarantee a complete explanation and may truncate responses.\n\nThe following is a sample output, limited to 100 tokens, obtained from running the lab. The completion was truncated at 100 tokens, equivalent to 84 words:\n\n\"The Two Sum problem is a classic algorithmic problem often encountered in coding interviews and competitive programming. The problem can be stated as follows:\n\nGiven an array of integers `nums` and an integer `target`, you need to determine if there are two distinct indices `i` and `j` in the array such that the sum of the elements at these indices equals the target value. In other words, you need to find two numbers in the array that add up to the specified target.\n\n### Problem Statement\"\n\nAlthough this output is truncated, it may still provide sufficient information to understand the Two Sum problem.\n\nNote\n\nQuestion: Can you compute the cost of the lab22.py request?\n\nAnswer:\n\n>> (28 * 0.15 + 100 * 0.6) / 1000000>> 0.000064\n\nThis is approximately 1/150 of a cent.\n\n35",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "36\n\nYour Quickstart Guide to OpenAI API\n\nMultiple service responses with Lab 2.2 Reflecting on Lab 2.2, we can now better understand the impact that payload request parameters have on the initial output from the Chat service.\n\nIn that lab, we set the n parameter to 1, which is also the default value. However, in some GenAI applications, it may be beneficial to generate more than one response. For example, in tools such as GitHub Copilot or other code completion services, displaying multiple suggestions can enhance user experience. In such cases, increasing the number of responses with the n argument allows the model to provide multiple outputs. Keep in mind that the billing will include all generated outputs along with the input tokens.\n\nIn Lab 2.2, modify the n argument to request three different responses, and increase the temperature parameter to encourage more randomness and variety in the output:\n\nn=3\n\ntemperature = 2\n\nNow, update the print statements to include the additional outputs from the Chat service:\n\nprint(\"Output 1: \", completion.choices[0].message.content)\n\nprint(\"Output 2: \", completion.choices[1].message.content)\n\nprint(\"Output 3: \", completion.choices[2].message.content)\n\nRun the lab again to view the three different responses from the Chat service explaining the Two Sum problem. Feel free to experiment with the parameters further to see how they affect the output.\n\nNext, we will explore how to utilize OpenAI API for coding-related tasks, such as generating a Python code implementation based on a function’s signature.\n\nLab 2.3 – using OpenAI API to generate code Now that we have learned how to execute OpenAI API calls and configure various parameters, we can use them to create our own basic code completion program. Previously, developers could select models such as code-davinci-002, which were specifically trained for code completion. These models have since been deprecated and now chat services can effectively handle code completion tasks.",
      "content_length": 1992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Chapter 2\n\nRouting chat capabilities to code completion In this lab, we will create a code completion program by following five steps:\n\n1. Define the input to function signatures only.\n\n2. Add system prompts to guide the model in generating the complete code.\n\n3. Wrap the function signature with a specific instruction for the model to complete the code.\n\n4. Get the Chat service response for our request.\n\n5. Extract only the code from the model’s output to present to the end user.\n\nThese steps demonstrate that applications built on top of LLM models require a tailored approach such as using wrapping designs and specific prompts to align the model’s output with the busi- ness problem at hand. We will cover that in depth later in the book.\n\nOur starter code consists of all five steps, which we will complete in the guided walk-through:\n\nimport openai\n\nfrom openai import OpenAI\n\nUSER_PROMPT = \"\"\"\n\nuser prompt here\n\n\"\"\"\n\nSYSTEM_PROMPT = \"system prompt here\"\n\ndef get_code_with_instructions(code: str) -> str:\n\n\"\"\"\n\nAdd a comment to the code for specific code completion instruction\n\n:param code: Python code as string\n\n:return: The code with additional instruction - \"Complete this code\"\n\n\"\"\"\n\nreturn code + \"your wrapping instructions here\"\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create())\n\n37",
      "content_length": 1382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "38\n\nYour Quickstart Guide to OpenAI API\n\nfor i in range(2):\n\noutput = completion.choices[i].message.content\n\nprint(f\"Output {i + 1}:\")\n\ntry:\n\nsuggested_code = output.split(\"```\")[1]\n\nprint(suggested_code)\n\nexcept IndexError:\n\nprint(output)\n\nIn this lab walk-through, we will learn how to complete the system prompt, wrapping instruc- tions for the get_code_with_instructions function, and payload arguments for client.chat. completions.create. Combined with a function signature as the user prompt, these elements will channel OpenAI’s Chat service capabilities toward a targeted code completion program.\n\nLab 2.3 guided walk-through Open Lab 2.3 in the book repository at ch2/labs/lab23.py and ensure that you have either set the OPENAI_API_KEY environment variable or configured your run settings to point to the .env file.\n\nFirst, set the user prompt to be a function signature for printing the Fibonacci sequence:\n\nUSER_PROMPT=\"\"\"\n\ndef print_fibonacci_sequence(n: int) -> None:\n\n\"\"\"\n\nNext, include a system prompt to set guidelines for the assistant to act as an AI pair programmer:\n\nSYSTEM_PROMPT = \"You will be provided with a Python function signature. Your task is to implement the function. Return code only. \"\n\nWe will also wrap the function signature to include a specific instruction for code completion. Update the get_code_with_instructions function to wrap the given code with an extended comment that instructs the model to complete the function signature:\n\nreturn code + \"\\n# Complete this code\"",
      "content_length": 1512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "Chapter 2\n\nNext, configure the payload arguments by specifying the appropriate model, the desired number of responses, and the level of randomness:\n\n1.\n\nSet the model to gpt-4o-mini:\n\nmodel = \"gpt-4o-mini\"\n\nIf you have not loaded credits to your OpenAI platform account yet, your access may be limited to certain models, excluding gpt-4o-mini. In that case, use the gpt-3.5-turbo model instead.\n\n2. Set a medium temperature argument to increase the randomness of the outputs:\n\ntemperature=1\n\n3. Set the n argument to instruct the model to return two different coding suggestions:\n\nn=2\n\n4. Finally, set up the request arguments to include the wrapped function signature as the\n\nuser prompt, along with a system prompt:\n\nmodel = \"gpt-4o-mini\"\n\ntemperature=1\n\nn=2\n\nmessages = [\n\n{\"role\": \"system\",\n\n\"content\": SYSTEM_PROMPT},\n\n{\"role\": \"user\",\n\n\"content\": get_code_with_instructions(USER_PROMPT)}\n\n]\n\nIf you followed all the steps correctly, your lab’s code should be implemented as follows:\n\nimport openai\n\nfrom openai import OpenAI\n\nUSER_PROMPT = \"\"\"\n\ndef print_fibonacci_sequence(n: int) -> None:\n\n\"\"\"\n\nSYSTEM_PROMPT = \"You will be provided with a Python function signature.\n\n39",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "40\n\nYour Quickstart Guide to OpenAI API\n\nYour task is to implement the function. Return code only.\"\n\ndef get_code_with_instructions(code: str) -> str:\n\n\"\"\"\n\nAdd a comment to the code for specific code completion instruction\n\n:param code: Python code as string\n\n:return: The code with additional instruction - \"Complete this code\"\n\n\"\"\"\n\nreturn code + \"\\n# Complete this code\"\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\ntemperature=1,\n\nn=2,\n\nmessages=[\n\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n\n{\"role\": \"user\", \"content\": get_code_with_ instructions(USER_PROMPT)}\n\n],\n\n)\n\n)\n\n...\n\nLab 2.3 is now ready to run and will print two coding suggestions for the Fibonacci sequence.\n\nLab 2.3 analysis In this lab, we wrapped the user prompt and added a supporting system prompt to explicitly define the task for the Chat service: to complete a function that prints the Fibonacci sequence based solely on its signature. The following is a sample output generated when running this lab with two outputs. The first response provides a shorter solution without type checking, printing the Fibonacci numbers one by one. The second response includes input validation, creates the sequence up to the n limit, and then prints the entire sequence:\n\nOutput 1:\n\ndef print_fibonacci_sequence(n: int) -> None:",
      "content_length": 1399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "Chapter 2\n\na, b = 0, 1\n\nfor _ in range(n):\n\nprint(a, end=' ')\n\na, b = b, a + b\n\nprint() # for a new line after the sequence\n\nOutput 2:\n\ndef print_fibonacci_sequence(n: int) -> None:\n\nif n <= 0:\n\nprint(\"Please enter a positive integer.\")\n\nreturn\n\nsequence = []\n\na, b = 0, 1\n\ncount = 0\n\nwhile count < n:\n\nsequence.append(a)\n\na, b = b, a + b\n\ncount += 1\n\nprint(sequence)\n\nTo further explore code completion, try adding additional system prompts such as add a docstring or add type hints and observe the results. You can also increase the number of re- turned outputs or modify the instructions in the wrapped user prompt to see how these changes affect the generated code.\n\nImportant note\n\nIn this lab, the system prompt includes specific instructions for the desired output: Return code only. In later chapters, we will explore how to craft more effective instructions to guide the model toward the desired output structure using best practices and prompt engineering techniques.\n\n41",
      "content_length": 981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "42\n\nYour Quickstart Guide to OpenAI API\n\nSummary In the first quickstart guide, we used the openai Python package to build three different programs based on AI-generated output. We experimented with various models, including GPT-4o and GPT-3.5, explored different request parameters, and combined user prompts with system prompts.\n\nWe also loaded credits to the OpenAI platform, which increased our rate limits and allowed us to use a broader range of models. We learned how request costs are calculated based on the number of input and output tokens.\n\nFinally, we developed a basic code completion program that wrapped a function signature, guiding the model to generate the rest of the function’s implementation.\n\nWith these foundations in place, we can progress to the next chapter, Quickstart Guide to GitHub Copilot, and better understand how Copilot generates coding suggestions and how to tailor them to fit our unique preferences when necessary.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: What is the difference between a system prompt and a user prompt in a Chat service request?\n\nAnswer: User prompts are the inputs provided by the end user, such as the questions or com- mands given to ChatGPT. They represent the direct interaction with the AI-generated content, such as the following:\n\nExplain the Two Sum problem\n\nHere is another example:\n\ndef print_fibonacci_sequence(n): # complete this code\n\nSystem prompts, however, define the guidelines and tone for the Chat interaction. They instruct the assistant on how to behave during the conversation with the end user, as in this example:\n\nYou are a technical interviewer for a software engineering internship",
      "content_length": 1757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Chapter 2\n\nHere is another example:\n\nYou will be provided with a Python function signature. Your task is to implement the function. Return code only.\n\nQuestion 2: What are tokens, and how do they impact the cost of an OpenAI API request?\n\nAnswer: Tokens are small segments of text, typically averaging around 0.75 words or 4 characters each when the text is split into tokens based on elements such as punctuation, root words, and word endings. The cost of an OpenAI API request is calculated based on the number of input tokens, output tokens, and the max_tokens argument, with different models having varying token costs.\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\nOpenAI Platform home page: https://platform.openai.com\n\nOpenAI API official documentation: https://platform.openai.com/docs/overview\n\nOpenAI Pricing: https://openai.com/api/pricing/\n\nOpenAI Rate Limits (General): https://platform.openai.com/docs/guides/rate-\n\nlimits\n\nOpenAI available models: https://platform.openai.com/docs/models/model-\n\nendpoint-compatibility\n\nOpenAI individual rate limits: https://platform.openai.com/settings/organization/\n\nlimits\n\n\n\nThe FizzBuzz wiki page: https://en.wikipedia.org/wiki/Fizz_buzz\n\n\n\nThe Two Sum Leet Code page: https://leetcode.com/problems/two-sum/description/\n\n43",
      "content_length": 1355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "44\n\nYour Quickstart Guide to OpenAI API\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.",
      "content_length": 449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "3\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nGitHub Copilot, an AI pair programmer, is enabling developers to supercharge their coding abil- ities and write code more efficiently. The application uses an underlying large language model (LLM) designed specifically for code completion tasks, code analysis, and chat for coding-related tasks.\n\nSince we built our own code completion program capable of completing a Python function based on its signature alone in Chapter 2, we can better understand the role of LLMs in Copilot’s imple- mentation, as well as what other preprocessing and postprocessing work is required.\n\nThis chapter is dedicated to working with GitHub Copilot in our integrated development en- vironment (IDE). First, we will set up a GitHub Copilot account and determine who is eligible for a free account. Then, we will connect our account to our IDE (either VS Code or PyCharm) and explore the three interaction modes of the tool: chat, completion, and analysis. We will also work with Copilot in a Jupyter Notebook using VS Code. PyCharm cannot run Jupyter notebooks yet with Copilot.\n\nThrough three interactive labs, we will implement a geometric mean calculator using all three GitHub Copilot interaction modes: chat, completion, and analysis. Additionally, we will exper- iment with keyboard shortcuts for core Copilot functionality to help streamline our workflow and boost productivity.",
      "content_length": 1440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "46\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nIn this chapter, we will cover the following key topics:\n\n\n\nIntroducing GitHub Copilot – our AI pair programmer\n\nUnderstanding GitHub Copilot’s free account limits, costs, and policies\n\n\n\nSetting up Copilot in PyCharm and VS Code\n\nUsing Copilot chat\n\n\n\nIntroducing completion and keyboard shortcuts for geometric mean calculations\n\n\n\nAnalyzing code with Copilot\n\nWorking with Copilot in VS Code’s Jupyter Notebook for code analysis\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub account\n\n\n\nAccess to IDE – either VS Code or PyCharm\n\n\n\nAccess to the book’s repository, which is available at https://github.com/\n\nPacktPublishing/Supercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in your preferred IDE, VS Code or PyCharm\n\nIf you require additional assistance in setting up your OpenAI account or a virtual environment in your IDE, refer to the Appendix for detailed instructions.\n\nIntroducing GitHub Copilot – our AI pair programmer GitHub Copilot is an AI-powered tool that uses LLMs trained on vast and diverse datasets contain- ing a significant amount of open source code from public repositories across various programming languages, including Python. By combining the model’s extensive knowledge with your code’s context, GitHub Copilot provides tailored coding suggestions, as well as chat and code analysis features, to assist in your development process.\n\nCode completion design overview To provide meaningful code completions, Copilot processes the input and output of the LLM, like the code completion program developed in Lab 2.3. For the input, Copilot structures a prompt based on the lines surrounding the cursor, function signatures, and additional context, such as recent edits, Git details, file names and open files. The LLM’s output is further processed, including validation to ensure it compiles successfully.",
      "content_length": 1969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Chapter 3\n\nWe can visualize Copilot’s code completion process as a flowchart, as shown in Figure 3.1.\n\nFigure 3.1: An overview of GitHub Copilot’s suggestion process\n\nIn addition to code completion, GitHub Copilot offers two other modes of interaction: chat (avail- able inline or through the chat window) and code analysis. Both modes are based on the under- lying code and comments but differ in their final output and the postprocessing steps involved. We will discuss these extensively later in this chapter.\n\nImportant note\n\nSome developers may think that GitHub Copilot sends the entire Git repository code they are working on to the LLM. This is very unlikely given two reasons: the cost of input tokens and the relevance of the entire code to the specific code completion task. It is far more likely that only relevant code is preprocessed by Copilot first. Then, only the necessary information including coding style preferences or relevant classes, code, and variables, is added to the prompt in addition to the cursor’s surrounding context.\n\n47",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "48\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nUnderstanding GitHub Copilot’s free account limits, costs, and policies To enable GitHub Copilot in your IDE, you first need to subscribe to the service through GitHub’s platform. Like many other GenAI services, GitHub Copilot is primarily a paid service, though some exceptions allow free access.\n\nWho is eligible for a free account? There are three ways to qualify for GitHub Copilot at no cost:\n\n\n\nFree account available to anyone, which includes up to 2,000 code completions and 50 chat requests per month.\n\n\n\nFree pro account for students and educators with a valid university email\n\n\n\nFree pro account for maintainers of popular open source repositories\n\nIf you are a student or educator with a university email and the required documentation, you can apply for GitHub’s Student Pack at https://education.github.com/pack.\n\nIf you are a leading maintainer of a popular open source repository, you may have free access to Copilot. GitHub determines eligibility based on internal criteria, without requiring an application process. If eligible, upon clicking your profile icon and selecting Your Copilot, you will be directed to a page titled GitHub Copilot Individual instead of a payment page, as detailed later in this chapter.\n\nSome organizations and enterprises cover the cost of GitHub Copilot access for their users. If you belong to such an organization or enterprise, you will find a request access button next to the organization’s name at https://github.com/settings/copilot. In these cases, you will not be charged for using Copilot, as the organization or enterprise will handle the licensing fees on your behalf.\n\nCompared to individual accounts, GitHub Copilot for business and enterprises offers additional features, such as pull request summaries, a knowledge base, and the ability to fine-tune a custom LLM for enterprise versions. These options come at a higher cost than individual memberships.",
      "content_length": 1992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Chapter 3\n\nPricing For individual users, the service is priced at $10 per month or $100 annually. For Copilot business, the rate is $19 per user per month, while the enterprise version increases to $39 per user per month.\n\nCode completion policies All GitHub Copilot users are governed by specific policies that define the scope of code suggestions, privacy, and access points, including your IDE and github.com.\n\nA notable policy is suggestions matching public code. Since the GitHub Copilot model is primarily trained on open source and publicly available code repositories, some of its suggestions may be subject to copyright protection. To mitigate potential issues, GitHub provides users with an option to exclude coding suggestions that replicate such code.\n\nFigure 3.2: Privacy and ownership options in Copilot\n\n49",
      "content_length": 821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "50\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nQuick tip: Need to see a high-resolution version of this image? Open this book\n\nin the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nHere, suggestions that match public code are allowed. We also recommend enabling additional underlying models such as Anthropic Claude and Google Gemini alongside to support further experimentation.\n\nFor business or enterprise accounts, the policies are pre-determined by the administrator of the organization or enterprise. Individual account users, however, can set their own policies during account setup or through the Copilot settings page: https://github.com/settings/copilot.\n\nActivating your GitHub Copilot account To participate in the labs and hands-on exercises in this book, you will need either a free, pro, business, or enterprise GitHub Copilot account. If you wish to set up an individual account, go to https://www.github.com, click on your profile icon, and select Your Copilot.\n\nFigure 3.3: Copilot access in the GitHub account",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Chapter 3\n\nIf you are a leading maintainer of a popular open source repository and qualify for free access to Copilot, you will be directed to a page titled GitHub Copilot Individual page. Otherwise, you will be directed to the payment page, where you can select a free trial option or a paid plan.\n\nSelect the plan that suits you, adjust settings to align with your individual or company policies (e.g., enable or disable public code matches), and enter your payment details.\n\nOnce completed, you will have access to Copilot services at https://github.com/settings/ copilot as shown in Figure 3.4:\n\nFigure 3.4: GitHub Copilot settings header\n\nThis page shows your access level, provides links to resources for setting up Copilot in your IDE, and outlines Copilot’s privacy and ownership policies.\n\nWith our account now activated, we are ready to proceed and connect Copilot to our IDE.\n\nSetting up Copilot in PyCharm and VS Code GitHub Copilot is available via a PyCharm plugin or a VS Code extension. The core features of GitHub Copilot in both IDEs are similar, offering the three interaction modes we will cover in this book: chat, completion, and analysis.\n\nHowever, Copilot is more tightly integrated with VS Code, meaning new features, such as Jupy- ter Notebook support or switching to newer LLMs, are often released in VS Code before they are available in the PyCharm extension.\n\nSince your GitHub Copilot subscription can be used across multiple IDEs, we encourage PyCharm users to try VS Code to explore Jupyter Notebook support. However, the labs in this book are tailored for both PyCharm and VS Code.\n\n51",
      "content_length": 1618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "52\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nGitHub Copilot plugin for PyCharm To install the GitHub Copilot plugin in PyCharm, go to PyCharm | Settings | Plugins, then search GitHub Copilot in the marketplace. After installation, you will be prompted to restart your IDE.\n\nFigure 3.5: GitHub Copilot Plugin in PyCharm\n\nWhen your IDE restarts, you will be prompted to log in to your GitHub account.\n\nTo check whether the service is available, look for the Copilot icon at the bottom of the IDE. Hov- ering over it should display Ready.\n\nFigure 3.6: GitHub Copilot plugin icon health check in PyCharm\n\nThis confirms that the plugin is set up correctly, allowing us to start using Copilot.",
      "content_length": 717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Chapter 3\n\nGitHub Copilot extension for VS Code To install the Copilot extension, navigate to Code | Extensions, search GitHub Copilot, and in- stall it. When you do this, the GitHub Copilot Chat extension will also be automatically installed.\n\nFigure 3.7: GitHub Copilot and Copilot Chat extension in VS Code\n\nAfter the installation, the GitHub Copilot extension window will open, prompting you to authen- ticate or sign up for a GitHub Copilot account.\n\nTo check whether the extension is working properly after authentication, click on the GitHub Copilot icon in the bottom-right corner to open GitHub Copilot Menu.\n\nFigure 3.8: GitHub Copilot icon in the bottom right window\n\nA drop-down list will appear, and if you are logged into your GitHub account with a stable internet connection, the status should display Ready.\n\n53",
      "content_length": 827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "54\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nFigure 3.9: Verifying GitHub Copilot extension status in VS Code\n\nThis confirms that the extension is set up correctly, allowing us to start using Copilot.\n\nThe three interaction modes – chat, completion, and analysis Originally designed to provide tailored code completion suggestions, GitHub Copilot now offers three key interaction modes powered by LLMs: chat, completion, and analysis:\n\n1. Chat is accessible via a chat window resembling the ChatGPT interface or through an inline compact view. We will use this mode when we want contextual assistance with coding-related questions.\n\n2. Completion is available automatically in our files. As we type, Copilot will suggest code completions based on the code we write. We will use this mode when implementing new code.\n\n3. Analysis includes features such as explain, fix, and tests, which are available through the chat window, the inline chat, and the edits window in VS Code. We will use this mode when interacting with existing code.\n\nIn the next three labs, we will experiment with each of these interaction modes.\n\nUsing Copilot chat The chat feature complements GitHub Copilot’s code completion capabilities. The chat window can assist with questions related to your code, external topics, terminal commands, and even the VS Code IDE itself.\n\nIn VS Code, you can open the Copilot chat window using Ctrl + Alt + I (or cmd + control + I on Mac). Alternatively, click on the Copilot icon next to the top search bar of the screen.",
      "content_length": 1559,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Chapter 3\n\nFigure 3.10: GitHub Copilot chat window in VS Code\n\nIn PyCharm, you can open the Copilot chat window using Ctrl + Shift + C or by selecting the GitHub Copilot Chat tab in the left-hand menu.\n\nFigure 3.11: GitHub Copilot chat window in PyCharm\n\n55",
      "content_length": 257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "56\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nLet us look at an example. Open a new chat window and enter this prompt:\n\nWhat can you do?\n\nCopilot will respond with examples of use cases across the three interaction modes, such as refactoring existing code (analysis), generating unit tests (completion), providing version con- trol assistance (chat), and more. In addition to the response itself, GitHub Copilot chat will also display the pre-processed context in addition to the prompt itself, and the steps taken to prepare the input for the LLMs such as the following:\n\n\n\nCollecting recent files\n\n\n\nRetrieving Git information\n\n\n\nAnalyzing problems and errors\n\nThis aligns with the GitHub Copilot process flow we discussed earlier in this chapter.\n\nHere is an example of the prompt, What can you do?, output in PyCharm:\n\nI can assist you with various programming tasks, such as:\n\nWriting and debugging code\n\nExplaining code snippets and concepts\n\nProviding code examples and templates\n\nAssisting with version control (e.g., Git)\n\nHelping with configuration and setup of development environments\n\nOffering best practices and optimization tips\n\nIf you have a specific question or need help with a particular piece of code, feel free to ask!\n\nHere is an example of the prompt, What can you do?, output in VS Code:\n\nI can assist with various programming tasks, including:\n\nAnswering general programming questions\n\nExplaining how the code in your active editor works\n\nReviewing the selected code in your active editor\n\nGenerating unit tests for the selected code\n\nProposing fixes for problems in the selected code\n\nScaffolding code for a new file or project in your workspace\n\nCreating a new Jupyter Notebook\n\nFinding relevant code to your query",
      "content_length": 1771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Chapter 3\n\nProposing fixes for test failures\n\nAnswering questions about VS Code\n\nGenerating query parameters for workspace search\n\nRunning commands\n\nExplaining terminal outputs\n\nProviding step-by-step guidance for terminal tasks\n\nHow can I assist you today?\n\nThe PyCharm plugin and the VS Code extension may produce different outputs for the same prompt. This occurs because the GitHub Copilot application handles files in PyCharm and VS Code slightly differently, offers distinct features in each, and incorporates an element of randomness. Keep in mind that, like any GenAI output, suggestions may not always be accurate or complete, as we will observe later in this chapter.\n\nNext, we will explore how the chat window can enhance code completion tasks within our IDE, even when further exploration of a topic is needed (such as an explanation of the geometric mean).\n\nLab 3.1 – calculating geometric mean with chat completion In Lab 3.1, we will explore the geometric mean through the chat interaction mode. The geometric mean is a way of averaging numbers that are multiplicative in nature, such as financial portfolio growth rates, population growth rates, or drug concentrations that vary exponentially over time. Unlike the arithmetic mean, which calculates the sum of a sequence of numbers divided by its length, the geometric mean is the n-th root of the product of the numbers:\n\n𝑛𝑛\n\n\n\nArithematic Mean =\n\n1 𝑛𝑛\n\n∑𝑥𝑥𝑖𝑖 𝑖𝑖𝑖𝑖\n\n𝑛𝑛\n\n𝑖 𝑛𝑛\n\nGeometric Mean = (∏𝑥𝑥𝑖𝑖\n\n)\n\n𝑖𝑖𝑖𝑖\n\nFor example, the geometric mean of 5 and 20 is 10, because of the following:\n\n1 2\n\n1 2\n\n\n\n(20.5)\n\n= (100)\n\n= √100 = 10\n\n57",
      "content_length": 1602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "58\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nSimilarly, the geometric mean of 5, 20, and 10 is also 10, because of the following:\n\n1 3\n\n1 3\n\n3\n\n\n\n(20 ∙ 5 ∙ 10)\n\n= (1000)\n\n= √1000\n\n= 10\n\nIn this lab, we will use the chat window along with code completion. First, we will obtain an ex- planation of the geometric mean using the chat window. Next, we will utilize Copilot to complete an implementation based on a function signature, including the name, parameters, type hints, and return values. Finally, we will print the function’s output by defining sample call parameters.\n\nLab 3.1 guided walk-through Open Lab 3.1 in the book’s repository at ch3/labs/lab31.py and ensure that you have a stable internet connection, and that GitHub Copilot is ready.\n\nThe starter code consists of three parts:\n\n# Part 1: Open the chat window and ask to explain the geometric mean.\n\n# Part 2: Implement the geometric mean function for two floating-point numbers.\n\n# Part 3: Call\n\nPart 1 is for practice only, Part 2 requires specifying the function signature for code completion, and Part 3 involves adding sample parameters and printing their response.\n\nFor Part 1, delete the comment, open the Copilot chat window and enter the following prompt:\n\nExplain the geometric mean.\n\nThis prompt will provide an explanation of the geometric mean. Note that the explanation may be truncated due to the max_tokens limit of Copilot, which we have no control over, or contain raw mathematical formatting, such as LaTeX code instead of compiled equations. Despite this, it should still provide enough information for us to understand the topic.\n\nFor Part 2, delete the comment and type the function signature to calculate the geometric mean of two floating-point numbers:\n\ndef get_geometric_mean_of_two_numbers(\n\na: float,\n\nb: float,\n\n) -> float:",
      "content_length": 1849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Chapter 3\n\nThis function signature, with its name, parameters, type hints, and return type, is considered a complete prompt to Copilot that should yield a desirable implementation.\n\nIn Part 3, delete the comment and add a sample call that tests the implementation. Enter the following prompt:\n\nnum1: float = 5.0\n\nnum2: float = 20.0\n\nprint\n\nThe print statement acts as the cue for Copilot, which will likely suggest a call to the get_ geometric_mean function using num1 and num2 as arguments.\n\nIf you followed all the steps correctly, your code should look like this:\n\ndef get_geometric_mean_of_two_numbers(\n\na: float,\n\nb: float,\n\n) -> float:\n\nreturn pow(a * b, 1 / 2)\n\nnum1: float = 5.0\n\nnum2: float = 20.0\n\nprint(get_geometric_mean_of_two_numbers(num1, num2))\n\nLab 3.1 is now ready to run and will print the geometric mean of 5 and 20, which is as follows:\n\n10.0\n\nLab 3.1 analysis In this lab, we used Copilot chat to learn about the geometric mean. We also leveraged code completion based on either a function signature as a starter code or a print call as a cue.\n\nFor your reference, here is the Copilot chat response to the prompt, Explain the geometric mean. We generated it using the VS Code chat window with the GPT-4o model. The response includes raw LaTeX code and is truncated before the full output:\n\n>> The geometric mean is a measure of central tendency that is particularly useful for sets of numbers whose values are meant to be multiplied together or are exponential in nature. It is defined as the nth root of the product of n numbers.\n\nFormula\n\n59",
      "content_length": 1565,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "60\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nFor a set of ( n ) positive numbers ( x_1, x_2, \\ldots, x_n ), the geometric mean ( G ) is given by:\n\n[ G = \\left( \\prod_{i=1}^{n} x_i \\right)^{\\frac{1}{n}} ]\n\nSteps to Calculate\n\nMultiply all the numbers together to get the product.\n\nTake the nth root of the product, where ( n ) is the total number of values.\n\nExample\n\nConsider the numbers 2, 8, and 4:\n\nMultiply the numbers: ( 2 \\times 8 \\times 4 = 64 )\n\nSince there are 3 numbers, take the cube root: ( \\sqrt[3]{64} =\n\nChoosing the underlying models to power the chat is available in the bottom-right corner of the chat window.\n\nFigure 3.12: Model selection for the Copilot chat window\n\nWe found this response lacking due to the truncation of the example, as Copilot imposes a max_ tokens limit on model outputs, and the formatting of mathematics as raw LaTeX code. However, for topics we are already familiar with, the chat window saves considerable time and eliminates the need to use external tools, such as Google or ChatGPT.\n\nNext, we will further explore GitHub Copilot’s code completion capabilities using a variety of keyboard shortcuts to make our experience even smoother.",
      "content_length": 1212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Chapter 3\n\nLab 3.2 – introducing completion and keyboard shortcuts for geometric mean calculations Keyboard shortcuts are vital for enhancing a programmer’s efficiency, especially when working with Copilot, which provides a wide range of options to accept or reject suggestions partially or fully. This lab will focus on implementing the geometric mean using code completion, incorpo- rating keyboard shortcuts to do the following:\n\n\n\nReject a suggestion\n\n\n\nRegenerate a rejected suggestion\n\n\n\nAccept a full suggestion or parts of it\n\n\n\nSwitch between different code options.\n\nIn this lab, we will implement the geometric mean for a sequence of floating-point numbers using a function signature and an instructive prompt to validate the input arguments. We will explore various implementation suggestions, selectively accepting or rejecting parts of them.\n\nImportant note\n\nThis lab includes standard keyboard shortcuts for working with Copilot. If any of these are missing or you would prefer different shortcuts, you can customize them by editing your keymap. In PyCharm, navigate to Settings | Keymap, and in VS Code, go to Settings | Keyboard Shortcuts. For further assistance with keyboard shortcuts, refer to the Appendix.\n\nLab 3.2 guided walk-through Open Lab 3.2 in the book repository at ch3/labs/lab32.py and ensure that you have a stable internet connection, and that GitHub Copilot is ready.\n\nThere is no starter code for this lab. Our task is to begin by typing an instructive function signature to calculate the geometric mean of a sequence of floating-point numbers.\n\nAdd the following code to the file. If you type it rather than copy and paste it, Copilot may offer early completion suggestions. Ignore these until you complete the prompt:\n\ndef get_geometric_mean(*nums):\n\n61",
      "content_length": 1791,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "62\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nAt this point, you should already see completion suggestions by Copilot. Use the following in- structions for both PyCharm and VS Code to explore these suggestions:\n\nFigure 3.13: Copilot code completion example in VS Code\n\nTo experiment with Copilot’s keyboard shortcuts, follow these steps. If any of the keyboard short- cuts are different or missing in your interface, make sure to find the alternate shortcuts:\n\n1. Accept the entire code completion by pressing Tab.\n\n2. Then, undo it by Ctrl + Z / cmd + Z.\n\n3.\n\nIn PyCharm, accept a single line from the suggestion by pressing Ctrl + Alt + the right arrow (cmd + control + right if you are a Mac user).\n\n4.\n\nIn VS Code, there is no default keyboard shortcut for this. Instead, hover over the suggested text, click on the ellipsis (...), and select Accept Line.\n\n5. Then, undo it by pressing Ctrl + Z / cmd + Z.\n\n6. Accept a single word from the suggestion by pressing Ctrl + the right arrow (cmd + the\n\nright arrow if you are a Mac user).\n\n7. Then, undo it by pressing Ctrl + Z / cmd + Z.\n\n8. Hide the suggestion by pressing Esc.\n\n9. Retrigger the Copilot suggestion by pressing Alt + \\ (option + \\ if you are a Mac user).\n\n10. Toggle to the next suggestion by pressing Alt + ] (option + ] if you are a Mac user). Go back\n\nto previous suggestions by pressing Alt + [ (option + [ if you are a Mac user).\n\n11.\n\nIn PyCharm, access the code completion pane by right-clicking on Copilot’s suggestions and selecting Copilot: Open Completion.\n\n12. In VS Code, access it by pressing Ctrl + Enter.",
      "content_length": 1616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Chapter 3\n\nNext, we encourage you to modify the prompt by adjusting the function name, arguments, type hints, or return type. Keep practicing with the keyboard shortcuts and observe how the comple- tions evolve with each change.\n\nLab 3.2 analysis In this lab, we practiced keyboard shortcuts in PyCharm and VS Code to streamline pairing with Copilot, making it easier to accept, reject, or switch between completion suggestions smoothly.\n\nFor your reference, a sample completion for the get_geometric_mean implementation in Py- Charm is shown in Figure 3.14.\n\nFigure 3.14: Copilot inline menu in PyCharm\n\nWhen right-clicking on the completion and selecting Copilot: Open Completion, the completion pane displays various suggestions provided by Copilot.\n\n63",
      "content_length": 756,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "64\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nFigure 3.15: Copilot code completion pane in PyCharm\n\nWe can then choose to accept any of these suggested solutions.\n\nSimilarly, VS Code will also display a variety of different completion suggestions.\n\nFigure 3.16: VS Code Copilot code completion pane",
      "content_length": 327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Chapter 3\n\nHere too, we can then choose to accept any of these suggested solutions.\n\nImportant Note\n\nIn later chapters, we will explore best practices for prompt precision and prompt engineering techniques. These will help us better structure a function’s content by covering aspects like input validation, formatting with type hints, and handling side effects such as monitoring and logging.\n\nNext, we will explore Copilot’s code analysis options, demonstrating how they complement code completion when working with existing code.\n\nAnalyzing code with Copilot The recently introduced code analysis features in both PyCharm and VS Code complement GitHub Copilot’s code completion capabilities. It can be accessed via the Copilot menu or the inline chat window using commands such as /fix /explain and /test\n\nTo use these features, right-click on your code, select Copilot, and you will see the available op- tions. This menu is accessible in both PyCharm and VS Code.\n\nHere is how the menu appears in PyCharm:\n\nFigure 3.17: Copilot code analysis in PyCharm\n\n65",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "66\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nHere is how the menu appears in VS Code:\n\nFigure 3.18: Copilot code analysis in VS Code\n\nFor quicker access, you can use Copilot inline chat commands. In VS Code, press Ctrl + I (cmd + I for Mac).\n\nFigure 3.19: Copilot editor inline chat in VS Code\n\nIn PyCharm, press Ctrl + Shift + I (cmd + shift + I for older Mac versions of PyCharm).\n\nFigure 3.20: Copilot inline chat in PyCharm\n\nIn the inline chat window, type your code analysis command, such as /fix or /explain.",
      "content_length": 544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Chapter 3\n\nAnalyzing code with Copilot editor window in VS Code GitHub Copilot has recently introduced a dedicated editor window to enhance editing tasks and streamline working across multiple files simultaneously. To access the window, open the chat window and select Edit, as shown in Figure 3.21:\n\nFigure 3.21: Accessing Copilot’s edits window\n\nIn this editor window, you can add files you want to work on or target specific sections using the annotation #selection, as shown in Figure 3.22:\n\nFigure 3.22: Working with #selection in Copilot edits window\n\n67",
      "content_length": 560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "68\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nThe edits window supports the slash commands such as /fix, /explain, and /test, as well as tailored instructions to modify your code. For instance, to add input validation for float inputs to the following function:\n\ndef get_geometric_mean_for_three_numbers(a, b, c):\n\nWe can type the prompt:\n\n#selection add input validation for float inputs.\n\nCopilot will generate suggestions to include input validation for float types, as shown in Figure 3.23:\n\nFigure 3.23: Adding input validation with the edits window\n\nWith this prompt, Copilot presents suggestions in a version control format within the working set file, allowing us to approve or revert each change individually. We can also click Accept to apply all changes at once.\n\nIn the next chapters, we will explore best practices for crafting precise prompts to achieve more desirable results with Copilot, including its edits window. Later in the book, we will delve deeper into using the edits window for prompt engineering techniques and refactoring code.\n\nNow that we have explored how to analyze and edit code with Copilot, we can practice addressing and fixing a faulty implementation of the geometric mean, using both VS Code and PyCharm.",
      "content_length": 1272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Chapter 3\n\nLab 3.3 – working with Copilot in VS Code’s Jupyter Notebook for Code Analysis The following lab focuses on code analysis. We will use the /fix command to fix an incorrect implementation of the geometric mean for three floating-point numbers. We will then use the /explain command to get a brief explanation of an implementation for the geometric mean of a sequence of numbers.\n\nThe lab is available in two formats: a Jupyter Notebook or a Python script. If you are using VS Code, choose the Jupyter Notebook version to explore Copilot’s features for notebooks. Since Jupyter Notebook support is not yet available in PyCharm, a Python script version is also provided. In Lab 3.3 guided walk-through, we will cover both formats and use Copilot’s code analysis features to fix and understand code more efficiently.\n\nLab 3.3 guided walk-through Open Lab 3.3 from the book repository: If you are using VS Code, go to ch3/labs/lab33.ipynb, and if you are using PyCharm, go to ch3/labs/lab33.py. Make sure you have a stable internet connection, and that GitHub Copilot is ready.\n\nOur starter code includes two functions that we will apply code analysis to. The first function, get_geometric_mean_for_three_numbers, incorrectly implements the geometric mean for three floating-point numbers, as it takes the square root of their product instead of the cube root.\n\nThe second function, get_geometric_mean, calculates the geometric mean for a sequence of numbers of any length. It first validates that there is at least one item in the sequence. Then, it computes the product of all the numbers using a reduce operation. It also checks that the root results in a real number, which occurs if either the product is positive, or the required root is odd. Finally, after validation, it returns the n-th root of the product for a sequence of length n:\n\nfrom functools import reduce\n\ndef get_geometric_mean_for_three_numbers(a, b, c):\n\nreturn (a*b*c)**(1/2)\n\ndef get_geometric_mean(*nums: float) -> float:\n\n\"\"\"\n\nGet the geometric mean of a sequence of numbers\n\n\"\"\"\n\n69",
      "content_length": 2065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "70\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nif not len(nums):\n\nraise ValueError(\"Cannot calculate the geometric mean of an empty sequence\")\n\nproduct = reduce(lambda a, b: a * b, nums)\n\nif product < 0 and len(nums) % 2 == 0:\n\nraise ValueError(\"Cannot calculate the geometric mean\")\n\nreturn pow(product, 1 / len(nums))\n\nThe calculation in get_geometric_mean_for_three_numbers is incorrect, as it takes the square root of the product of the numbers instead of the cube root.\n\nTo fix this code, choose the fix option or type /fix in the inline chat.\n\nCopilot should suggest replacing the incorrect square root with the cube root:\n\nreturn (a*b*c)**(1/3)\n\nFor your reference, here is a sample output in VS Code with the proposed fix (note that this fix could be implemented in your code using the Accept button):\n\nFigure 3.24: Copilot fix suggestion in VS Code",
      "content_length": 885,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Chapter 3\n\nIn PyCharm, here is a sample output with the proposed fix. To accept it, click the Preview button and then Accept.\n\nFigure 3.25: Copilot fix suggestion in PyCharm\n\nIn the top-left corner, the preview icon is the first on the left. Click it to open the preview mode:\n\nFigure 3.26: The Preview button for accepting Copilot changes in PyCharm\n\nThe preview window displays the original code on the left and the proposed fix on the right, with an Accept button to confirm the changes.\n\n71",
      "content_length": 494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "72\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nFigure 3.27: Accepting Copilot fix suggestions in PyCharm\n\nAccept this proposed implementation.\n\nNext, move your cursor to the get_geometric_mean function and examine how Copilot explains its four parts: validating the input length, computing the product, verifying that the geometric mean is real, and returning the n-th root of the product.\n\nRight-click on the code, select Copilot | explain, or use the keyboard shortcuts to open the inline chat and type /explain. Copilot should cover all four steps of the function.\n\nLab 3.3 analysis In this lab, we used the code analysis mode to efficiently review existing code. Since a sizable portion of our work as software developers involves maintaining code rather than generating new code, these extended capabilities are invaluable for enhancing our productivity.",
      "content_length": 887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Chapter 3\n\nFor reference, here is the code explanation we obtained in VS Code, covering the four steps of the function:\n\nThe provided code defines a function get_geometric_mean that calculates the geometric mean of a sequence of numbers. The function accepts a variable number of floating-point arguments (*nums) and returns a floating-point result.\n\nThe function begins by checking if the sequence of numbers is empty using if not len(nums). If the sequence is empty, it raises a ValueError with the message \"Cannot calculate the geometric mean of an empty sequence\". This ensures that the function does not proceed with an invalid input.\n\nNext, the function calculates the product of all the numbers in the sequence using the reduce function combined with a lambda function (lambda a, b: a * b). The reduce function applies the lambda function cumulatively to the items of the sequence, effectively multiplying all the numbers together.\n\nAfter calculating the product, the function checks if the product is negative and if the number of elements in the sequence is even (if product < 0 and len(nums) % 2 == 0). If both conditions are true, it raises a ValueError with the message \"Cannot calculate the geometric mean\". This check is necessary because the geometric mean of a set of numbers that includes an even number of negative values is not defined in the real number system.\n\nFinally, the function returns the geometric mean by taking the nth root of the product, where n is the number of elements in the sequence (pow(product, 1 / len(nums))). The pow function is used to compute the nth root, which is equivalent to raising the product to the power of the reciprocal of the number of elements.\n\nOverall, the function is designed to handle various edge cases and ensure that the geometric mean is calculated correctly for valid inputs.\n\n73",
      "content_length": 1847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "74\n\nA Guide to GitHub Copilot with PyCharm, VS Code, and Jupyter Notebook\n\nOverall, Copilot explains all four steps of the function well and addresses its input. However, the interpretation of the real geometric mean output is missing from Copilot’s explanation, as shown in the highlighted part of the output:\n\nAfter calculating the product, the function checks if the product is negative and if the number of elements in the sequence is even...\n\nRather than being a limitation of the tool, this suggests that our implementation could be im- proved. We will explore this further in Chapter 4.\n\nSummary In this chapter, we took our first steps with the GitHub Copilot by setting up subscriptions and enabling Copilot in our IDE. Through three labs, we explored its three interaction modes: chat, completion, and analysis.\n\nAfter implementing a basic code completion program in Chapter 2, we were able to better under- stand how GitHub Copilot is designed. Copilot enhances the code to be completed with contextual information, such as recent edits, coding preferences, and Git logs. This information is processed into user and system prompts, which are then fed to an LLM. The model’s output is further pro- cessed before being presented to the user.\n\nThroughout the three labs, we utilized the three interaction modes to implement and refine several calculators of the geometric mean. We saw how a function signatures can be used as a prompt to Copilot. We also experimented with various completion options using keyboard shortcuts. While mastering these techniques requires some practice, they can significantly boost productivity when working with Copilot.\n\nIn the next chapter, we will discuss the three pillars of good model output: model mastery, eval- uation metrics, and precise prompts. We will also learn about the best practices in making our prompt precise with the five S’s: having a structured prompt, providing surrounding context, focusing on a single task, and having specific and short instructions.",
      "content_length": 2017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Chapter 3\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions.\n\nQuestion 1: Is Copilot a paid service?\n\nAnswer: Copilot is primarily a paid service, with a few exceptions including free limited access. Students, educators, and maintainers of leading open-source repositories are also eligible for a free subscription.\n\nQuestion 2: What are the three interaction modes for GitHub Copilot?\n\nAnswer: Copilot’s original task of code completion is complemented by two additional features: chat, which is available inline or through the chat window, and code analysis, which is accessible via the Copilot right-click menu, using slash commands, such as /fix and /explain, or in the edits window within VS Code.\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\nGitHub Copilot documentation: https://docs.github.com/en/copilot/quickstart\n\n\n\nCopilot subscription plans: https://docs.github.com/en/copilot/about-github-\n\ncopilot/subscription-plans-for-github-copilot\n\n\n\nCopilot setup in your IDE: https://docs.github.com/en/copilot/setting-up-github-\n\ncopilot/setting-up-github-copilot-for-yourself\n\n\n\nCopilot best practices: https://docs.github.com/en/copilot/using-github-copilot/ best-practices-for-using-github-copilot\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n75",
      "content_length": 1534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "4\n\nBest Practices for Prompting with ChatGPT\n\nGenAI applications excel in certain aspects of coding and often achieve high scores in coding assignment tests. Ongoing improvements to these models continue to push the boundaries, as they begin to master a wide range of coding-related tasks and supercharge the way we develop and refine code.\n\nTo achieve good results from GenAI applications, we will focus on three pillars: model mastery of the specific task at hand, evaluation metrics to critically assess the output, and crafting precise prompts to clearly achieve the desired outcome.\n\nThis chapter focuses on best practices for crafting prompts in coding-related tasks, guided by the five S’s: structured prompt, including surrounding context, focusing on a single task, providing specific instructions, and keeping the prompt as short as possible. Later in the book, we will delve into advanced prompt engineering techniques, methods to construct manual and automatic evaluation metrics, and strategies for improving the model’s mastery of specific tasks.\n\nThrough two interactive labs, we will explore how following best practices for crafting prompts for coding-related tasks leads to reusable prompt structures. We will apply these principles to refine a prompt for translating Graphical User Interface (GUI) actions into Command-Line In- terface (CLI) commands. We will also review a sample prompt published by OpenAI and analyze it through the lens of the five S’s for crafting prompts. This analysis will help us understand how these examples yield desirable outcomes and highlight potential improvements, which we will explore further in later chapters of the book.",
      "content_length": 1677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "78\n\nBest Practices for Prompting with ChatGPT\n\nIn this chapter, we will cover the following topics:\n\n\n\nCan we trust GenAI for coding tasks?\n\n\n\nBest practices for crafting prompts for coding tasks\n\n\n\nCrafting prompts for ChatGPT\n\n\n\nPrompt samples by OpenAI\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nOpenAI account with access to ChatGPT\n\n\n\nAccess to the book’s repository available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\nIf you need help creating an OpenAI account, refer to the Appendix for detailed guidance.\n\nCan we trust GenAI for coding tasks? Large language models (LLMs) are continuously improving their capabilities in mathematics and coding, achieving impressive results in both areas. For instance, internal research by OpenAI showed that the GPT-o1 model scored correctly on 11 out of 15 questions in the American Invi- tational Mathematics Examination (AIME). This performance not only exceeds the threshold required to qualify for the USA Mathematical Olympiad (USAMO) or the USA Junior Mathe- matical Olympiad (USAJMO) but also places the model in the top percentile of participants. Sim- ilarly, when evaluated on coding problems from the Codeforces competition website, the internal research showed that the model performed at the 89th percentile, highlighting its proficiency in tackling challenging coding tasks.\n\nYet, the web is filled with screenshots, videos, and examples of highly undesirable outputs from certain prompts. Not long ago, the author encountered a pull request (PR) containing a file named new.env that exposed seven secret keys. A post-mortem meeting dedicated to that issue revealed that the data scientist who committed the file had used the following prompt:\n\nHow can I commit my files to GitHub from the PyCharm terminal?\n\nThe output included the following instruction:\n\ngit add .",
      "content_length": 1906,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Chapter 4\n\nThe data scientist, who typically utilized PyCharm’s Git tool window and was used to choosing the files to commit manually, failed to notice that the file named new.env with the secret keys had been included in the commit. This incident highlights that following GenAI-generated in- structions can sometimes lead to risky outcomes.\n\nThis example does not imply that we should distrust all GenAI output or validate every detail. The fault did not occur because of the model’s hindsight about exposing secret keys through a .env file. Instead, it stemmed from a poorly crafted prompt combined with the model’s bias to- ward suggesting CLI commands such as git add ., which frequently appear in online manuals.\n\nWe argue that crafting a better prompt, such as the following, would lead to a more desirable model output:\n\nCONTEXT: You will be provided with PyCharm GUI steps enclosed with {{{ STEPS }}} to execute a process enclosed with {{{ PROCESS }}}.\n\nTASK: Convert the steps to CLI commands.\n\nPROCESS: {{{ Commit and push files to a remote git branch }}}\n\nSTEPS: {{{\n\n1. Review changed files\n\n2. Stage the desired files\n\n3. Add a commit message\n\n4. Commit the files\n\n5. Validate branch name\n\n6. Push the changes to the remote branch\n\n}}}\n\nCLI COMMANDS:\n\nThis prompt, which follows the best practices for crafting effective prompts, is structured, provides surrounding context for the problem, focuses on a single task, is specific, and is short enough. In this chapter, we will explore how to create such prompts and understand why following these practices leads to better GenAI outputs.\n\nThe three pillars of good outputs To achieve good results from GenAI applications, we will focus on the three pillars: model mastery, evaluation metrics, and crafting precise prompts.\n\nModel mastery refers to an LLM’s ability to handle a specific task described in a prompt. For instance, GPT models are extensively trained in Python code and problems such as those found on LeetCode, a platform for coding interview preparation. However, when the task deviates from\n\n79",
      "content_length": 2072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "80\n\nBest Practices for Prompting with ChatGPT\n\nmainstream topics, such as integrating Python threading to parallelize code execution across cores and virtual cores, there is significantly less training material available online, leading to outputs that may not match the quality of those for well-covered topics. Keep in mind that for such topics, LLM outputs may be suboptimal. However, we can enhance model mastery using techniques such as fine-tuning to achieve more desirable outputs. This will be covered in greater detail later in the book.\n\nEvaluation metrics refer to the manual and automatic methods we use to assess the quality of GenAI outputs. Effective prompting always involves evaluating the GenAI output, as it is not guaranteed that good results will be achieved on the first attempt. Manual evaluations involve tasks we can personally verify, such as the following:\n\n\n\nReading code in a language we recognize, such as Python\n\n\n\nAsking for CLI commands we can identify as incorrect\n\n\n\nEvaluating visual outputs such as plots to see whether they align with our expectations\n\n\n\nVerifying straightforward outputs, such as ensuring a requested CSS color matches light pink\n\nAutomatic evaluations rely on computational checks, such as:\n\n\n\nCompiling suggested code to ensure it runs without errors.\n\n\n\nRunning unit tests to validate functional correctness.\n\nUsing frameworks such as OpenAI Evals to systematically assess outputs.\n\nThese evaluations help detect faulty outputs and guide iterative improvements toward better suggestions.\n\nOpenAI Evals in brief\n\nEvals is a system designed to assess the performance of LLM applications by using a collection of prompts and expected outputs. These can include prompts and out- puts provided by the system itself or custom examples that we create. Accessible via both Python package installers and a CLI, it compares the application’s outputs against the expected results and assigns a performance score to the full set. In later chapters, we will delve into strategies for improving evaluation metrics and explore how to effectively leverage the OpenAI Evals system for evaluating GenAI outputs.",
      "content_length": 2152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Chapter 4\n\nPrecise prompts refer to how closely the instructions given to the model align with the actual task we want it to perform. When prompts are ambiguous, the model is likely to fill in gaps with assumptions based on its training data, which may not always align with the task at hand.\n\nFor example, the prompt How can I commit my files to GitHub from the PyCharm terminal? is less precise than a prompt providing specific steps performed in the PyCharm GUI and a request to convert these steps to CLI commands.\n\nNext, we will explore best practices for designing prompts for coding tasks and learn how to refine these prompts into precise inputs that produce high-quality outputs.\n\nBest practices for crafting prompts for coding tasks When working with GenAI applications for coding-related tasks, the best practices for crafting precise prompts can be summarized by the five S’s:\n\n\n\nStructured prompts with a clear separation between the instructions and provided data\n\n\n\nSurrounding information that provides context for the problem, such as the code language for the provided code and whether it compiles\n\n\n\nSingle-task focus to ensure each prompt addresses only one objective\n\n\n\nSpecific instructions detailing how the coding task should be performed, rather than using generic requests\n\n\n\nShort and minimal prompts that avoid fluff, redundant, or overly verbose instructions\n\nLet us dive into each of these practices.\n\nStructured Just as well-written, functional Python code separates logic from data, a good prompt should also distinguish instructions from provided data. When including elements such as steps, code snippets, or code drafts, ensure they are separated from the main instructions and aliased ap- propriately for clarity and reusability.\n\nReplace unstructured prompts that mix supporting data with instructions, such as the following:\n\nConvert the COBOL code\n\nIDENTIFICATION DIVISION.\n\nPROGRAM-ID.\n\n...\n\nto Python.\n\n81",
      "content_length": 1946,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "82\n\nBest Practices for Prompting with ChatGPT\n\nUse a structured approach that separates the instructions from the data:\n\nCONTEXT: You will be provided with a COBOL code enclosed with {{{ COBOL CODE }}}.\n\nTASK: Your task is to convert it to Python, with the following guidelines:\n\n...\n\nCOBOL CODE: {{{\n\n}}}\n\nPython Code:\n\nThis structured prompt is simple to reuse with other COBOL snippets.\n\nSurrounding information When specifying the surrounding context, include information about the data relevant to the prob- lem. Here are three examples of three different prompt pieces that could be used for different tasks:\n\n\n\nYou are provided with Python code / a function / a function signature.\n\n\n\nYou are provided with a Python function that contains a bug.\n\n\n\nYou are provided with GUI steps.\n\nThese instructions are general and avoid specific details, making them reusable:\n\n\n\nThe first and second examples do not specify any function\n\n\n\nThe third example does not describe specific GUI steps\n\nAs a result, these prompt pieces are reusable across various tasks.\n\nSingle task per prompt Always aim for a single task in your prompts. This approach not only improves your ability to evaluate the model’s output by focusing on a single task but also increases the likelihood that the model has encountered similar tasks during its training, enhancing its mastery of the topic.\n\nFor example, in refactoring tasks, refrain from combining multiple objectives, as follows:\n\nExplain the following function and fix compilation bugs.\n\nWhile LLMs are well trained on both individual tasks, combining them into a single prompt may be less common, reducing the model’s mastery of the topic. Instead, focus on either explaining the function or fixing its compilation bugs.",
      "content_length": 1754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Chapter 4\n\nSpecific instructions As software developers, it is tempting to use keywords such as optimize, improve, refactor, or scale. However, these are broad and ambiguous, often leaving room for interpretation by the model.\n\nTo achieve better results, use specific instructions, such as the following:\n\n\n\nUse list comprehensions instead of for loops.\n\n\n\nUse np.array to vectorize for loop computations.\n\n\n\nConvert hard-coded integers to global constants.\n\n\n\nCache function results using lru_cache.\n\nSpecific instructions eliminate ambiguity, ensuring the model understands exactly what changes or actions are expected, and increase the likelihood of producing the desired outcome.\n\nShort prompts Focus on including only relevant information, leaving fluff out, and avoiding ambiguity in your language. Note that this does not mean that you should avoid valuable and relevant information. With the GPT-o1 model supporting over 65,000 tokens (about 164 pages), it can handle longer prompts if its content is minimal and relevant.\n\nFor example, avoid phrases such as the following:\n\nI think that the following function has a bug [...]\n\nPrefer instructions such as the following:\n\nfind the compilation error in [...]\n\nAdditionally, avoid vague phrases such as the following:\n\nIt looks like this for loop could be converted to a list comprehension.\n\nPrefer instead the following instruction:\n\nConvert the for loop to a list comprehension.\n\nKeeping your prompts minimal and relevant and avoiding fluff and ambiguity allows the model to focus on the specific instructions at hand.\n\nNext, we will see how to adapt these best practices to crafting prompts for ChatGPT.\n\n83",
      "content_length": 1666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "84\n\nBest Practices for Prompting with ChatGPT\n\nCrafting prompts for ChatGPT When crafting prompts for ChatGPT, we can apply the structured approach suggested earlier:\n\nCONTEXT: ...\n\nTASK: ...\n\nSUPPORTING_DATA: {{{\n\n}}}\n\nCOMPLETION:\n\nReplace SUPPORTING_DATA with terms such as CODE, STEPS, OLD_CODE, or any other appropriate label for the provided data. Replace COMPLETION with terms such as SOLUTION, CLI COMMANDS, NEW_CODE, or another label that matches the desired model output.\n\nWhen working with the GPT-4o model with the integrated canvas editor, apply the following structure:\n\nCONTEXT: ...\n\nTASK: ...\n\nSUPPORTING_DATA: {{{ }}}\n\nTo access it, start a new GPT-4o chat and type the following to launch the canvas editor:\n\nOpen Python editor\n\nThen, add your supporting code to the Python editor as you would in any IDE, as shown in Figure 4.1:\n\nFigure 4.1: GPT-4o with canvas editor for Python",
      "content_length": 896,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Chapter 4\n\nChatGPT will edit your code based on your request and provide additional output in the chat window, as shown in Figure 4.2:\n\nFigure 4.2: Sample edited code from canvas editor\n\nLater in this chapter, we will use the canvas editor in ChatGPT to fix errors in a function.\n\nIn the upcoming lab, we will follow these practices to refine a ChatGPT prompt for converting PyCharm GUI steps into Git CLI commands. We will also evaluate how these improvements improve the model output.\n\nLab 4.1 – converting PyCharm GUI steps to CLI When transitioning from GUI steps to working directly with the CLI for Git operations, we can use ChatGPT to construct the steps required for committing and pushing staged files to a remote repository. A critical part of this process involves manually reviewing the staged files to ensure that only the desired ones are committed to our remote branch.\n\nBy applying the best practices for prompt precision, we will refine the following initial prompt into a prompt that produces a more reliable output:\n\nHow can I commit my files to GitHub from the PyCharm terminal?\n\nFor reference, using this initial prompt alone may result in a response such as the one shown in the following figure when entering it into the GitHub Copilot chat window:\n\n85",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "86\n\nBest Practices for Prompting with ChatGPT\n\nFigure 4.3: Copilot chat suggestion\n\nQuick tip: Need to see a high-resolution version of this image? Open this book\n\nin the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.",
      "content_length": 470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Chapter 4\n\nIn our guided walk-through, we will apply the following best practices to craft a more effective prompt:\n\n\n\nStructured format: Separate the logic of the task from the actual steps to be converted and the desired GUI process\n\n\n\nSurrounding context: Provide the context of the data and structure\n\n\n\nSingle-task focus: Narrow the task to converting GUI steps into CLI commands\n\n\n\nSpecific instructions: Clearly indicate all steps previously executed from the GUI, avoid- ing a generic request\n\n\n\nShort prompt: Eliminate redundant details such as PyCharm terminal\n\nWe will then feed this refined prompt to ChatGPT and observe how applying these best practices enhances the trustworthiness and reliability of the model’s output.\n\nLab 4.1 guided walk-through Navigate to https://chatgpt.com/ and start a new chat with the GPT-o1-mini model. This model has high proficiency in science, technology, engineering, and mathematics (STEM) tasks, par- ticularly coding and mathematics. As of November 2024, it achieves the highest performance among available GPT models on the Codeforces coding exam.\n\nFigure 4.4: ChatGPT model menu\n\n87",
      "content_length": 1134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "88\n\nBest Practices for Prompting with ChatGPT\n\nOur starter information consists of the following:\n\n\n\nScenario: A data scientist aims to work with the terminal instead of the PyCharm GUI to push changes into a desired branch\n\n\n\nInitial prompt:\n\nHow can I commit my files to GitHub from the PyCharm\n\nterminal?\n\nWe will now address each of the five S’s best practices for coding-related prompts following these four guidelines:\n\n\n\nStructure the prompt\n\n\n\nAdd surrounding context\n\n\n\nFocus on a specific task\n\n\n\nInclude specific instructions\n\nThroughout the steps, we will follow the fifth best practice of keeping prompts short, ensuring they remain minimal and free of unnecessary details and fluff.\n\nStep 1 – structure the prompt\n\nA well-structured prompt separates the instructions from the data, ensuring clarity and allowing for simple adjustments. In this case, the instructions outline the task, while the data contains the specific steps required to push the files to the remote branch. For ChatGPT, one strategy is to alias the data by enclosing it within delimiters such as {{{ DATA }}} for easy reference:\n\nInstructions with reference to data enclosed with {{{ DATA }}}\n\n...\n\nDATA: {{{\n\n...\n\n}}}\n\nSolution:",
      "content_length": 1213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Chapter 4\n\nImportant note\n\nWe recommend using {{{ DATA }}} as an alias because the {{{ }}} delimiter is rare in both code and other types of data. However, choosing alternative separators is also acceptable if they do not commonly appear in your code. For example, enclosers such as \"\"\" \"\"\" are used in Python for multi-line strings and could cause confusion for the model if used as delimiters.\n\nStep 2 – add surrounding context\n\nA precise prompt should include information about the surrounding context of the problem and the provided data. When converting GUI commands into terminal commands, a clear and relevant surrounding context might look like this:\n\nCONTEXT: You will be provided with PyCharm GUI steps enclosed with {{{ STEPS }}} to execute a process enclosed with {{{ PROCESS }}}.\n\nBy avoiding references to specific processes, such as committing and pushing files to a remote Git branch, this context remains flexible and can be reused for similar tasks in the future.\n\nAdd this context at the beginning of your ChatGPT prompt to establish the context of the task.\n\nStep 3 – single task\n\nOur single task in this lab is to translate GUI steps into CLI commands. A clear phrasing of this task could be as follows:\n\nTASK: Convert the steps to CLI commands.\n\nThis prompt piece specifies that the GUI step needs to be converted into corresponding CLI com- mands.\n\nAdd this prompt piece directly below the surrounding context information. Then, complete the structured prompt with the following line at the end:\n\nCLI COMMANDS:\n\nThis signals to the model that the output should be the list of CLI commands to execute.\n\n89",
      "content_length": 1627,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "90\n\nBest Practices for Prompting with ChatGPT\n\nStep 4 – specific instructions\n\nIn this lab, we aim to provide detailed and accurate instructions reflecting what is typically done when pushing changed files to a remote branch. In PyCharm, the first step of reviewing changed files can be performed by navigating to the Git tool window icon in the left menu:\n\nFigure 4.5: Git tool window\n\nIt is good practice to manually review your staged files in the working directory, ensuring that files such as new.env, which should be ignored by Git, are not included in the commit.\n\nAfter adding a commit message and clicking on Commit and Push, PyCharm pops up the push window, indicating that the selected files are about to be pushed to the remote branch.",
      "content_length": 747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Chapter 4\n\nFigure 4.6: Pushing a commit to a remote branch from PyCharm\n\nIn this figure, we can see that a single file, the solution to Lab 4.1, is being pushed to a new remote branch called ch4-labs.\n\nDescribing these steps may look like this:\n\nSTEPS: {{{\n\n1. Review changed files\n\n2. Stage the desired files\n\n3. Add a commit message\n\n4. Commit the files\n\n5. Validate branch name\n\n6. Push the changes to the remote branch\n\n}}}\n\n91",
      "content_length": 431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "92\n\nBest Practices for Prompting with ChatGPT\n\nInclude these steps in the prompt, placing them directly beneath the TASK line.\n\nIf you have followed all the steps correctly, your final prompt should look like this:\n\nCONTEXT: You will be provided with PyCharm GUI steps enclosed with {{{ STEPS }}} to execute a process enclosed with {{{ PROCESS }}}.\n\nTASK: Convert the steps to CLI commands.\n\nPROCESS: {{{ Commit and push files to a remote Git branch }}}\n\nSTEPS: {{{\n\n1. Review changed files\n\n2. Stage the desired files\n\n3. Add a commit message\n\n4. Commit the files\n\n5. Validate branch name\n\n6. Push the changes to the remote branch\n\n}}}\n\nCLI COMMANDS:\n\nYour prompt is now ready to use in ChatGPT. Execute it and refer to the lab analysis for a sample output generated by the model.\n\nLab 4.1 analysis In this lab, we crafted a precise prompt by applying the five S’s best practices. When we ran the prompt in ChatGPT using the GPT-o1-mini model, the output provided a set of CLI commands for committing and pushing the desired files to a remote branch. The generated commands were as follows:",
      "content_length": 1091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Chapter 4\n\nFigure 4.7: ChatGPT’s response to the Lab 4.1 prompt\n\nThis output is more desirable than the initial prompt because it directly references each of the six steps we would perform using the GUI, providing explanations for each corresponding command. Additionally, the output aligns with the prompt’s specificity, offering only the CLI commands with minimal surrounding explanation.\n\nImportant note\n\nRefining prompts using the five S’s best practices may seem like a lot of work initially. However, over time, seasoned prompters develop their own reusable prompt tem- plates, making the process more efficient. Well-structured prompts are reproducible across different tasks and adapting them with varied data can often be done at scale. In the next chapters, we will reuse prompts crafted with these best practices for a variety of tasks.\n\n93",
      "content_length": 851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "94\n\nBest Practices for Prompting with ChatGPT\n\nHaving introduced and implemented the best practices for crafting precise prompts, we can now observe how applying the five S’s can further enhance outputs, even from prompts already considered strong. Next, we will delve into OpenAI’s examples of good prompts, analyze them through the lens of the five S’s, and refine a prompt further to achieve a more desirable output.\n\nPrompt samples by OpenAI OpenAI occasionally updates a collection of sample prompts tailored to various tasks, including coding, content generation, translations, and grammar corrections. These prompts display the core capabilities where the underlying models exhibit strong mastery. They have been carefully crafted and have demonstrated consistent success across these use cases.\n\nYou can explore the full list of prompts and filter the samples by category at https://platform. openai.com/docs/examples.\n\nFigure 4.8: OpenAI sample prompts\n\nWhen clicking on a prompt sample from this page, we can observe the prompt itself, divided into a system prompt and a user prompt, a sample response, and a code snippet demonstrating how to implement the prompt using OpenAI API.",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "Chapter 4\n\nFigure 4.9: Improve code efficiency prompt example\n\nIn this example, the system prompt contains general instructions describing the task to be per- formed, while the user prompt includes the import statement and a function that could benefit from efficiency improvements.\n\nNext, we will critically analyze a sample prompt to assess how well it aligns with the five S’s.\n\nLab 4.2 – code debugging In this lab, we will examine a Python bug fixer use case through the lens of our best practices.\n\nNavigate to https://platform.openai.com/docs/examples and search for Python bug fixer. This should yield the following prompt example to instruct the model to find a bug in a Python pro- gram that quizzes a user on multiplication problems:\n\n95",
      "content_length": 748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "96\n\nBest Practices for Prompting with ChatGPT\n\nFigure 4.10: Sample prompt for Python bug fixer\n\nThis prompt, like the code efficiency improvement prompt example by OpenAI, separates the system and user prompts while providing a sample response. Scrolling further down the window reveals the code necessary to implement this prompt with OpenAI API.\n\nIn the guided walk-through, we will identify the errors in the provided code, so we can better evaluate the output of the model. In the analysis, we will analyze the usage of the five S’s and see how we can improve the prompt even further.",
      "content_length": 588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Chapter 4\n\nLab 4.2 guided walk-through Before we can analyze the output of the model, here are the errors in the user-provided functions:\n\n\n\nCompilation errors:\n\n\n\nTypo in import statement: import Random should be corrected to import random.\n\n\n\nType error: The question variable cannot concatenate integers a and b with strings directly. This needs to be resolved by converting integers to strings.\n\nMissing colon: The if statement is missing a : character at the end, leading to\n\nSyntaxError.\n\n\n\nInvalid assignment: The answer = a*b statement attempts to assign a value within the if condition. Modern Python compilers will raise SyntaxError as this is an invalid syntax.\n\n\n\nInvalid print statement: The string Well done! is not enclosed with quotes and therefore is an invalid syntax.\n\n\n\nLogical errors:\n\n\n\nThe a and b variables are generated once before the loop and remain constant through all 10 iterations. This is not desirable.\n\n\n\nThe output from the user is a string, while the comparison is made against integers. As a result, answer of type str can never be equal to a * b, which is of type int.\n\n\n\nReproducibility issue:\n\n\n\nThe code with random numbers is not reproducible across runs due to the absence of a random seed set. Without this, the output will vary unpredictably between executions.\n\nConsidering the output published on the OpenAI website, here is a summary of identified and fixed code errors and issues:\n\nError type\n\nProblem\n\nError\n\nFound\n\nFixed\n\nCompilation\n\nInvalid case\n\nimport Random\n\nYes\n\nYes\n\nCompilation\n\nConcatenate int and str\n\na + \" x \" + b\n\nYes\n\nYes\n\nCompilation\n\nMissing colon in if statement\n\nif statement\n\n...\n\nNo\n\nYes\n\n97",
      "content_length": 1662,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "98\n\nBest Practices for Prompting with ChatGPT\n\nError type\n\nProblem\n\nError\n\nFound\n\nFixed\n\nCompilation\n\nAssignment in if statement\n\nif answer = a * b\n\nYes\n\nYes\n\nCompilation\n\nMissing quotes for str\n\nprint(Well done!)\n\nYes\n\nYes\n\nLogic\n\nRandom numbers generated once\n\na, b outside the for loop\n\nNo\n\nNo\n\nLogic\n\nstr compared to int\n\nanswer = a * b\n\nYes\n\nYes\n\nReproduce\n\nNo seed set\n\nMissing random.seed(...), or equivalent initialization\n\nNo\n\nNo\n\nTable 4.1: OpenAI API’s performance in detecting and fixing errors\n\nThe proposed solution also introduced a new issue related to casting the input to an integer; there is no proper error handling if an input such as hello cannot be cast.\n\nThe following is the full model response, as published on the OpenAI website:\n\nThere are several bugs in the provided code:\n\n1. The import statement is incorrect. Python is case-sensitive, so \"Random\" should be \"random\".\n\n2. Variables `a` and `b` are defined outside the loop, so they will not change for each iteration. They should be inside the loop if you want to generate different questions.\n\n3. In the question string, `a` and `b` are integers, so you need to convert them to strings using the `str()` function to concatenate them.\n\n4. In the if statement, you are using a single equals sign `=` which is an assignment operator. You should use the double equals sign `==` for comparison.\n\n5. The print statement \"Well done!\" is not enclosed in quotes. In Python, strings must be enclosed in quotes.\n\n6. The input from the user is a string, so you need to convert it to an integer using the `int()` function before comparing it with the product of `a` and `b`.\n\nHere is the corrected code:",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Chapter 4\n\n```python\n\nimport random\n\nfor i in range(10):\n\na = random.randint(1,12)\n\nb = random.randint(1,12)\n\nquestion = \"What is \"+ str(a) +\" x \"+ str(b) +\"? \"\n\nanswer = int(input(question))\n\nif answer == a*b:\n\nprint (\"Well done!\")\n\nelse:\n\nprint(\"No.\")\n\n```\n\nNext, we will analyze the prompt through the lens of the five S’s, evaluate the quality of the out- put based on the errors identified, fixed, and introduced, and propose improvements to achieve a better outcome.\n\nLab 4.2 analysis The prompt introduced in this example mostly aligns with the five S’s guidelines:\n\nGood structure: The instructions are clearly separated from the code, making the prompt\n\nreusable and adaptable for other tasks.\n\n\n\nSufficient surrounding context: The prompt provides context by mentioning Python code as the supporting data.\n\nNo single task: The prompt combines two related tasks, identifying and fixing errors in the code. While these tasks are interconnected, focusing solely on fixing the errors could improve the model’s response and responsibility.\n\n\n\nSpecific: The task specifies that the model should identify and fix the errors in the code. However, the instructions could be improved by explicitly stating the types of errors to address, such as compilation, logical scoping, reproducibility, or error handling.\n\n\n\nShort: The prompt is concise and avoids unnecessary or irrelevant details.\n\nOverall, following most of the guidelines resulted in a decent output. Since we have strong eval- uation capabilities and are aware of all the errors the model should address, we can continue iterating to resolve all remaining issues in the code.\n\n99",
      "content_length": 1641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "100\n\nBest Practices for Prompting with ChatGPT\n\nAlternatively, we could better align the best practices with the following prompt:\n\nCONTEXT: You will be provided with Python code in the Python editor with explanation enclosed with\n\n{{{ CODE_EXPLANATION }}} that may contain errors enclosed with {{{ ERRORS }}}.\n\nTASK: Fix the errors in the code.\n\nCODE_EXPLANATION: {{{\n\nRun 10 random multiplication quizzes and provide feedback about each to the user\n\n}}}\n\nERRORS: {{{\n\n1. Compilation\n\n2. Reproducibility of random number generation\n\n3. Logical scoping of variable assignments\n\n4. Error handling for input casting\n\n}}}\n\nUsing this prompt with GPT-4o with canvas resulted in an output that addressed all the required fixes.\n\nFigure 4.11: Better output",
      "content_length": 750,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Chapter 4\n\nAll compilation, logic, reproducibility, and error handling are addressed in the fixes.\n\nEven better prompts In the next chapters, we will explore advanced prompt engineering techniques such as iterative prompting for creating a feedback loop with LLMs and chain-of-thought prompting for guiding model reasoning. These techniques will enable us to refine prompts that yield decent results for manual use and scale them effectively for broader applications.\n\nSummary In this chapter, we explored how to achieve high-quality GenAI outputs by focusing on the three pillars: model mastery for handling a specific topic, evaluation metrics, and precise prompts. We examined how imprecise prompts can lead to risky outcomes, such as CLI commands exposing secret keys, and demonstrated how following best practices can produce far better and more reliable model outputs.\n\nWe also learned about the five S’s of prompt precision best practices for coding-related tasks: keeping a structured prompt, providing surrounding context, maintaining a single task per prompt to best leverage model mastery and evaluation capabilities, ensuring that prompts are specific rather than general and vague, and keeping the input as short as possible by removing irrelevant information and fluff.\n\nWith two interactive labs, we explored how to implement those best practices in ChatGPT. With an OpenAI example of a good prompt to a bug fix, we saw that even good prompts can be improved to achieve more desirable results.\n\nIn the next chapter, we will explore how to apply the five S’s using OpenAI API and GitHub Copilot. In later chapters, we will delve into advanced prompt engineering techniques to further refine prompts and meet specific engineering needs in the Software Development Life Cycle (SDLC).\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: What are the three factors that determine the quality of output of an LLM?\n\nAnswer: Model mastery, evaluation capabilities, and prompt precision. Model mastery means how much the LLM trained on this specific task, evaluation capabilities means how much we can assess the desirability of the outcome, and prompt precision means to what extent the prompt is crafted to tell the model what exactly we need.\n\n101",
      "content_length": 2335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "102\n\nBest Practices for Prompting with ChatGPT\n\nQuestion 2: What are the best practices for a coding-related task prompt?\n\nAnswer: The best practices are the five S’s, structuring the prompt to accommodate supporting data such as code and steps, including surrounding context on the task, focusing on a single task per prompt, and keeping specific and short instructions.\n\nQuestion 3: Which parts of a prompt can be reproducible?\n\nAnswer: When following the best practices of precise prompts, we can reuse the surrounding con- text and the single task. These can be leveraged to many prompts and reused with a change of the details, such as the function attached, steps to be converted to CLI commands, or improvements required to a given function. This is highly efficient for large-scope refactoring tasks, repetitive tasks, and many more examples we will see in the book.\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\nOpenAI on GPT-o1 performances: https://openai.com/index/openai-o1-mini-\n\nadvancing-cost-efficient-reasoning/\n\n\n\nAmerican Invitational Mathematics Examination: https://en.wikipedia.org/wiki/\n\nAmerican_Invitational_Mathematics_Examination\n\n\n\nCodeforces: https://codeforces.com/\n\n\n\nPrompt best practices in OpenAI: https://help.openai.com/en/articles/6654000-\n\nbest-practices-for-prompt-engineering-with-the-openai-api\n\n\n\nPrompt examples in OpenAI: https://platform.openai.com/docs/examples\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.",
      "content_length": 1889,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "5\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nIn the concluding chapter of Part 1, we will explore how to apply best practices for crafting prompts tailored to OpenAI API and GitHub Copilot. These practices are encapsulated in the five S’s: structuring the prompt, providing surrounding context, focusing on a single task, including specific instructions, and keeping prompts short and fluff-free. While the core principles remain consistent, their application varies slightly across different GenAI tools.\n\nWe will delve into working with Python objects such as functions, classes, and methods for cod- ing-related tasks. We will leverage the built-in inspect package to extract key details about objects, such as their source code, docstrings, and filenames. We will also learn how to integrate this information into the system and user prompts offered by OpenAI API, combining context, task focus, and concise, structured instructions.\n\nAdditionally, we will examine how GitHub Copilot inherently provides structure, context, and single-task focus while generating short, specific prompts for us. We will learn techniques to ex- tend Copilot’s capabilities by incorporating additional structure and precision, ensuring minimal fluff and redundant comments.",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "104\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nThrough two interactive labs, we will implement these practices using a real-world example: a Singleton design pattern to enforce single-instance creation per class. Using OpenAI API, we will craft a docstring for the Singleton __call__ method, which controls the instantiation process of a new class instance. Then, with Copilot, we will generate a unit test to validate the functionality and fix a faulty implementation.\n\nIn this chapter, we will cover the following topics:\n\n\n\nExtracting properties from Python objects for OpenAI API\n\n\n\nCrafting precise prompts for OpenAI API\n\nGenerating docstrings with OpenAI API\n\n\n\nCrafting precise prompts for GitHub Copilot\n\n\n\nFixing faulty implementation with GitHub Copilot\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIntegrated development environment (IDE) – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, which is available at https://github.com/\n\nPacktPublishing/Supercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 2. For instructions on setting up OpenAI API access and token generation, see Chapter 3. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nExtracting properties from Python objects for OpenAI API When using OpenAI for coding tasks, we often include details about Python objects such as variables, functions, and classes in our prompts. This helps with various tasks such as debugging, generating docstrings, creating unit test suites, or improving code. The information provided may include the object name, arguments, return type, filename, docstring, and source code.",
      "content_length": 1943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Chapter 5\n\nThroughout this book, we frequently use the built-in inspect package to extract relevant details from Python objects. Additionally, we leverage some special attributes, identified by a double underscore (dunder), such as obj.__attribute__, which store metadata about the object or class. Here are some more examples:\n\n\n\nobj.__name__ retrieves the name of the object\n\n\n\nobj.__class__.__name__ retrieves the class name of an instance or function or the meta- class name of a class\n\n\n\nobj.__doc__ retrieves the docstring of the object\n\nTo extract the source code and filename of an object, we typically use Python’s built-in inspect library:\n\n\n\ninspect.getsource(obj) retrieves the source code of the object\n\n\n\ninspect.getfile(obj) retrieves the file name of the object\n\nFor example, we can use inspect.getsource(obj) to retrieve the source code for the faulty implementation of the multiplication_quiz function discussed in Lab 4.2 in Chapter 4. This function is intended to execute 10 different multiplication quizzes, as outlined in its docstring, but contains implementation bugs that we need to address:\n\ndef multiplication_quiz() -> None:\n\n\"\"\"\n\nThis function generates a multiplication quiz of ten different random multiplication questions between 1 and 12.\n\nThe user is prompted to answer each question. If the user answers correctly, the function prints \"Well done!\".\n\nIf the user answers incorrectly, the function prints \"No.\".\n\n\"\"\"\n\na = random.randint(1, 12)\n\nb = random.randint(1, 12)\n\nscore = 0\n\nfor _ in range(10):\n\nquestion = \"What is \" + a + \" x \" + b + \"? \"\n\nanswer = input(question)\n\nif answer == a * b:\n\nprint(\"Well done!\")\n\nscore += 1\n\n105",
      "content_length": 1666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "106\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nelse:\n\nprint(\"No.\")\n\nprint(f\"Your score is {score} out of 10.\")\n\nQuick tip: Enhance your coding experience with the AI Code Explainer and Quick Copy features. Open this book in the next-gen Packt Reader. Click the Copy button\n\n(1) to quickly copy code into your coding environment, or click the Explain button\n\n(2) to get the AI assistant to explain a block of code to you.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR visit packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nThe code for the function can be extracted using the getsource function:\n\ninspect.getsource(multiplication_quiz)\n\nThe returned value will be a string containing the function’s code as text, as shown here:\n\n'def multiplication_quiz() -> None:\\n \"\"\"\\n This function generates a multiplication quiz of ten different random multiplication questions between 1 and 12. ... the rest of the code is omitted for brevity ...'\n\nThis format is easily interpreted by GenAI applications and large language models (LLMs), and we will use it to include Python objects and code blocks in our prompts, enclosed within {{{ }}}.\n\nNext, we will explore how these objects are integrated into OpenAI prompts using the five S’s best practices for crafting precise and effective prompts.",
      "content_length": 1452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "Chapter 5\n\nCrafting precise prompts for OpenAI API Applying best practices for building a precise prompt with OpenAI API enables us to automate and scale the prompting process while ensuring quality outputs from the underlying LLMs.\n\nWe will revisit the five S’s best practices, beginning with structuring OpenAI API prompts. We will craft system prompts that incorporate the surrounding context and focus on a single task, enabling the development of reproducible and adaptable templates. Additionally, we will explore how incorporating specific and short instructions helps transform diverse inputs into concise user prompts.\n\nStructured prompts To structure prompts for OpenAI API, we use system prompts and user prompts to separate the surrounding context and single task objective from specific details such as steps, source code, and data. System prompts define the context and task, allowing for reuse across varying details such as different source codes. In later chapters of the book, we will also explore how to integrate assistant prompts to achieve even more refined results from our prompts.\n\nIn Python, this prompt structure usually includes a variable called SURROUND to provide the sur- rounding context and a variable called SINGLE_TASK to specify the task for the model. It may be structured as follows:\n\nSURROUND = \" Your surrounding context here \"\n\nSINGLE_TASK = \" Your task here \"\n\ndef get_user_prompt(*args, **kwargs) -> str:\n\nreturn \"\"\"\"\n\n# your code here\n\nCOMPLETION:\n\n\"\"\"\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(...)\n\nmessages = [\n\n107",
      "content_length": 1597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "108\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\n{\"role\": \"system\", \"content\": \"system_prompt\"\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n]\n\nIn this structure, the messages parameter combines both system_prompt and user_prompt to form a precise prompt for OpenAI API. system_prompt integrates SURROUND and SINGLE_TASK, while the get_user_prompt function provides details such as steps, source code, and object in- formation, concluding with a lead-in cue such as COMPLETION to align with SINGLE_TASK, as we will discuss in depth throughout this chapter.\n\nNow that we have examined the structure that combines both system prompts and user prompts, let us dive deeper into each, starting with system prompts.\n\nSystem prompts with the surrounding context and single task In OpenAI API, system prompts are used to define the surrounding context and specify a single task. These prompts are designed to be reproducible, accommodating varying supporting data.\n\nFor instance, a system prompt might state that the model is provided with a code snippet and tasked with identifying the programming language:\n\nSURROUND = \"You will be provided with a code snippet enclosed with {{{ CODE }}}.\"\n\nSINGLE_TASK = \"Your task is to identify the programming language of the provided code snippet.\"\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nThe SURROUND context specifies that the provided data is a code snippet in an unidentified lan- guage, while SINGLE_TASK clarifies that the expected output should identify the programming language of the code.\n\nIn another scenario, the task might involve generating documentation for a Python function where the programming language (Python) and object type (function) are already known. In such cases, this information should be included in the surrounding context to help the model focus on generating the docstring rather than determining the language or object type:\n\nSURROUND = \"You will be provided with a Python function enclosed with {{{ FUNCTION }}}.\"\n\nSINGLE_TASK = \"Your task is to generate a Google Style docstring for it.\"\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"",
      "content_length": 2126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Chapter 5\n\nIn this case, SURROUND provides context about the Python function, enabling the model to con- centrate on the task of generating docstrings, as specified in SINGLE_TASK.\n\nUser prompts with specific instructions To create an effective user prompt for OpenAI API call, we typically define a get_user_prompt function that encapsulates the necessary task details along with a lead-in cue to align with the system prompt.\n\nFor example, if the task is to identify the programming language from the source code, the func- tion implementation might look like this:\n\ndef get_user_prompt(code_source: str) -> str:\n\nreturn f\"\"\"\n\nCODE: {{{{{{ {code_source} }}}}}}\n\nPROGRAMMING LANGUAGE:\n\n\"\"\"\n\nIn the preceding implementation, {{ in f-strings are used to represent a literal {. As a result, {{{{{{ is required to represent {{{. Also, the source code is retrieved as a string, requiring no further conversion.\n\nIn a different scenario involving Python objects, such as extracting the docstring for a class method, the source code can be directly obtained from the object. The implementation of get_user_prompt in this case might look like this:\n\ndef get_user_prompt(func: callable) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nDOCSTRING:\n\n\"\"\"\n\nIn this implementation, the inspect.getsource function is used to extract the source code from the function.\n\nNext, we will combine system prompts and user prompts to observe how they work together to produce high-quality outputs.\n\n109",
      "content_length": 1504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "110\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nLab 5.1 – Generating docstrings with OpenAI API In this lab, we will utilize OpenAI API to generate a docstring for the __call__ method in a Singleton design pattern, as outlined in the following steps. The __call__ method defines the instantiation process, which involves creating or managing a new instance of the class.\n\nSingleton design pattern As Python developers, we often use design patterns to define and control the behavior of Python objects, such as functions and classes. One such pattern is the Singleton pattern, which ensures that only one instance of a class is created and used throughout the program’s execution.\n\nRegular classes create a new instance from scratch with each instantiation, using either pro- vided arguments or default parameters. For example, consider the following program with an Environment class that defaults to the name Production:\n\nclass Environment:\n\ndef __init__(self, name: str = \"Production\"):\n\nself.name = name\n\nenv1 = Environment(\"Development\")\n\nenv2 = Environment()\n\nprint(\"env1 name: \", env1.name)\n\nprint(\"env2 name: \", env2.name)\n\nIn this case, env1 and env2 are two separate instances of the Environment class. For instance, the name attribute of env1 is set to Development, while the name of env2 is set to Production based on the provided default argument.\n\nThe output of this program is as follows:\n\nenv1 name: Development\n\nenv2 name: Production\n\nThe Singleton design pattern ensures that only one instance of a class is created and maintained throughout the program’s runtime. It is typically implemented using a metaclass, which acts as a class of classes, defining how other classes are structured and behave.",
      "content_length": 1738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "Chapter 5\n\nA typical Singleton implementation maintains a registry dictionary of instantiated classes. For each class defined with metaclass=Singleton, whenever a new instance is requested, the Single- ton checks its registry for an existing instance of that class. If an instance is found, it is returned; otherwise, a new instance is created and added to the registry. Here is an example of such an implementation:\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n\nreturn cls._instances[cls]\n\nIn this implementation, the __call__ method is invoked whenever a class of the Singleton type, referred to as cls (a common name for a class argument), attempts to instantiate a new instance. If an instance of the class already exists, it is stored in the _instances attribute with the class’s hash as the key, and the existing instance is returned. Otherwise, a new instance of cls is created as usual and added to the _instances registry for future use.\n\nTo use classes of the Singleton type, such as Environment, we specify that the metaclass for the class is Singleton, as shown here:\n\nclass Environment(metaclass=Singleton):\n\ndef __init__(self, name: str = \"Production\"):\n\nself.name = name\n\nAssigning the Singleton metaclass ensures that all instances of the class refer to the same object. When two instances are created, one with the Development name and the other without any parameters, both Environment instances will share the name Development:\n\nenv1 = Environment(\"Development\")\n\nenv2 = Environment()\n\nprint(\"env1 name: \", env1.name)\n\nprint(\"env2 name: \", env2.name)\n\n111",
      "content_length": 1709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "112\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nIn this case, env1 and env2 are the same instance and env2.name is also expected to be Development. The program output confirms this, as shown here:\n\nenv1 name: Development\n\nenv2 name: Development\n\nNext, we will see how to leverage OpenAI API to generate a docstring for the Singleton __call__ method.\n\nGenerating a docstring A docstring is a special string literal in Python that provides a clear description of an object’s purpose, behavior, arguments, and return values, typically for functions or classes, and would be followed by the code implementation. For instance, here is a Google-style docstring for a function that calculates the nth Fibonacci number:\n\ndef fibonacci(n):\n\n\"\"\"\n\nCalculate the n-th Fibonacci number.\n\nThe Fibonacci sequence is a series of numbers where each number is the sum\n\nof the two preceding ones, starting from 0 and 1. This implementation uses\n\nan iterative approach for better performance with large values of n.\n\nArgs:\n\nn (int): The position in the Fibonacci sequence (0-indexed).\n\nReturns:\n\nint: The n-th Fibonacci number.\n\nRaises:\n\nValueError: If n is a negative integer.\n\nExamples:\n\n>>> fibonacci(0)\n\n0\n\n>>> fibonacci(1)\n\n1\n\n>>> fibonacci(5)",
      "content_length": 1250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Chapter 5\n\n5\n\n\"\"\"\n\n...\n\nWe will explore how to effectively create and utilize high-quality docstrings for Python objects in greater detail in Chapter 18.\n\nIn this lab, we will focus on crafting a simple Google-style docstring for the __call__ method of the Singleton class, incorporating information about the source code of the Python object.\n\nLab 5.1 guided walk-through Open Lab 5.1 in the book repository at ch5/labs/lab51.py and ensure that you have either set the OPENAI_API_KEY environment variable or configured your run settings to point to the .env file.\n\nThe starter code includes the implementation of the Singleton metaclass, along with placehold- ers for the SURROUND and SINGLE_TASK strings, the get_user_prompt function, and the messages parameter for OpenAI API:\n\nimport inspect\n\nfrom openai import OpenAI\n\nfrom openai.types.chat import ChatCompletion\n\nSURROUND = \"\"\n\nSINGLE_TASK = \"\"\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n\nreturn cls._instances[cls]\n\ndef get_user_prompt(func: callable) -> str:\n\n...\n\n113",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "114\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[],\n\n)\n\nprint(\"Docstring:\", completion.choices[0].message.content)\n\nIn the guided walk-through, we will complete the SURROUND and SINGLE_TASK strings, the get_ user_prompt function, and the messages parameter.\n\nFirst, we will define the surrounding context and single task for the reusable system prompt. Update the SURROUND and SINGLE_TASK strings as follows:\n\nSURROUND = \"You will be provided with a Python function enclosed with {{{ Function }}}.\"\n\nSINGLE_TASK = \"Your task is to generate Google Style docstring for it.\"\n\nThese strings inform the model that the provided source code is a Python function and specify that the generated docstring should follow Google-style conventions.\n\nNext, we will integrate specific and short instructions into the user prompt. Implement the get_ user_prompt function to encapsulate the source code of a provided function, along with a lead-in cue to generate a Google-style docstring:\n\ndef get_user_prompt(func: callable) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nGOOGLE STYLE DOCSTRING:\n\n\"\"\"\n\nFinally, complete the messages argument for OpenAI API call by including the system prompt with the SURROUND and SINGLE_TASK strings, along with the output of get_user_prompt encapsulating the __call__ method of the Singleton class:\n\nmessages=[\n\n{\"role\": \"system\", \"content\": f\"{SURROUND} {SINGLE_TASK}\"},\n\n{\"role\": \"user\", \"content\": get_user_prompt(Singleton.__call__)},\n\n],",
      "content_length": 1659,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Chapter 5\n\nIf you have completed all the steps correctly, your lab code should appear as follows:\n\nimport inspect\n\nfrom openai import OpenAI\n\nfrom openai.types.chat import ChatCompletion\n\nSURROUND = \"You will be provide with a Python function enclosed with {{{ Function }}}.\"\n\nSINGLE_TASK = \"Your task is to generate Google Style docstring for it\"\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n\nreturn cls._instances[cls]\n\ndef get_user_prompt(func: callable) -> str:\n\nreturn f\"\"\"\n\nFunction: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nGOOGLE STYLE DOCSTRING:\n\n\"\"\"\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": f\"{SURROUND} {SINGLE_TASK}\"},\n\n{\"role\": \"user\", \"content\": get_user_prompt(Singleton.__ call__)},\n\n],\n\n)\n\nprint(\"Docstring:\", completion.choices[0].message.content)\n\n115",
      "content_length": 1046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "116\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nYour lab is now ready to execute and generate a Google-style docstring. Stay tuned for the lab analysis, where we will review a sample output from the run.\n\nLab 5.1 analysis In this lab, we demonstrated how applying the five S’s best practices for precise prompts can yield good results with just a few lines of code. Using OpenAI API, we successfully generated a Google-style docstring for the __call__ method of the Singleton class.\n\nFor reference, here is a sample output from the lab, which produced a Google-style docstring that clearly explains the method’s general purpose, its arguments, and its return value, ensuring that the same single instance of the class is returned for every instantiation:\n\nDocstring: ```python\n\ndef __call__(cls, *args, **kwargs):\n\n\"\"\"Overrides the default behavior of instance creation to ensure that only one instance of\n\nthe class is created (Singleton Pattern).\n\nThis method checks if the class has already been instantiated. If not, it creates a new instance\n\nand stores it in the class's `_instances` dictionary. Subsequent calls will return the existing\n\ninstance.\n\nArgs:\n\ncls: The class that is being instantiated.\n\nargs: Variable length argument list for the class constructor.\n\n**kwargs: Arbitrary keyword arguments for the class constructor.\n\nReturns:\n\nThe single instance of the class.\n\n\"\"\"\n\n```\n\nThis docstring accurately describes the _instances attribute of the Singleton metaclass, which stores the single instances of the class, as well as the arguments of the __call__ method and its return value.",
      "content_length": 1620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Chapter 5\n\nHaving explored how to craft prompts for OpenAI API using the five S’s, including system and user prompts that integrate information about Python objects for coding tasks, identifying the programming language of a code snippet, and generating a Google-style docstring, we will now turn to implementing the five S’s best practices for prompt precision when crafting prompts for GitHub Copilot.\n\nCrafting precise prompts for GitHub Copilot When working with GitHub Copilot’s three interaction modes, code completion, chat, and code analysis, many best practices are already built in. Copilot structures the prompt automatically, providing surrounding context for these modes, each with a clearly defined single task, whether it is completing code, engaging in a chat, or analyzing code. The prompts are also designed to be specific and short.\n\nHowever, we can use the five S’s more precisely and achieve even better results. In this section, we will use the Singleton design pattern as an example to explore how Copilot supports code completion, code analysis, and chat.\n\nWith an understanding of this pattern and its applications, let us examine how the five S’s are implemented and can be extended within Copilot’s three interaction modes.\n\nStructuring with a lead-in cue Copilot automatically structures prompts for its three interaction modes while considering sur- rounding context such as the filename, open files, code before and after the cursor, and Git history to generate a prompt on your behalf.\n\nTo improve Copilot’s structured prompts, you can provide lead-in cues such as def func_name, the start of a variable declaration, env1 =, or the beginning of a return statement for an f-string, such as return \"\"\". These lead-ins, like the closing cues used in ChatGPT and OpenAI prompts, help clarify the starting point for Copilot’s generation and ensure it proceeds as intended.\n\nFor example, consider a prompt such as the following:\n\nclass Singleton\n\nThis is more effective than a comment such as the following:\n\n# Implement a Singleton meta-class that ensures only one instance of a class is created.\n\n117",
      "content_length": 2127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "118\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nThe first option provides a clear lead-in cue indicating the class to be implemented, while the latter is likely to result in another comment instead of initiating the actual implementation.\n\nFor reference, here is a sample completion generated by Copilot, which includes an additional comment on the next line:\n\nFigure 5.1: Comment completion with a code comment instead of an implementation\n\nCopilot interprets the request as an opportunity to elaborate further on the Singleton implemen- tation as a comment rather than starting the actual implementation.\n\nHere is a sample completion based on the lead-in cue, class Singleton:\n\nFigure 5.2: Lead-in code for Copilot to implement the Singleton metaclass\n\nIn this case, Copilot correctly inferred that it should generate the metaclass implementation even before the Singleton class name was fully specified.\n\nSurrounding with imports and hashtags GitHub Copilot already supplies the underlying LLM with extensive context across its three in- teraction modes. This includes information such as the filename, analysis of open files for coding preferences and customization, import statements, code before and after the cursor, the program- ming language being used, and preferred styling guidelines.",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Chapter 5\n\nWhen writing new code, we can enhance the surrounding context by explicitly including cues about the import statements we plan to use. For instance, if we plan to use the built-in @dataclass decorator, which simplifies data-focused classes by automatically generating boilerplate code such as the __init__ method, adding an import statement such as from dataclasses import dataclass helps Copilot understand the decorators we want to work with:\n\nFigure 5.3: Import cues for dataclass decorators\n\nIn this example, Copilot autocompletes the @ cue with the appropriate @dataclass decorator. See the Further reading section for additional resources on data classes.\n\nWhen using Copilot in VS Code, you can enhance the surrounding context in the chat by refer- encing elements such as @workspace or @terminal. These annotations, while beginning with @, do not refer to decorators but rather specify the environment the chat should consider.\n\nAdditionally, specific files or code selections can be referenced using the hashtag, #.\n\n119",
      "content_length": 1040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "120\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nFor example, to request an explanation of the __call__ method in the Singleton metaclass, you can select the method, type #selection in the chat, and then use the /explain command, as shown in Figure 5.4.\n\nFigure 5.4: Using #selection /explain in VS Code/Copilot Chat\n\nQuick tip: Need to see a high-resolution version of this image? Open this book\n\nin the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nThis gives the chat context about the specific code snippet you want explained. For reference, here is the explanation we received for the selected code:\n\nThis code defines a Singleton metaclass in Python. A Singleton is a design pattern that ensures a class has only one instance and provides a global point of access to it.\n\nHere's a breakdown of the code:\n\n1. Class Definition: class Singleton(type):\n\nThis defines a metaclass named Singleton. In Python, a metaclass is a class of a class that defines how a class behaves. A class that uses this",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Chapter 5\n\nmetaclass will follow the Singleton pattern.\n\n2. Class Attribute: _instances = {}\n\nThis dictionary will store the single instances of the classes that use this metaclass.\n\n3. __call__ Method: def __call__(cls, *args, **kwargs):\n\nThis method is overridden to control the instantiation of the class.\n\ncls refers to the class being instantiated.\n\nargs and **kwargs are used to pass any arguments to the class constructor.\n\n4. Instance Check: if cls not in cls._instances:\n\nThis checks if an instance of the class already exists in the _instances dictionary.\n\n5. Instance Creation: cls._instances[cls] = super(Singleton, cls).__call__ (*args, **kwargs) If the class is not in _instances, it creates a new instance using super() and stores it in the _instances dictionary.\n\n6. Return Instance: return cls._instances[cls]\n\nThis returns the instance of the class, ensuring that only one instance exists.\n\nIn summary, this metaclass ensures that any class using it will only have one instance, implementing the Singleton pattern.\n\nCopilot identified the purpose of the Singleton metaclass and provided a clear explanation of its key components.\n\nNote\n\nAs of January 2025, adding additional surrounding context to Copilot Chat is not yet supported in PyCharm.\n\nFurther narrowing the single task While Copilot is already tailored to the three interaction modes, each aligned with a specific task, there are cases where the task scope can be further refined.\n\nFor example, extracting hardcoded strings with arguments or global variables requires providing more specific guidance instead of relying solely on the straightforward /fix command. Without additional context, the /fix command might generate broader suggestions.\n\n121",
      "content_length": 1727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "122\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nFor reference, here is an example of a /fix suggestion that unnecessarily proposes adding a __post_init__ method to the Environment class (see Figure 5.5).\n\nFigure 5.5: Copilot offers irrelevant fix\n\nAlways aim to include single-task instructions, even when using / commands. For instance, if the goal is to extract hardcoded values, you can specify that the desired fix, extract the hard-coded default values to global constants, alongside the /fix command. Narrowing down the single task in this way helps Copilot better understand your intent and produces more accurate recommendations:\n\nFigure 5.6: Single task narrowed to extracting hardcoded values\n\nIn this example, Copilot effectively understands the task and successfully extracts the hardcoded values, proposing to extract DEFAULT_ENV_NAME and DEFAULT_ENV_VERSION:",
      "content_length": 894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "Chapter 5\n\nFigure 5.7: Successful extraction of hardcoded values\n\nIn the green highlighted text in Figure 5.7, we can see that Copilot suggested extracting the hard- coded default values into global constants named DEFAULT_ENV_NAME and DEFAULT_ENV_VERSION.\n\nThis demonstrates how applying the single task principle and narrowing it further, even when using backslash commands such as \\fix, helps us achieve more effective results when working with Copilot.\n\nSpecific instructions with type hints, docstrings, descriptive names, and unit tests Copilot already preprocesses your files and surrounding context to include specific instructions, ensuring tailored code completions, analyses, and chat responses.\n\nTo follow the specificity practice more precisely in our prompts, we can incorporate type hints, docstrings, descriptive names, and unit tests into our code. These practices not only improve the quality and readability of the code but also help Copilot generate better implementations.\n\nFor instance, if our code involves handling environment versioning as integers, adding clear type hints and a meaningful name enables Copilot to better suggest appropriate default values:\n\nFigure 5.8: Using type hints to specify desired completions\n\n123",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "124\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nIn Lab 5.2, we will explore how adding a failing unit test can assist Copilot in identifying and addressing issues in our code.\n\nShort prompts without comment fluff When working with Copilot, the goal is to supercharge our work without cluttering the code base we work on. To follow the short principle, we should avoid adding comment fluff, which we define as comments we would not include if not using Copilot and would likely remove after code generation, as they add no value to the implementation.\n\nA better approach is to use meaningful names, type hints, docstrings, and unit tests when gen- erating code, along with a lead-in cue to hint to Copilot that it should start its implementation.\n\nFor example, to instruct Copilot for code completion, it is better to provide a meaningful class signature that aligns with the intended implementation:\n\nclass TestSingleton(TestCase):\n\nThat is better than adding unnecessary comments such as the following:\n\n# Implement a test case to verify the singleton behavior of the Environment class\n\nSimilarly, prefer a docstring instead of comments:\n\ndef recursive_fibonacci(n):\n\n\"\"\"\n\nCalculate the n-th Fibonacci number using a recursive approach.\n\nArgs:\n\nn (int): The position in the Fibonacci sequence (0-indexed).\n\nReturns:\n\nint: The n-th Fibonacci number.\n\nRaises:\n\nValueError: If n is a negative integer.\n\nExamples:\n\n>>> recursive_fibonacci(0)\n\n0",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Chapter 5\n\n>>> recursive_fibonacci(1)\n\n1\n\n>>> recursive_fibonacci(5)\n\n5\n\n\"\"\"\n\nThat is better than using comments such as the following:\n\n# implement the recursive_fibonacci function here\n\n# The function should calculate the n-th fibonacci number using a recursive approach\n\n# The function should raise a ValueError if n is a negative integer\n\n# the argument is n and the return type is an integer\n\n# Examples:\n\n# recursive_fibonacci(0) => 0\n\n# recursive_fibonacci(1) => 1\n\n# recursive_fibonacci(5) => 5\n\nThe former approach is not only more predictable for Copilot but also helps avoid clutter in the code base if the comment is kept after the class generation, or confusion about the code’s origin if the comment is removed.\n\nAlso, prefer variable names with type hints instead of inline comments:\n\nproduct: float =\n\nUse the preceding code instead of the following:\n\n# initialize a product variable to 1\n\nHere, too, the former approach is more predictable to Copilot and avoids clutter in the code base.\n\nNext, we will explore how to implement the five S’s best practices with Copilot and apply them in the upcoming lab to fix a faulty implementation of a Singleton metaclass.\n\nLab 5.2 – Fixing faulty implementation with GitHub Copilot In this lab, we will address a faulty Singleton implementation by adding a unit test to verify its Singleton behavior, then use Copilot to analyze the error and suggest a fix. This lab is inspired by a bug fix the author encountered earlier in her career, where an Environment class, intended to function as a Singleton, failed to preserve attribute changes across instances.\n\n125",
      "content_length": 1618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "126\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nThe initial faulty implementation of the Singleton metaclass was as follows:\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls.__name__] = super(Singleton, cls).__call__ (*args, **kwargs)\n\nreturn cls._instances[cls.__name__]\n\nThis implementation is close to the correct implementation of a Singleton __call__ function:\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n\nreturn cls._instances[cls]\n\nHowever, the faulty implementation searches for an instantiated class instance using the cls hash key but stores it under the cls.__name__ key. This inconsistency causes a bug that prevents the Singleton from maintaining its intended behavior.\n\nIn this lab, we will address this logical error by first using Copilot to implement a unit test that evaluates the behavior of the metaclass and then asking Copilot to fix the bug.\n\nLab 5.2 guided walk-through Open Lab 5.2 in the book repository at ch5/labs/lab52.py and ensure that you have a stable internet connection and that GitHub Copilot is ready.\n\nThe starter code includes the faulty Singleton implementation along with an import statement for the TestCase class and the main runner from Python’s unittest package:\n\nfrom dataclasses import dataclass\n\nfrom unittest import TestCase, main\n\nclass Singleton(type):\n\n_instances = {}",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Chapter 5\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls.__name__] = super(Singleton, cls).__call__ (*args, **kwargs)\n\nreturn cls._instances[cls.__name__]\n\n@dataclass\n\nclass Environment(metaclass=Singleton):\n\nname: str = 'Production'\n\nversion: int = 1\n\nif __name__ == \"__main__\":\n\nmain()\n\nIn the next steps, we will create a unit test to debug the faulty behavior of the Singleton metaclass, analyze why it fails, and correct the implementation accordingly.\n\nTo test a metaclass’s behavior, we might want to include a class implementation that utilizes the Singleton and compare different instances of it, as follows:\n\ndef test_something(self):\n\n@dataclass\n\nClass A(metaclass=Singleton):\n\na: int = 0\n\nself.assertEqual(A(2), A())\n\nTo ensure Copilot follows this guideline, we will provide a lead-in cue to start the test case im- plementation by beginning with the class name. Type the following lead-in:\n\nclass TestSingleton\n\nAs you type, Copilot should recognize that you are attempting to test the Singleton metaclass. It may suggest a complete test case, but to tailor it more specifically to our requirements, accept only the first line, which should be as follows:\n\nclass TestSingleton(TestCase):\n\n127",
      "content_length": 1247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "128\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nContinue by adding a descriptive name for the test case:\n\ndef test_singleton_should_return_same_instance(self):\n\nUsing descriptive names for unit tests is good practice, as it clearly conveys the purpose of the test and serves as a documentation tool.\n\nAt this point, Copilot may suggest an implementation involving the Environment class. However, the preferred approach is to test a designated class that uses the Singleton metaclass, as our goal is to validate the metaclass itself rather than the Environment class. Begin the implementation with the following:\n\n@dataclass\n\nclass A(\n\nAt this point, Copilot should detect that we are testing an implementation of the metaclass. By combining the test name and the lead-in cue, it may suggest a unit test such as the following:\n\n@dataclass\n\nclass A(metaclass=Singleton):\n\na: int = 0\n\nself.assertEqual(A(2), A())\n\nThis unit test verifies that an instance of A initialized with a non-default value of 2 should be equal to a new instantiation of the class without additional parameters, ensuring the Singleton behavior is maintained.\n\nWe will delve deeper into writing unit tests with GenAI applications in Chapter 16. For now, accept the suggested test case if it resembles the preceding one or tests similar functionality.\n\nIf you have completed all the steps correctly, your test class should look like this:\n\nclass TestSingleton(TestCase):\n\ndef test_singleton_should_return_same_instance(self):\n\n@dataclass\n\nclass A(metaclass=Singleton):\n\na: int = 0\n\nself.assertEqual(A(2), A())",
      "content_length": 1599,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Chapter 5\n\nPlease note that this test will fail because the Singleton implementation has not yet been cor- rected.\n\nNext, execute the lab using the following shell command:\n\npython -m unittest ch5/labs/lab52.py\n\nThis should result in a failing unit test:\n\nFigure 5.9: Failing unit test for Singleton behavior\n\nFailure is expected because the __call__ method of the Singleton is not properly implemented. When a new instance of A is created, its attribute, a, is set to the default value of 0 instead of retaining the value from the previous instantiation of a=2.\n\nIf you are using VS Code, you can get an explanation of the terminal error by typing the following in the Chat window:\n\n@terminal /explain the failing unit test\n\n129",
      "content_length": 729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "130\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nThis prompt specifies that the context is the terminal, instructing Copilot to explain the test failure as shown in the terminal trace. Here is an example explanation generated by Copilot Chat using the GPT-4o model:\n\nFigure 5.10: VS Code explanation for the failing unit test\n\nNote that the objects in the explanation are clickable for added convenience. Clicking on objects such as A will direct you to their location in the code:\n\nFigure 5.11: Clicking on A would highlight the objects in the file",
      "content_length": 570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Chapter 5\n\nThe explanation correctly identifies that the incorrect implementation of the Singleton metaclass results in two different instances of the A class, causing the test to fail.\n\nIf you are using PyCharm, copy the failing traceback and paste it into the chat as follows, follow- ing the five S’s:\n\nCONTEXT: You will be provided with a failing Python unit test traceback enclosed with {{{ TRACEBACK }}}.\n\nTASK: Explain why the unit test is failing.\n\nTRACEBACK: {{{\n\npython -m unittest ch5/labs/lab52_singleton.py\n\nF\n\n======================================================================\n\nFAIL: test_singleton_behavior_should_return_same_instance (ch5.labs. lab52_singleton.TestSingleton.test_singleton_behavior_should_return_same_ instance)\n\n----------------------------------------------------------------------\n\nTraceback (most recent call last):\n\nFile \"supercharge/ch5/labs/lab52.py\", line 26, in test_singleton_ behavior_should_return_same_instance\n\nself.assertEqual(A(2), A())\n\nAssertionError: TestS[13 chars]_singleton_behavior_should_return_same_ instance.<locals>.A(a=2) != TestS[13 chars]_singleton_behavior_should_ return_same_instance.<locals>.A(a=0)\n\n----------------------------------------------------------------------\n\nRan 1 test in 0.001s\n\nFAILED (failures=1)\n\n}}}\n\nEXPLANATION:\n\nThis structured prompt, which includes surrounding context, a specific task to explain the failing unit test, the traceback with an enclosed alias, and a lead-in explanation cue, will help us under- stand why our test is failing.\n\n131",
      "content_length": 1539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "132\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nFor reference, here is a sample output from Copilot Chat in PyCharm that correctly identifies the cause of the failing test, specifically, the error in the singleton __call__ method implementation:\n\nFigure 5.12: Copilot Chat explanation for the failing unit test\n\nAlthough this chat does not contain clickable elements, it accurately identifies the problem in the __call__ method that is not implemented correctly.\n\nNow that we have identified the issue with the Singleton metaclass, we can adjust the __call__ method to properly maintain the single class instance design pattern.",
      "content_length": 650,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Chapter 5\n\nTo fix the issue, navigate to the __call__ method, highlight it, and prompt Copilot to fix the cls instantiation, as follows:\n\n/fix cls instantiation\n\nThis prompt will suggest the correct fix for the cls._instances registry. For your reference, here is a sample output we received when using the Copilot extension in VS Code with the GPT-4o model:\n\nFigure 5.13: Suggested fix for the cls instantiation\n\nCopilot provided the correct implementation of the Singleton metaclass using cls._ instances[cls] instead of cls._instances[cls.__name__].\n\nAccept this solution. At this point, your code should appear as follows:\n\nfrom dataclasses import dataclass\n\nfrom unittest import TestCase, main\n\nclass Singleton(type):\n\n_instances = {}\n\ndef __call__(cls, *args, **kwargs):\n\nif cls not in cls._instances:\n\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n\nreturn cls._instances[cls]\n\n133",
      "content_length": 911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "134\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nRerun the lab using the following shell command:\n\npython -m unittest ch5/labs/lab52.py\n\nThe unit test should pass, confirming that the singleton is correctly implemented:\n\nFigure 5.14: Test ran with no errors\n\nThe unit test verifies that a new instantiation of the A class retrieves the previously created instance with the argument of a=2, and the main() runner completes without errors.\n\nLab 5.2 analysis In this lab, we debugged the Singleton implementation by creating a unit test that failed. Based on the failure explanation, we used Copilot to fix the error. To accomplish this, we applied the five S’s best practices across five different prompts. Let us examine how each prompt emphasizes different practices of the five S’s, helping us achieve more desirable results.\n\nPrompt 1 – following the structured prompt principle with a lead- in cue To create a test for the Singleton metaclass, we began with a code lead-in cue for Copilot:\n\nclass TestSingleton\n\nWe also followed the short best practice, by avoiding unnecessary comments. This was enough for Copilot to suggest the name class TestSingleton(TestCase) for the test class.\n\nPrompt 2– providing specific and short prompt principles To generate a unit test that utilizes the singleton as a metaclass, we used a descriptive test name, test_singleton_should_return_same_instance, along with a lead-in @dataclass decorator and class implementation:\n\n@dataclass\n\nclass A(metaclass=Singleton):",
      "content_length": 1523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Chapter 5\n\nThis was sufficient for Copilot to understand that the test should validate the correct instantia- tion of the Singleton:\n\nclass TestSingleton(TestCase):\n\ndef test_singleton_should_return_same_instance(self):\n\n@dataclass\n\nclass A(metaclass=Singleton):\n\na: int = 0\n\nself.assertEqual(A(2), A())\n\nThis test case compares two instances of the A class: one created with the default parameter and another with a specified parameter. If the Singleton is implemented correctly, the two instances should be equal.\n\nPrompt 3 – adding surrounding context and focusing on a single task with VS Code After running the failing unit test, we enhanced the prompt with the additional surrounding context and a single task by using the following:\n\n@terminal /explain the failing unit test\n\nThis provided detailed information about the incorrect implementation of the __call__ method.\n\nPrompt 4 – applying the five S’s with PyCharm chat When using PyCharm chat, we applied the same prompt techniques that work with ChatGPT. We used a structured prompt with the surrounding context, focusing on a single task, and created a specific and short prompt that led to an explanation of the traceback for the failing unit test.\n\nPrompt 5 – narrowing down the single task to fix a method implementation Once Copilot identified that the issue was with the __call__ method’s cls instantiation, we provided the following /fix command with a narrowed-down single task:\n\n/fix cls instantiation\n\nThis allowed Copilot to focus on the correct fix required from this method.\n\n135",
      "content_length": 1553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "136\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nIn summary, the five S’s for crafting precise prompts are essential when interacting with any of GitHub Copilot’s three interaction modes. They help refine the structure by incorporating lead-in cues, adding surrounding context to both chats and input statements, and narrowing the focus to the specific task. This approach makes instructions more precise while avoiding unnecessary comments and keeping prompts short and minimal.\n\nIn the upcoming chapters, we will explore advanced prompt engineering techniques to further enhance our ability to achieve even better results from the GenAI applications.\n\nSummary In this chapter, we explored how to apply the five S’s best practices for crafting precise prompts using both OpenAI API and GitHub Copilot. With OpenAI API, we learned how to extract valuable information from Python objects, including attributes such as source code, docstrings, filenames, object types, and more. When working with Copilot, we examined which aspects of the five S’s are already implemented and identified how we can use those more precisely to improve outputs from the GenAI application.\n\nThrough two interactive labs, we utilized the Singleton design pattern, commonly employed by Python developers to enforce a single class instance with a designated metaclass. Using OpenAI API, we generated documentation for the __call__ method of the Singleton. With GitHub Copilot, we constructed a unit test that failed due to an undesirable behavior caused by faulty implemen- tation, analyzed the test failure, and assisted Copilot in fixing the implementation.\n\nWith this, we conclude Part 1 of this book, in which we explored the best practices for crafting precise prompts in ChatGPT, OpenAI API, and GitHub Copilot.\n\nBy now, we have learned how to use OpenAI API, ChatGPT, and GitHub Copilot for a variety of use cases. We examined the costs associated with each tool and observed the OpenAI rate limits. We also explored Copilot’s architecture, built on the foundation of OpenAI API, and gained insights into using ChatGPT’s canvas editor effectively.\n\nWe discussed the three pillars of achieving quality outcomes from a GenAI application: model mastery, evaluation metrics, and precise prompts. We emphasized the role of precise prompts by implementing the five S’s: structured prompts, surrounding context, single-task focus, specific instructions, and short prompts without unnecessary fluff.",
      "content_length": 2494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Chapter 5\n\nNext, we move on to Part 2, where we will delve deeper into understanding the underlying LLMs, analyzing their risks and limitations, and learning how to take ownership of GenAI-generated code, including critically reviewing code created from colleagues’ prompts.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: How can we incorporate information from Python objects when working with OpenAI API?\n\nAnswer: When working with OpenAI API, we often include data about Python objects such as functions, methods, classes, and variables. To extract details such as source code, docstrings, filenames, and object types, we can use special attributes (dunder) methods, or the built-in inspect package.\n\nQuestion 2: Which of the five S’s for precise prompts are implemented in OpenAI API system prompts?\n\nAnswer: OpenAI API system prompts incorporate surrounding context and single-task specifi- cations, separated by a space. These prompts are reusable across different user prompts that are typically handled by a function such as get_user_prompt.\n\nQuestion 3: How can we extend the structured prompt practice with GitHub Copilot?\n\nAnswer: GitHub Copilot handles much of the structuring automatically. Adding a lead-in cue, such as a function name (def func_name), variable type hint (product: float =), or a class name (class ClassName), serves as an effective cue for Copilot to begin the desired implementation.\n\nQuestion 4: Are code comments such as # implement the following function effective when working with Copilot?\n\nAnswer: Such comments are considered fluff and do not enhance the code’s readability or Copi- lot’s output. Always prefer using lead-in cues instead of trivial comments.\n\n137",
      "content_length": 1787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "138\n\nBest Practices for Prompting with OpenAI API and GitHub Copilot\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\n\n\nSingleton design pattern, by Geeks for Geeks: https://www.geeksforgeeks.org/\n\nsingleton-method-python-design-patterns/\n\nMetaclasses, by Real Python: https://realpython.com/python-metaclasses/\n\n\n\nPython dataclass documentation: https://docs.python.org/3/library/dataclasses.\n\nhtml\n\n\n\nThe 3S’s best practices for GitHub Copilot for VS Code: https://techcommunity. microsoft.com/blog/azuredevcommunityblog/single-short-specific---prompting-\n\ngithub-copilot-for-visual-studio/4117226\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Part 2\n\nBasics to Advanced LLM Prompting for GenAI Coding\n\nIn Part 2 of this book, we explore how LLMs can be applied to consistently generate quality code. It begins with a high-level guide on how LLMs work and then applies them to different tasks. By the end of this part, you will understand which tools (ChatGPT, OpenAI API, and GitHub Copilot) can be best applied for a given task.\n\nThis part expands into prompt engineering, including advanced topics such as few-shot learning, Chain-of-Thought reasoning, iterative prompting, and chaining to better understand and refactor code. It concludes with an exploration of fine-tuning methods.\n\nThis part contains the following chapters:\n\nChapter 6, Behind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nChapter 7, Reading and Understanding Code Bases with GenAI\n\nChapter 8, An Introduction to Prompt Engineering\n\nChapter 9, Advanced Prompt Engineering for Coding-Related Tasks\n\nChapter 10, Refactoring Code with GenAI\n\nChapter 11, Fine-Tuning Models with OpenAI",
      "content_length": 1026,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "6\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nWhile most of the chapters in this book provide hands-on guides to using GenAI models for coding, it is also useful to understand how they work under the hood. ChatGPT, OpenAI API, and GitHub Copilot are based around large language models (LLMs) that have been responsible for the sud- den rise of GenAI not only in the technology space but across industries. By understanding how LLMs work, you will understand when they can be most effective, know their limitations, and appreciate their role in prompting. This will maximize your overall success with GenAI.\n\nThis chapter will introduce the following concepts:\n\n\n\nStatistical approach to finding similar letters\n\n\n\nSmall and large language models for words\n\n\n\nImproving LLM output performance\n\nWhat are the limitations of LLMs?\n\n\n\nChatGPT\n\nGitHub Copilot",
      "content_length": 879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "142\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nStatistical approach to LLMs Many users of ChatGPT and other LLMs often have an incorrect impression of how LLMs work. They ask questions such as What fraction of the internet do they understand?. To introduce the un- derlying concepts, this section focuses on a simpler problem using just statistics. This will allow us to provide a foundation for discussing how models work before we shift into AI and deep learning. The question we will address is whether we can use probabilities to predict the next letter given a set of preceding letters. If we can, then we should be able to create true English from data without any understanding of linguistic orthography, phonology, morphology, or even general language.\n\nFor this sample problem, we will use a medium-sized dataset called the Reuters-21578 dataset. It is available at the UCI Machine Learning Repository (https://archive.ics.uci.edu/) and consists of 21,578 different news wire articles, which are just text stories across different topics. While the main use of this dataset is to train and assess text classification algorithms, we are using this as a large corpus of text that has over 15 million letters. The rest of this section describes the process of using this corpus to predict the next letter using a statistical approach. A curious reader can experiment further with the provided code available in the GitHub repository for Chapter 6.\n\nOur first analysis step is to look at the frequencies of individual letters. Although the dataset has not only letters but also numbers, punctuation, and other symbols, we will focus on just the letters and the spaces between words. We will also ignore the case and convert everything to lowercase. The frequencies of the individual letters are shown in Figure 6.1. The frequency of",
      "content_length": 2542,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Chapter 6\n\nwords typically follows Zipf’s law, but not for letters. However, we do see the expected tapering of frequencies when sorted by letter, although the distribution is not as smooth due to the limited size of the dataset.\n\nFigure 6.1: Letter frequencies for the Reuters-21578 dataset\n\nWith the frequencies or probabilities of the individual letters from Figure 6.1, we can produce words by sampling from this distribution. We can sample proportional to the frequency of the letters in the dataset and try to craft English words. We can do this by generating a random number with a uniform distribution between 0 and 100%. If it’s < 12%, it would be an e. If it’s in the 12 to (12+9)% range, then it would be t. If it’s in the 19 to (19+8.5)% range, it would be a, and so on. The results of this approach are shown in Figure 6.2. Each row represents a different example to show some variation. As you can see, we have not replicated anything that looks vaguely like English:\n\nFigure 6.2: Creating “words” from using just the individual word frequencies\n\nUsing the individual frequencies of words doesn’t use any context, and so it produces words that do not resemble English. Context in this case refers to the order of the letters that come before it. For instance, in the first line, no English words start with wwu or tte or dn. Similarly, the letter q is usually followed by u in English.\n\n143",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "144\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nTo augment this with the context of preceding letters, we can use conditional probabilities. In statistical terms, p(x|y) is read as the probability of x given y, and it means the probability of x occurring given that y occurs. Mathematically, the formal definition is as follows:\n\n\n\n𝑝𝑝(𝑥𝑥𝑥𝑦𝑦) 𝑝𝑝(𝑥𝑥)\n\n𝑝𝑝(𝑥𝑥|𝑦𝑦) =\n\nIf we think of playing cards, the probability of the queen of diamonds is 1/52, but the probability of the queen of diamonds given it is a queen is 1/4. The probability of a queen of diamonds given it is a diamond is 1/13.\n\nIn our context, we are going to use these conditional probabilities to compute the probability of the next letter. So, given the current letter is a q, the next letter is usually a u. In fact, we can build a table of all the possibilities written as percentages for each combination of letters. Figure 6.3 shows the first letter as the column, and the conditional probability of the next letter as a row. For instance, p(u|q) is 90.3%.\n\nFigure 6.3: Percentage of conditional probabilities for next letters (rows) given letters (columns)",
      "content_length": 1150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Chapter 6\n\nThe sampling is more complicated in the conditional probability case, so let’s use an example. To create text, we might assume that our last letter is a q. To choose the next letter, we randomly sample from the possibilities across all rows of the q column. We then generate a random value from 0 to 100% representing a random probability drawn from a uniform distribution for the next letter. We then consider which bucket the random number fell in. The letter a is 0.7%, b through h are all 0.0%. The letter I has a 1.1% chance. The letter t has a 7.6% chance, and the letter u has the largest probability of 90.3%. While it appears that these numbers do not sum to 100%, this is due to round-off errors only. If we select based on these probabilities, the most likely letter is u. We repeat the process with the u column, where the most likely letters are t, r, n, and s, and the least likely letters are q and u. You are likely wondering which words have these strange letters, but remember that there may be acronyms, email addresses, and foreign names included in the articles.\n\nNow that we can generate words for conditional probabilities, does this approach work? Figure 6.4 shows the results. The word lengths are much closer, and there are a couple of real words, such as it, as, and mag, but otherwise, they do not look like English words:\n\nFigure 6.4: Conditional probabilities for p(next_letter | current letter)\n\nWe can extend the model so that instead of just looking at a letter given its preceding single letter, we can consider a wider context. Specifically, we can consider the next letter given the preceding j letters using the probability p(next_letter | preceding j letters). This makes the table much larger since the number of columns would increase from 26 to combinations of 2 letters, 3 letters, or more. For the next letter, based on all combinations of the 2 previous letters, the table would be 26 by 262 = 676. For the next letter based on the previous 3 letters, the table would be 26 by 263 = 17,576. However, with a longer context, we would expect better results. We will not show these larger tables of percentages or probabilities, but show the results instead.\n\n145",
      "content_length": 2214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "146\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nThe result for the preceding two letters is shown here:\n\nFigure 6.5: Conditional probabilities for p(next_letter | preceding 2 letters)\n\nFor the preceding three letters, the result is as follows:\n\nFigure 6.6: Conditional probabilities for p(next_letter | preceding 3 letters)\n\nFor the preceding four letters, this is the result:\n\nFigure 6.7: Conditional probabilities for p(next_letter | preceding 4 letters)\n\nFor the preceding five letters, the result is as follows:\n\nFigure 6.8: Conditional probabilities for p(next_letter | preceding 5 letters)",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Chapter 6\n\nFinally, the result for the preceding six letters is the following:\n\nFigure 6.9: Conditional probabilities for p(next_letter | preceding 6 letters)\n\nAs we can see, the results improve substantially as we increase the context window. In fact, most of the output has real English words once we use the four preceding letters. With six preceding letters, the sentence flow starts to make some sense, which may be surprising since we are only looking at single letters.\n\nIt is important to understand what the model is learning. Although the statistics appear to be producing English words and even some meaningful sentences in Figure 6.9, it does not under- stand the concept of an English word or sentence structure. Furthermore, the model is not storing knowledge or facts of the English language or words. It is only storing the probabilities of the next letter from the preceding context. Our model focuses only on letters at this point, and the next section describes the extension to word-level models. If we had training data that was a thousand or a million times larger, it would be merely doing a better job of estimating the same frequencies.\n\nExtending the model to words Our English model, which builds language from letters, is quite limited. To produce meaningful language, we would need a much longer context of preceding words to understand the relation- ships between words spaced throughout a sentence. Furthermore, we don’t think of language in terms of letters—we think in terms of words. With our 26 letters in English and a space, each of the preceding j positions can only have 27 possibilities, which means our model is relatively small.\n\n147",
      "content_length": 1675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "148\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nNote\n\nIf we consider all combinations of the preceding 3 letters, there are 27 x 27 x 27, or about 20,000 possibilities of letters. With 15 million letters in the dataset and 20,000 combinations, we have a 750:1 ratio of training to combinations, which provides excellent training. The average person knows 42,000 words, so if we use all combi- nations of the preceding 3 words, it would be 42,000 x 42,000 x 42,000 ~ 7.4E13, or 74 trillion 3-word combinations possible, which is about 10x more than any PC hard disk can handle today. However, if we have 15 million letters and the average word plus a space character is 6 letters, then there would be 2.5 million words. The ratio of training to combinations drops to a 2.5 million:74 trillion ratio, or 1 in 30 million.\n\nBased on the preceding information, we would need billions of times more data for similar ac- curacy using the statistical model.\n\nAs a model becomes more complex, it requires more training data. We can think of a model as a general function where we try to approximate a y value from different x values. This approxi- mation process determines the best function parameters from a set of training data (x, y) pairs representing the input, x, and the output, y. In the case of a linear model, the model must learn the best slope and y-intercept to represent the data. The training data is collected and designed to be representative of future data so that when the model fits well, it can be useful to predict future y values.\n\nOf course, most models are not simple linear models or functions. There may be multivariate inputs with billions of parameters and large output dimensionality. Learning from such large datasets requires significantly larger training data. Consider fitting a linear or a higher-order linear model to a set of data points, as shown in Figure 6.10:\n\nFigure 6.10: The number of training data points increases as the model becomes more complex",
      "content_length": 2011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Chapter 6\n\nWe can fit a line with just two points, perhaps, but the seventh-order model on the right needs far more data points to accurately fit. AI models often require data for trillions of parameters, which introduces a problem where the model may suffer from insufficient training data available. The accuracy of the model depends on the number of parameters in the model and the size of its training dataset.\n\nTo summarize, the example with letters showed that a statistical model could approximate English language words with a moderate training dataset since the number of combinations is reasonably small. However, we need to store the probabilities of each letter given all preceding letter contexts, which can be quite large. For even the context of just three preceding words, the frequencies are prohibitive to store. In addition to the storage challenge, we would need vast quantities of data to fill a word frequency table to achieve modest prediction accuracy, which is not readily available. The next section uses AI rather than statistics to overcome these challenges for words.\n\nSmall and large language models for words The statistical model clearly worked well for a medium-sized dataset with a small model based on letters. Since grammar and meaning function at the word level, we need to be able to accu- rately predict the next word rather than the next letter. Since the number of possible next words is far larger than the number of next letters, the model will need to be many orders of magnitude larger. If we think about code or even text, there are no datasets that are billions of times larger. AI methods, and specifically LLMs, provide solutions that enable predictions more efficiently than just probabilities. We describe two methods in this section: word2vec and the Transformer deep learning model.\n\nWord2vec The word2vec model is arguably a small language model that uses a simple neural network. In- spired by biology, neural networks are a particular type of machine learning algorithm that learn from data. Although there are a variety of neural network models, this type learns a mathematical function. Each output neuron is a nonlinear function of the weighted sum of the input values. In the training step, the weights are adjusted so that the input values will predict the output values.\n\nAs an example, a neural network could learn to predict whether today is a good day to play tennis based on the inputs of temperature, wind, precipitation, and humidity. The training data would use past decisions to play tennis (output=1) or not play (output=0), and the corresponding weather conditions. With only this data, a neural network could learn to classify future good days for tennis based on the weather. The learned weights for wind and precipitation might be large negative values since high wind speeds and precipitation make tennis unplayable.\n\n149",
      "content_length": 2897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "150\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nDeveloped by Google in 2013, the word2vec algorithm (https://en.wikipedia.org/wiki/ Word2vec) trained a model to predict the target word from the context of two words before and after that word. They also developed the opposite problem of predicting the surrounding context from a single word. Essentially, this is equivalent to the next word prediction using the presence of adjacent words rather than only preceding words. Backed with an efficient training algorithm, the results proved reasonably predictive. Of greater importance, the word2vec weights formed an embedding of the word or numeric vector representation that quantified the semantic and syntax func- tion of words. In other words, the word2vec model learns to associate every trained word with a numeric vector of numbers. These vectors can be compared, which is useful for comparing the similarity of word meanings.\n\nDeep learning Deep learning is a subset of neural networks that involves learning functions of functions, essen- tially. That is, the outputs of one layer form the inputs to a second layer, creating a deep stack of functions. The Universal Approximation Theorem proved that neural networks could learn to approximate any mathematical function, given a two-layer network under certain conditions. Research has found that creating deep neural networks with tens, hundreds, or thousands of layers is an efficient way of achieving accurate results. In each layer, the model adjusts its weights based on the training data to learn functions with the results passed as input to subsequent layers.\n\nIn 2017, a new deep learning architecture called the Transformer was published in a famous paper called Attention is All You Need from Google, the link to which can be found in the Further reading section. Rather than relying on the context of adjacent words, the Transformer architecture ef- ficiently learns the relationship between all words in a sentence. The training process learns the relationship between words using a masking process, through which it hides each word separately and learns to predict it from all the other words in each sentence.\n\nA deep learning network describes an architecture in machine learning with a specific number of inputs, outputs, layers, nodes per layer, inter-node connections, and weights throughout. Like the previously described linear model, the architecture describes the solution and is trained to solve a specific problem. LLMs are the result of training a deep learning network on large volumes of text. They learn to correctly predict the next word much better than statistical and word2vec models. Similar to the statistical next-letter prediction models, they can iteratively predict the next word to generate sentences, paragraphs, and full texts. Researchers have found that by training on ever larger volumes of text (or code), the accuracy continues to improve, so many LLMs have been developed that use increasingly larger network sizes and more training data.",
      "content_length": 3068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "Chapter 6\n\nLLMs by themselves offer significant capabilities in next-word prediction, but they are continuing to expand into new areas of intelligence. Many of the breakthroughs are now coming from various techniques that improve the performance of their respective LLMs. The next section describes some steps that users can take with an LLM to improve the outputs of LLMs.\n\nImproving LLM output performance If the LLM does not answer questions well, there are four general courses of action: improving prompt engineering, adding more data through the context window, providing examples (few- shot learning), fine-tuning, and retrieval-augmented generation (RAG). These will be discussed in this section.\n\nPrompt engineering The discipline of prompt engineering has recently emerged as a human interface to the LLM. Us- ers or programs can send commands to, ask questions of, and apply research reasoning through the LLM. The discipline involves applying specific wording to maximize the chance of correct outputs without producing erroneous or biased outputs, including hallucinations. The process of writing prompts is often an iterative process where it may require multiple rounds to achieve the desired output. Most of the prompts discussed in this book focus on a single task with specific and clear wording, which is considered good practice. The five S’s framework outlined in this book provides a useful general-purpose structured approach to writing the prompts. We’ll cover more on prompt engineering techniques in later chapters.\n\nAdding context As discussed in this chapter, LLMs are not databases that store data, nor are they an oracle of all knowledge (yet). They can perform a limited number of tasks and answer a finite number of questions without further enhancements. To enable question-answering, interactive chats, or other tasks such as code generation, the user can send the LLM both the question as well as additional information within a context window.\n\nNote\n\nThe context window is somewhat analogous to a human’s short-term memory in that it has a limited size and may forget older information. However, the context window is generally large enough to include documents, earlier inputs, and returned outputs. A document can be provided as context, and the prompt engineer can ask questions of the document.\n\n151",
      "content_length": 2339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "152\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nDepending on the size of the context window, all or part of earlier outputs from the system and past inputs are automatically included in the next prompts associated with that user, so an entire dialog or thought process is included in every interaction with the LLM. A short context window might only allow a few papers to be uploaded and summarized together, while a longer context window could enable a book to be uploaded with multiple interactive sessions kept as context. This approach has several advantages over an LLM system, which will be discussed next as ways of improving results.\n\nProviding examples The context window allows for documents and other data to be included along with the prompt or question, but it also allows for examples. A prompt may ask the LLM to return a particular nugget of information in a specific format. In many cases, this will be successful, and this is called zero-shot learning. That is, there are no examples of how the LLM should form a response—it just uses its baseline approach. However, the prompts can include one example of an input and an output as part of the prompt. This is called one-shot learning. If multiple examples are provided, this is called few-shot learning. While many examples can be used depending on the context window length, there is a law of diminishing returns. However, it is an effective approach to instruct the LLM to tailor its outputs to meet a specific need. We will discuss few-shot learning in depth in Chapter 8.\n\nFine-tuning Training an LLM from scratch may cost $100 million to learn the trillions of weights that are trained on prose, poetry, drama, literature, and science. Due to the prohibitive cost, it will not be trained often. In fact, it is typically two years out of date, which means it does not have any recent events or information. It also likely doesn’t have personal or corporate documents includ- ed in its training. One solution discussed is to provide these in a context window. As discussed in Chapter 2, for pay-for-service LLMs such as OpenAI, every transaction has two costs: a cost to send information to the LLM and a cost for the output produced, both based on the length of the information. As the amount of input information and the number of calls with the same information increase, it may become a costly solution.\n\nFine-tuning offers an alternative to repeatedly sending a large context to the LLM. It essentially adds a limited supplemental dataset to the LLM training in the form of documents or examples. The training fine-tunes the weights of the neural network by making minor adjustments that are",
      "content_length": 2694,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Chapter 6\n\nsufficient to produce improved outputs. Fine-tuning provides a cost-efficient way of improving outputs without repeatedly sending large contexts of data to the LLM. We will dive deeper into fine-tuning in Chapter 11.\n\nRetrieval-augmented generation RAG solves two major problems with the LLMs. First, OpenAI and ChatGPT are not databases of information, so they cannot retrieve information like a web search. Second, their training data is typically two years old and does not include personal or corporate documents. The RAG architecture integrates the LLM with external information from a web search, database, or set of documents. RAG uses the LLM to interpret the input from a user and then retrieves blocks of text from various sources that it finds to be closely related to the input. The LLM then processes the combination of the input and retrieved data to form an answer or response using the stan- dard LLM processing. Essentially, RAG fills the context window with candidate answers relevant to the question based on a search and then instructs the LLM to perform its usual summary or question-answering on that context window. This enables a full system with an LLM to perform web searches on current information or focus on specific documents when generating output.\n\nEach of the approaches described in this section is widely used to improve LLM results. While research continues to advance LLM capabilities, it is important to understand their limitations, which are discussed in the next section.\n\nWhat are the limitations of LLMs? With both the statistical approach for the letters and the LLM approach for words, there is no formal knowledge that many expect, given its ability to answer many questions. There is no extensive knowledge base of topics, common sense, or anything except the word patterns it has learned. What is surprising to researchers is the fact that the learned patterns and their relation- ships somehow enable LLMs to perform new tasks for which they have never been trained. For instance, LLMs have even shown some capability in some foreign languages that are not part of their training set, through a process called zero-shot cross-linguistic transfer. Even if the output has been trained, one may get slightly different results each time from a prompt due to the randomness in the system. Much of the research in the prompt engineering area focuses on how to receive consistent and correct outputs from LLMs by crafting the prompts with specific phrases, commands, and context.\n\n153",
      "content_length": 2537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "154\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nThe deep learning approach will almost always produce syntactically correct and plausible-looking output, but some of the output may be prone to hallucinations. There have been some famous cases where the output was completely incorrect, technically wrong, and sometimes just made up. A professor was accused of assault on a school trip that never happened. Many students have been caught using ChatGPT since none of their references existed. Even a US government health report was caught citing facts of a made-up reference. These incidents can be found in the Further reading section. Hallucinations often occur when there is insufficient training to support a particular output. The statistical methods do not have such an issue since they only sample from the previously seen patterns.\n\nIt is currently nearly impossible to know in which areas exactly the LLM has expertise, and in which areas they have limited training. A few years ago, the training of LLMs was generally re- leased as part of its research publication. In the past couple of years, much of the LLM training has become more secretive, so we do not know exactly which sources were used and cannot exactly predict their domains of capability. However, OpenAI has been trained extensively on both texts and code in multiple languages. OpenAI’s philosophy is to provide safe and beneficial general AI across many domains. ChatGPT is a specialization using an OpenAI LLM and is discussed in the next section.\n\nChatGPT Launched by OpenAI in November 2022, ChatGPT was developed on the GPT-3.5 model. It be- came a focal point of the AI growth since it worked through a conversational interface, unlike the programming interface of other LLMs. The dialog-driven interface requires it to track the history of the conversation, even across sessions. ChatGPT is a specialization of the LLM with improved conversational capability.\n\nOne of the key technology improvements was the use of reinforcement learning from human feedback (RLHF), which improved the dialog capability through training and feedback to give better responses. As discussed earlier, fine-tuning can improve the LLM’s performance for specific tasks. RLHF goes one step further and incorporates feedback from users to adjust the weights. As a result, it delivers results that incorporate human preferences and even reasoning capabilities in terms of responses and answers to questions.",
      "content_length": 2488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Chapter 6\n\nChatGPT continues to be a leader in the growth of AI, in part due to its easy web interface that makes it widely accessible with minimal training. It has led to greater research in natural lan- guage processing (NLP) and multiple use cases. Although it can write code, its capabilities are limited as compared to GitHub Copilot, which extends the GPT model for generative coding, as described in the next section.\n\nGitHub Copilot GitHub Copilot was officially announced as a collaboration between GitHub, Microsoft, and OpenAI, specifically for writing code, in 2021, and opened for technical review. At the time, it started with the GPT-3 instance from OpenAI, and the three companies collaboratively trained it to focus on code development, resulting in the OpenAI Codex. After GPT-4 was released, GitHub Copilot adopted it and released GitHub Copilot X. Its optimization focuses on code completion, fixing bugs, code comments, and tests based on billions of lines of code training beyond the GPT models. The combination of the underlying GPT model with the code-training focus enables the two-way natural language and computer language code writing and explanation.\n\nGitHub Copilot uses a combination of technologies to provide its coding experience. When writing code from a single file, it sends at least part of the current file through the internet to the Copilot server. It will also use the prompt and other open files that are open within their respective VS Code or PyCharm IDE for context. It uses this context to search for similar code using the RAG approach, and then sequentially generates code one token at a time. The amount of code sent to the Copilot server is based on the context length and certainly costs.\n\nIf part of your open code and possibly data is being sent to Copilot, it is natural to be concerned about security. The information is encrypted when sent to and from Copilot. The exchange context resides in memory but is deleted without being stored or used for training. The GitHub Copilot Trust Center page, available at https://copilot.github.trust.page/, provides more infor- mation about their data security, privacy, and related issues. It includes suggestions, feedback, prompts, and user engagement data. None of the code exchanged while using GitHub Copilot is used for training their system.\n\n155",
      "content_length": 2349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "156\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nSummary Statistical and AI approaches can learn patterns from sequential data such as text or code, which can efficiently predict the next symbol. Due to the range of possible words, deep learning methods scale well to efficiently predict the next words much better than statistical techniques and have become the standard. Even so, they require vast training sets of text and code and an expensive computational process to be effective.\n\nWhile LLMs are highly effective, they are not perfect and are limited by their training set of data and perhaps the time it was trained. The output can be limited for various domains and time pe- riods, which can result in hallucinations. While research continues in these areas, some solutions to improve results include effective prompt engineering, providing contextual information such as documents or examples, fine-tuning, and the RAG approach.\n\nOpenAI and ChatGPT are trained on general data and code, and are quite effective at generating code. GitHub Copilot is built upon OpenAI and, with significant additional training on code, is more efficient at generating code, tests, comments, and more.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: How is the deep learning approach for words similar to and different from the sta- tistical approach described for letters in this chapter?\n\nAnswer: Both the deep learning approaches and statistical approaches generate new text or code using a combination of their trained model and randomness. Even though they are both trained on data, they rely on randomness to select from the probable next words, which means their outputs can be different each time. In both cases, if they are trained with insufficient data, they may not produce satisfactory results. Neither has a conceptual understanding of the inputs or outputs, so both are relying on patterns of inputs to predict patterns of outputs.\n\nThe deep learning approach is orders of magnitude more efficient in predicting the range of possible next words or code using complex, less interpretable models. While both could easily predict the next letters with a large training set, the conditional probabilities would not be able to scale to predict next words as the deep learning approach can.",
      "content_length": 2398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Chapter 6\n\nQuestion 2: How is GitHub Copilot different from OpenAI in terms of code development?\n\nAnswer: OpenAI was optimized for general knowledge, and GitHub Copilot was optimized for code generation. GitHub Copilot took the human language capabilities of GPT-4 from OpenAI and further trained it on billions of lines of code, comments, and documentation. In this way, it serves as a pair programmer with the ability to generate code from text, explain code as text, write tests from code, and write comments from code.\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nZipf’s law: https://en.wikipedia.org/wiki/Zipf’s_law\n\nMikolov, Tomas; Chen, Kai; Corrado, Greg; Dean, Jeffrey (16 January 2013). Efficient Estima- tion of Word Representations in Vector Space: https://en.wikipedia.org/wiki/Word2vec\n\n\n\nVaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). Attention is All You Need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (ed.). 31st Conference on Neural Information Processing Systems (NIPS). Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc.: https://arxiv.\n\norg/abs/1706.03762\n\n\n\nPranav Dixit. US law professor claims ChatGPT falsely accused him of sexual assault, says ‘cited article was never written’. Business Today: https://www.businesstoday.in/technology/ news/story/openai-chatgpt-falsely-accuses-us-law-professor-of-sexual-\n\nharassment-376630-2023-04-08\n\n\n\nLoreben Tuquero. RFK Jr.’s health report shows how AI slips fake studies into research: https://\n\nwww.poynter.org/fact-checking/2025/rfk-jr-fake-citations-medical-journals/\n\nGitHub Copilot Trust Center page: https://copilot.github.trust.page/\n\n157",
      "content_length": 1878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "158\n\nBehind the Scenes: How ChatGPT, GitHub Copilot, and Other LLMs Work\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "7\n\nReading and Understanding Code Bases with GenAI\n\nWriting code is rarely an isolated task. We will typically have to incorporate our Python snippets into a larger code base that includes project files, often structured under a /src folder, along with dependencies listed in a requirements.txt file and other non-project files, such as a Dockerfile that defines container specifications.\n\nIn this chapter, we will work with a code base that computes the Manhattan distance between two matrices represented as pandas DataFrames. With the help of ChatGPT, GitHub Copilot, and OpenAI API, we will explore how various GenAI tools can support us in efficiently understanding both non-project files and the main business logic. We will also examine how generating calls with sample parameters can improve our understanding of more complex code.\n\nHere are the key topics covered in this chapter:\n\n\n\nReading and understanding the code base structure\n\nUsing LLMs to explain code\n\n\n\nInterpreting non-project files\n\n\n\nCreating calls with example parameters",
      "content_length": 1046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "160\n\nReading and Understanding Code Bases with GenAI\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/ Supercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nReading and understanding the code base structure So far in the book, we have mostly explored code snippets that are limited to a single Python file. In this chapter, we will shift our focus and see how GenAI tools can support us in reading and understanding a broader code base.\n\nGetting to know a code base usually involves reading through a variety of files. For instance, when a developer encounters a new project, they might do the following:\n\nExplore entry points such as main.py or app.py\n\nIdentify the main project files, often found in a /src directory\n\nLook for container and dependency management files such as Dockerfile and requirements.txt\n\nExperiment with specific parts of the code\n\nIn this book’s repository, we provide a small code base example located under ch7/. Here is a directory tree view of its contents:\n\nch7/\n\n├── app.py\n\n├── src/\n\n│ └── manhattan.py",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Chapter 7\n\n├── Dockerfile\n\n├── README.md\n\n├── requirements.txt\n\nIn this tree view, the core business logic that we will explore next is located in src/manhattan. py. In addition, we have the following:\n\napp.py, which serves as the entry point and handles basic Hypertext Transfer Protocol (HTTP) POST requests, routing them to the business logic. As introduced in Chapter 2, HTTP POST is a standard method for sending data to a server.\n\nDockerfile, a text file that outlines the steps and dependencies needed to build a container image for running the application. We will return to this later in the chapter.\n\nrequirements.txt, which lists the application’s dependencies, such as pandas and Flask, as in- troduced in Chapter 2.\n\nREADME.md, a markdown file commonly used to provide documentation about a code base. In our case, it includes examples of how to use the application.\n\nNext, we will introduce the Manhattan distance, which serves as the core business logic of our application.\n\nIntroducing the Manhattan distance In this chapter, our code focuses on calculating the Manhattan distance between two points. This distance is defined as the sum of the absolute differences along each dimension. In two-di- mensional space, we can relate this to classic computer games such as Pacman, where movement is restricted to vertical or horizontal directions, with no diagonal steps allowed. For example, consider Pacman and a ghost positioned in locations (1,1) and (3,2), respectively, in Figure 7.1.\n\nFigure 7.1: Pacman and ghost positions\n\n161",
      "content_length": 1546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "162\n\nReading and Understanding Code Bases with GenAI\n\nIn this figure, the characters can move either horizontally or vertically. The Manhattan distance between them is two units along the x axis and one unit along the y axis, which totals 3:\n\nManhattan\n\nDistance\n\n\n\n=  |3 − 1|  +  |2 − 1| = 3\n\nA simple Python implementation for calculating this distance might look like this:\n\ndef get_manhattan_distance(\n\nx1: Tuple[int, int],\n\nx2: Tuple[int, int],\n\n) -> int:\n\nx_dist: int = abs(x1[0] - x2[0])\n\ny_dist: int = abs(x1[1] - x2[1])\n\ndist: int = x_dist + y_dist\n\nreturn dist\n\nIn this implementation, we use two-dimensional vectors represented as Tuple[int, int] to represent a position on a board. However, the Manhattan distance can be generalized to higher dimensions computed between matrices of floats, as we will see next.\n\nManhattan distance for matrices The Manhattan distance, also referred to as the L1 norm, extends naturally to higher-dimensional data. It is widely used in image analysis when comparing two images and in machine learning research when evaluating prediction errors.\n\nTo compute the Manhattan distance between matrices, we can use pandas DataFrame objects. pandas is a standard library for data manipulation and should be listed in the book’s requirements. txt file and already installed in your virtual environment.\n\nHere is an example of a DataFrame with two rows, A and B, and three columns labeled a, b, and c:\n\nimport pandas as pd\n\ndf1: pd.DataFrame = pd.DataFrame(\n\ndata={\n\n\"A\": [1, 2, 3],\n\n\"B\": [4, 5, 6],\n\n},\n\nindex=[\"a\", \"b\", \"c\"],\n\n)",
      "content_length": 1567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "Chapter 7\n\nWe can implement the Manhattan distance using the pandas methods .abs() for absolute values and .sum() for summing along axes:\n\ndef get_manhattan_distance(\n\ndf1: pd.DataFrame,\n\ndf2: pd.DataFrame,\n\n) -> np.float64:\n\nelement_wise_dist: pd.DataFrame = (df1 - df2).abs()\n\ndist: float = element_wise_dist.sum().sum().astype(float)\n\nreturn dist\n\nThe distance calculation uses .sum() twice: first to sum across rows, and then again to sum the resulting column totals. We convert the result to a float to standardize the output, since summing over integer values may return a NumPy int64 data type.\n\nLet’s take these two DataFrames:\n\nA = [[1 3]\n\n[2 4]]\n\nB = [[5 7]\n\n[6 8]]\n\nCalling get_manhattan_distance on these will return the following:\n\n|5-1| + |6-2| + |7-3| + |8-4| = 16\n\nNow that we have a clear understanding of the core business logic and supporting code around the distance computation, we can explore how GitHub Copilot, ChatGPT, and OpenAI API can help us navigate and familiarize ourselves with the code base.\n\nUsing LLMs to explain code In Chapter 3, we noted that GitHub Copilot does not include the entire code base in the prompt. This is due to context window limitations, potential cost, and the fact that much of the content may not be relevant to the specific task.\n\nHowever, this does not prevent GenAI from helping us understand the code bases we are working with. Rather than relying blindly on Large Language Models (LLMs) to identify key components, we will explore how to request explanations for both simple elements and more complex scenarios.\n\n163",
      "content_length": 1579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "164\n\nReading and Understanding Code Bases with GenAI\n\nUsing the /explain command with GitHub Copilot GitHub Copilot includes several slash commands, among them the /explain command, which is available in both the inline chat and the chat window. Unlike code completion tasks, the output of this prompt is explanatory text covering arguments, data transformations, and return values.\n\nThe command can be used alone or with extra instructions. For example, in PyCharm, we can ask Copilot to explain the get_manhattan_distance function by placing the cursor anywhere within the function signature, opening the inline chat window, typing /explain, and hitting Enter, as shown in Figure 7.2:\n\nFigure 7.2: Using GitHub Copilot /explain for methods\n\nAnother available option is to select the contents of the function, open a chat window, and type /explain. Note that in VS Code, you will need to choose @workspace /explain by either typing it directly or selecting the /explain @workspace option in the drop-down menu after typing / explain, as shown in Figure 7.3:\n\nFigure 7.3: Choosing @workspace /explain in VS Code chat\n\nFor either IDE, Copilot is likely to yield a general explanation about the purpose of the function, the data transformation, and the returned value, as shown in Figure 7.4.",
      "content_length": 1290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Chapter 7\n\nFigure 7.4: Sample /explain output in VS Code\n\nIf we wanted a broader analysis of the arguments and their type, for example, we could use this prompt:\n\n/explain argument types\n\nThis should yield an extended explanation about the arguments, as shown in Figure 7.5.\n\nFigure 7.5: Extended /explain argument types with GitHub Copilot\n\n165",
      "content_length": 345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "166\n\nReading and Understanding Code Bases with GenAI\n\nQuick tip: Need to see a high-resolution version of this image? Open this book\n\nin the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nHere, we see a detailed explanation of the arguments df1 and df2, including a note that df2 is expected to have the same shape as df1 for the function to work properly. This expectation is not mentioned in the plain /explain prompt, but when we ask about a specific aspect of the code, we get a more in-depth explanation.\n\nNext, we will see how we can replicate this outcome through direct prompting with ChatGPT and OpenAI API.\n\nUsing ChatGPT to explain code While Copilot includes a built-in slash command for code explanations, we can apply the five S’s framework introduced in Chapter 4 to achieve similar results with ChatGPT, even when us- ing older or more cost-effective models such as GPT-4o. Unlike GitHub Copilot, which follows predefined instructions about what to explain based on the Python object, ChatGPT allows us to tailor the explanation to our specific needs.\n\nFor instance, the following prompt focuses on describing the function’s purpose, its arguments and types, the data flow, expected output, and possible edge cases:\n\nCONTEXT: you are provided with:\n\n1. A Python function enclosed with {{{ FUNCTION }}}\n\n2. Explanation points enclosed with {{ HEADERS }}}.\n\nTASK: Explain the function using the explanation points.\n\nFUNCTION: {{{ ...omitted for brevity ... }}}\n\nHEADERS: {{{\n\n1. Function's purpose",
      "content_length": 1765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Chapter 7\n\n2. Arguments and their types\n\n3. Step-by-step data flow\n\n4. Output and its types\n\n5. Potential edge cases\n\n}}}\n\nEXPLANATION:\n\nWhen using GPT-4o with this structure, we received an explanation that covered all bullet points with the headers, as partially shown in Figure 7.6:\n\nFigure 7.6: ChatGPT returned extended explanation\n\nFor reference, here is the output generated by ChatGPT under the edge cases header, which adds value by uncovering situations where the function might fail or produce unexpected behavior:\n\n5. Edge cases\n\nShape mismatch: If df1 and df2 do not have the same shape, the subtraction operation will raise a ValueError.\n\nNon-numeric columns: If the DataFrames contain non-numeric data, the subtraction operation will raise a TypeError or produce NaNs that affect the result.\n\nMissing values (NaNs): If either DataFrame contains NaNs, the output\n\n167",
      "content_length": 881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "168\n\nReading and Understanding Code Bases with GenAI\n\ndistance may be NaN unless explicitly handled beforehand.\n\nEmpty DataFrames: If both inputs are empty DataFrames, the result will be 0.0.\n\nAlthough writing a prompt for ChatGPT might take slightly more effort compared to using the /explain command, it can be worthwhile when we need more targeted explanations, such as uncovering edge cases.\n\nNext, we will demonstrate how to achieve this using OpenAI API, which may be more appropri- ate for scenarios where the repository becomes larger and we want to generate explanations for many functions programmatically.\n\nUsing OpenAI API to explain code Using the same prompt structure from ChatGPT, we can make a programmatic call to OpenAI, following the best practices introduced in Chapter 5.\n\nWe will define the same system prompt components that mirror our ChatGPT prompt:\n\nSURROUND = \"\"\"... omitted for brevity ...\"\"\"\n\nSINGLE_TASK = \"... omitted for brevity...\"\n\nHEADERS = \"\"\"...omitted for brevity\"\"\"\n\nWe will add a get_user_prompt function that merges the HEADERS prompt piece with the source code of the function:\n\ndef get_user_prompt(func: callable) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nHEADERS: {{{{{{ {POINTS} }}}}}}\n\nEXPLANATION:\n\n\"\"\"\n\nThen, we can call OpenAI API using the gpt-4o-mini model:\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(get_manhattan_distance)\n\ncompletion: ChatCompletion = client.chat.completions.create(",
      "content_length": 1557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Chapter 7\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n],\n\n)\n\nprint(\"Explanation:\", completion.choices[0].message.content)\n\nJust as with ChatGPT, we will receive a detailed explanation of the function, covering all points specified. For reference, here is a sample output from running this script:\n\nFunction's purpose: < ... omitted for brevity ... >\n\nArguments and their types: The function takes in two arguments:\n\ndf1: a pandas DataFrame, which represents the first set of data.\n\ndf2: a pandas DataFrame, which represents the second set of data. Both dataframes are expected to have the same shape and structure for the calculation to be valid. Step-by-step data flow: < ... omitted for brevity ... >\n\nPotential edge cases:\n\nIf df1 and df2 have different shapes, the function may raise a value error due to the inability to perform element-wise operations on differently-sized frames.\n\nIf either of the DataFrames is empty, the function will return a distance of 0.0 as there are no elements to compute differences from.\n\nHandling of NaN (Not a Number) values might need to be considered, as this could affect the calculations when present in either of the DataFrames.\n\nThe output here also provides an overview of the edge cases. In later chapters of the book, we will explore how to make use of this when designing a test suite for functions.\n\nNext, we will explore how to extend the use of the /explain command beyond .py files using various GenAI tools.\n\nExplaining non-project files The /explain command is not limited to .py files or core business logic. For instance, in our Manhattan distance code base, we can use Copilot to interpret an entire non-project file or a specific section of it.\n\n169",
      "content_length": 1787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "170\n\nReading and Understanding Code Bases with GenAI\n\nExplaining an entire file There are times when we may want a summary of a complete non-project file. For example, con- sider the requirements.txt file in our project:\n\npandas==2.2.3\n\nflask==3.1.0\n\nTo get a general explanation, we can highlight the entire content with the cursor, then open the inline chat or chat window and enter /explain. Figure 7.7 shows an example of this using the chat window in VS Code:\n\nFigure 7.7: Chat window explanation of a full file highlighted content\n\nIn this case, Copilot accurately recognized that requirements.txt lists the dependencies needed for the code base to run properly. For common non-project files, we can generally expect reliable explanations since LLMs have encountered many similar examples during training.",
      "content_length": 811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Chapter 7\n\nNext, we will look at how to ask for a more detailed explanation of a specific line within a non-proj- ect file by providing the right context.\n\nExplaining with context While it is useful to understand a file as a whole, there are often specific lines that may be unclear and require a deeper explanation.\n\nTake the Dockerfile from our Manhattan distance project as an example. Docker is a widely used tool among software developers for packaging applications in a consistent and reproducible way. A key element in this process is the Dockerfile, a plain text file typically placed at the project root that outlines the environment setup. This includes specifying a base image (such as a Python environment), listing dependencies (like the pandas library), and providing commands to build and run the container (such as opening a port to receive HTTP POST requests).\n\nBelow is the Dockerfile we used for our Manhattan distance application:\n\nFROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"python\", \"app.py\"]\n\n171",
      "content_length": 1103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "172\n\nReading and Understanding Code Bases with GenAI\n\nCopilot can provide an explanation for the entire file, but there are times when we want to focus on a specific line. For instance, we may want to understand the meaning of EXPOSE 5000. Although it might seem like this command publishes port 5000 for incoming requests, it serves as docu- mentation only. The application will still need to explicitly expose this port when launching the container, using a shell command such as docker run -p 5000:5000. When we highlight just this line and request an explanation, Copilot incorporates context from nearby lines, the filename and extension, and other signals to generate a relevant and accurate response, as shown in Figure 7.8:\n\nFigure 7.8: Copilot explanation about exposed port from the Dockerfile\n\nCopilot correctly identified that the port specification indicates the port on which the application is listening. For common files such as Dockerfile, the plain /explain command usually provides a sufficient one-line explanation.\n\nNext, we will explore how to get similar explanations in non-project files working with ChatGPT.\n\nExplaining non-project files with ChatGPT When seeking an explanation for a specific line within a non-project file or the entire file itself, the best practice is to provide the full content of the file along with the line in question and the standard filename and extension. In the case of a Dockerfile, there is no extension (such as .txt or .py), so simply specifying Dockerfile is sufficient.",
      "content_length": 1532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Chapter 7\n\nFor instance, if we want to understand the role of an exposed port in a Dockerfile, we should include both the entire file contents and the specific line to get a more accurate explanation, as demonstrated in the following prompt:\n\nCONTEXT: you are provided with:\n\n1. A Dockerfile enclosed with {{{ DOCKERFILE }}}\n\n2. A line from the file enclosed with {{{ LINE }}}.\n\nTASK: Explain the purpose of the line.\n\nDOCKERFILE: {{{ ...omitted for brevity... }}}\n\nLINE: {{{EXPOSE 5000}}}\n\nEXPLANATION:\n\nThis prompt effectively identifies the function of the line in the Dockerfile, as shown in Figure 7.9:\n\nFigure 7.9: Expose 5000 line explanation in ChatGPT\n\nIn general, when using ChatGPT to explain a line of code or a line from a non-project file, it is helpful to include a broader context than just the target line. For example, when explaining a method within a class, include the entire class definition. If a function uses a custom argument type, include the definition of that type as well. This leads to more precise and useful explana- tions. Later in the chapter, we will introduce debugging strategies for complex scenarios where a simple textual explanation does not provide sufficient insight into the purpose of a code block.\n\nNext, we will explore how to use a similar approach when working with OpenAI API.\n\n173",
      "content_length": 1332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "174\n\nReading and Understanding Code Bases with GenAI\n\nExplaining non-project files with OpenAI API Similar to ChatGPT, we can use OpenAI API to explain specific lines from non-project files. Let us revisit the Dockerfile example and apply a comparable system prompt:\n\nSURROUND = \"\"\"you are provided with:\n\n1. A Dockerfile enclosed with {{{ DOCKERFILE }}}\n\n2. A line from the file enclosed with {{{ LINE }}}.\"\"\"\n\nSINGLE_TASK = \"Your task is to explain the purpose of the line.\"\n\nUnlike earlier examples where we passed a function’s source code to OpenAI API, this time we want to send the entire Dockerfile content. We can do this by using Python’s built-in open function inside the get_user_prompt function:\n\ndef get_user_prompt(path: str, line: str) -> str:\n\nwith open(path, 'r') as file:\n\ndockerfile_content = file.read()\n\nreturn f\"\"\"\n\nDOCKERFILE: {{{{{{ {dockerfile_content} }}}}}}\n\nLINE: {{{{{{ {line} }}}}}}\n\nEXPLANATION:\n\n\"\"\"\n\nWe can then call OpenAI API using the following code:\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(\n\n'../../ch7/Dockerfile', 'EXPOSE 5000')\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},",
      "content_length": 1344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "Chapter 7\n\n],\n\n)\n\nprint(\"Explanation:\", completion.choices[0].message.content)\n\nFor reference, running this script produced the expected result, identifying the line’s purpose as specifying the port to be exposed. Here is an example output:\n\nExplanation: The line `EXPOSE 5000` in the Dockerfile serves to inform Docker that the application running inside the container will be listening for incoming network connections on port 5000.\n\nWhen a container is created from this Docker image, port 5000 will be exposed for external communications, making it possible for users or other applications to connect to the service provided by the application running in the container. However, it is important to note that this command does not actually publish the port; to make the port accessible from outside the Docker environment, the user must explicitly map it to a port on the host machine when running the container, typically using the `-p` option with the `docker run` command.\n\nIn summary, `EXPOSE 5000` is a documentation feature that indicates the intended service port and enhances the understandability of the Docker container's purpose.\n\nTo conclude, both ChatGPT and OpenAI API can deliver accurate results even when using smaller models such as GPT-4o, if we supply the necessary context, including the filename and extension if it exists, its contents, and the specific line we want explained.\n\nGitHub Copilot is best suited for quick, informal explanations. For more control over the prompt and instructions, ChatGPT is a better fit. When a programmatic solution is required, OpenAI API becomes the preferred choice.\n\nNext, we will explore how to harness the power of GenAI to handle more advanced cases where a simple explanation is not enough, and interaction with the object is required for deeper un- derstanding.\n\nCreating calls with example parameters As our Python objects or non-project files become more complex, a straightforward explanation might not be enough to help us understand the code in front of us. This is especially true when the Python object is part of a class, lacks type hints in its signature, contains no docstring, and strays from best practices.\n\n175",
      "content_length": 2191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "176\n\nReading and Understanding Code Bases with GenAI\n\nTake, for instance, the private method _constructor_from_mgr in the widely used pandas DataFrame class. This method constructs a DataFrame from a set of data blocks and their as- sociated axes. For your reference, here is the source code, which lacks argument type hints, a docstring, or any reference to tests:\n\ndef _constructor_from_mgr(self, mgr, axes) -> DataFrame:\n\ndf = DataFrame._from_mgr(mgr, axes=axes)\n\nif type(self) is DataFrame:\n\n# This would also work `if self._constructor is DataFrame`, but\n\n# this check is slightly faster, benefiting the most-common case.\n\nreturn df\n\nelif type(self).__name__ == \"GeoDataFrame\":\n\n# Shim until geopandas can override their _constructor_from_mgr\n\n# bc they have different behavior for Managers than for DataFrames\n\nreturn self._constructor(mgr)\n\n# We assume that the subclass __init__ knows how to handle a\n\n# pd.DataFrame object.\n\nreturn self._constructor(df)\n\nOne issue here is the naming of the first argument. Although it is called manager, a name typically associated with the _mgr property of a DataFrame, it refers to a block of data, which is essentially the cell content without axis labels. This misleading name, combined with the absence of type hints, a docstring, or proper testing, makes the code block hard to understand for both humans and GenAI applications.\n\nFor reference, here is the explanation output from Copilot when prompting /explain from a PyCharm inline chat:\n\nThe _constructor_from_mgr method is a helper function in the DataFrame class. It is used to create a new DataFrame instance from a given BlockManager (or ArrayManager) and its associated axes.\n\nKey Points:\n\n1. Parameters:",
      "content_length": 1712,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "Chapter 7\n\nmgr: The BlockManager or ArrayManager that holds the actual data for the DataFrame.\n\naxes: The axes (index and columns) for the new DataFrame.\n\n2. Return Value:\n\nIt returns a new DataFrame object constructed from the provided manager and axes.\n\nExample Snippet:\n\ndef _constructor_from_mgr(self, mgr, axes) -> DataFrame:\n\ndf = DataFrame._from_mgr(mgr, axes=axes)\n\nHere, _from_mgr is a class method that initializes a DataFrame from the given manager and axes. This method ensures that the new DataFrame is properly aligned with the provided data structure.\n\nThis output includes all the elements we are looking for, such as arguments, the return value, and some level of data transformation. However, deciphering the explanation can be just as challenging as deciphering the code itself. We propose a better approach by debugging the code using the generated content.\n\nDebugging with GitHub Copilot Copilot is built for code generation, which makes it useful for interacting with our Python code. If we take the get_manhattan_distance function as an example, prompting Copilot with a partial call to the function can result in a completion that includes example arguments.\n\nWe can do this by writing the function call directly:\n\nget_manhattan_distance(\n\nOr, we can do it by assigning the result to a variable with a matching type hint for the function’s return value:\n\nd: float = get_manhattan_distance(\n\n177",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "178\n\nReading and Understanding Code Bases with GenAI\n\nIn both cases, Copilot is expected to generate two DataFrame arguments for the function. An example of this kind of completion in PyCharm is shown in Figure 7.10:\n\nFigure 7.10: Sample call completed by GitHub Copilot\n\nKeep in mind that if your IDE automatically inserts a closing bracket, ), after you type the opening one, (, you may need to press Enter to activate Copilot’s suggestion.\n\nThis generated completion can serve as an entry point for interacting with the function through the debugger. In PyCharm, we can initiate debug mode by right-clicking the filename and selecting Debug, as illustrated in Figure 7.11:\n\nFigure 7.11: Debug mode in PyCharm\n\nIn VS Code, debugging is done by clicking the play icon in the top-right corner and selecting Python Debugger: Debug Python File, as shown in Figure 7.12:",
      "content_length": 867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "Chapter 7\n\nFigure 7.12: Running the Debugger in VS Code\n\nRunning the file in debug mode allows us to step through the code line by line, offering a clearer under- standing of its internal workings. For example, when using PyCharm’s debugger, we can inspect the variables within the function and follow the data transformation as it happens, as seen in Figure 7.13:\n\nFigure 7.13: Debug mode in PyCharm showing internal function arguments\n\nThis gives us visibility into the intermediate values within get_manhattan_distance.\n\nThe authors frequently rely on debugging to interact with the code they read and write. This hands-on approach supports faster development of intended functionality and offers deeper insights into complex code logic. This far exceeds reading source code, using GenAI explanations, or consulting docstrings.\n\nNext, we will explore how generating sample calls can help us better understand complex code blocks, such as the _constructor_from_mgr method in the pandas DataFrame class.\n\n179",
      "content_length": 1009,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "180\n\nReading and Understanding Code Bases with GenAI\n\nDebugging dependency code Just like with the get_manhattan_distance case, we can use Copilot to help generate sample code for debugging the _constructor_from_mgr method. Since this is a class method, we will split our prompt into two parts. The first prompt generates a sample instance of the class, and the second invokes the method.\n\nWe will begin by asking Copilot to complete a DataFrame instance. A sample prompt may look like this:\n\ncoordinates_a : pd.DataFrame =\n\nThis prompt offers sufficient context for Copilot to suggest an assignment. As shown in Figure 7.14, Copilot begins by proposing an initial completion for a pd.DataFrame.\n\nFigure 7.14: Initial completion in PyCharm\n\nWhen we accept the suggestion, Copilot typically proposes sample data that fits the DataFrame’s name, as illustrated in Figure 7.15:\n\nFigure 7.15: Chained completion in PyCharm\n\nAt this point, we can either trigger additional columns by accepting chained suggestions or type } to flag to Copilot that we do not want any additional columns. We then move on to the method call with the following prompt:\n\ncoordinates_a._constructor_from_mgr(\n\nNote that your IDE may automatically complete the closing bracket, ). In that case, press Enter between the brackets to let Copilot generate a completion suggestion.",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "Chapter 7\n\nFor example, in Figure 7.16, Copilot proposes using the coordinates_a._mgr and coordinates_a. axes properties as the calling parameters.\n\nFigure 7.16: Parameter completion\n\nWith the two proposed parameters, we can debug the method as shown in Figure 7.17 and gain a better understanding of its behavior.\n\nFigure 7.17: Debugging _constructor_from_mgr using Copilot’s suggestion\n\nTo conclude, Copilot can supercharge the process of debugging sections of the code base, allowing us to better understand both simple and complex implementations. We rely heavily on debugging as a daily practice to deepen their understanding of the code they interact with.\n\nNext, we will explore how ChatGPT and OpenAI API offer distinct characteristics that we can utilize.\n\n181",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "182\n\nReading and Understanding Code Bases with GenAI\n\nCustomizing call parameters with ChatGPT and OpenAI GitHub Copilot is generally better suited for quickly generating debugging calls to trigger Python objects, as it can access relevant context, such as the class code, when we are working with a method. For cases where we want specific characteristics of the sample parameters, prompting ChatGPT and OpenAI may be more suitable.\n\nFor example, to generate suitable arguments for debugging the get_manhattan_distance function, we might want five rows and three columns, and can use the following prompt:\n\nCONTEXT: you are provided with:\n\n1. A Python function enclosed with {{{ FUNCTION }}}\n\n2. Arguments requirements enclosed with {{ REQUIREMENTS }}}.\n\nTASK: Call the function with the data requirements\n\nFUNCTION: {{{ ... omitted for brevity ... }}}\n\nREQUIREMENTS: {{{\n\n1. 5 rows\n\n2. 3 columns\n\n3. floats and integers in cells\n\n}}}\n\nCALL:\n\nFor reference, the output is shown in Figure 7.18:\n\nFigure 7.18: Call parameters for get_manhattan_distance",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Chapter 7\n\nIn this example, both DataFrames, df1 and df2, have the same shape of five rows and three col- umns, which makes them appropriate arguments for the function call.\n\nThe OpenAI API call is similar, and you can find the full script at ch7/prompts/openai_debug. py for later reference.\n\nTo conclude, we turn to ChatGPT and OpenAI API when our data requirements are beyond trivial for a quick completion that Copilot might otherwise handle well. Later in the book, we will explore how to build on these data generation methods to create a robust test suite.\n\nDebugging dependency code When using ChatGPT or OpenAI API to generate test code for a dependency package, it is import- ant to provide enough context for the model to produce a desirable call. The amount of context required depends on how widely the library is used. For standard packages such as pandas, in- cluding just the relevant method may be enough. For custom or private repository classes, it is usually best to include the entire class definition.\n\nFor example, the DataFrame class in pandas is about 12,000 lines long, which is too much to include in a prompt. However, because pandas is highly popular, we can provide only the _constructor_ from_mgr method and still receive a relevant response.\n\nBelow is a sample prompt used with ChatGPT to generate a call to the _constructor_from_mgr method:\n\nCONTEXT: Given Pandas DataFrame 2.2.3 class, you are provided with:\n\n1. Pandas DataFrame method to be called enclosed with {{{ FUNCTION }}}\n\n2. Arguments requirements enclosed with {{ REQUIREMENTS }}}.\n\nTASK: Generate a method call with sample parameters that meet the data requirements.\n\nFUNCTION: {{{ ... omitted for brevity ... }}}\n\nREQUIREMENTS: {{{\n\n1. 5 rows\n\n2. 3 columns\n\n3. floats and integers in cells\n\n}}}\n\nCALL:\n\n183",
      "content_length": 1803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "184\n\nReading and Understanding Code Bases with GenAI\n\nFigure 7.19 shows an example output from GPT-4o in response to this prompt.\n\nFigure 7.19: Sample test call to _constructor_from_mgr\n\nHere, ChatGPT successfully generated a method call using df._mgr and df.axes, producing a DataFrame with the required dimensions.\n\nSince OpenAI API follows a similar pattern, we have provided the full script in ch7/prompts/ openai_debug_pandas.py for reference.\n\nIn later chapters, we will build on this approach to supercharge the generation of robust test suites.\n\nPutting it all together Now that we have reviewed different approaches to shorter objects, explored tailored\n\nexplanations for Python objects and non-project files, considered programmatic concerns,\n\nand looked at scenarios where a textual explanation is not sufficient, we can summarize\n\neverything in a pros and cons table for each of the tools:",
      "content_length": 901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Chapter 7\n\nGenAI app\n\nPros\n\nCons\n\nGitHub Copilot\n\nProvides quick responses with relevant context. It supports the use of /explain as is and allows us to add additional requirements.\n\nDoes not support tailored explanations, such as when we need to include edge cases for get_manhattan_distance.\n\nChatGPT\n\nEnables tailored explanations and gives us flexibility to add or remove context.\n\nRequires more effort to craft prompts.\n\nOpenAI API\n\nSuitable for programmatic solutions and automation.\n\nSignificantly more effort compared to GitHub Copilot and ChatGPT.\n\nIn our own work, we typically begin with Copilot when we need an explanation. As our require- ments expand, we move to Copilot’s chat window, ChatGPT, or OpenAI API. When dealing with more complex objects, we always recommend incorporating debugging into the reading process to better understand the code.\n\nSummary In this chapter, we expanded our focus beyond .py files and explored how GenAI can help us un- derstand entire code bases. Using GitHub Copilot, we applied the /explain command in its plain form as well as with more tailored instructions to grasp the role of the get_manhattan_distance function. We also applied this command to understand non-project files such as requirements. txt and Dockerfile.\n\nNext, we used ChatGPT to craft prompts that offered useful explanations about edge cases and argument types. We then moved on to OpenAI API to generate a script that programmatically provided multiple explanations with one call.\n\nWe concluded by generating sample values that are useful for interacting with our code through the debugger. This approach is especially helpful when dealing with more complex implemen- tations.\n\nIn the next chapter, we will dive into prompt engineering techniques to improve the quality of our generated code. Through few-shot learning, also known as in-context learning, we will guide Copilot, ChatGPT, and OpenAI API to follow a specific style guide aligned with our production standards. We will also explore additional techniques to shape prompts for better outcomes.\n\n185",
      "content_length": 2080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "186\n\nReading and Understanding Code Bases with GenAI\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: How can we address explanations of complex Python objects?\n\nAnswer: In cases where the object is relatively simple, we can utilize the GitHub Copilot /explain slash command to help us better grasp the arguments, data transformation, and return values. For more complex objects, we can use debugging by generating calls to the object with GitHub Copilot, ChatGPT, and OpenAI API.\n\nQuestion 2: When asking ChatGPT to explain a single line in a Dockerfile, what context should be added?\n\nAnswer: It is recommended to include the full contents of the Dockerfile along with the filename and extension, and the specific line in question. This gives ChatGPT the context needed for a more accurate explanation.\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\nDockerfile, Wikipedia: https://realpython.com/primer-on-python-decorators/\n\n\n\nPyCharm debugging: https://www.jetbrains.com/help/pycharm/part-1-debugging- python-code.html#step\n\n\n\nVS Code debugging: https://code.visualstudio.com/docs/python/debugging\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "8\n\nAn Introduction to Prompt Engineering\n\nIn this chapter, we will introduce the practice of prompt engineering, a field of study that started in 2020, which offers advanced techniques for refining prompts to achieve more reliable, predict- able, and desirable outcomes in large language model (LLM)-based applications.\n\nWe will focus on an effective prompt engineering technique for code generation with GenAI: few-shot learning, also known as in-context learning. This approach, when used in the context of code-related tasks, guides the model toward producing outputs that align with a specific coding style or objective.\n\nWe will focus on how the few-shot learning technique can enhance the model’s output for more accurate and style-consistent code generation. While there are many different uses of few-shot learning in code-related tasks, the focus on style is a useful example for maintaining consisten- cy in a repository. This includes enforcing type hints, adding docstrings, or following a specific logging convention. By aligning the generated code with the required style guide, we can reduce the need for manual customization and accept the GenAI output as-is.\n\nIn this chapter, we will cover the following topics:\n\nUtilizing prompt engineering for coding\n\n\n\nEnhancing prompts with few-shot examples\n\n\n\nLeveraging few-shot learning with ChatGPT",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "188\n\nAn Introduction to Prompt Engineering\n\nWorking with few-shot learning for OpenAI API\n\n\n\nCrafting a style guide for GitHub Copilot\n\n\n\nIntroducing more prompt engineering techniques\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, which includes the prompt samples and style guides fea- tured in this chapter, available at https://github.com/PacktPublishing/Supercharged-\n\nCoding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nUtilizing prompt engineering for coding In Chapter 4, we explored the three pillars of achieving quality output: model mastery, evaluation metrics, and precise prompts. We also discussed how following the five S’s best practices for prompts (structured, surrounded, single-tasked, specific, and short) can significantly enhance the quality of model output. Using OpenAI’s example of an effective prompt, we demonstrated how aligning with these principles, such as focusing exclusively on error fixes and providing a clear list of issues to address, could improve results.\n\nAs tasks grow more complex, advanced techniques are essential to guide models toward achieving desired outcomes. LLMs may need additional instructions to adhere to a specific style guide, pass a unit test suite, or fix reproducibility issues.",
      "content_length": 1758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Chapter 8\n\nSince the advent of LLMs in 2020, prompt engineering has developed into a practice that refines and structures prompts to achieve better results and address more complex scenarios. There are now many strategies that guide the model toward more desirable outcomes, including the following:\n\n\n\nStructuring prompts in a way that aligns with LLM capabilities\n\n\n\nProviding additional context to clarify the problem\n\n\n\nIncluding specific guidelines for the desired output\n\nGuiding the model through a reasoning process\n\nDefining constraints and boundaries for the expected output\n\n\n\nProviding external feedback to iteratively refine results\n\nIn coding-related tasks, prompt engineering is valuable for a variety of applications including creating style guides for code refactoring, breaking down complex challenges into manageable steps, and generating production-standard code. We will explore in depth a range of prompt engineering techniques designed to supercharge various stages of the software development life cycle (SDLC) throughout this book.\n\nNext, we will take an in-depth look at the few-shot learning technique, which incorporates spe- cific guidelines for desired outputs, and examine its impact on shaping the coding style of the model’s suggestions. Later in the chapter, we will preview other prompt engineering techniques that will be explored in detail in later chapters.\n\nEnhancing prompts with few-shot examples Few-shot learning, also known as in-context learning, is a prompt engineering technique where we include a few examples of desirable outputs directly within the prompt. This method can be extended to incorporate examples of incorrect output or edge cases, enabling the model to differ- entiate between what constitutes a good result and what does not. By analyzing these patterns, the model can infer how to structure an optimal response for the given task.\n\nTo delve deeper into the research behind this approach, refer to Language Models are Few-Shot Learners by Brown et al. (https://arxiv.org/abs/2005.14165v4).\n\n189",
      "content_length": 2058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "190\n\nAn Introduction to Prompt Engineering\n\nFew-shot learning research demonstrates how, with a few examples (or shots) of input-output pairs, a model can infer patterns from the relationships between these examples and complete tasks in a style consistent with the provided samples. For instance, the following prompt, taken from the study, illustrates the predictability and consistency of an LLM when solving arithmetic problems. Utilizing few-shot prompting increases the predictability of the output, as the model is likely to generate responses that align with the style of the provided few-shot examples as shown here:\n\nInstruction: Solve the following arithmetic problems.\n\nExamples:\n\nProblem: What is 5 + 3?\n\nAnswer: 8\n\nProblem: What is 12 - 4?\n\nAnswer: 8\n\nProblem: What is 7 × 6?\n\nAnswer: 42\n\nProblem: What is 15 ÷ 3?\n\nAnswer:\n\nThe model’s output was as follows:\n\n5\n\nCompared to prompts that only specify the task (Solve the following arithmetic problem), this example-based approach reduces ambiguity. The model aligns its output more closely to the expected structure, providing only the number as the answer, without additional explanation or formatting inconsistencies.\n\nFew-shot prompting is a useful technique for a variety of coding-related tasks, such as code re- factoring, code analysis, and code generation. By including specific examples in the prompt, we can guide the model to produce outputs that align with our desired patterns.\n\nFew-shot learning for code refactoring In a code refactoring scenario, we may want to transform print statements into structured logger messages, a common practice for maintaining production-level software. Replacing the variety of print statements across many files is both error-prone and tedious. However, we can use an LLM with a prompt with clear instructions and a few examples. For example, we can enrich a request to refactor the following print message:\n\nprint('Error! File not found: passwords.txt')",
      "content_length": 1965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Chapter 8\n\nWe can refactor it to a log message, with a few examples:\n\nOld: print('Process started for config.txt')\n\nRefactored: logger.info('Processing started', extra={'stage': 'start', file: 'config.txt'})\n\nOld: print('Warning! Could not load user data from user_info.csv')\n\nRefactored: logger.warning('User data failed to load', extra={'module': 'user_loader', 'status': 'failure', file: 'user_info.csv'})\n\nOld: print('Error! File not found: passwords.txt')\n\nRefactored:\n\nQuick tip: Enhance your coding experience with the AI Code Explainer and Quick Copy features. Open this book in the next-gen Packt Reader. Click the Copy button\n\n(1) to quickly copy code into your coding environment, or click the Explain button\n\n(2) to get the AI assistant to explain a block of code to you.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR visit packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\n191",
      "content_length": 1033,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "192\n\nAn Introduction to Prompt Engineering\n\nThe examples in this prompt highlight the distinctions between different logging levels with ex- amples of transforming print messages into info and warning logging messages. The examples also demonstrate static log messages with additional information provided through the extra parameter.\n\nNext, we will see how enhancing our prompt with few-shot examples improves the output we get from ChatGPT.\n\nLeveraging few-shot prompting with ChatGPT When using ChatGPT, if applying the five S’s framework from Chapter 4 does not produce the desired results, we can enhance our prompts with a few-shot approach. In this case, we will still follow the five S’s framework but incorporate indexed variables that represent a series of inputs and outputs for the task.\n\nFew-shot structure A typical approach to implement few-shot prompting in ChatGPT is by extending the five S’s framework to include indexed variables such as {{{ Input_i }}} and {{{ Output_i }}}. This implies that the context references these variables, for example:\n\nCONTEXT: You are provided with:\n\n1. Python snippet enclosed with {{{ OLD }}}\n\n2. Examples enclosed with {{{ OLD_i }}} followed by a desirable output enclosed with {{{ REFACTORED_i }}}\n\nWe then supply the OLD_i and REFACTORED_i examples, followed by {{{ OLD }}}, as such:\n\nOLD_1: {{{ }}}\n\nREFACTORED_1: {{{ }}}\n\nOLD_2: {{{ }}}\n\nREFACTORED_2: {{{ }}}\n\n...\n\nOLD: {{{ }}}\n\nOverall, a ChatGPT prompt that uses few-shot examples can be structured in this way:\n\nCONTEXT: You are provided with:\n\n1. ...\n\n2. Examples enclosed with {{{ Input_i }}} followed by a desirable output",
      "content_length": 1636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Chapter 8\n\nenclosed with {{{ Output_i }}}\n\nTASK: ...\n\nInput_1: {{{ ... }}}\n\nOutput_1: {{{ ... }}}\n\nInput_2: {{{ ... }}}\n\nOutput_2: {{{ ... }}}\n\n...\n\nInput: {{{ }}}\n\nCUE:\n\nThe labels for inputs and outputs should reflect their relationship and remain consistent across examples. For example, when solving arithmetic problems, question and answer are appropriate labels. When transforming print statements to log messages or performing other refactoring tasks, old and refactored make more sense.\n\nImprovement in output with code refactoring Let us examine a case where we want to refactor print statements into structured logging. This in- volves using appropriate logging levels, enforcing static messages, and including extra parameters.\n\nTo observe the default behavior without applying a few-shot approach, we can navigate to https://chatgpt.com/, start a new conversation with the GPT-o3-mini-high model, and prompt it to refactor a print statement into a log message using the following:\n\nCONTEXT: You are provided with a Python snippet enclosed with {{{ OLD }}}.\n\nTASK: Refactor the snippet to a log message.\n\nOLD: {{{ print('Error! File not found: passwords.txt') }}}\n\nREFACTORED:\n\nThe GPT-o3-mini-high model excels at coding tasks and is likely to generate a log message with an error level. However, without explicit instructions on structuring the log message, it may not use a logger instance nor distinguish the static log message from additional details such as the filename, as shown in Figure 8.1:\n\n193",
      "content_length": 1517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "194\n\nAn Introduction to Prompt Engineering\n\nFigure 8.1: Working with Canvas in ChatGPT\n\nWe can improve the model’s output by providing few-shot examples demonstrating how to ex- tract key details from a print statement and use a logger instance. For example, we can refactor a print message to include a filename and verbosity setting:\n\nprint('Process started for config.txt with verbose=True')\n\nWe can refactor it into a structured log message:\n\nlogger.info('Processing started', extra={'verbose': True, file: 'config. txt'})\n\nThis example highlights the extraction of essential details to improve logging clarity. Similarly, we can add another example with print statements containing user and file path information:\n\nprint('Warning! Could not load user U-232 data from user_info.csv')\n\nWe can refactor it into the following structured log message:\n\nlogger.warning('User data failed to load', extra={'user': 'U-232', 'file': 'user_info.csv'})\n\nA complete prompt with these few-shot examples might be structured as follows:\n\nCONTEXT: You are provided with:\n\n1. Python snippet enclosed with {{{ OLD }}}\n\n2. Examples enclosed with {{{ OLD_i }}} followed by a desirable output enclosed with {{{ REFACTORED_i }}}",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Chapter 8\n\nTASK: Refactor the snippet to a log message.\n\nOLD_1: {{{ print('Process started for config.txt with verbose=True') }}}\n\nREFACTORED_1: {{{ logger.info('Processing started', extra={'verbose': True, file: 'config.txt'}) }}}\n\nOLD_2: {{{ print('Warning! Could not load user U-232 data from user_info. csv') }}}\n\nREFACTORED_2: {{{ logger.warning('User data failed to load', extra={'user': 'U-232', 'file': 'user_info.csv'}) }}}\n\nOLD: {{{ print('Error! File not found: passwords.txt') }}}\n\nREFACTORED:\n\nProviding these two examples as guidance for the model will capture the desired output, as illustrated in Figure 8.2:\n\nFigure 8.2: Improved logging suggestion\n\nThis time, the model successfully extracted the parameters from the static log message and used the logger instance instead of a direct logging message, following the patterns demonstrated in the few-shot examples. We will revisit logging practices in greater detail in Chapter 15, so stay tuned.\n\nNext, we will explore how adopting a Python-formatted style guide with Copilot can go beyond traditional linting and code formatting tools. This few-shot approach is proactive, adaptable, and context-aware, enabling Copilot to predict our intended implementation easily.\n\n195",
      "content_length": 1240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "196\n\nAn Introduction to Prompt Engineering\n\nCrafting a style guide for GitHub Copilot GitHub Copilot functions as a personalized AI assistant, predicting our next coding moves. Be- cause LLMs excel at imitating style, Copilot quickly adapts to any style preferences it observes during a session.\n\nFor example, suppose we have a file named ch8/code_samples/math_calculations.py with two functions (get_area and get_arithmetic_mean), both implemented without type hints and with single-line function signatures:\n\nimport numpy as np\n\ndef get_area(radius):\n\nreturn np.pi * radius ** 2\n\ndef get_arithmetic_mean(x1, x2):\n\nreturn (x1 + x2) / 2\n\nIf we then start typing a new function signature for get_euclidean_distance, Copilot is likely to generate an implementation in the same style, with no type hints and one-line signatures, as shown in Figure 8.3:\n\nFigure 8.3: Copilot preserving our coding style\n\nAlternatively, if our code implements type hints, hanging indents, and docstrings, Copilot will generate suggestions that reflect this updated style. For example, here is an alternate implemen- tation of get_area and get_arithmetic_mean:\n\nimport numpy as np\n\ndef get_area(\n\nradius: float,",
      "content_length": 1188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Chapter 8\n\n) -> float:\n\n\"\"\"\n\nCompute the area of a circle given its radius.\n\n\"\"\"\n\narea: float = np.pi * radius ** 2\n\nreturn area\n\ndef get_arithmetic_mean(\n\nx1: float,\n\nx2: float,\n\n) -> float:\n\n\"\"\"\n\nCompute the arithmetic mean of two numbers.\n\n\"\"\"\n\narithmetic_mean: float = (x1 + x2) / 2\n\nreturn arithmetic_mean\n\nBased on this coding preference, Figure 8.4 shows the suggested implementation of get_euclidean_ distance reflecting the new coding style:\n\nFigure 8.4: Copilot’s adapted coding style\n\n197",
      "content_length": 499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "198\n\nAn Introduction to Prompt Engineering\n\nAs developers, we would like a more robust method for informing Copilot of our preferred style guidelines without having to repeat them every session. We can achieve this by using a few-shot learning approach: creating a dedicated style guide file, as we will see next.\n\nStyle guide file A simple way to include our preferred style guide in Copilot is to maintain a single file outside the main project folder that we can copy and paste in place when we start a new Copilot session. Copying and pasting this file’s content makes our coding style part of the active session, although we hope that future versions of Copilot will introduce a more convenient method. For example, a sample file located at ch8/code_samples/style_guide.py, shown in Figure 8.5, includes two functions: get_area and get_arithmetic_mean.\n\nFigure 8.5: Style guide\n\nThis file does not include a docstring, as adding one is considered a separate task from imple- menting the function. We will explore this topic in more detail in later chapters.\n\nNext, if we open a new file at ch8/code_samples/distances.py and begin typing the signature of get_euclidean_distance, Copilot is likely to generate an implementation that aligns with that formatting, as shown in Figure 8.6:",
      "content_length": 1288,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "Chapter 8\n\nFigure 8.6: Preserved coding style\n\nOur coding preferences remain consistent in the new file, showing that Copilot has effectively captured our desired style. In later chapters, we will explore how adding more Python elements, such as decorators, to the style guide can further refine and expand our preferred coding approach.\n\nStyle guide as a next-generation code formatter\n\nIf you are used to code formatters or linters, you will notice that implementing a style guide may remind you of a code formatting initiative, but is more advanced. First, because the style guide approach is proactive rather than reactive, we get suggestions already formatted the way we want. Second, it offers more flexibility: we can adopt the show, don’t tell method by simply demonstrating our preferred style to Copilot. Finally, we can go beyond traditional formatters by including preferences such as type hints, which are not always clear using older methods.\n\nNext, we will explore how to apply few-shot learning with OpenAI, using this technique to re- factor code at scale.\n\nWorking with few-shot learning for OpenAI API For large-scale tasks, such as generating code implementations from function signatures, we can use OpenAI API programmatically and apply few-shot learning to illustrate the desired outcome effectively. To do this, we provide indexed examples of input and output and reference these examples in the system prompt.\n\n199",
      "content_length": 1439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "200\n\nAn Introduction to Prompt Engineering\n\nThe system prompt should reference the indexed input and output examples as follows:\n\nSURROUND = \"\"\"You are provided with:\n\n1. A [...] enclosed with {{{ INPUT }}}.\n\n2. Example [...] enclosed with {{{ INPUT_i }}} followed by the corresponding [...] enclosed with {{{ OUTPUT_i }}}.\"\"\" SINGLE_TASK = \"Your task is to [...].\"\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nNext, we update the get_user_prompt function to include the provided examples, ensuring that the labels align with those in the system prompt:\n\ndef get_user_prompt(some_input: str, few_shots: list) -> str:\n\nprompt = \"\"\n\nfor i, (input_, output_) in enumerate(few_shots):\n\nprompt += f\"\"\"\n\nINPUT_{i + 1}: {{{{{{ {input_} }}}}}}\n\nOUTPUT_{i + 1}: {{{{{{ {output_} }}}}}}\"\"\"\n\nprompt += f\"\"\"\n\nINPUT: {{{{{{ {some_input} }}}}}}\n\nOUTPUT:\n\n\"\"\"\n\nreturn prompt\n\nSimilarly, we replace INPUT_i and OUTPUT_i with task-specific labels, such as FUNCTION_i and CODE_i for code completion tasks.\n\nImplementing code with OpenAI API In Chapter 2, we built a code completion program that generates a Python implementation based on a function signature. For example, when implementing print_fibonacci_sequence, we pro- vided a prompt that requested only the function’s implementation, without additional explana- tions or output samples.\n\nA more effective approach is to include a few examples that demonstrate the desired output format. For instance, we can provide implementations for get_area and get_arithmetic_mean, both based solely on their function signatures:\n\nINPUT_1 = \"\"\"def get_area(radius: float) -> float:\"\"\"",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Chapter 8\n\nOUTPUT_1 = \"\"\"def get_area(radius: float) -> float:\n\narea: float = np.pi * radius ** 2\n\nreturn area\"\"\"\n\nINPUT_2 = \"\"\"def get_arithmetic_mean(x1: float, x2: float) -> float:\"\"\"\n\nOUTPUT_2 = \"\"\"def get_arithmetic_mean(x1: float, x2: float) -> float:\n\narithmetic_mean: float = (x1 + x2) / 2\n\nreturn arithmetic_mean\"\"\"\n\nFEW_SHOTS = [\n\n(INPUT_1, OUTPUT_1),\n\n(INPUT_2, OUTPUT_2),\n\n]\n\nWith these examples, we can construct a system prompt and user prompt that encourage the model to generate only the function implementation without additional explanations. The system prompt would be structured as follows:\n\nSURROUND = \"\"\"You are provided with:\n\n1. A Python function signature enclosed with {{{ FUNCTION }}}.\n\n2. Example signatures enclosed with {{{ FUNCTION_i }}} followed by the corresponding implementation enclosed with {{{ CODE_i }}}. \"\"\"\n\nSINGLE_TASK = \"Your task is to implement the function.\"\n\nNext, we define a function to generate the user prompt, ensuring consistency with the system prompt’s structure:\n\ndef get_user_prompt(signature: str, few_shots: list) -> str:\n\nprompt = \"\"\n\nfor i, (input_, output_) in enumerate(few_shots):\n\nprompt += f\"\"\"\n\nFUNCTION_{i + 1}: {{{{{{ {input_} }}}}}}\n\nCODE_{i + 1}: {{{{{{ {output_} }}}}}}\"\"\"\n\nprompt += f\"\"\"\n\nFUNCTION: {{{{{{ {signature} }}}}}}\n\n201",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "202\n\nAn Introduction to Prompt Engineering\n\nCODE:\n\n\"\"\"\n\nreturn prompt\n\nFor the implementation of print_fibonacci_sequence, we define the system and user prompts as follows:\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(\"def print_fibonacci_sequence(n: int) -> None:\", FEW_SHOTS)\n\nFinally, we call the API with the following snippet:\n\nclient: OpenAI = OpenAI()\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n],\n\n))\n\nprint(completion.choices[0].message.content)\n\nBy structuring the prompt this way, the model is more likely to follow the expected output format. Running the preceding example, which is available in ch8/code_samples/openai_fibonacci.py, we got the following function implementation:\n\ndef print_fibonacci_sequence(n: int) -> None:\n\na, b = 0, 1\n\nfor _ in range(n):\n\nprint(a, end=' ')\n\na, b = b, a + b\n\nprint() # To add a newline after printing the sequence\n\nThis implementation aligns with our provided examples and does not include surrounding ex- planations.",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Chapter 8\n\nFine-tuning as a better approach to OpenAI API\n\nWhen scaling projects, fine-tuning offers a more effective way to specialize a model for a specific task, such as generating code in a preferred style. This process involves training a pre-trained model such as GPT-4o-mini on a set of high-quality examples, allowing it to adjust its internal settings to better align with our preferences. We will explore this topic in greater detail in Chapter 11.\n\nHaving explored few-shot learning with ChatGPT, GitHub Copilot, and OpenAI API, we will now introduce additional prompt engineering techniques, which we will examine in greater detail in the upcoming and later chapters of the book.\n\nIntroducing more prompt engineering techniques Variations of few-shot prompting can include edge cases, progressively complex examples, or contrastive examples that highlight both correct and incorrect outcomes. These variations on few-shot prompting enhance a model’s ability to generalize across diverse task requirements. The examples in this chapter on type hinting and style formatting are just the tip of the iceberg of how prompt engineering can be used.\n\nIn other scenarios of the SDLC, we will see how using different prompt engineering techniques can become handy.\n\nBug fix at scale with prompt engineering Refer to the OpenAI example of effective prompts for fixing buggy code, enhanced with the re- finements discussed in Chapter 4:\n\nCONTEXT: You will be provided with Python code in the Python editor with description enclosed with {{{ DESCRIPTION }}}\n\nthat may contain errors enclosed with {{{ ERRORS }}}.\n\nTASK: Fix the errors in the code.\n\nDESCRIPTION: {{{ Execute 10 multiplication quizzes for the user and validate the answer }}}\n\n203",
      "content_length": 1745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "204\n\nAn Introduction to Prompt Engineering\n\nERRORS: {{{\n\n1. Compilation\n\n2. Reproducibility of random number generation\n\n3. Logical scoping of variable assignments\n\n4. Error handling for input casting\n\n}}}\n\nWhile this prompt successfully identified all errors in the code, we cannot guarantee that GenAI applications will consistently capture all errors in different code snippets.\n\nTo improve results, we can explore techniques such as iterative prompting, where the output is verified for compilation and refined through model reiteration if needed. Template-based prompting can enforce a specific format for error descriptions, ensuring consistency and clarity in the output. Additionally, chain-of-thought prompting helps guide the model through a logical reasoning process, improving its ability to understand and fix code issues. Let’s explore these in the next subsections.\n\nIterative prompting Iterative prompting involves engaging the model repeatedly within a feedback loop that refines its output based on evaluations of previous responses. This feedback can be manual, such as asking ChatGPT to revise its response to address unidentified errors, or automated, such as compiling code generated by OpenAI API or running it against a suite of unit tests. The resulting feedback, whether a traceback error message or failed test results, can be reintroduced to the model, con- tinuing the cycle until specific criteria are satisfied.\n\nFor instance, if the goal is to address compilation issues, an iterative mechanism could involve attempting to compile the output and feeding the traceback error messages back to the model for refinement if the code fails to compile:\n\nfor i in range(10):\n\ntry:\n\nexec(user_code)\n\nprint(f\"successfully compiled:\\n {user_code}\")\n\nbreak\n\nexcept SyntaxError as se:\n\nuser_code = get_refined_output(user_code, traceback.format_exc())",
      "content_length": 1870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Chapter 8\n\nHere, we will repeatedly prompt the model with traceback messages from the suggested code until it successfully compiles. Alternatively, we could evaluate the code by running it against a test suite or using other evaluation mechanisms, which we will explore in later chapters of the book.\n\nTemplate-based prompting Template-based prompting involves providing specific guidelines for structuring the desired output. For instance, when fixing code, we may want to understand the type of error being ad- dressed, such as whether it is a compilation error, a logical issue, or something else. In such cases, the output could follow this structure:\n\nThis code requires fix because of { ERROR TYPE}. The fixed code is { CODE }.\n\nTo guide the model toward generating this output, we include hints about the template within the context and provide a structure like this:\n\nCONTEXT: You will be provided with Python code enclosed with {{{ CODE }}} that does not compile, and an error traceback enclosed with {{{ TRACEBACK }}}, along with a template of the output enclosed with {{{ TEMPLATE }}}.\n\nTASK: Fix the errors in the code using the provided template.\n\nTEMPLATE: This code requires a fix because of { ERROR TYPE }. The fixed code is { CODE }.\n\nHere, we guide the model by providing a specific template that includes placeholders for the error type and the fixed code. This approach differs from few-shot prompting in how the output is constructed; rather than offering input-output examples, we define the structure of the output with variables for the model to populate. These two techniques can be used independently or in combination to achieve the desired results.\n\nChain-of-thought prompting Chain-of-thought (CoT) prompting involves guiding the model through a step-by-step reason- ing process toward a final answer. In coding, we apply CoT by referencing functions that are yet to be implemented, serving as a reasoning framework for the GenAI tool, which later fills in the implementation details. For instance, when validating that a code has no bugs, the process may involve the following steps:\n\n1. Validating syntax\n\n2. Checking compilation\n\n3. Ensuring reproducibility across runs\n\n205",
      "content_length": 2206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "206\n\nAn Introduction to Prompt Engineering\n\nUsing CoT, we can explicitly reference functions that handle each of these steps while leaving the actual implementation for GenAI to complete. In code, this might look like this:\n\ndef validate_no_bugs_in_source_code(sc: str) -> None:\n\nvalidate_syntax(sc)\n\nvalidate_compilation(sc)\n\nvalidate_reoroducibility_across_runs(sc)\n\nHere, validate_no_bugs_in_source_code outlines the logical steps needed to verify that the source code is bug-free but does not contain implementation details. GenAI tools are particularly effective at generating low-level implementations, and when typing the first referenced function, validate_syntax, Copilot can generate a suitable implementation, as illustrated in Figure 8.7:\n\nFigure 8.7: GitHub Copilot suggested implementation of validate_syntax\n\nCoT is one of the most effective prompt engineering techniques for coding tasks, and we will explore its applications further in the upcoming chapters.\n\nExploring additional methods This book covers a selection of prompt engineering techniques among the many available. For a broader exploration of prompting strategies, Unlocking the Secrets of Prompt Engineering: Master the art of creative language generation to accelerate your journey from novice to pro by Packt delves into numerous other techniques applicable to diverse use cases.",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "Chapter 8\n\nSummary In this chapter, we introduced few-shot learning as a prompt engineering technique to guide LLMs toward more predictable and desired outcomes. We demonstrated its application in sce- narios such as implementing a specific logging structure and following a particular coding style.\n\nWe explored how to implement few-shot prompting across different GenAI applications. In ChatGPT and OpenAI API, we utilized specific keyword combinations such as question and answer or old and refactored to structure the few-shot examples. With GitHub Copilot, we leveraged a style guide file to influence code completion output.\n\nWe also introduced additional prompt engineering techniques that are valuable for scaling bug fixes. Iterative prompting enables models to refine their output by utilizing feedback from com- pilation checks until the code compiles successfully. Template-based prompting leads the model toward producing outputs with a specific structure. Furthermore, CoT prompting breaks down solutions into clear, step-by-step reasoning processes.\n\nIn the next chapters, we will delve deeper into the mechanics of CoT, and chaining, and see how to apply them with ChatGPT, OpenAI API, and GitHub Copilot.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: When should we use prompt engineering?\n\nAnswer: Prompt engineering should follow best practices to achieve desirable outcomes from LLMs. However, when standard guidelines are insufficient, advanced prompting techniques may be necessary. In this chapter, we saw how few-shot learning helped align outputs with a specific style guide, making results more predictable and desirable.\n\nQuestion 2: Can we use plain English to describe the desired outcome instead of providing few- shot examples in code?\n\nAnswer: While it is theoretically possible to use plain English to describe coding styles and desired outputs, this approach is less effective. Like mathematical equations or music notes, code acts as a concise and precise notation system. LLMs, trained extensively on code, interpret it more accurately than plain-text descriptions.\n\n207",
      "content_length": 2191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "208\n\nAn Introduction to Prompt Engineering\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nPEP-8 Style Guide for Python Code: https://peps.python.org/pep-0008/\n\n\n\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ..., and Amodei, D. (2020). Language Models are Few-Shot Learners. OpenAI. Retrieved from https://arxiv.\n\norg/abs/2005.14165v4\n\n\n\nSahoo, P., Singh, A. K., Saha, S., Jain, V., Mondal, S., and Chadha, A. (2024). A systematic survey of prompt engineering in large language models: Techniques and applications. Indian In- stitute of Technology Patna, Stanford University, and Amazon AI. Retrieved from https:// arxiv.org/abs/2402.07927v1\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "9\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nSome coding tasks extend beyond simple, single-task initiatives. For these more complex scenarios, advanced prompt engineering techniques such as chain of thought (CoT) and chaining can be highly effective when working with large language models (LLMs). This chapter will explore how to use these techniques when working with OpenAI API, ChatGPT, and GitHub Copilot on challenges that often require more than just the initial prompt to achieve the desired output.\n\nWe will walk through an example of implementing a geometric mean calculator for portfolio returns that handles negative net returns. By applying CoT prompting, we will guide the model with explicit reasoning steps and achieve desirable results, even with a lighter model such as GPT-4o mini. Then, using chaining, we will improve the initial implementation by enhancing its type hints and refining the docstring.\n\nThis chapter covers the following topics:\n\n\n\nExtending the prompt implementation scope\n\n\n\nLeveraging CoT reasoning for code completion\n\nUsing prompt chaining with LLMs",
      "content_length": 1103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "210\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance with setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nExtending the prompt implementation scope Until now, we have mainly worked with short functions spanning only a few lines of code, ap- plying our best practices for prompt precision to generate effective code using GitHub Copilot, OpenAI, and ChatGPT.\n\nHowever, in many real-world scenarios, our tasks will not fit within such a narrow scope. We may need to handle input casting, switch cases, or adapt implementations beyond a raw mathe- matical formula. This chapter explores how we can extend the scope of coding tasks by applying advanced prompt engineering techniques.\n\nExtending the geometric mean scope Let us revisit the geometric mean implementation from Chapter 3. The geometric mean is a method of averaging numbers that have a multiplicative nature, such as financial portfolio growth rates, population growth rates, or drug concentrations that change exponentially over time. The geo- metric mean is defined as the nth root of the product of the inputs, as follows:\n\n𝑛𝑛\n\n𝑖 𝑛𝑛\n\nGeometric Mean = (∏𝑥𝑥𝑖𝑖\n\n𝑖𝑖𝑖𝑖\n\nFor a more detailed explanation of the concept, refer to Chapter 3.\n\n)",
      "content_length": 1834,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "Chapter 9\n\nHowever, when some of the inputs are negative, such as net portfolio returns, the standard for- mula becomes insufficient. For example, consider IBM’s stock returns from 2000 to 2002, where two years had negative returns:\n\nIBM_YEARLY_RETURNS: Dict[str, float] = {\n\n\"2000\": -0.2084,\n\n\"2001\": 0.4300,\n\n\"2002\": -0.3547\n\n}\n\nIf we were to use the geometric mean formula as is, we would get an average yearly return of 32%:\n\n1 3\n\n1 3\n\n\n\n𝐺𝐺𝐺 𝐺 𝐺(−0.2084𝐺 ⋅ 0.4300𝐺 ⋅ 𝐺 − 0.3547)\n\n𝐺 (0.031)\n\n𝐺 0.32\n\nThis result, however, is incorrect. A $1,000 investment in IBM stock in 2000 would shrink to approximately $730 by the end of 2002, reflecting a total decline of about 27%. Thus, we would expect an average annual return closer to -10%.\n\nOne way to handle negative returns is by using gross returns instead. This involves converting net returns to gross values, computing their product, and extracting the nth root:\n\nfrom typing import Dict\n\nimport numpy as np\n\ndef get_average_return(\n\nnet_returns: Dict[str, float],\n\n) -> float:\n\n# Step 1: Calculate the yearly gross returns\n\ngross_returns: np.ndarray = np.array(list(net_returns.values())) + 1\n\n# Step 2: Calculate the geometric mean of the gross returns\n\npower: float = 1 / len(gross_returns)\n\ngross_average: float = np.prod(gross_returns) ** power\n\n# Step 3: Calculate the net average return\n\nnet_average: float = gross_average - 1\n\nreturn net_average\n\n211",
      "content_length": 1414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "212\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nApplying this function to IBM’s returns produces an average return of -10%:\n\nnet_returns: [-0.2084, 0.43, -0.3547]\n\ngross_returns: [0.7916, 1.43, 0.6453]\n\ngross_average: 0.9006\n\nnet_average: -0.099\n\nThis implementation is more extensive than the functions we have generated so far, so it may require more advanced techniques to get desirable results with minimal iterations.\n\nNext, we will compare baseline responses from GitHub Copilot, ChatGPT, and OpenAI API, ex- amining their limitations and potential pitfalls.\n\nEvaluating baseline outputs In Chapter 6, we discussed how GitHub Copilot, ChatGPT, and OpenAI API each have strengths in different coding-related tasks. Copilot excels at code completion but can fall short when gener- ating larger blocks of starter code. ChatGPT is better suited for producing more extensive initial code, though it often includes longer functions with excessive comments and computations. OpenAI API requires more effort to craft effective prompts but offers greater flexibility through programmatic automation and control over prompt history. For a refresher on these differences, you can revisit Chapter 6.\n\nThese differences become apparent when implementing the geometric mean for portfolio returns over multiple years. Each tool requires different strategies to refine and enhance the quality of the generated output.\n\nGitHub Copilot baseline To apply best practices for prompt precision with Copilot, we can start by providing the function signature for get_geometric_mean as follows:\n\ndef get_geometric_mean(\n\nnet_returns: Dict[str, float],\n\n) -> float:\n\n...",
      "content_length": 1661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "Chapter 9\n\nThis gives Copilot a clear starting point with the function name, an argument specifying the use case of net returns, and an expected return type. However, this prompt alone may not be sufficient to guide Copilot in handling negative inputs, as demonstrated in Figure 9.1.\n\nFigure 9.1: Copilot’s completion does not support negative net returns\n\nThis implementation would yield incorrect results for negative inputs and should not be used. Later in the chapter, we will see how CoT can help us get the correct implementation we are after.\n\nChatGPT baseline ChatGPT, on the other hand, presents a different challenge. While newer models, such as o3, excel at deeper reasoning (as discussed later in this chapter) and may produce more accurate imple- mentations, more effort is required to prompt ChatGPT effectively than with Copilot. Additionally, achieving a coding format that aligns with our preferences is more challenging.\n\nTo replicate the GitHub Copilot implementation, we used the following prompt:\n\nCONTEXT: You are provided with a Python function signature enclosed with {{{ FUNCTION }}}.\n\nTASK: implement the function.\n\nFUNCTION: {{{ defget_geometric_mean(\\n\\t\\tnet_returns: Dict[str, float],\\n) -> float: }}}\n\nCODE:\n\n213",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "214\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nUsing the o3-mini-high model, we generated a functionally correct implementation. Howev- er, the resulting code was cluttered with excessive inline comments and docstrings, making it time-consuming to clean up. Figure 9.2 illustrates this output in ChatGPT.\n\nFigure 9.2: ChatGPT implementation\n\nQuick tip: Need to see a high-resolution version of this image? Open this book\n\nin the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.",
      "content_length": 740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "Chapter 9\n\nFor reference, the proposed code by ChatGPT is as follows:\n\nimport math\n\nfrom typing import Dict\n\ndef get_geometric_mean(net_returns: Dict[str, float]) -> float:\n\n\"\"\" [... omitted for brevity ...] \"\"\"\n\nif not net_returns:\n\nreturn 0.0\n\n# Calculate the product of (1 + return) for each net return\n\nproduct = math.prod(1 + r for r in net_returns.values())\n\nn = len(net_returns)\n\n# Compute the nth root of the product and subtract 1 to get the geometric mean\n\ngeometric_mean = product ** (1 / n) - 1\n\nreturn geometric_mean\n\nThis code is functionally correct but includes several undesired elements:\n\nUnnecessary docstring\n\n\n\nInput validation\n\n\n\nExcessive comments\n\n\n\nInline computations\n\nLater in the chapter, we will explore how CoT and chaining requests to the initial implementation can help us achieve the desired output.\n\nOpenAI API baseline We used the o3-mini-2025-01-31 model via OpenAI API and found that its output closely mir- rored what ChatGPT produced. To evaluate this, we ran the script located at ch9/baseline/ baseline_openai.py, using the following system and user prompts:\n\nSURROUND = \"You are provided with a Python function signature enclosed with {{{ FUNCTION }}}.\"\n\nSINGLE_TASK = \"Your task is to implement the function.\"\n\nSRC_CODE = \"\"\"def get_geometric_mean(\\n\\tnet_returns: Dict[str, float],\\n) -> float:\"\"\"\n\n215",
      "content_length": 1346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "216\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\ndef get_user_prompt(src: str) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {src} }}}}}}\n\nCODE:\n\n\"\"\"\n\nWe then called the model using the following code:\n\nif __name__ == '__main__':\n\nclient: OpenAI = OpenAI()\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(SRC_CODE)\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"o3-mini-2025-01-31\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n],\n\n)\n\noutput = completion.choices[0].message.content\n\nprint(output)\n\nThe output generated by OpenAI API was very similar to the one produced by ChatGPT, as seen in the following sample result:\n\ndef get_geometric_mean(net_returns: Dict[str, float]) -> float:\n\n# Return 0.0 if there are no returns (could alternatively raise an exception)\n\nif not net_returns:\n\nreturn 0.0\n\nimport math\n\n# Calculate the product of (1 + return) for each period\n\nproduct = 1.0\n\nn = len(net_returns)\n\nfor r in net_returns.values():\n\nproduct *= (1 + r)",
      "content_length": 1069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Chapter 9\n\n# Compute the geometric mean: nth root of product then subtract 1\n\ngeom_mean = math.pow(product, 1 / n) - 1\n\nreturn geom_mean\n\nIn this implementation, the functionality is correct, but the formatting is less than ideal due to excessive inline comments, unnecessary input validation, and even an import statement placed inside the function. Later in the chapter, we will see how to refine this implementation using CoT and chaining to achieve desirable results even when working with lower-cost models such as GPT-4o mini.\n\nIn summary, as our implementation scope expands beyond simple and short functions, applying more advanced techniques becomes essential to achieving the desired results.\n\nNext, we will see how we can utilize advanced prompt engineering techniques and get the imple- mentation we want faster, and see which application (either Copilot, OpenAI API, or ChatGPT) is most suitable for each technique.\n\nLeveraging CoT reasoning with LLM In Chapter 8, we explored how guided reasoning with CoT can help LLMs produce better code by following a step-by-step reasoning process. This step-by-step thinking encourages models to tackle problems in a structured way, leading to more accurate and desirable outputs.\n\nAs a simple exercise, consider the following sequence:\n\n17, 34, 14, 28, 8, 16\n\nThis sequence follows two rules – first, multiply the previous number by 2 to get the next one; then subtract 20 to get the following number:\n\n34 = 17 x 2\n\n14 = 34 – 20\n\n28 = 14 x 2\n\n8 = 28 – 20\n\n16 = 8 x 2\n\n217",
      "content_length": 1525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "218\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nThe next number should be -4, which is 16 - 20. However, prompting Copilot to predict the next number in the sequence is not straightforward, as shown in Figure 9.3.\n\nFigure 9.3: Copilot fails to predict the correct next number\n\nIn this case, Copilot failed to predict the next number accurately, suggesting 4 instead of -4. This demonstrates that the model did not fully grasp the sequence’s logic. To improve the outcome, we can guide the model by prompting it to reason through the relationship between the current number and the previous one before making a final prediction, as shown in Figure 9.4.\n\nFigure 9.4: Reasoning steps provided for the next number",
      "content_length": 720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "Chapter 9\n\nFigure 9.4 shows that, by adding comments in rows 4-8, Copilot predicted the next number in the sequence correctly and also explained its reasoning for the final number, 6. Similarly, as we will explore later in this chapter, incorporating reasoning steps into our coding tasks can help the model produce more desirable outcomes.\n\nCoT in the GPT-o series CoT has proven so effective across a range of domains requiring complex reasoning that it was built into the GPT-o series. This means the model will automatically identify the necessary steps to complete a task before arriving at the final answer.\n\nFor the sequence completion example we explored earlier in the chapter, we could use the GPT-o1 model without additional guidance, as shown in Figure 9.5.\n\nFigure 9.5: Built-in CoT in o1\n\nFrom the ChatGPT output, we can see that the model recognized this as a sequence pattern task, with an internal process for handling reasoning in such cases. The model took 47 seconds to process the task, which included problem analysis, problem recognition, and estimating the next number, as shown in Figure 9.6.\n\n219",
      "content_length": 1122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "220\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nFigure 9.6: Thought process in ChatGPT\n\nHere, we can see that ChatGPT is analyzing the sequence, which is one of the reasoning steps in CoT to get to the final answer.\n\no-series adoption Models from the o series are now available through ChatGPT and OpenAI API. However, they come at a higher cost than their predecessors, as illustrated in Figure 9.7:\n\nFigure 9.7: Costs of o-series models for OpenAI API calls\n\nThe o1 model, for example, is priced at $15 per million input tokens and $60 per million output tokens, making it six times more expensive than GPT-4o and 1,000 times more expensive than GPT-4o mini, as shown in Figure 9.8.",
      "content_length": 695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "Chapter 9\n\nFigure 9.8: GPT-4o pricing for OpenAI API calls\n\nDue to this significant cost difference, o-series models are used less frequently in applications. When selecting the right model for yourself or your company, it is important to weigh the trade-off between cost and output quality. If more affordable models, such as GPT-4o, deliver satisfactory results and meet expectations, it may be prudent to continue using them. However, the o1 and o3 models have been shown to give significantly better outcomes, so it might be worth using them first or as a fallback, even at a higher cost.\n\nUsing CoT for GitHub Copilot The CoT approach is one of our most frequently used strategies when working with GitHub Co- pilot. When applied effectively, it can simplify the development of entire code segments, leading to cleaner and more readable implementations.\n\nThis method involves structuring the reasoning process by referencing functions that have yet to be implemented. For example, when calculating the geometric mean for net returns, we can break it down into three logical steps:\n\n1. Convert input to gross returns.\n\n2. Compute the geometric mean of the gross returns.\n\n3. Convert back to the net returns.\n\n221",
      "content_length": 1216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "222\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nExpressing these steps in code involves defining each operation as a function, as shown in the following code:\n\ndef get_average_return(\n\nnet_returns: Dict[str, float],\n\n) -> float:\n\ngross_returns: np.ndarray = get_gross_returns(net_returns)\n\ngross_average: float = get_geometric_mean(gross_returns)\n\nnet_average: float = get_net_average(gross_average)\n\nreturn net_average\n\nThis function does not include any implementation details and only outlines the functions or steps to be completed. Since the functions are not yet implemented, typing their names in an IDE may look like an error, as shown in Figure 9.9:\n\nFigure 9.9: Error underline in the CoT definition\n\nThis underlying error indicator is expected and resolved as Copilot generates the implementation for the function at hand.\n\nThis reasoning approach facilitates the generation of accurate and predictable code when work- ing with the AI pair programmer. As each function is implemented (such as get_gross_returns, get_geometric_mean, etc.), Copilot learns enough context to write the correct provided param- eters, how the return value is used, and the expected type hints.\n\nAdditionally, we can predict how Copilot will complete the code and in what order. The imple- mentation will follow the sequence of function calls in get_average_return. Indeed, as we press Enter twice and start typing def, Copilot will suggest an implementation for get_gross_returns, as shown in Figure 9.10:",
      "content_length": 1505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "Chapter 9\n\nFigure 9.10: Copilot predicts the implementation of the first called function\n\nOnce we accept the implementation and hit Enter twice again, Copilot will suggest the next function in the call sequence, get_geometric_mean, as shown in Figure 9.11:\n\nFigure 9.11: Copilot predicting the next function definition\n\n223",
      "content_length": 323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "224\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nHere, we did not need to type anything. Just a two-line break, resembling the space between get_average_return and get_gross_returns, was enough to trigger a fully predictable imple- mentation in this file. This demonstrates the optimal use of Copilot, truly leveraging the power of an AI pair programmer. For your reference, here is the full implementation suggested by Copilot:\n\ndef get_gross_returns(\n\nnet_returns: Dict[str, float],\n\n) -> np.ndarray:\n\ngross_returns: np.ndarray = np.array(\n\nlist(net_returns.values())\n\n) + 1\n\nreturn gross_returns\n\ndef get_geometric_mean(\n\ngross_returns: np.ndarray,\n\n) -> float:\n\ngross_average: float = np.prod(gross_returns) ** (\n\n1 / len(gross_returns)\n\n)\n\nreturn gross_average\n\ndef get_net_average(\n\ngross_average: float,\n\n) -> float:\n\nnet_average: float = gross_average - 1\n\nreturn net_average\n\nThis implementation successfully captures the intended logic, and its structuring across different levels of abstraction is superior to what even the most advanced o3 models produce via ChatGPT and OpenAI API. This serves as a strong example of how effective prompting techniques can outperform more powerful and costly models.\n\nEducating your colleagues\n\nNext time you notice a colleague trying to use inline comments in GitHub Copilot such as # step 1: Compute the gross returns, suggest they try using CoT in- stead. This approach will not only be more efficient and a better way to use Copilot, but it will also help the team maintain higher-quality code.",
      "content_length": 1554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Chapter 9\n\nNext, we will explore how to replicate the desired outputs with ChatGPT and OpenAI API, even when using more affordable models such as GPT-4o mini.\n\nCoT for the geometric mean with ChatGPT The same CoT approach can be applied to coding tasks, such as implementing a get_average_ return function that supports negative returns. Rather than supplying the function signature, we will provide a full implementation of get_average_return as we did with Copilot and prompt the model to complete the lower-level details. A prompt for this might look like the following:\n\nCONTEXT: You are provided with a Python function enclosed with {{{ FUNCTION }}} that calls functions that should be completed.\n\nTASK: Implement the missing functions.\n\nFUNCTION: {{{def get_average_return(\\n\\t\\tnet_returns: Dict[str, float],\\n) -> float: \\n\\tgross_returns: np.ndarray = get_gross_ returns(net_returns) \\n\\tgross_average: float = get_geometric_mean(gross_ returns) \\n\\tnet_average: float = get_net_average(gross_average) \\n\\ treturn net_average }}}\n\nCODE:\n\nThis prompt outlines the necessary reasoning steps to generate the correct code. For reference, here is the output produced by ChatGPT using the GPT-4o mini model with the given prompt:\n\ndef get_gross_returns(net_returns: Dict[str, float]) -> np.ndarray:\n\n\"\"\" ... omitted for brevity ... \"\"\"\n\nreturn np.array([1 + net_return for net_return in net_returns. values()])\n\ndef get_geometric_mean(values: np.ndarray) -> float:\n\n\"\"\" ... omitted for brevity ... \"\"\"\n\nreturn np.prod(values) ** (1 / len(values))\n\ndef get_net_average(gross_average: float) -> float:\n\n\"\"\" ... omitted for brevity ... \"\"\"\n\nreturn gross_average - 1\n\ndef get_average_return(net_returns: Dict[str, float]) -> float:\n\ngross_returns: np.ndarray = get_gross_returns(net_returns)\n\ngross_average: float = get_geometric_mean(gross_returns)\n\n225",
      "content_length": 1853,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "226\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nnet_average: float = get_net_average(gross_average)\n\nreturn net_average\n\nThis output is more satisfactory than the baseline, even though it was generated with GPT-4o model instead of the more advanced O-series. It no longer includes inline comments or input val- idation, which significantly reduces the amount of manual editing required. Later in the chapter, we will see how chaining can help produce even more polished results that are ready to use as is.\n\nNext, we will apply this approach using OpenAI API.\n\nCoT with OpenAI API We can leverage CoT prompting and provide a full implementation of get_average_return as we did with Copilot and ChatGPT. The system and user prompts may look as follows:\n\nSURROUND = \"You are provided with a Python function enclosed with {{{ FUNCTION }}} that calls functions that should be completed.\"\n\nSINGLE_TASK = \"Your task is to implement the missing functions.\"\n\ndef get_user_prompt(func: callable) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {inspect.getsource(func)} }}}}}}\n\nCODE:\n\n\"\"\"\n\nWe will integrate these prompt pieces into an OpenAI API call, including the system prompt, user prompt, and a GPT-4o mini model, as shown here:\n\nif __name__ == '__main__':\n\nclient: OpenAI = OpenAI()\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(get_average_return)\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n],",
      "content_length": 1568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Chapter 9\n\n)\n\noutput = completion.choices[0].message.content\n\nprint(output)\n\nNote that the code sample can be found in this book’s repository at ch9/cot/cot_openai.py. For reference, here is sample output from the call:\n\nimport numpy as np\n\nfrom typing import Dict\n\ndef get_gross_returns(net_returns: Dict[str, float]) -> np.ndarray:\n\n\"\"\"Convert net returns to gross returns.\"\"\"\n\nreturn np.array([1 + r for r in net_returns.values()])\n\ndef get_geometric_mean(gross_returns: np.ndarray) -> float:\n\n\"\"\"Calculate the geometric mean of a numpy array of gross returns.\"\"\"\n\nreturn np.prod(gross_returns) ** (1 / len(gross_returns))\n\ndef get_net_average(gross_average: float) -> float:\n\n\"\"\"Convert gross average back to net average.\"\"\"\n\nreturn gross_average – 1\n\ndef get_average_return(net_returns: Dict[str, float]) -> float:\n\n... omitted for brevity ...\n\nHere, we obtained a better implementation than the baseline, maintaining the desired level of abstraction and avoiding input validation and inline comments.\n\nWhile leveraging CoT is highly effective when using GitHub Copilot, it is not always sufficient to produce ready-to-use code with ChatGPT or OpenAI API. To address this, we can incorporate chaining through follow-up prompts, as we will demonstrate next.\n\nUsing prompt chaining for LLMs Prompt chaining is another highly effective prompt engineering technique that helps us achieve better results from LLMs. It involves breaking down tasks into smaller, sequential steps that are more efficiently completed individually.\n\n227",
      "content_length": 1532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "228\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nFor instance, when implementing get_average_return, we may want to enhance ChatGPT’s or OpenAI’s initial output by adding type hints and avoiding inline calculations in the return statement. With GitHub Copilot, we might want to construct a barebones implementation first and add a Google Style docstring later.\n\nAlthough we could include all these elements in the initial prompt, it is often more natural and effective to start with an implementation that is functionally correct. From there, we can refine the code step by step through a series of follow-up prompts.\n\nChaining with ChatGPT Applying chaining with ChatGPT is very intuitive given that the UI is already designed for a con- versational style. The chat history helps us reference previous prompt pieces without needing to retype them.\n\nFor instance, say that we wanted to improve the implementation of get_average_return with a Google Style docstring rather than an arbitrary one, and type hints to variables in all functions. Including all these elements in one prompt would defy the single-task best practice and result in suboptimal outputs. Instead, we recommend the following approach:\n\n\n\nFor chained prompts with the same context, focus only on the new task\n\n\n\nFor chained prompts with a different context, such as adding test cases, use a full prompt following the five S’s best practices\n\nFor the task of coding get_average_return, adding type hints for variables, revising the docstring to be in a Google Style format, and adding a test suite can be done with the following strategy:\n\n\n\nPrompt 1: Write the full code prompt using the five S’s for the get_average_return im- plementation with a CoT.\n\n\n\nPrompt 2: Add type hints to all variables.\n\n\n\nPrompt 3: Change the docstring to Google Style.\n\n\n\nPrompt 4: Write unit tests. In this step, we will specify the test cases we want to cover, creating a new context. As a result, we need to provide a complete prompt that follows the five S’s best practices.",
      "content_length": 2038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Chapter 9\n\nFor example, prompt 2, which focuses on the single task of removing comments, is simply as follows:\n\nAdd type hints to all variables.\n\nThis will produce an updated implementation without the inline comments, as shown in Figure 9.12.\n\nFigure 9.12: Added type hints to variables\n\nThe initial implementation has been updated to include type hints for variables across the different functions. For instance, in the first function, get_gross_returns, the gross_returns variable is now correctly annotated with the np.ndarray type and returned in the following line.\n\nIn prompt 3, the focus shifts to improving the docstring. We continue with a single-task prompt:\n\ninclude Google Style docstring.\n\n229",
      "content_length": 707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "230\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nThis prompt is expected to yield a better implementation, as demonstrated in Figure 9.13.\n\nFigure 9.13: Refined docstring\n\nThe result confirms that we successfully obtained the desired docstring for the get_gross_return function in Google Style format. For reference, here are the docstrings generated by ChatGPT for the get_geometric_mean, get_net_average, and get_average_return functions:\n\ndef get_geometric_mean(gross_returns: np.ndarray) -> float:\n\n\"\"\"Calculates the geometric mean of gross returns.\n\nArgs:\n\ngross_returns (np.ndarray): An array of gross returns.\n\nReturns:\n\nfloat: The geometric mean of the gross returns.\n\n\"\"\"\n\n... omitted for brevity ...\n\ndef get_net_average(gross_average: float) -> float: \"\"\"Converts gross average return to net average return.\n\nArgs:\n\ngross_average (float): The geometric mean of gross returns.",
      "content_length": 896,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Chapter 9\n\nReturns:\n\nfloat: The corresponding net average return.\n\n\"\"\"\n\n... omitted for brevity ...\n\ndef get_average_return(net_returns: Dict[str, float]) -> float: \"\"\"Calculates the geometric mean of net returns from a dictionary of net returns.\n\nArgs:\n\nnet_returns (Dict[str, float]): A dictionary of net returns per period.\n\nReturns:\n\nfloat: The geometric mean of the net returns.\n\n\"\"\"\n\n... omitted for brevity ...\n\nIn later chapters of the book, we will explore a more robust approach to generating docstrings, along with additional chained tasks such as input validation, monitoring, and unit testing.\n\nOne limitation of the ChatGPT interface is the lack of control over how prompts are constructed and sent to the underlying model, especially as the conversation grows longer. In the next section, we will see how to gain more control over the chat history by using OpenAI API.\n\nSelective history with OpenAI API When working with OpenAI API for chained tasks, we can take a selective approach and explicitly choose which parts of the conversation history to include in each prompt.\n\nLet us consider the first three prompts from the previous ChatGPT session:\n\n\n\nPrompt 1: Leverage CoT for the get_average_return implementation\n\n\n\nPrompt 2: Add type hints\n\n\n\nPrompt 3: Refine the docstring\n\nA straightforward way to replicate this workflow with OpenAI API is to continuously append the chained prompt and the corresponding output to our messages list, as shown here:\n\nmessages = [{\"role\": \"system\", \"content\": system_prompt}]\n\nprompt_1 = get_user_prompt(get_average_return)\n\n231",
      "content_length": 1583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "232\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nprompt_2 = \"Add type hints to all variables.\"\n\nprompt_3 = \"include Google Style docstring.\"\n\nfor prompt in [prompt_1, prompt_2, prompt_3]:\n\nmessages.append({\"role\": \"user\", \"content\": prompt})\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=messages,\n\n)\n\noutput: str = completion.choices[0].message.content\n\nmessages.append({\"role\": \"assistant\", \"content\": output})\n\nIn this naive setup, each user prompt and corresponding model response is appended to the con- versation history. By the time we reach prompt_3, we expect the output to closely resemble what ChatGPT would generate. For reference, here is a sample response from the API that successfully produced a correct implementation, complete with Google Style docstrings and properly typed variables:\n\ndef get_gross_returns(net_returns: Dict[str, float]) -> np.ndarray:\n\n\"\"\"Convert net returns to gross returns.\n\nArgs:\n\nnet_returns (Dict[str, float]): A dictionary where keys are identifiers\n\n(such as tickers) and values are net return values (as decimal).\n\nReturns:\n\nnp.ndarray: An array of gross returns corresponding to the net returns.\n\n\"\"\"\n\ngross_returns: np.ndarray = np.array([1 + return_value for return_ value in net_returns.values()])\n\nreturn gross_returns\n\nFor reference, the full script can be found in ch9/chaining/naive_chaining_openai.py.",
      "content_length": 1415,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "Chapter 9\n\nThe naive approach works reasonably well when there are fewer than 5 follow-up prompts. How- ever, as the chain grows, so does the length of the accumulated prompt, leading to increased token usage. For instance, running the preceding script peaked at 1,253 tokens by prompt_3, as illustrated in Figure 9.14.\n\nFigure 9.14: prompt_3 with 1,253 tokens\n\nAs more prompts are added, the total token count increases, which results in higher usage costs and the risk of distracting the LLM from the specific task at hand.\n\nA more efficient alternative for longer chains is a selective history strategy, where only the relevant parts of the interaction are passed along. Instead of including the entire interaction history in each follow-up request, we can simply pass the code generated in response to the first prompt, along with a new system prompt tailored to the next transformation task.\n\nTo isolate just the code from the assistant’s response, we can use the following utility function:\n\ndef get_refactor_user_prompt(assistant_output: str) -> str:\n\ncode: str = re.sub(\n\nr\"(.*?)```python(.*?)```(.*)\",\n\nr\"\\2\",\n\nassistant_output,\n\nflags=re.DOTALL\n\n).strip()\n\nuser_prompt: str = f\"\"\"\n\nOLD: {{{{{{ {code} }}}}}}\n\nREFACTORED:\n\n\"\"\"\n\nreturn user_prompt\n\n233",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "234\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nThis extracts the relevant code and formats it into a new user prompt for the next step in the chain.\n\nIn contrast to the static system prompt used in the naive approach, we now update the system prompt with each task:\n\nprompt_1: str = f\"{SURROUND} {SINGLE_TASK}\"\n\nprompt_2: str = \"You are provided with a Python code enclosed in {{{ FUNCTION }}}. Your task is to add type hints to all variables.\"\n\nprompt_3: str = \"You are provided with a Python code enclosed in {{{ FUNCTION }}}. Your task is to include Google Style docstring.\"\n\nWe can then use a prompts dictionary to associate each prompt with the appropriate input-gen- eration function, linking prompt_1 to get_user_prompt for the initial implementation, and the subsequent prompts to get_refactor_user_prompt for the refactoring steps:\n\nprompts: Dict[str, callable] = {\n\nprompt_1: get_user_prompt,\n\nprompt_2: get_refactor_user_prompt,\n\nprompt_3: get_refactor_user_prompt}\n\nThis allows us to generate the user prompt dynamically for each task using the correct function:\n\nfor prompt, func in prompts.items():\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": prompt},\n\n{\"role\": \"user\", \"content\": func(next_input)},\n\n],\n\n)\n\nnext_input: str = completion.choices[0].message.content\n\nNotice how each API call now contains only a system prompt and a single user message. This keeps the interaction focused, passing along only what is necessary rather than the full chain of previous prompts and outputs.\n\nFor reference, here is a sample output from prompt_3 using the selective history strategy:\n\ndef get_gross_returns(net_returns: Dict[str, float]) -> np.ndarray:\n\n\"\"\"\n\nConvert net returns to gross returns.",
      "content_length": 1797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "Chapter 9\n\nGross Return is calculated as:\n\nGross Return = 1 + Net Return.\n\nArgs:\n\nnet_returns (Dict[str, float]): A dictionary where keys are identifiers (e.g., asset names) and values are the corresponding net returns.\n\nReturns:\n\nnp.ndarray: An array of gross returns derived from the provided net returns.\n\n\"\"\"\n\ngross_returns: np.ndarray = np.array([1 + net_return for net_return in net_returns.values()])\n\nreturn gross_returns\n\nThis output mirrors the quality of the naive approach, including type hints and a Google Style docstring. The key difference is efficiency: using selective history, we reduced the token count to just 296, which is less than 25% of the naive approach, as shown in Figure 9.15:\n\nFigure 9.15: Selective history reduces input tokens by over 75%\n\nAs chained tasks become more complex, the selective history strategy helps keep prompts short and focused. In later chapters, we will explore additional use cases for chaining across various stages of the Software Development Life Cycle (SDLC).\n\n235",
      "content_length": 1022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "236\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nIn summary, OpenAI API allows both automated execution of chaining and control over prompt history. By including only relevant information, we can reduce costs and help the model concen- trate on the specific task at hand.\n\nIn the final subsection of this chapter, we will look at how chaining can be applied with GitHub Copilot, where we aim for the initial implementations to already follow the desired formatting.\n\nChaining with Copilot When working with Copilot for code completion tasks, we can also leverage chaining by breaking down the task into smaller steps. For tasks that go beyond the barebones implementation, such as adding docstrings, input validations, logs, and tests, it is best to align with a chained approach. For the chained tasks, we can use the chat interaction mode with Copilot. In later chapters of the book, we will see how to leverage code completion for chained tasks as well.\n\nFor example, a chaining flow to extend get_average_return with a docstring and input valida- tion might look as follows:\n\n1.\n\nInitial task: Implement get_average_return with CoT.\n\n2. Chained task 2: Add docstrings to all functions.\n\n3. Chained task 3: Add input validation.\n\nAfter generating the initial implementation of get_average_return, as shown earlier in the chapter, we can proceed to the second chained task, which focuses on adding docstrings to all functions in the file.\n\nIn VS Code, we can use Copilot Edits for this task by opening the chat window and selecting Edit. We will ensure that the working set includes the current file, ch9/chaining/get_average_return. py, and enter the following prompt:\n\nAdd Google Style docstring to all functions.\n\nThis prompt will result in suggestions for adding type hints to the variables, as shown in Figure 9.16.",
      "content_length": 1832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "Chapter 9\n\nFigure 9.16: Suggested edits to add a docstring\n\nHere, we can see that Copilot correctly added a docstring to all functions with our preferred Google Style; for example, in line 26, a correct description of get_gross_return was added to the function.\n\nIn PyCharm, open GitHub Copilot Chat by clicking the Copilot icon in the bottom menu and selecting Open GitHub Copilot Chat or using the Ctrl + Shift + C keyboard shortcut. Enter the following prompt:\n\nAdd Google Style docstring to all functions.\n\nCopilot will suggest the necessary edits, adding a docstring to all functions, as seen in Figure 9.17:\n\nFigure 9.17: Suggested edits to add a docstring using PyCharm\n\n237",
      "content_length": 681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "238\n\nAdvanced Prompt Engineering for Coding-Related Tasks\n\nIn both VS Code and PyCharm, Copilot suggests the appropriate docstring for each function. Later in this book, we will apply chaining along with other prompt engineering techniques to address SDLC tasks. For example, in Chapter 14, we will explore how to combine CoT and chaining to profile the runtime and space complexity of our code.\n\nSummary In this chapter, we explored how to achieve desirable outcomes from LLMs by effectively applying CoT and chaining for coding tasks with an extended scope.\n\nWith CoT prompting, we saw how introducing reasoning steps into our prompts enables the model to handle more nuanced challenges, such as implementing a geometric mean function that supports negative net returns. We used function names as intermediate reasoning steps, while relying on Copilot, ChatGPT, and OpenAI API to fill in the implementation details.\n\nThrough chaining, we began with an initial implementation that is functionally correct and iteratively improved by adding type hints and refining docstrings. When using OpenAI API, we introduced a selective history approach to make chaining more efficient, which still holds as the chain of tasks gets longer.\n\nIn the next chapter, we will delve deeper into refactoring code with GenAI applications. Later in the book, we will introduce advanced prompt engineering techniques at various stages of the SDLC.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: Is prompt engineering always necessary when working with GenAI?\n\nAnswer: Not always. For tasks with a smaller scope, such as implementing a short function, the five S’s framework is often enough to guide the model toward a good result. Prompt engineering becomes more important when the task grows in complexity, requiring us to either break it into smaller parts or guide the model with reasoning steps to help it succeed.\n\nQuestion 2: How is CoT applied with GitHub Copilot?\n\nAnswer: To apply CoT with Copilot, we specify our reasoning steps in abstracted functions. This approach helps Copilot understand the reasoning behind the desired implementation, allowing it to generate the next lines of code with minimal input, often just by hitting Enter.",
      "content_length": 2309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "Chapter 9\n\nQuestion 3: Is chat history irrelevant when using prompt chaining?\n\nAnswer: No. In chaining, we make use of the previously provided context and supporting data, such as code specifications and reasoning steps, and refer to them again, just as we would ref- erence variables in our code. When additional context is needed, such as test cases to include in our test suite, we rebuild the full prompt.\n\nFurther reading To learn more about the topics that were covered in this chapter, refer to the following resources:\n\nGeometric mean in financial applications, Wikipedia: https://en.wikipedia.org/wiki/\n\nGeometric_mean#Financial\n\nOpenAI learning to reason (CoT): https://openai.com/index/learning-to-reason-\n\nwith-llms/\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n239",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "10\n\nRefactoring Code with GenAI\n\nSoftware developers write in programming languages to instruct a computer to perform specific tasks. The source code in the programming language is translated by a compiler or interpreter into executable computer instructions. One might conclude that programming is only artificial communication with a computer, but it is much more than that.\n\nThe true, implicit goal of a computer program is to communicate with other developers. Once code is written, it is frequently read many times, hours, weeks, or even years later, by the same or other developers. The developers will read the code to understand it, debug it, and integrate it with other systems. The goal must be to communicate with the precision required by software, while maintaining clarity for future developers.\n\nThus, code is written once but read many times. Before GenAI code generation became common, Python practitioners invested significant effort into code quality and style. Today, we can use prompting techniques with GenAI to enhance code quality and style more efficiently.\n\nIn Chapter 7, we introduced techniques for quickly reading and debugging GenAI-generated code. In this chapter, we will focus on using GenAI for refactoring, which means rewriting existing code to improve readability, structure, or performance while preserving its original functionality.\n\nWe will examine a poorly implemented function that computes the distance between two matri- ces. By applying the five S’s framework for prompt precision as introduced in Chapter 4, we will convert a nested loop into a vectorized operation. With chain-of-thought (CoT) prompting, we will restructure the function and delegate implementation details to GitHub Copilot, ChatGPT, and OpenAI API.",
      "content_length": 1765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "242\n\nRefactoring Code with GenAI\n\nTopics covered in this chapter include the following:\n\n\n\nIntroducing code refactoring\n\n\n\nRefactoring for better structure with GenAI\n\n\n\nRefactoring for performance with GenAI\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to Appendix for detailed guidance.\n\nIntroducing code refactoring Code is intended as communication to future software developers. Code that effectively and clearly communicates the purpose and mechanisms is described as clean code.\n\nAs described by Robert C. Martin (better known as Uncle Bob), clean code can be achieved through refactoring. Code refactoring is not merely modifying code, but a disciplined approach to contin- uously improving code. One aspect is removing the indiscriminate code smell that qualitatively characterizes poorly written code (see Further reading). A second aspect is improving the code’s actual design, which should improve its readability and maintainability. This is critical since firms spend an estimated 75% of their software development costs on the evolution and maintenance of software (Sommerville).",
      "content_length": 1688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Chapter 10\n\n243\n\nSoftware developers frequently dedicate time to refactoring, which involves rewriting sections of existing code while keeping the original functionality and purpose intact. The goal is to improve readability, structure, or performance. This is particularly evident in Agile software development, where continuous refactoring is linked to the sustainable evolution of code. The Extreme Pro- gramming (XP) agile methodology explicitly embraces refactoring along with ideals of collective ownership and the use of Test-Driven Development (TDD), as will be discussed in Chapter 13.\n\nIDEs such as PyCharm and VS Code offer built-in tools for tasks such as extracting variables, func- tions, and methods, renaming code elements, and formatting code. Still, they fall short when it comes to more complex scenarios such as extracting a function from non-consecutive lines of code.\n\nNext, we will use an example of a function that computes two types of distances and identify hidden issues caused by poor implementation. Later in the chapter, we will refactor it using GenAI tools.\n\nManhattan and Euclidean distance In Chapter 7, we introduced the Manhattan distance between two matrices, also known as the L1 norm, defined as the sum of absolute differences between corresponding elements. The code example provided used a Flask application that receives matrix data through HTTP requests. If you need a refresher on this topic, refer to Chapter 7.\n\nAnother way to measure the distance between matrices is the Euclidean distance, which is simply the straight-line distance. In two dimensions, the distance between points (x1, y1) and (x2,y2) is calculated as follows:\n\nDistance\n\n\n\n2\n\n2\n\n= √(𝑥𝑥1 − 𝑥𝑥2) When applied to a pair of two-dimensional matrices number of rows and columns), the distance is calculated as follows:\n\n+ (𝑦𝑦1 − 𝑦𝑦2) and 𝑌 𝑋\n\nof the same shape (with the same\n\nDistance\n\n𝑛𝑛\n\n𝑚𝑚\n\n\n\n2\n\n= √ ∑∑(𝑋𝑋𝑖𝑖𝑖𝑖 − 𝑌𝑌𝑖𝑖𝑖𝑖) 𝑖𝑖=1\n\n𝑖𝑖=1\n\nThis measure is also referred to as the L2 norm or Frobenius norm. It is widely used in tasks such as image similarity and machine learning, where it often represents the squared error of a prediction.",
      "content_length": 2150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "244\n\nRefactoring Code with GenAI\n\nImplementing the distance function If we want to implement a function that computes both L1 and L2 norms based on a request, one way to approach this is to build upon our Chapter 7 implementation of the Manhattan distance (L1) and extend it to include the Euclidean distance (L2) as well. A sample implementation, al- though poorly written, is as follows:\n\n@app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\ndata = request.get_json()\n\ndist_type = data.get(\"distance\")\n\nif dist_type == \"L1\":\n\nprint(\"Info: computing L1 distance...\")\n\na = data.get(\"df1\")\n\nb = data.get(\"df2\")\n\ndist = np.sum(np.abs(a - b))\n\nreturn jsonify({\"distance\": dist})\n\nelif dist_type == \"L2\":\n\nprint(\"Info: computing L2 distance...\")\n\na = data.get(\"df1\")\n\nb = data.get(\"df2\")\n\ndist_2 = 0\n\nfor i in range(len(a)):\n\nfor j in range(len(a[i])):\n\ndist_2 += (a[i][j] - b[i][j]) ** 2\n\ndist = np.sqrt(dist_2)\n\nreturn jsonify({\"distance\": dist})",
      "content_length": 962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "Chapter 10\n\nQuick tip: Enhance your coding experience with the AI Code Explainer and Quick Copy features. Open this book in the next-gen Packt Reader. Click the Copy button\n\n(1) to quickly copy code into your coding environment, or click the Explain button\n\n(2) to get the AI assistant to explain a block of code to you.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR visit packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nMaintaining this code may be challenging. For example, reading the parameters a and b is dupli- cated, so if we want to add validations such as checking that both matrices have the same shape, we need to do it twice. Such duplication of code is considered poor programming practice. If we plan to add unit tests, every test needs to mock the request. Adding another distance function would make the function even longer and harder to read.\n\nNext, we will categorize the several types of refactoring and see how they apply to this example.\n\nTypes of refactoring When we refactor code, our goal is to maintain its functionality while improving its quality. There is always a risk that the modifications will break the code or inadvertently change the functionality. To avoid this, unit tests should be in place before making any significant changes to ensure that the code still runs correctly. In addition, refactoring should be performed as a set of incremental changes that do not alter the behavior of the code.\n\n245",
      "content_length": 1585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "246\n\nRefactoring Code with GenAI\n\nThe term refactoring alone is not specific enough to define a single task, according to the five S’s framework introduced in Chapter 4. The changes we aim to make can vary from simple tasks such as renaming a variable to more extensive efforts such as redesigning an entire module.\n\nTo achieve clean code, there are several common refactoring approaches. First, larger and more complex functions should be divided into several that perform a single task. Second, duplicate code should be removed or abstracted by the Do not Repeat Yourself (DRY) principle. Variables and functions should be named for clarity. This principle also applies to arbitrary numeric con- stants and strings, so there should not be hardcoded values (also referred to as magic numbers), but instead have these values assigned to a meaningful constant name.\n\nWe typically divide refactoring tasks into three categories:\n\n\n\nReadability: This includes tasks such as renaming objects, removing redundant imports or excessive comments, introducing constants instead of hardcoded numbers, or adding type hints. For example, within the implementation of calculate_distance, we might change the name of the variable a to df1 to indicate that it represents a matrix in the request. We might also want to add type hints to the request parameters.\n\n\n\nStructure change: This involves organizing code in a better way, such as extracting long code blocks into functions or classes. For example, within the implementation of calculate_distance, we might replace print statements with logger calls, improving how our code handles output. We might also extract the L1 and L2 calculations into separate functions.\n\n\n\nPerformance: This means adjusting the implementation to better handle space or runtime constraints. For example, within the implementation of calculate_distance, instead of using a nested for loop to calculate L2, we might apply vectorization using the NumPy library to improve efficiency.\n\nThere are already good capabilities for refactoring code, particularly for readability, using the built-in tools of our IDEs, as we will see next.",
      "content_length": 2144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "Chapter 10\n\nImproving readability before GenAI Modern IDEs such as VS Code and PyCharm offer built-in refactoring tools that help improve code readability. For example, in PyCharm, we can right-click a parameter we would like to rename and select Refactor | Rename to update the variable and all its references within the relevant scope (in this case, the calculate_distance function). This option is illustrated in Figure 10.1.\n\nFigure 10.1: PyCharm built-in refactoring tools for refactoring/renaming\n\nThis figure also highlights other refactoring options such as changing a callable signature, ex- tracting constants or methods, and more. The authors frequently use these features when refining existing code.\n\n247",
      "content_length": 717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "248\n\nRefactoring Code with GenAI\n\nSimilarly, VS Code allows us to right-click the parameter we want to change and select Rename Symbol to update the variable and all its references within the relevant scope, as shown in Figure 10.2.\n\nFigure 10.2: VS Code Rename Symbol\n\nThe Rename Symbol option appears fourth from the bottom in the figure and performs a similar renaming operation as in PyCharm.\n\nNext, we will explore the limitations of these built-in tools. Later in the chapter, we will use GenAI to address those challenges.\n\nLimitations of code refactoring with IDE tools PyCharm and VS Code offer powerful refactoring features, but these are still limited.\n\nFor example, we may want to extract argument parsing logic for a, b, and dist_type into a sep- arate function, such as the following:\n\ndef parse_request_parameters(r: request) -> Tuple[np.ndarray, np.ndarray, str]:\n\na = np.array(r[\"df1\"])\n\nb = np.array(r[\"df2\"])\n\ndist_type = r[\"distance\"]\n\nreturn a, b, dist_type",
      "content_length": 978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "Chapter 10\n\nThis kind of structural refactoring helps reduce code duplication. However, current IDE tools are unable to extract these three parameters into a single function because they are not written in consecutive lines in the original implementation, as shown in Figure 10.3.\n\nFigure 10.3: Limitation of extracting a code block to a function\n\nQuick tip: Need to see a high-resolution version of this image? Open this book in the next-gen Packt Reader or view it in the PDF/ePub copy.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR go to packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nIn this figure, we cannot extract dist_type, a, and b into a separate function since the lines are not written in a continuous block. As functions become more complex, this approach might no longer be practical.\n\nNext, we will examine how to handle structural refactoring challenges that are beyond the ca- pabilities of IDE tools alone.\n\n249",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "250\n\nRefactoring Code with GenAI\n\nRefactoring with chain-of-thought In Chapter 9, we introduced chain-of-thought (CoT) as a key prompt engineering technique for working with GenAI to generate code. This method involves writing a high-level function as the prompt, while leaving the implementation details for the GenAI application to complete.\n\nWhen refactoring code, we want to include the old implementation as an additional context for the model. This can help it better understand the intended functionality. For instance, in the case of the calculate_distance function, the previous implementation can clarify which parameters need to be extracted from the JSON request: a, b, and dist_type.\n\nRefactoring for a better structure When reviewing the implementation of the calculate_distance function, we can break it down into three main steps:\n\n1. Extracting the request parameters\n\n2. Deciding whether to compute L1 (Manhattan) or L2 (Euclidean) distance\n\n3. Calculating the distance\n\nA CoT prompt that reflects these steps could look like this:\n\n@app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\na, b, dist_type = parse_request_parameters(request)\n\ndist_func = {\"L1\": get_manhattan_dist, \"L2\": get_euclidean_dist}. get(dist_type)\n\ndist = dist_func(a, b)\n\nreturn jsonify({\"distance\": dist})\n\nIn this high-level implementation, we can observe the three main steps:\n\n1. Call a function called parse_request_parameters that should return the values for a, b,\n\nand dist_type.\n\n2. Map dist_function based on the value of dist_type from the request.\n\n3. Compute the distance using a and b.",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "Chapter 10\n\nNote that we have considerable flexibility in how we map dist_type to a function. This could involve more than an inline dictionary, such as using a function or an external dictionary. A less Pythonic but also acceptable approach would be to use a switch-case structure or an if-else block.\n\nWith this function in place, we can now explore how to implement a complete prompt for GitHub Copilot, ChatGPT, and OpenAI API.\n\nCoT for refactoring with GitHub Copilot When using GitHub Copilot, the best practice is to start our prompt with the same function signa- ture as the function we want to refactor. This tells Copilot that we are planning to re-implement the function. Note that Copilot may already suggest an implementation based on the signature alone. However, such an implementation is only as good as asking ChatGPT to refactor a function, since the prompt is not specific enough. For reference, one early completion that Copilot produced from the signature alone is shown in Figure 10.4.\n\nFigure 10.4: Copilot produced a weak refactoring suggestion\n\n251",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "252\n\nRefactoring Code with GenAI\n\nIn this example, the refactoring involved converting the a and b parameters to NumPy arrays. While this vectorization can be beneficial in some cases, it may be unnecessary in others. To guide Copilot toward a more suitable output, we use the CoT approach by providing the full CoT high-level implementation as shown earlier in the chapter. Like in Chapter 9, Copilot is expected to generate the implementations of the missing functions called by the high-level implementation, as illustrated in Figure 10.5.\n\nFigure 10.5: CoT approach triggers an implementation of missing functions\n\nIn this case, Copilot correctly proposed starting with the parse_request_parameters function. Once accepted, it suggested a correct implementation for parsing a, b, and dist_function, as shown in Figure 10.6.\n\nFigure 10.6: Copilot implementation for parse_request_parameters",
      "content_length": 893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Chapter 10\n\nHere, Copilot effectively used the context from the previous version of the function. It suggested reading the value of a from the df1 key in the request JSON, b from df2, and dist_type from distance. This highlights how providing relevant context can lead to desirable results.\n\nFor reference, here are the three implementations Copilot generated:\n\ndef parse_request_parameters(request):\n\ndata = request.get_json()\n\na = np.array(data.get(\"df1\"))\n\nb = np.array(data.get(\"df2\"))\n\ndist_type = data.get(\"distance\")\n\nreturn a, b, dist_type\n\ndef get_manhattan_dist(a, b):\n\nprint(\"Info: computing L1 distance...\")\n\nreturn np.sum(np.abs(a - b))\n\ndef get_euclidean_dist(a, b):\n\nprint(\"Info: computing L2 distance...\")\n\ndist_2 = 0\n\nfor i in range(len(a)):\n\nfor j in range(len(a[i])):\n\ndist_2 += (a[i][j] - b[i][j]) ** 2\n\nreturn np.sqrt(dist_2)\n\nThese functions mirror the original logic from calculate_distance while offering improved abstraction and avoiding code duplication. In a later section of the chapter, we will explore how to refactor for performance, including replacing nested loops with vectorized operations.\n\nNext, we will explore how to apply the CoT approach for refactoring with ChatGPT and OpenAI API. This requires a bit more effort to set up effectively.\n\n253",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "254\n\nRefactoring Code with GenAI\n\nLeveraging CoT for refactoring with ChatGPT and OpenAI API Just like GitHub Copilot, we can also make use of ChatGPT and OpenAI API to refactor our code to have a better structure. To achieve this, we need to provide these tools with both the original implementation for context and the new structure we want to complete.\n\nA structural refactoring prompt for ChatGPT can follow this general template:\n\nCONTEXT: You are provided with\n\n1. Python function enclosed with {{{ fUNCTION }}} with calls to missing implementations.\n\n2. Old implementation enclosed with {{{ OLD }}} for reference.\n\nTASK: implement the missing functions.\n\nFUNCTION: {{{ ... omitted for brevity ... }}}\n\nOLD: {{{ ... omitted for brevity ...}}}\n\nCODE:\n\nNote that in contrast to CoT for new code, refactoring prompts requires giving ChatGPT both the new structure of the function and the reference implementation enclosed in {{{ OLD }}}. This format is like how we prompt Copilot and produces desirable results that preserve functionality, even when using more cost-effective models such as GPT-4o. Figure 10.7 shows the output we received from GPT-4o for this prompt.",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "Chapter 10\n\nFigure 10.7: ChatGPT suggested refactoring\n\nHere is the implementation for the missing functions proposed by ChatGPT:\n\ndef parse_request_parameters(req):\n\ndata = req.get_json()\n\ndist_type = data.get(\"distance\")\n\na = np.array(data.get(\"df1\"))\n\nb = np.array(data.get(\"df2\"))\n\nreturn a, b, dist_type\n\ndef get_manhattan_dist(a, b):\n\nprint(\"Info: computing L1 distance...\")\n\nreturn float(np.sum(np.abs(a - b)))\n\ndef get_euclidean_dist(a, b):\n\nprint(\"Info: computing L2 distance...\")\n\nreturn float(np.sqrt(np.sum((a - b) ** 2)))\n\n255",
      "content_length": 539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "256\n\nRefactoring Code with GenAI\n\nThis output resembles the structure Copilot suggested, focusing on extracting the distance calcu- lations into smaller functions and removing redundancy. One notable difference is that ChatGPT was more consistent and used NumPy vectorization for both distance functions, and not just the Manhattan distance method.\n\nOpenAI API prompt When we refactor code for improved structure, GitHub Copilot and ChatGPT often prove to be more suitable than OpenAI API. This is because using the API typically requires more manual effort to craft the prompt programmatically. OpenAI API becomes more practical only when we need to scale changes across multiple similar functions, and we find that refactoring the structure with CoT might not be one of these cases.\n\nIn summary, GenAI supports the restructuring of existing code by combining prior implemen- tations with new instructions. We recommend trying this technique with code you encounter in your day-to-day work or even in third-party libraries.\n\nNext, we will briefly introduce refactoring for performance using GenAI, a topic we will cover more extensively in Chapter 14.\n\nPerformance refactoring with GenAI Performance refactoring refers to changes made to the code that preserve its functionality while improving runtime or memory efficiency. One common approach is to use vectorized compu- tations. These can reduce the runtime by benefiting from cache, lower overhead, and parallel computation.\n\nFor instance, in the GitHub Copilot implementation of calculate_distance example, this would mean replacing a nested for loop with a vectorized computation of the Euclidean distance. We will explore further runtime and space complexity in Chapter 14, including when and why to scale system capacity and the trade-offs involved. Until then, let us demonstrate how a simple optimization through vectorization might be applied.\n\nPerformance refactoring with GitHub Copilot As with the CoT approach, we will leverage the context of existing code along with our desired structure. This time, we will specify the library we would like to implement a code block instead of the original implementation.",
      "content_length": 2175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Chapter 10\n\nFor example, consider the function Copilot extracted for us to compute the Euclidean distance:\n\ndef get_euclidean_dist(a, b):\n\nprint(\"Info: computing L2 distance...\")\n\ndist_2 = 0\n\nfor i in range(len(a)):\n\nfor j in range(len(a[i])):\n\ndist_2 += (a[i][j] - b[i][j]) ** 2\n\nreturn np.sqrt(dist_2)\n\nThis function computes the squared distance dist_2 and then returns its square root.\n\nTo vectorize the squared distance computation, we can use NumPy, a widely used Python library that supports optimized operations on arrays and matrices. In our prompt, our goal is to preserve the original logic:\n\n1. Compute the squared distance\n\n2. Compute the distance\n\nTo guide Copilot toward the desired refactoring, we can add a cue using a NumPy function call. The first prompt would be placed near the squared distance step:\n\nDist_2 = np.\n\nThis is shown in Figure 10.8:\n\nFigure 10.8: Prompt with an np. library call as a cue\n\nThen, for the square root step, we would add another cue:\n\ndist = np.\n\n257",
      "content_length": 997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "258\n\nRefactoring Code with GenAI\n\nThis is sufficient context for Copilot to complete the correct implementation using NumPy, as shown in Figure 10.9:\n\nFigure 10.9: Copilot recognized the correct function from the cue\n\nThese inline completions can be highly effective. However, before applying performance opti- mizations, it is important to determine whether scaling or optimizing code is necessary. We will explore these considerations further in Chapter 14.\n\nNext, we will look at how to achieve the same outcome using ChatGPT and OpenAI.\n\nPerformance refactoring with ChatGPT and OpenAI When using ChatGPT and OpenAI API for performance refactoring tasks such as vectorizing computations, common transformations such as adopting the NumPy package can be achieved by following the five S’s best practices, as introduced in Chapter 4. Like in Chapter 7, where we include the full file content in the prompt along with the specific line we want to explain, here, we also provide the complete function implementation together with the files that need refactoring.\n\nHere is a sample prompt template that can be used in ChatGPT:\n\nCONTEXT: You are provided with:\n\n1. A Python function implementation enclosed with {{{ FUNCTION }}}\n\n2. Lines to be refactored enclosed with {{{ OLD }}}\n\n3. A library to be used in the new code enclosed with {{{ LIBRARY }}}\n\nTASK: Return a new implementation for the old lines using the specified library.\n\nFUNCTION: {{{ ... }}}\n\nOLD: {{{ ... }}}",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "Chapter 10\n\nLIBRARY: {{{ ... }}}\n\nREFACTORED CODE:\n\nNote that in this template, we are not requiring the model to follow the same sequence of steps as in Copilot, such as first computing dist_2 and then applying the square root. This allows ChatGPT to suggest more common implementations that may take a different route. By specifying LIBRARY, we are explicitly guiding the GenAI tools toward an expected solution.\n\nImportant note\n\nIn Chapter 14, we will learn how to replace the library constraint with constraints on runtime and space capacity. If we do not specify either constraints or a library, the model will suggest the most popular implementation, which may not align with our goals. As a result, the response often lacks specific instructions and does not follow the five S’s framework.\n\nFor reference, let us consider a vectorization prompt for the get_euclidean_distance function that is available in this book’s repository at ch10/prompts/performance_refactoring_chatgpt. txt. The response we got from ChatGPT with the GPT-4o economic model is shown in Figure 10.10.\n\nFigure 10.10: ChatGPT proposed vectorized implementation\n\n259",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "260\n\nRefactoring Code with GenAI\n\nThe implementation shown in the figure is a common implementation for calculating Euclide- an distance. It vectorizes the element-wise distance and then uses the linear algebra module in NumPy to compute the norm. This version is typically easier to read and more flexible than what Copilot suggested.\n\nNote\n\nThis prompt structure can be reused when comparing implementations across mul- tiple libraries offering similar functionality. For instance, when evaluating alter- natives between SciPy and NumPy, this format allows us to generate and compare implementations quickly.\n\nNext, we will look at how to achieve equivalent results using OpenAI API.\n\nVectorizing with OpenAI API Creating a prompt for OpenAI API usually takes more effort than using ChatGPT to get comparable results. This approach is most suitable when we want to produce multiple new implementations at scale, such as converting many for loops to vectorized NumPy expressions.\n\nTo create a system prompt, we can adapt our ChatGPT prompt structure into two components:\n\nSURROUND = \"\"\"You are provided with:\n\n1. A Python function implementation enclosed with {{{ FUNCTION }}}\n\n2. Lines to be refactored enclosed with {{{ OLD }}}\n\n3. A library to be used in the new code enclosed with {{{ LIBRARY }}}.\"\"\"\n\nSINGLE_TASK = \"Your task is to return a new implementation for the old lines using the specified library.\"\n\nLINES = \"\"\"dist_2 = 0\n\nfor i in range(len(a)):\n\nfor j in range(len(a[i])):\n\ndist_2 += (a[i][j] - b[i][j]) ** 2\n\n\"\"\"\n\nWe can define a get_user_prompt function as such:\n\ndef get_user_prompt(func: callable, library: str, lines: str) -> str:\n\nreturn f\"\"\"\n\nFUNCTION: {{{{{{ {inspect.getsource(func)} }}}}}}",
      "content_length": 1716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "Chapter 10\n\nLINES: {{{{{{ {lines} }}}}}}\n\nLIBRARY: {{{{{{ {library} }}}}}}\n\nREFACTORD:\n\n\"\"\"\n\nWe can then use both prompts when calling the service:\n\nif __name__ == \"__main__\":\n\nclient: OpenAI = OpenAI()\n\nsystem_prompt = f\"{SURROUND} {SINGLE_TASK}\"\n\nuser_prompt = get_user_prompt(get_euclidean_distance, \"NumPy\", LINES)\n\ncompletion: ChatCompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": system_prompt},\n\n{\"role\": \"user\", \"content\": user_prompt},\n\n],\n\n)\n\nprint(\"Explanation:\", completion.choices[0].message.content)\n\nFor reference, the full script is available in ch10/prompts/performance_refactoring_openai. py. The output we obtained was equivalent to the result from ChatGPT.\n\nTo conclude, GitHub Copilot is the quickest way to get performance refactoring suggestions. ChatGPT is more suitable when we want widely adopted implementations or to quickly assess different libraries’ implementations with a reusable prompt. OpenAI API is helpful in less com- mon cases when we want to generate bulk transformations across many files or a large code base.\n\nMore advanced performance refactoring In Part 3 of the book, Chapter 14 will address runtime and space complexity optimization. We will demonstrate how combining code completion tools with prompt engineering techniques such as chaining can help us find the right balance between space and time complexity.\n\n261",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "262\n\nRefactoring Code with GenAI\n\nWe will use GitHub Copilot, ChatGPT, and OpenAI to assess the program’s current performance, estimate its runtime and space complexity in terms of Big-O notation, and introduce optimized solutions to extend input capacity with minimal trade-off. Stay tuned for a detailed exploration of these topics.\n\nSummary In this chapter, we explored how GitHub Copilot, ChatGPT, and OpenAI API can assist in rewriting code while preserving its original functionality. These tools can enhance readability, improve structure, and boost performance.\n\nClean code is a shared goal among software developers, especially as projects grow and become complex. Most IDEs already offer strong built-in tools for refactoring, which we frequently use. However, when those tools reach their limits, GenAI can step in. It can support structural refac- toring by using CoT techniques with the original implementation, or it can optimize performance through the five S’s framework.\n\nAlthough refactoring is useful, writing clean and structured code from the start is usually simpler. In the next chapter, we will introduce fine-tuning with OpenAI API as an alternative to few- shot learning. Fine-tuning enables us to adapt a pretrained model to a specific domain, such as generating clean and well-organized code. Unlike prompt engineering, fine-tuning separates the specialization process from the prompt itself, which can be beneficial for larger-scale projects.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: Why do we need GenAI for refactoring tasks if IDEs already provide built-in tools?\n\nAnswer: While built-in tools are powerful, they have limitations. For example, they can only ex- tract code blocks to external functions when the lines are written consecutively in a single block. GenAI allows us to restructure Python objects more flexibly, using the CoT technique with the original implementation as a reference.\n\nQuestion 2: How does the CoT approach differ when refactoring existing code compared to generating new code?",
      "content_length": 2127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Chapter 10\n\nAnswer: When generating new code, we typically describe the high-level function we want GenAI to complete. For refactoring, we also include the original implementation in the prompt. This helps GenAI understand the intended functionality, such as parsing specific JSON request keys or assigning them to specific variable names.\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\nClean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin (aka Uncle Bob):\n\nhttps://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882\n\nCode smell: https://en.wikipedia.org/wiki/Code_smell\n\nRefactoring: Improving the Design of Existing Code, Second Edition: https://www.amazon.com/\n\nRefactoring-Improving-Existing-Addison-Wesley-Signature/dp/0134757599\n\nDesign Patterns: Elements of Reusable Object-Oriented Software: https://www.amazon.com/Design-\n\nPatterns-Elements-Reusable-Object-Oriented/dp/0201633612\n\nMariane Anaya. Clean Code in Python: Develop maintainable and efficient code, Second Edition, 2021:\n\nhttps://www.packtpub.com/en-us/product/clean-code-in-python-9781800560215\n\nKent Beck and Cynthia Andres, Extreme Programming Explained: Embrace Change, 2004: https://\n\nwww.amazon.com/Extreme-Programming-Explained-Embrace-Change/dp/0321278658\n\nIan Sommerville. Software Engineering, 2015: https://www.amazon.com/Software-Engineering-\n\n10th-Ian-Sommerville/dp/0133943038\n\n263",
      "content_length": 1477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "264\n\nRefactoring Code with GenAI\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.",
      "content_length": 442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "11\n\nFine-Tuning Models with OpenAI\n\nConcluding Part 2, this chapter explores how to train large language models (LLMs) to specialize in completing a specific task. This is achieved by fine-tuning the model’s parameters, known as weights, through the OpenAI platform. This is quite like the prompt engineering strategy few- shot learning, sometimes referred to as in-context learning, which we introduced earlier in the book as a technique for helping the model to specialize in a particular domain. Fine-tuning, however, is best suited for larger-scale projects where we want to separate the specialization aspect from the actual prompt.\n\nWe will delve into a use case where this specialization of the model is required to produce code while avoiding comments clutter. We will examine how outputs from the GPT-4o mini model may be programmatically correct but fail to follow our style guidelines and requirements for concise explanations and code comments. We will construct a fine-tuning JSONL file with high-quality examples that illustrate desired outcomes, initiate a fine-tuning job through the OpenAI website, and compare the improved results with the output of the base model.\n\nThis chapter covers the following topics:\n\n\n\nFine-tuning LLMs toward a specialization\n\n\n\nExperimenting with the OpenAI Playground\n\n\n\nCrafting a JSONL file for fine-tuning\n\n\n\nFine-tuning a model with the OpenAI GUI\n\n\n\nComparing our results",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "266\n\nFine-Tuning Models with OpenAI\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\n\n\nAn OpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\nFor instructions on setting up OpenAI API access, see Chapter 2. If you need help with creating an OpenAI account, refer to the Appendix for detailed guidance.\n\nFine-tuning LLMs toward a specialization When working on complex coding tasks with LLMs, we may need to employ prompt engineering techniques. In Chapter 8, we saw how including a few examples in the prompt implicitly instructs the model to follow the expected coding style when writing code. This show, don’t tell approach effectively guides the model by enriching our existing prompt, as illustrated in Figure 11.1:\n\nFigure 11.1: Few-shot learning prompt enrichment\n\nBy using this approach, the model adapts to produce outputs that align with the examples pro- vided, effectively specializing in the given task.",
      "content_length": 1067,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "Chapter 11\n\nLLMs specialization A specialization for LLMs focuses on a specific task or a set of closely related tasks. Here are some examples of specializations you might want your model to develop:\n\nGenerating completion code that follows a specific coding style\n\n\n\nCreating a unit test suite that covers key scenarios such as happy paths, bad inputs, edge cases, and error handling\n\n\n\nAssessing the quality of a function’s docstring\n\n\n\nRefactoring for loops into comprehensions\n\nWe typically identify a specialization as either a single system prompt or a set of closely related system prompts, each incorporating the surrounding context and a single-task focus. These tasks are generally non-trivial for an LLM, such as code completion to reverse a string, requiring additional effort in prompt crafting, such as prompt engineering or fine-tuning.\n\nFor certain tasks, few-shot learning can be particularly effective. For example, if refactoring for loops into comprehensions never involves more than two levels of nesting, providing a few ex- amples in the prompt may be sufficient to achieve reliable results.\n\nHowever, for deeper specializations, such as breaking longer functions into helper functions to improve code quality, we may need to incorporate a larger set of training examples with the desired output, rather than just a few. We might include examples of naming conventions and indentation. Some examples may show the extraction of code blocks into helper functions, and others may show how to eliminate unnecessary explanations that come along with the code. Including all these examples directly in the prompt increases the prompt’s length and, therefore, the cost associated with the input token length. It also makes it harder to maintain.\n\nFine-tuning an LLM Fine-tuning a model is a process that aims to achieve better outputs by using a pre-trained model such as GPT-4o mini and training it toward a specialization using a small set of training exam- ples that depict the intended output. The fine-tuning process adjusts the model’s parameters (or weights, as discussed in Chapter 6) to achieve the desired outcomes shown in the provided examples. This specialization process is shown in Figure 11.2:\n\n267",
      "content_length": 2230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "268\n\nFine-Tuning Models with OpenAI\n\nFigure 11.2: Leveraging fine-tuning for a better output\n\nThe specialization process uses the crafted training examples to fine-tune the LLM, resulting in a new LLM that is ready to be prompted without requiring further examples in the prompts.\n\nFavoring fine-tuning over prompt engineering Based on studies, and confirmed with our experience, fine-tuning requires more training examples than few-shot learning because the examples are retraining the model’s parameters, rather than simply being part of the prompt. In this chapter, we use 15 examples, which is slightly above the OpenAI API’s minimum of 10 for initiating a fine-tuning job. For more complex cases, tens or even hundreds of examples may be necessary, which can be time-consuming to find.\n\nFortunately, for most of our daily coding tasks, we will find that few-shot learning is sufficient where only a few examples are needed. However, for larger-scale projects, or when few-shot learning does not meet our needs, fine-tuning may be a better alternative. Generally, it is good practice to begin with the few-shot technique and turn to fine-tuning only when the results are not satisfactory.\n\nTask specializing in cleaner code In Chapter 7, we noted that LLMs often include explanations and example usage along with the code. The LLMs also add code comments when function implementations become longer to compensate for less readable code. When working with higher-quality code, we may prefer the model to return only the code without extra explanations. Additionally, rather than relying on comments in lengthy functions, we might want the model to break the logic into smaller, more manageable functions.",
      "content_length": 1707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "Chapter 11\n\nThis is a perfect use case for fine-tuning since the effort invested in creating a training file of examples is justified as it can be applied to an entire repository using short prompts alone. Later in the book, we will explore how fine-tuning supports advanced refactoring initiatives, such as extracting logging, monitoring, and error-handling responsibilities into separate functions.\n\nNext, we will explore how to experiment with the OpenAI API Playground and observe that the GPT-4o mini model requires improvements in its output implementation. Later in the chapter, we will address these improvements through the fine-tuning process.\n\nExperimenting with the OpenAI API Playground In this section, we will use OpenAI’s graphical user interface (GUI) Playground to quickly view our results for a given preset, which represents a combination of a model and the selected call parameters. This interface is an excellent tool for rapid experimentation and for comparing models before implementing them programmatically in our software.\n\nTo access the Playground, ensure that your OpenAI account has credits applied to unlock tier 1, as explained in Chapter 2. Then, go to https://platform.openai.com/playground/chat. You can also reach it by navigating to the home page, https://platform.openai.com, selecting Play- ground from the top-right menu, and then choosing the Chat option from the left menu. The opening screen of the Playground is shown in Figure 11.3:\n\nFigure 11.3: Opening screen of the Playground interface for OpenAI\n\n269",
      "content_length": 1550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "270\n\nFine-Tuning Models with OpenAI\n\nOn this page, we can craft prompts for OpenAI, as introduced in Chapter 2, and try different pre- sets. This includes the system and user prompts, along with parameters such as temperature and maximum tokens. Additional parameters include the following:\n\n\n\nTop P is an alternative to the temperature, setting a minimal probability threshold that the model’s output must meet. In some cases, higher values can prevent hallucinations.\n\n\n\nThe Store logs checkbox saves the prompt so it can be restored later.\n\nWe can also modify the response format and add functions. These topics will be discussed in detail in later chapters of this book.\n\nNote that the experimentation in the Playground is free for tier 1 or higher accounts, but not all models are available. For instance, as of April 2025, GPT-o1 and GPT-o1-preview models are not available in the reasoning models for experimentation. Figure 11.4 shows the available models.\n\nFigure 11.4: Available reasoning models in the OpenAI API Playground",
      "content_length": 1034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "Chapter 11\n\nFor the examples in this chapter, we will use the GPT-4o mini 2024-07-18 model, which is cheaper to use with the API compared to the GPT-o series models.\n\nComputing quadratic roots with GPT-4o mini As a benchmark for the model output, we will evaluate how GPT-4o mini handles implementing functions that extend beyond a few lines of code, such as computing the quadratic roots of the equation . To find the roots of this equation, we compute the two values that satisfy the quadratic formula:\n\n2\n\n𝑎𝑎𝑎𝑎\n\n+ 𝑏𝑏𝑎𝑎 + 𝑐𝑐 𝑐 𝑐\n\n2\n\n\n\n−𝑏𝑏 𝑏 √𝑏𝑏 2𝑎𝑎\n\n− 4𝑎𝑎𝑎𝑎\n\n𝑥𝑥1,2 =\n\nIn our case, we want to return the roots only when they are real, which occurs when the discrim- inant\n\n is larger than or equal to 0.\n\n2\n\nTo obtain a function implementation that returns the real roots only, we define the function signature:\n\n𝑏𝑏\n\n− 4𝑎𝑎𝑎𝑎\n\ndef get_quadratic_roots_only_if_real (a:int, b:int, c:int) -> Tuple[float, float]\n\nWe will use this function signature in our prompt to the model. As usual, we will structure our prompt according to the five S’s framework, as introduced in Chapter 4, where the system prompt provides the surrounding context and specifies the single task to be completed, and the user prompt includes supplementary details, such as the function signature and a lead-in cue.\n\nFor the system prompt, we will add the following instruction:\n\nYou will be provided with a Python function signature enclosed with {{{ FUNCTION }}}. Your task is to implement it.\n\nThis instruction provides the model with sufficient context for the code completion task using a Python function signature.\n\nFor the user prompt, we will ask it to implement the get_quadratic_roots_only_if_real func- tion and include the CODE lead-in cue:\n\nFUNCTION: {{{def get_quadratic_roots_only_if_real (a:int, b:int, c:int) -> Tuple[float, float] }}}\n\nCODE:\n\n271",
      "content_length": 1835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "272\n\nFine-Tuning Models with OpenAI\n\nThe function’s name, get_quadratic_roots_only_if_real, signals the model to return the qua- dratic roots only if they are real.\n\nImplementing the quadratic roots calculator A typical approach to computing quadratic roots and returning them if they are real involves the following steps:\n\n1. Compute the discriminant\n\n.\n\n2 2. Verify that the discriminant is positive so that the roots are real.\n\n𝑏𝑏 3. Return the two roots if real.\n\n− 4𝑎𝑎𝑎𝑎\n\nSince this function requires more than just a few lines of code, the LLM is likely to introduce comments within the code and include surrounding explanations with the implementation.\n\nFor reference, Figure 11.5 shows the explanation provided by the model before the actual code implementation.\n\nFigure 11.5: Base model output with supplementary explanations",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "Chapter 11\n\nThe following is the code output generated by the model:\n\n```python\n\nimport math\n\nfrom typing import Tuple\n\ndef get_quadratic_roots_only_if_real(a: int, b: int, c: int) -> Tuple[float, float]:\n\n# Calculate the discriminant\n\ndiscriminant = b * b - 4 * a * c\n\n# If discriminant is negative, there are no real roots\n\nif discriminant < 0:\n\nraise ValueError(\"The equation does not have real roots.\")\n\n# Calculate the two solutions using the quadratic formula\n\nsqrt_disc = math.sqrt(discriminant)\n\nx1 = (-b + sqrt_disc) / (2 * a)\n\nx2 = (-b - sqrt_disc) / (2 * a)\n\nreturn (x1, x2)\n\nThis implementation covers all three required steps: computing the discriminant, checking for real roots, and returning the two solutions when appropriate. However, the solution includes redundant explanations and a function that is cluttered with comments. Later in the chapter, we will see how fine-tuning can help us achieve improved results by fine-tuning the LLM to return only code for code completion tasks.\n\nNext, we will see how to prepare the training data to fine-tune the GPT-4o mini model. We will later compare this model with those of the default GPT-4o mini model.\n\nCrafting a JSONL file for fine-tuning One of the initial challenges in fine-tuning a model is gathering enough training examples. When fine-tuning a model through OpenAI, a minimum of 10 training examples is required. However, for most tasks that are beyond trivial, tens or even hundreds of examples might be necessary, depending on the task. According to OpenAI’s official documentation, satisfactory results are typically achieved with 50 training examples.\n\n273",
      "content_length": 1634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "274\n\nFine-Tuning Models with OpenAI\n\nIn our case, where we want to fine-tune a model to produce outputs consisting solely of code without comments, 15 examples are sufficient, as we will see later in the chapter. When crafting our training set, we will utilize two techniques: desirable examples that are like few-shot learning and contrastive learning. Contrastive learning builds on the idea of in-context learning by first presenting an incorrect or incomplete output, followed by an additional user-assistant prompt pair that provides the desired output. For more details, see the Further reading section in this chapter.\n\nFine-tuning file structure We will upload a JSONL file as the training data for our fine-tuning process, where each line in the file is a valid JSON object representing one training example. A minimal prompt should include a system prompt and a training example with a user prompt and an assistant prompt. Each as- sistant prompt is attributed with a weight of 0 for undesirable output and 1 for desirable output. In the following desirable example, the assistant prompt has a weight of 1, and line breaks have been added for clarity:\n\n{\"messages\": [\n\n{\"role\": \"system\", \"content\": \"You will be provided with a Python function signature enclosed with {{{ FUNCTION }}}. Your task is to implement it.\"},\n\n{\"role\": \"user\", \"content\": \"FUNCTION: {{{def get_euclidean_distance(x1, y1, x2, y2):}}}\\n CODE: \"},\n\n{\"role\": \"assistant\", \"content\": \"def get_euclidean_distance(x1, y1, x2, y2): \\n return ((x1-x2)**2 + (y1-y2)**2)**0.5\", \"weight\": 1}\n\n]}\n\nThis format may look familiar, as it resembles the messages parameter used in OpenAI API calls. However, in this case, we assign a weight to the assistant message. The preceding example is a training instance included in the fine_tuning.jsonl file for this chapter, located at ch11/fine_ tuning.jsonl, demonstrating the expected implementation of the get_euclidean_distance function.\n\nA minimal training line template should be as follows, containing at least one user prompt and one assistant prompt. Here, too, line spaces are added for clarity:\n\n{\"messages\": [\n\n{\"role\": \"user\", \"content\": \"Your Content Here\"},\n\n{\"role\": \"assistant\", \"content\": \"Your desired output here\", \"weight\": 1}]}",
      "content_length": 2262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "Chapter 11\n\nWe could also incorporate a system prompt if it helps refine the specialization, as we will explore later in the chapter.\n\nWith contrastive learning examples, we will include additional user prompt and assistant prompt pairs. The first pair represents an undesirable output and is assigned a weight of 0, as such:\n\n{\"messages\": [\n\n{\"role\": \"system\", \"content\": \"...omitted for brevity...\"},\n\n{\"role\": \"user\", \"content\": \"FUNCTION: {{{def sum_of_squares(n: int) -> int:}}}\\n CODE: \"},\n\n{\"role\": \"assistant\", \"content\": \"Sure! here is the code:\\n\\n```python\\ ndef sum_of_squares(n: int) -> int:\\n return sum(i * i for i in range(1, n + 1))\\n```\", \"weight\": 0},\n\n{\"role\": \"user\", \"content\": \"Include only code.\"},\n\n{\"role\": \"assistant\", \"content\": \"def sum_of_squares(n: int) -> int:\\n return sum(i * i for i in range(1, n + 1))\", \"weight\": 1}]}\n\nIn this example, we provide two sets of user and assistant messages. The initial assistant response is undesirable because it includes explanatory text alongside the code, so it is assigned a weight of 0. The second user prompt corrects this by explicitly instructing the assistant to provide only the code, and this response is given a weight of 1.\n\nStrategy A practical approach to constructing a fine-tuning training file is to start with the simplest cases of desired outputs and gradually introduce more complex examples to cover the specialization we want the model to develop. While the few-shot examples we saw in Chapter 8 included one or two cases, for fine-tuning, we will use 15 examples, allocating 7-8 examples to each of the following objectives:\n\n\n\nCode-only: Positive and contrastive\n\nNo comments: Positive and contrastive\n\nThese examples will be sufficient to generate code without surrounding explanations and with- out comments within the function. The complete JSONL file is available in this book’s repository at ch11/fine_tuning.jsonl. Next, we will present a single example from each of the preceding objectives.\n\n275",
      "content_length": 1997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "276\n\nFine-Tuning Models with OpenAI\n\nCode-only – five training examples In these cases, we will include simple implementations of functions spanning 1-2 lines. Each example consists of a single user and assistant prompt pair, ensuring that the output consists solely of code. Take the following example:\n\n{\"messages\": [\n\n{\"role\": \"system\", \"content\": \"You will be provided with a Python function signature enclosed with {{{ FUNCTION }}}. Your task is to implement it.\"},\n\n{\"role\": \"user\", \"content\": \"FUNCTION: {{{def get_arithmetic_mean(a, b)}}}\\n CODE: \"},\n\n{\"role\": \"assistant\", \"content\": \"def get_arithmetic_mean(a, b): \\n return (a+b)/2\", \"weight\": 1}]}\n\nIn this example, we provide a desirable implementation of get_arithmetic_mean without any surrounding explanations. Note that the actual JSONL line contains no extra line breaks. This is an example of the training pattern with just a single positive example indicated by the weight 1. Additionally, we will include three more examples for the get_euclidean_distance, reverse_ string, is_even, and is_odd functions.\n\nContrastive code-only – three training examples In addition to providing high-quality examples, we can also include contrastive examples. These scenarios involve an initial assistant response that is undesirable, such as including explanations or comments in the code, followed by a chained request to refine the output into a more desirable version. The chained user prompt should follow the best practices introduced in Chapter 9, such as the following:\n\n\n\nInclude code only\n\n\n\nRemove code comments\n\n\n\nExtract lower-level details to functions\n\nAs introduced in Chapter 9, when crafting chained prompts, we avoid unnecessary phrasing such as “Your answer is wrong!” since it does not follow the short best practice and adds no value. Instead, we should determine whether the assistant’s output is significantly incorrect or if only minor adjustments are needed.",
      "content_length": 1939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "Chapter 11\n\nIn the first case, if the output is far off, the user prompt should be refined to produce a more desir- able result, and this example should not be included in our training data. In the second case, if the model’s output only requires minor refinements, we should focus on providing clear guidance on the necessary adjustments, such as instructing the model to return only code or extract low-level details into helper functions. This type of direction is more precise and informative.\n\nFor example, the following is a contrastive code-only training example for an implementation of is_palindrome, where the first assistant prompt includes an explanation along with the code. This output is marked with a weight of 0 to indicate undesirability. It is followed by a chained user prompt instructing the assistant to return only the code, leading to a refined implementation with a weight of 1:\n\n{\"messages\": [\n\n{\"role\": \"system\", \"content\": \"...omitted...\"},\n\n{\"role\": \"user\", \"content\": \"FUNCTION: {{{def is_palindrome(s: str) -> bool:}}}\\nCODE: \"},\n\n{\"role\": \"assistant\", \"content\": \"A palindrome is a word, phrase, or [...] otherwise `False`.\\n\\n```python\\ndef is_palindrome(s: str) -> bool:\\n s = s.lower()\\n return s == s[::-1]\\n```\", \"weight\": 0},\n\n{\"role\": \"user\", \"content\": \"Include code only.\"},\n\n{\"role\": \"assistant\", \"content\": \"def is_palindrome(s: str) -> bool:\\n s = s.lower()\\n return s == s[::-1]\", \"weight\": 1}]}\n\nThe final assistant message consists solely of the implementation code and is assigned a weight of 1 to indicate a desirable output. The fine-tuning file also includes similar contrastive examples for sum_of_squares and flatten_list_of_lists.\n\nCode with no comments – five examples In these examples, we focus on functions that go beyond simple 1-2 line implementations, breaking down more complex tasks into lower-level abstraction functions. This approach enhances read- ability in the main function and eliminates the need for inline comments. For instance, consider the implementation of get_reversed_longer_string, which takes two strings as input and returns the reverse of the longer string. It can be structured as follows:\n\ndef get_reversed_longer_string(s1:str, s2: str) -> str:\n\nlonger_s = get_longer_string(s1, s2)\n\nreversed_longer_s = reverse_string(longer_s)\n\nreturn reversed_longer_s\n\n277",
      "content_length": 2345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "278\n\nFine-Tuning Models with OpenAI\n\ndef get_longer_string(s1, s2):\n\nreturn s1 if len(s1) > len(s2) else s2\n\ndef reverse_string(s):\n\nreturn s[::-1]\n\nWhile more concise implementations exist, they may be less readable. The preceding structure keeps get_reversed_longer_string clear and shifts the implementation details to dedicated lower-level functions.\n\nTo include this approach in our training examples, we will add the following:\n\n{\"messages\": [{\"role\": \"system\", \"content\": \"...omitted for brevity...\"}, {\"role\": \"user\", \"content\": \"FUNCTION: {{{def get_reversed_longer_ string(s1, s2):}}}\\n CODE: \"},\n\n{\"role\": \"assistant\", \"content\": \"def get_reversed_longer_string(s1, s2):\\n longer_s = get_longer_string(s1, s2)\\n reversed_longer_s = reverse_ string(longer_s)\\n return reversed_longer_s\\n\\ndef get_longer_string(s1, s2):\\n return s1 if len(s1) > len(s2) else s2\\n\\ndef reverse_string(s):\\n return s[::-1]\", \"weight\": 1}]}\n\nThis aligns with our preferred coding style of extracting lower-level functionality into separate functions. Similarly, we will include example implementations for the following:\n\n• • •\n\nget_max_multiplication_from_lists\n\nget_maximal_second_element_squared\n\nget_sum_of_absolute_differences\n\nget_highest_even\n\nThese five implementations will demonstrate the separation of abstraction levels in functions, serving as a guideline for the fine-tuning process.\n\nContrastive no comments – two examples To complete our dataset, we will include examples of initial implementations with excessive comments, followed by a refined version where low-level details are extracted into separate functions. For instance, consider a function that, given a list of lists, computes the average of the second elements of each sublist:\n\nget_average_of_second_elements(numbers: List[List[int]])",
      "content_length": 1804,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "Chapter 11\n\nThis function involves more complex logic than our previous examples, which may lead to an implementation with excessive comments. To steer the model toward a solution that separates levels of abstraction, we first provide an implementation with inline comments, followed by a chained prompt that refines the output. An initial incorrect assistant response might look like this:\n\ndef get_average_of_second_elements(numbers: List[List[int]]) -> float:\n\nif not numbers: # Check if the input list is empty\n\nreturn 0\n\nsecond_elements = [sublist[1] for sublist in numbers] # Extract second elements\n\nreturn sum(second_elements) / len(second_elements) # Compute the average\n\nWhile this implementation is correct, the inline comments add unnecessary fluff. When including this negative example in our dataset, we assign it a weight of 0.\n\nNext, we chain a user prompt requesting a refinement:\n\nExtract low level details to functions\n\nThis results in a cleaner implementation:\n\nimport statistics\n\ndef get_average_of_second_elements(numbers: List[List[int]]) -> float:\n\nsecond_elements = get_second_elements(numbers)\n\nreturn statistics.mean(second_elements)\n\ndef get_second_elements(numbers: List[List[int]]) -> List[int]:\n\nreturn [sublist[1] for sublist in numbers]\n\nBy abstracting the extraction logic into a separate function, get_average_of_second_elements remains more readable. The corresponding example looks like this:\n\n{\"messages\": [{\"role\": \"system\", \"content\": \"...omitted for brevity...\"}, {\"role\": \"user\", \"content\": \"FUNCTION: {{{def get_average_of_second_ elements(numbers: List[List[int]]):}}}\\n CODE: \"}, {\"role\": \"assistant\", \"content\": \"def get_average_of_second_elements(numbers: List[List[int]]) -> float:\\n if not numbers: # Check if the input list is empty\\n return 0\\n \\n second_elements = [sublist[1] for sublist in numbers] # Extract second elements\\n return sum(second_elements) / len(second_elements) # Compute the average\", \"weight\": 0},\n\n{\"role\": \"user\", \"content\": \"Extract low level details to functions\"},\n\n{\"role\": \"assistant\", \"content\": \"import statistics\\n\\n\\ndef get_average_\n\n279",
      "content_length": 2121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "280\n\nFine-Tuning Models with OpenAI\n\nof_second_elements(numbers: List[List[int]]) -> float:\\n second_elements = get_second_elements(numbers)\\n return statistics.mean(second_elements)\\ n\\n\\ndef get_second_elements(numbers: List[List[int]]) -> List[int]:\\n return [sublist[1] for sublist in numbers]\", \"weight\": 1}]}\n\nAdditionally, we will include a similar contrastive example for get_smallest_squared, bringing our total to 15 training examples.\n\nWith the finalized JSONL dataset, we can now proceed with fine-tuning using OpenAI’s GUI.\n\nFine-tuning a model with the OpenAI GUI Fine-tuning an LLM with OpenAI can be done either programmatically via an API call or through OpenAI’s GUI. In this chapter, we will focus on using the GUI to initiate fine-tuning jobs.\n\nCosts of fine-tuning a model Fine-tuning an LLM comes with two types of costs:\n\n\n\nJob cost: As of April 2025, fine-tuning a model costs $3 per 1M tokens. For reference, the JSONL file provided earlier, which contains approximately 12K tokens (2K tokens in the file  6 iterations), would cost around $0.04 to fine-tune. There are no associated storage costs for the model or training files.\n\n×\n\n\n\nApplication: The pricing of using a fine-tuned model depends on the base model used. For instance, the cost for GPT-4o mini-based models is $0.30 per 1M input tokens and $1.20 per 1M output tokens, and much cheaper than the GPT-4o models, as illustrated in Figure 11.6:\n\nFigure 11.6: Fine-tuning costs for GPT-4o models",
      "content_length": 1481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "Chapter 11\n\nAs of April 2025, using a fine-tuned GPT-4o mini model costs exactly twice as much as using the model without fine-tuning. However, while the per-token cost is higher, a fine-tuned model can be more efficient, as its specialization allows for more concise inputs and outputs, potentially reducing overall costs.\n\nTo check the latest pricing for current and legacy models, visit OpenAI’s pricing page: https:// openai.com/api/pricing/.\n\nWith these costs in mind, we can now proceed with initiating the fine-tuning process.\n\nInitiating a fine-tuning job We can fine-tune a model using our JSONL file directly through OpenAI’s API GUI. While fine-tun- ing can also be done programmatically, we will focus on the GUI method since it is by far the most common way.\n\nTo initiate the job, go to OpenAI’s platform home page at https://platform.openai.com/, click on Dashboard in the upper-right menu, and then select Fine-tuning from the left-hand menu, as shown in Figure 11.7:\n\nFigure 11.7: Fine-tuning page in the OpenAI API platform\n\n281",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "282\n\nFine-Tuning Models with OpenAI\n\nTo start a new fine-tuning job, click the Create button in the upper-right corner of the screen. This will open the setup popup, as shown in Figure 11.8:\n\nFigure 11.8: Fine-tuning job popup\n\nWe used the following settings for fine-tuning:\n\nMethod: Supervised mode. Based on our experience, this is the most effective approach\n\nfor coding style specialization.\n\n\n\nBase Model: gpt-4o-mini 2024-07-18 (or the latest available mini model). This model provides sufficient results for our use case and is cost-effective.\n\n\n\nTraining data: The JSONL file introduced earlier in the chapter.\n\n\n\nSuffix (optional): This is appended to the model’s name for identification and does not need to be unique across fine-tuning jobs. We added the clean-code suffix for future reference.",
      "content_length": 806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "Chapter 11\n\nNow, we can start the fine-tuning job by clicking Create.\n\nWith 15 training examples, the process should take about five minutes to complete. Once finished, a success message will appear, as shown in Figure 11.9:\n\nFigure 11.9: Successful completion of fine-tuning job\n\nThe fine-tuning job processed 12,500 tokens, running six iterations (or epochs) of our file, which contains approximately 2,000 tokens. The fine-tuned model can be used in any OpenAI API call or accessed through the Playground, as we will explore later in this chapter.\n\nTo monitor the cost of the fine-tuning job, navigate to the OpenAI usage page at https://platform. openai.com/usage, scroll down to Spend categories, and review the fine-tuning expenses, as shown in Figure 11.10:\n\nFigure 11.10: Incurred fine-tuning costs\n\n283",
      "content_length": 811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "284\n\nFine-Tuning Models with OpenAI\n\nHere, we can see that fine-tuning the model with 15 training examples and 12K tokens resulted in a total cost of $0.04.\n\nUsing the fine-tuned model The fine-tuned model can be accessed either through the OpenAI Playground or programmati- cally. In Figure 11.9, OpenAI gave a formal name for our fine-tuned model under Output model, in the following format:\n\nft:{base model}:{account name}:{fine-tuning suffix}:{id}\n\nFor example, our model’s name is shown here, but your model will be named differently:\n\nft:gpt-4o-mini-2024-07-18:pazpaz-the-coder:clean-code:B5Ayo5q1\n\nTo use the fine-tuned model in code, replace the standard model reference:\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[{...]))\n\nThe updated call using the fine-tuned model will look as follows:\n\ncompletion: openai.ChatCompletion = (\n\nclient.chat.completions.create(\n\nmodel=\"ft:gpt-4o-mini-2024-07-18:pazpaz-the-coder:clean- code:B5Ayo5q1\",\n\nmessages=[{...] ))\n\nThe fine-tuned model is also available in the OpenAI Playground, as shown in Figure 11.11:\n\nFigure 11.11: Playground models, including our fine-tuned model",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "Chapter 11\n\nWithin the Playground interface, we can compare its results with the base model, which we will explore next.\n\nComparing our results Now that we have fine-tuned our custom model, we can evaluate how its outputs compare to those of the base model. To do this, navigate to the OpenAI Playground, https://platform. openai.com/playground/chat, select gpt-4o-mini-2024-07-18 as the base model, and click on Compare, as shown in Figure 11.12:\n\nFigure 11.12: Playground comparison selecting the base model to compare\n\nNext, add the fine-tuned model, as shown in Figure 11.13:\n\nFigure 11.13: Adding a model for comparison\n\nWe will use the same system message from earlier in the chapter that will be applied to both models:\n\nYou will be provided with a Python function signature enclosed with {{{ FUNCTION }}}. Your task is to implement it.\n\n285",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "286\n\nFine-Tuning Models with OpenAI\n\nNext, we will enter the same user message as earlier in the chapter:\n\nFUNCTION: {{{def get_quadratic_roots_only_if_real(a:int, b:int, c:int) -> Tuple[float, float] }}}\n\nCODE:\n\nOur comparison can be run by clicking Run, generating responses from both models. The base model’s output includes explanations and inline comments, while the fine-tuned model provides a more structured implementation, as shown in Figure 11.14:\n\nFigure 11.14: Comparing the base model (left) and the fine-tuned model (right)\n\nThe fine-tuned model returns a cleaner implementation that abstracts computations into sep- arate functions, keeping the main function concise and readable. For your reference, here is the full implementation proposed:\n\ndef get_quadratic_roots_only_if_real(a: int, b: int, c: int) -> Tuple[float, float]:\n\ndiscriminant = calculate_discriminant(a, b, c)\n\nif discriminant < 0:\n\nraise ValueError(\"Complex roots\")\n\nreturn calculate_quadratic_roots(a, b, discriminant)\n\ndef calculate_discriminant(a: int, b: int, c: int) -> float:\n\nreturn b**2 - 4*a*c\n\ndef calculate_quadratic_roots(a: int, b: int, discriminant: float) -> Tuple[float, float]:",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "Chapter 11\n\nroot1 = (-b + discriminant**0.5) / (2*a)\n\nroot2 = (-b - discriminant**0.5) / (2*a)\n\nreturn root1, root2\n\nThis output aligns with our fine-tuning objective of separating computations into helper functions while keeping the primary function, get_quadratic_roots_only_if_real, clear. Additionally, the base model generates 535 tokens, compared to 179 tokens from the fine-tuned model. Despite the higher per-token cost of the fine-tuned model, it is ultimately more cost-efficient.\n\nTo conclude, we successfully fine-tuned the model to specialize in returning clean, code-only outputs. This targeted specialization was achieved with 15 JSONL examples, using a combination of positive and contrastive learning to guide the model’s behavior.\n\nFine-tuning enables us to adapt a pre-trained LLM for specific tasks, making it especially valu- able for automated and large-scale coding projects. In later chapters, we will explore ways to further improve fine-tuning, such as refining the training data to separate validation logic from core functionality.\n\nFine-tuning in action Currently, we use just two fine-tuned models for coding tasks. The first is for code implementation as we constructed in this chapter, providing an alternative perspective on new implementations. The other is for docstring maintenance, ensuring that a function’s code remains consistent with its docstring. For more narrowly scoped tasks, few-shot learning is usually sufficient and prefer- able since it requires less effort. In general, fine-tuning is only considered when simpler prompt engineering techniques are ineffective or when the project operates at a repository-wide scale, making the investment worthwhile.\n\nWe encourage you to experiment frequently with prompt engineering and fine-tuning. In the next section of the book, we will introduce additional software development life cycle (SDLC) use cases that you can start applying to your code base right away.\n\nSummary In Part 2 of the book, we explored LLMs in greater depth. We explained how they work, what they excel at, and how to leverage prompt engineering techniques to achieve more effective results. We also covered strategies for evaluating their outputs to ensure reliability.\n\n287",
      "content_length": 2239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "288\n\nFine-Tuning Models with OpenAI\n\nThis chapter took the concept of few-shot learning a step further by demonstrating how to fine- tune an LLM to specialize on a given task. Through positive and contrastive training examples, we guided the model to generate function implementations based solely on their signatures, returning clean code without inline comments. This approach can be applied more broadly to tasks such as generating unit test suites, maintaining docstring quality, or refactoring for loops across an entire repository.\n\nWith this deeper understanding of LLMs, prompt engineering, and output evaluation, we now have the essential tools to become supercharged coders. We can determine the best tool for a given task, whether ChatGPT, OpenAI API, or Copilot, and craft prompts that maximize effectiveness using best practices and advanced techniques. Moreover, we now have the skills to assess the quality of the model’s output with confidence.\n\nIn the third part of the book, we will apply this knowledge to advanced SDLC tasks. We will explore how GenAI can assist in areas such as documentation, testing, scaling applications for runtime and memory efficiency, logging, monitoring, and error handling. Some tasks will rely on the five S’s framework for coding-related tasks, while others will require more advanced prompt engineering strategies or fine-tuning to achieve optimal results.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: When is fine-tuning a good approach?\n\nAnswer: Fine-tuning is beneficial for large-scale projects where a repetitive task needs to be performed consistently, such as generating function implementations from signatures while following specific coding standards. Instead of relying on few-shot learning or complex prompt engineering techniques within each request, fine-tuning allows the specialization process to be separate from the prompt, making it easier to maintain and, in some cases, more cost-effective.\n\nQuestion 2: What are the costs involved in fine-tuning?\n\nAnswer: Fine-tuning involves two types of costs:\n\n\n\nTraining cost: A one-time cost incurred when running the fine-tuning job.\n\nUsage cost: Fine-tuned models typically have higher per-token costs for input and output compared to their base models. For example, with GPT-4o mini, using fine-tuned models costs twice as much as those from the base model.",
      "content_length": 2457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "Chapter 11\n\nHowever, the true cost of fine-tuning lies in the time and effort required to carefully construct training examples that ensure the model aligns with the desired specialization.\n\nQuestion 3: Why does fine-tuning require more training samples than few-shot learning?\n\nAnswer: The specialization process in fine-tuning is fundamentally different from few-shot learn- ing. In few-shot learning, examples are simply provided within the prompt, guiding the model without altering its internal parameters.\n\nFine-tuning, on the other hand, adjusts the model’s weights to align with the desired behavior. Given that the base model was pre-trained on vast amounts of data, requiring tens or even hun- dreds of fine-tuning examples is relatively modest in comparison to the scale of its original training.\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. OpenAI. Retrieved from https://arxiv.\n\norg/abs/2005.14165v4\n\nDeepchecks on contrastive learning: https://www.deepchecks.com/glossary/\n\ncontrastive-learning/\n\nOpenAI API fine-tuning: https://www.datacamp.com/tutorial/fine-tuning-large-\n\nlanguage-models\n\nOpenAI API request parameters: https://platform.openai.com/docs/api-reference/\n\nresponses/create\n\nData camp fine-tuning tutorial for LLMs: https://www.datacamp.com/tutorial/fine-\n\ntuning-large-language-models\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.\n\n289",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "Part 3\n\nFrom Code to Production with GenAI\n\nIn Part 3 of this book, we branch from simple coding to the full software development life cycle (SDLC). We first focus on writing docstrings, followed by testing, since these are almost universally required for production-ready code. The next advanced topic is scaling applications for systems in terms of runtime and memory. The part will also cover logging, monitoring, and error handling.\n\nThe techniques for completing these SDLC steps build on both the five S’s framework and ad- vanced prompt engineering techniques discussed in Part 2.\n\nThe final chapter examines the current trends, including vibe coding, and suggests what might be next for the field of GenAI.\n\nThis part contains the following chapters:\n\nChapter 12, Documenting Code with GenAI\n\nChapter 13, Writing and Maintaining Unit Tests\n\nChapter 14, GenAI for Runtime and Memory Management\n\nChapter 15, Going Live with GenAI: Logging, Monitoring, and Errors\n\nChapter 16, Architecture, Design, and the Future",
      "content_length": 1018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "12\n\nDocumenting Code with GenAI\n\nIn Part 3 of the book, we will introduce advanced practices from the software development life- cycle (SDLC) when working with ChatGPT, OpenAI API, and GitHub Copilot. Using the five S’s framework introduced in Chapter 4, and prompt engineering techniques introduced in Chapter 8 and Chapter 9, we will see how to productionize our Python applications.\n\nIn this chapter, we will introduce docstrings for Python objects, which already has a built-in slash command (i.e., /doc) with GitHub Copilot. We can use this slash command with the vanilla format or extend it with specific docstring requirements. Docstrings are already a feature of most IDEs as a template structure where arguments will be formatted with a fixed style, but the standard template does not include any descriptive information about the function or its parameters beyond their name. GitHub Copilot will usually produce a full docstring for high-quality code.\n\nThe topics covered in this chapter include the following:\n\n\n\nIntroducing software documentation\n\nUsing GenAI to write docstrings\n\n\n\nFinding outdated docstrings with GitHub Copilot, ChatGPT, and OpenAI API\n\n\n\nPractical insights",
      "content_length": 1189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "294\n\nDocumenting Code with GenAI\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or PyCharm\n\nOpenAI account with access to OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/\n\nSupercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nIntroducing software documentation Software engineers not only write a lot of code, but they also need to read and understand code written by others. The code could be written the same day by other developers or be part of a legacy system maintained over multiple decades. While clean, well-written code is a critical aspect, code documentation is also a fundamental way of communicating with other developers.\n\nSoftware documentation comes in multiple forms that serve different needs. Technical documen- tation includes the comments in source code, API descriptions, database schemas, and software architecture diagrams. User documentation is a guide on how an end user can use software. The software engineering process also adds specific documentation on requirements, specifications, testing plans, CI/CD pipelines, sprint planning, and much more. This chapter focuses on the documentation of the source code.\n\nA general guideline is that the documentation should focus on the “why” rather than the “what.” That is, the code is readable and one can figure out what it does, but the reason for its implemen- tation is what should be documented. The rationale is that well-written code is self-explanatory about what it does, but the reason for the code (the why) can be better understood through documentation. The code documentation can therefore explain the code, improve its readability, and aid later software maintenance.",
      "content_length": 2142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "Chapter 12\n\nIn Python, the convention for documentation within the code is the docstring. Docstrings typically occur at the beginning of Python modules, classes, and methods. The purpose is to explain their function and the reason for their inclusion. Python’s docstrings are enclosed in triple quotes (single or double quotes) even if they are just one line. However, for the most common docstrings used to document methods, they should include arguments, return types, and any exceptions raised.\n\nDocstrings serve a few different purposes. First, they provide documentation for a module, class, method, or function for developers. They can be compiled into formal documentation resources. In addition, these forms of documentation are accessible using Python’s help() function. Using docstrings makes the code easier to maintain, which reduces the lifetime cost of code.\n\nThe second purpose of docstrings is based on Miller’s Law, also known as the Magical Number Seven, Plus or Minus Two. Based on George Miller’s 1956 paper (https://psychclassics.yorku. ca/Miller/), it states that humans can retain 5 to 9 pieces of information in their working memory. A software developer trying to understand even a medium-length Python file must understand the interaction of several methods. The docstring and naming conventions facilitate a quick understanding of the abstracted methods.\n\nAlthough docstrings may appear to be standard, there are actually four common formats. The PEP 257 convention is the official Python standard, consisting of a single line with no string formatting that emphasizes readability. The Google style is among the most common and still emphasizes readability. It uses multi-line formatting for the arguments passed in, method returns, and exceptions raised. The NumPy/SciPy style is widely used, but primarily for scientific software. It has similar content to the Google style but uses different formatting. The reStructuredText (reST) or Sphinx style includes more detailed information than the others. This style is used by the Sphinx documentation generator, which produces HTML, PDF, and LaTeX documentation manuals directly from the source code with full hyperlinking and other advanced features. This chapter will focus on both single-line PEP 257 and Google styles due to their widespread use, but other styles are directly supported by requesting specific docstring styles. We have included a few sources in the Further reading section that describe comment styles for Python in more detail.\n\n295",
      "content_length": 2530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "296\n\nDocumenting Code with GenAI\n\nWorking with the /explain GitHub Copilot command As a working example in this chapter, we use Flask routines that support the distance calculation found in Chapter 10. The base code is shown in the GitHub repository and copied for reference here:\n\nfrom flask import Flask, request, jsonify\n\nimport numpy as np\n\napp = Flask(__name__)\n\n@app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\ndata = request.get_json()\n\ndist_type = data.get(\"distance\")\n\na = np.asarray(data.get(\"df1\"))\n\nb = np.asarray(data.get(\"df2\"))\n\nif a.shape != b.shape:\n\nreturn jsonify({\"error\": \"Matrices must have the same shape\"})\n\nif dist_type == \"L1\":\n\ndist = np.sum(np.abs(a - b))\n\nreturn jsonify({\"distance\": dist})\n\nelif dist_type == \"L2\":\n\ndist = np.sqrt(np.sum((a - b) ** 2))\n\nreturn jsonify({\"distance\": dist})\n\nelse:\n\nreturn jsonify({\"error\": \"Invalid distance type\"})\n\nIn VS Code, by simply highlighting the method, right-clicking and selecting Copilot, and selecting Generate Docs as shown in Figure 12.1, a docstring can be automatically written. Note that we have ignored Generate Docstring shown under Copilot. That command is VS Code’s template approach for docstrings, which creates placeholders for the docstring and parameters but does not use any AI to write the comment or description.",
      "content_length": 1327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "Chapter 12\n\nFigure 12.1: By highlighting the method and right-clicking, Copilot generates a docstring\n\nThe generated docstring is shown in Figure 12.2. Trying this several times, we obtained several variations on the docstring, some of which had exceedingly long lines or different formats, and some included different information. Regardless, the docstring format has a simple one-line description, but the remainder has a random format that does not follow any of the style guides. Specifically, the “expects…” section of the docstring is unclear.\n\n297",
      "content_length": 554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "298\n\nDocumenting Code with GenAI\n\nFigure 12.2: GitHub Copilot produces a vague docstring for a poorly constructed function\n\nAnother common case is for GitHub Copilot to produce multiple lines of text as the docstring, excluding the parameters, returns, and so on. Figure 12.2 fails in this respect, with two lines of explanation followed by the expects (This endpoint expects...) and returns sections. While both multi-line and one-line docstrings exist, the preference is for one line rather than an elab- orate description, particularly for low-level methods. If such a method requires multiple lines, it is often considered good practice to refactor the method into multiple simpler methods that each perform a single task.\n\nCopilot’s docstring creation is usually quite good, but can occasionally be incorrect. As in the previous example, you will encounter cases where it does not perform well. Some of these cases are due to the complexity of the underlying code. Rather than fixing the docstring, a preferred solution would be to refactor the code into a simpler form and recreate docstrings. In many cases, the /explain function is quite informative. In Figure 12.3, we apply /explain to the same code by using the GitHub Copilot Editor Inline Chat from within the same function:",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "Chapter 12\n\nFigure 12.3: Using /explain for calculate_distance describes a complex method\n\nA similar functionality occurs within the GitHub Copilot Chat with the /explain calculate_ distance() command, which often returns explanations that are more verbose. Although both provide a clear explanation, their content is technically complex, which is why we recommend refactoring the code as shown:\n\nimport numpy as np\n\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\na, b, dist_type = parse_request_parameters(request)\n\ndist_func = {\"L1\": get_manhattan_dist, \"L2\": get_euclidean_dist}. get(dist_type)\n\ndist = dist_func(a, b)\n\nreturn jsonify({\"distance\": dist})\n\n299",
      "content_length": 752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "300\n\nDocumenting Code with GenAI\n\ndef parse_request_parameters(request):\n\ndata = request.get_json()\n\na = np.array(data.get(\"df1\",))\n\nb = np.array(data.get(\"df2\",))\n\ndist_type = data.get(\"distance\")\n\nreturn a, b, dist_type\n\ndef get_manhattan_dist(a: np.ndarray, b: np.ndarray) -> float:\n\nreturn np.sum(np.abs(a - b))\n\ndef get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\nreturn np.sum((a - b) ** 2)\n\nThis section has introduced one way of instructing GenAI to write a docstring. The next section provides other approaches.\n\nUsing GenAI to write docstrings This section describes different techniques for creating docstrings for a single method and for a full Python file. Docstrings improve code readability and are often required by organizations as part of their continuous integration/continuous development (CI/CD) pipelines. Using GenAI to create docstrings can increase the speed of the software development process.\n\nDocstring for a single method The simplest approach to generating a docstring in VS Code is simply typing three double quotes “““ or the equivalent in single quotes on a blank line following the method signature, as shown in Figure 12.4, with results shown in the code block that follows, which can be accepted by pressing tab:\n\nFigure 12.4: Initiating the request for a docstring with three single or double quotes\n\ndef get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n'''Calculates the squared Euclidean distance between two arrays.'''\n\nreturn np.sum((a - b) ** 2)",
      "content_length": 1511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Chapter 12\n\nAs we can see, it describes the method but is the simplest form of the PEP 257 style with a single line string and no parameters or return values.\n\nIf we want to have a different docstring style, we can request it using the Copilot Editor Inline Chat (/doc Google style) or any other specified style. Figure 12.5 shows the results for the Google style.\n\nFigure 12.5: Docstring in Google style using the inline /doc Google style\n\nThis approach with the online Editor Inline Chat will not create docstrings for all the methods within a file. Other techniques for working at a file level are discussed next.\n\nSimilarly, with the GitHub Copilot chat, we can highlight a single method, such as get_euclidean_ dist, and request a Google-style docstring using the /doc Google style command. Shown on the left of Figure 12.6, the original (above) and new (below) docstrings are presented with the options to keep or undo the changes.\n\nFigure 12.6: Using GitHub Copilot chat on a single method to change to Google style\n\n301",
      "content_length": 1027,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "302\n\nDocumenting Code with GenAI\n\nDocstrings for an entire file GitHub Copilot and ChatGPT can be used to produce docstrings for all methods within a single file at once using the chat mode, which can be quite useful. This may not work with the inline chat, depending on the version you are using of VS Code or PyCharm, so the code may be produced in the chat window and will have to be copied and pasted back into the code base. We find this approach useful at the end of the day, when you are trying to submit code before a deadline. In practice, though, we recommend writing docstrings with the code since the GenAI should be verified, which is best accomplished when writing the code. A later section, Finding outdated doc- strings with GitHub Copilot, ChatGPT, and OpenAI API, will describe strategies for stale comments.\n\nThe first step is asking GitHub Copilot to write the docstrings for your current Python file, as shown in Figure 12.7. We will use the same code as we did earlier, using the Flask-based distance calculation.\n\nFigure 12.7: GitHub Copilot chat request to produce docstrings for the entire file\n\nAfter uploading or pasting the file, the results are shown in the following code block. The results appear in the chat and must be copied from it to the code development window. We have not specified the type of docstring, and it may default to the Google style or the equivalent of /doc Google style. By modifying the prompt to /doc single line style, the code will produce the corresponding single-line docstrings. Both versions are included in the GitHub repository for this book.\n\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():",
      "content_length": 1741,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "Chapter 12\n\n\"\"\"Calculate distance between two vectors.\"\"\"\n\na, b, dist_type = parse_request_parameters(request)\n\ndist_func = {\"L1\": get_manhattan_dist, \"L2\": get_euclidean_dist}. get(dist_type)\n\ndist = dist_func(a, b)\n\nreturn jsonify({\"distance\": dist})\n\ndef parse_request_parameters(request):\n\n\"\"\"Parse and validate request parameters.\"\"\"\n\ndata = request.get_json()\n\na = np.array(data.get(\"df1\",))\n\nb = np.array(data.get(\"df2\",))\n\ndist_type = data.get(\"distance\")\n\nreturn a, b, dist_type\n\ndef get_manhattan_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n\"\"\"Calculate Manhattan distance.\"\"\"\n\nreturn np.sum(np.abs(a - b))\n\ndef get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n\"\"\"Calculate Euclidean distance.\"\"\"\n\nreturn np.sum((a - b) ** 2)\n\n303",
      "content_length": 753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "304\n\nDocumenting Code with GenAI\n\nQuick tip: Enhance your coding experience with the AI Code Explainer and Quick Copy features. Open this book in the next-gen Packt Reader. Click the Copy button\n\n(1) to quickly copy code into your coding environment, or click the Explain button\n\n(2) to get the AI assistant to explain a block of code to you.\n\nThe next-gen Packt Reader is included for free with the purchase of this book. Scan the QR code OR visit packtpub.com/unlock, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.\n\nWith these methods, we can create documentation for a single method or an entire file. Refac- toring code often leads to a problem where the code and the docstrings are out of sync. The next section discusses strategies to overcome this problem.\n\nFinding outdated docstrings with GitHub Copilot, ChatGPT, and OpenAI API A frequent debate among software engineers is whether documentation should be written when writing the code or later, when the code has stabilized. One line of thinking is that the docstring (and test cases) should be written at the same time. The primary argument is that the intention of the code is clearest when the developer is working through its logic and understands its pur- pose. The problem is that, as code is routinely refactored, the docstring comments and test cases must be adapted, making the original versions obsolete. The other line of thinking is that the documentation can and should be written later, once the code settles, to minimize the repeated rewriting of the comments.",
      "content_length": 1607,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "Chapter 12\n\nNew GenAI technology makes the debate irrelevant since it offers two solutions. The first solution is simply to regenerate and update all the comments at the method or file level. The previous section described this strategy. We can use GitHub Copilot or ChatGPT to write the docstrings for the entire file, which will update or redo all the docstrings.\n\nThe second strategy is more specific in that it can detect outdated or stale comments so that these can be manually or automatically adjusted. The advantage of this approach is that individual methods can be updated, leaving previously generated or verified comments unchanged. If we have individually written, verified, or otherwise invested effort in the docstrings, we do not want them to be overwritten.\n\nFor the comments, we have taken the previous example and created two versions of the Euclidean distance by copying one and modifying the code. The new code is shown here. As you can see, the first method returns the Euclidean distance and the second method returns the Euclidean distance squared, yet the docstrings have not been updated and are, in fact, identical due to our copy and paste approach.\n\ndef get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n\"\"\"Return the squared Euclidean (L2) distance between two arrays.\"\"\"\n\nreturn np.sqrt(np.sum((a - b) ** 2))\n\ndef get_euclidean_squared_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n\"\"\"Return the squared Euclidean (L2) distance between two arrays.\"\"\"\n\nreturn np.sum((a - b) ** 2)\n\nUsing GitHub Copilot or ChatGPT, we can simply ask whether any of the docstrings are out of sync, as shown in Figure 12.8. The approach cleverly recognizes the squared root function call in get_euclidean_dist and suggests an update to the docstring:\n\n305",
      "content_length": 1776,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "306\n\nDocumenting Code with GenAI\n\nFigure 12.8: Using GitHub Copilot to identify stale docstrings\n\nA better approach is to use OpenAI API to compare the code and docstrings individually across a file. The sample code can be found in the repository for this chapter called openai_docstring_ review.py, which runs on the distance_versions.py file. The resulting output is shown next for each of the methods.\n\nThe output explains the logic and provides an answer as to whether the docstring correctly corre- sponds to the code implementation. It correctly identifies that the get_euclidean_dist method does not match.\n\nFunction: calculate_distance\n\nDocstring: Calculate distance between two arrays from a POST request.\n\nSource: @app.route(\"/distances\", methods=[\"POST\"])\n\ndef calculate_distance():\n\n<…code deleted for space…>\n\nMatches: The docstring matches the implementation. Both the docstring and the function's purpose convey that the function is designed to calculate the distance between two arrays based on parameters received from a POST request. Therefore, the conclusion is:\n\nMATCHES: Yes\n\nFunction: get_euclidean_dist\n\nDocstring: Return the squared Euclidean (L2) distance between two arrays.",
      "content_length": 1200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "Chapter 12\n\nSource: def get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n<…code deleted for space…>\n\nMatches: The provided docstring states that the function returns the \"squared Euclidean (L2) distance between two arrays,\" but the implementation actually returns the Euclidean distance (not squared) by taking the square root of the sum of squared differences. Therefore, the docstring does not accurately describe the implementation.\n\nThus, the conclusion is that the docstring does NOT match the implementation.\n\nFunction: get_euclidean_squared_dist\n\nDocstring: Return the squared Euclidean (L2) distance between two arrays.\n\nSource: def get_euclidean_squared_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n<…code deleted for space…>\n\nMatches: The docstring and the function implementation match. The function `get_euclidean_squared_dist` computes the squared Euclidean distance (also known as the L2 distance) between two numpy arrays `a` and `b`, and the docstring accurately describes this functionality.\n\nTherefore, the answer is:\n\nMATCHES\n\nFunction: get_manhattan_dist\n\nDocstring: Return the Manhattan (L1) distance between two arrays.\n\nSource: def get_manhattan_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n<…code deleted for space…>\n\nMatches: MATCHES: Yes\n\nThe chapter has covered several techniques for creating docstrings. It has also introduced different strategies for finding problems with stale comments. In the next section, we provide recommen- dations based on how we approach writing docstrings with GenAI.\n\n307",
      "content_length": 1542,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "308\n\nDocumenting Code with GenAI\n\nPractical insights Given the choices of GitHub Copilot, ChatGPT, and OpenAI API, we find that Copilot is the most convenient for writing docstrings. The convenience of pair programming in the integrated envi- ronment simply makes it easier to request documentation, rather than copying and pasting code to other places. Copilot chat can usually accurately locate stale comments by itself. One issue we have encountered is that the context of sending code to Copilot is not visible. If you copy code and ask if the comments are current, it may miss some methods that were not included in the context. This can be solved by copying and pasting the code in place to renew the context.\n\nWe recommend that the docstrings be created after writing the code for each method, or at least after the code for a method has stabilized, if not after each refactoring. We further suggest check- ing that the docstring accurately reflects the implementation. At the beginning of the chapter, we discussed that documentation should reflect why a particular method was written rather than describing what the code does. The current technology is quite reasonable for the what but ignores the why, which is an opportunity for software engineers to elaborate.\n\nSummary Docstrings are a standard practice for maintaining high-quality code. They enable future readers of the software to understand the code, which is important for debugging, feature enhancements, and more. We recommend docstrings be included as soon as the code has been written. GitHub Copilot offers multiple ways of writing docstrings for a single method, including three single or double quotes, using the Copilot menus on a highlighted method. These approaches will produce single-line docstrings, the Google docstring style, or others when specified as a command.\n\nA challenge in writing software is keeping docstrings up to date with the code. All methods can identify discrepancies between the code and docstrings, which is the preferred method for iden- tifying issues that can be resolved. Alternatively, docstrings can be updated at a method or file level, replacing any existing docstrings.",
      "content_length": 2182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "Chapter 12\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: What are three ways of generating docstrings for code?\n\nAnswer: The three ways discussed in this chapter are as follows:\n\n1. GitHub Copilot can recommend code by opening \"\"\" or ''' for a method on the line after\n\nthe method signature\n\n2. GitHub Copilot can be invoked by highlighting a method and using its menu to generate\n\na docstring\n\n3. ChatGPT or Copilot can be used to add docstrings to all methods\n\nQuestion 2: What are two approaches for keeping docstrings consistent with the underlying code?\n\nAnswer: The first approach is simply overwriting any docstrings. This ensures that the GenAI pro- duces a docstring for all methods based on the underlying code. If a docstring were inconsistent, then it would be overwritten. Unfortunately, this overwrites all previously created docstrings, which may be undesirable if you have invested time and effort into writing or checking any of them.\n\nThe second approach uses GitHub Copilot or ChatGPT to ask if there are discrepancies between the code and the docstrings. One can also use OpenAI API to match each code implementation against the docstring and return whether they match on a case-by-case basis as was shown with the openai_docstring_review.py, sample code. Any of the options in the second approach are preferred if docstrings are current and verified.\n\nQuestion 3: Given that GenAI can produce quality docstrings, is it necessary to check the doc- strings, or can they simply be regenerated?\n\nAnswer: While the quality is usually quite high, inaccurate docstrings can confound future de- velopers. It is strongly recommended to use GenAI to save time, but to use human intelligence to verify that they are correct. At the present time, trust but verify is the best approach.\n\n309",
      "content_length": 1884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "310\n\nDocumenting Code with GenAI\n\nFurther reading\n\nMiller’s Law, also known as Magic Number 7 plus or minus 2: https://en.wikipedia.\n\norg/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two\n\n\n\nTypes of docstring formats: https://www.geeksforgeeks.org/python-docstrings/\n\n\n\nFurther formatting of docstrings: https://www.linkedin.com/pulse/python- docstrings-formats-samuel-thomas/\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.",
      "content_length": 786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "13\n\nWriting and Maintaining Unit Tests\n\nThe task of software engineering can be interpreted as the ability to deliver quality products that contribute to the business value of customers. In this age of technology, customers have high standards and demand excellence.\n\nThe notion of quality has been interpreted in different ways over the years. However, here are two key aspects:\n\n\n\nThat it must meet the user requirements\n\n\n\nIt must be free of defects\n\nThese two requirements are captured by verification and validation processes, which assert that the software achieves the intended goal without any defects. There are many software engineer- ing techniques that aim to achieve these goals, including code reviews, pair programming, and so on, but the most common is testing. Although there are many books that focus on the art and practice of software testing, this chapter focuses on unit testing, which is the most widely used approach. Test-driven development (TDD) is a popular agile approach for software development. Rather than writing the code first and ensuring it functions correctly with unit tests, TDD involves writing the unit test cases first and subsequently writing the minimum code that passes the tests. Proponents of the practice claim superior code quality and an inherently testable design. In this chapter, we will refer to the primary code under development as the implementation code, and the unit tests as the test code.",
      "content_length": 1449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "312\n\nWriting and Maintaining Unit Tests\n\nWith GenAI coding, one can argue that testing is even more critical than conventional program- ming. The generative AI process involves crafting code based on comments and/or method signa- tures from other code, which lacks the precision that a developer would normally apply, given the trade-off for supercharged, faster coding. Unit testing serves as a check that the code functions correctly, as specified by tests. Combined with code inspection while creating the code, it offers a check of correctness.\n\nThis chapter is dedicated to working with both GitHub Copilot and ChatGPT to supercharge our creation of unit tests. In this chapter, we will cover the following topics:\n\nUnit tests with GenAI\n\nData-driven tests\n\n\n\nTest-driven development\n\n\n\nSample data creation with GenAI\n\n\n\nPractical insights\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\n\n\nA GitHub account\n\n\n\nAccess to your preferred IDE – either VS Code or PyCharm\n\n\n\nAccess to the book’s repository available at https://github.com/PacktPublishing/ Supercharged-Coding-with-Gen-AI\n\n\n\nA virtual environment set up in your preferred IDE, VS Code, or PyCharm\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nUnit tests with GenAI GenAI can save considerable time in writing unit tests. As the name suggests, each unit test is designed to test a single unit or aspect of the code. In fact, they are focused on the smallest frag- ments of code that can be separated and tested in isolation. Together, they provide confidence that the code functions correctly at the component level and provide an early signal of issues when the code is refactored. The same developer typically writes the code implementation and unit tests. The unit tests prove that the code functions correctly, as understood, and may also help define the requirements.",
      "content_length": 2135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "Chapter 13\n\nBy their nature, unit tests require a deep understanding of the business and software requirements and how each method fulfills those requirements. One must consider the intended functionality as well as the variety of edge cases for each method separately and incorporate those into the test framework. This includes decisions on the test framework, method call, test nomenclature, asser- tion form, and the actual test. IDEs can certainly help with the test framework, but the actual test requires developer time, thought, and perhaps a strategy. These often become hurdles for junior or less motivated developers who perhaps see them as superfluous to the core software coding task. The current GenAI tools provide two-way functionality in writing unit tests for existing code as a common approach or writing code from existing unit tests, which is TDD.\n\nThe GenAI capability is not a substitute for testers or testing, but makes skilled testers much more efficient. The developer can write more tests in less time and focus on the more complex cases. The generic and simpler unit tests can be written by GenAI, usually with high accuracy. However, if the resulting tests are incomplete, one can request or prompt the GenAI to create additional tests to provide better coverage and confidence in the code.\n\nAs example code, we use the concept of creating n-grams for a given text string, which we intro- duced in Chapter 6. N-grams are simply adjacent groups of N letters starting at any and every point within a string without exceeding the left or right side of the string. For example, the 3-grams from the sequence “abcde” are [“abc”, “bcd”, “cde”]. Since we wanted to limit the range of possible characters, we only considered lowercase letters and spaces, but left tabs and line feeds. Uppercase letters were converted to lowercase, and both numbers and symbols were removed. Additionally, since English type uses single spaces, we replaced all adjacent spaces with a single space. This portion of the code is as follows:\n\nimport re\n\ndef lowercase_remove_punct_numbers(text, supercharte=True):\n\nreturn re.sub(r'[^a-z\\s]', '', text.lower())\n\ndef multiple_to_single_spaces(text):\n\nletters_single_spaces = re.sub(r'\\s+', ' ', text)\n\nreturn letters_single_spaces\n\ndef create_ngrams(text, n) -> list:\n\n'''create a list of n-gram tuples from the input text.'''\n\nprocessed_text = lowercase_remove_punct_numbers(text)\n\nsingle_space_processed = multiple_to_single_spaces(processed_text)\n\n313",
      "content_length": 2503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "314\n\nWriting and Maintaining Unit Tests\n\nu = [single_space_processed[i:i+n] for i in range(len(single_space_ processed)-n+1)]\n\nreturn u\n\nif __name__ == \"__main__\":\n\ntext = \"This is a sample text $ABC% for creating n-grams.\"\n\nn = 3\n\nprint(create_ngrams(text, n))\n\nThe output of this function is a list of 3-grams that span the text input string:\n\n['thi', 'his', 'is ', 's i', ' is', 'is ', 's a', ' a ', 'a s', ' sa', 'sam', 'amp', 'mpl', 'ple', 'le ', 'e t', ' te', 'tex', 'ext', 'xt ', 't a', ' ab', 'abc', 'bc ', 'c f', ' fo', 'for', 'or ', 'r c', ' cr', 'cre', 'rea', 'eat', 'ati', 'tin', 'ing', 'ng ', 'g n', ' ng', 'ngr', 'gra', 'ram', 'ams']\n\nIf we start unit testing and have not set up the testing framework, we can use GitHub Copilot Chat with the prompt @workspace /setupTests, which will present an option to use either of the two main frameworks: pytest or unittest, as shown in Figure 13.1. The instructions are useful if you are installing unittest for the first time.\n\nFigure 13.1: @workspace /setupTests provides a choice of the two main test frameworks\n\nTo generate unit tests, first select a single method from the implementation code, such as the lowercase_remove_punct_numbers code. Then, type the @workspace /tests generate unit tests command. Alternatively, explicitly type the name of the target method, such as @workspace",
      "content_length": 1345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "Chapter 13\n\n/tests generate unit tests for lowercase_remove_punct_numbers, into the GitHub Copilot Chat. It will first explain the tests that it will generate and then write the corresponding unit test file. The test explanation is shown in Figure 13.2:\n\nFigure 13.2: The test plan from GitHub Copilot is provided along with the code for the method\n\nThe user will be prompted to save the resulting test file. The following code shows an example of what the output unit tests may look like, although it will vary in order and even the number of tests:\n\nimport unittest\n\nfrom src.ngrams import lowercase_remove_punct_numbers\n\n# File: test_ngrams.py\n\nclass TestLowercaseRemovePunctNumbers(unittest.TestCase):\n\ndef test_mixed_input(self):\n\ntext = \"Hello, World! 123\"\n\nexpected = \"hello world\"\n\nself.assertEqual(lowercase_remove_punct_numbers(text), expected)\n\ndef test_lowercase_only(self):\n\ntext = \"this is a test\"\n\nexpected = \"this is a test\"\n\nself.assertEqual(lowercase_remove_punct_numbers(text), expected)\n\n315",
      "content_length": 1011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "316\n\nWriting and Maintaining Unit Tests\n\ndef test_punctuation_and_numbers(self):\n\ntext = \"!@#$%^&*()1234567890\"\n\nexpected = \"\"\n\nself.assertEqual(lowercase_remove_punct_numbers(text), expected)\n\ndef test_empty_string(self):\n\ntext = \"\"\n\nexpected = \"\"\n\nself.assertEqual(lowercase_remove_punct_numbers(text), expected)\n\ndef test_spaces_and_special_characters(self):\n\ntext = \" Hello!! World?? \"\n\nexpected = \" hello world \"\n\nself.assertEqual(lowercase_remove_punct_numbers(text), expected)\n\nif __name__ == \"__main__\":\n\nunittest.main()\n\nThe test code is quite reasonable in the sense that it mostly proves that the regular expressions and lowercase are correct. The tests can be run from the parent level using the python -m unittest test_lowercase_remove_punct_numbers.py command.\n\nCreating tests is an iterative process, and some of the test functions created by GenAI may be un- necessary or incorrect. In other cases, additional tests may be warranted, and the GitHub Copilot Editor Inline Chat is useful for creating those tests from within the test file. As an example, we used /tests with a request to include 10-digit phone numbers to prove they would be correctly handled, which produced inline code in the test file. This process is shown in Figure 13.3:\n\nFigure 13.3: Using Editor Inline Chat within the test code to add a new test",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "Chapter 13\n\nIn some cases, the Editor Inline Chat is more convenient since it will add directly to the code, whereas GitHub Copilot Chat may create a new test file. In either case, a request for a single test, such as a phone number or punctuation, may result in multiple additional tests. The developer can accept or reject these tests individually with the click of a button. Reviewing tests is important since the accuracy of the tests relates to the correctness of the code.\n\nRather than write tests for each method separately, the entire file can be converted into unit tests. Using GitHub Copilot chat and the @workspace /tests create unit tests for ngrams. py command, Copilot will often, but not always, produce a test plan for each of the methods in ngrams.py. Then, it will create an output test file that you must name to save. Using the unittest conventions, each method will have its own class of one or more unit tests within that file. The full set of tests is not shown here but can be found in the test_ngrams.py file in the book’s GitHub repository.\n\nThere are guidelines for adding additional tests to the test file using a comment format. By typing a comment (starting with # in Python), GitHub Copilot reads the comment and suggests code. Although it should be equivalent, we have found this approach to sometimes be less accurate than using the Editor Inline Chat or GitHub Copilot Chat. As shown in Figure 13.4, the expected answer incorrectly includes numbers that should have been removed:\n\nFigure 13.4: This test, generated from the #comment, incorrectly includes numbers\n\nRevising the test with the #test create_ngrams with a 10-digit phone number and make sure the numbers are removed request resulted in fewer numbers included in the test. However, the variability of the unit tests with correct and incorrect responses means that checking them must be part of the verification process.\n\nIf we repeat a similar process of adding a zip code of the form 12345-6789 using the Editor Inline Chat feature, the results are more promising, as shown in Figure 13.5. Not only did it correctly produce the test, but it also added several additional tests. However, as a note of caution, the Inline Chat did not always produce accurate tests and sometimes incorrectly included numbers.\n\n317",
      "content_length": 2308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "318\n\nWriting and Maintaining Unit Tests\n\nFigure 13.5: The tests from Editor Inline Chat produced a correct test and others as well\n\nThis section has described how GitHub Copilot can create unit tests from the implementation code, either for a single method or an entire file. The next section describes how ChatGPT can achieve similar results.\n\nChatGPT for testing ChatGPT can also be used to write unit tests. An example prompt to write Python unit tests is as follows:\n\nYou are a Python testing assistant.\n\nGiven Python code enclosed within {{{ }}}, generate unit tests using the unittest framework. For each function or method in the code:\n\n1.Create a corresponding test method within a unittest.TestCase subclass.\n\n2.Use meaningful test method names that reflect the function being tested.\n\n3.Include appropriate assertions based on the function's logic and expected behavior.\n\n4.Use mock objects or patching where necessary (e.g., for I/O, APIs, or external dependencies).\n\n5.If a function has multiple logical branches or edge cases, include test cases for them.\n\n6.Do not include the original code in the output—only the test code.",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "Chapter 13\n\n7.Import any modules or classes necessary for the tests to run.\n\n8.Format your output as a complete, valid Python test file using the unittest module.\n\n________________________________________\n\nInput:\n\npython\n\n{{{\n\n# Paste Python code here\n\n}}}\n\n________________________________________\n\nOutput:\n\nApplying this template to the ngrams.py code produced seven different tests without further elaboration. The file is listed in test_ngrams_chatgpt.py in the GitHub repository for the book. One of the tests created was incorrect and subsequently failed, as shown in Figure 13.6. The test incorrectly suggests that a $ and s are equivalent, instead of replacing the dollar symbol with a space.\n\nFigure 13.6: ChatGPT produced one incorrect test out of seven for ngrams.py\n\n319",
      "content_length": 782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "320\n\nWriting and Maintaining Unit Tests\n\nWhen prompted Are there additional tests that should be included?, ChatGPT produced an additional set of tests shown in Figure 13.7, each with code, and offered to convert them into a code library:\n\nFigure 13.7: Additional ChatGPT tests that it suggested when prompted\n\nAs a supplement to the unit tests, additional tests can also be run directly within ChatGPT and included in the overall test suite. The test_ngrams_chatgpt_extra.py file in the GitHub repos- itory shows the tests generated. In addition to running the tests in ChatGPT, this platform also explains why tests fail. Figure 13.8 presents a valuable discussion of the test failure and the han- dling of tabs and newlines, which could be a failed test or an issue with the original code. Such insight into the code will help improve the overall quality. ChatGPT made suggestions to change the tests and rerun them:\n\nFigure 13.8: Explanation of a failed text from ChatGPT",
      "content_length": 975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "Chapter 13\n\nChatGPT is clearly effective at writing unit tests, even if it requires multiple requests. It is also pos- sible to write other kinds of tests, such as data-driven tests, which is the topic of the next section.\n\nData-driven tests The GitHub Copilot or ChatGPT prompt can enable data-driven or random testing that supple- ments the unit test coverage. Using the Generate data driven tests for lowercase_remove_ punct_numbers prompt with Copilot gave the following code:\n\nimport unittest\n\nfrom src.ngrams import lowercase_remove_punct_numbers\n\nclass TestLowercaseRemovePunctNumbers(unittest.TestCase):\n\ndef test_data_driven(self):\n\ntest_cases = [\n\n# (input, expected_output)\n\n(\"Hello, World! 123\", \"hello world \"),\n\n(\"ABCdef\", \"abcdef\"),\n\n(\"1234!@#$\", \"\"),\n\n(\"A1! b2@ C3#\", \"a b c\"),\n\n(\"\", \"\"),\n\n(\" \", \" \"),\n\n(\"Python3.8_is#awesome!\", \"pythonisawesome\"),\n\n(\"No PUNCTUATION\", \"no punctuation\"),\n\n(\"MiXeD CaSe &*^%\", \"mixed case \"),\n\n(\"newline\\nTab\\tSpace \", \"newline\\ttab\\tspace \"),\n\n]\n\nfor input_text, expected in test_cases:\n\nwith self.subTest(input=input_text):\n\nself.assertEqual(lowercase_remove_punct_numbers(input_ text), expected)\n\nif __name__ == \"__main__\":\n\nunittest.main()\n\n321",
      "content_length": 1196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "322\n\nWriting and Maintaining Unit Tests\n\nThis file can be found as test_data_driven_ngrams.py in the book’s GitHub repository. This example and the unit tests in the previous section show how GenAI can create tests from the implementation code. GitHub Copilot and ChatGPT use this code to identify meaningful test cases, including many edge cases. To improve the coverage, additional tests can be written man- ually, semi-manually with GenAI, or by simply requesting additional tests. These unit tests can be extended similarly to other types of testing, such as integration testing, performance testing, and A/B testing.\n\nAs discussed in these two sections, both unit tests and data-driven tests can be written using GenAI from the implementation code. While creating test code from the implementation code works, one should consider writing the tests first rather than the implementation. This approach is called test-driven development and is described in the next section.\n\nTest-driven development Test-driven development (TDD) is one component of the Extreme Programming (XP) agile soft- ware methodology proposed by Kent Beck in the 1990s. Two of his books can be found in the Further reading section. The main concept of TDD is that the unit tests are written first and subsequently fail since there is no code to support them. The software engineer then writes the code until the tests pass. It is widely accepted that this practice improves code design and enhances testability. Empirical studies such as that by Mäkinen & Jürgen Münch (https://link.springer.com/cha pter/10.1007/978-3-319-03602-1_10) generally support the value of TDD. The approach also focuses the software engineer on passing tests rather than adding other features.\n\nWhether to write tests first or last often becomes a religious debate with devout followers on either side. We will not attempt to resolve the issue here. If we believe that GenAI always writes perfect code that exactly and cleanly meets all the requirements, then it does not matter whether the tests or the code are used to write the other. A recent 2024 IEEE study by Jiri et al. (https:// ieeexplore.ieee.org/document/10685204) showed that unit tests written by humans have fewer errors than unit tests by GenAI, but the technology is advancing rapidly and closing this gap. This leaves us with a conundrum that the code may be flawed, and the tests may not be perfect, yet we need both to be accurate. The more common convention of software engineers is to write the code using GenAI and then use GenAI to write the tests, which mirrors their development practice. With TDD, there is a potential to write the perfect tests, perhaps with GenAI, and then use GenAI to prove that the implementation passes the tests and their perceived requirements. In fact, we can go further and use GenAI to iterate and craft the code that meets these requirements.",
      "content_length": 2901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "Chapter 13\n\nAs an example for this section, we will use the intersection of two rectangles problem, which simply identifies the overlapping area between two rectangles. To simplify the problem, we restrict the rectangle definition to use two points—the x and y coordinates of the lower-left and upper-right corners of the rectangle. In our notation, the rectangle is represented as (xll, yll, xur, yur) as shown in Figure 13.9:\n\nFigure 13.9: Intersecting rectangle problem\n\nRather than immediately start coding the result, we will instead consider the various test cases, which are broken down into the following:\n\n1.\n\nIntersecting rectangles (Figure 13.10):\n\nFigure 13.10: Example test cases for overlapping rectangles\n\n2. Non-intersecting rectangles (Figure 13.11):\n\nFigure 13.11: Example test cases for non-overlapping rectangles\n\n323",
      "content_length": 837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "324\n\nWriting and Maintaining Unit Tests\n\n3.\n\nInvalid rectangles (Figure 13.12):\n\nFigure 13.12: Example test cases for invalid rectangles that fail our definition\n\nAs part of the TDD process, we begin by coding the unit tests using GenAI. Using descriptive test names facilitates the accurate creation of the test code. A few tests are shown to illustrate the idea in the following code snippet (the full file with 21 unit tests is available in the book’s GitHub repository as test_rectangle_intersection.py):\n\nimport unittest\n\nfrom rectangle_intersection import rect_intersection_area\n\nclass TestRectangleIntersectionArea(unittest.TestCase):\n\ndef test_intersecting_rectangles(self):\n\nrect1 = (0, 0, 4, 4)\n\nrect2 = (2, 2, 6, 6)\n\nself.assertEqual(rect_intersection_area(rect1, rect2), 4)\n\ndef test_intersecting_rectangles_swapped(self):\n\nrect1 = (2, 2, 6, 6)\n\nrect2 = (0, 0, 4, 4)\n\nself.assertEqual(rect_intersection_area(rect1, rect2), 4)\n\ndef test_non_intersecting_rectangles(self):\n\nrect1 = (0, 0, 2, 2)\n\nrect2 = (3, 3, 5, 5)\n\nself.assertEqual(rect_intersection_area(rect1, rect2), 0)\n\nFor the tests to appropriately fail as the next step of the TDD process, we need a stub file, rectangle_ intersection.py, containing rect_intersection_area(rect1, rect), which arbitrarily returns None. An example of this code is shown as follows and is stored as rectangle_intersection.",
      "content_length": 1373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "Chapter 13\n\npy in the same folder. These names are based on the import statement at the beginning of the preceding test file code, as shown:\n\ndef rect_intersection_area(rect1, rect):\n\nreturn None\n\nWith the TDD process, we have encoded the requirements of the code as tests. The only missing piece is the actual implementation. Normally, we would iterate between writing the implemen- tation and running the tests to ensure that the code functions as expected. Using GenAI, we can replace this process with a request to GenAI to iteratively write the code until it passes all the tests. We will discuss how this can be achieved using GitHub Copilot in both VS Code and PyCharm IDEs, and then show the TDD approach using ChatGPT.\n\nTDD using GitHub Copilot within VS Code The TDD approach within VS Code is arguably the most straightforward. First, we ensure that both rectangle_intersection.py and test_rectangle_intersection.py are open, then run the latter unit test file. Then, we can use this command in the Copilot chat window:\n\n@terminal /fix rect_intersection_area according to the unit test failure\n\nThis produces an updated set of code in the same chat window that we copied and pasted as a replacement for the rectangle_intersection.py implementation. This is shown as follows:\n\ndef rect_intersection_area(rect1, rect2):\n\n\"\"\"\n\nCalculate the area of intersection between two rectangles.\n\nEach rectangle is represented as a tuple (x1, y1, x2, y2):\n\n(x1, y1): bottom-left corner\n\n(x2, y2): top-right corner\n\n\"\"\"\n\n# Calculate the overlapping region\n\nx_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]))\n\ny_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]))\n\n# If there is an overlap, calculate the area\n\nreturn x_overlap * y_overlap\n\n325",
      "content_length": 1775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "326\n\nWriting and Maintaining Unit Tests\n\nRunning the same unit tests against this new implementation reduced the number of tests that failed from 21 to 9. Most of the failures are caused by the lack of exceptions raised for invalid rectangles. These were included in the tests but ignored by the GenAI.\n\nWe repeated the process with the same /fix command to produce the next iteration of code implementation, but the errors persisted. In some cases, running the process two or three times resolved the errors. In other cases, we needed to manually revise the /fix command with more explicit instructions:\n\n@workspace /fix rectangle_intersection_area according to the unit test failure and be sure to raise ValueError for invalid rectangle dimensions\n\nThe generated implementation passed all the unit tests and is shown as follows:\n\ndef rect_intersection_area(rect1, rect2):\n\n\"\"\"\n\nCalculate the area of intersection between two rectangles.\n\nEach rectangle is represented as a tuple (x1, y1, x2, y2):\n\n(x1, y1): bottom-left corner\n\n(x2, y2): top-right corner\n\n\"\"\"\n\n# Ensure the rectangles are valid\n\nif rect1[0] >= rect1[2] or rect1[1] >= rect1[3]:\n\nraise ValueError(f\"Invalid rectangle dimensions for rect1: {rect1}\")\n\nif rect2[0] >= rect2[2] or rect2[1] >= rect2[3]:\n\nraise ValueError(f\"Invalid rectangle dimensions for rect2: {rect2}\")\n\n# Calculate the overlapping region\n\nx_left = max(rect1[0], rect2[0])\n\ny_bottom = max(rect1[1], rect2[1])\n\nx_right = min(rect1[2], rect2[2])\n\ny_top = min(rect1[3], rect2[3])\n\n# Check if there is an actual overlap\n\nif x_left < x_right and y_bottom < y_top:\n\nreturn (x_right - x_left) * (y_top - y_bottom)\n\nelse:\n\nreturn 0 # No overlap",
      "content_length": 1670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "Chapter 13\n\nIn this section, we used a simple one-line command within the VS Code chat to create an im- plementation. It uses the implementation code, unit tests, and the output of the tests directly to write an improved version of the implementation code.\n\nTDD using GitHub Copilot within PyCharm Using the TDD approach within PyCharm is not quite as simple as with VS Code. As of January 2025, GitHub Copilot cannot access PyCharm’s terminal window, which means it cannot access the failed unit tests directly. The output of failed tests informs Copilot what needs to be fixed in the implementation, and so this is a critical component. As a solution, we can paste the error message of the failed messages into the GitHub Copilot chat window:\n\nCONTEXT: You will be given an error message enclosed in {{{ERROR}}}.\n\nTASK: Fix the implementation of rectangle_intersection_area _mean so it passes all tests.\n\nERROR: {{{ ... <omitted for brevity> ... }}}\n\nFIXED CODE:\n\nThis prompt, whether used in an inline chat or a chat window, should generate a fully functional implementation that passes the unit tests.\n\nFor TDD with PyCharm, we need to use a more complete template call within the chat that passes the output of the failed unit tests. With this information, as well as the implementation and test code, a new version of the implementation code aims to solve the issues and pass the unit tests. A similar approach can be used with ChatGPT, which will be described next.\n\nTDD using ChatGPT As part of the TDD process, ChatGPT can also be used to generate improved implementation code based on the combination of the existing implementation code, unit test code, and the errors or results of the unit test execution. We use triple brackets to enclose each of these components:\n\nCONTEXT: You are provided with\n\n1. Python function enclosed with {{{ FUNCTION }}} that has a bug.\n\n2. Test suite code enclosed with {{{ UNIT TESTS }}}.\n\n3. Error message enclosed with {{{ ERROR }}}.\n\nTASK: Fix the function implementation\n\nFUNCTION: {{{def rect_intersection_area(rect1, rect2):\n\nreturn None\n\n327",
      "content_length": 2090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "328\n\nWriting and Maintaining Unit Tests\n\n}}}\n\nUNIT TESTS: {{{ ... omitted for brevity ... }}}\n\nERROR: {{{ ... omitted for brevity ... }}}\n\nCODE:\n\nIn this instance, ChatGPT produced the following output code, which is similar but mildly differ- ent from the GitHub Copilot output. ChatGPT explained the implementation, and all the created tests passed in the first iteration.\n\ndef rect_intersection_area(rect1, rect2):\n\nx1_1, y1_1, x2_1, y2_1 = rect1\n\nx1_2, y1_2, x2_2, y2_2 = rect2\n\n# Validate input rectangles\n\ndef is_invalid_rect(x1, y1, x2, y2):\n\nreturn x1 >= x2 or y1 >= y2\n\nif is_invalid_rect(x1_1, y1_1, x2_1, y2_1) or is_invalid_rect(x1_2, y1_2, x2_2, y2_2):\n\nraise ValueError(\"Invalid rectangle with non-positive width or height\")\n\n# Find intersection bounds\n\ninter_left = max(x1_1, x1_2)\n\ninter_right = min(x2_1, x2_2)\n\ninter_bottom = max(y1_1, y1_2)\n\ninter_top = min(y2_1, y2_2)\n\n# Check if rectangles intersect\n\nif inter_left >= inter_right or inter_bottom >= inter_top:\n\nreturn 0\n\n# Compute and return intersection area\n\nreturn (inter_right - inter_left) * (inter_top - inter_bottom)",
      "content_length": 1095,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "Chapter 13\n\nIf the tests had not passed, the new code would need to be copied and pasted into the IDE, replacing the previous version. The unit tests would need to be rerun against the new code. The revised im- plementation code and resulting unit test errors must be copied again into the ChatGPT template.\n\nThe advantage of the TDD approach is that it asserts that the implementation is correct based on the fact that the code passes the unit tests. The code produced in this simple example is quite reasonable, but more complex tasks may produce functional code of lower quality. In such cases, the code should be refactored. However, this is much easier with a full set of tests that continue to prove the code is meeting the functional requirements.\n\nWhether TDD is the ideal strategy is still up for debate. Without TDD, the user focuses on the implementation, from which the GenAI can certainly create reasonable tests. As we observed, some of the tests may fail, and some may indicate limitations of the implementation. The software developer must then understand all the tests and provide enough logical coverage to be confident in the implementation. With TDD, the cognitive effort is in assuring the tests are sufficient, from which code can be generated. If the GenAI provides an implementation, the most the developer will have to do is refactor the code.\n\nWhether the code or tests are written first, the tests often require meaningful datasets. The next section describes how GenAI can be used to create realistic data.\n\nSample data creation with GenAI One aspect of testing is the availability of data that is representative of the kind you will encoun- ter in the wild. If you have been involved in creating mock data in years past, you will certainly appreciate GenAI approaches to this problem. The new techniques can create a variety of types of data with as specific a prompt as you need. For example, the following prompt works in either ChatGPT or GitHub Copilot:\n\ncreate a table with 10 rows and the following columns:\n\n5 digit integer id called \"ID\"\n\nfirst name\n\nlast name\n\naddress consisting of a number, street name, and whether it is a st., dr. or ave.\n\ncity\n\ncountry\n\ndisease\n\nage\n\n329",
      "content_length": 2214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "330\n\nWriting and Maintaining Unit Tests\n\nThe resulting table output is shown in Figure 13.13, which includes typical names, address formats, diseases, and so on. This kind of data can be saved to a file and used for testing. Although such data could be generated on the fly, the purpose of unit testing is to create repeatable tests that achieve consistent results. Tests are usually added to a regression test package as part of the CI/ CD methodology and rerun periodically to ensure that past errors do not recur as code versions evolve. Therefore, the data should be stored and reused.\n\nFigure 13.13: Example table of mock data produced by GitHub Copilot\n\nOf course, the data is not limited to addresses but can be more technical, such as the Create a FASTA file with 3 sequences of length 15 to 25 nucleotides prompt, which returns three bioinformatics DNA sequences as shown in Figure 13.14:\n\nFigure 13.14: A FASTA bioinformatics sequence file with three DNA sequences",
      "content_length": 974,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "Chapter 13\n\nAs can be seen from the previous output, it suggests saving the file as a .fasta file. In fact, it can create and save data in any format, including tables, JSON, CSV, and so on.\n\nAnother use case that is often convenient when scripting is to create some temporary data for a function, perhaps in a Jupyter notebook. Consider the Euclidean distance method:\n\ndef get_euclidean_dist(a: np.ndarray, b: np.ndarray) -> float:\n\n\"\"\"Return the squared Euclidean (L2) distance between two arrays.\"\"\"\n\nreturn np.sqrt(np.sum((a - b) ** 2))\n\nGitHub Copilot used the Create sample inputs for get_euclidean_distance() prompt to produce a few samples of inputs that can be directly applied, as shown in Figure 13.15:\n\nFigure 13.15: GitHub Copilot creates example inputs for functions for informal testing\n\nIn the TDD process, developers write the unit test code first, and the implementation code is written to pass the unit tests. The unit tests can be written by developers using standard Ge- nAI approaches to verify that the implementation code will meet the requirements. From either VS Code or PyCharm, GitHub Copilot can iteratively create the implementation code from the unit test code alone. ChatGPT offers similar functionality through a prompt template. The next section provides recommendations on how GenAI approaches can best be used to complete the implementation and test code.\n\n331",
      "content_length": 1396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "332\n\nWriting and Maintaining Unit Tests\n\nPractical insights Testing has been a focus of technology from the templates of “vintage AI” to the present time with GenAI. There has always been a tendency to write the code and eventually write the tests to make it acceptable to your organization. These new GenAI tools can definitely increase the velocity of the coding process. The verification process to check the code should not be just a one-time visual inspection but should involve unit tests that can be applied repeatedly to assert the correctness of the code.\n\nThe different GenAI solutions produce slightly different results, but we generally prefer GitHub Copilot for creating tests since the close integration with the code and iterative approach is easier using the single tool within the IDE, whether VS Code or PyCharm. By keeping similar test code open in the browser, accuracy and style improve due to the extra context.\n\nRegardless of the particular GenAI tool used, it is critical to check the implementation and test code. Using GenAI to create the test code from the implementation or vice versa helps minimize errors since the implementation code that passes unit tests achieves at least a first level of re- quirement verification. Failing unit tests provide a chance to rethink code requirements for the implementation and test code.\n\nSummary In this chapter, we explored various aspects of testing using GenAI. Although there are many types of tests and they are the topic of many books, the most common tests are unit tests. In unit testing, each test should test a single point of functionality and should be independent of other tests. Together, a complete set of unit tests proves that the code implementation functions as expected. If the code is refactored, augmented, or integrated in the future, the same unit tests assert the correctness of the functionality.\n\nThe chapter described a few approaches to creating tests from the implementation. GitHub Co- pilot and ChatGPT can both be used to easily write unit tests for a single method or an entire Python file. Both techniques have some variability in terms of the types of tests and coverage that they provide in a first pass. In some cases, the tests may not be correct and may need additional prompting or manual correction. In other scenarios, prompting may be useful to add further tests to improve the coverage of the edge cases. Developers must be cautious that the tests accurately capture the requirements and are sufficient to assert the correctness of the code.",
      "content_length": 2553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "Chapter 13\n\nTDD is a methodology from Extreme Programming where unit tests are written first and, by defi- nition, fail. The implementation code is written to pass all the unit tests. GenAI can help suggest the test code, especially when descriptive method names are used. Using a set of tests, either GitHub Copilot or ChatGPT can iteratively generate the implementation code automatically. The process repeats until the unit tests pass. By writing the full set of tests first, the developer conveys the scope of the tests and behavior in various edge cases. The only required interaction with the generated implementation code is possibly refactoring it to improve quality, which is easier to achieve with the unit tests.\n\nTwo additional topics were covered in the chapter, which involved enriching data for tests. GenAI can create more elaborate tests by synthesizing different scenarios. Data-driven testing creates test pairs for inputs and outputs to test multiple scenarios. In addition, the technology can create realistic test sets applicable to a wide range of disciplines.\n\nIn the next chapter, we introduce ways that GenAI can identify capacity limits in terms of execution speed and memory management. We will also show how GenAI can optimize code to improve performance under these two constraints.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: What are the advantages of TDD?\n\nAnswer: TDD, or test-driven development, is a methodology where the tests are written first, and the implementation code follows. The tests cover the range of required functionality and edge cases to ensure they meet expectations. Using TDD ensures that the tests are written and that the code (written by humans or GenAI) functions correctly. Empirical results show that the implementations are generally structured better and are more amenable to tests.\n\nQuestion 2: If software includes both implementation and unit tests, does it need to be manually verified?\n\nAnswer: Without question, a full set of unit tests and correct implementation help identify failures or misinterpretations of the requirements. A failed test is likely due to a flawed test or a flawed implementation and provides an opportunity to review the requirements. While testing helps identify such issues, we have observed many cases where the generated tests lack full coverage and may be incorrect. Similarly, there are numerous examples of incorrect implementations. The authors recommend manual verification in addition to complete unit tests.\n\n333",
      "content_length": 2602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "334\n\nWriting and Maintaining Unit Tests\n\nFurther reading\n\n\n\nKent Beck’s book on extreme programming. 2000. Extreme Programming Explained: Em- brace Change.\n\n\n\nKent Beck’s book on test-driven development. 2002. Test-Driven Development by Example.\n\n\n\nEffects of Test-Driven Development: A Comparative Analysis of Empirical Studies. Simo Mäkinen & Jürgen Münch: https://link.springer.com/chapter/10.1007/978-3-319-03602-1_10\n\n\n\nLeveraging Large Language Models for Python Unit Test. Medlen Jiri, Bari Emese, Patrick Medlen. 2024: https://ieeexplore.ieee.org/document/10685204.\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.",
      "content_length": 758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "14\n\nGenAI for Runtime and Memory Management\n\nIn the age of big data, artificial intelligence, and intensive data processing, an essential char- acteristic of well written Python programs is their ability to efficiently manage both memory usage and runtime performance. As part of the Software Development Life Cycle (SDLC), it is common practice to profile our programs, anticipate performance limitations and strive for efficiency in both aspects.\n\nIn addition to utilizing traditional tools for profiling program performance and addressing inef- ficiencies, we can leverage the model mastery of Large Language Models (LLMs) to code profile snippets, analyze maximal capacities, and optimize code to handle larger workloads.\n\nIn this chapter, we will examine two examples of inefficient implementations: a Fibonacci calcu- lator with significant runtime overhead and a statistical analysis function that demands excessive memory for large matrices. By utilizing prompt precision best practices and chaining prompt engineering technique, we will demonstrate how GenAI applications can help profile performance metrics, estimate maximum capacities, and optimize the code to handle larger inputs effectively.\n\nThe topics covered in this chapter include:\n\n\n\nIntroducing time and space complexity analysis\n\n\n\nProfiling runtime and memory consumption with GitHub Copilot\n\n\n\nAnalyzing maximal capacity with ChatGPT\n\nOptimizing code with chained prompts",
      "content_length": 1446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "336\n\nGenAI for Runtime and Memory Management\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\nGitHub Copilot account\n\n\n\nIDE – either VS Code or Pycharm\n\nOpenAI account with access to ChatGPT and OpenAI API\n\n\n\nAccess to the book’s repository available at: https://github.com/PacktPublishing/ Supercharged-Coding-with-Gen-AI\n\n\n\nVirtual environment set up in VS Code or PyCharm\n\nOpenAI API token\n\nFor assistance setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nIntroducing time and space analysis In our current technology cycle, most companies are leveraging big data and artificial intelligence to improve their businesses. For example, videos on the YouTube and TikTok platforms are viewed over a billion times per day which generates recommendations and advertising value. In these cases, even 1% gains in efficiency for storage or computation may save millions of dollars.\n\nAccording to Lambda Labs, training the generative pre-trained transformer model GPT-3 involved about 700 gigabytes of Random Access Memory (RAM) and thousands of powerful Nvidia GPUs operating in parallel, with a cumulative compute time equivalent to 355 years. Even a minor inefficiency in implementing these models could lead to substantial costs or, in the worst case, create a bottleneck that would prevent the model from running effectively.\n\nThe runtime of a program The runtime of a program refers to the time it takes to execute the tasks defined in the source code on a specific hardware configuration with a given input. The runtime duration of a program depends on factors such as the computational complexity (the number of operations required to complete the task) and the hardware such as the central processing unit (CPU).",
      "content_length": 1990,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "Chapter 14\n\n337\n\nFor example, consider an inefficient implementation of the Fibonacci number computation using recursion:\n\ndef fibonacci_recursive(n):\n\nif n <= 1:\n\nreturn n\n\nreturn fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2)\n\nIn this implementation, each Fibonacci number computation involves two recursive calls: one for the previous number and another for the one before that. Each of these calls triggers two additional recursive calls, and so on, until the base condition n <= 1 is met.\n\nTo illustrate how quickly the runtime of this implementation grows, here are examples of recorded runtimes on a MacBook Pro with an M1 chip using this implementation:\n\nRuntime for fibonacci_recursive(5): 0.000006 seconds\n\nRuntime for fibonacci_recursive(35): 1.7051 seconds\n\nRuntime of fibonacci_recursive(40): 19.5045 seconds\n\nComputing the fibonacci_recursive(100) is estimated to take 15 trillion seconds, or approxi- mately 475,000 years, making it completely impractical. This exponential growth represents one of the least desirable complexities in algorithm design as it remains inefficient regardless of the hardware used, as we will now explore.\n\nRuntime complexity refresher Since computers vary in speed, it is widespread practice to measure runtime complexity in general terms using Big-O notation, denoted as . This notation describes how the runtime scales relatively to the size of the input data.\n\n𝑂𝑂(…)\n\nFor instance:\n\n\n\nFinding the minimum number in an unsorted array of length of\n\n\n\nhas a runtime complexity\n\n𝑛𝑛𝑛\n\n\n\nPrinting all 𝑂𝑂(𝑛𝑛) plexity of\n\ncombinations of elements in an array of length\n\n. (𝑖𝑖𝑖𝑖𝑖𝑖) 2\n\nhas a runtime com-\n\n𝑛𝑛𝑛\n\n𝑂𝑂(𝑛𝑛\n\n)\n\nRetrieving the nth Fibonacci number using fibonacci_recursive has a runtime complexity of\n\n.\n\n𝑛𝑛\n\n𝑂𝑂(2\n\n)",
      "content_length": 1777,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "338\n\nGenAI for Runtime and Memory Management\n\nIf you need a further refresher on Big-O notation or are unfamiliar with it, we recommend checking the Further Reading section at the end of this chapter. Big-O notation is an essential concept for developers, both in application design and day-to-day problem-solving.\n\nThe space consumption of a program The RAM consumed by a program during execution reflects its memory space usage. For instance, a MacBook Pro with 16 gigabytes (GB) of RAM can hold about 2 billion float64 values where each float64 occupies 8 bytes.\n\nConsider the following inefficient function get_top_video that reads a comma-separated values (CSV) file into memory as a Pandas DataFrame and returns the video with the highest average watch percentage:\n\ndef get_top_video(path):\n\ninteractions = pd.read_csv(path)\n\navg_ratio = interactions.mean(axis=0, skipna=True)\n\nreturn avg_ratio.idxmax()\n\nThe input CSV file may represent a matrix as follows, where each cell indicates the percentage of a specific video watched by a user:\n\nvideo_1\n\nvideo_2\n\nvideo_3\n\nvideo_4\n\nvideo_5\n\nvideo_6\n\n...\n\nuser_1\n\n0.5\n\n1\n\nuser_2\n\n0.1\n\n0.7\n\n0.9\n\nIf the input dataset contains 50,000 users (rows) and 50,000 videos (columns), this would involve loading 2.5 billion cells into memory. Such a large dataset exceeds the 16 GB RAM limit, leading to an out-of-memory error and causing the function to fail.\n\nSpace complexity To assess memory usage, developers often use space complexity, expressed in Big-O notation, to describe how memory usage scales with the size of the data structure. For example, in get_top_ video, the space complexity is columns, as the function  for a CSV file with loads the entire dataset into memory.\n\nrows and\n\n𝑂𝑂(𝑚𝑚 𝑚 𝑚𝑚)\n\n𝑚𝑚𝑚\n\n𝑛𝑛𝑛",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "Chapter 14\n\nBalancing efficiency tradeoffs When designing programs, it is common to encounter tradeoffs between memory usage and runtime efficiency. For instance, caching is the process of storing information such as a function’s calculated output values in RAM rather than recomputing them. This can significantly improve the speed of repeated calculations but also increases memory consumption. In a program that calls fibonacci_recursive repeatedly with the same input, such as n=35 which previously took 1.7 seconds to run, caching the result could save 1.7 seconds for each subsequent call.\n\nAnother tradeoff is with the get_top_video we read the entire file into memory before starting the calculations. It may be more practical to read smaller batches of data from disk and compute the cumulative top videos for each batch sequentially rather than handling the entire dataset at once. Reading and processing smaller chunks of large CSV files might take more time but helps avoid exceeding RAM limitations.\n\nNow that we grasp the importance of runtime and space efficiency, how to assess their complexity, and the trade-offs involved, we can move forward and explore how GenAI can assist with each step in managing and optimizing program efficiency.\n\nProfiling runtime and memory consumption with GitHub Copilot Profiling runtime and memory usage is a straightforward process. The built-in time module is useful for tracking runtime and the third-party memory_profiler library monitors memory usage. The next two sub-sections describe how GitHub Copilot can assist by either completing our imple- mentation or generating the code from scratch for these two cases. At the end of this section, we will ask ChatGPT to predict runtimes and memory size constraints using these analyses as input.\n\nProfiling runtime Measuring the runtime of a function in Python helps evaluate whether the time taken to complete the tasks in the source code aligns with acceptable thresholds and identifies potential areas for improvement. This can be achieved using a script like the following:\n\nstart = time.process_time()\n\nrecursive_fibonacci(35)\n\nend = time.process_time()\n\nprint(f\"Time taken: {end – start:.3f} seconds\")\n\n339",
      "content_length": 2213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "340\n\nGenAI for Runtime and Memory Management\n\nBy recording the time before and after execution, we can calculate the runtime as the difference between these times to compute the 35th Fibonacci number.\n\nIn Chapter 15, we will explore how to use decorators to log application performance. For now, we will simply use start and end times to measure program runtime.\n\nSince the profiling code is short and straightforward, the authors mostly use GitHub Copilot for such tasks. The following example demonstrates how Copilot can understand the purpose of our file, profile_runtime.py, by applying the five S’s for prompt precision as shown in Figure 14.1:\n\nFigure 14.1: Copilot completion\n\nIn this example, we named the file profile_runtime.py and included the necessary imports for the time package and the fibonacci_recursive function to establish a clear context for Copilot. Based on these, Copilot is likely to identify our goal of profiling the fibonacci_recursive func- tion and suggests a complete implementation for runtime profiling.\n\nAlternatively, we can use chat, ChatGPT or OpenAI API to generate the profiling script from scratch. For example, we could use the following prompt in Copilot chat:\n\nprint the runtime of fibonacci_recursive for n=35",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "Chapter 14\n\nBelow is a sample implementation provided by Copilot in PyCharm:\n\nFigure 14.2: Copilot chat to write the full method from a profiling suggestion\n\nThis code can be used as-is to profile the runtime of the function. Alternatively, we can make slight modifications, such as rounding the runtime value for better readability.\n\nHere is an output from running the suggested profiling script:\n\nResult: 9227465\n\nRuntime: 1.7093181610107422 seconds\n\nWhile a runtime of 1.7 seconds for n=35 might seem reasonable, the runtime for larger inputs, such as 50 or 100, becomes impractical. Later in this chapter, we will explore techniques to identify and address these challenges.\n\n341",
      "content_length": 683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "342\n\nGenAI for Runtime and Memory Management\n\nProfiling space usage Profiling a program’s memory usage can be achieved with the @profile decorator from the memory_profiler package. This lightweight tool monitors memory usage for every line within a function. The package should be included in the requirements.txt file of this book’s repository, so it should already be accessible in your virtual environment.\n\nFor example, to profile the function get_top_video, which retrieves the video with the highest average watch percentage, we can add the @profile decorator before the function definition. A call to the function with the file interactions_10_000.csv that records 10,000 users’ watch percentages of 10,000 videos will log the memory consumption for each line:\n\nimport pandas as pd\n\nfrom memory_profiler import profile\n\n@profile\n\ndef get_top_video(path):\n\ninteractions = pd.read_csv(path)\n\navg_ratio = interactions.mean(axis=0, skipna=True)\n\nreturn avg_ratio.idxmax()\n\nget_top_video('interactions_10_000.csv')\n\nThe profiling output provides details such as line numbers, total memory usage, incremental memory usage, the number of times each statement was executed, and the content of those executions. For reference, Figure 14.3 shows the output from the code above:\n\nFigure 14.3: Profiler output showing memory consumption by line",
      "content_length": 1339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "Chapter 14\n\nThe file interactions_10_000.csv contains 10,000 x 10,000 cells of type float64, requiring 8 bytes each, and the expected memory usage is approximately:\n\nMB\n\n10,000 × 10,000 × 8  ≈ 800\n\nThis aligns closely with the memory consumption recorded at line 7, where loading the CSV file into a Pandas DataFrame consumed 889.7 MB. This includes both the memory required for the cell data and the overhead associated with the Pandas DataFrame object such as column and row names, metadata, and type.\n\nHaving learned how to use GitHub Copilot to simplify the creation of profiling scripts for runtime and space complexity, we can move forward and explore how to use Copilot to profile multiple runs and ChatGPT to estimate the maximum input capacity that our functions can manage.\n\nAnalyzing maximal capacity with ChatGPT Identifying the maximal capacity of a function means determining the largest input beyond which it will fail due to either memory limitations or unacceptable runtime. One strategy for such predictions involves profiling its performance for a few input sizes, identifying the runtime or space growth, and then estimating the maximal capacity the function can handle. GenAI can assist in this two-step process by automating profiling for different inputs and using the results as supplementary data for a prompt to compute the maximal capacity.\n\nProfiling runtime across multiple runs Copilot can assist us in creating a profiling loop for fibonacci_recursive like we did for a single profile. For instance, if we want to profile the runtime of the function for inputs ranging from 10 to 40 in steps of 5, we can start by adding the following import statements to the file:\n\nimport time\n\nfrom fibonacci import fibonacci_recursive\n\nThen use a prompt in Copilot chat like this:\n\nprint the runtime of fibonacci_recursive for n=10:41:5\n\n343",
      "content_length": 1860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "344\n\nGenAI for Runtime and Memory Management\n\nThis prompt, along with the surrounding context of the import statements in the file, is likely to generate a code to profile runtimes for varying arguments:\n\nFigure 14.4: Profiling suggestion for profiling multiple Fibonacci inputs\n\nWe can use the generated code as-is or make minor adjustments, such as rounding the recorded times to four decimal places or changing the range’s stop value to 41. When executed on an M1 MacBook Pro with a 16GHz CPU, the following runtime profiling was recorded:\n\nRuntime for fibonacci_recursive(10): 0.0000 seconds\n\nRuntime for fibonacci_recursive(15): 0.0002 seconds\n\nRuntime for fibonacci_recursive(20): 0.0013 seconds\n\nRuntime for fibonacci_recursive(25): 0.0150 seconds\n\nRuntime for fibonacci_recursive(30): 0.1571 seconds\n\nRuntime for fibonacci_recursive(35): 1.7051 seconds\n\nRuntime for fibonacci_recursive(40): 19.5045 seconds\n\nThis runtime profiling will be used as input for a GenAI application to analyze the function’s complexity and determine the input size capacity.",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "Chapter 14\n\nProfiling memory consumption across multiple runs Profiling the memory consumption across multiple runs of get_top_video function can be done with a straightforward approach:\n\npaths = ['interactions_100.csv',\n\n'interactions_1000.csv',\n\n'interactions_10_000.csv']\n\nfor p in paths:\n\nprint(\"top video: \", get_top_video(p))\n\nIn this example, the files interactions_100.csv, interactions_1000.csv, and interactions_10_000.csv represent datasets of user-video interactions of row and column length corresponding to the numbers in their filenames.\n\nCopilot can assist by completing the for loop, after including the filenames, as shown below:\n\nFigure 14.5: Copilot code completion suggestion\n\nThe generated code can be used as-is or modified slightly, for instance, by enhancing the print statement to include information about the file size.\n\n345",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "346\n\nGenAI for Runtime and Memory Management\n\nFor reference, running this code produced the following profiling:\n\nFigure 14.6: profiling memory usage output\n\nThe memory usage for reading the CSV files grows as follows:\n\n\n\n0.6 MB for a 100 x 100 matrix\n\n\n\n29.9 MB for a 1,000 x 1,000 matrix\n\n\n\n889 MB for a 10,000 x 10,000 matrix\n\nThis memory consumption data can be included as supplementary information in our prompts included as supplementary information in our prompts to estimate maximum matrix sizes that the function can handle, as we see next in this chapter.",
      "content_length": 566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "Chapter 14\n\nDetermining maximal capacity within runtime constraints The GPT-4o1 model is specifically designed to excel in chain-of-thought reasoning, making it highly effective for determining maximum capacities under runtime constraints. To leverage this capability, we can create precise prompts following the five S’s, including the function’s source code and associated profiling. An ideal structure for these prompts might look as follows:\n\nCONTEXT: You are provided with:\n\n1. Python function enclosed with {{{ FUNCTION }}}\n\n2. Runtime profiling enclosed with {{{ PROFILING }}}.\n\n3. Runtime limit enclosed with {{{ LIMIT }}}\n\nTASK: What is the maximal input the function can run in the time limit?\n\nFUNCTION: {{{ ... }}}\n\nPROFILING: {{{ ... }}}\n\nLIMIT: {{{ ... }}}\n\nMAXIMAL INPUT:\n\nFor example, when using this prompt with a runtime limit of 100 seconds, the GPTo1-mini model in ChatGPT provided the following reasoning steps:\n\n1. Profiling Analysis: Repeatedly reviewed the reported profiling.\n\n2. Observations: Identified the exponential growth in runtime.\n\n3. Calculations: Computed the largest input within the time limit.\n\n4. Conclusion: Determined the maximum input to be 43.\n\nThese reasoning steps result in the final answer, the conclusion step, as illustrated in Figure 14.7:\n\nFigure 14.7: GPT-4o1-mini maximal capacity conclusion\n\n347",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "348\n\nGenAI for Runtime and Memory Management\n\nThe model predicted a runtime of 78.5 seconds for n=43 and 123 seconds for n=44. When we mea- sured the actual runtime using our script, it was 81.08 seconds and 129.07 seconds respectively, demonstrating a close match and correct identification of the maximal capacity:\n\nElapsed time for n=43: 81.08 seconds\n\nElapsed time for n=44: 129.07 seconds\n\nFor your reference, here is the full prompt used in this example:\n\nCONTEXT: You are provided with:\n\n1. Python function enclosed with {{{ FUNCTION }}}\n\n2. Runtime profiling enclosed with {{{ PROFILING }}}.\n\n3. Runtime limit enclosed with {{{ LIMIT }}}\n\nTASK: What is the maximal input the function can run in the time limit?\n\nFUNCTION: {{{\n\ndef fibonacci_recursive(n):\n\nif n <= 0:\n\nreturn 0\n\nif n == 1:\n\nreturn 1\n\nreturn fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2)\n\n}}}\n\nPROFILING: {{{\n\nRuntime for fibonacci_recursive(10): 0.0000 seconds\n\nRuntime for fibonacci_recursive(15): 0.0002 seconds\n\nRuntime for fibonacci_recursive(20): 0.0013 seconds\n\nRuntime for fibonacci_recursive(25): 0.0150 seconds\n\nRuntime for fibonacci_recursive(30): 0.1571 seconds\n\nRuntime for fibonacci_recursive(35): 1.7051 seconds\n\nRuntime for fibonacci_recursive(40): 19.5045 seconds\n\n}}}\n\nLIMIT: {{{ 100 seconds }}}\n\nMAXIMAL INPUT:\n\nIn the upcoming chapters, we will explore how to intentionally collect data for integration with OpenAI API as part of SDLC automation.",
      "content_length": 1449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "Chapter 14\n\nDetermining maximal capacity within memory constraints A similar prompt structure to the one used for identifying maximum capacities under runtime constraints can also be applied to memory constraints:\n\nCONTEXT: You are provided with:\n\n1. Python function enclosed with {{{ FUNCTION }}}\n\n2. RAM usage profiling enclosed with {{{ PROFILING }}}.\n\n3. RAM usage limit enclosed with {{{ LIMIT }}}\n\nTASK: What is the maximal input the function can run in the RAM limit?\n\nFUNCTION: {{{ ... }}}\n\nPROFILING: {{{ ... }}}\n\nLIMIT: {{{ ... }}}\n\nMAXIMAL ROWS / MAXIMAL COLUMNS:\n\nHere, the cue in the prompt is adjusted slightly to MAXIMAL ROWS / MAXIMAL COLUMNS, to empha- size the 2D dimensions of the input.\n\nFor instance, when using this template for the get_top_video function in ChatGPT with the GPT-4o1 model, along with recorded results from sparse matrices of sizes 100 x 100, 1,000 x 1,000, and 10,000 x 10,000, the model generated the following reasoning steps:\n\n1. Profiling Analysis: Included an understanding of the memory usage data and its scaling\n\nbehavior.\n\n2. Conclusion: Estimated the largest input for a memory limit of 5GB.\n\n3. Final Recommendation: Suggested a matrix size of 22,000 x 22,000 as the largest fea-\n\nsible input.\n\nThese reasoning steps result in the final answer, the final recommendation step, as illustrated in Figure 14.8:\n\n349",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "350\n\nGenAI for Runtime and Memory Management\n\nFigure 14.8: GPT-4o1-mini reasoning for maximal capacity\n\nWhen executing the script with a file containing a 22,000 x 22,000 matrix, the recorded memory usage peak was 4.63 GB, closely aligning with the model’s prediction.\n\nFigure 14.9: Memory usage below 5GB\n\nThis memory usage aligns with ChatGPT’s capacity assessment and is below the required 5GB RAM limit.\n\nFor reference, here is the complete prompt used to generate this output:\n\nCONTEXT: You are provided with:\n\n1. Python function enclosed with {{{ FUNCTION }}}\n\n2. RAM usage profiling enclosed with {{{ PROFILING }}}.\n\n3. RAM usage limit enclosed with {{{ LIMIT }}}\n\nTASK: What is the maximal input the function can run in the RAM limit?\n\nFUNCTION: {{{ def get_top_video(path):\n\ninteractions = pd.read_csv(path)\n\navg_ratio = interactions.mean(axis=0, skipna=True)",
      "content_length": 868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "Chapter 14\n\nreturn avg_ratio.idxmax() }}}\n\nPROFILING: {{{...}}}\n\nLIMIT: {{{ 5 GB }}}\n\nMAXIMAL ROWS / MAXIMAL COLUMNS:\n\nThe profiling information was omitted due to brevity constraints and includes a direct copy-paste of the terminal output, as shown in Figure 14.6. The full prompt can be found at ch14/code_samples/ chatgpt_maximum_input_prompt.txt. This prompt successfully predicted a maximum input size of 22,000 rows and 22,000 columns.\n\nIn both these cases, ChatGPT is certainly extrapolating the input values to predict the runtime or maximum dimensions. It requires sufficient input values to make such a prediction and may not be as accurate as performing your own calculations, but it is a reasonable approximation.\n\nNext in this chapter, we will explore how to use GenAI and increase the capacity of the functions such as algorithms with better runtime complexity and dealing with chunks.\n\nOptimizing code with chained prompts The previous examples depict the simple coding and estimation capability of LLMs. However, LLMs can do much more. They can create code for improved algorithms and even balance specific runtime and memory efficiency goals.\n\nTo effectively increase the input capacities, we will chain our optimization prompt to the results of the maximal capacity prompt. The initial prompt already provides essential details, such as the function, runtime or memory limits, and profiling various inputs. The optimization prompt should also include the following key information:\n\nOptimization goal: Clearly specify the primary goal, either on improving runtime com- plexity, reducing memory usage, or both. Detail the desired approach as much as possible, such as batching, or improving algorithm complexity.\n\n\n\nFunction docstring: Provide a concise description of the function’s purpose, along with any constraints or requirements.\n\n\n\nInput requirements: Define the new input goals under the runtime or memory constraints, such as computing the 1,000,000th Fibonacci number or handling a matrix with 100,000 rows and 100,000 columns.\n\n351",
      "content_length": 2060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "352\n\nGenAI for Runtime and Memory Management\n\nFor more complex scenarios, additional details may be helpful, such as previous optimization attempts, dependencies on external functions or resources, frequency of function calls or con- current usage patterns, and acceptable trade-offs, such as allowing longer runtimes in exchange for reduced memory usage.\n\nThis information will be integrated into a chained prompt, following the five S’s for precision and clarity.\n\nRuntime optimization Let us examine a runtime optimization for the fibonacci_recursive function. Below is an exam- ple of a chained prompt, building on an initial prompt of maximal capacity as presented earlier in the chapter. This prompt requests an improved implementation with better runtime complexity to handle an input of 1,000,000:\n\nCONTEXT: You are provided with the function's docstring enclosed with {{{ DOCSTRING }}} and the required input enclosed with {{{ REQUIRED_INPUT }}}.\n\nTASK: Optimize the function to run in a better time complexity within the runtime limit.\n\nDOCSTRING: {{{ \"\"\"\n\nCalculate the nth Fibonacci number using a recursive approach.\n\nArgs:\n\nn (int): The position in the Fibonacci sequence.\n\nReturns:\n\nint: The nth Fibonacci number.\n\n\"\"\"\n\n}}}\n\nREQUIRED_INPUT = {{{ 1,000,000 }}}\n\nIMPROVED IMPLEMENTATION:\n\nThis prompt includes a docstring describing out specific task of optimizing runtime complexity to achieve the target input of 1,000,000. Providing different constraints, such as a smaller input size, will yield a different output from the model.\n\nA typical approach for Fibonacci calculations has a ed a more efficient implementation for large inputs, achieving a runtime complexity of through the fast-doubling recursive approach:\n\n complexity, however GPT-4o1 suggest- \n\n𝑂𝑂(𝑛𝑛)\n\n𝑂𝑂(log𝑛𝑛)",
      "content_length": 1794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "Chapter 14\n\n353\n\nFigure 14.10: Fast-doubling approach for handling large inputs\n\nAnd here is the code proposed by GPT-4o1:\n\ndef fib_pair(n):\n\nif n == 0:\n\nreturn (0, 1)\n\na, b = fib_pair(n // 2)\n\nc = a * ((2 * b) - a)\n\nd = a * a + b * b\n\nif n & 1:\n\nreturn (d, c + d)\n\nelse:\n\nreturn (c, d)\n\nWhile this implementation also uses recursion, it makes a call to reducing the number of function invocations to approximately here are performance comparisons for calculating the 1,000,000th Fibonacci number:\n\n ( without remainder) . To illustrate its efficiency, 𝑛𝑛𝑛 𝑛 𝑛2\n\n𝑛𝑛𝑛 𝑛𝑛⁄\n\nlog(𝑛𝑛)\n\nFast-doubling approach: 0.057893 seconds\n\nIterative (memoization) approach: 9.175633 seconds\n\nAs another benchmark, computing the 10,000,000th Fibonacci number with fibonacci_fast_ doubling took 2.26 seconds, whereas the iterative approach did not complete within a 10-minute time limit.",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "354\n\nGenAI for Runtime and Memory Management\n\nThis method leverages the mathematical identities:\n\n\n\n𝐹𝐹(2𝑘𝑘)  =  𝐹𝐹(𝑘𝑘)  ⋅ (2𝐹𝐹(𝑘𝑘 𝑘 𝑘) − 𝐹𝐹(𝑘𝑘)) \n\n2\n\n2\n\nWhere\n\n𝐹𝐹(2𝑘𝑘 𝑘 𝑘)  =  𝐹𝐹(𝑘𝑘 𝑘 𝑘)\n\n is the k-th Fibonacci number.\n\n𝑘𝐹𝐹 (𝑘𝑘)\n\nUsing these identities, once  can be de- termined efficiently. This means we do not need to compute all Fibonacci numbers sequentially up to the 1,000,000th number but only focus on its halves, significantly reducing the required computations.\n\n𝐹𝐹(𝑘𝑘)\n\n and\n\n are calculated,\n\n and\n\n𝐹𝐹(𝑘𝑘)\n\n𝐹𝐹(𝑘𝑘 𝑘 𝑘)\n\n𝐹𝐹(2𝑘𝑘)\n\n𝐹𝐹(2𝑘𝑘 𝑘 𝑘)\n\nSpace optimization Like the runtime optimization prompt, we can chain a request to optimize a function to fit within RAM constraints. The prompt structure remains similar, but it describes a memory limit instead of a runtime limit, along with an optimization request to address memory usage.\n\nFor example, consider the following chained prompt requesting an optimization for the get_top_ video function to handle a matrix of 100,000 rows by 100,000 columns within a 5GB RAM limit:\n\nCONTEXT: You are provided with the function's docstring enclosed with {{{ DOCSTRING }}} and the required input enclosed with {{{ REQUIRED_INPUT }}}.\n\nTASK: Optimize the function to run in batches within the RAM limit.\n\nDOCSTRING: {{{ \"\"\"\n\nReads a CSV file containing video interactions, calculates the average watch ratio for each video,\n\nand returns the video with the highest average watch ratio.\n\nArgs:\n\npath (str): The file path to the CSV file containing video interactions.\n\nReturns:\n\nstr: The identifier of the video with the highest average watch ratio.\n\n\"\"\"\n\n}}}",
      "content_length": 1615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "Chapter 14\n\nREQUIRED_INPUT = {{{ 100,000 rows x 100,000 columns matrix}}}\n\nIMPROVED IMPLEMENTATION:\n\nThis prompt specifies the size of the input matrix (100,000 rows by 100,000 columns) and builds upon the previously defined RAM limit of 5GB.\n\nFigure 14.11: ChatGPT suggestion for chunk-based processing\n\nBased on this information, ChatGPT suggested processing the CSV file in batches of 1,000 rows, which is an appropriate solution for handling data of this size efficiently.\n\nIn this scenario, specifying the input requirements is vital. For larger matrices, working with a Pandas DataFrame may become impractical, necessitating an alternative solution. Conversely, for matrices with fewer columns, larger column chunks might be sufficient.\n\nFor your reference, here is the implementation for processing the matrix in chunks provided by ChatGPT with the prompt above:\n\ndef get_top_video(path):\n\ncumulative_sum = None\n\ncumulative_count = None\n\nchunksize = 1000\n\nfor chunk in pd.read_csv(path, chunksize=chunksize):\n\nchunk_sum = chunk.sum(skipna=True)\n\nchunk_count = chunk.count()\n\n355",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "356\n\nGenAI for Runtime and Memory Management\n\nif cumulative_sum is None:\n\ncumulative_sum = chunk_sum\n\ncumulative_count = chunk_count\n\nelse:\n\ncumulative_sum += chunk_sum\n\ncumulative_count += chunk_count\n\naverage_ratio = cumulative_sum / cumulative_count\n\ntop_video = average_ratio.idxmax()\n\nreturn top_video\n\nWhen profiling the implementation with an input of 100,000 rows and 100,000 columns, the chunked implementation concluded with a memory usage of 4,326.1 GB:\n\nFigure 14.12: Improved memory utilization with chunking\n\nIn line 13, we can see that the matrix was processed in 100 chunks, with each chunk containing 1,000 columns.",
      "content_length": 632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "Chapter 14\n\nThe proposed solutions can be further refined to meet both memory and runtime requirements. It is always advisable to test new implementations against both criteria to ensure the solution is satisfactory in terms of both efficiency and performance.\n\nFurther runtime and space optimization The previous optimizations focused on single CPU algorithms. GenAI can go beyond these and recommend additional techniques. For instance, it may be appropriate to use parallelization with threads, leverage graphical processing units (GPUs), or adopt more efficient file formats like sparse matrices or Apache Parquet. In the function get_top_video, processing different videos can be performed in parallel by integrating threading. Further acceleration could be achieved by leveraging GPUs, as each column can be processed independently of others. Alternatively, converting the file format from CSV to a more efficient option could enhance performance. For- mats like compressed sparse row (CSR) or compressed sparse column (CSC) from SciPy are well-suited for sparse data, while columnar storage formats like Apache Parquet are ideal for handling large datasets.\n\nSummary In this chapter, we explored how LLMs can assist in ensuring efficient applications by profiling runtime and memory usage, identifying maximal capacities, and suggesting optimized code to increase those capacities.\n\nUsing the recursive Fibonacci example, we saw how adopting a more efficient algorithm sig- nificantly reduces runtime. With the get_top_video function, we tackled large matrices under RAM constraints, assisting GenAI to optimize memory usage through chunking. GitHub Copilot assisted in profiling runtime and memory consumption and profiling runs across different in- puts. ChatGPT estimated the maximal capacity within runtime and RAM constraints. Leveraging chained prompts, ChatGPT demonstrated the ability to vastly improve the implementation to achieve larger capacities and can do much more.\n\nIn the next chapter, we will further explore how to integrate GenAI into the SDLC, focusing on logging, monitoring applications, and error handling.\n\n357",
      "content_length": 2142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "358\n\nGenAI for Runtime and Memory Management\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: How can Copilot assist in profiling the runtime and memory usage of a program?\n\nAnswer: Profiling runtime and memory usage in Python can be achieved using the built-in time module and the third-party memory-profiler package. Copilot can assist by either completing a starter code or generating that functionality from scratch based on a precise prompt.\n\nQuestion 2: How can ChatGPT help identify maximal inputs under runtime or RAM constraints?\n\nAnswer: By crafting precise prompts that include the function’s source code, runtime or RAM constraints, and profiling results, we can utilize the GPT-4o1 model, designed for chain-of-thought reasoning, to calculate the largest input manageable within those constraints.\n\nQuestion 3: How can chaining prompts help increase program input capacity?\n\nAnswer: After identifying the maximal input a function can handle without becoming a bottle- neck, a chained prompt can include the desired input size and a specific optimization request. GPT-4o1 can leverage this information to propose an optimized solution that follows the runtime and/or memory constraints.\n\nFurther reading To learn more about the topics that were covered in this chapter, look at the following resources:\n\nWikipedia, CPU: https://en.wikipedia.org/wiki/Central_processing_unit\n\nGeeks for geeks memory consumption: https://www.geeksforgeeks.org/how-to-get-\n\ncurrent-cpu-and-ram-usage-in-python\n\n\n\nIntroduction to memory profiling in Python: https://www.datacamp.com/tutorial/ memory-profiling-python\n\n\n\nBig-o Python package: https://pypi.org/project/big-O-calculator/\n\n\n\nLambda Labs on GPT-3 training costs: https://lambda.ai/blog/demystifying-gpt-3",
      "content_length": 1839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "Chapter 14\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.\n\n359",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "15\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nLarge Language Models (LLMs) can supercharge our going-live processes, which refers to de- ploying code to production. But if prompted incorrectly, they can also degrade code quality. In this chapter, we will examine how, even when following the five S’s to prompt GenAI for logging, monitoring, and input validation, the resulting code can become cluttered, combining these elements with core functionality and violating the single responsibility principle. For instance, models sometimes suggest using print statements instead of proper logging, handling input validation within functions, or incrementing counters without reporting their values. This vio- lates the single responsibility principle and reduces overall code quality.\n\nTo address this, we will apply the Chain-of-Thought (CoT) prompt engineering technique to GitHub Copilot to generate well-monitored code without interfering with the function’s core functionality, leveraging decorators as a design pattern. We will also leverage few-shot learning to guide it toward following a coding style. While GitHub Copilot is well suited for these tasks, we will explore how similar results can be achieved using CoT with few-shot learning in ChatGPT and CoT with fine-tuning in OpenAI API.\n\nThe key topics covered in this chapter include the following:\n\n\n\nIntroducing logging, monitoring, and raising errors\n\n\n\nLeveraging GenAI for higher-level coding patterns\n\n\n\nApplying inverse CoT with ChatGPT and OpenAI\n\nUtilizing few-shot learning and fine-tuning as style guides",
      "content_length": 1589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "362\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nTechnical requirements To get the most out of this chapter, ensure you have the following:\n\n\n\nA GitHub Copilot account\n\n\n\nAn integrated development environment (IDE) – either VS Code or PyCharm\n\n\n\nAn OpenAI account with access to ChatGPT and OpenAI API\n\n\n\nAccess to the book’s repository, available at https://github.com/PacktPublishing/ Supercharged-Coding-with-Gen-AI\n\n\n\nA virtual environment set up in VS Code or PyCharm\n\n\n\nAn OpenAI API token\n\nFor assistance with setting up a GitHub Copilot account, refer to Chapter 3. For instructions on setting up OpenAI API access and token generation, see Chapter 2. If you need help with creating an OpenAI account or setting up a virtual environment in your IDE, refer to the Appendix for detailed guidance.\n\nIntroducing logging, monitoring, and raising errors When Python software goes live in the production stage and serves clients, it is important to verify that it is consistently working as expected in real-world use cases. If some problems are identified, understanding the behavior of these issues is critical. Thus, logging, monitoring, and raising errors for problematic inputs are an essential aspect of the Software Development Life Cycle (SDLC). They involve tracking key checkpoints within internal processes, which are usually performed by loggers, and these facilitate debugging and other analyses. Recording performance statistics such as counting function calls is often used with profiling to improve performance. Lastly, han- dling unexpected scenarios, such as receiving arguments of an invalid type, is a process to ensure that the code is robust against the wide range of user inputs. These three aspects help maintain software quality, minimize unexpected issues, and simplify debugging when problems occur.\n\nIn this chapter, we will focus on these three fundamental practices since these are typically the first going-live initiatives in any project, and they are likely to be relevant to your software as well. As your strategy expands to additional metrics and error-handling techniques, these standard approaches using GenAI tools can easily be generalized and extended from what we introduce in this chapter.",
      "content_length": 2245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "Chapter 15\n\nThe FizzBuzz use case To explore how monitoring efforts can be integrated into our code, let us consider developing a program that generates the FizzBuzz sequence, a coding challenge introduced in Chapter 2. As part of the monitoring efforts for this program, we want to log each function call along with its arguments, maintain a counter of total function calls, and ensure inputs are positive integers less than 500.\n\nAs a starting point, a basic implementation of FizzBuzz can be generated from the function sig- nature:\n\ndef print_fizzbuzz(limit: int) -> None:\n\nFigure 15.1 shows an implementation suggested by GitHub Copilot in PyCharm:\n\nFigure 15.1: Copilot’s initial FizzBuzz implementation\n\nThis implementation serves as a foundation for incorporating going-live features. Before intro- ducing best practices for monitoring with prompt engineering techniques, we will first explore how simply chaining logging, monitoring, and error-raising requests can degrade code quality. Adding GenAI implementation for these tasks results in a longer and more complex function. However, techniques such as CoT and few-shot learning can help add these functionalities yet maintain clean, well-structured code.\n\n363",
      "content_length": 1222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "364\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nLogging Logs are generated text records that serve as a diary of our program’s execution. For example, at the start of the FizzBuzz sequence, we may want to log the call’s arguments. While using a simple print statement is an option, the built-in logging library is a much better choice as it can write to the console but also provides more control over the output and where the logs can be written, such as files and remote servers. It enables the structured formatting of records and log-level management, which can report all outputs for debugging, or a subset of warnings based on importance.\n\nTo use the logging library, we typically define a logger instance with the module’s name:\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nThis creates a logger instance linked to the module name using the __name__ special attribute (also referred to as a magic attribute). This association allows better filtering and customization of log messages in that file.\n\nWhen using GenAI for logging, it is essential to separate the logging task into its own prompt, rather than embedding it within the implementation prompt. This approach aligns with the single-task principle, one of the five S’s best practices introduced earlier in the book.\n\nFor example, when working with Copilot in VS Code’s edits window, we can select the print_ fizzbuzz implementation and use the following prompt:\n\n#selection log the function arguments\n\nFor your reference, here is the suggestion we got when using the 3o-mini model as appears in Figure 15.2:",
      "content_length": 1595,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "Chapter 15\n\nFigure 15.2: Log suggestion for FizzBuzz\n\nIn PyCharm, we can leverage the inline chat after selecting the function’s code, and prompt the following to yield similar results:\n\nLog the function arguments\n\nWhile the generated log statement correctly logs the function’s argument, it also introduces two lines of unnecessary code clutter within the function, which should remain focused on printing the FizzBuzz sequence. Later in the chapter, we will explore how to maintain cleaner code with prompt engineering techniques.\n\nHandling unexpected input When our program receives unexpected inputs at runtime, such as incorrect argument types (for example, float instead of int) or values that are beyond a manageable range for our program, we want to control the program’s behavior by raising errors rather than failing or producing unpredictable results.\n\nFor instance, in the print_fizzbuzz function, the limit argument is expected to be a positive integer so that we can print numbers, fizz, or buzz from 0 up to that value. An additional con- straint might be that the input should be less than 500 to ensure the output fits on the screen. To enforce both requirements with error handling, we can leverage Copilot.\n\n365",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "366\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nWhen using Copilot in VS Code’s edit window, we can select the print_fizzbuzz code and use the following prompt:\n\n#selection validate limit type, and that it is in the range [0,500]\n\nIn PyCharm, we can use the inline chat by selecting the print_fizzbuzz function and entering the following prompt:\n\nvalidate limit type, and that it is in the range [0,500]\n\nFor reference, here is the result obtained using Copilot for VS Code with the o3 mini model:\n\nFigure 15.3: VS Code output correctly handles inputs but creates code clutter\n\nThis recommendation verifies both the argument’s type and its range limits, as requested. How- ever, like the logging prompt, it introduces an additional responsibility of validating inputs to the function. Later in this chapter, we will explore how prompt engineering can help generate higher-quality code.\n\nMonitoring Another key aspect of going live is tracking the usage of different functions. By recording metrics, we can collect valuable statistics on performance, call frequency, and execution rates.\n\nA simple starting point is to track the number of function calls in our program. To do this with print_fizzbuzz, we can define a global counter variable, FIZZBUZZ_COUNTER = 0, and instruct Copilot to increment it before or after each successful execution. For example, when working with Copilot in VS Code’s edits window, we can select the print_fizzbuzz implementation and",
      "content_length": 1474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "Chapter 15\n\nuse the following prompt:\n\n#selection increment FIZZBUZZ_COUNTER after a successful execution.\n\nSimilarly, in PyCharm, we can select the code and use the inline chat to prompt the following:\n\nIncrement FIZZBUZZ_COUNTER after a successful execution.\n\nFigure 15.4 illustrates a suggested completion for this prompt when using Copilot in VS Code with the o3 mini model:\n\nFigure 15.4: VS Code adds a record usage counter\n\nAt this point, our print_fizzbuzz function integrates multiple monitoring components: logging, handling invalid inputs, and counting calls. While these fulfill our monitoring needs, they lead to a lengthy and less readable function. This violates clean coding principles, particularly the single responsibility principle, which emphasizes that functions should focus on a single task.\n\nCode created by GenAI using simple prompts tends to generate code and add it within existing methods which can lead to cluttered implementations. To maintain clean and structured code, we can leverage CoT prompting. This approach enables us to handle monitoring at a higher ab- straction level, as we will explore next.\n\n367",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "368\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nLeveraging GenAI for higher-level coding patterns To balance clean code principles, such as the single responsibility principle, with the need for going-live requirements, a Pythonic approach is to use higher-level coding patterns such as dec- orators. Rather than embedding logging statements within a function, we delegate this respon- sibility to a decorator pattern:\n\ndef log_function_args(func: callable):\n\n# logging logic here\n\n@log_function_args\n\ndef print_fizzbuzz(limit: int) -> None:\n\n# main functionality here\n\nHere, log_function_args is responsible for logging function calls, allowing print_fizzbuzz to focus on handling the FizzBuzz sequence. This approach ensures cleaner, more modular code while enabling logging for multiple functions.\n\nGenAI applications may not suggest this pattern unless we actively steer them toward clean coding practices. As introduced in Chapter 9, CoT prompting helps LLMs reason through complex tasks. We can direct the model toward better design choices by using CoT prompting and explicitly writing a decorator call, such as @log_function_args, prior to implementing the actual decorator function. This will result in cleaner and more maintainable implementations.\n\nUsing decorators to separate responsibilities A decorator is a structural design pattern that enables stacking tasks before and after a function call without altering the function’s code. It is a specialized use case of a closure, where a function retains access to its associated arguments. A typical decorator implementation looks like this:\n\ndef sample_decorator(func: callable) -> callable:\n\ndef wrapper(*args, **kwargs):\n\nprint(\"Function is wrapped\")\n\nreturn func(*args, **kwargs)\n\nreturn wrapper\n\nA common usage involves applying the @ annotation, as shown here:\n\n@sample_decorator\n\ndef foo(*num):\n\nreturn len(nums)",
      "content_length": 1894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "Chapter 15\n\nHere, the sample_decorator decorator accepts a function object and returns a wrapper function that takes the same arguments as the original function but prints Function is wrapped before execution.\n\nLLMs can certainly include decorators in their generated code as their training data contains many common decorator implementations. For well-known use cases such as timers, loggers, argument validation, and retry mechanisms, the models can generate decent implementations based on the decorator’s signature when prompted to.\n\nNext, we will explore how to apply inverse CoT prompting to generate the desired implementation.\n\nInverse CoT for decorator implementation In Chapter 9, we explored how defining a function that calls low-level implementations provides strong guidance for GitHub Copilot. With inverse CoT, we take a similar approach by referencing functions that have not yet been defined. We use the term inverse because we want the decorators to appear earlier in the file than the function that uses them. To achieve this, we place the cursor higher in the file, allowing Copilot to generate code above an existing implementation.\n\nAs a review, here is an example of CoT prompting used in Chapter 9 to implement a variation of a geometric mean calculator, where Copilot successfully generated the missing low-level functions:\n\nFigure 15.5: CoT-driven implementation of a higher-level function with GitHub Copilot\n\nIn this approach, the function is broken into named functions that are intentionally called undefined. Copilot understands that its role is to generate these necessary implementations. Such an approach enables Copilot to implement the missing functions, producing structured and predictable code.\n\n369",
      "content_length": 1739,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "370\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nWe can apply the same technique when implementing going-live functionality, by defining a dec- orator structure and allowing Copilot to generate the implementation. By merely adding calls to the required decorators, we guide Copilot to generate their corresponding implementations. For instance, in our FizzBuzz use case, we can layer three monitoring functionalities onto our function:\n\n1. Validate the input: Define a decorator with parameters to enforce input bounds, that is,\n\n@validate_args_types_and_limits(0, 500).\n\n2.\n\nIncrement a global counter:\n\n3. @increment_counter.\n\n4. Log function calls:\n\n5. @log_function_args.\n\nFor reference, here is how stacking these decorators would appear before Copilot generates their implementations:\n\nFigure 15.6: Inverse CoT approach\n\nHere, as usual with the CoT technique, the IDE will correctly mark the decorators as undefined with the red underlines. After calling the decorators, we move the cursor to line 7 before the call to the first decorator, press Enter three times to create space for their implementations, then return to line 8 and type the following:\n\ndef l",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "Chapter 15\n\nThis prompt is sufficient for Copilot to infer that it should generate the first decorator, interpreting l as a reference to log_function_args. Moving the cursor up and pressing Enter three times is all that is involved in inverse CoT, but it is very effective. As shown in Figure 15.7, this cue enables Copilot to generate a complete implementation of a logging mechanism, including the function’s arguments.\n\nFigure 15.7: Copilot’s decorator implementation\n\nThe suggested implementation of log_function_args correctly logs the function name and arguments as an info-level log statement.\n\nAfter the initial def l prompt, pressing the Enter key twice is enough for Copilot to generate the next decorator, increment_counter, as shown in Figure 15.8:\n\n371",
      "content_length": 765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "372\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nFigure 15.8: Enter key as a cue for Copilot to implement the next decorator\n\nThe Enter key serves as a sufficient cue for the next implementation. This highlights how Copi- lot can supercharge our workflow. For your reference, here is the suggested implementation of increment_counter by Copilot:\n\ndef increment_counter(func):\n\ndef wrapper(*args, **kwargs):\n\nglobal FIZZBUZZ_COUNTER\n\nFIZZBUZZ_COUNTER += 1\n\nlogger.info(f\"Function {func.__name__} called {FIZZBUZZ_COUNTER} times\")\n\nreturn func(*args, **kwargs)\n\nreturn wrapper\n\nThis decorator correctly increments the previously initialized global variable, FIZZBUZZ_COUNTER, before the function call. If we prefer to increment it after execution, we could have used a different decorator’s call, such as @increment_counter_after_call.",
      "content_length": 845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "Chapter 15\n\nFinally, Copilot’s suggested implementation for argument validation efficiently manages both type-checking and value constraints using a parameterized decorator. This type of decorator introduces an additional closure layer, allowing us to specify arguments, such as the minimum and maximum limits, in this case.\n\nBy calling @validate_args_types_and_limits(0, 500), we provide a clear cue for Copilot to recognize the need for a parameterized decorator with the given constraints. Here is the suggested implementation:\n\ndef validate_args_types_and_limits(min_limit: int, max_limit: int):\n\ndef decorator(func):\n\ndef wrapper(limit: int):\n\nif not isinstance(limit, int):\n\nraise TypeError(f\"Argument 'limit' must be of type int, got {type(limit)}\")\n\nif limit < min_limit or limit > max_limit:\n\nraise ValueError(f\"Argument 'limit' must be between {min_ limit} and {max_limit}, got {limit}\")\n\nreturn func(limit)\n\nreturn wrapper\n\nreturn decorator\n\nHere, the parameterized decorator, validate_args_types_and_limits, ensures that the argument falls within the specified inclusive range before executing the function.\n\nFor reference, here is an example of calling print_fizzbuzz(-15), which should raise a value error due to the negative input:\n\nFile \"supercharge/ch15/application/fizzbuzz_printer.py\", line 32, in wrapper\n\nValueError: Argument 'limit' must be between 0 and 500, got -15\n\nvalidate_args_types_and_limits raised an error due to a bad input, preventing the function from being called with a negative limit. Similarly, if we had called print_fizbuzz(3.14), we would have received a TypeError error, indicating that it handles integers but not floating-point values.\n\n373",
      "content_length": 1685,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "374\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nTo see how our log_function_args and increment_counter decorators operate, we will add a basic logging configuration at the beginning of the file, capturing INFO-level and higher logs, in a file named fizzbuzz.log:\n\nlogging.basicConfig(level=logging.INFO,\n\nfilename=\"fizzbuzz.log\")\n\nThis ensures that the log messages are stored in fizzbuzz.log. In most software systems, print statements should not be used and should be replaced by logging that can direct output to the console or a file.\n\nNext, we add two function calls with valid inputs:\n\nprint_fizzbuzz(5)\n\nprint_fizzbuzz(50)\n\nFor reference, the fizzbuzz_printer.py output is shown in Figure 15.9:\n\nFigure 15.9: Console output for the two function calls\n\nThis produces a clean output, containing the FizzBuzz results. Meanwhile, the fizzbuzz.log log file records log messages in the specified format, as illustrated in Figure 15.10:",
      "content_length": 949,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "Chapter 15\n\nFigure 15.10: fizzbuzz.log output containing logs and counter messages\n\nThis implementation follows clean code principles, separating core functionality from monitor- ing while leveraging inverse CoT prompting in Copilot to generate neat and maintainable code. However, decorators can be styled in various ways. Later in the chapter, we will explore how few-shot learning can help refine their structure for even better results.\n\nNext, we will apply the inverse CoT technique when prompting ChatGPT and OpenAI API.\n\nApplying inverse CoT with ChatGPT and OpenAI GitHub Copilot is the preferred tool for integrating going-live functionality, as it provides the fastest code completion experience. However, we will also demonstrate how to apply the same CoT prompting technique with ChatGPT and OpenAI API. To do this effectively, we will structure our prompts using the five S’s best practices for prompt precision introduced earlier in this book.\n\nUsing ChatGPT to monitor decorators Similar to our approach with GitHub Copilot, we can leverage ChatGPT to generate the required decorators for monitoring. Since we submit the full code rather than moving the cursor up in the file, this approach aligns with a standard CoT technique. The following is an example of a structured prompt that provides context about the incomplete Python implementation, focusing on a single task, with specific and short instructions, followed by a lead-in cue:\n\nCONTEXT: You are provided with a partial Python script in {{{ FIZZBUZZ_ PRINTER }}}, where some of the code is missing.\n\nTASK: Implement the decorators with the missing implementation in the code.\n\nFIZZBUZZ_PRINTER: {{{\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n375",
      "content_length": 1732,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "376\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nFIZZBUZZ_COUNTER = 0\n\n@log_function_args\n\n@increment_counter\n\n@validate_args_types_and_limits(0, 500)\n\ndef print_fizzbuzz(limit: int) -> None: ... full code implementation omitted for brevity ... }}}\n\nCODE:\n\nWhen used in ChatGPT with the o3-mini high model (available to Plus subscribers as of April 2025), this prompt generated a complete script including the full implementation of the missing decorators, as follows:\n\ndef log_function_args(func: callable) -> callable:\n\ndef wrapper(*args, **kwargs):\n\nlogger.info(\n\nf\"Calling function '{func.__name__}' with args: {args} and kwargs: {kwargs}\"\n\nreturn func(*args, **kwargs)\n\nreturn wrapper\n\ndef increment_counter(func: callable) -> callable:\n\ndef wrapper(*args, **kwargs):\n\nglobal FIZZBUZZ_COUNTER\n\nFIZZBUZZ_COUNTER += 1\n\nlogger.info(f\"FIZZBUZZ_COUNTER incremented to {FIZZBUZZ_COUNTER}\")\n\nreturn func(*args, **kwargs)\n\nreturn wrapper\n\ndef validate_args_types_and_limits(min_limit: int, max_limit: int):\n\ndef decorator(func: callable) -> callable:\n\ndef wrapper(*args, **kwargs):\n\n# Use inspect to bind the arguments to the function's signature\n\nsig = inspect.signature(func)\n\nbound_args = sig.bind(*args, **kwargs)\n\nbound_args.apply_defaults()\n\n# Expecting the argument to be named 'limit'",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "Chapter 15\n\nif 'limit' not in bound_args.arguments:\n\nraise ValueError(\"Missing required argument 'limit'.\")\n\nlimit = bound_args.arguments['limit']\n\nif not isinstance(limit, int):\n\nraise TypeError(\"Argument 'limit' must be an integer.\")\n\nif not (min_limit <= limit <= max_limit):\n\nraise ValueError(f\"Argument 'limit' must be between {min_ limit} and {max_limit}.\")\n\nreturn func(*args, **kwargs)\n\nreturn wrapper\n\nreturn decorator\n\nThis implementation retains the same functionality as the decorators generated by Copilot but differs in style. It includes docstrings and the addition of the @functools.wraps(func) decora- tor, which helps preserve the original function’s attributes. Later in the chapter, we will explore how to guide ChatGPT toward a more desirable implementation style using few-shot learning.\n\nUsing OpenAI API to monitor decorators The approach for using OpenAI API closely mirrors the ChatGPT prompt structure. We construct a system prompt that provides context and a single task, as was done for ChatGPT. Additionally, we use a user prompt that includes the existing script along with a cue for code completion.\n\nThe system prompt should look as follows:\n\nSURROUND = \"You are provided with a partial Python script in {{{ FIZZBUZZ_ PRINTER }}}, where some of the code is missing.\"\n\nSINGLE_TASK = \"Implement the decorators with the missing implementation in the code.\"\n\nThese prompt pieces serve the same role as the context and task definitions in the ChatGPT prompt. The user prompt should incorporate the incomplete implementation, for example, by reading the code from a file, as shown here:\n\ndef get_user_prompt(script_path: str) -> str:\n\nwith open(script_path, 'r') as file:\n\nincomplete_code = file.read()\n\nreturn f\"\"\"\n\n377",
      "content_length": 1747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "378\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nFIZZBUZZ_PRINTER: {{{{{{ {incomplete_code} }}}}}}\n\nCODE:\n\n\"\"\"\n\nBy providing the incomplete script alongside the CODE: cue, we can guide OpenAI API toward generating the missing decorator implementations. Calling OpenAI API with these prompts should yield a decorator implementation like the outputs from ChatGPT and GitHub Copilot. The full script is available for your reference at ch15/inverse_cot_openai.py.\n\nNow that we have seen how to use GenAI tools to implement monitoring decorators, the next step is refining their style to better fit our needs. This can be achieved using few-shot learning to guide Copilot and ChatGPT with structured examples or by fine-tuning a model through OpenAI API GUI. We will explore both approaches in the next section.\n\nUtilizing few-shot learning and fine-tuning as style guides While the decorators generated by GenAI applications provide the desired functionality, we can further refine their style using few-shot learning techniques, as introduced in Chapter 8.\n\nFor instance, we might want to include the @functools.wraps decorator to preserve the original function’s attributes and enforce a specific logging style with the extra parameters. Both objec- tives can be achieved by supplying few-shot examples to Copilot and ChatGPT or adding desired prompt outputs to our fine-tuning file to be used by OpenAI API.\n\nFew-shot decorator style for GitHub Copilot As introduced in Chapter 8, enforcing a specific style with GitHub Copilot requires a few exam- ples, which can be provided as a style guide script. This few-shot example achieves the desired implementation.\n\nTo achieve this, we can create a style_guide_decorator.py file, which should be treated like another non-project file and is typically placed outside the src folder in a project repository.\n\nThis style guide should include all the elements we want Copilot to follow, such as log message formatting, type hints, and spacing. These examples guide Copilot and reduce the need for chained prompts or additional edits.",
      "content_length": 2086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "Chapter 15\n\nWhen creating a style guide for decorators, we suggest including a time_it decorator to log a function’s runtime. The implementation should demonstrate the use of @functools.wraps to retain the decorated function’s attributes, type hints for arguments and variables, and a hanging indent logging style with extra parameters:\n\nimport logging\n\nimport time\n\nfrom functools import wraps\n\nfrom typing import Any\n\nlogger: logging.Logger = logging.getLogger(__name__)\n\ndef time_it(func: callable) -> callable:\n\n@wraps(func)\n\ndef wrapper(*args, **kwargs):\n\nstart_time: float = time.time()\n\nres: Any = func(*args, **kwargs)\n\nend_time: float = time.time()\n\nlogger.info(\n\n\"Function called.\",\n\nextra={\n\n\"function\": func.__name__,\n\n\"args\": args,\n\n\"kwargs\": kwargs,\n\n\"error\": \"\",\n\n\"timing\": f\"{end_time - start_time} sec\"})\n\nreturn res\n\nreturn wrapper\n\nIn this case, the logger message combines static text with the extra parameter, resulting in a structured log message that is better suited for large-scale logging. For instance, to quickly iden- tify all timing messages where execution exceeded five seconds, we could filter logs based on the timing parameter with a condition such as timing > 5.\n\n379",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "380\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nTo incorporate the style guide into the session, we open decorators_style_guide.py, then cut and paste the entire content of the file in place. This trick ensures that Copilot recognizes the code in the file as part of the active session and incorporates its style when making suggestions.\n\nNext, we return to print_fizzbuzz.py and repeat the inverse CoT process. This time, Copilot’s suggestions should align with the style guide defined in style_guide_decorator.py. As shown in Figure 15.11, Copilot includes the @functools.wraps decorator in its suggested implementation, while the logging indentation and extra parameters maintain the same formatting as specified in the style guide:\n\nFigure 15.11: Decorator implementation using the style guide in GitHub Copilot\n\nThis implementation demonstrates that Copilot follows our preferred logging style, using a static log message and extra parameters to support a more structured logging format.\n\nNext, we will explore how to apply the style guide when prompting with ChatGPT. We will then show how to incorporate it into a fine-tuning approach when working with OpenAI API.\n\nFew-shot learning in ChatGPT When using ChatGPT, we can apply a few-shot approach to deploying decorators, like that we explored in Chapter 8. For this task, we recommend the o3-mini high model since it is better at complex coding tasks.",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "Chapter 15\n\nWe first enhance the prompt’s surrounding context by including a reference example:\n\nCONTEXT: You are provided with a partial Python script enclosed with {{{FIZZBUZZ_PRINTER}}} where some of the code is missing, and examples of a good implementation enclosed with {{{ EXAMPLES }}}.\n\nTASK: Implement the decorators with the missing implementation in the code while following the style guide.\n\nFor the few-shot example, we will label the input and output code snippets, with INCOMPLETE_CODE used for the input, indicating the missing time_it decorator, and COMPLETE_CODE for the fully implemented script. This distinction clearly highlights the intended transformation:\n\nINCOMPLETE_CODE: {{{\n\nimport logging\n\nlogger: logging.Logger = logging.getLogger(__name__)\n\n@time_it\n\ndef my_func(a: int, b: int) -> int:\n\nreturn a + b\n\n}}}\n\nWith this incomplete code snippet, we expect the model to generate a fully implemented decorator that follows a specific style. To guide the model, we will provide a fully implemented version of the desired code, as shown here:\n\nCOMPLETE_CODE: {{{\n\nimport logging\n\nimport time\n\nfrom functools import wraps\n\nfrom typing import Any\n\nlogger: logging.Logger = logging.getLogger(__name__)\n\ndef time_it(func: callable) -> callable:\n\n@wraps(func)\n\n...omitted for brevity but using \"extra\" parameter...\n\n381",
      "content_length": 1338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "382\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\n@time_it\n\ndef my_func(a: int, b: int) -> int:\n\nreturn a + b\n\n}}}\n\nProviding this example of the desired output helps guide ChatGPT toward generating the expected implementation. The following is the structure of the prompt, with previously introduced parts omitted for brevity:\n\nCONTEXT: You are provided with a partial Python script enclosed with {{{FIZZBUZZ_PRINTER}}} where some of the code is missing, and examples of a good implementation enclosed with {{{ EXAMPLES }}}\n\nTASK: Implement the decorators with the missing implementation in the code while following the style guide.\n\nEXAMPLES:\n\nINCOMPLETE_CODE: {{{...omitted for brevity...}}}\n\nCOMPLETE_CODE: {{{...omitted for brevity...}}}\n\nFIZZBUZZ_PRINTER: {{{...omitted for brevity...}}}\n\nCODE:\n\nThe full prompt is available in the book’s repository at ch15/decorators_few_shot_chatgpt.txt. For reference, the results generated using this prompt closely resemble Copilot’s code completion, as shown in Figure 15.12:\n\nFigure 15.12: Few-shot learning using ChatGPT",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "Chapter 15\n\nIn ChatGPT’s implementation, we can see that the model followed the style guide from our exam- ple, incorporating the extra parameter alongside the static log message and using the @functools. wraps decorator to preserve the function’s attributes. However, this is a relatively lengthy prompt to achieve a code completion that GitHub Copilot can generate much faster. Therefore, we rec- ommend using Copilot whenever possible for a more efficient workflow.\n\nLeveraging fine-tuning for OpenAI API With OpenAI, we can either apply the few-shot technique, as in the ChatGPT input-output exam- ples, or train a fine-tuned model by adding a few training samples in JSONL format, as introduced in Chapter 11. This approach would take even longer than ChatGPT to achieve what GitHub Co- pilot can generate almost instantly. Fine-tuning would be more practical for large-scale projects, such as scanning an entire repository to implement logging, monitoring, and error handling for all files automatically, at scale.\n\nAs we saw in Chapter 11, fine-tuning generally requires more examples than few-shot learning because of the differences in how models handle prompts versus how they update weights during fine-tuning. However, considering our starter fine-tuning file, fine_tuning.jsonl, from Chapter 11, which has already established the preferred logging style, indentation, and spacing, a single decorator may be sufficient to achieve the desired implementation.\n\nFor example, here is a time_it decorator added to ch13/fine_tuning.jsonl, structured with a system and user prompt as used in ChatGPT, along with a desirable assistant response assigned a weight of 1:\n\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are provided with a partial Python script enclosed with {{{ INCOMPLETE_CODE }}} where some of the code is missing. Your task is to implement the decorators with the missing implementation\"}, {\"role\": \"user\", \"content\": \"INCOMPLETE_CODE: {{{import logging\\nimport time\\n\\nlogger: logging.Logger = logging. getLogger(__name__)\\n\\n@time_it\\ndef my_func(a: int, b: int) -> int:\\n return a + b\\n}}}\\n COMPLETE_CODE:\"}, {\"role\": \"assistant\", \"content\": \"...omitted for brevity...\", \"weight\": 1}]}\n\n383",
      "content_length": 2217,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "384\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nWe can then upload the fine-tuning file by going to platform.openai.com, selecting Dashboard | Fine-tune, and uploading the JSONL file, as shown in Figure 15.13:\n\nFigure 15.13: Upload the fine-tuned JSONL file\n\nAfter the upload is complete, the fine-tuned model will be trained, which takes around 15 minutes. When done, a Succeeded icon will appear, as shown in Figure 15.14:",
      "content_length": 437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "Chapter 15\n\nFigure 15.14: Successful fine-tuning job indicates the model has finished training\n\nTo compare the benefits of fine-tuning against the base GPT-4o mini model, we can use the Playground page. This allows us to compare the fine-tuned model’s output against the baseline results, highlighting improvements. It is always recommended to validate these enhancements on key prompts once the fine-tuning process is complete.\n\nWe will select gpt-4o-mini-2024-07-18 as the base model on the left and choose the fine-tuned model from the drop-down menu on the right, as shown in Figure 15.15:\n\nFigure 15.15: Base model and fine-tuned model comparison\n\n385",
      "content_length": 656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "386\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nThis comparison allows us to assess the performance of both models. Next, we add the system instructions on both sides of the screen, mirroring the surrounding context and single-task ap- proach used in ChatGPT:\n\nYou are provided with a partial Python script in {{{FIZZBUZZ_PRINTER}}}, where some of the code is missing. Your task is to implement the decorators with the missing implementation in the code.\n\nThe user message follows a format like our ChatGPT prompt:\n\nFIZZBUZZ_PRINTER: {{{ ...omitted for brevity... }}}\n\nCODE:\n\nAt this stage, our prompt is ready to run, producing more precise results when using the fine- tuned model. Compared to gpt-4o-mini-2024-07-18, the fine-tuned model returns only the code with our preferred logging style, and including the @functools.wraps decorator, as shown in Figure 15.16:\n\nFigure 15.16: Comparison of the fine-tuned model and the base model\n\nIn this figure, we can see that the assistant’s response begins directly with the code, reflecting the influence of the fine-tuned examples. Additionally, the preferred logging style is consistently applied, aligning with the provided sample.",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "Chapter 15\n\nWhile fine-tuning helps maintain the desired structure and style, it requires significantly more effort than using Copilot for going-live tasks related to code completion. For this reason, we rec- ommend Copilot as your primary tool for such tasks. Fine-tuning may be a viable option if you’re building a custom GenAI application for internal use and have dozens of training examples, such as scanning an entire repository for suggestions or developing a GenAI-powered solution to serve other customers. However, Copilot with few-shot learning tends to be a more efficient method of achieving results without having to invest time in finding many use cases.\n\nSummary In this chapter, we explored how going-live tasks, such as adding logs, monitoring usage, and handling bad input, can introduce unnecessary complexity to functions. GenAI does not inherently suggest using decorators to separate these objectives from the core functionality of the program, which can lead to lower-quality code.\n\nThe most effective way to leverage GenAI for these tasks is through CoT prompting with GitHub Copilot. This approach involves first implementing the core function and then specifying addi- tional requirements for decorators to handle logging, metric recording, or argument validation.\n\nTo ensure that the decorator implementation follows our desired style, we can use few-shot learning with Copilot and ChatGPT or fine-tune a model for OpenAI API. This allows us to achieve more desirable results, minimizing the requirements for chained edits to the code.\n\nIn the final chapter of the book, we will summarize the key takeaways from the 15 chapters and explore how to enhance our coding efforts with GenAI tools.\n\nQuiz time Before you proceed to the next chapter, make sure that you can confidently answer the following questions:\n\nQuestion 1: Do LLMs guarantee higher coding standards?\n\nAnswer: No, not necessarily. Simply asking GenAI applications to add monitoring responsibilities can lead to undesirable suggestions, such as merging monitoring tasks with core functionality instead of properly separating responsibilities.\n\n387",
      "content_length": 2139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "388\n\nGoing Live with GenAI: Logging, Monitoring, and Errors\n\nQuestion 2: What is the inverse CoT for Copilot?\n\nAnswer: It involves defining higher-level coding structures, such as decorators, first and then moving the cursor up to let Copilot complete the implementation details.\n\nQuestion 3: How can we guide GenAI applications to follow a style guide that aligns with our internal standards?\n\nAnswer: We can use a style guide file for Copilot, provide a few examples when prompting ChatGPT, and apply fine-tuning when working with OpenAI API.\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nReal Python on decorators: https://realpython.com/primer-on-python-decorators/\n\n\n\nPython’s logging documentation: https://docs.python.org/3/howto/logging.html\n\nMetrics of code, Wikipedia: http://en.wikipedia.org/wiki/Software_metric\n\n\n\nException handling, Wikipedia: https://en.wikipedia.org/wiki/Exception_handling_ (programming)\n\nUnlock this book’s exclusive benefits now Scan this QR code or go to packtpub.com/ unlock, then search for this book by name.\n\nNote: Keep your purchase invoice ready before you start.",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "16\n\nArchitecture, Design, and the Future\n\nIt seems that every discussion about the future of artificial intelligence (AI) bears considering the famous quote of Niels Bohr: “Prediction is very difficult, especially if it’s about the future.” When it comes to AI, though, we already have ideas about what the future will bring. That is, we have read various science fiction books by Asimov and other authors. We have watched The Terminator and other futuristic movies and TV shows. Even cartoons across different generations embody different versions of the future, whether it’s The Jetsons, Futurama, or various anime. We have conceptions of what the future looks like; the uncertainty rests on how long it will take to get there.\n\nWith the increasing investments in AI and their exponential growth, the fields of AI and GenAI continue to evolve and bring new capabilities faster than most practitioners expect, let alone can keep up with. The landscape of software engineering is changing as a result of the new capa- bilities brought about by AI and GenAI for coding. Before we can look forward and estimate the timeframes for advancements in the future, it is useful to look back and understand the major technological advances that have accelerated the transformation over the past 10 years. This chapter briefly reviews these major advances as a conceptual framework for what happens next.\n\nThis book enables readers to embrace these changes and gain the skills to effectively use them for not just coding but a range of software engineering skills. As we all continue to learn, it is useful to understand the impact of these advances on the industry. These include the impact on software engineers and their companies, as well as what the future might bring.",
      "content_length": 1763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "390\n\nArchitecture, Design, and the Future\n\nIn this chapter, we explore the following topics:\n\n\n\nThe rapid rise of GenAI\n\n\n\nThe economics of faster software development\n\n\n\nThe shifting landscape of software developers\n\nWill GenAI change programming languages?\n\n\n\nThe future of GenAI in software engineering\n\nThe rapid rise of GenAI It was only in late 2012 that Thomas Davenport and DJ Patil declared that data science is the “sexiest job of the 21st century” in their Harvard Business Review article (https://hbr.org/2012/10/ data-scientist-the-sexiest-job-of-the-21st-century). Just over a decade later, the pace of transformation in the industry has been nothing short of remarkable.\n\nIn the same period of the last decade, neural networks were finally making their third resurgence after a couple of AI winters. Although neural network research started back in the 1950s, it faced difficulties in reaching its extremely hyped expectations, which resulted in periods of minimal funding and research known as the “AI winters.” By 2012, new neural network architectures had emerged. AlexNet, a type of convolutional neural network (CNN), drew widespread attention from the computer vision community after winning the ImageNet Large Scale Visual Recogni- tion Challenge (ILSVRC) in 2012 by a wide margin. This changed the course of computer vision as all image understanding work quickly shifted to CNNs. The ensuing research over the next few years led to the ability of CNNs to extract information from images as accurately as humans by 2016, according to the ILSVRC competition, although human accuracy levels are still disputed.\n\nWhile computer vision work was advancing through the CNN architecture, the transformer ar- chitecture was introduced through a 2017 paper from Google. This Attention Is All You Need paper (https://arxiv.org/abs/1706.03762) describes an attention mechanism and the transformer architecture. In the years since, this architecture has had a massive impact on neural networks and the field of AI. Although it has improved solutions for many diverse problems within AI, its primary beneficiary has been the subfield of natural language processing (NLP). The work of this paper led to the adoption of large language models (LLMs).\n\nLLMs have had a profound impact on the public perception of AI, though the early generations of this technology were far less impressive. GPT-1 and GPT-2 were released by OpenAI in 2018 and 2019 and served as a step forward in LLMs. These models are essentially prediction models trained on large volumes of text to predict the next word. The transformer architecture learned the",
      "content_length": 2638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "Chapter 16\n\nrelationship between the words in a sentence to predict the next word. Empirical research found that the LLMs performed significantly well with ever larger models trained on ever more data. GPT-3 was released in 2020 with 175 billion parameters as compared to the 1.5 billion parameters of GPT-2. GPT-4, released in March 2023, scaled up to an estimated 1.76 trillion parameters, a 10-fold increase over GPT-3.\n\nInstructGPT and ChatGPT were developed as new tools that utilize underlying GPT LLMs. In- structGPT was designed to generate text for specific tasks that follow user instructions. These tasks could be emails, summaries, or even code. InstructGPT included AI predictions based on training data as well as a reinforcement learning technique based on human feedback to adjust the next word predictions based on a human feedback mechanism. In contrast, the release of OpenAI’s ChatGPT in November 2022 was based on GPT-3.5 and was arguably the first time an average user could interact with an LLM without having to program. It focused on conversations with users and general dialogue but also included the reinforcement learning method. These two tools developed many of the LLM methods that led to the prompting techniques field.\n\nThe LLM and ChatGPT methods focus on accurately predicting the next words across languages. The same models have opened other research avenues by training on other types of data. Instead of training on text, researchers in biology have found that the same results can be applied to genomes and protein sequence predictions. Chemists are using the technology to create new drugs and chemical substances with specialized properties. This book, of course, is based on the same idea of applying the same prediction techniques to code by training on public code bases.\n\nA few years ago, AI was rarely discussed in the media. Today, the financial news seems to focus on the recent technology advances and their implications for the markets and the larger economy. The next section discusses some of the economic issues that GenAI coding may offer.\n\nEconomics of faster software development Software development is one of the key enablers of technology and the digital economy. Software impacts many aspects of modern society, from governments to businesses. Most industries rely on software. Software is closely tied to the productivity of workers across the globe, making it a key economic measure.\n\n391",
      "content_length": 2451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "392\n\nArchitecture, Design, and the Future\n\nThe field of software economics researches and estimates the value provided by software against the costs associated with its development. Software platforms and tools are designed to provide incremental business/economic value to the organizations using them. The key decisions of buying commercial off-the-shelf (COTS) software, licensing software, customizing third-party software, or building it in-house are based on the estimated incremental value proposition.\n\nWith GenAI, software engineers can write quality code substantially faster. With the techniques described in this book, the efficiency in writing code should improve and the quality of the pro- duced code should remain quite high. The ability to automate the testing and documentation further reduces the costs of software development. In financial terms, the use of GenAI by the same workforce would result in an increased supply of software. The current costs for the GenAI coding empowerment remain $25/month or under, which is arguably a bargain. The reduced cost will increase the demand for software, creating greater automation and adding value for businesses. There are always more problems to solve, and the lower development cost barrier will alter the economics of software engineering. To reap these benefits, software engineers need to begin using this technology, which is discussed in the next section.\n\nWill GenAI be adopted by software engineers? The improved efficiency, speed, and capability that GenAI brings for software engineers is at least a paradigm shift but may be a revolution in the way that software is being developed. As discussed in this book, the current capabilities of GenAI include many of the tasks that software engineers engage in daily. These include writing code and tests, generating documentation, refactoring, and optimizing code. GenAI aids with the cognitively complex tasks of code debugging down to the less mindful tasks of conforming to code style.\n\nGartner predicts that 50% of software engineers will use AI-powered coding tools by 2027, which is up from the 5% in 2023 when GenAI was at the peak of the hype cycle (https://www.gartner. com/en/newsroom/press-releases/2023-11-28-gartner-hype-cycle-shows-ai-practices- and-platform-engineering-will-reach-mainstream-adoption-in-software-engineering-i- n-two-to-five-years). Over the last two years, GenAI tools have rapidly shifted from marginally useful due to their many errors to widespread tools that software engineers and corporations find valuable. In this brief period, KPMG claims that GenAI has become a software engineer’s most valuable coding partner (https://kpmg.com/us/en/articles/2023/generative-artificial- intelligence.html). Over the next few years, these tools will continue to evolve and produce ever higher quality code, test instances, docstrings, and documentation.",
      "content_length": 2902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "Chapter 16\n\nGiven the improved accuracy and overall efficiency that GenAI coding provides, the effect may not be uniform across all ranks of developers. The next section examines how the tools may affect different classes of programmers.\n\nShifting landscape of software developers At the current market pricing, GenAI tools are generally available to all developers. It is interest- ing to speculate how the software engineer landscape may change due to this recent technology. Some have suggested that GenAI tools will level the playing field since any developer will be able to program with increased efficiency in any language. They argue that junior developers may take over more roles and increase the volume of code delivered, and more senior developers will be pushed out of organizations. Others argue the opposite.\n\nThe rise of GenAI tools has also lowered the barrier for software development. Programming in each language has a steep learning curve since it includes syntax, algorithms, libraries, integrated development environments (IDEs), compiler messages, and more. Printing the “hello world” message in any language is usually the first accomplishment. If coding a new language is like reading the front page of a foreign language newspaper, GenAI for coding is a tutor that shows the meaning of all words and explains the grammar upon request. For coding alone, a junior developer may quickly become productive in straightforward coding.\n\nTools such as ChatGPT and GitHub Copilot will empower developers to quickly learn and become productive in new programming languages. There are always new languages and frameworks that all software engineers must continually learn and apply to projects. The implication is that developers can more quickly get up to speed with new languages and frameworks and become productive in far less time. This has implications for not only new languages but also older lan- guages such as COBOL, FORTRAN, and others that are still running in legacy systems, described later in this section.\n\nOur perception is that the landscape will shift toward senior developers, and more junior develop- ers may be displaced by the GenAI tools. If we consider a typical software engineering or computer science undergraduate curriculum, the primary technical courses include calculus, programming, data structures, algorithms, and maybe software engineering. The more advanced courses will include networking, compilers, operating systems, big data, AI, programming language theory, optimizations, cloud computing, and much more.\n\n393",
      "content_length": 2569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "394\n\nArchitecture, Design, and the Future\n\nJunior developers tend to focus more on writing code, tests, and documentation, while the more senior developers focus on the complex interplay between the system requirements, stakeholders, architecture, design, and other aspects that tend to require more experience. The GenAI efficiencies clearly overlap the skillsets of the junior developers, but not the senior developers.\n\nGenAI as related to the Software Engineering Body of Knowledge Another viewpoint is that GenAI performs very well within a limited scope of coding within software engineering. The IEEE Computer Society produces a Software Engineering Body of Knowledge (SWEBOK). The current version, developed in 2024, captures the collective skills, methodologies, knowledge, and so on for the software engineering profession. It has been curated by experts with the goal of reaching a consensus on the core body of knowledge. Table 16.1 lists the 18 knowledge areas (KAs) found in the SWEBOK guide, available at https://www.computer. org/education/bodies-of-knowledge/software-engineering:\n\nSoftware requirements\n\nSoftware architecture\n\nSoftware design\n\nSoftware construction\n\nSoftware testing\n\nSoftware engineering operations\n\nSoftware maintenance\n\nSoftware configuration management\n\nSoftware engineering management\n\nSoftware engineering process\n\nSoftware engineering models and methods\n\nSoftware quality\n\nSoftware security\n\nSoftware engineering professional practice\n\nSoftware engineering economics\n\nComputing foundations\n\nMathematical foundations\n\nEngineering foundations\n\nTable 16.1: Software Engineering Body of Knowledge (SWEBOK) 2024 V4\n\nCompared to the SWEBOK, GenAI enhances a few processes but only a fraction of the overall skillset currently. Specifically, it directly enhances software construction and software testing. Indirectly, it helps support software maintenance and software quality KAs.",
      "content_length": 1917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "Chapter 16\n\nDemocratization of software engineers The GenAI tools will help a wider audience than many people expect. We have discussed how junior software engineers and those getting up to speed in a new language can receive help with their programming. Senior developers can leverage their architectural, design, and other software systems experience. There is a wide class of former developers who have software experience but have shifted into other roles in computational biology, finance, business, or management.\n\nFormer programmers who have left the field to take on different responsibilities may return to coding in the near future with the GenAI tools. If they have been out of programming for several years, they would need to learn new languages, cloud computing, new IDEs, and so on. Nor- mally, they would be unlikely to invest considerable time learning this multitude of new skills. However, with GenAI, they are now empowered to write niche applications within their specific roles that use their expertise. The implications are that part of software engineering may shift somewhat from being a domain-independent technical skill to an extra skill of domain experts. That is, rather than hiring expert software engineers who have traditionally applied their techni- cal skills to write code for banking, healthcare, or other industries, these industries may turn to experienced practitioners in their field who can also program. We started to experience some of this with robotic process automation (RPA), where the subject matter experts learned the tools and drove specific workflows. In the last couple of years, we have seen similar domain experts learning prompt engineering to leverage LLMs in their workflow pipelines. We see a similar shift in the democratization of AI, where subject matter experts may be replaced by subject matter experts with AI experience.\n\nLegacy software and flexible teams With the explainability and guidance of GenAI technology, software development will certainly change corporate and industry approaches to software, especially legacy systems. Many compa- nies and industries rely on legacy software systems developed decades ago and continue to run. The systems may be decades old and written in a language that only one or two people in the company have ever learned. The software systems pose a significant risk to organizations. They may run on legacy hardware, but they are still part of existing workflows. Efforts to understand, let alone rewrite, such systems have been cost-prohibitive.\n\nGenAI systems can now be trained on the code bases and both provide support for understanding the code and perhaps enable them to be quickly rewritten. Using similar technology to human language translation, the code can use GenAI techniques to efficiently rewrite the full code base, adhere to the more recent documentation and testing strategies, and eliminate the risk. In terms\n\n395",
      "content_length": 2938,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "396\n\nArchitecture, Design, and the Future\n\nof staffing, this would reduce the need to keep developers with specialized language ability or experience with these legacy systems.\n\nAs the barriers with legacy systems decrease, software engineers may become empowered to work on a wider set of projects. While subject matter knowledge in the industry domain is critical, the coding implementation has become more accessible through GenAI.\n\nWith GenAI, coders can more quickly get up to speed with new code bases and become productive, but this has implications. The speed provided by GenAI increases the flexibility of developer teams. Developers can shift more quickly between projects and gear up for new projects. With agile project development practices and changing requirements, this is certainly a positive outcome. However, this flexibility would also presumably lead to increased outsourcing of code development since offshore teams could also quickly ramp up to tackle projects in a similar fashion. This is not a new trend, and many teams still rely on senior developers, designers, and business analysts who interact with customers.\n\nThis section focused on the people involved in software engineering and the impact of GenAI on the field. The next section discusses the potential opportunities for computer languages to change based on GenAI capabilities.\n\nWill GenAI change programming languages? The traditional art and science of writing software involves creating a set of instructions that a computer can follow. This communication currently occurs through a specific programming language. The instructions usually include combinations of data structures that store and manip- ulate data. The instructions may include algorithms that efficiently sort, manipulate, or transform data to add business value. Most programming languages can perform these operations, but their true power is only achieved when using added libraries or packages that enable user interfaces, networking, database interaction, and much more.\n\nOur GenAI coding tools today focus on writing and modifying our code. That is, ChatGPT, OpenAI API, and GitHub Copilot serve as programming assistants that guess the specific programming language code lines from the method signature. While this speeds up our programming at the method level, it requires that the developer have knowledge of how the problem breaks down into multiple methods and the transmission of information between the methods as dictated by their signature.",
      "content_length": 2510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "Chapter 16\n\nWe can draw parallels between writing a document using an LLM and writing code using GenAI. ChatGPT and other LLMs are remarkable at rephrasing text for technical audiences, doctors, or children. This is equivalent to refactoring code. To write a letter from scratch, it’s not a single command but a series of prompts that map out the instructions and sequence. Similarly, one does not write the entire code file but must break it down into a series of commands that can each be developed into code. Throughout this book, we have provided many examples that illustrate that GenAI will usually write reasonable functional code. However, by breaking functions into smaller units, the GenAI will produce higher-quality and more readable code that is easier to maintain.\n\nThe next stage of GenAI software development is possibly a more natural language way of com- municating with computers. While computer languages emphasize the precision of commands, they require an unnatural and more formal structure for communicating ideas. This requires a learning curve and arguably slows down development. Natural language could speed up the translation into computer-readable form and enable greater accessibility to software.\n\nThe rise of natural languages for computer instructions will likely require some time and face some hurdles that can be overcome with pseudocode. Three of the formal steps of computer lan- guages are sequences, iterations, and selection (conditions). The interaction of these elements is the formal control flow of computer programs. Natural language can handle the sequences and conditional flows reasonably well, but the notion of iteration loops is arguably not commonly part of languages. Pseudocode represents a somewhat arbitrary syntax but captures the three formal steps well. General pseudocode that can be interpreted by LLMs to produce formal programming languages may further empower a wider set of developers.\n\nThe future of GenAI in software engineering This section first discusses vibe coding and then breaks down the future of software engineering into two different horizons. The shorter term considers the next few years, and the longer term considers what the field might look like in five or more years.\n\nVibe coding Vibe coding is a recent trend where the developer describes what they want to do in natural lan- guage and leaves the technical coding aspects up to the LLM. Using LLMs to write code is certainly a theme of this book, and vibe coding essentially takes this to an extreme. Essentially, vibe coding provides a form of rapid prototyping that is a valuable part of the overall software engineering process, particularly in defining requirements. We believe the software community will come to recognize its value as such. However, for larger projects, code quality is important since most of\n\n397",
      "content_length": 2860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "398\n\nArchitecture, Design, and the Future\n\nthe costs associated with software are in its maintenance, such as adding new features. Many unhappy memes describe the difficulty of “vibe debugging” the relatively poor-quality code produced by vibe coding. we expect a future that lands somewhere between where we are now and vibe coding. We will describe the shorter- and longer-term expectations next.\n\nShorter term Over the last year, the GenAI market has blossomed, and many developers are now working with this technology. It is available as a VSCode extension, Jetbrains products including PyCharm plugin, and other integrated development environments (IDEs) so it’s readily available. In addition, new IDEs with built-in GenAI capabilities have entered the market, including Cursor, Windsurf, and Replit. Other tools, such as AWS CodeWhisperer and Tabnine, offer code suggestions for multiple languages. All of these are offered at a low price, compared to the potential savings of developer time. In the short term, all of these will increase their accuracy and lead to the creation of better software in less time.\n\nLonger term After many have adopted the GenAI coding approaches and can use them effectively, the technol- ogy push will focus on trying to move from formal requirements and specifications to code. As one of the first steps in the software development life cycle, requirements gathering is a formal process to understand the intended goals, the user interfaces, non-functional requirements, and so on. In many organizations, there are more formal diagrams, such as Unified Markup Language (UML), that describe the design. Others include sample user interface designs from Balsamiq and Figma. Efforts are underway to automatically convert these diagrams directly into code.\n\nGenAI has not solved many of the other aspects of software development processes. The planning, designing, higher-level integration, DevOps, deployment, orchestration, and other aspects of software have had minimal impact from GenAI. We are excited about the long-term prospects, but do not have an opinion on when GenAI techniques will make meaningful strides in these areas.\n\nWill AI replace software engineering? The need for software engineering will continue for two reasons. First, we know that GenAI is much more effective at writing parts rather than full end-to-end content. For prose text, we don’t expect GenAI to draft full reports, but we can break it down into sections and paragraphs and use prompt engineering. It’s no surprise that some of the main use cases are web searches and summarization, which have weak parallels in software engineering and improving text, which is essentially refactoring. With software engineering, knowledge of software structure and sup-",
      "content_length": 2777,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "Chapter 16\n\nporting architectures is needed to break code into modules, files, and succinct methods. The need for human skills for these tasks will likely continue for several years, even though individual methods may be fully crafted using GenAI. Humans will shift to roles in architecture, design, and integration, and will still ultimately be responsible for validating the code.\n\nThe second reason is that GenAI is most effective when working with widely available code bases but often fails without such expansive sets of examples. Most software engineers need to learn a new language every 3–5 years on average. Certainly, some of this is due to switching jobs. However, new languages, frameworks, and new versions of both continue to emerge—none of which begin with sufficiently large code bases for training GenAI models. Domain-specific lan- guages are another good example of limited training data. In addition, the newest technologies in databases, cybersecurity, quantum computing, and so on need software engineers since their respective code bases are still being written.\n\nRisk and governance Humans are currently ultimately responsible for the code that is produced, but the use of GenAI raises a number of challenges. In heavily regulated industries such as transportation, banking, and healthcare, the use of AI will undergo significant scrutiny. The ability to provide explainability and understanding of which programmers, agents, or AI created different sections of code will likely become standard. The risks, legal responsibilities, and ethical considerations will need to address responsibilities and safeguards for software. The ethical, moral, and legal issues around self-driving cars today provide a good example of some of the considerations. MIT’s trolley prob- lem (https://en.wikipedia.org/wiki/Trolley_problem) and the Moral Machine (https:// en.wikipedia.org/wiki/Moral_Machine) provide further examples of the challenges.\n\nAnother challenge is the copyright issue of software. For regular texts, News Corp, the Financial Times, the Associated Press, and other media companies have negotiated agreements to license their text to OpenAI. Microsoft and Google have also negotiated with content providers to scale their training sets without violating copyright laws. For software code, the laws are less clear since it’s a relatively new problem that has not yet been resolved. It is certainly possible that the GenAI could duplicate copyrighted code. Since the accuracy of the GenAI depends in part on the size of the training set, more code licensing may be required to improve performance.\n\nThe education in programming languages and the evaluation of skills must also adapt. Schools and universities have taught programming skills for decades. Just as spelling and grammar checkers are standard practice for writing reports, GenAI is likely to eventually not just be accepted but be a required part of software development. Universities have tended to focus more on the language\n\n399",
      "content_length": 3019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "400\n\nArchitecture, Design, and the Future\n\nnuances and syntax rather than the set of available tools, such as compilers, debuggers, and linters. However, ignoring GenAI will be difficult since it will be used extensively. Job interviewers have often used LeetCode forms of technical questions to weed out candidates, even though many of the questions are more academic than common industry practices. With GenAI, such low-level coding skills have become even less relevant, and we expect these forms of interview questions to evolve as well.\n\nSummary AI and GenAI capabilities have advanced rapidly over the past several years through a number of major technology innovations. With GenAI’s boost in software engineering productivity at a low price point, it will increase the volume of code written and lower the overall costs. This will change the software engineer landscape, perhaps to favor more senior developers with more ex- perience in architecture, design, and SWEBOK. It may also enable specialists with deep domain experience to develop more code. Legacy systems may be rewritten with GenAI support, which will free companies to use software developers across projects in a more agile manner.\n\nThere is a strong movement toward adopting GenAI for coding, and it has already been integrat- ed into many IDEs. In the longer term, we will see how it surpasses the coding tasks and enters more aspects of software engineering. GenAI still requires the communication of commands to computers, and natural language may eventually supplement or replace programming languages, but pseudocode may be the next frontier. The world will still need software engineers for the foreseeable future. There are still many issues to manage, from risk to compliance, but GenAI has arrived and is already supercharging software engineering.\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nThomas Davenport and DJ Patil. Harvard Business Review. https://hbr.org/2012/10/\n\ndata-scientist-the-sexiest-job-of-the-21st-century\n\n\n\nAI winters: https://www.historyofdatascience.com/ai-winter-the-highs-and-lows-\n\nof-artificial-intelligence/\n\n\n\nAlex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton. ImageNet Classification with Deep Convo- lutional Neural Networks. NeurIPS 2012. https://proceedings.neurips.cc/paper_files/\n\npaper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf",
      "content_length": 2441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "Chapter 16\n\n\n\nImageNet Large Scale Visual Recognition Challenge. https://arxiv.org/pdf/1409.0575\n\n\n\nAttention Is All You Need. https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\n\nGartner hype cycle for AI practices. https://www.gartner.com/en/newsroom/press- releases/2023-11-28-gartner-hype-cycle-shows-ai-practices-and-platform- engineering-will-reach-mainstream-adoption-in-software-engineering-in-two- to-five-years\n\n\n\nThe startling power generative AI is bringing to software development. https://kpmg.com/us/\n\nen/articles/2023/generative-artificial-intelligence.html\n\nH. Washizaki, eds., Guide to the Software Engineering Body of Knowledge (SWEBOK Guide), Version 4.0, IEEE Computer Society, 2024. https://www.computer.org/education/ bodies-of-knowledge/software-engineering\n\nMoral Machine. https://en.wikipedia.org/wiki/Moral_Machine\n\n\n\nTrolley problem. https://en.wikipedia.org/wiki/Trolley_problem\n\nSubscribe for a free eBook New frameworks, evolving architectures, research drops, production breakdowns—AI_Distilled filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.\n\nSubscribe at https://packt.link/TRO5B or scan the QR code below.\n\n401",
      "content_length": 1326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "Appendix\n\nSet up an OpenAI Account Here are the steps to create an OpenAI account:\n\n1. Navigate to https://openai.com/. At the top left of the page, click on either ChatGPT or\n\nAPI Platform to start the account creation process, as shown in Figure 1:\n\nFigure 1: Access to ChatGPT and API Platform from OpenAI API\n\n2. Each of these links will take you to an account creation page, where you can sign up using popular platforms such as Google, Microsoft, or Apple, or register with an email address, as shown in Figure 2:",
      "content_length": 519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "404\n\nFigure 2: OpenAI platform sign-up page\n\n3. To register with an email address, click on Sign up.\n\nImportant Note\n\nAs of August 2025, you will need to create two separate accounts: one for ChatGPT and one for the OpenAI platform to access the API.\n\nIn the next section, we will go through the setup of a virtual environment.\n\nCreating a virtual environment A virtual environment in Python is an isolated setup that ensures the reproducibility of depen- dencies, which are external libraries our project depends on.\n\nFor example, consider the following code:\n\nimport pandas as pd\n\ndf = pd.read_csv('df.csv')\n\ndf.to_markdown()\n\nAppendix",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "Here, we depend on Pandas, a third-party and open-source library available at: https://github. com/pandas-dev/pandas. Pandas provides the read_csv function, which returns a DataFrame. Since version 1.0.0, which can only run on Python 3.6.1 or higher, the DataFrame class also includes the to_markdown method. If we use earlier versions of Python or Pandas, this code may not run.\n\nGenerally, developers specify their required dependencies in a file named requirements.txt. In this book’s repository, you will find a specification of the external libraries that we use in requirements.txt, as shown in Figure 3:\n\nFigure 3: Requirements.txt file for this book\n\nIn later chapters of the book, where we dive deeper into the software development lifecycle (SDLC) functions, the requirement files for these projects will include specific versions, as shown in Figure 4:\n\nFigure 4: requirements.txt with versioning from Chapter 10\n\n405",
      "content_length": 928,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "406\n\nJust like in this figure, our production projects also include specific versioning. For the dependency versioning resolution, we leverage dependency and environment management tools such as pipenv that proposes dependency management for specific versioning resolution. Other developers may use Conda, poetry, or other tools. In this book, however, we will use Python’s built-in venv tool for our environment setup.\n\nPrompting virtual environment In this book, we leveraged GenAI for every step of the SDLC, and installing a virtual environment is no exception. This task is a good candidate for using the agent mode within the GitHub Copilot chat window, with a prompt that follows the Five S’s discussed in detail in Chapter 4.\n\nA prompt to install our dependencies should include the following components:\n\n1. Python version: Which version of Python should be used to run this project, in our case,\n\nit is Python 3.11.\n\n2. Environment tool: Either venv or its alternatives.\n\n3. Requirements file: The default is requirements.txt placed in the root of the project.\n\n4. Environment name: A common practice is to name the environment .venv. We can also customize it to suit the project name with a prefix of a period, such as .supercharged.\n\n5. Environment location: The library code is typically installed at the project root.\n\nConsidering these, here is an example of a prompt we can use with GitHub Copilot, either in PyCharm or VS Code:\n\nCONTEXT: You are provided with:\n\n1. Python version enclosed with {{{ VERSION }}}\n\n2. Requirements file path enclosed with {{{ PATH }}}\n\n3. Environment tool enclosed with {{{ TOOL }}}\n\n4. Environment name enclosed with {{{ ENV NAME }}}\n\n5. Location for the environment enclosed with {{{ Location }}}\n\nTASK: Set up a virtual environment using the Python version, requirements file path, using the provided name and location for this project.\n\nVERSION: {{{ Python 3.11 }}\n\nPATH: {{{ requirements.txt }}}\n\nTOOL: {{{ venv }}}\n\nAppendix",
      "content_length": 1976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "ENV NAME: {{{ .supercharged }}}\n\nLOCATION: {{{ project root }}}\n\nFor reference, the output of the setup using agent mode with GitHub Copilot and the underlying Claude Sonnet 4.0 specifies the steps for setting up an environment, activating it, and installing its dependencies. Figure 5 demonstrates creating the environment with our desired name and activating it:\n\nFigure 5: Activating the virtual environment\n\nFigure 6 illustrates the installation of dependencies as specified in requirements.txt:\n\nFigure 6: Installation of the dependencies\n\n407",
      "content_length": 548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "408\n\nOnce installation is complete, Copilot may propose some tests, such as validating the correct versioning of the packages, concluding with a success message as shown in Figure 7:\n\nFigure 7: Successful installation message\n\nAfter the setup is complete, we can see that a .supercharged folder is added to the repository, as shown in Figure 8:\n\nFigure 8: Added files to the repository\n\nAppendix",
      "content_length": 395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "For reference, here are the typical three shell commands we can use to manually install the virtual environment:\n\npython3.11 -m venv .supercharged\n\nsource .supercharged/bin/activate\n\npip install -r requirements.txt\n\nNext, we will see how to instruct git to ignore the dependencies files.\n\nAdding virtual environment to .gitignore Since requirements.txt is sufficient for replicating the dependencies on any machine, there is no need to include the actual environment code in the repository. For that reason, it is best practice to ignore the installed dependencies. We can do this by adding the path of the installed requirements to the .gitignore file. We can chain another prompt in agent mode:\n\nAdd the environment path to .gitignore file.\n\nThis prompt successfully adds the path of the virtual environment to .gitignore, as shown in Figure 9.\n\nFigure 9: Adding the path to the .gitignore file\n\nIf you named your environment differently, Copilot would likely add the correct name to the file. If you used .supercharged or .venv, no changes are required because these names are already in our files.\n\nImportant Note\n\nNow that you are a supercharged coder, we encourage you to help your colleagues use these prompts when setting up a new virtual environment. These can be adapt- ed for different environment tools, environment names, and projects, reducing the chances of errors when working with Agent mode\n\n409",
      "content_length": 1413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "410\n\nFurther reading To learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n\n\nVenv, virtual environment built-in: https://docs.python.org/3/library/venv.html\n\n\n\nPipenv, dependency and environment management tools: https://pipenv.pypa.io/\n\nen/latest/\n\n\n\nPoetry, dependency and environment management tools: https://python-poetry.org/\n\ndocs/dependency-specification/\n\n\n\nConda, dependency and environment management tools: https://anaconda.org/ anaconda/conda\n\nAppendix",
      "content_length": 518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "packtpub.com\n\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.\n\nWhy subscribe?\n\n\n\nSpend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals\n\n\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\n\n\nFully searchable for easy access to vital information\n\n\n\nCopy and paste, print, and bookmark content\n\nDid you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at packtpub.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at customercare@packtpub. com for more details.\n\nAt www.packtpub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.",
      "content_length": 1099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "Other Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other books by Packt:\n\nBuilding Agentic AI Systems\n\nAnjanava Biswas, Wrick Talukdar\n\nISBN: 978-1-80323-875-3\n\nMaster the core principles of GenAI and agentic systems\n\nUnderstand how AI agents operate, reason, and adapt in dynamic environments\n\n\n\nEnable AI agents to analyze their own actions and improvise\n\n\n\nImplement systems where AI agents can leverage external tools and plan complex tasks\n\n\n\nApply methods to enhance transparency, accountability, and reliability in AI\n\n\n\nExplore real-world implementations of AI agents across industries",
      "content_length": 626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "414\n\nOther Books You May Enjoy\n\nGenerative AI with LangChain - Second Edition\n\nBen Auffarth, Leonid Kuligin\n\nISBN: 978-1-83702-201-4\n\nDesign and implement multi-agent systems using LangGraph\n\n\n\nImplement testing strategies that identify issues before deployment\n\nDeploy observability and monitoring solutions for production environments\n\n\n\nBuild agentic RAG systems with re-ranking capabilities\n\n\n\nArchitect scalable, production-ready AI agents using LangGraph and MCP\n\nWork with the latest LLMs and providers like Google Gemini, Anthropic, Mistral,\n\nDeepSeek, and OpenAI’s o3-mini\n\nDesign secure, compliant AI systems aligned with modern ethical practices",
      "content_length": 656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "Other Books You May Enjoy\n\nPackt is searching for authors like you If you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general appli- cation, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\n\nShare Your Thoughts Now you’ve finished Supercharged Coding with GenAI, we’d love to hear your thoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon review page for this book and share your feedback or leave a review on the site that you purchased it from.\n\nYour review is important to us and the tech community and will help us make sure we’re deliv- ering excellent quality content.\n\n415",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "416\n\nOther Books You May Enjoy\n\nJoin our Discord and Reddit spaces You’re not the only one navigating fragmented tools, constant updates, and unclear best prac- tices. Join a growing community of professionals exchanging insights that don’t make it into documentation.\n\nStay informed with updates, discussions, and behind-the-scenes insights from our authors.\n\nJoin our Discord space at https://packt. link/z8ivB or scan the QR code below:\n\nConnect with peers, share ideas, and discuss real-world GenAI challenges. Follow us on Reddit at https://packt.link/0rExL or scan the QR code below:",
      "content_length": 589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "Symbols\n\n__call__ method 110\n\nA\n\nAI pair programmer 45\n\nAI winter 6\n\nAmerican Invitational Mathematics Examination (AIME) 78\n\napplication programming interface\n\n(API) 10, 17\n\nartificial intelligence (AI) 6, 389\n\nassistant prompts 19, 107\n\nB\n\nBig-O notation 262, 337\n\nC\n\ncalculate_distance function 250\n\ncalls\n\ncall parameters, customizing with ChatGPT\n\nand OpenAI 182, 183 code debugging, with GitHub\n\nCopilot 177-179\n\ncreating, with example parameters 175, 176\n\ncentral processing unit (CPU) 336\n\nIndex\n\nchained prompts, code optimization 351\n\nruntime optimization 352-354 space optimization 354-357\n\nchaining 209\n\nchain-of-thought (CoT) 209, 241, 361\n\nrefactoring 250, 251 refactoring, with ChatGPT 254, 255 refactoring, with GitHub Copilot 251-253 refactoring, with OpenAI API 254, 255\n\nChatGPT 9, 154, 155 chaining 228-231 for testing 318-320 used, for explaining code 166-168 used, for performance refactoring 258, 259 used, for refactoring chain-of-thought\n\n(CoT) 254, 255\n\nused, for searching outdated docstrings 304-307\n\nChatGPT baseline 213, 215\n\nChatGPT maximal capacity\n\nanalyzing 343 determining, within memory constraints 349-351 determining, within runtime constraints 347, 348\n\nprofiling runtime 344 profiling space usage 345, 346",
      "content_length": 1245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "418\n\nClaude 7\n\ncode analysis and refactoring tool 5\n\ncode base structure reading 160, 161\n\ncode completion design\n\noverview 46, 47\n\ncode optimization\n\nwith chained prompts 351\n\ncode readability\n\nimproving 247, 248\n\ncode refactoring 242\n\nlimitations, with IDE tools 248, 249\n\ncode smell 5\n\ncomma-separated values (CSV) 338\n\ncommercial off-the-shelf (COTS) 392\n\ncompressed sparse column (CSC) 357\n\ncompressed sparse row (CSR) 357\n\ncomputer programming 4\n\ncontext window 151\n\nexamples 152\n\ncontinuous integration and continuous\n\ndeployment (CI/CD) systems 5\n\ncontinuous integration/continuous\n\ndevelopment (CI/CD) pipelines 300\n\ncontrastive learning 274\n\nconvolutional neural network (CNN) 390\n\nCopilot\n\nchaining 236, 237\n\nCoT reasoning with LLM\n\nfor GitHub Copilot 221-224 geometric mean with ChatGPT 225, 226 geometric mean with OpenAI API 226, 227 GPT-o series 219 using 217, 219\n\nD\n\ndata-driven tests 321, 322\n\ndecorators 361, 368\n\nusing 368, 369\n\ndeep learning 7, 150\n\ndependency code\n\ndebugging 183, 184\n\ndistance function\n\nimplementing 244, 245\n\nDockerfile 159, 161\n\ndocstrings\n\ngenerating, for Singleton __call__\n\nmethod 112, 113\n\ngenerating, with OpenAI API 110 Google-style docstring 113-116\n\ndocstrings, with GenAI\n\nwriting 300 writing, for entire file 302, 304 writing, for single method 300, 301\n\ndouble underscore (dunder) 105\n\nE\n\nEuclidean distance 243\n\nevaluation metrics 80\n\nexpert systems 6\n\nExtreme Programming (XP) 10, 243, 322\n\nF\n\nfew-shot decorator style\n\nfor GitHub Copilot 378, 380\n\nfew-shot\n\nlearning 152, 189, 190, 265, 274, 361\n\ncode implementation, with OpenAI\n\nAPI 200-203\n\nIndex",
      "content_length": 1605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "Index\n\nfor code refactoring 190, 192 in ChatGPT 381-383 using, as style guides 378 working with, for OpenAI API 199\n\nfew-shot prompting, with ChatGPT\n\nleveraging 192 output improvement, with code\n\nrefactoring 193-195\n\nstructure 192\n\nfine-tuning 152, 203\n\nused, for OpenAI API 383-387 using, as style guides 378 versus prompt engineering 268\n\nFizzBuzz 363 use case 363\n\nFrobenius norm 243\n\nG\n\nGemini 7\n\nGenAI 390, 391\n\nadopting, by software engineers 392, 393 for coding tasks 78, 79 for unit tests 312-317 programming languages 396, 397 sample data creation 329-331\n\nGenAI for higher-level coding patterns\n\ndecorators, using 368, 369 inverse CoT for decorator\n\nimplementation 369-374\n\nusing 368\n\nGenAI future 397 longer term 398 risk and governance 399 shorter term 398 software engineering, need for 398, 399 vibe coding 397\n\nGenAI toolkit 9 ChatGPT 9 GitHub Copilot 10 OpenAI API 10\n\nGenerative artificial intelligence\n\n(GenAI) 3, 17 benefits, of coding 12 downsides, of coding 12, 13 software development, improving 11\n\nGenerative Pretrained Transformer\n\n(GPT) 7, 9\n\ngeometric mean analysis 59, 60 calculating, with chat completion 57 guided walk-through 58, 59\n\ngeometric mean calculations\n\nanalysis 63-65 completion and keyboard shortcuts 61 guided walk-through 61, 62\n\ngeometric mean scope 210, 211\n\nGitHub Copilot 10, 46, 155, 164, 196\n\naccount, activating 50, 51 chat feature, using 54, 56 code, analyzing 65, 66 code, analyzing with Copilot editor window\n\nin VS Code 67, 68\n\ncode completion design overview 46, 47 code completion policies 49 debugging with 177-179 dependency code, debugging 180, 181 /explain command, using with 164, 165 extension, installing for VS Code 53 faulty implementation, fixing 125 free account limits 48 imports and hashtags 118-121 interaction modes 54 lead-in cue structure 117, 118\n\n419",
      "content_length": 1827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "420\n\nplugin, installing for PyCharm 52 precise prompts, crafting 117 pricing 49 setting up, in PyCharm 51 setting up, in VS Code 51 short prompts, without comment\n\nfluff 124, 125 single task 121-123 style guide 196-199 unit test, evaluating 126-134 used, for profiling runtime 339 used, for profiling space usage 339 used, for refactoring chain-of-thought\n\n(CoT) 251-253\n\nused, for searching outdated docstrings 304-307\n\nwith descriptive names instructions 123 with docstrings instructions 123 with type hints instructions 123 with unit tests instructions 123 working, in VS Code Jupyter Notebook\n\nfor Code Analysis 69\n\nGitHub Copilot baseline 213\n\nGitHub Copilot command working with 296-300\n\nGitHub Copilot, prompt analysis\n\nprompt techniques, applying with\n\nPyCharm chat 135\n\nsingle task, to fix method\n\nimplementation 135, 136 single task, with VS Code 135 specific and short prompt\n\nprinciples 134, 135\n\nstructured prompt principle, with\n\nlead-in cue 134\n\nsurrounding context, with VS Code 135\n\nGoogle style 295\n\nGoogle-style docstring 112-116\n\nanalysis 116, 117\n\nGPT-3.5 model 154\n\nGPT-4o mini 209\n\nused, for computing quadratic roots 271\n\nGPT-o series 219 adoption 220\n\ngraphical processing units (GPUs) 357\n\ngraphical user interface (GUI) 269\n\nH\n\nHypertext Transfer Protocol (HTTP) 19, 161\n\nI\n\nIEBUPDTE 5\n\nImageNet Large Scale Visual Recognition\n\nChallenge (ILSVRC) 390\n\nimplementation code 311\n\nin-context learning 189, 190, 265\n\nintegrated development environment (IDE) 5, 45, 104, 362, 393\n\ninteraction modes\n\nanalysis 54 chat 54 completion 54\n\ninverse CoT for decorator\n\nimplementation 369-374\n\ninverse CoT with ChatGPT\n\napplying 375 decorators, monitoring 375, 377\n\ninverse CoT with OpenAI\n\napplying 375 decorators, monitoring 377, 378\n\nIndex",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "Index\n\nJ\n\nJSONL file\n\ncrafting, for fine-tuning 273 file structure, fine-tuning 274, 275 strategy 275\n\nJSONL file, strategy code-only 276 code with no comments 277, 278 contrastive code-only 276, 277 contrastive no comments 278-280\n\nK\n\nKite 6\n\nL\n\nL1 norm 243\n\nL2 norm 243\n\nlanguage models (LMs) 7\n\nlarge language models (LLMs) 4, 18, 45, 78, 106, 149, 209, 265, 335, 361, 390 ChatGPT, for explaining code 166-168 code task, specializing 268 costs 280, 281 /explain command, using with GitHub\n\nCopilot 164, 166 fine-tuned model, using 284 fine-tuning 267, 268 fine-tuning in action 287 fine-tuning job, initiating 281-284 fine-tuning, with OpenAI GUI 280 limitations 153, 154 OpenAI API, for explaining code 168, 169 outputs, comparing 285-287 rise 6, 7 used, for explaining code 163\n\nlinters 5\n\nLlama 7\n\nLLM output performance\n\ncontext window, adding 151, 152 fine-tuning 152 improving 151 prompt engineering 151 retrieval-augmented generation 153\n\nLLMs specialization 267\n\nexamples 267\n\nlogging 362-365\n\nlonger term 398\n\nM\n\nmachine learning (ML) 6\n\nMaestro I 5\n\nmagic attribute 364\n\nManhattan distance 159-162, 243\n\nfor matrices 162, 163\n\nmemory space usage 338\n\nefficiency tradeoffs, balancing 339 space complexity 338\n\nMiller’s Law 295\n\nmodel mastery 79\n\nmonitoring 362, 366, 367\n\nN\n\nnatural language generation (NLG) 7\n\nnatural language processing (NLP) 6, 7, 18, 155, 390\n\nneural networks 6\n\nneurons 7\n\nn-grams 313\n\n421",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "422\n\nnon-project files 169\n\nentire file, explaining 170 explaining, with ChatGPT 172, 173 explaining, with context 171, 172 explaining, with OpenAI API 174, 175\n\nNumPy/SciPy style 295\n\nO\n\none-shot learning 152\n\nOpenAI 17\n\nbilling, computing 30, 31 project API keys, obtaining 22-24 used, for performance refactoring 258, 259\n\nOpenAI accounts rate limits 31 reference link 403 setting up 403, 404 usage restrictions 31\n\nOpenAI API 10, 18\n\nbody 19 endpoint 19 headers 19 history 231-235 HTTP method 19 precise prompts, crafting 107 prompt 256 routing chat capabilities, to code\n\ncompletion 37-41 structured prompts 107, 108 system prompts, with surrounding context\n\nand single task 108\n\nthrough RESTful HTTP request 19 used, for explaining code 168, 169 used, for extracting Python objects\n\nproperties 104-106\n\nused, for generating docstrings 110\n\nused, for refactoring chain-of-thought\n\n(CoT) 254, 255\n\nused, for searching outdated docstrings 304-307\n\nuser prompts, with specific instructions 109 using, to generate code 36 vectorizing 260, 261\n\nOpenAI API baseline 215-217\n\nOpenAI API Playground\n\nexperimenting with 269-271 quadratic roots, computing\n\nwith GPT-4o mini 271\n\nOpenAI API request cost calculating 28 parameters analyzing 32-36\n\nOpenAI GUI\n\nused, for fine-tuning large language models\n\n(LLMs) 280\n\nopenai Python package\n\ninstalling 21 OpenAI API request, sending 24-27\n\nOpenAI Tokenizer reference link 29\n\noutdated docstrings\n\nsearching, with ChatGPT 304-307 searching, with GitHub Copilot 304-307 searching, with OpenAI API 304-307\n\nP\n\npair programmer 10\n\nparameterized decorator 373\n\npattern matching 5\n\nPEP 257 295\n\nIndex",
      "content_length": 1636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "Index\n\nperformance refactoring advanced features 261 with ChatGPT 258, 259 with GenAI 256 with GitHub Copilot 256-258 with OpenAI 258, 259\n\nprecise prompts 81\n\ncrafting, for GitHub Copilot 117\n\nproduction system 8\n\nprofiling runtime 339-341 with GitHub Copilot 339\n\nprofiling space usage 342 with GitHub Copilot 339\n\nprogram runtime 336, 337\n\nruntime complexity, measuring 337\n\nprompt chaining for LLM\n\nChatGPT, chaining with 228-231 Copilot, chaining 236, 237 OpenAI API, history 231-235 using 227\n\nprompt engineering 10, 151\n\nfor coding 188, 189 versus fine-tuning 268\n\nprompt engineering, techniques 203, 266\n\nadditional methods 206 bug fix at scale 203, 204 chain-of-thought (CoT) prompting 205, 206 iterative prompting 204, 205 template-based prompting 205\n\nprompt implementation scope 210\n\nprompting techniques 391\n\nprompts 18\n\nassistant prompts 19 system prompts 19 user prompts 18\n\nprompt samples by OpenAI 94, 95\n\nanalysis 99-101 chain-of-thought prompting 101 code debugging 95, 96 guided walk-through 97-99 iterative prompting 101\n\nprompts for ChatGPT\n\nanalysis 92- 94 crafting 84, 85 guided walk-through 87, 88 single task 89 specific instructions 90-92 structuring 88 surrounding context, adding 89\n\nprompts for coding tasks, best practices 81\n\nshort prompts 83 single task per prompt 82 specific instructions 83 structured 81 surrounding information 82\n\nproof of concept (POC) 9\n\npull request (PR) 78\n\nPyCharm 5\n\nused, for setting up GitHub Copilot 51\n\nPyCharm GUI\n\nconverting, to CLI 85, 87\n\nPython objects properties\n\nextracting, for OpenAI API 104-106\n\nPython threading 80\n\nQ\n\nquadratic roots\n\ncalculator, implementing 272, 273 computing, with GPT-4o mini 271\n\n423",
      "content_length": 1681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "424\n\nR\n\nraising errors 362\n\nRandom Access Memory (RAM) 336\n\nReddit 6\n\nrefactoring 241 types 245, 246\n\nrefactoring tasks\n\nperformance 246 readability 246 structure change 246\n\nregression test package 330\n\nreinforcement learning 391\n\nreinforcement learning from human\n\nfeedback (RLHF) 154\n\nRepresentational State Transfer (REST) 19\n\nrequests per day (RPD) 31\n\nrequests per minute (RPM) 31\n\nReSharper 5\n\nRESTful HTTP request 17\n\nreStructuredText (reST) 295\n\nretrieval-augmented generation 153\n\nretrieval-augmented generation (RAG) 151\n\nReuters-21578 dataset 142\n\nRevision Control System 5\n\nrobotic process automation (RPA) 395\n\nS\n\nsingle responsibility principle 361, 367\n\nsingle-task principle 364\n\nSingleton design pattern 104, 110, 111\n\nsmall language model 149\n\nsoftware development 391, 392\n\ntools, evolution 4-6\n\nsoftware development life\n\ncycle (SDLC) 4-8, 235, 287, 293, 335, 362, 405\n\nsoftware development tools 393, 394\n\nlegacy software and flexible teams 395, 396 Software Engineering Body of Knowledge\n\n(SWEBOK) 394\n\nsoftware engineers democratization 395\n\nsoftware documentation 294, 295\n\nsoftware engineering 4\n\nSoftware Engineering Body of Knowledge\n\n(SWEBOK) 394\n\nsoftware testing 5\n\nSonarQube 5\n\nSphinx style 295\n\nSpyder 5\n\nStack Overflow 6\n\nstatistical approach 142-147\n\nstructured prompts 107, 108\n\nsystem prompts 19, 107 with single task 108 with surrounding context 108\n\nT\n\ntest code 311\n\nTest-Driven Development\n\n(TDD) 9, 243, 311, 313, 322-325\n\nwith ChatGPT 327, 329 within PyCharm 327 within VS Code 325, 326\n\ntime and space analysis 336 memory space usage 338 program runtime 336, 337\n\ntokens 17, 28, 29\n\ntokens per minute (TPM) 31\n\nIndex",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "Index\n\ntransformer architecture 390\n\nTurbo Pascal 5\n\nU\n\nUCI Machine Learning Repository 142\n\nunexpected input\n\nhandling 365, 366\n\nUnified Markup Language (UML) 398\n\nunittest frameworks 5\n\nunit tests\n\nwith GenAI 312-317\n\nUniversal Approximation Theorem 150\n\nUSA Junior Mathematical Olympiad\n\n(USAJMO) 78\n\nUSA Mathematical Olympiad (USAMO) 78\n\nuser prompts 18, 107\n\nwith specific instructions 109\n\nV\n\nvectorized computations 256\n\nversion control systems 5\n\nvibe coding 13, 397\n\nvirtual environment 404 adding, to .gitignore 409 creating 404, 405 prompting 406-408\n\nVisual Studio 5\n\nVisual Studio Code (VS Code) 5\n\nused, for setting up GitHub Copilot 51\n\nVS Code Jupyter Notebook for Code Analysis\n\nanalysis 72, 74 GitHub Copilot, working with 69 guided walk-through 69-71\n\nW\n\nword2vec model 149, 150\n\nword-level models extension 147-149\n\nZ\n\nzero-shot cross-linguistic transfer 153\n\nzero-shot learning 152\n\nZipf’s law 142\n\n425",
      "content_length": 923,
      "extraction_method": "Unstructured"
    }
  ]
}