{
  "metadata": {
    "title": "MongoDB The Definitive Guide",
    "author": "Shannon Bradshaw;Eoin Brazil;Kristina Chodorow;",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 514,
    "conversion_date": "2025-12-25T18:15:03.189743",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "MongoDB The Definitive Guide.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-10)",
      "start_page": 1,
      "end_page": 10,
      "detection_method": "topic_boundary",
      "content": "Third Edition\n\nMongoDB MongoDB The Deﬁ nitive Guide The Deﬁ nitive Guide Powerful and Scalable Data Storage Powerful and Scalable Data Storage\n\nShannon Bradshaw, Eoin Brazil Shannon Bradshaw, Eoin Brazil & Kristina Chodorow\n\nTHIRD EDITION\n\nMongoDB: The Definitive Guide Powerful and Scalable Data Storage\n\nShannon Bradshaw, Eoin Brazil, and Kristina Chodorow\n\nBeijing Beijing\n\nBoston Boston\n\nFarnham Sebastopol Farnham Sebastopol\n\nTokyo Tokyo\n\nMongoDB: The Definitive Guide by Shannon Bradshaw, Eoin Brazil, and Kristina Chodorow\n\nCopyright © 2020 Shannon Bradshaw and Eoin Brazil. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nEditor: Nicole Taché Production Editor: Kristen Brown Copyeditor: Rachel Head Proofreader: Christina Edwards\n\nIndexer: Judith McConville Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Rebecca Demarest\n\nSeptember 2010: May 2013: December 2019:\n\nFirst Edition Second Edition Third Edition\n\nRevision History for the Third Edition 2019-12-09: First Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491954461 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. MongoDB: The Definitive Guide, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-491-95446-1\n\n[LSI]\n\nThis book is dedicated to our families for the time, space, and support they provided to make our work on this book possible and for their love.\n\nFor Anna, Sigourney, Graham, and Beckett. —Shannon\n\nAnd for Gemma, Clodagh, and Bronagh. —Eoin\n\nTable of Contents\n\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv\n\nPart I.\n\nIntroduction to MongoDB\n\n1.\n\nIntroduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Ease of Use 3 Designed to Scale 4 Rich with Features… 5 …Without Sacrificing Speed 6 The Philosophy 6\n\n2. Getting Started. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Documents 7 Collections 8 Dynamic Schemas 8 Naming 9 Databases 10 Getting and Starting MongoDB 11 Introduction to the MongoDB Shell 13 Running the Shell 13 A MongoDB Client 14 Basic Operations with the Shell 14 Data Types 16 Basic Data Types 16 Dates 18 v\n\nArrays 19 Embedded Documents 19 _id and ObjectIds 20 Using the MongoDB Shell 22 Tips for Using the Shell 22 Running Scripts with the Shell 23 Creating a .mongorc.js 25 Customizing Your Prompt 26 Editing Complex Variables 27 Inconvenient Collection Names 28\n\n3. Creating, Updating, and Deleting Documents. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Inserting Documents 29 insertMany 29 Insert Validation 32 insert 33 Removing Documents 33 drop 34 Updating Documents 35 Document Replacement 35 Using Update Operators 37 Upserts 46 Updating Multiple Documents 49 Returning Updated Documents 49\n\n4. Querying. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 Introduction to find 53 Specifying Which Keys to Return 54 Limitations 55 Query Criteria 55 Query Conditionals 55 OR Queries 56 $not 57 Type-Specific Queries 57 null 57 Regular Expressions 58 Querying Arrays 59 Querying on Embedded Documents 63 $where Queries 65 Cursors 66 Table of Contents\n\n4. Querying. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 Introduction to find 53 Specifying Which Keys to Return 54 Limitations 55 Query Criteria 55 Query Conditionals 55 OR Queries 56 $not 57 Type-Specific Queries 57 null 57 Regular Expressions 58 Querying Arrays 59 Querying on Embedded Documents 63 $where Queries 65 Cursors 66 Table of Contents\n\nLimits, Skips, and Sorts 67 Avoiding Large Skips 68 Immortal Cursors 70\n\nPart II. Designing Your Application\n\n5.\n\nIndexes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 Introduction to Indexes 75 Creating an Index 78 Introduction to Compound Indexes 81 How MongoDB Selects an Index 84 Using Compound Indexes 85 How $ Operators Use Indexes 104 Indexing Objects and Arrays 114 Index Cardinality 116 explain Output 116 When Not to Index 125 Types of Indexes 126 Unique Indexes 126 Partial Indexes 128 Index Administration 129 Identifying Indexes 130 Changing Indexes 130\n\n6. Special Index and Collection Types. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 Geospatial Indexes 133 Types of Geospatial Queries 134 Using Geospatial Indexes 136 Compound Geospatial Indexes 144 2d Indexes 144 Indexes for Full Text Search 146 Creating a Text Index 147 Text Search 148 Optimizing Full-Text Search 151 Searching in Other Languages 151 Capped Collections 151 Creating Capped Collections 154 Tailable Cursors 154 Time-To-Live Indexes 155 vii\n\nStoring Files with GridFS 156 Getting Started with GridFS: mongofiles 156 Working with GridFS from the MongoDB Drivers 157 Under the Hood 158\n\n7.\n\nIntroduction to the Aggregation Framework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 Pipelines, Stages, and Tunables 161 Getting Started with Stages: Familiar Operations 163 Expressions 168 $project 169 $unwind 174 Array Expressions 181 Accumulators 186 Using Accumulators in Project Stages 186 Introduction to Grouping 187 The _id Field in Group Stages 192 Group Versus Project 195 Writing Aggregation Pipeline Results to a Collection 198\n\n8. Transactions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 Introduction to Transactions 199 A Definition of ACID 200 How to Use Transactions 200 Tuning Transaction Limits for Your Application 205 Timing and Oplog Size Limits 205\n\n9. Application Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 Schema Design Considerations 207 Schema Design Patterns 208 Normalization Versus Denormalization 211 Examples of Data Representations 212 Cardinality 216 Friends, Followers, and Other Inconveniences 216 Optimizations for Data Manipulation 219 Removing Old Data 219 Planning Out Databases and Collections 220 Managing Consistency 221 Migrating Schemas 222 Managing Schemas 223 When Not to Use MongoDB 223\n\nviii\n\n|\n\nTable of Contents\n\nPart III. Replication\n\n10. Setting Up a Replica Set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 Introduction to Replication 227 Setting Up a Replica Set, Part 1 228 Networking Considerations 229 Security Considerations 230 Setting Up a Replica Set, Part 2 230 Observing Replication 233 Changing Your Replica Set Configuration 238 How to Design a Set 241 How Elections Work 243 Member Configuration Options 244 Priority 244 Hidden Members 245 Election Arbiters 246 Building Indexes 247\n\n11. Components of a Replica Set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 Syncing 249 Initial Sync 251 Replication 253 Handling Staleness 253 Heartbeats 253 Member States 254 Elections 255 Rollbacks 255 When Rollbacks Fail 259\n\n12. Connecting to a Replica Set from Your Application. . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 Client−to−Replica Set Connection Behavior 261 Waiting for Replication on Writes 263 Other Options for “w” 265 Custom Replication Guarantees 265 Guaranteeing One Server per Data Center 265 Guaranteeing a Majority of Nonhidden Members 267 Creating Other Guarantees 267 Sending Reads to Secondaries 268 Consistency Considerations 268\n\nTable of Contents\n\n|\n\nix\n\nLoad Considerations 269 Reasons to Read from Secondaries 269\n\n13. Administration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271 Starting Members in Standalone Mode 271 Replica Set Configuration 272 Creating a Replica Set 272 Changing Set Members 273 Creating Larger Sets 274 Forcing Reconfiguration 274 Manipulating Member State 275 Turning Primaries into Secondaries 275 Preventing Elections 275 Monitoring Replication 275 Getting the Status 276 Visualizing the Replication Graph 279 Replication Loops 280 Disabling Chaining 281 Calculating Lag 281 Resizing the Oplog 282 Building Indexes 283 Replication on a Budget 285\n\nPart IV.\n\nSharding\n\n14.\n\nIntroduction to Sharding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289 What Is Sharding? 289 Understanding the Components of a Cluster 290 Sharding on a Single-Machine Cluster 291\n\n15. Configuring Sharding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303 When to Shard 303 Starting the Servers 304 Config Servers 304 The mongos Processes 305 Adding a Shard from a Replica Set 306 Adding Capacity 310 Sharding Data 310 How MongoDB Tracks Cluster Data 311\n\nx\n\n|\n\nTable of Contents",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 11-18)",
      "start_page": 11,
      "end_page": 18,
      "detection_method": "topic_boundary",
      "content": "Chunk Ranges 312 Splitting Chunks 314 The Balancer 316 Collations 317 Change Streams 317\n\n16. Choosing a Shard Key. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319 Taking Stock of Your Usage 319 Picturing Distributions 320 Ascending Shard Keys 320 Randomly Distributed Shard Keys 323 Location-Based Shard Keys 325 Shard Key Strategies 327 Hashed Shard Key 327 Hashed Shard Keys for GridFS 328 The Firehose Strategy 329 Multi-Hotspot 330 Shard Key Rules and Guidelines 334 Shard Key Limitations 334 Shard Key Cardinality 334 Controlling Data Distribution 334 Using a Cluster for Multiple Databases and Collections 335 Manual Sharding 336\n\n17. Sharding Administration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339 Seeing the Current State 339 Getting a Summary with sh.status() 339 Seeing Configuration Information 341 Tracking Network Connections 348 Getting Connection Statistics 348 Limiting the Number of Connections 354 Server Administration 356 Adding Servers 356 Changing Servers in a Shard 356 Removing a Shard 356 Balancing Data 359 The Balancer 360 Changing Chunk Size 361 Moving Chunks 362 Jumbo Chunks 364 xi\n\nRefreshing Configurations 367\n\nPart V. Application Administration\n\n18. Seeing What Your Application Is Doing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371 Seeing the Current Operations 371 Finding Problematic Operations 374 Killing Operations 375 False Positives 375 Preventing Phantom Operations 375 Using the System Profiler 376 Calculating Sizes 379 Documents 379 Collections 380 Databases 385 Using mongotop and mongostat 386\n\n19. An Introduction to MongoDB Security. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389 MongoDB Authentication and Authorization 389 Authentication Mechanisms 389 Authorization 390 Using x.509 Certificates to Authenticate Both Members and Clients 392 A Tutorial on MongoDB Authentication and Transport Layer Encryption 395 Establish a CA 395 Generate and Sign Member Certificates 400 Generate and Sign Client Certificates 401 Bring Up the Replica Set Without Authentication and Authorization Enabled 401 Create the Admin User 402 Restart the Replica Set with Authentication and Authorization Enabled 403\n\n20. Durability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405 Durability at the Member Level Through Journaling 405 Durability at the Cluster Level Using Write Concern 407 The w and wtimeout Options for writeConcern 407 The j (Journaling) Option for writeConcern 408 Durability at a Cluster Level Using Read Concern 408 Durability of Transactions Using a Write Concern 409 What MongoDB Does Not Guarantee 410\n\nxii\n\n|\n\nTable of Contents\n\nChecking for Corruption 410\n\nPart VI.\n\nServer Administration\n\n21. Setting Up MongoDB in Production. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 Starting from the Command Line 415 File-Based Configuration 419 Stopping MongoDB 420 Security 421 Data Encryption 422 SSL Connections 423 Logging 423\n\n22. Monitoring MongoDB. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425 Monitoring Memory Usage 425 Introduction to Computer Memory 426 Tracking Memory Usage 426 Tracking Page Faults 427 I/O Wait 429 Calculating the Working Set 429 Some Working Set Examples 431 Tracking Performance 431 Tracking Free Space 433 Monitoring Replication 433\n\n23. Making Backups. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437 Backup Methods 437 Backing Up a Server 438 Filesystem Snapshot 438 Copying Data Files 442 Using mongodump 443 Specific Considerations for Replica Sets 446 Specific Considerations for Sharded Clusters 446 Backing Up and Restoring an Entire Cluster 447 Backing Up and Restoring a Single Shard 447\n\n24. Deploying MongoDB. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449 Designing the System 449 Choosing a Storage Medium 449\n\nTable of Contents\n\n|\n\nxiii\n\nRecommended RAID Configurations 450 CPU 451 Operating System 451 Swap Space 452 Filesystem 452 Virtualization 453 Memory Overcommitting 453 Mystery Memory 453 Handling Network Disk I/O Issues 453 Using Non-Networked Disks 455 Configuring System Settings 455 Turning Off NUMA 455 Setting Readahead 457 Disabling Transparent Huge Pages (THP) 458 Choosing a Disk Scheduling Algorithm 458 Disabling Access Time Tracking 459 Modifying Limits 460 Configuring Your Network 461 System Housekeeping 462 Synchronizing Clocks 462 The OOM Killer 463 Turn Off Periodic Tasks 463\n\nA. Installing MongoDB. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465\n\nB. MongoDB Internals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469\n\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475\n\nxiv\n\n|\n\nTable of Contents\n\nPreface\n\nHow This Book Is Organized This book is split up into six sections, covering development, administration, and deployment information.\n\nGetting Started with MongoDB In Chapter 1 we provide background on MongoDB: why it was created, the goals it is trying to accomplish, and why you might choose to use it for a project. We go into more detail in Chapter 2, which provides an introduction to the core concepts and vocabulary of MongoDB. Chapter 2 also provides a first look at working with Mon‐ goDB, getting you started with the database and the shell. The next two chapters cover the basic material that developers need to know to work with MongoDB. In Chapter 3, we describe how to perform those basic write operations, including how to do them with different levels of safety and speed. Chapter 4 explains how to find documents and create complex queries. This chapter also covers how to iterate through results and gives options for limiting, skipping, and sorting results.\n\nDeveloping with MongoDB Chapter 5 covers what indexing is and how to index your MongoDB collections. Chapter 6 explains how to use several special types of indexes and collections. Chap‐ ter 7 covers a number of techniques for aggregating data with MongoDB, including counting, finding distinct values, grouping documents, the aggregation framework, and writing these results to a collection. Chapter 8 introduces transactions: what they are, how best to use them for your application, and how to tune. Finally, this section finishes with a chapter on designing your application: Chapter 9 goes over tips for writing an application that works well with MongoDB.\n\nxv\n\nReplication The replication section starts with Chapter 10, which gives you a quick way to set up a replica set locally and covers many of the available configuration options. Chap‐ ter 11 then covers the various concepts related to replication. Chapter 12 shows how replication interacts with your application and Chapter 13 covers the administrative aspects of running a replica set.\n\nSharding The sharding section starts in Chapter 14 with a quick local setup. Chapter 15 then gives an overview of the components of the cluster and how to set them up. Chap‐ ter 16 has advice on choosing a shard key for a variety of applications. Finally, Chap‐ ter 17 covers administering a sharded cluster.\n\nApplication Administration The next two chapters cover many aspects of MongoDB administration from the per‐ spective of your application. Chapter 18 discusses how to introspect what MongoDB is doing. Chapter 19 covers security in MongoDb and how to configure authentica‐ tion as well as authorization for your deployment. Chapter 20 explains how Mon‐ goDB stores data durably.\n\nServer Administration The final section is focused on server administration. Chapter 21 covers common options when starting and stopping MongoDB. Chapter 22 discusses what to look for and how to read stats when monitoring. Chapter 23 describes how to take and restore backups for each type of deployment. Finally, Chapter 24 discusses a number of sys‐ tem settings to keep in mind when deploying MongoDB.\n\nAppendixes Appendix A explains MongoDB’s versioning scheme and how to install it on Win‐ dows, OS X, and Linux. Appendix B details how MongoDB works internally: its stor‐ age engine, data format, and wire protocol.\n\nConventions Used in This Book The following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, collection names, database names, filenames, and file extensions.\n\nxvi\n\n| Preface\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to program ele‐ ments such as variable or function names, command-line utilities, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nConstant width italic\n\nShows text that should be replaced with user-supplied values or by values deter‐ mined by context.\n\nThis element signifies a tip or suggestion.\n\nThis element signifies a general note.\n\nThis element indicates a warning or caution.\n\nUsing Code Examples Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/mongodb-the-definitive-guide-3e/mongodb-the-definitive-guide-3e.\n\nIf you have a technical question or a problem using the code examples, please send email to bookquestions@oreilly.com.\n\nThis book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of\n\nPreface\n\n|\n\nxvii\n\nexample code from this book into your product’s documentation does require per‐ mission.\n\nWe appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “MongoDB: The Defini‐ tive Guide, Third Edition by Shannon Bradshaw, Eoin Brazil, and Kristina Chodorow (O’Reilly). Copyright 2020 Shannon Bradshaw and Eoin Brazil, 978-1-491-95446-1.”\n\nIf you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.\n\nO’Reilly Online Learning\n\nFor more than 40 years, O’Reilly Media has provided technol‐ ogy and business training, knowledge, and insight to help companies succeed.\n\nOur unique network of experts and innovators share their knowledge and expertise through books, articles, conferences, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in- depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, please visit http://oreilly.com.\n\nHow to Contact Us Please address comments and questions concerning this book to the publisher:\n\nO’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax)\n\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/mongoDB_TDG_3e.\n\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\n\nFor more information about our books, courses, conferences, and news, see our web‐ site at http://www.oreilly.com.\n\nFind us on Facebook: http://facebook.com/oreilly\n\nxviii\n\n| Preface",
      "page_number": 11
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 19-28)",
      "start_page": 19,
      "end_page": 28,
      "detection_method": "topic_boundary",
      "content": "Follow us on Twitter: http://twitter.com/oreillymedia\n\nWatch us on YouTube: http://www.youtube.com/oreillymedia\n\nPreface\n\n|\n\nxix\n\nPART I Introduction to MongoDB\n\nCHAPTER 1 Introduction\n\nMongoDB is a powerful, flexible, and scalable general-purpose database. It combines the ability to scale out with features such as secondary indexes, range queries, sorting, aggregations, and geospatial indexes. This chapter covers the major design decisions that made MongoDB what it is.\n\nEase of Use MongoDB is a document-oriented database, not a relational one. The primary reason for moving away from the relational model is to make scaling out easier, but there are some other advantages as well.\n\nA document-oriented database replaces the concept of a “row” with a more flexible model, the “document.” By allowing embedded documents and arrays, the document- oriented approach makes it possible to represent complex hierarchical relationships with a single record. This fits naturally into the way developers in modern object- oriented languages think about their data.\n\nThere are also no predefined schemas: a document’s keys and values are not of fixed types or sizes. Without a fixed schema, adding or removing fields as needed becomes easier. Generally, this makes development faster as developers can quickly iterate. It is also easier to experiment. Developers can try dozens of models for the data and then choose the best one to pursue.\n\n3\n\nDesigned to Scale Dataset sizes for applications are growing at an incredible pace. Increases in available bandwidth and cheap storage have created an environment where even small-scale applications need to store more data than many databases were meant to handle. A terabyte of data, once an unheard-of amount of information, is now commonplace.\n\nAs the amount of data that developers need to store grows, developers face a difficult decision: how should they scale their databases? Scaling a database comes down to the choice between scaling up (getting a bigger machine) or scaling out (partitioning data across more machines). Scaling up is often the path of least resistance, but it has drawbacks: large machines are often very expensive, and eventually a physical limit is reached where a more powerful machine cannot be purchased at any cost. The alter‐ native is to scale out: to add storage space or increase throughput for read and write operations, buy additional servers, and add them to your cluster. This is both cheaper and more scalable; however, it is more difficult to administer a thousand machines than it is to care for one.\n\nMongoDB was designed to scale out. The document-oriented data model makes it easier to split data across multiple servers. MongoDB automatically takes care of bal‐ ancing data and load across a cluster, redistributing documents automatically and routing reads and writes to the correct machines, as shown in Figure 1-1.\n\nFigure 1-1. Scaling out MongoDB using sharding across multiple servers\n\nThe topology of a MongoDB cluster, or whether there is in fact a cluster rather than a single node at the other end of a database connection, is transparent to the applica‐ tion. This allows developers to focus on programming the application, not scaling it.\n\n4\n\n|\n\nChapter 1: Introduction\n\nLikewise, if the topology of an existing deployment needs to change in order to, for example, scale to support greater load, the application logic can remain the same.\n\nRich with Features… MongoDB is a general-purpose database, so aside from creating, reading, updating, and deleting data, it provides most of the features you would expect from a database management system and many others that set it apart. These include:\n\nIndexing\n\nMongoDB supports generic secondary indexes and provides unique, compound, geospatial, and full-text indexing capabilities as well. Secondary indexes on hier‐ archical structures such as nested documents and arrays are also supported and enable developers to take full advantage of the ability to model in ways that best suit their applications.\n\nAggregation\n\nMongoDB provides an aggregation framework based on the concept of data pro‐ cessing pipelines. Aggregation pipelines allow you to build complex analytics engines by processing data through a series of relatively simple stages on the server side, taking full advantage of database optimizations.\n\nSpecial collection and index types\n\nMongoDB supports time-to-live (TTL) collections for data that should expire at a certain time, such as sessions and fixed-size (capped) collections, for holding recent data, such as logs. MongoDB also supports partial indexes limited to only those documents matching a criteria filter in order to increase efficiency and reduce the amount of storage space required.\n\nFile storage\n\nMongoDB supports an easy-to-use protocol for storing large files and file metadata.\n\nSome features common to relational databases are not present in MongoDB, notably complex joins. MongoDB supports joins in a very limited way through use of the $lookup aggregation operator introduced in the 3.2 release. In the 3.6 release, more complex joins are possible using multiple join conditions as well as unrelated subqu‐ eries. MongoDB’s treatment of joins were architectural decisions to allow for greater scalability, because both of those features are difficult to provide efficiently in a dis‐ tributed system.\n\nRich with Features…\n\n|\n\n5\n\n…Without Sacrificing Speed Performance is a driving objective for MongoDB, and has shaped much of its design. It uses opportunistic locking in its WiredTiger storage engine to maximize concur‐ rency and throughput. It uses as much RAM as it can as its cache and attempts to automatically choose the correct indexes for queries. In short, almost every aspect of MongoDB was designed to maintain high performance.\n\nAlthough MongoDB is powerful, incorporating many features from relational sys‐ tems, it is not intended to do everything that a relational database does. For some functionality, the database server offloads processing and logic to the client side (han‐ dled either by the drivers or by a user’s application code). Its maintenance of this streamlined design is one of the reasons MongoDB can achieve such high performance.\n\nThe Philosophy Throughout this book, we will take the time to note the reasoning or motivation behind particular decisions made in the development of MongoDB. Through those notes we hope to share the philosophy behind MongoDB. The best way to summarize the MongoDB project, however, is by referencing its main focus—to create a full- featured data store that is scalable, flexible, and fast.\n\n6\n\n|\n\nChapter 1: Introduction\n\nCHAPTER 2 Getting Started\n\nMongoDB is powerful but easy to get started with. In this chapter we’ll introduce some of the basic concepts of MongoDB:\n\nA document is the basic unit of data for MongoDB and is roughly equivalent to a row in a relational database management system (but much more expressive).\n\nSimilarly, a collection can be thought of as a table with a dynamic schema.\n\nA single instance of MongoDB can host multiple independent databases, each of which contains its own collections.\n\nEvery document has a special key, \"_id\", that is unique within a collection.\n\nMongoDB is distributed with a simple but powerful tool called the mongo shell. The mongo shell provides built-in support for administering MongoDB instances and manipulating data using the MongoDB query language. It is also a fully func‐ tional JavaScript interpreter that enables users to create and load their own scripts for a variety of purposes.\n\nDocuments At the heart of MongoDB is the document: an ordered set of keys with associated val‐ ues. The representation of a document varies by programming language, but most languages have a data structure that is a natural fit, such as a map, hash, or dictionary. In JavaScript, for example, documents are represented as objects:\n\n{\"greeting\" : \"Hello, world!\"}\n\nThis simple document contains a single key, \"greeting\", with a value of \"Hello, world!\". Most documents will be more complex than this simple one and often will contain multiple key/value pairs:\n\n7\n\n{\"greeting\" : \"Hello, world!\", \"views\" : 3}\n\nAs you can see, values in documents are not just “blobs.” They can be one of several different data types (or even an entire embedded document—see “Embedded Docu‐ ments” on page 19). In this example the value for \"greeting\" is a string, whereas the value for \"views\" is an integer.\n\nThe keys in a document are strings. Any UTF-8 character is allowed in a key, with a few notable exceptions:\n\nKeys must not contain the character \\0 (the null character). This character is used to signify the end of a key.\n\nThe . and $ characters have some special properties and should be used only in certain circumstances, as described in later chapters. In general, they should be considered reserved, and drivers will complain if they are used inappropriately.\n\nMongoDB is type-sensitive and case-sensitive. For example, these documents are distinct:\n\n{\"count\" : 5} {\"count\" : \"5\"}\n\nas are these:\n\n{\"count\" : 5} {\"Count\" : 5}\n\nA final important thing to note is that documents in MongoDB cannot contain dupli‐ cate keys. For example, the following is not a legal document:\n\n{\"greeting\" : \"Hello, world!\", \"greeting\" : \"Hello, MongoDB!\"}\n\nCollections A collection is a group of documents. If a document is the MongoDB analog of a row in a relational database, then a collection can be thought of as the analog to a table.\n\nDynamic Schemas Collections have dynamic schemas. This means that the documents within a single collection can have any number of different “shapes.” For example, both of the follow‐ ing documents could be stored in a single collection:\n\n{\"greeting\" : \"Hello, world!\", \"views\": 3} {\"signoff\": \"Good night, and good luck\"}\n\nNote that the previous documents have different keys, different numbers of keys, and values of different types. Because any document can be put into any collection, the question often arises: “Why do we need separate collections at all?” With no need for\n\n8\n\n|\n\nChapter 2: Getting Started\n\nseparate schemas for different kinds of documents, why should we use more than one collection? There are several good reasons:\n\nKeeping different kinds of documents in the same collection can be a nightmare for developers and admins. Developers need to make sure that each query is only returning documents adhering to a particular schema or that the application code performing a query can handle documents of different shapes. If we’re querying for blog posts, it’s a hassle to weed out documents containing author data.\n\nIt’s much faster to get a list of collections than to extract a list of the types of documents in a collection. For example, if we had a \"type\" field in each docu‐ ment that specified whether the document was a “skim,” “whole,” or “chunky monkey,” it would be much slower to find those three values in a single collection than to have three separate collections and query the correct collection.\n\nGrouping documents of the same kind together in the same collection allows for data locality. Getting several blog posts from a collection containing only posts will likely require fewer disk seeks than getting the same posts from a collection containing posts and author data.\n\nWe begin to impose some structure on our documents when we create indexes. (This is especially true in the case of unique indexes.) These indexes are defined per collection. By putting only documents of a single type into the same collec‐ tion, we can index our collections more efficiently.\n\nThere are sound reasons for creating a schema and for grouping related types of documents together. While not required by default, defining schemas for your appli‐ cation is good practice and can be enforced through the use of MongoDB’s documen‐ tation validation functionality and object–document mapping libraries available for many programming languages.\n\nNaming A collection is identified by its name. Collection names can be any UTF-8 string, with a few restrictions:\n\nThe empty string (\"\") is not a valid collection name. • Collection names may not contain the character \\0 (the null character), because this delineates the end of a collection name.\n\nYou should not create any collections with names that start with system., a prefix reserved for internal collections. For example, the system.users collection con‐ tains the database’s users, and the system.namespaces collection contains informa‐ tion about all of the database’s collections.\n\nCollections\n\n|\n\n9\n\nUser-created collections should not contain the reserved character $ in their names. The various drivers available for the database do support using $ in col‐ lection names because some system-generated collections contain it, but you should not use $ in a name unless you are accessing one of these collections.\n\nSubcollections\n\nOne convention for organizing collections is to use namespaced subcollections sepa‐ rated by the . character. For example, an application containing a blog might have a collection named blog.posts and a separate collection named blog.authors. This is for organizational purposes only—there is no relationship between the blog collection (it doesn’t even have to exist) and its “children.”\n\nAlthough subcollections do not have any special properties, they are useful and are incorporated into many MongoDB tools. For instance:\n\nGridFS, a protocol for storing large files, uses subcollections to store file meta‐ data separately from content chunks (see Chapter 6 for more information about GridFS).\n\nMost drivers provide some syntactic sugar for accessing a subcollection of a given collection. For example, in the database shell, db.blog will give you the blog col‐ lection, and db.blog.posts will give you the blog.posts collection.\n\nSubcollections are a good way to organize data in MongoDB for many use cases.\n\nDatabases In addition to grouping documents by collection, MongoDB groups collections into databases. A single instance of MongoDB can host several databases, each grouping together zero or more collections. A good rule of thumb is to store all data for a single application in the same database. Separate databases are useful when storing data for several applications or users on the same MongoDB server.\n\nLike collections, databases are identified by name. Database names can be any UTF-8 string, with the following restrictions:\n\nThe empty string (“”) is not a valid database name.\n\nA database name cannot contain any of these characters: /, \\, ., \", *, <, >, :, |, ?, $, (a single space), or \\0 (the null character). Basically, stick with alphanumeric ASCII.\n\nDatabase names are case-insensitive.\n\nDatabase names are limited to a maximum of 64 bytes.\n\n10\n\n|\n\nChapter 2: Getting Started",
      "page_number": 19
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 29-36)",
      "start_page": 29,
      "end_page": 36,
      "detection_method": "topic_boundary",
      "content": "Historically, prior to the use of the WiredTiger storage engine, database names became files on your filesystem. It is no longer the case. This explains why many of the previous restrictions exist in the first place.\n\nThere are also some reserved database names, which you can access but which have special semantics. These are as follows:\n\nadmin\n\nThe admin database plays a role in authentication and authorization. In addition, access to this database is required for some administrative operations. See Chap‐ ter 19 for more information about the admin database.\n\nlocal\n\nThis database stores data specific to a single server. In replica sets, local stores data used in the replication process. The local database itself is never replicated. (See Chapter 10 for more information about replication and the local database.)\n\nconfig\n\nSharded MongoDB clusters (see Chapter 14) use the config database to store information about each shard.\n\nBy concatenating a database name with a collection in that database you can get a fully qualified collection name, which is called a namespace. For instance, if you are using the blog.posts collection in the cms database, the namespace of that collection would be cms.blog.posts. Namespaces are limited to 120 bytes in length and, in prac‐ tice, should be fewer than 100 bytes long. For more on namespaces and the internal representation of collections in MongoDB, see Appendix B.\n\nGetting and Starting MongoDB To start the server, run the mongod executable in the Unix command-line environ‐ ment of your choice:\n\n$ mongod 2016-04-27T22:15:55.871-0400 I CONTROL [initandlisten] MongoDB starting : pid=8680 port=27017 dbpath=/data/db 64-bit host=morty 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] db version v4.2.0 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] git version: 34e65e5383f7ea1726332cb175b73077ec4a1b02 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] allocator: system 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] modules: none 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] build environment: 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] distarch: x86_64 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] target_arch: x86_64 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] options: {} 2016-04-27T22:15:55.889-0400 I JOURNAL [initandlisten] journal dir=/data/db/journal 2016-04-27T22:15:55.889-0400 I JOURNAL [initandlisten] recover :\n\nGetting and Starting MongoDB\n\n|\n\n11\n\nno journal files present, no recovery needed 2016-04-27T22:15:55.909-0400 I JOURNAL [durability] Durability thread started 2016-04-27T22:15:55.909-0400 I JOURNAL [journal writer] Journal writer thread started 2016-04-27T22:15:55.909-0400 I CONTROL [initandlisten] 2016-04-27T22:15:56.777-0400 I NETWORK [HostnameCanonicalizationWorker] Starting hostname canonicalization worker 2016-04-27T22:15:56.778-0400 I FTDC [initandlisten] Initializing full-time diagnostic data capture with directory '/data/db/diagnostic.data' 2016-04-27T22:15:56.779-0400 I NETWORK [initandlisten] waiting for connections on port 27017\n\nIf you’re on Windows, run this:\n\n> mongod.exe\n\nFor detailed information on installing MongoDB on your system, see Appendix A or the appropriate installation tutorial in the Mon‐ goDB documentation.\n\nWhen run with no arguments, mongod will use the default data directory, /data/db/ (or \\data\\db\\ on the current volume on Windows). If the data directory does not already exist or is not writable, the server will fail to start. It is important to create the data directory (e.g., mkdir -p /data/db/) and to make sure your user has permission to write to the directory before starting MongoDB.\n\nOn startup, the server will print some version and system information and then begin waiting for connections. By default MongoDB listens for socket connections on port 27017. The server will fail to start if that port is not available—the most common cause of this is another instance of MongoDB that is already running.\n\nYou should always secure your mongod instances. See Chapter 19 for more information on securing MongoDB.\n\nYou can safely stop mongod by typing Ctrl-C in the command-line-environment from which you launched the mongod server.\n\nFor more information on starting or stopping MongoDB, see Chapter 21.\n\n12\n\n|\n\nChapter 2: Getting Started\n\nIntroduction to the MongoDB Shell MongoDB comes with a JavaScript shell that allows interaction with a MongoDB instance from the command line. The shell is useful for performing administrative functions, inspecting a running instance, or just exploring MongoDB. The mongo shell is a crucial tool for using MongoDB. We’ll use it extensively throughout the rest of the text.\n\nRunning the Shell To start the shell, run the mongo executable:\n\n$ mongo MongoDB shell version: 4.2.0 connecting to: test >\n\nThe shell automatically attempts to connect to a MongoDB server running on the local machine on startup, so make sure you start mongod before starting the shell.\n\nThe shell is a full-featured JavaScript interpreter, capable of running arbitrary Java‐ Script programs. To illustrate this, let’s perform some basic math:\n\n> x = 200; 200 > x / 5; 40\n\nWe can also leverage all of the standard JavaScript libraries:\n\n> Math.sin(Math.PI / 2); 1 > new Date(\"20109/1/1\"); ISODate(\"2019-01-01T05:00:00Z\") > \"Hello, World!\".replace(\"World\", \"MongoDB\"); Hello, MongoDB!\n\nWe can even define and call JavaScript functions:\n\n> function factorial (n) { ... if (n <= 1) return 1; ... return n * factorial(n - 1); ... } > factorial(5); 120\n\nNote that you can create multiline commands. The shell will detect whether the Java‐ Script statement is complete when you press Enter. If the statement is not complete, the shell will allow you to continue writing it on the next line. Pressing Enter three times in a row will cancel the half-formed command and get you back to the > prompt.\n\nIntroduction to the MongoDB Shell\n\n|\n\n13\n\nA MongoDB Client Although the ability to execute arbitrary JavaScript is useful, the real power of the shell lies in the fact that it is also a standalone MongoDB client. On startup, the shell connects to the test database on a MongoDB server and assigns this database connec‐ tion to the global variable db. This variable is the primary access point to your Mon‐ goDB server through the shell.\n\nTo see the database to which db is currently assigned, type in db and hit Enter:\n\n> db test\n\nThe shell contains some add-ons that are not valid JavaScript syntax but were imple‐ mented because of their familiarity to users of SQL shells. The add-ons do not pro‐ vide any extra functionality, but they are nice syntactic sugar. For instance, one of the most important operations is selecting which database to use:\n\n> use video switched to db video\n\nNow if you look at the db variable, you can see that it refers to the video database:\n\n> db video\n\nBecause this is a JavaScript shell, typing a variable name will cause the name to be evaluated as an expression. The value (in this case, the database name) is then printed.\n\nYou may access collections from the db variable. For example:\n\n> db.movies\n\nreturns the movies collection in the current database. Now that we can access a collec‐ tion in the shell, we can perform almost any database operation.\n\nBasic Operations with the Shell We can use the four basic operations, create, read, update, and delete (CRUD), to manipulate and view data in the shell.\n\nCreate\n\nThe insertOne function adds a document to a collection. For example, suppose we want to store a movie. First, we’ll create a local variable called movie that is a Java‐ Script object representing our document. It will have the keys \"title\", \"director\", and \"year\" (the year it was released):\n\n14\n\n|\n\nChapter 2: Getting Started\n\n> movie = {\"title\" : \"Star Wars: Episode IV - A New Hope\", ... \"director\" : \"George Lucas\", ... \"year\" : 1977} {\n\n\"title\" : \"Star Wars: Episode IV - A New Hope\", \"director\" : \"George Lucas\", \"year\" : 1977\n\n}\n\nThis object is a valid MongoDB document, so we can save it to the movies collection using the insertOne method:\n\n> db.movies.insertOne(movie) {\n\n\"acknowledged\" : true, \"insertedId\" : ObjectId(\"5721794b349c32b32a012b11\")\n\n}\n\nThe movie has been saved to the database. We can see it by calling find on the collection:\n\n> db.movies.find().pretty() {\n\n\"_id\" : ObjectId(\"5721794b349c32b32a012b11\"), \"title\" : \"Star Wars: Episode IV - A New Hope\", \"director\" : \"George Lucas\", \"year\" : 1977\n\n}\n\nWe can see that an \"_id\" key was added and that the other key/value pairs were saved as we entered them. The reason for the sudden appearance of the \"_id\" field is explained at the end of this chapter.\n\nRead\n\nfind and findOne can be used to query a collection. If we just want to see one docu‐ ment from a collection, we can use findOne:\n\n> db.movies.findOne() {\n\n\"_id\" : ObjectId(\"5721794b349c32b32a012b11\"), \"title\" : \"Star Wars: Episode IV - A New Hope\", \"director\" : \"George Lucas\", \"year\" : 1977\n\n}\n\nfind and findOne can also be passed criteria in the form of a query document. This will restrict the documents matched by the query. The shell will automatically display up to 20 documents matching a find, but more can be fetched. (See Chapter 4 for more information on querying.)\n\nIntroduction to the MongoDB Shell\n\n|\n\n15\n\nUpdate\n\nIf we would like to modify our post, we can use updateOne. updateOne takes (at least) two parameters: the first is the criteria to find which document to update, and the second is a document describing the updates to make. Suppose we decide to enable reviews for the movie we created earlier. We’ll need to add an array of reviews as the value for a new key in our document.\n\nTo perform the update, we’ll need to use an update operator, set:\n\n> db.movies.updateOne({title : \"Star Wars: Episode IV - A New Hope\"}, ... {$set : {reviews: []}}) WriteResult({\"nMatched\": 1, \"nUpserted\": 0, \"nModified\": 1})\n\nNow the document has a \"reviews\" key. If we call find again, we can see the new key:\n\n> db.movies.find().pretty() {\n\n\"_id\" : ObjectId(\"5721794b349c32b32a012b11\"), \"title\" : \"Star Wars: Episode IV - A New Hope\", \"director\" : \"George Lucas\", \"year\" : 1977, \"reviews\" : [ ]\n\n}\n\nSee “Updating Documents” on page 35 for detailed information on updating documents.\n\nDelete\n\ndeleteOne and deleteMany permanently delete documents from the database. Both methods take a filter document specifying criteria for the removal. For example, this would remove the movie we just created:\n\n> db.movies.deleteOne({title : \"Star Wars: Episode IV - A New Hope\"})\n\nUse deleteMany to delete all documents matching a filter.\n\nData Types The beginning of this chapter covered the basics of what a document is. Now that you are up and running with MongoDB and can try things in the shell, this section will dive a little deeper. MongoDB supports a wide range of data types as values in docu‐ ments. In this section, we’ll outline all the supported types.\n\nBasic Data Types Documents in MongoDB can be thought of as “JSON-like” in that they are conceptu‐ ally similar to objects in JavaScript. JSON is a simple representation of data: the speci‐\n\n16\n\n|\n\nChapter 2: Getting Started\n\nfication can be described in about one paragraph (the website proves it) and lists only six data types. This is a good thing in many ways: it’s easy to understand, parse, and remember. On the other hand, JSON’s expressive capabilities are limited because the only types are null, boolean, numeric, string, array, and object.\n\nAlthough these types allow for an impressive amount of expressivity, there are a cou‐ ple of additional types that are crucial for most applications, especially when working with a database. For example, JSON has no date type, which makes working with dates even more annoying than it usually is. There is a number type, but only one— there is no way to differentiate floats and integers, never mind any distinction between 32-bit and 64-bit numbers. There is no way to represent other commonly used types, either, such as regular expressions or functions.\n\nMongoDB adds support for a number of additional data types while keeping JSON’s essential key/value–pair nature. Exactly how values of each type are represented varies by language, but this is a list of the commonly supported types and how they are represented as part of a document in the shell. The most common types are:\n\nNull\n\nThe null type can be used to represent both a null value and a nonexistent field:\n\n{\"x\" : null}\n\nBoolean\n\nThere is a boolean type, which can be used for the values true and false:\n\n{\"x\" : true}\n\nNumber\n\nThe shell defaults to using 64-bit floating-point numbers. Thus, these numbers both look “normal” in the shell:\n\n{\"x\" : 3.14}\n\n{\"x\" : 3}\n\nFor integers, use the NumberInt or NumberLong classes, which represent 4-byte or 8-byte signed integers, respectively.\n\n{\"x\" : NumberInt(\"3\")} {\"x\" : NumberLong(\"3\")}\n\nString\n\nAny string of UTF-8 characters can be represented using the string type:\n\n{\"x\" : \"foobar\"}\n\nDate\n\nMongoDB stores dates as 64-bit integers representing milliseconds since the Unix epoch (January 1, 1970). The time zone is not stored:\n\nData Types\n\n|\n\n17\n\n{\"x\" : new Date()}\n\nRegular expression\n\nQueries can use regular expressions using JavaScript’s regular expression syntax:\n\n{\"x\" : /foobar/i}\n\nArray\n\nSets or lists of values can be represented as arrays:\n\n{\"x\" : [\"a\", \"b\", \"c\"]}\n\nEmbedded document\n\nDocuments can contain entire documents embedded as values in a parent document:\n\n{\"x\" : {\"foo\" : \"bar\"}}\n\nObject ID\n\nAn object ID is a 12-byte ID for documents:\n\n{\"x\" : ObjectId()}\n\nSee the section “_id and ObjectIds” on page 20 for details.\n\nThere are also a few less common types that you may need, including:\n\nBinary data\n\nBinary data is a string of arbitrary bytes. It cannot be manipulated from the shell. Binary data is the only way to save non-UTF-8 strings to the database.\n\nCode\n\nMongoDB also makes it possible to store arbitrary JavaScript in queries and documents:\n\n{\"x\" : function() { /* ... */ }}\n\nFinally, there are a few types that are mostly used internally (or superseded by other types). These will be described in the text as needed.\n\nFor more information on MongoDB’s data format, see Appendix B.\n\nDates In JavaScript, the Date class is used for MongoDB’s date type. When creating a new Date object, always call new Date(), not just Date(). Calling the constructor as a function (i.e., not including new) returns a string representation of the date, not an actual Date object. This is not MongoDB’s choice; it is how JavaScript works. If you are not careful to always use the Date constructor, you can end up with a mishmash of strings and dates. Strings do not match dates and vice versa, so this can cause prob‐ lems with removing, updating, querying…pretty much everything.\n\n18\n\n|\n\nChapter 2: Getting Started",
      "page_number": 29
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 37-44)",
      "start_page": 37,
      "end_page": 44,
      "detection_method": "topic_boundary",
      "content": "For a full explanation of JavaScript’s Date class and acceptable formats for the con‐ structor, see section 15.9 of the ECMAScript specification.\n\nDates in the shell are displayed using local time zone settings. However, dates in the database are just stored as milliseconds since the epoch, so they have no time zone information associated with them. (Time zone information could, of course, be stored as the value for another key.)\n\nArrays Arrays are values that can be used interchangeably for both ordered operations (as though they were lists, stacks, or queues) and unordered operations (as though they were sets).\n\nIn the following document, the key \"things\" has an array value:\n\n{\"things\" : [\"pie\", 3.14]}\n\nAs you can see from this example, arrays can contain different data types as values (in this case, a string and a floating-point number). In fact, array values can be any of the supported value types for normal key/value pairs, even nested arrays.\n\nOne of the great things about arrays in documents is that MongoDB “understands” their structure and knows how to reach inside of arrays to perform operations on their contents. This allows us to query on arrays and build indexes using their con‐ tents. For instance, in the previous example, MongoDB can query for all documents where 3.14 is an element of the \"things\" array. If this is a common query, you can even create an index on the \"things\" key to improve the query’s speed.\n\nMongoDB also allows atomic updates that modify the contents of arrays, such as reaching into the array and changing the value \"pie\" to pi. We’ll see more examples of these types of operations throughout the text.\n\nEmbedded Documents A document can be used as the value for a key. This is called an embedded document. Embedded documents can be used to organize data in a more natural way than just a flat structure of key/value pairs.\n\nFor example, if we have a document representing a person and want to store that per‐ son’s address, we can nest this information in an embedded \"address\" document:\n\n{ \"name\" : \"John Doe\", \"address\" : { \"street\" : \"123 Park Street\", \"city\" : \"Anytown\", \"state\" : \"NY\"\n\nData Types\n\n|\n\n19\n\n} }\n\nThe value for the \"address\" key in this example is an embedded document with its own key/value pairs for \"street\", \"city\", and \"state\".\n\nAs with arrays, MongoDB “understands” the structure of embedded documents and is able to reach inside them to build indexes, perform queries, or make updates.\n\nWe’ll discuss schema design in-depth later, but even from this basic example we can begin to see how embedded documents can change the way we work with data. In a relational database, the previous document would probably be modeled as two sepa‐ rate rows in two different tables (people and addresses). With MongoDB we can embed the \"address\" document directly within the \"person\" document. Thus, when used properly, embedded documents can provide a more natural representation of information.\n\nThe flip side of this is that there can be more data repetition with MongoDB. Suppose addresses was a separate table in a relational database and we needed to fix a typo in an address. When we did a join with people and addresses, we’d get the updated address for everyone who shares it. With MongoDB, we’d need to fix the typo in each person’s document.\n\n_id and ObjectIds Every document stored in MongoDB must have an \"_id\" key. The \"_id\" key’s value can be any type, but it defaults to an ObjectId. In a single collection, every document must have a unique value for \"_id\", which ensures that every document in a collec‐ tion can be uniquely identified. That is, if you had two collections, each one could have a document where the value for \"_id\" was 123. However, neither collection could contain more than one document with an \"_id\" of 123.\n\nObjectIds\n\nObjectId is the default type for \"_id\". The ObjectId class is designed to be light‐ weight, while still being easy to generate in a globally unique way across different machines. MongoDB’s distributed nature is the main reason why it uses ObjectIds as opposed to something more traditional, like an autoincrementing primary key: it is difficult and time-consuming to synchronize autoincrementing primary keys across multiple servers. Because MongoDB was designed to be a distributed database, it was important to be able to generate unique identifiers in a sharded environment.\n\nObjectIds use 12 bytes of storage, which gives them a string representation that is 24 hexadecimal digits: 2 digits for each byte. This causes them to appear larger than they are, which makes some people nervous. It’s important to note that even though an\n\n20\n\n|\n\nChapter 2: Getting Started\n\nObjectId is often represented as a giant hexadecimal string, the string is actually twice as long as the data being stored.\n\nIf you create multiple new ObjectIds in rapid succession, you can see that only the last few digits change each time. In addition, a couple of digits in the middle of the ObjectId will change if you space the creations out by a couple of seconds. This is because of the manner in which ObjectIds are created. The 12 bytes of an ObjectId are generated as follows:\n\n0 2 Timestamp Random Counter (random start value)\n\n1\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\nThe first four bytes of an ObjectId are a timestamp in seconds since the epoch. This provides a couple of useful properties:\n\nThe timestamp, when combined with the next five bytes (which will be described in a moment), provides uniqueness at the granularity of a second.\n\nBecause the timestamp comes first, ObjectIds will sort in rough insertion order. This is not a strong guarantee but does have some nice properties, such as mak‐ ing ObjectIds efficient to index.\n\nIn these four bytes exists an implicit timestamp of when each document was cre‐ ated. Most drivers expose a method for extracting this information from an ObjectId.\n\nBecause the current time is used in ObjectIds, some users worry that their servers will need to have synchronized clocks. Although synchronized clocks are a good idea for other reasons (see “Synchronizing Clocks” on page 462), the actual timestamp doesn’t matter to ObjectIds, only that it is often new (once per second) and increasing.\n\nThe next five bytes of an ObjectId are a random value. The final three bytes are a counter that starts with a random value to avoid generating colliding ObjectIds on different machines.\n\nThese first nine bytes of an ObjectId therefore guarantee its uniqueness across machines and processes for a single second. The last three bytes are simply an incre‐ menting counter that is responsible for uniqueness within a second in a single pro‐ cess. This allows for up to 2563 (16,777,216) unique ObjectIds to be generated per process in a single second.\n\nData Types\n\n|\n\n21\n\nAutogeneration of _id\n\nAs stated earlier, if there is no \"_id\" key present when a document is inserted, one will be automatically added to the inserted document. This can be handled by the MongoDB server but will generally be done by the driver on the client side.\n\nUsing the MongoDB Shell This section covers how to use the shell as part of your command-line toolkit, cus‐ tomize it, and use some of its more advanced functionality.\n\nAlthough we connected to a local mongod instance above, you can connect your shell to any MongoDB instance that your machine can reach. To connect to a mongod on a different machine or port, specify the hostname, port, and database when starting the shell:\n\n$ mongo some-host:30000/myDB MongoDB shell version: 4.2.0 connecting to: some-host:30000/myDB >\n\ndb will now refer to some-host:30000’s myDB database.\n\nSometimes it is handy to not connect to a mongod at all when starting the mongo shell. If you start the shell with --nodb, it will start up without attempting to connect to anything:\n\n$ mongo --nodb MongoDB shell version: 4.2.0 >\n\nOnce started, you can connect to a mongod at your leisure by running new Mongo(\"hostname\"):\n\n> conn = new Mongo(\"some-host:30000\") connection to some-host:30000 > db = conn.getDB(\"myDB\") myDB\n\nAfter these two commands, you can use db normally. You can use these commands to connect to a different database or server at any time.\n\nTips for Using the Shell Because mongo is simply a JavaScript shell, you can get a great deal of help for it by simply looking up JavaScript documentation online. For MongoDB-specific function‐ ality, the shell includes built-in help that can be accessed by typing help:\n\n22\n\n|\n\nChapter 2: Getting Started\n\n> help db.help() help on db methods db.mycoll.help() help on collection methods sh.help() sharding helpers ...\n\nshow dbs show database names show collections show collections in current database show users show users in current database ...\n\nDatabase-level help db.foo.help().\n\nis provided by db.help() and collection-level help by\n\nA good way of figuring out what a function is doing is to type it without the paren‐ theses. This will print the JavaScript source code for the function. For example, if you are curious about how the update function works or cannot remember the order of parameters, you can do the following:\n\n> db.movies.updateOne function (filter, update, options) { var opts = Object.extend({}, options || {});\n\n// Check if first key in update statement contains a $ var keys = Object.keys(update); if (keys.length == 0) { throw new Error(\"the update operation document must contain at least one atomic operator\"); } ...\n\nRunning Scripts with the Shell In addition to using the shell interactively, you can also pass the shell JavaScript files to execute. Simply pass in your scripts at the command line:\n\n$ mongo script1.js script2.js script3.js MongoDB shell version: 4.2.1 connecting to: mongodb://127.0.0.1:27017 MongoDB server version: 4.2.1\n\nloading file: script1.js I am script1.js loading file: script2.js I am script2.js loading file: script3.js I am script3.js ...\n\nThe mongo shell will execute each script listed and exit.\n\nUsing the MongoDB Shell\n\n|\n\n23\n\nIf you want to run a script using a connection to a nondefault host/port mongod, specify the address first, then the script(s):\n\n$ mongo server-1:30000/foo --quiet script1.js script2.js script3.js\n\nThis would execute the three scripts with db set to the foo database on server-1:30000.\n\nYou can print to stdout in scripts (as the preceding scripts did) using the print func‐ tion. This allows you to use the shell as part of a pipeline of commands. If you’re planning to pipe the output of a shell script to another command, use the --quiet option to prevent the “MongoDB shell version v4.2.0” banner from printing.\n\nYou can also run scripts from within the interactive shell using the load function:\n\n> load(\"script1.js\") I am script1.js true >\n\nScripts have access to the db variable (as well as any other global). However, shell helpers such as use db or show collections do not work from files. There are valid JavaScript equivalents to each of these, as shown in Table 2-1.\n\nTable 2-1. JavaScript equivalents to shell helpers\n\nHelper use video\n\nEquivalent db.getSisterDB(\"video\")\n\nshow dbs\n\ndb.getMongo().getDBs()\n\nshow collections db.getCollectionNames()\n\nYou can also use scripts to inject variables into the shell. For example, you could have a script that simply initializes helper functions that you commonly use. The following script, for instance, may be helpful for Part III and Part IV. It defines a function, con nectTo, that connects to the locally running database on the given port and sets db to that connection:\n\n// defineConnectTo.js\n\n/** * Connect to a database and set db. */ var connectTo = function(port, dbname) { if (!port) { port = 27017; }\n\nif (!dbname) { dbname = \"test\"; }\n\n24\n\n|\n\nChapter 2: Getting Started\n\ndb = connect(\"localhost:\"+port+\"/\"+dbname); return db; };\n\nIf you load this script in the shell, connectTo is now defined:\n\n> typeof connectTo undefined > load('defineConnectTo.js') > typeof connectTo function\n\nIn addition to adding helper functions, you can use scripts to automate common tasks and administrative activities.\n\nBy default, the shell will look in the directory that you started the shell in (use pwd() to see what directory that is). If the script is not in your current directory, you can give the shell a relative or absolute path to it. For example, if you wanted to put your shell scripts in ~/my-scripts, you could load defineConnectTo.js with load(\"/home/ myUser/my-scripts/defineConnectTo.js\"). Note that load cannot resolve ~.\n\nYou can use run to run command-line programs from the shell. You can pass argu‐ ments to the function as parameters:\n\n> run(\"ls\", \"-l\", \"/home/myUser/my-scripts/\") sh70352| -rw-r--r-- 1 myUser myUser 2012-12-13 13:15 defineConnectTo.js sh70532| -rw-r--r-- 1 myUser myUser 2013-02-22 15:10 script1.js sh70532| -rw-r--r-- 1 myUser myUser 2013-02-22 15:12 script2.js sh70532| -rw-r--r-- 1 myUser myUser 2013-02-22 15:13 script3.js\n\nThis is of limited use, generally, as the output is formatted oddly and it doesn’t sup‐ port pipes.\n\nCreating a .mongorc.js If you have frequently loaded scripts, you might want to put them in your .mongorc.js file. This file is run whenever you start up the shell.\n\nFor example, suppose you would like the shell to greet you when you log in. Create a file called .mongorc.js in your home directory, and then add the following lines to it:\n\n// .mongorc.js\n\nvar compliment = [\"attractive\", \"intelligent\", \"like Batman\"]; var index = Math.floor(Math.random()*3);\n\nprint(\"Hello, you're looking particularly \"+compliment[index]+\" today!\");\n\nThen, when you start the shell, you’ll see something like:\n\nUsing the MongoDB Shell\n\n|\n\n25\n\n$ mongo MongoDB shell version: 4.2.1 connecting to: test Hello, you're looking particularly like Batman today! >\n\nMore practically, you can use this script to set up any global variables you’d like to use, alias long names to shorter ones, and override built-in functions. One of the most common uses for .mongorc.js is to remove some of the more “dangerous” shell helpers. You can override functions like dropDatabase or deleteIndexes with no- ops or undefine them altogether:\n\nvar no = function() { print(\"Not on my watch.\"); };\n\n// Prevent dropping databases db.dropDatabase = DB.prototype.dropDatabase = no;\n\n// Prevent dropping collections DBCollection.prototype.drop = no;\n\n// Prevent dropping an index DBCollection.prototype.dropIndex = no;\n\n// Prevent dropping indexes DBCollection.prototype.dropIndexes = no;\n\nNow if you try to call any of these functions, it will simply print an error message. Note that this technique does not protect you against malicious users; it can only help with fat-fingering.\n\nYou can disable loading your .mongorc.js by using the --norc option when starting the shell.\n\nCustomizing Your Prompt The default shell prompt can be overridden by setting the prompt variable to either a string or a function. For example, if you are running a query that takes minutes to complete, you may want to have a prompt that displays the current time so you can see when the last operation finished:\n\nprompt = function() { return (new Date())+\"> \"; };\n\nAnother handy prompt might show the current database you’re using:\n\n26\n\n|\n\nChapter 2: Getting Started",
      "page_number": 37
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 45-54)",
      "start_page": 45,
      "end_page": 54,
      "detection_method": "topic_boundary",
      "content": "prompt = function() { if (typeof db == 'undefined') { return '(nodb)> '; }\n\n// Check the last db operation try { db.runCommand({getLastError:1}); } catch (e) { print(e); }\n\nreturn db+\"> \"; };\n\nNote that prompt functions should return strings and be very cautious about catching exceptions: it can be extremely confusing if your prompt turns into an exception!\n\nIn general, your prompt function should include a call to getLastError. This catches errors on writes and reconnects you automatically if the shell gets disconnected (e.g., if you restart mongod).\n\nThe .mongorc.js file is a good place to set your prompt if you want to always use a custom one (or set up a couple of custom prompts that you can switch between in the shell).\n\nEditing Complex Variables The multiline support in the shell is somewhat limited: you cannot edit previous lines, which can be annoying when you realize that the first line has a typo and you’re currently working on line 15. Thus, for larger blocks of code or objects, you may want to edit them in an editor. To do so, set the EDITOR variable in the shell (or in your environment, but since you’re already in the shell…):\n\n> EDITOR=\"/usr/bin/emacs\"\n\nNow, if you want to edit a variable, you can say edit varname—for example:\n\n> var wap = db.books.findOne({title: \"War and Peace\"}); > edit wap\n\nWhen you’re done making changes, save and exit the editor. The variable will be parsed and loaded back into the shell.\n\nAdd EDITOR=\"/path/to/editor\"; to your .mongorc.js file and you won’t have to worry about setting it again.\n\nUsing the MongoDB Shell\n\n|\n\n27\n\nInconvenient Collection Names Fetching a collection with the db.collectionName syntax almost always works, unless the collection name is a reserved word or is an invalid JavaScript property name.\n\nFor example, suppose we are trying to access the version collection. We cannot say db.version because db.version is a method on db (it returns the version of the run‐ ning MongoDB server):\n\n> db.version function () { return this.serverBuildInfo().version; }\n\nTo actually access the version collection, you must use the getCollection function:\n\n> db.getCollection(\"version\"); test.version\n\nThis can also be used for collection names with characters that aren’t valid JavaScript property names, such as foo-bar-baz and 123abc (JavaScript property names can only contain letters, numbers, $ and _, and cannot start with a number).\n\nAnother way of getting around invalid properties is to use array-access syntax. In JavaScript, x.y is identical to x['y']. This means that subcollections can be accessed using variables, not just literal names. Thus, if you needed to perform some operation on every blog subcollection, you could iterate through them with something like this:\n\nvar collections = [\"posts\", \"comments\", \"authors\"];\n\nfor (var i in collections) { print(db.blog[collections[i]]); }\n\ninstead of this:\n\nprint(db.blog.posts); print(db.blog.comments); print(db.blog.authors);\n\nNote that you cannot do db.blog.i, which would be interpreted as test.blog.i, not test.blog.posts. You must use the db.blog[i] syntax for i to be interpreted as a variable.\n\nYou can use this technique to access awkwardly named collections:\n\n> var name = \"@#&!\" > db[name].find()\n\nAttempting to query db.@#&! would be illegal, but db[name] would work.\n\n28\n\n|\n\nChapter 2: Getting Started\n\nCHAPTER 3 Creating, Updating, and Deleting Documents\n\nThis chapter covers the basics of moving data into and out of the database, including the following:\n\nAdding new documents to a collection\n\nRemoving documents from a collection\n\nUpdating existing documents\n\nChoosing the correct level of safety versus speed for all of these operations\n\nInserting Documents Inserts are the basic method for adding data to MongoDB. To insert a single docu‐ ment, use the collection’s insertOne method:\n\n> db.movies.insertOne({\"title\" : \"Stand by Me\"})\n\ninsertOne will add an \"_id\" key to the document (if you do not supply one) and store the document in MongoDB.\n\ninsertMany If you need to insert multiple documents into a collection, you can use insertMany. This method enables you to pass an array of documents to the database. This is far more efficient because your code will not make a round trip to the database for each document inserted, but will insert them in bulk.\n\nIn the shell, you can try this out as follows:\n\n29\n\n> db.movies.drop() true > db.movies.insertMany([{\"title\" : \"Ghostbusters\"}, ... {\"title\" : \"E.T.\"}, ... {\"title\" : \"Blade Runner\"}]); { \"acknowledged\" : true, \"insertedIds\" : [ ObjectId(\"572630ba11722fac4b6b4996\"), ObjectId(\"572630ba11722fac4b6b4997\"), ObjectId(\"572630ba11722fac4b6b4998\") ] } > db.movies.find() { \"_id\" : ObjectId(\"572630ba11722fac4b6b4996\"), \"title\" : \"Ghostbusters\" } { \"_id\" : ObjectId(\"572630ba11722fac4b6b4997\"), \"title\" : \"E.T.\" } { \"_id\" : ObjectId(\"572630ba11722fac4b6b4998\"), \"title\" : \"Blade Runner\" }\n\nSending dozens, hundreds, or even thousands of documents at a time can make inserts significantly faster.\n\ninsertMany is useful if you are inserting multiple documents into a single collection. If you are just importing raw data (e.g., from a data feed or MySQL), there are command-line tools like mongoimport that can be used instead of a batch insert. On the other hand, it is often handy to munge data before saving it to MongoDB (con‐ verting dates to the date type or adding a custom \"_id\", for example). In such cases insertMany can be used for importing data, as well.\n\nCurrent versions of MongoDB do not accept messages longer than 48 MB, so there is a limit to how much can be inserted in a single batch insert. If you attempt to insert more than 48 MB, many drivers will split up the batch insert into multiple 48 MB batch inserts. Check your driver documentation for details.\n\nWhen performing a bulk insert using insertMany, if a document halfway through the array produces an error of some type, what happens depends on whether you have opted for ordered or unordered operations. As the second parameter to insertMany you may specify an options document. Specify true for the key \"ordered\" in the options document to ensure documents are inserted in the order they are provided. Specify false and MongoDB may reorder the inserts to increase performance. Ordered inserts is the default if no ordering is specified. For ordered inserts, the array passed to insertMany defines the insertion order. If a document produces an inser‐ tion error, no documents beyond that point in the array will be inserted. For unor‐ dered inserts, MongoDB will attempt to insert all documents, regardless of whether some insertions produce errors.\n\nIn this example, because ordered inserts is the default, only the first two documents will be inserted. The third document will produce an error, because you cannot insert two documents with the same \"_id\":\n\n30\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents\n\n> db.movies.insertMany([ ... {\"_id\" : 0, \"title\" : \"Top Gun\"}, ... {\"_id\" : 1, \"title\" : \"Back to the Future\"}, ... {\"_id\" : 1, \"title\" : \"Gremlins\"}, ... {\"_id\" : 2, \"title\" : \"Aliens\"}]) 2019-04-22T12:27:57.278-0400 E QUERY [js] BulkWriteError: write error at item 2 in bulk operation : BulkWriteError({ \"writeErrors\" : [ { \"index\" : 2, \"code\" : 11000, \"errmsg\" : \"E11000 duplicate key error collection: test.movies index: _id_ dup key: { _id: 1.0 }\", \"op\" : { \"_id\" : 1, \"title\" : \"Gremlins\" } } ], \"writeConcernErrors\" : [ ], \"nInserted\" : 2, \"nUpserted\" : 0, \"nMatched\" : 0, \"nModified\" : 0, \"nRemoved\" : 0, \"upserted\" : [ ] }) BulkWriteError@src/mongo/shell/bulk_api.js:367:48 BulkWriteResult/this.toError@src/mongo/shell/bulk_api.js:332:24 Bulk/this.execute@src/mongo/shell/bulk_api.js:1186:23 DBCollection.prototype.insertMany@src/mongo/shell/crud_api.js:314:5 @(shell):1:1\n\nIf instead we specify unordered inserts, the first, second, and fourth documents in the array are inserted. The only insert that fails is the third document, again because of a duplicate \"_id\" error:\n\n> db.movies.insertMany([ ... {\"_id\" : 3, \"title\" : \"Sixteen Candles\"}, ... {\"_id\" : 4, \"title\" : \"The Terminator\"}, ... {\"_id\" : 4, \"title\" : \"The Princess Bride\"}, ... {\"_id\" : 5, \"title\" : \"Scarface\"}], ... {\"ordered\" : false}) 2019-05-01T17:02:25.511-0400 E QUERY [thread1] BulkWriteError: write error at item 2 in bulk operation : BulkWriteError({ \"writeErrors\" : [ { \"index\" : 2, \"code\" : 11000, \"errmsg\" : \"E11000 duplicate key error index: test.movies.$_id_\n\nInserting Documents\n\n|\n\n31\n\ndup key: { : 4.0 }\", \"op\" : { \"_id\" : 4, \"title\" : \"The Princess Bride\" } } ], \"writeConcernErrors\" : [ ], \"nInserted\" : 3, \"nUpserted\" : 0, \"nMatched\" : 0, \"nModified\" : 0, \"nRemoved\" : 0, \"upserted\" : [ ] }) BulkWriteError@src/mongo/shell/bulk_api.js:367:48 BulkWriteResult/this.toError@src/mongo/shell/bulk_api.js:332:24 Bulk/this.execute@src/mongo/shell/bulk_api.js:1186.23 DBCollection.prototype.insertMany@src/mongo/shell/crud_api.js:314:5 @(shell):1:1\n\nIf you study these examples closely, you might note that the output of these two calls to insertMany hints that other operations besides simply inserts might be supported for bulk writes. While insertMany does not support operations other than insert, MongoDB does support a Bulk Write API that enables you to batch together a num‐ ber of operations of different types in one call. While that is beyond the scope of this chapter, you can read about the Bulk Write API in the MongoDB documentation.\n\nInsert Validation MongoDB does minimal checks on data being inserted: it checks the document’s basic structure and adds an \"_id\" field if one does not exist. One of the basic struc‐ ture checks is size: all documents must be smaller than 16 MB. This is a somewhat arbitrary limit (and may be raised in the future); it is mostly intended to prevent bad schema design and ensure consistent performance. To see the Binary JSON (BSON) size, in bytes, of the document doc, run Object.bsonsize(doc) from the shell.\n\nTo give you an idea of how much data 16 MB is, the entire text of War and Peace is just 3.14 MB.\n\nThese minimal checks also mean that it is fairly easy to insert invalid data (if you are trying to). Thus, you should only allow trusted sources, such as your application servers, to connect to the database. All of the MongoDB drivers for major languages (and most of the minor ones, too) do check for a variety of invalid data (documents that are too large, contain non-UTF-8 strings, or use unrecognized types) before sending anything to the database.\n\n32\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents\n\ninsert In versions of MongoDB prior to 3.0, insert was the primary method for inserting documents into MongoDB. MongoDB drivers introduced a new CRUD API at the same time as the MongoDB 3.0 server release. As of MongoDB 3.2 the mongo shell also supports this API, which includes insertOne and insertMany as well as several other methods. The goal of the current CRUD API is to make the semantics of all CRUD operations consistent and clear across the drivers and the shell. While meth‐ ods such as insert are still supported for backward compatibility, they should not be used in applications going forward. You should instead prefer insertOne and insert Many for creating documents.\n\nRemoving Documents Now that there’s data in our database, let’s delete it. The CRUD API provides deleteOne and deleteMany for this purpose. Both of these methods take a filter document as their first parameter. The filter specifies a set of criteria to match against in removing documents. To delete the document with the \"_id\" value of 4, we use deleteOne in the mongo shell as illustrated here:\n\n> db.movies.find() { \"_id\" : 0, \"title\" : \"Top Gun\"} { \"_id\" : 1, \"title\" : \"Back to the Future\"} { \"_id\" : 3, \"title\" : \"Sixteen Candles\"} { \"_id\" : 4, \"title\" : \"The Terminator\"} { \"_id\" : 5, \"title\" : \"Scarface\"} > db.movies.deleteOne({\"_id\" : 4}) { \"acknowledged\" : true, \"deletedCount\" : 1 } > db.movies.find() { \"_id\" : 0, \"title\" : \"Top Gun\"} { \"_id\" : 1, \"title\" : \"Back to the Future\"} { \"_id\" : 3, \"title\" : \"Sixteen Candles\"} { \"_id\" : 5, \"title\" : \"Scarface\"}\n\nIn this example, we used a filter that could only match one document since \"_id\" values are unique in a collection. However, we can also specify a filter that matches multiple documents in a collection. In this case, deleteOne will delete the first docu‐ ment found that matches the filter. Which document is found first depends on several factors, including the order in which the documents were inserted, what updates were made to the documents (for some storage engines), and what indexes are specified. As with any database operation, be sure you know what effect your use of deleteOne will have on your data.\n\nTo delete all the documents that match a filter, use deleteMany:\n\nRemoving Documents\n\n|\n\n33\n\n> db.movies.find() { \"_id\" : 0, \"title\" : \"Top Gun\", \"year\" : 1986 } { \"_id\" : 1, \"title\" : \"Back to the Future\", \"year\" : 1985 } { \"_id\" : 3, \"title\" : \"Sixteen Candles\", \"year\" : 1984 } { \"_id\" : 4, \"title\" : \"The Terminator\", \"year\" : 1984 } { \"_id\" : 5, \"title\" : \"Scarface\", \"year\" : 1983 } > db.movies.deleteMany({\"year\" : 1984}) { \"acknowledged\" : true, \"deletedCount\" : 2 } > db.movies.find() { \"_id\" : 0, \"title\" : \"Top Gun\", \"year\" : 1986 } { \"_id\" : 1, \"title\" : \"Back to the Future\", \"year\" : 1985 } { \"_id\" : 5, \"title\" : \"Scarface\", \"year\" : 1983 }\n\nAs a more realistic use case, suppose you want to remove every user from the mail‐ ing.list collection where the value for \"opt-out\" is true:\n\n> db.mailing.list.deleteMany({\"opt-out\" : true})\n\nIn versions of MongoDB prior to 3.0, remove was the primary method for deleting documents. MongoDB drivers introduced the deleteOne and deleteMany methods at the same time as the MongoDB 3.0 server release, and the shell began supporting these methods in MongoDB 3.2. While remove is still supported for backward com‐ patibility, you should use deleteOne and deleteMany in your applications. The cur‐ rent CRUD API provides a cleaner set of semantics and, especially for multidocument operations, helps application developers avoid a couple of common pitfalls with the previous API.\n\ndrop It is possible to use deleteMany to remove all documents in a collection:\n\n> db.movies.find() { \"_id\" : 0, \"title\" : \"Top Gun\", \"year\" : 1986 } { \"_id\" : 1, \"title\" : \"Back to the Future\", \"year\" : 1985 } { \"_id\" : 3, \"title\" : \"Sixteen Candles\", \"year\" : 1984 } { \"_id\" : 4, \"title\" : \"The Terminator\", \"year\" : 1984 } { \"_id\" : 5, \"title\" : \"Scarface\", \"year\" : 1983 } > db.movies.deleteMany({}) { \"acknowledged\" : true, \"deletedCount\" : 5 } > db.movies.find()\n\nRemoving documents is usually a fairly quick operation. However, if you want to clear an entire collection, it is faster to drop it:\n\n> db.movies.drop() true\n\nand then recreate any indexes on the empty collection.\n\nOnce data has been removed, it is gone forever. There is no way to undo a delete or drop operation or recover deleted documents, except, of course, by restoring a\n\n34\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents\n\npreviously backed up version of the data. See Chapter 23 for a detailed discussion of MongoDB backup and restore.\n\nUpdating Documents Once a document is stored in the database, it can be changed using one of several update methods: updateOne, updateMany, and replaceOne. updateOne and update Many each take a filter document as their first parameter and a modifier document, which describes changes to make, as the second parameter. replaceOne also takes a filter as the first parameter, but as the second parameter replaceOne expects a docu‐ ment with which it will replace the document matching the filter.\n\nUpdating a document is atomic: if two updates happen at the same time, whichever one reaches the server first will be applied, and then the next one will be applied. Thus, conflicting updates can safely be sent in rapid-fire succession without any documents being corrupted: the last update will “win.” The Document Versioning pattern (see “Schema Design Patterns” on page 208) is worth considering if you don’t want the default behavior.\n\nDocument Replacement replaceOne fully replaces a matching document with a new one. This can be useful to do a dramatic schema migration (see Chapter 9 for scheme migration strategies). For example, suppose we are making major changes to a user document, which looks like the following:\n\n{ \"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7a\"), \"name\" : \"joe\", \"friends\" : 32, \"enemies\" : 2 }\n\nWe want to move the \"friends\" and \"enemies\" fields to a \"relationships\" subdo‐ cument. We can change the structure of the document in the shell and then replace the database’s version with a replaceOne:\n\n> var joe = db.users.findOne({\"name\" : \"joe\"}); > joe.relationships = {\"friends\" : joe.friends, \"enemies\" : joe.enemies}; { \"friends\" : 32, \"enemies\" : 2 } > joe.username = joe.name; \"joe\" > delete joe.friends; true > delete joe.enemies;\n\nUpdating Documents\n\n|\n\n35\n\ntrue > delete joe.name; true > db.users.replaceOne({\"name\" : \"joe\"}, joe);\n\nNow, doing a findOne shows that the structure of the document has been updated:\n\n{ \"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7a\"), \"username\" : \"joe\", \"relationships\" : { \"friends\" : 32, \"enemies\" : 2 } }\n\nA common mistake is matching more than one document with the criteria and then creating a duplicate \"_id\" value with the second parameter. The database will throw an error for this, and no documents will be updated.\n\nFor example, suppose we create several documents with the same value for \"name\", but we don’t realize it:\n\n> db.people.find() {\"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7b\"), \"name\" : \"joe\", \"age\" : 65} {\"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7c\"), \"name\" : \"joe\", \"age\" : 20} {\"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7d\"), \"name\" : \"joe\", \"age\" : 49}\n\nNow, if it’s Joe #2’s birthday, we want to increment the value of his \"age\" key, so we might say this:\n\n> joe = db.people.findOne({\"name\" : \"joe\", \"age\" : 20}); { \"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7c\"), \"name\" : \"joe\", \"age\" : 20 } > joe.age++; > db.people.replaceOne({\"name\" : \"joe\"}, joe); E11001 duplicate key on update\n\nWhat happened? When you do the update, the database will look for a document matching {\"name\" : \"joe\"}. The first one it finds will be the 65-year-old Joe. It will attempt to replace that document with the one in the joe variable, but there’s already a document in this collection with the same \"_id\". Thus, the update will fail, because \"_id\" values must be unique. The best way to avoid this situation is to make sure that your update always specifies a unique document, perhaps by matching on a key like \"_id\". For the preceding example, this would be the correct update to use:\n\n> db.people.replaceOne({\"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7c\")}, joe)\n\n36\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "page_number": 45
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 55-62)",
      "start_page": 55,
      "end_page": 62,
      "detection_method": "topic_boundary",
      "content": "Using \"_id\" for the filter will also be efficient since\"_id\" values form the basis for the primary index of a collection. We’ll cover primary and secondary indexes and how indexing affects updates and other operations more in Chapter 5.\n\nUsing Update Operators Usually only certain portions of a document need to be updated. You can update spe‐ cific fields in a document using atomic update operators. Update operators are special keys that can be used to specify complex update operations, such as altering, adding, or removing keys, and even manipulating arrays and embedded documents.\n\nSuppose we’re keeping website analytics in a collection and want to increment a counter each time someone visits a page. We can use update operators to do this increment atomically. Each URL and its number of page views is stored in a docu‐ ment that looks like this:\n\n{ \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"), \"url\" : \"www.example.com\", \"pageviews\" : 52 }\n\nEvery time someone visits a page, we can find the page by its URL and use the \"$inc\" modifier to increment the value of the \"pageviews\" key:\n\n> db.analytics.updateOne({\"url\" : \"www.example.com\"}, ... {\"$inc\" : {\"pageviews\" : 1}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n\nNow, if we do a findOne, we see that \"pageviews\" has increased by one:\n\n> db.analytics.findOne() { \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"), \"url\" : \"www.example.com\", \"pageviews\" : 53 }\n\nWhen using operators, the value of \"_id\" cannot be changed. (Note that \"_id\" can be changed by using whole-document replacement.) Values for any other key, including other uniquely indexed keys, can be modified.\n\nGetting started with the “$set” modifier\n\n\"$set\" sets the value of a field. If the field does not yet exist, it will be created. This can be handy for updating schemas or adding user-defined keys. For example, sup‐ pose you have a simple user profile stored as a document that looks something like the following:\n\nUpdating Documents\n\n|\n\n37\n\n> db.users.findOne() { \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"), \"name\" : \"joe\", \"age\" : 30, \"sex\" : \"male\", \"location\" : \"Wisconsin\" }\n\nThis is a pretty bare-bones user profile. If the user wanted to store his favorite book in his profile, he could add it using \"$set\":\n\n> db.users.updateOne({\"_id\" : ObjectId(\"4b253b067525f35f94b60a31\")}, ... {\"$set\" : {\"favorite book\" : \"War and Peace\"}})\n\nNow the document will have a \"favorite book\" key:\n\n> db.users.findOne() { \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"), \"name\" : \"joe\", \"age\" : 30, \"sex\" : \"male\", \"location\" : \"Wisconsin\", \"favorite book\" : \"War and Peace\" }\n\nIf the user decides that he actually enjoys a different book, \"$set\" can be used again to change the value:\n\n> db.users.updateOne({\"name\" : \"joe\"}, ... {\"$set\" : {\"favorite book\" : \"Green Eggs and Ham\"}})\n\n\"$set\" can even change the type of the key it modifies. For instance, if our fickle user decides that he actually likes quite a few books, he can change the value of the \"favor ite book\" key into an array:\n\n> db.users.updateOne({\"name\" : \"joe\"}, ... {\"$set\" : {\"favorite book\" : ... [\"Cat's Cradle\", \"Foundation Trilogy\", \"Ender's Game\"]}})\n\nIf the user realizes that he actually doesn’t like reading, he can remove the key alto‐ gether with \"$unset\":\n\n> db.users.updateOne({\"name\" : \"joe\"}, ... {\"$unset\" : {\"favorite book\" : 1}})\n\nNow the document will be the same as it was at the beginning of this example.\n\nYou can also use \"$set\" to reach in and change embedded documents:\n\n> db.blog.posts.findOne() { \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"),\n\n38\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents\n\n\"title\" : \"A Blog Post\", \"content\" : \"...\", \"author\" : { \"name\" : \"joe\", \"email\" : \"joe@example.com\" } } > db.blog.posts.updateOne({\"author.name\" : \"joe\"}, ... {\"$set\" : {\"author.name\" : \"joe schmoe\"}})\n\n> db.blog.posts.findOne() { \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"), \"title\" : \"A Blog Post\", \"content\" : \"...\", \"author\" : { \"name\" : \"joe schmoe\", \"email\" : \"joe@example.com\" } }\n\nYou must always use a $-modifier for adding, changing, or removing keys. A com‐ mon error people make when starting out is to try to set the value of a key to some other value by doing an update that resembles this:\n\n> db.blog.posts.updateOne({\"author.name\" : \"joe\"}, ... {\"author.name\" : \"joe schmoe\"})\n\nThis will result in an error. The update document must contain update operators. Previous versions of the CRUD API did not catch this type of error. Earlier update methods would simply complete a whole document replacement in such situations. It is this type of pitfall that led to the creation of a new CRUD API.\n\nIncrementing and decrementing\n\nThe \"$inc\" operator can be used to change the value for an existing key or to create a new key if it does not already exist. It’s useful for updating analytics, karma, votes, or anything else that has a changeable, numeric value.\n\nSuppose we are creating a game collection where we want to save games and update scores as they change. When a user starts playing, say, a game of pinball, we can insert a document that identifies the game by name and the user playing it:\n\n> db.games.insertOne({\"game\" : \"pinball\", \"user\" : \"joe\"})\n\nWhen the ball hits a bumper, the game should increment the player’s score. Since points in pinball are given out pretty freely, let’s say that the base unit of points a player can earn is 50. We can use the \"$inc\" modifier to add 50 to the player’s score:\n\n> db.games.updateOne({\"game\" : \"pinball\", \"user\" : \"joe\"}, ... {\"$inc\" : {\"score\" : 50}})\n\nUpdating Documents\n\n|\n\n39\n\nIf we look at the document after this update, we’ll see the following:\n\n> db.games.findOne() { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"game\" : \"pinball\", \"user\" : \"joe\", \"score\" : 50 }\n\nThe \"score\" key did not already exist, so it was created by \"$inc\" and set to the increment amount: 50.\n\nIf the ball lands in a “bonus” slot, we want to add 10,000 to the score. We can do this by passing a different value to \"$inc\":\n\n> db.games.updateOne({\"game\" : \"pinball\", \"user\" : \"joe\"}, ... {\"$inc\" : {\"score\" : 10000}})\n\nNow if we look at the game, we’ll see the following:\n\n> db.games.findOne() { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"game\" : \"pinball\", \"user\" : \"joe\", \"score\" : 10050 }\n\nThe \"score\" key existed and had a numeric value, so the server added 10,000 to it.\n\n\"$inc\" is similar to \"$set\", but it is designed for incrementing (and decrementing) numbers. \"$inc\" can be used only on values of type integer, long, double, or decimal. If it is used on any other type of value, it will fail. This includes types that many lan‐ guages will automatically cast into numbers, like nulls, booleans, or strings of numeric characters:\n\n> db.strcounts.insert({\"count\" : \"1\"}) WriteResult({ \"nInserted\" : 1 }) > db.strcounts.update({}, {\"$inc\" : {\"count\" : 1}}) WriteResult({ \"nMatched\" : 0, \"nUpserted\" : 0, \"nModified\" : 0, \"writeError\" : { \"code\" : 16837, \"errmsg\" : \"Cannot apply $inc to a value of non-numeric type. {_id: ObjectId('5726c0d36855a935cb57a659')} has the field 'count' of non-numeric type String\" } })\n\n40\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents\n\nAlso, the value of the \"$inc\" key must be a number. You cannot increment by a string, array, or other nonnumeric value. Doing so will give a “Modifier \"$inc\" allowed for numbers only” error message. To modify other types, use \"$set\" or one of the following array operators.\n\nArray operators\n\nAn extensive class of update operators exists for manipulating arrays. Arrays are common and powerful data structures: not only are they lists that can be referenced by index, but they can also double as sets.\n\nAdding elements. \"$push\" adds elements to the end of an array if the array exists and creates a new array if it does not. For example, suppose that we are storing blog posts and want to add a \"comments\" key containing an array. We can push a comment onto the nonexistent \"comments\" array, which will create the array and add the comment:\n\n> db.blog.posts.findOne() { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"title\" : \"A blog post\", \"content\" : \"...\" } > db.blog.posts.updateOne({\"title\" : \"A blog post\"}, ... {\"$push\" : {\"comments\" : ... {\"name\" : \"joe\", \"email\" : \"joe@example.com\", ... \"content\" : \"nice post.\"}}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 } > db.blog.posts.findOne() { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"title\" : \"A blog post\", \"content\" : \"...\", \"comments\" : [ { \"name\" : \"joe\", \"email\" : \"joe@example.com\", \"content\" : \"nice post.\" } ] }\n\nNow, if we want to add another comment, we can simply use \"$push\" again:\n\n> db.blog.posts.updateOne({\"title\" : \"A blog post\"}, ... {\"$push\" : {\"comments\" : ... {\"name\" : \"bob\", \"email\" : \"bob@example.com\", ... \"content\" : \"good post.\"}}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 } > db.blog.posts.findOne() {\n\nUpdating Documents\n\n|\n\n41\n\n\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"title\" : \"A blog post\", \"content\" : \"...\", \"comments\" : [ { \"name\" : \"joe\", \"email\" : \"joe@example.com\", \"content\" : \"nice post.\" }, { \"name\" : \"bob\", \"email\" : \"bob@example.com\", \"content\" : \"good post.\" } ] }\n\nThis is the “simple” form of \"push\", but you can use it for more complex array opera‐ tions as well. The MongoDB query language provides modifiers for some operators, including \"$push\". You can push multiple values in one operation using the \"$each\" modifer for \"$push\":\n\n> db.stock.ticker.updateOne({\"_id\" : \"GOOG\"}, ... {\"$push\" : {\"hourly\" : {\"$each\" : [562.776, 562.790, 559.123]}}})\n\nThis would push three new elements onto the array.\n\nIf you only want the array to grow to a certain length, you can use the \"$slice\" modifier with \"$push\" to prevent an array from growing beyond a certain size, effec‐ tively making a “top N” list of items:\n\n> db.movies.updateOne({\"genre\" : \"horror\"}, ... {\"$push\" : {\"top10\" : {\"$each\" : [\"Nightmare on Elm Street\", \"Saw\"], ... \"$slice\" : -10}}})\n\nThis example limits the array to the last 10 elements pushed.\n\nIf the array is smaller than 10 elements (after the push), all elements will be kept. If the array is larger than 10 elements, only the last 10 elements will be kept. Thus, \"$slice\" can be used to create a queue in a document.\n\nFinally, you can apply the \"$sort\" modifier to \"$push\" operations before trimming:\n\n> db.movies.updateOne({\"genre\" : \"horror\"}, ... {\"$push\" : {\"top10\" : {\"$each\" : [{\"name\" : \"Nightmare on Elm Street\", ... \"rating\" : 6.6}, ... {\"name\" : \"Saw\", \"rating\" : 4.3}], ... \"$slice\" : -10, ... \"$sort\" : {\"rating\" : -1}}}})\n\n42\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents\n\nThis will sort all of the objects in the array by their \"rating\" field and then keep the first 10. Note that you must include \"$each\"; you cannot just \"$slice\" or \"$sort\" an array with \"$push\".\n\nUsing arrays as sets. You might want to treat an array as a set, only adding values if they are not present. This can be done using \"$ne\" in the query document. For exam‐ ple, to push an author onto a list of citations, but only if they aren’t already there, use the following:\n\n> db.papers.updateOne({\"authors cited\" : {\"$ne\" : \"Richie\"}}, ... {$push : {\"authors cited\" : \"Richie\"}})\n\nThis can also be done with \"$addToSet\", which is useful for cases where \"$ne\" won’t work or where \"$addToSet\" describes what is happening better.\n\nFor example, suppose you have a document that represents a user. You might have a set of email addresses that they have added:\n\n> db.users.findOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"username\" : \"joe\", \"emails\" : [ \"joe@example.com\", \"joe@gmail.com\", \"joe@yahoo.com\" ] }\n\nWhen adding another address, you can use “$addToSet\" to prevent duplicates:\n\n> db.users.updateOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}, ... {\"$addToSet\" : {\"emails\" : \"joe@gmail.com\"}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 0 } > db.users.findOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"username\" : \"joe\", \"emails\" : [ \"joe@example.com\", \"joe@gmail.com\", \"joe@yahoo.com\", ] } > db.users.updateOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}, ... {\"$addToSet\" : {\"emails\" : \"joe@hotmail.com\"}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 } > db.users.findOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"username\" : \"joe\",\n\nUpdating Documents\n\n|\n\n43\n\n\"emails\" : [ \"joe@example.com\", \"joe@gmail.com\", \"joe@yahoo.com\", \"joe@hotmail.com\" ] }\n\nYou can also use \"$addToSet\" in conjunction with \"$each\" to add multiple unique values, which cannot be done with the \"$ne\"/\"$push\" combination. For instance, you could use these operators if the user wanted to add more than one email address:\n\n> db.users.updateOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}, ... {\"$addToSet\" : {\"emails\" : {\"$each\" : ... [\"joe@php.net\", \"joe@example.com\", \"joe@python.org\"]}}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 } > db.users.findOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"username\" : \"joe\", \"emails\" : [ \"joe@example.com\", \"joe@gmail.com\", \"joe@yahoo.com\", \"joe@hotmail.com\" \"joe@php.net\" \"joe@python.org\" ] }\n\nRemoving elements. There are a few ways to remove elements from an array. If you want to treat the array like a queue or a stack, you can use \"$pop\", which can remove elements from either end. {\"$pop\" : {\"key\" : 1}} removes an element from the end of the array. {\"$pop\" : {\"key\" : -1}} removes it from the beginning.\n\nSometimes an element should be removed based on specific criteria, rather than its position in the array. \"$pull\" is used to remove elements of an array that match the given criteria. For example, suppose we have a list of things that need to be done, but not in any specific order:\n\n> db.lists.insertOne({\"todo\" : [\"dishes\", \"laundry\", \"dry cleaning\"]})\n\nIf we do the laundry first, we can remove it from the list with the following:\n\n> db.lists.updateOne({}, {\"$pull\" : {\"todo\" : \"laundry\"}})\n\nNow if we do a find, we’ll see that there are only two elements remaining in the array:\n\n> db.lists.findOne() { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"todo\" : [\n\n44\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "page_number": 55
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 63-70)",
      "start_page": 63,
      "end_page": 70,
      "detection_method": "topic_boundary",
      "content": "\"dishes\", \"dry cleaning\" ] }\n\nPulling removes all matching documents, not just a single match. If you have an array that looks like [1, 1, 2, 1] and pull 1, you’ll end up with a single-element array, [2].\n\nArray operators can be used only on keys with array values. For example, you cannot push onto an integer or pop off of a string. Use \"$set\" or \"$inc\" to modify scalar values.\n\nPositional array modifications. Array manipulation becomes a little trickier when you have multiple values in an array and want to modify some of them. There are two ways to manipulate values in arrays: by position or by using the position operator (the $ character).\n\nArrays use 0-based indexing, and elements can be selected as though their index were a document key. For example, suppose we have a document containing an array with a few embedded documents, such as a blog post with comments:\n\n> db.blog.posts.findOne() { \"_id\" : ObjectId(\"4b329a216cc613d5ee930192\"), \"content\" : \"...\", \"comments\" : [ { \"comment\" : \"good post\", \"author\" : \"John\", \"votes\" : 0 }, { \"comment\" : \"i thought it was too short\", \"author\" : \"Claire\", \"votes\" : 3 }, { \"comment\" : \"free watches\", \"author\" : \"Alice\", \"votes\" : -5 }, { \"comment\" : \"vacation getaways\", \"author\" : \"Lynn\", \"votes\" : -7 } ] }\n\nUpdating Documents\n\n|\n\n45\n\nIf we want to increment the number of votes for the first comment, we can say the following:\n\n> db.blog.updateOne({\"post\" : post_id}, ... {\"$inc\" : {\"comments.0.votes\" : 1}})\n\nIn many cases, though, we don’t know what index of the array to modify without querying for the document first and examining it. To get around this, MongoDB has a positional operator, $, that figures out which element of the array the query docu‐ ment matched and updates that element. For example, if we have a user named John who updates his name to Jim, we can replace it in the comments by using the posi‐ tional operator:\n\n> db.blog.updateOne({\"comments.author\" : \"John\"}, ... {\"$set\" : {\"comments.$.author\" : \"Jim\"}})\n\nThe positional operator updates only the first match. Thus, if John had left more than one comment, his name would be changed only for the first comment he left.\n\nUpdates using array filters. MongoDB 3.6 introduced another option for updating indi‐ vidual array elements: arrayFilters. This option enables us to modify array ele‐ ments matching particular critera. For example, if we want to hide all comments with five or more down votes, we can do something like the following:\n\ndb.blog.updateOne( {\"post\" : post_id }, { $set: { \"comments.$[elem].hidden\" : true } }, { arrayFilters: [ { \"elem.votes\": { $lte: -5 } } ] } )\n\nThis command defines elem as the identifier for each matching element in the \"com ments\" array. If the votes value for the comment identified by elem is less than or equal to -5, we will add a field called \"hidden\" to the \"comments\" document and set its value to true.\n\nUpserts An upsert is a special type of update. If no document is found that matches the filter, a new document will be created by combining the criteria and updated documents. If a matching document is found, it will be updated normally. Upserts can be handy because they can eliminate the need to “seed” your collection: you can often have the same code create and update documents.\n\nLet’s go back to our example that records the number of views for each page of a web‐ site. Without an upsert, we might try to find the URL and increment the number of\n\n46\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents\n\nviews or create a new document if the URL doesn’t exist. If we were to write this out as a JavaScript program it might look something like the following:\n\n// check if we have an entry for this page blog = db.analytics.findOne({url : \"/blog\"})\n\n// if we do, add one to the number of views and save if (blog) { blog.pageviews++; db.analytics.save(blog); } // otherwise, create a new document for this page else { db.analytics.insertOne({url : \"/blog\", pageviews : 1}) }\n\nThis means we are making a round trip to the database, plus sending an update or insert, every time someone visits a page. If we are running this code in multiple pro‐ cesses, we are also subject to a race condition where more than one document can be inserted for a given URL.\n\nWe can eliminate the race condition and cut down the amount of code by just send‐ ing an upsert to the database (the third parameter to updateOne and updateMany is an options document that enables us to specify this):\n\n> db.analytics.updateOne({\"url\" : \"/blog\"}, {\"$inc\" : {\"pageviews\" : 1}}, ... {\"upsert\" : true})\n\nThis line does exactly what the previous code block does, except it’s faster and atomic! The new document is created by using the criteria document as a base and applying any modifier documents to it.\n\nFor example, if you do an upsert that matches a key and increments to the value of that key, the increment will be applied to the match:\n\n> db.users.updateOne({\"rep\" : 25}, {\"$inc\" : {\"rep\" : 3}}, {\"upsert\" : true}) WriteResult({ \"acknowledged\" : true, \"matchedCount\" : 0, \"modifiedCount\" : 0, \"upsertedId\" : ObjectId(\"5a93b07aaea1cb8780a4cf72\") }) > db.users.findOne({\"_id\" : ObjectId(\"5727b2a7223502483c7f3acd\")} ) { \"_id\" : ObjectId(\"5727b2a7223502483c7f3acd\"), \"rep\" : 28 }\n\nThe upsert creates a new document with a \"rep\" of 25 and then increments that by 3, giving us a document where \"rep\" is 28. If the upsert option were not specified, {\"rep\" : 25} would not match any documents, so nothing would happen.\n\nUpdating Documents\n\n|\n\n47\n\nIf we run the upsert again (with the criterion {\"rep\" : 25}), it will create another new document. This is because the criterion does not match the only document in the collection. (Its \"rep\" is 28.)\n\nSometimes a field needs to be set when a document is created, but not changed on subsequent updates. This is what \"$setOnInsert\" is for. \"$setOnInsert\" is an opera‐ tor that only sets the value of a field when the document is being inserted. Thus, we could do something like this:\n\n> db.users.updateOne({}, {\"$setOnInsert\" : {\"createdAt\" : new Date()}}, ... {\"upsert\" : true}) { \"acknowledged\" : true, \"matchedCount\" : 0, \"modifiedCount\" : 0, \"upsertedId\" : ObjectId(\"5727b4ac223502483c7f3ace\") } > db.users.findOne() { \"_id\" : ObjectId(\"5727b4ac223502483c7f3ace\"), \"createdAt\" : ISODate(\"2016-05-02T20:12:28.640Z\") }\n\nIf we run this update again, it will match the existing document, nothing will be inserted, and so the \"createdAt\" field will not be changed:\n\n> db.users.updateOne({}, {\"$setOnInsert\" : {\"createdAt\" : new Date()}}, ... {\"upsert\" : true}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 0 } > db.users.findOne() { \"_id\" : ObjectId(\"5727b4ac223502483c7f3ace\"), \"createdAt\" : ISODate(\"2016-05-02T20:12:28.640Z\") }\n\nNote that you generally do not need to keep a \"createdAt\" field, as ObjectIds con‐ tain a timestamp of when the document was created. However, \"$setOnInsert\" can be useful for creating padding, initializing counters, and for collections that do not use ObjectIds.\n\nThe save shell helper\n\nsave is a shell function that lets you insert a document if it doesn’t exist and update it if it does. It takes one argument: a document. If the document contains an \"_id\" key, save will do an upsert. Otherwise, it will do an insert. save is really just a conve‐ nience function so that programmers can quickly modify documents in the shell:\n\n> var x = db.testcol.findOne() > x.num = 42\n\n48\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents\n\n42 > db.testcol.save(x)\n\nWithout save, the last line would have been more cumbersome:\n\ndb.testcol.replaceOne({\"_id\" : x._id}, x)\n\nUpdating Multiple Documents So far in this chapter we have used updateOne to illustrate update operations. updateOne updates only the first document found that matches the filter criteria. If there are more matching documents, they will remain unchanged. To modify all of the documents matching a filter, use updateMany. updateMany follows the same semantics as updateOne and takes the same parameters. The key difference is in the number of documents that might be changed.\n\nupdateMany provides a powerful tool for performing schema migrations or rolling out new features to certain users. Suppose, for example, we want to give a gift to every user who has a birthday on a certain day. We can use updateMany to add a \"gift\" to their accounts. For example:\n\n> db.users.insertMany([ ... {birthday: \"10/13/1978\"}, ... {birthday: \"10/13/1978\"}, ... {birthday: \"10/13/1978\"}]) { \"acknowledged\" : true, \"insertedIds\" : [ ObjectId(\"5727d6fc6855a935cb57a65b\"), ObjectId(\"5727d6fc6855a935cb57a65c\"), ObjectId(\"5727d6fc6855a935cb57a65d\") ] } > db.users.updateMany({\"birthday\" : \"10/13/1978\"}, ... {\"$set\" : {\"gift\" : \"Happy Birthday!\"}}) { \"acknowledged\" : true, \"matchedCount\" : 3, \"modifiedCount\" : 3 }\n\nThe call to updateMany adds a \"gift\" field to each of the three documents we inser‐ ted into the users collection immediately before.\n\nReturning Updated Documents For some use cases it is important to return the document modified. In earlier ver‐ sions of MongoDB, findAndModify was the method of choice in such situations. It is handy for manipulating queues and performing other operations that need get-and- set−style atomicity. However, findAndModify is prone to user error because it’s a complex method combining the functionality of three different types of operations: delete, replace, and update (including upserts).\n\nUpdating Documents\n\n|\n\n49\n\nMongoDB 3.2 introduced three new collection methods to the shell to accommodate the functionality of findAndModify, but with semantics that are easier to learn and remember: findOneAndDelete, findOneAndReplace, and findOneAndUpdate. The primary difference between these methods and, for example, updateOne is that they enable you to atomically get the value of a modified document. MongoDB 4.2 exten‐ ded findOneAndUpdate to accept an aggregation pipeline for the update. The pipeline can consist of the following stages: $addFields and its alias $set, $project and its alias $unset, and $replaceRoot and its alias $replaceWith.\n\nSuppose we have a collection of processes run in a certain order. Each is represented with a document that has the following form:\n\n{ \"_id\" : ObjectId(), \"status\" : \"state\", \"priority\" : N }\n\n\"status\" is a string that can be \"READY\", \"RUNNING\", or \"DONE\". We need to find the job with the highest priority in the \"READY\" state, run the process function, and then update the status to \"DONE\". We might try querying for the ready processes, sorting by priority, and updating the status of the highest-priority process to mark it as \"RUNNING\". Once we have processed it, we update the status to \"DONE\". This looks something like the following:\n\nvar cursor = db.processes.find({\"status\" : \"READY\"}); ps = cursor.sort({\"priority\" : -1}).limit(1).next(); db.processes.updateOne({\"_id\" : ps._id}, {\"$set\" : {\"status\" : \"RUNNING\"}}); do_something(ps); db.processes.updateOne({\"_id\" : ps._id}, {\"$set\" : {\"status\" : \"DONE\"}});\n\nThis algorithm isn’t great because it is subject to a race condition. Suppose we have two threads running. If one thread (call it A) retrieved the document and another thread (call it B) retrieved the same document before A had updated its status to \"RUNNING\", then both threads would be running the same process. We can avoid this by checking the result as part of the update query, but this becomes complex:\n\nvar cursor = db.processes.find({\"status\" : \"READY\"}); cursor.sort({\"priority\" : -1}).limit(1); while ((ps = cursor.next()) != null) { var result = db.processes.updateOne({\"_id\" : ps._id, \"status\" : \"READY\"}, {\"$set\" : {\"status\" : \"RUNNING\"}});\n\nif (result.modifiedCount === 1) { do_something(ps); db.processes.updateOne({\"_id\" : ps._id}, {\"$set\" : {\"status\" : \"DONE\"}}); break; } cursor = db.processes.find({\"status\" : \"READY\"});\n\n50\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents\n\ncursor.sort({\"priority\" : -1}).limit(1); }\n\nAlso, depending on timing, one thread may end up doing all the work while another thread uselessly trails it. Thread A could always grab the process, and then B would try to get the same process, fail, and leave A to do all the work.\n\nSituations like this are perfect for findOneAndUpdate. findOneAndUpdate can return the item and update it in a single operation. In this case, it looks like the following:\n\n> db.processes.findOneAndUpdate({\"status\" : \"READY\"}, ... {\"$set\" : {\"status\" : \"RUNNING\"}}, ... {\"sort\" : {\"priority\" : -1}}) { \"_id\" : ObjectId(\"4b3e7a18005cab32be6291f7\"), \"priority\" : 1, \"status\" : \"READY\" }\n\nNotice that the status is still \"READY\" in the returned document because the findOneAndUpdate method defaults to returning the state of the document before it was modified. It will return the updated document if we set the \"returnNewDocu ment\" field in the options document to true. An options document is passed as the third parameter to findOneAndUpdate:\n\n> db.processes.findOneAndUpdate({\"status\" : \"READY\"}, ... {\"$set\" : {\"status\" : \"RUNNING\"}}, ... {\"sort\" : {\"priority\" : -1}, ... \"returnNewDocument\": true}) { \"_id\" : ObjectId(\"4b3e7a18005cab32be6291f7\"), \"priority\" : 1, \"status\" : \"RUNNING\" }\n\nThus, the program becomes the following:\n\nps = db.processes.findOneAndUpdate({\"status\" : \"READY\"}, {\"$set\" : {\"status\" : \"RUNNING\"}}, {\"sort\" : {\"priority\" : -1}, \"returnNewDocument\": true}) do_something(ps) db.process.updateOne({\"_id\" : ps._id}, {\"$set\" : {\"status\" : \"DONE\"}})\n\nIn addition to this one, there are two other methods you should be aware of. findOneAndReplace takes the same parameters and returns the document matching the filter either before or after the replacement, depending on the value of returnNew Document. findOneAndDelete is similar except it does not take an update document as a parameter and has a subset of the options of the other two methods. findOneAnd Delete returns the deleted document.\n\nUpdating Documents\n\n|\n\n51\n\nCHAPTER 4 Querying\n\nThis chapter looks at querying in detail. The main areas covered are as follows:\n\nYou can query for ranges, set inclusion, inequalities, and more by using $ conditionals.\n\nQueries return a database cursor, which lazily returns batches of documents as you need them.\n\nThere are a lot of metaoperations you can perform on a cursor, including skip‐ ping a certain number of results, limiting the number of results returned, and sorting results.\n\nIntroduction to find The find method is used to perform queries in MongoDB. Querying returns a subset of documents in a collection, from no documents at all to the entire collection. Which documents get returned is determined by the first argument to find, which is a docu‐ ment specifying the query criteria.\n\nAn empty query document (i.e., {}) matches everything in the collection. If find isn’t given a query document, it defaults to {}. For example, the following:\n\n> db.c.find()\n\nmatches every document in the collection c (and returns these documents in batches).\n\nWhen we start adding key/value pairs to the query document, we begin restricting our search. This works in a straightforward way for most types: numbers match num‐ bers, booleans match booleans, and strings match strings. Querying for a simple type is as easy as specifying the value that you are looking for. For example, to find all\n\n53",
      "page_number": 63
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 71-80)",
      "start_page": 71,
      "end_page": 80,
      "detection_method": "topic_boundary",
      "content": "documents where the value for \"age\" is 27, we can add that key/value pair to the query document:\n\n> db.users.find({\"age\" : 27})\n\nIf we have a string we want to match, such as a \"username\" key with the value \"joe\", we use that key/value pair instead:\n\n> db.users.find({\"username\" : \"joe\"})\n\nMultiple conditions can be strung together by adding more key/value pairs to the query document, which gets interpreted as “condition1 AND condition2 AND … AND conditionN.” For instance, to get all users who are 27-year-olds with the user‐ name “joe,” we can query for the following:\n\n> db.users.find({\"username\" : \"joe\", \"age\" : 27})\n\nSpecifying Which Keys to Return Sometimes you do not need all of the key/value pairs in a document returned. If this is the case, you can pass a second argument to find (or findOne) specifying the keys you want. This reduces both the amount of data sent over the wire and the time and memory used to decode documents on the client side.\n\nFor example, if you have a user collection and you are interested only in the \"username\" and \"email\" keys, you could return just those keys with the following query:\n\n> db.users.find({}, {\"username\" : 1, \"email\" : 1}) { \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523620\"), \"username\" : \"joe\", \"email\" : \"joe@example.com\" }\n\nAs you can see from the previous output, the \"_id\" key is returned by default, even if it isn’t specifically requested.\n\nYou can also use this second parameter to exclude specific key/value pairs from the results of a query. For instance, you may have documents with a variety of keys, and the only thing you know is that you never want to return the \"fatal_weakness\" key:\n\n> db.users.find({}, {\"fatal_weakness\" : 0})\n\nThis can also prevent \"_id\" from being returned:\n\n> db.users.find({}, {\"username\" : 1, \"_id\" : 0}) { \"username\" : \"joe\", }\n\n54\n\n|\n\nChapter 4: Querying\n\nLimitations There are some restrictions on queries. The value of a query document must be a constant as far as the database is concerned. (It can be a normal variable in your own code.) That is, it cannot refer to the value of another key in the document. For exam‐ ple, if we were keeping inventory and we had both \"in_stock\" and \"num_sold\" keys, we couldn’t compare their values by querying the following:\n\n> db.stock.find({\"in_stock\" : \"this.num_sold\"}) // doesn't work\n\nThere are ways to do this (see “$where Queries” on page 65), but you will usually get better performance by restructuring your document slightly, such that a “normal” query will suffice. In this example, we could instead use the keys \"initial_stock\" and \"in_stock\". Then, every time someone buys an item, we decrement the value of the \"in_stock\" key by one. Finally, we can do a simple query to check which items are out of stock:\n\n> db.stock.find({\"in_stock\" : 0})\n\nQuery Criteria Queries can go beyond the exact matching described in the previous section; they can match more complex criteria, such as ranges, OR-clauses, and negation.\n\nQuery Conditionals \"$lt\", \"$lte\", \"$gt\", and \"$gte\" are all comparison operators, corresponding to <, <=, >, and >=, respectively. They can be combined to look for a range of values. For example, to look for users who are between the ages of 18 and 30, we can do this:\n\n> db.users.find({\"age\" : {\"$gte\" : 18, \"$lte\" : 30}})\n\nThis would find all documents where the \"age\" field was greater than or equal to 18 AND less than or equal to 30.\n\nThese types of range queries are often useful for dates. For example, to find people who registered before January 1, 2007, we can do this:\n\n> start = new Date(\"01/01/2007\") > db.users.find({\"registered\" : {\"$lt\" : start}})\n\nDepending on how you create and store dates, an exact match might be less useful, since dates are stored with millisecond precision. Often you want a whole day, week, or month, making a range query necessary.\n\nTo query for documents where a key’s value is not equal to a certain value, you must use another conditional operator, \"$ne\", which stands for “not equal.” If you want to find all users who do not have the username “joe,” you can query for them using this:\n\nQuery Criteria\n\n|\n\n55\n\n> db.users.find({\"username\" : {\"$ne\" : \"joe\"}})\n\n\"$ne\" can be used with any type.\n\nOR Queries There are two ways to do an OR query in MongoDB. \"$in\" can be used to query for a variety of values for a single key. \"$or\" is more general; it can be used to query for any of the given values across multiple keys.\n\nIf you have more than one possible value to match for a single key, use an array of criteria with \"$in\". For instance, suppose we’re running a raffle and the winning ticket numbers are 725, 542, and 390. To find all three of these documents, we can construct the following query:\n\n> db.raffle.find({\"ticket_no\" : {\"$in\" : [725, 542, 390]}})\n\n\"$in\" is very flexible and allows you to specify criteria of different types as well as values. For example, if we are gradually migrating our schema to use usernames instead of user ID numbers, we can query for either by using this:\n\n> db.users.find({\"user_id\" : {\"$in\" : [12345, \"joe\"]}})\n\nThis matches documents with a \"user_id\" equal to 12345 and documents with a \"user_id\" equal to \"joe\".\n\nIf \"$in\" is given an array with a single value, it behaves the same as directly matching the value. For instance, {ticket_no : {$in : [725]}} matches the same documents as {ticket_no : 725}.\n\nThe opposite of \"$in\" is \"$nin\", which returns documents that don’t match any of the criteria in the array. If we want to return all of the people who didn’t win anything in the raffle, we can query for them with this:\n\n> db.raffle.find({\"ticket_no\" : {\"$nin\" : [725, 542, 390]}})\n\nThis query returns everyone who did not have tickets with those numbers.\n\n\"$in\" gives you an OR query for a single key, but what if we need to find documents where \"ticket_no\" is 725 or \"winner\" is true? For this type of query, we’ll need to use the \"$or\" conditional. \"$or\" takes an array of possible criteria. In the raffle case, using \"$or\" would look like this:\n\n> db.raffle.find({\"$or\" : [{\"ticket_no\" : 725}, {\"winner\" : true}]})\n\n\"$or\" can contain other conditionals. If, for example, we want to match any of the three \"ticket_no\" values or the \"winner\" key, we can use this:\n\n> db.raffle.find({\"$or\" : [{\"ticket_no\" : {\"$in\" : [725, 542, 390]}}, ... {\"winner\" : true}]})\n\n56\n\n|\n\nChapter 4: Querying\n\nWith a normal AND-type query, you want to narrow down your results as far as pos‐ sible in as few arguments as possible. OR-type queries are the opposite: they are most efficient if the first arguments match as many documents as possible.\n\nWhile \"$or\" will always work, use \"$in\" whenever possible as the query optimizer handles it more efficiently.\n\n$not \"$not\" is a metaconditional: it can be applied on top of any other criteria. As an example, let’s consider the modulus operator, \"$mod\". \"$mod\" queries for keys whose values, when divided by the first value given, have a remainder of the second value:\n\n> db.users.find({\"id_num\" : {\"$mod\" : [5, 1]}})\n\nThe previous query returns users with \"id_num\"s of 1, 6, 11, 16, and so on. If we want, instead, to return users with \"id_num\"s of 2, 3, 4, 5, 7, 8, 9, 10, 12, etc., we can use \"$not\":\n\n> db.users.find({\"id_num\" : {\"$not\" : {\"$mod\" : [5, 1]}}})\n\n\"$not\" can be particularly useful in conjunction with regular expressions to find all documents that don’t match a given pattern (regular expression usage is described in the section “Regular Expressions” on page 58).\n\nType-Specific Queries As covered in Chapter 2, MongoDB has a wide variety of types that can be used in a document. Some of these types have special behavior when querying.\n\nnull null behaves a bit strangely. It does match itself, so if we have a collection with the following documents:\n\n> db.c.find() { \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523621\"), \"y\" : null } { \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523622\"), \"y\" : 1 } { \"_id\" : ObjectId(\"4ba0f148d22aa494fd523623\"), \"y\" : 2 }\n\nwe can query for documents whose \"y\" key is null in the expected way:\n\n> db.c.find({\"y\" : null}) { \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523621\"), \"y\" : null }\n\nHowever, null also matches “does not exist.” Thus, querying for a key with the value null will return all documents lacking that key:\n\n> db.c.find({\"z\" : null}) { \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523621\"), \"y\" : null }\n\nType-Specific Queries\n\n|\n\n57\n\n{ \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523622\"), \"y\" : 1 } { \"_id\" : ObjectId(\"4ba0f148d22aa494fd523623\"), \"y\" : 2 }\n\nIf we only want to find keys whose value is null, we can check that the key is null and exists using the \"$exists\" conditional:\n\n> db.c.find({\"z\" : {\"$eq\" : null, \"$exists\" : true}})\n\nRegular Expressions \"$regex\" provides regular expression capabilities for pattern matching strings in queries. Regular expressions are useful for flexible string matching. For example, if we want to find all users with the name “Joe” or “joe,” we can use a regular expression to do case-insensitive matching:\n\n> db.users.find( {\"name\" : {\"$regex\" : /joe/i } })\n\nRegular expression flags (e.g., i) are allowed but not required. If we want to match not only various capitalizations of “joe,” but also “joey,” we can continue to improve our regular expression:\n\n> db.users.find({\"name\" : /joey?/i})\n\nMongoDB uses the Perl Compatible Regular Expression (PCRE) library to match reg‐ ular expressions; any regular expression syntax allowed by PCRE is allowed in MongoDB. It is a good idea to check your syntax with the JavaScript shell before using it in a query to make sure it matches what you think it matches.\n\nMongoDB can leverage an index for queries on prefix regular expressions (e.g., /^joey/). Indexes cannot be used for case- insensitive searches (/^joey/i). A regular expression is a “prefix expression” when it starts with either a caret (^) or a left anchor (\\A). If the regular expression uses a case-sensitive query, then if an index exists for the field, the matches can be conducted against val‐ ues in the index. If it also is a prefix expression, then the search can be limited to the values within the range created by that prefix from the index.\n\nRegular expressions can also match themselves. Very few people insert regular expressions into the database, but if you insert one, you can match it with itself:\n\n> db.foo.insertOne({\"bar\" : /baz/}) > db.foo.find({\"bar\" : /baz/}) { \"_id\" : ObjectId(\"4b23c3ca7525f35f94b60a2d\"), \"bar\" : /baz/ }\n\n58\n\n|\n\nChapter 4: Querying\n\nQuerying Arrays Querying for elements of an array is designed to behave the way querying for scalars does. For example, if the array is a list of fruits, like this:\n\n> db.food.insertOne({\"fruit\" : [\"apple\", \"banana\", \"peach\"]})\n\nthe following query will successfully match the document:\n\n> db.food.find({\"fruit\" : \"banana\"})\n\nWe can query for it in much the same way as we would if we had a document that looked like the (illegal) document {\"fruit\" : \"apple\", \"fruit\" : \"banana\", \"fruit\" : \"peach\"}.\n\n“$all”\n\nIf you need to match arrays by more than one element, you can use \"$all\". This allows you to match a list of elements. For example, suppose we create a collection with three elements:\n\n> db.food.insertOne({\"_id\" : 1, \"fruit\" : [\"apple\", \"banana\", \"peach\"]}) > db.food.insertOne({\"_id\" : 2, \"fruit\" : [\"apple\", \"kumquat\", \"orange\"]}) > db.food.insertOne({\"_id\" : 3, \"fruit\" : [\"cherry\", \"banana\", \"apple\"]})\n\nThen we can find all documents with both \"apple\" and \"banana\" elements by query‐ ing with \"$all\":\n\n> db.food.find({fruit : {$all : [\"apple\", \"banana\"]}}) {\"_id\" : 1, \"fruit\" : [\"apple\", \"banana\", \"peach\"]} {\"_id\" : 3, \"fruit\" : [\"cherry\", \"banana\", \"apple\"]}\n\nOrder does not matter. Notice \"banana\" comes before \"apple\" in the second result. Using a one-element array with \"$all\" is equivalent to not using \"$all\". For instance, {fruit : {$all : ['apple']} will match the same documents as {fruit : 'apple'}.\n\nYou can also query by exact match using the entire array. However, exact match will not match a document if any elements are missing or superfluous. For example, this will match the first of our three documents:\n\n> db.food.find({\"fruit\" : [\"apple\", \"banana\", \"peach\"]})\n\nBut this will not:\n\n> db.food.find({\"fruit\" : [\"apple\", \"banana\"]})\n\nand neither will this:\n\n> db.food.find({\"fruit\" : [\"banana\", \"apple\", \"peach\"]})\n\nIf you want to query for a specific element of an array, you can specify an index using the syntax key.index:\n\nType-Specific Queries\n\n|\n\n59\n\n> db.food.find({\"fruit.2\" : \"peach\"})\n\nArrays are always 0-indexed, so this would match the third array element against the string \"peach\".\n\n“$size”\n\nA useful conditional for querying arrays is \"$size\", which allows you to query for arrays of a given size. Here’s an example:\n\n> db.food.find({\"fruit\" : {\"$size\" : 3}})\n\nOne common query is to get a range of sizes. \"$size\" cannot be combined with another $ conditional (in this example, \"$gt\"), but this query can be accomplished by adding a \"size\" key to the document. Then, every time you add an element to the array, increment the value of \"size\". If the original update looked like this:\n\n> db.food.update(criteria, {\"$push\" : {\"fruit\" : \"strawberry\"}})\n\nit can simply be changed to this:\n\n> db.food.update(criteria, ... {\"$push\" : {\"fruit\" : \"strawberry\"}, \"$inc\" : {\"size\" : 1}})\n\nIncrementing is extremely fast, so any performance penalty is negligible. Storing documents like this allows you to do queries such as this:\n\n> db.food.find({\"size\" : {\"$gt\" : 3}})\n\nUnfortunately, this technique doesn’t work as well with the \"$addToSet\" operator.\n\n“$slice”\n\nAs mentioned earlier in this chapter, the optional second argument to find specifies the keys to be returned. The special \"$slice\" operator can be used to return a subset of elements for an array key.\n\nFor example, suppose we had a blog post document and we wanted to return the first 10 comments:\n\n> db.blog.posts.findOne(criteria, {\"comments\" : {\"$slice\" : 10}})\n\nAlternatively, if we wanted the last 10 comments, we could use −10:\n\n> db.blog.posts.findOne(criteria, {\"comments\" : {\"$slice\" : -10}})\n\n\"$slice\" can also return pages in the middle of the results by taking an offset and the number of elements to return:\n\n> db.blog.posts.findOne(criteria, {\"comments\" : {\"$slice\" : [23, 10]}})\n\nThis would skip the first 23 elements and return the 24th through 33rd. If there were fewer than 33 elements in the array, it would return as many as possible.\n\n60\n\n|\n\nChapter 4: Querying\n\nUnless otherwise specified, all keys in a document are returned when \"$slice\" is used. This is unlike the other key specifiers, which suppress unmentioned keys from being returned. For instance, if we had a blog post document that looked like this:\n\n{ \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"title\" : \"A blog post\", \"content\" : \"...\", \"comments\" : [ { \"name\" : \"joe\", \"email\" : \"joe@example.com\", \"content\" : \"nice post.\" }, { \"name\" : \"bob\", \"email\" : \"bob@example.com\", \"content\" : \"good post.\" } ] }\n\nand we did a \"$slice\" to get the last comment, we’d get this:\n\n> db.blog.posts.findOne(criteria, {\"comments\" : {\"$slice\" : -1}}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"title\" : \"A blog post\", \"content\" : \"...\", \"comments\" : [ { \"name\" : \"bob\", \"email\" : \"bob@example.com\", \"content\" : \"good post.\" } ] }\n\nBoth \"title\" and \"content\" are still returned, even though they weren’t explicitly included in the key specifier.\n\nReturning a matching array element\n\n\"$slice\" is helpful when you know the index of the element, but sometimes you want whichever array element matched your criteria. You can return the matching element with the $ operator. Given the previous blog example, you could get Bob’s comment back with:\n\n> db.blog.posts.find({\"comments.name\" : \"bob\"}, {\"comments.$\" : 1}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"comments\" : [\n\nType-Specific Queries\n\n|\n\n61\n\n{ \"name\" : \"bob\", \"email\" : \"bob@example.com\", \"content\" : \"good post.\" } ] }\n\nNote that this only returns the first match for each document: if Bob had left multiple comments on this post, only the first one in the \"comments\" array would be returned.\n\nArray and range query interactions\n\nScalars (nonarray elements) in documents must match each clause of a query’s crite‐ ria. For example, if you queried for {\"x\" : {\"$gt\" : 10, \"$lt\" : 20}}, \"x\" would have to be both greater than 10 and less than 20. However, if a document’s \"x\" field is an array, the document matches if there is an element of \"x\" that matches each part of the criteria but each query clause can match a different array element.\n\nThe best way to understand this behavior is to see an example. Suppose we have the following documents:\n\n{\"x\" : 5} {\"x\" : 15} {\"x\" : 25} {\"x\" : [5, 25]}\n\nIf we wanted to find all documents where \"x\" is between 10 and 20, we might naively structure a query as db.test.find({\"x\" : {\"$gt\" : 10, \"$lt\" : 20}}) and expect to get back one document: {\"x\" : 15}. However, running this, we get two:\n\n> db.test.find({\"x\" : {\"$gt\" : 10, \"$lt\" : 20}}) {\"x\" : 15} {\"x\" : [5, 25]}\n\nNeither 5 nor 25 is between 10 and 20, but the document is returned because 25 matches the first clause (it is greater than 10) and 5 matches the second clause (it is less than 20).\n\nThis makes range queries against arrays essentially useless: a range will match any multielement array. There are a couple of ways to get the expected behavior.\n\nFirst, you can use \"$elemMatch\" to force MongoDB to compare both clauses with a single array element. However, the catch is that \"$elemMatch\" won’t match nonarray elements:\n\n> db.test.find({\"x\" : {\"$elemMatch\" : {\"$gt\" : 10, \"$lt\" : 20}}}) > // no results\n\nThe document {\"x\" : 15} no longer matches the query, because the \"x\" field is not an array. That said, you should have a good reason for mixing array and scalar values\n\n62\n\n|\n\nChapter 4: Querying\n\nin a field. Many uses cases do not require mixing. For those, \"$elemMatch\" provides a good solution for range queries on array elements.\n\nIf you have an index over the field that you’re querying on (see Chapter 5), you can use min and max to limit the index range traversed by the query to your \"$gt\" and \"$lt\" values:\n\n> db.test.find({\"x\" : {\"$gt\" : 10, \"$lt\" : 20}}).min({\"x\" : 10}).max({\"x\" : 20}) {\"x\" : 15}\n\nNow this will only traverse the index from 10 to 20, missing the 5 and 25 entries. You can only use min and max when you have an index on the field you are querying for, though, and you must pass all fields of the index to min and max.\n\nUsing min and max when querying for ranges over documents that may include arrays is generally a good idea. The index bounds for a \"$gt\"/\"$lt\" query over an array is inefficient. It basically accepts any value, so it will search every index entry, not just those in the range.\n\nQuerying on Embedded Documents There are two ways of querying for an embedded document: querying for the whole document or querying for its individual key/value pairs.\n\nQuerying for an entire embedded document works identically to a normal query. For example, if we have a document that looks like this:\n\n{ \"name\" : { \"first\" : \"Joe\", \"last\" : \"Schmoe\" }, \"age\" : 45 }\n\nwe can query for someone named Joe Schmoe with the following:\n\n> db.people.find({\"name\" : {\"first\" : \"Joe\", \"last\" : \"Schmoe\"}})\n\nHowever, a query for a full subdocument must exactly match the subdocument. If Joe decides to add a middle name field, suddenly this query won’t work anymore; it doesn’t match the entire embedded document! This type of query is also order- sensitive: {\"last\" : \"Schmoe\", \"first\" : \"Joe\"} would not be a match.\n\nIf possible, it’s usually a good idea to query for just a specific key or keys of an embed‐ ded document. Then, if your schema changes, all of your queries won’t suddenly break because they’re no longer exact matches. You can query for embedded keys using dot notation:\n\n> db.people.find({\"name.first\" : \"Joe\", \"name.last\" : \"Schmoe\"})\n\nType-Specific Queries\n\n|\n\n63",
      "page_number": 71
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 81-92)",
      "start_page": 81,
      "end_page": 92,
      "detection_method": "topic_boundary",
      "content": "Now, if Joe adds more keys, this query will still match his first and last names.\n\nThis dot notation is the main difference between query documents and other docu‐ ment types. Query documents can contain dots, which mean “reach into an embed‐ ded document.” Dot notation is also the reason that documents to be inserted cannot contain the . character. Oftentimes people run into this limitation when trying to save URLs as keys. One way to get around it is to always perform a global replace before inserting or after retrieving, substituting a character that isn’t legal in URLs for the dot character.\n\nEmbedded document matches can get a little tricky as the document structure gets more complicated. For example, suppose we are storing blog posts and we want to find comments by Joe that were scored at least a 5. We could model the post as follows:\n\n> db.blog.find() { \"content\" : \"...\", \"comments\" : [ { \"author\" : \"joe\", \"score\" : 3, \"comment\" : \"nice post\" }, { \"author\" : \"mary\", \"score\" : 6, \"comment\" : \"terrible post\" } ] }\n\nNow, we can’t query using db.blog.find({\"comments\" : {\"author\" : \"joe\", \"score\" : {\"$gte\" : 5}}}). Embedded document matches have to match the whole document, and this doesn’t match the \"comment\" key. It also wouldn’t work to do db.blog.find({\"comments.author\" : \"joe\", \"comments.score\" : {\"$gte\" : 5}}), because the author criterion could match a different comment than the score criterion. That is, it would return the document shown above: it would match \"author\" : \"joe\" in the first comment and \"score\" : 6 in the second comment.\n\nTo correctly group criteria without needing to specify every key, use \"$elemMatch\". This vaguely named conditional allows you to partially specify criteria to match a sin‐ gle embedded document in an array. The correct query looks like this:\n\n> db.blog.find({\"comments\" : {\"$elemMatch\" : ... {\"author\" : \"joe\", \"score\" : {\"$gte\" : 5}}}})\n\n\"$elemMatch\" allows you to “group” your criteria. As such, it’s only needed when you have more than one key you want to match on in an embedded document.\n\n64\n\n|\n\nChapter 4: Querying\n\n$where Queries Key/value pairs are a fairly expressive way to query, but there are some queries that they cannot represent. For queries that cannot be done any other way, there are \"$where\" clauses, which allow you to execute arbitrary JavaScript as part of your query. This allows you to do (almost) anything within a query. For security, use of \"$where\" clauses should be highly restricted or eliminated. End users should never be allowed to execute arbitrary \"$where\" clauses.\n\nThe most common case for using \"$where\" is to compare the values for two keys in a document. For instance, suppose we have documents that look like this:\n\n> db.foo.insertOne({\"apple\" : 1, \"banana\" : 6, \"peach\" : 3}) > db.foo.insertOne({\"apple\" : 8, \"spinach\" : 4, \"watermelon\" : 4})\n\nWe’d like to return documents where any two of the fields are equal. For example, in the second document, \"spinach\" and \"watermelon\" have the same value, so we’d like that document returned. It’s unlikely MongoDB will ever have a $ conditional for this, so we can use a \"$where\" clause to do it with JavaScript:\n\n> db.foo.find({\"$where\" : function () { ... for (var current in this) { ... for (var other in this) { ... if (current != other && this[current] == this[other]) { ... return true; ... } ... } ... } ... return false; ... }});\n\nIf the function returns true, the document will be part of the result set; if it returns false, it won’t be.\n\n\"$where\" queries should not be used unless strictly necessary: they are much slower than regular queries. Each document has to be converted from BSON to a JavaScript object and then run through the \"$where\" expression. Indexes cannot be used to sat‐ isfy a \"$where\" either. Hence, you should use \"$where\" only when there is no other way of doing the query. You can cut down on the penalty by using other query filters in combination with \"$where\". If possible, an index will be used to filter based on the non-$where clauses; the \"$where\" expression will be used only to fine-tune the results. MongoDB 3.6 added the $expr operator which allows the use of aggregation expressions with the MongoDB query language. It is faster than $where as it does not execute JavaScript and is recommended as a replacement to this operator where possible.\n\nAnother way of doing complex queries is to use one of the aggregation tools, which are covered in Chapter 7.\n\n$where Queries\n\n|\n\n65\n\nCursors The database returns results from find using a cursor. The client-side implementa‐ tions of cursors generally allow you to control a great deal about the eventual output of a query. You can limit the number of results, skip over some number of results, sort results by any combination of keys in any direction, and perform a number of other powerful operations.\n\nTo create a cursor with the shell, put some documents into a collection, do a query on them, and assign the results to a local variable (variables defined with \"var\" are local). Here, we create a very simple collection and query it, storing the results in the cursor variable:\n\n> for(i=0; i<100; i++) { ... db.collection.insertOne({x : i}); ... } > var cursor = db.collection.find();\n\nThe advantage of doing this is that you can look at one result at a time. If you store the results in a global variable or no variable at all, the MongoDB shell will automati‐ cally iterate through and display the first couple of documents. This is what we’ve been seeing up until this point, and it is often the behavior you want for seeing what’s in a collection but not doing actual programming with the shell.\n\nTo iterate through the results, you can use the next method on the cursor. You can use hasNext to check whether there is another result. A typical loop through result looks like the following:\n\n> while (cursor.hasNext()) { ... obj = cursor.next(); ... // do stuff ... }\n\ncursor.hasNext() checks that the next result exists, and cursor.next() fetches it.\n\nThe cursor class also implements JavaScript’s iterator interface, so you can use it in a forEach loop:\n\n> var cursor = db.people.find(); > cursor.forEach(function(x) { ... print(x.name); ... }); adam matt zak\n\nWhen you call find, the shell does not query the database immediately. It waits until you start requesting results to send the query, which allows you to chain additional options onto a query before it is performed. Almost every method on a cursor object\n\n66\n\n|\n\nChapter 4: Querying\n\nreturns the cursor itself, so that you can chain options in any order. For instance, all of the following are equivalent:\n\n> var cursor = db.foo.find().sort({\"x\" : 1}).limit(1).skip(10); > var cursor = db.foo.find().limit(1).sort({\"x\" : 1}).skip(10); > var cursor = db.foo.find().skip(10).limit(1).sort({\"x\" : 1});\n\nAt this point, the query has not been executed yet. All of these functions merely build the query. Now, suppose we call the following:\n\n> cursor.hasNext()\n\nAt this point, the query will be sent to the server. The shell fetches the first 100 results or first 4 MB of results (whichever is smaller) at once so that the next calls to next or hasNext will not have to make trips to the server. After the client has run through the first set of results, the shell will again contact the database and ask for more results with a getMore request. getMore requests basically contain an identifier for the cursor and ask the database if there are any more results, returning the next batch if there are. This process continues until the cursor is exhausted and all results have been returned.\n\nLimits, Skips, and Sorts The most common query options are limiting the number of results returned, skip‐ ping a number of results, and sorting. All these options must be added before a query is sent to the database.\n\nTo set a limit, chain the limit function onto your call to find. For example, to only return three results, use this:\n\n> db.c.find().limit(3)\n\nIf there are fewer than three documents matching your query in the collection, only the number of matching documents will be returned; limit sets an upper limit, not a lower limit.\n\nskip works similarly to limit:\n\n> db.c.find().skip(3)\n\nThis will skip the first three matching documents and return the rest of the matches. If there are fewer than three documents in your collection, it will not return any documents.\n\nsort takes an object: a set of key/value pairs where the keys are key names and the values are the sort directions. The sort direction can be 1 (ascending) or −1 (descend‐ ing). If multiple keys are given, the results will be sorted in that order. For instance, to sort the results by \"username\" ascending and \"age\" descending, we do the following:\n\n> db.c.find().sort({username : 1, age : -1})\n\nCursors\n\n|\n\n67\n\nThese three methods can be combined. This is often handy for pagination. For exam‐ ple, suppose that you are running an online store and someone searches for mp3. If you want 50 results per page sorted by price from high to low, you can do the following:\n\n> db.stock.find({\"desc\" : \"mp3\"}).limit(50).sort({\"price\" : -1})\n\nIf that person clicks Next Page to see more results, you can simply add a skip to the query, which will skip over the first 50 matches (which the user already saw on page 1):\n\n> db.stock.find({\"desc\" : \"mp3\"}).limit(50).skip(50).sort({\"price\" : -1})\n\nHowever, large skips are not very performant; there are suggestions for how to avoid them in the next section.\n\nComparison order\n\nMongoDB has a hierarchy as to how types compare. Sometimes you will have a single key with multiple types: for instance, integers and booleans, or strings and nulls. If you do a sort on a key with a mix of types, there is a predefined order that they will be sorted in. From least to greatest value, this ordering is as follows:\n\n1. Minimum value\n\n2. Null\n\n3. Numbers (integers, longs, doubles, decimals)\n\n4. Strings\n\n5. Object/document\n\n6. Array\n\n7. Binary data\n\n8. Object ID\n\n9. Boolean\n\n10. Date\n\n11. Timestamp\n\n12. Regular expression\n\n13. Maximum value\n\nAvoiding Large Skips Using skip for a small number of documents is fine. But for a large number of results, skip can be slow, since it has to find and then discard all the skipped results. Most\n\n68\n\n|\n\nChapter 4: Querying\n\ndatabases keep more metadata in the index to help with skips, but MongoDB does not yet support this, so large skips should be avoided. Often you can calculate the results of the next query based on the previous one.\n\nPaginating results without skip\n\nThe easiest way to do pagination is to return the first page of results using limit and then return each subsequent page as an offset from the beginning:\n\n> // do not use: slow for large skips > var page1 = db.foo.find(criteria).limit(100) > var page2 = db.foo.find(criteria).skip(100).limit(100) > var page3 = db.foo.find(criteria).skip(200).limit(100) ...\n\nHowever, depending on your query, you can usually find a way to paginate without skips. For example, suppose we want to display documents in descending order based on \"date\". We can get the first page of results with the following:\n\n> var page1 = db.foo.find().sort({\"date\" : -1}).limit(100)\n\nThen, assuming the date is unique, we can use the \"date\" value of the last document as the criterion for fetching the next page:\n\nvar latest = null;\n\n// display first page while (page1.hasNext()) { latest = page1.next(); display(latest); }\n\n// get next page var page2 = db.foo.find({\"date\" : {\"$lt\" : latest.date}}); page2.sort({\"date\" : -1}).limit(100);\n\nNow the query does not need to include a skip.\n\nFinding a random document\n\nOne fairly common problem is how to get a random document from a collection. The naive (and slow) solution is to count the number of documents and then do a find, skipping a random number of documents between zero and the size of the collection:\n\n> // do not use > var total = db.foo.count() > var random = Math.floor(Math.random()*total) > db.foo.find().skip(random).limit(1)\n\nIt is actually highly inefficient to get a random element this way: you have to do a count (which can be expensive if you are using criteria), and skipping large numbers of elements can be time-consuming.\n\nCursors\n\n|\n\n69\n\nIt takes a little forethought, but if you know you’ll be looking up a random element in a collection, there’s a much more efficient way to do so. The trick is to add an extra random key to each document when it is inserted. For instance, if we’re using the shell, we could use the Math.random() function (which creates a random number between 0 and 1):\n\n> db.people.insertOne({\"name\" : \"joe\", \"random\" : Math.random()}) > db.people.insertOne({\"name\" : \"john\", \"random\" : Math.random()}) > db.people.insertOne({\"name\" : \"jim\", \"random\" : Math.random()})\n\nNow, when we want to find a random document from the collection, we can calculate a random number and use that as a query criterion, instead of using skip:\n\n> var random = Math.random() > result = db.people.findOne({\"random\" : {\"$gt\" : random}})\n\nThere is a slight chance that random will be greater than any of the \"random\" values in the collection, and no results will be returned. We can guard against this by simply returning a document in the other direction:\n\n> if (result == null) { ... result = db.people.findOne({\"random\" : {\"$lte\" : random}}) ... }\n\nIf there aren’t any documents in the collection, this technique will end up returning null, which makes sense.\n\nThis technique can be used with arbitrarily complex queries; just make sure to have an index that includes the random key. For example, if we want to find a random plumber in California, we can create an index on \"profession\", \"state\", and \"random\":\n\n> db.people.ensureIndex({\"profession\" : 1, \"state\" : 1, \"random\" : 1})\n\nThis allows us to quickly find a random result (see Chapter 5 for more information on indexing).\n\nImmortal Cursors There are two sides to a cursor: the client-facing cursor and the database cursor that the client-side one represents. We have been talking about the client-side one up until now, but we are going to take a brief look at what’s happening on the server.\n\nOn the server side, a cursor takes up memory and resources. Once a cursor runs out of results or the client sends a message telling it to die, the database can free the resources it was using. Freeing these resources lets the database use them for other things, which is good, so we want to make sure that cursors can be freed quickly (within reason).\n\n70\n\n|\n\nChapter 4: Querying\n\nThere are a couple of conditions that can cause the death (and subsequent cleanup) of a cursor. First, when a cursor finishes iterating through the matching results, it will clean itself up. Another way is that, when a cursor goes out of scope on the client side, the drivers send the database a special message to let it know that it can kill that cur‐ sor. Finally, even if the user hasn’t iterated through all the results and the cursor is still in scope, after 10 minutes of inactivity, a database cursor will automatically “die.” This way, if a client crashes or is buggy, MongoDB will not be left with thousands of open cursors.\n\nThis “death by timeout” is usually the desired behavior: very few applications expect their users to sit around for minutes at a time waiting for results. However, some‐ times you might know that you need a cursor to last for a long time. In that case, many drivers have implemented a function called immortal, or a similar mechanism, which tells the database not to time out the cursor. If you turn off a cursor’s timeout, you must iterate through all of its results or kill it to make sure it gets closed. Other‐ wise, it will sit around in the database hogging resources until the server is restarted.\n\nCursors\n\n|\n\n71\n\nPART II Designing Your Application\n\nCHAPTER 5 Indexes\n\nThis chapter introduces MongoDB indexes. Indexes enable you to perform queries efficiently. They’re an important part of application development and are even required for certain types of queries. In this chapter we will cover:\n\nWhat indexes are and why you’d want to use them\n\nHow to choose which fields to index\n\nHow to enforce and evaluate index usage\n\nAdministrative details on creating and removing indexes\n\nAs you’ll see, choosing the right indexes for your collections is critical to performance.\n\nIntroduction to Indexes A database index is similar to a book’s index. Instead of looking through the whole book, the database takes a shortcut and just looks at an ordered list with references to the content. This allows MongoDB to query orders of magnitude faster.\n\nA query that does not use an index is called a collection scan, which means that the server has to “look through the whole book” to find a query’s results. This process is basically what you’d do if you were looking for information in a book without an index: you’d start at page 1 and read through the whole thing. In general, you want to avoid making the server do collection scans because the process is very slow for large collections.\n\nLet’s look at an example. To get started, we’ll create a collection with 1 million docu‐ ments in it (or 10 million or 100 million, if you have the patience):\n\n75\n\n> for (i=0; i<1000000; i++) { ... db.users.insertOne( ... { ... \"i\" : i, ... \"username\" : \"user\"+i, ... \"age\" : Math.floor(Math.random()*120), ... \"created\" : new Date() ... } ... ); ... }\n\nThen we’ll look at the differences in performance for queries on this collection, first without an index and then with an index.\n\nIf we do a query on this collection, we can use the explain command to see what MongoDB is doing when it executes the query. The preferred way to use the explain command is through the cursor helper method that wraps this command. The explain cursor method provides information on the execution of a variety of CRUD operations. This method may be run in several verbosity modes. We’ll look at execu tionStats mode since this helps us understand the effect of using an index to satisfy queries. Try querying on a specific username to see an example:\n\n> db.users.find({\"username\": \"user101\"}).explain(\"executionStats\") { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"username\" : { \"$eq\" : \"user101\" } }, \"winningPlan\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"username\" : { \"$eq\" : \"user101\" } }, \"direction\" : \"forward\" }, \"rejectedPlans\" : [ ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 1, \"executionTimeMillis\" : 419, \"totalKeysExamined\" : 0, \"totalDocsExamined\" : 1000000, \"executionStages\" : {\n\n76\n\n|\n\nChapter 5: Indexes\n\n\"stage\" : \"COLLSCAN\", \"filter\" : { \"username\" : { \"$eq\" : \"user101\" } }, \"nReturned\" : 1, \"executionTimeMillisEstimate\" : 375, \"works\" : 1000002, \"advanced\" : 1, \"needTime\" : 1000000, \"needYield\" : 0, \"saveState\" : 7822, \"restoreState\" : 7822, \"isEOF\" : 1, \"invalidates\" : 0, \"direction\" : \"forward\", \"docsExamined\" : 1000000 } }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\n“explain Output” on page 116 will explain the output fields; for now you can ignore almost all of them. For this example, we want to look at the nested document that is the value of the \"executionStats\" field. In this document, \"totalDocsExamined\" is the number of documents MongoDB looked at while trying to satisfy the query, which, as you can see, is every document in the collection. That is, MongoDB had to look through every field in every document. This took nearly half a second to accom‐ plish on my laptop (the \"executionTimeMillis\" field shows the number of milli‐ seconds it took to execute the query).\n\nThe \"nReturned\" field of the \"executionStats\" document shows the number of results returned: 1, which makes sense because there is only one user with the user‐ name \"user101\". Note that MongoDB had to look through every document in the collection for matches because it did not know that usernames are unique.\n\nTo enable MongoDB to respond to queries efficiently, all query patterns in your appli‐ cation should be supported by an index. By query patterns, we simply mean the dif‐ ferent types of questions your application asks of the database. In this example, we queried the users collection by username. That is an example of a specific query pat‐ tern. In many applications, a single index will support several query patterns. We will discuss tailoring indexes to query patterns in a later section.\n\nIntroduction to Indexes\n\n|\n\n77",
      "page_number": 81
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 93-102)",
      "start_page": 93,
      "end_page": 102,
      "detection_method": "topic_boundary",
      "content": "Creating an Index Now let’s try creating an index on the \"username\" field. To create an index, we’ll use the createIndex collection method:\n\n> db.users.createIndex({\"username\" : 1}) { \"createdCollectionAutomatically\" : false, \"numIndexesBefore\" : 1, \"numIndexesAfter\" : 2, \"ok\" : 1 }\n\nCreating the index should take no longer than a few seconds, unless you made your collection especially large. If the createIndex call does not return after a few seconds, run db.currentOp() (in a different shell) or check your mongod’s log to see the index build’s progress.\n\nOnce the index build is complete, try repeating the original query:\n\n> db.users.find({\"username\": \"user101\"}).explain(\"executionStats\") { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"username\" : { \"$eq\" : \"user101\" } }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"[\\\"user101\\\", \\\"user101\\\"]\" ]\n\n78\n\n|\n\nChapter 5: Indexes\n\n} } }, \"rejectedPlans\" : [ ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 1, \"executionTimeMillis\" : 1, \"totalKeysExamined\" : 1, \"totalDocsExamined\" : 1, \"executionStages\" : { \"stage\" : \"FETCH\", \"nReturned\" : 1, \"executionTimeMillisEstimate\" : 0, \"works\" : 2, \"advanced\" : 1, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 0, \"restoreState\" : 0, \"isEOF\" : 1, \"invalidates\" : 0, \"docsExamined\" : 1, \"alreadyHasObj\" : 0, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"nReturned\" : 1, \"executionTimeMillisEstimate\" : 0, \"works\" : 2, \"advanced\" : 1, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 0, \"restoreState\" : 0, \"isEOF\" : 1, \"invalidates\" : 0, \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [\n\nIntroduction to Indexes\n\n|\n\n79\n\n\"[\\\"user101\\\", \\\"user101\\\"]\" ] }, \"keysExamined\" : 1, \"seeks\" : 1, \"dupsTested\" : 0, \"dupsDropped\" : 0, \"seenInvalidated\" : 0 } } }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nThis explain output is more complex, but for now you can continue to ignore all the fields other than \"nReturned\", \"totalDocsExamined\", and \"executionTimeMillis\" in the \"executionStats\" nested document. As you can see, the query is now almost instantaneous and, even better, has a similar runtime when querying, for example, for any username:\n\n> db.users.find({\"username\": \"user999999\"}).explain(\"executionStats\")\n\nAn index can make a dramatic difference in query times. However, indexes have their price: write operations (inserts, updates, and deletes) that modify an indexed field will take longer. This is because in addition to updating the document, MongoDB has to update indexes when your data changes. Typically, the tradeoff is worth it. The tricky part becomes figuring out which fields to index.\n\nMongoDB’s indexes work almost identically to typical relational database indexes, so if you are familiar with those, you can just skim this section for syntax specifics.\n\nTo choose which fields to create indexes for, look through your frequent queries and queries that need to be fast and try to find a common set of keys from those. For instance, in the preceding example, we were querying on \"username\". If that were a particularly common query or were becoming a bottleneck, indexing \"username\" would be a good choice. However, if this were an unusual query or one that’s only done by administrators who don’t care how long it takes, it would not be a good choice for indexing.\n\n80\n\n|\n\nChapter 5: Indexes\n\nIntroduction to Compound Indexes The purpose of an index is to make your queries as efficient as possible. For many query patterns it is necessary to build indexes based on two or more keys. For exam‐ ple, an index keeps all of its values in a sorted order, so it makes sorting documents by the indexed key much faster. However, an index can only help with sorting if it is a prefix of the sort. For example, the index on \"username\" wouldn’t help much for this sort:\n\n> db.users.find().sort({\"age\" : 1, \"username\" : 1})\n\nThis sorts by \"age\" and then \"username\", so a strict sorting by \"username\" isn’t terri‐ bly helpful. To optimize this sort, you could make an index on \"age\" and \"username\":\n\n> db.users.createIndex({\"age\" : 1, \"username\" : 1})\n\nThis is called a compound index and is useful if your query has multiple sort direc‐ tions or multiple keys in the criteria. A compound index is an index on more than one field.\n\nSuppose we have a users collection that looks something like this, if we run a query with no sorting (called natural order):\n\n> db.users.find({}, {\"_id\" : 0, \"i\" : 0, \"created\" : 0}) { \"username\" : \"user0\", \"age\" : 69 } { \"username\" : \"user1\", \"age\" : 50 } { \"username\" : \"user2\", \"age\" : 88 } { \"username\" : \"user3\", \"age\" : 52 } { \"username\" : \"user4\", \"age\" : 74 } { \"username\" : \"user5\", \"age\" : 104 } { \"username\" : \"user6\", \"age\" : 59 } { \"username\" : \"user7\", \"age\" : 102 } { \"username\" : \"user8\", \"age\" : 94 } { \"username\" : \"user9\", \"age\" : 7 } { \"username\" : \"user10\", \"age\" : 80 } ...\n\nIf we index this collection by {\"age\" : 1, \"username\" : 1}, the index will have a form we can represent as follows:\n\n[0, \"user100020\"] -> 8623513776 [0, \"user1002\"] -> 8599246768 [0, \"user100388\"] -> 8623560880 ... [0, \"user100414\"] -> 8623564208 [1, \"user100113\"] -> 8623525680 [1, \"user100280\"] -> 8623547056 [1, \"user100551\"] -> 8623581744 ... [1, \"user100626\"] -> 8623591344 [2, \"user100191\"] -> 8623535664\n\nIntroduction to Indexes\n\n|\n\n81\n\n[2, \"user100195\"] -> 8623536176 [2, \"user100197\"] -> 8623536432 ...\n\nEach index entry contains an age and a username and points to a record identifier. A record identifier is used internally by the storage engine to locate the data for a docu‐ ment. Note that \"age\" fields are ordered to be strictly ascending and, within each age, usernames are also in ascending order. In this example dataset, each age has approxi‐ mately 8,000 usernames associated with it. Here we’ve included only those necessary to convey the general idea.\n\nThe way MongoDB uses this index depends on the type of query you’re doing. These are the three most common ways:\n\ndb.users.find({\"age\" : 21}).sort({\"username\" : -1})\n\nThis is an equality query, which searches for a single value. There may be multi‐ ple documents with that value. Due to the second field in the index, the results are already in the correct order for the sort: MongoDB can start with the last match for {\"age\" : 21} and traverse the index in order:\n\n[21, \"user100154\"] -> 8623530928 [21, \"user100266\"] -> 8623545264 [21, \"user100270\"] -> 8623545776 [21, \"user100285\"] -> 8623547696 [21, \"user100349\"] -> 8623555888 ...\n\nThis type of query is very efficient: MongoDB can jump directly to the correct age and doesn’t need to sort the results because traversing the index returns the data in the correct order.\n\nNote that sort direction doesn’t matter: MongoDB can traverse the index in either direction.\n\ndb.users.find({\"age\" : {\"$gte\" : 21, \"$lte\" : 30}})\n\nThis is a range query, which looks for documents matching multiple values (in this case, all ages between 21 and 30). MongoDB will use the first key in the index, \"age\", to return the matching documents, like so:\n\n[21, \"user100154\"] -> 8623530928 [21, \"user100266\"] -> 8623545264 [21, \"user100270\"] -> 8623545776 ... [21, \"user999390\"] -> 8765250224 [21, \"user999407\"] -> 8765252400 [21, \"user999600\"] -> 8765277104 [22, \"user100017\"] -> 8623513392 ... [29, \"user999861\"] -> 8765310512\n\n82\n\n|\n\nChapter 5: Indexes\n\n[30, \"user100098\"] -> 8623523760 [30, \"user100155\"] -> 8623531056 [30, \"user100168\"] -> 8623532720 ...\n\nIn general, if MongoDB uses an index for a query it will return the resulting documents in index order.\n\ndb.users.find({\"age\" : {\"$gte\" : 21, \"$lte\" : 30}}).sort({\"username\" : 1})\n\nThis is a multivalue query, like the previous one, but this time it has a sort. As before, MongoDB will use the index to match the criteria. However, the index doesn’t return the usernames in sorted order and the query requested that the results be sorted by username. This means MongoDB will need to sort the results in memory before returning them, rather than simply traversing an index in which the documents are already sorted in the desired order. This type of query is usually less efficient as a consequence.\n\nOf course, the speed depends on how many results match your criteria: if your result set is only a couple of documents MongoDB won’t have much work to do to sort them, but if there are more results it will be slower or may not work at all. If you have more than 32 MB of results MongoDB will just error out, refusing to sort that much data:\n\nError: error: { \"ok\" : 0, \"errmsg\" : \"Executor error during find command: OperationFailed: Sort operation used more than the maximum 33554432 bytes of RAM. Add an index, or specify a smaller limit.\", \"code\" : 96, \"codeName\" : \"OperationFailed\" }\n\nIf you need to avoid this error, then you must create an index sup‐ porting the sort operation (https://docs.mongodb.com/manual/refer ence/method/cursor.sort/index.html#sort-index-use) or use sort in conjunction with limit to reduce the results to below 32 MB.\n\nOne other index you can use in the last example is the same keys in reverse order: {\"username\" : 1, \"age\" : 1}. MongoDB will then traverse all the index entries, but in the order you want them back in. It will pick out the matching documents using the \"age\" part of the index:\n\nIntroduction to Indexes\n\n|\n\n83\n\n[user0, 4] [user1, 67] [user10, 11] [user100, 92] [user1000, 10] [user10000, 31] [user100000, 21] -> 8623511216 [user100001, 52] [user100002, 69] [user100003, 27] -> 8623511600 [user100004, 22] -> 8623511728 [user100005, 95] ...\n\nThis is good in that it does not require any giant in-memory sorts. However, it does have to scan the entire index to find all the matches. Putting the sort key first is gen‐ erally a good strategy when designing compound indexes. As we’ll see shortly, this is one of several best practices when considering how to construct compound indexes with consideration for equality queries, multivalue queries, and sorting.\n\nHow MongoDB Selects an Index Now let’s take a look at how MongoDB chooses an index to satisfy a query. Let’s imag‐ ine we have five indexes. When a query comes in, MongoDB looks at the query’s shape. The shape has to do with what fields are being searched on and additional information, such as whether or not there is a sort. Based on that information, the system identifies a set of candidate indexes that it might be able to use in satisfying the query.\n\nLet’s assume we have a query come in, and three of our five indexes are identified as candidates for this query. MongoDB will then create three query plans, one for each of these indexes, and run the query in three parallel threads, each using a different index. The objective here is to see which one is able to return results the fastest.\n\nVisually, we can think of this as a race, as pictured in Figure 5-1. The idea here is that the first query plan to reach a goal state is the winner. But more importantly, going forward it will be selected as the index to use for queries that have that same query shape. The plans are raced against each other for a period (referred to as the trial period), after which the results of each race are used to calculate the overall winning plan.\n\n84\n\n|\n\nChapter 5: Indexes\n\nFigure 5-1. How the MongoDB Query Planner selects an index, visualized as a race\n\nTo win the race, a query thread must be the first to either return all the query results or return a trial number of results in sort order. The sort order portion of this is important given how expensive it is to perform in-memory sorts.\n\nThe real value of racing several query plans against one another is that for subsequent queries that have the same query shape, the MongoDB server will know which index to select. The server maintains a cache of query plans. A winning plan is stored in the cache for future use for queries of that shape. Over time, as a collection changes and as the indexes change, eventually a query plan might be evicted from the cache and MongoDB will, again, experiment with possible query plans to find the one that works best for the current collection and set of indexes. Other events that will lead to plans being evicted from the cache are if we rebuild a given index, add or drop an index, or explicitly clear the plan cache. Finally, the query plan cache does not survive a restart of a mongod process.\n\nUsing Compound Indexes In the previous sections, we’ve been using compound indexes, which are indexes with more than one key in them. Compound indexes are a little more complicated to think about than single-key indexes, but they are very powerful. This section covers them in more depth.\n\nIntroduction to Indexes\n\n|\n\n85\n\nHere, we will walk through an example that gives you an idea of the type of thinking you need to do when you are designing compound indexes. The goal is for our read and write operations to be as efficient as possible—but as with so many things, this requires some upfront thinking and some experimentation.\n\nTo be sure we get the right indexes in place, it is necessary to test our indexes under some real-world workloads and make adjustments from there. However, there are some best practices we can apply as we design our indexes.\n\nFirst, we need to consider the selectivity of the index. We are interested in the degree to which, for a given query pattern, the index is going to minimize the number of records scanned. We need to consider selectivity in light of all operations necessary to satisfy a query, and sometimes make tradeoffs. We will need to consider, for example, how sorts are handled.\n\nLet’s look at an example. For this, we will use a student dataset containing approxi‐ mately one million records. Documents in this dataset resemble the following:\n\n{ \"_id\" : ObjectId(\"585d817db4743f74e2da067c\"), \"student_id\" : 0, \"scores\" : [ { \"type\" : \"exam\", \"score\" : 38.05000060199827 }, { \"type\" : \"quiz\", \"score\" : 79.45079445008987 }, { \"type\" : \"homework\", \"score\" : 74.50150548699534 }, { \"type\" : \"homework\", \"score\" : 74.68381684615845 } ], \"class_id\" : 127 }\n\nWe will begin with two indexes and look at how MongoDB uses these indexes (or doesn’t) in order to satisfy queries. These two indexes are created as follows:\n\n> db.students.createIndex({\"class_id\": 1}) > db.students.createIndex({student_id: 1, class_id: 1})\n\nIn working with this dataset, we will consider the following query, because it illus‐ trates several of the issues that we have to think about in designing our indexes:\n\n86\n\n|\n\nChapter 5: Indexes\n\n> db.students.find({student_id:{$gt:500000}, class_id:54}) ... .sort({student_id:1}) ... .explain(\"executionStats\")\n\nNote that in this query we are requesting all records with an ID greater than 500,000, so about half of the records. We are also constraining the search to records for the class with ID 54. There are about 500 classes represented in this dataset. Finally, we are sorting in ascending order based on \"student_id\". Note that this is the same field on which we are doing a multivalue query. Throughout this example we will look at the execution stats that the explain method provides to illustrate how MongoDB will handle this query.\n\nIf we run the query, the output of the explain method tells us how MongoDB used indexes to satisfy it:\n\n{ \"queryPlanner\": { \"plannerVersion\": 1, \"namespace\": \"school.students\", \"indexFilterSet\": false, \"parsedQuery\": { \"$and\": [ { \"class_id\": { \"$eq\": 54 } }, { \"student_id\": { \"$gt\": 500000 } } ] }, \"winningPlan\": { \"stage\": \"FETCH\", \"inputStage\": { \"stage\": \"IXSCAN\", \"keyPattern\": { \"student_id\": 1, \"class_id\": 1 }, \"indexName\": \"student_id_1_class_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"student_id\": [ ], \"class_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false,\n\nIntroduction to Indexes\n\n|\n\n87",
      "page_number": 93
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 103-111)",
      "start_page": 103,
      "end_page": 111,
      "detection_method": "topic_boundary",
      "content": "88\n\n\"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"student_id\": [ \"(500000.0, inf.0]\" ], \"class_id\": [ \"[54.0, 54.0]\" ] } } }, \"rejectedPlans\": [ { \"stage\": \"SORT\", \"sortPattern\": { \"student_id\": 1 }, \"inputStage\": { \"stage\": \"SORT_KEY_GENERATOR\", \"inputStage\": { \"stage\": \"FETCH\", \"filter\": { \"student_id\": { \"$gt\": 500000 } }, \"inputStage\": { \"stage\": \"IXSCAN\", \"keyPattern\": { \"class_id\": 1 }, \"indexName\": \"class_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\" ] } } } } } ]\n\n|\n\nChapter 5: Indexes\n\n}, \"executionStats\": { \"executionSuccess\": true, \"nReturned\": 9903, \"executionTimeMillis\": 4325, \"totalKeysExamined\": 850477, \"totalDocsExamined\": 9903, \"executionStages\": { \"stage\": \"FETCH\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 3485, \"works\": 850478, \"advanced\": 9903, \"needTime\": 840574, \"needYield\": 0, \"saveState\": 6861, \"restoreState\": 6861, \"isEOF\": 1, \"invalidates\": 0, \"docsExamined\": 9903, \"alreadyHasObj\": 0, \"inputStage\": { \"stage\": \"IXSCAN\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 2834, \"works\": 850478, \"advanced\": 9903, \"needTime\": 840574, \"needYield\": 0, \"saveState\": 6861, \"restoreState\": 6861, \"isEOF\": 1, \"invalidates\": 0, \"keyPattern\": { \"student_id\": 1, \"class_id\": 1 }, \"indexName\": \"student_id_1_class_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"student_id\": [ ], \"class_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"student_id\": [ \"(500000.0, inf.0]\" ],\n\nIntroduction to Indexes\n\n|\n\n89\n\n\"class_id\": [ \"[54.0, 54.0]\" ] }, \"keysExamined\": 850477, \"seeks\": 840575, \"dupsTested\": 0, \"dupsDropped\": 0, \"seenInvalidated\": 0 } } }, \"serverInfo\": { \"host\": \"SGB-MBP.local\", \"port\": 27017, \"version\": \"3.4.1\", \"gitVersion\": \"5e103c4f5583e2566a45d740225dc250baacfbd7\" }, \"ok\": 1 }\n\nAs with most data output from MongoDB, the explain output is JSON. Let’s look first at the bottom half of this output, which is almost entirely the execution stats. The \"executionStats\" field contains statistics that describe the completed query execu‐ tion for the winning query plan. We will look at query plans and the query plan out‐ put from explain a little later.\n\nWithin \"executionStats\", first we will look at \"totalKeysExamined\". This is how many keys within the index MongoDB walked through in order to generate the result set. We can compare \"totalKeysExamined\" to \"nReturned\" to get a sense for how much of the index MongoDB had to traverse in order to find just the documents matching the query. In this case, 850,477 index keys were examined in order to locate the 9,903 matching documents.\n\nThis means that the index used in order to satisfy this query was not very selective. This is further emphasized by the fact that this query took more than 4.3 seconds to run, as indicated by the \"executionTimeMillis\" field. Selectivity is one of our key objectives when we are designing an index, so let’s figure out where we went wrong with the existing indexes for this query.\n\nNear the top of the explain output is the winning query plan (see the field \"winning Plan\"). A query plan describes the steps MongoDB used to satisfy a query. This is, in JSON form, the specific outcome of racing a couple of different query plans against one another. In particular, we are interested in what indexes were used and whether MongoDB had to do an in-memory sort. Below the winning plan are the rejected plans. We’ll look at both.\n\n90\n\n|\n\nChapter 5: Indexes\n\nIn this case, the winning plan used a compound index based on \"student_id\" and \"class_id\". This is evident in the following portion of the explain output:\n\n\"winningPlan\": { \"stage\": \"FETCH\", \"inputStage\": { \"stage\": \"IXSCAN\", \"keyPattern\": { \"student_id\": 1, \"class_id\": 1 },\n\nThe explain output presents the query plan as a tree of stages. A stage can have one or more input stages, depending on how many child stages it has. An input stage pro‐ vides the documents or index keys to its parent. In this case, there was one input stage, an index scan, and that scan provided the record IDs for documents matching the query to its parent, the \"FETCH\" stage. The \"FETCH\" stage, then, will retrieve the documents themselves and return them in batches as the client requests them.\n\nThe losing query plan—there is only one—would have used an index based on \"class_id\" but then it would have had to do an in-memory sort. That is what the following portion of this particular query plan means. When you see a \"SORT\" stage in a query plan, it means that MongoDB would have been unable to sort the result set in the database using an index and instead would have had to do an in-memory sort:\n\n\"rejectedPlans\": [ { \"stage\": \"SORT\", \"sortPattern\": { \"student_id\": 1 },\n\nFor this query, the index that won is one that was able to return sorted output. To win it only had to reach a trial number of sorted result documents. For the other plan to win, that query thread would have had to return the entire result set (nearly 10,000 documents) first, since those would then need to be sorted in memory.\n\nThe issue here is one of selectivity. The multivalue query we are running specifies a broad range of \"student_id\" values, because it’s requesting records for which the \"student_id\" is greater than 500,000. That’s about half the records in our collection. Here again, for convenience, is the query we are running:\n\n> db.students.find({student_id:{$gt:500000}, class_id:54}) ... .sort({student_id:1}) ... .explain(\"executionStats\")\n\nNow, I’m sure you can see where we are headed here. This query contains both a mul‐ tivalue portion and an equality portion. The equality portion is that we are asking for all records in which \"class_id\" is equal to 54. There are only about 500 classes in\n\nIntroduction to Indexes\n\n|\n\n91\n\nthis dataset, and while there are a large number of students with grades in those classes, \"class_id\" would serve as a much more selective basis on which to execute this query. It is this value that constrains our result set to just under 10,000 records rather than the approximately 850,000 that were identified by the multivalue portion of this query.\n\nIn other words, it would be better, given the indexes we have, if we were to use the index based on just \"class_id\"—the one in the losing query plan. MongoDB pro‐ vides two ways of forcing the database to use a particular index. However, I cannot stress strongly enough that you should use these ways of overriding what would be the outcome of the query planner with caution. These are not techniques you should use in a production deployment.\n\nThe cursor hint method enables us to specify a particular index to use, either by specifying its shape or its name. An index filter uses a query shape, which is a combi‐ nation of a query, sort, and projection specification. The planCacheSetFilter func‐ tion can be used with an index filter to limit the query optimizer to only considering indexes specified in the index filter. If an index filter exists for a query shape, Mon‐ goDB will ignore hint. Index filters only persist for the duration of the mongod server process; they do not persist after shutdown.\n\nIf we change our query slightly to use hint, as in the following example, the explain output will be quite different:\n\n> db.students.find({student_id:{$gt:500000}, class_id:54}) ... .sort({student_id:1}) ... .hint({class_id:1}) ... .explain(\"executionStats\")\n\nThe resulting output shows that we are now down from having scanned roughly 850,000 index keys to just about 20,000 in order to get to our result set of just under 10,000. In addition, the execution time is only 272 milliseconds rather than the 4.3 seconds we saw with the query plan using the other index:\n\n{ \"queryPlanner\": { \"plannerVersion\": 1, \"namespace\": \"school.students\", \"indexFilterSet\": false, \"parsedQuery\": { \"$and\": [ { \"class_id\": { \"$eq\": 54 } }, { \"student_id\": { \"$gt\": 500000\n\n92\n\n|\n\nChapter 5: Indexes\n\n} } ] }, \"winningPlan\": { \"stage\": \"SORT\", \"sortPattern\": { \"student_id\": 1 }, \"inputStage\": { \"stage\": \"SORT_KEY_GENERATOR\", \"inputStage\": { \"stage\": \"FETCH\", \"filter\": { \"student_id\": { \"$gt\": 500000 } }, \"inputStage\": { \"stage\": \"IXSCAN\", \"keyPattern\": { \"class_id\": 1 }, \"indexName\": \"class_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\" ] } } } } }, \"rejectedPlans\": [ ] }, \"executionStats\": { \"executionSuccess\": true, \"nReturned\": 9903, \"executionTimeMillis\": 272, \"totalKeysExamined\": 20076, \"totalDocsExamined\": 20076, \"executionStages\": { \"stage\": \"SORT\",\n\nIntroduction to Indexes\n\n|\n\n93\n\n94\n\n\"nReturned\": 9903, \"executionTimeMillisEstimate\": 248, \"works\": 29982, \"advanced\": 9903, \"needTime\": 20078, \"needYield\": 0, \"saveState\": 242, \"restoreState\": 242, \"isEOF\": 1, \"invalidates\": 0, \"sortPattern\": { \"student_id\": 1 }, \"memUsage\": 2386623, \"memLimit\": 33554432, \"inputStage\": { \"stage\": \"SORT_KEY_GENERATOR\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 203, \"works\": 20078, \"advanced\": 9903, \"needTime\": 10174, \"needYield\": 0, \"saveState\": 242, \"restoreState\": 242, \"isEOF\": 1, \"invalidates\": 0, \"inputStage\": { \"stage\": \"FETCH\", \"filter\": { \"student_id\": { \"$gt\": 500000 } }, \"nReturned\": 9903, \"executionTimeMillisEstimate\": 192, \"works\": 20077, \"advanced\": 9903, \"needTime\": 10173, \"needYield\": 0, \"saveState\": 242, \"restoreState\": 242, \"isEOF\": 1, \"invalidates\": 0, \"docsExamined\": 20076, \"alreadyHasObj\": 0, \"inputStage\": { \"stage\": \"IXSCAN\", \"nReturned\": 20076, \"executionTimeMillisEstimate\": 45, \"works\": 20077, \"advanced\": 20076,\n\n|\n\nChapter 5: Indexes\n\n\"needTime\": 0, \"needYield\": 0, \"saveState\": 242, \"restoreState\": 242, \"isEOF\": 1, \"invalidates\": 0, \"keyPattern\": { \"class_id\": 1 }, \"indexName\": \"class_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\" ] }, \"keysExamined\": 20076, \"seeks\": 1, \"dupsTested\": 0, \"dupsDropped\": 0, \"seenInvalidated\": 0 } } } } }, \"serverInfo\": { \"host\": \"SGB-MBP.local\", \"port\": 27017, \"version\": \"3.4.1\", \"gitVersion\": \"5e103c4f5583e2566a45d740225dc250baacfbd7\" }, \"ok\": 1 }\n\nHowever, what we really want to see is \"nReturned\" very close to \"totalKeysExa mined\". In addition, we would like avoid having to use hint in order to more effi‐ ciently execute this query. The way to address both of these concerns is to design a better index.\n\nA better index for the query pattern in question is one based on \"class_id\" and \"stu dent_id\", in that order. With \"class_id\" as the prefix, we are using the equality fil‐ ter in our query to restrict the keys considered within the index. This is the most\n\nIntroduction to Indexes\n\n|\n\n95\n\nselective component of our query, and therefore effectively constrains the number of keys MongoDB needs to consider to satisfy this query. We can build this index as follows:\n\n> db.students.createIndex({class_id:1, student_id:1})\n\nWhile not true for absolutely every dataset, in general you should design compound indexes such that fields on which you will be using equality filters come before those on which your application will use multivalue filters.\n\nWith our new index in place, if we rerun our query, this time no hinting is required and we can see from the \"executionStats\" field in the explain output that we have a fast query (37 milliseconds) for which the number of results returned (\"nRe turned\") is equal to the number of keys scanned in the index (\"totalKeysExa mined\"). We can also see that this is due to the fact that the \"executionStages\", which reflect the winning query plan, contain an index scan that makes use of the new index we created:\n\n... \"executionStats\": { \"executionSuccess\": true, \"nReturned\": 9903, \"executionTimeMillis\": 37, \"totalKeysExamined\": 9903, \"totalDocsExamined\": 9903, \"executionStages\": { \"stage\": \"FETCH\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 36, \"works\": 9904, \"advanced\": 9903, \"needTime\": 0, \"needYield\": 0, \"saveState\": 81, \"restoreState\": 81, \"isEOF\": 1, \"invalidates\": 0, \"docsExamined\": 9903, \"alreadyHasObj\": 0, \"inputStage\": { \"stage\": \"IXSCAN\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 0, \"works\": 9904, \"advanced\": 9903, \"needTime\": 0, \"needYield\": 0, \"saveState\": 81, \"restoreState\": 81, \"isEOF\": 1, \"invalidates\": 0,\n\n96\n\n|\n\nChapter 5: Indexes",
      "page_number": 103
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 112-121)",
      "start_page": 112,
      "end_page": 121,
      "detection_method": "topic_boundary",
      "content": "\"keyPattern\": { \"class_id\": 1, \"student_id\": 1 }, \"indexName\": \"class_id_1_student_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ], \"student_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\" ], \"student_id\": [ \"(500000.0, inf.0]\" ] }, \"keysExamined\": 9903, \"seeks\": 1, \"dupsTested\": 0, \"dupsDropped\": 0, \"seenInvalidated\": 0 } } },\n\nConsidering what we know about how indexes are built, you can probably see why this works. The [class_id, student_id] index is composed of key pairs such as the following. Since the student IDs are ordered within these key pairs, in order to satisfy our sort MongoDB simply needs to walk all the key pairs beginning with the first one for class_id 54:\n\n... [53, 999617] [53, 999780] [53, 999916] [54, 500001] [54, 500009] [54, 500048] ...\n\nIn considering the design of a compound index, we need to know how to address equality filters, multivalue filters, and sort components of common query patterns that will make use of the index. It is necessary to consider these three factors for all compound indexes, and if you design your index to balance these concerns correctly,\n\nIntroduction to Indexes\n\n|\n\n97\n\nyou will get the best performance out of MongoDB for your queries. While we’ve addressed all three factors for our example query with the [class_id, student_id] index, the query as written represents a special case of the compound index problem because we’re sorting on one of the fields we are also filtering on.\n\nTo remove the special-case nature of this example, let’s sort on final grade instead, changing our query to the following:\n\n> db.students.find({student_id:{$gt:500000}, class_id:54}) ... .sort({final_grade:1}) ... .explain(\"executionStats\")\n\nIf we run this query and look at the explain output, we see that we’re now doing an in-memory sort. While the query is still fast at only 136 milliseconds, it is an order of magnitude slower than when sorting on \"student_id\", because we are now doing an in-memory sort. We can see that we are doing an in-memory sort because the win‐ ning query plan now contains a \"SORT\" stage:\n\n... \"executionStats\": { \"executionSuccess\": true, \"nReturned\": 9903, \"executionTimeMillis\": 136, \"totalKeysExamined\": 9903, \"totalDocsExamined\": 9903, \"executionStages\": { \"stage\": \"SORT\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 36, \"works\": 19809, \"advanced\": 9903, \"needTime\": 9905, \"needYield\": 0, \"saveState\": 315, \"restoreState\": 315, \"isEOF\": 1, \"invalidates\": 0, \"sortPattern\": { \"final_grade\": 1 }, \"memUsage\": 2386623, \"memLimit\": 33554432, \"inputStage\": { \"stage\": \"SORT_KEY_GENERATOR\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 24, \"works\": 9905, \"advanced\": 9903, \"needTime\": 1, \"needYield\": 0, \"saveState\": 315,\n\n98\n\n|\n\nChapter 5: Indexes\n\n\"restoreState\": 315, \"isEOF\": 1, \"invalidates\": 0, \"inputStage\": { \"stage\": \"FETCH\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 24, \"works\": 9904, \"advanced\": 9903, \"needTime\": 0, \"needYield\": 0, \"saveState\": 315, \"restoreState\": 315, \"isEOF\": 1, \"invalidates\": 0, \"docsExamined\": 9903, \"alreadyHasObj\": 0, \"inputStage\": { \"stage\": \"IXSCAN\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 12, \"works\": 9904, \"advanced\": 9903, \"needTime\": 0, \"needYield\": 0, \"saveState\": 315, \"restoreState\": 315, \"isEOF\": 1, \"invalidates\": 0, \"keyPattern\": { \"class_id\": 1, \"student_id\": 1 }, \"indexName\": \"class_id_1_student_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ], \"student_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\" ], \"student_id\": [ \"(500000.0, inf.0]\" ] },\n\nIntroduction to Indexes\n\n|\n\n99\n\n\"keysExamined\": 9903, \"seeks\": 1, \"dupsTested\": 0, \"dupsDropped\": 0, \"seenInvalidated\": 0 } } } } }, ...\n\nIf we can avoid an in-memory sort with a better index design, we should. This will allow us to scale more easily with respect to dataset size and system load.\n\nBut to do that, we are going to have to make a tradeoff. This is commonly the case when designing compound indexes.\n\nAs is so often necessary for compound indexes, in order to avoid an in-memory sort we need to examine more keys than the number of documents we return. To use the index to sort, MongoDB needs to be able to walk the index keys in order. This means that we need to include the sort field among the compound index keys.\n\nThe keys in our new compound index should be ordered as follows: [class_id, final_grade, student_id]. Note that we include the sort component immediately after the equality filter, but before the multivalue filter. This index will very selectively narrow the set of keys considered for this query. Then, by walking the key triplets matching the equality filter in this index, MongoDB can identify the records that match the multivalue filter and those records will be ordered properly by final grade in ascending order.\n\nThis compound index forces MongoDB to examine keys for more documents than will end up being in our result set. However, by using the index to ensure we have sorted documents, we save execution time. We can construct the new index using the following command:\n\n> db.students.createIndex({class_id:1, final_grade:1, student_id:1})\n\nNow, if we once again issue our query:\n\n> db.students.find({student_id:{$gt:500000}, class_id:54}) ... .sort({final_grade:1}) ... .explain(\"executionStats\")\n\nwe get the following \"executionStats\" in the output from explain. This will vary depending on your hardware and what else is going on in the system, but you can see that the winning plan no longer includes an in-memory sort. It is instead using the index we just created to satisfy the query, including the sort:\n\n100\n\n|\n\nChapter 5: Indexes\n\n\"executionStats\": { \"executionSuccess\": true, \"nReturned\": 9903, \"executionTimeMillis\": 42, \"totalKeysExamined\": 9905, \"totalDocsExamined\": 9903, \"executionStages\": { \"stage\": \"FETCH\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 34, \"works\": 9905, \"advanced\": 9903, \"needTime\": 1, \"needYield\": 0, \"saveState\": 82, \"restoreState\": 82, \"isEOF\": 1, \"invalidates\": 0, \"docsExamined\": 9903, \"alreadyHasObj\": 0, \"inputStage\": { \"stage\": \"IXSCAN\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 24, \"works\": 9905, \"advanced\": 9903, \"needTime\": 1, \"needYield\": 0, \"saveState\": 82, \"restoreState\": 82, \"isEOF\": 1, \"invalidates\": 0, \"keyPattern\": { \"class_id\": 1, \"final_grade\": 1, \"student_id\": 1 }, \"indexName\": \"class_id_1_final_grade_1_student_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ], \"final_grade\": [ ], \"student_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\"\n\nIntroduction to Indexes\n\n|\n\n101\n\n], \"final_grade\": [ \"[MinKey, MaxKey]\" ], \"student_id\": [ \"(500000.0, inf.0]\" ] }, \"keysExamined\": 9905, \"seeks\": 2, \"dupsTested\": 0, \"dupsDropped\": 0, \"seenInvalidated\": 0 } } },\n\nThis section has provided a concrete example of some best practices for designing compound indexes. While these guidelines do not hold for every situation, they do for most and should be the first ideas you consider when constructing a compound index.\n\nTo recap, when designing a compound index:\n\nKeys for equality filters should appear first.\n\nKeys used for sorting should appear before multivalue fields.\n\nKeys for multivalue filters should appear last.\n\nDesign your compound index using these guidelines and then test it under real-world workloads for the range of query patterns your index is designed to support.\n\nChoosing key directions\n\nSo far, all of our index entries have been sorted in ascending, or least-to-greatest, order. However, if you need to sort on two (or more) criteria, you may need to have index keys go in different directions. For example, going back to our earlier example with the users collection, suppose we wanted to sort the collection by age from youngest to oldest and by name from Z−A. Our previous indexes would not be very efficient for this problem: within each age group users were sorted by username in ascending order (A−Z, not Z−A). The compound indexes we’ve been using so far do not hold the values in any useful order for getting \"age\" ascending and \"username\" descending.\n\nTo optimize compound sorts in different directions, we need to use an index with matching directions. In this example, we could use {\"age\" : 1, \"username\" : -1}, which would organize the data as follows:\n\n102\n\n|\n\nChapter 5: Indexes\n\n[21, user999600] -> 8765277104 [21, user999407] -> 8765252400 [21, user999390] -> 8765250224 ... [21, user100270] -> 8623545776 [21, user100266] -> 8623545264 [21, user100154] -> 8623530928 ... [30, user100168] -> 8623532720 [30, user100155] -> 8623531056 [30, user100098] -> 8623523760\n\nThe ages are arranged from youngest to oldest, and within each age, the usernames are sorted from Z to A (or rather 9 to 0, given our usernames).\n\nIf our application also needed to optimize sorting by {\"age\" : 1, \"username\" : 1}, we would have to create a second index with those directions. To figure out which directions to use for an index, simply match the directions your sort is using. Note that inverse indexes (multiplying each direction by −1) are equivalent: {\"age\" : 1, \"username\" : -1} suits the same queries that {\"age\" : -1, \"username\" : 1} does.\n\nIndex direction only really matters when you’re sorting based on multiple criteria. If you’re only sorting by a single key, MongoDB can just as easily read the index in the opposite order. For example, if you had a sort on {\"age\" : -1} and an index on {\"age\" : 1}, MongoDB could optimize it just as well as if you had an index on {\"age\" : -1} (so don’t create both!). The direction only matters for multikey sorts.\n\nUsing covered queries\n\nIn the preceding examples, the index was always used to find the correct document and then follow a pointer back to fetch the actual document. However, if your query is only looking for the fields that are included in the index, it does not need to fetch the document. When an index contains all the values requested by a query, the query is considered to be covered. Whenever practical, use covered queries in preference to going back to documents. You can make your working set much smaller that way.\n\nTo make sure a query can use the index only, you should use projections (which limit the fields returned to only those specified in your query; see “Specifying Which Keys to Return” on page 54) to avoid returning the \"_id\" field (unless it is part of the index). You may also have to index fields that you aren’t querying on, so you should balance your need for faster queries with the overhead this will add on writes.\n\nIf you run explain on a covered query, the result has an \"IXSCAN\" stage that is not a descendant of a \"FETCH\" stage, and in the \"executionStats\", the value of \"totalDoc sExamined\" is 0.\n\nIntroduction to Indexes\n\n|\n\n103\n\nImplicit indexes\n\nCompound indexes can do “double duty” and act like different indexes for different queries. If we have an index on {\"age\" : 1, \"username\" : 1}, the \"age\" field is sorted identically to the way it would be if we had an index on just {\"age\" : 1}. Thus, the compound index can be used the way an index on {\"age\" : 1} by itself would be.\n\nThis can be generalized to as many keys as necessary: if an index has N keys, you get a “free” index on any prefix of those keys. For example, if we have an index that looks like {\"a\": 1, \"b\": 1, \"c\": 1, ..., \"z\": 1}, we effectively have indexes on {\"a\": 1}, {\"a\": 1, \"b\" : 1}, {\"a\": 1, \"b\": 1, \"c\": 1}, and so on.\n\nNote that this doesn’t hold for any subset of keys: queries that would use the index {\"b\": 1} or {\"a\": 1, \"c\": 1} (for example) will not be optimized. Only queries that can use a prefix of the index can take advantage of it.\n\nHow $ Operators Use Indexes Some queries can use indexes more efficiently than others; some queries cannot use indexes at all. This section covers how various query operators are handled by MongoDB.\n\nInefficient operators\n\nIn general, negation is inefficient. \"$ne\" queries can use an index, but not very well. They must look at all the index entries other than the one specified by \"$ne\", so they basically have to scan the entire index. For example, for a collection with an index on the field named \"i\", here are the index ranges traversed for such a query:\n\ndb.example.find({\"i\" : {\"$ne\" : 3}}).explain() { \"queryPlanner\" : { ..., \"parsedQuery\" : { \"i\" : { \"$ne\" : \"3\" } }, \"winningPlan\" : { { ..., \"indexBounds\" : { \"i\" : [ [ { \"$minElement\" : 1 }, 3\n\n104\n\n|\n\nChapter 5: Indexes\n\n], [ 3, { \"$maxElement\" : 1 } ] ] } } }, \"rejectedPlans\" : [ ] }, \"serverInfo\" : { ..., } }\n\nThis query looks at all index entries less than 3 and all index entries greater than 3. This can be efficient if a large swath of your collection is 3, but otherwise it must check almost everything.\n\n\"$not\" can sometimes use an index but often does not know how. It can reverse basic ranges ({\"key\" : {\"$lt\" : 7}} becomes {\"key\" : {\"$gte\" : 7}}) and regular expressions. However, most other queries with \"$not\" will fall back to doing a table scan. \"$nin\" always uses a table scan.\n\nIf you need to perform one of these types of queries quickly, figure out if there’s another clause that you could add to the query that could use an index to filter the result set down to a small number of documents before MongoDB attempts to do nonindexed matching.\n\nRanges\n\nCompound indexes can help MongoDB efficiently execute queries with multiple clau‐ ses. When designing an index with multiple fields, put fields that will be used in exact matches first (e.g., \"x\" : 1) and ranges last (e.g., \"y\": {\"$gt\" : 3, \"$lt\" : 5}). This allows the query to find an exact value for the first index key and then search within that for a second index range. For example, suppose we were querying for a specific age and a range of usernames using an {\"age\" : 1, \"username\" : 1} index. We would get fairly exact index bounds:\n\n> db.users.find({\"age\" : 47, \"username\" : ... {\"$gt\" : \"user5\", \"$lt\" : \"user8\"}}).explain('executionStats') { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false,\n\nIntroduction to Indexes\n\n|\n\n105\n\n106\n\n\"parsedQuery\" : { \"$and\" : [ { \"age\" : { \"$eq\" : 47 } }, { \"username\" : { \"$lt\" : \"user8\" } }, { \"username\" : { \"$gt\" : \"user5\" } } ] }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[47.0, 47.0]\" ], \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ] } } }, \"rejectedPlans\" : [ { \"stage\" : \"FETCH\", \"filter\" : {\n\n|\n\nChapter 5: Indexes",
      "page_number": 112
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 122-133)",
      "start_page": 122,
      "end_page": 133,
      "detection_method": "topic_boundary",
      "content": "\"age\" : { \"$eq\" : 47 } }, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ] } } } ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 2742, \"executionTimeMillis\" : 5, \"totalKeysExamined\" : 2742, \"totalDocsExamined\" : 2742, \"executionStages\" : { \"stage\" : \"FETCH\", \"nReturned\" : 2742, \"executionTimeMillisEstimate\" : 0, \"works\" : 2743, \"advanced\" : 2742, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 23, \"restoreState\" : 23, \"isEOF\" : 1, \"invalidates\" : 0, \"docsExamined\" : 2742, \"alreadyHasObj\" : 0, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"nReturned\" : 2742, \"executionTimeMillisEstimate\" : 0,\n\nIntroduction to Indexes\n\n|\n\n107\n\n\"works\" : 2743, \"advanced\" : 2742, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 23, \"restoreState\" : 23, \"isEOF\" : 1, \"invalidates\" : 0, \"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[47.0, 47.0]\" ], \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ] }, \"keysExamined\" : 2742, \"seeks\" : 1, \"dupsTested\" : 0, \"dupsDropped\" : 0, \"seenInvalidated\" : 0 } } }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nThe query goes directly to \"age\" : 47 and then searches within that for usernames between \"user5\" and \"user8\".\n\n108\n\n|\n\nChapter 5: Indexes\n\nConversely, suppose we use an index on {\"username\" : 1, \"age\" : 1}. This changes the query plan, as the query must look at all users between \"user5\" and \"user8\" and pick out the ones with \"age\" : 47:\n\n> db.users.find({\"age\" : 47, \"username\" : {\"$gt\" : \"user5\", \"$lt\" : \"user8\"}}) .explain('executionStats') { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ { \"age\" : { \"$eq\" : 47 } }, { \"username\" : { \"$lt\" : \"user8\" } }, { \"username\" : { \"$gt\" : \"user5\" } } ] }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"filter\" : { \"age\" : { \"$eq\" : 47 } }, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\",\n\nIntroduction to Indexes\n\n|\n\n109\n\n110\n\n\"indexBounds\" : { \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ] } } }, \"rejectedPlans\" : [ { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1, \"age\" : 1 }, \"indexName\" : \"username_1_age_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ], \"age\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ], \"age\" : [ \"[47.0, 47.0]\" ] } } } ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 2742, \"executionTimeMillis\" : 369, \"totalKeysExamined\" : 333332, \"totalDocsExamined\" : 333332, \"executionStages\" : { \"stage\" : \"FETCH\", \"filter\" : { \"age\" : { \"$eq\" : 47 } },\n\n|\n\nChapter 5: Indexes\n\n\"nReturned\" : 2742, \"executionTimeMillisEstimate\" : 312, \"works\" : 333333, \"advanced\" : 2742, \"needTime\" : 330590, \"needYield\" : 0, \"saveState\" : 2697, \"restoreState\" : 2697, \"isEOF\" : 1, \"invalidates\" : 0, \"docsExamined\" : 333332, \"alreadyHasObj\" : 0, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"nReturned\" : 333332, \"executionTimeMillisEstimate\" : 117, \"works\" : 333333, \"advanced\" : 333332, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 2697, \"restoreState\" : 2697, \"isEOF\" : 1, \"invalidates\" : 0, \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ] }, \"keysExamined\" : 333332, \"seeks\" : 1, \"dupsTested\" : 0, \"dupsDropped\" : 0, \"seenInvalidated\" : 0 } } }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\",\n\nIntroduction to Indexes\n\n|\n\n111\n\n\"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nThis forces MongoDB to scan 100 times the number of index entries as using the pre‐ vious index would. Using two ranges in a query basically always forces this less- efficient query plan.\n\nOR queries\n\nAs of this writing, MongoDB can only use one index per query. That is, if you create one index on {\"x\" : 1} and another index on {\"y\" : 1} and then do a query on {\"x\" : 123, \"y\" : 456}, MongoDB will use one of the indexes you created, not both. The only exception to this rule is \"$or\". \"$or\" can use one index per \"$or\" clause, as \"$or\" performs two queries and then merges the results:\n\ndb.foo.find({\"$or\" : [{\"x\" : 123}, {\"y\" : 456}]}).explain() { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"foo.foo\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$or\" : [ { \"x\" : { \"$eq\" : 123 } }, { \"y\" : { \"$eq\" : 456 } } ] }, \"winningPlan\" : { \"stage\" : \"SUBPLAN\", \"inputStage\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"OR\", \"inputStages\" : [ { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"x\" : 1 },\n\n112\n\n|\n\nChapter 5: Indexes\n\n\"indexName\" : \"x_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"x\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"x\" : [ \"[123.0, 123.0]\" ] } }, { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"y\" : 1 }, \"indexName\" : \"y_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"y\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"y\" : [ \"[456.0, 456.0]\" ] } } ] } } }, \"rejectedPlans\" : [ ] }, \"serverInfo\" : { ..., }, \"ok\" : 1 }\n\nAs you can see, this explain required two separate queries on the two indexes (as indicated by the two \"IXSCAN\" stages). In general, doing two queries and merging the\n\nIntroduction to Indexes\n\n|\n\n113\n\nresults is much less efficient than doing a single query; thus, whenever possible, pre‐ fer \"$in\" to \"$or\".\n\nIf you must use an \"$or\", keep in mind that MongoDB needs to look through the results of both queries and remove any duplicates (documents that matched more than one \"$or\" clause).\n\nWhen running \"$in\" queries there is no way, other than sorting, to control the order of documents returned. For example, {\"x\" : {\"$in\" : [1, 2, 3]}} will return documents in the same order as {\"x\" : {\"$in\" : [3, 2, 1]}}.\n\nIndexing Objects and Arrays MongoDB allows you to reach into your documents and create indexes on nested fields and arrays. Embedded object and array fields can be combined with top-level fields in compound indexes, and although they are special in some ways, they mostly behave the way “normal” index fields behave.\n\nIndexing embedded docs\n\nIndexes can be created on keys in embedded documents in the same way that they are created on normal keys. If we had a collection where each document represented a user, we might have an embedded document that described each user’s location:\n\n{ \"username\" : \"sid\", \"loc\" : { \"ip\" : \"1.2.3.4\", \"city\" : \"Springfield\", \"state\" : \"NY\" } }\n\nWe could put an index on one of the subfields of \"loc\", say \"loc.city\", to speed up queries using that field:\n\n> db.users.createIndex({\"loc.city\" : 1})\n\nYou can go as deep as you’d like with these: you could index \"x.y.z.w.a.b.c\" (and so on) if you wanted.\n\nNote that indexing the embedded document itself (\"loc\") has very different behavior than indexing a field of that embedded document (\"loc.city\"). Indexing the entire subdocument will only help queries that are querying for the entire subdocument. The query optimizer could only use an index on \"loc\" for queries that described the whole subdocument with fields in the correct order (e.g., db.users.find({\"loc\" : {\"ip\" : \"123.456.789.000\", \"city\" : \"Shelbyville\", \"state\" : \"NY\"}}})). It\n\n114\n\n|\n\nChapter 5: Indexes\n\ncould not use the index for queries that looked like db.users.find({\"loc.city\" : \"Shelbyville\"}).\n\nIndexing arrays\n\nYou can also index arrays, which allows you to use the index to search for specific array elements efficiently.\n\nSuppose we have a collection of blog posts where each document is a post. Each post has a \"comments\" field, which is an array of \"comment\" subdocuments. If we wanted to be able to find the most recently commented-on blog posts, we could create an index on the \"date\" key in the array of embedded \"comments\" documents of our blog post collection:\n\n> db.blog.createIndex({\"comments.date\" : 1})\n\nIndexing an array creates an index entry for each element of the array, so if a post had 20 comments, it would have 20 index entries. This makes array indexes more expen‐ sive than single-value ones: for a single insert, update, or remove, every array entry might have to be updated (potentially thousands of index entries).\n\nUnlike the \"loc\" example in the previous section, you cannot index an entire array as a single entity: indexing an array field indexes each element of the array, not the array itself.\n\nIndexes on array elements do not keep any notion of position: you cannot use an index for a query that is looking for a specific array element, such as \"comments.4\".\n\nYou can, incidentally, index a specific array entry, as in:\n\n> db.blog.createIndex({\"comments.10.votes\": 1})\n\nHowever, this index would only be useful for queries for exactly the 11th array ele‐ ment (arrays start at index 0).\n\nOnly one field in an index entry can be from an array. This is to avoid the explosive number of index entries you’d get from multiple multikey indexes: every possible pair of elements would have to be indexed, causing indexes to be n*m entries per docu‐ ment. For example, suppose we had an index on {\"x\" : 1, \"y\" : 1}:\n\n> // x is an array - legal > db.multi.insert({\"x\" : [1, 2, 3], \"y\" : 1}) > > // y is an array - still legal > db.multi.insert({\"x\" : 1, \"y\" : [4, 5, 6]}) > > // x and y are arrays - illegal! > db.multi.insert({\"x\" : [1, 2, 3], \"y\" : [4, 5, 6]}) cannot index parallel arrays [y] [x]\n\nIntroduction to Indexes\n\n|\n\n115\n\nWere MongoDB to index the final example, it would have to create index entries for {\"x\" : 1, \"y\" : 4}, {\"x\" : 1, \"y\" : 5}, {\"x\" : 1, \"y\" : 6}, {\"x\" : 2, \"y\" : 4}, {\"x\" : 2, \"y\" : 5}, {\"x\" : 2, \"y\" : 6}, {\"x\" : 3, \"y\" : 4}, {\"x\" : 3, \"y\" : 5}, and {\"x\" : 3, \"y\" : 6} (and these arrays are only three elements long).\n\nMultikey index implications\n\nIf any document has an array field for the indexed key, the index immediately is flag‐ ged as a multikey index. You can see whether an index is multikey from explain’s output: if a multikey index was used, the \"isMultikey\" field will be true. Once an index has been flagged as multikey, it can never be un-multikeyed, even if all of the documents containing arrays in that field are removed. The only way to un-multikey it is to drop and recreate it.\n\nMultikey indexes may be a bit slower than non-multikey indexes. Many index entries can point at a single document, so MongoDB may need to do some deduplication before returning results.\n\nIndex Cardinality Cardinality refers to how many distinct values there are for a field in a collection. Some fields, such as \"gender\" or \"newsletter opt-out\", might only have two possi‐ ble values, which is considered a very low cardinality. Others, such as \"username\" or \"email\", might have a unique value for every document in the collection, which is high cardinality. Still others fall somewhere in between, such as \"age\" or \"zip code\".\n\nIn general, the greater the cardinality of a field, the more helpful an index on that field can be. This is because the index can quickly narrow the search space to a much smaller result set. For a low-cardinality field, an index generally cannot eliminate as many possible matches.\n\nFor example, suppose we had an index on \"gender\" and were looking for women named Susan. We could only narrow down the result space by approximately 50% before referring to individual documents to look up \"name\". Conversely, if we indexed by \"name\", we could immediately narrow down our result set to the tiny frac‐ tion of users named Susan, and then we could refer to those documents to check the gender.\n\nAs a rule of thumb, try to create indexes on high-cardinality keys or at least put high- cardinality keys first in compound indexes (before low-cardinality keys).\n\nexplain Output As you’ve seen, explain gives you lots of information about your queries. It’s one of the most important diagnostic tools there is for slow queries. You can find out which\n\n116\n\n|\n\nChapter 5: Indexes\n\nindexes are being used and how by looking at a query’s \"explain\" output. For any query, you can add a call to explain at the end (the way you would add a sort or limit, but explain must be the last call).\n\nThere are two types of explain output that you’ll see most commonly: for indexed and nonindexed queries. Special index types may create slightly different query plans, but most fields should be similar. Also, sharding returns a conglomerate of explains (as covered in Chapter 14), as it runs the query on multiple servers.\n\nThe most basic type of explain is on a query that doesn’t use an index. You can tell that a query doesn’t use an index because it uses a \"COLLSCAN\".\n\nThe output of an explain on a query that uses an index varies, but in the simplest case it looks something like this if we add an index on imdb.rating:\n\n> db.users.find({\"age\" : 42}).explain('executionStats') { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"age\" : { \"$eq\" : 42 } }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[42.0, 42.0]\" ], \"username\" : [ \"[MinKey, MaxKey]\"\n\nexplain Output\n\n|\n\n117\n\n118\n\n] } } }, \"rejectedPlans\" : [ ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 8449, \"executionTimeMillis\" : 15, \"totalKeysExamined\" : 8449, \"totalDocsExamined\" : 8449, \"executionStages\" : { \"stage\" : \"FETCH\", \"nReturned\" : 8449, \"executionTimeMillisEstimate\" : 10, \"works\" : 8450, \"advanced\" : 8449, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 66, \"restoreState\" : 66, \"isEOF\" : 1, \"invalidates\" : 0, \"docsExamined\" : 8449, \"alreadyHasObj\" : 0, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"nReturned\" : 8449, \"executionTimeMillisEstimate\" : 0, \"works\" : 8450, \"advanced\" : 8449, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 66, \"restoreState\" : 66, \"isEOF\" : 1, \"invalidates\" : 0, \"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2,\n\n|\n\nChapter 5: Indexes",
      "page_number": 122
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 134-142)",
      "start_page": 134,
      "end_page": 142,
      "detection_method": "topic_boundary",
      "content": "\"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[42.0, 42.0]\" ], \"username\" : [ \"[MinKey, MaxKey]\" ] }, \"keysExamined\" : 8449, \"seeks\" : 1, \"dupsTested\" : 0, \"dupsDropped\" : 0, \"seenInvalidated\" : 0 } } }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nThis output first tells you what index was used: imdb.rating. Next is how many documents were actually returned as a result: \"nReturned\". Note that this doesn’t necessarily reflect how much work MongoDB did to answer the query (i.e., how many indexes and documents it had to search). \"totalKeysExamined\" reports the number of index entries scanned while \"totalDocsExamined\" indicates how many documents were scanned. The number of documents scanned is reflected in \"nscan nedObjects\".\n\nThe output also shows that there were no rejectedPlans and that it used a bounded search on the index within the value 42.0.\n\n\"executionTimeMillis\" reports how fast the query was executed, from the server receiving the request to when it sent a response. However, it may not always be the number you are looking for. If MongoDB tried multiple query plans, \"executionTime Millis\" will reflect how long it took all of them to run, not the one chosen as the best.\n\nNow that you know the basics, here is a breakdown of some of the more important fields in more detail:\n\n\"isMultiKey\" : false\n\nIf this query used a multikey index (see “Indexing Objects and Arrays” on page 114).\n\nexplain Output\n\n|\n\n119\n\n\"nReturned\" : 8449\n\nThe number of documents returned by the query.\n\n\"totalDocsExamined\" : 8449\n\nThe number of times MongoDB had to follow an index pointer to the actual document on disk. If the query contains criteria that are not part of the index or requests fields that aren’t contained in the index, MongoDB must look up the document each index entry points to.\n\n\"totalKeysExamined\" : 8449\n\nThe number of index entries looked at, if an index was used. If this was a table scan, it is the number of documents examined.\n\n\"stage\" : \"IXSCAN\"\n\nIf MongoDB was able to fulfill this query using an index; if not \"COLSCAN\" would indicate it had to perform a collection scan to fulfill the query.\n\nIn this example, MongoDB found all matching documents using the index, which we know because \"totalKeysExamined\" is the same as \"totalDocsExa mined\". However, the query was told to return every field in the matching docu‐ ments and the index only contained the \"age\" and \"username\" fields.\n\n\"needYield\" : 0\n\nThe number of times this query yielded (paused) to allow a write request to pro‐ ceed. If there are writes waiting to go, queries will periodically release their lock and allow them to continue. On this system, there were no writes waiting so the query never yielded.\n\n\"executionTimeMillis\" : 15\n\nThe number of milliseconds it took the database to execute the query. The lower this number is, the better.\n\n\"indexBounds\" : {...}\n\nA description of how the index was used, giving ranges of the index traversed. In this example, as the first clause in the query was an exact match, the index only needed to look at that value: 42. The second index key was a free variable, because the query didn’t specify any restrictions to it. Thus, the database looked for values between negative infinity (\"$minElement\" : 1) and infinity (\"$maxEle ment\" : 1) for usernames within \"age\" : 42.\n\nLet’s take a look at a slightly more complicated example. Suppose you have an index on {\"username\" : 1, \"age\" : 1} and an index on {\"age\" : 1, \"username\" : 1}. What happens if you query for \"username\" and \"age\"? Well, it depends on the query:\n\n> db.users.find({\"age\" : {$gt : 10}, \"username\" : \"user2134\"}).explain() {\n\n120\n\n|\n\nChapter 5: Indexes\n\n\"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ { \"username\" : { \"$eq\" : \"user2134\" } }, { \"age\" : { \"$gt\" : 10 } } ] }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"filter\" : { \"age\" : { \"$gt\" : 10 } }, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"[\\\"user2134\\\", \\\"user2134\\\"]\" ] } } }, \"rejectedPlans\" : [ { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\",\n\nexplain Output\n\n|\n\n121\n\n\"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"(10.0, inf.0]\" ], \"username\" : [ \"[\\\"user2134\\\", \\\"user2134\\\"]\" ] } } } ] }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nWe are querying for an exact match on \"username\" and a range of values for \"age\", so the database chooses to use the {\"username\" : 1, \"age\" : 1} index, reversing the terms of the query. If, on the other hand, we query for an exact age and a range of names, MongoDB will use the other index:\n\n> db.users.find({\"age\" : 14, \"username\" : /.*/}).explain() { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ { \"age\" : { \"$eq\" : 14\n\n122\n\n|\n\nChapter 5: Indexes\n\n} }, { \"username\" : { \"$regex\" : \".*\" } } ] }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"filter\" : { \"username\" : { \"$regex\" : \".*\" } }, \"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[14.0, 14.0]\" ], \"username\" : [ \"[\\\"\\\", {})\", \"[/.*/, /.*/]\" ] } } }, \"rejectedPlans\" : [ { \"stage\" : \"FETCH\", \"filter\" : { \"age\" : { \"$eq\" : 14 } },\n\nexplain Output\n\n|\n\n123\n\n\"inputStage\" : { \"stage\" : \"IXSCAN\", \"filter\" : { \"username\" : { \"$regex\" : \".*\" } }, \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"[\\\"\\\", {})\", \"[/.*/, /.*/]\" ] } } } ] }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nIf you find that Mongo is using different indexes than you want it to for a query, you can force it to use a certain index by using hint. For instance, if you want to make sure MongoDB uses the {\"username\" : 1, \"age\" : 1} index on the previous query, you could say the following:\n\n> db.users.find({\"age\" : 14, \"username\" : /.*/}).hint({\"username\" : 1, \"age\" : 1})\n\nIf a query is not using the index that you want it to and you use a hint to change it, run an explain on the hinted query before deploying. If you force MongoDB to use an index on a query that it does not know how to use an index for, you could end up making the query less efficient than it was without the index.\n\n124\n\n|\n\nChapter 5: Indexes\n\nWhen Not to Index Indexes are most effective at retrieving small subsets of data, and some types of quer‐ ies are faster without indexes. Indexes become less and less efficient as you need to get larger percentages of a collection because using an index requires two lookups: one to look at the index entry and one following the index’s pointer to the document. A collection scan only requires one: looking at the document. In the worst case (returning all of the documents in a collection) using an index would take twice as many lookups and would generally be significantly slower than a collection scan.\n\nUnfortunately, there isn’t a hard-and-fast rule about when an index helps and when it hinders as it really depends on the size of your data, indexes, documents, and average result set (Table 5-1). As a rule of thumb, an index often speeds things up if the query is returning 30% or more of the collection. However, this number can vary from 2% to 60%. Table 5-1 summarizes the conditions in which indexes or collection scans tend to work better.\n\nTable 5-1. Properties that affect the effectiveness of indexes\n\nIndexes often work well for Large collections Large documents Selective queries\n\nCollection scans often work well for Small collections Small documents Nonselective queries\n\nLet’s say we have an analytics system that collects statistics. Our application queries the system for all documents for a given account to generate a nice graph of all data from an hour ago to the beginning of time:\n\n> db.entries.find({\"created_at\" : {\"$lt\" : hourAgo}})\n\nWe index \"created_at\" to speed up this query.\n\nWhen we first launch, the result set is tiny and the query returns instantly. But after a couple of weeks, it starts being a lot of data, and after a month this query is already taking too long to run.\n\nFor most applications, this is probably the “wrong” query: do you really want a query that’s returning most of your dataset? Most applications, particularly those with large datasets, do not. However, there are some legitimate cases where you may want most or all of your data. For example, you might be exporting this data to a reporting sys‐ tem or using it for a batch job. In these cases, you would like to return this large pro‐ portion of the dataset as fast as possible.\n\nWhen Not to Index\n\n|\n\n125\n\nTypes of Indexes There are a few index options you can specify when building an index that change the way the index behaves. The most common variations are described in the following sections, and more advanced or special-case options are described in the next chapter.\n\nUnique Indexes Unique indexes guarantee that each value will appear at most once in the index. For example, if you want to make sure no two documents can have the same value in the \"username\" key, you can create a unique index with a partialFilterExpression for only documents with a firstname field (more on this option later in the chapter):\n\n> db.users.createIndex({\"firstname\" : 1}, ... {\"unique\" : true, \"partialFilterExpression\":{ \"firstname\": {$exists: true } } } ) { \"createdCollectionAutomatically\" : false, \"numIndexesBefore\" : 3, \"numIndexesAfter\" : 4, \"ok\" : 1 }\n\nFor example, suppose you tried to insert the following documents in the users collection:\n\n> db.users.insert({firstname: \"bob\"}) WriteResult({ \"nInserted\" : 1 }) > db.users.insert({firstname: \"bob\"}) WriteResult({ \"nInserted\" : 0, \"writeError\" : { \"code\" : 11000, \"errmsg\" : \"E11000 duplicate key error collection: test.users index: firstname_1 dup key: { : \\\"bob\\\" }\" } })\n\nIf you check the collection, you’ll see that only the first \"bob\" was stored. Throwing duplicate key exceptions is not very efficient, so use the unique constraint for the occasional duplicate, not to filter out zillions of duplicates a second.\n\nA unique index that you are probably already familiar with is the index on \"_id\", which is automatically created whenever you create a collection. This is a normal unique index (aside from the fact that it cannot be dropped, as other unique indexes can be).\n\n126\n\n|\n\nChapter 5: Indexes\n\nIf a key does not exist, the index stores its value as null for that document. This means that if you create a unique index and try to insert more than one document that is missing the indexed field, the inserts will fail because you already have a document with a value of null. See “Partial Indexes” on page 128 for advice on han‐ dling this.\n\nIn some cases a value won’t be indexed. Index buckets are of limited size and if an index entry exceeds it, it just won’t be included in the index. This can cause confusion as it makes a document “invisible” to queries that use the index. Prior to MongoDB 4.2, a field was required to be smaller than 1,024 bytes to be included in an index. In MongoDB 4.2 and later, this constraint was removed. MongoDB does not return any sort of error or warning if a document’s fields cannot be indexed due to size. This means that keys longer than 8 KB will not be subject to the unique index constraints: you can insert identical 8 KB strings, for example.\n\nCompound unique indexes\n\nYou can also create a compound unique index. If you do this, individual keys can have the same values, but the combination of values across all keys in an index entry can appear in the index at most once.\n\nFor example, if we had a unique index on {\"username\" : 1, \"age\" : 1}, the fol‐ lowing inserts would be legal:\n\n> db.users.insert({\"username\" : \"bob\"}) > db.users.insert({\"username\" : \"bob\", \"age\" : 23}) > db.users.insert({\"username\" : \"fred\", \"age\" : 23})\n\nHowever, attempting to insert a second copy of any of these documents would cause a duplicate key exception.\n\nGridFS, the standard method for storing large files in MongoDB (see “Storing Files with GridFS” on page 156), uses a compound unique index. The collection that holds the file content has a unique index on {\"files_id\" : 1, \"n\" : 1}, which allows documents that look like (in part) the following:\n\n{\"files_id\" : ObjectId(\"4b23c3ca7525f35f94b60a2d\"), \"n\" : 1} {\"files_id\" : ObjectId(\"4b23c3ca7525f35f94b60a2d\"), \"n\" : 2} {\"files_id\" : ObjectId(\"4b23c3ca7525f35f94b60a2d\"), \"n\" : 3} {\"files_id\" : ObjectId(\"4b23c3ca7525f35f94b60a2d\"), \"n\" : 4}\n\nNote that all of the values for \"files_id\" are the same, but \"n\" is different.\n\nDropping duplicates\n\nIf you attempt to build a unique index on an existing collection, it will fail to build if there are any duplicate values:\n\nTypes of Indexes\n\n|\n\n127",
      "page_number": 134
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 143-154)",
      "start_page": 143,
      "end_page": 154,
      "detection_method": "topic_boundary",
      "content": "> db.users.createIndex({\"age\" : 1}, {\"unique\" : true}) WriteResult({ \"nInserted\" : 0, \"writeError\" : { \"code\" : 11000, \"errmsg\" : \"E11000 duplicate key error collection: test.users index: age_1 dup key: { : 12 }\" } })\n\nGenerally, you’ll need to process your data (the aggregation framework can help) and figure out where the duplicates are and what to do with them.\n\nPartial Indexes As mentioned in the previous section, unique indexes count null as a value, so you cannot have a unique index with more than one document missing the key. However, there are lots of cases where you may want the unique index to be enforced only if the key exists. If you have a field that may or may not exist but must be unique when it does, you can combine the \"unique\" option with the \"partial\" option.\n\nPartial indexes in MongoDB are only created on a subset of the data. This is unlike sparse indexes on relational databases, which create fewer index entries pointing to a block of data—however, all blocks of data will have an associated sparse index entry in RDBMS.\n\nTo create a partial index, include the \"partialFilterExpression\" option. Partial indexes represent a superset of the functionality offered by sparse indexes, with a document representing the filter expression you wish to create it on. For example, if providing an email address was optional but, if provided, should be unique, we could do:\n\n> db.users.ensureIndex({\"email\" : 1}, {\"unique\" : true, \"partialFilterExpression\" : ... { email: { $exists: true } }})\n\nPartial indexes do not necessarily have to be unique. To make a nonunique partial index, simply do not include the \"unique\" option.\n\nOne thing to be aware of is that the same query can return different results depending on whether or not it uses the partial index. For example, suppose we have a collection where most of the documents have \"x\" fields, but one does not:\n\n> db.foo.find() { \"_id\" : 0 } { \"_id\" : 1, \"x\" : 1 } { \"_id\" : 2, \"x\" : 2 } { \"_id\" : 3, \"x\" : 3 }\n\n128\n\n|\n\nChapter 5: Indexes\n\nWhen we do a query on \"x\", it will return all matching documents:\n\n> db.foo.find({\"x\" : {\"$ne\" : 2}}) { \"_id\" : 0 } { \"_id\" : 1, \"x\" : 1 } { \"_id\" : 3, \"x\" : 3 }\n\nIf we create a partial index on \"x\", the \"_id\" : 0 document won’t be included in the index. So now if we query on \"x\", MongoDB will use the index and not return the {\"_id\" : 0} document:\n\n> db.foo.find({\"x\" : {\"$ne\" : 2}}) { \"_id\" : 1, \"x\" : 1 } { \"_id\" : 3, \"x\" : 3 }\n\nYou can use hint to force it to do a table scan if you need documents with missing fields.\n\nIndex Administration As shown in the previous section, you can create new indexes using the createIndex function. An index only needs to be created once per collection. If you try to create the same index again, nothing will happen.\n\nAll of the information about a database’s indexes is stored in the system.indexes col‐ lection. This is a reserved collection, so you cannot modify its documents or remove documents from it. You can manipulate it only through the createIndex, createIn dexes, and dropIndexes database commands.\n\nWhen you create an index, you can see its metainformation in system.indexes. You can also run db.collectionName.getIndexes() to see information about all the indexes on a given collection:\n\n> db.students.getIndexes() [ { \"v\" : 2, \"key\" : { \"_id\" : 1 }, \"name\" : \"_id_\", \"ns\" : \"school.students\" }, { \"v\" : 2, \"key\" : { \"class_id\" : 1 }, \"name\" : \"class_id_1\", \"ns\" : \"school.students\"\n\nIndex Administration\n\n|\n\n129\n\n}, { \"v\" : 2, \"key\" : { \"student_id\" : 1, \"class_id\" : 1 }, \"name\" : \"student_id_1_class_id_1\", \"ns\" : \"school.students\" } ]\n\nThe important fields are \"key\" and \"name\". The key can be used for hinting and other places where an index must be specified. This is a place where field order matters: an index on {\"class_id\" : 1, \"student_id\" : 1} is not the same as an index on {\"student_id\" : 1, \"class_id\" : 1}. The index name is used as an identifier for a lot of administrative index operations, such as dropIndexes. Whether or not the index is multikey is not specified in its spec.\n\nThe \"v\" field is used internally for index versioning. If you have any indexes that do not have at least a \"v\" : 1 field, they are being stored in an older, less efficient for‐ mat. You can upgrade them by ensuring that you’re running at least MongoDB ver‐ sion 2.0 and dropping and rebuilding the indexes.\n\nIdentifying Indexes Each index in a collection has a name that uniquely identifies that index and is used by the server to delete or manipulate it. Index names are, by default, key name1_dir1_keyname2_dir2_..._keynameN_dirN, where keynameX is the index’s key and dirX is the index’s direction (1 or -1). This can get unwieldy if indexes contain more than a couple of keys, so you can specify your own name as one of the options to createIndex:\n\n> db.soup.createIndex({\"a\" : 1, \"b\" : 1, \"c\" : 1, ..., \"z\" : 1}, ... {\"name\" : \"alphabet\"})\n\nThere is a limit to the number of characters in an index name, so complex indexes may need custom names to be created. A call to getLastError will show if the index creation succeeded or why it didn’t.\n\nChanging Indexes As your application grows and changes, you may find that your data or queries have changed and that indexes that used to work well no longer do. You can remove unneeded indexes using the dropIndex command:\n\n> db.people.dropIndex(\"x_1_y_1\") { \"nIndexesWas\" : 3, \"ok\" : 1 }\n\n130\n\n|\n\nChapter 5: Indexes\n\nUse the \"name\" field from the index description to specify which index to drop.\n\nBuilding new indexes is time-consuming and resource-intensive. Prior to version 4.2, MongoDB will build an index as fast as possible, blocking all reads and writes on a database until the index build has finished. If you would like your database to remain somewhat responsive to reads and writes, use the \"background\" option when build‐ ing an index. This forces the index build to occasionally yield to other operations, but may still have a severe impact on your application (see “Building Indexes” on page 283 for more information). Background indexing is also much slower than fore‐ ground indexing. MongoDB version 4.2 introduced a new approach, the hybrid index build. It only holds the exclusive lock at the beginning and end of the index build. The rest of the build process yields to interleaving read and write operations. This replaces both the foreground and the background index build type in MongoDB 4.2.\n\nIf you have the choice, creating indexes on existing documents is slightly faster than creating the index first and then inserting all documents.\n\nThere is more on the operational aspects of building indexes in Chapter 19.\n\nIndex Administration\n\n|\n\n131\n\nCHAPTER 6 Special Index and Collection Types\n\nThis chapter covers the special collections and index types MongoDB has available, including:\n\nCapped collections for queue-like data\n\nTTL indexes for caches\n\nFull-text indexes for simple string searching\n\nGeospatial indexes for 2D and spherical geometries\n\nGridFS for storing large files\n\nGeospatial Indexes MongoDB has two types of geospatial indexes: 2dsphere and 2d. 2dsphere indexes work with spherical geometries that model the surface of the earth based on the WGS84 datum. This datum models the surface of the earth as an oblate spheroid, meaning that there is some flattening at the poles. Distance calculations using 2sphere indexes, therefore, take the shape of the earth into account and provide a more accurate treatment of distance between, for example, two cities, than do 2d indexes. Use 2d indexes for points stored on a two-dimensional plane.\n\n2dsphere allows you to specify geometries for points, lines, and polygons in the Geo‐ JSON format. A point is given by a two-element array, representing [longitude, lati tude]:\n\n133\n\n{ \"name\" : \"New York City\", \"loc\" : { \"type\" : \"Point\", \"coordinates\" : [50, 2] } }\n\nA line is given by an array of points:\n\n{ \"name\" : \"Hudson River\", \"loc\" : { \"type\" : \"LineString\", \"coordinates\" : [[0,1], [0,2], [1,2]] } }\n\nA polygon is specified the same way a line is (an array of points), but with a different \"type\":\n\n{ \"name\" : \"New England\", \"loc\" : { \"type\" : \"Polygon\", \"coordinates\" : [[0,1], [0,2], [1,2]] } }\n\nThe field that we are naming, \"loc\" in this example, can be called anything, but the field names in the embedded object are specified by GeoJSON and cannot be changed.\n\nYou can create a geospatial index using the \"2dsphere\" type with createIndex:\n\n> db.openStreetMap.createIndex({\"loc\" : \"2dsphere\"})\n\nTo create a 2dsphere index, pass a document to createIndex that specifies the field containing geometries you want to index for the collection in question and specify \"2dsphere\" as the value.\n\nTypes of Geospatial Queries There are three types of geospatial queries that you can perform: intersection, within, and nearness. You specify what you’re looking for as a GeoJSON object that looks like {\"$geometry\" : geoJsonDesc}.\n\n134\n\n|\n\nChapter 6: Special Index and Collection Types\n\nFor example, you can find documents that intersect the query’s location using the \"$geoIntersects\" operator:\n\n> var eastVillage = { ... \"type\" : \"Polygon\", ... \"coordinates\" : [ ... [ ... [ -73.9732566, 40.7187272 ], ... [ -73.9724573, 40.7217745 ], ... [ -73.9717144, 40.7250025 ], ... [ -73.9714435, 40.7266002 ], ... [ -73.975735, 40.7284702 ], ... [ -73.9803565, 40.7304255 ], ... [ -73.9825505, 40.7313605 ], ... [ -73.9887732, 40.7339641 ], ... [ -73.9907554, 40.7348137 ], ... [ -73.9914581, 40.7317345 ], ... [ -73.9919248, 40.7311674 ], ... [ -73.9904979, 40.7305556 ], ... [ -73.9907017, 40.7298849 ], ... [ -73.9908171, 40.7297751 ], ... [ -73.9911416, 40.7286592 ], ... [ -73.9911943, 40.728492 ], ... [ -73.9914313, 40.7277405 ], ... [ -73.9914635, 40.7275759 ], ... [ -73.9916003, 40.7271124 ], ... [ -73.9915386, 40.727088 ], ... [ -73.991788, 40.7263908 ], ... [ -73.9920616, 40.7256489 ], ... [ -73.9923298, 40.7248907 ], ... [ -73.9925954, 40.7241427 ], ... [ -73.9863029, 40.7222237 ], ... [ -73.9787659, 40.719947 ], ... [ -73.9772317, 40.7193229 ], ... [ -73.9750886, 40.7188838 ], ... [ -73.9732566, 40.7187272 ] ... ] ... ]} > db.openStreetMap.find( ... {\"loc\" : {\"$geoIntersects\" : {\"$geometry\" : eastVillage}}})\n\nThis would find all point-, line-, and polygon-containing documents that had a point in the East Village in New York City.\n\nYou can use \"$geoWithin\" to query for things that are completely contained in an area (for instance, “What restaurants are in the East Village?”):\n\n> db.openStreetMap.find({\"loc\" : {\"$geoWithin\" : {\"$geometry\" : eastVillage}}})\n\nGeospatial Indexes\n\n|\n\n135\n\nUnlike our first query, this will not return things that merely pass through the East Village (such as streets) or partially overlap it (such as a polygon describing Manhattan).\n\nFinally, you can query for nearby locations with \"$near\":\n\n> db.openStreetMap.find({\"loc\" : {\"$near\" : {\"$geometry\" : eastVillage}}})\n\nNote that \"$near\" is the only geospatial operator that implies a sort: results from \"$near\" are always returned in order of distance, from closest to farthest.\n\nUsing Geospatial Indexes MongoDB’s geospatial indexing allows you to efficiently execute spatial queries on a collection that contains geospatial shapes and points. To showcase the capabilities of geospatial features and compare different approaches, we will go through the process of writing queries for a simple geospatial application. We’ll go a little deeper into a few concepts central to geospatial indexes and then demonstrate their use with \"$geo Within\", \"$geoIntersects\", and \"$geoNear\".\n\nSuppose we are designing a mobile application to help users find restaurants in New York City. The application must:\n\nDetermine the neighborhood the user is currently in.\n\nShow the number of restaurants in that neighborhood.\n\nFind restaurants within a specified distance.\n\nWe will use a 2dsphere index to query on this spherical geometry data.\n\n2D versus spherical geometry in queries\n\nGeospatial queries can use either spherical or 2D (flat) geometries, depending on both the query and the type of index in use. Table 6-1 shows what kind of geometry each geospatial operator uses.\n\nTable 6-1. Query types and geometries in MongoDB\n\nQuery type $near (GeoJSON point, 2dsphere index) $near (legacy coordinates, 2d index) $geoNear (GeoJSON point, 2dsphere index) $geoNear (legacy coordinates, 2d index) $nearSphere (GeoJSON point, 2dsphere index) $nearSphere (legacy coordinates, 2d index)a $geoWithin : { $geometry: ... }\n\nGeometry type Spherical Flat Spherical Flat Spherical Spherical Spherical\n\n136\n\n|\n\nChapter 6: Special Index and Collection Types\n\nQuery type $geoWithin: { $box: ... }\n\n$geoWithin: { $polygon: ... }\n\n$geoWithin : { $center: ... }\n\nGeometry type Flat Flat Flat\n\n$geoWithin : { $centerSphere: ... } Spherical $geoIntersects Spherical a Use GeoJSON points instead.\n\nNote also that 2d indexes support both flat geometries and distance-only calculations on spheres (i.e., using $nearSphere). However, queries using spherical geometries will be more performant and accurate with a 2dsphere index.\n\nNote also that the $geoNear operator is an aggregation operator. The aggregation framework is discussed in Chapter 7. In addition to the $near query operation, the $geoNear aggregation operator and the special command geoNear enable us to query for nearby locations. Keep in mind that the $near query operator will not work on collections that are distributed using sharding, MongoDB’s scaling solution (see Chapter 15).\n\nThe geoNear command and the $geoNear aggregation operator require that a collec‐ tion have at most one 2dsphere index and at most one 2d index, whereas geospatial query operators (e.g., $near and $geoWithin) permit collections to have multiple geospatial indexes.\n\nThe geospatial index restriction for the geoNear command and the $geoNear aggre‐ gation operator exists because neither the geoNear command nor the $geoNear syn‐ tax includes the location field. As such, index selection among multiple 2d indexes or 2dsphere indexes is ambiguous.\n\nNo such restriction applies for geospatial query operators; these operators take a loca‐ tion field, eliminating the ambiguity.\n\nDistortion\n\nSpherical geometry will appear distorted when visualized on a map due to the nature of projecting a three-dimensional sphere, such as the earth, onto a flat plane.\n\nFor example, take the specification of the spherical square defined by the longitude, latitude points (0,0), (80,0), (80,80), and (0,80). Figure 6-1 depicts the area covered by this region.\n\nGeospatial Indexes\n\n|\n\n137\n\nFigure 6-1. The spherical square defined by the points (0,0), (80,0), (80, 80), and (0,80)\n\nSearching for restaurants\n\nIn this example, we will work with neighborhood and restaurant datasets based in New York City. You can download the example datasets from GitHub.\n\nWe can import the datasets into our database using the mongoimport tool as follows:\n\n$ mongoimport <path to neighborhoods.json> -c neighborhoods $ mongoimport <path to restaurants.json> -c restaurants\n\n138\n\n|\n\nChapter 6: Special Index and Collection Types\n\nWe can create a 2dsphere index on each collection using the createIndex command in the mongo shell:\n\n> db.neighborhoods.createIndex({location:\"2dsphere\"}) > db.restaurants.createIndex({location:\"2dsphere\"})\n\nExploring the data\n\nWe can get a sense for the schema used for documents in these collections with a cou‐ ple of quick queries in the mongo shell:\n\n> db.neighborhoods.find({name: \"Clinton\"}) { \"_id\": ObjectId(\"55cb9c666c522cafdb053a4b\"), \"geometry\": { \"coordinates\": [ [ [-73.99,40.77], . . . [-73.99,40.77], [-73.99,40.77]] ] ], \"type\": \"Polygon\" }, \"name\": \"Clinton\" }\n\n> db.restaurants.find({name: \"Little Pie Company\"}) { \"_id\": ObjectId(\"55cba2476c522cafdb053dea\"), \"location\": { \"coordinates\": [ -73.99331699999999, 40.7594404 ], \"type\": \"Point\" }, \"name\": \"Little Pie Company\" }\n\nThe neighborhood document in the previous code corresponds to the area of New York City shown in Figure 6-2.\n\nGeospatial Indexes\n\n|\n\n139\n\nFigure 6-2. The Hell’s Kitchen (Clinton) neighborhood of New York City\n\nThe bakery corresponds to the location shown in Figure 6-3.\n\n140\n\n|\n\nChapter 6: Special Index and Collection Types",
      "page_number": 143
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 155-162)",
      "start_page": 155,
      "end_page": 162,
      "detection_method": "topic_boundary",
      "content": "Figure 6-3. The Little Pie Company at 424 West 43rd Street\n\nFinding the current neighborhood\n\nAssuming the user’s mobile device can give a reasonably accurate location user, it is simple to find the user’s current neighborhood with $geoIntersects.\n\nSuppose the user is located at −73.93414657 longitude and 40.82302903 latitude. To find the current neighborhood (Hell’s Kitchen), we can specify a point using the spe‐ cial $geometry field in GeoJSON format:\n\n> db.neighborhoods.findOne({geometry:{$geoIntersects:{$geometry:{type:\"Point\", ... coordinates:[-73.93414657,40.82302903]}}}})\n\nThis query will return the following result:\n\n{ \"_id\":ObjectId(\"55cb9c666c522cafdb053a68\"),\n\nGeospatial Indexes\n\n|\n\n141\n\n\"geometry\":{ \"type\":\"Polygon\", \"coordinates\":[[[-73.93383000695911,40.81949109558767],...]]}, \"name\":\"Central Harlem North-Polo Grounds\" }\n\nFinding all restaurants in the neighborhood\n\nWe can also query to find all restaurants contained in a given neighborhood. To do so, we can execute the following in the mongo shell to find the neighborhood contain‐ ing the user, and then count the restaurants within that neighborhood. For example, to find all the restaurants in the Hell’s Kitchen neighborhood:\n\n> var neighborhood = db.neighborhoods.findOne({ geometry: { $geoIntersects: { $geometry: { type: \"Point\", coordinates: [-73.93414657,40.82302903] } } } });\n\n> db.restaurants.find({ location: { $geoWithin: { // Use the geometry from the neighborhood object we retrieved above $geometry: neighborhood.geometry } } }, // Project just the name of each matching restaurant {name: 1, _id: 0});\n\nThis query will tell you that there are 127 restaurants in the requested neighborhood that have the following names:\n\n{ \"name\": \"White Castle\" } { \"name\": \"Touch Of Dee'S\" } { \"name\": \"Mcdonald'S\" } { \"name\": \"Popeyes Chicken & Biscuits\" } { \"name\": \"Make My Cake\"\n\n142\n\n|\n\nChapter 6: Special Index and Collection Types\n\n} { \"name\": \"Manna Restaurant Ii\" } ... { \"name\": \"Harlem Coral Llc\" }\n\nFinding restaurants within a distance\n\nTo find restaurants within a specified distance of a point, you can use either \"$geoWi thin\" with \"$centerSphere\" to return results in unsorted order, or \"$nearSphere\" with \"$maxDistance\" if you need results sorted by distance.\n\nTo find restaurants within a circular region, use \"$geoWithin\" with \"$center Sphere\". \"$centerSphere\" is a MongoDB-specific syntax to denote a circular region by specifying the center and the radius in radians. \"$geoWithin\" does not return the documents in any specific order, so it might return the furthest documents first.\n\nThe following will find all restaurants within five miles of the user:\n\n> db.restaurants.find({ location: { $geoWithin: { $centerSphere: [ [-73.93414657,40.82302903], 5/3963.2 ] } } })\n\n\"$centerSphere\"’s second argument accepts the radius in radians. The query con‐ verts the distance to radians by dividing by the approximate equatorial radius of the earth, 3963.2 miles.\n\nApplications can use \"$centerSphere\" without having a geospatial index. However, geospatial indexes support much faster queries than the unindexed equivalents. Both 2dsphere and 2d geospatial indexes support \"$centerSphere\".\n\nYou may also use \"$nearSphere\" and specify a \"$maxDistance\" term in meters. This will return all restaurants within five miles of the user in sorted order from nearest to farthest:\n\n> var METERS_PER_MILE = 1609.34; db.restaurants.find({ location: { $nearSphere: { $geometry: { type: \"Point\",\n\nGeospatial Indexes\n\n|\n\n143\n\ncoordinates: [-73.93414657,40.82302903] }, $maxDistance: 5*METERS_PER_MILE } } });\n\nCompound Geospatial Indexes As with other types of indexes, you can combine geospatial indexes with other fields to optimize more complex queries. A possible query mentioned earlier was: “What restaurants are in Hell’s Kitchen?” Using only a geospatial index, we could narrow the field to everything in Hell’s Kitchen, but narrowing it down to only “restaurants” or “pizza” would require another field in the index:\n\n> db.openStreetMap.createIndex({\"tags\" : 1, \"location\" : \"2dsphere\"})\n\nThen we can quickly find a pizza place in Hell’s Kitchen:\n\n> db.openStreetMap.find({\"loc\" : {\"$geoWithin\" : ... {\"$geometry\" : hellsKitchen.geometry}}, ... \"tags\" : \"pizza\"})\n\nWe can have the “vanilla” index field either before or after the \"2dsphere\" field, depending on whether we’d like to filter by the vanilla field or the location first. Choose whichever is more selective (i.e., will filter out more results as the first index term).\n\n2d Indexes For nonspherical maps (videogame maps, time series data, etc.) you can use a \"2d\" index instead of \"2dsphere\":\n\n> db.hyrule.createIndex({\"tile\" : \"2d\"})\n\n2d indexes assume a perfectly flat surface, instead of a sphere. Thus, 2d indexes should not be used with spheres unless you don’t mind massive distortion around the poles.\n\nDocuments should use a two-element array for their \"2d\" indexed field. The elements in this array should reflect the longitude and lattitude coordinates, respectively. A sample document might look like this:\n\n{ \"name\" : \"Water Temple\", \"tile\" : [ 32, 22 ] }\n\nDo not use a 2d index if you plan to store GeoJSON data—they can only index points. You can store an array of points, but it will be stored as exactly that: an array of\n\n144\n\n|\n\nChapter 6: Special Index and Collection Types\n\npoints, not a line. This is an important distinction for \"$geoWithin\" queries, in par‐ ticular. If you store a street as an array of points, the document will match \"$geoWithin\" if one of those points is within the given shape. However, the line cre‐ ated by those points might not be wholly contained in the shape.\n\nBy default, 2d indexes assume that your values are going to range from −180 to 180. If you are expecting larger or smaller bounds, you can specify what the minimum and maximum values will be as options to createIndex:\n\n> db.hyrule.createIndex({\"light-years\" : \"2d\"}, {\"min\" : -1000, \"max\" : 1000})\n\nThis will create a spatial index calibrated for a 2,000 × 2,000 square.\n\n2d indexes support the \"$geoWithin\", \"$nearSphere\", and \"$near\" query selectors. Use \"$geoWithin\" to query for points within a shape defined on a flat surface. \"$geo Within\" can query for all points within a rectangle, polygon, circle, or sphere; it uses the \"$geometry\" operator to specify the GeoJSON object. Returning to our grid indexed as follows:\n\n> db.hyrule.createIndex({\"tile\" : \"2d\"})\n\nthe following queries for documents within a rectangle defined by [10, 10] at the bottom-left corner and by [100, 100] at the top-right corner:\n\n> db.hyrule.find({ tile: { $geoWithin: { $box: [[10, 10], [100, 100]] } } })\n\n$box takes a two-element array: the first element specifies the coordinates of the lower-left corner and the second element the upper right.\n\nTo query for documents that are within the circle centered on [−17 , 20.5] and with a radius of 25 we can issue the following command:\n\n> db.hyrule.find({ tile: { $geoWithin: { $center: [[-17, 20.5] , 25] } } })\n\nThe following query returns all documents with coordinates that exist within the pol‐ ygon defined by [0, 0], [3, 6], and [6 , 0]:\n\n> db.hyrule.find({ tile: { $geoWithin: {\n\nGeospatial Indexes\n\n|\n\n145\n\n$polygon: [[0, 0], [3, 6], [6, 0]] } } })\n\nYou specify a polygon as an array of points. The final point in the list will be “connec‐ ted to” the first point to form the polygon. This example would locate all documents containing points within the given triangle.\n\nMongoDB also supports rudimentary spherical queries on flat 2d indexes for legacy reasons. In general, spherical calculations should use a 2dsphere index, as described in “2D versus spherical geometry in queries” on page 136. However, to query for leg‐ acy coordinate pairs within a sphere, use \"$geoWithin\" with the “$centerSphere” operator. Specify an array that contains:\n\nThe grid coordinates of the circle’s center point\n\nThe circle’s radius measured in radians\n\nFor example:\n\n> db.hyrule.find({ loc: { $geoWithin: { $centerSphere: [[88, 30], 10/3963.2] } } })\n\nTo query for nearby points, use \"$near\". Proximity queries return the documents with coordinate pairs closest to the defined point and sort the results by distance. This finds all of the documents in the hyrule collection in order by distance from the point (20, 21):\n\n> db.hyrule.find({\"tile\" : {\"$near\" : [20, 21]}})\n\nA default limit of 100 documents is applied if no limit is specified. If you don’t need that many results, you should set a limit to conserve server resources. For example, the following code returns the 10 documents nearest to (20, 21):\n\n> db.hyrule.find({\"tile\" : {\"$near\" : [20, 21]}}).limit(10)\n\nIndexes for Full Text Search text indexes in MongoDB support full-text search requirements. This type of text index should not be confused with the MongoDB Atlas Full-Text Search Indexes, which utilize Apache Lucene for additional text search capabilities when compared to MongoDB text indexes. Use a text index if your application needs to enable users to\n\n146\n\n|\n\nChapter 6: Special Index and Collection Types\n\nsubmit keyword queries that should match titles, descriptions, and text in other fields within a collection.\n\nIn previous chapters, we’ve queried for strings using exact matches and regular expressions, but these techniques have some limitations. Searching a large block of text for a regular expression is slow, and it’s tough to take morphology (e.g., that “entry” should match “entries”) and other challenges presented by human language into account. text indexes give you the ability to search text quickly and provide sup‐ port for common search engine requirements such as language-appropriate tokeniza‐ tion, stop words, and stemming.\n\ntext indexes require a number of keys proportional to the words in the fields being indexed. As a consequence, creating a text index can consume a large amount of sys‐ tem resources. You should create such an index at a time when it will not negatively impact the performance of your application for users or build the index in the back‐ ground, if possible. To ensure good performance, as with all indexes, you should also take care that any text index you create fits in RAM. See Chapter 19 for more infor‐ mation on creating indexes with minimal impact on your application.\n\nWrites to a collection require that all indexes are updated. If you are using text search, strings will be tokenized and stemmed and the index updated in, potentially, many places. For this reason, writes involving text indexes are usually more expensive than writes to single-field, compound, or even multikey indexes. Thus, you will tend to see poorer write performance on text-indexed collections than on others. They will also slow down data movement if you are sharding: all text must be reindexed when it is migrated to a new shard.\n\nCreating a Text Index Suppose we have a collection of Wikipedia articles that we want to index. To run a search over the text, we first need to create a text index. The following call to crea teIndex will create the index based on the terms in both the \"title\" and \"body\" fields:\n\n> db.articles.createIndex({\"title\": \"text\", \"body\" : \"text\"})\n\nThis is not like a “normal” compound index where there is an ordering on the keys. By default, each field is given equal consideration in a text index. You can control the relative importance MongoDB attaches to each field by specifying weights:\n\n> db.articles.createIndex({\"title\": \"text\", \"body\": \"text\"}, {\"weights\" : { \"title\" : 3, \"body\" : 2}})\n\nIndexes for Full Text Search\n\n|\n\n147\n\nThis would weight the \"title\" field at a ratio of 3:2 in comparison to the \"body\" field.\n\nYou cannot change field weights after index creation (without dropping the index and recreating it), so you may want to play with weights on a sample dataset before creat‐ ing the index on your production data.\n\nFor some collections, you may not know which fields a document will contain. You can create a full-text index on all string fields in a document by creating an index on \"$**\"—this not only indexes all top-level string fields, but also searches embedded documents and arrays for string fields:\n\n> db.articles.createIndex({\"$**\" : \"text\"})\n\nText Search Use the \"$text\" query operator to perform text searches on a collection with a text index. \"$text\" will tokenize the search string using whitespace and most punctua‐ tion as delimiters, and perform a logical OR of all such tokens in the search string. For example, you could use the following query to find all articles containing any of the terms “impact,” “crater,” or “lunar.” Note that because our index is based on terms in both the title and body of an article, this query will match documents in which those terms are found in either field. For the purposes of this example, we will project the title so that we can fit more results on the page:\n\n> db.articles.find({\"$text\": {\"$search\": \"impact crater lunar\"}}, {title: 1} ).limit(10) { \"_id\" : \"170375\", \"title\" : \"Chengdu\" } { \"_id\" : \"34331213\", \"title\" : \"Avengers vs. X-Men\" } { \"_id\" : \"498834\", \"title\" : \"Culture of Tunisia\" } { \"_id\" : \"602564\", \"title\" : \"ABC Warriors\" } { \"_id\" : \"40255\", \"title\" : \"Jupiter (mythology)\" } { \"_id\" : \"80356\", \"title\" : \"History of Vietnam\" } { \"_id\" : \"22483\", \"title\" : \"Optics\" } { \"_id\" : \"8919057\", \"title\" : \"Characters in The Legend of Zelda series\" } { \"_id\" : \"20767983\", \"title\" : \"First inauguration of Barack Obama\" } { \"_id\" : \"17845285\", \"title\" : \"Kushiel's Mercy\" }\n\nYou can see that the results with our initial query are not terribly relevant. As with all technologies, it’s important to have a good grasp of how text indexes work in Mon‐ goDB in order to use them effectively. In this case, there are two problems with the way we’ve issued the query. The first is that our query is pretty broad, given that MongoDB issues the query using a logical OR of “impact,” “crater,” and “lunar.” The second problem is that, by default, a text search does not sort the results by relevance.\n\nWe can begin to address the problem of the query itself by using a phrase in our query. You can search for exact phrases by wrapping them in double quotes. For\n\n148\n\n|\n\nChapter 6: Special Index and Collection Types",
      "page_number": 155
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 163-172)",
      "start_page": 163,
      "end_page": 172,
      "detection_method": "topic_boundary",
      "content": "example, the following will find all documents containing the phrase “impact crater.” Possibly surprising is that MongoDB will issue this query as “impact crater” AND “lunar”:\n\n> db.articles.find({$text: {$search: \"\\\"impact crater\\\" lunar\"}}, {title: 1} ).limit(10) { \"_id\" : \"2621724\", \"title\" : \"Schjellerup (crater)\" } { \"_id\" : \"2622075\", \"title\" : \"Steno (lunar crater)\" } { \"_id\" : \"168118\", \"title\" : \"South Pole–Aitken basin\" } { \"_id\" : \"1509118\", \"title\" : \"Jackson (crater)\" } { \"_id\" : \"10096822\", \"title\" : \"Victoria Island structure\" } { \"_id\" : \"968071\", \"title\" : \"Buldhana district\" } { \"_id\" : \"780422\", \"title\" : \"Puchezh-Katunki crater\" } { \"_id\" : \"28088964\", \"title\" : \"Svedberg (crater)\" } { \"_id\" : \"780628\", \"title\" : \"Zeleny Gai crater\" } { \"_id\" : \"926711\", \"title\" : \"Fracastorius (crater)\" }\n\nTo make sure the semantics of this are clear, let’s look at an expanded example. For the following query, MongoDB will issue the query as “impact crater” AND (“lunar” OR “meteor”). MongoDB performs a logical AND of the phrase with the individual terms in the search string and a logical OR of the individual terms with one another:\n\n> db.articles.find({$text: {$search: \"\\\"impact crater\\\" lunar meteor\"}}, {title: 1} ).limit(10)\n\nIf you want to issue a logical AND between individual terms in a query, treat each term as a phrase by wrapping it in quotes. The following query will return documents containing “impact crater” AND “lunar” AND “meteor”:\n\n> db.articles.find({$text: {$search: \"\\\"impact crater\\\" \\\"lunar\\\" \\\"meteor\\\"\"}}, {title: 1}\n\n).limit(10)\n\n{ \"_id\" : \"168118\", \"title\" : \"South Pole–Aitken basin\" } { \"_id\" : \"330593\", \"title\" : \"Giordano Bruno (crater)\" } { \"_id\" : \"421051\", \"title\" : \"Opportunity (rover)\" } { \"_id\" : \"2693649\", \"title\" : \"Pascal Lee\" } { \"_id\" : \"275128\", \"title\" : \"Tektite\" } { \"_id\" : \"14594455\", \"title\" : \"Beethoven quadrangle\" } { \"_id\" : \"266344\", \"title\" : \"Space debris\" } { \"_id\" : \"2137763\", \"title\" : \"Wegener (lunar crater)\" } { \"_id\" : \"929164\", \"title\" : \"Dawes (lunar crater)\" } { \"_id\" : \"24944\", \"title\" : \"Plate tectonics\" }\n\nNow that you have a better understanding of using phrases and logical ANDs in your queries, let’s return to the problem of the results not being sorted by relevance. While the preceding results are certainly relevant, this is mostly due to the fairly strict query we’ve issued. We can do better by sorting for relevance.\n\nIndexes for Full Text Search\n\n|\n\n149\n\nText queries cause some metadata to be associated with each query result. The meta‐ data is not displayed in the query results unless we explicitly project it using the $meta operator. So, in addition to the title, we will project the relevance score calcula‐ ted for each document. The relevance score is stored in the metadata field named \"textScore\". For this example, we’ll return to our query of “impact crater” AND “lunar”:\n\n> db.articles.find({$text: {$search: \"\\\"impact crater\\\" lunar\"}}, {title: 1, score: {$meta: \"textScore\"}}\n\n).limit(10)\n\n{\"_id\": \"2621724\", \"title\": \"Schjellerup (crater)\", \"score\": 2.852987132352941} {\"_id\": \"2622075\", \"title\": \"Steno (lunar crater)\", \"score\": 2.4766639610389607} {\"_id\": \"168118\", \"title\": \"South Pole–Aitken basin\", \"score\": 2.980198136295181} {\"_id\": \"1509118\", \"title\": \"Jackson (crater)\", \"score\": 2.3419137286324787} {\"_id\": \"10096822\", \"title\": \"Victoria Island structure\", \"score\": 1.782051282051282} {\"_id\": \"968071\", \"title\": \"Buldhana district\", \"score\": 1.6279783393501805} {\"_id\": \"780422\", \"title\": \"Puchezh-Katunki crater\", \"score\": 1.9295977011494254} {\"_id\": \"28088964\", \"title\": \"Svedberg (crater)\", \"score\": 2.497767857142857} {\"_id\": \"780628\", \"title\": \"Zeleny Gai crater\", \"score\": 1.4866071428571428} {\"_id\": \"926711\", \"title\": \"Fracastorius (crater)\", \"score\": 2.7511877111486487}\n\nNow you can see the relevance score projected with the title for each result. Note that they are not sorted. To sort the results in order of relevance score, we must add a call to sort, again using $meta to specify the \"textScore\" field value. Note that we must use the same field name in our sort as we used in our projection. In this case, we used the field name \"score\" for the relevance score value displayed in our search results. As you can see, the results are now sorted in decreasing order of relevance:\n\n> db.articles.find({$text: {$search: \"\\\"impact crater\\\" lunar\"}}, {title: 1, score: {$meta: \"textScore\"}}\n\n).sort({score: {$meta: \"textScore\"}}).limit(10)\n\n{\"_id\": \"1621514\", \"title\": \"Lunar craters\", \"score\": 3.1655242042922014} {\"_id\": \"14580008\", \"title\": \"Kuiper quadrangle\", \"score\": 3.0847527829208814} {\"_id\": \"1019830\", \"title\": \"Shackleton (crater)\", \"score\": 3.076471119932001} {\"_id\": \"2096232\", \"title\": \"Geology of the Moon\", \"score\": 3.064981949458484} {\"_id\": \"927269\", \"title\": \"Messier (crater)\", \"score\": 3.0638183133686008} {\"_id\": \"206589\", \"title\": \"Lunar geologic timescale\", \"score\": 3.062029540854157} {\"_id\": \"14536060\", \"title\": \"Borealis quadrangle\", \"score\": 3.0573010719646687} {\"_id\": \"14609586\", \"title\": \"Michelangelo quadrangle\", \"score\": 3.057224063486582} {\"_id\": \"14568465\", \"title\": \"Shakespeare quadrangle\", \"score\": 3.0495256481056443} {\"_id\": \"275128\", \"title\": \"Tektite\", \"score\" : 3.0378807169646915}\n\nText search is also available in the aggregation pipeline. We discuss the aggregation pipeline in Chapter 7.\n\n150\n\n|\n\nChapter 6: Special Index and Collection Types\n\nOptimizing Full-Text Search There are a couple of ways to optimize full-text searches. If you can first narrow your search results by other criteria, you can create a compound index with a prefix of those criteria and then the full-text fields:\n\n> db.blog.createIndex({\"date\" : 1, \"post\" : \"text\"})\n\nThis is referred to as partitioning the full-text index, as it breaks it into several smaller trees based on \"date\" (in this example). This makes full-text searches for a specific date or date range much faster.\n\nYou can also use a postfix of other criteria to cover queries with the index. For exam‐ ple, if we were only returning the \"author\" and \"post\" fields, we could create a com‐ pound index on both:\n\n> db.blog.createIndex({\"post\" : \"text\", \"author\" : 1})\n\nThese prefix and postfix forms can be combined:\n\n> db.blog.createIndex({\"date\" : 1, \"post\" : \"text\", \"author\" : 1})\n\nSearching in Other Languages When a document is inserted (or the index is first created), MongoDB looks at the index’s fields and stems each word, reducing it to an essential unit. However, different languages stem words in different ways, so you must specify what language the index or document is in. text indexes allow a \"default_language\" option to be specified, which defaults to \"english\" but can be set to a number of other languages (see the online documentation for an up-to-date list).\n\nFor example, to create a French-language index, we could say:\n\n> db.users.createIndex({\"profil\" : \"text\", \"intérêts\" : \"text\"}, {\"default_language\" : \"french\"})\n\nThen French would be used for stemming, unless otherwise specified. You can, on a per-document basis, specify another stemming language by having a \"language\" field that describes the document’s language:\n\n> db.users.insert({\"username\" : \"swedishChef\", ... \"profile\" : \"Bork de bork\", language : \"swedish\"})\n\nCapped Collections “Normal” collections in MongoDB are created dynamically and automatically grow in size to fit additional data. MongoDB also supports a different type of collection, called a capped collection, which is created in advance and is fixed in size (see Figure 6-4).\n\nCapped Collections\n\n|\n\n151\n\nFigure 6-4. New documents are inserted at the end of the queue\n\nHaving fixed-size collections brings up an interesting question: what happens when we try to insert into a capped collection that is already full? The answer is that capped collections behave like circular queues: if we’re out of space, the oldest document will be deleted, and the new one will take its place (see Figure 6-5). This means that cap‐ ped collections automatically age out the oldest documents as new documents are inserted.\n\nCertain operations are not allowed on capped collections. Documents cannot be removed or deleted (aside from the automatic age-out described earlier), and updates that would cause documents to grow in size are disallowed. By preventing these two operations, we guarantee that documents in a capped collection are stored in inser‐ tion order and that there is no need to maintain a free list for space from removed documents.\n\n152\n\n|\n\nChapter 6: Special Index and Collection Types\n\nFigure 6-5. When the queue is full, the oldest element will be replaced by the newest\n\nCapped collections have a different access pattern than most MongoDB collections: data is written sequentially over a fixed section of disk. This makes them tend to per‐ form writes quickly on spinning disks, especially if they can be given their own disk (so as not to be “interrupted” by other collections’ random writes).\n\nIn general, MongoDB TTL indexes are recommended over capped collections because they perform better with the WiredTiger storage engine. TTL indexes expire and remove data from normal collections based on the value of a date-typed field and a TTL value for the index. These are covered in more depth later in this chapter.\n\nCapped collections cannot be sharded. If an update or a replace‐ ment operation changes the document size in a capped collection, the operation will fail.\n\nCapped collections tend to be useful for logging, although they lack flexibility: you cannot control when data ages out, other than setting a size when you create the collection.\n\nCapped Collections\n\n|\n\n153\n\nCreating Capped Collections Unlike normal collections, capped collections must be explicitly created before they are used. To create a capped collection, use the create command. From the shell, this can be done using createCollection:\n\n> db.createCollection(\"my_collection\", {\"capped\" : true, \"size\" : 100000});\n\nThe previous command creates a capped collection, my_collection, that has a fixed size of 100,000 bytes.\n\ncreateCollection can also specify a limit on the number of documents in a capped collection:\n\n> db.createCollection(\"my_collection2\", {\"capped\" : true, \"size\" : 100000, \"max\" : 100});\n\nYou could use this to keep, say, the latest 10 news articles or limit a user to 1,000 documents.\n\nOnce a capped collection has been created, it cannot be changed (it must be dropped and recreated if you wish to change its properties). Thus, you should think carefully about the size of a large collection before creating it.\n\nWhen limiting the number of documents in a capped collection, you must specify a size limit as well. Age-out will be based on whichever limit is reached first: it can neither hold more than \"max\" documents nor take up more than \"size\" space.\n\nAnother option for creating a capped collection is to convert an existing regular col‐ lection into a capped collection. This can be done using the convertToCapped command—in the following example, we convert the test collection to a capped col‐ lection of 10,000 bytes:\n\n> db.runCommand({\"convertToCapped\" : \"test\", \"size\" : 10000}); { \"ok\" : true }\n\nThere is no way to “uncap” a capped collection (other than dropping it).\n\nTailable Cursors Tailable cursors are a special type of cursor that are not closed when their results are exhausted. They were inspired by the tail -f command and, similar to that com‐ mand, will continue fetching output for as long as possible. Because the cursors do not die when they run out of results, they can continue to fetch new results as docu‐ ments are added to the collection. Tailable cursors can be used only on capped collec‐ tions, since insert order is not tracked for normal collections. For the vast majority of uses, change streams, covered in Chapter 16, are recommended over tailable cursors\n\n154\n\n|\n\nChapter 6: Special Index and Collection Types\n\nas they offer vastly more control and configuration plus they work with normal collections.\n\nTailable cursors are often used for processing documents as they are inserted onto a “work queue” (the capped collection). Because tailable cursors will time out after 10 minutes of no results, it is important to include logic to requery the collection if they die. The mongo shell does not allow you to use tailable cursors, but using one in PHP looks something like the following:\n\n$cursor = $collection->find([], [ 'cursorType' => MongoDB\\Operation\\Find::TAILABLE_AWAIT, 'maxAwaitTimeMS' => 100, ]);\n\nwhile (true) { if ($iterator->valid()) { $document = $iterator->current(); printf(\"Consumed document created at: %s\\n\", $document->createdAt); }\n\n$iterator->next(); }\n\nThe cursor will process results or wait for more results to arrive until it times out or someone kills the query operation.\n\nTime-To-Live Indexes As mentioned in the previous section, capped collections give you limited control over when their contents are overwritten. If you need a more flexible age-out system, TTL indexes allow you to set a timeout for each document. When a document rea‐ ches a preconfigured age, it will be deleted. This type of index is useful for caching use cases such as session storage.\n\nYou can create a TTL index by specifying the \"expireAfterSeconds\" option in the second argument to createIndex:\n\n> // 24-hour timeout > db.sessions.createIndex({\"lastUpdated\" : 1}, {\"expireAfterSeconds\" : 60*60*24})\n\nThis creates a TTL index on the \"lastUpdated\" field. If a document’s \"lastUpdated\" field exists and is a date, the document will be removed once the server time is \"expireAfterSeconds\" seconds ahead of the document’s time.\n\nTo prevent an active session from being removed, you can update the \"lastUpdated\" field to the current time whenever there is activity. Once \"lastUpdated\" is 24 hours old, the document will be removed.\n\nTime-To-Live Indexes\n\n|\n\n155\n\nMongoDB sweeps the TTL index once per minute, so you should not depend on to- the-second granularity. You can change the \"expireAfterSeconds\" using the coll Mod command:\n\n> db.runCommand( {\"collMod\" : \"someapp.cache\" , \"index\" : { \"keyPattern\" : ... {\"lastUpdated\" : 1} , \"expireAfterSeconds\" : 3600 } } );\n\nYou can have multiple TTL indexes on a given collection. They cannot be compound indexes but can be used like “normal” indexes for the purposes of sorting and query optimization.\n\nStoring Files with GridFS GridFS is a mechanism for storing large binary files in MongoDB. There are several reasons why you might consider using GridFS for file storage:\n\nUsing GridFS can simplify your stack. If you’re already using MongoDB, you might be able to use GridFS instead of a separate tool for file storage.\n\nGridFS will leverage any existing replication or autosharding that you’ve set up for MongoDB, so getting failover and scale-out for file storage is easier.\n\nGridFS can alleviate some of the issues that certain filesystems can exhibit when being used to store user uploads. For example, GridFS does not have issues with storing large numbers of files in the same directory.\n\nThere are some downsides, too:\n\nPerformance is slower. Accessing files from MongoDB will not be as fast as going directly through the filesystem.\n\nYou can only modify documents by deleting them and resaving the whole thing. MongoDB stores files as multiple documents, so it cannot lock all of the chunks in a file at the same time.\n\nGridFS is generally best when you have large files you’ll be accessing in a sequential fashion that won’t be changing much.\n\nGetting Started with GridFS: mongofiles The easiest way to try out GridFS is by using the mongofiles utility. mongofiles is included with all MongoDB distributions and can be used to upload, download, list, search for, or delete files in GridFS.\n\nAs with any of the other command-line tools, run mongofiles --help to see the options available for mongofiles.\n\n156\n\n|\n\nChapter 6: Special Index and Collection Types\n\nThe following session shows how to use mongofiles to upload a file from the filesys‐ tem to GridFS, list all of the files in GridFS, and download a file that we’ve previously uploaded:\n\n$ echo \"Hello, world\" > foo.tx $ mongofiles put foo.txt 2019-10-30T10:12:06.588+0000 connected to: localhost 2019-10-30T10:12:06.588+0000 added file: foo.txt $ mongofiles list 2019-10-30T10:12:41.603+0000 connected to: localhost foo.txt 13 $ rm foo.txt $ mongofiles get foo.txt 2019-10-30T10:13:23.948+0000 connected to: localhost 2019-10-30T10:13:23.955+0000 finished writing to foo.txt $ cat foo.txt Hello, world\n\nIn the previous example, we perform three basic operations using mongofiles: put, list, and get. The put operation takes a file in the filesystem and adds it to GridFS. list will list any files that have been added to GridFS. get does the inverse of put: it takes a file from GridFS and writes it to the filesystem. mongofiles also supports two other operations: search for finding files in GridFS by filename and delete for removing a file from GridFS.\n\nWorking with GridFS from the MongoDB Drivers All the client libraries have GridFS APIs. For example, with PyMongo (the Python driver for MongoDB) you can perform the same series of operations (this assumes Python 3 and a locally running mongod on port 27017) as we did with mongofiles as follows:\n\n>>> import pymongo >>> import gridfs >>> client = pymongo.MongoClient() >>> db = client.test >>> fs = gridfs.GridFS(db) >>> file_id = fs.put(b\"Hello, world\", filename=\"foo.txt\") >>> fs.list() ['foo.txt'] >>> fs.get(file_id).read() b'Hello, world'\n\nThe API for working with GridFS from PyMongo is very similar to that of mongofiles: you can easily perform the basic put, get, and list operations. Almost all the MongoDB drivers follow this basic pattern for working with GridFS, while often exposing more advanced functionality as well. For driver-specific information on GridFS, please check out the documentation for the specific driver you’re using.\n\nStoring Files with GridFS\n\n|\n\n157\n\nUnder the Hood GridFS is a lightweight specification for storing files that is built on top of normal MongoDB documents. The MongoDB server actually does almost nothing to “special-case” the handling of GridFS requests; all the work is handled by the client- side drivers and tools.\n\nThe basic idea behind GridFS is that we can store large files by splitting them up into chunks and storing each chunk as a separate document. Because MongoDB supports storing binary data in documents, we can keep the storage overhead for chunks to a minimum. In addition to storing each chunk of a file, we store a single document that groups the chunks together and contains metadata about the file.\n\nThe chunks for GridFS are stored in their own collection. By default chunks will use the collection fs.chunks, but this can be overridden. Within the chunks collection the structure of the individual documents is pretty simple:\n\n{ \"_id\" : ObjectId(\"...\"), \"n\" : 0, \"data\" : BinData(\"...\"), \"files_id\" : ObjectId(\"...\") }\n\nLike any other MongoDB document, a chunk has its own unique \"_id\". In addition, it has a couple of other keys:\n\n\"files_id\"\n\nThe \"_id\" of the file document that contains the metadata for the file this chunk is from\n\n\"n\"\n\nThe chunk’s position in the file, relative to the other chunks\n\n\"data\"\n\nThe bytes in this chunk of the file\n\nThe metadata for each file is stored in a separate collection, which defaults to fs.files. Each document in the files collection represents a single file in GridFS and can con‐ tain any custom metadata that should be associated with that file. In addition to any user-defined keys, there are a couple of keys that are mandated by the GridFS specification:\n\n\"_id\"\n\nA unique ID for the file—this is what will be stored in each chunk as the value for the \"files_id\" key.\n\n158\n\n|\n\nChapter 6: Special Index and Collection Types",
      "page_number": 163
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 173-182)",
      "start_page": 173,
      "end_page": 182,
      "detection_method": "topic_boundary",
      "content": "\"length\"\n\nThe total number of bytes making up the content of the file.\n\n\"chunkSize\"\n\nThe size of each chunk comprising the file, in bytes. The default is 255 KB, but this can be adjusted if needed.\n\n\"uploadDate\"\n\nA timestamp representing when this file was stored in GridFS.\n\n\"md5\"\n\nAn MD5 checksum of this file’s contents, generated on the server side.\n\nOf all the required keys, perhaps the most interesting (or least self-explanatory) is \"md5\". The value for \"md5\" is generated by the MongoDB server using the filemd5 command, which computes the MD5 checksum of the uploaded chunks. This means that users can check the value of the \"md5\" key to ensure that a file was uploaded correctly.\n\nAs mentioned previously, you are not limited to the required fields in fs.files: feel free to keep any other file metadata in this collection as well. You might want to keep information such as download count, MIME type, or user rating with a file’s meta‐ data.\n\nOnce you understand the underlying GridFS specification, it becomes trivial to implement features that the driver you’re using might not provide helpers for. For example, you can use the distinct command to get a list of unique filenames stored in GridFS:\n\n> db.fs.files.distinct(\"filename\") [ \"foo.txt\" , \"bar.txt\" , \"baz.txt\" ]\n\nThis allows your application a great deal of flexibility in loading and collecting infor‐ mation about files. We’ll change direction slightly in the next chapter, as we introduce the aggregation framework. It offers a range of data analytic tools to process the data in your database.\n\nStoring Files with GridFS\n\n|\n\n159\n\nCHAPTER 7 Introduction to the Aggregation Framework\n\nMany applications require data analysis of one form or another. MongoDB provides powerful support for running analytics natively using the aggregation framework. In this chapter, we introduce this framework and some of the fundamental tools it pro‐ vides. We’ll cover:\n\nThe aggregation framework\n\nAggregation stages\n\nAggregation expressions\n\nAggregation accumulators\n\nIn the next chapter we’ll dive deeper and look at more advanced aggregation features, including the ability to perform joins across collections.\n\nPipelines, Stages, and Tunables The aggregation framework is a set of analytics tools within MongoDB that allow you to do analytics on documents in one or more collections.\n\nThe aggregation framework is based on the concept of a pipeline. With an aggrega‐ tion pipeline we take input from a MongoDB collection and pass the documents from that collection through one or more stages, each of which performs a different opera‐ tion on its inputs (Figure 7-1). Each stage takes as input whatever the stage before it produced as output. The inputs and outputs for all stages are documents—a stream of documents, if you will.\n\n161\n\nFigure 7-1. The aggregation pipeline\n\nIf you’re familiar with pipelines in a Linux shell, such as bash, this is a very similar idea. Each stage has a specific job that it does. It expects a specific form of document and produces a specific output, which is itself a stream of documents. At the end of the pipeline we get access to the output, in much the same way that we would by exe‐ cuting a find query. That is, we get a stream of documents back that we can then use to do additional work, whether it’s creating a report of some kind, generating a web‐ site, or some other type of task.\n\nNow, let’s dive in a little deeper and consider the individual stages. An individual stage of an aggregation pipeline is a data processing unit. It takes in a stream of input documents one at a time, processes each document one at a time, and produces an output stream of documents one at a time (Figure 7-2).\n\nFigure 7-2. Stages of the aggregation pipeline\n\nEach stage provides a set of knobs, or tunables, that we can control to parameterize the stage to perform whatever task we’re interested in doing. A stage performs a generic, general-purpose task of some kind, and we parameterize the stage for the particular collection that we’re working with and exactly what we would like that stage to do with those documents.\n\nThese tunables typically take the form of operators that we can supply that will mod‐ ify fields, perform arithmetic operations, reshape documents, or do some sort of accumulation task or a variety of other things.\n\nBefore we start looking at some concrete examples, there’s one more aspect of pipe‐ lines that is especially important to keep in mind as you begin to work with them. Frequently, we want to include the same type of stage multiple times within a single pipeline (Figure 7-3). For example, we may want to perform an initial filter so that we\n\n162\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\ndon’t have to pass the entire collection into our pipeline. Later, following some addi‐ tional processing, we might then want to filter further, applying a different set of criteria.\n\nFigure 7-3. Repeated stages in the aggregation pipeline\n\nTo recap, pipelines work with MongoDB collections. They’re composed of stages, each of which does a different data processing task on its input and produces docu‐ ments as output to be passed to the next stage. Finally, at the end of the processing, a pipeline produces output that we can then do something with in our application or that we can send to a collection for later use. In many cases, in order to perform the analysis we need to do, we will include the same type of stage multiple times within an individual pipeline.\n\nGetting Started with Stages: Familiar Operations To get started developing aggregation pipelines, we will look at building some pipe‐ lines that involve operations that are already familiar to you. For this we will look at the match, project, sort, skip, and limit stages.\n\nTo work through these aggregation examples, we will use a collection of company data. The collection has a number of fields that specify details about the companies, such as name, a short description of the company, and when the company was founded.\n\nThere are also fields describing the rounds of funding a company has gone through, important milestones for the company, whether or not the company has been through an initial public offering (IPO), and, if so, the details of the IPO. Here’s an example document containing data on Facebook, Inc.:\n\n{ \"_id\" : \"52cdef7c4bab8bd675297d8e\", \"name\" : \"Facebook\", \"category_code\" : \"social\", \"founded_year\" : 2004, \"description\" : \"Social network\", \"funding_rounds\" : [{ \"id\" : 4, \"round_code\" : \"b\", \"raised_amount\" : 27500000,\n\nGetting Started with Stages: Familiar Operations\n\n|\n\n163\n\n164\n\n\"raised_currency_code\" : \"USD\", \"funded_year\" : 2006, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Meritech Capital Partners\", \"permalink\" : \"meritech-capital-partners\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Founders Fund\", \"permalink\" : \"founders-fund\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" }, \"person\" : null } ] }, { \"id\" : 2197, \"round_code\" : \"c\", \"raised_amount\" : 15000000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2008, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"European Founders Fund\", \"permalink\" : \"european-founders-fund\" }, \"person\" : null }\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\n] }], \"ipo\" : { \"valuation_amount\" : NumberLong(\"104000000000\"), \"valuation_currency_code\" : \"USD\", \"pub_year\" : 2012, \"pub_month\" : 5, \"pub_day\" : 18, \"stock_symbol\" : \"NASDAQ:FB\" } }\n\nAs our first aggregation example, let’s do a simple filter looking for all companies that were founded in 2004:\n\ndb.companies.aggregate([ {$match: {founded_year: 2004}}, ])\n\nThis is equivalent to the following operation using find:\n\ndb.companies.find({founded_year: 2004})\n\nNow let’s add a project stage to our pipeline to reduce the output to just a few fields include \"name\" and the \"_id\" per document. We’ll exclude \"founded_year\". Our pipeline will be as follows:\n\nfield, but\n\ndb.companies.aggregate([ {$match: {founded_year: 2004}}, {$project: { _id: 0, name: 1, founded_year: 1 }} ])\n\nIf we run this, we get output that looks like the following:\n\n{\"name\": \"Digg\", \"founded_year\": 2004 } {\"name\": \"Facebook\", \"founded_year\": 2004 } {\"name\": \"AddThis\", \"founded_year\": 2004 } {\"name\": \"Veoh\", \"founded_year\": 2004 } {\"name\": \"Pando Networks\", \"founded_year\": 2004 } {\"name\": \"Jobster\", \"founded_year\": 2004 } {\"name\": \"AllPeers\", \"founded_year\": 2004 } {\"name\": \"blinkx\", \"founded_year\": 2004 } {\"name\": \"Yelp\", \"founded_year\": 2004 } {\"name\": \"KickApps\", \"founded_year\": 2004 } {\"name\": \"Flickr\", \"founded_year\": 2004 } {\"name\": \"FeedBurner\", \"founded_year\": 2004 } {\"name\": \"Dogster\", \"founded_year\": 2004 } {\"name\": \"Sway\", \"founded_year\": 2004 } {\"name\": \"Loomia\", \"founded_year\": 2004 } {\"name\": \"Redfin\", \"founded_year\": 2004 }\n\nGetting Started with Stages: Familiar Operations\n\n|\n\n165\n\n{\"name\": \"Wink\", \"founded_year\": 2004 } {\"name\": \"Techmeme\", \"founded_year\": 2004 } {\"name\": \"Eventful\", \"founded_year\": 2004 } {\"name\": \"Oodle\", \"founded_year\": 2004 } ...\n\nLet’s unpack this aggregation pipeline in a little more detail. The first thing you will notice is that we’re using the aggregate method. This is the method we call when we want to run an aggregation query. To aggregate, we pass in an aggregation pipeline. A pipeline is an array with documents as elements. Each of the documents must stipu‐ late a particular stage operator. In this example, we have a pipeline that has two stages: a match stage for filtering and a project stage with which we’re limiting the output to just two fields per document.\n\nThe match stage filters against the collection and passes the resulting documents to the project stage one at a time. The project stage then performs its operation, reshap‐ ing the documents, and passes the output out of the pipeline and back to us.\n\nNow let’s extend our pipeline a bit further to include a limit stage. We’re going to match using the same query, but we’ll limit our result set to five and then project out the fields we want. For simplicity, let’s limit our output to just the names of each company:\n\ndb.companies.aggregate([ {$match: {founded_year: 2004}}, {$limit: 5}, {$project: { _id: 0, name: 1}} ])\n\nThe result is as follows:\n\n{\"name\": \"Digg\"} {\"name\": \"Facebook\"} {\"name\": \"AddThis\"} {\"name\": \"Veoh\"} {\"name\": \"Pando Networks\"}\n\nNote that we’ve constructed this pipeline so that we limit before the project stage. If we ran the project stage first and then the limit, as in the following query, we would get exactly the same results, but we’d have to pass hundreds of documents through the project stage before finally limiting the results to five:\n\ndb.companies.aggregate([ {$match: {founded_year: 2004}}, {$project: { _id: 0, name: 1}}, {$limit: 5} ])\n\n166\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\nRegardless of what types of optimizations the MongoDB query planner might be capable of in a given release, you should always consider the efficiency of your aggre‐ gation pipeline. Ensure that you are limiting the number of documents that need to be passed on from one stage to another as you build your pipeline.\n\nThis requires careful consideration of the entire flow of documents through a pipe‐ line. In the case of the preceding query, we’re only interested in the first five docu‐ ments that match our query, regardless of how they are sorted, so it’s perfectly fine to limit as our second stage.\n\nHowever, if the order matters, then we’ll need to sort before the limit stage. Sorting works in a manner similar to what we have seen already, except that in the aggrega‐ tion framework, we specify sort as a stage within a pipeline as follows (in this case, we will sort by name in ascending order):\n\ndb.companies.aggregate([ { $match: { founded_year: 2004 } }, { $sort: { name: 1} }, { $limit: 5 }, { $project: { _id: 0, name: 1 } } ])\n\nWe get the following result from our companies collection:\n\n{\"name\": \"1915 Studios\"} {\"name\": \"1Scan\"} {\"name\": \"2GeeksinaLab\"} {\"name\": \"2GeeksinaLab\"} {\"name\": \"2threads\"}\n\nNote that we’re looking at a different set of five companies now, getting instead the first five documents in alphanumeric order by name.\n\nFinally, let’s take a look at including a skip stage. Here, we sort first, then skip the first 10 documents and again limit our result set to 5 documents:\n\ndb.companies.aggregate([ {$match: {founded_year: 2004}}, {$sort: {name: 1}}, {$skip: 10}, {$limit: 5}, {$project: { _id: 0, name: 1}}, ])\n\nLet’s review our pipeline one more time. We have five stages. First, we’re filtering the companies collection, looking only for documents where the \"founded_year\" is 2004. Then we’re sorting based on the name in ascending order, skipping the first 10\n\nGetting Started with Stages: Familiar Operations\n\n|\n\n167\n\nmatches, and limiting our end results to 5. Finally, we pass those five documents on to the project stage, where we reshape the documents such that our output docu‐ ments contain just the company name.\n\nHere, we’ve looked at constructing pipelines using stages that perform operations that should already be familiar to you. These operations are provided in the aggregation framework because they are necessary for the types of analytics that we’ll want to accomplish using stages discussed in later sections. As we move through the rest of this chapter, we will take a deep dive into the other operations that the aggregation framework provides.\n\nExpressions As we move deeper into our discussion of the aggregation framework, it is important to have a sense of the different types of expressions available for use as you construct aggregation pipelines. The aggregation framework supports many different classes of expressions:\n\nBoolean expressions allow us to use AND, OR, and NOT expressions.\n\nSet expressions allow us to work with arrays as sets. In particular, we can get the intersection or union of two or more sets. We can also take the difference of two sets and perform a number of other set operations.\n\nComparison expressions enable us to express many different types of range filters.\n\nArithmetic expressions enable us to calculate the ceiling, floor, natural log, and log, as well as perform simple arithmetic operations like multiplication, division, addition, and subtraction. We can even do more complex operations, such as cal‐ culating the square root of a value.\n\nString expressions allow us to concatenate, find substrings, and perform opera‐ tions having to do with case and text search operations.\n\nArray expressions provide a lot of power for manipulating arrays, including the ability to filter array elements, slice an array, or just take a range of values from a specific array.\n\nVariable expressions, which we won’t dive into too deeply, allow us to work with literals, expressions for parsing date values, and conditional expressions.\n\nAccumulators provide the ability to calculate sums, descriptive statistics, and many other types of values.\n\n168\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\n$project Now we’re going to take a deeper dive into the project stage and reshaping docu‐ ments, exploring the types of reshaping operations that should be most common in the applications that you develop. We have seen some simple projections in aggrega‐ tion pipelines, and now we’ll take a look at some that are a little more complex.\n\nFirst, let’s look at promoting nested fields. In the following pipeline, we are doing a match:\n\ndb.companies.aggregate([ {$match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\" }}, {$project: { _id: 0, name: 1, ipo: \"$ipo.pub_year\", valuation: \"$ipo.valuation_amount\", funders: \"$funding_rounds.investments.financial_org.permalink\" }} ]).pretty()\n\nAs an example of the relevant fields for documents in our companies collection, let’s again look at a portion of the Facebook document:\n\n{ \"_id\" : \"52cdef7c4bab8bd675297d8e\", \"name\" : \"Facebook\", \"category_code\" : \"social\", \"founded_year\" : 2004, \"description\" : \"Social network\", \"funding_rounds\" : [{ \"id\" : 4, \"round_code\" : \"b\", \"raised_amount\" : 27500000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2006, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Meritech Capital Partners\", \"permalink\" : \"meritech-capital-partners\" }, \"person\" : null\n\n$project\n\n|\n\n169",
      "page_number": 173
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 183-190)",
      "start_page": 183,
      "end_page": 190,
      "detection_method": "topic_boundary",
      "content": "}, { \"company\" : null, \"financial_org\" : { \"name\" : \"Founders Fund\", \"permalink\" : \"founders-fund\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" }, \"person\" : null } ] }, { \"id\" : 2197, \"round_code\" : \"c\", \"raised_amount\" : 15000000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2008, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"European Founders Fund\", \"permalink\" : \"european-founders-fund\" }, \"person\" : null } ] }], \"ipo\" : { \"valuation_amount\" : NumberLong(\"104000000000\"), \"valuation_currency_code\" : \"USD\", \"pub_year\" : 2012, \"pub_month\" : 5, \"pub_day\" : 18, \"stock_symbol\" : \"NASDAQ:FB\" } }\n\nGoing back to our match:\n\ndb.companies.aggregate([ {$match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\" }}, {$project: { _id: 0, name: 1,\n\n170\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\nipo: \"$ipo.pub_year\", valuation: \"$ipo.valuation_amount\", funders: \"$funding_rounds.investments.financial_org.permalink\" }} ]).pretty()\n\nwe are filtering for all companies that had a funding round in which Greylock Part‐ ners participated. The permalink value, \"greylock\", is the unique identifier for such documents. Here is another view of the Facebook document with just the relevant fields displayed:\n\n{ ... \"name\" : \"Facebook\", ... \"funding_rounds\" : [{ ... \"investments\" : [{ ... \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" }, ... }, { ... \"financial_org\" : { \"name\" : \"Meritech Capital Partners\", \"permalink\" : \"meritech-capital-partners\" }, ... }, { ... \"financial_org\" : { \"name\" : \"Founders Fund\", \"permalink\" : \"founders-fnd\" }, ... }, { \"company\" : null, \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" }, ... }], ... ]}, {\n\n$project\n\n|\n\n171\n\n... \"investments\" : [{ ... \"financial_org\" : { \"name\" : \"European Founders Fund\", \"permalink\" : \"european-founders-fund\" }, ... }] }], \"ipo\" : { \"valuation_amount\" : NumberLong(\"104000000000\"), \"valuation_currency_code\" : \"USD\", \"pub_year\" : 2012, \"pub_month\" : 5, \"pub_day\" : 18, \"stock_symbol\" : \"NASDAQ:FB\" } }\n\nThe project stage we have defined in this aggregation pipeline will suppress the \"_id\" and include the \"name\". It will also promote some nested fields. This project uses dot notation to express field paths that reach into the \"ipo\" field and the \"fund ing_rounds\" field to select values from those nested documents and arrays. This project stage will make those the values of top-level fields in the documents it pro‐ duces as output, as shown here:\n\n{ \"name\" : \"Digg\", \"funders\" : [ [ \"greylock\", \"omidyar-network\" ], [ \"greylock\", \"omidyar-network\", \"floodgate\", \"sv-angel\" ], [ \"highland-capital-partners\", \"greylock\", \"omidyar-network\", \"svb-financial-group\" ] ] } { \"name\" : \"Facebook\", \"ipo\" : 2012, \"valuation\" : NumberLong(\"104000000000\"),\n\n172\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\n\"funders\" : [ [ \"accel-partners\" ], [ \"greylock\", \"meritech-capital-partners\", \"founders-fund\", \"sv-angel\" ], ... [ \"goldman-sachs\", \"digital-sky-technologies-fo\" ] ] } { \"name\" : \"Revision3\", \"funders\" : [ [ \"greylock\", \"sv-angel\" ], [ \"greylock\" ] ] } ...\n\nIn the output, each document has a \"name\" field and a \"funders\" field. For those companies that have gone through an IPO, the \"ipo\" field contains the year the com‐ pany went public and the \"valuation\" field contains the value of the company at the time of the IPO. Note that in all of these documents, these are top-level fields and the values for those fields were promoted from nested documents and arrays.\n\nThe $ character used to specify the values for ipo, valuation, and funders in our project stage indicates that the values should be interpreted as field paths and used to select the value that should be projected for each field, respectively.\n\nOne thing you might have noticed is that we’re seeing multiple values printed out for funders. In fact, we’re seeing an array of arrays. Based on our review of the Facebook example document, we know that all of the funders are listed within an array called \"investments\". Our stage specifies that we want to project the financial_org.perma link value for each entry in the \"investments\" array, for every funding round. So, an array of arrays of funders’ names is built up.\n\nIn later sections we will look at how to perform arithmetic and other operations on strings, dates, and a number of other value types to project documents of all shapes\n\n$project\n\n|\n\n173\n\nand sizes. Just about the only thing we can’t do from a project stage is change the data type for a value.\n\n$unwind When working with array fields in an aggregation pipeline, it is often necessary to include one or more unwind stages. This allows us to produce output such that there is one output document for each element in a specified array field.\n\nFigure 7-4. $unwind takes an array from the input document and creates an output document for each element in that array\n\nIn the example in Figure 7-4, we have an input document that has three keys and their corresponding values. The third key has as its value an array with three ele‐ ments. $unwind if run on this type of input document and configured to unwind the key3 field will produce documents that look like those shown at the bottom of Figure 7-4. The thing that might not be intuitive to you about this is that in each of these output documents there will be a key3 field, but that field will contain a single value rather than an array value, and there will be a separate document for each one of the elements that were in this array. In other words, if there were 10 elements in the array, the unwind stage would produce 10 output documents.\n\nLet’s go back to our companies example, and take a look at the use of an unwind stage. We’ll start with the following aggregation pipeline. Note that in this pipeline, as in the previous section, we are simply matching on a specific funder and promoting values from embedded funding_rounds documents using a project stage:\n\ndb.companies.aggregate([ {$match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, {$project: { _id: 0, name: 1, amount: \"$funding_rounds.raised_amount\", year: \"$funding_rounds.funded_year\" }} ])\n\n174\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\nOnce again, here’s an example of the data model for documents in this collection:\n\n{ \"_id\" : \"52cdef7c4bab8bd675297d8e\", \"name\" : \"Facebook\", \"category_code\" : \"social\", \"founded_year\" : 2004, \"description\" : \"Social network\", \"funding_rounds\" : [{ \"id\" : 4, \"round_code\" : \"b\", \"raised_amount\" : 27500000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2006, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Meritech Capital Partners\", \"permalink\" : \"meritech-capital-partners\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Founders Fund\", \"permalink\" : \"founders-fund\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" }, \"person\" : null } ] }, { \"id\" : 2197, \"round_code\" : \"c\", \"raised_amount\" : 15000000,\n\n$unwind\n\n|\n\n175\n\n\"raised_currency_code\" : \"USD\", \"funded_year\" : 2008, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"European Founders Fund\", \"permalink\" : \"european-founders-fund\" }, \"person\" : null } ] }], \"ipo\" : { \"valuation_amount\" : NumberLong(\"104000000000\"), \"valuation_currency_code\" : \"USD\", \"pub_year\" : 2012, \"pub_month\" : 5, \"pub_day\" : 18, \"stock_symbol\" : \"NASDAQ:FB\" } }\n\nOur aggregation query will produce results such as the following:\n\n{ \"name\" : \"Digg\", \"amount\" : [ 8500000, 2800000, 28700000, 5000000 ], \"year\" : [ 2006, 2005, 2008, 2011 ] } { \"name\" : \"Facebook\", \"amount\" : [ 500000, 12700000, 27500000, ...\n\nThe query produces documents that have arrays for both \"amount\" and \"year\", because we’re accessing the \"raised_amount\" and \"funded_year\" for every element in the \"funding_rounds\" array.\n\n176\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\nTo fix this, we can include an unwind stage before our project stage in this aggrega‐ tion pipeline, and parameterize this by specifying that it is the \"funding_rounds\" array that should be unwound (Figure 7-5).\n\nFigure 7-5. The outline of our aggregation pipeline so far, matching for “greylock” then unwinding the “funding_rounds”, and finally projecting out the name, amount, and year for each of the funding rounds\n\nReturning again to our Facebook example, we can see that for each funding round there is a \"raised_amount\" field and a \"funded_year\" field.\n\nThe unwind stage will produce an output document for each element of the \"fund ing_rounds\" array. In this example our values are strings, but regardless of the type of value, the unwind stage will produce an output document for each one. Here’s the updated aggregation query:\n\ndb.companies.aggregate([ { $match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, { $unwind: \"$funding_rounds\" }, { $project: { _id: 0, name: 1, amount: \"$funding_rounds.raised_amount\", year: \"$funding_rounds.funded_year\" } } ])\n\nThe unwind stage produces an exact copy of every one of the documents that it receives as input. All the fields will have the same key and value, with the exception of the \"funding_rounds\" field. Rather than being an array of \"funding_rounds\" docu‐ ments, instead it will have a value that is a single document, which corresponds to an individual funding round:\n\n{\"name\": \"Digg\", \"amount\": 8500000, \"year\": 2006 } {\"name\": \"Digg\", \"amount\": 2800000, \"year\": 2005 } {\"name\": \"Digg\", \"amount\": 28700000, \"year\": 2008 } {\"name\": \"Digg\", \"amount\": 5000000, \"year\": 2011 } {\"name\": \"Facebook\", \"amount\": 500000, \"year\": 2004 } {\"name\": \"Facebook\", \"amount\": 12700000, \"year\": 2005 } {\"name\": \"Facebook\", \"amount\": 27500000, \"year\": 2006 } {\"name\": \"Facebook\", \"amount\": 240000000, \"year\": 2007 } {\"name\": \"Facebook\", \"amount\": 60000000, \"year\": 2007 }\n\n$unwind\n\n|\n\n177",
      "page_number": 183
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 191-201)",
      "start_page": 191,
      "end_page": 201,
      "detection_method": "topic_boundary",
      "content": "{\"name\": \"Facebook\", \"amount\": 15000000, \"year\": 2008 } {\"name\": \"Facebook\", \"amount\": 100000000, \"year\": 2008 } {\"name\": \"Facebook\", \"amount\": 60000000, \"year\": 2008 } {\"name\": \"Facebook\", \"amount\": 200000000, \"year\": 2009 } {\"name\": \"Facebook\", \"amount\": 210000000, \"year\": 2010 } {\"name\": \"Facebook\", \"amount\": 1500000000, \"year\": 2011 } {\"name\": \"Revision3\", \"amount\": 1000000, \"year\": 2006 } {\"name\": \"Revision3\", \"amount\": 8000000, \"year\": 2007 } ...\n\nNow let’s add an additional field to our output documents. In doing so, we’ll actually identify a small problem with this aggregation pipeline as currently written:\n\ndb.companies.aggregate([ { $match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, { $unwind: \"$funding_rounds\" }, { $project: { _id: 0, name: 1, funder: \"$funding_rounds.investments.financial_org.permalink\", amount: \"$funding_rounds.raised_amount\", year: \"$funding_rounds.funded_year\" } } ])\n\nIn adding the \"funder\" field we now have a field path value that will access the \"investments\" field of the \"funding_rounds\" embedded document that it gets from the unwind stage and, for the financial organization, selects the permalink value. Note that this is very similar to what we’re doing in our match filter. Let’s have a look at our output:\n\n{ \"name\" : \"Digg\", \"funder\" : [ \"greylock\", \"omidyar-network\" ], \"amount\" : 8500000, \"year\" : 2006 } { \"name\" : \"Digg\", \"funder\" : [ \"greylock\", \"omidyar-network\", \"floodgate\", \"sv-angel\" ], \"amount\" : 2800000, \"year\" : 2005 } {\n\n178\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\n\"name\" : \"Digg\", \"funder\" : [ \"highland-capital-partners\", \"greylock\", \"omidyar-network\", \"svb-financial-group\" ], \"amount\" : 28700000, \"year\" : 2008 } ... { \"name\" : \"Farecast\", \"funder\" : [ \"madrona-venture-group\", \"wrf-capital\" ], \"amount\" : 1500000, \"year\" : 2004 } { \"name\" : \"Farecast\", \"funder\" : [ \"greylock\", \"madrona-venture-group\", \"wrf-capital\" ], \"amount\" : 7000000, \"year\" : 2005 } { \"name\" : \"Farecast\", \"funder\" : [ \"greylock\", \"madrona-venture-group\", \"par-capital-management\", \"pinnacle-ventures\", \"sutter-hill-ventures\", \"wrf-capital\" ], \"amount\" : 12100000, \"year\" : 2007 }\n\nTo understand what we’re seeing here, we need to go back to our document and look at the \"investments\" field.\n\nThe \"funding_rounds.investments\" field is itself an array. Multiple funders can par‐ ticipate in each funding round, so \"investments\" will list every one of those funders. Looking at the results, as we originally saw with the \"raised_amount\" and \"fun\n\n$unwind\n\n|\n\n179\n\nded_year\" fields, we’re now seeing an array for \"funder\" because \"investments\" is an array-valued field.\n\nAnother problem is that because of the way we’ve written our pipeline, many docu‐ ments are passed to the project stage that represent funding rounds that Greylock did not participate in. We can see this by looking at the funding rounds for Farecast. This problem stems from the fact that our match stage selects all companies where Grey‐ lock participated in at least one funding round. If we are interested in considering only those funding rounds in which Greylock actually participated, we need to figure out a way to filter differently.\n\nOne possibility is to reverse the order of our unwind and match stages—that is to say, do the unwind first and then do the match. This guarantees that we will only match documents coming out of the unwind stage. But in thinking through this approach, it quickly becomes clear that, with unwind as the first stage, we would be doing a scan through the entire collection.\n\nFor efficiency, we want to match as early as possible in our pipeline. This enables the aggregation framework to make use of indexes, for example. So, in order to select only those funding rounds in which Greylock participated, we can include a second match stage:\n\ndb.companies.aggregate([ { $match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, { $unwind: \"$funding_rounds\" }, { $match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, { $project: { _id: 0, name: 1, individualFunder: \"$funding_rounds.investments.person.permalink\", fundingOrganization: \"$funding_rounds.investments.financial_org.permalink\", amount: \"$funding_rounds.raised_amount\", year: \"$funding_rounds.funded_year\" } } ])\n\nThis pipeline will first filter for companies where Greylock participated in at least one funding round. It will then unwind the funding rounds and filter again, so that only documents that represent funding rounds that Greylock actually participated in will be passed on to the project stage.\n\nAs mentioned at the beginning of this chapter, it is often the case that we need to include multiple stages of the same type. This is a good example: we’re filtering to reduce the number of documents that we’re looking at initially by narrowing down our set of documents for consideration to those for which Greylock participated in at least one funding round. Then, through our unwind stage, we end up with a number of documents that represent funding rounds from companies that Greylock did, in fact, fund, but individual funding rounds that Greylock did not participate in. We can\n\n180\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\nget rid of all the funding rounds we’re not interested in by simply including another filter, using a second match stage.\n\nArray Expressions Now let’s turn our attention to array expressions. As part of our deep dive, we’ll take a look at using array expressions in project stages.\n\nThe first expression we’ll examine is a filter expression. A filter expression selects a subset of the elements in an array based on filter criteria.\n\nWorking again with our companies dataset, we’ll match using the same criteria for funding rounds in which Greylock participated. Take a look at the rounds field in this pipeline:\n\ndb.companies.aggregate([ { $match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, { $project: { _id: 0, name: 1, founded_year: 1, rounds: { $filter: { input: \"$funding_rounds\", as: \"round\", cond: { $gte: [\"$$round.raised_amount\", 100000000] } } } } }, { $match: {\"rounds.investments.financial_org.permalink\": \"greylock\" } }, ]).pretty()\n\nThe rounds field uses a filter expression. The $filter operator is designed to work with array fields and specifies the options we must supply. The first option to $filter is input. For input, we simply specify an array. In this case, we use a field path speci‐ fier to identify the \"funding_rounds\" array found in documents in our companies collection. Next, we specify the name we’d like to use for this \"funding_rounds\" array throughout the rest of our filter expression. Then, as the third option, we need to specify a condition. The condition should provide criteria used to filter whatever array we’ve provided as input, selecting a subset. In this case, we’re filtering such that we only select elements where the \"raised_amount\" for a \"funding_round\" is greater than or equal to 100 million.\n\nIn specifying the condition, we’ve made use of $$. We use $$ to reference a variable defined within the expression we’re working in. The as clause defines a variable within our filter expression. This variable has the name \"round\" because that’s what we labeled it in the as clause. This is to disambiguate a reference to a variable from a field path. In this case, our comparison expression takes an array of two values and will return true if the first value provided is greater than or equal to the second value.\n\nArray Expressions\n\n|\n\n181\n\nNow let’s consider what documents the project stage of this pipeline will produce, given this filter. The output documents will have \"name\", \"founded_year\", and \"rounds\" fields. The values for \"rounds\" will be arrays composed of the elements that match our filter condition: that the raised amount is greater than $100,000,000.\n\nIn the match stage that follows, as we did previously, we will simply filter the input documents for those that were funded in some way by Greylock. Documents output by this pipeline will resemble the following:\n\n{ \"name\" : \"Dropbox\", \"founded_year\" : 2007, \"rounds\" : [ { \"id\" : 25090, \"round_code\" : \"b\", \"source_description\" : \"Dropbox Raises $250M In Funding, Boasts 45 Million Users\", \"raised_amount\" : 250000000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2011, \"investments\" : [ { \"financial_org\" : { \"name\" : \"Index Ventures\", \"permalink\" : \"index-ventures\" } }, { \"financial_org\" : { \"name\" : \"RIT Capital Partners\", \"permalink\" : \"rit-capital-partners\" } }, { \"financial_org\" : { \"name\" : \"Valiant Capital Partners\", \"permalink\" : \"valiant-capital-partners\" } }, { \"financial_org\" : { \"name\" : \"Benchmark\", \"permalink\" : \"benchmark-2\" } }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Goldman Sachs\", \"permalink\" : \"goldman-sachs\"\n\n182\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\n}, \"person\" : null }, { \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" } }, { \"financial_org\" : { \"name\" : \"Institutional Venture Partners\", \"permalink\" : \"institutional-venture-partners\" } }, { \"financial_org\" : { \"name\" : \"Sequoia Capital\", \"permalink\" : \"sequoia-capital\" } }, { \"financial_org\" : { \"name\" : \"Accel Partners\", \"permalink\" : \"accel-partners\" } }, { \"financial_org\" : { \"name\" : \"Glynn Capital Management\", \"permalink\" : \"glynn-capital-management\" } }, { \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" } } ] } ] }\n\nOnly the \"rounds\" array items for which the raised amount exceeds $100,000,000 will pass through the filter. In the case of Dropbox, there is just one round that meets that criterion. You have a lot of flexibility in how you set up filter expressions, but this is the basic form and provides a concrete example of a use case for this particular array expression.\n\nArray Expressions\n\n|\n\n183\n\nNext, let’s look at the array element operator. We’ll continue working with funding rounds, but in this case we simply want to pull out the first round and the last round. We might be interested, for example, in seeing when these rounds occurred or in comparing their amounts. These are things we can do with date and arithmetic expressions, as we’ll see in the next section.\n\nThe $arrayElemAt operator enables us to select an element at a particular slot within an array. The following pipeline provides an example of using $arrayElemAt:\n\ndb.companies.aggregate([ { $match: { \"founded_year\": 2010 } }, { $project: { _id: 0, name: 1, founded_year: 1, first_round: { $arrayElemAt: [ \"$funding_rounds\", 0 ] }, last_round: { $arrayElemAt: [ \"$funding_rounds\", -1 ] } } } ]).pretty()\n\nNote the syntax for using $arrayElemAt within a project stage. We define a field that we want projected out and as the value specify a document with $arrayElemAt as the field name and a two-element array as the value. The first element should be a field path that specifies the array field we want to select from. The second element identi‐ fies the slot within that array that we want. Remember that arrays are 0-indexed.\n\nIn many cases, the length of an array is not readily available. To select array slots starting from the end of the array, use negative integers. The last element in an array is identified with -1.\n\nA simple output document for this aggregation pipeline would resemble the following:\n\n{ \"name\" : \"vufind\", \"founded_year\" : 2010, \"first_round\" : { \"id\" : 19876, \"round_code\" : \"angel\", \"source_url\" : \"\", \"source_description\" : \"\", \"raised_amount\" : 250000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2010, \"funded_month\" : 9, \"funded_day\" : 1, \"investments\" : [ ] }, \"last_round\" : { \"id\" : 57219, \"round_code\" : \"seed\",\n\n184\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\n\"source_url\" : \"\", \"source_description\" : \"\", \"raised_amount\" : 500000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2012, \"funded_month\" : 7, \"funded_day\" : 1, \"investments\" : [ ] } }\n\nRelated to $arrayElemAt is the $slice expression. This allows us to return not just one but multiple items from an array in sequence, beginning with a particular index:\n\ndb.companies.aggregate([ { $match: { \"founded_year\": 2010 } }, { $project: { _id: 0, name: 1, founded_year: 1, early_rounds: { $slice: [ \"$funding_rounds\", 1, 3 ] } } } ]).pretty()\n\nHere, again with the funding_rounds array, we begin at index 1 and take three ele‐ ments from the array. Perhaps we know that in this dataset the first funding round isn’t all that interesting, or we simply want some early ones but not the very first one.\n\nFiltering and selecting individual elements or slices of arrays are among the more common operations we need to perform on arrays. Probably the most common, however, is determining an array’s size or length. To do this we can use the $size operator:\n\ndb.companies.aggregate([ { $match: { \"founded_year\": 2004 } }, { $project: { _id: 0, name: 1, founded_year: 1, total_rounds: { $size: \"$funding_rounds\" } } } ]).pretty()\n\nWhen used in a project stage, a $size expression will simply provide a value that is the number of elements in the array.\n\nIn this section, we’ve explored some of the most common array expressions. There are many more, and the list grows with each release. Please review the Aggregation Pipeline Quick Reference in the MongoDB documentation for a summary of all expressions that are available.\n\nArray Expressions\n\n|\n\n185\n\nAccumulators At this point, we’ve covered a few different types of expressions. Next, let’s look at what accumulators the aggregation framework has to offer. Accumulators are essen‐ tially another type of expression, but we think about them in their own class because they calculate values from field values found in multiple documents.\n\nAccumulators the aggregation framework provides enable us to perform operations such as summing all values in a particular field ($sum), calculating an average ($avg), etc. We also consider $first and $last to be accumulators because these consider values in all documents that pass through the stage in which they are used. $max and $min are two more examples of accumulators that consider a stream of documents and save just one of the values they see. We can use $mergeObjects to combine mul‐ tiple documents into a single document.\n\nWe also have accumulators for arrays. We can $push values onto an array as docu‐ ments pass through a pipeline stage. $addToSet is very similar to $push except that it ensures no duplicate values are included in the resulting array.\n\nThen there are some expressions for calculating descriptive statistics—for example, for calculating the standard deviation of a sample and of a population. Both work with a stream of documents that pass through a pipeline stage.\n\nPrior to MongoDB 3.2, accumulators were available only in the group stage. Mon‐ goDB 3.2 introduced the ability to access a subset of accumulators within the project stage. The primary difference between the accumulators in the group stage and the project stage is that in the project stage accumulators such as $sum and $avg must operate on arrays within a single document, whereas accumulators in the group stage, as we’ll see in a later section, provide you with the ability to perform calculations on values across multiple documents.\n\nThat’s a quick overview of accumulators to provide some context and set the stage for our deep dive into examples.\n\nUsing Accumulators in Project Stages We’ll begin with an example of using an accumulator in a project stage. Note that our match stage filters for documents that contain a \"funding_rounds\" field and for which the funding_rounds array is not empty:\n\ndb.companies.aggregate([ { $match: { \"funding_rounds\": { $exists: true, $ne: [ ]} } }, { $project: { _id: 0, name: 1, largest_round: { $max: \"$funding_rounds.raised_amount\" }\n\n186\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\n} } ])\n\nBecause the value for $funding_rounds is an array within each company document, we can use an accumulator. Remember that in project stages accumulators must work on an array-valued field. In this case, we’re able to do something pretty cool here. We are easily identifying the largest value in an array by reaching into an embedded document within that array and projecting the max value in the output documents:\n\n{ \"name\" : \"Wetpaint\", \"largest_round\" : 25000000 } { \"name\" : \"Digg\", \"largest_round\" : 28700000 } { \"name\" : \"Facebook\", \"largest_round\" : 1500000000 } { \"name\" : \"Omnidrive\", \"largest_round\" : 800000 } { \"name\" : \"Geni\", \"largest_round\" : 10000000 } { \"name\" : \"Twitter\", \"largest_round\" : 400000000 } { \"name\" : \"StumbleUpon\", \"largest_round\" : 17000000 } { \"name\" : \"Gizmoz\", \"largest_round\" : 6500000 } { \"name\" : \"Scribd\", \"largest_round\" : 13000000 } { \"name\" : \"Slacker\", \"largest_round\" : 40000000 } { \"name\" : \"Lala\", \"largest_round\" : 20000000 } { \"name\" : \"eBay\", \"largest_round\" : 6700000 } { \"name\" : \"MeetMoi\", \"largest_round\" : 2575000 } { \"name\" : \"Joost\", \"largest_round\" : 45000000 } { \"name\" : \"Babelgum\", \"largest_round\" : 13200000 } { \"name\" : \"Plaxo\", \"largest_round\" : 9000000 } { \"name\" : \"Cisco\", \"largest_round\" : 2500000 } { \"name\" : \"Yahoo!\", \"largest_round\" : 4800000 } { \"name\" : \"Powerset\", \"largest_round\" : 12500000 } { \"name\" : \"Technorati\", \"largest_round\" : 10520000 } ...\n\nAs another example, let’s use the $sum accumulator to calculate the total funding for each company in our collection:\n\ndb.companies.aggregate([ { $match: { \"funding_rounds\": { $exists: true, $ne: [ ]} } }, { $project: { _id: 0, name: 1, total_funding: { $sum: \"$funding_rounds.raised_amount\" } } } ])\n\nThis is just a taste of what you can do using accumulators in project stages. Again, you’re encouraged to review the Aggregation Pipeline Quick Reference in the Mon‐ goDB docs for a complete overview of the accumulator expressions available.\n\nIntroduction to Grouping Historically, accumulators were the province of the group stage in the MongoDB aggregation framework. The group stage performs a function that is similar to the\n\nIntroduction to Grouping\n\n|\n\n187\n\nSQL GROUP BY command. In a group stage, we can aggregate together values from multiple documents and perform some type of aggregation operation on them, such as calculating an average. Let’s take a look at an example:\n\ndb.companies.aggregate([ { $group: { _id: { founded_year: \"$founded_year\" }, average_number_of_employees: { $avg: \"$number_of_employees\" } } }, { $sort: { average_number_of_employees: -1 } }\n\n])\n\nHere, we’re using a group stage to aggregate together all companies based on the year they were founded, then calculate the average number of employees for each year. The output for this pipeline resembles the following:\n\n{ \"_id\" : { \"founded_year\" : 1847 }, \"average_number_of_employees\" : 405000 } { \"_id\" : { \"founded_year\" : 1896 }, \"average_number_of_employees\" : 388000 } { \"_id\" : { \"founded_year\" : 1933 }, \"average_number_of_employees\" : 320000 } { \"_id\" : { \"founded_year\" : 1915 }, \"average_number_of_employees\" : 186000 } { \"_id\" : { \"founded_year\" : 1903 }, \"average_number_of_employees\" : 171000 } { \"_id\" : { \"founded_year\" : 1865 }, \"average_number_of_employees\" : 125000 } { \"_id\" : { \"founded_year\" : 1921 }, \"average_number_of_employees\" : 107000 } { \"_id\" : { \"founded_year\" : 1835 }, \"average_number_of_employees\" : 100000 } { \"_id\" : { \"founded_year\" : 1952 }, \"average_number_of_employees\" : 92900 } { \"_id\" : { \"founded_year\" : 1946 }, \"average_number_of_employees\" : 91500 } { \"_id\" : { \"founded_year\" : 1947 }, \"average_number_of_employees\" : 88510.5 } { \"_id\" : { \"founded_year\" : 1898 }, \"average_number_of_employees\" : 80000 } { \"_id\" : { \"founded_year\" : 1968 }, \"average_number_of_employees\" : 73550 } { \"_id\" : { \"founded_year\" : 1957 }, \"average_number_of_employees\" : 70055 } { \"_id\" : { \"founded_year\" : 1969 }, \"average_number_of_employees\" : 67635.1 } { \"_id\" : { \"founded_year\" : 1928 }, \"average_number_of_employees\" : 51000 } { \"_id\" : { \"founded_year\" : 1963 }, \"average_number_of_employees\" : 50503 } { \"_id\" : { \"founded_year\" : 1959 }, \"average_number_of_employees\" : 47432.5 } { \"_id\" : { \"founded_year\" : 1902 }, \"average_number_of_employees\" : 41171.5 } { \"_id\" : { \"founded_year\" : 1887 }, \"average_number_of_employees\" : 35000 } ...\n\nThe output includes documents that have a document as their \"_id\" value, and then a report on the average number of employees. This is the type of analysis we might do as a first step in assessing the correlation between the year in which a company was founded and its growth, possibly normalizing for how old the company is.\n\nAs you can see, the pipeline we built has two stages: a group stage and a sort stage. Fundamental to the group stage is the \"_id\" field that we specify as part of the docu‐ ment. This is the value of the $group operator itself, using a very strict interpretation.\n\nWe use this field to define what the group stage uses to organize the documents that it sees. Since the group stage is first, the aggregate command will pass all documents in the companies collection through this stage. The group stage will take every docu‐\n\n188\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "page_number": 191
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 202-209)",
      "start_page": 202,
      "end_page": 209,
      "detection_method": "topic_boundary",
      "content": "ment that has the same value for \"founded_year\" and treat them as a single group. In constructing the value for this field, this stage will use the $avg accumulator to calcu‐ late an average number of employees for all companies with the same \"founded_year\".\n\nYou can think of it this way. Each time the group stage encounters a document with a specific founding year, it adds the value for \"number_of_employees\" from that docu‐ ment to a running sum of the number of employees and adds one to a count of the number of documents seen so far for that year. Once all documents have passed through the group stage, it can then calculate the average using that running sum and count for every grouping of documents it identified based on the year of founding.\n\nAt the end of this pipeline, we sort the documents into descending order by average_number_of_employees.\n\nLet’s look at another example. One field we’ve not yet considered in the companies dataset is the relationships. The relationships field appears in documents in the fol‐ lowing form:\n\n{ \"_id\" : \"52cdef7c4bab8bd675297d8e\", \"name\" : \"Facebook\", \"permalink\" : \"facebook\", \"category_code\" : \"social\", \"founded_year\" : 2004, ... \"relationships\" : [ { \"is_past\" : false, \"title\" : \"Founder and CEO, Board Of Directors\", \"person\" : { \"first_name\" : \"Mark\", \"last_name\" : \"Zuckerberg\", \"permalink\" : \"mark-zuckerberg\" } }, { \"is_past\" : true, \"title\" : \"CFO\", \"person\" : { \"first_name\" : \"David\", \"last_name\" : \"Ebersman\", \"permalink\" : \"david-ebersman\" } }, ... ], \"funding_rounds\" : [ ... {\n\nIntroduction to Grouping\n\n|\n\n189\n\n190\n\n\"id\" : 4, \"round_code\" : \"b\", \"source_url\" : \"http://www.facebook.com/press/info.php?factsheet\", \"source_description\" : \"Facebook Funding\", \"raised_amount\" : 27500000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2006, \"funded_month\" : 4, \"funded_day\" : 1, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Meritech Capital Partners\", \"permalink\" : \"meritech-capital-partners\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Founders Fund\", \"permalink\" : \"founders-fund\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" }, \"person\" : null } ] }, ... \"ipo\" : { \"valuation_amount\" : NumberLong(\"104000000000\"), \"valuation_currency_code\" : \"USD\", \"pub_year\" : 2012, \"pub_month\" : 5, \"pub_day\" : 18, \"stock_symbol\" : \"NASDAQ:FB\"\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\n}, ... }\n\nThe \"relationships\" field gives us the ability to dive in and look for people who have, in one way or another, been associated with a relatively large number of compa‐ nies. Let’s take a look at this aggregation:\n\ndb.companies.aggregate( [ { $match: { \"relationships.person\": { $ne: null } } }, { $project: { relationships: 1, _id: 0 } }, { $unwind: \"$relationships\" }, { $group: { _id: \"$relationships.person\", count: { $sum: 1 } } }, { $sort: { count: -1 } } ]).pretty()\n\nWe’re matching on relationships.person. If we look at our Facebook example document, we can see how relationships are structured and get a sense for what it means to do this. We are filtering for all relationships for which \"person\" is not null. Then we project out all relationships for documents that match. We will pass only relationships to the next stage in the pipeline, which is unwind. We unwind the rela‐ tionships so that every relationship in the array comes through to the group stage that follows. In the group stage, we use a field path to identify the person within each \"relationship\" document. All documents with the same \"person\" value will be grouped together. As we saw previously, it’s perfectly fine for a document to be the value around which we group. So, every match to a document for a first name, last name, and permalink for a person will be aggregated together. We use the $sum accu‐ mulator to count the number of relationships in which each person has participated. Finally, we sort into descending order. The output for this pipeline resembles the following:\n\n{ \"_id\" : { \"first_name\" : \"Tim\", \"last_name\" : \"Hanlon\", \"permalink\" : \"tim-hanlon\" }, \"count\" : 28 } { \"_id\" : { \"first_name\" : \"Pejman\", \"last_name\" : \"Nozad\", \"permalink\" : \"pejman-nozad\" }, \"count\" : 24 }\n\nIntroduction to Grouping\n\n|\n\n191\n\n{ \"_id\" : { \"first_name\" : \"David S.\", \"last_name\" : \"Rose\", \"permalink\" : \"david-s-rose\" }, \"count\" : 24 } { \"_id\" : { \"first_name\" : \"Saul\", \"last_name\" : \"Klein\", \"permalink\" : \"saul-klein\" }, \"count\" : 24 } ...\n\nTim Hanlon is the individual who has participated in the most relationships with companies in this collection. It could be that Mr. Hanlon has actually had a relation‐ ship with 28 companies, but we can’t know that for sure, because it’s also possible that he has had multiple relationships with one or more companies, each with a different title. This example illustrates a very important point about aggregation pipelines: make sure you fully understand what it is you’re working with as you do calculations, particularly when you’re calculating aggregate values using accumulator expressions of some kind.\n\nIn this case, we can say that Tim Hanlon appears 28 times in \"relationships\" docu‐ ments throughout the companies in our collection. We would have to dig a little deeper to see exactly how many unique companies he was associated with, but we’ll leave the construction of that pipeline to you as an exercise.\n\nThe _id Field in Group Stages Before we go any further with our discussion of the group stage, let’s talk a little more about the _id field and look at some best practices for constructing values for this field in group aggregation stages. We’ll walk through a few examples that illustrate several different ways in which we commonly group documents. As our first example, consider this pipeline:\n\ndb.companies.aggregate([ { $match: { founded_year: { $gte: 2013 } } }, { $group: { _id: { founded_year: \"$founded_year\"}, companies: { $push: \"$name\" } } }, { $sort: { \"_id.founded_year\": 1 } } ]).pretty()\n\n192\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\nThe output for this pipeline resembles the following:\n\n{ \"_id\" : { \"founded_year\" : 2013 }, \"companies\" : [ \"Fixya\", \"Wamba\", \"Advaliant\", \"Fluc\", \"iBazar\", \"Gimigo\", \"SEOGroup\", \"Clowdy\", \"WhosCall\", \"Pikk\", \"Tongxue\", \"Shopseen\", \"VistaGen Therapeutics\" ] } ...\n\nIn our output we have documents with two fields: \"_id\" and \"companies\". Each of these documents contains a list of the companies founded in whatever the \"founded_year\" is, \"companies\" being an array of company names.\n\nNotice here how we’ve constructed the \"_id\" field in the group stage. Why not just provide the founding year rather than putting it inside a document with a field labeled \"founded_year\". The reason we don’t do it that way is that if we don’t label the group value, it’s not explicit that we are grouping on the year in which the com‐ pany was founded. In order to avoid confusion, it is a best practice to explicitly label values on which we group.\n\nIn some circumstances it might be necessary to use another approach in which our _id value is a document composed of multiple fields. In this case, we’re actually grouping documents on the basis of their founding year and category code:\n\ndb.companies.aggregate([ { $match: { founded_year: { $gte: 2010 } } }, { $group: { _id: { founded_year: \"$founded_year\", category_code: \"$category_code\" }, companies: { $push: \"$name\" } } }, { $sort: { \"_id.founded_year\": 1 } } ]).pretty()\n\nIt is perfectly fine to use documents with multiple fields as our _id value in group stages. In other cases, it might also be necessary to do something like this:\n\nIntroduction to Grouping\n\n|\n\n193\n\ndb.companies.aggregate([ { $group: { _id: { ipo_year: \"$ipo.pub_year\" }, companies: { $push: \"$name\" } } }, { $sort: { \"_id.ipo_year\": 1 } } ]).pretty()\n\nIn this case, we’re grouping documents based on the year in which the companies had their IPO, and that year is actually a field of an embedded document. It is common practice to use field paths that reach into embedded documents as the value on which to group in a group stage. In this case, the output will resemble the following:\n\n{ \"_id\" : { \"ipo_year\" : 1999 }, \"companies\" : [ \"Akamai Technologies\", \"TiVo\", \"XO Group\", \"Nvidia\", \"Blackberry\", \"Blue Coat Systems\", \"Red Hat\", \"Brocade Communications Systems\", \"Juniper Networks\", \"F5 Networks\", \"Informatica\", \"Iron Mountain\", \"Perficient\", \"Sitestar\", \"Oxford Instruments\" ] }\n\nNote that the examples in this section use an accumulator we haven’t seen before: $push. As the group stage processes documents in its input stream, a $push expres‐ sion will add the resulting value to an array that it builds throughout its run. In the case of the preceding pipeline, the group stage is building an array composed of com‐ pany names.\n\nOur final example is one we’ve already seen, but it’s included here for the sake of completeness:\n\ndb.companies.aggregate( [ { $match: { \"relationships.person\": { $ne: null } } }, { $project: { relationships: 1, _id: 0 } }, { $unwind: \"$relationships\" }, { $group: { _id: \"$relationships.person\", count: { $sum: 1 }\n\n194\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\n} }, { $sort: { count: -1 } } ] )\n\nIn the preceding example where we were grouping on IPO year, we used a field path that resolved to a scalar value—the IPO year. In this case, our field path resolves to a document containing three fields: \"first_name“, \"last_name\", and \"permalink\". This demonstrates that the group stage supports grouping on document values.\n\nYou’ve now seen several ways in which we can construct _id values in group stages. In general, bear in mind that what we want to do here is make sure that in our output, the semantics of our _id value are clear.\n\nGroup Versus Project To round out our discussion of the group aggregation stage, we’ll take a look at a cou‐ ple of additional accumulators that are not available in the project stage. This is to encourage you to think a little more deeply about what we can do in a project stage with respect to accumulators, and what we can do in group. As an example, consider this aggregation query:\n\ndb.companies.aggregate([ { $match: { funding_rounds: { $ne: [ ] } } }, { $unwind: \"$funding_rounds\" }, { $sort: { \"funding_rounds.funded_year\": 1, \"funding_rounds.funded_month\": 1, \"funding_rounds.funded_day\": 1 } }, { $group: { _id: { company: \"$name\" }, funding: { $push: { amount: \"$funding_rounds.raised_amount\", year: \"$funding_rounds.funded_year\" } } } }, ] ).pretty()\n\nHere, we begin by filtering for documents for which the array funding_rounds is not empty. Then we unwind funding_rounds. Therefore, the sort and group stages will see one document for each element of the funding_rounds array for every company.\n\nOur sort stage in this pipeline sorts on first year, then month, then day, all in ascend‐ ing order. This means that this stage will output the oldest funding rounds first. And as you are aware from Chapter 5, we can support this type of sort with a compound index.\n\nIn the group stage that follows the sort, we group by company name and use the $push accumulator to construct a sorted array of funding rounds. The fund\n\nIntroduction to Grouping\n\n|\n\n195\n\ning_rounds array will be sorted for each company because we sorted all funding rounds, globally, in the sort stage.\n\nDocuments output from this pipeline will resemble the following:\n\n{ \"_id\" : { \"company\" : \"Green Apple Media\" }, \"funding\" : [ { \"amount\" : 30000000, \"year\" : 2013 }, { \"amount\" : 100000000, \"year\" : 2013 }, { \"amount\" : 2000000, \"year\" : 2013 } ] }\n\nIn this pipeline, with $push, we are accumulating an array. In this case, we have speci‐ fied our $push expression so that it adds documents to the end of the accumulation array. Since the funding rounds are in chronological order, pushing onto the end of the array guarantees that the the funding amounts for each company are sorted in chronological order.\n\n$push expressions only work in group stages. This is because group stages are designed to take an input stream of documents and accumulate values by processing each document in turn. Project stages, on the other hand, work with each document in their input stream individually.\n\nLet’s take a look at one other example. This is a little longer, but it builds on the previ‐ ous one:\n\ndb.companies.aggregate([ { $match: { funding_rounds: { $exists: true, $ne: [ ] } } }, { $unwind: \"$funding_rounds\" }, { $sort: { \"funding_rounds.funded_year\": 1, \"funding_rounds.funded_month\": 1, \"funding_rounds.funded_day\": 1 } }, { $group: { _id: { company: \"$name\" }, first_round: { $first: \"$funding_rounds\" }, last_round: { $last: \"$funding_rounds\" }, num_rounds: { $sum: 1 }, total_raised: { $sum: \"$funding_rounds.raised_amount\" }\n\n196\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "page_number": 202
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 210-220)",
      "start_page": 210,
      "end_page": 220,
      "detection_method": "topic_boundary",
      "content": "} }, { $project: { _id: 0, company: \"$_id.company\", first_round: { amount: \"$first_round.raised_amount\", article: \"$first_round.source_url\", year: \"$first_round.funded_year\" }, last_round: { amount: \"$last_round.raised_amount\", article: \"$last_round.source_url\", year: \"$last_round.funded_year\" }, num_rounds: 1, total_raised: 1, } }, { $sort: { total_raised: -1 } } ] ).pretty()\n\nAgain, we are unwinding funding_rounds and sorting chronologically. However, in this case, instead of accumulating an array of entries, each entry representing a single funding_rounds, we are using two accumulators we’ve not yet seen in action: $first and $last. A $first expression simply saves the first value that passes through the input stream for the stage. A $last expression simply tracks the values that pass through the group stage and hangs onto the last one.\n\nAs with $push, we can’t use $first and $last in project stages because, again, project stages are not designed to accumulate values based on multiple documents streaming through them. Rather, they are designed to reshape documents individually.\n\nIn addition to $first and $last, we also use $sum in this example to calculate the total number of funding rounds. For this expression we can just specify the value, 1. A $sum expression like this simply serves to count the number of documents that it sees in each grouping.\n\nFinally, this pipeline includes a fairly complex project stage. However, all it is really doing is making the output prettier. Rather than show the first_round values, or entire documents for the first and last funding rounds, this project stage creates a summary. Note that this maintains good semantics, because each value is clearly labeled. For first_round we’ll produce a simple embedded document that contains just the essential details of amount, article, and year, pulling those values from the original funding round document that will be the value of $first_round. The project stage does something similar for $last_round. Finally, this project stage just passes through to output documents the num_rounds and total_raised values for docu‐ ments it receives in its input stream.\n\nDocuments output from this pipeline resemble the following:\n\nIntroduction to Grouping\n\n|\n\n197\n\n{ \"first_round\" : { \"amount\" : 7500000, \"article\" : \"http://www.teslamotors.com/display_data/pressguild.swf\", \"year\" : 2004 }, \"last_round\" : { \"amount\" : 10000000, \"article\" : \"http://www.bizjournals.com/sanfrancisco/news/2012/10/10/ tesla-motors-to-get-10-million-from.html\", \"year\" : 2012 }, \"num_rounds\" : 11, \"total_raised\" : 823000000, \"company\" : \"Tesla Motors\" }\n\nAnd with that, we’ve concluded an overview of the group stage.\n\nWriting Aggregation Pipeline Results to a Collection There are two specific stages, $out and $merge, that can write documents resulting from the aggregation pipeline to a collection. You can use only one of these two stages, and it must be the last stage of an aggregation pipeline. $merge was introduced in MongoDB version 4.2 and is the preferred stage for writing to a collection, if avail‐ able. $out has some limitations: it can only write to the same database, it overwrites any existing collection if present, and it cannot write to a sharded collection. $merge can write to any database and collection, sharded or not. $merge can also incorporate results (insert new documents, merge with existing documents, fail the operation, keep existing documents, or process all documents with a custom update) when working with an existing collection. But the real advantage of using $merge is that it can create on-demand materialized views, where the content of the output collection is incrementally updated when the pipeline is run.\n\nIn this chapter, we have covered a number of different accumulators, some that are available in the project stage, and we’ve also covered how to think about when to use group versus project when considering various accumulators. Next, we’ll take a look at transactions in MongoDB.\n\n198\n\n|\n\nChapter 7: Introduction to the Aggregation Framework\n\nCHAPTER 8 Transactions\n\nTransactions are logical groups of processing in a database, and each group or trans‐ action can contain one or more operations such as reads and/or writes across multi‐ ple documents. MongoDB supports ACID-compliant transactions across multiple operations, collections, databases, documents, and shards. In this chapter, we intro‐ duce transactions, define what ACID means for a database, highlight how you use these in your applications, and provide tips for tuning transactions in MongoDB. We will cover:\n\nWhat a transaction is\n\nHow to use transactions\n\nTuning transaction limits for your application\n\nIntroduction to Transactions As we mentioned above, a transaction is a logical unit of processing in a database that includes one or more database operations, which can be read or write operations. There are situations where your application may require reads and writes to multiple documents (in one or more collections) as part of this logical unit of processing. An important aspect of a transaction is that it is never partially completed—it either suc‐ ceeds or fails.\n\nIn order to use transactions, your MongoDB deployment must be on MongoDB version 4.2 or later and your MongoDB drivers must be updated for MongoDB 4.2 or later. MongoDB provides a Driver Compatibility Reference page that you can use to ensure your MongoDB Driver version is compatible.\n\n199\n\nA Definition of ACID ACID is the accepted set of properties a transaction must meet to be a “true” transac‐ tion. ACID is an acronym for Atomicity, Consistency, Isolation, and Durability. ACID transactions guarantee the validity of your data and of your database’s state even where power failures or other errors occur.\n\nAtomicity ensures that all operations inside a transaction will either be applied or nothing will be applied. A transaction can never be partially applied; either it is com‐ mitted or it aborts.\n\nConsistency ensures that if a transaction succeeds, the database will move from one consistent state to the next consistent state.\n\nIsolation is the property that permits multiple transactions to run at the same time in your database. It guarantees that a transaction will not view the partial results of any other transaction, which means multiple parallel transactions will have the same results as running each of the transactions sequentially.\n\nDurability ensures that when a transaction is committed all data will persist even in the case of a system failure.\n\nA database is said to be ACID-compliant when it ensures that all these properties are met and that only successful transactions can be processed. In situations where a fail‐ ure occurs before a transaction is completed, ACID compliance ensures that no data will be changed.\n\nMongoDB is a distributed database with ACID compliant transactions across replica sets and/or across shards. The network layer adds an additional level of complexity. The engineering team at MongoDB provided several chalk and talk videos that describe how they implemented the necessary features to support ACID transactions.\n\nHow to Use Transactions MongoDB provides two APIs to use transactions. The first is a similar syntax to rela‐ tional databases (e.g., start_transaction and commit_transaction) called the core API and the second is called the callback API, which is the recommended approach to using transactions.\n\nThe core API does not provide retry logic for the majority of errors and requires the developer to code the logic for the operations, the transaction commit function, and any retry and error logic required.\n\n200\n\n|\n\nChapter 8: Transactions\n\nThe callback API provides a single function that wraps a large degree of functionality when compared to the core API, including starting a transaction associated with a specified logical session, executing a function supplied as the callback function, and then committing the transaction (or aborting on error). This function also includes retry logic that handle commit errors. The callback API was added in MongoDB 4.2 to simplify application development with transactions as well as make it easier to add application retry logic to handle any transaction errors.\n\nIn both APIs, the developer is responsible for starting the logical session that will be used by the transaction. Both APIs require operations in a transaction to be associ‐ ated with a specific logical session (i.e., pass in the session to each operation). A logi‐ cal session in MongoDB tracks the time and sequencing of the operations in the context of the entire MongoDB deployment. A logical session or server session is part of the underlying framework used by client sessions to support retryable writes and causal consistency in MongoDB—both of these features were added in MongoDB version 3.6 as part of the foundation required to support transactions. A specific sequence of read and write operations that have a causal relationship reflected by their ordering is defined as a causally consistent client session in MongoDB. A client session is started by an application and used to interact with a server session.\n\nIn 2019, six senior engineers from MongoDB published a paper at the SIGMOD 2019 conference entitled “Implementation of Cluster-wide Logical Clock and Causal Con‐ sistency in MongoDB”.1 This paper provides a deeper technical explanation of the mechanics behind logical sessions and causal consistency in MongoDB. The paper documents the efforts from a multiteam, multiyear engineering project. The work involved changing aspects of the storage layer, adding a new replication consensus protocol, modifying the sharding architecture, refactoring sharding cluster metadata, and adding a global logical clock. These changes provide the foundation required by the database before ACID-compliant transactions can be added.\n\nThe complexity and additional coding required in applications are the main reasons to recommend the callback API over the core API. These differences between the APIs are summarized in Table 8-1.\n\n1 The authors are Misha Tyulenev, staff software engineer for sharding; Andy Schwerin, vice president for Dis‐ tributed Systems; Asya Kamsky, principal product manager for Distributed Systems; Randolph Tan, senior software engineer for sharding; Alyson Cabral, product manager for Distributed Systems; and Jack Mulrow, software engineer for sharding.\n\nHow to Use Transactions\n\n|\n\n201\n\nTable 8-1. Comparison of Core API versus Callback API\n\nCore API Requires explicit call to start the transaction and commit the transaction. Does not incorporate error-handling logic for TransientTransactionError and UnknownTransactionCommitResult, and instead provides the flexibility to incorporate custom error handling for these errors. Requires explicit logical session to be passed to API for the specific transaction.\n\nCallback API Starts a transaction, executes the specified operations, and commits (or aborts on error). Automatically incorporates error-handling logic for TransientTransactionError and UnknownTransactionCommitResult.\n\nRequires explicit logical session to be passed to API for the specific transaction.\n\nTo understand the differences between these two APIs, we can compare the APIs using a simple transaction example for an ecommerce site where an order is placed and the corresponding items are removed from the available stock as they are sold. This involves two documents in different collections in a single transaction. The two operations, which will be the core of our transaction example, are:\n\norders.insert_one({\"sku\": \"abc123\", \"qty\": 100}, session=session) inventory.update_one({\"sku\": \"abc123\", \"qty\": {\"$gte\": 100}}, {\"$inc\": {\"qty\": -100}}, session=session)\n\nFirst, let’s see how the core API can be used in Python for our transaction example. The two operations of our transaction are highlighted in Step 1 of the program listing below:\n\n# Define the uriString using the DNS Seedlist Connection Format # for the connection uri = 'mongodb+srv://server.example.com/' client = MongoClient(uriString)\n\nmy_wc_majority = WriteConcern('majority', wtimeout=1000)\n\n# Prerequisite / Step 0: Create collections, if they don't already exist. # CRUD operations in transactions must be on existing collections.\n\nclient.get_database( \"webshop\", write_concern=my_wc_majority).orders.insert_one({\"sku\": \"abc123\", \"qty\":0}) client.get_database( \"webshop\", write_concern=my_wc_majority).inventory.insert_one( {\"sku\": \"abc123\", \"qty\": 1000})\n\n# Step 1: Define the operations and their sequence within the transaction def update_orders_and_inventory(my_session): orders = session.client.webshop.orders inventory = session.client.webshop.inventory\n\n202\n\n|\n\nChapter 8: Transactions\n\nwith session.start_transaction( read_concern=ReadConcern(\"snapshot\"), write_concern=WriteConcern(w=\"majority\"), read_preference=ReadPreference.PRIMARY):\n\norders.insert_one({\"sku\": \"abc123\", \"qty\": 100}, session=my_session) inventory.update_one({\"sku\": \"abc123\", \"qty\": {\"$gte\": 100}}, {\"$inc\": {\"qty\": -100}}, session=my_session) commit_with_retry(my_session)\n\n# Step 2: Attempt to run and commit transaction with retry logic def commit_with_retry(session): while True: try: # Commit uses write concern set at transaction start. session.commit_transaction() print(\"Transaction committed.\") break except (ConnectionFailure, OperationFailure) as exc: # Can retry commit if exc.has_error_label(\"UnknownTransactionCommitResult\"): print(\"UnknownTransactionCommitResult, retrying \" \"commit operation ...\") continue else: print(\"Error during commit ...\") raise\n\n# Step 3: Attempt with retry logic to run the transaction function txn_func def run_transaction_with_retry(txn_func, session): while True: try: txn_func(session) # performs transaction break except (ConnectionFailure, OperationFailure) as exc: # If transient error, retry the whole transaction if exc.has_error_label(\"TransientTransactionError\"): print(\"TransientTransactionError, retrying transaction ...\") continue else: raise\n\n# Step 4: Start a session. with client.start_session() as my_session:\n\n# Step 5: Call the function 'run_transaction_with_retry' passing it the function # to call 'update_orders_and_inventory' and the session 'my_session' to associate # with this transaction.\n\ntry: run_transaction_with_retry(update_orders_and_inventory, my_session) except Exception as exc:\n\nHow to Use Transactions\n\n|\n\n203\n\n# Do something with error. The error handling code is not # implemented for you with the Core API. raise\n\nNow, let’s look at how the the callback API can be used in Python for this same trans‐ action example. The two operations of our transaction are highlighted in Step 1 of the program listing below:\n\n# Define the uriString using the DNS Seedlist Connection Format # for the connection uriString = 'mongodb+srv://server.example.com/' client = MongoClient(uriString)\n\nmy_wc_majority = WriteConcern('majority', wtimeout=1000)\n\n# Prerequisite / Step 0: Create collections, if they don't already exist. # CRUD operations in transactions must be on existing collections.\n\nclient.get_database( \"webshop\", write_concern=my_wc_majority).orders.insert_one({\"sku\": \"abc123\", \"qty\":0}) client.get_database( \"webshop\", write_concern=my_wc_majority).inventory.insert_one( {\"sku\": \"abc123\", \"qty\": 1000})\n\n# Step 1: Define the callback that specifies the sequence of operations to # perform inside the transactions.\n\ndef callback(my_session): orders = my_session.client.webshop.orders inventory = my_session.client.webshop.inventory\n\n# Important:: You must pass the session variable 'my_session' to # the operations.\n\norders.insert_one({\"sku\": \"abc123\", \"qty\": 100}, session=my_session) inventory.update_one({\"sku\": \"abc123\", \"qty\": {\"$gte\": 100}}, {\"$inc\": {\"qty\": -100}}, session=my_session)\n\n#. Step 2: Start a client session.\n\nwith client.start_session() as session:\n\n# Step 3: Use with_transaction to start a transaction, execute the callback, # and commit (or abort on error).\n\nsession.with_transaction(callback, read_concern=ReadConcern('local'), write_concern=my_write_concern_majority, read_preference=ReadPreference.PRIMARY) }\n\n204\n\n|\n\nChapter 8: Transactions\n\nIn MongoDB multidocument transactions, you may only perform read/write (CRUD) operations on existing collections or databases. As shown in our example, you must first create a collection outside of a transaction if you wish to insert it into a transaction. Create, drop, or index operations are not permitted in a transaction.\n\nTuning Transaction Limits for Your Application There are a few parameters that are important to be aware of when using transac‐ tions. They can be adjusted to ensure your application can make the optimal use of transactions.\n\nTiming and Oplog Size Limits There are two main categories of limits in MongoDB transactions. The first relates to timing limits of the transaction, controlling how long a specific transaction can run, the time a transaction will wait to acquire locks, and the maximum length that all transactions will run. The second category specifically relates to the MongoDB oplog entry and size limits for an individual entry.\n\nTime limits\n\nThe default maximum runtime of a transaction is one minute or less. This can be increased by modifying the limit controlled by transactionLifetimeLimitSec onds at a mongod instance level. In the case of sharded clusters, the parameter must be set on all shard replica set members. After this time has elapsed, a trans‐ action will be considered expired and will be aborted by a cleanup process, which runs periodically. The cleanup process will run once every 60 seconds or every transactionLifetimeLimitSeconds/2, whichever is lower.\n\nTo explicitly set a time limit on a transaction, it is recommended that you specify a maxTimeMS on commitTransaction. If maxTimeMS is not set then transaction LifetimeLimitSeconds will be used or if it is set but would exceed transaction LifetimeLimitSeconds then transactionLifetimeLimitSeconds will be used instead.\n\nThe default maximum time a transaction will wait to acquire the locks it needs for the operations in the transaction is 5 ms. This can be increased by modifying the limit controlled by maxTransactionLockRequestTimeoutMillis. If the transaction is unable to acquire the locks within this time, it will abort. maxTran sactionLockRequestTimeoutMillis can be set to 0, -1, or a number greater than 0. Setting it to 0 means a transaction will abort if it is unable to immediately acquire all the locks it requires. A setting of -1 will use the operation-specific timeout as specified by maxTimeMS. Any number greater than 0 configures the\n\nTuning Transaction Limits for Your Application\n\n|\n\n205\n\nwait time to that time in seconds as the specified period that a transaction will attempt to acquire the required locks.\n\nOplog size limits\n\nMongoDB will create as many oplog entries as required for the write operations in a transaction. However, each oplog entry must be within the BSON document size limit of 16MB.\n\nTransactions provide a useful feature in MongoDB to ensure consistency, but they should be used with the rich document model. The flexibility of this model and using best practices such as schema design patterns will help avoid the use of transactions for most situations. Transactions are a powerful feature, best used sparingly in your applications.\n\n206\n\n|\n\nChapter 8: Transactions\n\nCHAPTER 9 Application Design\n\nThis chapter covers designing applications to work effectively with MongoDB. It discusses:\n\nSchema design considerations\n\nTrade-offs when deciding whether to embed data or to reference it\n\nTips for optimization\n\nConsistency considerations\n\nHow to migrate schemas\n\nHow to manage schemas\n\nWhen MongoDB isn’t a good choice of data store\n\nSchema Design Considerations A key aspect of data representation is the design of the schema, which is the way your data is represented in your documents. The best approach to this design is to repre‐ sent the data the way your application wants to see it. Thus, unlike in relational data‐ bases, you first need to understand your queries and data access patterns before modeling your schema.\n\nHere are the key aspects you need to consider when designing a schema:\n\nConstraints\n\nYou need to understand any database or hardware limitations. You also need to consider a number of MongoDB’s specific aspects, such as the maximum docu‐ ment size of 16 MB, that full documents get read and written from disk, that an\n\n207",
      "page_number": 210
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 221-228)",
      "start_page": 221,
      "end_page": 228,
      "detection_method": "topic_boundary",
      "content": "update rewrites the whole document, and that atomic updates are at the docu‐ ment level.\n\nAccess patterns of your queries and of your writes\n\nYou will need to identify and quantify the workload of your application and of the wider system. The workload encompasses both the reads and the writes in your application. Once you know when queries are running and how frequently, you can identify the most common queries. These are the queries you need to design your schema to support. Once you have identified these queries, you should try to minimize the number of queries and ensure in your design that data that gets queried together is stored in the same document.\n\nData not used in these queries should be put into a different collection. Data that is infrequently used should also be moved to a different collection. It is worth considering if you can separate your dynamic (read/write) data and your static (mostly read) data. The best performance results occur when you prioritize your schema design for your most common queries.\n\nRelation types\n\nYou should consider which data is related in terms of your application’s needs, as well as the relationships between documents. You can then determine the best approaches to embed or reference the data or documents. You will need to work out how you can reference documents without having to perform additional queries, and how many documents are updated when there is a relationship change. You must also consider if the data structure is easy to query, such as with nested arrays (arrays in arrays), which support modeling certain relationships.\n\nCardinality\n\nOnce you have determined how your documents and your data are related, you should consider the cardinality of these relationships. Specifically, is it one-to- one, one-to-many, many-to-many, one-to-millions, or many-to-billions? It is very important to establish the cardinality of the relationships to ensure you use the best format to model them in your MongoDB schema. You should also con‐ sider whether the object on the many/millions side is accessed separately or only in the context of the parent object, as well as the ratio of updates to reads for the data field in question. The answers to these questions will help you to determine whether you should embed documents or reference documents and if you should be denormalizing data across documents.\n\nSchema Design Patterns Schema design is important in MongoDB, as it impacts directly on application perfor‐ mance. There are many common issues in schema design that can be addressed through the use of known patterns, or “building blocks.” It is best practice in schema design to use one or more of these patterns together.\n\n208\n\n|\n\nChapter 9: Application Design\n\nScheme design patterns that might apply include:\n\nPolymorphic pattern\n\nThis is suitable where all documents in a collection have a similar, but not identi‐ cal, structure. It involves identifying the common fields across the documents that support the common queries that will be run by the application. Tracking specific fields in the documents or subdocuments will help identify the differ‐ ences between the data and different code paths or classes/subclasses that can be coded in your application to manage these differences. This allows for the use of simple queries in a single collection of not-quite-identical documents to improve query performance.\n\nAttribute pattern\n\nThis is suitable when there are a subset of fields in a document that share com‐ mon features on which you want to sort or query, or when the fields you need to sort on only exist in a subset of the documents, or when both of these conditions are true. It involves reshaping the data into an array of key/value pairs and creat‐ ing an index on the elements in this array. Qualifiers can be added as additional fields to these key/value pairs. This pattern assists in targeting many similar fields per document so that fewer indexes are required and queries become simpler to write.\n\nBucket pattern\n\nThis is suitable for time series data where the data is captured as a stream over a period of time. It is much more efficient in MongoDB to “bucket” this data into a set of documents each holding the data for a particular time range than it is to create a document per point in time/data point. For example, you might use a one-hour bucket and place all readings for that hour in an array in a single docu‐ ment. The document itself will have start and end times indicating the period this “bucket” covers.\n\nOutlier pattern\n\nThis addresses the rare instances where a few queries of documents fall outside the normal pattern for the application. It is an advanced schema pattern designed for situations where popularity is a factor. This can be seen in social networks with major influencers, book sales, movie reviews, etc. It uses a flag to indicate the document is an outlier and stores the additional overflow into one or more documents that refer back to the first document via the \"_id\". The flag will be used by your application code to make the additional queries to retrieve the over‐ flow document(s).\n\nComputed pattern\n\nThis is used when data needs to be computed frequently, and it can also be used when the data access pattern is read-intensive. This pattern recommends that the\n\nSchema Design Considerations\n\n|\n\n209\n\ncalculations be done in the background, with the main document being updated periodically. This provides a valid approximation of the computed fields or docu‐ ments without having to continuously generate these for individual queries. This can significantly reduce the strain on the CPU by avoiding repetition of the same calculations, particularly in use cases where reads trigger the calculation and you have a high read-to-write ratio.\n\nSubset pattern\n\nThis is used when you have a working set that exceeds the available RAM of the machine. This can be caused by large documents that contain a lot of informa‐ tion that isn’t being used by your application. This pattern suggests that you split frequently used data and infrequently used data into two separate collections. A typical example might be an ecommerce application keeping the 10 most recent reviews of a product in the “main” (frequently accessed) collection and moving all the older reviews into a second collection queried only if the application needs more than the last 10 reviews.\n\nExtended Reference pattern\n\nThis is used for scenarios where you have many different logical entities or “things,” each with their own collection, but you may want to gather these entities together for a specific function. A typical ecommerce schema might have sepa‐ rate collections for orders, customers, and inventory. This can have a negative performance impact when we want to collect together all the information for a single order from these separate collections. The solution is to identify the fre‐ quently accessed fields and duplicate these within the order document. In the case of an ecommerce order, this would be the name and address of the customer we are shipping the item to. This pattern trades off the duplication of data for a reduction in the number of queries necessary to collate the information together.\n\nApproximation pattern\n\nThis is useful for situations where resource-expensive (time, memory, CPU cycles) calculations are needed but where exact precision is not absolutely required. An example of this is an image or post like/love counter or a page view counter, where knowing the exact count (e.g., whether it’s 999,535 or 1,000,0000) isn’t necessary. In these situations, applying this pattern can greatly reduce the number of writes—for example, by only updating the counter after every 100 or more views instead of after every view.\n\nTree pattern\n\nThis can be applied when you have a lot of queries and have data that is primarily hierarchical in structure. It follows the earlier concept of storing data together that is typically queried together. In MongoDB, you can easily store a hierarchy in an array within the same document. In the example of the ecommerce site, specifically its product catalog, there are often products that belong to multiple\n\n210\n\n|\n\nChapter 9: Application Design\n\ncategories or to categories that are part of other categories. An example might be “Hard Drive,” which is itself a category but comes under the “Storage” category, which itself is under the “Computer Parts” category, which is part of the “Elec‐ tronics” category. In this kind of scenario, we would have a field that would track the entire hierarchy and another field that would hold the immediate category (“Hard Drive”). The entire hierarchy field, kept in an array, provides the ability to use a multikey index on those values. This ensures all items related to categories in the hierarchy will be easily found. The immediate category field allows all items directly related to this category to be found.\n\nPreallocation pattern\n\nThis was primarily used with the MMAP storage engine, but there are still uses for this pattern. The pattern recommends creating an initial empty structure that will be populated later. An example use could be for a reservation system that manages a resource on a day-by-day basis, keeping track of whether it is free or already booked/unavailable. A two-dimensional structure of resources (x) and days (y) makes it trivially easy to check availability and perform calculations.\n\nDocument Versioning pattern\n\nThis provides a mechanism to enable retention of older revisions of documents. It requires an extra field to be added to each document to track the document version in the “main” collection, and an additional collection that contains all the revisions of the documents. This pattern makes a few assumptions: specifically, that each document has a limited number of revisions, that there are not large numbers of documents that need to be versioned, and that the queries are pri‐ marily done on the current version of each document. In situations where these assumptions are not valid, you may need to modify the pattern or consider a dif‐ ferent schema design pattern.\n\nMongoDB provides several useful resources online on patterns and schema design. MongoDB University offers a free course, M320 Data Modeling, as well as a “Building with Patterns” blog series.\n\nNormalization Versus Denormalization There are many ways to represent data, and one of the most important issues to con‐ sider is how much you should normalize your data. Normalization refers to dividing up data into multiple collections with references between collections. Each piece of data lives in one collection, although multiple documents may reference it. Thus, to change the data, only one document must be updated. The MongoDB Aggregation Framework offers joins with the $lookup stage, which performs a left outer join by adding documents to the “joined” collection where there is a matching document in the source collection—it adds a new array field to each matched document in the\n\nNormalization Versus Denormalization\n\n|\n\n211\n\n“joined” collection with the details of the document from the source collection. These reshaped documents are then available in the next stage for further processing.\n\nDenormalization is the opposite of normalization: embedding all of the data in a sin‐ gle document. Instead of documents containing references to one definitive copy of the data, many documents may have copies of the data. This means that multiple documents need to be updated if the information changes, but enables all related data to be fetched with a single query.\n\nDeciding when to normalize and when to denormalize can be difficult: typically, nor‐ malizing makes writes faster and denormalizing makes reads faster. Thus, you need to decide what trade-offs make sense for your application.\n\nExamples of Data Representations Suppose we are storing information about students and the classes that they are tak‐ ing. One way to represent this would be to have a students collection (each student is one document) and a classes collection (each class is one document). Then we could have a third collection (studentClasses) that contains references to the students and the classes they are taking:\n\n> db.studentClasses.findOne({\"studentId\" : id}) { \"_id\" : ObjectId(\"512512c1d86041c7dca81915\"), \"studentId\" : ObjectId(\"512512a5d86041c7dca81914\"), \"classes\" : [ ObjectId(\"512512ced86041c7dca81916\"), ObjectId(\"512512dcd86041c7dca81917\"), ObjectId(\"512512e6d86041c7dca81918\"), ObjectId(\"512512f0d86041c7dca81919\") ] }\n\nIf you are familiar with relational databases, you may have seen this type of join table before (although typically you’d have one student and one class per document, instead of a list of class \"_id\"s). It’s a bit more MongoDB-ish to put the classes in an array, but you usually wouldn’t want to store the data this way because it requires a lot of querying to get to the actual information.\n\nSuppose we wanted to find the classes a student was taking. We’d query for the stu‐ dent in the students collection, query studentClasses for the course \"_id\"s, and then query the classes collection for the class information. Thus, finding this information would take three trips to the server. This is generally not the way you want to struc‐ ture data in MongoDB, unless the classes and students are changing constantly and reading the data does not need to be done quickly.\n\nWe can remove one of the dereferencing queries by embedding class references in the student’s document:\n\n212\n\n|\n\nChapter 9: Application Design\n\n{ \"_id\" : ObjectId(\"512512a5d86041c7dca81914\"), \"name\" : \"John Doe\", \"classes\" : [ ObjectId(\"512512ced86041c7dca81916\"), ObjectId(\"512512dcd86041c7dca81917\"), ObjectId(\"512512e6d86041c7dca81918\"), ObjectId(\"512512f0d86041c7dca81919\") ] }\n\nThe \"classes\" field keeps an array of \"_id\"s of classes that John Doe is taking. When we want to find out information about those classes, we can query the classes collection with those \"_id\"s. This only takes two queries. This is a fairly popular way to structure data that does not need to be instantly accessible and changes, but not constantly.\n\nIf we need to optimize reads further, we can get all of the information in a single query by fully denormalizing the data and storing each class as an embedded docu‐ ment in the \"classes\" field:\n\n{ \"_id\" : ObjectId(\"512512a5d86041c7dca81914\"), \"name\" : \"John Doe\", \"classes\" : [ { \"class\" : \"Trigonometry\", \"credits\" : 3, \"room\" : \"204\" }, { \"class\" : \"Physics\", \"credits\" : 3, \"room\" : \"159\" }, { \"class\" : \"Women in Literature\", \"credits\" : 3, \"room\" : \"14b\" }, { \"class\" : \"AP European History\", \"credits\" : 4, \"room\" : \"321\" } ] }\n\nThe upside of this is that it only takes one query to get the information. The down‐ sides are that it takes up more space and is more difficult to keep in sync. For exam‐ ple, if it turns out that physics was supposed to be worth four credits (not three),\n\nNormalization Versus Denormalization\n\n|\n\n213\n\nevery student in the physics class would need to have their document updated (instead of just updating a central “Physics” document).\n\nFinally, you can use the Extended Reference pattern mentioned earlier, which is a hybrid of embedding and referencing—you create an array of subdocuments with the frequently used information, but with a reference to the actual document for more information:\n\n{ \"_id\" : ObjectId(\"512512a5d86041c7dca81914\"), \"name\" : \"John Doe\", \"classes\" : [ { \"_id\" : ObjectId(\"512512ced86041c7dca81916\"), \"class\" : \"Trigonometry\" }, { \"_id\" : ObjectId(\"512512dcd86041c7dca81917\"), \"class\" : \"Physics\" }, { \"_id\" : ObjectId(\"512512e6d86041c7dca81918\"), \"class\" : \"Women in Literature\" }, { \"_id\" : ObjectId(\"512512f0d86041c7dca81919\"), \"class\" : \"AP European History\" } ] }\n\nThis approach is also a nice option because the amount of information embedded can change over time as your requirements change: if you want to include more or less information on a page, you can embed more or less of it in the document.\n\nAnother important consideration is how often this information will change, versus how often it’s read. If it will be updated regularly, then normalizing it is a good idea. However, if it changes infrequently, then there is little benefit to optimizing the update process at the expense of every read your application performs.\n\nFor example, a textbook normalization use case is to store a user and their address in separate collections. However, people’s addresses rarely change, so you generally shouldn’t penalize every read on the off chance that someone’s moved. Your applica‐ tion should embed the address in the user document.\n\nIf you decide to use embedded documents and you need to update them, you should set up a cron job to ensure that any updates you do are successfully propagated to every document. For example, suppose you attempt to do a multi-update but the server crashes before all of the documents have been updated. You need a way to detect this and retry the update.\n\n214\n\n|\n\nChapter 9: Application Design\n\nIn terms of update operators, \"$set\" is idempotent but \"$inc\" is not. Idempotent operations will have the same outcome whether tried once or several times; in the case of a network error, retrying the operation will be sufficient for the update to occur. In the case of operators that are not idempotent, the operation should be bro‐ ken into two operations that are individually idempotent and safe to retry. This can be achieved by including a unique pending token in the first operation and having the second operation use both a unique key and the unique pending token. This approach allows \"$inc\" to be idempotent because each individual updateOne opera‐ tion is idempotent.\n\nTo some extent, the more information you are generating, the less of it you should embed. If the content of the embedded fields or number of embedded fields is sup‐ posed to grow without bound then they should generally be referenced, not embed‐ ded. Things like comment trees or activity lists should be stored as their own documents, not embedded. It is also worth considering using the Subset pattern (described in “Schema Design Patterns” on page 208) to store the most recent items (or some other subset) in the document.\n\nFinally, the fields that are included should be integral to the data in the document. If a field is almost always excluded from your results when you query for a document, it’s a good sign that it may belong in another collection. These guidelines are summar‐ ized in Table 9-1.\n\nTable 9-1. Comparison of embedding versus references\n\nEmbedding is better for... Small subdocuments Data that does not change regularly When eventual consistency is acceptable Documents that grow by a small amount Data that you’ll often need to perform a second query to fetch Data that you’ll often exclude from the results Fast reads\n\nReferences are better for... Large subdocuments Volatile data When immediate consistency is necessary Documents that grow by a large amount\n\nFast writes\n\nSuppose we had a users collection. Here are some example fields we might have in the user documents and an indication of whether or not they should be embedded:\n\nAccount preferences\n\nThese are only relevant to this user document, and will probably be exposed with other user information in the document. Account preferences should generally be embedded.\n\nRecent activity\n\nThis depends on how much recent activity grows and changes. If it is a fixed-size field (say, the last 10 things), it might be useful to embed this information or to implement the Subset pattern.\n\nNormalization Versus Denormalization\n\n|\n\n215",
      "page_number": 221
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 229-236)",
      "start_page": 229,
      "end_page": 236,
      "detection_method": "topic_boundary",
      "content": "Friends\n\nGenerally this information should not be embedded, or at least not fully. See “Friends, Followers, and Other Inconveniences” on page 216.\n\nAll of the content this user has produced This should not be embedded.\n\nCardinality Cardinality is an indication of how many references a collection has to another collec‐ tion. Common relationships are one-to-one, one-to-many, or many-to-many. For example, suppose we had a blog application. Each post has a title, so that’s a one-to- one relationship. Each author has many posts, so that’s a one-to-many relationship. And posts have many tags and tags refer to many posts, so that’s a many-to-many relationship.\n\nWhen using MongoDB, it can be conceptually useful to split “many” into subcatego‐ ries: “many” and “few.” For example, you might have a one-to-few relationship between authors and posts: each author only writes a few posts. You might have many-to-few relation between blog posts and tags: you probably have many more blog posts than you have tags. However, you’d have a one-to-many relationship between blog posts and comments: each post has many comments.\n\nDetermining few versus many relations can help you decide what to embed versus what to reference. Generally, “few” relationships will work better with embedding, and “many” relationships will work better as references.\n\nFriends, Followers, and Other Inconveniences Keep your friends close and your enemies embedded.\n\nThis section covers considerations for social graph data. Many social applications need to link people, content, followers, friends, and so on. Figuring out how to bal‐ ance embedding and referencing this highly connected information can be tricky, but generally following, friending, or favoriting can be simplified to a publication/ subscription system: one user is subscribing to notifications from another. Thus, there are two basic operations that need to be efficient: storing subscribers and noti‐ fying all interested parties of an event.\n\nThere are three ways people typically implement subscribing. The first option is to put the producer in the subscriber’s document, which looks something like this:\n\n{ \"_id\" : ObjectId(\"51250a5cd86041c7dca8190f\"), \"username\" : \"batman\", \"email\" : \"batman@waynetech.com\" \"following\" : [ ObjectId(\"51250a72d86041c7dca81910\"),\n\n216\n\n|\n\nChapter 9: Application Design\n\nObjectId(\"51250a7ed86041c7dca81936\") ] }\n\nNow, given a user’s document, you can issue a query like the following to find all of the activities that have been published that they might be interested in:\n\ndb.activities.find({\"user\" : {\"$in\" : user[\"following\"]}})\n\nHowever, if you need to find everyone who is interested in a newly published activity, you’d have to query the \"following\" field across all users.\n\nAlternatively, you could append the followers to the producer’s document, like so:\n\n{ \"_id\" : ObjectId(\"51250a7ed86041c7dca81936\"), \"username\" : \"joker\", \"email\" : \"joker@mailinator.com\" \"followers\" : [ ObjectId(\"512510e8d86041c7dca81912\"), ObjectId(\"51250a5cd86041c7dca8190f\"), ObjectId(\"512510ffd86041c7dca81910\") ] }\n\nWhenever this user does something, all the users you need to notify are right there. The downside is that now you need to query the whole users collection to find every‐ one a user follows (the opposite limitation as in the previous case).\n\nEither of these options comes with an additional downside: they make your user documents larger and more volatile. The \"following\" (or \"followers\") field often won’t even need to be returned: how often do you want to list every follower? Thus, the final option neutralizes these downsides by normalizing even further and storing subscriptions in another collection. Normalizing this far is often overkill, but it can be useful for an extremely volatile field that often isn’t returned with the rest of the document. \"followers\" may be a sensible field to normalize this way.\n\nIn this case you keep a collection that matches publishers to subscribers, with docu‐ ments that look something like this:\n\n{ \"_id\" : ObjectId(\"51250a7ed86041c7dca81936\"), // followee's \"_id\" \"followers\" : [ ObjectId(\"512510e8d86041c7dca81912\"), ObjectId(\"51250a5cd86041c7dca8190f\"), ObjectId(\"512510ffd86041c7dca81910\") ] }\n\nThis keeps your user documents svelte but means an extra query is needed to get the followers.\n\nNormalization Versus Denormalization\n\n|\n\n217\n\nDealing with the Wil Wheaton effect\n\nRegardless of which strategy you use, embedding only works with a limited number of subdocuments or references. If you have celebrity users, they may overflow any document that you’re storing followers in. The typical way of compensating for this is to use the Outlier pattern discussed in “Schema Design Patterns” on page 208 and have a “continuation” document, if necessary. For example, you might have:\n\n> db.users.find({\"username\" : \"wil\"}) { \"_id\" : ObjectId(\"51252871d86041c7dca8191a\"), \"username\" : \"wil\", \"email\" : \"wil@example.com\", \"tbc\" : [ ObjectId(\"512528ced86041c7dca8191e\"), ObjectId(\"5126510dd86041c7dca81924\") ] \"followers\" : [ ObjectId(\"512528a0d86041c7dca8191b\"), ObjectId(\"512528a2d86041c7dca8191c\"), ObjectId(\"512528a3d86041c7dca8191d\"), ... ] } { \"_id\" : ObjectId(\"512528ced86041c7dca8191e\"), \"followers\" : [ ObjectId(\"512528f1d86041c7dca8191f\"), ObjectId(\"512528f6d86041c7dca81920\"), ObjectId(\"512528f8d86041c7dca81921\"), ... ] } { \"_id\" : ObjectId(\"5126510dd86041c7dca81924\"), \"followers\" : [ ObjectId(\"512673e1d86041c7dca81925\"), ObjectId(\"512650efd86041c7dca81922\"), ObjectId(\"512650fdd86041c7dca81923\"), ... ] }\n\nThen add application logic to support fetching the documents in the “to be contin‐ ued” (\"tbc\") array.\n\n218\n\n|\n\nChapter 9: Application Design\n\nOptimizations for Data Manipulation To optimize your application, you must first determine what its bottleneck is by eval‐ uating its read and write performance. Optimizing reads generally involves having the correct indexes and returning as much of the information as possible in a single document. Optimizing writes usually involves minimizing the number of indexes you have and making updates as efficient as possible.\n\nThere is often a trade-off between schemas that are optimized for writing quickly and those that are optimized for reading quickly, so you may have to decide which is more important for your application. Factor in not only the importance of reads versus writes, but also their proportions: if writes are more important but you’re doing a thousand reads to every write, you may still want to optimize reads first.\n\nRemoving Old Data Some data is only important for a brief time: after a few weeks or months it is just wasting storage space. There are three popular options for removing old data: using capped collections, using TTL collections, and dropping collections per time period.\n\nThe easiest option is to use a capped collection: set it to a large size and let old data “fall off” the end. However, capped collections pose certain limitations on the opera‐ tions you can do and are vulnerable to spikes in traffic, temporarily lowering the length of time that they can hold. See “Capped Collections” on page 151 for more information.\n\nThe second option is to use a TTL collections. This gives you finer-grain control over when documents are removed, but it may not be fast enough for collections with a very high write volume: it removes documents by traversing the TTL index the same way a user-requested remove would. If a TTL collection can keep up, though, this is probably the easiest solution to implement. See “Time-To-Live Indexes” on page 155 for more information about TTL indexes.\n\nThe final option is to use multiple collections: for example, one collection per month. Every time the month changes, your application starts using this month’s (empty) col‐ lection and searching for data in both the current and previous months’ collections. Once a collection is older than, say, six months, you can drop it. This strategy can keep up with nearly any volume of traffic, but it’s more complex to build an applica‐ tion around because you have to use dynamic collection (or database) names and possibly query multiple databases.\n\nOptimizations for Data Manipulation\n\n|\n\n219\n\nPlanning Out Databases and Collections Once you have sketched out what your documents look like, you must decide what collections or databases to put them in. This is often a fairly intuitive process, but there are some guidelines to keep in mind.\n\nIn general, documents with a similar schema should be kept in the same collection. MongoDB generally disallows combining data from multiple collections, so if there are documents that need to be queried or aggregated together, those are good candi‐ dates for putting in one big collection. For example, you might have documents that are fairly different “shapes,” but if you’re going to be aggregating them, they should all live in the same collection (or you can use the $merge stage if they are in separate col‐ lections or databases).\n\nFor collections, the big issues to consider are locking (you get a read/write lock per document) and storage. Generally, if you have a high-write workload you may need to consider using multiple physical volumes to reduce I/O bottlenecks. Each database can reside in its own directory when you use the --directoryperdb option, allowing you to mount different databases to different volumes. Thus, you may want all items within a database to be of similar “quality,” with a similar access pattern or similar traffic levels.\n\nFor example, suppose you have an application with several components: a logging component that creates a huge amount of not-very-valuable data, a user collection, and a couple of collections for user-generated data. These collections are high-value: it is important that user data is safe. There is also a high-traffic collection for social activities, which is of lower importance but not quite as unimportant as the logs. This collection is mainly used for user notifications, so it is almost an append-only collection.\n\nSplitting these up by importance, you might end up with three databases: logs, activi‐ ties, and users. The nice thing about this strategy is that you may find that your highest-value data is also what you have the least of (e.g., users probably don’t gener‐ ate as much data as logging does). You might not be able to afford an SSD for your entire dataset, but you might be able to get one for your users, or you might use RAID10 for users and RAID0 for logs and activities.\n\nBe aware that there are some limitations when using multiple databases prior to MongoDB 4.2 and the introduction of the $merge operator in the Aggregation Framework, which allows you to store results from an aggregation from one database to a different database and a different collection within that database. An additional point to note is that the renameCollection command is slower when copying an existing collection from one database to a different database, as it must copy all the documents to the new database.\n\n220\n\n|\n\nChapter 9: Application Design\n\nManaging Consistency You must figure out how consistent your application’s reads need to be. MongoDB supports a huge variety of consistency levels, from always being able to read your own writes to reading data of unknown oldness. If you’re reporting on the last year of activity, you might only need data that’s correct to the last couple of days. Conversely, if you’re doing real-time trading, you might need to immediately read the latest writes.\n\nTo understand how to achieve these varying levels of consistency, it is important to understand what MongoDB is doing under the hood. The server keeps a queue of requests for each connection. When the client sends a request, it will be placed at the end of its connection’s queue. Any subsequent requests on the connection will occur after the previously enqueued operation is processed. Thus, a single connection has a consistent view of the database and can always read its own writes.\n\nNote that this is a per-connection queue: if we open two shells, we will have two con‐ nections to the database. If we perform an insert in one shell, a subsequent query in the other shell might not return the inserted document. However, within a single shell, if we query for a document after inserting it, the document will be returned. This behavior can be difficult to duplicate by hand, but on a busy server interleaved inserts and queries are likely to occur. Often developers run into this when they insert data in one thread and then check that it was successfully inserted in another. For a moment or two, it looks like the data was not inserted, and then it suddenly appears.\n\nThis behavior is especially worth keeping in mind when using the Ruby, Python, and Java drivers, because all three use connection pooling. For efficiency, these drivers open multiple connections (a pool) to the server and distribute requests across them. They all, however, have mechanisms to guarantee that a series of requests is processed by a single connection. There is detailed documentation on connection pooling for the various languages in the MongoDB Drivers Connection Monitoring and Pooling specification.\n\nWhen you send reads to a replica set secondary (see Chapter 12), this becomes an even larger issue. Secondaries may lag behind the primary, leading to reading data from seconds, minutes, or even hours ago. There are several ways to deal with this, the easiest being to simply send all reads to the primary if you care about staleness.\n\nMongoDB offers the readConcern option to control the consistency and isolation properties of the data being read. It can be combined with writeConcern to control the consistency and availability guarantees made to your application. There are five levels: \"local\", \"available\", \"majority\", \"linearizable\", and \"snapshot\". Depending on the application, in cases where you want to avoid read staleness you could consider using \"majority\", which returns only durable data that has been acknowledged by the majority of the replica set members and will not be rolled back.\n\nManaging Consistency\n\n|\n\n221\n\n\"linearizable\" may also be an option: it returns data that reflects all successful majority-acknowledged writes that have completed prior to the start of the read oper‐ ation. MongoDB may wait for concurrently executing writes to finish before return‐ ing the results with the \"linearizable\" readConcern.\n\nThree senior engineers from MongoDB published a paper called “Tunable Consis‐ tency in MongoDB” at the PVLDB conference in 2019.1 This paper outlines the dif‐ ferent MongoDB consistency models used for replication and how application developers can utilize the various models.\n\nMigrating Schemas As your application grows and your needs change, your schema may have to grow and change as well. There are a couple of ways of accomplishing this, but regardless of the method you choose, you should carefully document each schema that your application has used. Ideally, you should consider if the Document Versioning pattern (see “Schema Design Patterns” on page 208) is applicable.\n\nThe simplest method is to simply have your schema evolve as your application requires, making sure that your application supports all old versions of the schema (e.g., accepting the existence or nonexistence of fields or dealing with multiple possi‐ ble field types gracefully). But this technique can become messy, particularly if you have conflicting schema versions. For instance, one version might require a \"mobile\" field, another version might require not having a \"mobile\" field but instead require a different field, and yet another version might treat the \"mobile\" field as optional. Keeping track of these shifting requirements can gradually turn your code into spaghetti.\n\nTo handle changing requirements in a slightly more structured way, you can include a \"version\" field (or just \"v\") in each document and use that to determine what your application will accept for document structure. This enforces your schema more rig‐ orously: a document has to be valid for some version of the schema, if not the current one. However, it still requires supporting old versions.\n\nThe final option is to migrate all of your data when the schema changes. Generally this is not a good idea: MongoDB allows you to have a dynamic schema in order to avoid migrates because they put a lot of pressure on your system. However, if you do decide to change every document, you will need to ensure that all the documents were successfully updated. MongoDB supports transactions, which support this type\n\n1 The authors are William Schultz, senior software engineer for replication; Tess Avitabile, team lead of the rep‐\n\nlication team; and Alyson Cabral, product manager for Distributed Systems.\n\n222\n\n|\n\nChapter 9: Application Design\n\nof migration. If MongoDB crashes in the middle of a transaction, the older schema will be retained.\n\nManaging Schemas MongoDB introduced schema validation in version 3.2, which allows for validation during updates and insertions. In version 3.6 it added JSON Schema validation via the $jsonSchema operator, which is now the recommended method for all schema validation in MongoDB. At the time of writing MongoDB supports draft 4 of JSON Schema, but please check the documentation for the most up-to-date information on this feature.\n\nValidation does not check existing documents until they are modified, and it is con‐ figured per collection. To add validation to an existing collection, you use the coll Mod command with the validator option. You can add validation to a new collection by specifying the validator option when using db.createCollection(). MongoDB also provides two additional options, validationLevel and validationAction. vali dationLevel determines how strictly validation rules are applied to existing docu‐ ments during an update, and validationAction decides whether an error plus rejection or a warning with allowance for illegal documents should occur.\n\nWhen Not to Use MongoDB While MongoDB is a general-purpose database that works well for most applications, it isn’t good at everything. There are a few reasons you might need to avoid it:\n\nJoining many different types of data across many different dimensions is some‐ thing relational databases are fantastic at. MongoDB isn’t supposed to do this well and most likely never will.\n\nOne of the big (if, hopefully, temporary) reasons to use a relational database over MongoDB is if you’re using tools that don’t support it. From SQLAlchemy to WordPress, there are thousands of tools that just weren’t built to support Mon‐ goDB. The pool of tools that do support it is growing, but its ecosystem is hardly the size of relational databases’ yet.\n\nManaging Schemas\n\n|\n\n223",
      "page_number": 229
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 237-244)",
      "start_page": 237,
      "end_page": 244,
      "detection_method": "topic_boundary",
      "content": "PART III Replication\n\nCHAPTER 10 Setting Up a Replica Set\n\nThis chapter introduces MongoDB’s high-availability system: replica sets. It covers:\n\nWhat replica sets are\n\nHow to set up a replica set\n\nWhat configuration options are available for replica set members\n\nIntroduction to Replication Since the first chapter, we’ve been using a standalone server, a single mongod server. It’s an easy way to get started but a dangerous way to run in production. What if your server crashes or becomes unavailable? Your database will be unavailable for at least a little while. If there are problems with the hardware, you might have to move your data to another machine. In the worst case, disk or network issues could leave you with corrupt or inaccessible data.\n\nReplication is a way of keeping identical copies of your data on multiple servers and is recommended for all production deployments. Replication keeps your application running and your data safe, even if something happens to one or more of your servers.\n\nWith MongoDB, you set up replication by creating a replica set. A replica set is a group of servers with one primary, the server taking writes, and multiple secondaries, servers that keep copies of the primary’s data. If the primary crashes, the secondaries can elect a new primary from amongst themselves.\n\nIf you are using replication and a server goes down, you can still access your data from the other servers in the set. If the data on a server is damaged or inaccessible, you can make a new copy of the data from one of the other members of the set.\n\n227\n\nThis chapter introduces replica sets and covers how to set up replication on your sys‐ tem. If you are less interested in replication mechanics and simply want to create a replica set for testing/development or production, use MongoDB’s cloud solution, MongoDB Atlas. It’s easy to use and provides a free-tier option for experimentation. Alternatively, to manage MongoDB clusters in your own infrastructure, you can use Ops Manager.\n\nSetting Up a Replica Set, Part 1 In this chapter, we’ll show you how to set up a three-node replica set on a single machine so you can start experimenting with replica set mechanics. This is the type of setup that you might script just to get a replica set up and running and then poke at it with administrative commands in the mongo shell or simulate network partitions or server failures to better understand how MongoDB handles high availability and disaster recovery. In production, you should always use a replica set and allocate a dedicated host to each member to avoid resource contention and provide isolation against server failure. To provide further resilience, you should also use the DNS Seedlist Connection format to specify how your applications connect to your replica set. The advantage to using DNS is that servers hosting your MongoDB replica set members can be changed in rotation without needing to reconfigure the clients (specifically, their connection strings).\n\nGiven the variety of virtualization and cloud options available, it is nearly as easy to bring up a test replica set with each member on a dedicated host. We’ve provided a Vagrant script to allow you to experiment with this option.1\n\nTo get started with our test replica set, let’s first create separate data directories for each node. On Linux or macOS, run the following command in the terminal to create the three directories:\n\n$ mkdir -p ~/data/rs{1,2,3}\n\nThis will create the directories ~/data/rs1, ~/data/rs2, and ~/data/rs3 (~ identifies your home directory).\n\nOn Windows, to create these directories, run the following in the Command Prompt (cmd) or PowerShell:\n\n> md c:\\data\\rs1 c:\\data\\rs2 c:\\data\\rs3\n\nThen, on Linux or macOS, run each of the following commands in a separate terminal:\n\n1 See https://github.com/mongodb-the-definitive-guide-3e/mongodb-the-definitive-guide-3e.\n\n228\n\n|\n\nChapter 10: Setting Up a Replica Set\n\n$ mongod --replSet mdbDefGuide --dbpath ~/data/rs1 --port 27017 \\ --smallfiles --oplogSize 200 $ mongod --replSet mdbDefGuide --dbpath ~/data/rs2 --port 27018 \\ --smallfiles --oplogSize 200 $ mongod --replSet mdbDefGuide --dbpath ~/data/rs3 --port 27019 \\ --smallfiles --oplogSize 200\n\nOn Windows, run each of the following commands in its own Command Prompt or PowerShell window:\n\n> mongod --replSet mdbDefGuide --dbpath c:\\data\\rs1 --port 27017 \\ --smallfiles --oplogSize 200 > mongod --replSet mdbDefGuide --dbpath c:\\data\\rs2 --port 27018 \\ --smallfiles --oplogSize 200 > mongod --replSet mdbDefGuide --dbpath c:\\data\\rs3 --port 27019 \\ --smallfiles --oplogSize 200\n\nOnce you’ve started them, you should have three separate mongod processes running.\n\nIn general, the principles we will walk through in the rest of this chapter apply to replica sets used in production deployments where each mongod has a dedicated host. However, there are additional details pertaining to securing replica sets that we address in Chap‐ ter 19; we’ll touch on those just briefly here as a preview.\n\nNetworking Considerations Every member of a set must be able to make connections to every other member of the set (including itself). If you get errors about members not being able to reach other members that you know are running, you may have to change your network configuration to allow connections between them.\n\nThe processes you’ve launched can just as easily be running on separate servers. However, with the release of MongoDB 3.6, mongod binds to localhost (127.0.0.1) only by default. In order for each member of replica set to communicate with the oth‐ ers, you must also bind to an IP address that is reachable by other members. If we were running a mongod instance on a server with a network interface having an IP address of 198.51.100.1 and we wanted to run it as a member of replica set with each member on different servers, we could specify the command-line parameter -- bind_ip or use bind_ip in the configuration file for this instance:\n\n$ mongod --bind_ip localhost,192.51.100.1 --replSet mdbDefGuide \\ --dbpath ~/data/rs1 --port 27017 --smallfiles --oplogSize 200\n\nWe would make similar modifications to launch the other mongods as well in this case, regardless of whether we’re running on Linux, macOS, or Windows.\n\nNetworking Considerations\n\n|\n\n229\n\nSecurity Considerations Before you bind to IP addresses other than localhost, when configuring a replica set, you should enable authorization controls and specify an authentication mechanism. In addition, it is a good idea to encrypt data on disk and communication among rep‐ lica set members and between the set and clients. We’ll go into more detail on secur‐ ing replica sets in Chapter 19.\n\nSetting Up a Replica Set, Part 2 Returning to our example, with the work we’ve done so far, each mongod does not yet know that the others exist. To tell them about one another, we need to create a config‐ uration that lists each of the members and send this configuration to one of our mon‐ god processes. It will take care of propagating the configuration to the other members.\n\nIn a fourth terminal, Windows Command Prompt, or PowerShell window, launch a mongo shell that connects to one of the running mongod instances. You can do this by typing the following command. With this command, we’ll connect to the mongod running on port 27017:\n\n$ mongo --port 27017\n\nThen, in the mongo shell, create a configuration document and pass this to the rs.ini tiate() helper to initiate a replica set. This will initiate a replica set containing three members and propagate the configuration to the rest of the mongods so that a replica set is formed:\n\n> rsconf = { _id: \"mdbDefGuide\", members: [ {_id: 0, host: \"localhost:27017\"}, {_id: 1, host: \"localhost:27018\"}, {_id: 2, host: \"localhost:27019\"} ] } > rs.initiate(rsconf) { \"ok\" : 1, \"operationTime\" : Timestamp(1501186502, 1) }\n\nThere are several important parts of a replica set configuration document. The con‐ fig’s \"_id\" is the name of the replica set that you passed in on the command line (in this example, \"mdbDefGuide\"). Make sure that this name matches exactly.\n\nThe next part of the document is an array of members of the set. Each of these needs two fields: an \"_id\" that is an integer and unique among the replica set members, and a hostname.\n\n230\n\n|\n\nChapter 10: Setting Up a Replica Set\n\nNote that we are using localhost as a hostname for the members in this set. This is for example purposes only. In later chapters where we discuss securing replica sets, we’ll look at configurations that are more appropriate for production deployments. Mon‐ goDB allows all-localhost replica sets for testing locally but will protest if you try to mix localhost and non-localhost servers in a config.\n\nThis config document is your replica set configuration. The member running on localhost:27017 will parse the configuration and send messages to the other members, alerting them of the new configuration. Once they have all loaded the configuration, they will elect a primary and start handling reads and writes.\n\nUnfortunately, you cannot convert a standalone server to a replica set without some downtime for restarting it and initializing the set. Thus, even if you only have one server to start out with, you may want to configure it as a one-member replica set. That way, if you want to add more members later, you can do so without downtime.\n\nIf you are starting a brand-new set, you can send the configuration to any member in the set. If you are starting with data on one of the members, you must send the con‐ figuration to the member with data. You cannot initiate a replica set with data on more than one member.\n\nOnce initiated, you should have a fully functional replica set. The replica set should elect a primary. You can view the status of a replica set using rs.status(). The out‐ put from rs.status() tells you quite a bit about the replica set, including a number of things we’ve not yet covered, but don’t worry, we’ll get there! For now, take a look at the members array. Note that all three of our mongod instances are listed in this array and that one of them, in this case the mongod running on port 27017, has been elected primary. The other two are secondaries. If you try this for yourself you will certainly have different values for \"date\" and the several Timestamp values in this output, but you might also find that a different mongod was elected primary (that’s totally fine):\n\n> rs.status() { \"set\" : \"mdbDefGuide\", \"date\" : ISODate(\"2017-07-27T20:23:31.457Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : { \"lastCommittedOpTime\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"appliedOpTime\" : {\n\nSetting Up a Replica Set, Part 2\n\n|\n\n231\n\n232\n\n\"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) } }, \"members\" : [ { \"_id\" : 0, \"name\" : \"localhost:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 688, \"optime\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2017-07-27T20:23:26Z\"), \"electionTime\" : Timestamp(1501186514, 1), \"electionDate\" : ISODate(\"2017-07-27T20:15:14Z\"), \"configVersion\" : 1, \"self\" : true }, { \"_id\" : 1, \"name\" : \"localhost:27018\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 508, \"optime\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2017-07-27T20:23:26Z\"), \"optimeDurableDate\" : ISODate(\"2017-07-27T20:23:26Z\"), \"lastHeartbeat\" : ISODate(\"2017-07-27T20:23:30.818Z\"), \"lastHeartbeatRecv\" : ISODate(\"2017-07-27T20:23:30.113Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"localhost:27017\", \"configVersion\" : 1 }, { \"_id\" : 2, \"name\" : \"localhost:27019\",\n\n|\n\nChapter 10: Setting Up a Replica Set\n\n\"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 508, \"optime\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2017-07-27T20:23:26Z\"), \"optimeDurableDate\" : ISODate(\"2017-07-27T20:23:26Z\"), \"lastHeartbeat\" : ISODate(\"2017-07-27T20:23:30.818Z\"), \"lastHeartbeatRecv\" : ISODate(\"2017-07-27T20:23:30.113Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"localhost:27017\", \"configVersion\" : 1 } ], \"ok\" : 1, \"operationTime\" : Timestamp(1501187006, 1) }\n\nrs Helper Functions rs is a global variable that contains replication helper functions (run rs.help() to see the helpers it exposes). These functions are almost always just wrappers around data‐ base commands. For example, the following database command is equivalent to rs.initiate(config):\n\n> db.adminCommand({\"replSetInitiate\" : config})\n\nIt is good to have familiarity with both the helpers and the underlying commands, because it might be easier to use the command form instead of the helper.\n\nObserving Replication If your replica set elected the mongod on port 27017 as primary, then the mongo shell used to initiate the replica set is currently connected to the primary. You should see the prompt change to something like the following:\n\nmdbDefGuide:PRIMARY>\n\nThis indicates that we are connected to the primary of the replica set having the \"_id\" \"mdbDefGuide\". To simplify and for the sake of clarity, we’ll abbreviate the mongo shell prompt to just > throughout the replication examples.\n\nObserving Replication\n\n|\n\n233",
      "page_number": 237
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 245-257)",
      "start_page": 245,
      "end_page": 257,
      "detection_method": "topic_boundary",
      "content": "If your replica set elected a different node primary, quit the shell and connect to the primary by specifying the correct port number in the command line, as we did when launching the mongo shell earlier. For example, if your set’s primary is on port 27018, connect using the following command:\n\n$ mongo --port 27018\n\nNow that you’re connected to the primary, try doing some writes and see what hap‐ pens. First, insert 1,000 documents:\n\n> use test > for (i=0; i<1000; i++) {db.coll.insert({count: i})} > > // make sure the docs are there > db.coll.count() 1000\n\nNow check one of the secondaries and verify that it has a copy of all of these docu‐ ments. You could do this by quitting the shell and connecting using the port number of one of the secondaries, but it’s easy to acquire a connection to one of the seconda‐ ries by instantiating a connection object using the Mongo constructor within the shell you’re already running.\n\nFirst, use your connection to the test database on the primary to run the isMaster command. This will show you the status of the replica set, in a much more concise form than rs.status(). It is also a convenient means of determining which member is primary when writing application code or scripting:\n\n> db.isMaster() { \"hosts\" : [ \"localhost:27017\", \"localhost:27018\", \"localhost:27019\" ], \"setName\" : \"mdbDefGuide\", \"setVersion\" : 1, \"ismaster\" : true, \"secondary\" : false, \"primary\" : \"localhost:27017\", \"me\" : \"localhost:27017\", \"electionId\" : ObjectId(\"7fffffff0000000000000004\"), \"lastWrite\" : { \"opTime\" : { \"ts\" : Timestamp(1501198208, 1), \"t\" : NumberLong(4) }, \"lastWriteDate\" : ISODate(\"2017-07-27T23:30:08Z\") }, \"maxBsonObjectSize\" : 16777216, \"maxMessageSizeBytes\" : 48000000,\n\n234\n\n|\n\nChapter 10: Setting Up a Replica Set\n\n\"maxWriteBatchSize\" : 1000, \"localTime\" : ISODate(\"2017-07-27T23:30:08.722Z\"), \"maxWireVersion\" : 6, \"minWireVersion\" : 0, \"readOnly\" : false, \"compression\" : [ \"snappy\" ], \"ok\" : 1, \"operationTime\" : Timestamp(1501198208, 1) }\n\nIf at any point an election is called and the mongod you’re connected to becomes a secondary, you can use the isMaster command to determine which member has become primary. The output here tells us that localhost:27018 and localhost:27019 are both secondaries, so we can use either for our purposes. Let’s instantiate a connection to localhost:27019:\n\n> secondaryConn = new Mongo(\"localhost:27019\") connection to localhost:27019 > > secondaryDB = secondaryConn.getDB(\"test\") test\n\nNow, if we attempt to do a read on the collection that has been replicated to the sec‐ ondary, we’ll get an error. Let’s attempt to do a find on this collection and then review the error and why we get it:\n\n> secondaryDB.coll.find() Error: error: { \"operationTime\" : Timestamp(1501200089, 1), \"ok\" : 0, \"errmsg\" : \"not master and slaveOk=false\", \"code\" : 13435, \"codeName\" : \"NotMasterNoSlaveOk\" }\n\nSecondaries may fall behind the primary (or lag) and not have the most current writes, so secondaries will refuse read requests by default to prevent applications from accidentally reading stale data. Thus, if you attempt to query a secondary, you’ll get an error stating that it’s not the primary. This is to protect your application from acci‐ dentally connecting to a secondary and reading stale data. To allow queries on the secondary, we can set an “I’m okay with reading from secondaries” flag, like so:\n\n> secondaryConn.setSlaveOk()\n\nNote that slaveOk is set on the connection (secondaryConn), not the database (secondaryDB).\n\nNow you’re all set to read from this member. Query it normally:\n\nObserving Replication\n\n|\n\n235\n\n> secondaryDB.coll.find() { \"_id\" : ObjectId(\"597a750696fd35621b4b85db\"), \"count\" : 0 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85dc\"), \"count\" : 1 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85dd\"), \"count\" : 2 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85de\"), \"count\" : 3 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85df\"), \"count\" : 4 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e0\"), \"count\" : 5 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e1\"), \"count\" : 6 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e2\"), \"count\" : 7 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e3\"), \"count\" : 8 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e4\"), \"count\" : 9 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e5\"), \"count\" : 10 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e6\"), \"count\" : 11 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e7\"), \"count\" : 12 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e8\"), \"count\" : 13 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e9\"), \"count\" : 14 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85ea\"), \"count\" : 15 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85eb\"), \"count\" : 16 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85ec\"), \"count\" : 17 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85ed\"), \"count\" : 18 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85ee\"), \"count\" : 19 } Type \"it\" for more\n\nYou can see that all of our documents are there.\n\nNow, try to write to a secondary:\n\n> secondaryDB.coll.insert({\"count\" : 1001}) WriteResult({ \"writeError\" : { \"code\" : 10107, \"errmsg\" : \"not master\" } }) > secondaryDB.coll.count() 1000\n\nYou can see that the secondary does not accept the write. A secondary will only per‐ form writes that it gets through replication, not from clients.\n\nThere is one other interesting feature that you should try out: automatic failover. If the primary goes down, one of the secondaries will automatically be elected primary. To test this, stop the primary:\n\n> db.adminCommand({\"shutdown\" : 1})\n\nYou’ll see some error messages generated when you run this command because the mongod running on port 27017 (the member we’re connected to) will terminate and the shell we’re using will lose its connection:\n\n2017-07-27T20:10:50.612-0400 E QUERY [thread1] Error: error doing query: failed: network error while attempting to run command 'shutdown' on host '127.0.0.1:27017' : DB.prototype.runCommand@src/mongo/shell/db.js:163:1 DB.prototype.adminCommand@src/mongo/shell/db.js:179:16 @(shell):1:1 2017-07-27T20:10:50.614-0400 I NETWORK [thread1] trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed\n\n236\n\n|\n\nChapter 10: Setting Up a Replica Set\n\n2017-07-27T20:10:50.615-0400 I NETWORK [thread1] reconnect 127.0.0.1:27017 (127.0.0.1) ok MongoDB Enterprise mdbDefGuide:SECONDARY> 2017-07-27T20:10:56.051-0400 I NETWORK [thread1] trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed 2017-07-27T20:10:56.051-0400 W NETWORK [thread1] Failed to connect to 127.0.0.1:27017, in(checking socket for error after poll), reason: Connection refused 2017-07-27T20:10:56.051-0400 I NETWORK [thread1] reconnect 127.0.0.1:27017 (127.0.0.1) failed failed MongoDB Enterprise > MongoDB Enterprise > secondaryConn.isMaster() 2017-07-27T20:11:15.422-0400 E QUERY [thread1] TypeError: secondaryConn.isMaster is not a function : @(shell):1:1\n\nThis isn’t a problem. It won’t cause the shell to crash. Go ahead and run isMaster on the secondary to see who has become the new primary:\n\n> secondaryDB.isMaster()\n\nThe output from isMaster should look something like this:\n\n{ \"hosts\" : [ \"localhost:27017\", \"localhost:27018\", \"localhost:27019\" ], \"setName\" : \"mdbDefGuide\", \"setVersion\" : 1, \"ismaster\" : true, \"secondary\" : false, \"primary\" : \"localhost:27018\", \"me\" : \"localhost:27019\", \"electionId\" : ObjectId(\"7fffffff0000000000000005\"), \"lastWrite\" : { \"opTime\" : { \"ts\" : Timestamp(1501200681, 1), \"t\" : NumberLong(5) }, \"lastWriteDate\" : ISODate(\"2017-07-28T00:11:21Z\") }, \"maxBsonObjectSize\" : 16777216, \"maxMessageSizeBytes\" : 48000000, \"maxWriteBatchSize\" : 1000, \"localTime\" : ISODate(\"2017-07-28T00:11:28.115Z\"), \"maxWireVersion\" : 6, \"minWireVersion\" : 0, \"readOnly\" : false, \"compression\" : [ \"snappy\" ],\n\nObserving Replication\n\n|\n\n237\n\n\"ok\" : 1, \"operationTime\" : Timestamp(1501200681, 1) }\n\nNote that the primary has switched to 27018. Your primary may be the other server; whichever secondary noticed that the primary was down first will be elected. Now you can send writes to the new primary.\n\nisMaster is a very old command, predating replica sets to when MongoDB only supported master/slave replication. Thus, it does not use the replica set terminology consistently: it still calls the pri‐ mary a “master.” You can generally think of “master” as equivalent to “primary” and “slave” as equivalent to “secondary.”\n\nGo ahead and bring back up the server we had running at localhost:27017. You simply need to find the command-line interface from which you launched it. You’ll see some messages indicating that it terminated. Just run it again using the same command you used to launch it originally.\n\nCongratulations! You just set up, used, and even poked a little at a replica set to force a shutdown and an election for a new primary.\n\nThere are a few key concepts to remember:\n\nClients can send a primary all the same operations they could send a standalone server (reads, writes, commands, index builds, etc.).\n\nClients cannot write to secondaries.\n\nClients, by default, cannot read from secondaries. You can enable this by explic‐ itly setting an “I know I’m reading from a secondary” setting on the connection.\n\nChanging Your Replica Set Configuration Replica set configurations can be changed at any time: members can be added, removed, or modified. There are shell helpers for some common operations. For example, to add a new member to the set, you can use rs.add:\n\n> rs.add(\"localhost:27020\")\n\nSimilarly, you can remove members:\n\n> rs.remove(\"localhost:27017\") { \"ok\" : 1, \"operationTime\" : Timestamp(1501202441, 2) }\n\nYou can check that a reconfiguration succeeded by running rs.config() in the shell. It will print the current configuration:\n\n238\n\n|\n\nChapter 10: Setting Up a Replica Set\n\n> rs.config() { \"_id\" : \"mdbDefGuide\", \"version\" : 3, \"protocolVersion\" : NumberLong(1), \"members\" : [ { \"_id\" : 1, \"host\" : \"localhost:27018\", \"arbiterOnly\" : false, \"buildIndexes\" : true, \"hidden\" : false, \"priority\" : 1, \"tags\" : {\n\n}, \"slaveDelay\" : NumberLong(0), \"votes\" : 1 }, { \"_id\" : 2, \"host\" : \"localhost:27019\", \"arbiterOnly\" : false, \"buildIndexes\" : true, \"hidden\" : false, \"priority\" : 1, \"tags\" : {\n\n}, \"slaveDelay\" : NumberLong(0), \"votes\" : 1 }, { \"_id\" : 3, \"host\" : \"localhost:27020\", \"arbiterOnly\" : false, \"buildIndexes\" : true, \"hidden\" : false, \"priority\" : 1, \"tags\" : {\n\n}, \"slaveDelay\" : NumberLong(0), \"votes\" : 1 } ], \"settings\" : { \"chainingAllowed\" : true, \"heartbeatIntervalMillis\" : 2000, \"heartbeatTimeoutSecs\" : 10, \"electionTimeoutMillis\" : 10000, \"catchUpTimeoutMillis\" : -1,\n\nChanging Your Replica Set Configuration\n\n|\n\n239\n\n\"getLastErrorModes\" : {\n\n}, \"getLastErrorDefaults\" : { \"w\" : 1, \"wtimeout\" : 0 }, \"replicaSetId\" : ObjectId(\"597a49c67e297327b1e5b116\") } }\n\nEach time you change the configuration, the \"version\" field will increase. It starts at version 1.\n\nYou can also modify existing members, not just add and remove them. To make mod‐ ifications, create the configuration document that you want in the shell and call rs.reconfig(). For example, suppose we have a configuration such as the one shown here:\n\n> rs.config() { \"_id\" : \"testReplSet\", \"version\" : 2, \"members\" : [ { \"_id\" : 0, \"host\" : \"198.51.100.1:27017\" }, { \"_id\" : 1, \"host\" : \"localhost:27018\" }, { \"_id\" : 2, \"host\" : \"localhost:27019\" } ] }\n\nSomeone accidentally added member 0 by IP address, instead of its hostname. To change that, first we load the current configuration in the shell and then we change the relevant fields:\n\n> var config = rs.config() > config.members[0].host = \"localhost:27017\"\n\nNow that the config document is correct, we need to send it to the database using the rs.reconfig() helper:\n\n> rs.reconfig(config)\n\n240\n\n|\n\nChapter 10: Setting Up a Replica Set\n\nrs.reconfig() is often more useful than rs.add() and rs.remove() for complex operations, such as modifying members’ configurations or adding/removing multiple members at once. You can use it to make any legal configuration change you need: simply create the config document that represents your desired configuration and pass it to rs.reconfig().\n\nHow to Design a Set To plan out your set, there are certain concepts that you must be familiar with. The next chapter goes into more detail about these, but the most important is that replica sets are all about majorities: you need a majority of members to elect a primary, a pri‐ mary can only stay primary as long as it can reach a majority, and a write is safe when it’s been replicated to a majority. This majority is defined to be “more than half of all members in the set,” as shown in Table 10-1.\n\nTable 10-1. What is a majority?\n\nNumber of members in the set Majority of the set 1 2 3 4 5 6 7\n\n1 2 2 3 3 4 4\n\nNote that it doesn’t matter how many members are down or unavailable; majority is based on the set’s configuration.\n\nFor example, suppose that we have a five-member set and three members go down, as shown in Figure 10-1. There are still two members up. These two members cannot reach a majority of the set (at least three members), so they cannot elect a primary. If one of them were primary, it would step down as soon as it noticed that it could not reach a majority. After a few seconds, your set would consist of two secondaries and three unreachable members.\n\nHow to Design a Set\n\n|\n\n241\n\nFigure 10-1. With a minority of the set available, all members will be secondaries\n\nMany users find this frustrating: why can’t the two remaining members elect a pri‐ mary? The problem is that it’s possible that the other three members didn’t actually go down, and that it was instead the network that went down, as shown in Figure 10-2. In this case, the three members on the left will elect a primary, since they can reach a majority of the set (three members out of five). In the case of a network partition, we do not want both sides of the partition to elect a primary, because then the set would have two primaries. Both primaries would be writing to the database, and the datasets would diverge. Requiring a majority to elect or stay a primary is a neat way of avoid‐ ing ending up with more than one primary.\n\nFigure 10-2. For the members, a network partition looks identical to servers on the other side of the partition going down\n\nIt is important to configure your set in such a way that you’ll usually be able to have one primary. For example, in the five-member set described here, if members 1, 2, and 3 are in one data center and members 4 and 5 are in another, there should almost always be a majority available in the first data center (it’s more likely to have a net‐ work break between data centers than within them).\n\nThere are a couple of common configurations that are recommended:\n\nA majority of the set in one data center, as in Figure 10-2. This is a good design if you have a primary data center where you always want your replica set’s primary to be located. So long as your primary data center is healthy, you will have a\n\n242\n\n|\n\nChapter 10: Setting Up a Replica Set\n\nprimary. However, if that data center becomes unavailable, your secondary data center will not be able to elect a new primary.\n\nAn equal number of servers in each data center, plus a tie-breaking server in a third location. This is a good design if your data centers are “equal” in preference, since generally servers from either data center will be able to see a majority of the set. However, it involves having three separate locations for servers.\n\nMore complex requirements might require different configurations, but you should keep in mind how your set will acquire a majority under adverse conditions.\n\nAll of these complexities would disappear if MongoDB supported having more than one primary. However, this would bring its own host of complexities. With two pri‐ maries, you would have to handle conflicting writes (e.g., if someone updates a docu‐ ment on one primary and someone deletes it on another primary). There are two popular ways of handling conflicts in systems that support multiple writers: manual reconciliation or having the system arbitrarily pick a “winner.” Neither of these options is a very easy model for developers to code against, seeing as you can’t be sure that the data you’ve written won’t change out from under you. Thus, MongoDB chose to only support having a single primary. This makes development easier but can result in periods when the replica set is read-only.\n\nHow Elections Work When a secondary cannot reach a primary, it will contact all the other members and request that it be elected primary. These other members do several sanity checks: Can they reach a primary that the member seeking election cannot? Is the member seek‐ ing election up to date with replication? Is there any member with a higher priority available that should be elected instead?\n\nIn version 3.2, MongoDB introduced version 1 of the replication protocol. Protocol version 1 is based on the RAFT consensus protocol developed by Diego Ongaro and John Ousterhout at Stanford University. It is best described as RAFT-like and is tail‐ ored to include a number of replication concepts that are specific to MongoDB, such as arbiters, priority, nonvoting members, write concern, etc. Protocol version 1 pro‐ vided the foundation for new features such as a shorter failover time and greatly reduces the time to detect false primary situations. It also prevents double voting through the use of term IDs.\n\nRAFT is a consensus algorithm that is broken into relatively inde‐ pendent subproblems. Consensus is the process through which multiple servers or processes agree on values. RAFT ensures con‐ sensus such that the same series of commands produces the same series of results and arrives at the same series of states across the members of a deployment.\n\nHow to Design a Set\n\n|\n\n243\n\nReplica set members send heartbeats (pings) to each other every two seconds. If a heartbeat does not return from a member within 10 seconds, the other members mark the delinquent member as inaccessible. The election algorithm will make a “best-effort” attempt to have the secondary with the highest priority available call an election. Member priority affects both the timing and the outcome of elections; sec‐ ondaries with higher priority call elections relatively sooner than secondaries with lower priority, and are also more likely to win. However, a lower-priority instance can be elected as primary for brief periods, even if a higher-priority secondary is avail‐ able. Replica set members continue to call elections until the highest-priority member available becomes primary.\n\nTo be elected primary, a member must be up to date with replication, as far as the members it can reach know. All replicated operations are strictly ordered by an ascending identifier, so the candidate must have operations later than or equal to those of any member it can reach.\n\nMember Configuration Options The replica sets we have set up so far have been fairly uniform in that every member has the same configuration as every other member. However, there are many situa‐ tions when you don’t want members to be identical: you might want one member to preferentially be primary or make a member invisible to clients so that no read requests can be routed to it. These and many other configuration options can be specified in the member subdocuments of the replica set configuration. This section outlines the member options that you can set.\n\nPriority Priority is an indication of how strongly this member “wants” to become primary. Its value can range from 0 to 100, and the default is 1. Setting \"priority\" to 0 has a spe‐ cial meaning: members with a priority of 0 can never become primary. These are called passive members.\n\nThe highest-priority member will always be elected primary (so long as it can reach a majority of the set and has the most up-to-date data). For example, suppose you add a member with a priority of 1.5 to the set, like so:\n\n> rs.add({\"host\" : \"server-4:27017\", \"priority\" : 1.5})\n\nAssuming the other members of the set have priority 1, once server-4 caught up with the rest of the set, the current primary would automatically step down and server-4 would elect itself. If server-4 was, for some reason, unable to catch up, the current pri‐ mary would stay primary. Setting priorities will never cause your set to go primary- less. It will also never cause a member that is behind to become primary (until it has caught up).\n\n244\n\n|\n\nChapter 10: Setting Up a Replica Set\n\nThe absolute value of \"priority\" only matters in relation to whether it is greater or less than the other priorities in the set: members with priorities of 100, 1, and 1 will behave the same way as members of another set with priorities 2, 1, and 1.\n\nHidden Members Clients do not route requests to hidden members, and hidden members are not pre‐ ferred as replication sources (although they will be used if more desirable sources are not available). Thus, many people will hide less powerful or backup servers.\n\nFor example, suppose you had a set that looked like this:\n\n> rs.isMaster() { ... \"hosts\" : [ \"server-1:27107\", \"server-2:27017\", \"server-3:27017\" ], ... }\n\nTo hide server-3, you could add the hidden: true field to its configuration. A mem‐ ber must have a priority of 0 to be hidden (you can’t have a hidden primary):\n\n> var config = rs.config() > config.members[2].hidden = true 0 > config.members[2].priority = 0 0 > rs.reconfig(config)\n\nNow running isMaster will show:\n\n> rs.isMaster() { ... \"hosts\" : [ \"server-1:27107\", \"server-2:27017\" ], ... }\n\nrs.status() and rs.config() will still show the member; it only disappears from isMaster. When clients connect to a replica set, they call isMaster to determine the members of the set. Thus, hidden members will never be used for read requests.\n\nTo unhide a member, change the hidden option to false or remove the option entirely.\n\nMember Configuration Options\n\n|\n\n245\n\nElection Arbiters A two-member set has clear disadvantages for majority requirements. However, many people with small deployments do not want to keep three copies of their data, feeling that two is enough and that keeping a third copy is not worth the administrative, operational, and financial costs.\n\nFor these deployments, MongoDB supports a special type of member called an arbiter, whose only purpose is to participate in elections. Arbiters hold no data and aren’t used by clients: they just provide a majority for two-member sets. In general, deployments without arbiters are preferable.\n\nAs arbiters don’t have any of the traditional responsibilities of a mongod server, you can run an arbiter as a lightweight process on a wimpier server than you’d generally use for MongoDB. It’s often a good idea, if possible, to run an arbiter in a separate failure domain from the other members, so that it has an “outside perspective” on the set, as described in the deployment recommendations in “How to Design a Set” on page 241.\n\nYou start up an arbiter in the same way that you start a normal mongod, using the --replSet name option and an empty data directory. You can add it to the set using the rs.addArb() helper:\n\n> rs.addArb(\"server-5:27017\")\n\nEquivalently, you can specify configuration:\n\nthe \"arbiterOnly\" option\n\nin\n\nthe member\n\n> rs.add({\"_id\" : 4, \"host\" : \"server-5:27017\", \"arbiterOnly\" : true})\n\nAn arbiter, once added to the set, is an arbiter forever: you cannot reconfigure an arbiter to become a nonarbiter, or vice versa.\n\nOne other thing that arbiters are good for is breaking ties in larger clusters. If you have an even number of nodes, you may have half the nodes vote for one member and half for another. An arbiter can cast the deciding vote. There are a few things to keep in mind when using arbiters, though; we’ll look at these next.\n\nUse at most one arbiter\n\nNote that, in both of the use cases just described, you need at most one arbiter. You do not need an arbiter if you have an odd number of nodes. A common misconception seems to be that you should add extra arbiters “just in case.” However, it doesn’t help elections go any faster or provide any additional data safety to add extra arbiters.\n\nSuppose you have a three-member set. Two members are required to elect a primary. If you add an arbiter, you’ll have a four-member set, so three members will be\n\n246\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "page_number": 245
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 258-267)",
      "start_page": 258,
      "end_page": 267,
      "detection_method": "topic_boundary",
      "content": "required to choose a primary. Thus, your set is potentially less stable: instead of requiring 67% of your set to be up, you’re now requiring 75%.\n\nHaving extra members can also make elections take longer. If you have an even num‐ ber of nodes because you added an arbiter, your arbiters can cause ties, not prevent them.\n\nThe downside to using an arbiter\n\nIf you have a choice between a data node and an arbiter, choose a data node. Using an arbiter instead of a data node in a small set can make some operational tasks more difficult. For example, suppose you are running a replica set with two “normal” mem‐ bers and one arbiter, and one of the data-holding members goes down. If that mem‐ ber is well and truly dead (the data is unrecoverable), you will have to get a copy of the data from the current primary to the new server you’ll be using as a secondary. Copying data can put a lot of stress on a server, and thus slow down your application. (Generally, copying a few gigabytes to a new server is trivial but more than a hundred starts becoming impractical.)\n\nConversely, if you have three data-holding members, there’s more “breathing room” if a server completely dies. You can use the remaining secondary to bootstrap a new server instead of depending on your primary.\n\nIn the two-member-plus-arbiter scenario, the primary is the last remaining good copy of your data and the one trying to handle load from your application while you’re trying to get another copy of your data online.\n\nThus, if possible, use an odd number of “normal” members instead of an arbiter.\n\nIn three-member replica sets with a primary-secondary-arbiter (PSA) architecture or sharded clusters with a three-member PSA shard, there is a known issue with cache pressure increasing if either of the two data-bearing nodes are down and the \"majority\" read concern is enabled. Ideally, you should replace the arbiter with a data-bearing member for these deployments. Alternatively, to prevent storage cache pressure the \"majority\" read concern can be disabled on each of the mongod instances in the deployment or shards.\n\nBuilding Indexes Sometimes a secondary does not need to have the same (or any) indexes that exist on the primary. If you are using a secondary only for backup data or offline batch jobs, you might want to specify \"buildIndexes\" : false in the member’s configuration. This option prevents the secondary from building any indexes.\n\nMember Configuration Options\n\n|\n\n247\n\nThis is a permanent setting: members that have \"buildIndexes\" : false specified can never be reconfigured to be “normal” index-building members again. If you want to change a non-index-building member to an index-building one, you must remove it from the set, delete all of its data, add it to the set again, and allow it to resync from scratch.\n\nAs with hidden members, this option requires the member’s priority to be 0.\n\n248\n\n|\n\nChapter 10: Setting Up a Replica Set\n\nCHAPTER 11 Components of a Replica Set\n\nThis chapter covers how the pieces of a replica set fit together, including:\n\nHow replica set members replicate new data\n\nHow bringing up new members works\n\nHow elections work\n\nPossible server and network failure scenarios\n\nSyncing Replication is concerned with keeping an identical copy of data on multiple servers. The way MongoDB accomplishes this is by keeping a log of operations, or oplog, con‐ taining every write that a primary performs. This is a capped collection that lives in the local database on the primary. The secondaries query this collection for opera‐ tions to replicate.\n\nEach secondary maintains its own oplog, recording each operation it replicates from the primary. This allows any member to be used as a sync source for any other mem‐ ber, as shown in Figure 11-1. Secondaries fetch operations from the member they are syncing from, apply the operations to their dataset, and then write the operations to their oplog. If applying an operation fails (which should only happen if the underly‐ ing data has been corrupted or in some way differs from the primary’s), the secon‐ dary will exit.\n\n249\n\nFigure 11-1. Oplogs keep an ordered list of write operations that have occurred; each member has its own copy of the oplog, which should be identical to the primary’s (mod‐ ulo some lag)\n\nIf a secondary goes down for any reason, when it restarts it will start syncing from the last operation in its oplog. As operations are applied to data and then written to the oplog, the secondary may replay operations that it has already applied to its data. MongoDB is designed to handle this correctly: replaying oplog ops multiple times yields the same result as replaying them once. Each operation in the oplog is idempo‐ tent. That is, oplog operations produce the same results whether applied once or mul‐ tiple times to the target dataset.\n\nBecause the oplog is a fixed size, it can only hold a certain number of operations. In general, the oplog will use space at approximately the same rate as writes come into the system: if you’re writing 1 KB/minute on the primary, your oplog is probably going to fill up at about 1 KB/minute. However, there are a few exceptions: operations that affect multiple documents, such as removes or a multi-updates, will be exploded into many oplog entries. The single operation on the primary will be split into one oplog op per document affected. Thus, if you remove 1,000,000 documents from a collection with db.coll.remove(), it will become 1,000,000 oplog entries removing one document at a time. If you are doing lots of bulk operations, this can fill up your oplog more quickly than you might expect.\n\nIn most cases, the default oplog size is sufficient. If you can predict your replica set’s workload to resemble one of the following patterns, then you might want to create an oplog that is larger than the default. Conversely, if your application predominantly performs reads with a minimal amount of write operations, a smaller oplog may be sufficient. These are the kinds of workloads that might require a larger oplog size:\n\nUpdates to multiple documents at once\n\nThe oplog must translate multi-updates into individual operations in order to maintain idempotency. This can use a great deal of oplog space without a corre‐ sponding increase in data size or disk use.\n\n250\n\n|\n\nChapter 11: Components of a Replica Set\n\nDeletions equal the same amount of data as inserts\n\nIf you delete roughly the same amount of data as you insert, the database will not grow significantly in terms of disk use, but the size of the operation log can be quite large.\n\nSignificant number of in-place updates\n\nIf a significant portion of the workload is updates that do not increase the size of the documents, the database records a large number of operations but the quan‐ tity of data on disk does not change.\n\nBefore mongod creates an oplog, you can specify its size with the oplogSizeMB option. However, after you have started a replica set member for the first time, you can only change the size of the oplog using the “Change the Size of the Oplog” procedure.\n\nMongoDB uses two forms of data synchronization: an initial sync to populate new members with the full dataset, and replication to apply ongoing changes to the entire dataset. Let’s take a closer look at each of these.\n\nInitial Sync MongoDB performs an initial sync to copy all the data from one member of the rep‐ lica set to another member. When a member of the set starts up, it will check if it is in a valid state to begin syncing from someone. If it is in a valid state, it will attempt to make a full copy of the data from another member of the set. There are several steps to the process, which you can follow in the mongod’s log.\n\nFirst, MongoDB clones all databases except the local database. The mongod scans every collection in each source database and inserts all the data into its own copies of these collections on the target member. Prior to beginning the clone operations, any existing data on the target member will be dropped.\n\nOnly do an initial sync for a member if you do not want the data in your data directory or have moved it elsewhere, as mongod’s first action is to delete it all.\n\nIn MongoDB 3.4 and later, the initial sync builds all the collection indexes as the documents are copied for each collection (in earlier versions, only the \"_id\" indexes are built during this stage). It also pulls newly added oplog records during the data copy, so you should ensure that the target member has enough disk space in the local database to store these records during this data copy stage.\n\nOnce all the databases are cloned, the mongod uses the oplog from the source to update its dataset to reflect the current state of the replica set, applying all changes to the dataset that occurred while the copy was in progress. These changes might\n\nSyncing\n\n|\n\n251\n\ninclude any type of write (inserts, updates, and deletes), and this process might mean that mongod has to reclone certain documents that were moved and therefore missed by the cloner.\n\nThis is roughly what the logs will look like if some documents had to be recloned. Depending on the level of traffic and the types of operations that where happening on the sync source, you may or may not have missing objects:\n\nMon Jan 30 15:38:36 [rsSync] oplog sync 1 of 3 Mon Jan 30 15:38:36 [rsBackgroundSync] replSet syncing to: server-1:27017 Mon Jan 30 15:38:37 [rsSyncNotifier] replset setting oplog notifier to server-1:27017 Mon Jan 30 15:38:37 [repl writer worker 2] replication update of non-mod failed: { ts: Timestamp 1352215827000|17, h: -5618036261007523082, v: 2, op: \"u\", ns: \"db1.someColl\", o2: { _id: ObjectId('50992a2a7852201e750012b7') }, o: { $set: { count.0: 2, count.1: 0 } } } Mon Jan 30 15:38:37 [repl writer worker 2] replication info adding missing object Mon Jan 30 15:38:37 [repl writer worker 2] replication missing object not found on source. presumably deleted later in oplog\n\nAt this point, the data should exactly match the dataset as it existed at some point on the primary. The member finishes the initial sync process and transitions to normal syncing, which allows it to become a secondary.\n\nDoing an initial sync is very easy from an operator’s perspective: just start up a mon‐ god with a clean data directory. However, it is often preferable to restore from a backup instead, as covered in Chapter 23. Restoring from a backup is often faster than copying all of your data through mongod.\n\nAlso, cloning can ruin the sync source’s working set. Many deployments end up with a subset of their data that’s frequently accessed and always in memory (because the OS is accessing it often). Performing an initial sync forces the member to page all of its data into memory, evicting the frequently used data. This can slow down a mem‐ ber dramatically as requests that were being handled by data in RAM are suddenly forced to go to disk. However, for small datasets and servers with some breathing room, initial syncing is a good, easy option.\n\nOne of the most common issues people run into with initial sync is it taking too long. In these cases, the new member can “fall off” the end of sync source’s oplog: it gets so far behind the sync source that it can no longer catch up because the sync source’s oplog has overwritten the data the member would need to use to continue replicating.\n\nThere is no way to fix this other than attempting the initial sync at a less busy time or restoring from a backup. The initial sync cannot proceed if the member has fallen off of the sync source’s oplog. “Handling Staleness” on page 253 covers this in more depth.\n\n252\n\n|\n\nChapter 11: Components of a Replica Set\n\nReplication The second type of synchronization MongoDB performs is replication. Secondary members replicate data continuously after the initial sync. They copy the oplog from their sync source and apply these operations in an asynchronous process. Secondaries may automatically change their sync-from source as needed, in response to changes in the ping time and the state of other members’ replication. There are several rules that govern which members a given node can sync from. For example, replica set members with one vote cannot sync from members with zero votes, and secondaries avoid syncing from delayed members and hidden members. Elections and different classes of replica set members are discussed in later sections.\n\nHandling Staleness If a secondary falls too far behind the actual operations being performed on the sync source, the secondary will go stale. A stale secondary is unable to catch up because every operation in the sync source’s oplog is too far ahead: it would be skipping oper‐ ations if it continued to sync. This could happen if the secondary has had downtime, has more writes than it can handle, or is too busy handling reads.\n\nWhen a secondary goes stale, it will attempt to replicate from each member of the set in turn to see if there’s anyone with a longer oplog that it can bootstrap from. If there is no one with a long-enough oplog, replication on that member will halt and it will need to be fully resynced (or restored from a more recent backup).\n\nTo avoid out-of-sync secondaries, it’s important to have a large oplog so that the pri‐ mary can store a long history of operations. A larger oplog will obviously use more disk space, but in general this is a good tradeoff to make because disk space tends to be cheap and little of the oplog is typically in use, so it doesn’t take up much RAM. A general rule of thumb is that the oplog should provide coverage (replication window) for two to three days’ worth of normal operations. For more information on sizing the oplog, see “Resizing the Oplog” on page 282.\n\nHeartbeats Members need to know about the other members’ states: who’s primary, who they can sync from, and who’s down. To keep an up-to-date view of the set, a member sends out a heartbeat request to every other member of the set every two seconds. A heart‐ beat request is a short message that checks everyone’s state.\n\nOne of the most important functions of heartbeats is to let the primary know if it can reach a majority of the set. If a primary can no longer reach a majority of the servers, it will demote itself and become a secondary (see “How to Design a Set” on page 241).\n\nHeartbeats\n\n|\n\n253\n\nMember States Members also communicate what state they are in via heartbeats. We’ve already dis‐ cussed two states: primary and secondary. There are several other normal states that you’ll often see members be in:\n\nSTARTUP\n\nThis is the state a member is in when it’s first started, while MongoDB is attempt‐ ing to load its replica set configuration. Once the configuration has been loaded, it transitions to STARTUP2.\n\nSTARTUP2\n\nThis state lasts throughout the initial sync process, which typically takes just a few seconds. The member forks off a couple of threads to handle replication and elections and then transitions into the next state: RECOVERING.\n\nRECOVERING\n\nThis state indicates that the member is operating correctly but is not available for reads. You may see it in a variety of situations.\n\nOn startup, a member has to make a few checks to make sure it’s in a valid state before accepting reads; therefore, all members go through the RECOVERING state briefly on startup before becoming secondaries. A member can also go into this state during long-running operations such as compacting or in response to the replSetMaintenance command.\n\nA member will also go into the RECOVERING state if it has fallen too far behind the other members to catch up. This is, generally, a failure state that requires resyncing the member. The member does not go into an error state at this point because it lives in hope that someone will come online with a long-enough oplog that it can bootstrap itself back to non-staleness.\n\nARBITER\n\nArbiters (see “Election Arbiters” on page 246) have a special state and should always be in this state during normal operation.\n\nThere are also a few states that indicate a problem with the system. These include:\n\nDOWN\n\nIf a member was up but then becomes unreachable, it will enter this state. Note that a member reported as “down” might, in fact, still be up, just unreachable due to network issues.\n\nUNKNOWN\n\nIf a member has never been able to reach another member, it will not know what state it’s in, so it will report it as UNKNOWN. This generally indicates that the\n\n254\n\n|\n\nChapter 11: Components of a Replica Set\n\nunknown member is down or that there are network problems between the two members.\n\nREMOVED\n\nThis is the state of a member that has been removed from the set. If a removed member is added back into the set, it will transition back into its “normal” state.\n\nROLLBACK\n\nThis state is used when a member is rolling back data, as described in “Rollbacks” on page 255. At the end of the rollback process, a server will transition back into the RECOVERING state and then become a secondary.\n\nElections A member will seek election if it cannot reach a primary (and is itself eligible to become primary). A member seeking election will send out a notice to all of the members it can reach. These members may know why this member is an unsuitable primary: it may be behind in replication or there may already be a primary that the member seeking election cannot reach. In these cases, the other members will vote against the candidate.\n\nAssuming that there is no reason to object, the other members will vote for the mem‐ ber seeking election. If the member seeking election receives votes from a majority of the set, the election was successful and the member will transition into PRIMARY state. If it did not receive a majority if votes, it will remain a secondary and may try to become a primary again later. A primary will remain primary until it cannot reach a majority of members, goes down, or is stepped down, or the set is reconfigured.\n\nAssuming that the network is healthy and a majority of the servers are up, elections should be fast. It will take a member up to two seconds to notice that a primary has gone down (due to the heartbeats mentioned earlier) and it will immediately start an election, which should only take a few milliseconds. However, the situation is often nonoptimal: an election may be triggered due to networking issues or overloaded servers responding too slowly. In these cases, an election might take more time—even up to a few minutes.\n\nRollbacks The election process described in the previous section means that if a primary does a write and goes down before the secondaries have a chance to replicate it, the next pri‐ mary elected may not have the write. For example, suppose we have two data centers, one with the primary and a secondary, and the other with three secondaries, as shown in Figure 11-2.\n\nElections\n\n|\n\n255\n\nFigure 11-2. A possible two-data-center configuration\n\nSuppose that there is a network partition between the two data centers, as shown in Figure 11-3. The servers in the first data center are up to operation 126, but that data center hasn’t yet replicated to the servers in the other data center.\n\nFigure 11-3. Replication across data centers can be slower than within a single data center\n\nThe servers in the other data center can still reach a majority of the set (three out of five servers). Thus, one of them may be elected primary. This new primary begins taking its own writes, as shown in Figure 11-4.\n\nFigure 11-4. Unreplicated writes won’t match writes on the other side of a network partition\n\n256\n\n|\n\nChapter 11: Components of a Replica Set",
      "page_number": 258
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 268-277)",
      "start_page": 268,
      "end_page": 277,
      "detection_method": "topic_boundary",
      "content": "When the network is repaired, the servers in the first data center will look for opera‐ tion 126 to start syncing from the other servers, but will not be able to find it. When this happens, A and B will begin a process called rollback. Rollback is used to undo ops that were not replicated before failover. The servers with 126 in their oplogs will look back through the oplogs of the servers in the other data center for a common point. They’ll find that operation 125 is the latest operation that matches. Figure 11-5 shows what the oplogs would look like. A apparently crashed before replicating ops 126−128, so these operations are not present on B, which has more recent operations. A will have to roll back these three operations before resuming syncing.\n\nFigure 11-5. Two members with conflicting oplogs—the last common op was 125, so as B has more recent operations A will need to roll back ops 126-128\n\nAt this point, the server will go through the ops it has and write its version of each document affected by those ops to a .bson file in a rollback directory of your data directory. Thus, if (for example) operation 126 was an update, it will write the docu‐ ment updated by 126 to <collectionName>.bson. Then it will copy the version of that document from the current primary.\n\nThe following is a paste of the log entries generated from a typical rollback:\n\nFri Oct 7 06:30:35 [rsSync] replSet syncing to: server-1 Fri Oct 7 06:30:35 [rsSync] replSet our last op time written: Oct 7 06:30:05:3 Fri Oct 7 06:30:35 [rsSync] replset source's GTE: Oct 7 06:30:31:1 Fri Oct 7 06:30:35 [rsSync] replSet rollback 0 Fri Oct 7 06:30:35 [rsSync] replSet ROLLBACK Fri Oct 7 06:30:35 [rsSync] replSet rollback 1 Fri Oct 7 06:30:35 [rsSync] replSet rollback 2 FindCommonPoint Fri Oct 7 06:30:35 [rsSync] replSet info rollback our last optime: Oct 7 06:30:05:3 Fri Oct 7 06:30:35 [rsSync] replSet info rollback their last optime: Oct 7 06:30:31:2 Fri Oct 7 06:30:35 [rsSync] replSet info rollback diff in end of log times: -26 seconds Fri Oct 7 06:30:35 [rsSync] replSet rollback found matching events at Oct 7 06:30:03:4118 Fri Oct 7 06:30:35 [rsSync] replSet rollback findcommonpoint scanned : 6 Fri Oct 7 06:30:35 [rsSync] replSet replSet rollback 3 fixup Fri Oct 7 06:30:35 [rsSync] replSet rollback 3.5 Fri Oct 7 06:30:35 [rsSync] replSet rollback 4 n:3\n\nRollbacks\n\n|\n\n257\n\nFri Oct 7 06:30:35 [rsSync] replSet minvalid=Oct 7 06:30:31 4e8ed4c7:2 Fri Oct 7 06:30:35 [rsSync] replSet rollback 4.6 Fri Oct 7 06:30:35 [rsSync] replSet rollback 4.7 Fri Oct 7 06:30:35 [rsSync] replSet rollback 5 d:6 u:0 Fri Oct 7 06:30:35 [rsSync] replSet rollback 6 Fri Oct 7 06:30:35 [rsSync] replSet rollback 7 Fri Oct 7 06:30:35 [rsSync] replSet rollback done Fri Oct 7 06:30:35 [rsSync] replSet RECOVERING Fri Oct 7 06:30:36 [rsSync] replSet syncing to: server-1 Fri Oct 7 06:30:36 [rsSync] replSet SECONDARY\n\nThe server begins syncing from another member (server-1, in this case) and realizes that it cannot find its latest operation on the sync source. At that point, it starts the rollback process by going into the ROLLBACK state (replSet ROLLBACK).\n\nAt step 2, it finds the common point between the two oplogs, which was 26 seconds ago. It then begins undoing the operations from the last 26 seconds from its oplog. Once the rollback is complete, it transitions into the RECOVERING state and begins syncing normally again.\n\nTo apply operations that have been rolled back to the current primary, first use mon‐ gorestore to load them into a temporary collection:\n\n$ mongorestore --db stage --collection stuff \\ /data/db/rollback/important.stuff.2018-12-19T18-27-14.0.bson\n\nThen examine the documents (using the shell) and compare them to the current con‐ tents of the collection from whence they came. For example, if someone had created a “normal” index on the rollback member and a unique index on the current primary, you’d want to make sure that there weren’t any duplicates in the rolled-back data and resolve them if there were.\n\nOnce you have a version of the documents that you like in your staging collection, load it into your main collection:\n\n> staging.stuff.find().forEach(function(doc) { ... prod.stuff.insert(doc); ... })\n\nIf you have any insert-only collections, you can directly load the rollback documents into the collection. However, if you are doing updates on the collection you will need to be more careful about how you merge rollback data.\n\nOne often-misused member configuration option is the number of votes each mem‐ ber has. Manipulating the number of votes is almost always not what you want to do and causes a lot of rollbacks (which is why it was not included in the list of member configuration options in the last chapter). Do not change the number of votes unless you are prepared to deal with regular rollbacks.\n\nFor more information on preventing rollbacks, see Chapter 12.\n\n258\n\n|\n\nChapter 11: Components of a Replica Set\n\nWhen Rollbacks Fail In older versions of MongoDB, it could decide that the rollback was too large to undertake. Since MongoDB version 4.0, there is no limit on the amount of data that can be rolled back. A rollback in versions before 4.0 can fail if there are more than 300 MB of data or about 30 minutes of operations to roll back. In these cases, you must resync the node that is stuck in rollback.\n\nThe most common cause of this is when secondaries are lagging and the primary goes down. If one of the secondaries becomes primary, it will be missing a lot of oper‐ ations from the old primary. The best way to make sure you don’t get a member stuck in rollback is to keep your secondaries as up to date as possible.\n\nRollbacks\n\n|\n\n259\n\nCHAPTER 12 Connecting to a Replica Set from Your Application\n\nThis chapter covers how applications interact with replica sets, including:\n\nHow connections and failovers work\n\nWaiting for replication on writes\n\nRouting reads to the correct member\n\nClient−to−Replica Set Connection Behavior MongoDB client libraries (“drivers” in MongoDB parlance) are designed to manage communication with MongoDB servers, regardless of whether the server is a stand‐ alone MongoDB instance or a replica set. For replica sets, by default, drivers will con‐ nect to the primary and route all traffic to it. Your application can perform reads and writes as though it were talking to a standalone server while your replica set quietly keeps hot standbys ready in the background.\n\nConnections to a replica set are similar to connections to a single server. Use the MongoClient class (or equivalent) in your driver and provide a seed list for the driver to connect to. A seed list is simply a list of server addresses. Seeds are members of the replica set your application will read from and write data to. You do not need to list all members in the seed list (although you can). When the driver connects to the seeds, it will discover the other members from them. A connection string usually looks something like this:\n\n\"mongodb://server-1:27017,server-2:27017,server-3:27017\"\n\nSee your driver’s documentation for details.\n\n261\n\nTo provide further resilience, you should also use the DNS Seedlist Connection for‐ mat to specify how your applications connect to your replica set. The advantage to using DNS is that servers hosting your MongoDB replica set members can be changed in rotation without needing to reconfigure the clients (specifically, their con‐ nection strings).\n\nAll MongoDB drivers adhere to the server discovery and monitoring (SDAM) spec. They persistently monitor the topology of your replica set to detect any changes in your application’s ability to reach all members of the set. In addition, the drivers monitor the set to maintain information on which member is the primary.\n\nThe purpose of replica sets is to make your data highly available in the face of net‐ work partitions or servers going down. In ordinary circumstances, replica sets respond gracefully to such problems by electing a new primary so that applications can continue to read and write data. If a primary goes down, the driver will automati‐ cally find the new primary (once one is elected) and will route requests to it as soon as possible. However, while there is no reachable primary, your application will be unable to perform writes.\n\nThere may be no primary available for a brief time (during an election) or for an extended period of time (if no reachable member can become primary). By default, the driver will not service any requests—read or write—during this period. If neces‐ sary to your application, you can configure the driver to use secondaries for read requests.\n\nA common desire is to have the driver hide the entire election process (the primary going away and a new primary being elected) from the user. However, no driver han‐ dles failover this way, for a few reasons. First, a driver can only hide a lack of primary for so long. Second, a driver often finds out that the primary went down because an operation failed, which means that the driver doesn’t know whether or not the pri‐ mary processed the operation before going down. This is a fundamental distributed systems problem that is impossible to avoid, so we need a strategy for dealing with it when it emerges. Should we retry the operation on the new primary, if one is elected quickly? Assume it got through on the old primary? Check and see if the new pri‐ mary has the operation?\n\nThe correct strategy, it turns out, is to retry at most one time. Huh? To explain, let’s consider our options. These boil down to the following: don’t retry, give up after retrying some fixed number of times, or retry at most once. We also need to consider the type of error that could be the source of our problem. There are three types of errors we might see in attempting to write to a replica set: a transient network error, a persistent outage (either network or server), or an error caused by a command the server rejects as incorrect (e.g., not authorized). For each type of error, let’s consider our retry options.\n\n262\n\n|\n\nChapter 12: Connecting to a Replica Set from Your Application\n\nFor the sake of this discussion, let’s look at the example of a write to simply increment a counter. If our application attempts to increment our counter but gets no response from the server, we don’t know whether the server received the message and per‐ formed the update. So, if we follow a strategy of not retrying this write, for a transient network error, we might undercount. For a persistent outage or a command error not retrying is the correct strategy, because no amount of retrying the write operation will have the desired effect.\n\nIf we follow a strategy of retrying some fixed number of times, for transient network errors, we might overcount (in the case where our first attempt succeeded). For a per‐ sistent outage or command error, retrying multiple times will simply waste cycles.\n\nLet’s look now at the strategy of retrying just once. For a transient network error, we might overcount. For a persistent outage or command error, this is the correct strat‐ egy. However, what if we could ensure that our operations are idempotent? Idempo‐ tent operations are those that have the same outcome whether we do them once or multiple times. With idempotent operations, retrying network errors once has the best chance of correctly dealing with all three types of errors.\n\nAs of MongoDB 3.6, the server and all MongoDB drivers support a retryable writes option. See your driver’s documentation for details on how to use this option. With retryable writes, the driver will automatically follow the retry-at-most-once strategy. Command errors will be returned to the application for client-side handling. Net‐ work errors will be retried once after an appropriate delay that should accommodate a primary election under ordinary circumstances. With retryable writes turned on, the server maintains a unique identifier for each write operation and can therefore determine when the driver is attempting to retry a command that already succeeded. Rather than apply the write again, it will simply return a message indicating the write succeeded and thereby overcome the problem caused by the transient network issue.\n\nWaiting for Replication on Writes Depending on the needs of your application, you might want to require that all writes are replicated to a majority of the replica set before they are acknowledged by the server. In the rare circumstance where the primary of a set goes down and the newly elected primary (formerly a secondary) did not replicate the very last writes to the former primary, those writes will be rolled back when the former primary comes back up. They can be recovered, but it requires manual intervention. For many applica‐ tions, having a small number of writes rolled back is not a problem. In a blog applica‐ tion, for example, there is little real danger in rolling back one or two comments from one reader.\n\nHowever, for other applications, rollback of any writes should be avoided. Suppose your application sends a write to the primary. It receives confirmation that the write\n\nWaiting for Replication on Writes\n\n|\n\n263\n\nwas written, but the primary crashes before any secondaries have had a chance to replicate that write. Your application thinks that it’ll be able to access that write, but the current members of the replica set don’t have a copy of it.\n\nAt some point, a secondary may be elected primary and start taking new writes. When the former primary comes back up, it will discover that it has writes that the current primary does not. To correct this, it will undo any writes that do not match the sequence of operations on the current primary. These operations are not lost, but they are written to special rollback files that have to be manually applied to the cur‐ rent primary. MongoDB cannot automatically apply these writes, since they may con‐ flict with other writes that have happened since the crash. Thus, the writes essentially disappear until an admin gets a chance to apply the rollback files to the current pri‐ mary (see Chapter 11 for more details on rollbacks).\n\nThe requirement of writing to a majority prevents this situation: if the application gets a confirmation that a write succeeded, then the new primary will have to have a copy of the write to be elected (a member must be up to date to be elected primary). If the application does not receive acknowledgment from the server or receives an error, then it will know to try again, because the write was not propagated to a major‐ ity of the set before the primary crashed.\n\nThus, to ensure that writes will be persisted no matter what happens to the set, we must ensure that each write propagates to a majority of the members of the set. We can achieve this using writeConcern.\n\nAs of MongoDB 2.6, writeConcern is integrated with write operations. For example, in JavaScript, we can use writeConcern as follows:\n\ntry { db.products.insertOne( { \"_id\": 10, \"item\": \"envelopes\", \"qty\": 100, type: \"Self-Sealing\" }, { writeConcern: { \"w\" : \"majority\", \"wtimeout\" : 100 } } ); } catch (e) { print (e); }\n\nThe specific syntax in your driver will vary depending on the programming language, but the semantics remain the same. In the example here, we specify a write concern of \"majority\". Upon success, the server will respond with a message such as the following:\n\n{ \"acknowledged\" : true, \"insertedId\" : 10 }\n\nBut the server will not respond until this write operation has replicated to a majority of the members of the replica set. Only then will our application receive acknowledg‐ ment that this write succeeded. If the write does not succeed within the timeout we’ve specified, the server will respond with an error message:\n\n264\n\n|\n\nChapter 12: Connecting to a Replica Set from Your Application\n\nWriteConcernError({ \"code\" : 64, \"errInfo\" : { \"wtimeout\" : true }, \"errmsg\" : \"waiting for replication timed out\" })\n\nWrite concern majority and the replica set election protocol ensure that in the event of a primary election, only secondaries that are up to date with acknowledged writes can be elected primary. In this way, we guarantee that rollback will not happen. With the timeout option, we also have a tunable setting that enables us to detect and flag any long-running writes at the application layer.\n\nOther Options for “w” \"majority\" is not the only writeConcern option. MongoDB also lets you specify an arbitrary number of servers to replicate to by passing \"w\" a number, as shown here:\n\ndb.products.insertOne( { \"_id\": 10, \"item\": \"envelopes\", \"qty\": 100, type: \"Self-Sealing\" }, { writeConcern: { \"w\" : 2, \"wtimeout\" : 100 } } );\n\nThis will wait until two members (the primary and one secondary) have the write.\n\nNote that the \"w\" value includes the primary. If you want the write propagated to n secondaries, you should set \"w\" to n+1 (to include the primary). Setting \"w\" : 1 is the same as not passing the \"w\" option at all because it just checks that the write was successful on the primary.\n\nThe downside to using a literal number is that you have to change your application if your replica set configuration changes.\n\nCustom Replication Guarantees Writing to a majority of a set is considered “safe.” However, some sets may have more complex requirements: you may want to make sure that a write makes it to at least one server in each data center or a majority of the nonhidden nodes. Replica sets allow you to create custom rules that you can pass to \"getLastError\" to guarantee replication to whatever combination of servers you need.\n\nGuaranteeing One Server per Data Center Network issues between data centers are much more common than within data cen‐ ters, and it is more likely for an entire data center to go dark than an equivalent smat‐ tering of servers across multiple data centers. Thus, you might want some data center −specific logic for writes. Guaranteeing a write to every data center before confirming\n\nCustom Replication Guarantees\n\n|\n\n265\n\nsuccess means that, in the case of a write followed by the data center going offline, every other data center will have at least one local copy.\n\nTo set this up, we first classify the members by data center. We do this by adding a \"tags\" field to their replica set configuration:\n\n> var config = rs.config() > config.members[0].tags = {\"dc\" : \"us-east\"} > config.members[1].tags = {\"dc\" : \"us-east\"} > config.members[2].tags = {\"dc\" : \"us-east\"} > config.members[3].tags = {\"dc\" : \"us-east\"} > config.members[4].tags = {\"dc\" : \"us-west\"} > config.members[5].tags = {\"dc\" : \"us-west\"} > config.members[6].tags = {\"dc\" : \"us-west\"}\n\nThe \"tags\" field is an object, and each member can have multiple tags. It might be a “high quality” server in the \"us-east\" data center, for example, in which case we’d want a \"tags\" field such as {\"dc\": \"us-east\", \"quality\" : \"high\"}.\n\nThe second step is to add a rule by creating a \"getLastErrorModes\" field in our rep‐ lica set config. The name \"getLastErrorModes\" is vestigial in the sense that prior to MongoDB 2.6, applications used a method called \"getLastError\" to specify write concern. In replica configs, for \"getLastErrorModes\" each rule is of the form \"name\" : {\"key\" : number}}. \"name\" is the name for the rule, which should describe what the rule does in a way that clients can understand, as they’ll be using this name when they call getLastError. In this example, we might call this rule \"eachDC\" or something more abstract such as \"user-level safe\".\n\nThe \"key\" field is the key field from the tags, so in this example it will be \"dc\". The number is the number of groups that are needed to fulfill this rule. In this case, number is 2 (because we want at least one server from \"us-east\" and one from \"us-west\"). number always means “at least one server from each of number groups.”\n\nWe add \"getLastErrorModes\" to the replica set config as follows and reconfigure to create the rule:\n\n> config.settings = {} > config.settings.getLastErrorModes = [{\"eachDC\" : {\"dc\" : 2}}] > rs.reconfig(config)\n\n\"getLastErrorModes\" lives in the \"settings\" subobject of a replica set config, which contains a few set-level optional settings.\n\nNow we can use this rule for writes:\n\ndb.products.insertOne( { \"_id\": 10, \"item\": \"envelopes\", \"qty\": 100, type: \"Self-Sealing\" }, { writeConcern: { \"w\" : \"eachDC\", wtimeout : 1000 } } );\n\n266\n\n|\n\nChapter 12: Connecting to a Replica Set from Your Application\n\nNote that rules are somewhat abstracted away from the application developer: they don’t have to know which servers are in \"eachDC\" to use the rule, and the rule can change without their application having to change. We could add a data center or change set members and the application would not have to know.\n\nGuaranteeing a Majority of Nonhidden Members Often, hidden members are somewhat second-class citizens: you’re never going to fail over to them and they certainly aren’t taking any reads. Thus, you may only care that nonhidden members received a write and let the hidden members sort it out for themselves.\n\nSuppose we have five members, host0 through host4, host4 being a hidden member. We want to make sure that a majority of the nonhidden members have a write—that is, at least three of host0, host1, host2, and host3. To create a rule for this, first we tag each of the nonhidden members with its own tag:\n\n> var config = rs.config() > config.members[0].tags = [{\"normal\" : \"A\"}] > config.members[1].tags = [{\"normal\" : \"B\"}] > config.members[2].tags = [{\"normal\" : \"C\"}] > config.members[3].tags = [{\"normal\" : \"D\"}]\n\nThe hidden member, host4, is not given a tag.\n\nNow we add a rule for the majority of these servers:\n\n> config.settings.getLastErrorModes = [{\"visibleMajority\" : {\"normal\" : 3}}] > rs.reconfig(config)\n\nFinally, we can use this rule in our application:\n\ndb.products.insertOne( { \"_id\": 10, \"item\": \"envelopes\", \"qty\": 100, type: \"Self-Sealing\" }, { writeConcern: { \"w\" : \"visibleMajority\", wtimeout : 1000 } } );\n\nThis will wait until at least three of the nonhidden members have the write.\n\nCreating Other Guarantees The rules you can create are limitless. Remember that there are two steps to creating a custom replication rule:\n\n1. Tag members by assigning them key/value pairs. The keys describe classifica‐ tions; for example, you might have keys such as \"data_center\" or \"region\" or \"serverQuality\". Values determine which group a server belongs to within a classification. For example, for the key \"data_center\", you might have some servers tagged \"us-east\", some \"us-west\", and others \"aust\".\n\nCustom Replication Guarantees\n\n|\n\n267",
      "page_number": 268
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 278-287)",
      "start_page": 278,
      "end_page": 287,
      "detection_method": "topic_boundary",
      "content": "2. Create a rule based on the classifications you create. Rules are always of the form {\"name\" : {\"key\" : number}}, where at least one server from number groups must have a write before it has succeeded. For example, you could create a rule {\"twoDCs\" : {\"data_center\" : 2}}, which would mean that at least one server in two of the data centers tagged must confirm a write before it is successful.\n\nThen you can use this rule in getLastErrorModes.\n\nRules are immensely powerful ways to configure replication, although they are com‐ plex to understand and set up. Unless you have fairly involved replication require‐ ments, you should be perfectly safe sticking with \"w\" : \"majority\".\n\nSending Reads to Secondaries By default, drivers will route all requests to the primary. This is generally what you want, but you can configure other options by setting read preferences in your driver. Read preferences let you specify the types of servers queries should be sent to.\n\nSending read requests to secondaries is generally a bad idea. There are some specific situations in which it makes sense, but you should generally send all traffic to the pri‐ mary. If you are considering sending reads to secondaries, make sure to weigh the pros and cons very carefully before allowing it. This section covers why it’s a bad idea and the specific conditions when it makes sense to do so.\n\nConsistency Considerations Applications that require strongly consistent reads should not read from secondaries.\n\nSecondaries should usually be within a few milliseconds of the primary. However, there is no guarantee of this. Sometimes secondaries can fall behind by minutes, hours, or even days due to load, misconfiguration, network errors, or other issues. Client libraries cannot tell how up to date a secondary is, so clients will cheerfully send queries to secondaries that are far behind. Hiding a secondary from client reads can be done but is a manual process. Thus, if your application needs data that is pre‐ dictably up to date, it should not read from secondaries.\n\nIf your application needs to read its own writes (e.g., insert a document and then query for it and find it) you should not send the read to a secondary (unless the write waits for replication to all secondaries using \"w\" as shown earlier). Otherwise, an application may perform a successful write, attempt to read the value, and not be able to find it (because it sent the read to a secondary that hasn’t replicated yet). Clients can issue requests faster than replication can copy operations.\n\nTo always send read requests to the primary, set your read preference to primary (or leave it alone, since primary is the default). If there is no primary, queries will error\n\n268\n\n|\n\nChapter 12: Connecting to a Replica Set from Your Application\n\nout. This means that your application cannot perform queries if the primary goes down. However, it is certainly an acceptable option if your application can deal with downtime during failovers or network partitions or if getting stale data is unacceptable.\n\nLoad Considerations Many users send reads to secondaries to distribute load. For example, if your servers can only handle 10,000 queries a second and you need to handle 30,000, you might set up a couple of secondaries and have them take some of the load. However, this is a dangerous way to scale because it’s easy to accidentally overload your system and dif‐ ficult to recover from once you do.\n\nFor example, suppose that you have the situation just described: 30,000 reads per sec‐ ond. You decide to create a replica set with four members (one of these would be con‐ figured as nonvoting, to prevent ties in elections) to handle this: each secondary is well below its maximum load and the system works perfectly.\n\nUntil one of the secondaries crashes.\n\nNow each of the remaining members are handling 100% of their possible load. If you need to rebuild the member that crashed, it may need to copy data from one of the other servers, overwhelming the remaining servers. Overloading a server often makes it perform slower, lowering the set’s capacity even further and forcing other members to take on more load, causing them to slow down in a death spiral.\n\nOverloading can also cause replication to slow down, making the remaining seconda‐ ries fall behind. Suddenly you have a member down and a member lagging, and everything is too overloaded to have any wiggle room.\n\nIf you have a good idea of how much load a server can take, you might feel like you can plan this out better: use five servers instead of four and the set won’t be overloa‐ ded if one goes down. However, even if you plan it out perfectly (and only lose the number of servers you expected), you still have to fix the situation with the other servers under more stress than they would be otherwise.\n\nA better choice is to use sharding to distribute load. We’ll cover how to set sharding up in Chapter 14.\n\nReasons to Read from Secondaries There are a few cases in which it’s reasonable to send application reads to secondaries. For instance, you may want your application to still be able to perform reads if the primary goes down (and you do not care if those reads are somewhat stale). This is the most common case for distributing reads to secondaries: you’d like a temporary\n\nSending Reads to Secondaries\n\n|\n\n269\n\nread-only mode when your set loses a primary. This read preference is called primary Preferred.\n\nOne common argument for reading from secondaries is to get low-latency reads. You can specify nearest as your read preference to route requests to the lowest-latency member based on average ping time from the driver to the replica set member. If your application needs to access the same document with low latency in multiple data centers, this is the only way to do it. If, however, your documents are more location- based (application servers in this data center need low-latency access to some of your data, or application servers in another data center need low-latency access to other data), this should be done with sharding. Note that you must use sharding if your application requires low-latency reads and low-latency writes: replica sets only allow writes to one location (wherever the primary is).\n\nYou must be willing to sacrifice consistency if you are reading from members that may not have replicated all the writes yet. Alternatively, you could sacrifice write speed if you wanted to wait until writes had been replicated to all members.\n\nIf your application can truly function acceptably with arbitrarily stale data, you can use the secondary or secondaryPreferred read preferences. secondary will always send read requests to a secondary. If there are no secondaries available, this will error out rather than send reads to the primary. It can be used for applications that do not care about stale data and want to use the primary for writes only. If you have any con‐ cerns about staleness of data, this is not recommended.\n\nsecondaryPreferred will send read requests to a secondary if one is available. If no secondaries are available, requests will be sent to the primary.\n\nSometimes, read load is drastically different than write load—i.e., you’re reading entirely different data than you’re writing. You might want dozens of indexes for off‐ line processing that you don’t want to have on the primary. In this case, you might want to set up a secondary with different indexes than the primary. If you’d like to use a secondary for this purpose, you’d probably create a connection directly to it from the driver, instead of using a replica set connection.\n\nConsider which of the options makes sense for your application. You can also com‐ bine options: if some read requests must be from the primary, use primary for those. If you are OK with other reads not having the most up-to-date data, use primaryPre ferred for those. And if certain requests require low latency over consistency, use nearest for those.\n\n270\n\n|\n\nChapter 12: Connecting to a Replica Set from Your Application\n\nCHAPTER 13 Administration\n\nThis chapter covers replica set administration, including:\n\nPerforming maintenance on individual members\n\nConfiguring sets under a variety of circumstances\n\nGetting information about and resizing your oplog\n\nDoing some more exotic set configurations\n\nConverting from master/slave to a replica set\n\nStarting Members in Standalone Mode A lot of maintenance tasks cannot be performed on secondaries (because they involve writes) and shouldn’t be performed on primaries because of the impact this could have on application performance. Thus, the following sections frequently mention starting up a server in standalone mode. This means restarting the member so that it is a standalone server, not a member of a replica set (temporarily).\n\nTo start up a member in standalone mode, first look at the command-line options used to start it. Suppose they look something like this:\n\n> db.serverCmdLineOpts() { \"argv\" : [ \"mongod\", \"-f\", \"/var/lib/mongod.conf\" ], \"parsed\" : { \"replSet\": \"mySet\", \"port\": \"27017\", \"dbpath\": \"/var/lib/db\" },\n\n271\n\n\"ok\" : 1 }\n\nTo perform maintenance on this server we can restart it without the replSet option. This will allow us to read and write to it as a normal standalone mongod. We don’t want the other servers in the set to be able to contact it, so we’ll make it listen on a different port (so that the other members won’t be able to find it). Finally, we want to keep the dbpath the same, as we are presumably starting it up this way to manipulate the server’s data somehow.\n\nFirst, we shut down the server from the mongo shell:\n\n> db.shutdownServer()\n\nThen, in an operating system shell (e.g., bash), we restart mongod on another port and without the replSet parameter:\n\n$ mongod --port 30000 --dbpath /var/lib/db\n\nIt will now be running as a standalone server, listening on port 30000 for connec‐ tions. The other members of the set will attempt to connect to it on port 27017 and assume that it is down.\n\nWhen we have finished performing maintenance on the server, we can shut it down and restart it with its original options. It will automatically sync up with the rest of the set, replicating any operations that it missed while it was “away.”\n\nReplica Set Configuration Replica set configuration is always kept in a document in the local.system.replset col‐ lection. This document is the same on all members of the set. Never update this docu‐ ment using update. Always use an rs helper or the replSetReconfig command.\n\nCreating a Replica Set You create a replica set by starting up the mongods that you want to be members and then passing one of them a configuration through rs.initiate():\n\n> var config = { ... \"_id\" : <setName>, ... \"members\" : [ ... {\"_id\" : 0, \"host\" : <host1>}, ... {\"_id\" : 1, \"host\" : <host2>}, ... {\"_id\" : 2, \"host\" : <host3>} ... ]} > rs.initiate(config)\n\n272\n\n|\n\nChapter 13: Administration\n\nYou should always pass a config object to rs.initiate(). If you do not, MongoDB will attempt to automatically generate a config for a one-member replica set; it might not use the hostname that you want or correctly configure the set.\n\nYou only call rs.initiate() on one member of the set. The member that receives the configuration will pass it on to the other members.\n\nChanging Set Members When you add a new set member, it should either have nothing in its data directory —in which case it will perform an initial sync—or have a copy of the data from another member (see Chapter 23 for more information about backing up and restor‐ ing replica set members).\n\nConnect to the primary and add a new member as follows:\n\n> rs.add(\"spock:27017\")\n\nAlternatively, you can specify a more complex member config as a document:\n\n> rs.add({\"host\" : \"spock:27017\", \"priority\" : 0, \"hidden\" : true})\n\nYou can also remove members by their \"host\" field:\n\n> rs.remove(\"spock:27017\")\n\nYou can change a member’s settings by reconfiguring. There are a few restrictions in changing a member’s settings:\n\nYou cannot change a member’s \"_id\".\n\nYou cannot make the member you’re sending the reconfig to (generally the pri‐ mary) priority 0.\n\nYou cannot turn an arbiter into a nonarbiter, or vice versa. • You cannot change a member’s \"buildIndexes\" field from false to true.\n\nNotably, you can change a member’s \"host\" field. Thus, if you incorrectly specify a host (say, if you use a public IP instead of a private one) you can later go back and simply change the config to use the correct IP.\n\nTo change a hostname, you could do something like this:\n\n> var config = rs.config() > config.members[0].host = \"spock:27017\" spock:27017 > rs.reconfig(config)\n\nReplica Set Configuration\n\n|\n\n273\n\nThis same strategy applies to changing any other option: fetch the config with rs.con fig(), modify any parts of it that you wish, and reconfigure the set by passing rs.reconfig() the new configuration.\n\nCreating Larger Sets Replica sets are limited to 50 members in total and only 7 voting members. This is to reduce the amount of network traffic required for everyone to heartbeat everyone else and to limit the amount of time elections take.\n\nIf you are creating a replica set that has more than seven members, every additional member must be given zero votes. You can do this by specifying it in the member’s config:\n\n> rs.add({\"_id\" : 7, \"host\" : \"server-7:27017\", \"votes\" : 0})\n\nThis prevents these members from casting positive votes in elections.\n\nForcing Reconfiguration When you permanently lose a majority of a set, you may want to reconfigure the set while it doesn’t have a primary. This is a little tricky, as usually you’d send the reconfig to the primary. In this case, you can force-reconfigure the set by sending a reconfig command to a secondary. Connect to a secondary in the shell and pass it a reconfig with the \"force\" option:\n\n> rs.reconfig(config, {\"force\" : true})\n\nForced reconfigurations follow the same rules as a normal reconfiguration: you must send a valid, well-formed configuration with the correct options. The \"force\" option doesn’t allow invalid configs; it just allows a secondary to accept a reconfig.\n\nForced reconfigurations bump the replica set \"version\" number by a large amount. You may see it jump by tens or hundreds of thousands. This is normal: it is to prevent version number collisions (just in case there’s a reconfig on either side of a network partition).\n\nWhen the secondary receives the reconfig, it will update its configuration and pass the new config along to the other members. The other members of the set will only pick up on a change of config if they recognize the sending server as a member of their current config. Thus, if some of your members have changed hostnames, you should force reconfig from a member that kept its old hostname. If every member has a new hostname, you should shut down each member of the set, start a new one up in standalone mode, change its local.system.replset document manually, and then restart the member.\n\n274\n\n|\n\nChapter 13: Administration\n\nManipulating Member State There are several ways to manually change a member’s state for maintenance or in response to load. Note that there is no way to force a member to become primary, however, other than configuring the set appropriately—in this case, by giving the rep‐ lica set member a priority higher than any other member of the set.\n\nTurning Primaries into Secondaries You can demote a primary to a secondary using the stepDown function:\n\n> rs.stepDown()\n\nThis makes the primary step down into SECONDARY state for 60 seconds. If no other primary is elected in that time period, it will be able to attempt a reelection. If you would like it to remain a secondary for a longer or shorter amount of time, you can specify your own number of seconds for it to stay in SECONDARY state:\n\n> rs.stepDown(600) // 10 minutes\n\nPreventing Elections If you need to do some maintenance on the primary but don’t want any of the other eligible members to become primary in the interim, you can force them to stay sec‐ ondaries by running freeze on each of them:\n\n> rs.freeze(10000)\n\nAgain, this takes a number of seconds for the member to remain a secondary.\n\nIf you finish whatever maintenance you’re doing on the primary before this time elapses and want to unfreeze the other members, simply run the command again on each of them, giving a timeout of 0 seconds:\n\n> rs.freeze(0)\n\nAn unfrozen member will be able to hold an election, if it chooses.\n\nYou can also unfreeze primaries that have been stepped down by running rs.freeze(0).\n\nMonitoring Replication It is important to be able to monitor the status of a set: not only that all members are up, but what states they are in and how up to date the replication is. There are several commands you can use to see replica set information. MongoDB hosting services and management tools including Atlas, Cloud Manager, and Ops Manager (see Chap‐\n\nManipulating Member State\n\n|\n\n275\n\nter 22) also provide mechanisms to monitor replication and dashboards on the key replication metrics.\n\nOften issues with replication are transient: a server could not reach another server, but now it can. The easiest way to see issues like this is to look at the logs. Make sure you know where the logs are being stored (and that they are being stored) and that you can access them.\n\nGetting the Status One of the most useful commands you can run is replSetGetStatus, which gets the current information about every member of the set (from the view of the member you’re running it on). There is a helper for this command in the shell:\n\n> rs.status()\n\n\"set\" : \"replset\", \"date\" : ISODate(\"2019-11-02T20:02:16.543Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : { \"lastCommittedOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"readConcernMajorityOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"appliedOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) } },\n\n\"members\" : [ { \"_id\" : 0, \"name\" : \"m1.example.net:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 269, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1)\n\n276\n\n|\n\nChapter 13: Administration\n\n}, \"optimeDate\" : ISODate(\"2019-11-02T20:02:14Z\"), \"infoMessage\" : \"could not find member to sync from\", \"electionTime\" : Timestamp(1478116933, 1), \"electionDate\" : ISODate(\"2019-11-02T20:02:13Z\"), \"configVersion\" : 1, \"self\" : true }, { \"_id\" : 1, \"name\" : \"m2.example.net:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 14, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2019-11-02T20:02:14Z\"), \"optimeDurableDate\" : ISODate(\"2019-11-02T20:02:14Z\"), \"lastHeartbeat\" : ISODate(\"2019-11-02T20:02:15.618Z\"), \"lastHeartbeatRecv\" : ISODate(\"2019-11-02T20:02:14.866Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"m3.example.net:27017\", \"configVersion\" : 1 }, { \"_id\" : 2, \"name\" : \"m3.example.net:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 14, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2019-11-02T20:02:14Z\"), \"optimeDurableDate\" : ISODate(\"2019-11-02T20:02:14Z\"), \"lastHeartbeat\" : ISODate(\"2019-11-02T20:02:15.619Z\"), \"lastHeartbeatRecv\" : ISODate(\"2019-11-02T20:02:14.787Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"m1.example.net:27018\",\n\nMonitoring Replication\n\n|\n\n277",
      "page_number": 278
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 288-297)",
      "start_page": 288,
      "end_page": 297,
      "detection_method": "topic_boundary",
      "content": "\"configVersion\" : 1 } ], \"ok\" : 1 }\n\nThese are some of the most useful fields:\n\n\"self\"\n\nThis field is only present in the member rs.status() was run on—in this case, server-2 (m1.example.net:27017).\n\n\"stateStr\"\n\nA string describing the state of the server. See “Member States” on page 254 for descriptions of the various states.\n\n\"uptime\"\n\nThe number of seconds a member has been reachable, or the time since this server was started for the \"self\" member. Thus, server-1 has been up for 269 sec‐ onds, and server-2 and server-3 for 14 seconds.\n\n\"optimeDate\"\n\nThe last optime in each member’s oplog (where that member is synced to). Note that this is the state of each member as reported by the heartbeat, so the optime reported here may be off by a couple of seconds.\n\n\"lastHeartbeat\"\n\nThe time this server last received a heartbeat from the \"self\" member. If there have been network issues or the server has been busy, this may be longer than two seconds ago.\n\n\"pingMs\"\n\nThe running average of how long heartbeats to this server have taken. This is used in determining which member to sync from.\n\n\"errmsg\"\n\nAny status message that the member chose to return in the heartbeat request. These are often merely informational, not error messages. For example, the \"errmsg\" field in server-3 indicates that this server is in the process of initial syncing. The hexadecimal number 507e9a30:851 is the timestamp of the opera‐ tion this member needs to get to to complete the initial sync.\n\nThere are several fields that give overlapping information. \"state\" is the same as \"stateStr\"; it’s simply the internal ID for the state. \"health\" merely reflects whether a given server is reachable (1) or unreachable (0), which is also shown by \"state\" and \"stateStr\" (they’ll be UNKNOWN or DOWN if the server is unreachable). Similarly, \"optime\" and \"optimeDate\" are the same value represented in two ways: one\n\n278\n\n|\n\nChapter 13: Administration\n\nrepresents milliseconds since the epoch (\"t\" : 135...) and the other is a more human-readable date.\n\nNote that this report is from the point of view of whichever mem‐ ber of the set you run it on: the information it contains may be incorrect or out of date due to network issues.\n\nVisualizing the Replication Graph If you run rs.status() on a secondary, there will be a top-level field called \"syncingTo\". This gives the host that this member is replicating from. By running the replSetGetStatus command on each member of the set, you can figure out the replication graph. For example, assuming server1 was a connection to server1, server2 was a connection to server2, and so on, you might have something like:\n\n> server1.adminCommand({replSetGetStatus: 1})['syncingTo'] server0:27017 > server2.adminCommand({replSetGetStatus: 1})['syncingTo'] server1:27017 > server3.adminCommand({replSetGetStatus: 1})['syncingTo'] server1:27017 > server4.adminCommand({replSetGetStatus: 1})['syncingTo'] server2:27017\n\nThus, server0 is the replication source for server1, server1 is the replication source for server2 and server3, and server2 is the replication source for server4.\n\nMongoDB determines who to sync to based on ping time. When one member heart‐ beats another, it times how long that request takes. MongoDB keeps a running aver‐ age of these times. When a member has to choose another member to sync from, it looks for the one that is closest to it and ahead of it in replication (thus, you cannot end up with a replication cycle: members will only replicate from the primary or sec‐ ondaries that are further ahead).\n\nThis means that if you bring up a new member in a secondary data center, it is more likely to sync from another member in that data center than a member in your pri‐ mary data center (thus minimizing WAN traffic), as shown in Figure 13-1.\n\nHowever, there is a downside to automatic replication chaining: more replication hops means that it takes a bit longer to replicate writes to all servers. For example, let’s say that everything is in one data center but, due to the vagaries of network speeds when you added members, MongoDB ends up replicating in a line, as shown in Figure 13-2.\n\nMonitoring Replication\n\n|\n\n279\n\nFigure 13-1. New secondaries will generally choose to sync from a member in the same data center\n\nFigure 13-2. As replication chains get longer, it takes longer for all members to get a copy of the data\n\nThis is highly unlikely, but not impossible. It is, however, probably undesirable: each secondary in the chain will have to be a bit further behind than the secondary “in front” of it. You can fix this by modifying the replication source for a member using the replSetSyncFrom command (or the rs.syncFrom() helper).\n\nConnect to the secondary whose replication source you want to change and run this command, passing it the server you’d prefer this member to sync from:\n\n> secondary.adminCommand({\"replSetSyncFrom\" : \"server0:27017\"})\n\nIt may take a few seconds to switch sync sources, but if you run rs.status() on that member again, you should see that the \"syncingTo\" field now says \"server0:27017\".\n\nThis member (server4) will now continue replicating from server0 until server0 becomes unavailable or, if it happened to be a secondary, falls significantly behind the other members.\n\nReplication Loops A replication loop is when members end up replicating from one another—for exam‐ ple, A is syncing from B who is syncing from C who is syncing from A. As none of the members in a replication loop can be a primary, the members will not receive any new operations to replicate and will fall behind.\n\n280\n\n|\n\nChapter 13: Administration\n\nReplication loops should be impossible when members choose who to sync from automatically. However, you can force replication loops using the replSetSyncFrom command. Inspect the rs.status() output carefully before manually changing sync targets, and be careful not to create loops. The replSetSyncFrom command will warn you if you do not choose to sync from a member that is strictly ahead, but it will allow it.\n\nDisabling Chaining Chaining is when a secondary syncs from another secondary (instead of the primary). As mentioned earlier, members may decide to sync from other members automati‐ cally. You can disable chaining, forcing everyone to sync from the primary, by chang‐ ing the \"chainingAllowed\" setting to false (if not specified, it defaults to true):\n\n> var config = rs.config() > // create the settings subobject, if it does not already exist > config.settings = config.settings || {} > config.settings.chainingAllowed = false > rs.reconfig(config)\n\nWith \"chainingAllowed\" set to false, all members will sync from the primary. If the primary becomes unavailable, they will fall back to syncing from secondaries.\n\nCalculating Lag One of the most important metrics to track for replication is how well the secondaries are keeping up with the primary. Lag is how far behind a secondary is, which means the difference between the timestamp of the last operation the primary has per‐ formed and the timestamp of the last operation the secondary has applied.\n\nYou can use rs.status() to see a member’s replication state, but you can also get a quick summary by running rs.printReplicationInfo() or rs.printSlaveReplica tionInfo().\n\nrs.printReplicationInfo() gives a summary of the primary’s oplog, including its size and the date range of its operations:\n\n> rs.printReplicationInfo(); configured oplog size: 10.48576MB log length start to end: 3590 secs (1.00hrs) oplog first event time: Tue Apr 10 2018 09:27:57 GMT-0400 (EDT) oplog last event time: Tue Apr 10 2018 10:27:47 GMT-0400 (EDT) now: Tue Apr 10 2018 10:27:47 GMT-0400 (EDT)\n\nIn this example, the oplog is about 10 MB (10 MiB) and is only able to fit about an hour of operations.\n\nMonitoring Replication\n\n|\n\n281\n\nIf this were a real deployment, the oplog should probably be larger (see the next sec‐ tion for instructions on changing oplog size). We want the log length to be at least as long as the time it takes to do a full resync. That way, we don’t run into a case where a secondary falls off the end of the oplog before finishing its initial sync.\n\nThe log length is computed by taking the time difference between the first and last operation in the oplog once the oplog has filled up. If the server has just started with nothing in the oplog, then the earliest operation will be relatively recent. In that case, the log length will be small, even though the oplog probably still has free space available. The length is a more useful metric for servers that have been operating long enough to write through their entire oplog at least once.\n\nYou can also use the rs.printSlaveReplicationInfo() function to get the syncedTo value for each member and the time when the last oplog entry was written to each secondary, as shown in the following example:\n\n> rs.printSlaveReplicationInfo(); source: m1.example.net:27017 syncedTo: Tue Apr 10 2018 10:27:47 GMT-0400 (EDT) 0 secs (0 hrs) behind the primary source: m2.example.net:27017 syncedTo: Tue Apr 10 2018 10:27:43 GMT-0400 (EDT) 0 secs (0 hrs) behind the primary source: m3.example.net:27017 syncedTo: Tue Apr 10 2018 10:27:39 GMT-0400 (EDT) 0 secs (0 hrs) behind the primary\n\nRemember that a replica set member’s lag is calculated relative to the primary, not against “wall time.” This usually is irrelevant, but on very low-write systems, this can cause phantom replication lag “spikes.” For example, suppose you do a write once an hour. Right after that write, before it’s replicated, the secondary will look like it’s an hour behind the primary. However, it’ll be able to catch up with that “hour” of opera‐ tions in a few milliseconds. This can sometimes cause confusion when monitoring a low-throughput system.\n\nResizing the Oplog Your primary’s oplog should be thought of as your maintenance window. If your pri‐ mary has an oplog that is an hour long, then you only have one hour to fix anything that goes wrong before your secondaries fall too far behind and must be resynced from scratch. Thus, you generally want to have an oplog that can hold a couple days’ to a week’s worth of data, to give yourself some breathing room if something goes wrong.\n\n282\n\n|\n\nChapter 13: Administration\n\nUnfortunately, there’s no easy way to tell how long your oplog is going to be before it fills up. The WiredTiger storage engine allows online resizing of your oplog while your server is running. You should perform these steps on each secondary replica set member first; once these have been changed, then and only then should you make the changes to your primary. Remember that each server that could become a primary should have a large enough oplog to give you a sane maintenance window.\n\nTo increase the size of your oplog, perform the following steps:\n\n1. Connect to the replica set member. If authentication is enabled, be sure to use a user with privileges that can modify the local database.\n\n2. Verify the current size of the oplog:\n\n> use local > db.oplog.rs.stats(1024*1024).maxSize\n\nThis will display the collection size in megabytes.\n\n3. Change the oplog size of the replica set member:\n\n> db.adminCommand({replSetResizeOplog: 1, size: 16000})\n\nThe following operation changes the oplog size of the replica set member to 16 gigabytes, or 16000 megabytes.\n\n4. Finally, if you have reduced the size of the oplog, you may need to run the com pact to reclaim the disk space allocated. This should not be run against a mem‐ ber while it is a primary. Please see the “Change the Size of the Oplog” tutorial in the MongoDB documentation for more details on this case and on the entire pro‐ cedure.\n\nYou generally should not decrease the size of your oplog: although it may be months long, there is usually ample disk space for it and it does not use up any valuable resources like RAM or CPU.\n\nBuilding Indexes If you send an index build to the primary, the primary will build the index normally and then the secondaries will build the index when they replicate the “build index”\n\nMonitoring Replication\n\n|\n\n283\n\noperation. Although this is the easiest way to build an index, index builds are resource-intensive operations that can make members unavailable. If all of your sec‐ ondaries start building an index at the same time, almost every member of your set will be offline until the index build completes. This process is only for replica sets; for a sharded cluster, please see the MongoDB documentation tutorial about building indexes on a sharded cluster.\n\nYou must stop all writes to a collection when you are creating a \"unique\" index. If the writes are not stopped, you can end up with inconsistent data across the replica set members.\n\nTherefore, you may want to build an index on one member at a time to minimize the impact on your application. To accomplish this, do the following:\n\n1. Shut down a secondary.\n\n2. Restart it as a standalone server.\n\n3. Build the index on the standalone server.\n\n4. When the index build is complete, restart the server as a member of the replica set. When restarting this member, you need to remove the disableLogicalSes sionCacheRefresh parameter if it is present in your command-line options or configuration file.\n\n5. Repeat steps 1 through 4 for each secondary in the replica set.\n\nYou should now have a set where every member other than the primary has the index built. Now there are two options, and you should choose the one that will impact your production system the least:\n\n1. Build the index on the primary. If you have an “off” time when you have less traf‐ fic, that would probably be a good time to build it. You also might want to modify read preferences to temporarily shunt more load onto secondaries while the build is in progress. The primary will replicate the index build to the secondaries, but they will already have the index so it will be a no-op for them.\n\n2. Step down the primary, then follow steps 2 through 4 of the procedure outlined previously. This requires a failover, but you will have a normally functioning pri‐ mary while the old primary is building its index. After its index build is com‐ plete, you can reintroduce it to the set.\n\nNote that you could also use this technique to build different indexes on a secondary than you have on the rest of the set. This could be useful for offline processing, but\n\n284\n\n|\n\nChapter 13: Administration\n\nmake sure a member with different indexes can never become primary: its priority should always be 0.\n\nIf you are building a unique index, make sure that the primary is not inserting dupli‐ cates or that you build the index on the primary first. Otherwise, the primary could be inserting duplicates that would then cause replication errors on secondaries. If this occurs, the secondary will shut itself down. You will have to restart it as a standalone server, remove the unique index, and restart it.\n\nReplication on a Budget If it is difficult to get more than one high-quality server, consider getting a secondary server that is strictly for disaster recovery, with less RAM and CPU, slower disk I/O, etc. The good server will always be your primary and the cheaper server will never handle any client traffic (configure your clients to send all reads to the primary). Here are the options to set for the cheaper box:\n\n\"priority\" : 0\n\nYou do not want this server to ever become primary.\n\n\"hidden\" : true\n\nYou do not want clients ever sending reads to this secondary.\n\n\"buildIndexes\" : false\n\nThis is optional, but it can decrease the load this server has to handle considera‐ bly. If you ever need to restore from this server, you’ll need to rebuild the indexes.\n\n\"votes\" : 0\n\nIf you only have two machines, set \"votes\" on this secondary to 0 so that the primary can stay primary if this machine goes down. If you have a third server (even just your application server), run an arbiter on that instead of setting \"votes\" to 0.\n\nThis will give you the safety and security of having a secondary without having to invest in two high-performance servers.\n\nMonitoring Replication\n\n|\n\n285\n\nPART IV Sharding\n\nCHAPTER 14 Introduction to Sharding\n\nThis chapter covers how to scale with MongoDB. We’ll look at:\n\nWhat sharding is and the components of a cluster\n\nHow to configure sharding\n\nThe basics of how sharding interacts with your application\n\nWhat Is Sharding? Sharding refers to the process of splitting data up across machines; the term partition‐ ing is also sometimes used to describe this concept. By putting a subset of data on each machine, it becomes possible to store more data and handle more load without requiring larger or more powerful machines—just a larger quantity of less-powerful machines. Sharding may be used for other purposes as well, including placing more frequently accessed data on more performant hardware or splitting a dataset based on geography to locate a subset of documents in a collection (e.g., for users based in a particular locale) close to the application servers from which they are most com‐ monly accessed.\n\nManual sharding can be done with almost any database software. With this approach, an application maintains connections to several different database servers, each of which are completely independent. The application manages storing different data on different servers and querying against the appropriate server to get data back. This setup can work well but becomes difficult to maintain when adding or removing nodes from the cluster or in the face of changing data distributions or load patterns.\n\nMongoDB supports autosharding, which tries to both abstract the architecture away from the application and simplify the administration of such a system. MongoDB\n\n289",
      "page_number": 288
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 298-308)",
      "start_page": 298,
      "end_page": 308,
      "detection_method": "topic_boundary",
      "content": "allows your application to ignore the fact that it isn’t talking to a standalone Mon‐ goDB server, to some extent. On the operations side, MongoDB automates balancing data across shards and makes it easier to add and remove capacity.\n\nSharding is the most complex way of configuring MongoDB, both from a develop‐ ment and an operational point of view. There are many components to configure and monitor, and data moves around the cluster automatically. You should be comfortable with standalone servers and replica sets before attempting to deploy or use a sharded cluster. Also, as with replica sets, the recommended means of configuring and deploying sharded clusters is through either MongoDB Ops Manager or MongoDB Atlas. Ops Manager is recommended if you need to maintain control of your com‐ puting infrastructure. MongoDB Atlas is recommended if you can leave the infra‐ structure management to MongoDB (you have the option of running in Amazon AWS, Microsoft Azure, or Google Compute Cloud).\n\nUnderstanding the Components of a Cluster MongoDB’s sharding allows you to create a cluster of many machines (shards) and break up a collection across them, putting a subset of data on each shard. This allows your application to grow beyond the resource limits of a standalone server or replica set.\n\nMany people are confused about the difference between replication and sharding. Remember that replication creates an exact copy of your data on multiple servers, so every server is a mirror image of every other server. Conversely, every shard contains a different subset of data.\n\nOne of the goals of sharding is to make a cluster of 2, 3, 10, or even hundreds of shards look like a single machine to your application. To hide these details from the application, we run one or more routing processes called a mongos in front of the shards. A mongos keeps a “table of contents” that tells it which shard contains which data. Applications can connect to this router and issue requests normally, as shown in Figure 14-1. The router, knowing what data is on which shard, is able to forward the requests to the appropriate shard(s). If there are responses to a request the router col‐ lects them and, if necessary, merges them, and sends them back to the application. As far as the application knows, it’s connected to a standalone mongod, as illustrated in Figure 14-2.\n\n290\n\n|\n\nChapter 14: Introduction to Sharding\n\nFigure 14-1. Sharded client connection\n\nFigure 14-2. Nonsharded client connection\n\nSharding on a Single-Machine Cluster We’ll start by setting up a quick cluster on a single machine. First, start a mongo shell with the --nodb and --norc options:\n\n$ mongo --nodb --norc\n\nTo create a cluster, use the ShardingTest class. Run the following in the mongo shell you just launched:\n\nst = ShardingTest({ name:\"one-min-shards\", chunkSize:1, shards:2, rs:{ nodes:3, oplogSize:10 }, other:{ enableBalancer:true } });\n\nThe chunksize option is covered in Chapter 17. For now, simply set it to 1. As for the other options passed to ShardingTest here, name simply provides a label for our\n\nSharding on a Single-Machine Cluster\n\n|\n\n291\n\nsharded cluster, shards specifies that our cluster will be composed of two shards (we do this to keep the resource requirements low for this example), and rs defines each shard as a three-node replica set with an oplogSize of 10 MiB (again, to keep resource shard as a three-node replica set with an oplogSize of 10 MiB (again, to keep resource utilization low). Though it is possible to run one standalone mongod for each shard, it paints a clearer picture of the typical architecture of a sharded clus‐ ter if we create each shard as a replica set. In the last option specified, we are instruct‐ ing ShardingTest to enable the balancer once the cluster is spun up. This will ensure that data is evenly distributed across both shards.\n\nShardingTest is a class designed for internal use by MongoDB Engineering and is therefore undocumented externally. However, because it ships with the MongoDB server, it provides the most straightforward means of experimenting with a sharded cluster. ShardingTest was originally designed to support server test suites and is still used for this purpose. By default it provides a number of conveniences that help in keeping resource utilization as small as possible and in setting up the relatively com‐ plex architecture of a sharded cluster. It makes an assumption about the presence of a /data /db directory on your machine; if ShardingTest fails to run then create this directory and rerun the command again.\n\nWhen you run this command, ShardingTest will do a lot for you automatically. It will create a new cluster with two shards, each of which is a replica set. It will config‐ ure the replica sets and launch each node with the necessary options to establish rep‐ lication protocols. It will launch a mongos to manage requests across the shards so that clients can interact with the cluster as if communicating with a standalone mon‐ god, to some extent. Finally, it will launch an additional replica set for the config servers that maintain the routing table information necessary to ensure queries are directed to the correct shard. Remember that the primary use cases for sharding are to split a dataset to address hardware and cost constraints or to provide better perfor‐ mance to applications (e.g., geographical partitioning). MongoDB sharding provides these capabilities in a way that is seamless to the application in many respects.\n\nOnce ShardingTest has finished setting up your cluster, you will have 10 processes up and running to which you can connect: two replica sets of three nodes each, one config server replica set of three nodes, and one mongos. By default, these processes should begin at port 20000. The mongos should be running at port 20009. Other pro‐ cesses you have running on your local machine and previous calls to ShardingTest can have an effect on which ports ShardingTest uses, but you should not have too much difficulty determining the ports on which your cluster processes are running.\n\nNext, you’ll connect to the mongos to play around with the cluster. Your entire cluster will be dumping its logs to your current shell, so open up a second terminal window and launch another mongo shell:\n\n$ mongo --nodb\n\n292\n\n|\n\nChapter 14: Introduction to Sharding\n\nUse this shell to connect to your cluster’s mongos. Again, your mongos should be run‐ ning on port 20009:\n\n> db = (new Mongo(\"localhost:20009\")).getDB(\"accounts\")\n\nNote that the prompt in your mongo shell should change to reflect that you are con‐ nected to a mongos. Now you are in the situation shown earlier, in Figure 14-1: the shell is the client and is connected to a mongos. You can start passing requests to the mongos and it’ll route them to the shards. You don’t really have to know anything about the shards, like how many there are or what their addresses are. So long as there are some shards out there, you can pass the requests to the mongos and allow it to forward them appropriately.\n\nStart by inserting some data:\n\n> for (var i=0; i<100000; i++) { ... db.users.insert({\"username\" : \"user\"+i, \"created_at\" : new Date()}); ... } > db.users.count() 100000\n\nAs you can see, interacting with mongos works the same way as interacting with a standalone server does.\n\nYou can get an overall view of your cluster by running sh.status(). It will give you a summary of your shards, databases, and collections:\n\n> sh.status() --- Sharding Status --- sharding version: { \"_id\": 1, \"minCompatibleVersion\": 5, \"currentVersion\": 6, \"clusterId\": ObjectId(\"5a4f93d6bcde690005986071\") } shards: { \"_id\" : \"one-min-shards-rs0\", \"host\" : \"one-min-shards-rs0/MBP:20000,MBP:20001,MBP:20002\", \"state\" : 1 } { \"_id\" : \"one-min-shards-rs1\", \"host\" : \"one-min-shards-rs1/MBP:20003,MBP:20004,MBP:20005\", \"state\" : 1 } active mongoses: \"3.6.1\" : 1 autosplit: Currently enabled: no balancer: Currently enabled: no Currently running: no\n\nSharding on a Single-Machine Cluster\n\n|\n\n293\n\nFailed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: { \"_id\" : \"accounts\", \"primary\" : \"one-min-shards-rs1\", \"partitioned\" : false } { \"_id\" : \"config\", \"primary\" : \"config\", \"partitioned\" : true } config.system.sessions shard key: { \"_id\" : 1 } unique: false balancing: true chunks: one-min-shards-rs0 { \"_id\" : { \"$minKey\" : 1 } } -->> { \"_id\" : { \"$maxKey\" : 1 } } on : one-min-shards-rs0 Timestamp(1, 0)\n\n1\n\nsh is similar to rs, but for sharding: it is a global variable that defines a number of sharding helper functions, which you can see by running sh.help(). As you can see from the sh.status() out‐ put, you have two shards and two databases (config is created automatically).\n\nYour accounts database may have a different primary shard than the one shown here. A primary shard is a “home base” shard that is randomly chosen for each database. All of your data will be on this primary shard. MongoDB cannot automatically dis‐ tribute your data yet because it doesn’t know how (or if) you want it to be distributed. You have to tell it, per collection, how you want it to distribute data.\n\nA primary shard is different from a replica set primary. A primary shard refers to the entire replica set composing a shard. A primary in a replica set is the single server in the set that can take writes.\n\nTo shard a particular collection, first enable sharding on the collection’s database. To do so, run the enableSharding command:\n\n> sh.enableSharding(\"accounts\")\n\nNow sharding is enabled on the accounts database, which allows you to shard collec‐ tions within the database.\n\nWhen you shard a collection, you choose a shard key. This is a field or two that Mon‐ goDB uses to break up data. For example, if you chose to shard on \"username\", Mon‐ goDB would break up the data into ranges of usernames: \"a1-steak-sauce\" through \"defcon\", \"defcon1\" through \"howie1998\", and so on. Choosing a shard key can be\n\n294\n\n|\n\nChapter 14: Introduction to Sharding\n\nthought of as choosing an ordering for the data in the collection. This is a similar concept to indexing, and for good reason: the shard key becomes the most important index on your collection as it gets bigger. To even create a shard key, the field(s) must be indexed.\n\nSo, before enabling sharding, you have to create an index on the key you want to shard by:\n\n> db.users.createIndex({\"username\" : 1})\n\nNow you can shard the collection by \"username\":\n\n> sh.shardCollection(\"accounts.users\", {\"username\" : 1})\n\nAlthough we are choosing a shard key without much thought here, it is an important decision that should be carefully considered in a real system. See Chapter 16 for more advice on choosing a shard key.\n\nIf you wait a few minutes and run sh.status() again, you’ll see that there’s a lot more information displayed than there was before:\n\n> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"5a4f93d6bcde690005986071\") } shards: { \"_id\" : \"one-min-shards-rs0\", \"host\" : \"one-min-shards-rs0/MBP:20000,MBP:20001,MBP:20002\", \"state\" : 1 } { \"_id\" : \"one-min-shards-rs1\", \"host\" : \"one-min-shards-rs1/MBP:20003,MBP:20004,MBP:20005\", \"state\" : 1 } active mongoses: \"3.6.1\" : 1 autosplit: Currently enabled: no balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: 6 : Success databases: { \"_id\" : \"accounts\", \"primary\" : \"one-min-shards-rs1\", \"partitioned\" : true } accounts.users\n\nSharding on a Single-Machine Cluster\n\n|\n\n295\n\nshard key: { \"username\" : 1 } unique: false balancing: true chunks: one-min-shards-rs0 6 one-min-shards-rs1 7 { \"username\" : { \"$minKey\" : 1 } } -->> { \"username\" : \"user17256\" } on : one-min-shards-rs0 Timestamp(2, 0) { \"username\" : \"user17256\" } -->> { \"username\" : \"user24515\" } on : one-min-shards-rs0 Timestamp(3, 0) { \"username\" : \"user24515\" } -->> { \"username\" : \"user31775\" } on : one-min-shards-rs0 Timestamp(4, 0) { \"username\" : \"user31775\" } -->> { \"username\" : \"user39034\" } on : one-min-shards-rs0 Timestamp(5, 0) { \"username\" : \"user39034\" } -->> { \"username\" : \"user46294\" } on : one-min-shards-rs0 Timestamp(6, 0) { \"username\" : \"user46294\" } -->> { \"username\" : \"user53553\" } on : one-min-shards-rs0 Timestamp(7, 0) { \"username\" : \"user53553\" } -->> { \"username\" : \"user60812\" } on : one-min-shards-rs1 Timestamp(7, 1) { \"username\" : \"user60812\" } -->> { \"username\" : \"user68072\" } on : one-min-shards-rs1 Timestamp(1, 7) { \"username\" : \"user68072\" } -->> { \"username\" : \"user75331\" } on : one-min-shards-rs1 Timestamp(1, 8) { \"username\" : \"user75331\" } -->> { \"username\" : \"user82591\" } on : one-min-shards-rs1 Timestamp(1, 9) { \"username\" : \"user82591\" } -->> { \"username\" : \"user89851\" } on : one-min-shards-rs1 Timestamp(1, 10) { \"username\" : \"user89851\" } -->> { \"username\" : \"user9711\" } on : one-min-shards-rs1 Timestamp(1, 11) { \"username\" : \"user9711\" } -->> { \"username\" : { \"$maxKey\" : 1 } } on : one-min-shards-rs1 Timestamp(1, 12) { \"_id\" : \"config\", \"primary\" : \"config\", \"partitioned\" : true } config.system.sessions shard key: { \"_id\" : 1 } unique: false balancing: true chunks: one-min-shards-rs0 1 { \"_id\" : { \"$minKey\" : 1 } } -->> { \"_id\" : { \"$maxKey\" : 1 } } on : one-min-shards-rs0 Timestamp(1, 0)\n\nThe collection has been split up into 13 chunks, where each chunk is a subset of your data. These are listed by shard key range (the {\"username\" : minValue} -->> {\"username\" : maxValue} denotes the range of each chunk). Looking at the \"on\" : shard part of the output, you can see that these chunks have been evenly distributed between the shards.\n\nThis process of a collection being split into chunks is shown graphically in Figures 14-3 through 14-5. Before sharding, the collection is essentially a single chunk. Sharding splits it into smaller chunks based on the shard key, as shown in\n\n296\n\n|\n\nChapter 14: Introduction to Sharding\n\nFigure 14-4. These chunks can then be distributed across the cluster, as Figure 14-5 shows.\n\nFigure 14-3. Before a collection is sharded, it can be thought of as a single chunk from the smallest value of the shard key to the largest\n\nFigure 14-4. Sharding splits the collection into many chunks based on shard key ranges\n\nFigure 14-5. Chunks are evenly distributed across the available shards\n\nNotice the keys at the beginning and end of the chunk list: $minKey and $maxKey. $minKey can be thought of as “negative infinity.” It is smaller than any other value in MongoDB. Similarly, $maxKey is like “positive infinity.” It is greater than any other value. Thus, you’ll always see these as the “caps” on your chunk ranges. The values for your shard key will always be between $minKey and $maxKey. These values are actually BSON types and should not be used in your application; they are mainly for internal use. If you wish to refer to them in the shell, use the MinKey and MaxKey constants.\n\nNow that the data is distributed across multiple shards, let’s try doing some queries. First, try a query on a specific username:\n\n> db.users.find({username: \"user12345\"}) { \"_id\" : ObjectId(\"5a4fb11dbb9ce6070f377880\"),\n\nSharding on a Single-Machine Cluster\n\n|\n\n297\n\n\"username\" : \"user12345\", \"created_at\" : ISODate(\"2018-01-05T17:08:45.657Z\") }\n\nAs you can see, querying works normally. However, let’s run an explain to see what MongoDB is doing under the covers:\n\n> db.users.find({username: \"user12345\"}}).explain() { \"queryPlanner\" : { \"mongosPlannerVersion\" : 1, \"winningPlan\" : { \"stage\" : \"SINGLE_SHARD\", \"shards\" : [{ \"shardName\" : \"one-min-shards-rs0\", \"connectionString\" : \"one-min-shards-rs0/MBP:20000,MBP:20001,MBP:20002\", \"serverInfo\" : { \"host\" : \"MBP\", \"port\" : 20000, \"version\" : \"3.6.1\", \"gitVersion\" : \"025d4f4fe61efd1fb6f0005be20cb45a004093d1\" }, \"plannerVersion\" : 1, \"namespace\" : \"accounts.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"username\" : { \"$eq\" : \"user12345\" } }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"SHARDING_FILTER\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"[\\\"user12345\\\", \\\"user12345\\\"]\"\n\n298\n\n|\n\nChapter 14: Introduction to Sharding\n\n] } } } }, \"rejectedPlans\" : [ ] }] } }, \"ok\" : 1, \"$clusterTime\" : { \"clusterTime\" : Timestamp(1515174248, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } }, \"operationTime\" : Timestamp(1515173700, 201) }\n\nFrom the \"winningPlan\" field in the explain output, we can see that our cluster sat‐ isfied this query using a single shard, one-min-shards-rs0. Based on the output of sh.status() shown earlier, we can see that user12345 does fall within the key range for the first chunk listed for that shard in our cluster.\n\nBecause \"username\" is the shard key, mongos was able to route the query directly to the correct shard. Contrast that with the results for querying for all of the users:\n\n> db.users.find().explain() { \"queryPlanner\":{ \"mongosPlannerVersion\":1, \"winningPlan\":{ \"stage\":\"SHARD_MERGE\", \"shards\":[ { \"shardName\":\"one-min-shards-rs0\", \"connectionString\": \"one-min-shards-rs0/MBP:20000,MBP:20001,MBP:20002\", \"serverInfo\":{ \"host\":\"MBP.fios-router.home\", \"port\":20000, \"version\":\"3.6.1\", \"gitVersion\":\"025d4f4fe61efd1fb6f0005be20cb45a004093d1\" }, \"plannerVersion\":1, \"namespace\":\"accounts.users\", \"indexFilterSet\":false, \"parsedQuery\":{\n\n}, \"winningPlan\":{\n\nSharding on a Single-Machine Cluster\n\n|\n\n299\n\n300\n\n\"stage\":\"SHARDING_FILTER\", \"inputStage\":{ \"stage\":\"COLLSCAN\", \"direction\":\"forward\" } }, \"rejectedPlans\":[\n\n] }, { \"shardName\":\"one-min-shards-rs1\", \"connectionString\": \"one-min-shards-rs1/MBP:20003,MBP:20004,MBP:20005\", \"serverInfo\":{ \"host\":\"MBP.fios-router.home\", \"port\":20003, \"version\":\"3.6.1\", \"gitVersion\":\"025d4f4fe61efd1fb6f0005be20cb45a004093d1\" }, \"plannerVersion\":1, \"namespace\":\"accounts.users\", \"indexFilterSet\":false, \"parsedQuery\":{\n\n}, \"winningPlan\":{ \"stage\":\"SHARDING_FILTER\", \"inputStage\":{ \"stage\":\"COLLSCAN\", \"direction\":\"forward\" } }, \"rejectedPlans\":[\n\n] } ] } }, \"ok\":1, \"$clusterTime\":{ \"clusterTime\":Timestamp(1515174893, 1), \"signature\":{ \"hash\":BinData(0, \"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\":NumberLong(0) } }, \"operationTime\":Timestamp(1515173709, 514) }\n\n|\n\nChapter 14: Introduction to Sharding",
      "page_number": 298
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 309-318)",
      "start_page": 309,
      "end_page": 318,
      "detection_method": "topic_boundary",
      "content": "As you can see from this explain, this query has to visit both shards to find all the data. In general, if we are not using the shard key in the query, mongos will have to send the query to every shard.\n\nQueries that contain the shard key and can be sent to a single shard or a subset of shards are called targeted queries. Queries that must be sent to all shards are called scatter-gather (broadcast) queries: mongos scatters the query to all the shards and then gathers up the results.\n\nOnce you are finished experimenting, shut down the set. Switch back to your original shell and hit Enter a few times to get back to the command line, then run st.stop() to cleanly shut down all of the servers:\n\n> st.stop()\n\nIf you are ever unsure of what an operation will do, it can be helpful to use ShardingTest to spin up a quick local cluster and try it out.\n\nSharding on a Single-Machine Cluster\n\n|\n\n301\n\nCHAPTER 15 Configuring Sharding\n\nIn the previous chapter, you set up a “cluster” on one machine. This chapter covers how to set up a more realistic cluster and how each piece fits. In particular, you’ll learn:\n\nHow to set up config servers, shards, and mongos processes\n\nHow to add capacity to a cluster\n\nHow data is stored and distributed\n\nWhen to Shard Deciding when to shard is a balancing act. You generally do not want to shard too early because it adds operational complexity to your deployment and forces you to make design decisions that are difficult to change later. On the other hand, you do not want to wait too long to shard because it is difficult to shard an overloaded sys‐ tem without downtime.\n\nIn general, sharding is used to:\n\nIncrease available RAM\n\nIncrease available disk space\n\nReduce load on a server\n\nRead or write data with greater throughput than a single mongod can handle\n\nThus, good monitoring is important to decide when sharding will be necessary. Care‐ fully measure each of these metrics. Generally people speed toward one of these bot‐ tlenecks much faster than the others, so figure out which one your deployment will\n\n303\n\nneed to provision for first and make plans well in advance about when and how you plan to convert your replica set.\n\nStarting the Servers The first step in creating a cluster is to start up all of the processes required. As men‐ tioned in the previous chapter, you need to set up the mongos and the shards. There’s also a third component, the config servers, which are an important piece. Config servers are normal mongod servers that store the cluster configuration: which replica sets host the shards, what collections are sharded by, and on which shard each chunk is located. MongoDB 3.2 introduced the use of replica sets as config servers. Replica sets replace the original syncing mechanism used by config servers; the ability to use that mechanism was removed in MongoDB 3.4.\n\nConfig Servers Config servers are the brains of your cluster: they hold all of the metadata about which servers hold what data. Thus, they must be set up first, and the data they hold is extremely important: make sure that they are running with journaling enabled and that their data is stored on nonephemeral drives. In production deployments, your config server replica set should consist of at least three members. Each config server should be on a separate physical machine, preferable geographically distributed.\n\nThe config servers must be started before any of the mongos processes, as mongos pulls its configuration from them. To begin, run the following commands on three separate machines to start your config servers:\n\n$ mongod --configsvr --replSet configRS --bind_ip localhost,198.51.100.51 mongod --dbpath /var/lib/mongodb\n\n$ mongod --configsvr --replSet configRS --bind_ip localhost,198.51.100.52 mongod --dbpath /var/lib/mongodb\n\n$ mongod --configsvr --replSet configRS --bind_ip localhost,198.51.100.53 mongod --dbpath /var/lib/mongodb\n\nThen initiate the config servers as a replica set. To do this, connect a mongo shell to one of the replica set members:\n\n$ mongo --host <hostname> --port <port>\n\nand use the rs.initiate() helper:\n\n> rs.initiate( { _id: \"configRS\", configsvr: true, members: [ { _id : 0, host : \"cfg1.example.net:27019\" },\n\n304\n\n|\n\nChapter 15: Configuring Sharding\n\n{ _id : 1, host : \"cfg2.example.net:27019\" }, { _id : 2, host : \"cfg3.example.net:27019\" } ] } )\n\nHere we’re using configRS as the replica set name. Note that this name appears both on the command line when instantiating each config server and in the call to rs.initiate().\n\nThe --configsvr option indicates to the mongod that you are planning to use it as a config server. On a server running with this option, clients (i.e., other cluster compo‐ nents) cannot write data to any database other than config or admin.\n\nThe admin database contains the collections related to authentication and authoriza‐ tion, as well as the other system.* collections for internal use. The config database con‐ tains the collections that hold the sharded cluster metadata. MongoDB writes data to the config database when the metadata changes, such as after a chunk migration or a chunk split.\n\nWhen writing to config servers, MongoDB uses a writeConcern level of \"majority\". Similarly, when reading from config servers, MongoDB uses a readConcern level of \"majority\". This ensures that sharded cluster metadata will not be committed to the config server replica set until it can’t be rolled back. It also ensures that only metadata that will survive a failure of the config servers will be read. This is necessary to ensure all mongos routers have a consistent view of how data is organized in a sharded cluster.\n\nIn terms of provisioning, config servers should be provisioned adequately in terms of networking and CPU resources. They only hold a table of contents of the data in the cluster so the storage resources required are minimal. They should be deployed on separate hardware to avoid contention for the machine’s resources.\n\nIf all of your config servers are lost, you must dig through the data on your shards to figure out which data is where. This is possible, but slow and unpleasant. Take frequent backups of config server data. Always take a backup of your config servers before perform‐ ing any cluster maintenance.\n\nThe mongos Processes Once you have three config servers running, start a mongos process for your applica‐ tion to connect to. mongos processes need to know where the config servers are, so you must always start mongos with the --configdb option:\n\n$ mongos --configdb \\ configRS/cfg1.example.net:27019, \\\n\nStarting the Servers\n\n|\n\n305\n\ncfg2.example.net:27019,cfg3.example.net:27019 \\ --bind_ip localhost,198.51.100.100 --logpath /var/log/mongos.log\n\nBy default, mongos runs on port 27017. Note that it does not need a data directory (mongos holds no data itself; it loads the cluster configuration from the config servers on startup). Make sure that you set --logpath to save the mongos log somewhere safe.\n\nYou should start a small number of mongos processes and locate them as close to all the shards as possible. This improves performance of queries that need to access mul‐ tiple shards or which perform scatter/gather operations. The minimal setup is at least two mongos processes to ensure high availability. It is possible to run tens or hun‐ dreds of mongos processes but this causes resource contention on the config servers. The recommended approach is to provide a small pool of routers.\n\nAdding a Shard from a Replica Set Finally, you’re ready to add a shard. There are two possibilities: you may have an existing replica set or you may be starting from scratch. We will cover starting from an existing set. If you are starting from scratch, initialize an empty set and follow the steps outlined here.\n\nIf you already have a replica set serving your application, that will become your first shard. To convert it into a shard, you need to make some small configuration modifi‐ cations to the members and then tell the mongos how to find the replica set that will comprise the shard.\n\nFor example, if you have a replica set named rs0 on svr1.example.net, svr2.exam‐ ple.net, and svr3.example.net, you would first connect to one of the members using the mongo shell:\n\n$ mongo srv1.example.net\n\nThen use rs.status() to determine which member is the primary and which are secondaries:\n\n> rs.status() \"set\" : \"rs0\", \"date\" : ISODate(\"2018-11-02T20:02:16.543Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : {\n\n\"lastCommittedOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) },\n\n306\n\n|\n\nChapter 15: Configuring Sharding\n\n\"readConcernMajorityOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"appliedOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) } },\n\n\"members\" : [ { \"_id\" : 0, \"name\" : \"svr1.example.net:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 269, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2018-11-02T20:02:14Z\"), \"infoMessage\" : \"could not find member to sync from\", \"electionTime\" : Timestamp(1478116933, 1), \"electionDate\" : ISODate(\"2018-11-02T20:02:13Z\"), \"configVersion\" : 1, \"self\" : true }, { \"_id\" : 1, \"name\" : \"svr2.example.net:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 14, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2018-11-02T20:02:14Z\"), \"optimeDurableDate\" : ISODate(\"2018-11-02T20:02:14Z\"), \"lastHeartbeat\" : ISODate(\"2018-11-02T20:02:15.618Z\"), \"lastHeartbeatRecv\" : ISODate(\"2018-11-02T20:02:14.866Z\"),\n\nStarting the Servers\n\n|\n\n307\n\n\"pingMs\" : NumberLong(0), \"syncingTo\" : \"m1.example.net:27017\", \"configVersion\" : 1 }, { \"_id\" : 2, \"name\" : \"svr3.example.net:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 14, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2018-11-02T20:02:14Z\"), \"optimeDurableDate\" : ISODate(\"2018-11-02T20:02:14Z\"), \"lastHeartbeat\" : ISODate(\"2018-11-02T20:02:15.619Z\"), \"lastHeartbeatRecv\" : ISODate(\"2018-11-02T20:02:14.787Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"m1.example.net:27017\", \"configVersion\" : 1 } ], \"ok\" : 1 }\n\nBeginning with MongoDB 3.4, for sharded clusters, mongod instances for shards must be configured with the --shardsvr option, either via the configuration file setting sharding.clusterRole or via the command-line option --shardsvr.\n\nYou will need to do this for each of the members of the replica set you are in the pro‐ cess of converting to a shard. You’ll do this by first restarting each secondary in turn with the --shardsvr option, then stepping down the primary and restarting it with the --shardsvr option.\n\nAfter shutting down a secondary, restart it as follows:\n\n$ mongod --replSet \"rs0\" --shardsvr --port 27017 --bind_ip localhost,<ip address of member>\n\nNote that you’ll need to use the correct IP address for each secondary for the --bind_ip parameter.\n\nNow connect a mongo shell to the primary:\n\n$ mongo m1.example.net\n\n308\n\n|\n\nChapter 15: Configuring Sharding\n\nand step it down:\n\n> rs.stepDown()\n\nThen restart the former primary with the --shardsvr option:\n\n$ mongod --replSet \"rs0\" --shardsvr --port 27017 --bind_ip localhost,<ip address of the former primary>\n\nNow you’re ready to add your replica set as a shard. Connect a mongo shell to the admin database of the mongos:\n\n$ mongo mongos1.example.net:27017/admin\n\nAnd add a shard to the cluster using the sh.addShard() method:\n\n> sh.addShard( \"rs0/svr1.example.net:27017,svr2.example.net:27017,svr3.example.net:27017\" )\n\nYou can specify all the members of the set, but you do not have to. mongos will auto‐ matically detect any members that were not included in the seed list. If you run sh.status(), you’ll see that MongoDB soon lists the shard as\n\nrs0/svr1.example.net:27017,svr2.example.net:27017,svr3.example.net:27017\n\nThe set name, rs0, is taken on as an identifier for this shard. If you ever want to remove this shard or migrate data to it, you can use rs0 to describe it. This works bet‐ ter than using a specific server (e.g., svr1.example.net), as replica set membership and status can change over time.\n\nOnce you’ve added the replica set as a shard you can convert your application from connecting to the replica set to connecting to the mongos. When you add the shard, mongos registers that all the databases in the replica set are “owned” by that shard, so it will pass through all queries to your new shard. mongos will also automatically han‐ dle failover for your application as your client library would: it will pass the errors through to you.\n\nTest failing over a shard’s primary in a development environment to ensure that your application handles the errors received from mongos correctly (they should be identi‐ cal to the errors that you receive from talking to the primary directly).\n\nOnce you have added a shard, you must set up all clients to send requests to the mongos instead of contacting the replica set. Shard‐ ing will not function correctly if some clients are still making requests to the replica set directly (not through the mongos). Switch all clients to contacting the mongos immediately after adding the shard and set up a firewall rule to ensure that they are unable to connect directly to the shard.\n\nStarting the Servers\n\n|\n\n309\n\nPrior to MongoDB 3.6 it was possible to create a standalone mongod as a shard. This is no longer an option in versions of MongoDB later than 3.6. All shards must be rep‐ lica sets.\n\nAdding Capacity When you want to add more capacity, you’ll need to add more shards. To add a new, empty shard, create a replica set. Make sure it has a distinct name from any of your other shards. Once it is initialized and has a primary, add it to your cluster by run‐ ning the addShard command through mongos, specifying the new replica set’s name and its hosts as seeds.\n\nIf you have several existing replica sets that are not shards, you can add all of them as new shards in your cluster so long as they do not have any database names in com‐ mon. For example, if you had one replica set with a blog database, one with a calendar database, and one with mail, tel, and music databases, you could add each replica set as a shard and end up with a cluster with three shards and five databases. However, if you had a fourth replica set that also had a database named tel, mongos would refuse to add it to the cluster.\n\nSharding Data MongoDB won’t distribute your data automatically until you tell it how to do so. You must explicitly tell both the database and the collection that you want them to be dis‐ tributed. For example, suppose you wanted to shard the artists collection in the music database on the \"name\" key. First, you’d enable sharding for the database:\n\n> db.enableSharding(\"music\")\n\nSharding a database is always a prerequisite to sharding one of its collections.\n\nOnce you’ve enabled sharding on the database level, you can shard a collection by running sh.shardCollection():\n\n> sh.shardCollection(\"music.artists\", {\"name\" : 1})\n\nNow the artists collection will be sharded by the \"name\" key. If you are sharding an existing collection there must be an index on the \"name\" field; otherwise, the shard Collection call will return an error. If you get an error, create the index (mongos will return the index it suggests as part of the error message) and retry the shardCollec tion command.\n\nIf the collection you are sharding does not yet exist, mongos will automatically create the shard key index for you.\n\nThe shardCollection command splits the collection into chunks, which are the units MongoDB uses to move data around. Once the command returns successfully,\n\n310\n\n|\n\nChapter 15: Configuring Sharding\n\nMongoDB will begin balancing the collection across the shards in your cluster. This process is not instantaneous. For large collections it may take hours to finish this ini‐ tial balancing. This time can be reduced with presplitting where chunks are created on the shards prior to loading the data. Data loaded after this point will be inserted directly to the current shard without requiring additional balancing.\n\nHow MongoDB Tracks Cluster Data Each mongos must always know where to find a document, given its shard key. Theo‐ retically, MongoDB could track where each and every document lived, but this becomes unwieldy for collections with millions or billions of documents. Thus, Mon‐ goDB groups documents into chunks, which are documents in a given range of the shard key. A chunk always lives on a single shard, so MongoDB can keep a small table of chunks mapped to shards.\n\nFor example, if a user collection’s shard key is {\"age\" : 1}, one chunk might be all documents with an \"age\" field between 3 and 17. If mongos gets a query for {\"age\" : 5}, it can route the query to the shard where this chunk lives.\n\nAs writes occur, the number and size of the documents in a chunk might change. Inserts can make a chunk contain more documents, and removes fewer. For example, if we were making a game for children and preteens, our chunk for ages 3−17 might get larger and larger (one would hope). Almost all of our users would be in that chunk and so would be on a single shard, somewhat defeating the point of distribut‐ ing our data. Thus, once a chunk grows to a certain size, MongoDB automatically splits it into two smaller chunks. In this example, the original chunk might be split into one chunk containing documents with ages 3 through 11 and another with ages 12 through 17. Note that these two chunks still cover the entire age range that the original chunk covered: 3−17. As these new chunks grow, they can be split into still smaller chunks until there is a chunk for each age.\n\nYou cannot have chunks with overlapping ranges, like 3−15 and 12−17. If you could, MongoDB would need to check both chunks when attempting to find an age in the overlap, like 14. It is more efficient to only have to look in one place, particularly once chunks begin moving around the cluster.\n\nA document always belongs to one and only one chunk. One consequence of this rule is that you cannot use an array field as your shard key, since MongoDB creates multi‐ ple index entries for arrays. For example, if a document had [5, 26, 83] in its \"age\" field, it would belong in up to three chunks.\n\nHow MongoDB Tracks Cluster Data\n\n|\n\n311",
      "page_number": 309
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 319-328)",
      "start_page": 319,
      "end_page": 328,
      "detection_method": "topic_boundary",
      "content": "A common misconception is that the data in a chunk is physically grouped on disk. This is incorrect: chunks have no effect on how mongod stores collection data.\n\nChunk Ranges Each chunk is described by the range it contains. A newly sharded collection starts off with a single chunk, and every document lives in this chunk. This chunk’s bounds are negative infinity to infinity, shown as $minKey and $maxKey in the shell.\n\nAs this chunk grows, MongoDB will automatically split it into two chunks, with the range negative infinity to <some value> and <some value> to infinity. <some value> is the same for both chunks: the lower chunk contains everything up to (but not including) <some value>, and the upper chunk contains <some value> and every‐ thing higher.\n\nThis may be more intuitive with an example. Suppose we were sharding by \"age\" as described earlier. All documents with \"age\" between 3 and 17 are contained in one chunk: 3 ≤ \"age\" < 17. When this is split, we end up with two ranges: 3 ≤ \"age\" < 12 in one chunk and 12 ≤ \"age\" < 17 in the other. 12 is called the split point.\n\nChunk information is stored in the config.chunks collection. If you looked at the con‐ tents of that collection, you’d see documents that looked something like this (some fields have been omitted for clarity):\n\n> db.chunks.find(criteria, {\"min\" : 1, \"max\" : 1}) { \"_id\" : \"test.users-age_-100.0\", \"min\" : {\"age\" : -100}, \"max\" : {\"age\" : 23} } { \"_id\" : \"test.users-age_23.0\", \"min\" : {\"age\" : 23}, \"max\" : {\"age\" : 100} } { \"_id\" : \"test.users-age_100.0\", \"min\" : {\"age\" : 100}, \"max\" : {\"age\" : 1000} }\n\nBased on the config.chunks documents shown, here are a few examples of where vari‐ ous documents would live:\n\n312\n\n|\n\nChapter 15: Configuring Sharding\n\n{\"_id\" : 123, \"age\" : 50}\n\nThis document would live in the second chunk, as that chunk contains all docu‐ ments with \"age\" between 23 and 100.\n\n{\"_id\" : 456, \"age\" : 100}\n\nThis document would live in the third chunk, as lower bounds are inclusive. The second chunk contains all documents up to \"age\" : 100, but not any documents where \"age\" equals 100.\n\n{\"_id\" : 789, \"age\" : -101}\n\nThis document would not be in any of these chunks. It would be in some chunk with a range lower than the first chunk’s.\n\nWith a compound shard key, shard ranges work the same way that sorting by the two keys would work. For example, suppose that we had a shard key on {\"username\" : 1, \"age\" : 1}. Then we might have chunk ranges such as:\n\n{ \"_id\" : \"test.users-username_MinKeyage_MinKey\", \"min\" : { \"username\" : { \"$minKey\" : 1 }, \"age\" : { \"$minKey\" : 1 } }, \"max\" : { \"username\" : \"user107487\", \"age\" : 73 } } { \"_id\" : \"test.users-username_\\\"user107487\\\"age_73.0\", \"min\" : { \"username\" : \"user107487\", \"age\" : 73 }, \"max\" : { \"username\" : \"user114978\", \"age\" : 119 } } { \"_id\" : \"test.users-username_\\\"user114978\\\"age_119.0\", \"min\" : { \"username\" : \"user114978\", \"age\" : 119 }, \"max\" : { \"username\" : \"user122468\", \"age\" : 68 } }\n\nHow MongoDB Tracks Cluster Data\n\n|\n\n313\n\nThus, mongos can easily find which chunk someone with a given username (or a given username and age) lives in. However, given just an age, mongos would have to check all, or almost all, of the chunks. If we wanted to be able to target queries on age to the right chunk, we’d have to use the “opposite” shard key: {\"age\" : 1, \"user name\" : 1}. This is often a point of confusion: a range over the second half of a shard key will cut across multiple chunks.\n\nSplitting Chunks Each shard primary mongod tracks their current chunks and, once they reach a cer‐ tain threshold, checks if the chunk needs to be split, as shown in Figures 15-1 and 15-2. If the chunk does need to be split, the mongod will request the global chunk size configuration value from the config servers. It will then perform the chunk split and update the metadata on the config servers. New chunk documents are created on the config servers and the old chunk’s range (\"max\") is modified. If the chunk is the top chunk of the shard, then the mongod will request the balancer move this chunk to a different shard. The idea is to prevent a shard from becoming “hot” where the shard key uses a monotonically increasing key.\n\nA shard may not be able to find any split points, though, even for a large chunk, as there are a limited number of ways to legally split a chunk. Any two documents with the same shard key must live in the same chunk, so chunks can only be split between documents where the shard key’s value changes. For example, if the shard key was \"age\", the following chunk could be split at the points where the shard key changed, as indicated:\n\n{\"age\" : 13, \"username\" : \"ian\"} {\"age\" : 13, \"username\" : \"randolph\"} ------------ // split point {\"age\" : 14, \"username\" : \"randolph\"} {\"age\" : 14, \"username\" : \"eric\"} {\"age\" : 14, \"username\" : \"hari\"} {\"age\" : 14, \"username\" : \"mathias\"} ------------ // split point {\"age\" : 15, \"username\" : \"greg\"} {\"age\" : 15, \"username\" : \"andrew\"}\n\nThe primary mongod for the shard only requests that the top chunk for a shard when split be moved to the balancer. The other chunks will remain on the shard unless manually moved.\n\nIf, however, the chunk contained the following documents, it could not be split (unless the application started inserting fractional ages):\n\n{\"age\" : 12, \"username\" : \"kevin\"} {\"age\" : 12, \"username\" : \"spencer\"} {\"age\" : 12, \"username\" : \"alberto\"} {\"age\" : 12, \"username\" : \"tad\"}\n\n314\n\n|\n\nChapter 15: Configuring Sharding\n\nThus, having a variety of values for your shard key is important. Other important properties will be covered in the next chapter.\n\nIf one of the config servers is down when a mongod tries to do a split, the mongod won’t be able to update the metadata (as shown in Figure 15-3). All config servers must be up and reachable for splits to happen. If the mongod continues to receive write requests for the chunk, it will keep trying to split the chunk and fail. As long as the config servers are not healthy, splits will continue not to work, and all the split attempts can slow down the mongod and the shard involved (which repeats the pro‐ cess shown in Figures 15-1 through 15-3 for each incoming write). This process of mongod repeatedly attempting to split a chunk and being unable to is called a split storm. The only way to prevent split storms is to ensure that your config servers are up and healthy as much of the time as possible.\n\nFigure 15-1. When a client writes to a chunk, the mongod will check its split threshold for the chunk\n\nFigure 15-2. If the split threshold has been reached, the mongod will send a request to the balancer to migrate the top chunk; otherwise the chunk remains on the shard\n\nHow MongoDB Tracks Cluster Data\n\n|\n\n315\n\nFigure 15-3. The mongod chooses a split point and attempts to inform the config server, but cannot reach it; thus, it is still over its split threshold for the chunk and any subse‐ quent writes will trigger this process again\n\nThe Balancer The balancer is responsible for migrating data. It regularly checks for imbalances between shards and, if it finds an imbalance, will begin migrating chunks. In Mon‐ goDB version 3.4+, the balancer is located on the primary member of the config server replica set; prior to this version, each mongos used to play the part of “the bal‐ ancer” occasionally.\n\nThe balancer is a background process on the primary of the config server replica set, which monitors the number of chunks on each shard. It becomes active only when a shard’s number of chunks reaches a specific migration threshold.\n\nIn MongoDB 3.4+, the number of concurrent migrations increased to one migration per shard with a maximum number of concurrent migrations being half the total number of shards. In earlier ver‐ sions only one concurrent migration in total was supported.\n\nAssuming that some collections have hit the threshold, the balancer will begin migrating chunks. It chooses a chunk from the overloaded shard and asks the shard if it should split the chunk before migrating. Once it does any necessary splits, it migrates the chunk(s) to a machine with fewer chunks.\n\nAn application using the cluster does not need be aware that the data is moving: all reads and writes are routed to the old chunk until the move is complete. Once the metadata is updated, any mongos process attempting to access the data in the old location will get an error. These errors should not be visible to the client: the mongos will silently handle the error and retry the operation on the new shard.\n\nThis is a common cause of errors you might see in mongos logs that relate to being “unable to setShardVersion.” When a mongos gets this type of error, it looks up the new location of the data from the config servers, updates its chunk table, and attempts the request again. If it successfully retrieves the data from the new location,\n\n316\n\n|\n\nChapter 15: Configuring Sharding\n\nit will return it to the client as though nothing went wrong (but it will print a message in the log that the error occurred).\n\nIf the mongos is unable to retrieve the new chunk location because the config servers are unavailable, it will return an error to the client. This is another reason why it is important to always have config servers up and healthy.\n\nCollations Collations in MongoDB allow for the specification of language-specific rules for string comparison. Examples of these rules include how lettercase and accent marks are compared. It is possible to shard a collection that is a default collation. There are two requirements: the collection must have an index whose prefix is the shard key, and the index must also have the collation { locale: \"simple\" }.\n\nChange Streams Change Streams allow applications to track real-time changes to the data in the data‐ base. Prior to MongoDB 3.6, this was only possible by tailing the oplog and was a complex error-prone operation. Change streams provide a subscription mechanism for all data changes on a collection, a set of collections, a database, or across a full deployment. The aggregation framework is used by this feature. It allows applications to filter for specific changes or to transform the change notifications received. In a sharded cluster, all change stream operations must be issued against a mongos.\n\nThe changes across a sharded cluster are kept ordered through the use of a global log‐ ical clock. This guarantees the order of changes, and stream notifications can be safely interpreted by the order of their receipt. The mongos needs to check with each shard upon receipt of a change notification, to ensure that no shard has seen more recent changes. The activity level of the cluster and the geographical distribution of the shards can both impact the response time for this checking. The use of notifica‐ tion filters can improve the response time in these situations.\n\nThere are a few notes and caveats when using change streams with a sharded cluster. You open a change stream by issuing an open change stream operation. In sharded deployments, this must be issued against a mongos. If an update operation with multi: true is run against a sharded collection with an open change stream, then it is possible for notifications to be sent for orphaned docu‐ ments. If a shard is removed, it may cause an open change stream cursor to close—furthermore, that cursor may not be fully resumable.\n\nCollations\n\n|\n\n317\n\nCHAPTER 16 Choosing a Shard Key\n\nThe most important task when using sharding is choosing how your data will be dis‐ tributed. To make intelligent choices about this, you have to understand how MongoDB distributes data. This chapter helps you make a good choice of shard key by covering:\n\nHow to decide among multiple possible shard keys\n\nShard keys for several use cases\n\nWhat you can’t use as a shard key\n\nSome alternative strategies if you want to customize how data is distributed\n\nHow to manually shard your data\n\nIt assumes that you understand the basic components of sharding as covered in the previous two chapters.\n\nTaking Stock of Your Usage When you shard a collection you choose a field or two to use to split up the data. This key (or keys) is called a shard key. Once you shard a collection you cannot change your shard key, so it is important to choose correctly.\n\nTo choose a good shard key, you need to understand your workload and how your shard key is going to distribute your application’s requests. This can be difficult to picture, so try to work out some examples—or, even better, try it out on a backup dataset with sample traffic. This section has lots of diagrams and explanations, but there is no substitute for trying it on your own data.\n\nFor each collection that you’re planning to shard, start by answering the following questions:\n\n319\n\nHow many shards are you planning to grow to? A three-shard cluster has a great deal more flexibility than a thousand-shard cluster. As a cluster gets larger, you should not plan to fire off queries that can hit all shards, so almost all queries must include the shard key.\n\nAre you sharding to decrease read or write latency? (Latency refers to how long something takes; e.g., a write takes 20 ms, but you need it to take 10 ms.) Decreasing write latency usually involves sending requests to geographically closer or more powerful machines.\n\nAre you sharding to increase read or write throughput? (Throughput refers to how many requests the cluster can handle at the same time; e.g., the cluster can do 1,000 writes in 20 ms, but you need it to do 5,000 writes in 20 ms.) Increasing throughput usually involves adding more parallelization and making sure that requests are distributed evenly across the cluster.\n\nAre you sharding to increase system resources (e.g., give MongoDB more RAM per GB of data)? If so, you want to keep the working set size as small as possible.\n\nUse these answers to evaluate the following shard key descriptions and decide whether the shard key you’re considering would work well in your situation. Does it give you the targeted queries that you need? Does it change the throughput or latency of your system in the ways you need? If you need a compact working set, does it pro‐ vide that?\n\nPicturing Distributions The most common ways people choose to split their data are via ascending, random, and location-based keys. There are other types of keys that could be used, but most use cases fall into one of these categories. The different types of distributions are dis‐ cussed in the following sections.\n\nAscending Shard Keys Ascending shard keys are generally something like a \"date\" field or ObjectId—any‐ thing that steadily increases over time. An autoincrementing primary key is another example of an ascending field, albeit one that doesn’t show up in MongoDB much (unless you’re importing from another database).\n\nSuppose that we shard on an ascending field, like \"_id\" on a collection using ObjectIds. If we shard on \"_id\", then the data will be split into chunks of \"_id\" ranges, as in Figure 16-1. These chunks will be distributed across our sharded cluster of, let’s say, three shards, as shown in Figure 16-2.\n\n320\n\n|\n\nChapter 16: Choosing a Shard Key\n\nFigure 16-1. The collection is split into ranges of ObjectIds; each range is a chunk\n\nSuppose we create a new document. Which chunk will it be in? The answer is the chunk with the range ObjectId(\"5112fae0b4a4b396ff9d0ee5\") through $maxKey. This is called the max chunk, as it is the chunk containing $maxKey.\n\nIf we insert another document, it will also be in the max chunk. In fact, every subse‐ quent insert will be into the max chunk! Every insert’s \"_id\" field will be closer to infinity than the previous one (because ObjectIds are always ascending), so they will all go into the max chunk.\n\nPicturing Distributions\n\n|\n\n321\n\nFigure 16-2. Chunks are distributed across shards in a random order\n\nThis has a couple of interesting (and often undesirable) properties. First, all of your writes will be routed to one shard (shard0002, in this case). This chunk will be the only one growing and splitting, as it is the only one that receives inserts. As you insert data, new chunks will “fall off” of this chunk, as shown in Figure 16-3.\n\n322\n\n|\n\nChapter 16: Choosing a Shard Key",
      "page_number": 319
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 329-338)",
      "start_page": 329,
      "end_page": 338,
      "detection_method": "topic_boundary",
      "content": "Figure 16-3. The max chunk continues growing and being split into multiple chunks\n\nThis pattern often makes it more difficult for MongoDB to keep chunks evenly bal‐ anced because all the chunks are being created by one shard. Therefore, MongoDB must constantly move chunks to other shards instead of correcting the small imbal‐ ances that might occur in more evenly distributed systems.\n\nIn MongoDB 4.2, the move of the autosplit functionality to the shard primary mongod added top chunk optimization to address the ascending shard key pattern. The balancer will decide in which other shard to place the top chunk. This helps avoid a situation in which all new chunks are created on just one shard.\n\nRandomly Distributed Shard Keys At the other end of the spectrum are randomly distributed shard keys. Randomly dis‐ tributed keys could be usernames, email addresses, UUIDs, MD5 hashes, or any other key that has no identifiable pattern in your dataset.\n\nSuppose the shard key is a random number between 0 and 1. We’ll end up with a ran‐ dom distribution of chunks on the various shards, as shown in Figure 16-4.\n\nPicturing Distributions\n\n|\n\n323\n\nFigure 16-4. As in the previous section, chunks are distributed randomly around the cluster\n\nAs more data is inserted, the data’s random nature means that inserts should hit every chunk fairly evenly. You can prove this to yourself by inserting 10,000 documents and seeing where they end up:\n\n> var servers = {} > var findShard = function (id) { ... var explain = db.random.find({_id:id}).explain(); ... for (var i in explain.shards) { ... var server = explain.shards[i][0];\n\n324\n\n|\n\nChapter 16: Choosing a Shard Key\n\n... if (server.n == 1) { ... if (server.server in servers) { ... servers[server.server]++; ... } else { ... servers[server.server] = 1; ... } ... } ... } ... } > for (var i = 0; i < 10000; i++) { ... var id = ObjectId(); ... db.random.insert({\"_id\" : id, \"x\" : Math.random()}); ... findShard(id); ... } > servers { \"spock:30001\" : 2942, \"spock:30002\" : 4332, \"spock:30000\" : 2726 }\n\nAs writes are randomly distributed, the shards should grow at roughly the same rate, limiting the number of migrates that need to occur.\n\nThe only downside to randomly distributed shard keys is that MongoDB isn’t effi‐ cient at randomly accessing data beyond the size of RAM. However, if you have the capacity or don’t mind the performance hit, random keys nicely distribute load across your cluster.\n\nLocation-Based Shard Keys Location-based shard keys may be things like a user’s IP, latitude and longitude, or address. They’re not necessarily related to a physical location field: the “location” might be a more abstract way that data should be grouped together. In any case, a location-based key is a key where documents with some similarity fall into a range based on this field. This can be handy for both putting data close to its users and keeping related data together on disk. It may also be a legal requirement to remain compliant with GDPR or other similar data privacy legislation. MongoDB uses Zoned Sharding to manage this.\n\nIn MongoDB 4.0.3+, you can define the zones and the zone ranges prior to sharding a collection, which populates chunks for both the zone ranges and for the shard key values as well as performing an initial chunk distribution of these. This greatly reduces the com‐ plexity for sharded zone setup.\n\nPicturing Distributions\n\n|\n\n325\n\nFor example, suppose we have a collection of documents that are sharded on IP address. Documents will be organized into chunks based on their IPs and randomly spread across the cluster, as shown in Figure 16-5.\n\nFigure 16-5. A sample distribution of chunks in the IP address collection\n\nIf we wanted certain chunk ranges to be attached to certain shards, we could zone these shards and then assign chunk ranges to each zone. In this example, suppose that we wanted to keep certain IP blocks on certain shards: say, 56.*.*.* (the United States Postal Service’s IP block) on shard0000 and 17.*.*.* (Apple’s IP block) on either shard0000 or shard0002. We do not care where the other IPs live. We could request that the balancer do this by setting up zones:\n\n> sh.addShardToZone(\"shard0000\", \"USPS\") > sh.addShardToZone(\"shard0000\", \"Apple\") > sh.addShardToZone(\"shard0002\", \"Apple\")\n\nNext, we create the rules:\n\n> sh.updateZoneKeyRange(\"test.ips\", {\"ip\" : \"056.000.000.000\"}, ... {\"ip\" : \"057.000.000.000\"}, \"USPS\")\n\nThis attaches all IPs greater than or equal to 56.0.0.0 and less than 57.0.0.0 to the shard zoned as \"USPS\". Next, we add a rule for Apple:\n\n> sh.updateZoneKeyRange(\"test.ips\", {\"ip\" : \"017.000.000.000\"}, ... {\"ip\" : \"018.000.000.000\"}, \"Apple\")\n\nWhen the balancer moves chunks, it will attempt to move chunks with those ranges to those shards. Note that this process is not immediate. Chunks that were not cov‐ ered by a zone key range will be moved around normally. The balancer will continue attempting to distribute chunks evenly among shards.\n\n326\n\n|\n\nChapter 16: Choosing a Shard Key\n\nShard Key Strategies This section presents a number of shard key options for various types of applications.\n\nHashed Shard Key For loading data as fast as possible, hashed shard keys are the best option. A hashed shard key can make any field randomly distributed, so it is a good choice if you’re going to be using an ascending key in a lot of queries but want writes to be randomly distributed.\n\nThe trade-off is that you can never do a targeted range query with a hashed shard key. If you will not be doing range queries, though, hashed shard keys are a good option.\n\nTo create a hashed shard key, first create a hashed index:\n\n> db.users.createIndex({\"username\" : \"hashed\"})\n\nNext, shard the collection with:\n\n> sh.shardCollection(\"app.users\", {\"username\" : \"hashed\"}) { \"collectionsharded\" : \"app.users\", \"ok\" : 1 }\n\nIf you create a hashed shard key on a nonexistent collection, shardCollection behaves interestingly: it assumes that you want evenly distributed chunks, so it imme‐ diately creates a bunch of empty chunks and distributes them around your cluster. For example, suppose our cluster looked like this before creating the hashed shard key:\n\n> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"version\" : 3 } shards: { \"_id\" : \"shard0000\", \"host\" : \"localhost:30000\" } { \"_id\" : \"shard0001\", \"host\" : \"localhost:30001\" } { \"_id\" : \"shard0002\", \"host\" : \"localhost:30002\" } databases: { \"_id\" : \"admin\", \"partitioned\" : false, \"primary\" : \"config\" } { \"_id\" : \"test\", \"partitioned\" : true, \"primary\" : \"shard0001\" }\n\nImmediately after shardCollection returns there are two chunks on each shard, evenly distributing the key space across the cluster:\n\n> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"version\" : 3 } shards: { \"_id\" : \"shard0000\", \"host\" : \"localhost:30000\" } { \"_id\" : \"shard0001\", \"host\" : \"localhost:30001\" } { \"_id\" : \"shard0002\", \"host\" : \"localhost:30002\" }\n\nShard Key Strategies\n\n|\n\n327\n\ndatabases: { \"_id\" : \"admin\", \"partitioned\" : false, \"primary\" : \"config\" } { \"_id\" : \"test\", \"partitioned\" : true, \"primary\" : \"shard0001\" } test.foo shard key: { \"username\" : \"hashed\" } chunks: shard0000 2 shard0001 2 shard0002 2 { \"username\" : { \"$MinKey\" : true } } -->> { \"username\" : NumberLong(\"-6148914691236517204\") } on : shard0000 { \"t\" : 3000, \"i\" : 2 } { \"username\" : NumberLong(\"-6148914691236517204\") } -->> { \"username\" : NumberLong(\"-3074457345618258602\") } on : shard0000 { \"t\" : 3000, \"i\" : 3 } { \"username\" : NumberLong(\"-3074457345618258602\") } -->> { \"username\" : NumberLong(0) } on : shard0001 { \"t\" : 3000, \"i\" : 4 } { \"username\" : NumberLong(0) } -->> { \"username\" : NumberLong(\"3074457345618258602\") } on : shard0001 { \"t\" : 3000, \"i\" : 5 } { \"username\" : NumberLong(\"3074457345618258602\") } -->> { \"username\" : NumberLong(\"6148914691236517204\") } on : shard0002 { \"t\" : 3000, \"i\" : 6 } { \"username\" : NumberLong(\"6148914691236517204\") } -->> { \"username\" : { \"$MaxKey\" : true } } on : shard0002 { \"t\" : 3000, \"i\" : 7 }\n\nNote that there are no documents in the collection yet, but when you start inserting them, writes should be evenly distributed across the shards from the get-go. Ordinar‐ ily, you would have to wait for chunks to grow, split, and move to start writing to other shards. With this automatic priming, you’ll immediately have chunk ranges on all shards.\n\nThere are some limitations on what your shard key can be if you’re using a hashed shard key. First, you cannot use the unique option. As with other shard keys, you cannot use array fields. Finally, be aware that floating-point values will be rounded to whole numbers before hashing, so 1 and 1.999999 will both be hashed to the same value.\n\nHashed Shard Keys for GridFS Before attempting to shard GridFS collections, make sure that you understand how GridFS stores data (see Chapter 6 for an explanation).\n\nIn the following explanation, the term “chunks” is overloaded since GridFS splits files into chunks and sharding splits collections into chunks. Thus, the two types of chunks are referred to as “GridFS chunks” and “sharding chunks.”\n\n328\n\n|\n\nChapter 16: Choosing a Shard Key\n\nGridFS collections are generally excellent candidates for sharding, as they contain massive amounts of file data. However, neither of the indexes that are automatically created on fs.chunks are particularly good shard keys: {\"_id\" : 1} is an ascending key and {\"files_id\" : 1, \"n\" : 1} picks up fs.files’s \"_id\" field, so it is also an ascending key.\n\nHowever, if you create a hashed index on the \"files_id\" field, each file will be ran‐ domly distributed across the cluster, and a file will always be contained in a single chunk. This is the best of both worlds: writes will go to all shards evenly and reading a file’s data will only ever have to hit a single shard.\n\nTo set this up, you must create a new index on {\"files_id\" : \"hashed\"} (as of this writing, mongos cannot use a subset of the compound index as a shard key). Then shard the collection on this field:\n\n> db.fs.chunks.ensureIndex({\"files_id\" : \"hashed\"}) > sh.shardCollection(\"test.fs.chunks\", {\"files_id\" : \"hashed\"}) { \"collectionsharded\" : \"test.fs.chunks\", \"ok\" : 1 }\n\nAs a side note, the fs.files collection may or may not need to be sharded, as it will be much smaller than fs.chunks. You can shard it if you would like, but it is not likely to be necessary.\n\nThe Firehose Strategy If you have some servers that are more powerful than others, you might want to let them handle proportionally more load than your less-powerful servers. For example, suppose you have one shard that can handle 10 times the load of your other machines. Luckily, you have 10 other shards. You could force all inserts to go to the more powerful shard, and then allow the balancer to move older chunks to the other shards. This would give lower-latency writes.\n\nTo use this strategy, we have to pin the highest chunk to the more powerful shard. First, we zone this shard:\n\n> sh.addShardToZone(\"<shard-name>\", \"10x\")\n\nThen we pin the current value of the ascending key through infinity to that shard, so all new writes go to it:\n\n> sh.updateZoneKeyRange(\"<dbName.collName>\", {\"_id\" : ObjectId()}, ... {\"_id\" : MaxKey}, \"10x\")\n\nNow all inserts will be routed to this last chunk, which will always live on the shard zoned \"10x\".\n\nHowever, ranges from now through infinity will be trapped on this shard unless we modify the zone key range. To get around this, we could set up a cron job to update the key range once a day, like this:\n\nShard Key Strategies\n\n|\n\n329\n\n> use config > var zone = db.tags.findOne({\"ns\" : \"<dbName.collName>\", ... \"max\" : {\"<shardKey>\" : MaxKey}}) > zone.min.<shardKey> = ObjectId() > db.tags.save(zone)\n\nThen all of the previous day’s chunks would be able to move to other shards.\n\nAnother downside of this strategy is that it requires some changes to scale. If your most powerful server can no longer handle the number of writes coming in, there is no trivial way to split the load between this server and another.\n\nIf you do not have a high-performance server to firehose into or you are not using zone sharding, do not use an ascending key as the shard key. If you do, all writes will go to a single shard.\n\nMulti-Hotspot Standalone mongod servers are most efficient when doing ascending writes. This con‐ flicts with sharding, in that sharding is most efficient when writes are spread over the cluster. The technique described here basically creates multiple hotspots—optimally several on each shard—so that writes are evenly balanced across the cluster but, within a shard, ascending.\n\nTo accomplish this, we use a compound shard key. The first value in the compound key is a rough, random value with low-ish cardinality. You can picture each value in the first part of the shard key as a chunk, as shown in Figure 16-6. This will eventually work itself out as you insert more data, although it will probably never be divided up this neatly (right on the $minKey lines). However, if you insert enough data, you should eventually have approximately one chunk per random value. As you continue to insert data, you’ll end up with multiple chunks with the same random value, which brings us to the second part of the shard key.\n\n330\n\n|\n\nChapter 16: Choosing a Shard Key\n\nFigure 16-6. A subset of the chunks: each chunk contains a single state and a range of “_id” values\n\nThe second part of the shard key is an ascending key. This means that within a chunk, values are always increasing, as shown in the sample documents in Figure 16-7. Thus, if you had one chunk per shard, you’d have the perfect setup: ascending writes on every shard, as shown in Figure 16-8. Of course, having n chunks with n hotspots spread across n shards isn’t very extensible: add a new shard and it won’t get any writes because there’s no hotspot chunk to put on it. Thus, you want a few hotspot chunks per shard (to give you room to grow), but not too many. Having a few hotspot chunks will keep the effectiveness of ascending writes, but having, say, a thousand hotspots on a shard will end up being equivalent to random writes.\n\nShard Key Strategies\n\n|\n\n331\n\nFigure 16-7. A sample list of inserted documents (note that all “_id” values are increasing)\n\n332\n\n|\n\nChapter 16: Choosing a Shard Key",
      "page_number": 329
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 339-347)",
      "start_page": 339,
      "end_page": 347,
      "detection_method": "topic_boundary",
      "content": "Figure 16-8. The inserted documents, split into chunks (note that, within each chunk, the “_id” values are increasing)\n\nYou can picture this setup as each chunk being a stack of ascending documents. There are multiple stacks on each shard, each ascending until the chunk is split. Once a chunk is split, only one of the new chunks will be a hotspot chunk: the other chunk will essentially be “dead” and never grow again. If the stacks are evenly distributed across the shards, writes will be evenly distributed.\n\nShard Key Strategies\n\n|\n\n333\n\nShard Key Rules and Guidelines There are several practical restrictions to be aware of before choosing a shard key.\n\nDetermining which key to shard on and creating shard keys should be reminiscent of indexing because the two concepts are similar. In fact, often your shard key may just be the index you use most often (or some variation on it).\n\nShard Key Limitations Shard keys cannot be arrays. sh.shardCollection() will fail if any key has an array value, and inserting an array into that field is not allowed.\n\nOnce inserted, a document’s shard key value may be modified unless the shard key field is an immutable _id field. In older versions of MongoDB prior to 4.2, it was not possible to modify a document’s shard key value.\n\nMost special types of indexes cannot be used for shard keys. In particular, you cannot shard on a geospatial index. Using a hashed index for a shard key is allowed, as cov‐ ered previously.\n\nShard Key Cardinality Whether your shard key jumps around or increases steadily, it is important to choose a key with values that will vary. As with indexes, sharding performs better on high- cardinality fields. If, for example, you had a \"logLevel\" key that had only values \"DEBUG\", \"WARN\", or \"ERROR\", MongoDB wouldn’t be able to break up your data into more than three chunks (because there would be only three different values for the shard key). If you have a key with little variation and want to use it as a shard key anyway, you can do so by creating a compound shard key on that key and a key that varies more, like \"logLevel\" and \"timestamp\". It is important that the combination of keys has high cardinality.\n\nControlling Data Distribution Sometimes, automatic data distribution will not fit your requirements. This section gives you some options beyond choosing a shard key and allowing MongoDB to do everything automatically.\n\nAs your cluster gets larger or busier, these solutions become less practical. However, for small clusters, you may want more control.\n\n334\n\n|\n\nChapter 16: Choosing a Shard Key\n\nUsing a Cluster for Multiple Databases and Collections MongoDB evenly distributes collections across every shard in your cluster, which works well if you’re storing homogeneous data. However, if you have a log collection that is “lower value” than your other data, you might not want it taking up space on your more expensive servers. Or, if you have one powerful shard, you might want to use it for only a real-time collection and not allow other collections to use it. You can create separate clusters, but you can also give MongoDB specific directions about where you want it to put certain data.\n\nTo set this up, use the sh.addShardToZone() helper in the shell:\n\n> sh.addShardToZone(\"shard0000\", \"high\") > // shard0001 - no zone > // shard0002 - no zone > // shard0003 - no zone > sh.addShardToZone(\"shard0004\", \"low\") > sh.addShardToZone(\"shard0005\", \"low\")\n\nThen you can assign different collections to different shards. For instance, for your super-important real-time collection:\n\n> sh.updateZoneKeyRange(\"super.important\", {\"<shardKey>\" : MinKey}, ... {\"<shardKey>\" : MaxKey}, \"high\")\n\nThis says, “for negative infinity to infinity for this collection, store it on shards tagged \"high\".” This means that no data from the super.important collection will be stored on any other server. Note that this does not affect how other collections are dis‐ tributed: they will still be evenly distributed between this shard and the others.\n\nYou can perform a similar operation to keep the log collection on a low-quality server:\n\n> sh.updateZoneKeyRange(\"some.logs\", {\"<shardKey>\" : MinKey}, ... {\"<shardKey>\" : MaxKey}, \"low\")\n\nThe log collection will now be split evenly between shard0004 and shard0005.\n\nAssigning a zone key range to a collection does not affect it instantly. It is an instruc‐ tion to the balancer stating that, when it runs, these are the viable targets to move the collection to. Thus, if the entire log collection is on shard0002 or evenly distributed among the shards, it will take a little while for all of the chunks to be migrated to shard0004 and shard0005.\n\nAs another example, perhaps you have a collection that you don’t want on the shard zoned \"high\", but you don’t care which other shard it goes on. You can zone all of the non-high-performance shards to create a new grouping. Shards can have as many zones as you need:\n\nControlling Data Distribution\n\n|\n\n335\n\n> sh.addShardToZone(\"shard0001\", \"whatever\") > sh.addShardToZone(\"shard0002\", \"whatever\") > sh.addShardToZone(\"shard0003\", \"whatever\") > sh.addShardToZone(\"shard0004\", \"whatever\") > sh.addShardToZone(\"shard0005\", \"whatever\")\n\nNow you can specify that you want this collection (call it normal.coll) distributed across these five shards:\n\n> sh.updateZoneKeyRange(\"normal.coll\", {\"<shardKey>\" : MinKey}, ... {\"<shardKey>\" : MaxKey}, \"whatever\")\n\nYou cannot assign collections dynamically—i.e., you can’t say, “when a collection is created, randomly home it to a shard.” How‐ ever, you could have a cron job that went through and did this for you.\n\nIf you make a mistake or change your mind, you can remove a shard from a zone with sh.removeShardFromZone():\n\n> sh.removeShardFromZone(\"shard0005\", \"whatever\")\n\nIf you remove all shards from zones described by a zone key range (e.g., if you remove shard0000 from the zone \"high\"), the balancer won’t distribute the data any‐ where because there aren’t any valid locations listed. All the data will still be readable and writable; it just won’t be able to migrate until you modify your tags or tag ranges.\n\nTo remove a key range from a zone, use sh.removeRangeFromZone(). The following is an example. The range specified must be an exact match to a range previously defined for the namespace some.logs and a given zone:\n\n> sh.removeRangeFromZone(\"some.logs\", {\"<shardKey>\" : MinKey}, ... {\"<shardKey>\" : MaxKey})\n\nManual Sharding Sometimes, for complex requirements or special situations, you may prefer to have complete control over which data is distributed where. You can turn off the balancer if you don’t want data to be automatically distributed and use the moveChunk com‐ mand to manually distribute data.\n\nTo turn off the balancer, connect to a mongos (any mongos is fine) using the mongo shell and disable the balancer using the shell helper sh.stopBalancer():\n\n> sh.stopBalancer()\n\nIf there is currently a migrate in progress, this setting will not take effect until the migrate has completed. However, once any in-flight migrations have finished, the\n\n336\n\n|\n\nChapter 16: Choosing a Shard Key\n\nbalancer will stop moving data around. To verify no migrations are in progress after disabling, issue the following in the mongo shell:\n\n> use config > while(sh.isBalancerRunning()) { ... print(\"waiting...\"); ... sleep(1000); ... }\n\nOnce the balancer is off, you can move data around manually (if necessary). First, find out which chunks are where by looking at config.chunks:\n\n> db.chunks.find()\n\nNow, use the moveChunk command to migrate chunks to other shards. Specify the lower bound of the chunk to be migrated and give the name of the shard that you want to move the chunk to:\n\n> sh.moveChunk( ... \"test.manual.stuff\", ... {user_id: NumberLong(\"-1844674407370955160\")}, ... \"test-rs1\")\n\nHowever, unless you are in an exceptional situation, you should use MongoDB’s auto‐ matic sharding instead of doing it manually. If you end up with a hotspot on a shard that you weren’t expecting, you might end up with most of your data on that shard.\n\nIn particular, do not combine setting up unusual distributions manually with running the balancer. If the balancer detects an uneven number of chunks it will simply reshuffle all of your work to get the collection evenly balanced again. If you want uneven distribution of chunks, use the zone sharding technique discussed in “Using a Cluster for Multiple Databases and Collections” on page 335.\n\nControlling Data Distribution\n\n|\n\n337\n\nCHAPTER 17 Sharding Administration\n\nAs with replica sets, you have a number of options for administering sharded clusters. Manual administration is one option. These days it is becoming increasingly com‐ mon to use tools such as Ops Manager and Cloud Manager and the Atlas Database- as-a-Service (DBaaS) offering for all cluster administration. In this chapter, we will demonstrate how to administer a sharded cluster manually, including:\n\nInspecting the cluster’s state: who its members are, where data is held, and what connections are open\n\nAdding, removing, and changing members of a cluster\n\nAdministering data movement and manually moving data\n\nSeeing the Current State There are several helpers available to find out what data is where, what the shards are, and what the cluster is doing.\n\nGetting a Summary with sh.status() sh.status() gives you an overview of your shards, databases, and sharded collec‐ tions. If you have a small number of chunks, it will print a breakdown of which chunks are where as well. Otherwise it will simply give the collection’s shard key and report how many chunks each shard has:\n\n> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5,\n\n339\n\n340\n\n\"currentVersion\" : 6, \"clusterId\" : ObjectId(\"5bdf51ecf8c192ed922f3160\") } shards: { \"_id\" : \"shard01\", \"host\" : \"shard01/localhost:27018,localhost:27019,localhost:27020\", \"state\" : 1 } { \"_id\" : \"shard02\", \"host\" : \"shard02/localhost:27021,localhost:27022,localhost:27023\", \"state\" : 1 } { \"_id\" : \"shard03\", \"host\" : \"shard03/localhost:27024,localhost:27025,localhost:27026\", \"state\" : 1 } active mongoses: \"4.0.3\" : 1 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: 6 : Success databases: { \"_id\" : \"config\", \"primary\" : \"config\", \"partitioned\" : true } config.system.sessions shard key: { \"_id\" : 1 } unique: false balancing: true chunks: shard01 { \"_id\" : { \"$minKey\" : 1 } } -->> { \"_id\" : { \"$maxKey\" : 1 } } on : shard01 Timestamp(1, 0) { \"_id\" : \"video\", \"primary\" : \"shard02\", \"partitioned\" : true, \"version\" : { \"uuid\" : UUID(\"3d83d8b8-9260-4a6f-8d28-c3732d40d961\"), \"lastMod\" : 1 } } video.movies shard key: { \"imdbId\" : \"hashed\" } unique: false balancing: true chunks: shard01 3 shard02 4 shard03 3 { \"imdbId\" : { \"$minKey\" : 1 } } -->> { \"imdbId\" : NumberLong(\"-7262221363006655132\") } on : shard01 Timestamp(2, 0) { \"imdbId\" : NumberLong(\"-7262221363006655132\") } -->> { \"imdbId\" : NumberLong(\"-5315530662268120007\") } on : shard03 Timestamp(3, 0) { \"imdbId\" : NumberLong(\"-5315530662268120007\") } -->>\n\n1\n\n|\n\nChapter 17: Sharding Administration\n\n{ \"imdbId\" : NumberLong(\"-3362204802044524341\") } on : shard03 Timestamp(4, 0) { \"imdbId\" : NumberLong(\"-3362204802044524341\") } -->> { \"imdbId\" : NumberLong(\"-1412311662519947087\") } on : shard01 Timestamp(5, 0) { \"imdbId\" : NumberLong(\"-1412311662519947087\") } -->> { \"imdbId\" : NumberLong(\"524277486033652998\") } on : shard01 Timestamp(6, 0) { \"imdbId\" : NumberLong(\"524277486033652998\") } -->> { \"imdbId\" : NumberLong(\"2484315172280977547\") } on : shard03 Timestamp(7, 0) { \"imdbId\" : NumberLong(\"2484315172280977547\") } -->> { \"imdbId\" : NumberLong(\"4436141279217488250\") } on : shard02 Timestamp(7, 1) { \"imdbId\" : NumberLong(\"4436141279217488250\") } -->> { \"imdbId\" : NumberLong(\"6386258634539951337\") } on : shard02 Timestamp(1, 7) { \"imdbId\" : NumberLong(\"6386258634539951337\") } -->> { \"imdbId\" : NumberLong(\"8345072417171006784\") } on : shard02 Timestamp(1, 8) { \"imdbId\" : NumberLong(\"8345072417171006784\") } -->> { \"imdbId\" : { \"$maxKey\" : 1 } } on : shard02 Timestamp(1, 9)\n\nOnce there are more than a few chunks, sh.status() will summarize the chunk stats instead of printing each chunk. To see all chunks, run sh.status(true) (the true tells sh.status() to be verbose).\n\nAll the information sh.status() shows is gathered from your config database.\n\nSeeing Configuration Information All of the configuration information about your cluster is kept in collections in the config database on the config servers. The shell has several helpers for exposing this information in a more readable way. However, you can always directly query the con‐ fig database for metadata about your cluster.\n\nNever connect directly to your config servers, as you do not want to take the chance of accidentally changing or removing config server data. Instead, connect to the mongos process and use the config database to see its data, as you would for any other database:\n\n> use config\n\nIf you manipulate config data through mongos (instead of connect‐ ing directly to the config servers), mongos will ensure that all of your config servers stay in sync and prevent various dangerous actions like accidentally dropping the config database.\n\nSeeing the Current State\n\n|\n\n341\n\nIn general, you should not directly change any data in the config database (exceptions are noted in the following sections). If you change anything, you will generally have to restart all of your mongos servers to see its effect.\n\nThere are several collections in the config database. This section covers what each one contains and how it can be used.\n\nconfig.shards\n\nThe shards collection keeps track of all the shards in the cluster. A typical document in the shards collection might look something like this:\n\n> db.shards.find() { \"_id\" : \"shard01\", \"host\" : \"shard01/localhost:27018,localhost:27019,localhost:27020\", \"state\" : 1 } { \"_id\" : \"shard02\", \"host\" : \"shard02/localhost:27021,localhost:27022,localhost:27023\", \"state\" : 1 } { \"_id\" : \"shard03\", \"host\" : \"shard03/localhost:27024,localhost:27025,localhost:27026\", \"state\" : 1 }\n\nThe shard’s \"_id\" is picked up from the replica set name, so each replica set in your cluster must have a unique name.\n\nWhen you update your replica set configuration (e.g., adding or removing members), the \"host\" field will be updated automatically.\n\nconfig.databases\n\nThe databases collection keeps track of all of the databases, sharded and not, that the cluster knows about:\n\n> db.databases.find() { \"_id\" : \"video\", \"primary\" : \"shard02\", \"partitioned\" : true, \"version\" : { \"uuid\" : UUID(\"3d83d8b8-9260-4a6f-8d28-c3732d40d961\"), \"lastMod\" : 1 } }\n\nIf enableSharding has been run on a database, \"partitioned\" will be true. The \"pri mary\" is the database’s “home base.” By default, all new collections in that database will be created on the database’s primary shard.\n\nconfig.collections\n\nThe collections collection keeps track of all sharded collections (nonsharded collec‐ tions are not shown). A typical document looks something like this:\n\n> db.collections.find().pretty() { \"_id\" : \"config.system.sessions\",\n\n342\n\n|\n\nChapter 17: Sharding Administration",
      "page_number": 339
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 348-355)",
      "start_page": 348,
      "end_page": 355,
      "detection_method": "topic_boundary",
      "content": "\"lastmodEpoch\" : ObjectId(\"5bdf53122ad9c6907510c22d\"), \"lastmod\" : ISODate(\"1970-02-19T17:02:47.296Z\"), \"dropped\" : false, \"key\" : { \"_id\" : 1 }, \"unique\" : false, \"uuid\" : UUID(\"7584e4cd-fac4-4305-a9d4-bd73e93621bf\") } { \"_id\" : \"video.movies\", \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\"), \"lastmod\" : ISODate(\"1970-02-19T17:02:47.305Z\"), \"dropped\" : false, \"key\" : { \"imdbId\" : \"hashed\" }, \"unique\" : false, \"uuid\" : UUID(\"e6580ffa-fcd3-418f-aa1a-0dfb71bc1c41\") }\n\nThe important fields are:\n\n\"_id\"\n\nThe namespace of the collection.\n\n\"key\"\n\nThe shard key. In this case, it is a hashed shard key on \"imdbId\".\n\n\"unique\"\n\nIndicates that the shard key is not a unique index. By default, the shard key is not unique.\n\nconfig.chunks\n\nThe chunks collection keeps a record of each chunk in all the collections. A typical document in the chunks collection looks something like this:\n\n> db.chunks.find().skip(1).limit(1).pretty() { \"_id\" : \"video.movies-imdbId_MinKey\", \"lastmod\" : Timestamp(2, 0), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\"), \"ns\" : \"video.movies\", \"min\" : { \"imdbId\" : { \"$minKey\" : 1 } }, \"max\" : { \"imdbId\" : NumberLong(\"-7262221363006655132\") }, \"shard\" : \"shard01\", \"history\" : [\n\nSeeing the Current State\n\n|\n\n343\n\n{ \"validAfter\" : Timestamp(1541370579, 3096), \"shard\" : \"shard01\" } ] }\n\nThe most useful fields are:\n\n\"_id\"\n\nThe unique identifier for the chunk. Generally this is the namespace, shard key, and lower chunk boundary.\n\n\"ns\"\n\nThe collection that this chunk is from.\n\n\"min\"\n\nThe smallest value in the chunk’s range (inclusive).\n\n\"max\"\n\nAll values in the chunk are smaller than this value.\n\n\"shard\"\n\nWhich shard the chunk resides on.\n\nThe \"lastmod\" field tracks chunk versioning. For example, if the chunk \"video.movies-imdbId_MinKey\" were split into two chunks, we’d want a way of dis‐ tinguishing the new, smaller \"video.movies-imdbId_MinKey\" chunks from their pre‐ vious incarnation as a single chunk. Thus, the first component of the Timestamp value reflects the number of times a chunk has been migrated to a new shard. The second component of this value reflects the number of splits. The \"lastmodEpoch\" field specifies the collection’s creation epoch. It is used to differentiate requests for the same collection name in the cases where the collection was dropped and immediately recreated.\n\nsh.status() uses the config.chunks collection to gather most of its information.\n\nconfig.changelog\n\nThe changelog collection is useful for keeping track of what a cluster is doing, since it records all of the splits and migrations that have occurred.\n\nSplits are recorded in a document that looks like this:\n\n> db.changelog.find({what: \"split\"}).pretty() { \"_id\" : \"router1-2018-11-05T09:58:58.915-0500-5be05ab2f8c192ed922ffbe7\", \"server\" : \"bob\", \"clientAddr\" : \"127.0.0.1:64621\", \"time\" : ISODate(\"2018-11-05T14:58:58.915Z\"),\n\n344\n\n|\n\nChapter 17: Sharding Administration\n\n\"what\" : \"split\", \"ns\" : \"video.movies\", \"details\" : { \"before\" : { \"min\" : { \"imdbId\" : NumberLong(\"2484315172280977547\") }, \"max\" : { \"imdbId\" : NumberLong(\"4436141279217488250\") }, \"lastmod\" : Timestamp(9, 1), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\") }, \"left\" : { \"min\" : { \"imdbId\" : NumberLong(\"2484315172280977547\") }, \"max\" : { \"imdbId\" : NumberLong(\"3459137475094092005\") }, \"lastmod\" : Timestamp(9, 2), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\") }, \"right\" : { \"min\" : { \"imdbId\" : NumberLong(\"3459137475094092005\") }, \"max\" : { \"imdbId\" : NumberLong(\"4436141279217488250\") }, \"lastmod\" : Timestamp(9, 3), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\") } } }\n\nThe \"details\" field gives information about what the original document looked like and what it was split into.\n\nThis output shows what the first chunk split of a collection looks like. Note that the second component of \"lastmod\" for each new chunk was updated so that the values are Timestamp(9, 2) and Timestamp(9, 3), respectively.\n\nMigrations are a bit more complicated and actually create four separate changelog documents: one noting the start of the migrate, one for the “from” shard, one for the “to” shard, and one for the commit that occurs when the migration is finalized. The middle two documents are of interest because these give a breakdown of how long each step in the process took. This can give you an idea of whether it’s the disk, net‐ work, or something else that is causing a bottleneck on migrates.\n\nFor example, the document created by the “from” shard looks like this:\n\nSeeing the Current State\n\n|\n\n345\n\n> db.changelog.findOne({what: \"moveChunk.to\"}) { \"_id\" : \"router1-2018-11-04T17:29:39.702-0500-5bdf72d32ad9c69075112f08\", \"server\" : \"bob\", \"clientAddr\" : \"\", \"time\" : ISODate(\"2018-11-04T22:29:39.702Z\"), \"what\" : \"moveChunk.to\", \"ns\" : \"video.movies\", \"details\" : { \"min\" : { \"imdbId\" : { \"$minKey\" : 1 } }, \"max\" : { \"imdbId\" : NumberLong(\"-7262221363006655132\") }, \"step 1 of 6\" : 965, \"step 2 of 6\" : 608, \"step 3 of 6\" : 15424, \"step 4 of 6\" : 0, \"step 5 of 6\" : 72, \"step 6 of 6\" : 258, \"note\" : \"success\" } }\n\nEach of the steps listed in \"details\" is timed and the \"stepN of N\" messages show how long each step took, in milliseconds.\n\nWhen the “from” shard receives a moveChunk command from the mongos, it:\n\n1. Checks the command parameters.\n\n2. Confirms with the config servers that it can acquire a distributed lock for the migrate.\n\n3. Tries to contact the “to” shard.\n\n4. Copies the data. This is referred to and logged as “the critical section.”\n\n5. Coordinates with the “to” shard and config servers to confirm the migration.\n\nNote that the “to” and “from” shards must be in close communication starting at \"step4 of 6\": the shards directly talk to one another and the config server to per‐ form the migration. If the “from” server has flaky network connectivity during the final steps, it may end up in a state where it cannot undo the migration and cannot move forward with it. In this case, the mongod will shut down.\n\nThe “to” shard’s changelog document is similar to the “from” shard’s, but the steps are a bit different. It looks like this:\n\n> db.changelog.find({what: \"moveChunk.from\", \"details.max.imdbId\": NumberLong(\"-7262221363006655132\")}).pretty() {\n\n346\n\n|\n\nChapter 17: Sharding Administration\n\n\"_id\" : \"router1-2018-11-04T17:29:39.753-0500-5bdf72d321b6e3be02fabf0b\", \"server\" : \"bob\", \"clientAddr\" : \"127.0.0.1:64743\", \"time\" : ISODate(\"2018-11-04T22:29:39.753Z\"), \"what\" : \"moveChunk.from\", \"ns\" : \"video.movies\", \"details\" : { \"min\" : { \"imdbId\" : { \"$minKey\" : 1 } }, \"max\" : { \"imdbId\" : NumberLong(\"-7262221363006655132\") }, \"step 1 of 6\" : 0, \"step 2 of 6\" : 4, \"step 3 of 6\" : 191, \"step 4 of 6\" : 17000, \"step 5 of 6\" : 341, \"step 6 of 6\" : 39, \"to\" : \"shard01\", \"from\" : \"shard02\", \"note\" : \"success\" } }\n\nWhen the “to” shard receives a command from the “from” shard, it:\n\n1. Migrates indexes. If this shard has never held chunks from the migrated collec‐ tion before, it needs to know what fields are indexed. If this isn’t the first time a chunk from this collection is being moved to this shard, then this should be a no-op.\n\n2. Deletes any existing data in the chunk range. There might be data left over from a failed migration or restore procedure that we wouldn’t want to interfere with the current data.\n\n3. Copies all documents in the chunk to the “to” shard.\n\n4. Replays any operations that happened to these documents during the copy (on the “to” shard).\n\n5. Waits for the “to” shard to have replicated the newly migrated data to a majority of servers.\n\n6. Commits the migrate by changing the chunk’s metadata to say that it lives on the “to” shard.\n\nconfig.settings\n\nThis collection contains documents representing the current balancer settings and chunk size. By changing the documents in this collection, you can turn the balancer\n\nSeeing the Current State\n\n|\n\n347\n\non or off or change the chunk size. Note that you should always connect to mongos, not the config servers directly, to change values in this collection.\n\nTracking Network Connections There are a lot of connections between the components of a cluster. This section cov‐ ers some sharding-specific information (see Chapter 24 for more information on networking).\n\nGetting Connection Statistics The command connPoolStats returns information regarding the open outgoing con‐ nections from the current database instance to other members of the sharded cluster or replica set.\n\nTo avoid interference with any running operations, connPoolStats does not take any locks. As such, the counts may change slightly as connPoolStats gathers information, resulting in slight differences between the hosts and pools connection counts:\n\n> db.adminCommand({\"connPoolStats\": 1}) { \"numClientConnections\" : 10, \"numAScopedConnections\" : 0, \"totalInUse\" : 0, \"totalAvailable\" : 13, \"totalCreated\" : 86, \"totalRefreshing\" : 0, \"pools\" : { \"NetworkInterfaceTL-TaskExecutorPool-0\" : { \"poolInUse\" : 0, \"poolAvailable\" : 2, \"poolCreated\" : 2, \"poolRefreshing\" : 0, \"localhost:27027\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 1, \"refreshing\" : 0 }, \"localhost:27019\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 1, \"refreshing\" : 0 } }, \"NetworkInterfaceTL-ShardRegistry\" : { \"poolInUse\" : 0, \"poolAvailable\" : 1, \"poolCreated\" : 13,\n\n348\n\n|\n\nChapter 17: Sharding Administration\n\n\"poolRefreshing\" : 0, \"localhost:27027\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 13, \"refreshing\" : 0 } }, \"global\" : { \"poolInUse\" : 0, \"poolAvailable\" : 10, \"poolCreated\" : 71, \"poolRefreshing\" : 0, \"localhost:27026\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27027\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 1, \"refreshing\" : 0 }, \"localhost:27023\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 7, \"refreshing\" : 0 }, \"localhost:27024\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 6, \"refreshing\" : 0 }, \"localhost:27022\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 9, \"refreshing\" : 0 }, \"localhost:27019\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27021\" : { \"inUse\" : 0, \"available\" : 1,\n\nTracking Network Connections\n\n|\n\n349\n\n350\n\n\"created\" : 8, \"refreshing\" : 0 }, \"localhost:27025\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 9, \"refreshing\" : 0 }, \"localhost:27020\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27018\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 7, \"refreshing\" : 0 } } }, \"hosts\" : { \"localhost:27026\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27027\" : { \"inUse\" : 0, \"available\" : 3, \"created\" : 15, \"refreshing\" : 0 }, \"localhost:27023\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 7, \"refreshing\" : 0 }, \"localhost:27024\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 6, \"refreshing\" : 0 }, \"localhost:27022\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 9,\n\n|\n\nChapter 17: Sharding Administration",
      "page_number": 348
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 356-364)",
      "start_page": 356,
      "end_page": 364,
      "detection_method": "topic_boundary",
      "content": "\"refreshing\" : 0 }, \"localhost:27019\" : { \"inUse\" : 0, \"available\" : 2, \"created\" : 9, \"refreshing\" : 0 }, \"localhost:27021\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27025\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 9, \"refreshing\" : 0 }, \"localhost:27020\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27018\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 7, \"refreshing\" : 0 } }, \"replicaSets\" : { \"shard02\" : { \"hosts\" : [ { \"addr\" : \"localhost:27021\", \"ok\" : true, \"ismaster\" : true, \"hidden\" : false, \"secondary\" : false, \"pingTimeMillis\" : 0 }, { \"addr\" : \"localhost:27022\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 },\n\nTracking Network Connections\n\n|\n\n351\n\n352\n\n{ \"addr\" : \"localhost:27023\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 } ] }, \"shard03\" : { \"hosts\" : [ { \"addr\" : \"localhost:27024\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 }, { \"addr\" : \"localhost:27025\", \"ok\" : true, \"ismaster\" : true, \"hidden\" : false, \"secondary\" : false, \"pingTimeMillis\" : 0 }, { \"addr\" : \"localhost:27026\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 } ] }, \"configRepl\" : { \"hosts\" : [ { \"addr\" : \"localhost:27027\", \"ok\" : true, \"ismaster\" : true, \"hidden\" : false, \"secondary\" : false, \"pingTimeMillis\" : 0 } ] }, \"shard01\" : { \"hosts\" : [\n\n|\n\nChapter 17: Sharding Administration\n\n{ \"addr\" : \"localhost:27018\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 }, { \"addr\" : \"localhost:27019\", \"ok\" : true, \"ismaster\" : true, \"hidden\" : false, \"secondary\" : false, \"pingTimeMillis\" : 0 }, { \"addr\" : \"localhost:27020\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 } ] } }, \"ok\" : 1, \"operationTime\" : Timestamp(1541440424, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541440424, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nIn this output:\n\n\"totalAvailable\" shows the total number of available outgoing connections from the current mongod/mongos instance to other members of the sharded clus‐ ter or replica set.\n\n\"totalCreated\" reports the total number of outgoing connections ever created by the current mongod/mongos instance to other members of the sharded cluster or replica set.\n\n\"totalInUse\" provides the total number of outgoing connections from the cur‐ rent mongod/mongos instance to other members of the sharded cluster or replica set that are currently in use.\n\nTracking Network Connections\n\n|\n\n353\n\n\"totalRefreshing\" displays the total number of outgoing connections from the current mongod/mongos instance to other members of the sharded cluster or rep‐ lica set that are currently being refreshed.\n\n\"numClientConnections\" identifies the number of active and stored outgoing synchronous connections from the current mongod/mongos instance to other members of the sharded cluster or replica set. These represent a subset of the connections and \"totalInUse\". by\n\n\"numAScopedConnection\" reports the number of active and stored outgoing scoped synchronous connections from the current mongod/mongos instance to other members of the sharded cluster or replica set. These represent a subset of the connections reported by \"totalAvailable\", \"totalCreated\", and \"totalInUse\".\n\n\"pools\" shows connection statistics (in use/available/created/refreshing) grou‐ ped by the connection pools. A mongod or mongos has two distinct families of outgoing connection pools: — DBClient-based pools (the “write path,” identified by the field name \"global\" in the \"pools\" document)\n\n— NetworkInterfaceTL-based pools (the “read path”)\n\n\"hosts\" shows connection statistics (in use/available/created/refreshing) grou‐ ped by the hosts. It reports on connections between the current mongod/mongos instance and each member of the sharded cluster or replica set.\n\nYou might see connections to other shards in the output of connPoolStats. These indicate that shards are connecting to other shards to migrate data. The primary of one shard will connect directly to the primary of another shard and “suck” its data.\n\nWhen a migrate occurs, a shard sets up a ReplicaSetMonitor (a process that moni‐ tors replica set health) to track the health of the shard on the other side of the migrate. mongod never destroys this monitor, so you may see messages in one replica set’s log about the members of another replica set. This is totally normal and should have no effect on your application.\n\nLimiting the Number of Connections When a client connects to a mongos, the mongos creates a connection to at least one shard to pass along the client’s request. Thus, every client connection into a mongos yields at least one outgoing connection from mongos to the shards.\n\nIf you have many mongos processes, they may create more connections than your shards can handle: by default a mongos will accept up to 65,536 connections (the\n\n354\n\n|\n\nChapter 17: Sharding Administration\n\nsame as mongod), so if you have 5 mongos processes with 10,000 client connections each, they may be attempting to create 50,000 connections to a shard!\n\nTo prevent this, you can use the --maxConns option to your command-line configura‐ tion for mongos to limit the number of connections it can create. The following for‐ mula can be used to calculate the maximum number of connections a shard can handle from a single mongos:\n\nmaxConns = maxConnsPrimary − (numMembersPerReplicaSet × 3) − (other x 3) / numMongosProcesses\n\nBreaking down the pieces of this formula:\n\nmaxConnsPrimary\n\nThe maximum number of connections on the Primary, typically set to 20,000 to avoid overwhelming the shard with connections from the mongos.\n\n(numMembersPerReplicaSet × 3)\n\nThe primary creates a connection to each secondary and each secondary creates two connections to the primary, for a total of three connections.\n\n(other x 3)\n\nOther is the number of miscellaneous processes that may connect to your mon‐ gods, such as monitoring or backup agents, direct shell connections (for adminis‐ tration), or connections to other shards for migrations.\n\nnumMongosProcesses\n\nThe total number of mongos in the sharded cluster.\n\nNote that --maxConns only prevents mongos from creating more than this many con‐ nections. It doesn’t do anything particularly helpful when this limit is reached: it will simply block requests, waiting for connections to be “freed.” Thus, you must prevent your application from using this many connections, especially as your number of mongos processes grows.\n\nWhen a MongoDB instance exits cleanly it closes all connections before stopping. The members that were connected to it will immediately get socket errors on those connections and be able to refresh them. However, if a MongoDB instance suddenly goes offline due to a power loss, crash, or network problems, it probably won’t cleanly close all of its sockets. In this case, other servers in the cluster may be under the impression that their connection is healthy until they try to perform an operation on it. At that point, they will get an error and refresh the connection (if the member is up again at that point).\n\nThis is a quick process when there are only a few connections. However, when there are thousands of connections that must be refreshed one by one you can get a lot of\n\nTracking Network Connections\n\n|\n\n355\n\nerrors because each connection to the downed member must be tried, determined to be bad, and reestablished. There isn’t a particularly good way of preventing this, aside from restarting processes that get bogged down in a reconnection storm.\n\nServer Administration As your cluster grows, you’ll need to add capacity or change configurations. This sec‐ tion covers how to add and remove servers in your cluster.\n\nAdding Servers You can add new mongos processes at any time. Make sure their --configdb option specifies the correct set of config servers and they should be immediately available for clients to connect to.\n\nTo add new shards, use the addShard command as shown in Chapter 15.\n\nChanging Servers in a Shard As you use your sharded cluster, you may want to change the servers in individual shards. To change a shard’s membership, connect directly to the shard’s primary (not through the mongos) and issue a replica set reconfig. The cluster configuration will pick up the change and update config.shards automatically. Do not modify con‐ fig.shards by hand.\n\nThe only exception to this is if you started your cluster with standalone servers as shards, not replica sets.\n\nChanging a shard from a standalone server to a replica set\n\nThe easiest way to do this is to add a new, empty replica set shard and then remove the standalone server shard (as discussed in the next section). Migrations will take care of moving your data to the new shard.\n\nRemoving a Shard In general, shards should not be removed from a cluster. If you are regularly adding and removing shards, you are putting a lot more stress on the system than necessary. If you add too many shards it is better to let your system grow into them, not remove them and add them back later. However, if necessary, you can remove shards.\n\nFirst make sure that the balancer is on. The balancer will be tasked with moving all the data on the shard you want to remove to other shards in a process called draining. To start draining, run the removeShard command. removeShard takes the shard’s name and drains all the chunks on that shard to the other shards:\n\n356\n\n|\n\nChapter 17: Sharding Administration\n\n> db.adminCommand({\"removeShard\" : \"shard03\"}) { \"msg\" : \"draining started successfully\", \"state\" : \"started\", \"shard\" : \"shard03\", \"note\" : \"you need to drop or movePrimary these databases\", \"dbsToMove\" : [ ], \"ok\" : 1, \"operationTime\" : Timestamp(1541450091, 2), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450091, 2), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nDraining can take a long time if there are a lot of chunks or large chunks to move. If you have jumbo chunks (see “Jumbo Chunks” on page 364), you may have to tem‐ porarily increase the chunk size to allow draining to move them.\n\nIf you want to keep tabs on how much has been moved, run removeShard again to give you the current status:\n\n> db.adminCommand({\"removeShard\" : \"shard02\"}) { \"msg\" : \"draining ongoing\", \"state\" : \"ongoing\", \"remaining\" : { \"chunks\" : NumberLong(3), \"dbs\" : NumberLong(0) }, \"note\" : \"you need to drop or movePrimary these databases\", \"dbsToMove\" : [ \"video\" ], \"ok\" : 1, \"operationTime\" : Timestamp(1541450139, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450139, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nYou can run removeShard as many times as you want.\n\nServer Administration\n\n|\n\n357\n\nChunks may have to be split to be moved, so you may see the number of chunks increase in the system during the drain. For example, suppose we have a five-shard cluster with the following chunk distributions:\n\ntest-rs0 10 test-rs1 10 test-rs2 10 test-rs3 11 test-rs4 11\n\nThis cluster has a total of 52 chunks. If we remove test-rs3, we might end up with:\n\ntest-rs0 15 test-rs1 15 test-rs2 15 test-rs4 15\n\nThe cluster now has 60 chunks, 18 of which came from shard test-rs3 (11 were there to start and 7 were created from draining splits).\n\nOnce all the chunks have been moved, if there are still databases that have the removed shard as their primary, you’ll need to remove them before the shard can be removed. Each database in a sharded cluster has a primary shard. If the shard you want to remove is also the primary of one of the cluster’s databases, removeShard lists the database in the \"dbsToMove\" field. To finish removing the shard, you must either move the database to a new shard after migrating all data from the shard or drop the database, deleting the associated data files. The output of removeShard will be some‐ thing like:\n\n> db.adminCommand({\"removeShard\" : \"shard02\"}) { \"msg\" : \"draining ongoing\", \"state\" : \"ongoing\", \"remaining\" : { \"chunks\" : NumberLong(3), \"dbs\" : NumberLong(0) }, \"note\" : \"you need to drop or movePrimary these databases\", \"dbsToMove\" : [ \"video\" ], \"ok\" : 1, \"operationTime\" : Timestamp(1541450139, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450139, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\n358\n\n|\n\nChapter 17: Sharding Administration\n\nTo finish the remove, move the listed databases with the movePrimary command:\n\n> db.adminCommand({\"movePrimary\" : \"video\", \"to\" : \"shard01\"}) { \"ok\" : 1, \"operationTime\" : Timestamp(1541450554, 12), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450554, 12), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nOnce you have done this, run removeShard one more time:\n\n> db.adminCommand({\"removeShard\" : \"shard02\"}) { \"msg\" : \"removeshard completed successfully\", \"state\" : \"completed\", \"shard\" : \"shard03\", \"ok\" : 1, \"operationTime\" : Timestamp(1541450619, 2), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450619, 2), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nThis is not strictly necessary, but it confirms that you have completed the process. If there are no databases that have this shard as their primary, you will get this response as soon as all chunks have been migrated off the shard.\n\nOnce you have started a shard draining, there is no built-in way to stop it.\n\nBalancing Data In general, MongoDB automatically takes care of balancing data. This section covers how to enable and disable this automatic balancing as well as how to intervene in the balancing process.\n\nBalancing Data\n\n|\n\n359",
      "page_number": 356
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 365-377)",
      "start_page": 365,
      "end_page": 377,
      "detection_method": "topic_boundary",
      "content": "The Balancer Turning off the balancer is a prerequisite to nearly any administrative activity. There is a shell helper to make this easier:\n\n> sh.setBalancerState(false) { \"ok\" : 1, \"operationTime\" : Timestamp(1541450923, 2), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450923, 2), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nWith the balancer off a new balancing round will not begin, but turning it off will not force an ongoing balancing round to stop immediately—migrations generally cannot stop on a dime. Thus, you should check the config.locks collection to see whether or not a balancing round is still in progress:\n\n> db.locks.find({\"_id\" : \"balancer\"})[\"state\"] 0\n\n0 means the balancer is off.\n\nBalancing puts load on your system: the destination shard must query the source shard for all the documents in a chunk and insert them, and then the source shard must delete them. There are two circumstances in particular where migrations can cause performance problems:\n\n1. Using a hotspot shard key will force constant migrations (as all new chunks will be created on the hotspot). Your system must have the capacity to handle the flow of data coming off of your hotspot shard.\n\n2. Adding a new shard will trigger a stream of migrations as the balancer attempts to populate it.\n\nIf you find that migrations are affecting your application’s performance, you can schedule a window for balancing in the config.settings collection. Run the following update to only allow balancing between 1 p.m. and 4 p.m. First make sure the bal‐ ancer is on, then schedule the window:\n\n> sh.setBalancerState( true ) { \"ok\" : 1, \"operationTime\" : Timestamp(1541451846, 4), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541451846, 4),\n\n360\n\n|\n\nChapter 17: Sharding Administration\n\n\"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } > db.settings.update( { _id: \"balancer\" }, { $set: { activeWindow : { start : \"13:00\", stop : \"16:00\" } } }, { upsert: true } ) WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n\nIf you set a balancing window, monitor it closely to ensure that mongos can actually keep your cluster balanced in the time that you have allotted it.\n\nYou must be careful if you plan to combine manual balancing with the automatic bal‐ ancer, since the automatic balancer always determines what to move based on the current state of the set and does not take into account the set’s history. For example, suppose you have shardA and shardB, each holding 500 chunks. shardA is getting a lot of writes, so you turn off the balancer and move 30 of the most active chunks to shardB. If you turn the balancer back on at this point, it will immediately swoop in and move 30 chunks (possibly a different 30) back from shardB to shardA to balance the chunk counts.\n\nTo prevent this, move 30 quiescent chunks from shardB to shardA before starting the balancer. That way there will be no imbalance between the shards and the balancer will be happy to leave things as they are. Alternatively, you could perform 30 splits on shardA’s chunks to even out the chunk counts.\n\nNote that the balancer only uses number of chunks as a metric, not size of data. Mov‐ ing a chunk is called a migration and is how MongoDB balances data across your cluster. Thus, a shard with a few large chunks may end up as the target of a migration from a shard with many small chunks (but a smaller data size).\n\nChanging Chunk Size There can be anywhere from zero to millions of documents in a chunk. Generally, the larger a chunk is, the longer it takes to migrate to another shard. In Chapter 14, we used a chunk size of 1 MB, so that we could see chunk movement easily and quickly. This is generally impractical in a live system; MongoDB would be doing a lot of unnecessary work to keep shards within a few megabytes of each other in size. By default chunks are 64 MB, which generally provides a good balance between ease of migration and migratory churn.\n\nSometimes you may find that migrations are taking too long with 64 MB chunks. To speed them up, you can decrease your chunk size. To do this, connect to mongos through the shell and update the config.settings collection:\n\nBalancing Data\n\n|\n\n361\n\n> db.settings.findOne() { \"_id\" : \"chunksize\", \"value\" : 64 } > db.settings.save({\"_id\" : \"chunksize\", \"value\" : 32}) WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n\nThe previous update would change your chunk size to 32 MB. Existing chunks would not be changed immediately, however; automatic splitting only occurs on insert or update. Thus, if you lower the chunk size, it may take time for all chunks to split to the new size.\n\nSplits cannot be undone. If you increase the chunk size, existing chunks grow only through insertion or updates until they reach the new size. The allowed range of the chunk size is between 1 and 1,024 MB, inclusive.\n\nNote that this is a cluster-wide setting: it affects all collections and databases. Thus, if you need a small chunk size for one collection and a large chunk size for another, you may have to compromise with a chunk size in between the two ideals (or put the col‐ lections in different clusters).\n\nIf MongoDB is doing too many migrations or your documents are large, you may want to increase your chunk size.\n\nMoving Chunks As mentioned earlier, all the data in a chunk lives on a certain shard. If that shard ends up with more chunks than the other shards, MongoDB will move some chunks off it.\n\nYou can manually move chunks using the moveChunk shell helper:\n\n> sh.moveChunk(\"video.movies\", {imdbId: 500000}, \"shard02\") { \"millis\" : 4079, \"ok\" : 1 }\n\nThis would move the chunk containing the document with an \"imdbId\" of 500000 to the shard named shard02. You must use the shard key (\"imdbId\", in this case) to find which chunk to move. Generally, the easiest way to specify a chunk is by its lower bound, although any value in the chunk will work (the upper bound will not, as it is not actually in the chunk). This command will move the chunk before returning, so it may take a while to run. The logs are the best place to see what it is doing if it takes a long time.\n\nIf a chunk is larger than the max chunk size, mongos will refuse to move it:\n\n362\n\n|\n\nChapter 17: Sharding Administration\n\n> sh.moveChunk(\"video.movies\", {imdbId: NumberLong(\"8345072417171006784\")}, \"shard02\") { \"cause\" : { \"chunkTooBig\" : true, \"estimatedChunkSize\" : 2214960, \"ok\" : 0, \"errmsg\" : \"chunk too big to move\" }, \"ok\" : 0, \"errmsg\" : \"move failed\" }\n\nIn this case, you must manually split the chunk before moving it, using the splitAt command:\n\n> db.chunks.find({ns: \"video.movies\", \"min.imdbId\": NumberLong(\"6386258634539951337\")}).pretty() { \"_id\" : \"video.movies-imdbId_6386258634539951337\", \"ns\" : \"video.movies\", \"min\" : { \"imdbId\" : NumberLong(\"6386258634539951337\") }, \"max\" : { \"imdbId\" : NumberLong(\"8345072417171006784\") }, \"shard\" : \"shard02\", \"lastmod\" : Timestamp(1, 9), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\"), \"history\" : [ { \"validAfter\" : Timestamp(1541370559, 4), \"shard\" : \"shard02\" } ] } > sh.splitAt(\"video.movies\", {\"imdbId\": NumberLong(\"7000000000000000000\")}) { \"ok\" : 1, \"operationTime\" : Timestamp(1541453304, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541453306, 5), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } > db.chunks.find({ns: \"video.movies\", \"min.imdbId\": NumberLong(\"6386258634539951337\")}).pretty() {\n\nBalancing Data\n\n|\n\n363\n\n\"_id\" : \"video.movies-imdbId_6386258634539951337\", \"lastmod\" : Timestamp(15, 2), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\"), \"ns\" : \"video.movies\", \"min\" : { \"imdbId\" : NumberLong(\"6386258634539951337\") }, \"max\" : { \"imdbId\" : NumberLong(\"7000000000000000000\") }, \"shard\" : \"shard02\", \"history\" : [ { \"validAfter\" : Timestamp(1541370559, 4), \"shard\" : \"shard02\" } ] }\n\nOnce the chunk has been split into smaller pieces, it should be movable. Alternatively, you can raise the max chunk size and then move it, but you should break up large chunks whenever possible. Sometimes, though, chunks cannot be broken up—we’ll look at this situation next.1\n\nJumbo Chunks Suppose you choose the \"date\" field as your shard key. The \"date\" field in this col‐ lection is a string that looks like \"year/month/day\", which means that mongos can create at most one chunk per day. This works fine for a while, until your application suddenly goes viral and gets a thousand times its typical traffic for one day.\n\nThis day’s chunk is going to be much larger than any other day’s, but it is also com‐ pletely unsplittable because every document has the same value for the shard key.\n\nOnce a chunk is larger than the max chunk size set in config.settings, the balancer will not be allowed to move the chunk. These unsplittable, unmovable chunks are called jumbo chunks and they are inconvenient to deal with.\n\nLet’s take an example. Suppose you have three shards, shard1, shard2, and shard3. If you use the hotspot shard key pattern described in “Ascending Shard Keys” on page 320, all your writes will be going to one shard—say, shard1. The shard primary mon‐ god will request that the balancer move each new top chunk evenly between the other\n\n1 MongoDB 4.4 is planning to add a new parameter (forceJumbo) in the moveChunk function, as well as a new balancer configuration setting attemptToBalanceJumboChunks to address jumbo chunks. The details are in this JIRA ticket describing the work.\n\n364\n\n|\n\nChapter 17: Sharding Administration\n\nshards, but the only chunks that the balancer can move are the nonjumbo chunks, so it will migrate all the small chunks off the hot shard.\n\nNow all the shards will have roughly the same number of chunks, but all of shard2 and shard3’s chunks will be less than 64 MB in size. And if jumbo chunks are being created, more and more of shard1’s chunks will be more than 64 MB in size. Thus, shard1 will fill up a lot faster than the other two shards, even though the number of chunks is perfectly balanced between the three.\n\nThus, one of the indicators that you have jumbo chunk problems is that one shard’s size is growing much faster than the others. You can also look at the output of sh.sta tus() to see if you have jumbo chunks—they will be marked with the jumbo attribute:\n\n> sh.status() ... { \"x\" : -7 } -->> { \"x\" : 5 } on : shard0001 { \"x\" : 5 } -->> { \"x\" : 6 } on : shard0001 jumbo { \"x\" : 6 } -->> { \"x\" : 7 } on : shard0001 jumbo { \"x\" : 7 } -->> { \"x\" : 339 } on : shard0001 ...\n\nYou can use the dataSize command to check chunk sizes. First, use the config.chunks collection to find the chunk ranges:\n\n> use config > var chunks = db.chunks.find({\"ns\" : \"acme.analytics\"}).toArray()\n\nThen use these chunk ranges to find possible jumbo chunks:\n\n> use <dbName> > db.runCommand({\"dataSize\" : \"<dbName.collName>\", ... \"keyPattern\" : {\"date\" : 1}, // shard key ... \"min\" : chunks[0].min, ... \"max\" : chunks[0].max}) { \"size\" : 33567917, \"numObjects\" : 108942, \"millis\" : 634, \"ok\" : 1, \"operationTime\" : Timestamp(1541455552, 10), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541455552, 10), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nBe careful, though—the dataSize command does have to scan the chunk’s data to figure out how big it is. If you can, narrow down your search by using your\n\nBalancing Data\n\n|\n\n365\n\nknowledge of your data: were jumbo chunks created on a certain date? For example, if July 1 was a really busy day, look for chunks with that day in their shard key range.\n\nIf you’re using GridFS and sharding by \"files_id\", you can look at the fs.files collection to find a file’s size.\n\nDistributing jumbo chunks\n\nTo fix a cluster thrown off-balance by jumbo chunks, you must evenly distribute them among the shards.\n\nThis is a complex manual process, but should not cause any downtime (it may cause slowness, as you’ll be migrating a lot of data). In the following description, the shard with the jumbo chunks is referred to as the “from” shard. The shards that the jumbo chunks are migrated to are called the “to” shards. Note that you may have multiple “from” shards that you wish to move chunks off of. Repeat these steps for each:\n\n1. Turn off the balancer. You don’t want the balancer trying to “help” during this process:\n\n> sh.setBalancerState(false)\n\n2. MongoDB will not allow you to move chunks larger than the max chunk size, so temporarily increase the chunk size. Make a note of what your original chunk size is and then change it to something large, like 10000. Chunk size is specified in megabytes: > use config > db.settings.findOne({\"_id\" : \"chunksize\"}) { \"_id\" : \"chunksize\", \"value\" : 64 } > db.settings.save({\"_id\" : \"chunksize\", \"value\" : 10000})\n\n3. Use the moveChunk command to move jumbo chunks off the “from” shard. 4. Run splitChunk on the remaining chunks on the “from” shard until it has roughly the same number of chunks as the “to” shards.\n\n5. Set the chunk size back to its original value:\n\n> db.settings.save({\"_id\" : \"chunksize\", \"value\" : 64})\n\n6. Turn on the balancer:\n\n> sh.setBalancerState(true)\n\n366\n\n|\n\nChapter 17: Sharding Administration\n\nWhen the balancer is turned on again, it will once again be unable to move the jumbo chunks; they are essentially held in place by their size.\n\nPreventing jumbo chunks\n\nAs the amount of data you are storing grows, the manual process described in the previous section becomes unsustainable. Thus, if you’re having problems with jumbo chunks, you should make it a priority to prevent them from forming.\n\nTo prevent jumbo chunks, modify your shard key to have more granularity. You want almost every document to have a unique value for the shard key, or at least to never have more than the chunk size’s worth of data with a single shard key value.\n\nFor example, if you were using the year/month/day key described earlier, it could quickly be made more fine-grained by adding hours, minutes, and seconds. Similarly, if you’re sharding on something coarse-grained like log level, you can add to your shard key a second field with a lot of granularity, such as an MD5 hash or UUID. Then you can always split a chunk, even if the first field is the same for many documents.\n\nRefreshing Configurations As a final tip, sometimes mongos will not update its configuration correctly from the config servers. If you ever get a configuration that you don’t expect or a mongos seems to be out of date or cannot find data that you know is there, use the flushRouterCon fig command to manually clear all caches:\n\n> db.adminCommand({\"flushRouterConfig\" : 1})\n\nIf flushRouterConfig does not work, restarting all your mongos or mongod processes clears any cached data.\n\nBalancing Data\n\n|\n\n367\n\nPART V Application Administration\n\nCHAPTER 18 Seeing What Your Application Is Doing\n\nOnce you have an application up and running, how do you know what it’s doing? This chapter covers how to figure out what kinds of queries MongoDB is running, how much data is being written, and other details about what it’s actually doing. You’ll learn about:\n\nFinding slow operations and killing them\n\nGetting and interpreting statistics about your collections and databases\n\nUsing command-line tools to give you a picture of what MongoDB is doing\n\nSeeing the Current Operations An easy way to find slow operations is to see what is running. Anything slow is more likely to show up and have been running for longer. It’s not guaranteed, but it’s a good first step to see what might be slowing down an application.\n\nTo see the operations that are running, use the db.currentOp() function:\n\n> db.currentOp() { \"inprog\": [{ \"type\" : \"op\", \"host\" : \"eoinbrazil-laptop-osx:27017\", \"desc\" : \"conn3\", \"connectionId\" : 3, \"client\" : \"127.0.0.1:57181\", \"appName\" : \"MongoDB Shell\", \"clientMetadata\" : { \"application\" : { \"name\" : \"MongoDB Shell\" },\n\n371\n\n372\n\n\"driver\" : { \"name\" : \"MongoDB Internal Client\", \"version\" : \"4.2.0\" }, \"os\" : { \"type\" : \"Darwin\", \"name\" : \"Mac OS X\", \"architecture\" : \"x86_64\", \"version\" : \"18.7.0\" } }, \"active\" : true, \"currentOpTime\" : \"2019-09-03T23:25:46.380+0100\", \"opid\" : 13594, \"lsid\" : { \"id\" : UUID(\"63b7df66-ca97-41f4-a245-eba825485147\"), \"uid\" : BinData(0,\"47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=\") }, \"secs_running\" : NumberLong(0), \"microsecs_running\" : NumberLong(969), \"op\" : \"insert\", \"ns\" : \"sample_mflix.items\", \"command\" : { \"insert\" : \"items\", \"ordered\" : false, \"lsid\" : { \"id\" : UUID(\"63b7df66-ca97-41f4-a245-eba825485147\") }, \"$readPreference\" : { \"mode\" : \"secondaryPreferred\" }, \"$db\" : \"sample_mflix\" }, \"numYields\" : 0, \"locks\" : { \"ParallelBatchWriterMode\" : \"r\", \"ReplicationStateTransition\" : \"w\", \"Global\" : \"w\", \"Database\" : \"w\", \"Collection\" : \"w\" }, \"waitingForLock\" : false, \"lockStats\" : { \"ParallelBatchWriterMode\" : { \"acquireCount\" : { \"r\" : NumberLong(4) } }, \"ReplicationStateTransition\" : { \"acquireCount\" : { \"w\" : NumberLong(4) }\n\n|\n\nChapter 18: Seeing What Your Application Is Doing\n\n}, \"Global\" : { \"acquireCount\" : { \"w\" : NumberLong(4) } }, \"Database\" : { \"acquireCount\" : { \"w\" : NumberLong(4) } }, \"Collection\" : { \"acquireCount\" : { \"w\" : NumberLong(4) } }, \"Mutex\" : { \"acquireCount\" : { \"r\" : NumberLong(196) } } }, \"waitingForFlowControl\" : false, \"flowControlStats\" : { \"acquireCount\" : NumberLong(4) } }], \"ok\": 1 }\n\nThis displays a list of operations that the database is performing. Here are some of the more important fields in the output:\n\n\"opid\"\n\nThe operation’s unique identifier. You can use this number to kill an operation (see “Killing Operations” on page 375).\n\n\"active\"\n\nWhether this operation is running. If this field is false, it means the operation has yielded or is waiting for a lock.\n\n\"secs_running\"\n\nThe duration of this operation in seconds. You can use this to find queries that are taking too long.\n\n\"microsecs_running\"\n\nThe duration of this operation in microseconds. You can use this to find queries that are taking too long.\n\nSeeing the Current Operations\n\n|\n\n373\n\n\"op\"\n\nThe type of operation. This is generally \"query\", \"insert\", \"update\", or \"remove\". Note that database commands are processed as queries.\n\n\"desc\"\n\nAn identifier for the client. This can be correlated with messages in the logs. Every log message related to the connection in our example will be prefixed with [conn3], so you can use this to grep the logs for relevant information.\n\n\"locks\"\n\nA description of the types of locks taken by this operation.\n\n\"waitingForLock\"\n\nWhether this operation is currently blocking, waiting to acquire a lock.\n\n\"numYields\"\n\nThe number of times this operation has yielded, releasing its lock to allow other operations to go. Generally, any operation that searches for documents (queries, updates, and removes) can yield. An operation will only yield if there are other operations enqueued and waiting to take its lock. Basically, if there are no opera‐ tions in the \"waitingForLock\" state, the current operations will not yield.\n\n\"lockstats.timeAcquiringMicros\"\n\nHow long it took this operation to acquire the locks it needed.\n\nYou can filter currentOp to only look for operations fulfilling certain criteria, such as operations on a certain namespace or ones that have been running for a certain length of time. You filter the results by passing in a query argument:\n\n> db.currentOp( { \"active\" : true, \"secs_running\" : { \"$gt\" : 3 }, \"ns\" : /^db1\\./ } )\n\nYou can query on any field in currentOp, using all the normal query operators.\n\nFinding Problematic Operations The most common use for db.currentOp() is looking for slow operations. You can use the filtering technique described in the previous section to find all queries that take longer than a certain amount of time, which may suggest a missing index or improper field filtering.\n\n374\n\n|\n\nChapter 18: Seeing What Your Application Is Doing",
      "page_number": 365
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 378-390)",
      "start_page": 378,
      "end_page": 390,
      "detection_method": "topic_boundary",
      "content": "Sometimes people will find that unexpected queries are running, generally because there’s an app server running an old or buggy version of the software. The \"client\" field can help you track down where unexpected operations are coming from.\n\nKilling Operations If you find an operation that you want to stop, you can kill it by passing db.killOp() its \"opid\":\n\n> db.killOp(123)\n\nNot all operations can be killed. In general, operations can only be killed when they yield—so updates, finds, and removes can all be killed, but operations holding or waiting for a lock usually cannot be killed.\n\nOnce you have sent a “kill” message to an operation, it will have a \"killed\" field in the db.currentOp() output. However, it won’t actually be dead until it disappears from the list of current operations.\n\nIn MongoDB 4.0, the killOP method was extended to allow it to run on a mongos. It can now kill queries (read operations) that are running across more than one shard in a cluster. In previous versions, this involved manually issuing the kill command across each shard on the respective primary mongod.\n\nFalse Positives If you look for slow operations, you may see some long-running internal operations listed. There are several long-running requests MongoDB may have running, depending on your setup. The most common are the replication thread (which will continue fetching more operations from the sync source for as long as possible) and the writeback listener for sharding. Any long-running query on local.oplog.rs can be ignored, as well as any writebacklistener commands.\n\nIf you kill either of these operations, MongoDB will just restart them. However, you generally should not do that. Killing the replication thread will briefly halt replica‐ tion, and killing the writeback listener may cause mongos to miss legitimate write errors.\n\nPreventing Phantom Operations There is an odd, MongoDB-specific issue that you may run into, particularly if you’re bulk-loading data into a collection. Suppose you have a job that is firing thousands of update operations at MongoDB and MongoDB is grinding to a halt. You quickly stop the job and kill off all the updates that are currently occurring. However, you con‐ tinue to see new updates appearing as soon as you kill the old ones, even though the job is no longer running!\n\nSeeing the Current Operations\n\n|\n\n375\n\nIf you are loading data using unacknowledged writes, your application will fire writes at MongoDB, potentially faster than MongoDB can process them. If MongoDB gets backed up, these writes will pile up in the operating system’s socket buffer. When you kill the writes MongoDB is working on, this allows MongoDB to start processing the writes in the buffer. Even if you stop the client from sending writes, any writes that made it into the buffer will get processed by MongoDB, since they’ve already been “received” (just not processed).\n\nThe best way to prevent these phantom writes is to do acknowledged writes: make each write wait until the previous write is complete, not just until the previous write is sitting in a buffer on the database server.\n\nUsing the System Profiler To find slow operations you can use the system profiler, which records operations in a special system.profile collection. The profiler can give you tons of information about operations that are taking a long time, but at a cost: it slows down mongod’s overall performance. Thus, you may only want to turn on the profiler periodically to capture a slice of traffic. If your system is already heavily loaded, you may wish to use another technique described in this chapter to diagnose issues.\n\nBy default, the profiler is off and does not record anything. You can turn it on by run‐ ning db.setProfilingLevel() in the shell:\n\n> db.setProfilingLevel(2) { \"was\" : 0, \"slowms\" : 100, \"ok\" : 1 }\n\nLevel 2 means “profile everything.” Every read and write request received by the data‐ base will be recorded in the system.profile collection of the current database. Profiling is enabled per-database and incurs a heavy performance penalty: every write has to be written an extra time and every read has to take a write lock (because it must write an entry to the system.profile collection). However, it will give you an exhaustive listing of what your system is doing:\n\n> db.foo.insert({x:1}) > db.foo.update({},{$set:{x:2}}) > db.foo.remove() > db.system.profile.find().pretty() { \"op\" : \"insert\", \"ns\" : \"sample_mflix.foo\", \"command\" : { \"insert\" : \"foo\", \"ordered\" : true, \"lsid\" : { \"id\" : UUID(\"63b7df66-ca97-41f4-a245-eba825485147\") }, \"$readPreference\" : {\n\n376\n\n|\n\nChapter 18: Seeing What Your Application Is Doing\n\n\"mode\" : \"secondaryPreferred\" }, \"$db\" : \"sample_mflix\" }, \"ninserted\" : 1, \"keysInserted\" : 1, \"numYield\" : 0, \"locks\" : { ... }, \"flowControl\" : { \"acquireCount\" : NumberLong(3) }, \"responseLength\" : 45, \"protocol\" : \"op_msg\", \"millis\" : 33, \"client\" : \"127.0.0.1\", \"appName\" : \"MongoDB Shell\", \"allUsers\" : [ ], \"user\" : \"\" } { \"op\" : \"update\", \"ns\" : \"sample_mflix.foo\", \"command\" : { \"q\" : {\n\n}, \"u\" : { \"$set\" : { \"x\" : 2 } }, \"multi\" : false, \"upsert\" : false }, \"keysExamined\" : 0, \"docsExamined\" : 1, \"nMatched\" : 1, \"nModified\" : 1, \"numYield\" : 0, \"locks\" : { ... }, \"flowControl\" : { \"acquireCount\" : NumberLong(1) }, \"millis\" : 0, \"planSummary\" : \"COLLSCAN\", \"execStats\" : { ... \"inputStage\" : { ... } }, \"ts\" : ISODate(\"2019-09-03T22:39:33.856Z\"), \"client\" : \"127.0.0.1\",\n\nUsing the System Profiler\n\n|\n\n377\n\n\"appName\" : \"MongoDB Shell\", \"allUsers\" : [ ], \"user\" : \"\" } { \"op\" : \"remove\", \"ns\" : \"sample_mflix.foo\", \"command\" : { \"q\" : {\n\n}, \"limit\" : 0 }, \"keysExamined\" : 0, \"docsExamined\" : 1, \"ndeleted\" : 1, \"keysDeleted\" : 1, \"numYield\" : 0, \"locks\" : { ... }, \"flowControl\" : { \"acquireCount\" : NumberLong(1) }, \"millis\" : 0, \"planSummary\" : \"COLLSCAN\", \"execStats\" : { ... \"inputStage\" : { ... } }, \"ts\" : ISODate(\"2019-09-03T22:39:33.858Z\"), \"client\" : \"127.0.0.1\", \"appName\" : \"MongoDB Shell\", \"allUsers\" : [ ], \"user\" : \"\" }\n\nYou can use the \"client\" field to see which users are sending which operations to the database. If you’re using authentication, you can see which user is doing each opera‐ tion, too.\n\nOften, you do not care about most of the operations that your database is doing, just the slow ones. For this, you can set the profiling level to 1. By default, level 1 profiles operations that take longer than 100 ms. You can also specify a second argument, which defines what “slow” means to you. This would record all operations that took longer than 500 ms:\n\n> db.setProfilingLevel(1, 500) { \"was\" : 2, \"slowms\" : 100, \"ok\" : 1 }\n\nTo turn profiling off, set the profiling level to 0:\n\n> db.setProfilingLevel(0) { \"was\" : 1, \"slowms\" : 500, \"ok\" : 1 }\n\n378\n\n|\n\nChapter 18: Seeing What Your Application Is Doing\n\nIt’s generally not a good idea to set slowms to a low value. Even with profiling off, slowms has an effect on mongod: it sets the threshold for printing slow operations in the log. Thus, if you set slowms to 2, every operation that takes longer than 2 ms will show up in the log, even with profiling off. So, if you lower slowms to profile some‐ thing, you might want to raise it again before turning off profiling.\n\nYou can see the current profiling level with db.getProfilingLevel(). The profiling level is not persistent: restarting the database clears the level.\n\nThere are command-line options for configuring the profiling level, namely -- profile level and --slowms time, but bumping up the profiling level is generally a temporary debugging measure, not something you want to add to your configuration long-term.\n\nIn MongoDB 4.2, profiler entries and diagnostic log messages were extended for read/write operations to help improve the identification of slow queries, with the addition of the queryHash and planCacheKey fields. The queryHash string represents a hash of the query shape and is dependent only on the query shape. Each query shape is associated with a queryHash, making it easier to highlight those queries using the same shape. The planCacheKey is the hash of the key for the plan cache entry associated with the query. It includes the details of both the query shape and the cur‐ rently available indexes for the shape. These help you correlate the available informa‐ tion from the profiler to assist with query performance diagnosis.\n\nIf you turn on profiling and the system.profile collection does not already exist, Mon‐ goDB creates a small capped collection for it (a few megabytes in size). If you want to run the profiler for an extended period of time, this may not be enough space for the number of operations you need to record. You can make a larger system.profile collec‐ tion by turning off profiling, dropping the system.profile collection, and creating a new system.profile capped collection that is the size you desire. Then enable profiling on the database.\n\nCalculating Sizes In order to provision the correct amount of disk and RAM, it is useful to know how much space documents, indexes, collections, and databases are taking up. See “Calcu‐ lating the Working Set” on page 429 for information on calculating your working set.\n\nDocuments The easiest way to get the size of a document is to use the shell’s Object.bsonsize() function. Pass in any document to get the size it would be when stored in MongoDB.\n\nFor example, you can see that storing _ids as ObjectIds is more efficient than storing them as strings:\n\nCalculating Sizes\n\n|\n\n379\n\n> Object.bsonsize({_id:ObjectId()}) 22 > // \"\"+ObjectId() converts the ObjectId to a string > Object.bsonsize({_id:\"\"+ObjectId()}) 39\n\nMore practically, you can pass in documents directly from your collections:\n\n> Object.bsonsize(db.users.findOne())\n\nThis shows you exactly how many bytes a document is taking up on disk. However, this does not count padding or indexes, which can often be significant factors in the size of a collection.\n\nCollections For seeing information about a whole collection, there is a stats function:\n\n>db.movies.stats() { \"ns\" : \"sample_mflix.movies\", \"size\" : 65782298, \"count\" : 45993, \"avgObjSize\" : 1430, \"storageSize\" : 45445120, \"capped\" : false, \"wiredTiger\" : { \"metadata\" : { \"formatVersion\" : 1 }, \"creationString\" : \"access_pattern_hint=none,allocation_size=4KB,\\ app_metadata=(formatVersion=1),assert=(commit_timestamp=none,\\ read_timestamp=none),block_allocation=best,block_compressor=\\ snappy,cache_resident=false,checksum=on,colgroups=,collator=,\\ columns=,dictionary=0,encryption=(keyid=,name=),exclusive=\\ false,extractor=,format=btree,huffman_key=,huffman_value=,\\ ignore_in_memory_cache_size=false,immutable=false,internal_item_\\ max=0,internal_key_max=0,internal_key_truncate=true,internal_\\ page_max=4KB,key_format=q,key_gap=10,leaf_item_max=0,leaf_key_\\ max=0,leaf_page_max=32KB,leaf_value_max=64MB,log=(enabled=true),\\ lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,bloom_\\ config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit\\ =0,chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_\\ generation=0,suffix=),merge_max=15,merge_min=0),memory_page_image\\ _max=0,memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,\\ prefix_compression=false,prefix_compression_min=4,source=,split_\\ deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,\\ value_format=u\", \"type\" : \"file\", \"uri\" : \"statistics:table:collection-14--2146526997547809066\", \"LSM\" : { \"bloom filter false positives\" : 0, \"bloom filter hits\" : 0,\n\n380\n\n|\n\nChapter 18: Seeing What Your Application Is Doing\n\n\"bloom filter misses\" : 0, \"bloom filter pages evicted from cache\" : 0, \"bloom filter pages read into cache\" : 0, \"bloom filters in the LSM tree\" : 0, \"chunks in the LSM tree\" : 0, \"highest merge generation in the LSM tree\" : 0, \"queries that could have benefited from a Bloom filter that did not exist\" : 0, \"sleep for LSM checkpoint throttle\" : 0, \"sleep for LSM merge throttle\" : 0, \"total size of bloom filters\" : 0 }, \"block-manager\" : { \"allocations requiring file extension\" : 0, \"blocks allocated\" : 1358, \"blocks freed\" : 1322, \"checkpoint size\" : 39219200, \"file allocation unit size\" : 4096, \"file bytes available for reuse\" : 6209536, \"file magic number\" : 120897, \"file major version number\" : 1, \"file size in bytes\" : 45445120, \"minor version number\" : 0 }, \"btree\" : { \"btree checkpoint generation\" : 22, \"column-store fixed-size leaf pages\" : 0, \"column-store internal pages\" : 0, \"column-store variable-size RLE encoded values\" : 0, \"column-store variable-size deleted values\" : 0, \"column-store variable-size leaf pages\" : 0, \"fixed-record size\" : 0, \"maximum internal page key size\" : 368, \"maximum internal page size\" : 4096, \"maximum leaf page key size\" : 2867, \"maximum leaf page size\" : 32768, \"maximum leaf page value size\" : 67108864, \"maximum tree depth\" : 0, \"number of key/value pairs\" : 0, \"overflow pages\" : 0, \"pages rewritten by compaction\" : 1312, \"row-store empty values\" : 0, \"row-store internal pages\" : 0, \"row-store leaf pages\" : 0 }, \"cache\" : { \"bytes currently in the cache\" : 40481692, \"bytes dirty in the cache cumulative\" : 40992192, \"bytes read into cache\" : 37064798, \"bytes written from cache\" : 37019396, \"checkpoint blocked page eviction\" : 0, \"data source pages selected for eviction unable to be evicted\" : 32,\n\nCalculating Sizes\n\n|\n\n381\n\n382\n\n\"eviction walk passes of a file\" : 0, \"eviction walk target pages histogram - 0-9\" : 0, \"eviction walk target pages histogram - 10-31\" : 0, \"eviction walk target pages histogram - 128 and higher\" : 0, \"eviction walk target pages histogram - 32-63\" : 0, \"eviction walk target pages histogram - 64-128\" : 0, \"eviction walks abandoned\" : 0, \"eviction walks gave up because they restarted their walk twice\" : 0, \"eviction walks gave up because they saw too many pages and found no candidates\" : 0, \"eviction walks gave up because they saw too many pages and found too few candidates\" : 0, \"eviction walks reached end of tree\" : 0, \"eviction walks started from root of tree\" : 0, \"eviction walks started from saved location in tree\" : 0, \"hazard pointer blocked page eviction\" : 0, \"in-memory page passed criteria to be split\" : 0, \"in-memory page splits\" : 0, \"internal pages evicted\" : 8, \"internal pages split during eviction\" : 0, \"leaf pages split during eviction\" : 0, \"modified pages evicted\" : 1312, \"overflow pages read into cache\" : 0, \"page split during eviction deepened the tree\" : 0, \"page written requiring cache overflow records\" : 0, \"pages read into cache\" : 1330, \"pages read into cache after truncate\" : 0, \"pages read into cache after truncate in prepare state\" : 0, \"pages read into cache requiring cache overflow entries\" : 0, \"pages requested from the cache\" : 3383, \"pages seen by eviction walk\" : 0, \"pages written from cache\" : 1334, \"pages written requiring in-memory restoration\" : 0, \"tracked dirty bytes in the cache\" : 0, \"unmodified pages evicted\" : 8 }, \"cache_walk\" : { \"Average difference between current eviction generation when the page was last considered\" : 0, \"Average on-disk page image size seen\" : 0, \"Average time in cache for pages that have been visited by the eviction server\" : 0, \"Average time in cache for pages that have not been visited by the eviction server\" : 0, \"Clean pages currently in cache\" : 0, \"Current eviction generation\" : 0, \"Dirty pages currently in cache\" : 0, \"Entries in the root page\" : 0, \"Internal pages currently in cache\" : 0, \"Leaf pages currently in cache\" : 0, \"Maximum difference between current eviction generation when the page was last considered\" : 0,\n\n|\n\nChapter 18: Seeing What Your Application Is Doing\n\n\"Maximum page size seen\" : 0, \"Minimum on-disk page image size seen\" : 0, \"Number of pages never visited by eviction server\" : 0, \"On-disk page image sizes smaller than a single allocation unit\" : 0, \"Pages created in memory and never written\" : 0, \"Pages currently queued for eviction\" : 0, \"Pages that could not be queued for eviction\" : 0, \"Refs skipped during cache traversal\" : 0, \"Size of the root page\" : 0, \"Total number of pages currently in cache\" : 0 }, \"compression\" : { \"compressed page maximum internal page size prior to compression\" : 4096, \"compressed page maximum leaf page size prior to compression \" : 131072, \"compressed pages read\" : 1313, \"compressed pages written\" : 1311, \"page written failed to compress\" : 1, \"page written was too small to compress\" : 22 }, \"cursor\" : { \"bulk loaded cursor insert calls\" : 0, \"cache cursors reuse count\" : 0, \"close calls that result in cache\" : 0, \"create calls\" : 1, \"insert calls\" : 0, \"insert key and value bytes\" : 0, \"modify\" : 0, \"modify key and value bytes affected\" : 0, \"modify value bytes modified\" : 0, \"next calls\" : 0, \"open cursor count\" : 0, \"operation restarted\" : 0, \"prev calls\" : 1, \"remove calls\" : 0, \"remove key bytes removed\" : 0, \"reserve calls\" : 0, \"reset calls\" : 2, \"search calls\" : 0, \"search near calls\" : 0, \"truncate calls\" : 0, \"update calls\" : 0, \"update key and value bytes\" : 0, \"update value size change\" : 0 }, \"reconciliation\" : { \"dictionary matches\" : 0, \"fast-path pages deleted\" : 0, \"internal page key bytes discarded using suffix compression\" : 0, \"internal page multi-block writes\" : 0, \"internal-page overflow keys\" : 0,\n\nCalculating Sizes\n\n|\n\n383\n\n\"leaf page key bytes discarded using prefix compression\" : 0, \"leaf page multi-block writes\" : 0, \"leaf-page overflow keys\" : 0, \"maximum blocks required for a page\" : 1, \"overflow values written\" : 0, \"page checksum matches\" : 0, \"page reconciliation calls\" : 1334, \"page reconciliation calls for eviction\" : 1312, \"pages deleted\" : 0 }, \"session\" : { \"object compaction\" : 4 }, \"transaction\" : { \"update conflicts\" : 0 } }, \"nindexes\" : 5, \"indexBuilds\" : [ ], \"totalIndexSize\" : 46292992, \"indexSizes\" : { \"_id_\" : 446464, \"$**_text\" : 44474368, \"genres_1_imdb.rating_1_metacritic_1\" : 724992, \"tomatoes_rating\" : 307200, \"getMovies\" : 339968 }, \"scaleFactor\" : 1, \"ok\" : 1 }\n\nstats starts with the namespace (\"sample_mflix.movies\") and then the count of all documents in the collection. The next couple of fields have to do with the size of the collection. \"size\" is what you’d get if you called Object.bsonsize() on each element in the collection and added up all the sizes: it’s the actual number of bytes in memory the documents in the collection are taking up when uncompressed. Equivalently, if you take the \"avgObjSize\" and multiply it by \"count\", you’ll get \"size\" uncom‐ pressed in memory.\n\nAs mentioned earlier, a total count of the documents’ bytes leaves out the space saved by compressing a collection. \"storageSize\" can be a smaller figure than \"size\", reflecting the space saved by compression.\n\n\"nindexes\" is the number of indexes on the collection. An index is not counted in \"nindexes\" until it finishes being built and cannot be used until it appears in this list. In general, indexes will be a lot larger than the amount of data they store. You can minimize this free space by having right-balanced indexes (as described in “Introduc‐ tion to Compound Indexes” on page 81). Indexes that are randomly distributed will\n\n384\n\n|\n\nChapter 18: Seeing What Your Application Is Doing\n\ngenerally be approximately 50% free space, whereas ascending-order indexes will be 10% free space.\n\nAs your collections get bigger, it may become difficult to read stats output with sizes in the billions of bytes or beyond. Thus, you can pass in a scaling factor: 1024 for kil‐ obytes, 1024*1024 for megabytes, and so on. For example, this would get the collec‐ tion stats in terabytes:\n\n> db.big.stats(1024*1024*1024*1024)\n\nDatabases Databases have a stats function that’s similar to collections’:\n\n> db.stats() { \"db\" : \"sample_mflix\", \"collections\" : 5, \"views\" : 0, \"objects\" : 98308, \"avgObjSize\" : 819.8680982219148, \"dataSize\" : 80599593, \"storageSize\" : 53620736, \"numExtents\" : 0, \"indexes\" : 12, \"indexSize\" : 47001600, \"scaleFactor\" : 1, \"fsUsedSize\" : 355637043200, \"fsTotalSize\" : 499963174912, \"ok\" : 1 }\n\nFirst, we have the name of the database, the number of collections it contains, and the number of views for the database. \"objects\" is the total count of documents across all collections in this database.\n\nThe bulk of the document contains information about the size of your data. \"fsTotalSize\" should always be the largest: it is the total size of the disk capacity on the filesystem where the MongoDB instance stores data. \"fsUsedSize\" represents the total space used in that filesystem by MongoDB currently. This should correspond to the total space used by all the files in your data directory.\n\nThe next-largest field is generally going to be \"dataSize\", which is the size of the uncompressed data held in this database. This doesn’t match \"storageSize\" because data is typically compressed in WiredTiger. \"indexSize\" is the amount of space all of the indexes for this database take up.\n\ndb.stats() can take a scale argument the same way that the collections’ stats func‐ tion can. If you call db.stats() on a nonexistent database, the values will all be zero.\n\nCalculating Sizes\n\n|\n\n385\n\nKeep in mind that listing databases on a system with a high lock percent can be very slow and block other operations. Avoid doing it, if possible.\n\nUsing mongotop and mongostat MongoDB comes with a few command-line tools that can help you determine what it’s doing by printing stats every few seconds.\n\nmongotop is similar to the top Unix utility: it gives you an overview of which collec‐ tions are busiest. You can also run mongotop --locks to give you locking statistics for each database.\n\nmongostat gives server-wide information. By default, mongostat prints out a list of statistics once per second, although this is configurable by passing a different number of seconds on the command line. Each of the fields gives a count of how many times the activity has happened since the field was last printed:\n\ninsert/query/update/delete/getmore/command\n\nSimple counts of how many of each of these operations there have been.\n\nflushes\n\nHow many times mongod has flushed data to disk.\n\nmapped\n\nThe amount of memory mongod has mapped. This is generally roughly the size of your data directory.\n\nvsize\n\nThe amount of virtual memory mongod is using. This is generally twice the size of your data directory (once for the mapped files, once again for journaling).\n\nres\n\nThe amount of memory mongod is using. This should generally be as close as possible to all the memory on the machine.\n\nlocked db\n\nThe database that spent the most time locked in the last timeslice. This field reports the percent of time the database was locked combined with how long the global lock was held, meaning that this value might be over 100%.\n\nidx miss %\n\nThe percentage of index accesses that had to page fault (because the index entry or section of index being searched was not in memory, so mongod had to go to disk). This is the most confusingly named field in the output.\n\n386\n\n|\n\nChapter 18: Seeing What Your Application Is Doing\n\nqr|qw\n\nThe queue size for reads and writes (i.e., how many reads and writes are block‐ ing, waiting to be processed).\n\nar|aw\n\nHow many active clients there are (i.e., clients currently performing reads and writes).\n\nnetIn\n\nThe number of network bytes in, as counted by MongoDB (not necessarily the same as what the OS would measure).\n\nnetOut\n\nThe number of network bytes out, as counted by MongoDB.\n\nconn\n\nThe number of connections this server has open, both incoming and outgoing.\n\ntime\n\nThe time at which these statistics were taken.\n\nYou can run mongostat on a replica set or sharded cluster. If you use the --discover option, mongostat will try to find all the members of the set or cluster from the mem‐ ber it initially connects to and will print one line per server per second for each. For a large cluster, this can get unmanageable fast, but it can be useful for small clusters and tools that can consume the data and present it in a more readable form.\n\nmongostat is a great way to get a quick snapshot of what your database is doing, but for long-term monitoring a tool like MongoDB Atlas or Ops Manager is preferred (see Chapter 22).\n\nUsing mongotop and mongostat\n\n|\n\n387",
      "page_number": 378
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 391-401)",
      "start_page": 391,
      "end_page": 401,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 19 An Introduction to MongoDB Security\n\nTo protect your MongoDB cluster and the data it holds, you will want to employ the following security measures:\n\nEnable authorization and enforce authentication\n\nEncrypt communication\n\nEncrypt data\n\nThis chapter demonstrates how to address the first two security measures with a tuto‐ rial on using MongoDB’s support for x.509 to configure authentication and transport layer encryption to ensure secure communications among clients and servers in a MongoDB replica set. We will touch on encrypting data at the storage layer in a later chapter.\n\nMongoDB Authentication and Authorization While authentication and authorization are closely connected, it is important to note that authentication is distinct from authorization. The purpose of authentication is to verify the identity of a user, while authorization determines the verified user’s access to resources and operations.\n\nAuthentication Mechanisms Enabling authorization on a MongoDB cluster enforces authentication and ensures users can only perform actions they are authorized for, as determined by their roles. The Community version of MongoDB provides support for SCRAM (Salted Chal‐ lenge Response Authentication Mechanism) and x.509 certificate authentication. In addition to SCRAM and x.509, MongoDB Enterprise supports Kerberos authentica‐ tion and LDAP proxy authentication. See the documentation for details on the vari‐\n\n389\n\nous authentication mechanisms that MongoDB supports. In this chapter, we will focus on x.509 authentication. An x.509 digital certificate uses the widely accepted x. 509 public key infrastructure (PKI) standard to verify that a public key belongs to the presenter.\n\nAuthorization When adding a user in MongoDB, you must create the user in a specific database. That database is the authentication database for the user; you can use any database for this purpose. The username and authentication database serves as a unique identifier for a user. However, a user’s privileges are not limited to their authentication data‐ base. When creating a user, you can specify the operations the user may perform on any resources to which they should have access. Resources include the cluster, data‐ bases, and collections.\n\nMongoDB provides a number of built-in roles that grant commonly needed permis‐ sions for database users. These include the following:\n\nread\n\nRead data on all nonsystem collections and on the following system collections: system.indexes, system.js, and system.namespaces.\n\nreadWrite\n\nProvides same privileges as read, plus the ability to modify data on all nonsystem collections and the system.js collection.\n\ndbAdmin\n\nPerform administrative tasks such as schema-related tasks, indexing, and gather‐ ing statistics (does not grant privileges for user and role management).\n\nuserAdmin\n\nCreate and modify roles and users on the current database.\n\ndbOwner\n\nCombines the privileges granted by the readWrite, dbAdmin, and userAdmin roles.\n\nclusterManager\n\nPerform management and monitoring actions on the cluster.\n\nclusterMonitor\n\nProvides read-only access to monitoring tools such as the MongoDB Cloud Man‐ ager and Ops Manager monitoring agent.\n\nhostManager\n\nMonitor and manage servers.\n\n390\n\n|\n\nChapter 19: An Introduction to MongoDB Security\n\nclusterAdmin\n\nCombines the privileges granted by the clusterManager, clusterMonitor, and host‐ Manager roles, plus the dropDatabase action.\n\nbackup\n\nProvides sufficient privileges to use the MongoDB Cloud Manager backup agent or the Ops Manager backup agent, or to use mongodump to back up an entire mongod instance.\n\nrestore\n\nProvides privileges needed to restore data from backups that do not include sys‐ tem.profile collection data.\n\nreadAnyDatabase\n\nProvides same privileges as read on all databases except local and config, plus the listDatabases action on the cluster as a whole.\n\nreadWriteAnyDatabase\n\nProvides same privileges as readWrite on all databases except local and config, plus the listDatabases action on the cluster as a whole.\n\nuserAdminAnyDatabase\n\nProvides same privileges as userAdmin on all databases except local and config (effectively a superuser role).\n\ndbAdminAnyDatabase\n\nProvides same privileges as dbAdmin on all databases except local and config, plus the listDatabases action on the cluster as a whole.\n\nroot\n\nProvides access to the operations and all the resources of the readWriteAnyData‐ base, dbAdminAnyDatabase, userAdminAnyDatabase, clusterAdmin, restore, and backup roles combined.\n\nYou may also create what are known as “user-defined roles,” which are custom roles that group together authorization to perform specific operations and label them with a name so that you may grant this set of permissions to multiple users easily.\n\nA deep dive on built-in roles or user-defined roles is beyond the scope of this chapter. However, this introduction should give you a pretty good idea of what’s possible with MongoDB authorization. For greater detail, please see the authorization section of the MongoDB documentation.\n\nTo ensure that you can add new users as needed, you must first create an admin user. MongoDB does not create a default root or admin user when enabling authentication and authorization, regardless of the authentication mode you are using (x.509 is no exception).\n\nMongoDB Authentication and Authorization\n\n|\n\n391\n\nIn MongoDB, authentication and authorization are not enabled by default. You must explicitly enable them by using the --auth option to the mongod command or specify‐ ing a value of \"enabled\" for the security.authorization setting in a MongoDB config file.\n\nTo configure a replica set, first bring it up without authentication and authorization enabled, then create the admin user and the users you’ll need for each client.\n\nUsing x.509 Certificates to Authenticate Both Members and Clients Given that all production MongoDB clusters are composed of multiple members, to secure a cluster, it is essential that all services communicating within the cluster authenticate with one another. Each member of a replica set must authenticate with the others in order to exchange data. Likewise, clients must authenticate with the pri‐ mary and any secondaries that they communicate with.\n\nFor x.509, it’s necessary that a trusted certification authority (CA) sign all certificates. Signing certifies that the named subject of a certificate owns the public key associated with that certificate. A CA acts as a trusted third party to prevent man-in-the-middle attacks.\n\nFigure 19-1 depicts x.509 authentication used to secure a three-member MongoDB replica set. Note the authentication among the client and members of the replica set and the trust relationships with the CA.\n\n392\n\n|\n\nChapter 19: An Introduction to MongoDB Security\n\nFigure 19-1. Overview of the trust hierarchy for X.509 authentication for the three- member replica set used in this chapter\n\nThe members and the client each have their own certificate signed by the CA. For production use, your MongoDB deployment should use valid certificates generated and signed by a single certificate authority. You or your organization can generate and maintain an independent certificate authority, or you can use certificates generated by a third-party TLS/SSL vendor.\n\nWe will refer to certificates used for internal authentication to verify membership in a cluster as member certificates. Both member certificates and client certificates (used to authenticate clients) have a structure resembling the following:\n\nCertificate: Data: Version: 1 (0x0) Serial Number: 1 (0x1) Signature Algorithm: sha256WithRSAEncryption Issuer: C=US, ST=NY, L=New York, O=MongoDB, CN=CA-SIGNER Validity Not Before: Nov 11 22:00:03 2018 GMT Not After : Nov 11 22:00:03 2019 GMT Subject: C=US, ST=NY, L=New York, O=MongoDB, OU=MyServers, CN=server1 Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit)\n\nMongoDB Authentication and Authorization\n\n|\n\n393\n\n394\n\nModulus: 00:d3:1c:29:ba:3d:29:44:3b:2b:75:60:95:c8:83: fc:32:1a:fa:29:5c:56:f3:b3:66:88:7f:f9:f9:89: ff:c2:51:b9:ca:1d:4c:d8:b8:5a:fd:76:f5:d3:c9: 95:9c:74:52:e9:8d:5f:2e:6b:ca:f8:6a:16:17:98: dc:aa:bf:34:d0:44:33:33:f3:9d:4b:7e:dd:7a:19: 1b:eb:3b:9e:21:d9:d9:ba:01:9c:8b:16:86:a3:52: a3:e6:e4:5c:f7:0c:ab:7a:1a:be:c6:42:d3:a6:01: 8e:0a:57:b2:cd:5b:28:ee:9d:f5:76:ca:75:7a:c1: 7c:42:d1:2a:7f:17:fe:69:17:49:91:4b:ca:2e:39: b4:a5:e0:03:bf:64:86:ca:15:c7:b2:f7:54:00:f7: 02:fe:cf:3e:12:6b:28:58:1c:35:68:86:3f:63:46: 75:f1:fe:ac:1b:41:91:4f:f2:24:99:54:f2:ed:5b: fd:01:98:65:ac:7a:7a:57:2f:a8:a5:5a:85:72:a6: 9e:fb:44:fb:3b:1c:79:88:3f:60:85:dd:d1:5c:1c: db:62:8c:6a:f7:da:ab:2e:76:ac:af:6d:7d:b1:46: 69:c1:59:db:c6:fb:6f:e1:a3:21:0c:5f:2e:8e:a7: d5:73:87:3e:60:26:75:eb:6f:10:c2:64:1d:a6:19: f3:0b Exponent: 65537 (0x10001) Signature Algorithm: sha256WithRSAEncryption 5d:dd:b2:35:be:27:c2:41:4a:0d:c7:8c:c9:22:05:cd:eb:88: 9d:71:4f:28:c1:79:71:3c:6d:30:19:f4:9c:3d:48:3a:84:d0: 19:00:b1:ec:a9:11:02:c9:a6:9c:74:e7:4e:3c:3a:9f:23:30: 50:5a:d2:47:53:65:06:a7:22:0b:59:71:b0:47:61:62:89:3d: cf:c6:d8:b3:d9:cc:70:20:35:bf:5a:2d:14:51:79:4b:7c:00: 30:39:2d:1d:af:2c:f3:32:fe:c2:c6:a5:b8:93:44:fa:7f:08: 85:f0:01:31:29:00:d4:be:75:7e:0d:f9:1a:f5:e9:75:00:9a: 7b:d0:eb:80:b1:01:00:c0:66:f8:c9:f0:35:6e:13:80:70:08: 5b:95:53:4b:34:ec:48:e3:02:88:5c:cd:a0:6c:b4:bc:65:15: 4d:c8:41:9d:00:f5:e7:f2:d7:f5:67:4a:32:82:2a:04:ae:d7: 25:31:0f:34:e8:63:a5:93:f2:b5:5a:90:71:ed:77:2a:a6:15: eb:fc:c3:ac:ef:55:25:d1:a1:31:7a:2c:80:e3:42:c2:b3:7d: 5e:9a:fc:e4:73:a8:39:50:62:db:b1:85:aa:06:1f:42:27:25: 4b:24:cf:d0:40:ca:51:13:94:97:7f:65:3e:ed:d9:3a:67:08: 79:64:a1:ba -----BEGIN CERTIFICATE----- MIIDODCCAiACAQEwDQYJKoZIhvcNAQELBQAwWTELMAkGA1UEBhMCQ04xCzAJBgNV BAgMAkdEMREwDwYDVQQHDAhTaGVuemhlbjEWMBQGA1UECgwNTW9uZ29EQiBDaGlu YTESMBAGA1UEAwwJQ0EtU0lHTkVSMB4XDTE4MTExMTIyMDAwM1oXDTE5MTExMTIy MDAwM1owazELMAkGA1UEBhMCQ04xCzAJBgNVBAgMAkdEMREwDwYDVQQHDAhTaGVu emhlbjEWMBQGA1UECgwNTW9uZ29EQiBDaGluYTESMBAGA1UECwwJTXlTZXJ2ZXJz MRAwDgYDVQQDDAdzZXJ2ZXIxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC AQEA0xwpuj0pRDsrdWCVyIP8Mhr6KVxW87NmiH/5+Yn/wlG5yh1M2Lha/Xb108mV nHRS6Y1fLmvK+GoWF5jcqr800EQzM/OdS37dehkb6zueIdnZugGcixaGo1Kj5uRc 9wyrehq+xkLTpgGOCleyzVso7p31dsp1esF8QtEqfxf+aRdJkUvKLjm0peADv2SG yhXHsvdUAPcC/s8+EmsoWBw1aIY/Y0Z18f6sG0GRT/IkmVTy7Vv9AZhlrHp6Vy+o pVqFcqae+0T7Oxx5iD9ghd3RXBzbYoxq99qrLnasr219sUZpwVnbxvtv4aMhDF8u jqfVc4c+YCZ1628QwmQdphnzCwIDAQABMA0GCSqGSIb3DQEBCwUAA4IBAQBd3bI1 vifCQUoNx4zJIgXN64idcU8owXlxPG0wGfScPUg6hNAZALHsqRECyaacdOdOPDqf IzBQWtJHU2UGpyILWXGwR2FiiT3Pxtiz2cxwIDW/Wi0UUXlLfAAwOS0dryzzMv7C xqW4k0T6fwiF8AExKQDUvnV+Dfka9el1AJp70OuAsQEAwGb4yfA1bhOAcAhblVNL\n\n|\n\nChapter 19: An Introduction to MongoDB Security\n\nNOxI4wKIXM2gbLS8ZRVNyEGdAPXn8tf1Z0oygioErtclMQ806GOlk/K1WpBx7Xcq phXr/MOs71Ul0aExeiyA40LCs31emvzkc6g5UGLbsYWqBh9CJyVLJM/QQMpRE5SX f2U+7dk6Zwh5ZKG6 -----END CERTIFICATE-----\n\nFor use with x.509 authentication in MongoDB, member certificates must have the following properties:\n\nA single CA must issue all x.509 certificates for the members of the cluster. • The Distinguished Name (DN), found in the subject of the member certificate, must specify a nonempty value for at least one of the following attributes: Orga‐ nization (O), Organizational Unit (OU), or Domain Component (DC).\n\nThe O, OU, and DC attributes must match those from the certificates for the other cluster members.\n\nThe Common Name (CN) or a Subject Alternative Name (SAN) must match the hostname of the server used by the other members of the cluster.\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption In this tutorial we will set up a root CA and an intermediate CA. Best practice recom‐ mends signing the server and client certificates with the intermediate CA.\n\nEstablish a CA Before we can generate signed certificates for the members of our replica set, we must first address the issue of a certificate authority. As mentioned previously, we can either generate and maintain an independent certificate authority or use certificates generated by a third-party TLS/SSL vendor. We will generate our own CA to use for the running example in this chapter. Note that you may access all the code examples in this chapter from the GitHub repository maintained for this book. The examples are drawn from a script you can use to deploy a secure replica set. You’ll see com‐ ments from this script throughout these examples.\n\nGenerate a root CA\n\nTo generate our CA, we will use OpenSSL. To follow along, please make sure you have access to OpenSSL on your local machine.\n\nA root CA is at the top of the certificate chain. This is the ultimate source of trust. Ideally, a third-party CA should be used. However, in the case of an isolated network (typical in a large enterprise environment) or for testing purposes, you’ll need to use a local CA.\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption\n\n|\n\n395\n\nFirst, we’ll initialize some variables:\n\ndn_prefix=\"/C=US/ST=NY/L=New York/O=MongoDB\" ou_member=\"MyServers\" ou_client=\"MyClients\" mongodb_server_hosts=( \"server1\" \"server2\" \"server3\" ) mongodb_client_hosts=( \"client1\" \"client2\" ) mongodb_port=27017\n\nThen, we’ll create a key pair and store it in the file root-ca.key:\n\n# !!! In production you will want to password-protect the keys # openssl genrsa -aes256 -out root-ca.key 4096 openssl genrsa -out root-ca.key 4096\n\nNext, we’ll create a configuration file to hold our OpenSSL settings that we will use to generate the certificates:\n\n# For the CA policy [ policy_match ] countryName = match stateOrProvinceName = match organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional\n\n[ req ] default_bits = 4096 default_keyfile = server-key.pem default_md = sha256 distinguished_name = req_dn req_extensions = v3_req x509_extensions = v3_ca # The extensions to add to the self-signed cert\n\n[ v3_req ] subjectKeyIdentifier = hash basicConstraints = CA:FALSE keyUsage = critical, digitalSignature, keyEncipherment nsComment = \"OpenSSL Generated Certificate\" extendedKeyUsage = serverAuth, clientAuth\n\n[ req_dn ] countryName = Country Name (2-letter code) countryName_default = US countryName_min = 2 countryName_max = 2\n\nstateOrProvinceName = State or Province Name (full name) stateOrProvinceName_default = NY stateOrProvinceName_max = 64\n\nlocalityName = Locality Name (eg, city)\n\n396\n\n|\n\nChapter 19: An Introduction to MongoDB Security\n\nlocalityName_default = New York localityName_max = 64\n\norganizationName = Organization Name (eg, company) organizationName_default = MongoDB organizationName_max = 64\n\norganizationalUnitName = Organizational Unit Name (eg, section) organizationalUnitName_default = Education organizationalUnitName_max = 64\n\ncommonName = Common Name (eg, YOUR name) commonName_max = 64\n\n[ v3_ca ] # Extensions for a typical CA\n\nsubjectKeyIdentifier = hash basicConstraints = critical,CA:true authorityKeyIdentifier = keyid:always,issuer:always\n\n# Key usage: this is typical for a CA certificate. However, since it will # prevent it being used as a test self-signed certificate it is best # left out by default. keyUsage = critical,keyCertSign,cRLSign\n\nThen, using the openssl req command, we will create the root certificate. Since the root is the very top of the authority chain, we’ll self-sign this certificate using the pri‐ vate key we created in the previous step (stored in root-ca.key). The -x509 option tells the openssl req command we want to self-sign the certificate using the private key supplied to the -key option. The output is a file called root-ca.crt:\n\nopenssl req -new -x509 -days 1826 -key root-ca.key -out root-ca.crt \\ -config openssl.cnf -subj \"$dn_prefix/CN=ROOTCA\"\n\nIf you take a look at the root-ca.crt file, you’ll find that it contains the public certificate for the root CA. You can verify the contents by taking a look at a human-readable version of the certificate produced by this command:\n\nopenssl x509 -noout -text -in root-ca.crt\n\nThe output from this command will resemble the following:\n\nCertificate: Data: Version: 3 (0x2) Serial Number: 1e:83:0d:9d:43:75:7c:2b:d6:2a:dc:7e:a2:a2:25:af:5d:3b:89:43 Signature Algorithm: sha256WithRSAEncryption Issuer: C = US, ST = NY, L = New York, O = MongoDB, CN = ROOTCA Validity Not Before: Sep 11 21:17:24 2019 GMT\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption\n\n|\n\n397\n\n398\n\nNot After : Sep 10 21:17:24 2024 GMT Subject: C = US, ST = NY, L = New York, O = MongoDB, CN = ROOTCA Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (4096 bit) Modulus: 00:e3:de:05:ae:ba:c9:e0:3f:98:37:18:77:02:35: e7:f6:62:bc:c3:ae:38:81:8d:04:88:da:6c:e0:57: c2:90:86:05:56:7b:d2:74:23:54:f8:ca:02:45:0f: 38:e7:e2:0b:69:ea:f6:c8:13:8f:6c:2d:d6:c1:72: 64:17:83:4e:68:47:cf:de:37:ed:6e:38:b2:ab:3a: e4:45:a8:fa:08:90:a0:f3:0d:3a:14:d8:9a:8d:69: e7:cf:93:1a:71:53:4f:13:29:50:b0:2f:b6:b8:19: 2a:40:21:15:90:43:e7:d8:d8:f3:51:e5:95:58:87: 6c:45:9f:61:fc:b5:97:cf:5b:4e:4a:1f:72:c9:0c: e9:8c:4c:d1:ca:df:b3:a4:da:b4:10:83:81:01:b1: c8:09:22:76:c7:1e:96:c7:e6:56:27:8d:bc:fb:17: ed:d9:23:3f:df:9c:ef:03:20:cc:c3:c4:55:cc:9f: ad:d4:8d:81:95:c3:f1:87:f8:d4:5a:5e:e0:a8:41: 27:c8:0d:52:91:e4:2b:db:25:d6:b7:93:8d:82:33: 7a:a7:b8:e8:cd:a8:e2:94:3d:d6:16:e1:4e:13:63: 3f:77:08:10:cf:23:f6:15:7c:71:24:97:ef:1c:a2: 68:0f:82:e2:f7:24:b3:aa:70:1a:4a:b4:ca:4d:05: 92:5e:47:a2:3d:97:82:f6:d8:c8:04:a7:91:6c:a4: 7d:15:8e:a8:57:70:5d:50:1c:0b:36:ba:78:28:f2: da:5c:ed:4b:ea:60:8c:39:e6:a1:04:26:60:b3:e2: ee:4f:9b:f9:46:3c:7e:df:82:88:29:c2:76:3e:1a: a4:81:87:1f:ce:9e:41:68:de:6c:f3:89:df:ae:02: e7:12:ee:93:20:f1:d2:d6:3d:36:58:ee:71:bf:b3: c5:e7:5a:4b:a0:12:89:ed:f7:cc:ec:34:c7:b2:28: a8:1a:87:c6:8b:5e:d2:c8:25:71:ba:ff:d0:82:1b: 5e:50:a9:8a:c6:0c:ea:4b:17:a6:cc:13:0a:53:36: c6:9d:76:f2:95:cc:ac:b9:64:d5:72:fc:ab:ce:6b: 59:b1:3a:f2:49:2f:2c:09:d0:01:06:e4:f2:49:85: 79:82:e8:c8:bb:1a:ab:70:e3:49:97:9f:84:e0:96: c2:6d:41:ab:59:0c:2e:70:9a:2e:11:c8:83:69:4b: f1:19:97:87:c3:76:0e:bb:b0:2c:92:4a:07:03:6f: 57:bf:a9:ec:19:85:d6:3d:f8:de:03:7f:1b:9a:2f: 6c:02:72:28:b0:69:d5:f9:fb:3d:2e:31:8f:61:50: 59:a6:dd:43:4b:89:e9:68:4b:a6:0d:9b:00:0f:9a: 94:61:71 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Key Identifier: 8B:D6:F8:BD:B7:82:FC:13:BC:61:3F:8B:FA:84:24:3F:A2:14:C8:27 X509v3 Basic Constraints: critical CA:TRUE X509v3 Authority Key Identifier: keyid:8B:D6:F8:BD:B7:82:FC:13:BC:61:3F:8B:FA:84:24:3F:A2:14:C8:27 DirName:/C=US/ST=NY/L=New York/O=MongoDB/CN=ROOTCA serial:1E:83:0D:9D:43:75:7C:2B:D6:2A:DC:7E:A2:A2:25:AF:5D:3B:89:43\n\n|\n\nChapter 19: An Introduction to MongoDB Security\n\nX509v3 Key Usage: critical Certificate Sign, CRL Sign Signature Algorithm: sha256WithRSAEncryption c2:cc:79:40:8b:7b:a1:87:3a:ec:4a:71:9d:ab:69:00:bb:6f: 56:0a:25:3b:8f:bd:ca:4d:4b:c5:27:28:3c:7c:e5:cf:84:ec: 2e:2f:0d:37:35:52:6d:f9:4b:07:fb:9b:da:ea:5b:31:0f:29: 1f:3c:89:6a:10:8e:ae:20:30:8f:a0:cf:f1:0f:41:99:6a:12: 5f:5c:ce:15:d5:f1:c9:0e:24:c4:81:70:df:ad:a0:e1:0a:cc: 52:d4:3e:44:0b:61:48:a9:26:3c:a3:3d:2a:c3:ca:4f:19:60: da:f7:7a:4a:09:9e:26:42:50:05:f8:74:13:4b:0c:78:f1:59: 39:1e:eb:2e:e1:e2:6c:cc:4d:96:95:79:c2:8b:58:41:e8:7a: e6:ad:37:e4:87:d7:ed:bb:7d:fa:47:dd:46:dd:e7:62:5f:e9: fe:17:4b:e3:7a:0e:a1:c5:80:78:39:b7:6c:a6:85:cf:ba:95: d2:8d:09:ab:2d:cb:be:77:9b:3c:22:12:ca:12:86:42:d8:c5: 3c:31:a0:ed:92:bc:7f:3f:91:2d:ec:db:01:bd:26:65:56:12: a3:56:ba:d8:d3:6e:f3:c3:13:84:98:2a:c7:b3:22:05:68:fa: 8e:48:6f:36:8e:3f:e5:4d:88:ef:15:26:4c:b1:d3:7e:25:84: 8c:bd:5b:d2:74:55:cb:b3:fa:45:3f:ee:ef:e6:80:e9:f7:7f: 25:a6:6e:f2:c4:22:f7:b8:40:29:02:f1:5e:ea:8e:df:80:e0: 60:f1:e5:3a:08:81:25:d5:cc:00:8f:5c:ac:a6:02:da:27:c0: cc:4e:d3:f3:14:60:c1:12:3b:21:b4:f7:29:9b:4c:34:39:3c: 2a:d1:4b:86:cc:c7:de:f3:f7:5e:8f:9d:47:2e:3d:fe:e3:49: 70:0e:1c:61:1c:45:a0:5b:d6:48:49:be:6d:f9:3c:49:26:d8: 8b:e6:a1:b2:61:10:fe:0c:e8:44:2c:33:cd:3c:1d:c2:de:c2: 06:98:7c:92:7b:c4:06:a5:1f:02:8a:03:53:ec:bd:b7:fc:31: f3:2a:c1:0e:6a:a5:a8:e4:ea:4d:cc:1d:07:a9:3f:f6:0e:35: 5d:99:31:35:b3:43:90:f3:1c:92:8e:99:15:13:2b:8f:f6:a6: 01:c9:18:05:15:2a:e3:d0:cc:45:66:d3:48:11:a2:b9:b1:20: 59:42:f7:88:15:9f:e0:0c:1d:13:ae:db:09:3d:bf:7a:9d:cf: b2:41:1e:7a:fa:6b:35:20:03:58:a1:6c:02:19:21:5f:25:fc: ba:2f:fc:79:d7:92:e7:37:77:14:10:d9:33:b6:e5:fb:7a:46: ab:d1:86:70:88:92:59:c3\n\nCreate an intermediate CA for signing\n\nNow that we’ve created our root CA, we will create an intermediate CA for signing member and client certificates. An intermediate CA is nothing more than a certificate signed using our root certificate. It is a best practice to use an intermediate CA to sign server (i.e., member) and client certificates. Typically, a CA will use different inter‐ mediate CAs for signing different categories of certificates. If the intermediate CA is compromised and the certificate needs to be revoked, only a portion of the trust tree is affected instead of all certificates signed by the CA, as would be the case if the root CA were used to sign all certificates.\n\n# again, in production you would want to password protect your signing key: # openssl genrsa -aes256 -out signing-ca.key 4096 openssl genrsa -out signing-ca.key 4096\n\nopenssl req -new -key signing-ca.key -out signing-ca.csr \\ -config openssl.cnf -subj \"$dn_prefix/CN=CA-SIGNER\" openssl x509 -req -days 730 -in signing-ca.csr -CA root-ca.crt -CAkey \\\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption\n\n|\n\n399",
      "page_number": 391
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 402-410)",
      "start_page": 402,
      "end_page": 410,
      "detection_method": "topic_boundary",
      "content": "root-ca.key -set_serial 01 -out signing-ca.crt -extfile openssl.cnf \\ -extensions v3_ca\n\nNote that in the statements above we are using the openssl req command followed by the openssl ca command to sign our signing certificate using our root certificate. The openssl req command creates a signing request and the openssl ca command uses that request as input to create a signed intermediate (signing) certificate.\n\nAs a last step in creating our signing CA, we will concatenate our root certificate (containing our root public key) and signing certificate (containing our signing pub‐ lic key) into a single pem file. This file will be supplied to our mongod or client pro‐ cess later as the value of the --tlsCAFile option.\n\ncat root-ca.crt > root-ca.pem cat signing-ca.crt >> root-ca.pem\n\nWith the root CA and signing CA set up, we are now ready to create the member and client certificates used for authentication in our MongoDB cluster.\n\nGenerate and Sign Member Certificates Member certificates are typically referred to as x.509 server certificates. Use this type of certificate for mongod and mongos processes. Members of a MongoDB cluster use these certificates to verify membership in the cluster. Stated another way, one mongod authenticates itself with other members of a replica set using a server certificate.\n\nTo generate certificates for the members of our replica set, we will use a for loop to generate multiple certificates.\n\n# Pay attention to the OU part of the subject in \"openssl req\" command for host in \"${mongodb_server_hosts[@]}\"; do echo \"Generating key for $host\" openssl genrsa -out ${host}.key 4096\n\nopenssl req -new -key ${host}.key -out ${host}.csr -config openssl.cnf \\ -subj \"$dn_prefix/OU=$ou_member/CN=${host}\" openssl x509 -req -days 365 -in ${host}.csr -CA signing-ca.crt -CAkey \\ signing-ca.key -CAcreateserial -out ${host}.crt -extfile openssl.cnf \\ -extensions v3_req cat ${host}.crt > ${host}.pem cat ${host}.key >> ${host}.pem done\n\nThree steps are involved with each certificate:\n\nUse the openssl genrsa command to create a new key pair.\n\nUse the openssl req command to generate a signing request for the key.\n\nUse the openssl x509 command to sign and output a certificate using the signing CA.\n\n400\n\n|\n\nChapter 19: An Introduction to MongoDB Security\n\nNotice the variable $ou_member. This signifies the difference between server certifi‐ cates and client certificates. Server and client certificates must differ in the organiza‐ tion part of the Distinguished Names. More specifically, they must differ in at least one of the O, OU, or DC values.\n\nGenerate and Sign Client Certificates Client certificates are used by the mongo shell, MongoDB Compass, MongoDB utilit‐ ies and tools and, of course, by applications using a MongoDB driver. Generating cli‐ ent certificates follows essentially the same process as for member certificates. The one difference is our use of the variable $ou_client. This ensure that the combina‐ tion of the O, OU, and DC values will be different from those of the server certificates generated above.\n\n# Pay attention to the OU part of the subject in \"openssl req\" command for host in \"${mongodb_client_hosts[@]}\"; do echo \"Generating key for $host\" openssl genrsa -out ${host}.key 4096 openssl req -new -key ${host}.key -out ${host}.csr -config openssl.cnf \\ -subj \"$dn_prefix/OU=$ou_client/CN=${host}\" openssl x509 -req -days 365 -in ${host}.csr -CA signing-ca.crt -CAkey \\ signing-ca.key -CAcreateserial -out ${host}.crt -extfile openssl.cnf \\ -extensions v3_req cat ${host}.crt > ${host}.pem cat ${host}.key >> ${host}.pem done\n\nBring Up the Replica Set Without Authentication and Authorization Enabled We can start each member of our replica set without auth enabled as follows. Previ‐ ously, when working with replica sets we’ve not enabled auth so this should look familiar. Here again we are making use of a few variables we defined in “Generate a root CA” on page 395 (or see the full script for this chapter) and a loop to launch each member (mongod) of our replica set.\n\nmport=$mongodb_port for host in \"${mongodb_server_hosts[@]}\"; do echo \"Starting server $host in non-auth mode\" mkdir -p ./db/${host} mongod --replSet set509 --port $mport --dbpath ./db/$host \\ --fork --logpath ./db/${host}.log let \"mport++\" done\n\nOnce each mongod has started, we can then initialize a replica set using these mongods.\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption\n\n|\n\n401\n\nmyhostname=`hostname` cat > init_set.js <<EOF rs.initiate(); mport=$mongodb_port; mport++; rs.add(\"localhost:\" + mport); mport++; rs.add(\"localhost:\" + mport); EOF mongo localhost:$mongodb_port init_set.js\n\nNote that the code above simply constructs a series of commands, stores these com‐ mands in a JavaScript file, and then runs the mongo shell to execute the small script that was created. Together, these commands, when executed in the mongo shell, will connect to the mongod running on port 27017 (value of the $mongodb_port variable set in “Generate a root CA” on page 395), initiate the replica set, and then add each of the other two mongods (on ports 27018 and 27019) to the replica set.\n\nCreate the Admin User Now, we’ll create an admin user based on one of the client certificates we created in “Generate and Sign Client Certificates” on page 401. We will authenticate as this user when connecting from the mongo shell or another client to perform administrative tasks. To authenticate with a client certificate, you must first add the value of the sub‐ ject from the client certificate as a MongoDB user. Each unique x.509 client certificate corresponds to a single MongoDB user; i.e., you cannot use a single client certificate to authenticate more than one MongoDB user. We must add the user in the $external database; i.e., the authentication database is the $external database.\n\nFirst, we’ll get the subject from our client certificate using the openssl x509 command.\n\nopenssl x509 -in client1.pem -inform PEM -subject -nameopt RFC2253 | grep subject\n\nThis should result in the following output:\n\nsubject= CN=client1,OU=MyClients,O=MongoDB,L=New York,ST=NY,C=US\n\nTo create our admin user, we’ll first connect to the primary of our replica set using the mongo shell.\n\nmongo --norc localhost:27017\n\nFrom within the mongo shell, we will issue the following command:\n\ndb.getSiblingDB(\"$external\").runCommand( { createUser: \"CN=client1,OU=MyClients,O=MongoDB,L=New York,ST=NY,C=US\", roles: [ { role: \"readWrite\", db: 'test' }, { role: \"userAdminAnyDatabase\", db: \"admin\" }, { role: \"clusterAdmin\", db:\"admin\"}\n\n402\n\n|\n\nChapter 19: An Introduction to MongoDB Security\n\n], writeConcern: { w: \"majority\" , wtimeout: 5000 } } );\n\nNote the use of the $external database in this command and the fact that we’ve speci‐ fied the subject of our client certificate as the user name.\n\nRestart the Replica Set with Authentication and Authorization Enabled Now that we have an admin user, we can restart the replica set with authentication and authorization enabled and connect as a client. Without a user of any kind, it would be impossible to connect to a replica set with auth enabled.\n\nLet’s stop the replica set in it’s current form (without auth enabled).\n\nkill $(ps -ef | grep mongod | grep set509 | awk '{print $2}')\n\nWe are now ready to restart the replica set with auth enabled. In a production envi‐ ronment, we would copy each of the certificate and key files to their corresponding hosts. Here we’re doing everything on localhost to make things easier. To initiate a secure replica set we will add the following command-line options to each invocation of mongod:\n\n--tlsMode • --clusterAuthMode • --tlsCAFile—root CA file (root-ca.key) • --tlsCertificateKeyFile—certificate file for the mongod • --tlsAllowInvalidHostnames—only used for testing; allows invalid hostnames\n\nHere the file we provide as the value of the tlsCAFile option is used to establish a trust chain. As you recall the root-ca.key file contains the certificate of the root CA as well as the signing CA. By providing this file to the mongod process, we are stating our desire to trust the certificate contained in this file as well as all other certificates signed by these certificates.\n\nOkay, let’s do this.\n\nmport=$mongodb_port for host in \"${mongodb_server_hosts[@]}\"; do echo \"Starting server $host\" mongod --replSet set509 --port $mport --dbpath ./db/$host \\ --tlsMode requireTLS --clusterAuthMode x509 --tlsCAFile root-ca.pem \\ --tlsAllowInvalidHostnames --fork --logpath ./db/${host}.log \\ --tlsCertificateKeyFile ${host}.pem --tlsClusterFile ${host}.pem \\ --bind_ip 127.0.0.1\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption\n\n|\n\n403\n\nlet \"mport++\" done\n\nAnd with that, we have a three-member replica set secured using x.509 certificates for authentication and transport-layer encryption. The only thing left to do is to connect with the mongo shell. We’ll use the client1 certificate to authenticate, because that is the certificate for which we created an admin user.\n\nmongo --norc --tls --tlsCertificateKeyFile client1.pem --tlsCAFile root-ca.pem \\ --tlsAllowInvalidHostnames --authenticationDatabase \"\\$external\" \\ --authenticationMechanism MONGODB-X509\n\nOnce connected, we encourage you to experiment by inserting some data to a collec‐ tion. You should also attempt to connect using any other user (e.g., using the client2.pem). Connections attempts will result in errors like the following.\n\nmongo --norc --tls --tlsCertificateKeyFile client2.pem --tlsCAFile root-ca.pem \\ --tlsAllowInvalidHostnames --authenticationDatabase \"\\$external\" \\ --authenticationMechanism MONGODB-X509 MongoDB shell version v4.2.0 2019-09-11T23:18:31.696+0100 W NETWORK [js] The server certificate does not match the host name. Hostname: 127.0.0.1 does not match 2019-09-11T23:18:31.702+0100 E QUERY [js] Error: Could not find user \"CN=client2,OU=MyClients,O=MongoDB,L=New York,ST=NY,C=US\" for db \"$external\" : connect@src/mongo/shell/mongo.js:341:17 @(connect):3:6 2019-09-11T23:18:31.707+0100 F - [main] exception: connect failed 2019-09-11T23:18:31.707+0100 E - [main] exiting with code 1\n\nIn the tutorial in this chapter, we’ve looked at an example of using x.509 certificates as a basis for authentication and to encrypt communication among clients and members of a replica set. The same procedure works for sharded clusters as well. With respect to securing a MongoDB cluster, please keep the following in mind:\n\nThe directories, root CA and signing CA, as well as the host itself where you gen‐ erate and sign certificates for the member machines or clients, should be pro‐ tected from unauthorized access.\n\nFor simplicity, the root CA and signing CA keys are not password protected in this tutorial. In production it is necessary to use passwords to protect the key from unauthorized use.\n\nWe encourage you to download and experiment with the demo scripts we have pro‐ vided for this chapter in the book’s GitHub repository.\n\n404\n\n|\n\nChapter 19: An Introduction to MongoDB Security\n\nCHAPTER 20 Durability\n\nDurability is a property of database systems that guarantees that write operations that have been committed to the database will survive permanently. For example, if a ticket reservation system reports that your concert seats have been booked, then your seats will remain booked even if some part of the reservation system crashes. For MongoDB, we need to consider durability at the cluster (or more specifically, replica set) level.\n\nIn this chapter, we will cover:\n\nHow MongoDB guarantees durability at the replica set member level through journaling\n\nHow MongoDB guarantees durability at the cluster level using write concern\n\nHow to configure your application and MongoDB cluster to give you the level of durability you need\n\nHow MongoDB guarantees durability at the cluster level using read concern\n\nHow to set the durability level for transactions in replica sets\n\nThroughout this chapter, we will discuss durability in replica sets. A three-member replica set is the most basic cluster recommended for production applications. The discussion here applies to replica sets with more members and to sharded clusters.\n\nDurability at the Member Level Through Journaling To provide durability in the event of a server failure, MongoDB uses a write-ahead log (WAL) called the journal. A WAL is a commonly used technique for durability in database systems. The idea is that we simply write a representation of the changes to be made to the database to a durable medium (i.e., to disk) before applying those\n\n405\n\nchanges to the database itself. In many database systems, a WAL is used to provide the atomicity database property as well. However, MongoDB uses other techniques to ensure atomic writes.\n\nBeginning in MongoDB 4.0, as an application performs writes to a replica set, for the data in all replicated collections MongoDB creates journal entries using the same for‐ mat as the oplog.1 As discussed in Chapter 11, MongoDB uses statement-based repli‐ cation based on an operations log, or oplog. The statements in the oplog are a representation of the actual MongoDB changes made to each document affected by a write. Therefore, oplog statements are easy to apply to any member of a replica set regardless of version, hardware, or any other differences between replica set mem‐ bers. In addition, each oplog statement is idempotent, meaning that it can be applied any number of times and the outcome will always be the same change to the database.\n\nLike most databases, MongoDB maintains in-memory views of both the journal and the database data files. By default, it flushes journal entries to disk every 50 milli‐ seconds and flushes database files to disk every 60 seconds. The 60-second interval for flushing data files is called a checkpoint. The journal is used to provide durability for data written since the last checkpoint. With respect to durability concerns, if the server suddenly stops, when it’s restarted the journal can be used to replay any writes that were not flushed to disk before the shutdown.\n\nFor the journal files, MongoDB creates a subdirectory named journal under the dbPath directory. WiredTiger (MongoDB’s default storage engine) journal files have names with the format WiredTigerLog.<sequence>, where <sequence> is a zero- padded number starting from 0000000001. Except for very small log records, Mon‐ goDB compresses the data written to the journal. Journal files have a maximum size limit of approximately 100 MB. Once a journal file exceeds that limit, MongoDB cre‐ ates a new journal file and begins writing new records there. Because journal files are only needed to recover data since the last checkpoint, MongoDB automatically removes “old” journal files—i.e., those written prior to the most recent checkpoint— once a new checkpoint is written.\n\nIf there is a crash (or kill -9), mongod will replay its journal files on startup. By default, the greatest extent of lost writes are those made in the last 100 ms plus the time it takes to flush the journal writes to disk.\n\nIf your application requires a shorter interval for journal flushes, you have two options. One is to change the interval using the --journalCommitInterval option to the mongod command. This option accepts values ranging from 1 to 500 ms. The other option, which we’ll look at in the next section, is to specify in the write concern\n\n1 MongoDB uses a different format for writes to the local database, which stores data used in the replication\n\nprocess and other instance-specific data, but the principles and application are similar.\n\n406\n\n|\n\nChapter 20: Durability\n\nthat all writes should journal to disk. Shortening the interval for journaling to disk will negatively impact performance, so you need to be sure of the implications for your applications before changing the journaling default.\n\nDurability at the Cluster Level Using Write Concern With write concern, you can specify what level of acknowledgment your application requires in response to write requests. In a replica set, network partitions, server fail‐ ures, or data center outages may keep writes from being replicated to every member, or even a majority of the members. When a normal state is restored to the replica set, it is possible that writes not replicated to a majority of members will be rolled back. In those situations, clients and the database may have a different view of what data has been committed.\n\nThere are applications for which it might be acceptable in some circumstances to have writes rolled back. For example, it might be okay to roll back a small number of comments in a social application of some kind. MongoDB supports a range of dura‐ bility guarantees at the cluster level to enable application designers to select the dura‐ bility level that works best for their use case.\n\nThe w and wtimeout Options for writeConcern The MongoDB query language supports specifying a write concern for all insert and update methods. As an example, suppose we have an ecommerce application and want to ensure that all orders are durable. Writing an order to the database might look something like the following:\n\ntry { db.products.insertOne( { sku: \"H1100335456\", item: \"Electric Toothbrush Head\", quantity: 3 }, { writeConcern: { w : \"majority\", wtimeout : 100 } } ); } catch (e) { print (e); }\n\nAll insert and update methods take a second parameter, a document. Within that document you can specify a value for writeConcern. In the preceding example, the write concern we have specified indicates that we want to see an acknowledgment from the server that the write completed successfully only if the write was successfully replicated to a majority of the members of our application’s replica set. In addition, the write should return an error if it is not replicated to a majority of replica set mem‐ bers in 100 ms or less. In the case of such an error, MongoDB does not undo success‐ ful data modifications performed before the write concern exceeded the time limit— it will be up to the application to choose how to handle timeouts in such situations. In general, you should configure the wtimeout value so that only in unusual circumstan‐\n\nDurability at the Cluster Level Using Write Concern\n\n|\n\n407\n\nces will the application experience timeouts and any actions your application takes in response to a timeout error will ensure the correct state for your data. In most cases, your application should attempt to determine whether the timeout was a result of a transient slowdown in network communications or something more signficant.\n\nAs the value for w in the write concern document, you may specify \"majority\" (as was done in this example). Alternatively, you may specify an integer between zero and the number of members in the replica set. Finally, it is possible to tag replica set members, say to identify those on SSDs versus spinning disks or those used for reporting versus OLTP workloads. You may specify a tag set as the value of w to ensure that writes will only be acknowledged once committed to at least one member of the replica set matching the provided tag set.\n\nThe j (Journaling) Option for writeConcern In addition to providing a value for the w option, you may also request acknowledg‐ ment that the write operation has been written to the journal by using the j option in the write concern document. With a value of true for j, MongoDB acknowledges a successful write only after the requested number of members (the value for w) have written the operation to their on-disk journal. Continuing our example, if we want to ensure all writes are journaled on a majority of members, we can update the code as follows:\n\ntry { db.products.insertOne( { sku: \"H1100335456\", item: \"Electric Toothbrush Head\", quantity: 3 }, { writeConcern: { w : \"majority\", wtimeout : 100, j : true } } ); } catch (e) { print (e); }\n\nWithout waiting for journaling, there is a brief window of about 100 ms on each member when, if the server process or hardware goes down, a write could be lost. However, waiting for journaling before acknowledging writes to members of a replica set does have a performance penalty.\n\nIt is essential that in addressing durability concerns for your applications, you care‐ fully evaluate the requirements your application has and weigh the performance impacts of the durability settings you select.\n\nDurability at a Cluster Level Using Read Concern In MongoDB, read concerns allow for the configuration of when results are read. This can allow clients to see write results before those writes are durable. A read con‐ cern can be used with a write concern to control the level of consistency and\n\n408\n\n|\n\nChapter 20: Durability",
      "page_number": 402
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 411-419)",
      "start_page": 411,
      "end_page": 419,
      "detection_method": "topic_boundary",
      "content": "availability guarantees made to an application. They should not be confused with read preferences, which deal with where the data is read from; specifically, read preferences determine the data bearing member(s) in the replica set. The default read preferences is to read from the primary.\n\nRead concern determines the consistency and isolation properties of the data being read. The default readConcern is local, which returns data with no guarantees that the data has been written to the majority of the data bearing replica set members. This can result in the data being rolled back in the future. The majority concern returns only durable data (will not be rolled back) that has been acknowledged by the majority of replica set members. In MongoDB 3.4, the linearizable concern was added. It ensures data returned reflects all successful majority-acknowledged writes that have completed prior to the start of the read operation. It may wait for concur‐ rently executing writes to finish before providing results.\n\nIn the same fashion, with write concerns you will need to weight the performance impacts of the read concerns against the durability and isolation guarantees they provide before selecting the appropriate concern for your application.\n\nDurability of Transactions Using a Write Concern In MongoDB, operations on individual documents are atomic. You can use embed‐ ded documents and arrays to express relationships between entities in a single docu‐ ment rather than using a normalized data model splitting entities and relationships across multiple collections. As a result, many applications do not require multi- document transactions.\n\nHowever, for use cases that require atomicity for updates to multiple documents, MongoDB provides the ability to perform multi-document transactions against rep‐ lica sets. Multi-document transactions can be used across multiple operations, docu‐ ments, collections, and databases.\n\nTransactions require that all data changes within the transaction are successful. If any operation fails, the transaction aborts and all data changes are discarded. If all opera‐ tions are successful, all data changes made in the transaction are saved and the writes become visible to future reads.\n\nAs with individual write operations, you may specify a write concern for transactions. You set the write concern at the transaction level, not at the individual operation level. At the time of the commit, transactions use the transaction-level write concern to commit the write operations. Write concerns set for individual operations inside the transaction will be ignored.\n\nYou can set the write concern for the transaction commit at the transaction start. A write concern of 0 is not supported for transactions. If you use a write concern of 1\n\nDurability of Transactions Using a Write Concern\n\n|\n\n409\n\nfor a transaction, it can be rolled back if there is a failover. You may use a writeCon cern of \"majority\" to ensure transactions are durable in the face of network and server failures that might force a failover in a replica set. The following provides an example:\n\nfunction updateEmployeeInfo(session) { employeesCollection = session.getDatabase(\"hr\").employees; eventsCollection = session.getDatabase(\"reporting\").events;\n\nsession.startTransaction( {writeConcern: { w: \"majority\" } } );\n\ntry{ employeesCollection.updateOne( { employee: 3 }, { $set: { status: \"Inactive\" } } ); eventsCollection.insertOne( { employee: 3, status: { new: \"Inactive\", old: \"Active\" } } ); } catch (error) { print(\"Caught exception during transaction, aborting.\"); session.abortTransaction(); throw error; }\n\ncommitWithRetry(session); }\n\nWhat MongoDB Does Not Guarantee There are a couple of situations where MongoDB cannot guarantee durability, such as if there are hardware issues or filesystem bugs. In particular, if a hard disk is corrupt, there is nothing MongoDB can do to protect your data.\n\nAlso, different varieties of hardware and software may have different durability guar‐ antees. For example, some cheaper or older hard disks report a write’s success while the write is queued up to be written, not when it has actually been written. MongoDB cannot defend against misreporting at this level: if the system crashes, data may be lost.\n\nBasically, MongoDB is only as safe as the underlying system: if the hardware or file‐ system destroys the data, MongoDB cannot prevent it. Use replication to defend against system issues. If one machine fails, hopefully another will still be functioning correctly.\n\nChecking for Corruption The validate command can be used to check a collection for corruption. To run validate on the movies collection, do:\n\n410\n\n|\n\nChapter 20: Durability\n\ndb.movies.validate({full: true}) {\n\n\"ns\" : \"sample_mflix.movies\", \"nInvalidDocuments\" : NumberLong(0), \"nrecords\" : 45993, \"nIndexes\" : 5, \"keysPerIndex\" : {\n\n\"_id_\" : 45993, \"$**_text\" : 3671341, \"genres_1_imdb.rating_1_metacritic_1\" : 94880, \"tomatoes_rating\" : 45993, \"getMovies\" : 45993\n\n}, \"indexDetails\" : {\n\n\"$**_text\" : {\n\n\"valid\" : true\n\n}, \"_id_\" : {\n\n\"valid\" : true\n\n}, \"genres_1_imdb.rating_1_metacritic_1\" : {\n\n\"valid\" : true\n\n}, \"getMovies\" : {\n\n\"valid\" : true\n\n}, \"tomatoes_rating\" : { \"valid\" : true\n\n}\n\n}, \"valid\" : true, \"warnings\" : [ ], \"errors\" : [ ], \"extraIndexEntries\" : [ ], \"missingIndexEntries\" : [ ], \"ok\" : 1\n\n}\n\nThe main field you’re looking for is \"valid\", which will hopefully be true. If it is not, validate will give some details about the corruption it found.\n\nMost of the output from validate describes internal structures of the collection and timestamps used to understand the order of operations across a cluster. These are not particularly useful for debugging. (See Appendix B for more information on collec‐ tion internals.)\n\nYou can only run validate on collections, and it will also check the associated indexes in the field indexDetails. However, this requires a full validate, which is configured with the { full: true } option.\n\nChecking for Corruption\n\n|\n\n411\n\nPART VI Server Administration\n\nCHAPTER 21 Setting Up MongoDB in Production\n\nIn Chapter 2, we covered the basics of starting MongoDB. This chapter will go into more detail about which options are important for setting up MongoDB in produc‐ tion, including:\n\nCommonly used options\n\nStarting up and shutting down MongoDB\n\nSecurity-related options\n\nLogging considerations\n\nStarting from the Command Line The MongoDB server is started with the mongod executable. mongod has many config‐ urable startup options; to view all of them, run mongod --help from the command line. A couple of the options are widely used and important to be aware of:\n\n--dbpath\n\nSpecify an alternate directory to use as the data directory; the default is /data/db/ (or, on Windows, \\data\\db\\ on the MongoDB binary’s volume). Each mongod process on a machine needs its own data directory, so if you are running three instances of mongod on one machine, you’ll need three separate data directories. When mongod starts up, it creates a mongod.lock file in its data directory, which prevents any other mongod process from using that directory. If you attempt to start another MongoDB server using the same data directory, it will give an error:\n\nexception in initAndListen: DBPathInUse: Unable to lock the lock file: \\ data/db/mongod.lock (Resource temporarily unavailable). Another mongod instance is already running on the\n\n415\n\ndata/db directory, \\ terminating\n\n--port\n\nSpecify the port number for the server to listen on. By default, mongod uses port 27017, which is unlikely to be used by another process (besides other mongod pro‐ cesses). If you would like to run more than one mongod process on a single machine, you’ll need to specify different ports for each one. If you try to start mongod on a port that is already being used, it will give an error:\n\nFailed to set up listener: SocketException: Address already in use.\n\n--fork\n\nOn Unix-based systems, fork the server process, running MongoDB as a daemon.\n\nIf you are starting up mongod for the first time (with an empty data directory), it can take the filesystem a few minutes to allocate database files. The parent pro‐ cess will not return from forking until the preallocation is done and mongod is ready to start accepting connections. Thus, fork may appear to hang. You can tail the log to see what it is doing. You must use --logpath if you specify --fork.\n\n--logpath\n\nSend all output to the specified file rather than outputting on the command line. This will create the file if it does not exist, assuming you have write permissions to the directory. It will also overwrite the log file if it already exists, erasing any older log entries. If you’d like to keep old logs around, use the --logappend option in addition to --logpath (highly recommended).\n\n--directoryperdb\n\nPut each database in its own directory. This allows you to mount different data‐ bases on different disks, if necessary or desired. Common uses for this are putting a local database on its own disk (replication) or moving a database to a different disk if the original one fills up. You could also put databases that handle more load on faster disks and databases with a lower load on slower disks. This basically gives you more flexibility to move things around later.\n\n--config\n\nUse a configuration file for additional options not specified on the command line. This is typically used to make sure options are the same between restarts. See “File-Based Configuration” on page 419 for details.\n\nFor example, to start the server as a daemon listening on port 5586 and sending all output to mongodb.log, we could run this:\n\n$ ./mongod --dbpath data/db --port 5586 --fork --logpath mongodb.log --logappend 2019-09-06T22:52:25.376-0500 I CONTROL [main]\n\n416\n\n|\n\nChapter 21: Setting Up MongoDB in Production\n\nAutomatically disabling TLS 1.0, \\ to force-enable TLS 1.0 specify --sslDisabledProtocols 'none' about to fork child process, waiting until server is ready for connections. forked process: 27610 child process started successfully, parent exiting\n\nWhen you first install and start MongoDB, it is a good idea to look at the log. This might be an easy thing to miss, especially if MongoDB is being started from an init script, but the log often contains important warnings that prevent later errors from occurring. If you don’t see any warnings in the MongoDB log on startup, then you are all set. (Startup warnings will also appear on shell startup.)\n\nIf there are any warnings in the startup banner, take note of them. MongoDB will warn you about a variety of issues: that you’re running on a 32-bit machine (which MongoDB is not designed for), that you have NUMA enabled (which can slow your application to a crawl), or that your system does not allow enough open file descrip‐ tors (MongoDB uses a lot of file descriptors).\n\nThe log preamble won’t change when you restart the database, so feel free to run MongoDB from an init script and ignore the logs, once you know what they say. However, it’s a good idea to check again each time you do an install, upgrade, or recover from a crash, just to make sure MongoDB and your system are on the same page.\n\nWhen you start the database, MongoDB will write a document to the local.startup_log collection that describes the version of MongoDB, underlying system, and flags used. We can look at this document using the mongo shell:\n\n> use local switched to db local > db.startup_log.find().sort({startTime: -1}).limit(1).pretty() { \"_id\" : \"server1-1544192927184\", \"hostname\" : \"server1.example.net\", \"startTime\" : ISODate(\"2019-09-06T22:50:47Z\"), \"startTimeLocal\" : \"Fri Sep 6 22:57:47.184\", \"cmdLine\" : { \"net\" : { \"port\" : 5586 }, \"processManagement\" : { \"fork\" : true }, \"storage\" : { \"dbPath\" : \"data/db\" }, \"systemLog\" : { \"destination\" : \"file\", \"logAppend\" : true, \"path\" : \"mongodb.log\" }\n\nStarting from the Command Line\n\n|\n\n417\n\n418\n\n}, \"pid\" : NumberLong(27278), \"buildinfo\" : { \"version\" : \"4.2.0\", \"gitVersion\" : \"a4b751dcf51dd249c5865812b390cfd1c0129c30\", \"modules\" : [ \"enterprise\" ], \"allocator\" : \"system\", \"javascriptEngine\" : \"mozjs\", \"sysInfo\" : \"deprecated\", \"versionArray\" : [ 4, 2, 0, 0 ], \"openssl\" : { \"running\" : \"Apple Secure Transport\" }, \"buildEnvironment\" : { \"distmod\" : \"\", \"distarch\" : \"x86_64\", \"cc\" : \"gcc: Apple LLVM version 8.1.0 (clang-802.0.42)\", \"ccflags\" : \"-mmacosx-version-min=10.10 -fno-omit\\ -frame-pointer -fno-strict-aliasing \\ -ggdb -pthread -Wall -Wsign-compare -Wno-unknown-pragmas \\ -Winvalid-pch -Werror -O2 -Wno-unused\\ -local-typedefs -Wno-unused-function -Wno-unused-private-field \\ -Wno-deprecated-declarations \\ -Wno-tautological-constant-out-of\\ -range-compare -Wno-unused-const-variable -Wno\\ -missing-braces -Wno-inconsistent\\ -missing-override -Wno-potentially-evaluated-expression \\ -Wno-exceptions -fstack-protector\\ -strong -fno-builtin-memcmp\", \"cxx\" : \"g++: Apple LLVM version 8.1.0 (clang-802.0.42)\", \"cxxflags\" : \"-Woverloaded-virtual -Werror=unused-result \\ -Wpessimizing-move -Wredundant-move \\ -Wno-undefined-var-template -stdlib=libc++ \\ -std=c++14\", \"linkflags\" : \"-mmacosx-version-min=10.10 -Wl, \\ -bind_at_load -Wl,-fatal_warnings \\ -fstack-protector-strong \\ -stdlib=libc++\", \"target_arch\" : \"x86_64\", \"target_os\" : \"macOS\" },\n\n|\n\nChapter 21: Setting Up MongoDB in Production\n\n\"bits\" : 64, \"debug\" : false, \"maxBsonObjectSize\" : 16777216, \"storageEngines\" : [ \"biggie\", \"devnull\", \"ephemeralForTest\", \"inMemory\", \"queryable_wt\", \"wiredTiger\" ] } }\n\nThis collection can be useful for tracking upgrades and changes in behavior.\n\nFile-Based Configuration MongoDB supports reading configuration information from a file. This can be useful if you have a large set of options you want to use or are automating the task of starting up MongoDB. To tell the server to get options from a configuration file, use the -f or --config flags. For example, run mongod --config ~/.mongodb.conf to use ~/.mongodb.conf as a configuration file.\n\nThe options supported in a configuration file are the same as those accepted at the command line. However, the format is different. As of MongoDB 2.6, MongoDB con‐ figuration files use the YAML format. Here’s an example configuration file:\n\nsystemLog: destination: file path: \"mongod.log\" logAppend: true storage: dbPath: data/db processManagement: fork: true net: port: 5586 ...\n\nThis configuration file specifies the same options we used earlier when starting with regular command-line arguments. Note that these same options are reflected in the startup_log collection document we looked at in the previous section. The only real difference is that the options are specified using JSON rather than YAML.\n\nIn MongoDB 4.2, expansion directives were added to allow the loading of specific configuration file options or loading of the entire configuration file. The advantage of expansion directives is that confidential information, such as passwords and security certificates, does not have to be stored in the config file directly. The --configExpand command-line option enables this feature and must include the expansion directives\n\nStarting from the Command Line\n\n|\n\n419",
      "page_number": 411
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 420-427)",
      "start_page": 420,
      "end_page": 427,
      "detection_method": "topic_boundary",
      "content": "you wish to enable. __rest and __exec are the current implementation of the expan‐ sion directives in MongoDB. The __rest expansion directive loads specific configu‐ ration file values or loads the entire configuration file from a REST endpoint. The __exec expansion directive loads specific configuration file values or loads the entire configuration file from a shell or terminal command.\n\nStopping MongoDB Being able to safely stop a running MongoDB server is at least as important as being able to start one. There are a couple of different options for doing this effectively.\n\nThe cleanest way to shut down a running server is to use the shutdown command, {\"shutdown\" : 1}. This is an admin command and must be run on the admin data‐ base. The shell features a helper function to make this easier:\n\n> use admin switched to db admin > db.shutdownServer() server should be down...\n\nWhen run on a primary, the shutdown command steps down the primary and waits for a secondary to catch up before shutting down the server. This minimizes the chance of rollback, but the shutdown isn’t guaranteed to succeed. If there is no secon‐ dary available that can catch up within a few seconds, the shutdown command will fail and the (former) primary will not shut down:\n\n> db.shutdownServer() { \"closest\" : NumberLong(1349465327), \"difference\" : NumberLong(20), \"errmsg\" : \"no secondaries within 10 seconds of my optime\", \"ok\" : 0 }\n\nYou can force the shutdown command to shut down a primary by using the force option:\n\ndb.adminCommand({\"shutdown\" : 1, \"force\" : true})\n\nThis is equivalent to sending a SIGINT or SIGTERM signal (all three of these options result in a clean shutdown, but there may be unreplicated data). If the server is run‐ ning as the foreground process in a terminal, a SIGINT can be sent by pressing Ctrl- C. Otherwise, a command like kill can be used to send the signal. If mongod had 10014 as its PID, the command would be kill -2 10014 (SIGINT) or kill 10014 (SIGTERM).\n\n420\n\n|\n\nChapter 21: Setting Up MongoDB in Production\n\nWhen mongod receives a SIGINT or SIGTERM, it will do a clean shutdown. This means it will wait for any running operations or file preallocations to finish (this could take a moment), close all open connections, flush all data to disk, and halt.\n\nSecurity Do not set up publicly addressable MongoDB servers. You should restrict access as tightly as possible between the outside world and MongoDB. The best way to do this is to set up firewalls and only allow MongoDB to be reachable on internal network addresses. Chapter 24 covers what connections it’s necessary to allow between MongoDB servers and clients.\n\nBeyond firewalls, there are a few options you can add to your config file to make it more secure:\n\n--bind_ip\n\nSpecify the interfaces that you want MongoDB to listen on. Generally you want this to be an internal IP: something application servers and other members of your cluster can access but that is inaccessible to the outside world. localhost is fine for mongos processes if you’re running the application server on the same machine. For config servers and shards, they’ll need to be addressable from other machines, so stick with non-localhost addresses.\n\nStarting in MongoDB 3.6, mongod and mongos processes bind to localhost by default. When bound only to localhost, mongod and mongos will only accept con‐ nections from clients running on the same machine. This helps limit the expo‐ sure of unsecured MongoDB instances. To bind to other addresses, use the net.bindIp configuration file setting or the --bind_ip command-line option to specify a list of hostnames or IP addresses.\n\n--nounixsocket\n\nDisable listening on the UNIX domain socket. If you’re not planning to connect via filesystem socket, you might as well disallow it. You would only connect via filesystem socket on a machine that is also running an application server: you must be local to use a filesystem socket.\n\n--noscripting\n\nDisable server-side JavaScript execution. Some security issues that have been reported with MongoDB have been JavaScript-related, so it’s generally safer to disallow it, if your application allows.\n\nSecurity\n\n|\n\n421\n\nSeveral shell helpers assume that JavaScript is available on the server, notably sh.status(). You will see errors if you attempt to run any of these helpers with JavaScript disabled.\n\nData Encryption Data encryption is available in MongoDB Enterprise. These options are not sup‐ ported in the Community version of MongoDB.\n\nThe data encryption process includes the following steps:\n\nGenerate a master key.\n\nGenerate keys for each database.\n\nEncrypt data with the database keys.\n\nEncrypt the database keys with the master key.\n\nWhen using data encryption, all data files are encrypted in the filesystem. Data is only unencrypted in memory and during transmission. To encrypt all of MongoDB’s net‐ work traffic, you can use TLS/SSL. The data encryption options that MongoDB Enterprise users can add to their config files are:\n\n--enableEncryption\n\nEnables encryption in the WiredTiger storage engine. With this option, data stored in memory and on disk will be encrypted. This is sometimes referred to as “encryption at rest.” You must set this to true in order to pass in encryption keys and to configure encryption. This option is false by default.\n\n--encryptionCipherMode\n\nSet the cipher mode for encryption at rest in WiredTiger. There are two modes available: AES256-CBC and AES256-GCM. AES256-CBC is an acronym for 256- bit Advanced Encryption Standard in Cipher Block Chaining Mode. AES256- GCM uses Galois/Counter Mode. Both are standard encryption ciphers. As of MongoDB 4.0, MongoDB Enterprise on Windows no longer supports AES256- GCM.\n\n--encryptionKeyFile\n\nSpecify the path to the local keyfile if you are managing keys using a process other than the Key Management Interoperability Protocol (KMIP).\n\nMongoDB Enterprise also supports key management using KMIP. A discussion of KMIP is beyond the scope of this book. Please see the MongoDB documentation for details on using KMIP with MongoDB.\n\n422\n\n|\n\nChapter 21: Setting Up MongoDB in Production\n\nSSL Connections As we saw in Chapter 18, MongoDB supports transport encryption using TLS/SSL. This feature is available in all editions of MongoDB. By default, connections to Mon‐ goDB transfer data unencrypted. However, TLS/SSL ensures transport encryption. MongoDB uses native TSL/SSL libraries available on your operating system. Use the option --tlsMode and related options to configure TLS/SSL. Refer to Chapter 18 for more detail, and consult your driver’s documentation on how to create TLS/SSL con‐ nections using your language.\n\nLogging By default, mongod sends its logs to stdout. Most init scripts use the --logpath option to send logs to a file. If you have multiple MongoDB instances on a single machine (say, a mongod and a mongos), make sure that their logs are stored in sepa‐ rate files. Be sure that you know where the logs are and have read access to the files.\n\nMongoDB spits out a lot of log messages, but please do not run with the --quiet option (which suppresses some of them). Leaving the log level at the default is usually perfect: there is enough information for basic debugging (why is this slow, why isn’t this starting up, etc.), but the logs do not take up too much space.\n\nIf you are debugging a specific issue with your application, there are a couple of options for getting more information from the logs. You can change the log level by running the setParameter command, or by setting the log level at startup time by passing it as a string using the --setParameter option.\n\n> db.adminCommand({\"setParameter\" : 1, \"logLevel\" : 3})\n\nYou can also change the log level for a particular component. This is helpful if you are debugging a specific aspect of your application and require more information, but only from that component. In this example, we set the default log verbosity to 1 and the query component verbosity to 2:\n\n> db.adminCommand({\"setParameter\" : 1, logComponentVerbosity: { verbosity: 1, query: { verbosity: 2 }}})\n\nRemember to turn the log level back down to 0 when you’re done debugging, or your logs may be needlessly noisy. You can turn the level all the way up to 5, at which point mongod will print out almost every action it takes, including the contents of every request handled. This can cause a lot of I/O as mongod writes everything to the log file, which can slow down a busy system. Turning on profiling is a better option if you need to see every operation as it’s happening.\n\nBy default, MongoDB logs information about queries that take longer than 100 ms to run. If 100 ms is too short or too long for your application, you can change the thres‐ hold with setProfilingLevel:\n\nLogging\n\n|\n\n423\n\n> // Only log queries that take longer than 500 ms > db.setProfilingLevel(1, 500) { \"was\" : 0, \"slowms\" : 100, \"ok\" : 1 } > db.setProfilingLevel(0) { \"was\" : 1, \"slowms\" : 500, \"ok\" : 1 }\n\nThe second line will turn off profiling, but the value in milliseconds given in the first line will continue to be used as a threshold for the log (across all databases). You can also set this parameter by restarting MongoDB with the --slowms option.\n\nFinally, set up a cron job that rotates your log every day or week. If MongoDB was started with --logpath, sending the process a SIGUSR1 signal will make it rotate the log. There is also a logRotate command that does the same thing:\n\n> db.adminCommand({\"logRotate\" : 1})\n\nYou cannot rotate logs if MongoDB was not started with --logpath.\n\n424\n\n|\n\nChapter 21: Setting Up MongoDB in Production\n\nCHAPTER 22 Monitoring MongoDB\n\nBefore you deploy, it is important to set up some type of monitoring. Monitoring should allow you to track what your server is doing and alert you if something goes wrong. This chapter will cover:\n\nHow to track MongoDB’s memory usage\n\nHow to track application performance metrics\n\nHow to diagnose replication issues\n\nWe’ll use example graphs from MongoDB Ops Manager to demonstrate what to look for when monitoring (see installation instructions for Ops Manager). The monitoring capabilities of MongoDB Atlas (MongoDB’s cloud database service) are very similar. MongoDB also offers a free monitoring service that monitors standalones and replica sets. It keeps the monitoring data for 24 hours after it has been uploaded and pro‐ vides coarse-grained statistics on operation execution times, memory usage, CPU usage, and operation counts.\n\nIf you do not want to use Ops Manager, Atlas, or MongoDB’s free monitoring service, please use some type of monitoring. It will help you detect potential issues before they cause problems and diagnose issues when they occur.\n\nMonitoring Memory Usage Accessing data in memory is fast, and accessing data on disk is slow. Unfortunately, memory is expensive (and disk is cheap), and typically MongoDB uses up memory before any other resource. This section covers how to monitor MongoDB’s interac‐ tions with the CPU, disk, and memory, and what to watch for.\n\n425\n\nIntroduction to Computer Memory Computers tend to have a small amount of fast-to-access memory and a large amount of slow-to-access disk. When you request a page of data that is stored on disk (and not yet in memory), your system page faults and copies the page from disk into mem‐ ory. It can then access the page in memory extremely quickly. If your program stops regularly using the page and your memory fills up with other pages, the old page will be evicted from memory and only live on disk again.\n\nCopying a page from disk into memory takes a lot longer than reading a page from memory. Thus, the less MongoDB has to copy data from disk, the better. If MongoDB can operate almost entirely in memory, it will be able to access data much faster. Thus, MongoDB’s memory usage is one of the most important stats to track.\n\nTracking Memory Usage MongoDB reports on three “types” of memory in Ops Manager: resident memory, virtual memory, and mapped memory. Resident memory is the memory that Mon‐ goDB explicitly owns in RAM. For example, if you query for a document and it is paged into memory, that page is added to MongoDB’s resident memory.\n\nMongoDB is given an address for that page. This address isn’t the literal address of the page in RAM; it’s a virtual address. MongoDB can pass it to the kernel and the kernel will look up where the page really lives. This way, if the kernel needs to evict the page from memory, MongoDB can still use the address to access it. MongoDB will request the memory from the kernel, the kernel will look at its page cache, see that the page is not there, page fault to copy the page into memory, and return it to MongoDB.\n\nIf your data fits entirely in memory, the resident memory should be approximately the size of your data. When we talk about data being “in memory,” we’re always talk‐ ing about the data being in RAM.\n\nMongoDB’s mapped memory includes all of the data MongoDB has ever accessed (all the pages of data it has addresses for). It will usually be about the size of your dataset.\n\nVirtual memory is an abstraction provided by the operating system that hides the physical storage details from the software process. Each process sees a contiguous address space of memory that it can use. In Ops Manager, the virtual memory use of MongoDB is typically twice the size of the mapped memory.\n\nFigure 22-1 shows the Ops Manager graph for memory information, which describes how much virtual, resident, and mapped memory MongoDB is using. Mapped mem‐ ory is relevant only for older (pre-4.0) deployments using the MMAP storage engine. Now that MongoDB uses the WiredTiger storage engine, you should see zero usage for mapped memory. On a machine dedicated to MongoDB, resident memory should\n\n426\n\n|\n\nChapter 22: Monitoring MongoDB\n\nbe a little less than the total memory size (assuming your working set is as large or larger than memory). Resident memory is the statistic that actually tracks how much data is in physical RAM, but by itself this does not tell you much about how MongoDB is using memory.\n\nFigure 22-1. From the top line to the bottom: virtual, resident, and mapped memory\n\nIf your data fits entirely in memory, resident should be approximately the size of your data. When we talk about data being “in memory,” we’re always talking about the data being in RAM.\n\nAs you can see from Figure 22-1, memory metrics tend to be fairly steady, but as your dataset grows virtual memory (top line) will grow with it. Resident memory (middle line) will grow to the size of your available RAM and then hold steady.\n\nTracking Page Faults You can use other statistics to find out how MongoDB is using memory, not just how much of each type it has. One useful stat is the number of page faults, which tells you how often the data MongoDB is looking for is not in RAM. Figures 22-2 and 22-3 are graphs that show page faults over time. Figure 22-3 is page faulting less than Figure 22-2, but by itself this information is not very useful. If the disk in Figure 22-2 can handle that many faults and the application can handle the delay of the disk seeks, there is no particular problem with having so many faults (or more). On the other hand, if your application cannot handle the increased latency of reading data from disk, you have no choice but to store all of your data in memory (or use SSDs).\n\nMonitoring Memory Usage\n\n|\n\n427",
      "page_number": 420
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 428-439)",
      "start_page": 428,
      "end_page": 439,
      "detection_method": "topic_boundary",
      "content": "Figure 22-2. A system that is page faulting hundreds of times a minute\n\nFigure 22-3. A system that is page faulting a few times a minute\n\nRegardless of how forgiving the application is, page faults become a problem when the disk is overloaded. The amount of load a disk can handle isn’t linear: once a disk begins getting overloaded, each operation must queue for a longer and longer period of time, creating a chain reaction. There is usually a tipping point where disk perfor‐ mance begins degrading quickly. Thus, it is a good idea to stay away from the maxi‐ mum load that your disk can handle.\n\nTrack your page fault numbers over time. If your application is behaving well with a certain number of page faults, you have a baseline for how many page faults the system can handle. If page faults begin to creep up and performance deteriorates, you have a threshold to alert on.\n\nYou can see page fault stats per database by looking at the \"page_faults\" field of serverStatus’s output:\n\n> db.adminCommand({\"serverStatus\": 1})[\"extra_info\"] { \"note\" : \"fields vary by platform\", \"page_faults\" : 50 }\n\n428\n\n|\n\nChapter 22: Monitoring MongoDB\n\n\"page_faults\" gives you a count of how many times MongoDB has had to go to disk (since startup).\n\nI/O Wait Page faults in general are closely tied to how long the CPU is idling waiting for the disk, called I/O wait. Some I/O wait is normal; MongoDB has to go to disk some‐ times, and although it tries not to block anything when it does, it cannot completely avoid it. The important thing is that I/O wait is not increasing or near 100%, as shown in Figure 22-4. This indicates that the disk is getting overloaded.\n\nFigure 22-4. I/O wait hovering around 100%\n\nCalculating the Working Set In general, the more data you have in memory, the faster MongoDB will perform. Thus, in order from fastest to slowest, an application could have:\n\n1. The entire dataset in memory. This is nice to have but is often too expensive or infeasible. It may be necessary for applications that depend on fast response times.\n\n2. The working set in memory. This is the most common choice.\n\nYour working set is the data and indexes that your application uses. This may be everything, but generally there’s a core dataset (e.g., the users collection and the last month of activity) that covers 90% of requests. If this working set fits in RAM, MongoDB will generally be fast: it only has to go to disk for a few “unusual” requests.\n\n3. The indexes in memory.\n\n4. The working set of indexes in memory.\n\n5. No useful subset of data in memory. If possible, avoid this. It will be slow.\n\nCalculating the Working Set\n\n|\n\n429\n\nYou must know what your working set is (and how large it is) to know if you can keep it in memory. The best way to calculate the size of the working set is to track common operations to find out how much your application is reading and writing. For exam‐ ple, suppose your application creates 2 GB of new data per week and 800 MB of that data is regularly accessed. Users tend to access data up to a month old, and data that’s older than that is mostly unused. Your working set size is probably about 3.2 GB (800 MB/week × 4 weeks), plus a fudge factor for indexes, so call it 5 GB.\n\nOne way to think about this is to track data accessed over time, as shown in Figure 22-5. If you choose a cutoff where 90% of your requests fall, like in Figure 22-6, then the data (and indexes) generated in that period of time form your working set. You can measure for that amount of time to figure out how much your dataset grows. Note that this example uses time, but it’s possible that there’s another access pattern that makes more sense for your application (time being the most com‐ mon one).\n\nFigure 22-5. A plot of data accesses by age of data\n\nFigure 22-6. The working set is data used in the requests before the cutoff of “frequent requests” (indicated by the vertical line in the graph)\n\n430\n\n|\n\nChapter 22: Monitoring MongoDB\n\nSome Working Set Examples Suppose that you have a 40 GB working set. A total of 90% of requests hit the work‐ ing set, and 10% hit other data. If you have 500 GB of data and 50 GB of RAM, your working set fits entirely in RAM. Once your application has accessed the data it usu‐ ally accesses (a process called preheating), it should never have to go to disk again for the working set. It then has 10 GB of space available for the 460 GB of less-frequently- accessed data. Obviously, MongoDB will almost always have to go to disk for the non‐ working set data.\n\nOn the other hand, suppose your working set does not fit in RAM—say, if you have only 35 GB of RAM. Then the working set will generally take up most of the RAM. The working set has a higher probability of staying in RAM because it’s accessed more frequently, but at some point the less-frequently-accessed data will have to be paged in, evicting the working set (or other less-frequently-accessed data). Thus, there is a constant churn back and forth from disk: accessing the working set does not have predictable performance anymore.\n\nTracking Performance Performance of queries is often important to track and keep consistent. There are several ways to track if MongoDB is having trouble with the current request load.\n\nCPU can be I/O bound with MongoDB (indicated by a high I/O wait). The Wire‐ dTiger storage engine is multithreaded and can take advantage of additional CPU cores. This can be seen in a higher level of usage across CPU metrics when compared with the older MMAP storage engine. However, if user or system time is approaching 100% (or 100% multiplied by the number of CPUs you have), the most common cause is that you’re missing an index on a frequently used query. It is a good idea to track CPU usage (particularly after deploying a new version of your application) to ensure that all your queries are behaving as they should.\n\nNote that the graph shown in Figure 22-7 is fine: if there is a low number of page faults, I/O wait may be dwarfed by other CPU activities. It is only when the other activities creep up that bad indexes may be a culprit.\n\nTracking Performance\n\n|\n\n431\n\nFigure 22-7. A CPU with minimal I/O wait: the top line is user and the lower line is system; the other stats are very close to 0%\n\nA similar metric is queuing: how many requests are waiting to be processed by Mon‐ goDB. A request is considered queued when it is waiting for the lock it needs to do a read or a write. Figure 22-8 shows a graph of read and write queues over time. No queues are preferred (basically an empty graph), but this graph is nothing to be alarmed about. In a busy system, it isn’t unusual for an operation to have to wait a bit for the correct lock to be available.\n\nFigure 22-8. Read and write queues over time\n\nThe WiredTiger storage engine provides document-level concurrency, which allows for multiple simultaneous writes to the same collection. This has drastically improved the performance of concurrent operations. The ticketing system used controls the number of threads in use to avoid starvation: it issues tickets for read and write oper‐ ations (128 of each, by default), after which point new read or write operations will queue. The wiredTiger.concurrentTransactions.read.available and wired Tiger.concurrentTransactions.write.available fields of serverStatus can be used to track when the number of available tickets reaches zero, indicating the respec‐ tive operations are now queuing up.\n\n432\n\n|\n\nChapter 22: Monitoring MongoDB\n\nYou can see if requests are piling up by looking at the number of requests enqueued. Generally, the queue size should be low. A large and ever-present queue is an indica‐ tion that mongod cannot keep up with its load. You should decrease the load on that server as fast as possible.\n\nTracking Free Space One other metric that is basic but important to monitor is disk usage. Sometimes users wait until their disk runs out of space before they think about how they want to handle it. By monitoring your disk usage and tracking free disk space, you can pre‐ dict how long your current drive will be sufficient and plan in advance what to do when it is not.\n\nAs you run out of space, there are several options:\n\nIf you are using sharding, add another shard.\n\nIf you have unused indexes, remove them. These can be identified using the aggregation $indexStats for a specific collection.\n\nIf you have not run a compaction operation, then do so on a secondary to see if it assists. This is normally only useful in cases where a large amount of data or indexes have been removed from a collection and will not be replaced.\n\nShut down each member of the replica set (one at a time) and copy its data to a larger disk, which can then be mounted. Restart the member and proceed to the next.\n\nReplace members of your replica set with members with a larger drive: remove an old member and add a new member, and allow that one to catch up with the rest of the set. Repeat for each member of the set.\n\nIf you are using the directoryperdb option and you have a particularly fast- growing database, move it to its own drive. Then mount the volume as a direc‐ tory in your data directory. This way the rest of your data doesn’t have to be moved.\n\nRegardless of the technique you choose, plan ahead to minimize the impact on your application. You need time to take backups, modify each member of your set in turn, and copy your data from place to place.\n\nMonitoring Replication Replication lag and oplog length are important metrics to track. Lag is when the sec‐ ondaries cannot keep up with the primary. It’s calculated by subtracting the time of the last op applied on a secondary from the time of the last op on the primary. For example, if a secondary just applied an op with the timestamp 3:26:00 p.m. and the\n\nTracking Free Space\n\n|\n\n433\n\nprimary just applied an op with the timestamp 3:29:45 p.m., the secondary is lagging by 3 minutes and 45 seconds. You want lag to be as close to 0 as possible, and it is generally on the order of milliseconds. If a secondary is keeping up with the primary, the replication lag should look something like the graph shown in Figure 22-9: basi‐ cally 0 all the time.\n\nFigure 22-9. A replica set with no lag; this is what you want to see\n\nIf a secondary cannot replicate writes as fast as the primary can write, you’ll start see‐ ing a nonzero lag. The most extreme case of this is when replication is stuck: the sec‐ ondary cannot apply any more operations for some reason. At this point, lag will grow by one second per second, creating the steep slope shown in Figure 22-10. This could be caused by network issues or a missing \"_id\" index, which is required on every collection for replication to function properly.\n\nIf a collection is missing an \"_id\" index, take the server out of the replica set, start it as a standalone server, and build the \"_id\" index. Make sure you create the \"_id\" index as a unique index. Once created, the \"_id\" index cannot be dropped or changed (other than by dropping the whole collection).\n\nIf a system is overloaded, a secondary may gradually fall behind. Some replication will still be happening, so you generally won’t see the characteristic “one second per second” slope in the graph. Still, it’s important to be aware if the secondaries cannot keep up with peak traffic or are gradually falling further behind.\n\n434\n\n|\n\nChapter 22: Monitoring MongoDB\n\nFigure 22-10. Replication getting stuck and, just before February 10, beginning to recover; the vertical lines are server restarts\n\nPrimaries do not throttle writes to “help” secondaries catch up, so it’s common for secondaries to fall behind on overloaded systems (particularly as MongoDB tends to prioritize writes over reads, which means replication can be starved on the primary). You can force throttling of the primary to some extent by using \"w\" with your write concern. You also might want to try removing load from the secondary by routing any requests it was handling to another member.\n\nIf you are on an extremely underloaded system, you may see another interesting pat‐ tern: sudden spikes in replication lag, as shown in Figure 22-11. The spikes shown are not actually lag—they are caused by variations in sampling. The mongod is processing one write every couple of minutes. Because lag is measured as the difference between timestamps on the primary and secondary, measuring the timestamp of the secon‐ dary right before a write on the primary makes it look minutes behind. If you increase the write rate, these spikes should disappear.\n\nFigure 22-11. A low-write system can cause “phantom” lag\n\nThe other important replication metric to track is the length of each member’s oplog. Every member that might become primary should have an oplog longer than a day. If\n\nMonitoring Replication\n\n|\n\n435\n\na member may be a sync source for another member, it should have an oplog longer than the time an initial sync takes to complete. Figure 22-12 shows what a standard oplog-length graph looks like. This oplog has an excellent length: 1,111 hours is over a month of data! In general, oplogs should be as long as you can afford the disk space to make them. Given the way they’re used, they take up basically no memory, and a long oplog can mean the difference between a painful ops experience and an easy one.\n\nFigure 22-12. A typical oplog-length graph\n\nFigure 22-13 shows a slightly unusual variation caused by a fairly short oplog and variable traffic. This is still healthy, but the oplog on this machine is probably too short (between 6 and 11 hours of maintenance). The administrator may want to make the oplog longer when they get a chance.\n\nFigure 22-13. Oplog-length graph of an application with daily traffic peaks\n\n436\n\n|\n\nChapter 22: Monitoring MongoDB\n\nCHAPTER 23 Making Backups\n\nIt is important to make regular backups of your system. Backups are good protection against most types of failure, and very little can’t be solved by restoring from a clean backup. This chapter covers the common options for making backups:\n\nSingle-server backups, including snapshot backup and restore procedure\n\nSpecial considerations for backing up replica sets\n\nBaking up a sharded cluster\n\nBackups are only useful if you are confident about deploying them in an emergency. Thus, for any backup technique you choose, be sure to practice both making backups and restoring from them until you are comfortable with the restore procedure.\n\nBackup Methods There are a number of options for backing up clusters in MongoDB. MongoDB Atlas, the official MongoDB cloud service, provides both continuous backups and cloud provider snapshots. Continuous backups take incremental backups of data in your cluster, ensuring your backups are typically just a few seconds behind the operating system. Cloud provider snapshots provide localized backup storage using the snap‐ shot functionality of the cluster’s cloud service provider (e.g., Amazon Web Services, Microsoft Azure, or Google Cloud Platform). The best backup solution for the major‐ ity of scenarios is continuous backups.\n\nMongoDB also provides backup capability through Cloud Manager and Ops Man‐ ager. Cloud Manager is a hosted backup, monitoring, and automation service for MongoDB. Ops Manager is an on-premise solution that has similar functionality to Cloud Manager.\n\n437\n\nFor individuals and teams managing MongoDB clusters directly, there are several backup strategies. We will outline these strategies in the rest of this chapter.\n\nBacking Up a Server There are a variety of ways to create backups. Regardless of the method, making a backup can cause strain on a system: it generally requires reading all your data into memory. Thus, backups should generally be done on replica set secondaries (as opposed to the primary) or, for standalone servers, at an off time.\n\nThe techniques in this section apply to any mongod, whether a standalone server or a member of a replica set, unless otherwise noted.\n\nFilesystem Snapshot Filesystem snapshots use system-level tools to create copies of the device that holds MongoDB’s data files. These methods complete quickly and work reliably, but require additional system configuration outside of MongoDB.\n\nMongoDB 3.2 added support for volume-level backup of MongoDB instances using the WiredTiger storage engine when those instances’ data files and journal files reside on separate volumes. However, to create a coherent backup, the database must be locked and all writes to the database must be suspended during the backup process.\n\nPrior to MongoDB 3.2, creating volume-level backups of MongoDB instances using WiredTiger required that the data files and journal reside on the same volume.\n\nSnapshots work by creating pointers between the live data and a special snapshot vol‐ ume. These pointers are theoretically equivalent to “hard links.” As the working data diverges from the snapshot, the snapshot process uses a copy-on-write strategy. As a result, the snapshot only stores modified data.\n\nAfter making the snapshot, you mount the snapshot image on your filesystem and copy data from the snapshot. The resulting backup contains a full copy of all data.\n\nThe database must be valid when the snapshot takes place. This means that all writes accepted by the database need to be fully written to disk: either to the journal or to data files. If there are writes that are not on disk when the backup occurs, the backup will not reflect these changes.\n\nFor the WiredTiger storage engine, the data files reflect a consistent state as of the last checkpoint. Checkpoints occur every minute.\n\nSnapshots create an image of an entire disk or volume. Unless you need to back up your entire system, consider isolating your MongoDB data files, journal (if applica‐ ble), and configuration on one logical disk that doesn’t contain any other data.\n\n438\n\n|\n\nChapter 23: Making Backups\n\nAlternatively, store all MongoDB data files on a dedicated device so that you can make backups without duplicating extraneous data.\n\nEnsure that you copy data from snapshots onto other systems. This ensures that data is safe from site failures.\n\nIf your mongod instance has journaling enabled, then you can use any kind of filesys‐ tem or volume/block-level snapshot tool to create backups.\n\nIf you manage your own infrastructure on a Linux-based system, configure your sys‐ tem using the Linux Logical Volume Manager (LVM) to provide your disk packages and provide snapshot capability. LVM allows for the flexible combination and divi‐ sion of physical disk partitions, enabling dynamically resizable filesystems. You can also use LVM-based setups within a cloud/virtualized environment.\n\nIn the initial setup of LVM, first we assign disk partitions to physical volumes (pvcreate), then one or more of these are then assigned to a volume group (vgcreate), and then we create logical volumes (lvcreate) referring to the volume groups. We can build a filesystem on the logical volume (mkfs), which when created can be mounted for use (mount).\n\nSnapshot backup and restore procedure\n\nThis section provides an overview of a simple backup process using LVM on a Linux system. While the tools, commands, and paths may be (slightly) different on your sys‐ tem, the following steps provide a high-level overview of the backup operation.\n\nOnly use the following procedure as a guideline for a backup system and infrastruc‐ ture. Production backup systems must consider a number of application-specific requirements and factors unique to specific environments.\n\nTo create a snapshot with LVM, issue a command as root in the following format:\n\n# lvcreate --size 100M --snapshot --name mdb-snap01 /dev/vg0/mongodb\n\nThis command creates an LVM snapshot (with the --snapshot option) named mdb- snap01 of the mongodb volume in the vg0 volume group, which will be located at /dev/vg0/mdb-snap01. The location and paths to your systems, volume groups, and devices may vary slightly depending on your operating system’s LVM configuration.\n\nThe snapshot has a cap of 100 MB, because of the parameter --size 100M. This size does not reflect the total amount of the data on the disk, but rather the amount of differences between the current state of /dev/vg0/mongodb and the snapshot (/dev/vg0/mdb-snap01).\n\nBacking Up a Server\n\n|\n\n439",
      "page_number": 428
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 440-450)",
      "start_page": 440,
      "end_page": 450,
      "detection_method": "topic_boundary",
      "content": "The snapshot will exist when the command returns. You can restore directly from the snapshot at any time, or create a new logical volume and restore from the snapshot to the alternate image.\n\nWhile snapshots are great for creating high-quality backups quickly, they are not ideal as a format for storing backup data. Snapshots typically depend and reside on the same storage infrastructure as the original disk images. Therefore, it’s crucial that you archive these snapshots and store them elsewhere.\n\nAfter creating a snapshot, mount the snapshot and copy the data to separate storage. Alternatively, take a block-level copy of the snapshot image, such as with the follow‐ ing procedure:\n\n# umount /dev/vg0/mdb-snap01\n\n# dd if=/dev/vg0/mdb-snap01 | gzip > mdb-snap01.gz\n\nThis command sequence does the following:\n\nEnsures that the /dev/vg0/mdb-snap01 device is not mounted • Performs a block-level copy of the entire snapshot image using the dd command and compresses the result in a gzipped file in the current working directory\n\nThe dd command will create a large .gz file in your current working directory. Make sure that you run this command in a filesystem that has enough free space.\n\nTo restore a snapshot created with LVM, issue the following sequence of commands:\n\n# lvcreate --size 1G --name mdb-new vg0\n\n# gzip -d -c mdb-snap01.gz | dd of=/dev/vg0/mdb-new\n\n# mount /dev/vg0/mdb-new /srv/mongodb\n\nThis sequence does the following:\n\nCreates a new logical volume named mdb-new, in the /dev/vg0 volume group. The path to the new device will be /dev/vg0/mdb-new. You can use a different name, and change 1G to your desired volume size.\n\nUncompresses and unarchives the mdb-snap01.gz file into the mdb-new disk image.\n\n440\n\n|\n\nChapter 23: Making Backups\n\nMounts the mdb-new disk image to the /srv/mongodb directory. Modify the mount point to correspond to your MongoDB data file location or other location as needed.\n\nThe restored snapshot will have a stale mongod.lock file. If you do not remove this file from the snapshot, MongoDB may assume that the stale lock file indicates an unclean shutdown. If you’re running with storage.journal.enabled enabled and you do not use db.fsyncLock(), you do not need to remove the mongod.lock file. If you use db.fsyncLock() you will need to remove the lock.\n\nTo restore a backup without writing to a compressed .gz file, use the following sequence of commands:\n\n# umount /dev/vg0/mdb-snap01\n\n# lvcreate --size 1G --name mdb-new vg0\n\n# dd if=/dev/vg0/mdb-snap01 of=/dev/vg0/mdb-new\n\n# mount /dev/vg0/mdb-new /srv/mongodb\n\nYou can implement off-system backups using the combined process and SSH. This sequence is identical to procedures explained previously, except that it archives and compresses the backup on a remote system using SSH:\n\numount /dev/vg0/mdb-snap01\n\ndd if=/dev/vg0/mdb-snap01 | ssh username@example.com gzip > /opt/backup/ mdb-snap01.gz\n\nlvcreate --size 1G --name mdb-new vg0\n\nssh username@example.com gzip -d -c /opt/backup/mdb-snap01.gz | dd of=/dev/vg0/mdb-new\n\nmount /dev/vg0/mdb-new /srv/mongodb\n\nStarting in MongoDB 3.2, for the purpose of volume-level backup of MongoDB instances using WiredTiger, the data files and the journal are no longer required to reside on a single volume. However, the database must be locked and all writes to the database must be suspended during the backup process to ensure the consistency of the backup.\n\nIf your mongod instance is either running without journaling or has the journal files on a separate volume, you must flush all writes to disk and lock the database to pre‐ vent writes during the backup process. If you have a replica set configuration, then for your backup use a secondary that is not receiving reads (i.e., a hidden member).\n\nBacking Up a Server\n\n|\n\n441\n\nTo do this, issue the db.fsyncLock() method in the mongo shell:\n\n> db.fsyncLock();\n\nThen perform the backup operation described previously.\n\nAfter the snapshot completes, unlock the database by issuing the following command in the mongo shell:\n\n> db.fsyncUnlock();\n\nThis process is described more fully in the following section.\n\nCopying Data Files Another way of creating single-server backups is to make a copy of everything in the data directory. Because you cannot copy all of the files at the same moment without filesystem support, you must prevent the data files from changing while you are mak‐ ing the copy. This can be accomplished with a command called fsyncLock:\n\n> db.fsyncLock()\n\nThis command locks the database against any further writes and then flushes all dirty data to disk (fsync), ensuring that the files in the data directory have the latest con‐ sistent information and are not changing.\n\nOnce this command has been run, mongod will enqueue all incoming writes. It will not process any further writes until it has been unlocked. Note that this command stops writes to all databases (not just the one db is connected to).\n\nOnce the fsyncLock command returns, copy all of the files in your data directory to a backup location. On Linux, this can be done with a command such as:\n\n$ cp -R /data/db/* /mnt/external-drive/backup\n\nMake sure that you copy absolutely every file and folder from the data directory to the backup location. Excluding files or directories may make the backup unusable or corrupt.\n\nOnce you have finished copying the data, unlock the database to allow it to take writes again:\n\n> db.fsyncUnlock()\n\nYour database will begin handling writes again normally.\n\nNote that there are some locking issues with authentication and fsyncLock. If you are using authentication, do not close the shell between calling fsyncLock and fsyncUnlock. If you disconnect, you may be unable to reconnect and have to restart mongod. The fsyncLock setting does not persist between restarts; mongod will always start up unlocked.\n\n442\n\n|\n\nChapter 23: Making Backups\n\nAs an alternative to fsyncLock, you can instead shut down mongod, copy the files, and then start mongod back up again. Shutting down mongod effectively flushes all changes to disk and prevents new writes from occurring during the backup.\n\nTo restore from the copy of the data directory, ensure that mongod is not running and that the data directory you want to restore into is empty. Copy the backed-up data files to the data directory, and then start mongod. For example, the following com‐ mand would restore the files backed up with the command shown earlier:\n\n$ cp -R /mnt/external-drive/backup/* /data/db/ $ mongod -f mongod.conf\n\nDespite the warnings about partial data directory copies, you can use this method to back up individual databases if you know what to copy and where they are using the --directoryperdb option. To back up an individual database (called, say, myDB), which is only available if you are using the --directoryperdb option, copy the entire myDB directory. Partial data directory copies are only possible with the -- directoryperdb option.\n\nYou can restore specific databases by copying just the files with the correct database name into your data directory. You must be starting from a clean shutdown to restore piecemeal like this. If you had a crash or a hard shutdown, do not attempt to restore a single database from the backup: replace the entire directory and start the mongod to allow the journal files to be replayed.\n\nNever use fsyncLock in conjunction with mongodump (described next). Depending on what else your database is doing, mongodump may hang forever if the database is locked.\n\nUsing mongodump The final way of making a single-server backup is to use mongodump. mongodump is mentioned last because it has some downsides. It is slower (both to get the backup and to restore from it) and it has some issues with replica sets, which are discussed in “Specific Considerations for Replica Sets” on page 446. However, it also has some benefits: it is a good way to back up individual databases, collections, and even sub‐ sets of collections.\n\nmongodump has a variety of options that you can see by running mongodump --help. Here, we will focus on the most useful ones to use for backing up.\n\nTo back up all databases, simply run mongodump. If you are running mongodump on the same machine as the mongod, you can simply specify the port mongod is running on:\n\nBacking Up a Server\n\n|\n\n443\n\n$ mongodump -p 31000\n\nmongodump will create a dump directory in the current directory, which contains a dump of all your data. This dump directory is organized by database and by collection into folders and subfolders. The actual data is stored in .bson files, which merely con‐ tain every document in a collection in BSON, concatenated together. You can exam‐ ine .bson files using the bsondump tool, which comes with MongoDB.\n\nYou do not even need to have a server running to use mongodump. You can use the --dbpath option to specify your data directory, and mongodump will use the data files to copy data:\n\n$ mongodump --dbpath /data/db\n\nYou should not use --dbpath if mongod is running.\n\nOne issue with mongodump is that it is not an instantaneous backup: the system may be taking writes while the backup occurs. Thus, you might end up with a situation where user A begins a backup that causes mongodump to dump the database A, but while this is happening user B drops A. However, mongodump has already dumped it, so you’ll end up with a snapshot of the data that is inconsistent with the state on the original server.\n\nTo avoid this, if you are running mongod with --replSet, you can use mongodump’s --oplog option. This will keep track of all operations that occur on the server while the dump is taking place, so these operations can be replayed when the backup is restored. This gives you a consistent point-in-time snapshot of data from the source server.\n\nIf you pass mongodump a replica set connection string (e.g., \"setName/ seed1,seed2,seed3\"), it will automatically select the primary to dump from. If you want to use a secondary, you can specify a read preference. The read preference can be specified by --uri connection string, by the uri readPreferenceTags option, or by the --readPreference command-line option. For more details on the various settings and options, please see the mongodump MongoDB documentation page.\n\nTo restore from a mongodump backup, use the mongorestore tool:\n\n$ mongorestore -p 31000 --oplogReplay dump/\n\nIf you used the --oplog option to dump the database, you must use the --oplogReplay option with mongorestore to get the point-in-time snapshot.\n\nIf you are replacing data on a running server, you may (or may not) wish to use the --drop option, which drops a collection before restoring it.\n\n444\n\n|\n\nChapter 23: Making Backups\n\nThe behavior of mongodump and mongorestore has changed over time. To prevent compatibility issues, try to use the same version of both utilities (you can see their versions by running mongodump --version and mongorestore --version).\n\nFrom MongoDB version 4.2 and up, you cannot use either mongo‐ dump or mongorestore as a strategy for backing up a sharded clus‐ ter. These tools do not maintain the atomicity guarantees of transactions across shards.\n\nMoving collections and databases with mongodump and mongorestore\n\nYou can restore into an entirely different database and collection than you dumped from. This can be useful if different environments use different database names (say, dev and prod) but the same collection names.\n\nTo restore a .bson file into a specific database and collection, specify the targets on the command line:\n\n$ mongorestore --db newDb --collection someOtherColl dump/oldDB/oldColl.bson\n\nIt is also possible to use these tools with SSH to perform data migration without any disk I/O using the archive feature of these tools. This simplifies three stages into one operation, when previously you had to back up to disk, then copy those backup files to a target server, and then run mongorestore on that server to restore the backups:\n\n$ ssh eoin@proxy.server.com mongodump --host source.server.com\\ --archive | ssh eoin@target.server.com mongorestore --archive\n\nCompression can be combined with the archive feature of these tools to further reduce the size of the information sent while performing a data migration. Here is the same SSH data migration example using both the archive and compression features of these tools:\n\n$ ssh eoin@proxy.server.com mongodump --host source.server.com\\ --archive --gzip | ssh eoin@target.server.com mongorestore --archive --gzip\n\nAdministrative complications with unique indexes\n\nIf you have a unique index (other than \"_id\") on any of your collections, you should consider using a different type of backup than mongodump/mongorestore. Unique indexes require that the data does not change in ways that would violate the unique index constraint during the copy. The safest way to ensure this is to choose a method that “freezes” the data, then make a backup as described in either of the previous two sections.\n\nIf you are determined to use mongodump/mongorestore, you may need to preprocess your data when you restore from a backup.\n\nBacking Up a Server\n\n|\n\n445\n\nSpecific Considerations for Replica Sets The main additional consideration when backing up a replica set is that as well as the data, you must also capture the state of the replica set to ensure an accurate point-in- time snapshot of your deployment is made.\n\nGenerally, you should make backups from a secondary: this keeps load off of the pri‐ mary, and you can lock a secondary without affecting your application (so long as your application isn’t sending it read requests). You can use any of the three methods outlined previously to back up a replica set member, but a filesystem snapshot or data file copy is recommended. Either of these techniques can be applied to replica set sec‐ ondaries with no modification.\n\nmongodump is not quite as simple to use when replication is enabled. First, if you are using mongodump, you must take your backups using the --oplog option to get a point-in-time snapshot; otherwise the backup’s state won’t match the state of any other members in the cluster. You must also create an oplog when you restore from a mongodump backup, or the restored member will not know where it was synced to.\n\nTo restore a replica set member from a mongodump backup, start the target replica set member as a standalone server with an empty data directory and run mongorestore on it (as described in the previous section) with the --oplogReplay option. Now it should have a complete copy of the data, but it still needs an oplog. Create an oplog using the createCollection command:\n\n> use local > db.createCollection(\"oplog.rs\", {\"capped\" : true, \"size\" : 10000000})\n\nSpecify the size of the collection in bytes. See “Resizing the Oplog” on page 282 for advice on oplog sizing.\n\nNow you need to populate the oplog. The easiest way to do this is to restore the oplog.bson backup file from the dump into the local.oplog.rs collection:\n\n$ mongorestore -d local -c oplog.rs dump/oplog.bson\n\nNote that this is not a dump of the oplog itself (dump/local/oplog.rs.bson), but rather of the oplog operations that occurred during the dump. Once this mongorestore is complete, you can restart this server as a replica set member.\n\nSpecific Considerations for Sharded Clusters The main additional consideration when backing up a sharded cluster using the approaches in this chapter is that you can only back up the pieces when they are active, and sharded clusters are impossible to “perfectly” back up while active: you can’t get a snapshot of the entire state of the cluster at a point in time. However, this limitation is generally sidestepped by the fact that as your cluster gets bigger, it\n\n446\n\n|\n\nChapter 23: Making Backups\n\nbecomes less and less likely that you’d ever have to restore the whole thing from a backup. Thus, when dealing with a sharded cluster, we focus on backing up pieces: the config servers and the replica sets individually. If you need the ability to back up the whole cluster to a particular point in time or would prefer an automated solution, you can avail yourself of MongoDB’s Cloud Manager or Atlas backup feature.\n\nTurn off the balancer before performing any of these operations on a sharded cluster (either backup or restore). You cannot get a consistent snapshot of the world with chunks flying around. See “Balancing Data” on page 359 for instructions on turning the balancer on and off.\n\nBacking Up and Restoring an Entire Cluster When a cluster is very small or in development, you may want to actually dump and restore the entire thing. You can accomplish this by turning off the balancer and then running mongodump through the mongos. This creates a backup of all of the shards on whatever machine mongodump is running on.\n\nTo restore from this type of backup, run mongorestore connected to a mongos.\n\nAlternatively, after turning off the balancer you can take filesystem or data directory backups of each shard and the config servers. However, you will inevitably get copies from each at slightly different times, which may or may not be a problem. Also, as soon as you turn on the balancer and a migrate occurs, some of the data you backed up from one shard will no longer be there.\n\nBacking Up and Restoring a Single Shard Most often, you’ll only need to restore a single shard in a cluster. If you are not too picky, you can restore from a backup of that shard using one of the single-server methods just described.\n\nThere is one important issue to be aware of, however. Suppose you make a backup of your cluster on Monday. On Thursday, your disk melts down and you have to restore from the backup. In the intervening days, new chunks may have moved to this shard. Your backup of the shard from Monday will not contain these new chunks. You may be able to use a config server backup to figure out where the disappearing chunks lived on Monday, but it is a lot more difficult than simply restoring the shard. In most cases, restoring the shard and losing the data in those chunks is the preferable route.\n\nYou can connect directly to a shard to restore from a backup (instead of going through mongos).\n\nSpecific Considerations for Sharded Clusters\n\n|\n\n447\n\nCHAPTER 24 Deploying MongoDB\n\nThis chapter gives recommendations for setting up a server to go into production. In particular, it covers:\n\nChoosing what hardware to buy and how to set it up\n\nUsing virtualized environments\n\nImportant kernel and disk I/O settings\n\nNetwork setup: who needs to connect to whom\n\nDesigning the System You generally want to optimize for data safety and the quickest access you can afford. This section discusses the best way to accomplish these goals when choosing disks, RAID configuration, CPUs, and other hardware and low-level software components.\n\nChoosing a Storage Medium In order of preference, we would like to store and retrieve data from:\n\n1. RAM\n\n2. SSD\n\n3. Spinning disk\n\nUnfortunately, most people have limited budgets or enough data that storing every‐ thing in RAM is impractical and SSDs are too expensive. Thus, the typical deploy‐ ment is a small amount of RAM (relative to total data size) and a lot of space on a\n\n449\n\nspinning disk. If you are in this camp, the important thing is that your working set is smaller than RAM, and you should be ready to scale out if the working set gets bigger.\n\nIf you are able to spend what you like on hardware, buy a lot of RAM and/or SSDs.\n\nReading data from RAM takes a few nanoseconds (say, 100). Conversely, reading from disk takes a few milliseconds (say, 10). It can be hard to picture the difference between these two numbers, so let’s scale them up to more relatable numbers: if accessing RAM took 1 second, accessing the disk would take over a day!\n\n100 nanoseconds × 10,000,000 = 1 second\n\n10 milliseconds × 10,000,000 = 1.16 days\n\nThese are very back-of-the-envelope calculations (your disk might be a bit faster or your RAM a bit slower), but the magnitude of this difference doesn’t change much. Thus, we want to access the disk as seldom as possible.\n\nRecommended RAID Configurations RAID is hardware or software that lets you treat multiple disks as though they were a single disk. It can be used for reliability, performance, or both. A set of disks using RAID is referred to as a RAID array (somewhat redundantly, as RAID stands for redundant array of inexpensive disks).\n\nThere are a number of ways to configure RAID, depending on the features you’re looking for—generally some combination of speed and fault tolerance. These are the most common varieties:\n\nRAID0\n\nStriping disks for improved performance. Each disk holds part of the data, simi‐ lar to MongoDB’s sharding. Because there are multiple underlying disks, lots of data can be written to disk at the same time. This improves throughput on writes. However, if a disk fails and data is lost, there are no copies of it. It also can cause slow reads, as some data volumes may be slower than others.\n\nRAID1\n\nMirroring for improved reliability. An identical copy of the data is written to each member of the array. This has lower performance than RAID0, as a single member with a slow disk can slow down all writes. However, if a disk fails, you will still have a copy of the data on another member of the array.\n\nRAID5\n\nStriping disks, plus keeping an extra piece of data about the other data that’s been stored to prevent data loss on server failure. Basically, RAID5 can handle one\n\n450\n\n|\n\nChapter 24: Deploying MongoDB\n\ndisk going down and hide that failure from the user. However, it is slower than any of the other varieties listed here because it needs to calculate this extra piece of information whenever data is written. This is particularly expensive with Mon‐ goDB, as a typical workload does many small writes.\n\nRAID10\n\nA combination of RAID0 and RAID1: data is striped for speed and mirrored for reliability.\n\nWe recommend using RAID10: it is safer than RAID0 and can smooth out perfor‐ mance issues that can occur with RAID1. However, some people feel that RAID1 on top of replica sets is overkill and opt for RAID0. It is a matter of personal preference: how much risk are you willing to trade for performance?\n\nDo not use RAID5: it is very, very slow.\n\nCPU MongoDB historically was very light on CPU, but with the use of the WiredTiger storage engine this is no longer the case. The WiredTiger storage engine is multi‐ threaded and can take advantage of additional CPU cores. You should therefore bal‐ ance your investment between memory and CPU.\n\nWhen choosing between speed and number of cores, go with speed. MongoDB is bet‐ ter at taking advantage of more cycles on a single processor than increased parallelization.\n\nOperating System 64-bit Linux is the operating system MongoDB runs best on. If possible, use some fla‐ vor of that. CentOS and Red Hat Enterprise Linux are probably the most popular choices, but any flavor should work (Ubuntu and Amazon Linux are also common). Be sure to use the most recent stable version of the operating system, because old, buggy packages or kernels can sometimes cause issues.\n\n64-bit Windows is also well supported.\n\nOther flavors of Unix are not as well supported: proceed with caution if you’re using Solaris or one of the BSD variants. Builds for these systems have, at least historically, had a lot of issues. MongoDB explicitly stopped supporting Solaris in August 2017, noting a lack of adoption among users.\n\nOne important note on cross-compatibility: MongoDB uses the same wire protocol and lays out data files identically on all systems, so you can deploy on a combination of operating systems. For example, you could have a mongos process running on Windows and the mongods that are its shards running on Linux. You can also copy data files from Windows to Linux or vice versa with no compatibility issues.\n\nDesigning the System\n\n|\n\n451",
      "page_number": 440
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 451-458)",
      "start_page": 451,
      "end_page": 458,
      "detection_method": "topic_boundary",
      "content": "Since version 3.4, MongoDB no longer supports 32-bit x86 platforms. Do not run any type of MongoDB server on a 32-bit machine.\n\nMongoDB works with little-endian architectures and one big-endian architecture: IBM’s zSeries. Most drivers support both little- and big-endian systems, so you can run clients on either. However, the server will typically be run on a little-endian machine.\n\nSwap Space You should allocate a small amount of swap in case memory limits are reached to pre‐ vent the kernel from killing MongoDB. It doesn’t usually use any swap space, but in extreme circumstances the WiredTiger storage engine might use some. If this occurs, then you should consider increasing the memory capacity of your machine or review‐ ing your workload to avoid this problematic situation for performance and for stability.\n\nThe majority of memory MongoDB uses is “slippery”: it’ll be flushed to disk and replaced with other memory as soon as the system requests the space for something else. Therefore, database data should never be written to swap space: it’ll be flushed back to disk first.\n\nHowever, occasionally MongoDB will use swap for operations that require ordering data: either building indexes or sorting. It attempts not to use too much memory for these types of operations, but by performing many of them at the same time you may be able to force swapping.\n\nIf your application is managing to make MongoDB use swap space, you should look into redesigning the application or reducing load on the swapping server.\n\nFilesystem For Linux, only the XFS filesystem is recommended for your data volumes with the WiredTiger storage engine. It is possible to use the ext4 filesystem with WiredTiger, but be aware there are known performance issues (specifically, that it may stall on WiredTiger checkpoints).\n\nOn Windows, either NTFS or FAT is fine.\n\nDo not use Network File Storage (NFS) directly mounted for Mon‐ goDB storage. Some client versions lie about flushing, randomly remount and flush the page cache, and do not support exclusive file locking. Using NFS can cause journal corruption and should be avoided at all costs.\n\n452\n\n|\n\nChapter 24: Deploying MongoDB\n\nVirtualization Virtualization is a great way to get cheap hardware and be able to expand fast. How‐ ever, there are some downsides—particularly unpredictable network and disk I/O. This section covers virtualization-specific issues.\n\nMemory Overcommitting The memory overcommit Linux kernel setting controls what happens when processes request too much memory from the operating system. Depending on how it’s set, the kernel may give memory to processes even if that memory is not actually available (in the hopes that it’ll become available by the time the process needs it). That’s called overcommitting: the kernel promises memory that isn’t actually there. This operating system kernel setting does not work well with MongoDB.\n\nThe possible values for vm.overcommit_memory are 0 (the kernel guesses about how much to overcommit); 1 (memory allocation always succeeds); or 2 (don’t commit more virtual address space than swap space plus a fraction of the overcommit ratio). The value 2 is complicated, but it’s the best option available. To set this, run:\n\n$ echo 2 > /proc/sys/vm/overcommit_memory\n\nYou do not need to restart MongoDB after changing this operating system setting.\n\nMystery Memory Sometimes the virtualization layer does not handle memory provisioning correctly. Thus, you may have a virtual machine that claims to have 100 GB of RAM available but only ever allows you to access 60 GB of it. Conversely, we’ve seen people that were supposed to have 20 GB of memory end up being able to fit an entire 100 GB dataset into RAM!\n\nAssuming you don’t end up on the lucky side, there isn’t much you can do. If your operating system readahead is set appropriately and your virtual machine just won’t use all the memory it should, you may just have to switch virtual machines.\n\nHandling Network Disk I/O Issues One of the biggest problems with using virtualized hardware is that you are generally sharing a disk with other tenants, which exacerbates the disk slowness mentioned previously because everyone is competing for disk I/O. Thus, virtualized disks can have very unpredictable performance: they can work fine while your neighbors aren’t busy and suddenly slow down to a crawl if someone else starts hammering the disks.\n\nThe other issue is that this storage is often not physically attached to the machine MongoDB is running on, so even when you have a disk all to yourself I/O will be\n\nVirtualization\n\n|\n\n453\n\nslower than it would be with a local disk. There is also the unlikely-but-possible sce‐ nario of your MongoDB server losing its network connection to your data.\n\nAmazon has what is probably the most widely used networked block store, called Elastic Block Store (EBS). EBS volumes can be connected to Elastic Compute Cloud (EC2) instances, allowing you to give a machine almost any amount of disk immedi‐ ately. If you are using EC2, you should also enable AWS Enhanced Networking if it’s available for the instance type, as well as disable the dynamic voltage and frequency scaling (DVFS) and CPU power-saving modes plus hyperthreading. On the plus side, EBS makes backups very easy (take a snapshot from a secondary, mount the EBS drive on another instance, and start up mongod). On the downside, you may encounter variable performance.\n\nIf you require more predictable performance, there are a couple of options. One is to host MongoDB on your own servers—that way, you know no one else is slowing things down. However, that’s not an option for a lot of people, so the next best thing is to get an instance in the cloud that guarantees a certain number of I/O Operations Per Second (IOPS). See http://docs.mongodb.org for up-to-date recommendations on hosted offerings.\n\nIf you can’t pursue either of these options and you need more disk I/O than an over‐ loaded EBS volume can sustain, there is a way to hack around it. Basically, what you can do is keep monitoring the volume MongoDB is using. If and when that volume slows down, immediately kill that instance and bring up a new one with a different data volume.\n\nThere are a couple of statistics to watch for:\n\nSpiking I/O utilization (“IO wait” on Cloud Manager/Atlas), for obvious reasons.\n\nPage fault rates spiking. Note that changes in application behavior could also cause working set changes: you should disable this assassination script before deploying new versions of your application.\n\nThe number of lost TCP packets going up (Amazon is particularly bad about this: when performance starts to fall, it drops TCP packets all over the place).\n\nMongoDB’s read and write queues spiking (this can be seen in Cloud Manager/ Atlas or in mongostat’s qr/qw column).\n\nIf your load varies over the day or week, make sure your script takes that into account: you don’t want a rogue cron job killing off all of your instances because of an unusually heavy Monday morning rush.\n\nThis hack relies on you having recent backups or relatively quick-to-sync datasets. If you have each instance holding terabytes of data, you might want to pursue an\n\n454\n\n|\n\nChapter 24: Deploying MongoDB\n\nalternative approach. Also, this is only likely to work: if your new volume is also being hammered, it will be just as slow as the old one.\n\nUsing Non-Networked Disks\n\nThis section uses Amazon-specific vocabulary. However, it may apply to other providers.\n\nEphemeral drives are the actual disks attached to the physical machine your VM is running on. They don’t have a lot of the problems networked storage does. Local disks can still be overloaded by other users on the same box, but with a large box you can be reasonably sure you’re not sharing disks with too many others. Even with a smaller instance, often an ephemeral drive will give better performance than a net‐ worked drive so long as the other tenants aren’t doing tons of IOPS.\n\nThe downside is in the name: these disks are ephemeral. If your EC2 instance goes down, there’s no guarantee you’ll end up on the same box when you restart the instance, and then your data will be gone.\n\nThus, ephemeral drives should be used with care. You should make sure that you do not store any important or unreplicated data on these disks. In particular, do not put the journal on these ephemeral drives, or your database on network storage. In gen‐ eral, think of ephemeral drives as a slow cache rather than a fast disk and use them accordingly.\n\nConfiguring System Settings There are several system settings that can help MongoDB run more smoothly, which are mostly related to disk and memory access. This section covers each of these options and how you should tweak them.\n\nTurning Off NUMA When machines had a single CPU, all RAM was basically the same in terms of access time. As machines started to have more processors, engineers realized that having all memory be equally far from each CPU (as shown in Figure 24-1) was less efficient than having each CPU have some memory that is especially close to it and fast for that particular CPU to access (Figure 24-2). This architecture, where each CPU has its own “local” memory, is called nonuniform memory architecture (NUMA).\n\nConfiguring System Settings\n\n|\n\n455\n\nFigure 24-1. Uniform memory architecture: all memory has the same access cost for each CPU\n\nFigure 24-2. Nonuniform memory architecture: certain memory is attached to a CPU, giving the CPU faster access to that memory; CPUs can still access other CPUs’ memory, but it is more expensive than accessing their own\n\nFor lots of applications, NUMA works well: the processors often need different data because they’re running different programs. However, this works terribly for data‐ bases in general and MongoDB in particular because databases have such different memory access patterns than other types of applications. MongoDB uses a massive amount of memory and needs to be able to access memory that is “local” to other CPUs. However, the default NUMA settings on many systems make this difficult.\n\nCPUs favor using the memory that is attached to them, and processes tend to favor one CPU over the others. This means that memory often fills up unevenly, potentially leaving you with one processor using 100% of its local memory and the other pro‐ cessors using only a fraction of their memory, as shown in Figure 24-3.\n\nFigure 24-3. Sample memory usage in a NUMA system\n\nIn the scenario in Figure 24-3, suppose CPU1 needs some data that isn’t in memory yet. It must use its local memory for data that doesn’t have a “home” yet, but its local memory is full. Thus, it has to evict some of the data in its local memory to make\n\n456\n\n|\n\nChapter 24: Deploying MongoDB\n\nroom for the new data, even though there’s plenty of space left in the memory attached to CPU2! This process tends to cause MongoDB to run much slower than expected, as it only has a fraction of the memory available that it should have. Mon‐ goDB vastly prefers semiefficient access to more data over extremely efficient access to less data.\n\nWhen running MongoDB servers and clients on NUMA hardware, you should con‐ figure a memory interleave policy so that the host behaves in a non-NUMA fashion. MongoDB checks NUMA settings on startup when deployed on Linux and Windows machines. If the NUMA configuration may degrade performance, MongoDB prints a warning.\n\nOn Windows, memory interleaving must be enabled through the machine’s BIOS. Consult your system documentation for details.\n\nWhen running MongoDB on Linux, you should disable zone reclaim in the sysctl set‐ tings using one of the following commands:\n\necho 0 | sudo tee /proc/sys/vm/zone_reclaim_mode\n\nsudo sysctl -w vm.zone_reclaim_mode=0\n\nThen, you should use numactl to start your mongod instances, including the config servers, mongos instances, and any clients. If you do not have the numactl command, refer to the documentation for your operating system to install the numactl package.\n\nThe following command demonstrates how to start a MongoDB instance using numactl:\n\nnumactl --interleave=all <path> <options>\n\nThe <path> is the path to the program you are starting and the <options> are any optional arguments to pass to the program.\n\nTo fully disable NUMA behavior, you must perform both operations. For more infor‐ mation, see the documentation.\n\nSetting Readahead Readahead is an optimization where the operating system reads more data from disk than was actually requested. This is useful because most workloads that computers handle are sequential: if you load the first 20 MB of a video, you are probably going to want the next couple of megabytes of it. Thus, the system will read more from disk than you actually request and store it in memory, just in case you need it soon.\n\nFor the WiredTiger storage engine, you should set readahead to between 8 and 32 regardless of the storage media type (spinning disk, SSD, etc.). Setting it higher bene‐ fits sequential I/O operations, but since MongoDB disk access patterns are typically random, a higher readahead value provides limited benefit and may even result in\n\nConfiguring System Settings\n\n|\n\n457\n\nperformance degradation. For most workloads, a readahead of between 8 and 32 pro‐ vides optimal MongoDB performance.\n\nIn general, you should set the readahead within this range unless testing shows that a higher value is measurably, repeatably, and reliably beneficial. MongoDB Professional Support can provide advice and guidance on nonzero readahead configurations.\n\nDisabling Transparent Huge Pages (THP) THP causes similar issues to high readahead. Do not use this feature unless:\n\nAll of your data fits into memory.\n\nYou have no plans for it to ever grow beyond memory.\n\nMongoDB needs to page in lots of tiny pieces of memory, so using THP can result in more disk I/O.\n\nSystems move data from disk to memory and back by the page. Pages are generally a couple of kilobytes (x86 defaults to 4,096-byte pages). If a machine has many giga‐ bytes of memory, keeping track of each of these (relatively tiny) pages can be slower than just tracking a few larger-granularity pages. THP is a solution that allows you to have pages that are up to 256 MB (on IA-64 architectures). However, using it means that you are keeping megabytes of data from one section of disk in memory. If your data does not fit in RAM, then swapping in larger pieces from disk will just fill up your memory quickly with data that will need to be swapped out again. Also, flushing any changes to disk will be slower, as the disk must write megabytes of “dirty” data, instead of a few kilobytes.\n\nTHP was actually developed to benefit databases, so this may be surprising to experi‐ enced database admins. However, MongoDB tends to do a lot less sequential disk access than relational databases do.\n\nOn Windows these are called Large Pages, not Huge Pages. Some versions of Windows have this feature enabled by default and some do not, so check and make sure it is turned off.\n\nChoosing a Disk Scheduling Algorithm The disk controller receives requests from the operating system and processes them in an order determined by a scheduling algorithm. Sometimes changing this algo‐ rithm can improve disk performance. For other hardware and workloads, it may not make a difference. The best way to decide which algorithm to use is to test them out\n\n458\n\n|\n\nChapter 24: Deploying MongoDB\n\nyourself on your workload. Deadline and completely fair queueing (CFQ) both tend to be good choices.\n\nThere are a couple of situations where the noop scheduler (a contraction of “no-op”) is the best choice. If you’re in a virtualized environment, use the noop scheduler. This scheduler basically passes the operations through to the underlying disk controller as quickly as possible. It is fastest to do this and let the real disk controller handle any reordering that needs to happen.\n\nSimilarly, on SSDs, the noop scheduler is generally the best choice. SSDs don’t have the same locality issues that spinning disks do.\n\nFinally, if you’re using a RAID controller with caching, use noop. The cache behaves like an SSD and will take care of propagating the writes to the disk efficiently.\n\nIf you are on a physical server that is not virtualized, the operating system should use the deadline scheduler. The deadline scheduler caps maximum latency per request and maintains a reasonable disk throughput that is best for disk-intensive database applications.\n\nYou can change the scheduling algorithm by setting the --elevator option in your boot configuration.\n\nThe option is called \"elevator\" because the scheduler behaves like an elevator, picking up people (I/O requests) from different floors (processes/times) and dropping them off where they want to go in an arguabley optimal way.\n\nOften all of the algorithms work pretty well; you may not see much of a difference between them.\n\nDisabling Access Time Tracking By default, the system tracks when files were last accessed. As the data files used by MongoDB are very high-traffic, you can get a performance boost by disabling this tracking. You can do this on Linux by changing atime to noatime in /etc/fstab:\n\n/dev/sda7 /data xfsf rw,noatime 1 2\n\nYou must remount the device for the changes to take effect.\n\natime is more of an issue on older kernels (e.g., ext3); newer ones use relatime as a default, which is less aggressively updated. Also, be aware that setting noatime can affect other programs using the partition, such as mutt or backup tools.\n\nSimilarly, on Windows you should set the disablelastaccess option. To turn off last access time recording, run:\n\nConfiguring System Settings\n\n|\n\n459",
      "page_number": 451
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 459-470)",
      "start_page": 459,
      "end_page": 470,
      "detection_method": "topic_boundary",
      "content": "C:\\> fsutil behavior set disablelastaccess 1\n\nYou must reboot for this setting to take effect. Setting this may affect the remote stor‐ age service, but you probably shouldn’t be using a service that automatically moves your data to other disks anyway.\n\nModifying Limits There are two limits that MongoDB tends to blow by: the number of threads a pro‐ cess is allowed to spawn and the number of file descriptors a process is allowed to open. Both of these should generally be set to unlimited.\n\nWhenever a MongoDB server accepts a connection, it spawns a thread to handle all activity on that connection. Therefore, if you have 3,000 connections to the database, the database will have 3,000 threads running (plus a few other threads for non-client- related tasks). Depending on your application server configuration, your client may spawn anywhere from a dozen to thousands of connections to MongoDB.\n\nIf your client will dynamically spawn more child processes as traffic increases (most application servers will do this), it is important to make sure that these child pro‐ cesses are not so numerous that they can max out MongoDB’s limits. For example, if you have 20 application servers, each one of which is allowed to spawn 100 child pro‐ cesses, and each child process can spawn 10 threads that all connect to MongoDB, that could result in the spawning of 20 × 100 × 10 = 20,000 connections at peak traf‐ fic. MongoDB is probably not going to be very happy about spawning tens of thou‐ sands of threads and, if you run out of threads per process, will simply start refusing new connections.\n\nThe other limit to modify is the number of file descriptors MongoDB is allowed to open. Every incoming and outgoing connection uses a file descriptor, so the client connection storm just mentioned would create 20,000 open filehandles.\n\nmongos in particular tends to create connections to many shards. When a client con‐ nects to a mongos and makes a request, the mongos opens connections to any and all shards necessary to fulfill that request. Thus, if a cluster has 100 shards and a client connects to a mongos and tries to query for all of its data, the mongos must open 100 connections: one connection to each shard. This can quickly lead to an explosion in the number of connections, as you can imagine from the previous example. Suppose a liberally configured app server made a hundred connections to a mongos process. This could get translated to 100 inbound connections × 100 shards = 10,000 connec‐ tions to shards! (This assumes a nontargeted query on each connection, which would be a bad design, so this is a somewhat extreme example.)\n\nThus, there are a few adjustments to make. Many people purposefully configure mon‐ gos processes to only allow a certain number of incoming connections by using the maxConns option. This is a good way to enforce that your client is behaving well.\n\n460\n\n|\n\nChapter 24: Deploying MongoDB\n\nYou should also increase the limit on the number of file descriptors, as the default (generally 1,024) is simply too low. Set the max number of file descriptors to unlimi‐ ted or, if you’re nervous about that, 20,000. Each system has a different way of chang‐ ing these limits, but in general, make sure that you change both the hard and soft limits. A hard limit is enforced by the kernel and can only be changed by an adminis‐ trator, whereas a soft limit is user-configurable.\n\nIf the maximum number of connections is left at 1,024, Cloud Manager will warn you by displaying the host in yellow in the host list. If low limits are the issue that trig‐ gered the warning, the Last Ping tab should display a message similar to that shown in Figure 24-4.\n\nFigure 24-4. Cloud Manager low ulimit (file descriptors) setting warning\n\nEven if you have a nonsharded setup and an application that only uses a small num‐ ber of connections, it’s a good idea to increase the hard and soft limits to at least 4,096. That will stop MongoDB from warning you about them and give you some breathing room, just in case.\n\nConfiguring Your Network This section covers which servers should have connectivity to which other servers. Often, for reasons of network security (and sensibility), you may want to limit the connectivity of MongoDB servers. Note that multiserver MongoDB deployments should handle networks being partitioned or down, but it isn’t recommended as a general deployment strategy.\n\nFor a standalone server, clients must be able to make connections to the mongod.\n\nMembers of a replica set must be able to make connections to every other member. Clients must be able to connect to all nonhidden, nonarbiter members. Depending on network configuration, members may also attempt to connect to themselves, so you should allow mongods to create connections to themselves.\n\nSharding is a bit more complicated. There are four components: mongos servers, shards, config servers, and clients. Connectivity can be summarized in the following three points:\n\nA client must be able to connect to a mongos.\n\nA mongos must be able to connect to the shards and config servers.\n\nConfiguring Your Network\n\n|\n\n461\n\nA shard must be able to connect to the other shards and the config servers.\n\nThe full connectivity chart is described in Table 24-1.\n\nTable 24-1. Sharding connectivity\n\nConnectivity\n\nfrom server type\n\nto server type mongos mongos Shard Config server Client\n\nNot required Required Required Not required\n\nShard Config server Not required Not required Not required Required Required Not required Not required Not required\n\nClient Required Not recommended Not recommended Not MongoDB-related\n\nThere are three possible values in the table. “Required” means that connectivity between these two components is required for sharding to work as designed. Mon‐ goDB will attempt to degrade gracefully if it loses these connections due to network issues, but you shouldn’t purposely configure it that way.\n\n“Not required” means that these two elements never talk in the direction specified, so no connectivity is needed.\n\n“Not recommended” means that these two elements should never talk, but due to user error they could. For example, it is recommended that clients only make connec‐ tions to the mongos, not the shards, so that clients do not inadvertently make requests directly to shards. Similarly, clients should not be able to directly access config servers so that they cannot accidentally modify config data.\n\nNote that mongos processes and shards talk to config servers, but config servers don’t make connections to anyone, even one another.\n\nShards must communicate during migrates: shards connect to one another directly to transfer data.\n\nAs mentioned earlier, replica set members that compose shards should be able to con‐ nect to themselves.\n\nSystem Housekeeping This section covers some common issues you should be aware of before deploying.\n\nSynchronizing Clocks In general, it’s safest to have your systems’ clocks within a second of each other. Rep‐ lica sets should be able to handle nearly any clock skew. Sharding can handle some skew (if it gets beyond a few minutes, you’ll start seeing warnings in the logs), but it’s\n\n462\n\n|\n\nChapter 24: Deploying MongoDB\n\nbest to minimize it. Having in-sync clocks also makes figuring out what’s happening from logs easier.\n\nYou can keep clocks synchronized using the w32tm tool on Windows and the ntp daemon on Linux.\n\nThe OOM Killer Very occasionally, MongoDB will allocate enough memory that it will be targeted by the out-of-memory (OOM) killer. This particularly tends to happen during index builds, as that is one of the only times when MongoDB’s resident memory should put any strain on the system.\n\nIf your MongoDB process suddenly dies with no errors or exit messages in the logs, check /var/log/messages (or wherever your kernel logs such things) to see if it has any messages about terminating mongod.\n\nIf the kernel has killed MongoDB for memory overuse, you should see something like this in the kernel log:\n\nkernel: Killed process 2771 (mongod) kernel: init invoked oom-killer: gfp_mask=0x201d2, order=0, oomkilladj=0\n\nIf you were running with journaling, you can simply restart mongod at this point. If you were not, restore from a backup or resync the data from a replica.\n\nThe OOM killer gets particularly nervous if you have no swap space and start run‐ ning low on memory, so a good way to prevent it from going on a spree is to config‐ ure a modest amount of swap. As mentioned earlier, MongoDB should never use it, but it makes the OOM killer happy.\n\nIf the OOM killer kills a mongos, you can simply restart it.\n\nTurn Off Periodic Tasks Check that there aren’t any cron jobs, antivirus scanners, or daemons that might peri‐ odically pop to life and steal resources. One culprit we’ve seen is package managers’ automatic update. These programs will come to life, consume a ton of RAM and CPU, and then disappear. This is not something you want running on your produc‐ tion server.\n\nSystem Housekeeping\n\n|\n\n463\n\nAPPENDIX A Installing MongoDB\n\nMongoDB binaries are available for Linux, macOS, Windows, and Solaris. This means that, on most platforms, you can download an archive from the MongoDB Download Center page, inflate it, and run the binary.\n\nThe MongoDB server requires a directory it can write database files to and a port it can listen for connections on. This section covers the entire install on the two var‐ iants of system: Windows and everything else (Linux/Unix/macOS).\n\nWhen we speak of “installing MongoDB,” generally what we are talking about is set‐ ting up mongod, the core database server. mongod can be used as a standalone server or as a member of a replica set. Most of the time, this will be the MongoDB process you are using.\n\nChoosing a Version MongoDB uses a fairly simple versioning scheme: even-point releases are stable, and odd-point releases are development versions. For example, anything starting with 4.2 is a stable release, such as 4.2.0, 4.2.1, and 4.2.8. Anything starting with 4.3 is a devel‐ opment release, such as 4.3.0, 4.3.2, or 4.3.12. Let’s take the 4.2/4.3 release as a sample case to demonstrate how the versioning timeline works:\n\n1. MongoDB 4.2.0 is released. This is a major release and will have an extensive changelog.\n\n2. After the developers start working on the milestones for 4.4 (the next major sta‐ ble release), they release 4.3.0. This is the new development branch, which is fairly similar to 4.2.0 but probably with an extra feature or two and maybe some bugs.\n\n465\n\n3. As the developers continue to add features, they will release 4.3.1, 4.3.2, and so on. These releases should not be used in production.\n\n4. Some minor bug fixes may be backported to the 4.2 branch, which will cause releases of 4.2.1, 4.2.2, and so on. Developers are conservative about what is backported; few new features are ever added to a stable release. Generally, only bug fixes are ported.\n\n5. After all of the major milestones have been reached for 4.4.0, 4.3.7 (or whatever the latest development release is) will be turned into 4.4.0-rc0.\n\n6. After extensive testing of 4.4.0-rc0, usually there are a couple minor bugs that need to be fixed. Developers fix these bugs and release 4.4.0-rc1.\n\n7. Developers repeat step 6 until no new bugs are apparent, and then 4.4.0-rc2 (or whatever the latest release ended up being) is renamed 4.4.0.\n\n8. Developers start over from step 1, incrementing all versions by 0.2.\n\nYou can see how close a production release is by browsing the core server roadmap on the MongoDB bug tracker.\n\nIf you are running in production, you should use a stable release. If you are planning to use a development release in production, ask about it first on the mailing list or IRC to get the developers’ advice.\n\nIf you are just starting development on a project, using a development release may be a better choice. By the time you deploy to production, there will probably be a stable release with the features you’re using (MongoDB attempts to stick to a regular cycle of stable releases every 12 months). However, you must balance this against the possi‐ bility that you may run into server bugs, which can be discouraging to a new user.\n\nWindows Install To install MongoDB on Windows, download the Windows .msi from the MongoDB Download Center page. Use the advice in the previous section to choose the correct version of MongoDB. When you click the link, it will download the .msi. Double- click the .msi file icon to launch the installer program.\n\nNow you need to make a directory in which MongoDB can write database files. By default, MongoDB tries to use the \\data\\db directory on the current drive as its data directory (e.g., if you’re running mongod on C: on Windows, it’ll use C:\\Program Files \\MongoDB\\Server\\&<VERSION>\\data). This will be created automatically for you by the installer. If you chose to use a directory other than \\data\\db, you’ll need to specify the path when you start MongoDB, which is covered in a moment.\n\nNow that you have a data directory, open the command prompt (cmd.exe). Navigate to the directory where you unzipped the MongoDB binaries and run the following:\n\n466\n\n| Appendix A: Installing MongoDB\n\n$ C:\\Program Files\\MongoDB\\Server\\&<VERSION>\\bin\\mongod.exe\n\nIf you chose a directory other than C:\\Program Files\\MongoDB\\Server\\&<VERSION> \\data, you’ll have to specify it here, with the --dbpath argument:\n\n$ C:\\Program Files\\MongoDB\\Server\\&<VERSION>\\bin\\mongod.exe \\ --dbpath C:\\Documents and Settings\\Username\\My Documents\\db\n\nSee Chapter 21 for more common options, or run mongod.exe --help to see all the options.\n\nInstalling as a Service MongoDB can also be installed as a service on Windows. To do this, simply run it with the full path, escape any spaces, and use the --install option. For example:\n\n$ C:\\Program Files\\MongoDB\\Server\\4.2.0\\bin\\mongod.exe \\ --dbpath \"\\\"C:\\Documents and Settings\\Username\\My Documents\\db\\\"\" \\ --install\n\nIt can then be started and stopped from the Control Panel.\n\nPOSIX (Linux and Mac OS X) Install Choose a version of MongoDB, based on the advice in the section “Choosing a Ver‐ sion” on page 465. Go to the MongoDB Download Center and select the correct ver‐ sion for your OS.\n\nIf you are using a Mac and are running macOS Catalina 10.15+, you should use /System/Volumes/Data/db instead of /data/db. This version made a change that renders the root folder read-only and resets upon reboot, which would result in the loss of your Mon‐ goDB data folder.\n\nYou must create a directory for the database to put its files in. By default the database will use /data/db, although you can specify any other directory. If you create the default directory, make sure it has the correct write permissions. You can create the directory and set the permissions by running the following commands:\n\n$ mkdir -p /data/db $ chown -R $USER:$USER /data/db\n\nmkdir -p creates the directory and all its parents, if necessary (i.e., if the /data direc‐ tory doesn’t exist, it will create the /data directory and then the /data/db directory). chown changes the ownership of /data/db so that your user can write to it. Of course, you can also just create a directory in your home folder and specify that MongoDB should use that when you start the database, to avoid any permissions issues.\n\nInstalling MongoDB\n\n|\n\n467\n\nDecompress the .tar.gz file you downloaded from the MongoDB Download Center:\n\n$ tar zxf mongodb-linux-x86_64-enterprise-rhel62-4.2.0.tgz $ cd mongodb-linux-x86_64-enterprise-rhel62-4.2.0\n\nNow you can start the database:\n\n$ bin/mongod\n\nOr, if you’d like to use an alternate database path, specify it with the --dbpath option:\n\n$ bin/mongod --dbpath ~/db\n\nYou can run mongod.exe --help to see all the possible options.\n\nInstalling from a Package Manager There are also many package managers that can be used to install MongoDB. If you prefer using one of these, there are official packages for Red Hat, Debian, and Ubuntu as well as unofficial packages for many other systems. If you use an unofficial version, make sure it installs a relatively recent version.\n\nOn macOS, there are unofficial packages for Homebrew and MacPorts. To use the MongoDB Homebrew tap, you first install the tap and then install the required ver‐ sion of MongoDB via Homebrew. The following example highlights how to install the latest production version of MongoDB Community Edition. You can add the custom tap in a macOS terminal session using:\n\n$ brew tap mongodb/brew\n\nThen install the latest available production release of MongoDB Community Server (including all command-line tools) using:\n\n$ brew install mongodb-community\n\nIf you go for the MacPorts version, be forewarned: it takes hours to compile all the Boost libraries, which are MongoDB prerequisites. Start the download and leave it overnight.\n\nRegardless of the package manager you use, it is a good idea to figure out where it is putting the MongoDB log files before you have a problem and need to find them. It’s important to make sure they’re being saved properly in advance of any possible issues.\n\n468\n\n| Appendix A: Installing MongoDB\n\nAPPENDIX B MongoDB Internals\n\nIt is not necessary to understand MongoDB’s internals to use it effectively, but they may be of interest to developers who wish to work on tools, contribute, or simply understand what’s happening under the hood. This appendix covers some of the basics. The MongoDB source code is available at https://github.com/mongodb/mongo.\n\nBSON Documents in MongoDB are an abstract concept—the concrete representation of a document varies depending on the driver/language being used. Because documents are used extensively for communication in MongoDB, there also needs to be a repre‐ sentation of documents that is shared by all drivers, tools, and processes in the Mon‐ goDB ecosystem. That representation is called Binary JSON, or BSON (no one knows where the J went).\n\nBSON is a lightweight binary format capable of representing any MongoDB docu‐ ment as a string of bytes. The database understands BSON, and BSON is the format in which documents are saved to disk.\n\nWhen a driver is given a document to insert, use as a query, and so on, it will encode that document to BSON before sending it to the server. Likewise, documents being returned to the client from the server are sent as BSON strings. This BSON data is decoded by the driver to its native document representation before being returned to the client.\n\nThe BSON format has three primary goals:\n\n469\n\nEfficiency\n\nBSON is designed to represent data efficiently, without using much extra space. In the worst case BSON is slightly less efficient than JSON, and in the best case (e.g., when storing binary data or large numerics), it is much more efficient.\n\nTraversability\n\nIn some cases, BSON does sacrifice space efficiency to make the format easier to traverse. For example, string values are prefixed with a length rather than relying on a terminator to signify the end of a string. This traversability is useful when the MongoDB server needs to introspect documents.\n\nPerformance\n\nFinally, BSON is designed to be fast to encode to and decode from. It uses C-style representations for types, which are fast to work with in most programming languages.\n\nFor the exact BSON specification, see http://www.bsonspec.org.\n\nWire Protocol Drivers access the MongoDB server using a lightweight TCP/IP wire protocol. The protocol is documented on the MongoDB documentation site but basically consists of a thin wrapper around BSON data. For example, an insert message consists of 20 bytes of header data (which includes a code telling the server to perform an insert and the message length), the collection name to insert into, and a list of BSON docu‐ ments to insert.\n\nData Files Inside the MongoDB data directory, which is /data/db/ by default, a separate file will be stored for each collection and each index. The filenames do not correspond to the names of the collections or indexes, but you can use the stats within the mongo shell to identify the related file for a specific collection. The \"wiredTiger.uri\" field will contain the name of the file to look for in the MongoDB data directory.\n\nUsing stats on the sample_mflix database for the movies collection provides “collection-14--2146526997547809066” as result in the \"wiredTiger.uri\" field:\n\n>db.movies.stats() { \"ns\" : \"sample_mflix.movies\", \"size\" : 65782298, \"count\" : 45993, \"avgObjSize\" : 1430, \"storageSize\" : 45445120, \"capped\" : false,\n\n470\n\n| Appendix B: MongoDB Internals\n\n\"wiredTiger\" : { \"metadata\" : { \"formatVersion\" : 1 }, \"creationString\" : \"access_pattern_hint=none,allocation_size=4KB,\\ app_metadata=(formatVersion=1),assert=(commit_timestamp=none,\\ read_timestamp=none),block_allocation=best,\\ block_compressor=snappy,cache_resident=false,checksum=on,\\ colgroups=,collator=,columns=,dictionary=0,\\ encryption=(keyid=,name=),exclusive=false,extractor=,format=btree,\\ huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,\\ immutable=false,internal_item_max=0,internal_key_max=0,\\ internal_key_truncate=true,internal_page_max=4KB,key_format=q,\\ key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,\\ leaf_value_max=64MB,log=(enabled=true),lsm=(auto_throttle=true,\\ bloom=true,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,\\ bloom_oldest=false,chunk_count_limit=0,chunk_max=5GB,\\ chunk_size=10MB,merge_custom=(prefix=,start_generation=0,suffix=),\\ merge_max=15,merge_min=0),memory_page_image_max=0,\\ memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,\\ prefix_compression=false,prefix_compression_min=4,source=,\\ split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,\\ type=file,value_format=u\", \"type\" : \"file\", \"uri\" : \"statistics:table:collection-14--2146526997547809066\", ... }\n\nThe file’s details can then be verified within the MongoDB data directory:\n\nls -alh collection-14--2146526997547809066.wt -rw------- 1 braz staff 43M 28 Sep 23:33 collection-14--2146526997547809066.wt\n\nIt’s possible to use the aggregation framework to find the URI for each index in a spe‐ cific collection using the following:\n\ndb.movies.aggregate([{ $collStats:{storageStats:{}}}]).next().storageStats.indexDetails { \"_id_\" : { \"metadata\" : { \"formatVersion\" : 8, \"infoObj\" : \"{ \\\"v\\\" : 2, \\\"key\\\" : { \\\"_id\\\" : 1 },\\ \\\"name\\\" : \\\"_id_\\\", \\\"ns\\\" : \\\"sample_mflix.movies\\\" }\" }, \"creationString\" : \"access_pattern_hint=none,allocation_size=4KB,\\ app_metadata=(formatVersion=8,infoObj={ \\\"v\\\" : 2, \\\"key\\\" : \\ { \\\"_id\\\" : 1 },\\\"name\\\" : \\\"_id_\\\", \\\"ns\\\" : \\\"sample_mflix.movies\\\" }),\\ assert=(commit_timestamp=none,read_timestamp=none),block_allocation=best,\\ block_compressor=,cache_resident=false,checksum=on,colgroups=,collator=,\\ columns=,dictionary=0,encryption=(keyid=,name=),exclusive=false,extractor=,\\ format=btree,huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,\\ immutable=false,internal_item_max=0,internal_key_max=0,\\\n\nMongoDB Internals\n\n|\n\n471\n\ninternal_key_truncate=true,internal_page_max=16k,key_format=u,key_gap=10,\\ leaf_item_max=0,leaf_key_max=0,leaf_page_max=16k,leaf_value_max=0,\\ log=(enabled=true),lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,\\ bloom_config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit=0,\\ chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_generation=0,\\ suffix=),merge_max=15,merge_min=0),memory_page_image_max=0,\\ memory_page_max=5MB,os_cache_dirty_max=0,os_cache_max=0,\\ prefix_compression=true,prefix_compression_min=4,source=,\\ split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,\\ value_format=u\", \"type\" : \"file\", \"uri\" : \"statistics:table:index-17--2146526997547809066\", ... \"$**_text\" : { ... \"uri\" : \"statistics:table:index-29--2146526997547809066\", ... \"genres_1_imdb.rating_1_metacritic_1\" : { ... \"uri\" : \"statistics:table:index-30--2146526997547809066\", ... }\n\nWiredTiger stores each collection or index in a single arbitrarily large file. The only limits that impact the potential maximum size of this file are filesystem size limits.\n\nWiredTiger writes a new copy of the full document whenever that document is upda‐ ted. The old copy on disk is flagged for reuse and will eventually be overwritten at a future point, typically during the next checkpoint. This recycles the space used within the WiredTiger file. The compact command can be run to move the data within this file to the start, leaving empty space at the end. At regular intervals, WiredTiger removes this excess empty space by truncating the file. At the end of the compaction process, the excess space is returned to the filesystem.\n\nNamespaces Each database is organized into namespaces, which are mapped to WiredTiger files. This abstraction separates the storage engine’s internal details from the MongoDB query layer.\n\nWiredTiger Storage Engine The default storage engine for MongoDB is the WiredTiger storage engine. When the server starts up, it opens the data files and begins the checkpointing and journaling processes. It works in conjunction with the operating system, whose responsibility is focused on paging data in and out as well as flushing data to disk. This storage engine has several important properties:\n\n472\n\n| Appendix B: MongoDB Internals",
      "page_number": 459
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 471-480)",
      "start_page": 471,
      "end_page": 480,
      "detection_method": "topic_boundary",
      "content": "Compression is on by default for collections and for indexes. The default com‐ pression algorithm is Google’s snappy. Other options include Facebook’s Zstan‐ dard (zstd) and zlib, or indeed no compression. This minimizes storage use in the database at the expense of additional CPU requirements.\n\nDocument-level concurrency allows for updates on different documents from multiple clients in a collection as the same time. WiredTiger uses MultiVersion Concurrency Control (MVCC) to isolate read and write operations to ensure cli‐ ents see a consistent point-in-time view of the data at the start of an operation.\n\nCheckpointing creates a consistent point-in-time snapshot of the data and occurs every 60 seconds. It involves writing all the data in the snapshot to disk and updating the related metadata.\n\nJournaling with checkpointing ensures there is no point in time where data might be lost if there was a failure of a mongod process. WiredTiger uses a write-ahead log (journal) that stores modifications before they are applied.\n\nMongoDB Internals\n\n|\n\n473\n\nSymbols $ (dollar sign)\n\n$ operators (see query operators) $$ (variable reference), 181 creating indexes on $**, 148 position operator, 45 querying arrays, 61 reserved character, 8, 10\n\n$indexStats operator, 433 $maxKey, 297, 321 $minKey, 297 --configdb option, 305 . (dot)\n\nin subcollections, 10 reserved character, 8 2d indexes, 133, 144-146 2dsphere indexes, 133 32-bit systems, 417, 452 64-bit systems, 451 \\0 (the null character), 8, 9\n\nA access time tracking, 459 accumulators\n\nfor arrays, 186 in group versus project stage, 186 purpose of, 186 using in project stage, 186, 195\n\nACID (Atomicity, Consistency, Isolation, and\n\nDurability), 200\n\nacknowledged writes, 376 $addToSet operator, 43, 186 admin database\n\ncontents of, 305\n\nIndex\n\nrole of, 11 shutdown command, 420\n\nadmin user, 391, 402 administration, of applications\n\napplication operations, 371-387 durability, 405-411 security considerations, 389-404\n\nadministration, of replica sets\n\nmanipulating member state, 275 monitoring replication, 275-285 replica set configuration, 272-274 replication on a budget, 285 starting members in standalone mode, 271\n\nadministration, of servers backups, 437-447 deployment, 449-463 monitoring, 425-436 production set up, 415-424\n\nadministration, of sharding adding servers, 356-359 balancing data, 359-367 seeing current state, 339-348 tracking network connections, 348-356\n\naggregation framework\n\n$project operator, 169-174 $unwind operator, 174-181 accumulators, 186 aggregate method, 166 array expressions in project stages, 181-185 concept of, 161 expression classes supported, 168 group stage\n\naggregate command, 188\n\n475\n\naggregating values from multiple docu‐\n\nments, 187\n\ngroup and sort stages, 188 _id field, 192-195 versus project stage, 195\n\nindividual stages, 162 match, project, sort, skip, and limit stages,\n\n163-168\n\npipeline efficiency, 167 presorting, 167 purpose of, 161 repeated stages in, 162 tunables in, 162 writing pipeline results to collections, 198\n\n$all operator, 59 antivirus scanners, 463 Apache Lucene, 146 application design\n\nmanaging consistency, 221 managing schemas, 223 migrating schemas, 222 normalization and denormalization benefits and drawbacks of, 213 cardinality, 216 data representation examples, 212 defined, 211 embedding versus references, 215 social graph data and, 216 update operators, 215 Wil Wheaton effect, 218\n\noptimizations for data manipulations, 219 planning out databases and collections, 220 schema design considerations, 207 schema design patterns, 208-211 when not to use MongoDB, 223\n\napplication operations calculating sizes\n\ncollections, 380 databases, 385 documents, 379 finding problematic, 374 finding slow, 371, 376 genuinely long-running, 375 important fields, 373 killing, 375 preventing phantom, 375 printing stats every few seconds, 386 seeing current, 371\n\napproximation schema design pattern, 210\n\n476\n\n|\n\nIndex\n\narbiters, 246 array expressions, using in project stages,\n\n181-185 array operators\n\nadding elements, 41 adding multiple unique values, 44 positional array modifications, 45 preventing duplicates, 43 removing elements, 44 updates using array filters, 46 using arrays as sets, 43 $arrayElemAt operator, 184 arrays\n\narray type, 18 atomic updates, 19 indexing, 115 manipulating, 41-46 querying, 59-63 uses for, 19 atomicity, 200 attribute schema design pattern, 209 --auth option, 392 authentication, 389 authorization, 390 automatic failover, 236 automatic replication chaining, 279, 281 autosharding, 289, 337 $avg operator, 186 AWS Enhanced Networking, 454\n\nB backup privileges, 391 backups\n\noptions for, 437 replica sets, 446 servers\n\ncopying data files, 442 filesystem snapshots, 438-442 using mongodump, 443 what to backup, 438 balancer (see also chunks)\n\ndraining process, 356-359 firehose strategy and, 329 purpose of, 323 requesting assignments from, 326 role of, 316 turning off, 336, 360\n\nbatch insert, 29 big-endian systems, 452\n\nbinary data type, 18 --bind_ip parameter, 308, 421 boolean type, 17 $box operator, 145 broadcast (scatter-gather) queries, 301 BSD variants, 451 BSON (Binary JSON) format, 469 bsondump tool, 444 bucket schema design pattern, 209\n\nC call back API, 200 capacity, adding with shards, 310 capped collections (see also collections)\n\naccess pattern in, 153 benefits and drawbacks of, 219 circular queue-like behavior, 152 creating, 154 inability to change, 154 limiting number of documents in, 154 oplogs, 249 restricted operation, 152 tailable cursors, 154 versus normal, 151\n\ncapped indexes, versus time-to-live indexes,\n\n153 cardinality\n\nin application design, 216 in shard keys, 334 causal consistency, 201 $centerSphere operator, 143, 146 certification authority (CA), 392 chaining, 281 change streams, 317 changelog collecion, 344-348 checkpoints, 406, 473 chunks\n\nallowed size range, 362 basics of, 311 changing chunk size, 361 checking chunk size, 365 chunk ranges, 312 compound shard keys and, 313 defined, 311 jumbo chunks, 364-367 max chunk, 321 moving, 362 seeing all, 341 sharding chunks versus GridFS chunks, 328\n\nsplitting chunks, 314 status overview, 339\n\ncircle, querying for points within, 145 client certificates, 393, 401 client libraries, purpose of, 261 clocks, synchronizing, 462 Cloud Manager, 437 clusterAdmin privileges, 391 clusterManager privileges, 390 clusterMonitor privileges, 390 clusters\n\nbackup options, 437, 446 components of, 290 durability of, 407 sharding on single-machine, 291-301 tracking cluster data, 311-315 using for multiple databases/collections, 335\n\ncode examples, obtaining and using, xvii code type, 18 collations, 317 collection scans, versus indexes, 75 collections\n\nbasics of, 8-10 benefits of separate, 8 calculating size of, 380 capped, 151-155 checking for corruption, 410 chunks and, 312 dealing with inconvenient names, 28 determining busiest, 386 dropping (clearing entire), 34 dynamic schemas, 8 finding random documents in, 69 internal, 9 locking and storage of, 220 moving with mongodump, 445 multiple, 219 naming, 9 planning in application design, 220 querying, 15 sharding, 296 subcollections, 10 time-to-live (TTL), 219 using clusters for multiple, 335 writing aggregation pipeline results to, 198\n\ncollMod command, 223 command line, starting MongoDB from,\n\n415-420\n\ncomments and questions, xviii\n\nIndex\n\n|\n\n477\n\ncompact command, 472 comparison operators, 55 comparison order, 68 compound indexes benefits of, 105 best practices, 84, 96 compound unique indexes, 127 defined, 81 geospatial, 144 keys in reverse order, 83 types of queries, 82 using, 85-104\n\navoiding in-memory sort, 100 choosing key directions, 102 covered queries, 103 design goals, 86, 97 implicit indexes, 104 index selectivity, 86 specifying which index to use, 92\n\ncompression, 473 computed schema design pattern, 209 computer memory (see memory) concurrency, 473 conditionals, query, 55 --config, 416 config database\n\ncontents of, 305 options increasing security of, 421 role of, 11 seeing configuration information\n\nconfig.changelog, 344 config.chunks, 343 config.collections, 342 config.databases, 342 config.settings, 347 config.shards, 342 connecting to mongos process, 341\n\nconfig servers\n\nbest practices, 304 initiating as replica sets, 304 purpose of, 304 starting, 304\n\n--configdb option, 356 --configExpand, 419 --configsvr option, 305 connection pools, 221 connPoolStats command, 348 consistency\n\ndefined, 200\n\n478\n\n|\n\nIndex\n\nmanaging in application design, 221\n\ncore API, 200 corruption, checking for, 410 CPU (central processing unit), 451 crashes, recovering from, 355 create command, 154 createCollection command, 154 cron jobs, turning off, 463 CRUD (create, read, update, and delete)\n\nbasic shell operation, 14-16 CRUD API, 33 explain cursor method, 76 optimizations for data manipulation, 219\n\ncursors\n\navoiding large skips, 68 benefits of, 66 creating, 66 cursor hint method, 92 explain cursor method, 76 immortal, 70 iterating through results, 66 limits, skips, and sort options, 67 sending queries to servers, 67 tailable cursors, 154\n\nD daemons, turning off, 463 data\n\nbalancing, 359-367 controlling distribution of, 334-337 copying data files, 442 data files internals, 470 encrypting, 422 importing, 30 munging, 30 optimizations for manipulating, 219 optimizing for safety and access, 449 removing old, 219 restoring deleted, 34 sharding, 310\n\ndata directories, specifying alternate, 415 data types\n\narrays, 19 basic, 16-18 comparison order, 68 dates, 18 embedded documents, 19 _id keys and ObjectIds, 20-22\n\ndatabases\n\nACID compliant, 200 authentication, 390 basics of, 10 calculating size of, 385 connecting to, 22 document-oriented, 3 durability of, 405 locking statistics on, 386 moving data into and out of, 29\n\n(see also documents)\n\nmoving with mongodump, 445 naming, 10 placing in own directory, 416 planning in application design, 220 reserved names, 11 user permissions, 390\n\n(see also security considerations)\n\nusing clusters for multiple, 335\n\ndataSize command, 365 Date(), 18 dates\n\ncreating new Date objects, 18 date type, 17 time zone information, 19\n\ndb.adminCommand(), 233 db.chunks.find(), 337 db.coll.remove(), 250 db.createCollection(), 223 db.currentOp(), 371, 374 db.fsyncLock(), 442, 442 db.getCollectionNames(), 24 db.getMongo().getDBs(), 24 db.getProfilingLevel(), 379 db.getSisterDB(), 24 db.help command, 23 db.killOp(), 375 db.serverCmdLineOpts(), 271 db.setProfilingLevel(), 376 db.shutdownServer(), 272 db.stats(), 385 db.users.find, 82 dbAdmin privileges, 390 dbAdminAnyDatabase privileges, 391 dbOwner privileges, 390 --dbpath, 415 debugging, 423 deleteMany method, 16, 33 deleteOne method, 16, 33 demormalization, 213\n\n(see also normalization and denormaliza‐\n\ntion) deployment\n\nnetwork configuration, 461 system design, 449-452 system housekeeping, 462 system settings configuration, 455-461\n\nchoosing disk scheduling algorithm, 458 disabling access time tracking, 459 disabling transparent huge pages, 458 modifying limits, 460 setting readahead, 457 turning off NUMA, 455\n\nvirtualization, 453-455 design (see application design) directories\n\nplacing databases in, 416 specifying alternate, 415 --directoryperdb, 416, 433, 443 disk scheduling algorithms, 458 disk usage, tracking, 433 disks, choosing, 449 distinct command, 159 DNS Seedlist Connection format, 228 document versioning schema design pattern,\n\n211\n\ndocument-level concurrency, 473 document-oriented databases, benefits of, 3 documents\n\nadding to collections, 14 basics of, 7 calculating size of, 379 combining multiple into single, 186 deleting from database, 16 embedded, 18 finding random, 69 inserting, 29-33 removing, 33 replacing, 35 returning updated, 49-51 updating, 35-49 updating multiple, 49, 250 _id key autogeneration, 22\n\ndraining process, 356-359 drivers, defined, 261 drop method, 34 dropIndexes command, 129 dump directory, 444 duplicates\n\nIndex\n\n|\n\n479\n\ndropping in indexes, 127 preventing in arrays, 43, 186\n\ndurability\n\nat cluster level using read concern, 408 at cluster level using write concern, 407 at member level through journaling, 405 checking for corruption, 410 defined, 200, 405 of transactions using write concern, 409 situations without guarantee, 410\n\ndynamic schemas, collections, 8 dynamic voltage and frequency scaling (DVFS),\n\n454\n\nE $each modifier, 42 EDITOR variable, 27 Elastic Block Store (EBS), 454 Elastic Compute Cloud (EC2), 454 elections\n\nelection arbiters, 246 how they work, 243 members seeking, 255 preventing, 275 preventing voting, 274 speed of, 255\n\n$elemMatch operator, 62, 64 embedded documents\n\nchanging/updating, 38 embedded document type, 18 indexing, 114 querying, 63 uses for, 19\n\nembedding, versus references, 215 --enableEncryption, 422 enableSharding command, 294 --encryptionCipherMode, 422 --encryptionKeyFile, 422 ephemeral drives, 455 exact phrase searches, 148 executionStats mode, 76 explain command, 76, 87 expressions, using array expression in project\n\nstages, 181-185\n\nextended reference schema design pattern, 210\n\nF files and filesystems\n\ncopying data files, 442\n\n480\n\n|\n\nIndex\n\ndata encryption, 422 file-based configuration, 419 filesystem snapshots, 438-442 recommended filesystem, 452 sending output to, 416 storing files with GridFS, 156-159\n\nfilter expressions, 181 $filter operator, 181 find method\n\narguments to, 53 limitations of, 55 multiple conditions, 54 querying collections, 15 specifying which keys to return, 54\n\nfindAndModify method, 49 findOne method, 15 findOneAndDelete method, 50 findOneAndReplace method, 50 findOneAndUpdate method, 50 firehose strategy, 329 firewalls, 421 $first operator, 186, 197 flushRouterConfig command, 367 force option, 420 --fork, 416 free space, tracking, 433 fs.chunks collection, 158, 329 fs.files collection, 158, 329 fsyncLock command, 442 full validate, 411 full-text indexes\n\nbenefits and drawbacks of, 147 creating, 147 optimizing, 151 phrases and logical ANDs in queries, 148 searching other languages, 151 sorting for relevance, 149 text search, 148 uses for, 146 versus MongoDB Atlas Full-Text Search\n\nIndexes, 146\n\nG $geoIntersects operator, 135, 136, 141 GeoJSON format, 133, 144 $geoNear operator, 136 geospatial indexes\n\n2d indexes, 144-146 compound, 144\n\ngeospatial query types, 134 query types and geometries in MongoDB,\n\n136 types of, 133 using, 136-143\n\n2D versus spherical geometry, 136 distortion, 137 exploring data, 139 finding all restaurants in neighborhoods,\n\n142\n\nfinding current neighborhood, 141 finding restaurants within a distance,\n\n143\n\nsearching for restaurants, 138 $geoWithin operator, 135, 136, 143, 145, 146 getCollection function, 28 getLastError function, 265 getLastErrorModes field, 266 GridFS\n\nbenefits and drawbacks of, 156 hashed shard keys for, 328 how it works, 158 keys in, 158 md5 key, 159 metadata location, 158 mongofiles utility, 156 working with from MongoDB drivers, 157\n\n$group operator, 188 $gt operator, 55 $gte operator, 55\n\nH hashed shard keys, 327-329 heartbeat requests, 253 help command, 22 hidden members, 245, 267 hint function, 124 hostManager privileges, 390 hotspot shard keys, 360 hyperthreading, 454\n\nI I/O Operations Per Second (IOPS), 454 I/O wait, 429, 431, 453 _id field, in group stages, 192 _id keys\n\nautogeneration of, 22 basics of, 20 immortal cursors, 70\n\n$in operator, 56, 114 in-place updates, 251 $inc modifier, 37, 39 $inc operator, 215 indexes\n\nbackground indexing, 131 basic operations\n\nchecking build progress, 78 choosing fields to index, 80 compound indexes, 81-104 creating, 78, 129, 131, 134 how MongoDB selects indexes, 84 index cardinality, 116, 208 objects and arrays, 114 query operators, 104-114 benefits and drawbacks of, 80 capped collections, 151-155 changing, 130 compound unique, 127 dropping duplicates in, 127 effectiveness of, 125 example of, 75 explain output\n\nexample of, 120 forcing index use, 124 important fields, 119 types of, 117 uses for, 116\n\nfor full text search, 146-151 geospatial, 133-146 GridFS file storage, 156-159 hashed, 327 hybrid index build, 131 identifying, 130 implicit, 104 metainformation on, 129 multikey, 116 partial, 128 purpose of, 75, 81 removing unneeded, 130 right-balanced, 384 selectivity of, 86, 90 supporting sort operation, 83 time-to-live (TTL), 155 types of, 126-129 unique, 126, 445 versus collection scans, 75 when not to index, 125\n\ninsert method, 33\n\nIndex\n\n|\n\n481\n\ninsertMany method, 29 insertOne method, 14, 29 inserts\n\nmultiple documents, 29 ordered versus unordered, 30 single documents, 29 validating, 32\n\ninstallation\n\ndownloading archives, 465 from package managers, 468 POSIX (Linux and Mac OS X), 467 version selection, 465 Windows, 466\n\nisMaster command, 237 isolation, 200\n\nJ joins, relational databases versus MongoDB,\n\n212, 223\n\n--journalCommitInterval, 406 journaling, 405, 473 JSON, MongoDB documents resembling, 16,\n\n469\n\n$jsonSchema operator, 223 jumbo chunks\n\nchecking chunk size, 365 defined, 364 distributing, 366 example of, 364 finding, 365 preventing, 367\n\nK Kerberos authentication, 389 Key Management Interoperability Protocol\n\n(KMIP), 422\n\nkeys\n\nadding, changing, or removing, 39 choosing key directions in compound\n\nindexes, 102\n\ndisallowed characters, 8 in documents, 7 in MongoDB, 17 limitations on, 64 shard keys, 294 specifying query returns, 54\n\nkill command, 420 killOP method, 375\n\n482\n\n|\n\nIndex\n\nL lag, calculating, 281 $last operator, 186, 197 latency\n\ncapping maximum, 459 defined, 320 firehose strategy and, 329 increased by reading from disk, 427 reading from secondaries and, 270\n\nLDAP proxy authentication, 389 limits, modifying, 460 Linux Logical Volume Manager (LVM), 439 little-endian systems, 452 load function, 24 local database, role of, 11 local.oplog.rs, 375 local.startup_log collection, 417 local.system.replset collection, 272 locking statistics, 386 logical sessions, 201 --logpath, 416, 423 logRotate command, 424 logs and logging (see also monitoring)\n\nbest practices, 423 changing log level, 423 checking log at startup, 417 debugging, 423 default options, 423 rotating logs, 424\n\nlong-running requests, 375 $lt operator, 55 $lte operator, 55\n\nM man-in-the-middle attacks, 392 manual sharding, 289, 336 many-to-many relationships, 216 mapped memory, 426 master/slave replication, 238 Math.random() function, 70 max chunk, 321 $max operator, 186 --maxConns option, 355 $maxDistance operator, 143 MaxKey constant, 297 md5 key, 159 members\n\nconfiguration options, 244-248 manipulating state, 275\n\nmember certificates, 393, 400 member states, 254 obtaining current status of, 276 starting in standalone mode, 271\n\nmemory\n\ncomputer memory basics, 426 I/O wait, 429 memory overcommitting, 453 mystery memory, 453 tracking memory usage, 426 tracking page faults, 427\n\n$merge operator, 198 $mergeObjects operator, 186 $meta operator, 150 $min operator, 186 MinKey constant, 297 $mod operator, 57 mongo shell\n\n.mongorc.js creation, 25 complex variable editing, 27 help command, 22 inconvenient collection names, 28 launching, 230 manipulating and viewing data in, 14-16 MongoDB client, 14 prompt customization, 26 prompt when connected, 293 purpose of, 13 running, 13 running scripts, 23-25 save shell helper, 48 using, 22 viewing startup log, 417\n\nMongoClient class, 261 mongod\n\nconnecting to, 22 replica set networking considerations, 229 running, 11 startup options, 415-420 stopping, 12\n\nMongoDB\n\nACID compliant transactions, 200 approach to learning, xv basic concepts, 7-11 benefits of, 3-6 consistency models, 222 data types, 16-22 getting and starting, 11 installing, 465-468\n\ninternals, 469-473 logical sessions and causal consistency in,\n\n201\n\nshell basics, 13-16 shell use, 22-28 stopping, 420 when not to use, 223\n\nMongoDB Atlas, 228, 290, 437 mongodump, 443-445 mongofiles utility, 156 mongoimport command-line tool, 30 .mongorc.js files, 25 mongorestore, 445 mongos\n\n--configdb option, 305 locating processes near shards, 306 purpose of, 290\n\nmongostat, 386 mongotop, 386 monitoring (see also logs and logging) calculating the working set, 429 memory usage, 425-429 replication, 433 services available, 425 tracking free space, 433 tracking performance, 431 moveChunk command, 337, 362 multi-hotspot strategy, 330 multiple collections, removing old data with,\n\n219\n\nMultiVersion Concurrency Control (MVCC),\n\n473\n\nmunging data, 30 mystery memory, 453\n\nN namespaces\n\nbasics of, 11, 472 filtering for operations on certain, 374\n\nnaming\n\ncollections, 9 databases, 10 dealing with inconvenient collection names,\n\n28\n\n$ne operator, 43, 55, 104 $near operator, 136 $nearSphere operator, 143 network configuration, 461 networked block stores, 454\n\nIndex\n\n|\n\n483",
      "page_number": 471
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 481-490)",
      "start_page": 481,
      "end_page": 490,
      "detection_method": "topic_boundary",
      "content": "networking\n\nfor replica sets, 229 tracking network connections, 348\n\nnew Date(), 18 new Mongo(\"hostname\"), 22 new users, adding, 391 $nin operator, 56 --nodb option, 22, 291 non-networked disks, 455 nonuniform memory architecture (NUMA),\n\n455\n\n--norc options, 291 normalization and denormalization benefits and drawbacks of, 213 cardinality, 216 data representation examples, 212 defined, 211 embedding versus references, 215 social graph data and, 216 update operators, 215 Wil Wheaton effect, 218\n\n--noscripting, 421 $not operator, 57, 105 --nounixsocket, 421 null type\n\nquerying on, 57 uses for, 17 number type, 17\n\nO object ID type, 18 Object.bsonsize(), 379 ObjectIDs\n\nbasics of, 20 storing _ids as, 379 objects, indexing, 114 one-to-many relationships, 216 one-to-one relationships, 216 operating system, selecting, 451 operations, killing, 375\n\n(see also application operations)\n\noplogs\n\navoiding out-of-sync secondaries, 253 changing size of, 251 defining size of, 292 purpose of, 249 resizing, 282 size limits, 206, 250 statement-based replication, 406\n\n484\n\n|\n\nIndex\n\nsyncing, 249\n\noplogSizeMB option, 251 Ops Manager, 228, 290, 437 $or operator, 56, 112 $out operator, 198 out-of-memory (OOM) killer, 463 outlier schema design pattern, 209 overcommitting, 453\n\nP page faults, tracking, 427, 431 partialFilterExpression, 126, 128 partitioning, 289 passive members, 244 performance, tracking, 431 (see also monitoring) periodic tasks, turning off, 463 Perl Compatible Regular Expression (PCRE)\n\nlibrary, 58 permissions, 390 phantom operations, 375 ping time, 279 polymorphic schema design pattern, 209 $pop operator, 44 --port, 416 position operator ($), 45 preallocation schema design pattern, 211 primary shards, 294 primary-secondary-arbiter (PSA) architecture,\n\n247\n\nproblematic operations, 374 production set up\n\nchecking log, 417 data encryption, 422 file-based configuration, 419 logging, 423 security, 421\n\n(see also security considerations)\n\nservers, 449 SSL connections, 423 starting from command line, 415-420 stopping MongoDB, 420\n\n--profile level, 379 $project operator, 169-174 public key infrastructure (PKI) standard, 390 publication/subscription systems, 216 $pull operator, 44 $push operator, 41, 186, 195 PyMongo, 157\n\nQ queries\n\n$where queries, 65 across multiple shards, 297 covered queries, 103 criteria for, 55-57 cursors for\n\navoiding large skips, 68 benefits of, 66 creating, 66 immortal cursors, 70 iterating through results, 66 limits, skips, and sorts, 67 sending queries to servers, 67\n\ndot notation and, 63 enabling efficient, 77 equality queries, 82 explain cursor method, 76 find method, 53-55 limitations, 55 multivalue queries, 83 OR queries, 56 overview of, 53 range queries, 55, 62, 82 scatter-gather (broadcast), 301 shape of, 84 targeted, 301 type-specific\n\narrays, 59-63 embedded documents, 63 null, 57 regular expressions, 58\n\nquery conditionals, 55 query documents, 15 query operators\n\ninefficient, 104 OR, 112 ranges, 105 query patterns defined, 77 indexes based on two or more keys, 81\n\nquestions and comments, xviii queueing, 432 --quiet option, 24\n\nR RAFT consensus protocol, 243 RAID (redundant array of independent disk),\n\n450\n\nRAM (random-access memory), 449 random numbers, creating, 70 read concern, 408 read privileges, 390 readahead, 457 readAnyDatabase privileges, 391 readConcern option, 221 reads\n\noptimizing, 219 sending to secondaries, 268-270\n\nreadWrite privileges, 390 readWriteAnyDatabase privileges, 391 reconfig command, 274 regular expressions\n\nprefix expressions, 58 querying with, 18, 58\n\nremove method, 34 removeShard command, 356 replaceOne method, 35 replica sets, components of\n\nbackups, 446 elections, 255 heartbeats, 253 member states, 254 rollbacks, 255-259 syncing, 249-253\n\nreplica sets, configuration of\n\nadding new set members, 273 changing hostnames, 273 changing set members, 273 configuration document, 272 creating larger sets, 274 creating replica sets, 272 forcing reconfiguration, 274 network configuration, 461 removing set members, 273 restrictions on changing, 273\n\nreplica sets, connecting to\n\nclient-to-replica set behavior, 261 custom replication guarantees, 265-268 purpose of replica sets, 262 retry strategy, 262 sending reads to secondaries, 268-270 waiting for replication on writes, 263\n\nreplica sets, setting up\n\nadding new members, 238 benefits of replica sets, 227 best practices, 228 changing configuration, 238-241\n\nIndex\n\n|\n\n485\n\nchecking for reconfiguration success, 238 common configurations, 242 configuration document, 230 designing sets, 241-243 how elections work, 243 key concepts, 238 listing and sending members, 230 localhost versus non-localhost servers, 231 majority of the set, 241 member configuration options\n\nbuilding indexes, 247 election arbiters, 246 hidden members, 245 priority, 244 specifying, 244 minority of the set, 242 modifying existing members, 240 networking considerations, 229 number of primaries, 243 observing replication, 233-238 removing members, 238 security considerations, 230 standalone server conversion, 231 test replica sets, 228 viewing status of, 231 replica sets, sharding and\n\nadding shards from replica sets, 306-310 initiating config servers as, 304 role of config servers, 304\n\nReplicaSetMonitor, 354 replication\n\nautomatic replication chaining, 279 budget approach to, 285 custom guarantees, 265-268 master/slave, 238 monitoring\n\nbuilding indexes, 283 calculating lag, 281 disabling chaining, 281 lag and oplog length, 433 obtaining current information, 276 replication graph, 279 replication loops, 280 resizing oplogs, 282 useful fields, 278 using logs for, 275\n\nobserving, 233-238 purpose of, 227, 249 replication protocol, 243\n\n486\n\n|\n\nIndex\n\nreplication thread, 375 statement-based, 406 versus sharding, 290 waiting for on writes, 263\n\nreplSet ROLLBACK, 258 replSetGetStatus command, 276 replSetReconfig command, 272 replSetSyncFrom command, 280 resident memory, 426 restore privileges, 391 retry logic, 200 retry-at-most-once strategy, 263 retryable writes option, 263 right-balanced indexes, 384 rollbacks\n\napplying operations to current primary, 258 avoiding, 263 defined, 257 failed, 259 loading documents into main collection,\n\n258\n\nmanipulating member votes, 258 ROLLBACK state, 258\n\nroot privileges, 391 rs global variable, 233 rs helper functions, 233, 272 rs.add command, 238, 273 rs.config(), 238, 245 rs.help(), 233 rs.initiate(), 230, 272, 304 rs.initiate(config), 233 rs.printReplicationInfo(), 281 rs.printSlaveReplicationInfo(), 281 rs.reconfig(), 240, 274 rs.remove command, 273 rs.status(), 231, 245, 276, 306 rs.syncFrom(), 280\n\nS sanity checks, 243 save shell helper, 48 scatter-gather (broadcast) queries, 301 schemas\n\ndesign considerations, 207 design patterns, 208-211 managing, 223 migrating, 35, 222 trade-off between efficient reads and writes,\n\n219\n\nupdating, 37\n\nSCRAM (Salted Challenge Response Authenti‐\n\ncation Mechanism), 389\n\nscripts\n\n.mongorc.js files for frequently loaded, 25 running with mongo shell, 23-25\n\nsecurity considerations\n\nauthentication mechanisms, 389 authorization, 390 config file options, 421 firewalls, 421 replica sets set up, 230 security.authorization setting, 392 tutorial\n\nbringing up replica set, 401 creating admin user, 402 establishing CA, 395-400 generating/signing client certificates, 401 generating/signing member certificates,\n\n400\n\nrestarting replica set, 403\n\nx.509 certificates, 392-395\n\nseed lists, 261 server administration backups, 437-447 deployment, 449-463 in sharded clusters, 356-359 monitoring, 425-436 production set up, 415-424\n\nserver discovery and monitoring (SDAM) spec‐\n\nification, 262\n\nservers\n\nbacking up, 438-445 config servers, 304 connecting to, 22 durability during failures, 405 forking server processes, 416 network configuration, 461 printing statistics about, 386 setting up for production, 449 shutting down, 420 standalone mode, 271, 356\n\n$set modifier, 37 $set operator, 215 $setOnInsert modifier, 48 setParameter command, 423 setProfilingLevel, 423 sh global variable, 294 sh.addShard() method, 309\n\nsh.addShardToZone(), 335 sh.help(), 294 sh.moveChunk(), 337 sh.status(), 294, 339 sh.status(true), 341 sh.stopBalancer(), 336 shard keys, choosing cardinality, 334 controlling data distribution, 334-337 distribution types\n\nascending shard keys, 320 location-based shard keys, 325 possibly types, 320 randomly distributed shard keys, 323\n\nhotspot shard keys, 360 initial steps, 319 limitations on shard keys, 328, 334 rules and guidelines, 334 shard key strategies\n\nfirehose strategy, 329 hashed shard keys, 327 hashed shard keys for GridFS, 328 multi-hotspot strategy, 330\n\nshardCollection command, 327 sharding, administration of\n\nbalancing data\n\nchanging chunk size, 361 jumbo chunks, 364-367 moving chunks, 362 turning balancer off, 360 refreshing configurations, 367 seeing current state\n\nconfiguration information, 341-348 status overview, 339 writeback listener, 375\n\nserver administration adding servers, 356 changing servers in shards, 356 removing shards, 356-359 tracking network connections\n\nconnection statistics, 348-354 limiting number of connections, 354\n\nsharding, basics of\n\nautosharding, 289, 337 benefits of sharding, 289 complexity of sharding, 290 compound shard keys, 313, 330 definition of sharding, 289 goals of sharding, 290\n\nIndex\n\n|\n\n487\n\nmanual sharding, 289, 336 number of concurrent migrations allowed,\n\n316\n\nprimary shards, 294 primary use case for, 292 processes involved, 304 querying, 297 removing shards, 356-359 sh global variable, 294 shard keys, 294-301\n\n(see also shard keys, choosing) sharding on single-machine clusters,\n\n291-301\n\nsharding versus replication, 290 uses for sharding, 303 when to shard, 303 sharding, configuration of\n\nbackups, 446 balancers, 316 change streams, 317 collations, 317 how MongoDB tracks cluster data, 311-315 network configuration, 461 processes involved, 304 starting the servers\n\nadding capacity, 310 adding shards from replica sets, 306-310 config servers, 304 mongo processes, 305 sharding data, 310 uses for sharding, 303 when to shard, 303 sharding.clusterRole, 308 ShardingTest class, 291 --shardsvr option, 308 shell (see mongo shell) show collections command, 24 show dbs command, 24 shutdown command, 420 SIGINT signal, 420 SIGTERM signal, 420 $size operator, 60, 185 skips\n\navoiding large, 68 finding random documents, 69 paginating results without, 69 skipping query results, 67\n\n$slice modifier, 42 $slice operator, 60, 185\n\n488\n\n|\n\nIndex\n\nslow application operations, 371, 376 --slowms, 379, 424 snapshots, 438-442 social graph data, 216 $sort modifier, 42 space, tracking disk usage, 433 split storms, 315 SSD (solid state drives), 449 SSL connections, 423 st.stop(), 301 staleness, handling, 253 standalone mode, 271, 356 statement-based replication, 406 stats function, 380 stats, printing, 386\n\n(see also application operations)\n\nstdout, 423 storage engine, default, 472 storage medium, choosing, 449 string type, 17 subcollections, 10 subset schema design pattern, 210 $sum operator, 186 swap space, allowance for, 452 syncing\n\ncloning and working sets, 252 handling staleness, 253 initial sync, 251 oplog size, 250 replication, 253 role of oplogs in, 249 system profiler, 376-379 system.indexes collection, 129\n\nT tailable cursors, 154 targeted queries, 301 TCP/IP wire protocol, 470 $text operator, 148 time-to-live (TTL) collections, removing old\n\ndata with, 219\n\ntime-to-live (TTL) indexes\n\ncreating, 155 uses for, 155 versus capped collections, 153\n\nTLS/SSL encryption, 423 --tlsMode, 423 top Unix utility, 386 transactions\n\nACID definition, 200 defined, 199 MongoDB versions and drivers supporting,\n\n199\n\ntuning transaction limits, 205-206 using, 200-205\n\ntransparent huge pages (THP), 458 transport encryption, 423 tree schema design pattern, 210 two-member-plus-arbiter scenario, 247\n\nU $unset modifier, 38 $unwind operator, 174-181 update operators\n\narray operators, 41-46 decrementing values, 40 idempotent, 215 incrementing values, 37, 39 removing field values, 38 setting field values, 37 uses for, 37\n\nupdateMany method, 47, 49 updateOne method, 16, 47 updates\n\natomic, 35 configuration document, 272 in-place updates, 251 methods available, 35 multiple documents, 49, 250 replacing documents, 35 returning updated documents, 49 using update operators, 37-46 using upserts, 46\n\nupserts, 46 use video command, 24 user-defined roles, 391 userAdmin privileges, 390 userAdminAnyDatabase privileges, 391 users, adding, 391\n\nV validate command, 410 validator option, 223 versioning scheme, 465 virtual memory, 426 virtualization\n\nbenefits and drawbacks of, 453 memory overcommitting, 453 mystery memory, 453 network disk I/O issues, 453\n\nvolume-level backup, 438\n\nW warnings, in startup banner, 417 WGS84 datum, 133 $where clauses, 65 Wil Wheaton effect, 218 wire protocol, 470 WiredTiger storage engine, 472 working set\n\ncalculating size of, 429 examples, 431\n\nwrite concern\n\nassuring propagation to majority of mem‐\n\nbers, 264\n\ndurability of transactions using, 409\n\nwrite-ahead logs (WAL), 405 writebacklistener commands, 375 writeConcern option, 221, 407 writes\n\nacknowledged, 376 optimizing, 219 waiting for replication on, 263\n\nX x.509 certificate authentication\n\nsupport for, 389 using, 392-395 XFS filesystem, 452\n\nIndex\n\n|\n\n489\n\nAbout the Authors\n\nShannon Bradshaw is VP of education at MongoDB. Shannon manages the Mon‐ goDB Documentation and MongoDB University teams. These teams develop and maintain the majority of MongoDB learning resources used by the MongoDB com‐ munity. Shannon holds a PhD in computer science from Northwestern University. Prior to MongoDB, Shannon was a computer science professor specializing in infor‐ mation systems and human-information interaction.\n\nEoin Brazil is a senior curriculum engineer at MongoDB. He works on online and instructor-led training products delivered through MongoDB University and previ‐ ously held various positions in the technical services support organization within MongoDB. Eoin holds a PhD and a MSc in computer science from the University of Limerick and a PgDip in technology commercialization from the National University of Ireland, Galway. Prior to MongoDB, he led teams in mobile services and in high- performance computing in the academic research sector.\n\nKristina Chodorow is a software engineer who worked on the MongoDB core for five years. She led MongoDB’s replica set development as well as writing the PHP and Perl drivers. She has given talks on MongoDB at meetups and conferences around the world and maintains a blog on technical topics at http://www.kchodorow.com. She currently works at Google.\n\nColophon\n\nThe animal on the cover of MongoDB: The Definitive Guide, Third Edition, is a mon‐ goose lemur, a member of a highly diverse group of primates endemic to Madagascar. Ancestral lemurs are believed to have inadvertently traveled to Madagascar from Africa (a trip of at least 350 miles) by raft some 65 million years ago. Freed from com‐ petition with other African species (such as monkeys and squirrels), lemurs adapted to fill a wide variety of ecological niches, branching into the almost 100 species known today. These animals’ otherworldly calls, nocturnal activity, and glowing eyes earned them their name, which comes from the lemures (specters) of Roman myth. Malagasy culture also associates lemurs with the supernatural, variously considering them the souls of ancestors, the source of taboo, or spirits bent on revenge. Some vil‐ lages identify a particular species of lemur as the ancestor of their group.\n\nMongoose lemurs (Eulemur mongoz) are medium-sized lemurs, about 12 to 18 inches long and 3 to 4 pounds. The bushy tail adds an additional 16 to 25 inches. Females and young lemurs have white beards, while males have red beards and cheeks. Mon‐ goose lemurs eat fruit and flowers and they act as pollinators for some plants; they are particularly fond of the nectar of the kapok tree. They may also eat leaves and insects.\n\nMongoose lemurs inhabit the dry forests of northwestern Madagascar. One of the two species of lemur found outside of Madagascar, they also live in the Comoros Islands (where they are believed to have been introduced by humans). They have the unusual quality of being cathemeral (alternately wakeful during the day and at night), chang‐ ing their activity patterns to suit the wet and dry seasons. Mongoose lemurs are threatened by habitat loss and they are classified as a vulnerable species.\n\nMany of the animals on O’Reilly covers are endangered; all of them are important to the world.\n\nThe cover illustration is by Karen Montgomery, based on a black and white engraving from Lydekker’s Royal Natural History. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n\nThere’s much more where this came from.\n\nExperience books, videos, live online training courses, and more from O’Reilly and our 200+ partners—all in one place.\n\nLearn more at oreilly.com/online-learning\n\n5 7 1\n\nc n\n\na d e M y\n\ni\n\nl l i\n\ne R O\n\n’\n\nf o k r a m e d a r t d e r e t s g e r\n\ni\n\na\n\ns\n\ny\n\nl l i\n\ne R O\n\n’\n\nc n\n\na d e M y\n\ni\n\nl l i\n\ne R O 9 1\n\n’\n\n0 2 ©",
      "page_number": 481
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 491-499)",
      "start_page": 491,
      "end_page": 499,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 491
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 500-508)",
      "start_page": 500,
      "end_page": 508,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 500
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 509-514)",
      "start_page": 509,
      "end_page": 514,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 509
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Third Edition\n\nMongoDB MongoDB The Deﬁ nitive Guide The Deﬁ nitive Guide Powerful and Scalable Data Storage Powerful and Scalable Data Storage\n\nShannon Bradshaw, Eoin Brazil Shannon Bradshaw, Eoin Brazil & Kristina Chodorow",
      "content_length": 223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "THIRD EDITION\n\nMongoDB: The Definitive Guide Powerful and Scalable Data Storage\n\nShannon Bradshaw, Eoin Brazil, and Kristina Chodorow\n\nBeijing Beijing\n\nBoston Boston\n\nFarnham Sebastopol Farnham Sebastopol\n\nTokyo Tokyo",
      "content_length": 217,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "MongoDB: The Definitive Guide by Shannon Bradshaw, Eoin Brazil, and Kristina Chodorow\n\nCopyright © 2020 Shannon Bradshaw and Eoin Brazil. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nEditor: Nicole Taché Production Editor: Kristen Brown Copyeditor: Rachel Head Proofreader: Christina Edwards\n\nIndexer: Judith McConville Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Rebecca Demarest\n\nSeptember 2010: May 2013: December 2019:\n\nFirst Edition Second Edition Third Edition\n\nRevision History for the Third Edition 2019-12-09: First Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491954461 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. MongoDB: The Definitive Guide, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-491-95446-1\n\n[LSI]",
      "content_length": 1917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "This book is dedicated to our families for the time, space, and support they provided to make our work on this book possible and for their love.\n\nFor Anna, Sigourney, Graham, and Beckett. —Shannon\n\nAnd for Gemma, Clodagh, and Bronagh. —Eoin",
      "content_length": 240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Table of Contents\n\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv\n\nPart I.\n\nIntroduction to MongoDB\n\n1.\n\nIntroduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Ease of Use 3 Designed to Scale 4 Rich with Features… 5 …Without Sacrificing Speed 6 The Philosophy 6\n\n2. Getting Started. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Documents 7 Collections 8 Dynamic Schemas 8 Naming 9 Databases 10 Getting and Starting MongoDB 11 Introduction to the MongoDB Shell 13 Running the Shell 13 A MongoDB Client 14 Basic Operations with the Shell 14 Data Types 16 Basic Data Types 16 Dates 18 v",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Arrays 19 Embedded Documents 19 _id and ObjectIds 20 Using the MongoDB Shell 22 Tips for Using the Shell 22 Running Scripts with the Shell 23 Creating a .mongorc.js 25 Customizing Your Prompt 26 Editing Complex Variables 27 Inconvenient Collection Names 28\n\n3. Creating, Updating, and Deleting Documents. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Inserting Documents 29 insertMany 29 Insert Validation 32 insert 33 Removing Documents 33 drop 34 Updating Documents 35 Document Replacement 35 Using Update Operators 37 Upserts 46 Updating Multiple Documents 49 Returning Updated Documents 49\n\n4. Querying. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 Introduction to find 53 Specifying Which Keys to Return 54 Limitations 55 Query Criteria 55 Query Conditionals 55 OR Queries 56 $not 57 Type-Specific Queries 57 null 57 Regular Expressions 58 Querying Arrays 59 Querying on Embedded Documents 63 $where Queries 65 Cursors 66 Table of Contents\n\n4. Querying. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 Introduction to find 53 Specifying Which Keys to Return 54 Limitations 55 Query Criteria 55 Query Conditionals 55 OR Queries 56 $not 57 Type-Specific Queries 57 null 57 Regular Expressions 58 Querying Arrays 59 Querying on Embedded Documents 63 $where Queries 65 Cursors 66 Table of Contents",
      "content_length": 1488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Limits, Skips, and Sorts 67 Avoiding Large Skips 68 Immortal Cursors 70\n\nPart II. Designing Your Application\n\n5.\n\nIndexes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 Introduction to Indexes 75 Creating an Index 78 Introduction to Compound Indexes 81 How MongoDB Selects an Index 84 Using Compound Indexes 85 How $ Operators Use Indexes 104 Indexing Objects and Arrays 114 Index Cardinality 116 explain Output 116 When Not to Index 125 Types of Indexes 126 Unique Indexes 126 Partial Indexes 128 Index Administration 129 Identifying Indexes 130 Changing Indexes 130\n\n6. Special Index and Collection Types. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 Geospatial Indexes 133 Types of Geospatial Queries 134 Using Geospatial Indexes 136 Compound Geospatial Indexes 144 2d Indexes 144 Indexes for Full Text Search 146 Creating a Text Index 147 Text Search 148 Optimizing Full-Text Search 151 Searching in Other Languages 151 Capped Collections 151 Creating Capped Collections 154 Tailable Cursors 154 Time-To-Live Indexes 155 vii",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Storing Files with GridFS 156 Getting Started with GridFS: mongofiles 156 Working with GridFS from the MongoDB Drivers 157 Under the Hood 158\n\n7.\n\nIntroduction to the Aggregation Framework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 Pipelines, Stages, and Tunables 161 Getting Started with Stages: Familiar Operations 163 Expressions 168 $project 169 $unwind 174 Array Expressions 181 Accumulators 186 Using Accumulators in Project Stages 186 Introduction to Grouping 187 The _id Field in Group Stages 192 Group Versus Project 195 Writing Aggregation Pipeline Results to a Collection 198\n\n8. Transactions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 Introduction to Transactions 199 A Definition of ACID 200 How to Use Transactions 200 Tuning Transaction Limits for Your Application 205 Timing and Oplog Size Limits 205\n\n9. Application Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 Schema Design Considerations 207 Schema Design Patterns 208 Normalization Versus Denormalization 211 Examples of Data Representations 212 Cardinality 216 Friends, Followers, and Other Inconveniences 216 Optimizations for Data Manipulation 219 Removing Old Data 219 Planning Out Databases and Collections 220 Managing Consistency 221 Migrating Schemas 222 Managing Schemas 223 When Not to Use MongoDB 223\n\nviii\n\n|\n\nTable of Contents",
      "content_length": 1492,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Part III. Replication\n\n10. Setting Up a Replica Set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 Introduction to Replication 227 Setting Up a Replica Set, Part 1 228 Networking Considerations 229 Security Considerations 230 Setting Up a Replica Set, Part 2 230 Observing Replication 233 Changing Your Replica Set Configuration 238 How to Design a Set 241 How Elections Work 243 Member Configuration Options 244 Priority 244 Hidden Members 245 Election Arbiters 246 Building Indexes 247\n\n11. Components of a Replica Set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 Syncing 249 Initial Sync 251 Replication 253 Handling Staleness 253 Heartbeats 253 Member States 254 Elections 255 Rollbacks 255 When Rollbacks Fail 259\n\n12. Connecting to a Replica Set from Your Application. . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 Client−to−Replica Set Connection Behavior 261 Waiting for Replication on Writes 263 Other Options for “w” 265 Custom Replication Guarantees 265 Guaranteeing One Server per Data Center 265 Guaranteeing a Majority of Nonhidden Members 267 Creating Other Guarantees 267 Sending Reads to Secondaries 268 Consistency Considerations 268\n\nTable of Contents\n\n|\n\nix",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Load Considerations 269 Reasons to Read from Secondaries 269\n\n13. Administration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271 Starting Members in Standalone Mode 271 Replica Set Configuration 272 Creating a Replica Set 272 Changing Set Members 273 Creating Larger Sets 274 Forcing Reconfiguration 274 Manipulating Member State 275 Turning Primaries into Secondaries 275 Preventing Elections 275 Monitoring Replication 275 Getting the Status 276 Visualizing the Replication Graph 279 Replication Loops 280 Disabling Chaining 281 Calculating Lag 281 Resizing the Oplog 282 Building Indexes 283 Replication on a Budget 285\n\nPart IV.\n\nSharding\n\n14.\n\nIntroduction to Sharding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289 What Is Sharding? 289 Understanding the Components of a Cluster 290 Sharding on a Single-Machine Cluster 291\n\n15. Configuring Sharding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303 When to Shard 303 Starting the Servers 304 Config Servers 304 The mongos Processes 305 Adding a Shard from a Replica Set 306 Adding Capacity 310 Sharding Data 310 How MongoDB Tracks Cluster Data 311\n\nx\n\n|\n\nTable of Contents",
      "content_length": 1315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Chunk Ranges 312 Splitting Chunks 314 The Balancer 316 Collations 317 Change Streams 317\n\n16. Choosing a Shard Key. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319 Taking Stock of Your Usage 319 Picturing Distributions 320 Ascending Shard Keys 320 Randomly Distributed Shard Keys 323 Location-Based Shard Keys 325 Shard Key Strategies 327 Hashed Shard Key 327 Hashed Shard Keys for GridFS 328 The Firehose Strategy 329 Multi-Hotspot 330 Shard Key Rules and Guidelines 334 Shard Key Limitations 334 Shard Key Cardinality 334 Controlling Data Distribution 334 Using a Cluster for Multiple Databases and Collections 335 Manual Sharding 336\n\n17. Sharding Administration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339 Seeing the Current State 339 Getting a Summary with sh.status() 339 Seeing Configuration Information 341 Tracking Network Connections 348 Getting Connection Statistics 348 Limiting the Number of Connections 354 Server Administration 356 Adding Servers 356 Changing Servers in a Shard 356 Removing a Shard 356 Balancing Data 359 The Balancer 360 Changing Chunk Size 361 Moving Chunks 362 Jumbo Chunks 364 xi",
      "content_length": 1235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Refreshing Configurations 367\n\nPart V. Application Administration\n\n18. Seeing What Your Application Is Doing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371 Seeing the Current Operations 371 Finding Problematic Operations 374 Killing Operations 375 False Positives 375 Preventing Phantom Operations 375 Using the System Profiler 376 Calculating Sizes 379 Documents 379 Collections 380 Databases 385 Using mongotop and mongostat 386\n\n19. An Introduction to MongoDB Security. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389 MongoDB Authentication and Authorization 389 Authentication Mechanisms 389 Authorization 390 Using x.509 Certificates to Authenticate Both Members and Clients 392 A Tutorial on MongoDB Authentication and Transport Layer Encryption 395 Establish a CA 395 Generate and Sign Member Certificates 400 Generate and Sign Client Certificates 401 Bring Up the Replica Set Without Authentication and Authorization Enabled 401 Create the Admin User 402 Restart the Replica Set with Authentication and Authorization Enabled 403\n\n20. Durability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405 Durability at the Member Level Through Journaling 405 Durability at the Cluster Level Using Write Concern 407 The w and wtimeout Options for writeConcern 407 The j (Journaling) Option for writeConcern 408 Durability at a Cluster Level Using Read Concern 408 Durability of Transactions Using a Write Concern 409 What MongoDB Does Not Guarantee 410\n\nxii\n\n|\n\nTable of Contents",
      "content_length": 1619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Checking for Corruption 410\n\nPart VI.\n\nServer Administration\n\n21. Setting Up MongoDB in Production. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 Starting from the Command Line 415 File-Based Configuration 419 Stopping MongoDB 420 Security 421 Data Encryption 422 SSL Connections 423 Logging 423\n\n22. Monitoring MongoDB. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425 Monitoring Memory Usage 425 Introduction to Computer Memory 426 Tracking Memory Usage 426 Tracking Page Faults 427 I/O Wait 429 Calculating the Working Set 429 Some Working Set Examples 431 Tracking Performance 431 Tracking Free Space 433 Monitoring Replication 433\n\n23. Making Backups. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437 Backup Methods 437 Backing Up a Server 438 Filesystem Snapshot 438 Copying Data Files 442 Using mongodump 443 Specific Considerations for Replica Sets 446 Specific Considerations for Sharded Clusters 446 Backing Up and Restoring an Entire Cluster 447 Backing Up and Restoring a Single Shard 447\n\n24. Deploying MongoDB. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449 Designing the System 449 Choosing a Storage Medium 449\n\nTable of Contents\n\n|\n\nxiii",
      "content_length": 1381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Recommended RAID Configurations 450 CPU 451 Operating System 451 Swap Space 452 Filesystem 452 Virtualization 453 Memory Overcommitting 453 Mystery Memory 453 Handling Network Disk I/O Issues 453 Using Non-Networked Disks 455 Configuring System Settings 455 Turning Off NUMA 455 Setting Readahead 457 Disabling Transparent Huge Pages (THP) 458 Choosing a Disk Scheduling Algorithm 458 Disabling Access Time Tracking 459 Modifying Limits 460 Configuring Your Network 461 System Housekeeping 462 Synchronizing Clocks 462 The OOM Killer 463 Turn Off Periodic Tasks 463\n\nA. Installing MongoDB. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465\n\nB. MongoDB Internals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469\n\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475\n\nxiv\n\n|\n\nTable of Contents",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Preface\n\nHow This Book Is Organized This book is split up into six sections, covering development, administration, and deployment information.\n\nGetting Started with MongoDB In Chapter 1 we provide background on MongoDB: why it was created, the goals it is trying to accomplish, and why you might choose to use it for a project. We go into more detail in Chapter 2, which provides an introduction to the core concepts and vocabulary of MongoDB. Chapter 2 also provides a first look at working with Mon‐ goDB, getting you started with the database and the shell. The next two chapters cover the basic material that developers need to know to work with MongoDB. In Chapter 3, we describe how to perform those basic write operations, including how to do them with different levels of safety and speed. Chapter 4 explains how to find documents and create complex queries. This chapter also covers how to iterate through results and gives options for limiting, skipping, and sorting results.\n\nDeveloping with MongoDB Chapter 5 covers what indexing is and how to index your MongoDB collections. Chapter 6 explains how to use several special types of indexes and collections. Chap‐ ter 7 covers a number of techniques for aggregating data with MongoDB, including counting, finding distinct values, grouping documents, the aggregation framework, and writing these results to a collection. Chapter 8 introduces transactions: what they are, how best to use them for your application, and how to tune. Finally, this section finishes with a chapter on designing your application: Chapter 9 goes over tips for writing an application that works well with MongoDB.\n\nxv",
      "content_length": 1652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Replication The replication section starts with Chapter 10, which gives you a quick way to set up a replica set locally and covers many of the available configuration options. Chap‐ ter 11 then covers the various concepts related to replication. Chapter 12 shows how replication interacts with your application and Chapter 13 covers the administrative aspects of running a replica set.\n\nSharding The sharding section starts in Chapter 14 with a quick local setup. Chapter 15 then gives an overview of the components of the cluster and how to set them up. Chap‐ ter 16 has advice on choosing a shard key for a variety of applications. Finally, Chap‐ ter 17 covers administering a sharded cluster.\n\nApplication Administration The next two chapters cover many aspects of MongoDB administration from the per‐ spective of your application. Chapter 18 discusses how to introspect what MongoDB is doing. Chapter 19 covers security in MongoDb and how to configure authentica‐ tion as well as authorization for your deployment. Chapter 20 explains how Mon‐ goDB stores data durably.\n\nServer Administration The final section is focused on server administration. Chapter 21 covers common options when starting and stopping MongoDB. Chapter 22 discusses what to look for and how to read stats when monitoring. Chapter 23 describes how to take and restore backups for each type of deployment. Finally, Chapter 24 discusses a number of sys‐ tem settings to keep in mind when deploying MongoDB.\n\nAppendixes Appendix A explains MongoDB’s versioning scheme and how to install it on Win‐ dows, OS X, and Linux. Appendix B details how MongoDB works internally: its stor‐ age engine, data format, and wire protocol.\n\nConventions Used in This Book The following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, collection names, database names, filenames, and file extensions.\n\nxvi\n\n| Preface",
      "content_length": 1924,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Constant width\n\nUsed for program listings, as well as within paragraphs to refer to program ele‐ ments such as variable or function names, command-line utilities, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nConstant width italic\n\nShows text that should be replaced with user-supplied values or by values deter‐ mined by context.\n\nThis element signifies a tip or suggestion.\n\nThis element signifies a general note.\n\nThis element indicates a warning or caution.\n\nUsing Code Examples Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/mongodb-the-definitive-guide-3e/mongodb-the-definitive-guide-3e.\n\nIf you have a technical question or a problem using the code examples, please send email to bookquestions@oreilly.com.\n\nThis book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of\n\nPreface\n\n|\n\nxvii",
      "content_length": 1468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "example code from this book into your product’s documentation does require per‐ mission.\n\nWe appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “MongoDB: The Defini‐ tive Guide, Third Edition by Shannon Bradshaw, Eoin Brazil, and Kristina Chodorow (O’Reilly). Copyright 2020 Shannon Bradshaw and Eoin Brazil, 978-1-491-95446-1.”\n\nIf you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.\n\nO’Reilly Online Learning\n\nFor more than 40 years, O’Reilly Media has provided technol‐ ogy and business training, knowledge, and insight to help companies succeed.\n\nOur unique network of experts and innovators share their knowledge and expertise through books, articles, conferences, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in- depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, please visit http://oreilly.com.\n\nHow to Contact Us Please address comments and questions concerning this book to the publisher:\n\nO’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax)\n\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/mongoDB_TDG_3e.\n\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\n\nFor more information about our books, courses, conferences, and news, see our web‐ site at http://www.oreilly.com.\n\nFind us on Facebook: http://facebook.com/oreilly\n\nxviii\n\n| Preface",
      "content_length": 1870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Follow us on Twitter: http://twitter.com/oreillymedia\n\nWatch us on YouTube: http://www.youtube.com/oreillymedia\n\nPreface\n\n|\n\nxix",
      "content_length": 128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "PART I Introduction to MongoDB",
      "content_length": 30,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "CHAPTER 1 Introduction\n\nMongoDB is a powerful, flexible, and scalable general-purpose database. It combines the ability to scale out with features such as secondary indexes, range queries, sorting, aggregations, and geospatial indexes. This chapter covers the major design decisions that made MongoDB what it is.\n\nEase of Use MongoDB is a document-oriented database, not a relational one. The primary reason for moving away from the relational model is to make scaling out easier, but there are some other advantages as well.\n\nA document-oriented database replaces the concept of a “row” with a more flexible model, the “document.” By allowing embedded documents and arrays, the document- oriented approach makes it possible to represent complex hierarchical relationships with a single record. This fits naturally into the way developers in modern object- oriented languages think about their data.\n\nThere are also no predefined schemas: a document’s keys and values are not of fixed types or sizes. Without a fixed schema, adding or removing fields as needed becomes easier. Generally, this makes development faster as developers can quickly iterate. It is also easier to experiment. Developers can try dozens of models for the data and then choose the best one to pursue.\n\n3",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Designed to Scale Dataset sizes for applications are growing at an incredible pace. Increases in available bandwidth and cheap storage have created an environment where even small-scale applications need to store more data than many databases were meant to handle. A terabyte of data, once an unheard-of amount of information, is now commonplace.\n\nAs the amount of data that developers need to store grows, developers face a difficult decision: how should they scale their databases? Scaling a database comes down to the choice between scaling up (getting a bigger machine) or scaling out (partitioning data across more machines). Scaling up is often the path of least resistance, but it has drawbacks: large machines are often very expensive, and eventually a physical limit is reached where a more powerful machine cannot be purchased at any cost. The alter‐ native is to scale out: to add storage space or increase throughput for read and write operations, buy additional servers, and add them to your cluster. This is both cheaper and more scalable; however, it is more difficult to administer a thousand machines than it is to care for one.\n\nMongoDB was designed to scale out. The document-oriented data model makes it easier to split data across multiple servers. MongoDB automatically takes care of bal‐ ancing data and load across a cluster, redistributing documents automatically and routing reads and writes to the correct machines, as shown in Figure 1-1.\n\nFigure 1-1. Scaling out MongoDB using sharding across multiple servers\n\nThe topology of a MongoDB cluster, or whether there is in fact a cluster rather than a single node at the other end of a database connection, is transparent to the applica‐ tion. This allows developers to focus on programming the application, not scaling it.\n\n4\n\n|\n\nChapter 1: Introduction",
      "content_length": 1829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Likewise, if the topology of an existing deployment needs to change in order to, for example, scale to support greater load, the application logic can remain the same.\n\nRich with Features… MongoDB is a general-purpose database, so aside from creating, reading, updating, and deleting data, it provides most of the features you would expect from a database management system and many others that set it apart. These include:\n\nIndexing\n\nMongoDB supports generic secondary indexes and provides unique, compound, geospatial, and full-text indexing capabilities as well. Secondary indexes on hier‐ archical structures such as nested documents and arrays are also supported and enable developers to take full advantage of the ability to model in ways that best suit their applications.\n\nAggregation\n\nMongoDB provides an aggregation framework based on the concept of data pro‐ cessing pipelines. Aggregation pipelines allow you to build complex analytics engines by processing data through a series of relatively simple stages on the server side, taking full advantage of database optimizations.\n\nSpecial collection and index types\n\nMongoDB supports time-to-live (TTL) collections for data that should expire at a certain time, such as sessions and fixed-size (capped) collections, for holding recent data, such as logs. MongoDB also supports partial indexes limited to only those documents matching a criteria filter in order to increase efficiency and reduce the amount of storage space required.\n\nFile storage\n\nMongoDB supports an easy-to-use protocol for storing large files and file metadata.\n\nSome features common to relational databases are not present in MongoDB, notably complex joins. MongoDB supports joins in a very limited way through use of the $lookup aggregation operator introduced in the 3.2 release. In the 3.6 release, more complex joins are possible using multiple join conditions as well as unrelated subqu‐ eries. MongoDB’s treatment of joins were architectural decisions to allow for greater scalability, because both of those features are difficult to provide efficiently in a dis‐ tributed system.\n\nRich with Features…\n\n|\n\n5",
      "content_length": 2143,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "…Without Sacrificing Speed Performance is a driving objective for MongoDB, and has shaped much of its design. It uses opportunistic locking in its WiredTiger storage engine to maximize concur‐ rency and throughput. It uses as much RAM as it can as its cache and attempts to automatically choose the correct indexes for queries. In short, almost every aspect of MongoDB was designed to maintain high performance.\n\nAlthough MongoDB is powerful, incorporating many features from relational sys‐ tems, it is not intended to do everything that a relational database does. For some functionality, the database server offloads processing and logic to the client side (han‐ dled either by the drivers or by a user’s application code). Its maintenance of this streamlined design is one of the reasons MongoDB can achieve such high performance.\n\nThe Philosophy Throughout this book, we will take the time to note the reasoning or motivation behind particular decisions made in the development of MongoDB. Through those notes we hope to share the philosophy behind MongoDB. The best way to summarize the MongoDB project, however, is by referencing its main focus—to create a full- featured data store that is scalable, flexible, and fast.\n\n6\n\n|\n\nChapter 1: Introduction",
      "content_length": 1258,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "CHAPTER 2 Getting Started\n\nMongoDB is powerful but easy to get started with. In this chapter we’ll introduce some of the basic concepts of MongoDB:\n\nA document is the basic unit of data for MongoDB and is roughly equivalent to a row in a relational database management system (but much more expressive).\n\nSimilarly, a collection can be thought of as a table with a dynamic schema.\n\nA single instance of MongoDB can host multiple independent databases, each of which contains its own collections.\n\nEvery document has a special key, \"_id\", that is unique within a collection.\n\nMongoDB is distributed with a simple but powerful tool called the mongo shell. The mongo shell provides built-in support for administering MongoDB instances and manipulating data using the MongoDB query language. It is also a fully func‐ tional JavaScript interpreter that enables users to create and load their own scripts for a variety of purposes.\n\nDocuments At the heart of MongoDB is the document: an ordered set of keys with associated val‐ ues. The representation of a document varies by programming language, but most languages have a data structure that is a natural fit, such as a map, hash, or dictionary. In JavaScript, for example, documents are represented as objects:\n\n{\"greeting\" : \"Hello, world!\"}\n\nThis simple document contains a single key, \"greeting\", with a value of \"Hello, world!\". Most documents will be more complex than this simple one and often will contain multiple key/value pairs:\n\n7",
      "content_length": 1488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "{\"greeting\" : \"Hello, world!\", \"views\" : 3}\n\nAs you can see, values in documents are not just “blobs.” They can be one of several different data types (or even an entire embedded document—see “Embedded Docu‐ ments” on page 19). In this example the value for \"greeting\" is a string, whereas the value for \"views\" is an integer.\n\nThe keys in a document are strings. Any UTF-8 character is allowed in a key, with a few notable exceptions:\n\nKeys must not contain the character \\0 (the null character). This character is used to signify the end of a key.\n\nThe . and $ characters have some special properties and should be used only in certain circumstances, as described in later chapters. In general, they should be considered reserved, and drivers will complain if they are used inappropriately.\n\nMongoDB is type-sensitive and case-sensitive. For example, these documents are distinct:\n\n{\"count\" : 5} {\"count\" : \"5\"}\n\nas are these:\n\n{\"count\" : 5} {\"Count\" : 5}\n\nA final important thing to note is that documents in MongoDB cannot contain dupli‐ cate keys. For example, the following is not a legal document:\n\n{\"greeting\" : \"Hello, world!\", \"greeting\" : \"Hello, MongoDB!\"}\n\nCollections A collection is a group of documents. If a document is the MongoDB analog of a row in a relational database, then a collection can be thought of as the analog to a table.\n\nDynamic Schemas Collections have dynamic schemas. This means that the documents within a single collection can have any number of different “shapes.” For example, both of the follow‐ ing documents could be stored in a single collection:\n\n{\"greeting\" : \"Hello, world!\", \"views\": 3} {\"signoff\": \"Good night, and good luck\"}\n\nNote that the previous documents have different keys, different numbers of keys, and values of different types. Because any document can be put into any collection, the question often arises: “Why do we need separate collections at all?” With no need for\n\n8\n\n|\n\nChapter 2: Getting Started",
      "content_length": 1965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "separate schemas for different kinds of documents, why should we use more than one collection? There are several good reasons:\n\nKeeping different kinds of documents in the same collection can be a nightmare for developers and admins. Developers need to make sure that each query is only returning documents adhering to a particular schema or that the application code performing a query can handle documents of different shapes. If we’re querying for blog posts, it’s a hassle to weed out documents containing author data.\n\nIt’s much faster to get a list of collections than to extract a list of the types of documents in a collection. For example, if we had a \"type\" field in each docu‐ ment that specified whether the document was a “skim,” “whole,” or “chunky monkey,” it would be much slower to find those three values in a single collection than to have three separate collections and query the correct collection.\n\nGrouping documents of the same kind together in the same collection allows for data locality. Getting several blog posts from a collection containing only posts will likely require fewer disk seeks than getting the same posts from a collection containing posts and author data.\n\nWe begin to impose some structure on our documents when we create indexes. (This is especially true in the case of unique indexes.) These indexes are defined per collection. By putting only documents of a single type into the same collec‐ tion, we can index our collections more efficiently.\n\nThere are sound reasons for creating a schema and for grouping related types of documents together. While not required by default, defining schemas for your appli‐ cation is good practice and can be enforced through the use of MongoDB’s documen‐ tation validation functionality and object–document mapping libraries available for many programming languages.\n\nNaming A collection is identified by its name. Collection names can be any UTF-8 string, with a few restrictions:\n\nThe empty string (\"\") is not a valid collection name. • Collection names may not contain the character \\0 (the null character), because this delineates the end of a collection name.\n\nYou should not create any collections with names that start with system., a prefix reserved for internal collections. For example, the system.users collection con‐ tains the database’s users, and the system.namespaces collection contains informa‐ tion about all of the database’s collections.\n\nCollections\n\n|\n\n9",
      "content_length": 2461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "User-created collections should not contain the reserved character $ in their names. The various drivers available for the database do support using $ in col‐ lection names because some system-generated collections contain it, but you should not use $ in a name unless you are accessing one of these collections.\n\nSubcollections\n\nOne convention for organizing collections is to use namespaced subcollections sepa‐ rated by the . character. For example, an application containing a blog might have a collection named blog.posts and a separate collection named blog.authors. This is for organizational purposes only—there is no relationship between the blog collection (it doesn’t even have to exist) and its “children.”\n\nAlthough subcollections do not have any special properties, they are useful and are incorporated into many MongoDB tools. For instance:\n\nGridFS, a protocol for storing large files, uses subcollections to store file meta‐ data separately from content chunks (see Chapter 6 for more information about GridFS).\n\nMost drivers provide some syntactic sugar for accessing a subcollection of a given collection. For example, in the database shell, db.blog will give you the blog col‐ lection, and db.blog.posts will give you the blog.posts collection.\n\nSubcollections are a good way to organize data in MongoDB for many use cases.\n\nDatabases In addition to grouping documents by collection, MongoDB groups collections into databases. A single instance of MongoDB can host several databases, each grouping together zero or more collections. A good rule of thumb is to store all data for a single application in the same database. Separate databases are useful when storing data for several applications or users on the same MongoDB server.\n\nLike collections, databases are identified by name. Database names can be any UTF-8 string, with the following restrictions:\n\nThe empty string (“”) is not a valid database name.\n\nA database name cannot contain any of these characters: /, \\, ., \", *, <, >, :, |, ?, $, (a single space), or \\0 (the null character). Basically, stick with alphanumeric ASCII.\n\nDatabase names are case-insensitive.\n\nDatabase names are limited to a maximum of 64 bytes.\n\n10\n\n|\n\nChapter 2: Getting Started",
      "content_length": 2234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Historically, prior to the use of the WiredTiger storage engine, database names became files on your filesystem. It is no longer the case. This explains why many of the previous restrictions exist in the first place.\n\nThere are also some reserved database names, which you can access but which have special semantics. These are as follows:\n\nadmin\n\nThe admin database plays a role in authentication and authorization. In addition, access to this database is required for some administrative operations. See Chap‐ ter 19 for more information about the admin database.\n\nlocal\n\nThis database stores data specific to a single server. In replica sets, local stores data used in the replication process. The local database itself is never replicated. (See Chapter 10 for more information about replication and the local database.)\n\nconfig\n\nSharded MongoDB clusters (see Chapter 14) use the config database to store information about each shard.\n\nBy concatenating a database name with a collection in that database you can get a fully qualified collection name, which is called a namespace. For instance, if you are using the blog.posts collection in the cms database, the namespace of that collection would be cms.blog.posts. Namespaces are limited to 120 bytes in length and, in prac‐ tice, should be fewer than 100 bytes long. For more on namespaces and the internal representation of collections in MongoDB, see Appendix B.\n\nGetting and Starting MongoDB To start the server, run the mongod executable in the Unix command-line environ‐ ment of your choice:\n\n$ mongod 2016-04-27T22:15:55.871-0400 I CONTROL [initandlisten] MongoDB starting : pid=8680 port=27017 dbpath=/data/db 64-bit host=morty 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] db version v4.2.0 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] git version: 34e65e5383f7ea1726332cb175b73077ec4a1b02 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] allocator: system 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] modules: none 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] build environment: 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] distarch: x86_64 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] target_arch: x86_64 2016-04-27T22:15:55.872-0400 I CONTROL [initandlisten] options: {} 2016-04-27T22:15:55.889-0400 I JOURNAL [initandlisten] journal dir=/data/db/journal 2016-04-27T22:15:55.889-0400 I JOURNAL [initandlisten] recover :\n\nGetting and Starting MongoDB\n\n|\n\n11",
      "content_length": 2487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "no journal files present, no recovery needed 2016-04-27T22:15:55.909-0400 I JOURNAL [durability] Durability thread started 2016-04-27T22:15:55.909-0400 I JOURNAL [journal writer] Journal writer thread started 2016-04-27T22:15:55.909-0400 I CONTROL [initandlisten] 2016-04-27T22:15:56.777-0400 I NETWORK [HostnameCanonicalizationWorker] Starting hostname canonicalization worker 2016-04-27T22:15:56.778-0400 I FTDC [initandlisten] Initializing full-time diagnostic data capture with directory '/data/db/diagnostic.data' 2016-04-27T22:15:56.779-0400 I NETWORK [initandlisten] waiting for connections on port 27017\n\nIf you’re on Windows, run this:\n\n> mongod.exe\n\nFor detailed information on installing MongoDB on your system, see Appendix A or the appropriate installation tutorial in the Mon‐ goDB documentation.\n\nWhen run with no arguments, mongod will use the default data directory, /data/db/ (or \\data\\db\\ on the current volume on Windows). If the data directory does not already exist or is not writable, the server will fail to start. It is important to create the data directory (e.g., mkdir -p /data/db/) and to make sure your user has permission to write to the directory before starting MongoDB.\n\nOn startup, the server will print some version and system information and then begin waiting for connections. By default MongoDB listens for socket connections on port 27017. The server will fail to start if that port is not available—the most common cause of this is another instance of MongoDB that is already running.\n\nYou should always secure your mongod instances. See Chapter 19 for more information on securing MongoDB.\n\nYou can safely stop mongod by typing Ctrl-C in the command-line-environment from which you launched the mongod server.\n\nFor more information on starting or stopping MongoDB, see Chapter 21.\n\n12\n\n|\n\nChapter 2: Getting Started",
      "content_length": 1857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Introduction to the MongoDB Shell MongoDB comes with a JavaScript shell that allows interaction with a MongoDB instance from the command line. The shell is useful for performing administrative functions, inspecting a running instance, or just exploring MongoDB. The mongo shell is a crucial tool for using MongoDB. We’ll use it extensively throughout the rest of the text.\n\nRunning the Shell To start the shell, run the mongo executable:\n\n$ mongo MongoDB shell version: 4.2.0 connecting to: test >\n\nThe shell automatically attempts to connect to a MongoDB server running on the local machine on startup, so make sure you start mongod before starting the shell.\n\nThe shell is a full-featured JavaScript interpreter, capable of running arbitrary Java‐ Script programs. To illustrate this, let’s perform some basic math:\n\n> x = 200; 200 > x / 5; 40\n\nWe can also leverage all of the standard JavaScript libraries:\n\n> Math.sin(Math.PI / 2); 1 > new Date(\"20109/1/1\"); ISODate(\"2019-01-01T05:00:00Z\") > \"Hello, World!\".replace(\"World\", \"MongoDB\"); Hello, MongoDB!\n\nWe can even define and call JavaScript functions:\n\n> function factorial (n) { ... if (n <= 1) return 1; ... return n * factorial(n - 1); ... } > factorial(5); 120\n\nNote that you can create multiline commands. The shell will detect whether the Java‐ Script statement is complete when you press Enter. If the statement is not complete, the shell will allow you to continue writing it on the next line. Pressing Enter three times in a row will cancel the half-formed command and get you back to the > prompt.\n\nIntroduction to the MongoDB Shell\n\n|\n\n13",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "A MongoDB Client Although the ability to execute arbitrary JavaScript is useful, the real power of the shell lies in the fact that it is also a standalone MongoDB client. On startup, the shell connects to the test database on a MongoDB server and assigns this database connec‐ tion to the global variable db. This variable is the primary access point to your Mon‐ goDB server through the shell.\n\nTo see the database to which db is currently assigned, type in db and hit Enter:\n\n> db test\n\nThe shell contains some add-ons that are not valid JavaScript syntax but were imple‐ mented because of their familiarity to users of SQL shells. The add-ons do not pro‐ vide any extra functionality, but they are nice syntactic sugar. For instance, one of the most important operations is selecting which database to use:\n\n> use video switched to db video\n\nNow if you look at the db variable, you can see that it refers to the video database:\n\n> db video\n\nBecause this is a JavaScript shell, typing a variable name will cause the name to be evaluated as an expression. The value (in this case, the database name) is then printed.\n\nYou may access collections from the db variable. For example:\n\n> db.movies\n\nreturns the movies collection in the current database. Now that we can access a collec‐ tion in the shell, we can perform almost any database operation.\n\nBasic Operations with the Shell We can use the four basic operations, create, read, update, and delete (CRUD), to manipulate and view data in the shell.\n\nCreate\n\nThe insertOne function adds a document to a collection. For example, suppose we want to store a movie. First, we’ll create a local variable called movie that is a Java‐ Script object representing our document. It will have the keys \"title\", \"director\", and \"year\" (the year it was released):\n\n14\n\n|\n\nChapter 2: Getting Started",
      "content_length": 1837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "> movie = {\"title\" : \"Star Wars: Episode IV - A New Hope\", ... \"director\" : \"George Lucas\", ... \"year\" : 1977} {\n\n\"title\" : \"Star Wars: Episode IV - A New Hope\", \"director\" : \"George Lucas\", \"year\" : 1977\n\n}\n\nThis object is a valid MongoDB document, so we can save it to the movies collection using the insertOne method:\n\n> db.movies.insertOne(movie) {\n\n\"acknowledged\" : true, \"insertedId\" : ObjectId(\"5721794b349c32b32a012b11\")\n\n}\n\nThe movie has been saved to the database. We can see it by calling find on the collection:\n\n> db.movies.find().pretty() {\n\n\"_id\" : ObjectId(\"5721794b349c32b32a012b11\"), \"title\" : \"Star Wars: Episode IV - A New Hope\", \"director\" : \"George Lucas\", \"year\" : 1977\n\n}\n\nWe can see that an \"_id\" key was added and that the other key/value pairs were saved as we entered them. The reason for the sudden appearance of the \"_id\" field is explained at the end of this chapter.\n\nRead\n\nfind and findOne can be used to query a collection. If we just want to see one docu‐ ment from a collection, we can use findOne:\n\n> db.movies.findOne() {\n\n\"_id\" : ObjectId(\"5721794b349c32b32a012b11\"), \"title\" : \"Star Wars: Episode IV - A New Hope\", \"director\" : \"George Lucas\", \"year\" : 1977\n\n}\n\nfind and findOne can also be passed criteria in the form of a query document. This will restrict the documents matched by the query. The shell will automatically display up to 20 documents matching a find, but more can be fetched. (See Chapter 4 for more information on querying.)\n\nIntroduction to the MongoDB Shell\n\n|\n\n15",
      "content_length": 1524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Update\n\nIf we would like to modify our post, we can use updateOne. updateOne takes (at least) two parameters: the first is the criteria to find which document to update, and the second is a document describing the updates to make. Suppose we decide to enable reviews for the movie we created earlier. We’ll need to add an array of reviews as the value for a new key in our document.\n\nTo perform the update, we’ll need to use an update operator, set:\n\n> db.movies.updateOne({title : \"Star Wars: Episode IV - A New Hope\"}, ... {$set : {reviews: []}}) WriteResult({\"nMatched\": 1, \"nUpserted\": 0, \"nModified\": 1})\n\nNow the document has a \"reviews\" key. If we call find again, we can see the new key:\n\n> db.movies.find().pretty() {\n\n\"_id\" : ObjectId(\"5721794b349c32b32a012b11\"), \"title\" : \"Star Wars: Episode IV - A New Hope\", \"director\" : \"George Lucas\", \"year\" : 1977, \"reviews\" : [ ]\n\n}\n\nSee “Updating Documents” on page 35 for detailed information on updating documents.\n\nDelete\n\ndeleteOne and deleteMany permanently delete documents from the database. Both methods take a filter document specifying criteria for the removal. For example, this would remove the movie we just created:\n\n> db.movies.deleteOne({title : \"Star Wars: Episode IV - A New Hope\"})\n\nUse deleteMany to delete all documents matching a filter.\n\nData Types The beginning of this chapter covered the basics of what a document is. Now that you are up and running with MongoDB and can try things in the shell, this section will dive a little deeper. MongoDB supports a wide range of data types as values in docu‐ ments. In this section, we’ll outline all the supported types.\n\nBasic Data Types Documents in MongoDB can be thought of as “JSON-like” in that they are conceptu‐ ally similar to objects in JavaScript. JSON is a simple representation of data: the speci‐\n\n16\n\n|\n\nChapter 2: Getting Started",
      "content_length": 1865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "fication can be described in about one paragraph (the website proves it) and lists only six data types. This is a good thing in many ways: it’s easy to understand, parse, and remember. On the other hand, JSON’s expressive capabilities are limited because the only types are null, boolean, numeric, string, array, and object.\n\nAlthough these types allow for an impressive amount of expressivity, there are a cou‐ ple of additional types that are crucial for most applications, especially when working with a database. For example, JSON has no date type, which makes working with dates even more annoying than it usually is. There is a number type, but only one— there is no way to differentiate floats and integers, never mind any distinction between 32-bit and 64-bit numbers. There is no way to represent other commonly used types, either, such as regular expressions or functions.\n\nMongoDB adds support for a number of additional data types while keeping JSON’s essential key/value–pair nature. Exactly how values of each type are represented varies by language, but this is a list of the commonly supported types and how they are represented as part of a document in the shell. The most common types are:\n\nNull\n\nThe null type can be used to represent both a null value and a nonexistent field:\n\n{\"x\" : null}\n\nBoolean\n\nThere is a boolean type, which can be used for the values true and false:\n\n{\"x\" : true}\n\nNumber\n\nThe shell defaults to using 64-bit floating-point numbers. Thus, these numbers both look “normal” in the shell:\n\n{\"x\" : 3.14}\n\n{\"x\" : 3}\n\nFor integers, use the NumberInt or NumberLong classes, which represent 4-byte or 8-byte signed integers, respectively.\n\n{\"x\" : NumberInt(\"3\")} {\"x\" : NumberLong(\"3\")}\n\nString\n\nAny string of UTF-8 characters can be represented using the string type:\n\n{\"x\" : \"foobar\"}\n\nDate\n\nMongoDB stores dates as 64-bit integers representing milliseconds since the Unix epoch (January 1, 1970). The time zone is not stored:\n\nData Types\n\n|\n\n17",
      "content_length": 1983,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "{\"x\" : new Date()}\n\nRegular expression\n\nQueries can use regular expressions using JavaScript’s regular expression syntax:\n\n{\"x\" : /foobar/i}\n\nArray\n\nSets or lists of values can be represented as arrays:\n\n{\"x\" : [\"a\", \"b\", \"c\"]}\n\nEmbedded document\n\nDocuments can contain entire documents embedded as values in a parent document:\n\n{\"x\" : {\"foo\" : \"bar\"}}\n\nObject ID\n\nAn object ID is a 12-byte ID for documents:\n\n{\"x\" : ObjectId()}\n\nSee the section “_id and ObjectIds” on page 20 for details.\n\nThere are also a few less common types that you may need, including:\n\nBinary data\n\nBinary data is a string of arbitrary bytes. It cannot be manipulated from the shell. Binary data is the only way to save non-UTF-8 strings to the database.\n\nCode\n\nMongoDB also makes it possible to store arbitrary JavaScript in queries and documents:\n\n{\"x\" : function() { /* ... */ }}\n\nFinally, there are a few types that are mostly used internally (or superseded by other types). These will be described in the text as needed.\n\nFor more information on MongoDB’s data format, see Appendix B.\n\nDates In JavaScript, the Date class is used for MongoDB’s date type. When creating a new Date object, always call new Date(), not just Date(). Calling the constructor as a function (i.e., not including new) returns a string representation of the date, not an actual Date object. This is not MongoDB’s choice; it is how JavaScript works. If you are not careful to always use the Date constructor, you can end up with a mishmash of strings and dates. Strings do not match dates and vice versa, so this can cause prob‐ lems with removing, updating, querying…pretty much everything.\n\n18\n\n|\n\nChapter 2: Getting Started",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "For a full explanation of JavaScript’s Date class and acceptable formats for the con‐ structor, see section 15.9 of the ECMAScript specification.\n\nDates in the shell are displayed using local time zone settings. However, dates in the database are just stored as milliseconds since the epoch, so they have no time zone information associated with them. (Time zone information could, of course, be stored as the value for another key.)\n\nArrays Arrays are values that can be used interchangeably for both ordered operations (as though they were lists, stacks, or queues) and unordered operations (as though they were sets).\n\nIn the following document, the key \"things\" has an array value:\n\n{\"things\" : [\"pie\", 3.14]}\n\nAs you can see from this example, arrays can contain different data types as values (in this case, a string and a floating-point number). In fact, array values can be any of the supported value types for normal key/value pairs, even nested arrays.\n\nOne of the great things about arrays in documents is that MongoDB “understands” their structure and knows how to reach inside of arrays to perform operations on their contents. This allows us to query on arrays and build indexes using their con‐ tents. For instance, in the previous example, MongoDB can query for all documents where 3.14 is an element of the \"things\" array. If this is a common query, you can even create an index on the \"things\" key to improve the query’s speed.\n\nMongoDB also allows atomic updates that modify the contents of arrays, such as reaching into the array and changing the value \"pie\" to pi. We’ll see more examples of these types of operations throughout the text.\n\nEmbedded Documents A document can be used as the value for a key. This is called an embedded document. Embedded documents can be used to organize data in a more natural way than just a flat structure of key/value pairs.\n\nFor example, if we have a document representing a person and want to store that per‐ son’s address, we can nest this information in an embedded \"address\" document:\n\n{ \"name\" : \"John Doe\", \"address\" : { \"street\" : \"123 Park Street\", \"city\" : \"Anytown\", \"state\" : \"NY\"\n\nData Types\n\n|\n\n19",
      "content_length": 2167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "} }\n\nThe value for the \"address\" key in this example is an embedded document with its own key/value pairs for \"street\", \"city\", and \"state\".\n\nAs with arrays, MongoDB “understands” the structure of embedded documents and is able to reach inside them to build indexes, perform queries, or make updates.\n\nWe’ll discuss schema design in-depth later, but even from this basic example we can begin to see how embedded documents can change the way we work with data. In a relational database, the previous document would probably be modeled as two sepa‐ rate rows in two different tables (people and addresses). With MongoDB we can embed the \"address\" document directly within the \"person\" document. Thus, when used properly, embedded documents can provide a more natural representation of information.\n\nThe flip side of this is that there can be more data repetition with MongoDB. Suppose addresses was a separate table in a relational database and we needed to fix a typo in an address. When we did a join with people and addresses, we’d get the updated address for everyone who shares it. With MongoDB, we’d need to fix the typo in each person’s document.\n\n_id and ObjectIds Every document stored in MongoDB must have an \"_id\" key. The \"_id\" key’s value can be any type, but it defaults to an ObjectId. In a single collection, every document must have a unique value for \"_id\", which ensures that every document in a collec‐ tion can be uniquely identified. That is, if you had two collections, each one could have a document where the value for \"_id\" was 123. However, neither collection could contain more than one document with an \"_id\" of 123.\n\nObjectIds\n\nObjectId is the default type for \"_id\". The ObjectId class is designed to be light‐ weight, while still being easy to generate in a globally unique way across different machines. MongoDB’s distributed nature is the main reason why it uses ObjectIds as opposed to something more traditional, like an autoincrementing primary key: it is difficult and time-consuming to synchronize autoincrementing primary keys across multiple servers. Because MongoDB was designed to be a distributed database, it was important to be able to generate unique identifiers in a sharded environment.\n\nObjectIds use 12 bytes of storage, which gives them a string representation that is 24 hexadecimal digits: 2 digits for each byte. This causes them to appear larger than they are, which makes some people nervous. It’s important to note that even though an\n\n20\n\n|\n\nChapter 2: Getting Started",
      "content_length": 2525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "ObjectId is often represented as a giant hexadecimal string, the string is actually twice as long as the data being stored.\n\nIf you create multiple new ObjectIds in rapid succession, you can see that only the last few digits change each time. In addition, a couple of digits in the middle of the ObjectId will change if you space the creations out by a couple of seconds. This is because of the manner in which ObjectIds are created. The 12 bytes of an ObjectId are generated as follows:\n\n0 2 Timestamp Random Counter (random start value)\n\n1\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\nThe first four bytes of an ObjectId are a timestamp in seconds since the epoch. This provides a couple of useful properties:\n\nThe timestamp, when combined with the next five bytes (which will be described in a moment), provides uniqueness at the granularity of a second.\n\nBecause the timestamp comes first, ObjectIds will sort in rough insertion order. This is not a strong guarantee but does have some nice properties, such as mak‐ ing ObjectIds efficient to index.\n\nIn these four bytes exists an implicit timestamp of when each document was cre‐ ated. Most drivers expose a method for extracting this information from an ObjectId.\n\nBecause the current time is used in ObjectIds, some users worry that their servers will need to have synchronized clocks. Although synchronized clocks are a good idea for other reasons (see “Synchronizing Clocks” on page 462), the actual timestamp doesn’t matter to ObjectIds, only that it is often new (once per second) and increasing.\n\nThe next five bytes of an ObjectId are a random value. The final three bytes are a counter that starts with a random value to avoid generating colliding ObjectIds on different machines.\n\nThese first nine bytes of an ObjectId therefore guarantee its uniqueness across machines and processes for a single second. The last three bytes are simply an incre‐ menting counter that is responsible for uniqueness within a second in a single pro‐ cess. This allows for up to 2563 (16,777,216) unique ObjectIds to be generated per process in a single second.\n\nData Types\n\n|\n\n21",
      "content_length": 2110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Autogeneration of _id\n\nAs stated earlier, if there is no \"_id\" key present when a document is inserted, one will be automatically added to the inserted document. This can be handled by the MongoDB server but will generally be done by the driver on the client side.\n\nUsing the MongoDB Shell This section covers how to use the shell as part of your command-line toolkit, cus‐ tomize it, and use some of its more advanced functionality.\n\nAlthough we connected to a local mongod instance above, you can connect your shell to any MongoDB instance that your machine can reach. To connect to a mongod on a different machine or port, specify the hostname, port, and database when starting the shell:\n\n$ mongo some-host:30000/myDB MongoDB shell version: 4.2.0 connecting to: some-host:30000/myDB >\n\ndb will now refer to some-host:30000’s myDB database.\n\nSometimes it is handy to not connect to a mongod at all when starting the mongo shell. If you start the shell with --nodb, it will start up without attempting to connect to anything:\n\n$ mongo --nodb MongoDB shell version: 4.2.0 >\n\nOnce started, you can connect to a mongod at your leisure by running new Mongo(\"hostname\"):\n\n> conn = new Mongo(\"some-host:30000\") connection to some-host:30000 > db = conn.getDB(\"myDB\") myDB\n\nAfter these two commands, you can use db normally. You can use these commands to connect to a different database or server at any time.\n\nTips for Using the Shell Because mongo is simply a JavaScript shell, you can get a great deal of help for it by simply looking up JavaScript documentation online. For MongoDB-specific function‐ ality, the shell includes built-in help that can be accessed by typing help:\n\n22\n\n|\n\nChapter 2: Getting Started",
      "content_length": 1711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "> help db.help() help on db methods db.mycoll.help() help on collection methods sh.help() sharding helpers ...\n\nshow dbs show database names show collections show collections in current database show users show users in current database ...\n\nDatabase-level help db.foo.help().\n\nis provided by db.help() and collection-level help by\n\nA good way of figuring out what a function is doing is to type it without the paren‐ theses. This will print the JavaScript source code for the function. For example, if you are curious about how the update function works or cannot remember the order of parameters, you can do the following:\n\n> db.movies.updateOne function (filter, update, options) { var opts = Object.extend({}, options || {});\n\n// Check if first key in update statement contains a $ var keys = Object.keys(update); if (keys.length == 0) { throw new Error(\"the update operation document must contain at least one atomic operator\"); } ...\n\nRunning Scripts with the Shell In addition to using the shell interactively, you can also pass the shell JavaScript files to execute. Simply pass in your scripts at the command line:\n\n$ mongo script1.js script2.js script3.js MongoDB shell version: 4.2.1 connecting to: mongodb://127.0.0.1:27017 MongoDB server version: 4.2.1\n\nloading file: script1.js I am script1.js loading file: script2.js I am script2.js loading file: script3.js I am script3.js ...\n\nThe mongo shell will execute each script listed and exit.\n\nUsing the MongoDB Shell\n\n|\n\n23",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "If you want to run a script using a connection to a nondefault host/port mongod, specify the address first, then the script(s):\n\n$ mongo server-1:30000/foo --quiet script1.js script2.js script3.js\n\nThis would execute the three scripts with db set to the foo database on server-1:30000.\n\nYou can print to stdout in scripts (as the preceding scripts did) using the print func‐ tion. This allows you to use the shell as part of a pipeline of commands. If you’re planning to pipe the output of a shell script to another command, use the --quiet option to prevent the “MongoDB shell version v4.2.0” banner from printing.\n\nYou can also run scripts from within the interactive shell using the load function:\n\n> load(\"script1.js\") I am script1.js true >\n\nScripts have access to the db variable (as well as any other global). However, shell helpers such as use db or show collections do not work from files. There are valid JavaScript equivalents to each of these, as shown in Table 2-1.\n\nTable 2-1. JavaScript equivalents to shell helpers\n\nHelper use video\n\nEquivalent db.getSisterDB(\"video\")\n\nshow dbs\n\ndb.getMongo().getDBs()\n\nshow collections db.getCollectionNames()\n\nYou can also use scripts to inject variables into the shell. For example, you could have a script that simply initializes helper functions that you commonly use. The following script, for instance, may be helpful for Part III and Part IV. It defines a function, con nectTo, that connects to the locally running database on the given port and sets db to that connection:\n\n// defineConnectTo.js\n\n/** * Connect to a database and set db. */ var connectTo = function(port, dbname) { if (!port) { port = 27017; }\n\nif (!dbname) { dbname = \"test\"; }\n\n24\n\n|\n\nChapter 2: Getting Started",
      "content_length": 1738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "db = connect(\"localhost:\"+port+\"/\"+dbname); return db; };\n\nIf you load this script in the shell, connectTo is now defined:\n\n> typeof connectTo undefined > load('defineConnectTo.js') > typeof connectTo function\n\nIn addition to adding helper functions, you can use scripts to automate common tasks and administrative activities.\n\nBy default, the shell will look in the directory that you started the shell in (use pwd() to see what directory that is). If the script is not in your current directory, you can give the shell a relative or absolute path to it. For example, if you wanted to put your shell scripts in ~/my-scripts, you could load defineConnectTo.js with load(\"/home/ myUser/my-scripts/defineConnectTo.js\"). Note that load cannot resolve ~.\n\nYou can use run to run command-line programs from the shell. You can pass argu‐ ments to the function as parameters:\n\n> run(\"ls\", \"-l\", \"/home/myUser/my-scripts/\") sh70352| -rw-r--r-- 1 myUser myUser 2012-12-13 13:15 defineConnectTo.js sh70532| -rw-r--r-- 1 myUser myUser 2013-02-22 15:10 script1.js sh70532| -rw-r--r-- 1 myUser myUser 2013-02-22 15:12 script2.js sh70532| -rw-r--r-- 1 myUser myUser 2013-02-22 15:13 script3.js\n\nThis is of limited use, generally, as the output is formatted oddly and it doesn’t sup‐ port pipes.\n\nCreating a .mongorc.js If you have frequently loaded scripts, you might want to put them in your .mongorc.js file. This file is run whenever you start up the shell.\n\nFor example, suppose you would like the shell to greet you when you log in. Create a file called .mongorc.js in your home directory, and then add the following lines to it:\n\n// .mongorc.js\n\nvar compliment = [\"attractive\", \"intelligent\", \"like Batman\"]; var index = Math.floor(Math.random()*3);\n\nprint(\"Hello, you're looking particularly \"+compliment[index]+\" today!\");\n\nThen, when you start the shell, you’ll see something like:\n\nUsing the MongoDB Shell\n\n|\n\n25",
      "content_length": 1908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "$ mongo MongoDB shell version: 4.2.1 connecting to: test Hello, you're looking particularly like Batman today! >\n\nMore practically, you can use this script to set up any global variables you’d like to use, alias long names to shorter ones, and override built-in functions. One of the most common uses for .mongorc.js is to remove some of the more “dangerous” shell helpers. You can override functions like dropDatabase or deleteIndexes with no- ops or undefine them altogether:\n\nvar no = function() { print(\"Not on my watch.\"); };\n\n// Prevent dropping databases db.dropDatabase = DB.prototype.dropDatabase = no;\n\n// Prevent dropping collections DBCollection.prototype.drop = no;\n\n// Prevent dropping an index DBCollection.prototype.dropIndex = no;\n\n// Prevent dropping indexes DBCollection.prototype.dropIndexes = no;\n\nNow if you try to call any of these functions, it will simply print an error message. Note that this technique does not protect you against malicious users; it can only help with fat-fingering.\n\nYou can disable loading your .mongorc.js by using the --norc option when starting the shell.\n\nCustomizing Your Prompt The default shell prompt can be overridden by setting the prompt variable to either a string or a function. For example, if you are running a query that takes minutes to complete, you may want to have a prompt that displays the current time so you can see when the last operation finished:\n\nprompt = function() { return (new Date())+\"> \"; };\n\nAnother handy prompt might show the current database you’re using:\n\n26\n\n|\n\nChapter 2: Getting Started",
      "content_length": 1576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "prompt = function() { if (typeof db == 'undefined') { return '(nodb)> '; }\n\n// Check the last db operation try { db.runCommand({getLastError:1}); } catch (e) { print(e); }\n\nreturn db+\"> \"; };\n\nNote that prompt functions should return strings and be very cautious about catching exceptions: it can be extremely confusing if your prompt turns into an exception!\n\nIn general, your prompt function should include a call to getLastError. This catches errors on writes and reconnects you automatically if the shell gets disconnected (e.g., if you restart mongod).\n\nThe .mongorc.js file is a good place to set your prompt if you want to always use a custom one (or set up a couple of custom prompts that you can switch between in the shell).\n\nEditing Complex Variables The multiline support in the shell is somewhat limited: you cannot edit previous lines, which can be annoying when you realize that the first line has a typo and you’re currently working on line 15. Thus, for larger blocks of code or objects, you may want to edit them in an editor. To do so, set the EDITOR variable in the shell (or in your environment, but since you’re already in the shell…):\n\n> EDITOR=\"/usr/bin/emacs\"\n\nNow, if you want to edit a variable, you can say edit varname—for example:\n\n> var wap = db.books.findOne({title: \"War and Peace\"}); > edit wap\n\nWhen you’re done making changes, save and exit the editor. The variable will be parsed and loaded back into the shell.\n\nAdd EDITOR=\"/path/to/editor\"; to your .mongorc.js file and you won’t have to worry about setting it again.\n\nUsing the MongoDB Shell\n\n|\n\n27",
      "content_length": 1588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Inconvenient Collection Names Fetching a collection with the db.collectionName syntax almost always works, unless the collection name is a reserved word or is an invalid JavaScript property name.\n\nFor example, suppose we are trying to access the version collection. We cannot say db.version because db.version is a method on db (it returns the version of the run‐ ning MongoDB server):\n\n> db.version function () { return this.serverBuildInfo().version; }\n\nTo actually access the version collection, you must use the getCollection function:\n\n> db.getCollection(\"version\"); test.version\n\nThis can also be used for collection names with characters that aren’t valid JavaScript property names, such as foo-bar-baz and 123abc (JavaScript property names can only contain letters, numbers, $ and _, and cannot start with a number).\n\nAnother way of getting around invalid properties is to use array-access syntax. In JavaScript, x.y is identical to x['y']. This means that subcollections can be accessed using variables, not just literal names. Thus, if you needed to perform some operation on every blog subcollection, you could iterate through them with something like this:\n\nvar collections = [\"posts\", \"comments\", \"authors\"];\n\nfor (var i in collections) { print(db.blog[collections[i]]); }\n\ninstead of this:\n\nprint(db.blog.posts); print(db.blog.comments); print(db.blog.authors);\n\nNote that you cannot do db.blog.i, which would be interpreted as test.blog.i, not test.blog.posts. You must use the db.blog[i] syntax for i to be interpreted as a variable.\n\nYou can use this technique to access awkwardly named collections:\n\n> var name = \"@#&!\" > db[name].find()\n\nAttempting to query db.@#&! would be illegal, but db[name] would work.\n\n28\n\n|\n\nChapter 2: Getting Started",
      "content_length": 1762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "CHAPTER 3 Creating, Updating, and Deleting Documents\n\nThis chapter covers the basics of moving data into and out of the database, including the following:\n\nAdding new documents to a collection\n\nRemoving documents from a collection\n\nUpdating existing documents\n\nChoosing the correct level of safety versus speed for all of these operations\n\nInserting Documents Inserts are the basic method for adding data to MongoDB. To insert a single docu‐ ment, use the collection’s insertOne method:\n\n> db.movies.insertOne({\"title\" : \"Stand by Me\"})\n\ninsertOne will add an \"_id\" key to the document (if you do not supply one) and store the document in MongoDB.\n\ninsertMany If you need to insert multiple documents into a collection, you can use insertMany. This method enables you to pass an array of documents to the database. This is far more efficient because your code will not make a round trip to the database for each document inserted, but will insert them in bulk.\n\nIn the shell, you can try this out as follows:\n\n29",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "> db.movies.drop() true > db.movies.insertMany([{\"title\" : \"Ghostbusters\"}, ... {\"title\" : \"E.T.\"}, ... {\"title\" : \"Blade Runner\"}]); { \"acknowledged\" : true, \"insertedIds\" : [ ObjectId(\"572630ba11722fac4b6b4996\"), ObjectId(\"572630ba11722fac4b6b4997\"), ObjectId(\"572630ba11722fac4b6b4998\") ] } > db.movies.find() { \"_id\" : ObjectId(\"572630ba11722fac4b6b4996\"), \"title\" : \"Ghostbusters\" } { \"_id\" : ObjectId(\"572630ba11722fac4b6b4997\"), \"title\" : \"E.T.\" } { \"_id\" : ObjectId(\"572630ba11722fac4b6b4998\"), \"title\" : \"Blade Runner\" }\n\nSending dozens, hundreds, or even thousands of documents at a time can make inserts significantly faster.\n\ninsertMany is useful if you are inserting multiple documents into a single collection. If you are just importing raw data (e.g., from a data feed or MySQL), there are command-line tools like mongoimport that can be used instead of a batch insert. On the other hand, it is often handy to munge data before saving it to MongoDB (con‐ verting dates to the date type or adding a custom \"_id\", for example). In such cases insertMany can be used for importing data, as well.\n\nCurrent versions of MongoDB do not accept messages longer than 48 MB, so there is a limit to how much can be inserted in a single batch insert. If you attempt to insert more than 48 MB, many drivers will split up the batch insert into multiple 48 MB batch inserts. Check your driver documentation for details.\n\nWhen performing a bulk insert using insertMany, if a document halfway through the array produces an error of some type, what happens depends on whether you have opted for ordered or unordered operations. As the second parameter to insertMany you may specify an options document. Specify true for the key \"ordered\" in the options document to ensure documents are inserted in the order they are provided. Specify false and MongoDB may reorder the inserts to increase performance. Ordered inserts is the default if no ordering is specified. For ordered inserts, the array passed to insertMany defines the insertion order. If a document produces an inser‐ tion error, no documents beyond that point in the array will be inserted. For unor‐ dered inserts, MongoDB will attempt to insert all documents, regardless of whether some insertions produce errors.\n\nIn this example, because ordered inserts is the default, only the first two documents will be inserted. The third document will produce an error, because you cannot insert two documents with the same \"_id\":\n\n30\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 2539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "> db.movies.insertMany([ ... {\"_id\" : 0, \"title\" : \"Top Gun\"}, ... {\"_id\" : 1, \"title\" : \"Back to the Future\"}, ... {\"_id\" : 1, \"title\" : \"Gremlins\"}, ... {\"_id\" : 2, \"title\" : \"Aliens\"}]) 2019-04-22T12:27:57.278-0400 E QUERY [js] BulkWriteError: write error at item 2 in bulk operation : BulkWriteError({ \"writeErrors\" : [ { \"index\" : 2, \"code\" : 11000, \"errmsg\" : \"E11000 duplicate key error collection: test.movies index: _id_ dup key: { _id: 1.0 }\", \"op\" : { \"_id\" : 1, \"title\" : \"Gremlins\" } } ], \"writeConcernErrors\" : [ ], \"nInserted\" : 2, \"nUpserted\" : 0, \"nMatched\" : 0, \"nModified\" : 0, \"nRemoved\" : 0, \"upserted\" : [ ] }) BulkWriteError@src/mongo/shell/bulk_api.js:367:48 BulkWriteResult/this.toError@src/mongo/shell/bulk_api.js:332:24 Bulk/this.execute@src/mongo/shell/bulk_api.js:1186:23 DBCollection.prototype.insertMany@src/mongo/shell/crud_api.js:314:5 @(shell):1:1\n\nIf instead we specify unordered inserts, the first, second, and fourth documents in the array are inserted. The only insert that fails is the third document, again because of a duplicate \"_id\" error:\n\n> db.movies.insertMany([ ... {\"_id\" : 3, \"title\" : \"Sixteen Candles\"}, ... {\"_id\" : 4, \"title\" : \"The Terminator\"}, ... {\"_id\" : 4, \"title\" : \"The Princess Bride\"}, ... {\"_id\" : 5, \"title\" : \"Scarface\"}], ... {\"ordered\" : false}) 2019-05-01T17:02:25.511-0400 E QUERY [thread1] BulkWriteError: write error at item 2 in bulk operation : BulkWriteError({ \"writeErrors\" : [ { \"index\" : 2, \"code\" : 11000, \"errmsg\" : \"E11000 duplicate key error index: test.movies.$_id_\n\nInserting Documents\n\n|\n\n31",
      "content_length": 1576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "dup key: { : 4.0 }\", \"op\" : { \"_id\" : 4, \"title\" : \"The Princess Bride\" } } ], \"writeConcernErrors\" : [ ], \"nInserted\" : 3, \"nUpserted\" : 0, \"nMatched\" : 0, \"nModified\" : 0, \"nRemoved\" : 0, \"upserted\" : [ ] }) BulkWriteError@src/mongo/shell/bulk_api.js:367:48 BulkWriteResult/this.toError@src/mongo/shell/bulk_api.js:332:24 Bulk/this.execute@src/mongo/shell/bulk_api.js:1186.23 DBCollection.prototype.insertMany@src/mongo/shell/crud_api.js:314:5 @(shell):1:1\n\nIf you study these examples closely, you might note that the output of these two calls to insertMany hints that other operations besides simply inserts might be supported for bulk writes. While insertMany does not support operations other than insert, MongoDB does support a Bulk Write API that enables you to batch together a num‐ ber of operations of different types in one call. While that is beyond the scope of this chapter, you can read about the Bulk Write API in the MongoDB documentation.\n\nInsert Validation MongoDB does minimal checks on data being inserted: it checks the document’s basic structure and adds an \"_id\" field if one does not exist. One of the basic struc‐ ture checks is size: all documents must be smaller than 16 MB. This is a somewhat arbitrary limit (and may be raised in the future); it is mostly intended to prevent bad schema design and ensure consistent performance. To see the Binary JSON (BSON) size, in bytes, of the document doc, run Object.bsonsize(doc) from the shell.\n\nTo give you an idea of how much data 16 MB is, the entire text of War and Peace is just 3.14 MB.\n\nThese minimal checks also mean that it is fairly easy to insert invalid data (if you are trying to). Thus, you should only allow trusted sources, such as your application servers, to connect to the database. All of the MongoDB drivers for major languages (and most of the minor ones, too) do check for a variety of invalid data (documents that are too large, contain non-UTF-8 strings, or use unrecognized types) before sending anything to the database.\n\n32\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 2082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "insert In versions of MongoDB prior to 3.0, insert was the primary method for inserting documents into MongoDB. MongoDB drivers introduced a new CRUD API at the same time as the MongoDB 3.0 server release. As of MongoDB 3.2 the mongo shell also supports this API, which includes insertOne and insertMany as well as several other methods. The goal of the current CRUD API is to make the semantics of all CRUD operations consistent and clear across the drivers and the shell. While meth‐ ods such as insert are still supported for backward compatibility, they should not be used in applications going forward. You should instead prefer insertOne and insert Many for creating documents.\n\nRemoving Documents Now that there’s data in our database, let’s delete it. The CRUD API provides deleteOne and deleteMany for this purpose. Both of these methods take a filter document as their first parameter. The filter specifies a set of criteria to match against in removing documents. To delete the document with the \"_id\" value of 4, we use deleteOne in the mongo shell as illustrated here:\n\n> db.movies.find() { \"_id\" : 0, \"title\" : \"Top Gun\"} { \"_id\" : 1, \"title\" : \"Back to the Future\"} { \"_id\" : 3, \"title\" : \"Sixteen Candles\"} { \"_id\" : 4, \"title\" : \"The Terminator\"} { \"_id\" : 5, \"title\" : \"Scarface\"} > db.movies.deleteOne({\"_id\" : 4}) { \"acknowledged\" : true, \"deletedCount\" : 1 } > db.movies.find() { \"_id\" : 0, \"title\" : \"Top Gun\"} { \"_id\" : 1, \"title\" : \"Back to the Future\"} { \"_id\" : 3, \"title\" : \"Sixteen Candles\"} { \"_id\" : 5, \"title\" : \"Scarface\"}\n\nIn this example, we used a filter that could only match one document since \"_id\" values are unique in a collection. However, we can also specify a filter that matches multiple documents in a collection. In this case, deleteOne will delete the first docu‐ ment found that matches the filter. Which document is found first depends on several factors, including the order in which the documents were inserted, what updates were made to the documents (for some storage engines), and what indexes are specified. As with any database operation, be sure you know what effect your use of deleteOne will have on your data.\n\nTo delete all the documents that match a filter, use deleteMany:\n\nRemoving Documents\n\n|\n\n33",
      "content_length": 2262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "> db.movies.find() { \"_id\" : 0, \"title\" : \"Top Gun\", \"year\" : 1986 } { \"_id\" : 1, \"title\" : \"Back to the Future\", \"year\" : 1985 } { \"_id\" : 3, \"title\" : \"Sixteen Candles\", \"year\" : 1984 } { \"_id\" : 4, \"title\" : \"The Terminator\", \"year\" : 1984 } { \"_id\" : 5, \"title\" : \"Scarface\", \"year\" : 1983 } > db.movies.deleteMany({\"year\" : 1984}) { \"acknowledged\" : true, \"deletedCount\" : 2 } > db.movies.find() { \"_id\" : 0, \"title\" : \"Top Gun\", \"year\" : 1986 } { \"_id\" : 1, \"title\" : \"Back to the Future\", \"year\" : 1985 } { \"_id\" : 5, \"title\" : \"Scarface\", \"year\" : 1983 }\n\nAs a more realistic use case, suppose you want to remove every user from the mail‐ ing.list collection where the value for \"opt-out\" is true:\n\n> db.mailing.list.deleteMany({\"opt-out\" : true})\n\nIn versions of MongoDB prior to 3.0, remove was the primary method for deleting documents. MongoDB drivers introduced the deleteOne and deleteMany methods at the same time as the MongoDB 3.0 server release, and the shell began supporting these methods in MongoDB 3.2. While remove is still supported for backward com‐ patibility, you should use deleteOne and deleteMany in your applications. The cur‐ rent CRUD API provides a cleaner set of semantics and, especially for multidocument operations, helps application developers avoid a couple of common pitfalls with the previous API.\n\ndrop It is possible to use deleteMany to remove all documents in a collection:\n\n> db.movies.find() { \"_id\" : 0, \"title\" : \"Top Gun\", \"year\" : 1986 } { \"_id\" : 1, \"title\" : \"Back to the Future\", \"year\" : 1985 } { \"_id\" : 3, \"title\" : \"Sixteen Candles\", \"year\" : 1984 } { \"_id\" : 4, \"title\" : \"The Terminator\", \"year\" : 1984 } { \"_id\" : 5, \"title\" : \"Scarface\", \"year\" : 1983 } > db.movies.deleteMany({}) { \"acknowledged\" : true, \"deletedCount\" : 5 } > db.movies.find()\n\nRemoving documents is usually a fairly quick operation. However, if you want to clear an entire collection, it is faster to drop it:\n\n> db.movies.drop() true\n\nand then recreate any indexes on the empty collection.\n\nOnce data has been removed, it is gone forever. There is no way to undo a delete or drop operation or recover deleted documents, except, of course, by restoring a\n\n34\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 2249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "previously backed up version of the data. See Chapter 23 for a detailed discussion of MongoDB backup and restore.\n\nUpdating Documents Once a document is stored in the database, it can be changed using one of several update methods: updateOne, updateMany, and replaceOne. updateOne and update Many each take a filter document as their first parameter and a modifier document, which describes changes to make, as the second parameter. replaceOne also takes a filter as the first parameter, but as the second parameter replaceOne expects a docu‐ ment with which it will replace the document matching the filter.\n\nUpdating a document is atomic: if two updates happen at the same time, whichever one reaches the server first will be applied, and then the next one will be applied. Thus, conflicting updates can safely be sent in rapid-fire succession without any documents being corrupted: the last update will “win.” The Document Versioning pattern (see “Schema Design Patterns” on page 208) is worth considering if you don’t want the default behavior.\n\nDocument Replacement replaceOne fully replaces a matching document with a new one. This can be useful to do a dramatic schema migration (see Chapter 9 for scheme migration strategies). For example, suppose we are making major changes to a user document, which looks like the following:\n\n{ \"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7a\"), \"name\" : \"joe\", \"friends\" : 32, \"enemies\" : 2 }\n\nWe want to move the \"friends\" and \"enemies\" fields to a \"relationships\" subdo‐ cument. We can change the structure of the document in the shell and then replace the database’s version with a replaceOne:\n\n> var joe = db.users.findOne({\"name\" : \"joe\"}); > joe.relationships = {\"friends\" : joe.friends, \"enemies\" : joe.enemies}; { \"friends\" : 32, \"enemies\" : 2 } > joe.username = joe.name; \"joe\" > delete joe.friends; true > delete joe.enemies;\n\nUpdating Documents\n\n|\n\n35",
      "content_length": 1902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "true > delete joe.name; true > db.users.replaceOne({\"name\" : \"joe\"}, joe);\n\nNow, doing a findOne shows that the structure of the document has been updated:\n\n{ \"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7a\"), \"username\" : \"joe\", \"relationships\" : { \"friends\" : 32, \"enemies\" : 2 } }\n\nA common mistake is matching more than one document with the criteria and then creating a duplicate \"_id\" value with the second parameter. The database will throw an error for this, and no documents will be updated.\n\nFor example, suppose we create several documents with the same value for \"name\", but we don’t realize it:\n\n> db.people.find() {\"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7b\"), \"name\" : \"joe\", \"age\" : 65} {\"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7c\"), \"name\" : \"joe\", \"age\" : 20} {\"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7d\"), \"name\" : \"joe\", \"age\" : 49}\n\nNow, if it’s Joe #2’s birthday, we want to increment the value of his \"age\" key, so we might say this:\n\n> joe = db.people.findOne({\"name\" : \"joe\", \"age\" : 20}); { \"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7c\"), \"name\" : \"joe\", \"age\" : 20 } > joe.age++; > db.people.replaceOne({\"name\" : \"joe\"}, joe); E11001 duplicate key on update\n\nWhat happened? When you do the update, the database will look for a document matching {\"name\" : \"joe\"}. The first one it finds will be the 65-year-old Joe. It will attempt to replace that document with the one in the joe variable, but there’s already a document in this collection with the same \"_id\". Thus, the update will fail, because \"_id\" values must be unique. The best way to avoid this situation is to make sure that your update always specifies a unique document, perhaps by matching on a key like \"_id\". For the preceding example, this would be the correct update to use:\n\n> db.people.replaceOne({\"_id\" : ObjectId(\"4b2b9f67a1f631733d917a7c\")}, joe)\n\n36\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 1899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Using \"_id\" for the filter will also be efficient since\"_id\" values form the basis for the primary index of a collection. We’ll cover primary and secondary indexes and how indexing affects updates and other operations more in Chapter 5.\n\nUsing Update Operators Usually only certain portions of a document need to be updated. You can update spe‐ cific fields in a document using atomic update operators. Update operators are special keys that can be used to specify complex update operations, such as altering, adding, or removing keys, and even manipulating arrays and embedded documents.\n\nSuppose we’re keeping website analytics in a collection and want to increment a counter each time someone visits a page. We can use update operators to do this increment atomically. Each URL and its number of page views is stored in a docu‐ ment that looks like this:\n\n{ \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"), \"url\" : \"www.example.com\", \"pageviews\" : 52 }\n\nEvery time someone visits a page, we can find the page by its URL and use the \"$inc\" modifier to increment the value of the \"pageviews\" key:\n\n> db.analytics.updateOne({\"url\" : \"www.example.com\"}, ... {\"$inc\" : {\"pageviews\" : 1}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n\nNow, if we do a findOne, we see that \"pageviews\" has increased by one:\n\n> db.analytics.findOne() { \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"), \"url\" : \"www.example.com\", \"pageviews\" : 53 }\n\nWhen using operators, the value of \"_id\" cannot be changed. (Note that \"_id\" can be changed by using whole-document replacement.) Values for any other key, including other uniquely indexed keys, can be modified.\n\nGetting started with the “$set” modifier\n\n\"$set\" sets the value of a field. If the field does not yet exist, it will be created. This can be handy for updating schemas or adding user-defined keys. For example, sup‐ pose you have a simple user profile stored as a document that looks something like the following:\n\nUpdating Documents\n\n|\n\n37",
      "content_length": 1996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "> db.users.findOne() { \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"), \"name\" : \"joe\", \"age\" : 30, \"sex\" : \"male\", \"location\" : \"Wisconsin\" }\n\nThis is a pretty bare-bones user profile. If the user wanted to store his favorite book in his profile, he could add it using \"$set\":\n\n> db.users.updateOne({\"_id\" : ObjectId(\"4b253b067525f35f94b60a31\")}, ... {\"$set\" : {\"favorite book\" : \"War and Peace\"}})\n\nNow the document will have a \"favorite book\" key:\n\n> db.users.findOne() { \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"), \"name\" : \"joe\", \"age\" : 30, \"sex\" : \"male\", \"location\" : \"Wisconsin\", \"favorite book\" : \"War and Peace\" }\n\nIf the user decides that he actually enjoys a different book, \"$set\" can be used again to change the value:\n\n> db.users.updateOne({\"name\" : \"joe\"}, ... {\"$set\" : {\"favorite book\" : \"Green Eggs and Ham\"}})\n\n\"$set\" can even change the type of the key it modifies. For instance, if our fickle user decides that he actually likes quite a few books, he can change the value of the \"favor ite book\" key into an array:\n\n> db.users.updateOne({\"name\" : \"joe\"}, ... {\"$set\" : {\"favorite book\" : ... [\"Cat's Cradle\", \"Foundation Trilogy\", \"Ender's Game\"]}})\n\nIf the user realizes that he actually doesn’t like reading, he can remove the key alto‐ gether with \"$unset\":\n\n> db.users.updateOne({\"name\" : \"joe\"}, ... {\"$unset\" : {\"favorite book\" : 1}})\n\nNow the document will be the same as it was at the beginning of this example.\n\nYou can also use \"$set\" to reach in and change embedded documents:\n\n> db.blog.posts.findOne() { \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"),\n\n38\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "\"title\" : \"A Blog Post\", \"content\" : \"...\", \"author\" : { \"name\" : \"joe\", \"email\" : \"joe@example.com\" } } > db.blog.posts.updateOne({\"author.name\" : \"joe\"}, ... {\"$set\" : {\"author.name\" : \"joe schmoe\"}})\n\n> db.blog.posts.findOne() { \"_id\" : ObjectId(\"4b253b067525f35f94b60a31\"), \"title\" : \"A Blog Post\", \"content\" : \"...\", \"author\" : { \"name\" : \"joe schmoe\", \"email\" : \"joe@example.com\" } }\n\nYou must always use a $-modifier for adding, changing, or removing keys. A com‐ mon error people make when starting out is to try to set the value of a key to some other value by doing an update that resembles this:\n\n> db.blog.posts.updateOne({\"author.name\" : \"joe\"}, ... {\"author.name\" : \"joe schmoe\"})\n\nThis will result in an error. The update document must contain update operators. Previous versions of the CRUD API did not catch this type of error. Earlier update methods would simply complete a whole document replacement in such situations. It is this type of pitfall that led to the creation of a new CRUD API.\n\nIncrementing and decrementing\n\nThe \"$inc\" operator can be used to change the value for an existing key or to create a new key if it does not already exist. It’s useful for updating analytics, karma, votes, or anything else that has a changeable, numeric value.\n\nSuppose we are creating a game collection where we want to save games and update scores as they change. When a user starts playing, say, a game of pinball, we can insert a document that identifies the game by name and the user playing it:\n\n> db.games.insertOne({\"game\" : \"pinball\", \"user\" : \"joe\"})\n\nWhen the ball hits a bumper, the game should increment the player’s score. Since points in pinball are given out pretty freely, let’s say that the base unit of points a player can earn is 50. We can use the \"$inc\" modifier to add 50 to the player’s score:\n\n> db.games.updateOne({\"game\" : \"pinball\", \"user\" : \"joe\"}, ... {\"$inc\" : {\"score\" : 50}})\n\nUpdating Documents\n\n|\n\n39",
      "content_length": 1946,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "If we look at the document after this update, we’ll see the following:\n\n> db.games.findOne() { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"game\" : \"pinball\", \"user\" : \"joe\", \"score\" : 50 }\n\nThe \"score\" key did not already exist, so it was created by \"$inc\" and set to the increment amount: 50.\n\nIf the ball lands in a “bonus” slot, we want to add 10,000 to the score. We can do this by passing a different value to \"$inc\":\n\n> db.games.updateOne({\"game\" : \"pinball\", \"user\" : \"joe\"}, ... {\"$inc\" : {\"score\" : 10000}})\n\nNow if we look at the game, we’ll see the following:\n\n> db.games.findOne() { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"game\" : \"pinball\", \"user\" : \"joe\", \"score\" : 10050 }\n\nThe \"score\" key existed and had a numeric value, so the server added 10,000 to it.\n\n\"$inc\" is similar to \"$set\", but it is designed for incrementing (and decrementing) numbers. \"$inc\" can be used only on values of type integer, long, double, or decimal. If it is used on any other type of value, it will fail. This includes types that many lan‐ guages will automatically cast into numbers, like nulls, booleans, or strings of numeric characters:\n\n> db.strcounts.insert({\"count\" : \"1\"}) WriteResult({ \"nInserted\" : 1 }) > db.strcounts.update({}, {\"$inc\" : {\"count\" : 1}}) WriteResult({ \"nMatched\" : 0, \"nUpserted\" : 0, \"nModified\" : 0, \"writeError\" : { \"code\" : 16837, \"errmsg\" : \"Cannot apply $inc to a value of non-numeric type. {_id: ObjectId('5726c0d36855a935cb57a659')} has the field 'count' of non-numeric type String\" } })\n\n40\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 1589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Also, the value of the \"$inc\" key must be a number. You cannot increment by a string, array, or other nonnumeric value. Doing so will give a “Modifier \"$inc\" allowed for numbers only” error message. To modify other types, use \"$set\" or one of the following array operators.\n\nArray operators\n\nAn extensive class of update operators exists for manipulating arrays. Arrays are common and powerful data structures: not only are they lists that can be referenced by index, but they can also double as sets.\n\nAdding elements. \"$push\" adds elements to the end of an array if the array exists and creates a new array if it does not. For example, suppose that we are storing blog posts and want to add a \"comments\" key containing an array. We can push a comment onto the nonexistent \"comments\" array, which will create the array and add the comment:\n\n> db.blog.posts.findOne() { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"title\" : \"A blog post\", \"content\" : \"...\" } > db.blog.posts.updateOne({\"title\" : \"A blog post\"}, ... {\"$push\" : {\"comments\" : ... {\"name\" : \"joe\", \"email\" : \"joe@example.com\", ... \"content\" : \"nice post.\"}}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 } > db.blog.posts.findOne() { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"title\" : \"A blog post\", \"content\" : \"...\", \"comments\" : [ { \"name\" : \"joe\", \"email\" : \"joe@example.com\", \"content\" : \"nice post.\" } ] }\n\nNow, if we want to add another comment, we can simply use \"$push\" again:\n\n> db.blog.posts.updateOne({\"title\" : \"A blog post\"}, ... {\"$push\" : {\"comments\" : ... {\"name\" : \"bob\", \"email\" : \"bob@example.com\", ... \"content\" : \"good post.\"}}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 } > db.blog.posts.findOne() {\n\nUpdating Documents\n\n|\n\n41",
      "content_length": 1765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"title\" : \"A blog post\", \"content\" : \"...\", \"comments\" : [ { \"name\" : \"joe\", \"email\" : \"joe@example.com\", \"content\" : \"nice post.\" }, { \"name\" : \"bob\", \"email\" : \"bob@example.com\", \"content\" : \"good post.\" } ] }\n\nThis is the “simple” form of \"push\", but you can use it for more complex array opera‐ tions as well. The MongoDB query language provides modifiers for some operators, including \"$push\". You can push multiple values in one operation using the \"$each\" modifer for \"$push\":\n\n> db.stock.ticker.updateOne({\"_id\" : \"GOOG\"}, ... {\"$push\" : {\"hourly\" : {\"$each\" : [562.776, 562.790, 559.123]}}})\n\nThis would push three new elements onto the array.\n\nIf you only want the array to grow to a certain length, you can use the \"$slice\" modifier with \"$push\" to prevent an array from growing beyond a certain size, effec‐ tively making a “top N” list of items:\n\n> db.movies.updateOne({\"genre\" : \"horror\"}, ... {\"$push\" : {\"top10\" : {\"$each\" : [\"Nightmare on Elm Street\", \"Saw\"], ... \"$slice\" : -10}}})\n\nThis example limits the array to the last 10 elements pushed.\n\nIf the array is smaller than 10 elements (after the push), all elements will be kept. If the array is larger than 10 elements, only the last 10 elements will be kept. Thus, \"$slice\" can be used to create a queue in a document.\n\nFinally, you can apply the \"$sort\" modifier to \"$push\" operations before trimming:\n\n> db.movies.updateOne({\"genre\" : \"horror\"}, ... {\"$push\" : {\"top10\" : {\"$each\" : [{\"name\" : \"Nightmare on Elm Street\", ... \"rating\" : 6.6}, ... {\"name\" : \"Saw\", \"rating\" : 4.3}], ... \"$slice\" : -10, ... \"$sort\" : {\"rating\" : -1}}}})\n\n42\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 1716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "This will sort all of the objects in the array by their \"rating\" field and then keep the first 10. Note that you must include \"$each\"; you cannot just \"$slice\" or \"$sort\" an array with \"$push\".\n\nUsing arrays as sets. You might want to treat an array as a set, only adding values if they are not present. This can be done using \"$ne\" in the query document. For exam‐ ple, to push an author onto a list of citations, but only if they aren’t already there, use the following:\n\n> db.papers.updateOne({\"authors cited\" : {\"$ne\" : \"Richie\"}}, ... {$push : {\"authors cited\" : \"Richie\"}})\n\nThis can also be done with \"$addToSet\", which is useful for cases where \"$ne\" won’t work or where \"$addToSet\" describes what is happening better.\n\nFor example, suppose you have a document that represents a user. You might have a set of email addresses that they have added:\n\n> db.users.findOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"username\" : \"joe\", \"emails\" : [ \"joe@example.com\", \"joe@gmail.com\", \"joe@yahoo.com\" ] }\n\nWhen adding another address, you can use “$addToSet\" to prevent duplicates:\n\n> db.users.updateOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}, ... {\"$addToSet\" : {\"emails\" : \"joe@gmail.com\"}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 0 } > db.users.findOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"username\" : \"joe\", \"emails\" : [ \"joe@example.com\", \"joe@gmail.com\", \"joe@yahoo.com\", ] } > db.users.updateOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}, ... {\"$addToSet\" : {\"emails\" : \"joe@hotmail.com\"}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 } > db.users.findOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"username\" : \"joe\",\n\nUpdating Documents\n\n|\n\n43",
      "content_length": 1878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "\"emails\" : [ \"joe@example.com\", \"joe@gmail.com\", \"joe@yahoo.com\", \"joe@hotmail.com\" ] }\n\nYou can also use \"$addToSet\" in conjunction with \"$each\" to add multiple unique values, which cannot be done with the \"$ne\"/\"$push\" combination. For instance, you could use these operators if the user wanted to add more than one email address:\n\n> db.users.updateOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}, ... {\"$addToSet\" : {\"emails\" : {\"$each\" : ... [\"joe@php.net\", \"joe@example.com\", \"joe@python.org\"]}}}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 } > db.users.findOne({\"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\")}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"username\" : \"joe\", \"emails\" : [ \"joe@example.com\", \"joe@gmail.com\", \"joe@yahoo.com\", \"joe@hotmail.com\" \"joe@php.net\" \"joe@python.org\" ] }\n\nRemoving elements. There are a few ways to remove elements from an array. If you want to treat the array like a queue or a stack, you can use \"$pop\", which can remove elements from either end. {\"$pop\" : {\"key\" : 1}} removes an element from the end of the array. {\"$pop\" : {\"key\" : -1}} removes it from the beginning.\n\nSometimes an element should be removed based on specific criteria, rather than its position in the array. \"$pull\" is used to remove elements of an array that match the given criteria. For example, suppose we have a list of things that need to be done, but not in any specific order:\n\n> db.lists.insertOne({\"todo\" : [\"dishes\", \"laundry\", \"dry cleaning\"]})\n\nIf we do the laundry first, we can remove it from the list with the following:\n\n> db.lists.updateOne({}, {\"$pull\" : {\"todo\" : \"laundry\"}})\n\nNow if we do a find, we’ll see that there are only two elements remaining in the array:\n\n> db.lists.findOne() { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"todo\" : [\n\n44\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 1872,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "\"dishes\", \"dry cleaning\" ] }\n\nPulling removes all matching documents, not just a single match. If you have an array that looks like [1, 1, 2, 1] and pull 1, you’ll end up with a single-element array, [2].\n\nArray operators can be used only on keys with array values. For example, you cannot push onto an integer or pop off of a string. Use \"$set\" or \"$inc\" to modify scalar values.\n\nPositional array modifications. Array manipulation becomes a little trickier when you have multiple values in an array and want to modify some of them. There are two ways to manipulate values in arrays: by position or by using the position operator (the $ character).\n\nArrays use 0-based indexing, and elements can be selected as though their index were a document key. For example, suppose we have a document containing an array with a few embedded documents, such as a blog post with comments:\n\n> db.blog.posts.findOne() { \"_id\" : ObjectId(\"4b329a216cc613d5ee930192\"), \"content\" : \"...\", \"comments\" : [ { \"comment\" : \"good post\", \"author\" : \"John\", \"votes\" : 0 }, { \"comment\" : \"i thought it was too short\", \"author\" : \"Claire\", \"votes\" : 3 }, { \"comment\" : \"free watches\", \"author\" : \"Alice\", \"votes\" : -5 }, { \"comment\" : \"vacation getaways\", \"author\" : \"Lynn\", \"votes\" : -7 } ] }\n\nUpdating Documents\n\n|\n\n45",
      "content_length": 1293,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "If we want to increment the number of votes for the first comment, we can say the following:\n\n> db.blog.updateOne({\"post\" : post_id}, ... {\"$inc\" : {\"comments.0.votes\" : 1}})\n\nIn many cases, though, we don’t know what index of the array to modify without querying for the document first and examining it. To get around this, MongoDB has a positional operator, $, that figures out which element of the array the query docu‐ ment matched and updates that element. For example, if we have a user named John who updates his name to Jim, we can replace it in the comments by using the posi‐ tional operator:\n\n> db.blog.updateOne({\"comments.author\" : \"John\"}, ... {\"$set\" : {\"comments.$.author\" : \"Jim\"}})\n\nThe positional operator updates only the first match. Thus, if John had left more than one comment, his name would be changed only for the first comment he left.\n\nUpdates using array filters. MongoDB 3.6 introduced another option for updating indi‐ vidual array elements: arrayFilters. This option enables us to modify array ele‐ ments matching particular critera. For example, if we want to hide all comments with five or more down votes, we can do something like the following:\n\ndb.blog.updateOne( {\"post\" : post_id }, { $set: { \"comments.$[elem].hidden\" : true } }, { arrayFilters: [ { \"elem.votes\": { $lte: -5 } } ] } )\n\nThis command defines elem as the identifier for each matching element in the \"com ments\" array. If the votes value for the comment identified by elem is less than or equal to -5, we will add a field called \"hidden\" to the \"comments\" document and set its value to true.\n\nUpserts An upsert is a special type of update. If no document is found that matches the filter, a new document will be created by combining the criteria and updated documents. If a matching document is found, it will be updated normally. Upserts can be handy because they can eliminate the need to “seed” your collection: you can often have the same code create and update documents.\n\nLet’s go back to our example that records the number of views for each page of a web‐ site. Without an upsert, we might try to find the URL and increment the number of\n\n46\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 2210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "views or create a new document if the URL doesn’t exist. If we were to write this out as a JavaScript program it might look something like the following:\n\n// check if we have an entry for this page blog = db.analytics.findOne({url : \"/blog\"})\n\n// if we do, add one to the number of views and save if (blog) { blog.pageviews++; db.analytics.save(blog); } // otherwise, create a new document for this page else { db.analytics.insertOne({url : \"/blog\", pageviews : 1}) }\n\nThis means we are making a round trip to the database, plus sending an update or insert, every time someone visits a page. If we are running this code in multiple pro‐ cesses, we are also subject to a race condition where more than one document can be inserted for a given URL.\n\nWe can eliminate the race condition and cut down the amount of code by just send‐ ing an upsert to the database (the third parameter to updateOne and updateMany is an options document that enables us to specify this):\n\n> db.analytics.updateOne({\"url\" : \"/blog\"}, {\"$inc\" : {\"pageviews\" : 1}}, ... {\"upsert\" : true})\n\nThis line does exactly what the previous code block does, except it’s faster and atomic! The new document is created by using the criteria document as a base and applying any modifier documents to it.\n\nFor example, if you do an upsert that matches a key and increments to the value of that key, the increment will be applied to the match:\n\n> db.users.updateOne({\"rep\" : 25}, {\"$inc\" : {\"rep\" : 3}}, {\"upsert\" : true}) WriteResult({ \"acknowledged\" : true, \"matchedCount\" : 0, \"modifiedCount\" : 0, \"upsertedId\" : ObjectId(\"5a93b07aaea1cb8780a4cf72\") }) > db.users.findOne({\"_id\" : ObjectId(\"5727b2a7223502483c7f3acd\")} ) { \"_id\" : ObjectId(\"5727b2a7223502483c7f3acd\"), \"rep\" : 28 }\n\nThe upsert creates a new document with a \"rep\" of 25 and then increments that by 3, giving us a document where \"rep\" is 28. If the upsert option were not specified, {\"rep\" : 25} would not match any documents, so nothing would happen.\n\nUpdating Documents\n\n|\n\n47",
      "content_length": 2006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "If we run the upsert again (with the criterion {\"rep\" : 25}), it will create another new document. This is because the criterion does not match the only document in the collection. (Its \"rep\" is 28.)\n\nSometimes a field needs to be set when a document is created, but not changed on subsequent updates. This is what \"$setOnInsert\" is for. \"$setOnInsert\" is an opera‐ tor that only sets the value of a field when the document is being inserted. Thus, we could do something like this:\n\n> db.users.updateOne({}, {\"$setOnInsert\" : {\"createdAt\" : new Date()}}, ... {\"upsert\" : true}) { \"acknowledged\" : true, \"matchedCount\" : 0, \"modifiedCount\" : 0, \"upsertedId\" : ObjectId(\"5727b4ac223502483c7f3ace\") } > db.users.findOne() { \"_id\" : ObjectId(\"5727b4ac223502483c7f3ace\"), \"createdAt\" : ISODate(\"2016-05-02T20:12:28.640Z\") }\n\nIf we run this update again, it will match the existing document, nothing will be inserted, and so the \"createdAt\" field will not be changed:\n\n> db.users.updateOne({}, {\"$setOnInsert\" : {\"createdAt\" : new Date()}}, ... {\"upsert\" : true}) { \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 0 } > db.users.findOne() { \"_id\" : ObjectId(\"5727b4ac223502483c7f3ace\"), \"createdAt\" : ISODate(\"2016-05-02T20:12:28.640Z\") }\n\nNote that you generally do not need to keep a \"createdAt\" field, as ObjectIds con‐ tain a timestamp of when the document was created. However, \"$setOnInsert\" can be useful for creating padding, initializing counters, and for collections that do not use ObjectIds.\n\nThe save shell helper\n\nsave is a shell function that lets you insert a document if it doesn’t exist and update it if it does. It takes one argument: a document. If the document contains an \"_id\" key, save will do an upsert. Otherwise, it will do an insert. save is really just a conve‐ nience function so that programmers can quickly modify documents in the shell:\n\n> var x = db.testcol.findOne() > x.num = 42\n\n48\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 1983,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "42 > db.testcol.save(x)\n\nWithout save, the last line would have been more cumbersome:\n\ndb.testcol.replaceOne({\"_id\" : x._id}, x)\n\nUpdating Multiple Documents So far in this chapter we have used updateOne to illustrate update operations. updateOne updates only the first document found that matches the filter criteria. If there are more matching documents, they will remain unchanged. To modify all of the documents matching a filter, use updateMany. updateMany follows the same semantics as updateOne and takes the same parameters. The key difference is in the number of documents that might be changed.\n\nupdateMany provides a powerful tool for performing schema migrations or rolling out new features to certain users. Suppose, for example, we want to give a gift to every user who has a birthday on a certain day. We can use updateMany to add a \"gift\" to their accounts. For example:\n\n> db.users.insertMany([ ... {birthday: \"10/13/1978\"}, ... {birthday: \"10/13/1978\"}, ... {birthday: \"10/13/1978\"}]) { \"acknowledged\" : true, \"insertedIds\" : [ ObjectId(\"5727d6fc6855a935cb57a65b\"), ObjectId(\"5727d6fc6855a935cb57a65c\"), ObjectId(\"5727d6fc6855a935cb57a65d\") ] } > db.users.updateMany({\"birthday\" : \"10/13/1978\"}, ... {\"$set\" : {\"gift\" : \"Happy Birthday!\"}}) { \"acknowledged\" : true, \"matchedCount\" : 3, \"modifiedCount\" : 3 }\n\nThe call to updateMany adds a \"gift\" field to each of the three documents we inser‐ ted into the users collection immediately before.\n\nReturning Updated Documents For some use cases it is important to return the document modified. In earlier ver‐ sions of MongoDB, findAndModify was the method of choice in such situations. It is handy for manipulating queues and performing other operations that need get-and- set−style atomicity. However, findAndModify is prone to user error because it’s a complex method combining the functionality of three different types of operations: delete, replace, and update (including upserts).\n\nUpdating Documents\n\n|\n\n49",
      "content_length": 1978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "MongoDB 3.2 introduced three new collection methods to the shell to accommodate the functionality of findAndModify, but with semantics that are easier to learn and remember: findOneAndDelete, findOneAndReplace, and findOneAndUpdate. The primary difference between these methods and, for example, updateOne is that they enable you to atomically get the value of a modified document. MongoDB 4.2 exten‐ ded findOneAndUpdate to accept an aggregation pipeline for the update. The pipeline can consist of the following stages: $addFields and its alias $set, $project and its alias $unset, and $replaceRoot and its alias $replaceWith.\n\nSuppose we have a collection of processes run in a certain order. Each is represented with a document that has the following form:\n\n{ \"_id\" : ObjectId(), \"status\" : \"state\", \"priority\" : N }\n\n\"status\" is a string that can be \"READY\", \"RUNNING\", or \"DONE\". We need to find the job with the highest priority in the \"READY\" state, run the process function, and then update the status to \"DONE\". We might try querying for the ready processes, sorting by priority, and updating the status of the highest-priority process to mark it as \"RUNNING\". Once we have processed it, we update the status to \"DONE\". This looks something like the following:\n\nvar cursor = db.processes.find({\"status\" : \"READY\"}); ps = cursor.sort({\"priority\" : -1}).limit(1).next(); db.processes.updateOne({\"_id\" : ps._id}, {\"$set\" : {\"status\" : \"RUNNING\"}}); do_something(ps); db.processes.updateOne({\"_id\" : ps._id}, {\"$set\" : {\"status\" : \"DONE\"}});\n\nThis algorithm isn’t great because it is subject to a race condition. Suppose we have two threads running. If one thread (call it A) retrieved the document and another thread (call it B) retrieved the same document before A had updated its status to \"RUNNING\", then both threads would be running the same process. We can avoid this by checking the result as part of the update query, but this becomes complex:\n\nvar cursor = db.processes.find({\"status\" : \"READY\"}); cursor.sort({\"priority\" : -1}).limit(1); while ((ps = cursor.next()) != null) { var result = db.processes.updateOne({\"_id\" : ps._id, \"status\" : \"READY\"}, {\"$set\" : {\"status\" : \"RUNNING\"}});\n\nif (result.modifiedCount === 1) { do_something(ps); db.processes.updateOne({\"_id\" : ps._id}, {\"$set\" : {\"status\" : \"DONE\"}}); break; } cursor = db.processes.find({\"status\" : \"READY\"});\n\n50\n\n|\n\nChapter 3: Creating, Updating, and Deleting Documents",
      "content_length": 2451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "cursor.sort({\"priority\" : -1}).limit(1); }\n\nAlso, depending on timing, one thread may end up doing all the work while another thread uselessly trails it. Thread A could always grab the process, and then B would try to get the same process, fail, and leave A to do all the work.\n\nSituations like this are perfect for findOneAndUpdate. findOneAndUpdate can return the item and update it in a single operation. In this case, it looks like the following:\n\n> db.processes.findOneAndUpdate({\"status\" : \"READY\"}, ... {\"$set\" : {\"status\" : \"RUNNING\"}}, ... {\"sort\" : {\"priority\" : -1}}) { \"_id\" : ObjectId(\"4b3e7a18005cab32be6291f7\"), \"priority\" : 1, \"status\" : \"READY\" }\n\nNotice that the status is still \"READY\" in the returned document because the findOneAndUpdate method defaults to returning the state of the document before it was modified. It will return the updated document if we set the \"returnNewDocu ment\" field in the options document to true. An options document is passed as the third parameter to findOneAndUpdate:\n\n> db.processes.findOneAndUpdate({\"status\" : \"READY\"}, ... {\"$set\" : {\"status\" : \"RUNNING\"}}, ... {\"sort\" : {\"priority\" : -1}, ... \"returnNewDocument\": true}) { \"_id\" : ObjectId(\"4b3e7a18005cab32be6291f7\"), \"priority\" : 1, \"status\" : \"RUNNING\" }\n\nThus, the program becomes the following:\n\nps = db.processes.findOneAndUpdate({\"status\" : \"READY\"}, {\"$set\" : {\"status\" : \"RUNNING\"}}, {\"sort\" : {\"priority\" : -1}, \"returnNewDocument\": true}) do_something(ps) db.process.updateOne({\"_id\" : ps._id}, {\"$set\" : {\"status\" : \"DONE\"}})\n\nIn addition to this one, there are two other methods you should be aware of. findOneAndReplace takes the same parameters and returns the document matching the filter either before or after the replacement, depending on the value of returnNew Document. findOneAndDelete is similar except it does not take an update document as a parameter and has a subset of the options of the other two methods. findOneAnd Delete returns the deleted document.\n\nUpdating Documents\n\n|\n\n51",
      "content_length": 2019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "CHAPTER 4 Querying\n\nThis chapter looks at querying in detail. The main areas covered are as follows:\n\nYou can query for ranges, set inclusion, inequalities, and more by using $ conditionals.\n\nQueries return a database cursor, which lazily returns batches of documents as you need them.\n\nThere are a lot of metaoperations you can perform on a cursor, including skip‐ ping a certain number of results, limiting the number of results returned, and sorting results.\n\nIntroduction to find The find method is used to perform queries in MongoDB. Querying returns a subset of documents in a collection, from no documents at all to the entire collection. Which documents get returned is determined by the first argument to find, which is a docu‐ ment specifying the query criteria.\n\nAn empty query document (i.e., {}) matches everything in the collection. If find isn’t given a query document, it defaults to {}. For example, the following:\n\n> db.c.find()\n\nmatches every document in the collection c (and returns these documents in batches).\n\nWhen we start adding key/value pairs to the query document, we begin restricting our search. This works in a straightforward way for most types: numbers match num‐ bers, booleans match booleans, and strings match strings. Querying for a simple type is as easy as specifying the value that you are looking for. For example, to find all\n\n53",
      "content_length": 1372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "documents where the value for \"age\" is 27, we can add that key/value pair to the query document:\n\n> db.users.find({\"age\" : 27})\n\nIf we have a string we want to match, such as a \"username\" key with the value \"joe\", we use that key/value pair instead:\n\n> db.users.find({\"username\" : \"joe\"})\n\nMultiple conditions can be strung together by adding more key/value pairs to the query document, which gets interpreted as “condition1 AND condition2 AND … AND conditionN.” For instance, to get all users who are 27-year-olds with the user‐ name “joe,” we can query for the following:\n\n> db.users.find({\"username\" : \"joe\", \"age\" : 27})\n\nSpecifying Which Keys to Return Sometimes you do not need all of the key/value pairs in a document returned. If this is the case, you can pass a second argument to find (or findOne) specifying the keys you want. This reduces both the amount of data sent over the wire and the time and memory used to decode documents on the client side.\n\nFor example, if you have a user collection and you are interested only in the \"username\" and \"email\" keys, you could return just those keys with the following query:\n\n> db.users.find({}, {\"username\" : 1, \"email\" : 1}) { \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523620\"), \"username\" : \"joe\", \"email\" : \"joe@example.com\" }\n\nAs you can see from the previous output, the \"_id\" key is returned by default, even if it isn’t specifically requested.\n\nYou can also use this second parameter to exclude specific key/value pairs from the results of a query. For instance, you may have documents with a variety of keys, and the only thing you know is that you never want to return the \"fatal_weakness\" key:\n\n> db.users.find({}, {\"fatal_weakness\" : 0})\n\nThis can also prevent \"_id\" from being returned:\n\n> db.users.find({}, {\"username\" : 1, \"_id\" : 0}) { \"username\" : \"joe\", }\n\n54\n\n|\n\nChapter 4: Querying",
      "content_length": 1850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Limitations There are some restrictions on queries. The value of a query document must be a constant as far as the database is concerned. (It can be a normal variable in your own code.) That is, it cannot refer to the value of another key in the document. For exam‐ ple, if we were keeping inventory and we had both \"in_stock\" and \"num_sold\" keys, we couldn’t compare their values by querying the following:\n\n> db.stock.find({\"in_stock\" : \"this.num_sold\"}) // doesn't work\n\nThere are ways to do this (see “$where Queries” on page 65), but you will usually get better performance by restructuring your document slightly, such that a “normal” query will suffice. In this example, we could instead use the keys \"initial_stock\" and \"in_stock\". Then, every time someone buys an item, we decrement the value of the \"in_stock\" key by one. Finally, we can do a simple query to check which items are out of stock:\n\n> db.stock.find({\"in_stock\" : 0})\n\nQuery Criteria Queries can go beyond the exact matching described in the previous section; they can match more complex criteria, such as ranges, OR-clauses, and negation.\n\nQuery Conditionals \"$lt\", \"$lte\", \"$gt\", and \"$gte\" are all comparison operators, corresponding to <, <=, >, and >=, respectively. They can be combined to look for a range of values. For example, to look for users who are between the ages of 18 and 30, we can do this:\n\n> db.users.find({\"age\" : {\"$gte\" : 18, \"$lte\" : 30}})\n\nThis would find all documents where the \"age\" field was greater than or equal to 18 AND less than or equal to 30.\n\nThese types of range queries are often useful for dates. For example, to find people who registered before January 1, 2007, we can do this:\n\n> start = new Date(\"01/01/2007\") > db.users.find({\"registered\" : {\"$lt\" : start}})\n\nDepending on how you create and store dates, an exact match might be less useful, since dates are stored with millisecond precision. Often you want a whole day, week, or month, making a range query necessary.\n\nTo query for documents where a key’s value is not equal to a certain value, you must use another conditional operator, \"$ne\", which stands for “not equal.” If you want to find all users who do not have the username “joe,” you can query for them using this:\n\nQuery Criteria\n\n|\n\n55",
      "content_length": 2267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "> db.users.find({\"username\" : {\"$ne\" : \"joe\"}})\n\n\"$ne\" can be used with any type.\n\nOR Queries There are two ways to do an OR query in MongoDB. \"$in\" can be used to query for a variety of values for a single key. \"$or\" is more general; it can be used to query for any of the given values across multiple keys.\n\nIf you have more than one possible value to match for a single key, use an array of criteria with \"$in\". For instance, suppose we’re running a raffle and the winning ticket numbers are 725, 542, and 390. To find all three of these documents, we can construct the following query:\n\n> db.raffle.find({\"ticket_no\" : {\"$in\" : [725, 542, 390]}})\n\n\"$in\" is very flexible and allows you to specify criteria of different types as well as values. For example, if we are gradually migrating our schema to use usernames instead of user ID numbers, we can query for either by using this:\n\n> db.users.find({\"user_id\" : {\"$in\" : [12345, \"joe\"]}})\n\nThis matches documents with a \"user_id\" equal to 12345 and documents with a \"user_id\" equal to \"joe\".\n\nIf \"$in\" is given an array with a single value, it behaves the same as directly matching the value. For instance, {ticket_no : {$in : [725]}} matches the same documents as {ticket_no : 725}.\n\nThe opposite of \"$in\" is \"$nin\", which returns documents that don’t match any of the criteria in the array. If we want to return all of the people who didn’t win anything in the raffle, we can query for them with this:\n\n> db.raffle.find({\"ticket_no\" : {\"$nin\" : [725, 542, 390]}})\n\nThis query returns everyone who did not have tickets with those numbers.\n\n\"$in\" gives you an OR query for a single key, but what if we need to find documents where \"ticket_no\" is 725 or \"winner\" is true? For this type of query, we’ll need to use the \"$or\" conditional. \"$or\" takes an array of possible criteria. In the raffle case, using \"$or\" would look like this:\n\n> db.raffle.find({\"$or\" : [{\"ticket_no\" : 725}, {\"winner\" : true}]})\n\n\"$or\" can contain other conditionals. If, for example, we want to match any of the three \"ticket_no\" values or the \"winner\" key, we can use this:\n\n> db.raffle.find({\"$or\" : [{\"ticket_no\" : {\"$in\" : [725, 542, 390]}}, ... {\"winner\" : true}]})\n\n56\n\n|\n\nChapter 4: Querying",
      "content_length": 2227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "With a normal AND-type query, you want to narrow down your results as far as pos‐ sible in as few arguments as possible. OR-type queries are the opposite: they are most efficient if the first arguments match as many documents as possible.\n\nWhile \"$or\" will always work, use \"$in\" whenever possible as the query optimizer handles it more efficiently.\n\n$not \"$not\" is a metaconditional: it can be applied on top of any other criteria. As an example, let’s consider the modulus operator, \"$mod\". \"$mod\" queries for keys whose values, when divided by the first value given, have a remainder of the second value:\n\n> db.users.find({\"id_num\" : {\"$mod\" : [5, 1]}})\n\nThe previous query returns users with \"id_num\"s of 1, 6, 11, 16, and so on. If we want, instead, to return users with \"id_num\"s of 2, 3, 4, 5, 7, 8, 9, 10, 12, etc., we can use \"$not\":\n\n> db.users.find({\"id_num\" : {\"$not\" : {\"$mod\" : [5, 1]}}})\n\n\"$not\" can be particularly useful in conjunction with regular expressions to find all documents that don’t match a given pattern (regular expression usage is described in the section “Regular Expressions” on page 58).\n\nType-Specific Queries As covered in Chapter 2, MongoDB has a wide variety of types that can be used in a document. Some of these types have special behavior when querying.\n\nnull null behaves a bit strangely. It does match itself, so if we have a collection with the following documents:\n\n> db.c.find() { \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523621\"), \"y\" : null } { \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523622\"), \"y\" : 1 } { \"_id\" : ObjectId(\"4ba0f148d22aa494fd523623\"), \"y\" : 2 }\n\nwe can query for documents whose \"y\" key is null in the expected way:\n\n> db.c.find({\"y\" : null}) { \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523621\"), \"y\" : null }\n\nHowever, null also matches “does not exist.” Thus, querying for a key with the value null will return all documents lacking that key:\n\n> db.c.find({\"z\" : null}) { \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523621\"), \"y\" : null }\n\nType-Specific Queries\n\n|\n\n57",
      "content_length": 2013,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "{ \"_id\" : ObjectId(\"4ba0f0dfd22aa494fd523622\"), \"y\" : 1 } { \"_id\" : ObjectId(\"4ba0f148d22aa494fd523623\"), \"y\" : 2 }\n\nIf we only want to find keys whose value is null, we can check that the key is null and exists using the \"$exists\" conditional:\n\n> db.c.find({\"z\" : {\"$eq\" : null, \"$exists\" : true}})\n\nRegular Expressions \"$regex\" provides regular expression capabilities for pattern matching strings in queries. Regular expressions are useful for flexible string matching. For example, if we want to find all users with the name “Joe” or “joe,” we can use a regular expression to do case-insensitive matching:\n\n> db.users.find( {\"name\" : {\"$regex\" : /joe/i } })\n\nRegular expression flags (e.g., i) are allowed but not required. If we want to match not only various capitalizations of “joe,” but also “joey,” we can continue to improve our regular expression:\n\n> db.users.find({\"name\" : /joey?/i})\n\nMongoDB uses the Perl Compatible Regular Expression (PCRE) library to match reg‐ ular expressions; any regular expression syntax allowed by PCRE is allowed in MongoDB. It is a good idea to check your syntax with the JavaScript shell before using it in a query to make sure it matches what you think it matches.\n\nMongoDB can leverage an index for queries on prefix regular expressions (e.g., /^joey/). Indexes cannot be used for case- insensitive searches (/^joey/i). A regular expression is a “prefix expression” when it starts with either a caret (^) or a left anchor (\\A). If the regular expression uses a case-sensitive query, then if an index exists for the field, the matches can be conducted against val‐ ues in the index. If it also is a prefix expression, then the search can be limited to the values within the range created by that prefix from the index.\n\nRegular expressions can also match themselves. Very few people insert regular expressions into the database, but if you insert one, you can match it with itself:\n\n> db.foo.insertOne({\"bar\" : /baz/}) > db.foo.find({\"bar\" : /baz/}) { \"_id\" : ObjectId(\"4b23c3ca7525f35f94b60a2d\"), \"bar\" : /baz/ }\n\n58\n\n|\n\nChapter 4: Querying",
      "content_length": 2085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Querying Arrays Querying for elements of an array is designed to behave the way querying for scalars does. For example, if the array is a list of fruits, like this:\n\n> db.food.insertOne({\"fruit\" : [\"apple\", \"banana\", \"peach\"]})\n\nthe following query will successfully match the document:\n\n> db.food.find({\"fruit\" : \"banana\"})\n\nWe can query for it in much the same way as we would if we had a document that looked like the (illegal) document {\"fruit\" : \"apple\", \"fruit\" : \"banana\", \"fruit\" : \"peach\"}.\n\n“$all”\n\nIf you need to match arrays by more than one element, you can use \"$all\". This allows you to match a list of elements. For example, suppose we create a collection with three elements:\n\n> db.food.insertOne({\"_id\" : 1, \"fruit\" : [\"apple\", \"banana\", \"peach\"]}) > db.food.insertOne({\"_id\" : 2, \"fruit\" : [\"apple\", \"kumquat\", \"orange\"]}) > db.food.insertOne({\"_id\" : 3, \"fruit\" : [\"cherry\", \"banana\", \"apple\"]})\n\nThen we can find all documents with both \"apple\" and \"banana\" elements by query‐ ing with \"$all\":\n\n> db.food.find({fruit : {$all : [\"apple\", \"banana\"]}}) {\"_id\" : 1, \"fruit\" : [\"apple\", \"banana\", \"peach\"]} {\"_id\" : 3, \"fruit\" : [\"cherry\", \"banana\", \"apple\"]}\n\nOrder does not matter. Notice \"banana\" comes before \"apple\" in the second result. Using a one-element array with \"$all\" is equivalent to not using \"$all\". For instance, {fruit : {$all : ['apple']} will match the same documents as {fruit : 'apple'}.\n\nYou can also query by exact match using the entire array. However, exact match will not match a document if any elements are missing or superfluous. For example, this will match the first of our three documents:\n\n> db.food.find({\"fruit\" : [\"apple\", \"banana\", \"peach\"]})\n\nBut this will not:\n\n> db.food.find({\"fruit\" : [\"apple\", \"banana\"]})\n\nand neither will this:\n\n> db.food.find({\"fruit\" : [\"banana\", \"apple\", \"peach\"]})\n\nIf you want to query for a specific element of an array, you can specify an index using the syntax key.index:\n\nType-Specific Queries\n\n|\n\n59",
      "content_length": 1988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "> db.food.find({\"fruit.2\" : \"peach\"})\n\nArrays are always 0-indexed, so this would match the third array element against the string \"peach\".\n\n“$size”\n\nA useful conditional for querying arrays is \"$size\", which allows you to query for arrays of a given size. Here’s an example:\n\n> db.food.find({\"fruit\" : {\"$size\" : 3}})\n\nOne common query is to get a range of sizes. \"$size\" cannot be combined with another $ conditional (in this example, \"$gt\"), but this query can be accomplished by adding a \"size\" key to the document. Then, every time you add an element to the array, increment the value of \"size\". If the original update looked like this:\n\n> db.food.update(criteria, {\"$push\" : {\"fruit\" : \"strawberry\"}})\n\nit can simply be changed to this:\n\n> db.food.update(criteria, ... {\"$push\" : {\"fruit\" : \"strawberry\"}, \"$inc\" : {\"size\" : 1}})\n\nIncrementing is extremely fast, so any performance penalty is negligible. Storing documents like this allows you to do queries such as this:\n\n> db.food.find({\"size\" : {\"$gt\" : 3}})\n\nUnfortunately, this technique doesn’t work as well with the \"$addToSet\" operator.\n\n“$slice”\n\nAs mentioned earlier in this chapter, the optional second argument to find specifies the keys to be returned. The special \"$slice\" operator can be used to return a subset of elements for an array key.\n\nFor example, suppose we had a blog post document and we wanted to return the first 10 comments:\n\n> db.blog.posts.findOne(criteria, {\"comments\" : {\"$slice\" : 10}})\n\nAlternatively, if we wanted the last 10 comments, we could use −10:\n\n> db.blog.posts.findOne(criteria, {\"comments\" : {\"$slice\" : -10}})\n\n\"$slice\" can also return pages in the middle of the results by taking an offset and the number of elements to return:\n\n> db.blog.posts.findOne(criteria, {\"comments\" : {\"$slice\" : [23, 10]}})\n\nThis would skip the first 23 elements and return the 24th through 33rd. If there were fewer than 33 elements in the array, it would return as many as possible.\n\n60\n\n|\n\nChapter 4: Querying",
      "content_length": 1994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Unless otherwise specified, all keys in a document are returned when \"$slice\" is used. This is unlike the other key specifiers, which suppress unmentioned keys from being returned. For instance, if we had a blog post document that looked like this:\n\n{ \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"title\" : \"A blog post\", \"content\" : \"...\", \"comments\" : [ { \"name\" : \"joe\", \"email\" : \"joe@example.com\", \"content\" : \"nice post.\" }, { \"name\" : \"bob\", \"email\" : \"bob@example.com\", \"content\" : \"good post.\" } ] }\n\nand we did a \"$slice\" to get the last comment, we’d get this:\n\n> db.blog.posts.findOne(criteria, {\"comments\" : {\"$slice\" : -1}}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"title\" : \"A blog post\", \"content\" : \"...\", \"comments\" : [ { \"name\" : \"bob\", \"email\" : \"bob@example.com\", \"content\" : \"good post.\" } ] }\n\nBoth \"title\" and \"content\" are still returned, even though they weren’t explicitly included in the key specifier.\n\nReturning a matching array element\n\n\"$slice\" is helpful when you know the index of the element, but sometimes you want whichever array element matched your criteria. You can return the matching element with the $ operator. Given the previous blog example, you could get Bob’s comment back with:\n\n> db.blog.posts.find({\"comments.name\" : \"bob\"}, {\"comments.$\" : 1}) { \"_id\" : ObjectId(\"4b2d75476cc613d5ee930164\"), \"comments\" : [\n\nType-Specific Queries\n\n|\n\n61",
      "content_length": 1397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "{ \"name\" : \"bob\", \"email\" : \"bob@example.com\", \"content\" : \"good post.\" } ] }\n\nNote that this only returns the first match for each document: if Bob had left multiple comments on this post, only the first one in the \"comments\" array would be returned.\n\nArray and range query interactions\n\nScalars (nonarray elements) in documents must match each clause of a query’s crite‐ ria. For example, if you queried for {\"x\" : {\"$gt\" : 10, \"$lt\" : 20}}, \"x\" would have to be both greater than 10 and less than 20. However, if a document’s \"x\" field is an array, the document matches if there is an element of \"x\" that matches each part of the criteria but each query clause can match a different array element.\n\nThe best way to understand this behavior is to see an example. Suppose we have the following documents:\n\n{\"x\" : 5} {\"x\" : 15} {\"x\" : 25} {\"x\" : [5, 25]}\n\nIf we wanted to find all documents where \"x\" is between 10 and 20, we might naively structure a query as db.test.find({\"x\" : {\"$gt\" : 10, \"$lt\" : 20}}) and expect to get back one document: {\"x\" : 15}. However, running this, we get two:\n\n> db.test.find({\"x\" : {\"$gt\" : 10, \"$lt\" : 20}}) {\"x\" : 15} {\"x\" : [5, 25]}\n\nNeither 5 nor 25 is between 10 and 20, but the document is returned because 25 matches the first clause (it is greater than 10) and 5 matches the second clause (it is less than 20).\n\nThis makes range queries against arrays essentially useless: a range will match any multielement array. There are a couple of ways to get the expected behavior.\n\nFirst, you can use \"$elemMatch\" to force MongoDB to compare both clauses with a single array element. However, the catch is that \"$elemMatch\" won’t match nonarray elements:\n\n> db.test.find({\"x\" : {\"$elemMatch\" : {\"$gt\" : 10, \"$lt\" : 20}}}) > // no results\n\nThe document {\"x\" : 15} no longer matches the query, because the \"x\" field is not an array. That said, you should have a good reason for mixing array and scalar values\n\n62\n\n|\n\nChapter 4: Querying",
      "content_length": 1967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "in a field. Many uses cases do not require mixing. For those, \"$elemMatch\" provides a good solution for range queries on array elements.\n\nIf you have an index over the field that you’re querying on (see Chapter 5), you can use min and max to limit the index range traversed by the query to your \"$gt\" and \"$lt\" values:\n\n> db.test.find({\"x\" : {\"$gt\" : 10, \"$lt\" : 20}}).min({\"x\" : 10}).max({\"x\" : 20}) {\"x\" : 15}\n\nNow this will only traverse the index from 10 to 20, missing the 5 and 25 entries. You can only use min and max when you have an index on the field you are querying for, though, and you must pass all fields of the index to min and max.\n\nUsing min and max when querying for ranges over documents that may include arrays is generally a good idea. The index bounds for a \"$gt\"/\"$lt\" query over an array is inefficient. It basically accepts any value, so it will search every index entry, not just those in the range.\n\nQuerying on Embedded Documents There are two ways of querying for an embedded document: querying for the whole document or querying for its individual key/value pairs.\n\nQuerying for an entire embedded document works identically to a normal query. For example, if we have a document that looks like this:\n\n{ \"name\" : { \"first\" : \"Joe\", \"last\" : \"Schmoe\" }, \"age\" : 45 }\n\nwe can query for someone named Joe Schmoe with the following:\n\n> db.people.find({\"name\" : {\"first\" : \"Joe\", \"last\" : \"Schmoe\"}})\n\nHowever, a query for a full subdocument must exactly match the subdocument. If Joe decides to add a middle name field, suddenly this query won’t work anymore; it doesn’t match the entire embedded document! This type of query is also order- sensitive: {\"last\" : \"Schmoe\", \"first\" : \"Joe\"} would not be a match.\n\nIf possible, it’s usually a good idea to query for just a specific key or keys of an embed‐ ded document. Then, if your schema changes, all of your queries won’t suddenly break because they’re no longer exact matches. You can query for embedded keys using dot notation:\n\n> db.people.find({\"name.first\" : \"Joe\", \"name.last\" : \"Schmoe\"})\n\nType-Specific Queries\n\n|\n\n63",
      "content_length": 2104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Now, if Joe adds more keys, this query will still match his first and last names.\n\nThis dot notation is the main difference between query documents and other docu‐ ment types. Query documents can contain dots, which mean “reach into an embed‐ ded document.” Dot notation is also the reason that documents to be inserted cannot contain the . character. Oftentimes people run into this limitation when trying to save URLs as keys. One way to get around it is to always perform a global replace before inserting or after retrieving, substituting a character that isn’t legal in URLs for the dot character.\n\nEmbedded document matches can get a little tricky as the document structure gets more complicated. For example, suppose we are storing blog posts and we want to find comments by Joe that were scored at least a 5. We could model the post as follows:\n\n> db.blog.find() { \"content\" : \"...\", \"comments\" : [ { \"author\" : \"joe\", \"score\" : 3, \"comment\" : \"nice post\" }, { \"author\" : \"mary\", \"score\" : 6, \"comment\" : \"terrible post\" } ] }\n\nNow, we can’t query using db.blog.find({\"comments\" : {\"author\" : \"joe\", \"score\" : {\"$gte\" : 5}}}). Embedded document matches have to match the whole document, and this doesn’t match the \"comment\" key. It also wouldn’t work to do db.blog.find({\"comments.author\" : \"joe\", \"comments.score\" : {\"$gte\" : 5}}), because the author criterion could match a different comment than the score criterion. That is, it would return the document shown above: it would match \"author\" : \"joe\" in the first comment and \"score\" : 6 in the second comment.\n\nTo correctly group criteria without needing to specify every key, use \"$elemMatch\". This vaguely named conditional allows you to partially specify criteria to match a sin‐ gle embedded document in an array. The correct query looks like this:\n\n> db.blog.find({\"comments\" : {\"$elemMatch\" : ... {\"author\" : \"joe\", \"score\" : {\"$gte\" : 5}}}})\n\n\"$elemMatch\" allows you to “group” your criteria. As such, it’s only needed when you have more than one key you want to match on in an embedded document.\n\n64\n\n|\n\nChapter 4: Querying",
      "content_length": 2092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "$where Queries Key/value pairs are a fairly expressive way to query, but there are some queries that they cannot represent. For queries that cannot be done any other way, there are \"$where\" clauses, which allow you to execute arbitrary JavaScript as part of your query. This allows you to do (almost) anything within a query. For security, use of \"$where\" clauses should be highly restricted or eliminated. End users should never be allowed to execute arbitrary \"$where\" clauses.\n\nThe most common case for using \"$where\" is to compare the values for two keys in a document. For instance, suppose we have documents that look like this:\n\n> db.foo.insertOne({\"apple\" : 1, \"banana\" : 6, \"peach\" : 3}) > db.foo.insertOne({\"apple\" : 8, \"spinach\" : 4, \"watermelon\" : 4})\n\nWe’d like to return documents where any two of the fields are equal. For example, in the second document, \"spinach\" and \"watermelon\" have the same value, so we’d like that document returned. It’s unlikely MongoDB will ever have a $ conditional for this, so we can use a \"$where\" clause to do it with JavaScript:\n\n> db.foo.find({\"$where\" : function () { ... for (var current in this) { ... for (var other in this) { ... if (current != other && this[current] == this[other]) { ... return true; ... } ... } ... } ... return false; ... }});\n\nIf the function returns true, the document will be part of the result set; if it returns false, it won’t be.\n\n\"$where\" queries should not be used unless strictly necessary: they are much slower than regular queries. Each document has to be converted from BSON to a JavaScript object and then run through the \"$where\" expression. Indexes cannot be used to sat‐ isfy a \"$where\" either. Hence, you should use \"$where\" only when there is no other way of doing the query. You can cut down on the penalty by using other query filters in combination with \"$where\". If possible, an index will be used to filter based on the non-$where clauses; the \"$where\" expression will be used only to fine-tune the results. MongoDB 3.6 added the $expr operator which allows the use of aggregation expressions with the MongoDB query language. It is faster than $where as it does not execute JavaScript and is recommended as a replacement to this operator where possible.\n\nAnother way of doing complex queries is to use one of the aggregation tools, which are covered in Chapter 7.\n\n$where Queries\n\n|\n\n65",
      "content_length": 2385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Cursors The database returns results from find using a cursor. The client-side implementa‐ tions of cursors generally allow you to control a great deal about the eventual output of a query. You can limit the number of results, skip over some number of results, sort results by any combination of keys in any direction, and perform a number of other powerful operations.\n\nTo create a cursor with the shell, put some documents into a collection, do a query on them, and assign the results to a local variable (variables defined with \"var\" are local). Here, we create a very simple collection and query it, storing the results in the cursor variable:\n\n> for(i=0; i<100; i++) { ... db.collection.insertOne({x : i}); ... } > var cursor = db.collection.find();\n\nThe advantage of doing this is that you can look at one result at a time. If you store the results in a global variable or no variable at all, the MongoDB shell will automati‐ cally iterate through and display the first couple of documents. This is what we’ve been seeing up until this point, and it is often the behavior you want for seeing what’s in a collection but not doing actual programming with the shell.\n\nTo iterate through the results, you can use the next method on the cursor. You can use hasNext to check whether there is another result. A typical loop through result looks like the following:\n\n> while (cursor.hasNext()) { ... obj = cursor.next(); ... // do stuff ... }\n\ncursor.hasNext() checks that the next result exists, and cursor.next() fetches it.\n\nThe cursor class also implements JavaScript’s iterator interface, so you can use it in a forEach loop:\n\n> var cursor = db.people.find(); > cursor.forEach(function(x) { ... print(x.name); ... }); adam matt zak\n\nWhen you call find, the shell does not query the database immediately. It waits until you start requesting results to send the query, which allows you to chain additional options onto a query before it is performed. Almost every method on a cursor object\n\n66\n\n|\n\nChapter 4: Querying",
      "content_length": 2018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "returns the cursor itself, so that you can chain options in any order. For instance, all of the following are equivalent:\n\n> var cursor = db.foo.find().sort({\"x\" : 1}).limit(1).skip(10); > var cursor = db.foo.find().limit(1).sort({\"x\" : 1}).skip(10); > var cursor = db.foo.find().skip(10).limit(1).sort({\"x\" : 1});\n\nAt this point, the query has not been executed yet. All of these functions merely build the query. Now, suppose we call the following:\n\n> cursor.hasNext()\n\nAt this point, the query will be sent to the server. The shell fetches the first 100 results or first 4 MB of results (whichever is smaller) at once so that the next calls to next or hasNext will not have to make trips to the server. After the client has run through the first set of results, the shell will again contact the database and ask for more results with a getMore request. getMore requests basically contain an identifier for the cursor and ask the database if there are any more results, returning the next batch if there are. This process continues until the cursor is exhausted and all results have been returned.\n\nLimits, Skips, and Sorts The most common query options are limiting the number of results returned, skip‐ ping a number of results, and sorting. All these options must be added before a query is sent to the database.\n\nTo set a limit, chain the limit function onto your call to find. For example, to only return three results, use this:\n\n> db.c.find().limit(3)\n\nIf there are fewer than three documents matching your query in the collection, only the number of matching documents will be returned; limit sets an upper limit, not a lower limit.\n\nskip works similarly to limit:\n\n> db.c.find().skip(3)\n\nThis will skip the first three matching documents and return the rest of the matches. If there are fewer than three documents in your collection, it will not return any documents.\n\nsort takes an object: a set of key/value pairs where the keys are key names and the values are the sort directions. The sort direction can be 1 (ascending) or −1 (descend‐ ing). If multiple keys are given, the results will be sorted in that order. For instance, to sort the results by \"username\" ascending and \"age\" descending, we do the following:\n\n> db.c.find().sort({username : 1, age : -1})\n\nCursors\n\n|\n\n67",
      "content_length": 2290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "These three methods can be combined. This is often handy for pagination. For exam‐ ple, suppose that you are running an online store and someone searches for mp3. If you want 50 results per page sorted by price from high to low, you can do the following:\n\n> db.stock.find({\"desc\" : \"mp3\"}).limit(50).sort({\"price\" : -1})\n\nIf that person clicks Next Page to see more results, you can simply add a skip to the query, which will skip over the first 50 matches (which the user already saw on page 1):\n\n> db.stock.find({\"desc\" : \"mp3\"}).limit(50).skip(50).sort({\"price\" : -1})\n\nHowever, large skips are not very performant; there are suggestions for how to avoid them in the next section.\n\nComparison order\n\nMongoDB has a hierarchy as to how types compare. Sometimes you will have a single key with multiple types: for instance, integers and booleans, or strings and nulls. If you do a sort on a key with a mix of types, there is a predefined order that they will be sorted in. From least to greatest value, this ordering is as follows:\n\n1. Minimum value\n\n2. Null\n\n3. Numbers (integers, longs, doubles, decimals)\n\n4. Strings\n\n5. Object/document\n\n6. Array\n\n7. Binary data\n\n8. Object ID\n\n9. Boolean\n\n10. Date\n\n11. Timestamp\n\n12. Regular expression\n\n13. Maximum value\n\nAvoiding Large Skips Using skip for a small number of documents is fine. But for a large number of results, skip can be slow, since it has to find and then discard all the skipped results. Most\n\n68\n\n|\n\nChapter 4: Querying",
      "content_length": 1482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "databases keep more metadata in the index to help with skips, but MongoDB does not yet support this, so large skips should be avoided. Often you can calculate the results of the next query based on the previous one.\n\nPaginating results without skip\n\nThe easiest way to do pagination is to return the first page of results using limit and then return each subsequent page as an offset from the beginning:\n\n> // do not use: slow for large skips > var page1 = db.foo.find(criteria).limit(100) > var page2 = db.foo.find(criteria).skip(100).limit(100) > var page3 = db.foo.find(criteria).skip(200).limit(100) ...\n\nHowever, depending on your query, you can usually find a way to paginate without skips. For example, suppose we want to display documents in descending order based on \"date\". We can get the first page of results with the following:\n\n> var page1 = db.foo.find().sort({\"date\" : -1}).limit(100)\n\nThen, assuming the date is unique, we can use the \"date\" value of the last document as the criterion for fetching the next page:\n\nvar latest = null;\n\n// display first page while (page1.hasNext()) { latest = page1.next(); display(latest); }\n\n// get next page var page2 = db.foo.find({\"date\" : {\"$lt\" : latest.date}}); page2.sort({\"date\" : -1}).limit(100);\n\nNow the query does not need to include a skip.\n\nFinding a random document\n\nOne fairly common problem is how to get a random document from a collection. The naive (and slow) solution is to count the number of documents and then do a find, skipping a random number of documents between zero and the size of the collection:\n\n> // do not use > var total = db.foo.count() > var random = Math.floor(Math.random()*total) > db.foo.find().skip(random).limit(1)\n\nIt is actually highly inefficient to get a random element this way: you have to do a count (which can be expensive if you are using criteria), and skipping large numbers of elements can be time-consuming.\n\nCursors\n\n|\n\n69",
      "content_length": 1931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "It takes a little forethought, but if you know you’ll be looking up a random element in a collection, there’s a much more efficient way to do so. The trick is to add an extra random key to each document when it is inserted. For instance, if we’re using the shell, we could use the Math.random() function (which creates a random number between 0 and 1):\n\n> db.people.insertOne({\"name\" : \"joe\", \"random\" : Math.random()}) > db.people.insertOne({\"name\" : \"john\", \"random\" : Math.random()}) > db.people.insertOne({\"name\" : \"jim\", \"random\" : Math.random()})\n\nNow, when we want to find a random document from the collection, we can calculate a random number and use that as a query criterion, instead of using skip:\n\n> var random = Math.random() > result = db.people.findOne({\"random\" : {\"$gt\" : random}})\n\nThere is a slight chance that random will be greater than any of the \"random\" values in the collection, and no results will be returned. We can guard against this by simply returning a document in the other direction:\n\n> if (result == null) { ... result = db.people.findOne({\"random\" : {\"$lte\" : random}}) ... }\n\nIf there aren’t any documents in the collection, this technique will end up returning null, which makes sense.\n\nThis technique can be used with arbitrarily complex queries; just make sure to have an index that includes the random key. For example, if we want to find a random plumber in California, we can create an index on \"profession\", \"state\", and \"random\":\n\n> db.people.ensureIndex({\"profession\" : 1, \"state\" : 1, \"random\" : 1})\n\nThis allows us to quickly find a random result (see Chapter 5 for more information on indexing).\n\nImmortal Cursors There are two sides to a cursor: the client-facing cursor and the database cursor that the client-side one represents. We have been talking about the client-side one up until now, but we are going to take a brief look at what’s happening on the server.\n\nOn the server side, a cursor takes up memory and resources. Once a cursor runs out of results or the client sends a message telling it to die, the database can free the resources it was using. Freeing these resources lets the database use them for other things, which is good, so we want to make sure that cursors can be freed quickly (within reason).\n\n70\n\n|\n\nChapter 4: Querying",
      "content_length": 2297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "There are a couple of conditions that can cause the death (and subsequent cleanup) of a cursor. First, when a cursor finishes iterating through the matching results, it will clean itself up. Another way is that, when a cursor goes out of scope on the client side, the drivers send the database a special message to let it know that it can kill that cur‐ sor. Finally, even if the user hasn’t iterated through all the results and the cursor is still in scope, after 10 minutes of inactivity, a database cursor will automatically “die.” This way, if a client crashes or is buggy, MongoDB will not be left with thousands of open cursors.\n\nThis “death by timeout” is usually the desired behavior: very few applications expect their users to sit around for minutes at a time waiting for results. However, some‐ times you might know that you need a cursor to last for a long time. In that case, many drivers have implemented a function called immortal, or a similar mechanism, which tells the database not to time out the cursor. If you turn off a cursor’s timeout, you must iterate through all of its results or kill it to make sure it gets closed. Other‐ wise, it will sit around in the database hogging resources until the server is restarted.\n\nCursors\n\n|\n\n71",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "PART II Designing Your Application",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "CHAPTER 5 Indexes\n\nThis chapter introduces MongoDB indexes. Indexes enable you to perform queries efficiently. They’re an important part of application development and are even required for certain types of queries. In this chapter we will cover:\n\nWhat indexes are and why you’d want to use them\n\nHow to choose which fields to index\n\nHow to enforce and evaluate index usage\n\nAdministrative details on creating and removing indexes\n\nAs you’ll see, choosing the right indexes for your collections is critical to performance.\n\nIntroduction to Indexes A database index is similar to a book’s index. Instead of looking through the whole book, the database takes a shortcut and just looks at an ordered list with references to the content. This allows MongoDB to query orders of magnitude faster.\n\nA query that does not use an index is called a collection scan, which means that the server has to “look through the whole book” to find a query’s results. This process is basically what you’d do if you were looking for information in a book without an index: you’d start at page 1 and read through the whole thing. In general, you want to avoid making the server do collection scans because the process is very slow for large collections.\n\nLet’s look at an example. To get started, we’ll create a collection with 1 million docu‐ ments in it (or 10 million or 100 million, if you have the patience):\n\n75",
      "content_length": 1395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "> for (i=0; i<1000000; i++) { ... db.users.insertOne( ... { ... \"i\" : i, ... \"username\" : \"user\"+i, ... \"age\" : Math.floor(Math.random()*120), ... \"created\" : new Date() ... } ... ); ... }\n\nThen we’ll look at the differences in performance for queries on this collection, first without an index and then with an index.\n\nIf we do a query on this collection, we can use the explain command to see what MongoDB is doing when it executes the query. The preferred way to use the explain command is through the cursor helper method that wraps this command. The explain cursor method provides information on the execution of a variety of CRUD operations. This method may be run in several verbosity modes. We’ll look at execu tionStats mode since this helps us understand the effect of using an index to satisfy queries. Try querying on a specific username to see an example:\n\n> db.users.find({\"username\": \"user101\"}).explain(\"executionStats\") { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"username\" : { \"$eq\" : \"user101\" } }, \"winningPlan\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"username\" : { \"$eq\" : \"user101\" } }, \"direction\" : \"forward\" }, \"rejectedPlans\" : [ ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 1, \"executionTimeMillis\" : 419, \"totalKeysExamined\" : 0, \"totalDocsExamined\" : 1000000, \"executionStages\" : {\n\n76\n\n|\n\nChapter 5: Indexes",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "\"stage\" : \"COLLSCAN\", \"filter\" : { \"username\" : { \"$eq\" : \"user101\" } }, \"nReturned\" : 1, \"executionTimeMillisEstimate\" : 375, \"works\" : 1000002, \"advanced\" : 1, \"needTime\" : 1000000, \"needYield\" : 0, \"saveState\" : 7822, \"restoreState\" : 7822, \"isEOF\" : 1, \"invalidates\" : 0, \"direction\" : \"forward\", \"docsExamined\" : 1000000 } }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\n“explain Output” on page 116 will explain the output fields; for now you can ignore almost all of them. For this example, we want to look at the nested document that is the value of the \"executionStats\" field. In this document, \"totalDocsExamined\" is the number of documents MongoDB looked at while trying to satisfy the query, which, as you can see, is every document in the collection. That is, MongoDB had to look through every field in every document. This took nearly half a second to accom‐ plish on my laptop (the \"executionTimeMillis\" field shows the number of milli‐ seconds it took to execute the query).\n\nThe \"nReturned\" field of the \"executionStats\" document shows the number of results returned: 1, which makes sense because there is only one user with the user‐ name \"user101\". Note that MongoDB had to look through every document in the collection for matches because it did not know that usernames are unique.\n\nTo enable MongoDB to respond to queries efficiently, all query patterns in your appli‐ cation should be supported by an index. By query patterns, we simply mean the dif‐ ferent types of questions your application asks of the database. In this example, we queried the users collection by username. That is an example of a specific query pat‐ tern. In many applications, a single index will support several query patterns. We will discuss tailoring indexes to query patterns in a later section.\n\nIntroduction to Indexes\n\n|\n\n77",
      "content_length": 1944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Creating an Index Now let’s try creating an index on the \"username\" field. To create an index, we’ll use the createIndex collection method:\n\n> db.users.createIndex({\"username\" : 1}) { \"createdCollectionAutomatically\" : false, \"numIndexesBefore\" : 1, \"numIndexesAfter\" : 2, \"ok\" : 1 }\n\nCreating the index should take no longer than a few seconds, unless you made your collection especially large. If the createIndex call does not return after a few seconds, run db.currentOp() (in a different shell) or check your mongod’s log to see the index build’s progress.\n\nOnce the index build is complete, try repeating the original query:\n\n> db.users.find({\"username\": \"user101\"}).explain(\"executionStats\") { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"username\" : { \"$eq\" : \"user101\" } }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"[\\\"user101\\\", \\\"user101\\\"]\" ]\n\n78\n\n|\n\nChapter 5: Indexes",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "} } }, \"rejectedPlans\" : [ ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 1, \"executionTimeMillis\" : 1, \"totalKeysExamined\" : 1, \"totalDocsExamined\" : 1, \"executionStages\" : { \"stage\" : \"FETCH\", \"nReturned\" : 1, \"executionTimeMillisEstimate\" : 0, \"works\" : 2, \"advanced\" : 1, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 0, \"restoreState\" : 0, \"isEOF\" : 1, \"invalidates\" : 0, \"docsExamined\" : 1, \"alreadyHasObj\" : 0, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"nReturned\" : 1, \"executionTimeMillisEstimate\" : 0, \"works\" : 2, \"advanced\" : 1, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 0, \"restoreState\" : 0, \"isEOF\" : 1, \"invalidates\" : 0, \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [\n\nIntroduction to Indexes\n\n|\n\n79",
      "content_length": 954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "\"[\\\"user101\\\", \\\"user101\\\"]\" ] }, \"keysExamined\" : 1, \"seeks\" : 1, \"dupsTested\" : 0, \"dupsDropped\" : 0, \"seenInvalidated\" : 0 } } }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nThis explain output is more complex, but for now you can continue to ignore all the fields other than \"nReturned\", \"totalDocsExamined\", and \"executionTimeMillis\" in the \"executionStats\" nested document. As you can see, the query is now almost instantaneous and, even better, has a similar runtime when querying, for example, for any username:\n\n> db.users.find({\"username\": \"user999999\"}).explain(\"executionStats\")\n\nAn index can make a dramatic difference in query times. However, indexes have their price: write operations (inserts, updates, and deletes) that modify an indexed field will take longer. This is because in addition to updating the document, MongoDB has to update indexes when your data changes. Typically, the tradeoff is worth it. The tricky part becomes figuring out which fields to index.\n\nMongoDB’s indexes work almost identically to typical relational database indexes, so if you are familiar with those, you can just skim this section for syntax specifics.\n\nTo choose which fields to create indexes for, look through your frequent queries and queries that need to be fast and try to find a common set of keys from those. For instance, in the preceding example, we were querying on \"username\". If that were a particularly common query or were becoming a bottleneck, indexing \"username\" would be a good choice. However, if this were an unusual query or one that’s only done by administrators who don’t care how long it takes, it would not be a good choice for indexing.\n\n80\n\n|\n\nChapter 5: Indexes",
      "content_length": 1810,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Introduction to Compound Indexes The purpose of an index is to make your queries as efficient as possible. For many query patterns it is necessary to build indexes based on two or more keys. For exam‐ ple, an index keeps all of its values in a sorted order, so it makes sorting documents by the indexed key much faster. However, an index can only help with sorting if it is a prefix of the sort. For example, the index on \"username\" wouldn’t help much for this sort:\n\n> db.users.find().sort({\"age\" : 1, \"username\" : 1})\n\nThis sorts by \"age\" and then \"username\", so a strict sorting by \"username\" isn’t terri‐ bly helpful. To optimize this sort, you could make an index on \"age\" and \"username\":\n\n> db.users.createIndex({\"age\" : 1, \"username\" : 1})\n\nThis is called a compound index and is useful if your query has multiple sort direc‐ tions or multiple keys in the criteria. A compound index is an index on more than one field.\n\nSuppose we have a users collection that looks something like this, if we run a query with no sorting (called natural order):\n\n> db.users.find({}, {\"_id\" : 0, \"i\" : 0, \"created\" : 0}) { \"username\" : \"user0\", \"age\" : 69 } { \"username\" : \"user1\", \"age\" : 50 } { \"username\" : \"user2\", \"age\" : 88 } { \"username\" : \"user3\", \"age\" : 52 } { \"username\" : \"user4\", \"age\" : 74 } { \"username\" : \"user5\", \"age\" : 104 } { \"username\" : \"user6\", \"age\" : 59 } { \"username\" : \"user7\", \"age\" : 102 } { \"username\" : \"user8\", \"age\" : 94 } { \"username\" : \"user9\", \"age\" : 7 } { \"username\" : \"user10\", \"age\" : 80 } ...\n\nIf we index this collection by {\"age\" : 1, \"username\" : 1}, the index will have a form we can represent as follows:\n\n[0, \"user100020\"] -> 8623513776 [0, \"user1002\"] -> 8599246768 [0, \"user100388\"] -> 8623560880 ... [0, \"user100414\"] -> 8623564208 [1, \"user100113\"] -> 8623525680 [1, \"user100280\"] -> 8623547056 [1, \"user100551\"] -> 8623581744 ... [1, \"user100626\"] -> 8623591344 [2, \"user100191\"] -> 8623535664\n\nIntroduction to Indexes\n\n|\n\n81",
      "content_length": 1966,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "[2, \"user100195\"] -> 8623536176 [2, \"user100197\"] -> 8623536432 ...\n\nEach index entry contains an age and a username and points to a record identifier. A record identifier is used internally by the storage engine to locate the data for a docu‐ ment. Note that \"age\" fields are ordered to be strictly ascending and, within each age, usernames are also in ascending order. In this example dataset, each age has approxi‐ mately 8,000 usernames associated with it. Here we’ve included only those necessary to convey the general idea.\n\nThe way MongoDB uses this index depends on the type of query you’re doing. These are the three most common ways:\n\ndb.users.find({\"age\" : 21}).sort({\"username\" : -1})\n\nThis is an equality query, which searches for a single value. There may be multi‐ ple documents with that value. Due to the second field in the index, the results are already in the correct order for the sort: MongoDB can start with the last match for {\"age\" : 21} and traverse the index in order:\n\n[21, \"user100154\"] -> 8623530928 [21, \"user100266\"] -> 8623545264 [21, \"user100270\"] -> 8623545776 [21, \"user100285\"] -> 8623547696 [21, \"user100349\"] -> 8623555888 ...\n\nThis type of query is very efficient: MongoDB can jump directly to the correct age and doesn’t need to sort the results because traversing the index returns the data in the correct order.\n\nNote that sort direction doesn’t matter: MongoDB can traverse the index in either direction.\n\ndb.users.find({\"age\" : {\"$gte\" : 21, \"$lte\" : 30}})\n\nThis is a range query, which looks for documents matching multiple values (in this case, all ages between 21 and 30). MongoDB will use the first key in the index, \"age\", to return the matching documents, like so:\n\n[21, \"user100154\"] -> 8623530928 [21, \"user100266\"] -> 8623545264 [21, \"user100270\"] -> 8623545776 ... [21, \"user999390\"] -> 8765250224 [21, \"user999407\"] -> 8765252400 [21, \"user999600\"] -> 8765277104 [22, \"user100017\"] -> 8623513392 ... [29, \"user999861\"] -> 8765310512\n\n82\n\n|\n\nChapter 5: Indexes",
      "content_length": 2015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "[30, \"user100098\"] -> 8623523760 [30, \"user100155\"] -> 8623531056 [30, \"user100168\"] -> 8623532720 ...\n\nIn general, if MongoDB uses an index for a query it will return the resulting documents in index order.\n\ndb.users.find({\"age\" : {\"$gte\" : 21, \"$lte\" : 30}}).sort({\"username\" : 1})\n\nThis is a multivalue query, like the previous one, but this time it has a sort. As before, MongoDB will use the index to match the criteria. However, the index doesn’t return the usernames in sorted order and the query requested that the results be sorted by username. This means MongoDB will need to sort the results in memory before returning them, rather than simply traversing an index in which the documents are already sorted in the desired order. This type of query is usually less efficient as a consequence.\n\nOf course, the speed depends on how many results match your criteria: if your result set is only a couple of documents MongoDB won’t have much work to do to sort them, but if there are more results it will be slower or may not work at all. If you have more than 32 MB of results MongoDB will just error out, refusing to sort that much data:\n\nError: error: { \"ok\" : 0, \"errmsg\" : \"Executor error during find command: OperationFailed: Sort operation used more than the maximum 33554432 bytes of RAM. Add an index, or specify a smaller limit.\", \"code\" : 96, \"codeName\" : \"OperationFailed\" }\n\nIf you need to avoid this error, then you must create an index sup‐ porting the sort operation (https://docs.mongodb.com/manual/refer ence/method/cursor.sort/index.html#sort-index-use) or use sort in conjunction with limit to reduce the results to below 32 MB.\n\nOne other index you can use in the last example is the same keys in reverse order: {\"username\" : 1, \"age\" : 1}. MongoDB will then traverse all the index entries, but in the order you want them back in. It will pick out the matching documents using the \"age\" part of the index:\n\nIntroduction to Indexes\n\n|\n\n83",
      "content_length": 1962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "[user0, 4] [user1, 67] [user10, 11] [user100, 92] [user1000, 10] [user10000, 31] [user100000, 21] -> 8623511216 [user100001, 52] [user100002, 69] [user100003, 27] -> 8623511600 [user100004, 22] -> 8623511728 [user100005, 95] ...\n\nThis is good in that it does not require any giant in-memory sorts. However, it does have to scan the entire index to find all the matches. Putting the sort key first is gen‐ erally a good strategy when designing compound indexes. As we’ll see shortly, this is one of several best practices when considering how to construct compound indexes with consideration for equality queries, multivalue queries, and sorting.\n\nHow MongoDB Selects an Index Now let’s take a look at how MongoDB chooses an index to satisfy a query. Let’s imag‐ ine we have five indexes. When a query comes in, MongoDB looks at the query’s shape. The shape has to do with what fields are being searched on and additional information, such as whether or not there is a sort. Based on that information, the system identifies a set of candidate indexes that it might be able to use in satisfying the query.\n\nLet’s assume we have a query come in, and three of our five indexes are identified as candidates for this query. MongoDB will then create three query plans, one for each of these indexes, and run the query in three parallel threads, each using a different index. The objective here is to see which one is able to return results the fastest.\n\nVisually, we can think of this as a race, as pictured in Figure 5-1. The idea here is that the first query plan to reach a goal state is the winner. But more importantly, going forward it will be selected as the index to use for queries that have that same query shape. The plans are raced against each other for a period (referred to as the trial period), after which the results of each race are used to calculate the overall winning plan.\n\n84\n\n|\n\nChapter 5: Indexes",
      "content_length": 1915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Figure 5-1. How the MongoDB Query Planner selects an index, visualized as a race\n\nTo win the race, a query thread must be the first to either return all the query results or return a trial number of results in sort order. The sort order portion of this is important given how expensive it is to perform in-memory sorts.\n\nThe real value of racing several query plans against one another is that for subsequent queries that have the same query shape, the MongoDB server will know which index to select. The server maintains a cache of query plans. A winning plan is stored in the cache for future use for queries of that shape. Over time, as a collection changes and as the indexes change, eventually a query plan might be evicted from the cache and MongoDB will, again, experiment with possible query plans to find the one that works best for the current collection and set of indexes. Other events that will lead to plans being evicted from the cache are if we rebuild a given index, add or drop an index, or explicitly clear the plan cache. Finally, the query plan cache does not survive a restart of a mongod process.\n\nUsing Compound Indexes In the previous sections, we’ve been using compound indexes, which are indexes with more than one key in them. Compound indexes are a little more complicated to think about than single-key indexes, but they are very powerful. This section covers them in more depth.\n\nIntroduction to Indexes\n\n|\n\n85",
      "content_length": 1441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Here, we will walk through an example that gives you an idea of the type of thinking you need to do when you are designing compound indexes. The goal is for our read and write operations to be as efficient as possible—but as with so many things, this requires some upfront thinking and some experimentation.\n\nTo be sure we get the right indexes in place, it is necessary to test our indexes under some real-world workloads and make adjustments from there. However, there are some best practices we can apply as we design our indexes.\n\nFirst, we need to consider the selectivity of the index. We are interested in the degree to which, for a given query pattern, the index is going to minimize the number of records scanned. We need to consider selectivity in light of all operations necessary to satisfy a query, and sometimes make tradeoffs. We will need to consider, for example, how sorts are handled.\n\nLet’s look at an example. For this, we will use a student dataset containing approxi‐ mately one million records. Documents in this dataset resemble the following:\n\n{ \"_id\" : ObjectId(\"585d817db4743f74e2da067c\"), \"student_id\" : 0, \"scores\" : [ { \"type\" : \"exam\", \"score\" : 38.05000060199827 }, { \"type\" : \"quiz\", \"score\" : 79.45079445008987 }, { \"type\" : \"homework\", \"score\" : 74.50150548699534 }, { \"type\" : \"homework\", \"score\" : 74.68381684615845 } ], \"class_id\" : 127 }\n\nWe will begin with two indexes and look at how MongoDB uses these indexes (or doesn’t) in order to satisfy queries. These two indexes are created as follows:\n\n> db.students.createIndex({\"class_id\": 1}) > db.students.createIndex({student_id: 1, class_id: 1})\n\nIn working with this dataset, we will consider the following query, because it illus‐ trates several of the issues that we have to think about in designing our indexes:\n\n86\n\n|\n\nChapter 5: Indexes",
      "content_length": 1833,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "> db.students.find({student_id:{$gt:500000}, class_id:54}) ... .sort({student_id:1}) ... .explain(\"executionStats\")\n\nNote that in this query we are requesting all records with an ID greater than 500,000, so about half of the records. We are also constraining the search to records for the class with ID 54. There are about 500 classes represented in this dataset. Finally, we are sorting in ascending order based on \"student_id\". Note that this is the same field on which we are doing a multivalue query. Throughout this example we will look at the execution stats that the explain method provides to illustrate how MongoDB will handle this query.\n\nIf we run the query, the output of the explain method tells us how MongoDB used indexes to satisfy it:\n\n{ \"queryPlanner\": { \"plannerVersion\": 1, \"namespace\": \"school.students\", \"indexFilterSet\": false, \"parsedQuery\": { \"$and\": [ { \"class_id\": { \"$eq\": 54 } }, { \"student_id\": { \"$gt\": 500000 } } ] }, \"winningPlan\": { \"stage\": \"FETCH\", \"inputStage\": { \"stage\": \"IXSCAN\", \"keyPattern\": { \"student_id\": 1, \"class_id\": 1 }, \"indexName\": \"student_id_1_class_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"student_id\": [ ], \"class_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false,\n\nIntroduction to Indexes\n\n|\n\n87",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "88\n\n\"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"student_id\": [ \"(500000.0, inf.0]\" ], \"class_id\": [ \"[54.0, 54.0]\" ] } } }, \"rejectedPlans\": [ { \"stage\": \"SORT\", \"sortPattern\": { \"student_id\": 1 }, \"inputStage\": { \"stage\": \"SORT_KEY_GENERATOR\", \"inputStage\": { \"stage\": \"FETCH\", \"filter\": { \"student_id\": { \"$gt\": 500000 } }, \"inputStage\": { \"stage\": \"IXSCAN\", \"keyPattern\": { \"class_id\": 1 }, \"indexName\": \"class_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\" ] } } } } } ]\n\n|\n\nChapter 5: Indexes",
      "content_length": 680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "}, \"executionStats\": { \"executionSuccess\": true, \"nReturned\": 9903, \"executionTimeMillis\": 4325, \"totalKeysExamined\": 850477, \"totalDocsExamined\": 9903, \"executionStages\": { \"stage\": \"FETCH\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 3485, \"works\": 850478, \"advanced\": 9903, \"needTime\": 840574, \"needYield\": 0, \"saveState\": 6861, \"restoreState\": 6861, \"isEOF\": 1, \"invalidates\": 0, \"docsExamined\": 9903, \"alreadyHasObj\": 0, \"inputStage\": { \"stage\": \"IXSCAN\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 2834, \"works\": 850478, \"advanced\": 9903, \"needTime\": 840574, \"needYield\": 0, \"saveState\": 6861, \"restoreState\": 6861, \"isEOF\": 1, \"invalidates\": 0, \"keyPattern\": { \"student_id\": 1, \"class_id\": 1 }, \"indexName\": \"student_id_1_class_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"student_id\": [ ], \"class_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"student_id\": [ \"(500000.0, inf.0]\" ],\n\nIntroduction to Indexes\n\n|\n\n89",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "\"class_id\": [ \"[54.0, 54.0]\" ] }, \"keysExamined\": 850477, \"seeks\": 840575, \"dupsTested\": 0, \"dupsDropped\": 0, \"seenInvalidated\": 0 } } }, \"serverInfo\": { \"host\": \"SGB-MBP.local\", \"port\": 27017, \"version\": \"3.4.1\", \"gitVersion\": \"5e103c4f5583e2566a45d740225dc250baacfbd7\" }, \"ok\": 1 }\n\nAs with most data output from MongoDB, the explain output is JSON. Let’s look first at the bottom half of this output, which is almost entirely the execution stats. The \"executionStats\" field contains statistics that describe the completed query execu‐ tion for the winning query plan. We will look at query plans and the query plan out‐ put from explain a little later.\n\nWithin \"executionStats\", first we will look at \"totalKeysExamined\". This is how many keys within the index MongoDB walked through in order to generate the result set. We can compare \"totalKeysExamined\" to \"nReturned\" to get a sense for how much of the index MongoDB had to traverse in order to find just the documents matching the query. In this case, 850,477 index keys were examined in order to locate the 9,903 matching documents.\n\nThis means that the index used in order to satisfy this query was not very selective. This is further emphasized by the fact that this query took more than 4.3 seconds to run, as indicated by the \"executionTimeMillis\" field. Selectivity is one of our key objectives when we are designing an index, so let’s figure out where we went wrong with the existing indexes for this query.\n\nNear the top of the explain output is the winning query plan (see the field \"winning Plan\"). A query plan describes the steps MongoDB used to satisfy a query. This is, in JSON form, the specific outcome of racing a couple of different query plans against one another. In particular, we are interested in what indexes were used and whether MongoDB had to do an in-memory sort. Below the winning plan are the rejected plans. We’ll look at both.\n\n90\n\n|\n\nChapter 5: Indexes",
      "content_length": 1942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "In this case, the winning plan used a compound index based on \"student_id\" and \"class_id\". This is evident in the following portion of the explain output:\n\n\"winningPlan\": { \"stage\": \"FETCH\", \"inputStage\": { \"stage\": \"IXSCAN\", \"keyPattern\": { \"student_id\": 1, \"class_id\": 1 },\n\nThe explain output presents the query plan as a tree of stages. A stage can have one or more input stages, depending on how many child stages it has. An input stage pro‐ vides the documents or index keys to its parent. In this case, there was one input stage, an index scan, and that scan provided the record IDs for documents matching the query to its parent, the \"FETCH\" stage. The \"FETCH\" stage, then, will retrieve the documents themselves and return them in batches as the client requests them.\n\nThe losing query plan—there is only one—would have used an index based on \"class_id\" but then it would have had to do an in-memory sort. That is what the following portion of this particular query plan means. When you see a \"SORT\" stage in a query plan, it means that MongoDB would have been unable to sort the result set in the database using an index and instead would have had to do an in-memory sort:\n\n\"rejectedPlans\": [ { \"stage\": \"SORT\", \"sortPattern\": { \"student_id\": 1 },\n\nFor this query, the index that won is one that was able to return sorted output. To win it only had to reach a trial number of sorted result documents. For the other plan to win, that query thread would have had to return the entire result set (nearly 10,000 documents) first, since those would then need to be sorted in memory.\n\nThe issue here is one of selectivity. The multivalue query we are running specifies a broad range of \"student_id\" values, because it’s requesting records for which the \"student_id\" is greater than 500,000. That’s about half the records in our collection. Here again, for convenience, is the query we are running:\n\n> db.students.find({student_id:{$gt:500000}, class_id:54}) ... .sort({student_id:1}) ... .explain(\"executionStats\")\n\nNow, I’m sure you can see where we are headed here. This query contains both a mul‐ tivalue portion and an equality portion. The equality portion is that we are asking for all records in which \"class_id\" is equal to 54. There are only about 500 classes in\n\nIntroduction to Indexes\n\n|\n\n91",
      "content_length": 2307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "this dataset, and while there are a large number of students with grades in those classes, \"class_id\" would serve as a much more selective basis on which to execute this query. It is this value that constrains our result set to just under 10,000 records rather than the approximately 850,000 that were identified by the multivalue portion of this query.\n\nIn other words, it would be better, given the indexes we have, if we were to use the index based on just \"class_id\"—the one in the losing query plan. MongoDB pro‐ vides two ways of forcing the database to use a particular index. However, I cannot stress strongly enough that you should use these ways of overriding what would be the outcome of the query planner with caution. These are not techniques you should use in a production deployment.\n\nThe cursor hint method enables us to specify a particular index to use, either by specifying its shape or its name. An index filter uses a query shape, which is a combi‐ nation of a query, sort, and projection specification. The planCacheSetFilter func‐ tion can be used with an index filter to limit the query optimizer to only considering indexes specified in the index filter. If an index filter exists for a query shape, Mon‐ goDB will ignore hint. Index filters only persist for the duration of the mongod server process; they do not persist after shutdown.\n\nIf we change our query slightly to use hint, as in the following example, the explain output will be quite different:\n\n> db.students.find({student_id:{$gt:500000}, class_id:54}) ... .sort({student_id:1}) ... .hint({class_id:1}) ... .explain(\"executionStats\")\n\nThe resulting output shows that we are now down from having scanned roughly 850,000 index keys to just about 20,000 in order to get to our result set of just under 10,000. In addition, the execution time is only 272 milliseconds rather than the 4.3 seconds we saw with the query plan using the other index:\n\n{ \"queryPlanner\": { \"plannerVersion\": 1, \"namespace\": \"school.students\", \"indexFilterSet\": false, \"parsedQuery\": { \"$and\": [ { \"class_id\": { \"$eq\": 54 } }, { \"student_id\": { \"$gt\": 500000\n\n92\n\n|\n\nChapter 5: Indexes",
      "content_length": 2146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "} } ] }, \"winningPlan\": { \"stage\": \"SORT\", \"sortPattern\": { \"student_id\": 1 }, \"inputStage\": { \"stage\": \"SORT_KEY_GENERATOR\", \"inputStage\": { \"stage\": \"FETCH\", \"filter\": { \"student_id\": { \"$gt\": 500000 } }, \"inputStage\": { \"stage\": \"IXSCAN\", \"keyPattern\": { \"class_id\": 1 }, \"indexName\": \"class_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\" ] } } } } }, \"rejectedPlans\": [ ] }, \"executionStats\": { \"executionSuccess\": true, \"nReturned\": 9903, \"executionTimeMillis\": 272, \"totalKeysExamined\": 20076, \"totalDocsExamined\": 20076, \"executionStages\": { \"stage\": \"SORT\",\n\nIntroduction to Indexes\n\n|\n\n93",
      "content_length": 763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "94\n\n\"nReturned\": 9903, \"executionTimeMillisEstimate\": 248, \"works\": 29982, \"advanced\": 9903, \"needTime\": 20078, \"needYield\": 0, \"saveState\": 242, \"restoreState\": 242, \"isEOF\": 1, \"invalidates\": 0, \"sortPattern\": { \"student_id\": 1 }, \"memUsage\": 2386623, \"memLimit\": 33554432, \"inputStage\": { \"stage\": \"SORT_KEY_GENERATOR\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 203, \"works\": 20078, \"advanced\": 9903, \"needTime\": 10174, \"needYield\": 0, \"saveState\": 242, \"restoreState\": 242, \"isEOF\": 1, \"invalidates\": 0, \"inputStage\": { \"stage\": \"FETCH\", \"filter\": { \"student_id\": { \"$gt\": 500000 } }, \"nReturned\": 9903, \"executionTimeMillisEstimate\": 192, \"works\": 20077, \"advanced\": 9903, \"needTime\": 10173, \"needYield\": 0, \"saveState\": 242, \"restoreState\": 242, \"isEOF\": 1, \"invalidates\": 0, \"docsExamined\": 20076, \"alreadyHasObj\": 0, \"inputStage\": { \"stage\": \"IXSCAN\", \"nReturned\": 20076, \"executionTimeMillisEstimate\": 45, \"works\": 20077, \"advanced\": 20076,\n\n|\n\nChapter 5: Indexes",
      "content_length": 980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "\"needTime\": 0, \"needYield\": 0, \"saveState\": 242, \"restoreState\": 242, \"isEOF\": 1, \"invalidates\": 0, \"keyPattern\": { \"class_id\": 1 }, \"indexName\": \"class_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\" ] }, \"keysExamined\": 20076, \"seeks\": 1, \"dupsTested\": 0, \"dupsDropped\": 0, \"seenInvalidated\": 0 } } } } }, \"serverInfo\": { \"host\": \"SGB-MBP.local\", \"port\": 27017, \"version\": \"3.4.1\", \"gitVersion\": \"5e103c4f5583e2566a45d740225dc250baacfbd7\" }, \"ok\": 1 }\n\nHowever, what we really want to see is \"nReturned\" very close to \"totalKeysExa mined\". In addition, we would like avoid having to use hint in order to more effi‐ ciently execute this query. The way to address both of these concerns is to design a better index.\n\nA better index for the query pattern in question is one based on \"class_id\" and \"stu dent_id\", in that order. With \"class_id\" as the prefix, we are using the equality fil‐ ter in our query to restrict the keys considered within the index. This is the most\n\nIntroduction to Indexes\n\n|\n\n95",
      "content_length": 1169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "selective component of our query, and therefore effectively constrains the number of keys MongoDB needs to consider to satisfy this query. We can build this index as follows:\n\n> db.students.createIndex({class_id:1, student_id:1})\n\nWhile not true for absolutely every dataset, in general you should design compound indexes such that fields on which you will be using equality filters come before those on which your application will use multivalue filters.\n\nWith our new index in place, if we rerun our query, this time no hinting is required and we can see from the \"executionStats\" field in the explain output that we have a fast query (37 milliseconds) for which the number of results returned (\"nRe turned\") is equal to the number of keys scanned in the index (\"totalKeysExa mined\"). We can also see that this is due to the fact that the \"executionStages\", which reflect the winning query plan, contain an index scan that makes use of the new index we created:\n\n... \"executionStats\": { \"executionSuccess\": true, \"nReturned\": 9903, \"executionTimeMillis\": 37, \"totalKeysExamined\": 9903, \"totalDocsExamined\": 9903, \"executionStages\": { \"stage\": \"FETCH\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 36, \"works\": 9904, \"advanced\": 9903, \"needTime\": 0, \"needYield\": 0, \"saveState\": 81, \"restoreState\": 81, \"isEOF\": 1, \"invalidates\": 0, \"docsExamined\": 9903, \"alreadyHasObj\": 0, \"inputStage\": { \"stage\": \"IXSCAN\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 0, \"works\": 9904, \"advanced\": 9903, \"needTime\": 0, \"needYield\": 0, \"saveState\": 81, \"restoreState\": 81, \"isEOF\": 1, \"invalidates\": 0,\n\n96\n\n|\n\nChapter 5: Indexes",
      "content_length": 1626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "\"keyPattern\": { \"class_id\": 1, \"student_id\": 1 }, \"indexName\": \"class_id_1_student_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ], \"student_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\" ], \"student_id\": [ \"(500000.0, inf.0]\" ] }, \"keysExamined\": 9903, \"seeks\": 1, \"dupsTested\": 0, \"dupsDropped\": 0, \"seenInvalidated\": 0 } } },\n\nConsidering what we know about how indexes are built, you can probably see why this works. The [class_id, student_id] index is composed of key pairs such as the following. Since the student IDs are ordered within these key pairs, in order to satisfy our sort MongoDB simply needs to walk all the key pairs beginning with the first one for class_id 54:\n\n... [53, 999617] [53, 999780] [53, 999916] [54, 500001] [54, 500009] [54, 500048] ...\n\nIn considering the design of a compound index, we need to know how to address equality filters, multivalue filters, and sort components of common query patterns that will make use of the index. It is necessary to consider these three factors for all compound indexes, and if you design your index to balance these concerns correctly,\n\nIntroduction to Indexes\n\n|\n\n97",
      "content_length": 1262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "you will get the best performance out of MongoDB for your queries. While we’ve addressed all three factors for our example query with the [class_id, student_id] index, the query as written represents a special case of the compound index problem because we’re sorting on one of the fields we are also filtering on.\n\nTo remove the special-case nature of this example, let’s sort on final grade instead, changing our query to the following:\n\n> db.students.find({student_id:{$gt:500000}, class_id:54}) ... .sort({final_grade:1}) ... .explain(\"executionStats\")\n\nIf we run this query and look at the explain output, we see that we’re now doing an in-memory sort. While the query is still fast at only 136 milliseconds, it is an order of magnitude slower than when sorting on \"student_id\", because we are now doing an in-memory sort. We can see that we are doing an in-memory sort because the win‐ ning query plan now contains a \"SORT\" stage:\n\n... \"executionStats\": { \"executionSuccess\": true, \"nReturned\": 9903, \"executionTimeMillis\": 136, \"totalKeysExamined\": 9903, \"totalDocsExamined\": 9903, \"executionStages\": { \"stage\": \"SORT\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 36, \"works\": 19809, \"advanced\": 9903, \"needTime\": 9905, \"needYield\": 0, \"saveState\": 315, \"restoreState\": 315, \"isEOF\": 1, \"invalidates\": 0, \"sortPattern\": { \"final_grade\": 1 }, \"memUsage\": 2386623, \"memLimit\": 33554432, \"inputStage\": { \"stage\": \"SORT_KEY_GENERATOR\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 24, \"works\": 9905, \"advanced\": 9903, \"needTime\": 1, \"needYield\": 0, \"saveState\": 315,\n\n98\n\n|\n\nChapter 5: Indexes",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "\"restoreState\": 315, \"isEOF\": 1, \"invalidates\": 0, \"inputStage\": { \"stage\": \"FETCH\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 24, \"works\": 9904, \"advanced\": 9903, \"needTime\": 0, \"needYield\": 0, \"saveState\": 315, \"restoreState\": 315, \"isEOF\": 1, \"invalidates\": 0, \"docsExamined\": 9903, \"alreadyHasObj\": 0, \"inputStage\": { \"stage\": \"IXSCAN\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 12, \"works\": 9904, \"advanced\": 9903, \"needTime\": 0, \"needYield\": 0, \"saveState\": 315, \"restoreState\": 315, \"isEOF\": 1, \"invalidates\": 0, \"keyPattern\": { \"class_id\": 1, \"student_id\": 1 }, \"indexName\": \"class_id_1_student_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ], \"student_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\" ], \"student_id\": [ \"(500000.0, inf.0]\" ] },\n\nIntroduction to Indexes\n\n|\n\n99",
      "content_length": 926,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "\"keysExamined\": 9903, \"seeks\": 1, \"dupsTested\": 0, \"dupsDropped\": 0, \"seenInvalidated\": 0 } } } } }, ...\n\nIf we can avoid an in-memory sort with a better index design, we should. This will allow us to scale more easily with respect to dataset size and system load.\n\nBut to do that, we are going to have to make a tradeoff. This is commonly the case when designing compound indexes.\n\nAs is so often necessary for compound indexes, in order to avoid an in-memory sort we need to examine more keys than the number of documents we return. To use the index to sort, MongoDB needs to be able to walk the index keys in order. This means that we need to include the sort field among the compound index keys.\n\nThe keys in our new compound index should be ordered as follows: [class_id, final_grade, student_id]. Note that we include the sort component immediately after the equality filter, but before the multivalue filter. This index will very selectively narrow the set of keys considered for this query. Then, by walking the key triplets matching the equality filter in this index, MongoDB can identify the records that match the multivalue filter and those records will be ordered properly by final grade in ascending order.\n\nThis compound index forces MongoDB to examine keys for more documents than will end up being in our result set. However, by using the index to ensure we have sorted documents, we save execution time. We can construct the new index using the following command:\n\n> db.students.createIndex({class_id:1, final_grade:1, student_id:1})\n\nNow, if we once again issue our query:\n\n> db.students.find({student_id:{$gt:500000}, class_id:54}) ... .sort({final_grade:1}) ... .explain(\"executionStats\")\n\nwe get the following \"executionStats\" in the output from explain. This will vary depending on your hardware and what else is going on in the system, but you can see that the winning plan no longer includes an in-memory sort. It is instead using the index we just created to satisfy the query, including the sort:\n\n100\n\n|\n\nChapter 5: Indexes",
      "content_length": 2051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "\"executionStats\": { \"executionSuccess\": true, \"nReturned\": 9903, \"executionTimeMillis\": 42, \"totalKeysExamined\": 9905, \"totalDocsExamined\": 9903, \"executionStages\": { \"stage\": \"FETCH\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 34, \"works\": 9905, \"advanced\": 9903, \"needTime\": 1, \"needYield\": 0, \"saveState\": 82, \"restoreState\": 82, \"isEOF\": 1, \"invalidates\": 0, \"docsExamined\": 9903, \"alreadyHasObj\": 0, \"inputStage\": { \"stage\": \"IXSCAN\", \"nReturned\": 9903, \"executionTimeMillisEstimate\": 24, \"works\": 9905, \"advanced\": 9903, \"needTime\": 1, \"needYield\": 0, \"saveState\": 82, \"restoreState\": 82, \"isEOF\": 1, \"invalidates\": 0, \"keyPattern\": { \"class_id\": 1, \"final_grade\": 1, \"student_id\": 1 }, \"indexName\": \"class_id_1_final_grade_1_student_id_1\", \"isMultiKey\": false, \"multiKeyPaths\": { \"class_id\": [ ], \"final_grade\": [ ], \"student_id\": [ ] }, \"isUnique\": false, \"isSparse\": false, \"isPartial\": false, \"indexVersion\": 2, \"direction\": \"forward\", \"indexBounds\": { \"class_id\": [ \"[54.0, 54.0]\"\n\nIntroduction to Indexes\n\n|\n\n101",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "], \"final_grade\": [ \"[MinKey, MaxKey]\" ], \"student_id\": [ \"(500000.0, inf.0]\" ] }, \"keysExamined\": 9905, \"seeks\": 2, \"dupsTested\": 0, \"dupsDropped\": 0, \"seenInvalidated\": 0 } } },\n\nThis section has provided a concrete example of some best practices for designing compound indexes. While these guidelines do not hold for every situation, they do for most and should be the first ideas you consider when constructing a compound index.\n\nTo recap, when designing a compound index:\n\nKeys for equality filters should appear first.\n\nKeys used for sorting should appear before multivalue fields.\n\nKeys for multivalue filters should appear last.\n\nDesign your compound index using these guidelines and then test it under real-world workloads for the range of query patterns your index is designed to support.\n\nChoosing key directions\n\nSo far, all of our index entries have been sorted in ascending, or least-to-greatest, order. However, if you need to sort on two (or more) criteria, you may need to have index keys go in different directions. For example, going back to our earlier example with the users collection, suppose we wanted to sort the collection by age from youngest to oldest and by name from Z−A. Our previous indexes would not be very efficient for this problem: within each age group users were sorted by username in ascending order (A−Z, not Z−A). The compound indexes we’ve been using so far do not hold the values in any useful order for getting \"age\" ascending and \"username\" descending.\n\nTo optimize compound sorts in different directions, we need to use an index with matching directions. In this example, we could use {\"age\" : 1, \"username\" : -1}, which would organize the data as follows:\n\n102\n\n|\n\nChapter 5: Indexes",
      "content_length": 1731,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "[21, user999600] -> 8765277104 [21, user999407] -> 8765252400 [21, user999390] -> 8765250224 ... [21, user100270] -> 8623545776 [21, user100266] -> 8623545264 [21, user100154] -> 8623530928 ... [30, user100168] -> 8623532720 [30, user100155] -> 8623531056 [30, user100098] -> 8623523760\n\nThe ages are arranged from youngest to oldest, and within each age, the usernames are sorted from Z to A (or rather 9 to 0, given our usernames).\n\nIf our application also needed to optimize sorting by {\"age\" : 1, \"username\" : 1}, we would have to create a second index with those directions. To figure out which directions to use for an index, simply match the directions your sort is using. Note that inverse indexes (multiplying each direction by −1) are equivalent: {\"age\" : 1, \"username\" : -1} suits the same queries that {\"age\" : -1, \"username\" : 1} does.\n\nIndex direction only really matters when you’re sorting based on multiple criteria. If you’re only sorting by a single key, MongoDB can just as easily read the index in the opposite order. For example, if you had a sort on {\"age\" : -1} and an index on {\"age\" : 1}, MongoDB could optimize it just as well as if you had an index on {\"age\" : -1} (so don’t create both!). The direction only matters for multikey sorts.\n\nUsing covered queries\n\nIn the preceding examples, the index was always used to find the correct document and then follow a pointer back to fetch the actual document. However, if your query is only looking for the fields that are included in the index, it does not need to fetch the document. When an index contains all the values requested by a query, the query is considered to be covered. Whenever practical, use covered queries in preference to going back to documents. You can make your working set much smaller that way.\n\nTo make sure a query can use the index only, you should use projections (which limit the fields returned to only those specified in your query; see “Specifying Which Keys to Return” on page 54) to avoid returning the \"_id\" field (unless it is part of the index). You may also have to index fields that you aren’t querying on, so you should balance your need for faster queries with the overhead this will add on writes.\n\nIf you run explain on a covered query, the result has an \"IXSCAN\" stage that is not a descendant of a \"FETCH\" stage, and in the \"executionStats\", the value of \"totalDoc sExamined\" is 0.\n\nIntroduction to Indexes\n\n|\n\n103",
      "content_length": 2432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Implicit indexes\n\nCompound indexes can do “double duty” and act like different indexes for different queries. If we have an index on {\"age\" : 1, \"username\" : 1}, the \"age\" field is sorted identically to the way it would be if we had an index on just {\"age\" : 1}. Thus, the compound index can be used the way an index on {\"age\" : 1} by itself would be.\n\nThis can be generalized to as many keys as necessary: if an index has N keys, you get a “free” index on any prefix of those keys. For example, if we have an index that looks like {\"a\": 1, \"b\": 1, \"c\": 1, ..., \"z\": 1}, we effectively have indexes on {\"a\": 1}, {\"a\": 1, \"b\" : 1}, {\"a\": 1, \"b\": 1, \"c\": 1}, and so on.\n\nNote that this doesn’t hold for any subset of keys: queries that would use the index {\"b\": 1} or {\"a\": 1, \"c\": 1} (for example) will not be optimized. Only queries that can use a prefix of the index can take advantage of it.\n\nHow $ Operators Use Indexes Some queries can use indexes more efficiently than others; some queries cannot use indexes at all. This section covers how various query operators are handled by MongoDB.\n\nInefficient operators\n\nIn general, negation is inefficient. \"$ne\" queries can use an index, but not very well. They must look at all the index entries other than the one specified by \"$ne\", so they basically have to scan the entire index. For example, for a collection with an index on the field named \"i\", here are the index ranges traversed for such a query:\n\ndb.example.find({\"i\" : {\"$ne\" : 3}}).explain() { \"queryPlanner\" : { ..., \"parsedQuery\" : { \"i\" : { \"$ne\" : \"3\" } }, \"winningPlan\" : { { ..., \"indexBounds\" : { \"i\" : [ [ { \"$minElement\" : 1 }, 3\n\n104\n\n|\n\nChapter 5: Indexes",
      "content_length": 1678,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "], [ 3, { \"$maxElement\" : 1 } ] ] } } }, \"rejectedPlans\" : [ ] }, \"serverInfo\" : { ..., } }\n\nThis query looks at all index entries less than 3 and all index entries greater than 3. This can be efficient if a large swath of your collection is 3, but otherwise it must check almost everything.\n\n\"$not\" can sometimes use an index but often does not know how. It can reverse basic ranges ({\"key\" : {\"$lt\" : 7}} becomes {\"key\" : {\"$gte\" : 7}}) and regular expressions. However, most other queries with \"$not\" will fall back to doing a table scan. \"$nin\" always uses a table scan.\n\nIf you need to perform one of these types of queries quickly, figure out if there’s another clause that you could add to the query that could use an index to filter the result set down to a small number of documents before MongoDB attempts to do nonindexed matching.\n\nRanges\n\nCompound indexes can help MongoDB efficiently execute queries with multiple clau‐ ses. When designing an index with multiple fields, put fields that will be used in exact matches first (e.g., \"x\" : 1) and ranges last (e.g., \"y\": {\"$gt\" : 3, \"$lt\" : 5}). This allows the query to find an exact value for the first index key and then search within that for a second index range. For example, suppose we were querying for a specific age and a range of usernames using an {\"age\" : 1, \"username\" : 1} index. We would get fairly exact index bounds:\n\n> db.users.find({\"age\" : 47, \"username\" : ... {\"$gt\" : \"user5\", \"$lt\" : \"user8\"}}).explain('executionStats') { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false,\n\nIntroduction to Indexes\n\n|\n\n105",
      "content_length": 1634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "106\n\n\"parsedQuery\" : { \"$and\" : [ { \"age\" : { \"$eq\" : 47 } }, { \"username\" : { \"$lt\" : \"user8\" } }, { \"username\" : { \"$gt\" : \"user5\" } } ] }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[47.0, 47.0]\" ], \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ] } } }, \"rejectedPlans\" : [ { \"stage\" : \"FETCH\", \"filter\" : {\n\n|\n\nChapter 5: Indexes",
      "content_length": 648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "\"age\" : { \"$eq\" : 47 } }, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ] } } } ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 2742, \"executionTimeMillis\" : 5, \"totalKeysExamined\" : 2742, \"totalDocsExamined\" : 2742, \"executionStages\" : { \"stage\" : \"FETCH\", \"nReturned\" : 2742, \"executionTimeMillisEstimate\" : 0, \"works\" : 2743, \"advanced\" : 2742, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 23, \"restoreState\" : 23, \"isEOF\" : 1, \"invalidates\" : 0, \"docsExamined\" : 2742, \"alreadyHasObj\" : 0, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"nReturned\" : 2742, \"executionTimeMillisEstimate\" : 0,\n\nIntroduction to Indexes\n\n|\n\n107",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "\"works\" : 2743, \"advanced\" : 2742, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 23, \"restoreState\" : 23, \"isEOF\" : 1, \"invalidates\" : 0, \"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[47.0, 47.0]\" ], \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ] }, \"keysExamined\" : 2742, \"seeks\" : 1, \"dupsTested\" : 0, \"dupsDropped\" : 0, \"seenInvalidated\" : 0 } } }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nThe query goes directly to \"age\" : 47 and then searches within that for usernames between \"user5\" and \"user8\".\n\n108\n\n|\n\nChapter 5: Indexes",
      "content_length": 893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Conversely, suppose we use an index on {\"username\" : 1, \"age\" : 1}. This changes the query plan, as the query must look at all users between \"user5\" and \"user8\" and pick out the ones with \"age\" : 47:\n\n> db.users.find({\"age\" : 47, \"username\" : {\"$gt\" : \"user5\", \"$lt\" : \"user8\"}}) .explain('executionStats') { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ { \"age\" : { \"$eq\" : 47 } }, { \"username\" : { \"$lt\" : \"user8\" } }, { \"username\" : { \"$gt\" : \"user5\" } } ] }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"filter\" : { \"age\" : { \"$eq\" : 47 } }, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\",\n\nIntroduction to Indexes\n\n|\n\n109",
      "content_length": 917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "110\n\n\"indexBounds\" : { \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ] } } }, \"rejectedPlans\" : [ { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1, \"age\" : 1 }, \"indexName\" : \"username_1_age_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ], \"age\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ], \"age\" : [ \"[47.0, 47.0]\" ] } } } ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 2742, \"executionTimeMillis\" : 369, \"totalKeysExamined\" : 333332, \"totalDocsExamined\" : 333332, \"executionStages\" : { \"stage\" : \"FETCH\", \"filter\" : { \"age\" : { \"$eq\" : 47 } },\n\n|\n\nChapter 5: Indexes",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "\"nReturned\" : 2742, \"executionTimeMillisEstimate\" : 312, \"works\" : 333333, \"advanced\" : 2742, \"needTime\" : 330590, \"needYield\" : 0, \"saveState\" : 2697, \"restoreState\" : 2697, \"isEOF\" : 1, \"invalidates\" : 0, \"docsExamined\" : 333332, \"alreadyHasObj\" : 0, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"nReturned\" : 333332, \"executionTimeMillisEstimate\" : 117, \"works\" : 333333, \"advanced\" : 333332, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 2697, \"restoreState\" : 2697, \"isEOF\" : 1, \"invalidates\" : 0, \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"(\\\"user5\\\", \\\"user8\\\")\" ] }, \"keysExamined\" : 333332, \"seeks\" : 1, \"dupsTested\" : 0, \"dupsDropped\" : 0, \"seenInvalidated\" : 0 } } }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\",\n\nIntroduction to Indexes\n\n|\n\n111",
      "content_length": 977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "\"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nThis forces MongoDB to scan 100 times the number of index entries as using the pre‐ vious index would. Using two ranges in a query basically always forces this less- efficient query plan.\n\nOR queries\n\nAs of this writing, MongoDB can only use one index per query. That is, if you create one index on {\"x\" : 1} and another index on {\"y\" : 1} and then do a query on {\"x\" : 123, \"y\" : 456}, MongoDB will use one of the indexes you created, not both. The only exception to this rule is \"$or\". \"$or\" can use one index per \"$or\" clause, as \"$or\" performs two queries and then merges the results:\n\ndb.foo.find({\"$or\" : [{\"x\" : 123}, {\"y\" : 456}]}).explain() { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"foo.foo\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$or\" : [ { \"x\" : { \"$eq\" : 123 } }, { \"y\" : { \"$eq\" : 456 } } ] }, \"winningPlan\" : { \"stage\" : \"SUBPLAN\", \"inputStage\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"OR\", \"inputStages\" : [ { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"x\" : 1 },\n\n112\n\n|\n\nChapter 5: Indexes",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "\"indexName\" : \"x_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"x\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"x\" : [ \"[123.0, 123.0]\" ] } }, { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"y\" : 1 }, \"indexName\" : \"y_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"y\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"y\" : [ \"[456.0, 456.0]\" ] } } ] } } }, \"rejectedPlans\" : [ ] }, \"serverInfo\" : { ..., }, \"ok\" : 1 }\n\nAs you can see, this explain required two separate queries on the two indexes (as indicated by the two \"IXSCAN\" stages). In general, doing two queries and merging the\n\nIntroduction to Indexes\n\n|\n\n113",
      "content_length": 783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "results is much less efficient than doing a single query; thus, whenever possible, pre‐ fer \"$in\" to \"$or\".\n\nIf you must use an \"$or\", keep in mind that MongoDB needs to look through the results of both queries and remove any duplicates (documents that matched more than one \"$or\" clause).\n\nWhen running \"$in\" queries there is no way, other than sorting, to control the order of documents returned. For example, {\"x\" : {\"$in\" : [1, 2, 3]}} will return documents in the same order as {\"x\" : {\"$in\" : [3, 2, 1]}}.\n\nIndexing Objects and Arrays MongoDB allows you to reach into your documents and create indexes on nested fields and arrays. Embedded object and array fields can be combined with top-level fields in compound indexes, and although they are special in some ways, they mostly behave the way “normal” index fields behave.\n\nIndexing embedded docs\n\nIndexes can be created on keys in embedded documents in the same way that they are created on normal keys. If we had a collection where each document represented a user, we might have an embedded document that described each user’s location:\n\n{ \"username\" : \"sid\", \"loc\" : { \"ip\" : \"1.2.3.4\", \"city\" : \"Springfield\", \"state\" : \"NY\" } }\n\nWe could put an index on one of the subfields of \"loc\", say \"loc.city\", to speed up queries using that field:\n\n> db.users.createIndex({\"loc.city\" : 1})\n\nYou can go as deep as you’d like with these: you could index \"x.y.z.w.a.b.c\" (and so on) if you wanted.\n\nNote that indexing the embedded document itself (\"loc\") has very different behavior than indexing a field of that embedded document (\"loc.city\"). Indexing the entire subdocument will only help queries that are querying for the entire subdocument. The query optimizer could only use an index on \"loc\" for queries that described the whole subdocument with fields in the correct order (e.g., db.users.find({\"loc\" : {\"ip\" : \"123.456.789.000\", \"city\" : \"Shelbyville\", \"state\" : \"NY\"}}})). It\n\n114\n\n|\n\nChapter 5: Indexes",
      "content_length": 1964,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "could not use the index for queries that looked like db.users.find({\"loc.city\" : \"Shelbyville\"}).\n\nIndexing arrays\n\nYou can also index arrays, which allows you to use the index to search for specific array elements efficiently.\n\nSuppose we have a collection of blog posts where each document is a post. Each post has a \"comments\" field, which is an array of \"comment\" subdocuments. If we wanted to be able to find the most recently commented-on blog posts, we could create an index on the \"date\" key in the array of embedded \"comments\" documents of our blog post collection:\n\n> db.blog.createIndex({\"comments.date\" : 1})\n\nIndexing an array creates an index entry for each element of the array, so if a post had 20 comments, it would have 20 index entries. This makes array indexes more expen‐ sive than single-value ones: for a single insert, update, or remove, every array entry might have to be updated (potentially thousands of index entries).\n\nUnlike the \"loc\" example in the previous section, you cannot index an entire array as a single entity: indexing an array field indexes each element of the array, not the array itself.\n\nIndexes on array elements do not keep any notion of position: you cannot use an index for a query that is looking for a specific array element, such as \"comments.4\".\n\nYou can, incidentally, index a specific array entry, as in:\n\n> db.blog.createIndex({\"comments.10.votes\": 1})\n\nHowever, this index would only be useful for queries for exactly the 11th array ele‐ ment (arrays start at index 0).\n\nOnly one field in an index entry can be from an array. This is to avoid the explosive number of index entries you’d get from multiple multikey indexes: every possible pair of elements would have to be indexed, causing indexes to be n*m entries per docu‐ ment. For example, suppose we had an index on {\"x\" : 1, \"y\" : 1}:\n\n> // x is an array - legal > db.multi.insert({\"x\" : [1, 2, 3], \"y\" : 1}) > > // y is an array - still legal > db.multi.insert({\"x\" : 1, \"y\" : [4, 5, 6]}) > > // x and y are arrays - illegal! > db.multi.insert({\"x\" : [1, 2, 3], \"y\" : [4, 5, 6]}) cannot index parallel arrays [y] [x]\n\nIntroduction to Indexes\n\n|\n\n115",
      "content_length": 2163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Were MongoDB to index the final example, it would have to create index entries for {\"x\" : 1, \"y\" : 4}, {\"x\" : 1, \"y\" : 5}, {\"x\" : 1, \"y\" : 6}, {\"x\" : 2, \"y\" : 4}, {\"x\" : 2, \"y\" : 5}, {\"x\" : 2, \"y\" : 6}, {\"x\" : 3, \"y\" : 4}, {\"x\" : 3, \"y\" : 5}, and {\"x\" : 3, \"y\" : 6} (and these arrays are only three elements long).\n\nMultikey index implications\n\nIf any document has an array field for the indexed key, the index immediately is flag‐ ged as a multikey index. You can see whether an index is multikey from explain’s output: if a multikey index was used, the \"isMultikey\" field will be true. Once an index has been flagged as multikey, it can never be un-multikeyed, even if all of the documents containing arrays in that field are removed. The only way to un-multikey it is to drop and recreate it.\n\nMultikey indexes may be a bit slower than non-multikey indexes. Many index entries can point at a single document, so MongoDB may need to do some deduplication before returning results.\n\nIndex Cardinality Cardinality refers to how many distinct values there are for a field in a collection. Some fields, such as \"gender\" or \"newsletter opt-out\", might only have two possi‐ ble values, which is considered a very low cardinality. Others, such as \"username\" or \"email\", might have a unique value for every document in the collection, which is high cardinality. Still others fall somewhere in between, such as \"age\" or \"zip code\".\n\nIn general, the greater the cardinality of a field, the more helpful an index on that field can be. This is because the index can quickly narrow the search space to a much smaller result set. For a low-cardinality field, an index generally cannot eliminate as many possible matches.\n\nFor example, suppose we had an index on \"gender\" and were looking for women named Susan. We could only narrow down the result space by approximately 50% before referring to individual documents to look up \"name\". Conversely, if we indexed by \"name\", we could immediately narrow down our result set to the tiny frac‐ tion of users named Susan, and then we could refer to those documents to check the gender.\n\nAs a rule of thumb, try to create indexes on high-cardinality keys or at least put high- cardinality keys first in compound indexes (before low-cardinality keys).\n\nexplain Output As you’ve seen, explain gives you lots of information about your queries. It’s one of the most important diagnostic tools there is for slow queries. You can find out which\n\n116\n\n|\n\nChapter 5: Indexes",
      "content_length": 2496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "indexes are being used and how by looking at a query’s \"explain\" output. For any query, you can add a call to explain at the end (the way you would add a sort or limit, but explain must be the last call).\n\nThere are two types of explain output that you’ll see most commonly: for indexed and nonindexed queries. Special index types may create slightly different query plans, but most fields should be similar. Also, sharding returns a conglomerate of explains (as covered in Chapter 14), as it runs the query on multiple servers.\n\nThe most basic type of explain is on a query that doesn’t use an index. You can tell that a query doesn’t use an index because it uses a \"COLLSCAN\".\n\nThe output of an explain on a query that uses an index varies, but in the simplest case it looks something like this if we add an index on imdb.rating:\n\n> db.users.find({\"age\" : 42}).explain('executionStats') { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"age\" : { \"$eq\" : 42 } }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[42.0, 42.0]\" ], \"username\" : [ \"[MinKey, MaxKey]\"\n\nexplain Output\n\n|\n\n117",
      "content_length": 1468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "118\n\n] } } }, \"rejectedPlans\" : [ ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 8449, \"executionTimeMillis\" : 15, \"totalKeysExamined\" : 8449, \"totalDocsExamined\" : 8449, \"executionStages\" : { \"stage\" : \"FETCH\", \"nReturned\" : 8449, \"executionTimeMillisEstimate\" : 10, \"works\" : 8450, \"advanced\" : 8449, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 66, \"restoreState\" : 66, \"isEOF\" : 1, \"invalidates\" : 0, \"docsExamined\" : 8449, \"alreadyHasObj\" : 0, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"nReturned\" : 8449, \"executionTimeMillisEstimate\" : 0, \"works\" : 8450, \"advanced\" : 8449, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 66, \"restoreState\" : 66, \"isEOF\" : 1, \"invalidates\" : 0, \"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2,\n\n|\n\nChapter 5: Indexes",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "\"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[42.0, 42.0]\" ], \"username\" : [ \"[MinKey, MaxKey]\" ] }, \"keysExamined\" : 8449, \"seeks\" : 1, \"dupsTested\" : 0, \"dupsDropped\" : 0, \"seenInvalidated\" : 0 } } }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nThis output first tells you what index was used: imdb.rating. Next is how many documents were actually returned as a result: \"nReturned\". Note that this doesn’t necessarily reflect how much work MongoDB did to answer the query (i.e., how many indexes and documents it had to search). \"totalKeysExamined\" reports the number of index entries scanned while \"totalDocsExamined\" indicates how many documents were scanned. The number of documents scanned is reflected in \"nscan nedObjects\".\n\nThe output also shows that there were no rejectedPlans and that it used a bounded search on the index within the value 42.0.\n\n\"executionTimeMillis\" reports how fast the query was executed, from the server receiving the request to when it sent a response. However, it may not always be the number you are looking for. If MongoDB tried multiple query plans, \"executionTime Millis\" will reflect how long it took all of them to run, not the one chosen as the best.\n\nNow that you know the basics, here is a breakdown of some of the more important fields in more detail:\n\n\"isMultiKey\" : false\n\nIf this query used a multikey index (see “Indexing Objects and Arrays” on page 114).\n\nexplain Output\n\n|\n\n119",
      "content_length": 1556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "\"nReturned\" : 8449\n\nThe number of documents returned by the query.\n\n\"totalDocsExamined\" : 8449\n\nThe number of times MongoDB had to follow an index pointer to the actual document on disk. If the query contains criteria that are not part of the index or requests fields that aren’t contained in the index, MongoDB must look up the document each index entry points to.\n\n\"totalKeysExamined\" : 8449\n\nThe number of index entries looked at, if an index was used. If this was a table scan, it is the number of documents examined.\n\n\"stage\" : \"IXSCAN\"\n\nIf MongoDB was able to fulfill this query using an index; if not \"COLSCAN\" would indicate it had to perform a collection scan to fulfill the query.\n\nIn this example, MongoDB found all matching documents using the index, which we know because \"totalKeysExamined\" is the same as \"totalDocsExa mined\". However, the query was told to return every field in the matching docu‐ ments and the index only contained the \"age\" and \"username\" fields.\n\n\"needYield\" : 0\n\nThe number of times this query yielded (paused) to allow a write request to pro‐ ceed. If there are writes waiting to go, queries will periodically release their lock and allow them to continue. On this system, there were no writes waiting so the query never yielded.\n\n\"executionTimeMillis\" : 15\n\nThe number of milliseconds it took the database to execute the query. The lower this number is, the better.\n\n\"indexBounds\" : {...}\n\nA description of how the index was used, giving ranges of the index traversed. In this example, as the first clause in the query was an exact match, the index only needed to look at that value: 42. The second index key was a free variable, because the query didn’t specify any restrictions to it. Thus, the database looked for values between negative infinity (\"$minElement\" : 1) and infinity (\"$maxEle ment\" : 1) for usernames within \"age\" : 42.\n\nLet’s take a look at a slightly more complicated example. Suppose you have an index on {\"username\" : 1, \"age\" : 1} and an index on {\"age\" : 1, \"username\" : 1}. What happens if you query for \"username\" and \"age\"? Well, it depends on the query:\n\n> db.users.find({\"age\" : {$gt : 10}, \"username\" : \"user2134\"}).explain() {\n\n120\n\n|\n\nChapter 5: Indexes",
      "content_length": 2223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "\"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ { \"username\" : { \"$eq\" : \"user2134\" } }, { \"age\" : { \"$gt\" : 10 } } ] }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"filter\" : { \"age\" : { \"$gt\" : 10 } }, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"[\\\"user2134\\\", \\\"user2134\\\"]\" ] } } }, \"rejectedPlans\" : [ { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\",\n\nexplain Output\n\n|\n\n121",
      "content_length": 715,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "\"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"(10.0, inf.0]\" ], \"username\" : [ \"[\\\"user2134\\\", \\\"user2134\\\"]\" ] } } } ] }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nWe are querying for an exact match on \"username\" and a range of values for \"age\", so the database chooses to use the {\"username\" : 1, \"age\" : 1} index, reversing the terms of the query. If, on the other hand, we query for an exact age and a range of names, MongoDB will use the other index:\n\n> db.users.find({\"age\" : 14, \"username\" : /.*/}).explain() { \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"test.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ { \"age\" : { \"$eq\" : 14\n\n122\n\n|\n\nChapter 5: Indexes",
      "content_length": 1056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "} }, { \"username\" : { \"$regex\" : \".*\" } } ] }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"filter\" : { \"username\" : { \"$regex\" : \".*\" } }, \"keyPattern\" : { \"age\" : 1, \"username\" : 1 }, \"indexName\" : \"age_1_username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"age\" : [ ], \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[14.0, 14.0]\" ], \"username\" : [ \"[\\\"\\\", {})\", \"[/.*/, /.*/]\" ] } } }, \"rejectedPlans\" : [ { \"stage\" : \"FETCH\", \"filter\" : { \"age\" : { \"$eq\" : 14 } },\n\nexplain Output\n\n|\n\n123",
      "content_length": 633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "\"inputStage\" : { \"stage\" : \"IXSCAN\", \"filter\" : { \"username\" : { \"$regex\" : \".*\" } }, \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"[\\\"\\\", {})\", \"[/.*/, /.*/]\" ] } } } ] }, \"serverInfo\" : { \"host\" : \"eoinbrazil-laptop-osx\", \"port\" : 27017, \"version\" : \"4.0.12\", \"gitVersion\" : \"5776e3cbf9e7afe86e6b29e22520ffb6766e95d4\" }, \"ok\" : 1 }\n\nIf you find that Mongo is using different indexes than you want it to for a query, you can force it to use a certain index by using hint. For instance, if you want to make sure MongoDB uses the {\"username\" : 1, \"age\" : 1} index on the previous query, you could say the following:\n\n> db.users.find({\"age\" : 14, \"username\" : /.*/}).hint({\"username\" : 1, \"age\" : 1})\n\nIf a query is not using the index that you want it to and you use a hint to change it, run an explain on the hinted query before deploying. If you force MongoDB to use an index on a query that it does not know how to use an index for, you could end up making the query less efficient than it was without the index.\n\n124\n\n|\n\nChapter 5: Indexes",
      "content_length": 1261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "When Not to Index Indexes are most effective at retrieving small subsets of data, and some types of quer‐ ies are faster without indexes. Indexes become less and less efficient as you need to get larger percentages of a collection because using an index requires two lookups: one to look at the index entry and one following the index’s pointer to the document. A collection scan only requires one: looking at the document. In the worst case (returning all of the documents in a collection) using an index would take twice as many lookups and would generally be significantly slower than a collection scan.\n\nUnfortunately, there isn’t a hard-and-fast rule about when an index helps and when it hinders as it really depends on the size of your data, indexes, documents, and average result set (Table 5-1). As a rule of thumb, an index often speeds things up if the query is returning 30% or more of the collection. However, this number can vary from 2% to 60%. Table 5-1 summarizes the conditions in which indexes or collection scans tend to work better.\n\nTable 5-1. Properties that affect the effectiveness of indexes\n\nIndexes often work well for Large collections Large documents Selective queries\n\nCollection scans often work well for Small collections Small documents Nonselective queries\n\nLet’s say we have an analytics system that collects statistics. Our application queries the system for all documents for a given account to generate a nice graph of all data from an hour ago to the beginning of time:\n\n> db.entries.find({\"created_at\" : {\"$lt\" : hourAgo}})\n\nWe index \"created_at\" to speed up this query.\n\nWhen we first launch, the result set is tiny and the query returns instantly. But after a couple of weeks, it starts being a lot of data, and after a month this query is already taking too long to run.\n\nFor most applications, this is probably the “wrong” query: do you really want a query that’s returning most of your dataset? Most applications, particularly those with large datasets, do not. However, there are some legitimate cases where you may want most or all of your data. For example, you might be exporting this data to a reporting sys‐ tem or using it for a batch job. In these cases, you would like to return this large pro‐ portion of the dataset as fast as possible.\n\nWhen Not to Index\n\n|\n\n125",
      "content_length": 2320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Types of Indexes There are a few index options you can specify when building an index that change the way the index behaves. The most common variations are described in the following sections, and more advanced or special-case options are described in the next chapter.\n\nUnique Indexes Unique indexes guarantee that each value will appear at most once in the index. For example, if you want to make sure no two documents can have the same value in the \"username\" key, you can create a unique index with a partialFilterExpression for only documents with a firstname field (more on this option later in the chapter):\n\n> db.users.createIndex({\"firstname\" : 1}, ... {\"unique\" : true, \"partialFilterExpression\":{ \"firstname\": {$exists: true } } } ) { \"createdCollectionAutomatically\" : false, \"numIndexesBefore\" : 3, \"numIndexesAfter\" : 4, \"ok\" : 1 }\n\nFor example, suppose you tried to insert the following documents in the users collection:\n\n> db.users.insert({firstname: \"bob\"}) WriteResult({ \"nInserted\" : 1 }) > db.users.insert({firstname: \"bob\"}) WriteResult({ \"nInserted\" : 0, \"writeError\" : { \"code\" : 11000, \"errmsg\" : \"E11000 duplicate key error collection: test.users index: firstname_1 dup key: { : \\\"bob\\\" }\" } })\n\nIf you check the collection, you’ll see that only the first \"bob\" was stored. Throwing duplicate key exceptions is not very efficient, so use the unique constraint for the occasional duplicate, not to filter out zillions of duplicates a second.\n\nA unique index that you are probably already familiar with is the index on \"_id\", which is automatically created whenever you create a collection. This is a normal unique index (aside from the fact that it cannot be dropped, as other unique indexes can be).\n\n126\n\n|\n\nChapter 5: Indexes",
      "content_length": 1753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "If a key does not exist, the index stores its value as null for that document. This means that if you create a unique index and try to insert more than one document that is missing the indexed field, the inserts will fail because you already have a document with a value of null. See “Partial Indexes” on page 128 for advice on han‐ dling this.\n\nIn some cases a value won’t be indexed. Index buckets are of limited size and if an index entry exceeds it, it just won’t be included in the index. This can cause confusion as it makes a document “invisible” to queries that use the index. Prior to MongoDB 4.2, a field was required to be smaller than 1,024 bytes to be included in an index. In MongoDB 4.2 and later, this constraint was removed. MongoDB does not return any sort of error or warning if a document’s fields cannot be indexed due to size. This means that keys longer than 8 KB will not be subject to the unique index constraints: you can insert identical 8 KB strings, for example.\n\nCompound unique indexes\n\nYou can also create a compound unique index. If you do this, individual keys can have the same values, but the combination of values across all keys in an index entry can appear in the index at most once.\n\nFor example, if we had a unique index on {\"username\" : 1, \"age\" : 1}, the fol‐ lowing inserts would be legal:\n\n> db.users.insert({\"username\" : \"bob\"}) > db.users.insert({\"username\" : \"bob\", \"age\" : 23}) > db.users.insert({\"username\" : \"fred\", \"age\" : 23})\n\nHowever, attempting to insert a second copy of any of these documents would cause a duplicate key exception.\n\nGridFS, the standard method for storing large files in MongoDB (see “Storing Files with GridFS” on page 156), uses a compound unique index. The collection that holds the file content has a unique index on {\"files_id\" : 1, \"n\" : 1}, which allows documents that look like (in part) the following:\n\n{\"files_id\" : ObjectId(\"4b23c3ca7525f35f94b60a2d\"), \"n\" : 1} {\"files_id\" : ObjectId(\"4b23c3ca7525f35f94b60a2d\"), \"n\" : 2} {\"files_id\" : ObjectId(\"4b23c3ca7525f35f94b60a2d\"), \"n\" : 3} {\"files_id\" : ObjectId(\"4b23c3ca7525f35f94b60a2d\"), \"n\" : 4}\n\nNote that all of the values for \"files_id\" are the same, but \"n\" is different.\n\nDropping duplicates\n\nIf you attempt to build a unique index on an existing collection, it will fail to build if there are any duplicate values:\n\nTypes of Indexes\n\n|\n\n127",
      "content_length": 2381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "> db.users.createIndex({\"age\" : 1}, {\"unique\" : true}) WriteResult({ \"nInserted\" : 0, \"writeError\" : { \"code\" : 11000, \"errmsg\" : \"E11000 duplicate key error collection: test.users index: age_1 dup key: { : 12 }\" } })\n\nGenerally, you’ll need to process your data (the aggregation framework can help) and figure out where the duplicates are and what to do with them.\n\nPartial Indexes As mentioned in the previous section, unique indexes count null as a value, so you cannot have a unique index with more than one document missing the key. However, there are lots of cases where you may want the unique index to be enforced only if the key exists. If you have a field that may or may not exist but must be unique when it does, you can combine the \"unique\" option with the \"partial\" option.\n\nPartial indexes in MongoDB are only created on a subset of the data. This is unlike sparse indexes on relational databases, which create fewer index entries pointing to a block of data—however, all blocks of data will have an associated sparse index entry in RDBMS.\n\nTo create a partial index, include the \"partialFilterExpression\" option. Partial indexes represent a superset of the functionality offered by sparse indexes, with a document representing the filter expression you wish to create it on. For example, if providing an email address was optional but, if provided, should be unique, we could do:\n\n> db.users.ensureIndex({\"email\" : 1}, {\"unique\" : true, \"partialFilterExpression\" : ... { email: { $exists: true } }})\n\nPartial indexes do not necessarily have to be unique. To make a nonunique partial index, simply do not include the \"unique\" option.\n\nOne thing to be aware of is that the same query can return different results depending on whether or not it uses the partial index. For example, suppose we have a collection where most of the documents have \"x\" fields, but one does not:\n\n> db.foo.find() { \"_id\" : 0 } { \"_id\" : 1, \"x\" : 1 } { \"_id\" : 2, \"x\" : 2 } { \"_id\" : 3, \"x\" : 3 }\n\n128\n\n|\n\nChapter 5: Indexes",
      "content_length": 2014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "When we do a query on \"x\", it will return all matching documents:\n\n> db.foo.find({\"x\" : {\"$ne\" : 2}}) { \"_id\" : 0 } { \"_id\" : 1, \"x\" : 1 } { \"_id\" : 3, \"x\" : 3 }\n\nIf we create a partial index on \"x\", the \"_id\" : 0 document won’t be included in the index. So now if we query on \"x\", MongoDB will use the index and not return the {\"_id\" : 0} document:\n\n> db.foo.find({\"x\" : {\"$ne\" : 2}}) { \"_id\" : 1, \"x\" : 1 } { \"_id\" : 3, \"x\" : 3 }\n\nYou can use hint to force it to do a table scan if you need documents with missing fields.\n\nIndex Administration As shown in the previous section, you can create new indexes using the createIndex function. An index only needs to be created once per collection. If you try to create the same index again, nothing will happen.\n\nAll of the information about a database’s indexes is stored in the system.indexes col‐ lection. This is a reserved collection, so you cannot modify its documents or remove documents from it. You can manipulate it only through the createIndex, createIn dexes, and dropIndexes database commands.\n\nWhen you create an index, you can see its metainformation in system.indexes. You can also run db.collectionName.getIndexes() to see information about all the indexes on a given collection:\n\n> db.students.getIndexes() [ { \"v\" : 2, \"key\" : { \"_id\" : 1 }, \"name\" : \"_id_\", \"ns\" : \"school.students\" }, { \"v\" : 2, \"key\" : { \"class_id\" : 1 }, \"name\" : \"class_id_1\", \"ns\" : \"school.students\"\n\nIndex Administration\n\n|\n\n129",
      "content_length": 1468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "}, { \"v\" : 2, \"key\" : { \"student_id\" : 1, \"class_id\" : 1 }, \"name\" : \"student_id_1_class_id_1\", \"ns\" : \"school.students\" } ]\n\nThe important fields are \"key\" and \"name\". The key can be used for hinting and other places where an index must be specified. This is a place where field order matters: an index on {\"class_id\" : 1, \"student_id\" : 1} is not the same as an index on {\"student_id\" : 1, \"class_id\" : 1}. The index name is used as an identifier for a lot of administrative index operations, such as dropIndexes. Whether or not the index is multikey is not specified in its spec.\n\nThe \"v\" field is used internally for index versioning. If you have any indexes that do not have at least a \"v\" : 1 field, they are being stored in an older, less efficient for‐ mat. You can upgrade them by ensuring that you’re running at least MongoDB ver‐ sion 2.0 and dropping and rebuilding the indexes.\n\nIdentifying Indexes Each index in a collection has a name that uniquely identifies that index and is used by the server to delete or manipulate it. Index names are, by default, key name1_dir1_keyname2_dir2_..._keynameN_dirN, where keynameX is the index’s key and dirX is the index’s direction (1 or -1). This can get unwieldy if indexes contain more than a couple of keys, so you can specify your own name as one of the options to createIndex:\n\n> db.soup.createIndex({\"a\" : 1, \"b\" : 1, \"c\" : 1, ..., \"z\" : 1}, ... {\"name\" : \"alphabet\"})\n\nThere is a limit to the number of characters in an index name, so complex indexes may need custom names to be created. A call to getLastError will show if the index creation succeeded or why it didn’t.\n\nChanging Indexes As your application grows and changes, you may find that your data or queries have changed and that indexes that used to work well no longer do. You can remove unneeded indexes using the dropIndex command:\n\n> db.people.dropIndex(\"x_1_y_1\") { \"nIndexesWas\" : 3, \"ok\" : 1 }\n\n130\n\n|\n\nChapter 5: Indexes",
      "content_length": 1949,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Use the \"name\" field from the index description to specify which index to drop.\n\nBuilding new indexes is time-consuming and resource-intensive. Prior to version 4.2, MongoDB will build an index as fast as possible, blocking all reads and writes on a database until the index build has finished. If you would like your database to remain somewhat responsive to reads and writes, use the \"background\" option when build‐ ing an index. This forces the index build to occasionally yield to other operations, but may still have a severe impact on your application (see “Building Indexes” on page 283 for more information). Background indexing is also much slower than fore‐ ground indexing. MongoDB version 4.2 introduced a new approach, the hybrid index build. It only holds the exclusive lock at the beginning and end of the index build. The rest of the build process yields to interleaving read and write operations. This replaces both the foreground and the background index build type in MongoDB 4.2.\n\nIf you have the choice, creating indexes on existing documents is slightly faster than creating the index first and then inserting all documents.\n\nThere is more on the operational aspects of building indexes in Chapter 19.\n\nIndex Administration\n\n|\n\n131",
      "content_length": 1253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "CHAPTER 6 Special Index and Collection Types\n\nThis chapter covers the special collections and index types MongoDB has available, including:\n\nCapped collections for queue-like data\n\nTTL indexes for caches\n\nFull-text indexes for simple string searching\n\nGeospatial indexes for 2D and spherical geometries\n\nGridFS for storing large files\n\nGeospatial Indexes MongoDB has two types of geospatial indexes: 2dsphere and 2d. 2dsphere indexes work with spherical geometries that model the surface of the earth based on the WGS84 datum. This datum models the surface of the earth as an oblate spheroid, meaning that there is some flattening at the poles. Distance calculations using 2sphere indexes, therefore, take the shape of the earth into account and provide a more accurate treatment of distance between, for example, two cities, than do 2d indexes. Use 2d indexes for points stored on a two-dimensional plane.\n\n2dsphere allows you to specify geometries for points, lines, and polygons in the Geo‐ JSON format. A point is given by a two-element array, representing [longitude, lati tude]:\n\n133",
      "content_length": 1089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "{ \"name\" : \"New York City\", \"loc\" : { \"type\" : \"Point\", \"coordinates\" : [50, 2] } }\n\nA line is given by an array of points:\n\n{ \"name\" : \"Hudson River\", \"loc\" : { \"type\" : \"LineString\", \"coordinates\" : [[0,1], [0,2], [1,2]] } }\n\nA polygon is specified the same way a line is (an array of points), but with a different \"type\":\n\n{ \"name\" : \"New England\", \"loc\" : { \"type\" : \"Polygon\", \"coordinates\" : [[0,1], [0,2], [1,2]] } }\n\nThe field that we are naming, \"loc\" in this example, can be called anything, but the field names in the embedded object are specified by GeoJSON and cannot be changed.\n\nYou can create a geospatial index using the \"2dsphere\" type with createIndex:\n\n> db.openStreetMap.createIndex({\"loc\" : \"2dsphere\"})\n\nTo create a 2dsphere index, pass a document to createIndex that specifies the field containing geometries you want to index for the collection in question and specify \"2dsphere\" as the value.\n\nTypes of Geospatial Queries There are three types of geospatial queries that you can perform: intersection, within, and nearness. You specify what you’re looking for as a GeoJSON object that looks like {\"$geometry\" : geoJsonDesc}.\n\n134\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 1205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "For example, you can find documents that intersect the query’s location using the \"$geoIntersects\" operator:\n\n> var eastVillage = { ... \"type\" : \"Polygon\", ... \"coordinates\" : [ ... [ ... [ -73.9732566, 40.7187272 ], ... [ -73.9724573, 40.7217745 ], ... [ -73.9717144, 40.7250025 ], ... [ -73.9714435, 40.7266002 ], ... [ -73.975735, 40.7284702 ], ... [ -73.9803565, 40.7304255 ], ... [ -73.9825505, 40.7313605 ], ... [ -73.9887732, 40.7339641 ], ... [ -73.9907554, 40.7348137 ], ... [ -73.9914581, 40.7317345 ], ... [ -73.9919248, 40.7311674 ], ... [ -73.9904979, 40.7305556 ], ... [ -73.9907017, 40.7298849 ], ... [ -73.9908171, 40.7297751 ], ... [ -73.9911416, 40.7286592 ], ... [ -73.9911943, 40.728492 ], ... [ -73.9914313, 40.7277405 ], ... [ -73.9914635, 40.7275759 ], ... [ -73.9916003, 40.7271124 ], ... [ -73.9915386, 40.727088 ], ... [ -73.991788, 40.7263908 ], ... [ -73.9920616, 40.7256489 ], ... [ -73.9923298, 40.7248907 ], ... [ -73.9925954, 40.7241427 ], ... [ -73.9863029, 40.7222237 ], ... [ -73.9787659, 40.719947 ], ... [ -73.9772317, 40.7193229 ], ... [ -73.9750886, 40.7188838 ], ... [ -73.9732566, 40.7187272 ] ... ] ... ]} > db.openStreetMap.find( ... {\"loc\" : {\"$geoIntersects\" : {\"$geometry\" : eastVillage}}})\n\nThis would find all point-, line-, and polygon-containing documents that had a point in the East Village in New York City.\n\nYou can use \"$geoWithin\" to query for things that are completely contained in an area (for instance, “What restaurants are in the East Village?”):\n\n> db.openStreetMap.find({\"loc\" : {\"$geoWithin\" : {\"$geometry\" : eastVillage}}})\n\nGeospatial Indexes\n\n|\n\n135",
      "content_length": 1617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "Unlike our first query, this will not return things that merely pass through the East Village (such as streets) or partially overlap it (such as a polygon describing Manhattan).\n\nFinally, you can query for nearby locations with \"$near\":\n\n> db.openStreetMap.find({\"loc\" : {\"$near\" : {\"$geometry\" : eastVillage}}})\n\nNote that \"$near\" is the only geospatial operator that implies a sort: results from \"$near\" are always returned in order of distance, from closest to farthest.\n\nUsing Geospatial Indexes MongoDB’s geospatial indexing allows you to efficiently execute spatial queries on a collection that contains geospatial shapes and points. To showcase the capabilities of geospatial features and compare different approaches, we will go through the process of writing queries for a simple geospatial application. We’ll go a little deeper into a few concepts central to geospatial indexes and then demonstrate their use with \"$geo Within\", \"$geoIntersects\", and \"$geoNear\".\n\nSuppose we are designing a mobile application to help users find restaurants in New York City. The application must:\n\nDetermine the neighborhood the user is currently in.\n\nShow the number of restaurants in that neighborhood.\n\nFind restaurants within a specified distance.\n\nWe will use a 2dsphere index to query on this spherical geometry data.\n\n2D versus spherical geometry in queries\n\nGeospatial queries can use either spherical or 2D (flat) geometries, depending on both the query and the type of index in use. Table 6-1 shows what kind of geometry each geospatial operator uses.\n\nTable 6-1. Query types and geometries in MongoDB\n\nQuery type $near (GeoJSON point, 2dsphere index) $near (legacy coordinates, 2d index) $geoNear (GeoJSON point, 2dsphere index) $geoNear (legacy coordinates, 2d index) $nearSphere (GeoJSON point, 2dsphere index) $nearSphere (legacy coordinates, 2d index)a $geoWithin : { $geometry: ... }\n\nGeometry type Spherical Flat Spherical Flat Spherical Spherical Spherical\n\n136\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 2023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Query type $geoWithin: { $box: ... }\n\n$geoWithin: { $polygon: ... }\n\n$geoWithin : { $center: ... }\n\nGeometry type Flat Flat Flat\n\n$geoWithin : { $centerSphere: ... } Spherical $geoIntersects Spherical a Use GeoJSON points instead.\n\nNote also that 2d indexes support both flat geometries and distance-only calculations on spheres (i.e., using $nearSphere). However, queries using spherical geometries will be more performant and accurate with a 2dsphere index.\n\nNote also that the $geoNear operator is an aggregation operator. The aggregation framework is discussed in Chapter 7. In addition to the $near query operation, the $geoNear aggregation operator and the special command geoNear enable us to query for nearby locations. Keep in mind that the $near query operator will not work on collections that are distributed using sharding, MongoDB’s scaling solution (see Chapter 15).\n\nThe geoNear command and the $geoNear aggregation operator require that a collec‐ tion have at most one 2dsphere index and at most one 2d index, whereas geospatial query operators (e.g., $near and $geoWithin) permit collections to have multiple geospatial indexes.\n\nThe geospatial index restriction for the geoNear command and the $geoNear aggre‐ gation operator exists because neither the geoNear command nor the $geoNear syn‐ tax includes the location field. As such, index selection among multiple 2d indexes or 2dsphere indexes is ambiguous.\n\nNo such restriction applies for geospatial query operators; these operators take a loca‐ tion field, eliminating the ambiguity.\n\nDistortion\n\nSpherical geometry will appear distorted when visualized on a map due to the nature of projecting a three-dimensional sphere, such as the earth, onto a flat plane.\n\nFor example, take the specification of the spherical square defined by the longitude, latitude points (0,0), (80,0), (80,80), and (0,80). Figure 6-1 depicts the area covered by this region.\n\nGeospatial Indexes\n\n|\n\n137",
      "content_length": 1952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Figure 6-1. The spherical square defined by the points (0,0), (80,0), (80, 80), and (0,80)\n\nSearching for restaurants\n\nIn this example, we will work with neighborhood and restaurant datasets based in New York City. You can download the example datasets from GitHub.\n\nWe can import the datasets into our database using the mongoimport tool as follows:\n\n$ mongoimport <path to neighborhoods.json> -c neighborhoods $ mongoimport <path to restaurants.json> -c restaurants\n\n138\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "We can create a 2dsphere index on each collection using the createIndex command in the mongo shell:\n\n> db.neighborhoods.createIndex({location:\"2dsphere\"}) > db.restaurants.createIndex({location:\"2dsphere\"})\n\nExploring the data\n\nWe can get a sense for the schema used for documents in these collections with a cou‐ ple of quick queries in the mongo shell:\n\n> db.neighborhoods.find({name: \"Clinton\"}) { \"_id\": ObjectId(\"55cb9c666c522cafdb053a4b\"), \"geometry\": { \"coordinates\": [ [ [-73.99,40.77], . . . [-73.99,40.77], [-73.99,40.77]] ] ], \"type\": \"Polygon\" }, \"name\": \"Clinton\" }\n\n> db.restaurants.find({name: \"Little Pie Company\"}) { \"_id\": ObjectId(\"55cba2476c522cafdb053dea\"), \"location\": { \"coordinates\": [ -73.99331699999999, 40.7594404 ], \"type\": \"Point\" }, \"name\": \"Little Pie Company\" }\n\nThe neighborhood document in the previous code corresponds to the area of New York City shown in Figure 6-2.\n\nGeospatial Indexes\n\n|\n\n139",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Figure 6-2. The Hell’s Kitchen (Clinton) neighborhood of New York City\n\nThe bakery corresponds to the location shown in Figure 6-3.\n\n140\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Figure 6-3. The Little Pie Company at 424 West 43rd Street\n\nFinding the current neighborhood\n\nAssuming the user’s mobile device can give a reasonably accurate location user, it is simple to find the user’s current neighborhood with $geoIntersects.\n\nSuppose the user is located at −73.93414657 longitude and 40.82302903 latitude. To find the current neighborhood (Hell’s Kitchen), we can specify a point using the spe‐ cial $geometry field in GeoJSON format:\n\n> db.neighborhoods.findOne({geometry:{$geoIntersects:{$geometry:{type:\"Point\", ... coordinates:[-73.93414657,40.82302903]}}}})\n\nThis query will return the following result:\n\n{ \"_id\":ObjectId(\"55cb9c666c522cafdb053a68\"),\n\nGeospatial Indexes\n\n|\n\n141",
      "content_length": 706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "\"geometry\":{ \"type\":\"Polygon\", \"coordinates\":[[[-73.93383000695911,40.81949109558767],...]]}, \"name\":\"Central Harlem North-Polo Grounds\" }\n\nFinding all restaurants in the neighborhood\n\nWe can also query to find all restaurants contained in a given neighborhood. To do so, we can execute the following in the mongo shell to find the neighborhood contain‐ ing the user, and then count the restaurants within that neighborhood. For example, to find all the restaurants in the Hell’s Kitchen neighborhood:\n\n> var neighborhood = db.neighborhoods.findOne({ geometry: { $geoIntersects: { $geometry: { type: \"Point\", coordinates: [-73.93414657,40.82302903] } } } });\n\n> db.restaurants.find({ location: { $geoWithin: { // Use the geometry from the neighborhood object we retrieved above $geometry: neighborhood.geometry } } }, // Project just the name of each matching restaurant {name: 1, _id: 0});\n\nThis query will tell you that there are 127 restaurants in the requested neighborhood that have the following names:\n\n{ \"name\": \"White Castle\" } { \"name\": \"Touch Of Dee'S\" } { \"name\": \"Mcdonald'S\" } { \"name\": \"Popeyes Chicken & Biscuits\" } { \"name\": \"Make My Cake\"\n\n142\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 1211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "} { \"name\": \"Manna Restaurant Ii\" } ... { \"name\": \"Harlem Coral Llc\" }\n\nFinding restaurants within a distance\n\nTo find restaurants within a specified distance of a point, you can use either \"$geoWi thin\" with \"$centerSphere\" to return results in unsorted order, or \"$nearSphere\" with \"$maxDistance\" if you need results sorted by distance.\n\nTo find restaurants within a circular region, use \"$geoWithin\" with \"$center Sphere\". \"$centerSphere\" is a MongoDB-specific syntax to denote a circular region by specifying the center and the radius in radians. \"$geoWithin\" does not return the documents in any specific order, so it might return the furthest documents first.\n\nThe following will find all restaurants within five miles of the user:\n\n> db.restaurants.find({ location: { $geoWithin: { $centerSphere: [ [-73.93414657,40.82302903], 5/3963.2 ] } } })\n\n\"$centerSphere\"’s second argument accepts the radius in radians. The query con‐ verts the distance to radians by dividing by the approximate equatorial radius of the earth, 3963.2 miles.\n\nApplications can use \"$centerSphere\" without having a geospatial index. However, geospatial indexes support much faster queries than the unindexed equivalents. Both 2dsphere and 2d geospatial indexes support \"$centerSphere\".\n\nYou may also use \"$nearSphere\" and specify a \"$maxDistance\" term in meters. This will return all restaurants within five miles of the user in sorted order from nearest to farthest:\n\n> var METERS_PER_MILE = 1609.34; db.restaurants.find({ location: { $nearSphere: { $geometry: { type: \"Point\",\n\nGeospatial Indexes\n\n|\n\n143",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "coordinates: [-73.93414657,40.82302903] }, $maxDistance: 5*METERS_PER_MILE } } });\n\nCompound Geospatial Indexes As with other types of indexes, you can combine geospatial indexes with other fields to optimize more complex queries. A possible query mentioned earlier was: “What restaurants are in Hell’s Kitchen?” Using only a geospatial index, we could narrow the field to everything in Hell’s Kitchen, but narrowing it down to only “restaurants” or “pizza” would require another field in the index:\n\n> db.openStreetMap.createIndex({\"tags\" : 1, \"location\" : \"2dsphere\"})\n\nThen we can quickly find a pizza place in Hell’s Kitchen:\n\n> db.openStreetMap.find({\"loc\" : {\"$geoWithin\" : ... {\"$geometry\" : hellsKitchen.geometry}}, ... \"tags\" : \"pizza\"})\n\nWe can have the “vanilla” index field either before or after the \"2dsphere\" field, depending on whether we’d like to filter by the vanilla field or the location first. Choose whichever is more selective (i.e., will filter out more results as the first index term).\n\n2d Indexes For nonspherical maps (videogame maps, time series data, etc.) you can use a \"2d\" index instead of \"2dsphere\":\n\n> db.hyrule.createIndex({\"tile\" : \"2d\"})\n\n2d indexes assume a perfectly flat surface, instead of a sphere. Thus, 2d indexes should not be used with spheres unless you don’t mind massive distortion around the poles.\n\nDocuments should use a two-element array for their \"2d\" indexed field. The elements in this array should reflect the longitude and lattitude coordinates, respectively. A sample document might look like this:\n\n{ \"name\" : \"Water Temple\", \"tile\" : [ 32, 22 ] }\n\nDo not use a 2d index if you plan to store GeoJSON data—they can only index points. You can store an array of points, but it will be stored as exactly that: an array of\n\n144\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 1835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "points, not a line. This is an important distinction for \"$geoWithin\" queries, in par‐ ticular. If you store a street as an array of points, the document will match \"$geoWithin\" if one of those points is within the given shape. However, the line cre‐ ated by those points might not be wholly contained in the shape.\n\nBy default, 2d indexes assume that your values are going to range from −180 to 180. If you are expecting larger or smaller bounds, you can specify what the minimum and maximum values will be as options to createIndex:\n\n> db.hyrule.createIndex({\"light-years\" : \"2d\"}, {\"min\" : -1000, \"max\" : 1000})\n\nThis will create a spatial index calibrated for a 2,000 × 2,000 square.\n\n2d indexes support the \"$geoWithin\", \"$nearSphere\", and \"$near\" query selectors. Use \"$geoWithin\" to query for points within a shape defined on a flat surface. \"$geo Within\" can query for all points within a rectangle, polygon, circle, or sphere; it uses the \"$geometry\" operator to specify the GeoJSON object. Returning to our grid indexed as follows:\n\n> db.hyrule.createIndex({\"tile\" : \"2d\"})\n\nthe following queries for documents within a rectangle defined by [10, 10] at the bottom-left corner and by [100, 100] at the top-right corner:\n\n> db.hyrule.find({ tile: { $geoWithin: { $box: [[10, 10], [100, 100]] } } })\n\n$box takes a two-element array: the first element specifies the coordinates of the lower-left corner and the second element the upper right.\n\nTo query for documents that are within the circle centered on [−17 , 20.5] and with a radius of 25 we can issue the following command:\n\n> db.hyrule.find({ tile: { $geoWithin: { $center: [[-17, 20.5] , 25] } } })\n\nThe following query returns all documents with coordinates that exist within the pol‐ ygon defined by [0, 0], [3, 6], and [6 , 0]:\n\n> db.hyrule.find({ tile: { $geoWithin: {\n\nGeospatial Indexes\n\n|\n\n145",
      "content_length": 1863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "$polygon: [[0, 0], [3, 6], [6, 0]] } } })\n\nYou specify a polygon as an array of points. The final point in the list will be “connec‐ ted to” the first point to form the polygon. This example would locate all documents containing points within the given triangle.\n\nMongoDB also supports rudimentary spherical queries on flat 2d indexes for legacy reasons. In general, spherical calculations should use a 2dsphere index, as described in “2D versus spherical geometry in queries” on page 136. However, to query for leg‐ acy coordinate pairs within a sphere, use \"$geoWithin\" with the “$centerSphere” operator. Specify an array that contains:\n\nThe grid coordinates of the circle’s center point\n\nThe circle’s radius measured in radians\n\nFor example:\n\n> db.hyrule.find({ loc: { $geoWithin: { $centerSphere: [[88, 30], 10/3963.2] } } })\n\nTo query for nearby points, use \"$near\". Proximity queries return the documents with coordinate pairs closest to the defined point and sort the results by distance. This finds all of the documents in the hyrule collection in order by distance from the point (20, 21):\n\n> db.hyrule.find({\"tile\" : {\"$near\" : [20, 21]}})\n\nA default limit of 100 documents is applied if no limit is specified. If you don’t need that many results, you should set a limit to conserve server resources. For example, the following code returns the 10 documents nearest to (20, 21):\n\n> db.hyrule.find({\"tile\" : {\"$near\" : [20, 21]}}).limit(10)\n\nIndexes for Full Text Search text indexes in MongoDB support full-text search requirements. This type of text index should not be confused with the MongoDB Atlas Full-Text Search Indexes, which utilize Apache Lucene for additional text search capabilities when compared to MongoDB text indexes. Use a text index if your application needs to enable users to\n\n146\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 1862,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "submit keyword queries that should match titles, descriptions, and text in other fields within a collection.\n\nIn previous chapters, we’ve queried for strings using exact matches and regular expressions, but these techniques have some limitations. Searching a large block of text for a regular expression is slow, and it’s tough to take morphology (e.g., that “entry” should match “entries”) and other challenges presented by human language into account. text indexes give you the ability to search text quickly and provide sup‐ port for common search engine requirements such as language-appropriate tokeniza‐ tion, stop words, and stemming.\n\ntext indexes require a number of keys proportional to the words in the fields being indexed. As a consequence, creating a text index can consume a large amount of sys‐ tem resources. You should create such an index at a time when it will not negatively impact the performance of your application for users or build the index in the back‐ ground, if possible. To ensure good performance, as with all indexes, you should also take care that any text index you create fits in RAM. See Chapter 19 for more infor‐ mation on creating indexes with minimal impact on your application.\n\nWrites to a collection require that all indexes are updated. If you are using text search, strings will be tokenized and stemmed and the index updated in, potentially, many places. For this reason, writes involving text indexes are usually more expensive than writes to single-field, compound, or even multikey indexes. Thus, you will tend to see poorer write performance on text-indexed collections than on others. They will also slow down data movement if you are sharding: all text must be reindexed when it is migrated to a new shard.\n\nCreating a Text Index Suppose we have a collection of Wikipedia articles that we want to index. To run a search over the text, we first need to create a text index. The following call to crea teIndex will create the index based on the terms in both the \"title\" and \"body\" fields:\n\n> db.articles.createIndex({\"title\": \"text\", \"body\" : \"text\"})\n\nThis is not like a “normal” compound index where there is an ordering on the keys. By default, each field is given equal consideration in a text index. You can control the relative importance MongoDB attaches to each field by specifying weights:\n\n> db.articles.createIndex({\"title\": \"text\", \"body\": \"text\"}, {\"weights\" : { \"title\" : 3, \"body\" : 2}})\n\nIndexes for Full Text Search\n\n|\n\n147",
      "content_length": 2492,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "This would weight the \"title\" field at a ratio of 3:2 in comparison to the \"body\" field.\n\nYou cannot change field weights after index creation (without dropping the index and recreating it), so you may want to play with weights on a sample dataset before creat‐ ing the index on your production data.\n\nFor some collections, you may not know which fields a document will contain. You can create a full-text index on all string fields in a document by creating an index on \"$**\"—this not only indexes all top-level string fields, but also searches embedded documents and arrays for string fields:\n\n> db.articles.createIndex({\"$**\" : \"text\"})\n\nText Search Use the \"$text\" query operator to perform text searches on a collection with a text index. \"$text\" will tokenize the search string using whitespace and most punctua‐ tion as delimiters, and perform a logical OR of all such tokens in the search string. For example, you could use the following query to find all articles containing any of the terms “impact,” “crater,” or “lunar.” Note that because our index is based on terms in both the title and body of an article, this query will match documents in which those terms are found in either field. For the purposes of this example, we will project the title so that we can fit more results on the page:\n\n> db.articles.find({\"$text\": {\"$search\": \"impact crater lunar\"}}, {title: 1} ).limit(10) { \"_id\" : \"170375\", \"title\" : \"Chengdu\" } { \"_id\" : \"34331213\", \"title\" : \"Avengers vs. X-Men\" } { \"_id\" : \"498834\", \"title\" : \"Culture of Tunisia\" } { \"_id\" : \"602564\", \"title\" : \"ABC Warriors\" } { \"_id\" : \"40255\", \"title\" : \"Jupiter (mythology)\" } { \"_id\" : \"80356\", \"title\" : \"History of Vietnam\" } { \"_id\" : \"22483\", \"title\" : \"Optics\" } { \"_id\" : \"8919057\", \"title\" : \"Characters in The Legend of Zelda series\" } { \"_id\" : \"20767983\", \"title\" : \"First inauguration of Barack Obama\" } { \"_id\" : \"17845285\", \"title\" : \"Kushiel's Mercy\" }\n\nYou can see that the results with our initial query are not terribly relevant. As with all technologies, it’s important to have a good grasp of how text indexes work in Mon‐ goDB in order to use them effectively. In this case, there are two problems with the way we’ve issued the query. The first is that our query is pretty broad, given that MongoDB issues the query using a logical OR of “impact,” “crater,” and “lunar.” The second problem is that, by default, a text search does not sort the results by relevance.\n\nWe can begin to address the problem of the query itself by using a phrase in our query. You can search for exact phrases by wrapping them in double quotes. For\n\n148\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 2670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "example, the following will find all documents containing the phrase “impact crater.” Possibly surprising is that MongoDB will issue this query as “impact crater” AND “lunar”:\n\n> db.articles.find({$text: {$search: \"\\\"impact crater\\\" lunar\"}}, {title: 1} ).limit(10) { \"_id\" : \"2621724\", \"title\" : \"Schjellerup (crater)\" } { \"_id\" : \"2622075\", \"title\" : \"Steno (lunar crater)\" } { \"_id\" : \"168118\", \"title\" : \"South Pole–Aitken basin\" } { \"_id\" : \"1509118\", \"title\" : \"Jackson (crater)\" } { \"_id\" : \"10096822\", \"title\" : \"Victoria Island structure\" } { \"_id\" : \"968071\", \"title\" : \"Buldhana district\" } { \"_id\" : \"780422\", \"title\" : \"Puchezh-Katunki crater\" } { \"_id\" : \"28088964\", \"title\" : \"Svedberg (crater)\" } { \"_id\" : \"780628\", \"title\" : \"Zeleny Gai crater\" } { \"_id\" : \"926711\", \"title\" : \"Fracastorius (crater)\" }\n\nTo make sure the semantics of this are clear, let’s look at an expanded example. For the following query, MongoDB will issue the query as “impact crater” AND (“lunar” OR “meteor”). MongoDB performs a logical AND of the phrase with the individual terms in the search string and a logical OR of the individual terms with one another:\n\n> db.articles.find({$text: {$search: \"\\\"impact crater\\\" lunar meteor\"}}, {title: 1} ).limit(10)\n\nIf you want to issue a logical AND between individual terms in a query, treat each term as a phrase by wrapping it in quotes. The following query will return documents containing “impact crater” AND “lunar” AND “meteor”:\n\n> db.articles.find({$text: {$search: \"\\\"impact crater\\\" \\\"lunar\\\" \\\"meteor\\\"\"}}, {title: 1}\n\n).limit(10)\n\n{ \"_id\" : \"168118\", \"title\" : \"South Pole–Aitken basin\" } { \"_id\" : \"330593\", \"title\" : \"Giordano Bruno (crater)\" } { \"_id\" : \"421051\", \"title\" : \"Opportunity (rover)\" } { \"_id\" : \"2693649\", \"title\" : \"Pascal Lee\" } { \"_id\" : \"275128\", \"title\" : \"Tektite\" } { \"_id\" : \"14594455\", \"title\" : \"Beethoven quadrangle\" } { \"_id\" : \"266344\", \"title\" : \"Space debris\" } { \"_id\" : \"2137763\", \"title\" : \"Wegener (lunar crater)\" } { \"_id\" : \"929164\", \"title\" : \"Dawes (lunar crater)\" } { \"_id\" : \"24944\", \"title\" : \"Plate tectonics\" }\n\nNow that you have a better understanding of using phrases and logical ANDs in your queries, let’s return to the problem of the results not being sorted by relevance. While the preceding results are certainly relevant, this is mostly due to the fairly strict query we’ve issued. We can do better by sorting for relevance.\n\nIndexes for Full Text Search\n\n|\n\n149",
      "content_length": 2463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Text queries cause some metadata to be associated with each query result. The meta‐ data is not displayed in the query results unless we explicitly project it using the $meta operator. So, in addition to the title, we will project the relevance score calcula‐ ted for each document. The relevance score is stored in the metadata field named \"textScore\". For this example, we’ll return to our query of “impact crater” AND “lunar”:\n\n> db.articles.find({$text: {$search: \"\\\"impact crater\\\" lunar\"}}, {title: 1, score: {$meta: \"textScore\"}}\n\n).limit(10)\n\n{\"_id\": \"2621724\", \"title\": \"Schjellerup (crater)\", \"score\": 2.852987132352941} {\"_id\": \"2622075\", \"title\": \"Steno (lunar crater)\", \"score\": 2.4766639610389607} {\"_id\": \"168118\", \"title\": \"South Pole–Aitken basin\", \"score\": 2.980198136295181} {\"_id\": \"1509118\", \"title\": \"Jackson (crater)\", \"score\": 2.3419137286324787} {\"_id\": \"10096822\", \"title\": \"Victoria Island structure\", \"score\": 1.782051282051282} {\"_id\": \"968071\", \"title\": \"Buldhana district\", \"score\": 1.6279783393501805} {\"_id\": \"780422\", \"title\": \"Puchezh-Katunki crater\", \"score\": 1.9295977011494254} {\"_id\": \"28088964\", \"title\": \"Svedberg (crater)\", \"score\": 2.497767857142857} {\"_id\": \"780628\", \"title\": \"Zeleny Gai crater\", \"score\": 1.4866071428571428} {\"_id\": \"926711\", \"title\": \"Fracastorius (crater)\", \"score\": 2.7511877111486487}\n\nNow you can see the relevance score projected with the title for each result. Note that they are not sorted. To sort the results in order of relevance score, we must add a call to sort, again using $meta to specify the \"textScore\" field value. Note that we must use the same field name in our sort as we used in our projection. In this case, we used the field name \"score\" for the relevance score value displayed in our search results. As you can see, the results are now sorted in decreasing order of relevance:\n\n> db.articles.find({$text: {$search: \"\\\"impact crater\\\" lunar\"}}, {title: 1, score: {$meta: \"textScore\"}}\n\n).sort({score: {$meta: \"textScore\"}}).limit(10)\n\n{\"_id\": \"1621514\", \"title\": \"Lunar craters\", \"score\": 3.1655242042922014} {\"_id\": \"14580008\", \"title\": \"Kuiper quadrangle\", \"score\": 3.0847527829208814} {\"_id\": \"1019830\", \"title\": \"Shackleton (crater)\", \"score\": 3.076471119932001} {\"_id\": \"2096232\", \"title\": \"Geology of the Moon\", \"score\": 3.064981949458484} {\"_id\": \"927269\", \"title\": \"Messier (crater)\", \"score\": 3.0638183133686008} {\"_id\": \"206589\", \"title\": \"Lunar geologic timescale\", \"score\": 3.062029540854157} {\"_id\": \"14536060\", \"title\": \"Borealis quadrangle\", \"score\": 3.0573010719646687} {\"_id\": \"14609586\", \"title\": \"Michelangelo quadrangle\", \"score\": 3.057224063486582} {\"_id\": \"14568465\", \"title\": \"Shakespeare quadrangle\", \"score\": 3.0495256481056443} {\"_id\": \"275128\", \"title\": \"Tektite\", \"score\" : 3.0378807169646915}\n\nText search is also available in the aggregation pipeline. We discuss the aggregation pipeline in Chapter 7.\n\n150\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 2974,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "Optimizing Full-Text Search There are a couple of ways to optimize full-text searches. If you can first narrow your search results by other criteria, you can create a compound index with a prefix of those criteria and then the full-text fields:\n\n> db.blog.createIndex({\"date\" : 1, \"post\" : \"text\"})\n\nThis is referred to as partitioning the full-text index, as it breaks it into several smaller trees based on \"date\" (in this example). This makes full-text searches for a specific date or date range much faster.\n\nYou can also use a postfix of other criteria to cover queries with the index. For exam‐ ple, if we were only returning the \"author\" and \"post\" fields, we could create a com‐ pound index on both:\n\n> db.blog.createIndex({\"post\" : \"text\", \"author\" : 1})\n\nThese prefix and postfix forms can be combined:\n\n> db.blog.createIndex({\"date\" : 1, \"post\" : \"text\", \"author\" : 1})\n\nSearching in Other Languages When a document is inserted (or the index is first created), MongoDB looks at the index’s fields and stems each word, reducing it to an essential unit. However, different languages stem words in different ways, so you must specify what language the index or document is in. text indexes allow a \"default_language\" option to be specified, which defaults to \"english\" but can be set to a number of other languages (see the online documentation for an up-to-date list).\n\nFor example, to create a French-language index, we could say:\n\n> db.users.createIndex({\"profil\" : \"text\", \"intérêts\" : \"text\"}, {\"default_language\" : \"french\"})\n\nThen French would be used for stemming, unless otherwise specified. You can, on a per-document basis, specify another stemming language by having a \"language\" field that describes the document’s language:\n\n> db.users.insert({\"username\" : \"swedishChef\", ... \"profile\" : \"Bork de bork\", language : \"swedish\"})\n\nCapped Collections “Normal” collections in MongoDB are created dynamically and automatically grow in size to fit additional data. MongoDB also supports a different type of collection, called a capped collection, which is created in advance and is fixed in size (see Figure 6-4).\n\nCapped Collections\n\n|\n\n151",
      "content_length": 2156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Figure 6-4. New documents are inserted at the end of the queue\n\nHaving fixed-size collections brings up an interesting question: what happens when we try to insert into a capped collection that is already full? The answer is that capped collections behave like circular queues: if we’re out of space, the oldest document will be deleted, and the new one will take its place (see Figure 6-5). This means that cap‐ ped collections automatically age out the oldest documents as new documents are inserted.\n\nCertain operations are not allowed on capped collections. Documents cannot be removed or deleted (aside from the automatic age-out described earlier), and updates that would cause documents to grow in size are disallowed. By preventing these two operations, we guarantee that documents in a capped collection are stored in inser‐ tion order and that there is no need to maintain a free list for space from removed documents.\n\n152\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 983,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Figure 6-5. When the queue is full, the oldest element will be replaced by the newest\n\nCapped collections have a different access pattern than most MongoDB collections: data is written sequentially over a fixed section of disk. This makes them tend to per‐ form writes quickly on spinning disks, especially if they can be given their own disk (so as not to be “interrupted” by other collections’ random writes).\n\nIn general, MongoDB TTL indexes are recommended over capped collections because they perform better with the WiredTiger storage engine. TTL indexes expire and remove data from normal collections based on the value of a date-typed field and a TTL value for the index. These are covered in more depth later in this chapter.\n\nCapped collections cannot be sharded. If an update or a replace‐ ment operation changes the document size in a capped collection, the operation will fail.\n\nCapped collections tend to be useful for logging, although they lack flexibility: you cannot control when data ages out, other than setting a size when you create the collection.\n\nCapped Collections\n\n|\n\n153",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "Creating Capped Collections Unlike normal collections, capped collections must be explicitly created before they are used. To create a capped collection, use the create command. From the shell, this can be done using createCollection:\n\n> db.createCollection(\"my_collection\", {\"capped\" : true, \"size\" : 100000});\n\nThe previous command creates a capped collection, my_collection, that has a fixed size of 100,000 bytes.\n\ncreateCollection can also specify a limit on the number of documents in a capped collection:\n\n> db.createCollection(\"my_collection2\", {\"capped\" : true, \"size\" : 100000, \"max\" : 100});\n\nYou could use this to keep, say, the latest 10 news articles or limit a user to 1,000 documents.\n\nOnce a capped collection has been created, it cannot be changed (it must be dropped and recreated if you wish to change its properties). Thus, you should think carefully about the size of a large collection before creating it.\n\nWhen limiting the number of documents in a capped collection, you must specify a size limit as well. Age-out will be based on whichever limit is reached first: it can neither hold more than \"max\" documents nor take up more than \"size\" space.\n\nAnother option for creating a capped collection is to convert an existing regular col‐ lection into a capped collection. This can be done using the convertToCapped command—in the following example, we convert the test collection to a capped col‐ lection of 10,000 bytes:\n\n> db.runCommand({\"convertToCapped\" : \"test\", \"size\" : 10000}); { \"ok\" : true }\n\nThere is no way to “uncap” a capped collection (other than dropping it).\n\nTailable Cursors Tailable cursors are a special type of cursor that are not closed when their results are exhausted. They were inspired by the tail -f command and, similar to that com‐ mand, will continue fetching output for as long as possible. Because the cursors do not die when they run out of results, they can continue to fetch new results as docu‐ ments are added to the collection. Tailable cursors can be used only on capped collec‐ tions, since insert order is not tracked for normal collections. For the vast majority of uses, change streams, covered in Chapter 16, are recommended over tailable cursors\n\n154\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 2268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "as they offer vastly more control and configuration plus they work with normal collections.\n\nTailable cursors are often used for processing documents as they are inserted onto a “work queue” (the capped collection). Because tailable cursors will time out after 10 minutes of no results, it is important to include logic to requery the collection if they die. The mongo shell does not allow you to use tailable cursors, but using one in PHP looks something like the following:\n\n$cursor = $collection->find([], [ 'cursorType' => MongoDB\\Operation\\Find::TAILABLE_AWAIT, 'maxAwaitTimeMS' => 100, ]);\n\nwhile (true) { if ($iterator->valid()) { $document = $iterator->current(); printf(\"Consumed document created at: %s\\n\", $document->createdAt); }\n\n$iterator->next(); }\n\nThe cursor will process results or wait for more results to arrive until it times out or someone kills the query operation.\n\nTime-To-Live Indexes As mentioned in the previous section, capped collections give you limited control over when their contents are overwritten. If you need a more flexible age-out system, TTL indexes allow you to set a timeout for each document. When a document rea‐ ches a preconfigured age, it will be deleted. This type of index is useful for caching use cases such as session storage.\n\nYou can create a TTL index by specifying the \"expireAfterSeconds\" option in the second argument to createIndex:\n\n> // 24-hour timeout > db.sessions.createIndex({\"lastUpdated\" : 1}, {\"expireAfterSeconds\" : 60*60*24})\n\nThis creates a TTL index on the \"lastUpdated\" field. If a document’s \"lastUpdated\" field exists and is a date, the document will be removed once the server time is \"expireAfterSeconds\" seconds ahead of the document’s time.\n\nTo prevent an active session from being removed, you can update the \"lastUpdated\" field to the current time whenever there is activity. Once \"lastUpdated\" is 24 hours old, the document will be removed.\n\nTime-To-Live Indexes\n\n|\n\n155",
      "content_length": 1953,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "MongoDB sweeps the TTL index once per minute, so you should not depend on to- the-second granularity. You can change the \"expireAfterSeconds\" using the coll Mod command:\n\n> db.runCommand( {\"collMod\" : \"someapp.cache\" , \"index\" : { \"keyPattern\" : ... {\"lastUpdated\" : 1} , \"expireAfterSeconds\" : 3600 } } );\n\nYou can have multiple TTL indexes on a given collection. They cannot be compound indexes but can be used like “normal” indexes for the purposes of sorting and query optimization.\n\nStoring Files with GridFS GridFS is a mechanism for storing large binary files in MongoDB. There are several reasons why you might consider using GridFS for file storage:\n\nUsing GridFS can simplify your stack. If you’re already using MongoDB, you might be able to use GridFS instead of a separate tool for file storage.\n\nGridFS will leverage any existing replication or autosharding that you’ve set up for MongoDB, so getting failover and scale-out for file storage is easier.\n\nGridFS can alleviate some of the issues that certain filesystems can exhibit when being used to store user uploads. For example, GridFS does not have issues with storing large numbers of files in the same directory.\n\nThere are some downsides, too:\n\nPerformance is slower. Accessing files from MongoDB will not be as fast as going directly through the filesystem.\n\nYou can only modify documents by deleting them and resaving the whole thing. MongoDB stores files as multiple documents, so it cannot lock all of the chunks in a file at the same time.\n\nGridFS is generally best when you have large files you’ll be accessing in a sequential fashion that won’t be changing much.\n\nGetting Started with GridFS: mongofiles The easiest way to try out GridFS is by using the mongofiles utility. mongofiles is included with all MongoDB distributions and can be used to upload, download, list, search for, or delete files in GridFS.\n\nAs with any of the other command-line tools, run mongofiles --help to see the options available for mongofiles.\n\n156\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 2054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "The following session shows how to use mongofiles to upload a file from the filesys‐ tem to GridFS, list all of the files in GridFS, and download a file that we’ve previously uploaded:\n\n$ echo \"Hello, world\" > foo.tx $ mongofiles put foo.txt 2019-10-30T10:12:06.588+0000 connected to: localhost 2019-10-30T10:12:06.588+0000 added file: foo.txt $ mongofiles list 2019-10-30T10:12:41.603+0000 connected to: localhost foo.txt 13 $ rm foo.txt $ mongofiles get foo.txt 2019-10-30T10:13:23.948+0000 connected to: localhost 2019-10-30T10:13:23.955+0000 finished writing to foo.txt $ cat foo.txt Hello, world\n\nIn the previous example, we perform three basic operations using mongofiles: put, list, and get. The put operation takes a file in the filesystem and adds it to GridFS. list will list any files that have been added to GridFS. get does the inverse of put: it takes a file from GridFS and writes it to the filesystem. mongofiles also supports two other operations: search for finding files in GridFS by filename and delete for removing a file from GridFS.\n\nWorking with GridFS from the MongoDB Drivers All the client libraries have GridFS APIs. For example, with PyMongo (the Python driver for MongoDB) you can perform the same series of operations (this assumes Python 3 and a locally running mongod on port 27017) as we did with mongofiles as follows:\n\n>>> import pymongo >>> import gridfs >>> client = pymongo.MongoClient() >>> db = client.test >>> fs = gridfs.GridFS(db) >>> file_id = fs.put(b\"Hello, world\", filename=\"foo.txt\") >>> fs.list() ['foo.txt'] >>> fs.get(file_id).read() b'Hello, world'\n\nThe API for working with GridFS from PyMongo is very similar to that of mongofiles: you can easily perform the basic put, get, and list operations. Almost all the MongoDB drivers follow this basic pattern for working with GridFS, while often exposing more advanced functionality as well. For driver-specific information on GridFS, please check out the documentation for the specific driver you’re using.\n\nStoring Files with GridFS\n\n|\n\n157",
      "content_length": 2041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Under the Hood GridFS is a lightweight specification for storing files that is built on top of normal MongoDB documents. The MongoDB server actually does almost nothing to “special-case” the handling of GridFS requests; all the work is handled by the client- side drivers and tools.\n\nThe basic idea behind GridFS is that we can store large files by splitting them up into chunks and storing each chunk as a separate document. Because MongoDB supports storing binary data in documents, we can keep the storage overhead for chunks to a minimum. In addition to storing each chunk of a file, we store a single document that groups the chunks together and contains metadata about the file.\n\nThe chunks for GridFS are stored in their own collection. By default chunks will use the collection fs.chunks, but this can be overridden. Within the chunks collection the structure of the individual documents is pretty simple:\n\n{ \"_id\" : ObjectId(\"...\"), \"n\" : 0, \"data\" : BinData(\"...\"), \"files_id\" : ObjectId(\"...\") }\n\nLike any other MongoDB document, a chunk has its own unique \"_id\". In addition, it has a couple of other keys:\n\n\"files_id\"\n\nThe \"_id\" of the file document that contains the metadata for the file this chunk is from\n\n\"n\"\n\nThe chunk’s position in the file, relative to the other chunks\n\n\"data\"\n\nThe bytes in this chunk of the file\n\nThe metadata for each file is stored in a separate collection, which defaults to fs.files. Each document in the files collection represents a single file in GridFS and can con‐ tain any custom metadata that should be associated with that file. In addition to any user-defined keys, there are a couple of keys that are mandated by the GridFS specification:\n\n\"_id\"\n\nA unique ID for the file—this is what will be stored in each chunk as the value for the \"files_id\" key.\n\n158\n\n|\n\nChapter 6: Special Index and Collection Types",
      "content_length": 1859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "\"length\"\n\nThe total number of bytes making up the content of the file.\n\n\"chunkSize\"\n\nThe size of each chunk comprising the file, in bytes. The default is 255 KB, but this can be adjusted if needed.\n\n\"uploadDate\"\n\nA timestamp representing when this file was stored in GridFS.\n\n\"md5\"\n\nAn MD5 checksum of this file’s contents, generated on the server side.\n\nOf all the required keys, perhaps the most interesting (or least self-explanatory) is \"md5\". The value for \"md5\" is generated by the MongoDB server using the filemd5 command, which computes the MD5 checksum of the uploaded chunks. This means that users can check the value of the \"md5\" key to ensure that a file was uploaded correctly.\n\nAs mentioned previously, you are not limited to the required fields in fs.files: feel free to keep any other file metadata in this collection as well. You might want to keep information such as download count, MIME type, or user rating with a file’s meta‐ data.\n\nOnce you understand the underlying GridFS specification, it becomes trivial to implement features that the driver you’re using might not provide helpers for. For example, you can use the distinct command to get a list of unique filenames stored in GridFS:\n\n> db.fs.files.distinct(\"filename\") [ \"foo.txt\" , \"bar.txt\" , \"baz.txt\" ]\n\nThis allows your application a great deal of flexibility in loading and collecting infor‐ mation about files. We’ll change direction slightly in the next chapter, as we introduce the aggregation framework. It offers a range of data analytic tools to process the data in your database.\n\nStoring Files with GridFS\n\n|\n\n159",
      "content_length": 1605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "CHAPTER 7 Introduction to the Aggregation Framework\n\nMany applications require data analysis of one form or another. MongoDB provides powerful support for running analytics natively using the aggregation framework. In this chapter, we introduce this framework and some of the fundamental tools it pro‐ vides. We’ll cover:\n\nThe aggregation framework\n\nAggregation stages\n\nAggregation expressions\n\nAggregation accumulators\n\nIn the next chapter we’ll dive deeper and look at more advanced aggregation features, including the ability to perform joins across collections.\n\nPipelines, Stages, and Tunables The aggregation framework is a set of analytics tools within MongoDB that allow you to do analytics on documents in one or more collections.\n\nThe aggregation framework is based on the concept of a pipeline. With an aggrega‐ tion pipeline we take input from a MongoDB collection and pass the documents from that collection through one or more stages, each of which performs a different opera‐ tion on its inputs (Figure 7-1). Each stage takes as input whatever the stage before it produced as output. The inputs and outputs for all stages are documents—a stream of documents, if you will.\n\n161",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Figure 7-1. The aggregation pipeline\n\nIf you’re familiar with pipelines in a Linux shell, such as bash, this is a very similar idea. Each stage has a specific job that it does. It expects a specific form of document and produces a specific output, which is itself a stream of documents. At the end of the pipeline we get access to the output, in much the same way that we would by exe‐ cuting a find query. That is, we get a stream of documents back that we can then use to do additional work, whether it’s creating a report of some kind, generating a web‐ site, or some other type of task.\n\nNow, let’s dive in a little deeper and consider the individual stages. An individual stage of an aggregation pipeline is a data processing unit. It takes in a stream of input documents one at a time, processes each document one at a time, and produces an output stream of documents one at a time (Figure 7-2).\n\nFigure 7-2. Stages of the aggregation pipeline\n\nEach stage provides a set of knobs, or tunables, that we can control to parameterize the stage to perform whatever task we’re interested in doing. A stage performs a generic, general-purpose task of some kind, and we parameterize the stage for the particular collection that we’re working with and exactly what we would like that stage to do with those documents.\n\nThese tunables typically take the form of operators that we can supply that will mod‐ ify fields, perform arithmetic operations, reshape documents, or do some sort of accumulation task or a variety of other things.\n\nBefore we start looking at some concrete examples, there’s one more aspect of pipe‐ lines that is especially important to keep in mind as you begin to work with them. Frequently, we want to include the same type of stage multiple times within a single pipeline (Figure 7-3). For example, we may want to perform an initial filter so that we\n\n162\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "don’t have to pass the entire collection into our pipeline. Later, following some addi‐ tional processing, we might then want to filter further, applying a different set of criteria.\n\nFigure 7-3. Repeated stages in the aggregation pipeline\n\nTo recap, pipelines work with MongoDB collections. They’re composed of stages, each of which does a different data processing task on its input and produces docu‐ ments as output to be passed to the next stage. Finally, at the end of the processing, a pipeline produces output that we can then do something with in our application or that we can send to a collection for later use. In many cases, in order to perform the analysis we need to do, we will include the same type of stage multiple times within an individual pipeline.\n\nGetting Started with Stages: Familiar Operations To get started developing aggregation pipelines, we will look at building some pipe‐ lines that involve operations that are already familiar to you. For this we will look at the match, project, sort, skip, and limit stages.\n\nTo work through these aggregation examples, we will use a collection of company data. The collection has a number of fields that specify details about the companies, such as name, a short description of the company, and when the company was founded.\n\nThere are also fields describing the rounds of funding a company has gone through, important milestones for the company, whether or not the company has been through an initial public offering (IPO), and, if so, the details of the IPO. Here’s an example document containing data on Facebook, Inc.:\n\n{ \"_id\" : \"52cdef7c4bab8bd675297d8e\", \"name\" : \"Facebook\", \"category_code\" : \"social\", \"founded_year\" : 2004, \"description\" : \"Social network\", \"funding_rounds\" : [{ \"id\" : 4, \"round_code\" : \"b\", \"raised_amount\" : 27500000,\n\nGetting Started with Stages: Familiar Operations\n\n|\n\n163",
      "content_length": 1876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "164\n\n\"raised_currency_code\" : \"USD\", \"funded_year\" : 2006, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Meritech Capital Partners\", \"permalink\" : \"meritech-capital-partners\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Founders Fund\", \"permalink\" : \"founders-fund\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" }, \"person\" : null } ] }, { \"id\" : 2197, \"round_code\" : \"c\", \"raised_amount\" : 15000000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2008, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"European Founders Fund\", \"permalink\" : \"european-founders-fund\" }, \"person\" : null }\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "] }], \"ipo\" : { \"valuation_amount\" : NumberLong(\"104000000000\"), \"valuation_currency_code\" : \"USD\", \"pub_year\" : 2012, \"pub_month\" : 5, \"pub_day\" : 18, \"stock_symbol\" : \"NASDAQ:FB\" } }\n\nAs our first aggregation example, let’s do a simple filter looking for all companies that were founded in 2004:\n\ndb.companies.aggregate([ {$match: {founded_year: 2004}}, ])\n\nThis is equivalent to the following operation using find:\n\ndb.companies.find({founded_year: 2004})\n\nNow let’s add a project stage to our pipeline to reduce the output to just a few fields include \"name\" and the \"_id\" per document. We’ll exclude \"founded_year\". Our pipeline will be as follows:\n\nfield, but\n\ndb.companies.aggregate([ {$match: {founded_year: 2004}}, {$project: { _id: 0, name: 1, founded_year: 1 }} ])\n\nIf we run this, we get output that looks like the following:\n\n{\"name\": \"Digg\", \"founded_year\": 2004 } {\"name\": \"Facebook\", \"founded_year\": 2004 } {\"name\": \"AddThis\", \"founded_year\": 2004 } {\"name\": \"Veoh\", \"founded_year\": 2004 } {\"name\": \"Pando Networks\", \"founded_year\": 2004 } {\"name\": \"Jobster\", \"founded_year\": 2004 } {\"name\": \"AllPeers\", \"founded_year\": 2004 } {\"name\": \"blinkx\", \"founded_year\": 2004 } {\"name\": \"Yelp\", \"founded_year\": 2004 } {\"name\": \"KickApps\", \"founded_year\": 2004 } {\"name\": \"Flickr\", \"founded_year\": 2004 } {\"name\": \"FeedBurner\", \"founded_year\": 2004 } {\"name\": \"Dogster\", \"founded_year\": 2004 } {\"name\": \"Sway\", \"founded_year\": 2004 } {\"name\": \"Loomia\", \"founded_year\": 2004 } {\"name\": \"Redfin\", \"founded_year\": 2004 }\n\nGetting Started with Stages: Familiar Operations\n\n|\n\n165",
      "content_length": 1581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "{\"name\": \"Wink\", \"founded_year\": 2004 } {\"name\": \"Techmeme\", \"founded_year\": 2004 } {\"name\": \"Eventful\", \"founded_year\": 2004 } {\"name\": \"Oodle\", \"founded_year\": 2004 } ...\n\nLet’s unpack this aggregation pipeline in a little more detail. The first thing you will notice is that we’re using the aggregate method. This is the method we call when we want to run an aggregation query. To aggregate, we pass in an aggregation pipeline. A pipeline is an array with documents as elements. Each of the documents must stipu‐ late a particular stage operator. In this example, we have a pipeline that has two stages: a match stage for filtering and a project stage with which we’re limiting the output to just two fields per document.\n\nThe match stage filters against the collection and passes the resulting documents to the project stage one at a time. The project stage then performs its operation, reshap‐ ing the documents, and passes the output out of the pipeline and back to us.\n\nNow let’s extend our pipeline a bit further to include a limit stage. We’re going to match using the same query, but we’ll limit our result set to five and then project out the fields we want. For simplicity, let’s limit our output to just the names of each company:\n\ndb.companies.aggregate([ {$match: {founded_year: 2004}}, {$limit: 5}, {$project: { _id: 0, name: 1}} ])\n\nThe result is as follows:\n\n{\"name\": \"Digg\"} {\"name\": \"Facebook\"} {\"name\": \"AddThis\"} {\"name\": \"Veoh\"} {\"name\": \"Pando Networks\"}\n\nNote that we’ve constructed this pipeline so that we limit before the project stage. If we ran the project stage first and then the limit, as in the following query, we would get exactly the same results, but we’d have to pass hundreds of documents through the project stage before finally limiting the results to five:\n\ndb.companies.aggregate([ {$match: {founded_year: 2004}}, {$project: { _id: 0, name: 1}}, {$limit: 5} ])\n\n166\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1966,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Regardless of what types of optimizations the MongoDB query planner might be capable of in a given release, you should always consider the efficiency of your aggre‐ gation pipeline. Ensure that you are limiting the number of documents that need to be passed on from one stage to another as you build your pipeline.\n\nThis requires careful consideration of the entire flow of documents through a pipe‐ line. In the case of the preceding query, we’re only interested in the first five docu‐ ments that match our query, regardless of how they are sorted, so it’s perfectly fine to limit as our second stage.\n\nHowever, if the order matters, then we’ll need to sort before the limit stage. Sorting works in a manner similar to what we have seen already, except that in the aggrega‐ tion framework, we specify sort as a stage within a pipeline as follows (in this case, we will sort by name in ascending order):\n\ndb.companies.aggregate([ { $match: { founded_year: 2004 } }, { $sort: { name: 1} }, { $limit: 5 }, { $project: { _id: 0, name: 1 } } ])\n\nWe get the following result from our companies collection:\n\n{\"name\": \"1915 Studios\"} {\"name\": \"1Scan\"} {\"name\": \"2GeeksinaLab\"} {\"name\": \"2GeeksinaLab\"} {\"name\": \"2threads\"}\n\nNote that we’re looking at a different set of five companies now, getting instead the first five documents in alphanumeric order by name.\n\nFinally, let’s take a look at including a skip stage. Here, we sort first, then skip the first 10 documents and again limit our result set to 5 documents:\n\ndb.companies.aggregate([ {$match: {founded_year: 2004}}, {$sort: {name: 1}}, {$skip: 10}, {$limit: 5}, {$project: { _id: 0, name: 1}}, ])\n\nLet’s review our pipeline one more time. We have five stages. First, we’re filtering the companies collection, looking only for documents where the \"founded_year\" is 2004. Then we’re sorting based on the name in ascending order, skipping the first 10\n\nGetting Started with Stages: Familiar Operations\n\n|\n\n167",
      "content_length": 1960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "matches, and limiting our end results to 5. Finally, we pass those five documents on to the project stage, where we reshape the documents such that our output docu‐ ments contain just the company name.\n\nHere, we’ve looked at constructing pipelines using stages that perform operations that should already be familiar to you. These operations are provided in the aggregation framework because they are necessary for the types of analytics that we’ll want to accomplish using stages discussed in later sections. As we move through the rest of this chapter, we will take a deep dive into the other operations that the aggregation framework provides.\n\nExpressions As we move deeper into our discussion of the aggregation framework, it is important to have a sense of the different types of expressions available for use as you construct aggregation pipelines. The aggregation framework supports many different classes of expressions:\n\nBoolean expressions allow us to use AND, OR, and NOT expressions.\n\nSet expressions allow us to work with arrays as sets. In particular, we can get the intersection or union of two or more sets. We can also take the difference of two sets and perform a number of other set operations.\n\nComparison expressions enable us to express many different types of range filters.\n\nArithmetic expressions enable us to calculate the ceiling, floor, natural log, and log, as well as perform simple arithmetic operations like multiplication, division, addition, and subtraction. We can even do more complex operations, such as cal‐ culating the square root of a value.\n\nString expressions allow us to concatenate, find substrings, and perform opera‐ tions having to do with case and text search operations.\n\nArray expressions provide a lot of power for manipulating arrays, including the ability to filter array elements, slice an array, or just take a range of values from a specific array.\n\nVariable expressions, which we won’t dive into too deeply, allow us to work with literals, expressions for parsing date values, and conditional expressions.\n\nAccumulators provide the ability to calculate sums, descriptive statistics, and many other types of values.\n\n168\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 2235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "$project Now we’re going to take a deeper dive into the project stage and reshaping docu‐ ments, exploring the types of reshaping operations that should be most common in the applications that you develop. We have seen some simple projections in aggrega‐ tion pipelines, and now we’ll take a look at some that are a little more complex.\n\nFirst, let’s look at promoting nested fields. In the following pipeline, we are doing a match:\n\ndb.companies.aggregate([ {$match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\" }}, {$project: { _id: 0, name: 1, ipo: \"$ipo.pub_year\", valuation: \"$ipo.valuation_amount\", funders: \"$funding_rounds.investments.financial_org.permalink\" }} ]).pretty()\n\nAs an example of the relevant fields for documents in our companies collection, let’s again look at a portion of the Facebook document:\n\n{ \"_id\" : \"52cdef7c4bab8bd675297d8e\", \"name\" : \"Facebook\", \"category_code\" : \"social\", \"founded_year\" : 2004, \"description\" : \"Social network\", \"funding_rounds\" : [{ \"id\" : 4, \"round_code\" : \"b\", \"raised_amount\" : 27500000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2006, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Meritech Capital Partners\", \"permalink\" : \"meritech-capital-partners\" }, \"person\" : null\n\n$project\n\n|\n\n169",
      "content_length": 1411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "}, { \"company\" : null, \"financial_org\" : { \"name\" : \"Founders Fund\", \"permalink\" : \"founders-fund\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" }, \"person\" : null } ] }, { \"id\" : 2197, \"round_code\" : \"c\", \"raised_amount\" : 15000000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2008, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"European Founders Fund\", \"permalink\" : \"european-founders-fund\" }, \"person\" : null } ] }], \"ipo\" : { \"valuation_amount\" : NumberLong(\"104000000000\"), \"valuation_currency_code\" : \"USD\", \"pub_year\" : 2012, \"pub_month\" : 5, \"pub_day\" : 18, \"stock_symbol\" : \"NASDAQ:FB\" } }\n\nGoing back to our match:\n\ndb.companies.aggregate([ {$match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\" }}, {$project: { _id: 0, name: 1,\n\n170\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "ipo: \"$ipo.pub_year\", valuation: \"$ipo.valuation_amount\", funders: \"$funding_rounds.investments.financial_org.permalink\" }} ]).pretty()\n\nwe are filtering for all companies that had a funding round in which Greylock Part‐ ners participated. The permalink value, \"greylock\", is the unique identifier for such documents. Here is another view of the Facebook document with just the relevant fields displayed:\n\n{ ... \"name\" : \"Facebook\", ... \"funding_rounds\" : [{ ... \"investments\" : [{ ... \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" }, ... }, { ... \"financial_org\" : { \"name\" : \"Meritech Capital Partners\", \"permalink\" : \"meritech-capital-partners\" }, ... }, { ... \"financial_org\" : { \"name\" : \"Founders Fund\", \"permalink\" : \"founders-fnd\" }, ... }, { \"company\" : null, \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" }, ... }], ... ]}, {\n\n$project\n\n|\n\n171",
      "content_length": 902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "... \"investments\" : [{ ... \"financial_org\" : { \"name\" : \"European Founders Fund\", \"permalink\" : \"european-founders-fund\" }, ... }] }], \"ipo\" : { \"valuation_amount\" : NumberLong(\"104000000000\"), \"valuation_currency_code\" : \"USD\", \"pub_year\" : 2012, \"pub_month\" : 5, \"pub_day\" : 18, \"stock_symbol\" : \"NASDAQ:FB\" } }\n\nThe project stage we have defined in this aggregation pipeline will suppress the \"_id\" and include the \"name\". It will also promote some nested fields. This project uses dot notation to express field paths that reach into the \"ipo\" field and the \"fund ing_rounds\" field to select values from those nested documents and arrays. This project stage will make those the values of top-level fields in the documents it pro‐ duces as output, as shown here:\n\n{ \"name\" : \"Digg\", \"funders\" : [ [ \"greylock\", \"omidyar-network\" ], [ \"greylock\", \"omidyar-network\", \"floodgate\", \"sv-angel\" ], [ \"highland-capital-partners\", \"greylock\", \"omidyar-network\", \"svb-financial-group\" ] ] } { \"name\" : \"Facebook\", \"ipo\" : 2012, \"valuation\" : NumberLong(\"104000000000\"),\n\n172\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "\"funders\" : [ [ \"accel-partners\" ], [ \"greylock\", \"meritech-capital-partners\", \"founders-fund\", \"sv-angel\" ], ... [ \"goldman-sachs\", \"digital-sky-technologies-fo\" ] ] } { \"name\" : \"Revision3\", \"funders\" : [ [ \"greylock\", \"sv-angel\" ], [ \"greylock\" ] ] } ...\n\nIn the output, each document has a \"name\" field and a \"funders\" field. For those companies that have gone through an IPO, the \"ipo\" field contains the year the com‐ pany went public and the \"valuation\" field contains the value of the company at the time of the IPO. Note that in all of these documents, these are top-level fields and the values for those fields were promoted from nested documents and arrays.\n\nThe $ character used to specify the values for ipo, valuation, and funders in our project stage indicates that the values should be interpreted as field paths and used to select the value that should be projected for each field, respectively.\n\nOne thing you might have noticed is that we’re seeing multiple values printed out for funders. In fact, we’re seeing an array of arrays. Based on our review of the Facebook example document, we know that all of the funders are listed within an array called \"investments\". Our stage specifies that we want to project the financial_org.perma link value for each entry in the \"investments\" array, for every funding round. So, an array of arrays of funders’ names is built up.\n\nIn later sections we will look at how to perform arithmetic and other operations on strings, dates, and a number of other value types to project documents of all shapes\n\n$project\n\n|\n\n173",
      "content_length": 1574,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "and sizes. Just about the only thing we can’t do from a project stage is change the data type for a value.\n\n$unwind When working with array fields in an aggregation pipeline, it is often necessary to include one or more unwind stages. This allows us to produce output such that there is one output document for each element in a specified array field.\n\nFigure 7-4. $unwind takes an array from the input document and creates an output document for each element in that array\n\nIn the example in Figure 7-4, we have an input document that has three keys and their corresponding values. The third key has as its value an array with three ele‐ ments. $unwind if run on this type of input document and configured to unwind the key3 field will produce documents that look like those shown at the bottom of Figure 7-4. The thing that might not be intuitive to you about this is that in each of these output documents there will be a key3 field, but that field will contain a single value rather than an array value, and there will be a separate document for each one of the elements that were in this array. In other words, if there were 10 elements in the array, the unwind stage would produce 10 output documents.\n\nLet’s go back to our companies example, and take a look at the use of an unwind stage. We’ll start with the following aggregation pipeline. Note that in this pipeline, as in the previous section, we are simply matching on a specific funder and promoting values from embedded funding_rounds documents using a project stage:\n\ndb.companies.aggregate([ {$match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, {$project: { _id: 0, name: 1, amount: \"$funding_rounds.raised_amount\", year: \"$funding_rounds.funded_year\" }} ])\n\n174\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Once again, here’s an example of the data model for documents in this collection:\n\n{ \"_id\" : \"52cdef7c4bab8bd675297d8e\", \"name\" : \"Facebook\", \"category_code\" : \"social\", \"founded_year\" : 2004, \"description\" : \"Social network\", \"funding_rounds\" : [{ \"id\" : 4, \"round_code\" : \"b\", \"raised_amount\" : 27500000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2006, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Meritech Capital Partners\", \"permalink\" : \"meritech-capital-partners\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Founders Fund\", \"permalink\" : \"founders-fund\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" }, \"person\" : null } ] }, { \"id\" : 2197, \"round_code\" : \"c\", \"raised_amount\" : 15000000,\n\n$unwind\n\n|\n\n175",
      "content_length": 947,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "\"raised_currency_code\" : \"USD\", \"funded_year\" : 2008, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"European Founders Fund\", \"permalink\" : \"european-founders-fund\" }, \"person\" : null } ] }], \"ipo\" : { \"valuation_amount\" : NumberLong(\"104000000000\"), \"valuation_currency_code\" : \"USD\", \"pub_year\" : 2012, \"pub_month\" : 5, \"pub_day\" : 18, \"stock_symbol\" : \"NASDAQ:FB\" } }\n\nOur aggregation query will produce results such as the following:\n\n{ \"name\" : \"Digg\", \"amount\" : [ 8500000, 2800000, 28700000, 5000000 ], \"year\" : [ 2006, 2005, 2008, 2011 ] } { \"name\" : \"Facebook\", \"amount\" : [ 500000, 12700000, 27500000, ...\n\nThe query produces documents that have arrays for both \"amount\" and \"year\", because we’re accessing the \"raised_amount\" and \"funded_year\" for every element in the \"funding_rounds\" array.\n\n176\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 886,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "To fix this, we can include an unwind stage before our project stage in this aggrega‐ tion pipeline, and parameterize this by specifying that it is the \"funding_rounds\" array that should be unwound (Figure 7-5).\n\nFigure 7-5. The outline of our aggregation pipeline so far, matching for “greylock” then unwinding the “funding_rounds”, and finally projecting out the name, amount, and year for each of the funding rounds\n\nReturning again to our Facebook example, we can see that for each funding round there is a \"raised_amount\" field and a \"funded_year\" field.\n\nThe unwind stage will produce an output document for each element of the \"fund ing_rounds\" array. In this example our values are strings, but regardless of the type of value, the unwind stage will produce an output document for each one. Here’s the updated aggregation query:\n\ndb.companies.aggregate([ { $match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, { $unwind: \"$funding_rounds\" }, { $project: { _id: 0, name: 1, amount: \"$funding_rounds.raised_amount\", year: \"$funding_rounds.funded_year\" } } ])\n\nThe unwind stage produces an exact copy of every one of the documents that it receives as input. All the fields will have the same key and value, with the exception of the \"funding_rounds\" field. Rather than being an array of \"funding_rounds\" docu‐ ments, instead it will have a value that is a single document, which corresponds to an individual funding round:\n\n{\"name\": \"Digg\", \"amount\": 8500000, \"year\": 2006 } {\"name\": \"Digg\", \"amount\": 2800000, \"year\": 2005 } {\"name\": \"Digg\", \"amount\": 28700000, \"year\": 2008 } {\"name\": \"Digg\", \"amount\": 5000000, \"year\": 2011 } {\"name\": \"Facebook\", \"amount\": 500000, \"year\": 2004 } {\"name\": \"Facebook\", \"amount\": 12700000, \"year\": 2005 } {\"name\": \"Facebook\", \"amount\": 27500000, \"year\": 2006 } {\"name\": \"Facebook\", \"amount\": 240000000, \"year\": 2007 } {\"name\": \"Facebook\", \"amount\": 60000000, \"year\": 2007 }\n\n$unwind\n\n|\n\n177",
      "content_length": 1954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "{\"name\": \"Facebook\", \"amount\": 15000000, \"year\": 2008 } {\"name\": \"Facebook\", \"amount\": 100000000, \"year\": 2008 } {\"name\": \"Facebook\", \"amount\": 60000000, \"year\": 2008 } {\"name\": \"Facebook\", \"amount\": 200000000, \"year\": 2009 } {\"name\": \"Facebook\", \"amount\": 210000000, \"year\": 2010 } {\"name\": \"Facebook\", \"amount\": 1500000000, \"year\": 2011 } {\"name\": \"Revision3\", \"amount\": 1000000, \"year\": 2006 } {\"name\": \"Revision3\", \"amount\": 8000000, \"year\": 2007 } ...\n\nNow let’s add an additional field to our output documents. In doing so, we’ll actually identify a small problem with this aggregation pipeline as currently written:\n\ndb.companies.aggregate([ { $match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, { $unwind: \"$funding_rounds\" }, { $project: { _id: 0, name: 1, funder: \"$funding_rounds.investments.financial_org.permalink\", amount: \"$funding_rounds.raised_amount\", year: \"$funding_rounds.funded_year\" } } ])\n\nIn adding the \"funder\" field we now have a field path value that will access the \"investments\" field of the \"funding_rounds\" embedded document that it gets from the unwind stage and, for the financial organization, selects the permalink value. Note that this is very similar to what we’re doing in our match filter. Let’s have a look at our output:\n\n{ \"name\" : \"Digg\", \"funder\" : [ \"greylock\", \"omidyar-network\" ], \"amount\" : 8500000, \"year\" : 2006 } { \"name\" : \"Digg\", \"funder\" : [ \"greylock\", \"omidyar-network\", \"floodgate\", \"sv-angel\" ], \"amount\" : 2800000, \"year\" : 2005 } {\n\n178\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "\"name\" : \"Digg\", \"funder\" : [ \"highland-capital-partners\", \"greylock\", \"omidyar-network\", \"svb-financial-group\" ], \"amount\" : 28700000, \"year\" : 2008 } ... { \"name\" : \"Farecast\", \"funder\" : [ \"madrona-venture-group\", \"wrf-capital\" ], \"amount\" : 1500000, \"year\" : 2004 } { \"name\" : \"Farecast\", \"funder\" : [ \"greylock\", \"madrona-venture-group\", \"wrf-capital\" ], \"amount\" : 7000000, \"year\" : 2005 } { \"name\" : \"Farecast\", \"funder\" : [ \"greylock\", \"madrona-venture-group\", \"par-capital-management\", \"pinnacle-ventures\", \"sutter-hill-ventures\", \"wrf-capital\" ], \"amount\" : 12100000, \"year\" : 2007 }\n\nTo understand what we’re seeing here, we need to go back to our document and look at the \"investments\" field.\n\nThe \"funding_rounds.investments\" field is itself an array. Multiple funders can par‐ ticipate in each funding round, so \"investments\" will list every one of those funders. Looking at the results, as we originally saw with the \"raised_amount\" and \"fun\n\n$unwind\n\n|\n\n179",
      "content_length": 973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "ded_year\" fields, we’re now seeing an array for \"funder\" because \"investments\" is an array-valued field.\n\nAnother problem is that because of the way we’ve written our pipeline, many docu‐ ments are passed to the project stage that represent funding rounds that Greylock did not participate in. We can see this by looking at the funding rounds for Farecast. This problem stems from the fact that our match stage selects all companies where Grey‐ lock participated in at least one funding round. If we are interested in considering only those funding rounds in which Greylock actually participated, we need to figure out a way to filter differently.\n\nOne possibility is to reverse the order of our unwind and match stages—that is to say, do the unwind first and then do the match. This guarantees that we will only match documents coming out of the unwind stage. But in thinking through this approach, it quickly becomes clear that, with unwind as the first stage, we would be doing a scan through the entire collection.\n\nFor efficiency, we want to match as early as possible in our pipeline. This enables the aggregation framework to make use of indexes, for example. So, in order to select only those funding rounds in which Greylock participated, we can include a second match stage:\n\ndb.companies.aggregate([ { $match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, { $unwind: \"$funding_rounds\" }, { $match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, { $project: { _id: 0, name: 1, individualFunder: \"$funding_rounds.investments.person.permalink\", fundingOrganization: \"$funding_rounds.investments.financial_org.permalink\", amount: \"$funding_rounds.raised_amount\", year: \"$funding_rounds.funded_year\" } } ])\n\nThis pipeline will first filter for companies where Greylock participated in at least one funding round. It will then unwind the funding rounds and filter again, so that only documents that represent funding rounds that Greylock actually participated in will be passed on to the project stage.\n\nAs mentioned at the beginning of this chapter, it is often the case that we need to include multiple stages of the same type. This is a good example: we’re filtering to reduce the number of documents that we’re looking at initially by narrowing down our set of documents for consideration to those for which Greylock participated in at least one funding round. Then, through our unwind stage, we end up with a number of documents that represent funding rounds from companies that Greylock did, in fact, fund, but individual funding rounds that Greylock did not participate in. We can\n\n180\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 2701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "get rid of all the funding rounds we’re not interested in by simply including another filter, using a second match stage.\n\nArray Expressions Now let’s turn our attention to array expressions. As part of our deep dive, we’ll take a look at using array expressions in project stages.\n\nThe first expression we’ll examine is a filter expression. A filter expression selects a subset of the elements in an array based on filter criteria.\n\nWorking again with our companies dataset, we’ll match using the same criteria for funding rounds in which Greylock participated. Take a look at the rounds field in this pipeline:\n\ndb.companies.aggregate([ { $match: {\"funding_rounds.investments.financial_org.permalink\": \"greylock\"} }, { $project: { _id: 0, name: 1, founded_year: 1, rounds: { $filter: { input: \"$funding_rounds\", as: \"round\", cond: { $gte: [\"$$round.raised_amount\", 100000000] } } } } }, { $match: {\"rounds.investments.financial_org.permalink\": \"greylock\" } }, ]).pretty()\n\nThe rounds field uses a filter expression. The $filter operator is designed to work with array fields and specifies the options we must supply. The first option to $filter is input. For input, we simply specify an array. In this case, we use a field path speci‐ fier to identify the \"funding_rounds\" array found in documents in our companies collection. Next, we specify the name we’d like to use for this \"funding_rounds\" array throughout the rest of our filter expression. Then, as the third option, we need to specify a condition. The condition should provide criteria used to filter whatever array we’ve provided as input, selecting a subset. In this case, we’re filtering such that we only select elements where the \"raised_amount\" for a \"funding_round\" is greater than or equal to 100 million.\n\nIn specifying the condition, we’ve made use of $$. We use $$ to reference a variable defined within the expression we’re working in. The as clause defines a variable within our filter expression. This variable has the name \"round\" because that’s what we labeled it in the as clause. This is to disambiguate a reference to a variable from a field path. In this case, our comparison expression takes an array of two values and will return true if the first value provided is greater than or equal to the second value.\n\nArray Expressions\n\n|\n\n181",
      "content_length": 2318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Now let’s consider what documents the project stage of this pipeline will produce, given this filter. The output documents will have \"name\", \"founded_year\", and \"rounds\" fields. The values for \"rounds\" will be arrays composed of the elements that match our filter condition: that the raised amount is greater than $100,000,000.\n\nIn the match stage that follows, as we did previously, we will simply filter the input documents for those that were funded in some way by Greylock. Documents output by this pipeline will resemble the following:\n\n{ \"name\" : \"Dropbox\", \"founded_year\" : 2007, \"rounds\" : [ { \"id\" : 25090, \"round_code\" : \"b\", \"source_description\" : \"Dropbox Raises $250M In Funding, Boasts 45 Million Users\", \"raised_amount\" : 250000000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2011, \"investments\" : [ { \"financial_org\" : { \"name\" : \"Index Ventures\", \"permalink\" : \"index-ventures\" } }, { \"financial_org\" : { \"name\" : \"RIT Capital Partners\", \"permalink\" : \"rit-capital-partners\" } }, { \"financial_org\" : { \"name\" : \"Valiant Capital Partners\", \"permalink\" : \"valiant-capital-partners\" } }, { \"financial_org\" : { \"name\" : \"Benchmark\", \"permalink\" : \"benchmark-2\" } }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Goldman Sachs\", \"permalink\" : \"goldman-sachs\"\n\n182\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "}, \"person\" : null }, { \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" } }, { \"financial_org\" : { \"name\" : \"Institutional Venture Partners\", \"permalink\" : \"institutional-venture-partners\" } }, { \"financial_org\" : { \"name\" : \"Sequoia Capital\", \"permalink\" : \"sequoia-capital\" } }, { \"financial_org\" : { \"name\" : \"Accel Partners\", \"permalink\" : \"accel-partners\" } }, { \"financial_org\" : { \"name\" : \"Glynn Capital Management\", \"permalink\" : \"glynn-capital-management\" } }, { \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" } } ] } ] }\n\nOnly the \"rounds\" array items for which the raised amount exceeds $100,000,000 will pass through the filter. In the case of Dropbox, there is just one round that meets that criterion. You have a lot of flexibility in how you set up filter expressions, but this is the basic form and provides a concrete example of a use case for this particular array expression.\n\nArray Expressions\n\n|\n\n183",
      "content_length": 968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Next, let’s look at the array element operator. We’ll continue working with funding rounds, but in this case we simply want to pull out the first round and the last round. We might be interested, for example, in seeing when these rounds occurred or in comparing their amounts. These are things we can do with date and arithmetic expressions, as we’ll see in the next section.\n\nThe $arrayElemAt operator enables us to select an element at a particular slot within an array. The following pipeline provides an example of using $arrayElemAt:\n\ndb.companies.aggregate([ { $match: { \"founded_year\": 2010 } }, { $project: { _id: 0, name: 1, founded_year: 1, first_round: { $arrayElemAt: [ \"$funding_rounds\", 0 ] }, last_round: { $arrayElemAt: [ \"$funding_rounds\", -1 ] } } } ]).pretty()\n\nNote the syntax for using $arrayElemAt within a project stage. We define a field that we want projected out and as the value specify a document with $arrayElemAt as the field name and a two-element array as the value. The first element should be a field path that specifies the array field we want to select from. The second element identi‐ fies the slot within that array that we want. Remember that arrays are 0-indexed.\n\nIn many cases, the length of an array is not readily available. To select array slots starting from the end of the array, use negative integers. The last element in an array is identified with -1.\n\nA simple output document for this aggregation pipeline would resemble the following:\n\n{ \"name\" : \"vufind\", \"founded_year\" : 2010, \"first_round\" : { \"id\" : 19876, \"round_code\" : \"angel\", \"source_url\" : \"\", \"source_description\" : \"\", \"raised_amount\" : 250000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2010, \"funded_month\" : 9, \"funded_day\" : 1, \"investments\" : [ ] }, \"last_round\" : { \"id\" : 57219, \"round_code\" : \"seed\",\n\n184\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "\"source_url\" : \"\", \"source_description\" : \"\", \"raised_amount\" : 500000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2012, \"funded_month\" : 7, \"funded_day\" : 1, \"investments\" : [ ] } }\n\nRelated to $arrayElemAt is the $slice expression. This allows us to return not just one but multiple items from an array in sequence, beginning with a particular index:\n\ndb.companies.aggregate([ { $match: { \"founded_year\": 2010 } }, { $project: { _id: 0, name: 1, founded_year: 1, early_rounds: { $slice: [ \"$funding_rounds\", 1, 3 ] } } } ]).pretty()\n\nHere, again with the funding_rounds array, we begin at index 1 and take three ele‐ ments from the array. Perhaps we know that in this dataset the first funding round isn’t all that interesting, or we simply want some early ones but not the very first one.\n\nFiltering and selecting individual elements or slices of arrays are among the more common operations we need to perform on arrays. Probably the most common, however, is determining an array’s size or length. To do this we can use the $size operator:\n\ndb.companies.aggregate([ { $match: { \"founded_year\": 2004 } }, { $project: { _id: 0, name: 1, founded_year: 1, total_rounds: { $size: \"$funding_rounds\" } } } ]).pretty()\n\nWhen used in a project stage, a $size expression will simply provide a value that is the number of elements in the array.\n\nIn this section, we’ve explored some of the most common array expressions. There are many more, and the list grows with each release. Please review the Aggregation Pipeline Quick Reference in the MongoDB documentation for a summary of all expressions that are available.\n\nArray Expressions\n\n|\n\n185",
      "content_length": 1640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Accumulators At this point, we’ve covered a few different types of expressions. Next, let’s look at what accumulators the aggregation framework has to offer. Accumulators are essen‐ tially another type of expression, but we think about them in their own class because they calculate values from field values found in multiple documents.\n\nAccumulators the aggregation framework provides enable us to perform operations such as summing all values in a particular field ($sum), calculating an average ($avg), etc. We also consider $first and $last to be accumulators because these consider values in all documents that pass through the stage in which they are used. $max and $min are two more examples of accumulators that consider a stream of documents and save just one of the values they see. We can use $mergeObjects to combine mul‐ tiple documents into a single document.\n\nWe also have accumulators for arrays. We can $push values onto an array as docu‐ ments pass through a pipeline stage. $addToSet is very similar to $push except that it ensures no duplicate values are included in the resulting array.\n\nThen there are some expressions for calculating descriptive statistics—for example, for calculating the standard deviation of a sample and of a population. Both work with a stream of documents that pass through a pipeline stage.\n\nPrior to MongoDB 3.2, accumulators were available only in the group stage. Mon‐ goDB 3.2 introduced the ability to access a subset of accumulators within the project stage. The primary difference between the accumulators in the group stage and the project stage is that in the project stage accumulators such as $sum and $avg must operate on arrays within a single document, whereas accumulators in the group stage, as we’ll see in a later section, provide you with the ability to perform calculations on values across multiple documents.\n\nThat’s a quick overview of accumulators to provide some context and set the stage for our deep dive into examples.\n\nUsing Accumulators in Project Stages We’ll begin with an example of using an accumulator in a project stage. Note that our match stage filters for documents that contain a \"funding_rounds\" field and for which the funding_rounds array is not empty:\n\ndb.companies.aggregate([ { $match: { \"funding_rounds\": { $exists: true, $ne: [ ]} } }, { $project: { _id: 0, name: 1, largest_round: { $max: \"$funding_rounds.raised_amount\" }\n\n186\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 2480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "} } ])\n\nBecause the value for $funding_rounds is an array within each company document, we can use an accumulator. Remember that in project stages accumulators must work on an array-valued field. In this case, we’re able to do something pretty cool here. We are easily identifying the largest value in an array by reaching into an embedded document within that array and projecting the max value in the output documents:\n\n{ \"name\" : \"Wetpaint\", \"largest_round\" : 25000000 } { \"name\" : \"Digg\", \"largest_round\" : 28700000 } { \"name\" : \"Facebook\", \"largest_round\" : 1500000000 } { \"name\" : \"Omnidrive\", \"largest_round\" : 800000 } { \"name\" : \"Geni\", \"largest_round\" : 10000000 } { \"name\" : \"Twitter\", \"largest_round\" : 400000000 } { \"name\" : \"StumbleUpon\", \"largest_round\" : 17000000 } { \"name\" : \"Gizmoz\", \"largest_round\" : 6500000 } { \"name\" : \"Scribd\", \"largest_round\" : 13000000 } { \"name\" : \"Slacker\", \"largest_round\" : 40000000 } { \"name\" : \"Lala\", \"largest_round\" : 20000000 } { \"name\" : \"eBay\", \"largest_round\" : 6700000 } { \"name\" : \"MeetMoi\", \"largest_round\" : 2575000 } { \"name\" : \"Joost\", \"largest_round\" : 45000000 } { \"name\" : \"Babelgum\", \"largest_round\" : 13200000 } { \"name\" : \"Plaxo\", \"largest_round\" : 9000000 } { \"name\" : \"Cisco\", \"largest_round\" : 2500000 } { \"name\" : \"Yahoo!\", \"largest_round\" : 4800000 } { \"name\" : \"Powerset\", \"largest_round\" : 12500000 } { \"name\" : \"Technorati\", \"largest_round\" : 10520000 } ...\n\nAs another example, let’s use the $sum accumulator to calculate the total funding for each company in our collection:\n\ndb.companies.aggregate([ { $match: { \"funding_rounds\": { $exists: true, $ne: [ ]} } }, { $project: { _id: 0, name: 1, total_funding: { $sum: \"$funding_rounds.raised_amount\" } } } ])\n\nThis is just a taste of what you can do using accumulators in project stages. Again, you’re encouraged to review the Aggregation Pipeline Quick Reference in the Mon‐ goDB docs for a complete overview of the accumulator expressions available.\n\nIntroduction to Grouping Historically, accumulators were the province of the group stage in the MongoDB aggregation framework. The group stage performs a function that is similar to the\n\nIntroduction to Grouping\n\n|\n\n187",
      "content_length": 2198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "SQL GROUP BY command. In a group stage, we can aggregate together values from multiple documents and perform some type of aggregation operation on them, such as calculating an average. Let’s take a look at an example:\n\ndb.companies.aggregate([ { $group: { _id: { founded_year: \"$founded_year\" }, average_number_of_employees: { $avg: \"$number_of_employees\" } } }, { $sort: { average_number_of_employees: -1 } }\n\n])\n\nHere, we’re using a group stage to aggregate together all companies based on the year they were founded, then calculate the average number of employees for each year. The output for this pipeline resembles the following:\n\n{ \"_id\" : { \"founded_year\" : 1847 }, \"average_number_of_employees\" : 405000 } { \"_id\" : { \"founded_year\" : 1896 }, \"average_number_of_employees\" : 388000 } { \"_id\" : { \"founded_year\" : 1933 }, \"average_number_of_employees\" : 320000 } { \"_id\" : { \"founded_year\" : 1915 }, \"average_number_of_employees\" : 186000 } { \"_id\" : { \"founded_year\" : 1903 }, \"average_number_of_employees\" : 171000 } { \"_id\" : { \"founded_year\" : 1865 }, \"average_number_of_employees\" : 125000 } { \"_id\" : { \"founded_year\" : 1921 }, \"average_number_of_employees\" : 107000 } { \"_id\" : { \"founded_year\" : 1835 }, \"average_number_of_employees\" : 100000 } { \"_id\" : { \"founded_year\" : 1952 }, \"average_number_of_employees\" : 92900 } { \"_id\" : { \"founded_year\" : 1946 }, \"average_number_of_employees\" : 91500 } { \"_id\" : { \"founded_year\" : 1947 }, \"average_number_of_employees\" : 88510.5 } { \"_id\" : { \"founded_year\" : 1898 }, \"average_number_of_employees\" : 80000 } { \"_id\" : { \"founded_year\" : 1968 }, \"average_number_of_employees\" : 73550 } { \"_id\" : { \"founded_year\" : 1957 }, \"average_number_of_employees\" : 70055 } { \"_id\" : { \"founded_year\" : 1969 }, \"average_number_of_employees\" : 67635.1 } { \"_id\" : { \"founded_year\" : 1928 }, \"average_number_of_employees\" : 51000 } { \"_id\" : { \"founded_year\" : 1963 }, \"average_number_of_employees\" : 50503 } { \"_id\" : { \"founded_year\" : 1959 }, \"average_number_of_employees\" : 47432.5 } { \"_id\" : { \"founded_year\" : 1902 }, \"average_number_of_employees\" : 41171.5 } { \"_id\" : { \"founded_year\" : 1887 }, \"average_number_of_employees\" : 35000 } ...\n\nThe output includes documents that have a document as their \"_id\" value, and then a report on the average number of employees. This is the type of analysis we might do as a first step in assessing the correlation between the year in which a company was founded and its growth, possibly normalizing for how old the company is.\n\nAs you can see, the pipeline we built has two stages: a group stage and a sort stage. Fundamental to the group stage is the \"_id\" field that we specify as part of the docu‐ ment. This is the value of the $group operator itself, using a very strict interpretation.\n\nWe use this field to define what the group stage uses to organize the documents that it sees. Since the group stage is first, the aggregate command will pass all documents in the companies collection through this stage. The group stage will take every docu‐\n\n188\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 3109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "ment that has the same value for \"founded_year\" and treat them as a single group. In constructing the value for this field, this stage will use the $avg accumulator to calcu‐ late an average number of employees for all companies with the same \"founded_year\".\n\nYou can think of it this way. Each time the group stage encounters a document with a specific founding year, it adds the value for \"number_of_employees\" from that docu‐ ment to a running sum of the number of employees and adds one to a count of the number of documents seen so far for that year. Once all documents have passed through the group stage, it can then calculate the average using that running sum and count for every grouping of documents it identified based on the year of founding.\n\nAt the end of this pipeline, we sort the documents into descending order by average_number_of_employees.\n\nLet’s look at another example. One field we’ve not yet considered in the companies dataset is the relationships. The relationships field appears in documents in the fol‐ lowing form:\n\n{ \"_id\" : \"52cdef7c4bab8bd675297d8e\", \"name\" : \"Facebook\", \"permalink\" : \"facebook\", \"category_code\" : \"social\", \"founded_year\" : 2004, ... \"relationships\" : [ { \"is_past\" : false, \"title\" : \"Founder and CEO, Board Of Directors\", \"person\" : { \"first_name\" : \"Mark\", \"last_name\" : \"Zuckerberg\", \"permalink\" : \"mark-zuckerberg\" } }, { \"is_past\" : true, \"title\" : \"CFO\", \"person\" : { \"first_name\" : \"David\", \"last_name\" : \"Ebersman\", \"permalink\" : \"david-ebersman\" } }, ... ], \"funding_rounds\" : [ ... {\n\nIntroduction to Grouping\n\n|\n\n189",
      "content_length": 1581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "190\n\n\"id\" : 4, \"round_code\" : \"b\", \"source_url\" : \"http://www.facebook.com/press/info.php?factsheet\", \"source_description\" : \"Facebook Funding\", \"raised_amount\" : 27500000, \"raised_currency_code\" : \"USD\", \"funded_year\" : 2006, \"funded_month\" : 4, \"funded_day\" : 1, \"investments\" : [ { \"company\" : null, \"financial_org\" : { \"name\" : \"Greylock Partners\", \"permalink\" : \"greylock\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Meritech Capital Partners\", \"permalink\" : \"meritech-capital-partners\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"Founders Fund\", \"permalink\" : \"founders-fund\" }, \"person\" : null }, { \"company\" : null, \"financial_org\" : { \"name\" : \"SV Angel\", \"permalink\" : \"sv-angel\" }, \"person\" : null } ] }, ... \"ipo\" : { \"valuation_amount\" : NumberLong(\"104000000000\"), \"valuation_currency_code\" : \"USD\", \"pub_year\" : 2012, \"pub_month\" : 5, \"pub_day\" : 18, \"stock_symbol\" : \"NASDAQ:FB\"\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "}, ... }\n\nThe \"relationships\" field gives us the ability to dive in and look for people who have, in one way or another, been associated with a relatively large number of compa‐ nies. Let’s take a look at this aggregation:\n\ndb.companies.aggregate( [ { $match: { \"relationships.person\": { $ne: null } } }, { $project: { relationships: 1, _id: 0 } }, { $unwind: \"$relationships\" }, { $group: { _id: \"$relationships.person\", count: { $sum: 1 } } }, { $sort: { count: -1 } } ]).pretty()\n\nWe’re matching on relationships.person. If we look at our Facebook example document, we can see how relationships are structured and get a sense for what it means to do this. We are filtering for all relationships for which \"person\" is not null. Then we project out all relationships for documents that match. We will pass only relationships to the next stage in the pipeline, which is unwind. We unwind the rela‐ tionships so that every relationship in the array comes through to the group stage that follows. In the group stage, we use a field path to identify the person within each \"relationship\" document. All documents with the same \"person\" value will be grouped together. As we saw previously, it’s perfectly fine for a document to be the value around which we group. So, every match to a document for a first name, last name, and permalink for a person will be aggregated together. We use the $sum accu‐ mulator to count the number of relationships in which each person has participated. Finally, we sort into descending order. The output for this pipeline resembles the following:\n\n{ \"_id\" : { \"first_name\" : \"Tim\", \"last_name\" : \"Hanlon\", \"permalink\" : \"tim-hanlon\" }, \"count\" : 28 } { \"_id\" : { \"first_name\" : \"Pejman\", \"last_name\" : \"Nozad\", \"permalink\" : \"pejman-nozad\" }, \"count\" : 24 }\n\nIntroduction to Grouping\n\n|\n\n191",
      "content_length": 1819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "{ \"_id\" : { \"first_name\" : \"David S.\", \"last_name\" : \"Rose\", \"permalink\" : \"david-s-rose\" }, \"count\" : 24 } { \"_id\" : { \"first_name\" : \"Saul\", \"last_name\" : \"Klein\", \"permalink\" : \"saul-klein\" }, \"count\" : 24 } ...\n\nTim Hanlon is the individual who has participated in the most relationships with companies in this collection. It could be that Mr. Hanlon has actually had a relation‐ ship with 28 companies, but we can’t know that for sure, because it’s also possible that he has had multiple relationships with one or more companies, each with a different title. This example illustrates a very important point about aggregation pipelines: make sure you fully understand what it is you’re working with as you do calculations, particularly when you’re calculating aggregate values using accumulator expressions of some kind.\n\nIn this case, we can say that Tim Hanlon appears 28 times in \"relationships\" docu‐ ments throughout the companies in our collection. We would have to dig a little deeper to see exactly how many unique companies he was associated with, but we’ll leave the construction of that pipeline to you as an exercise.\n\nThe _id Field in Group Stages Before we go any further with our discussion of the group stage, let’s talk a little more about the _id field and look at some best practices for constructing values for this field in group aggregation stages. We’ll walk through a few examples that illustrate several different ways in which we commonly group documents. As our first example, consider this pipeline:\n\ndb.companies.aggregate([ { $match: { founded_year: { $gte: 2013 } } }, { $group: { _id: { founded_year: \"$founded_year\"}, companies: { $push: \"$name\" } } }, { $sort: { \"_id.founded_year\": 1 } } ]).pretty()\n\n192\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1800,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "The output for this pipeline resembles the following:\n\n{ \"_id\" : { \"founded_year\" : 2013 }, \"companies\" : [ \"Fixya\", \"Wamba\", \"Advaliant\", \"Fluc\", \"iBazar\", \"Gimigo\", \"SEOGroup\", \"Clowdy\", \"WhosCall\", \"Pikk\", \"Tongxue\", \"Shopseen\", \"VistaGen Therapeutics\" ] } ...\n\nIn our output we have documents with two fields: \"_id\" and \"companies\". Each of these documents contains a list of the companies founded in whatever the \"founded_year\" is, \"companies\" being an array of company names.\n\nNotice here how we’ve constructed the \"_id\" field in the group stage. Why not just provide the founding year rather than putting it inside a document with a field labeled \"founded_year\". The reason we don’t do it that way is that if we don’t label the group value, it’s not explicit that we are grouping on the year in which the com‐ pany was founded. In order to avoid confusion, it is a best practice to explicitly label values on which we group.\n\nIn some circumstances it might be necessary to use another approach in which our _id value is a document composed of multiple fields. In this case, we’re actually grouping documents on the basis of their founding year and category code:\n\ndb.companies.aggregate([ { $match: { founded_year: { $gte: 2010 } } }, { $group: { _id: { founded_year: \"$founded_year\", category_code: \"$category_code\" }, companies: { $push: \"$name\" } } }, { $sort: { \"_id.founded_year\": 1 } } ]).pretty()\n\nIt is perfectly fine to use documents with multiple fields as our _id value in group stages. In other cases, it might also be necessary to do something like this:\n\nIntroduction to Grouping\n\n|\n\n193",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "db.companies.aggregate([ { $group: { _id: { ipo_year: \"$ipo.pub_year\" }, companies: { $push: \"$name\" } } }, { $sort: { \"_id.ipo_year\": 1 } } ]).pretty()\n\nIn this case, we’re grouping documents based on the year in which the companies had their IPO, and that year is actually a field of an embedded document. It is common practice to use field paths that reach into embedded documents as the value on which to group in a group stage. In this case, the output will resemble the following:\n\n{ \"_id\" : { \"ipo_year\" : 1999 }, \"companies\" : [ \"Akamai Technologies\", \"TiVo\", \"XO Group\", \"Nvidia\", \"Blackberry\", \"Blue Coat Systems\", \"Red Hat\", \"Brocade Communications Systems\", \"Juniper Networks\", \"F5 Networks\", \"Informatica\", \"Iron Mountain\", \"Perficient\", \"Sitestar\", \"Oxford Instruments\" ] }\n\nNote that the examples in this section use an accumulator we haven’t seen before: $push. As the group stage processes documents in its input stream, a $push expres‐ sion will add the resulting value to an array that it builds throughout its run. In the case of the preceding pipeline, the group stage is building an array composed of com‐ pany names.\n\nOur final example is one we’ve already seen, but it’s included here for the sake of completeness:\n\ndb.companies.aggregate( [ { $match: { \"relationships.person\": { $ne: null } } }, { $project: { relationships: 1, _id: 0 } }, { $unwind: \"$relationships\" }, { $group: { _id: \"$relationships.person\", count: { $sum: 1 }\n\n194\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "} }, { $sort: { count: -1 } } ] )\n\nIn the preceding example where we were grouping on IPO year, we used a field path that resolved to a scalar value—the IPO year. In this case, our field path resolves to a document containing three fields: \"first_name“, \"last_name\", and \"permalink\". This demonstrates that the group stage supports grouping on document values.\n\nYou’ve now seen several ways in which we can construct _id values in group stages. In general, bear in mind that what we want to do here is make sure that in our output, the semantics of our _id value are clear.\n\nGroup Versus Project To round out our discussion of the group aggregation stage, we’ll take a look at a cou‐ ple of additional accumulators that are not available in the project stage. This is to encourage you to think a little more deeply about what we can do in a project stage with respect to accumulators, and what we can do in group. As an example, consider this aggregation query:\n\ndb.companies.aggregate([ { $match: { funding_rounds: { $ne: [ ] } } }, { $unwind: \"$funding_rounds\" }, { $sort: { \"funding_rounds.funded_year\": 1, \"funding_rounds.funded_month\": 1, \"funding_rounds.funded_day\": 1 } }, { $group: { _id: { company: \"$name\" }, funding: { $push: { amount: \"$funding_rounds.raised_amount\", year: \"$funding_rounds.funded_year\" } } } }, ] ).pretty()\n\nHere, we begin by filtering for documents for which the array funding_rounds is not empty. Then we unwind funding_rounds. Therefore, the sort and group stages will see one document for each element of the funding_rounds array for every company.\n\nOur sort stage in this pipeline sorts on first year, then month, then day, all in ascend‐ ing order. This means that this stage will output the oldest funding rounds first. And as you are aware from Chapter 5, we can support this type of sort with a compound index.\n\nIn the group stage that follows the sort, we group by company name and use the $push accumulator to construct a sorted array of funding rounds. The fund\n\nIntroduction to Grouping\n\n|\n\n195",
      "content_length": 2038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "ing_rounds array will be sorted for each company because we sorted all funding rounds, globally, in the sort stage.\n\nDocuments output from this pipeline will resemble the following:\n\n{ \"_id\" : { \"company\" : \"Green Apple Media\" }, \"funding\" : [ { \"amount\" : 30000000, \"year\" : 2013 }, { \"amount\" : 100000000, \"year\" : 2013 }, { \"amount\" : 2000000, \"year\" : 2013 } ] }\n\nIn this pipeline, with $push, we are accumulating an array. In this case, we have speci‐ fied our $push expression so that it adds documents to the end of the accumulation array. Since the funding rounds are in chronological order, pushing onto the end of the array guarantees that the the funding amounts for each company are sorted in chronological order.\n\n$push expressions only work in group stages. This is because group stages are designed to take an input stream of documents and accumulate values by processing each document in turn. Project stages, on the other hand, work with each document in their input stream individually.\n\nLet’s take a look at one other example. This is a little longer, but it builds on the previ‐ ous one:\n\ndb.companies.aggregate([ { $match: { funding_rounds: { $exists: true, $ne: [ ] } } }, { $unwind: \"$funding_rounds\" }, { $sort: { \"funding_rounds.funded_year\": 1, \"funding_rounds.funded_month\": 1, \"funding_rounds.funded_day\": 1 } }, { $group: { _id: { company: \"$name\" }, first_round: { $first: \"$funding_rounds\" }, last_round: { $last: \"$funding_rounds\" }, num_rounds: { $sum: 1 }, total_raised: { $sum: \"$funding_rounds.raised_amount\" }\n\n196\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "} }, { $project: { _id: 0, company: \"$_id.company\", first_round: { amount: \"$first_round.raised_amount\", article: \"$first_round.source_url\", year: \"$first_round.funded_year\" }, last_round: { amount: \"$last_round.raised_amount\", article: \"$last_round.source_url\", year: \"$last_round.funded_year\" }, num_rounds: 1, total_raised: 1, } }, { $sort: { total_raised: -1 } } ] ).pretty()\n\nAgain, we are unwinding funding_rounds and sorting chronologically. However, in this case, instead of accumulating an array of entries, each entry representing a single funding_rounds, we are using two accumulators we’ve not yet seen in action: $first and $last. A $first expression simply saves the first value that passes through the input stream for the stage. A $last expression simply tracks the values that pass through the group stage and hangs onto the last one.\n\nAs with $push, we can’t use $first and $last in project stages because, again, project stages are not designed to accumulate values based on multiple documents streaming through them. Rather, they are designed to reshape documents individually.\n\nIn addition to $first and $last, we also use $sum in this example to calculate the total number of funding rounds. For this expression we can just specify the value, 1. A $sum expression like this simply serves to count the number of documents that it sees in each grouping.\n\nFinally, this pipeline includes a fairly complex project stage. However, all it is really doing is making the output prettier. Rather than show the first_round values, or entire documents for the first and last funding rounds, this project stage creates a summary. Note that this maintains good semantics, because each value is clearly labeled. For first_round we’ll produce a simple embedded document that contains just the essential details of amount, article, and year, pulling those values from the original funding round document that will be the value of $first_round. The project stage does something similar for $last_round. Finally, this project stage just passes through to output documents the num_rounds and total_raised values for docu‐ ments it receives in its input stream.\n\nDocuments output from this pipeline resemble the following:\n\nIntroduction to Grouping\n\n|\n\n197",
      "content_length": 2258,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "{ \"first_round\" : { \"amount\" : 7500000, \"article\" : \"http://www.teslamotors.com/display_data/pressguild.swf\", \"year\" : 2004 }, \"last_round\" : { \"amount\" : 10000000, \"article\" : \"http://www.bizjournals.com/sanfrancisco/news/2012/10/10/ tesla-motors-to-get-10-million-from.html\", \"year\" : 2012 }, \"num_rounds\" : 11, \"total_raised\" : 823000000, \"company\" : \"Tesla Motors\" }\n\nAnd with that, we’ve concluded an overview of the group stage.\n\nWriting Aggregation Pipeline Results to a Collection There are two specific stages, $out and $merge, that can write documents resulting from the aggregation pipeline to a collection. You can use only one of these two stages, and it must be the last stage of an aggregation pipeline. $merge was introduced in MongoDB version 4.2 and is the preferred stage for writing to a collection, if avail‐ able. $out has some limitations: it can only write to the same database, it overwrites any existing collection if present, and it cannot write to a sharded collection. $merge can write to any database and collection, sharded or not. $merge can also incorporate results (insert new documents, merge with existing documents, fail the operation, keep existing documents, or process all documents with a custom update) when working with an existing collection. But the real advantage of using $merge is that it can create on-demand materialized views, where the content of the output collection is incrementally updated when the pipeline is run.\n\nIn this chapter, we have covered a number of different accumulators, some that are available in the project stage, and we’ve also covered how to think about when to use group versus project when considering various accumulators. Next, we’ll take a look at transactions in MongoDB.\n\n198\n\n|\n\nChapter 7: Introduction to the Aggregation Framework",
      "content_length": 1815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "CHAPTER 8 Transactions\n\nTransactions are logical groups of processing in a database, and each group or trans‐ action can contain one or more operations such as reads and/or writes across multi‐ ple documents. MongoDB supports ACID-compliant transactions across multiple operations, collections, databases, documents, and shards. In this chapter, we intro‐ duce transactions, define what ACID means for a database, highlight how you use these in your applications, and provide tips for tuning transactions in MongoDB. We will cover:\n\nWhat a transaction is\n\nHow to use transactions\n\nTuning transaction limits for your application\n\nIntroduction to Transactions As we mentioned above, a transaction is a logical unit of processing in a database that includes one or more database operations, which can be read or write operations. There are situations where your application may require reads and writes to multiple documents (in one or more collections) as part of this logical unit of processing. An important aspect of a transaction is that it is never partially completed—it either suc‐ ceeds or fails.\n\nIn order to use transactions, your MongoDB deployment must be on MongoDB version 4.2 or later and your MongoDB drivers must be updated for MongoDB 4.2 or later. MongoDB provides a Driver Compatibility Reference page that you can use to ensure your MongoDB Driver version is compatible.\n\n199",
      "content_length": 1394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "A Definition of ACID ACID is the accepted set of properties a transaction must meet to be a “true” transac‐ tion. ACID is an acronym for Atomicity, Consistency, Isolation, and Durability. ACID transactions guarantee the validity of your data and of your database’s state even where power failures or other errors occur.\n\nAtomicity ensures that all operations inside a transaction will either be applied or nothing will be applied. A transaction can never be partially applied; either it is com‐ mitted or it aborts.\n\nConsistency ensures that if a transaction succeeds, the database will move from one consistent state to the next consistent state.\n\nIsolation is the property that permits multiple transactions to run at the same time in your database. It guarantees that a transaction will not view the partial results of any other transaction, which means multiple parallel transactions will have the same results as running each of the transactions sequentially.\n\nDurability ensures that when a transaction is committed all data will persist even in the case of a system failure.\n\nA database is said to be ACID-compliant when it ensures that all these properties are met and that only successful transactions can be processed. In situations where a fail‐ ure occurs before a transaction is completed, ACID compliance ensures that no data will be changed.\n\nMongoDB is a distributed database with ACID compliant transactions across replica sets and/or across shards. The network layer adds an additional level of complexity. The engineering team at MongoDB provided several chalk and talk videos that describe how they implemented the necessary features to support ACID transactions.\n\nHow to Use Transactions MongoDB provides two APIs to use transactions. The first is a similar syntax to rela‐ tional databases (e.g., start_transaction and commit_transaction) called the core API and the second is called the callback API, which is the recommended approach to using transactions.\n\nThe core API does not provide retry logic for the majority of errors and requires the developer to code the logic for the operations, the transaction commit function, and any retry and error logic required.\n\n200\n\n|\n\nChapter 8: Transactions",
      "content_length": 2221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "The callback API provides a single function that wraps a large degree of functionality when compared to the core API, including starting a transaction associated with a specified logical session, executing a function supplied as the callback function, and then committing the transaction (or aborting on error). This function also includes retry logic that handle commit errors. The callback API was added in MongoDB 4.2 to simplify application development with transactions as well as make it easier to add application retry logic to handle any transaction errors.\n\nIn both APIs, the developer is responsible for starting the logical session that will be used by the transaction. Both APIs require operations in a transaction to be associ‐ ated with a specific logical session (i.e., pass in the session to each operation). A logi‐ cal session in MongoDB tracks the time and sequencing of the operations in the context of the entire MongoDB deployment. A logical session or server session is part of the underlying framework used by client sessions to support retryable writes and causal consistency in MongoDB—both of these features were added in MongoDB version 3.6 as part of the foundation required to support transactions. A specific sequence of read and write operations that have a causal relationship reflected by their ordering is defined as a causally consistent client session in MongoDB. A client session is started by an application and used to interact with a server session.\n\nIn 2019, six senior engineers from MongoDB published a paper at the SIGMOD 2019 conference entitled “Implementation of Cluster-wide Logical Clock and Causal Con‐ sistency in MongoDB”.1 This paper provides a deeper technical explanation of the mechanics behind logical sessions and causal consistency in MongoDB. The paper documents the efforts from a multiteam, multiyear engineering project. The work involved changing aspects of the storage layer, adding a new replication consensus protocol, modifying the sharding architecture, refactoring sharding cluster metadata, and adding a global logical clock. These changes provide the foundation required by the database before ACID-compliant transactions can be added.\n\nThe complexity and additional coding required in applications are the main reasons to recommend the callback API over the core API. These differences between the APIs are summarized in Table 8-1.\n\n1 The authors are Misha Tyulenev, staff software engineer for sharding; Andy Schwerin, vice president for Dis‐ tributed Systems; Asya Kamsky, principal product manager for Distributed Systems; Randolph Tan, senior software engineer for sharding; Alyson Cabral, product manager for Distributed Systems; and Jack Mulrow, software engineer for sharding.\n\nHow to Use Transactions\n\n|\n\n201",
      "content_length": 2790,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Table 8-1. Comparison of Core API versus Callback API\n\nCore API Requires explicit call to start the transaction and commit the transaction. Does not incorporate error-handling logic for TransientTransactionError and UnknownTransactionCommitResult, and instead provides the flexibility to incorporate custom error handling for these errors. Requires explicit logical session to be passed to API for the specific transaction.\n\nCallback API Starts a transaction, executes the specified operations, and commits (or aborts on error). Automatically incorporates error-handling logic for TransientTransactionError and UnknownTransactionCommitResult.\n\nRequires explicit logical session to be passed to API for the specific transaction.\n\nTo understand the differences between these two APIs, we can compare the APIs using a simple transaction example for an ecommerce site where an order is placed and the corresponding items are removed from the available stock as they are sold. This involves two documents in different collections in a single transaction. The two operations, which will be the core of our transaction example, are:\n\norders.insert_one({\"sku\": \"abc123\", \"qty\": 100}, session=session) inventory.update_one({\"sku\": \"abc123\", \"qty\": {\"$gte\": 100}}, {\"$inc\": {\"qty\": -100}}, session=session)\n\nFirst, let’s see how the core API can be used in Python for our transaction example. The two operations of our transaction are highlighted in Step 1 of the program listing below:\n\n# Define the uriString using the DNS Seedlist Connection Format # for the connection uri = 'mongodb+srv://server.example.com/' client = MongoClient(uriString)\n\nmy_wc_majority = WriteConcern('majority', wtimeout=1000)\n\n# Prerequisite / Step 0: Create collections, if they don't already exist. # CRUD operations in transactions must be on existing collections.\n\nclient.get_database( \"webshop\", write_concern=my_wc_majority).orders.insert_one({\"sku\": \"abc123\", \"qty\":0}) client.get_database( \"webshop\", write_concern=my_wc_majority).inventory.insert_one( {\"sku\": \"abc123\", \"qty\": 1000})\n\n# Step 1: Define the operations and their sequence within the transaction def update_orders_and_inventory(my_session): orders = session.client.webshop.orders inventory = session.client.webshop.inventory\n\n202\n\n|\n\nChapter 8: Transactions",
      "content_length": 2298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "with session.start_transaction( read_concern=ReadConcern(\"snapshot\"), write_concern=WriteConcern(w=\"majority\"), read_preference=ReadPreference.PRIMARY):\n\norders.insert_one({\"sku\": \"abc123\", \"qty\": 100}, session=my_session) inventory.update_one({\"sku\": \"abc123\", \"qty\": {\"$gte\": 100}}, {\"$inc\": {\"qty\": -100}}, session=my_session) commit_with_retry(my_session)\n\n# Step 2: Attempt to run and commit transaction with retry logic def commit_with_retry(session): while True: try: # Commit uses write concern set at transaction start. session.commit_transaction() print(\"Transaction committed.\") break except (ConnectionFailure, OperationFailure) as exc: # Can retry commit if exc.has_error_label(\"UnknownTransactionCommitResult\"): print(\"UnknownTransactionCommitResult, retrying \" \"commit operation ...\") continue else: print(\"Error during commit ...\") raise\n\n# Step 3: Attempt with retry logic to run the transaction function txn_func def run_transaction_with_retry(txn_func, session): while True: try: txn_func(session) # performs transaction break except (ConnectionFailure, OperationFailure) as exc: # If transient error, retry the whole transaction if exc.has_error_label(\"TransientTransactionError\"): print(\"TransientTransactionError, retrying transaction ...\") continue else: raise\n\n# Step 4: Start a session. with client.start_session() as my_session:\n\n# Step 5: Call the function 'run_transaction_with_retry' passing it the function # to call 'update_orders_and_inventory' and the session 'my_session' to associate # with this transaction.\n\ntry: run_transaction_with_retry(update_orders_and_inventory, my_session) except Exception as exc:\n\nHow to Use Transactions\n\n|\n\n203",
      "content_length": 1675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "# Do something with error. The error handling code is not # implemented for you with the Core API. raise\n\nNow, let’s look at how the the callback API can be used in Python for this same trans‐ action example. The two operations of our transaction are highlighted in Step 1 of the program listing below:\n\n# Define the uriString using the DNS Seedlist Connection Format # for the connection uriString = 'mongodb+srv://server.example.com/' client = MongoClient(uriString)\n\nmy_wc_majority = WriteConcern('majority', wtimeout=1000)\n\n# Prerequisite / Step 0: Create collections, if they don't already exist. # CRUD operations in transactions must be on existing collections.\n\nclient.get_database( \"webshop\", write_concern=my_wc_majority).orders.insert_one({\"sku\": \"abc123\", \"qty\":0}) client.get_database( \"webshop\", write_concern=my_wc_majority).inventory.insert_one( {\"sku\": \"abc123\", \"qty\": 1000})\n\n# Step 1: Define the callback that specifies the sequence of operations to # perform inside the transactions.\n\ndef callback(my_session): orders = my_session.client.webshop.orders inventory = my_session.client.webshop.inventory\n\n# Important:: You must pass the session variable 'my_session' to # the operations.\n\norders.insert_one({\"sku\": \"abc123\", \"qty\": 100}, session=my_session) inventory.update_one({\"sku\": \"abc123\", \"qty\": {\"$gte\": 100}}, {\"$inc\": {\"qty\": -100}}, session=my_session)\n\n#. Step 2: Start a client session.\n\nwith client.start_session() as session:\n\n# Step 3: Use with_transaction to start a transaction, execute the callback, # and commit (or abort on error).\n\nsession.with_transaction(callback, read_concern=ReadConcern('local'), write_concern=my_write_concern_majority, read_preference=ReadPreference.PRIMARY) }\n\n204\n\n|\n\nChapter 8: Transactions",
      "content_length": 1758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "In MongoDB multidocument transactions, you may only perform read/write (CRUD) operations on existing collections or databases. As shown in our example, you must first create a collection outside of a transaction if you wish to insert it into a transaction. Create, drop, or index operations are not permitted in a transaction.\n\nTuning Transaction Limits for Your Application There are a few parameters that are important to be aware of when using transac‐ tions. They can be adjusted to ensure your application can make the optimal use of transactions.\n\nTiming and Oplog Size Limits There are two main categories of limits in MongoDB transactions. The first relates to timing limits of the transaction, controlling how long a specific transaction can run, the time a transaction will wait to acquire locks, and the maximum length that all transactions will run. The second category specifically relates to the MongoDB oplog entry and size limits for an individual entry.\n\nTime limits\n\nThe default maximum runtime of a transaction is one minute or less. This can be increased by modifying the limit controlled by transactionLifetimeLimitSec onds at a mongod instance level. In the case of sharded clusters, the parameter must be set on all shard replica set members. After this time has elapsed, a trans‐ action will be considered expired and will be aborted by a cleanup process, which runs periodically. The cleanup process will run once every 60 seconds or every transactionLifetimeLimitSeconds/2, whichever is lower.\n\nTo explicitly set a time limit on a transaction, it is recommended that you specify a maxTimeMS on commitTransaction. If maxTimeMS is not set then transaction LifetimeLimitSeconds will be used or if it is set but would exceed transaction LifetimeLimitSeconds then transactionLifetimeLimitSeconds will be used instead.\n\nThe default maximum time a transaction will wait to acquire the locks it needs for the operations in the transaction is 5 ms. This can be increased by modifying the limit controlled by maxTransactionLockRequestTimeoutMillis. If the transaction is unable to acquire the locks within this time, it will abort. maxTran sactionLockRequestTimeoutMillis can be set to 0, -1, or a number greater than 0. Setting it to 0 means a transaction will abort if it is unable to immediately acquire all the locks it requires. A setting of -1 will use the operation-specific timeout as specified by maxTimeMS. Any number greater than 0 configures the\n\nTuning Transaction Limits for Your Application\n\n|\n\n205",
      "content_length": 2529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "wait time to that time in seconds as the specified period that a transaction will attempt to acquire the required locks.\n\nOplog size limits\n\nMongoDB will create as many oplog entries as required for the write operations in a transaction. However, each oplog entry must be within the BSON document size limit of 16MB.\n\nTransactions provide a useful feature in MongoDB to ensure consistency, but they should be used with the rich document model. The flexibility of this model and using best practices such as schema design patterns will help avoid the use of transactions for most situations. Transactions are a powerful feature, best used sparingly in your applications.\n\n206\n\n|\n\nChapter 8: Transactions",
      "content_length": 702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "CHAPTER 9 Application Design\n\nThis chapter covers designing applications to work effectively with MongoDB. It discusses:\n\nSchema design considerations\n\nTrade-offs when deciding whether to embed data or to reference it\n\nTips for optimization\n\nConsistency considerations\n\nHow to migrate schemas\n\nHow to manage schemas\n\nWhen MongoDB isn’t a good choice of data store\n\nSchema Design Considerations A key aspect of data representation is the design of the schema, which is the way your data is represented in your documents. The best approach to this design is to repre‐ sent the data the way your application wants to see it. Thus, unlike in relational data‐ bases, you first need to understand your queries and data access patterns before modeling your schema.\n\nHere are the key aspects you need to consider when designing a schema:\n\nConstraints\n\nYou need to understand any database or hardware limitations. You also need to consider a number of MongoDB’s specific aspects, such as the maximum docu‐ ment size of 16 MB, that full documents get read and written from disk, that an\n\n207",
      "content_length": 1081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "update rewrites the whole document, and that atomic updates are at the docu‐ ment level.\n\nAccess patterns of your queries and of your writes\n\nYou will need to identify and quantify the workload of your application and of the wider system. The workload encompasses both the reads and the writes in your application. Once you know when queries are running and how frequently, you can identify the most common queries. These are the queries you need to design your schema to support. Once you have identified these queries, you should try to minimize the number of queries and ensure in your design that data that gets queried together is stored in the same document.\n\nData not used in these queries should be put into a different collection. Data that is infrequently used should also be moved to a different collection. It is worth considering if you can separate your dynamic (read/write) data and your static (mostly read) data. The best performance results occur when you prioritize your schema design for your most common queries.\n\nRelation types\n\nYou should consider which data is related in terms of your application’s needs, as well as the relationships between documents. You can then determine the best approaches to embed or reference the data or documents. You will need to work out how you can reference documents without having to perform additional queries, and how many documents are updated when there is a relationship change. You must also consider if the data structure is easy to query, such as with nested arrays (arrays in arrays), which support modeling certain relationships.\n\nCardinality\n\nOnce you have determined how your documents and your data are related, you should consider the cardinality of these relationships. Specifically, is it one-to- one, one-to-many, many-to-many, one-to-millions, or many-to-billions? It is very important to establish the cardinality of the relationships to ensure you use the best format to model them in your MongoDB schema. You should also con‐ sider whether the object on the many/millions side is accessed separately or only in the context of the parent object, as well as the ratio of updates to reads for the data field in question. The answers to these questions will help you to determine whether you should embed documents or reference documents and if you should be denormalizing data across documents.\n\nSchema Design Patterns Schema design is important in MongoDB, as it impacts directly on application perfor‐ mance. There are many common issues in schema design that can be addressed through the use of known patterns, or “building blocks.” It is best practice in schema design to use one or more of these patterns together.\n\n208\n\n|\n\nChapter 9: Application Design",
      "content_length": 2735,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Scheme design patterns that might apply include:\n\nPolymorphic pattern\n\nThis is suitable where all documents in a collection have a similar, but not identi‐ cal, structure. It involves identifying the common fields across the documents that support the common queries that will be run by the application. Tracking specific fields in the documents or subdocuments will help identify the differ‐ ences between the data and different code paths or classes/subclasses that can be coded in your application to manage these differences. This allows for the use of simple queries in a single collection of not-quite-identical documents to improve query performance.\n\nAttribute pattern\n\nThis is suitable when there are a subset of fields in a document that share com‐ mon features on which you want to sort or query, or when the fields you need to sort on only exist in a subset of the documents, or when both of these conditions are true. It involves reshaping the data into an array of key/value pairs and creat‐ ing an index on the elements in this array. Qualifiers can be added as additional fields to these key/value pairs. This pattern assists in targeting many similar fields per document so that fewer indexes are required and queries become simpler to write.\n\nBucket pattern\n\nThis is suitable for time series data where the data is captured as a stream over a period of time. It is much more efficient in MongoDB to “bucket” this data into a set of documents each holding the data for a particular time range than it is to create a document per point in time/data point. For example, you might use a one-hour bucket and place all readings for that hour in an array in a single docu‐ ment. The document itself will have start and end times indicating the period this “bucket” covers.\n\nOutlier pattern\n\nThis addresses the rare instances where a few queries of documents fall outside the normal pattern for the application. It is an advanced schema pattern designed for situations where popularity is a factor. This can be seen in social networks with major influencers, book sales, movie reviews, etc. It uses a flag to indicate the document is an outlier and stores the additional overflow into one or more documents that refer back to the first document via the \"_id\". The flag will be used by your application code to make the additional queries to retrieve the over‐ flow document(s).\n\nComputed pattern\n\nThis is used when data needs to be computed frequently, and it can also be used when the data access pattern is read-intensive. This pattern recommends that the\n\nSchema Design Considerations\n\n|\n\n209",
      "content_length": 2605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "calculations be done in the background, with the main document being updated periodically. This provides a valid approximation of the computed fields or docu‐ ments without having to continuously generate these for individual queries. This can significantly reduce the strain on the CPU by avoiding repetition of the same calculations, particularly in use cases where reads trigger the calculation and you have a high read-to-write ratio.\n\nSubset pattern\n\nThis is used when you have a working set that exceeds the available RAM of the machine. This can be caused by large documents that contain a lot of informa‐ tion that isn’t being used by your application. This pattern suggests that you split frequently used data and infrequently used data into two separate collections. A typical example might be an ecommerce application keeping the 10 most recent reviews of a product in the “main” (frequently accessed) collection and moving all the older reviews into a second collection queried only if the application needs more than the last 10 reviews.\n\nExtended Reference pattern\n\nThis is used for scenarios where you have many different logical entities or “things,” each with their own collection, but you may want to gather these entities together for a specific function. A typical ecommerce schema might have sepa‐ rate collections for orders, customers, and inventory. This can have a negative performance impact when we want to collect together all the information for a single order from these separate collections. The solution is to identify the fre‐ quently accessed fields and duplicate these within the order document. In the case of an ecommerce order, this would be the name and address of the customer we are shipping the item to. This pattern trades off the duplication of data for a reduction in the number of queries necessary to collate the information together.\n\nApproximation pattern\n\nThis is useful for situations where resource-expensive (time, memory, CPU cycles) calculations are needed but where exact precision is not absolutely required. An example of this is an image or post like/love counter or a page view counter, where knowing the exact count (e.g., whether it’s 999,535 or 1,000,0000) isn’t necessary. In these situations, applying this pattern can greatly reduce the number of writes—for example, by only updating the counter after every 100 or more views instead of after every view.\n\nTree pattern\n\nThis can be applied when you have a lot of queries and have data that is primarily hierarchical in structure. It follows the earlier concept of storing data together that is typically queried together. In MongoDB, you can easily store a hierarchy in an array within the same document. In the example of the ecommerce site, specifically its product catalog, there are often products that belong to multiple\n\n210\n\n|\n\nChapter 9: Application Design",
      "content_length": 2880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "categories or to categories that are part of other categories. An example might be “Hard Drive,” which is itself a category but comes under the “Storage” category, which itself is under the “Computer Parts” category, which is part of the “Elec‐ tronics” category. In this kind of scenario, we would have a field that would track the entire hierarchy and another field that would hold the immediate category (“Hard Drive”). The entire hierarchy field, kept in an array, provides the ability to use a multikey index on those values. This ensures all items related to categories in the hierarchy will be easily found. The immediate category field allows all items directly related to this category to be found.\n\nPreallocation pattern\n\nThis was primarily used with the MMAP storage engine, but there are still uses for this pattern. The pattern recommends creating an initial empty structure that will be populated later. An example use could be for a reservation system that manages a resource on a day-by-day basis, keeping track of whether it is free or already booked/unavailable. A two-dimensional structure of resources (x) and days (y) makes it trivially easy to check availability and perform calculations.\n\nDocument Versioning pattern\n\nThis provides a mechanism to enable retention of older revisions of documents. It requires an extra field to be added to each document to track the document version in the “main” collection, and an additional collection that contains all the revisions of the documents. This pattern makes a few assumptions: specifically, that each document has a limited number of revisions, that there are not large numbers of documents that need to be versioned, and that the queries are pri‐ marily done on the current version of each document. In situations where these assumptions are not valid, you may need to modify the pattern or consider a dif‐ ferent schema design pattern.\n\nMongoDB provides several useful resources online on patterns and schema design. MongoDB University offers a free course, M320 Data Modeling, as well as a “Building with Patterns” blog series.\n\nNormalization Versus Denormalization There are many ways to represent data, and one of the most important issues to con‐ sider is how much you should normalize your data. Normalization refers to dividing up data into multiple collections with references between collections. Each piece of data lives in one collection, although multiple documents may reference it. Thus, to change the data, only one document must be updated. The MongoDB Aggregation Framework offers joins with the $lookup stage, which performs a left outer join by adding documents to the “joined” collection where there is a matching document in the source collection—it adds a new array field to each matched document in the\n\nNormalization Versus Denormalization\n\n|\n\n211",
      "content_length": 2844,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "“joined” collection with the details of the document from the source collection. These reshaped documents are then available in the next stage for further processing.\n\nDenormalization is the opposite of normalization: embedding all of the data in a sin‐ gle document. Instead of documents containing references to one definitive copy of the data, many documents may have copies of the data. This means that multiple documents need to be updated if the information changes, but enables all related data to be fetched with a single query.\n\nDeciding when to normalize and when to denormalize can be difficult: typically, nor‐ malizing makes writes faster and denormalizing makes reads faster. Thus, you need to decide what trade-offs make sense for your application.\n\nExamples of Data Representations Suppose we are storing information about students and the classes that they are tak‐ ing. One way to represent this would be to have a students collection (each student is one document) and a classes collection (each class is one document). Then we could have a third collection (studentClasses) that contains references to the students and the classes they are taking:\n\n> db.studentClasses.findOne({\"studentId\" : id}) { \"_id\" : ObjectId(\"512512c1d86041c7dca81915\"), \"studentId\" : ObjectId(\"512512a5d86041c7dca81914\"), \"classes\" : [ ObjectId(\"512512ced86041c7dca81916\"), ObjectId(\"512512dcd86041c7dca81917\"), ObjectId(\"512512e6d86041c7dca81918\"), ObjectId(\"512512f0d86041c7dca81919\") ] }\n\nIf you are familiar with relational databases, you may have seen this type of join table before (although typically you’d have one student and one class per document, instead of a list of class \"_id\"s). It’s a bit more MongoDB-ish to put the classes in an array, but you usually wouldn’t want to store the data this way because it requires a lot of querying to get to the actual information.\n\nSuppose we wanted to find the classes a student was taking. We’d query for the stu‐ dent in the students collection, query studentClasses for the course \"_id\"s, and then query the classes collection for the class information. Thus, finding this information would take three trips to the server. This is generally not the way you want to struc‐ ture data in MongoDB, unless the classes and students are changing constantly and reading the data does not need to be done quickly.\n\nWe can remove one of the dereferencing queries by embedding class references in the student’s document:\n\n212\n\n|\n\nChapter 9: Application Design",
      "content_length": 2500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "{ \"_id\" : ObjectId(\"512512a5d86041c7dca81914\"), \"name\" : \"John Doe\", \"classes\" : [ ObjectId(\"512512ced86041c7dca81916\"), ObjectId(\"512512dcd86041c7dca81917\"), ObjectId(\"512512e6d86041c7dca81918\"), ObjectId(\"512512f0d86041c7dca81919\") ] }\n\nThe \"classes\" field keeps an array of \"_id\"s of classes that John Doe is taking. When we want to find out information about those classes, we can query the classes collection with those \"_id\"s. This only takes two queries. This is a fairly popular way to structure data that does not need to be instantly accessible and changes, but not constantly.\n\nIf we need to optimize reads further, we can get all of the information in a single query by fully denormalizing the data and storing each class as an embedded docu‐ ment in the \"classes\" field:\n\n{ \"_id\" : ObjectId(\"512512a5d86041c7dca81914\"), \"name\" : \"John Doe\", \"classes\" : [ { \"class\" : \"Trigonometry\", \"credits\" : 3, \"room\" : \"204\" }, { \"class\" : \"Physics\", \"credits\" : 3, \"room\" : \"159\" }, { \"class\" : \"Women in Literature\", \"credits\" : 3, \"room\" : \"14b\" }, { \"class\" : \"AP European History\", \"credits\" : 4, \"room\" : \"321\" } ] }\n\nThe upside of this is that it only takes one query to get the information. The down‐ sides are that it takes up more space and is more difficult to keep in sync. For exam‐ ple, if it turns out that physics was supposed to be worth four credits (not three),\n\nNormalization Versus Denormalization\n\n|\n\n213",
      "content_length": 1427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "every student in the physics class would need to have their document updated (instead of just updating a central “Physics” document).\n\nFinally, you can use the Extended Reference pattern mentioned earlier, which is a hybrid of embedding and referencing—you create an array of subdocuments with the frequently used information, but with a reference to the actual document for more information:\n\n{ \"_id\" : ObjectId(\"512512a5d86041c7dca81914\"), \"name\" : \"John Doe\", \"classes\" : [ { \"_id\" : ObjectId(\"512512ced86041c7dca81916\"), \"class\" : \"Trigonometry\" }, { \"_id\" : ObjectId(\"512512dcd86041c7dca81917\"), \"class\" : \"Physics\" }, { \"_id\" : ObjectId(\"512512e6d86041c7dca81918\"), \"class\" : \"Women in Literature\" }, { \"_id\" : ObjectId(\"512512f0d86041c7dca81919\"), \"class\" : \"AP European History\" } ] }\n\nThis approach is also a nice option because the amount of information embedded can change over time as your requirements change: if you want to include more or less information on a page, you can embed more or less of it in the document.\n\nAnother important consideration is how often this information will change, versus how often it’s read. If it will be updated regularly, then normalizing it is a good idea. However, if it changes infrequently, then there is little benefit to optimizing the update process at the expense of every read your application performs.\n\nFor example, a textbook normalization use case is to store a user and their address in separate collections. However, people’s addresses rarely change, so you generally shouldn’t penalize every read on the off chance that someone’s moved. Your applica‐ tion should embed the address in the user document.\n\nIf you decide to use embedded documents and you need to update them, you should set up a cron job to ensure that any updates you do are successfully propagated to every document. For example, suppose you attempt to do a multi-update but the server crashes before all of the documents have been updated. You need a way to detect this and retry the update.\n\n214\n\n|\n\nChapter 9: Application Design",
      "content_length": 2060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "In terms of update operators, \"$set\" is idempotent but \"$inc\" is not. Idempotent operations will have the same outcome whether tried once or several times; in the case of a network error, retrying the operation will be sufficient for the update to occur. In the case of operators that are not idempotent, the operation should be bro‐ ken into two operations that are individually idempotent and safe to retry. This can be achieved by including a unique pending token in the first operation and having the second operation use both a unique key and the unique pending token. This approach allows \"$inc\" to be idempotent because each individual updateOne opera‐ tion is idempotent.\n\nTo some extent, the more information you are generating, the less of it you should embed. If the content of the embedded fields or number of embedded fields is sup‐ posed to grow without bound then they should generally be referenced, not embed‐ ded. Things like comment trees or activity lists should be stored as their own documents, not embedded. It is also worth considering using the Subset pattern (described in “Schema Design Patterns” on page 208) to store the most recent items (or some other subset) in the document.\n\nFinally, the fields that are included should be integral to the data in the document. If a field is almost always excluded from your results when you query for a document, it’s a good sign that it may belong in another collection. These guidelines are summar‐ ized in Table 9-1.\n\nTable 9-1. Comparison of embedding versus references\n\nEmbedding is better for... Small subdocuments Data that does not change regularly When eventual consistency is acceptable Documents that grow by a small amount Data that you’ll often need to perform a second query to fetch Data that you’ll often exclude from the results Fast reads\n\nReferences are better for... Large subdocuments Volatile data When immediate consistency is necessary Documents that grow by a large amount\n\nFast writes\n\nSuppose we had a users collection. Here are some example fields we might have in the user documents and an indication of whether or not they should be embedded:\n\nAccount preferences\n\nThese are only relevant to this user document, and will probably be exposed with other user information in the document. Account preferences should generally be embedded.\n\nRecent activity\n\nThis depends on how much recent activity grows and changes. If it is a fixed-size field (say, the last 10 things), it might be useful to embed this information or to implement the Subset pattern.\n\nNormalization Versus Denormalization\n\n|\n\n215",
      "content_length": 2593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Friends\n\nGenerally this information should not be embedded, or at least not fully. See “Friends, Followers, and Other Inconveniences” on page 216.\n\nAll of the content this user has produced This should not be embedded.\n\nCardinality Cardinality is an indication of how many references a collection has to another collec‐ tion. Common relationships are one-to-one, one-to-many, or many-to-many. For example, suppose we had a blog application. Each post has a title, so that’s a one-to- one relationship. Each author has many posts, so that’s a one-to-many relationship. And posts have many tags and tags refer to many posts, so that’s a many-to-many relationship.\n\nWhen using MongoDB, it can be conceptually useful to split “many” into subcatego‐ ries: “many” and “few.” For example, you might have a one-to-few relationship between authors and posts: each author only writes a few posts. You might have many-to-few relation between blog posts and tags: you probably have many more blog posts than you have tags. However, you’d have a one-to-many relationship between blog posts and comments: each post has many comments.\n\nDetermining few versus many relations can help you decide what to embed versus what to reference. Generally, “few” relationships will work better with embedding, and “many” relationships will work better as references.\n\nFriends, Followers, and Other Inconveniences Keep your friends close and your enemies embedded.\n\nThis section covers considerations for social graph data. Many social applications need to link people, content, followers, friends, and so on. Figuring out how to bal‐ ance embedding and referencing this highly connected information can be tricky, but generally following, friending, or favoriting can be simplified to a publication/ subscription system: one user is subscribing to notifications from another. Thus, there are two basic operations that need to be efficient: storing subscribers and noti‐ fying all interested parties of an event.\n\nThere are three ways people typically implement subscribing. The first option is to put the producer in the subscriber’s document, which looks something like this:\n\n{ \"_id\" : ObjectId(\"51250a5cd86041c7dca8190f\"), \"username\" : \"batman\", \"email\" : \"batman@waynetech.com\" \"following\" : [ ObjectId(\"51250a72d86041c7dca81910\"),\n\n216\n\n|\n\nChapter 9: Application Design",
      "content_length": 2347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "ObjectId(\"51250a7ed86041c7dca81936\") ] }\n\nNow, given a user’s document, you can issue a query like the following to find all of the activities that have been published that they might be interested in:\n\ndb.activities.find({\"user\" : {\"$in\" : user[\"following\"]}})\n\nHowever, if you need to find everyone who is interested in a newly published activity, you’d have to query the \"following\" field across all users.\n\nAlternatively, you could append the followers to the producer’s document, like so:\n\n{ \"_id\" : ObjectId(\"51250a7ed86041c7dca81936\"), \"username\" : \"joker\", \"email\" : \"joker@mailinator.com\" \"followers\" : [ ObjectId(\"512510e8d86041c7dca81912\"), ObjectId(\"51250a5cd86041c7dca8190f\"), ObjectId(\"512510ffd86041c7dca81910\") ] }\n\nWhenever this user does something, all the users you need to notify are right there. The downside is that now you need to query the whole users collection to find every‐ one a user follows (the opposite limitation as in the previous case).\n\nEither of these options comes with an additional downside: they make your user documents larger and more volatile. The \"following\" (or \"followers\") field often won’t even need to be returned: how often do you want to list every follower? Thus, the final option neutralizes these downsides by normalizing even further and storing subscriptions in another collection. Normalizing this far is often overkill, but it can be useful for an extremely volatile field that often isn’t returned with the rest of the document. \"followers\" may be a sensible field to normalize this way.\n\nIn this case you keep a collection that matches publishers to subscribers, with docu‐ ments that look something like this:\n\n{ \"_id\" : ObjectId(\"51250a7ed86041c7dca81936\"), // followee's \"_id\" \"followers\" : [ ObjectId(\"512510e8d86041c7dca81912\"), ObjectId(\"51250a5cd86041c7dca8190f\"), ObjectId(\"512510ffd86041c7dca81910\") ] }\n\nThis keeps your user documents svelte but means an extra query is needed to get the followers.\n\nNormalization Versus Denormalization\n\n|\n\n217",
      "content_length": 2015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Dealing with the Wil Wheaton effect\n\nRegardless of which strategy you use, embedding only works with a limited number of subdocuments or references. If you have celebrity users, they may overflow any document that you’re storing followers in. The typical way of compensating for this is to use the Outlier pattern discussed in “Schema Design Patterns” on page 208 and have a “continuation” document, if necessary. For example, you might have:\n\n> db.users.find({\"username\" : \"wil\"}) { \"_id\" : ObjectId(\"51252871d86041c7dca8191a\"), \"username\" : \"wil\", \"email\" : \"wil@example.com\", \"tbc\" : [ ObjectId(\"512528ced86041c7dca8191e\"), ObjectId(\"5126510dd86041c7dca81924\") ] \"followers\" : [ ObjectId(\"512528a0d86041c7dca8191b\"), ObjectId(\"512528a2d86041c7dca8191c\"), ObjectId(\"512528a3d86041c7dca8191d\"), ... ] } { \"_id\" : ObjectId(\"512528ced86041c7dca8191e\"), \"followers\" : [ ObjectId(\"512528f1d86041c7dca8191f\"), ObjectId(\"512528f6d86041c7dca81920\"), ObjectId(\"512528f8d86041c7dca81921\"), ... ] } { \"_id\" : ObjectId(\"5126510dd86041c7dca81924\"), \"followers\" : [ ObjectId(\"512673e1d86041c7dca81925\"), ObjectId(\"512650efd86041c7dca81922\"), ObjectId(\"512650fdd86041c7dca81923\"), ... ] }\n\nThen add application logic to support fetching the documents in the “to be contin‐ ued” (\"tbc\") array.\n\n218\n\n|\n\nChapter 9: Application Design",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "Optimizations for Data Manipulation To optimize your application, you must first determine what its bottleneck is by eval‐ uating its read and write performance. Optimizing reads generally involves having the correct indexes and returning as much of the information as possible in a single document. Optimizing writes usually involves minimizing the number of indexes you have and making updates as efficient as possible.\n\nThere is often a trade-off between schemas that are optimized for writing quickly and those that are optimized for reading quickly, so you may have to decide which is more important for your application. Factor in not only the importance of reads versus writes, but also their proportions: if writes are more important but you’re doing a thousand reads to every write, you may still want to optimize reads first.\n\nRemoving Old Data Some data is only important for a brief time: after a few weeks or months it is just wasting storage space. There are three popular options for removing old data: using capped collections, using TTL collections, and dropping collections per time period.\n\nThe easiest option is to use a capped collection: set it to a large size and let old data “fall off” the end. However, capped collections pose certain limitations on the opera‐ tions you can do and are vulnerable to spikes in traffic, temporarily lowering the length of time that they can hold. See “Capped Collections” on page 151 for more information.\n\nThe second option is to use a TTL collections. This gives you finer-grain control over when documents are removed, but it may not be fast enough for collections with a very high write volume: it removes documents by traversing the TTL index the same way a user-requested remove would. If a TTL collection can keep up, though, this is probably the easiest solution to implement. See “Time-To-Live Indexes” on page 155 for more information about TTL indexes.\n\nThe final option is to use multiple collections: for example, one collection per month. Every time the month changes, your application starts using this month’s (empty) col‐ lection and searching for data in both the current and previous months’ collections. Once a collection is older than, say, six months, you can drop it. This strategy can keep up with nearly any volume of traffic, but it’s more complex to build an applica‐ tion around because you have to use dynamic collection (or database) names and possibly query multiple databases.\n\nOptimizations for Data Manipulation\n\n|\n\n219",
      "content_length": 2511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "Planning Out Databases and Collections Once you have sketched out what your documents look like, you must decide what collections or databases to put them in. This is often a fairly intuitive process, but there are some guidelines to keep in mind.\n\nIn general, documents with a similar schema should be kept in the same collection. MongoDB generally disallows combining data from multiple collections, so if there are documents that need to be queried or aggregated together, those are good candi‐ dates for putting in one big collection. For example, you might have documents that are fairly different “shapes,” but if you’re going to be aggregating them, they should all live in the same collection (or you can use the $merge stage if they are in separate col‐ lections or databases).\n\nFor collections, the big issues to consider are locking (you get a read/write lock per document) and storage. Generally, if you have a high-write workload you may need to consider using multiple physical volumes to reduce I/O bottlenecks. Each database can reside in its own directory when you use the --directoryperdb option, allowing you to mount different databases to different volumes. Thus, you may want all items within a database to be of similar “quality,” with a similar access pattern or similar traffic levels.\n\nFor example, suppose you have an application with several components: a logging component that creates a huge amount of not-very-valuable data, a user collection, and a couple of collections for user-generated data. These collections are high-value: it is important that user data is safe. There is also a high-traffic collection for social activities, which is of lower importance but not quite as unimportant as the logs. This collection is mainly used for user notifications, so it is almost an append-only collection.\n\nSplitting these up by importance, you might end up with three databases: logs, activi‐ ties, and users. The nice thing about this strategy is that you may find that your highest-value data is also what you have the least of (e.g., users probably don’t gener‐ ate as much data as logging does). You might not be able to afford an SSD for your entire dataset, but you might be able to get one for your users, or you might use RAID10 for users and RAID0 for logs and activities.\n\nBe aware that there are some limitations when using multiple databases prior to MongoDB 4.2 and the introduction of the $merge operator in the Aggregation Framework, which allows you to store results from an aggregation from one database to a different database and a different collection within that database. An additional point to note is that the renameCollection command is slower when copying an existing collection from one database to a different database, as it must copy all the documents to the new database.\n\n220\n\n|\n\nChapter 9: Application Design",
      "content_length": 2870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "Managing Consistency You must figure out how consistent your application’s reads need to be. MongoDB supports a huge variety of consistency levels, from always being able to read your own writes to reading data of unknown oldness. If you’re reporting on the last year of activity, you might only need data that’s correct to the last couple of days. Conversely, if you’re doing real-time trading, you might need to immediately read the latest writes.\n\nTo understand how to achieve these varying levels of consistency, it is important to understand what MongoDB is doing under the hood. The server keeps a queue of requests for each connection. When the client sends a request, it will be placed at the end of its connection’s queue. Any subsequent requests on the connection will occur after the previously enqueued operation is processed. Thus, a single connection has a consistent view of the database and can always read its own writes.\n\nNote that this is a per-connection queue: if we open two shells, we will have two con‐ nections to the database. If we perform an insert in one shell, a subsequent query in the other shell might not return the inserted document. However, within a single shell, if we query for a document after inserting it, the document will be returned. This behavior can be difficult to duplicate by hand, but on a busy server interleaved inserts and queries are likely to occur. Often developers run into this when they insert data in one thread and then check that it was successfully inserted in another. For a moment or two, it looks like the data was not inserted, and then it suddenly appears.\n\nThis behavior is especially worth keeping in mind when using the Ruby, Python, and Java drivers, because all three use connection pooling. For efficiency, these drivers open multiple connections (a pool) to the server and distribute requests across them. They all, however, have mechanisms to guarantee that a series of requests is processed by a single connection. There is detailed documentation on connection pooling for the various languages in the MongoDB Drivers Connection Monitoring and Pooling specification.\n\nWhen you send reads to a replica set secondary (see Chapter 12), this becomes an even larger issue. Secondaries may lag behind the primary, leading to reading data from seconds, minutes, or even hours ago. There are several ways to deal with this, the easiest being to simply send all reads to the primary if you care about staleness.\n\nMongoDB offers the readConcern option to control the consistency and isolation properties of the data being read. It can be combined with writeConcern to control the consistency and availability guarantees made to your application. There are five levels: \"local\", \"available\", \"majority\", \"linearizable\", and \"snapshot\". Depending on the application, in cases where you want to avoid read staleness you could consider using \"majority\", which returns only durable data that has been acknowledged by the majority of the replica set members and will not be rolled back.\n\nManaging Consistency\n\n|\n\n221",
      "content_length": 3078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "\"linearizable\" may also be an option: it returns data that reflects all successful majority-acknowledged writes that have completed prior to the start of the read oper‐ ation. MongoDB may wait for concurrently executing writes to finish before return‐ ing the results with the \"linearizable\" readConcern.\n\nThree senior engineers from MongoDB published a paper called “Tunable Consis‐ tency in MongoDB” at the PVLDB conference in 2019.1 This paper outlines the dif‐ ferent MongoDB consistency models used for replication and how application developers can utilize the various models.\n\nMigrating Schemas As your application grows and your needs change, your schema may have to grow and change as well. There are a couple of ways of accomplishing this, but regardless of the method you choose, you should carefully document each schema that your application has used. Ideally, you should consider if the Document Versioning pattern (see “Schema Design Patterns” on page 208) is applicable.\n\nThe simplest method is to simply have your schema evolve as your application requires, making sure that your application supports all old versions of the schema (e.g., accepting the existence or nonexistence of fields or dealing with multiple possi‐ ble field types gracefully). But this technique can become messy, particularly if you have conflicting schema versions. For instance, one version might require a \"mobile\" field, another version might require not having a \"mobile\" field but instead require a different field, and yet another version might treat the \"mobile\" field as optional. Keeping track of these shifting requirements can gradually turn your code into spaghetti.\n\nTo handle changing requirements in a slightly more structured way, you can include a \"version\" field (or just \"v\") in each document and use that to determine what your application will accept for document structure. This enforces your schema more rig‐ orously: a document has to be valid for some version of the schema, if not the current one. However, it still requires supporting old versions.\n\nThe final option is to migrate all of your data when the schema changes. Generally this is not a good idea: MongoDB allows you to have a dynamic schema in order to avoid migrates because they put a lot of pressure on your system. However, if you do decide to change every document, you will need to ensure that all the documents were successfully updated. MongoDB supports transactions, which support this type\n\n1 The authors are William Schultz, senior software engineer for replication; Tess Avitabile, team lead of the rep‐\n\nlication team; and Alyson Cabral, product manager for Distributed Systems.\n\n222\n\n|\n\nChapter 9: Application Design",
      "content_length": 2710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "of migration. If MongoDB crashes in the middle of a transaction, the older schema will be retained.\n\nManaging Schemas MongoDB introduced schema validation in version 3.2, which allows for validation during updates and insertions. In version 3.6 it added JSON Schema validation via the $jsonSchema operator, which is now the recommended method for all schema validation in MongoDB. At the time of writing MongoDB supports draft 4 of JSON Schema, but please check the documentation for the most up-to-date information on this feature.\n\nValidation does not check existing documents until they are modified, and it is con‐ figured per collection. To add validation to an existing collection, you use the coll Mod command with the validator option. You can add validation to a new collection by specifying the validator option when using db.createCollection(). MongoDB also provides two additional options, validationLevel and validationAction. vali dationLevel determines how strictly validation rules are applied to existing docu‐ ments during an update, and validationAction decides whether an error plus rejection or a warning with allowance for illegal documents should occur.\n\nWhen Not to Use MongoDB While MongoDB is a general-purpose database that works well for most applications, it isn’t good at everything. There are a few reasons you might need to avoid it:\n\nJoining many different types of data across many different dimensions is some‐ thing relational databases are fantastic at. MongoDB isn’t supposed to do this well and most likely never will.\n\nOne of the big (if, hopefully, temporary) reasons to use a relational database over MongoDB is if you’re using tools that don’t support it. From SQLAlchemy to WordPress, there are thousands of tools that just weren’t built to support Mon‐ goDB. The pool of tools that do support it is growing, but its ecosystem is hardly the size of relational databases’ yet.\n\nManaging Schemas\n\n|\n\n223",
      "content_length": 1945,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "PART III Replication",
      "content_length": 20,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "CHAPTER 10 Setting Up a Replica Set\n\nThis chapter introduces MongoDB’s high-availability system: replica sets. It covers:\n\nWhat replica sets are\n\nHow to set up a replica set\n\nWhat configuration options are available for replica set members\n\nIntroduction to Replication Since the first chapter, we’ve been using a standalone server, a single mongod server. It’s an easy way to get started but a dangerous way to run in production. What if your server crashes or becomes unavailable? Your database will be unavailable for at least a little while. If there are problems with the hardware, you might have to move your data to another machine. In the worst case, disk or network issues could leave you with corrupt or inaccessible data.\n\nReplication is a way of keeping identical copies of your data on multiple servers and is recommended for all production deployments. Replication keeps your application running and your data safe, even if something happens to one or more of your servers.\n\nWith MongoDB, you set up replication by creating a replica set. A replica set is a group of servers with one primary, the server taking writes, and multiple secondaries, servers that keep copies of the primary’s data. If the primary crashes, the secondaries can elect a new primary from amongst themselves.\n\nIf you are using replication and a server goes down, you can still access your data from the other servers in the set. If the data on a server is damaged or inaccessible, you can make a new copy of the data from one of the other members of the set.\n\n227",
      "content_length": 1549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "This chapter introduces replica sets and covers how to set up replication on your sys‐ tem. If you are less interested in replication mechanics and simply want to create a replica set for testing/development or production, use MongoDB’s cloud solution, MongoDB Atlas. It’s easy to use and provides a free-tier option for experimentation. Alternatively, to manage MongoDB clusters in your own infrastructure, you can use Ops Manager.\n\nSetting Up a Replica Set, Part 1 In this chapter, we’ll show you how to set up a three-node replica set on a single machine so you can start experimenting with replica set mechanics. This is the type of setup that you might script just to get a replica set up and running and then poke at it with administrative commands in the mongo shell or simulate network partitions or server failures to better understand how MongoDB handles high availability and disaster recovery. In production, you should always use a replica set and allocate a dedicated host to each member to avoid resource contention and provide isolation against server failure. To provide further resilience, you should also use the DNS Seedlist Connection format to specify how your applications connect to your replica set. The advantage to using DNS is that servers hosting your MongoDB replica set members can be changed in rotation without needing to reconfigure the clients (specifically, their connection strings).\n\nGiven the variety of virtualization and cloud options available, it is nearly as easy to bring up a test replica set with each member on a dedicated host. We’ve provided a Vagrant script to allow you to experiment with this option.1\n\nTo get started with our test replica set, let’s first create separate data directories for each node. On Linux or macOS, run the following command in the terminal to create the three directories:\n\n$ mkdir -p ~/data/rs{1,2,3}\n\nThis will create the directories ~/data/rs1, ~/data/rs2, and ~/data/rs3 (~ identifies your home directory).\n\nOn Windows, to create these directories, run the following in the Command Prompt (cmd) or PowerShell:\n\n> md c:\\data\\rs1 c:\\data\\rs2 c:\\data\\rs3\n\nThen, on Linux or macOS, run each of the following commands in a separate terminal:\n\n1 See https://github.com/mongodb-the-definitive-guide-3e/mongodb-the-definitive-guide-3e.\n\n228\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 2356,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "$ mongod --replSet mdbDefGuide --dbpath ~/data/rs1 --port 27017 \\ --smallfiles --oplogSize 200 $ mongod --replSet mdbDefGuide --dbpath ~/data/rs2 --port 27018 \\ --smallfiles --oplogSize 200 $ mongod --replSet mdbDefGuide --dbpath ~/data/rs3 --port 27019 \\ --smallfiles --oplogSize 200\n\nOn Windows, run each of the following commands in its own Command Prompt or PowerShell window:\n\n> mongod --replSet mdbDefGuide --dbpath c:\\data\\rs1 --port 27017 \\ --smallfiles --oplogSize 200 > mongod --replSet mdbDefGuide --dbpath c:\\data\\rs2 --port 27018 \\ --smallfiles --oplogSize 200 > mongod --replSet mdbDefGuide --dbpath c:\\data\\rs3 --port 27019 \\ --smallfiles --oplogSize 200\n\nOnce you’ve started them, you should have three separate mongod processes running.\n\nIn general, the principles we will walk through in the rest of this chapter apply to replica sets used in production deployments where each mongod has a dedicated host. However, there are additional details pertaining to securing replica sets that we address in Chap‐ ter 19; we’ll touch on those just briefly here as a preview.\n\nNetworking Considerations Every member of a set must be able to make connections to every other member of the set (including itself). If you get errors about members not being able to reach other members that you know are running, you may have to change your network configuration to allow connections between them.\n\nThe processes you’ve launched can just as easily be running on separate servers. However, with the release of MongoDB 3.6, mongod binds to localhost (127.0.0.1) only by default. In order for each member of replica set to communicate with the oth‐ ers, you must also bind to an IP address that is reachable by other members. If we were running a mongod instance on a server with a network interface having an IP address of 198.51.100.1 and we wanted to run it as a member of replica set with each member on different servers, we could specify the command-line parameter -- bind_ip or use bind_ip in the configuration file for this instance:\n\n$ mongod --bind_ip localhost,192.51.100.1 --replSet mdbDefGuide \\ --dbpath ~/data/rs1 --port 27017 --smallfiles --oplogSize 200\n\nWe would make similar modifications to launch the other mongods as well in this case, regardless of whether we’re running on Linux, macOS, or Windows.\n\nNetworking Considerations\n\n|\n\n229",
      "content_length": 2357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Security Considerations Before you bind to IP addresses other than localhost, when configuring a replica set, you should enable authorization controls and specify an authentication mechanism. In addition, it is a good idea to encrypt data on disk and communication among rep‐ lica set members and between the set and clients. We’ll go into more detail on secur‐ ing replica sets in Chapter 19.\n\nSetting Up a Replica Set, Part 2 Returning to our example, with the work we’ve done so far, each mongod does not yet know that the others exist. To tell them about one another, we need to create a config‐ uration that lists each of the members and send this configuration to one of our mon‐ god processes. It will take care of propagating the configuration to the other members.\n\nIn a fourth terminal, Windows Command Prompt, or PowerShell window, launch a mongo shell that connects to one of the running mongod instances. You can do this by typing the following command. With this command, we’ll connect to the mongod running on port 27017:\n\n$ mongo --port 27017\n\nThen, in the mongo shell, create a configuration document and pass this to the rs.ini tiate() helper to initiate a replica set. This will initiate a replica set containing three members and propagate the configuration to the rest of the mongods so that a replica set is formed:\n\n> rsconf = { _id: \"mdbDefGuide\", members: [ {_id: 0, host: \"localhost:27017\"}, {_id: 1, host: \"localhost:27018\"}, {_id: 2, host: \"localhost:27019\"} ] } > rs.initiate(rsconf) { \"ok\" : 1, \"operationTime\" : Timestamp(1501186502, 1) }\n\nThere are several important parts of a replica set configuration document. The con‐ fig’s \"_id\" is the name of the replica set that you passed in on the command line (in this example, \"mdbDefGuide\"). Make sure that this name matches exactly.\n\nThe next part of the document is an array of members of the set. Each of these needs two fields: an \"_id\" that is an integer and unique among the replica set members, and a hostname.\n\n230\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 2042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "Note that we are using localhost as a hostname for the members in this set. This is for example purposes only. In later chapters where we discuss securing replica sets, we’ll look at configurations that are more appropriate for production deployments. Mon‐ goDB allows all-localhost replica sets for testing locally but will protest if you try to mix localhost and non-localhost servers in a config.\n\nThis config document is your replica set configuration. The member running on localhost:27017 will parse the configuration and send messages to the other members, alerting them of the new configuration. Once they have all loaded the configuration, they will elect a primary and start handling reads and writes.\n\nUnfortunately, you cannot convert a standalone server to a replica set without some downtime for restarting it and initializing the set. Thus, even if you only have one server to start out with, you may want to configure it as a one-member replica set. That way, if you want to add more members later, you can do so without downtime.\n\nIf you are starting a brand-new set, you can send the configuration to any member in the set. If you are starting with data on one of the members, you must send the con‐ figuration to the member with data. You cannot initiate a replica set with data on more than one member.\n\nOnce initiated, you should have a fully functional replica set. The replica set should elect a primary. You can view the status of a replica set using rs.status(). The out‐ put from rs.status() tells you quite a bit about the replica set, including a number of things we’ve not yet covered, but don’t worry, we’ll get there! For now, take a look at the members array. Note that all three of our mongod instances are listed in this array and that one of them, in this case the mongod running on port 27017, has been elected primary. The other two are secondaries. If you try this for yourself you will certainly have different values for \"date\" and the several Timestamp values in this output, but you might also find that a different mongod was elected primary (that’s totally fine):\n\n> rs.status() { \"set\" : \"mdbDefGuide\", \"date\" : ISODate(\"2017-07-27T20:23:31.457Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : { \"lastCommittedOpTime\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"appliedOpTime\" : {\n\nSetting Up a Replica Set, Part 2\n\n|\n\n231",
      "content_length": 2436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "232\n\n\"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) } }, \"members\" : [ { \"_id\" : 0, \"name\" : \"localhost:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 688, \"optime\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2017-07-27T20:23:26Z\"), \"electionTime\" : Timestamp(1501186514, 1), \"electionDate\" : ISODate(\"2017-07-27T20:15:14Z\"), \"configVersion\" : 1, \"self\" : true }, { \"_id\" : 1, \"name\" : \"localhost:27018\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 508, \"optime\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2017-07-27T20:23:26Z\"), \"optimeDurableDate\" : ISODate(\"2017-07-27T20:23:26Z\"), \"lastHeartbeat\" : ISODate(\"2017-07-27T20:23:30.818Z\"), \"lastHeartbeatRecv\" : ISODate(\"2017-07-27T20:23:30.113Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"localhost:27017\", \"configVersion\" : 1 }, { \"_id\" : 2, \"name\" : \"localhost:27019\",\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "\"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 508, \"optime\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1501187006, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2017-07-27T20:23:26Z\"), \"optimeDurableDate\" : ISODate(\"2017-07-27T20:23:26Z\"), \"lastHeartbeat\" : ISODate(\"2017-07-27T20:23:30.818Z\"), \"lastHeartbeatRecv\" : ISODate(\"2017-07-27T20:23:30.113Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"localhost:27017\", \"configVersion\" : 1 } ], \"ok\" : 1, \"operationTime\" : Timestamp(1501187006, 1) }\n\nrs Helper Functions rs is a global variable that contains replication helper functions (run rs.help() to see the helpers it exposes). These functions are almost always just wrappers around data‐ base commands. For example, the following database command is equivalent to rs.initiate(config):\n\n> db.adminCommand({\"replSetInitiate\" : config})\n\nIt is good to have familiarity with both the helpers and the underlying commands, because it might be easier to use the command form instead of the helper.\n\nObserving Replication If your replica set elected the mongod on port 27017 as primary, then the mongo shell used to initiate the replica set is currently connected to the primary. You should see the prompt change to something like the following:\n\nmdbDefGuide:PRIMARY>\n\nThis indicates that we are connected to the primary of the replica set having the \"_id\" \"mdbDefGuide\". To simplify and for the sake of clarity, we’ll abbreviate the mongo shell prompt to just > throughout the replication examples.\n\nObserving Replication\n\n|\n\n233",
      "content_length": 1600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "If your replica set elected a different node primary, quit the shell and connect to the primary by specifying the correct port number in the command line, as we did when launching the mongo shell earlier. For example, if your set’s primary is on port 27018, connect using the following command:\n\n$ mongo --port 27018\n\nNow that you’re connected to the primary, try doing some writes and see what hap‐ pens. First, insert 1,000 documents:\n\n> use test > for (i=0; i<1000; i++) {db.coll.insert({count: i})} > > // make sure the docs are there > db.coll.count() 1000\n\nNow check one of the secondaries and verify that it has a copy of all of these docu‐ ments. You could do this by quitting the shell and connecting using the port number of one of the secondaries, but it’s easy to acquire a connection to one of the seconda‐ ries by instantiating a connection object using the Mongo constructor within the shell you’re already running.\n\nFirst, use your connection to the test database on the primary to run the isMaster command. This will show you the status of the replica set, in a much more concise form than rs.status(). It is also a convenient means of determining which member is primary when writing application code or scripting:\n\n> db.isMaster() { \"hosts\" : [ \"localhost:27017\", \"localhost:27018\", \"localhost:27019\" ], \"setName\" : \"mdbDefGuide\", \"setVersion\" : 1, \"ismaster\" : true, \"secondary\" : false, \"primary\" : \"localhost:27017\", \"me\" : \"localhost:27017\", \"electionId\" : ObjectId(\"7fffffff0000000000000004\"), \"lastWrite\" : { \"opTime\" : { \"ts\" : Timestamp(1501198208, 1), \"t\" : NumberLong(4) }, \"lastWriteDate\" : ISODate(\"2017-07-27T23:30:08Z\") }, \"maxBsonObjectSize\" : 16777216, \"maxMessageSizeBytes\" : 48000000,\n\n234\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 1767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "\"maxWriteBatchSize\" : 1000, \"localTime\" : ISODate(\"2017-07-27T23:30:08.722Z\"), \"maxWireVersion\" : 6, \"minWireVersion\" : 0, \"readOnly\" : false, \"compression\" : [ \"snappy\" ], \"ok\" : 1, \"operationTime\" : Timestamp(1501198208, 1) }\n\nIf at any point an election is called and the mongod you’re connected to becomes a secondary, you can use the isMaster command to determine which member has become primary. The output here tells us that localhost:27018 and localhost:27019 are both secondaries, so we can use either for our purposes. Let’s instantiate a connection to localhost:27019:\n\n> secondaryConn = new Mongo(\"localhost:27019\") connection to localhost:27019 > > secondaryDB = secondaryConn.getDB(\"test\") test\n\nNow, if we attempt to do a read on the collection that has been replicated to the sec‐ ondary, we’ll get an error. Let’s attempt to do a find on this collection and then review the error and why we get it:\n\n> secondaryDB.coll.find() Error: error: { \"operationTime\" : Timestamp(1501200089, 1), \"ok\" : 0, \"errmsg\" : \"not master and slaveOk=false\", \"code\" : 13435, \"codeName\" : \"NotMasterNoSlaveOk\" }\n\nSecondaries may fall behind the primary (or lag) and not have the most current writes, so secondaries will refuse read requests by default to prevent applications from accidentally reading stale data. Thus, if you attempt to query a secondary, you’ll get an error stating that it’s not the primary. This is to protect your application from acci‐ dentally connecting to a secondary and reading stale data. To allow queries on the secondary, we can set an “I’m okay with reading from secondaries” flag, like so:\n\n> secondaryConn.setSlaveOk()\n\nNote that slaveOk is set on the connection (secondaryConn), not the database (secondaryDB).\n\nNow you’re all set to read from this member. Query it normally:\n\nObserving Replication\n\n|\n\n235",
      "content_length": 1837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "> secondaryDB.coll.find() { \"_id\" : ObjectId(\"597a750696fd35621b4b85db\"), \"count\" : 0 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85dc\"), \"count\" : 1 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85dd\"), \"count\" : 2 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85de\"), \"count\" : 3 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85df\"), \"count\" : 4 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e0\"), \"count\" : 5 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e1\"), \"count\" : 6 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e2\"), \"count\" : 7 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e3\"), \"count\" : 8 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e4\"), \"count\" : 9 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e5\"), \"count\" : 10 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e6\"), \"count\" : 11 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e7\"), \"count\" : 12 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e8\"), \"count\" : 13 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85e9\"), \"count\" : 14 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85ea\"), \"count\" : 15 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85eb\"), \"count\" : 16 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85ec\"), \"count\" : 17 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85ed\"), \"count\" : 18 } { \"_id\" : ObjectId(\"597a750696fd35621b4b85ee\"), \"count\" : 19 } Type \"it\" for more\n\nYou can see that all of our documents are there.\n\nNow, try to write to a secondary:\n\n> secondaryDB.coll.insert({\"count\" : 1001}) WriteResult({ \"writeError\" : { \"code\" : 10107, \"errmsg\" : \"not master\" } }) > secondaryDB.coll.count() 1000\n\nYou can see that the secondary does not accept the write. A secondary will only per‐ form writes that it gets through replication, not from clients.\n\nThere is one other interesting feature that you should try out: automatic failover. If the primary goes down, one of the secondaries will automatically be elected primary. To test this, stop the primary:\n\n> db.adminCommand({\"shutdown\" : 1})\n\nYou’ll see some error messages generated when you run this command because the mongod running on port 27017 (the member we’re connected to) will terminate and the shell we’re using will lose its connection:\n\n2017-07-27T20:10:50.612-0400 E QUERY [thread1] Error: error doing query: failed: network error while attempting to run command 'shutdown' on host '127.0.0.1:27017' : DB.prototype.runCommand@src/mongo/shell/db.js:163:1 DB.prototype.adminCommand@src/mongo/shell/db.js:179:16 @(shell):1:1 2017-07-27T20:10:50.614-0400 I NETWORK [thread1] trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed\n\n236\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 2569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "2017-07-27T20:10:50.615-0400 I NETWORK [thread1] reconnect 127.0.0.1:27017 (127.0.0.1) ok MongoDB Enterprise mdbDefGuide:SECONDARY> 2017-07-27T20:10:56.051-0400 I NETWORK [thread1] trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed 2017-07-27T20:10:56.051-0400 W NETWORK [thread1] Failed to connect to 127.0.0.1:27017, in(checking socket for error after poll), reason: Connection refused 2017-07-27T20:10:56.051-0400 I NETWORK [thread1] reconnect 127.0.0.1:27017 (127.0.0.1) failed failed MongoDB Enterprise > MongoDB Enterprise > secondaryConn.isMaster() 2017-07-27T20:11:15.422-0400 E QUERY [thread1] TypeError: secondaryConn.isMaster is not a function : @(shell):1:1\n\nThis isn’t a problem. It won’t cause the shell to crash. Go ahead and run isMaster on the secondary to see who has become the new primary:\n\n> secondaryDB.isMaster()\n\nThe output from isMaster should look something like this:\n\n{ \"hosts\" : [ \"localhost:27017\", \"localhost:27018\", \"localhost:27019\" ], \"setName\" : \"mdbDefGuide\", \"setVersion\" : 1, \"ismaster\" : true, \"secondary\" : false, \"primary\" : \"localhost:27018\", \"me\" : \"localhost:27019\", \"electionId\" : ObjectId(\"7fffffff0000000000000005\"), \"lastWrite\" : { \"opTime\" : { \"ts\" : Timestamp(1501200681, 1), \"t\" : NumberLong(5) }, \"lastWriteDate\" : ISODate(\"2017-07-28T00:11:21Z\") }, \"maxBsonObjectSize\" : 16777216, \"maxMessageSizeBytes\" : 48000000, \"maxWriteBatchSize\" : 1000, \"localTime\" : ISODate(\"2017-07-28T00:11:28.115Z\"), \"maxWireVersion\" : 6, \"minWireVersion\" : 0, \"readOnly\" : false, \"compression\" : [ \"snappy\" ],\n\nObserving Replication\n\n|\n\n237",
      "content_length": 1575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "\"ok\" : 1, \"operationTime\" : Timestamp(1501200681, 1) }\n\nNote that the primary has switched to 27018. Your primary may be the other server; whichever secondary noticed that the primary was down first will be elected. Now you can send writes to the new primary.\n\nisMaster is a very old command, predating replica sets to when MongoDB only supported master/slave replication. Thus, it does not use the replica set terminology consistently: it still calls the pri‐ mary a “master.” You can generally think of “master” as equivalent to “primary” and “slave” as equivalent to “secondary.”\n\nGo ahead and bring back up the server we had running at localhost:27017. You simply need to find the command-line interface from which you launched it. You’ll see some messages indicating that it terminated. Just run it again using the same command you used to launch it originally.\n\nCongratulations! You just set up, used, and even poked a little at a replica set to force a shutdown and an election for a new primary.\n\nThere are a few key concepts to remember:\n\nClients can send a primary all the same operations they could send a standalone server (reads, writes, commands, index builds, etc.).\n\nClients cannot write to secondaries.\n\nClients, by default, cannot read from secondaries. You can enable this by explic‐ itly setting an “I know I’m reading from a secondary” setting on the connection.\n\nChanging Your Replica Set Configuration Replica set configurations can be changed at any time: members can be added, removed, or modified. There are shell helpers for some common operations. For example, to add a new member to the set, you can use rs.add:\n\n> rs.add(\"localhost:27020\")\n\nSimilarly, you can remove members:\n\n> rs.remove(\"localhost:27017\") { \"ok\" : 1, \"operationTime\" : Timestamp(1501202441, 2) }\n\nYou can check that a reconfiguration succeeded by running rs.config() in the shell. It will print the current configuration:\n\n238\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 1966,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "> rs.config() { \"_id\" : \"mdbDefGuide\", \"version\" : 3, \"protocolVersion\" : NumberLong(1), \"members\" : [ { \"_id\" : 1, \"host\" : \"localhost:27018\", \"arbiterOnly\" : false, \"buildIndexes\" : true, \"hidden\" : false, \"priority\" : 1, \"tags\" : {\n\n}, \"slaveDelay\" : NumberLong(0), \"votes\" : 1 }, { \"_id\" : 2, \"host\" : \"localhost:27019\", \"arbiterOnly\" : false, \"buildIndexes\" : true, \"hidden\" : false, \"priority\" : 1, \"tags\" : {\n\n}, \"slaveDelay\" : NumberLong(0), \"votes\" : 1 }, { \"_id\" : 3, \"host\" : \"localhost:27020\", \"arbiterOnly\" : false, \"buildIndexes\" : true, \"hidden\" : false, \"priority\" : 1, \"tags\" : {\n\n}, \"slaveDelay\" : NumberLong(0), \"votes\" : 1 } ], \"settings\" : { \"chainingAllowed\" : true, \"heartbeatIntervalMillis\" : 2000, \"heartbeatTimeoutSecs\" : 10, \"electionTimeoutMillis\" : 10000, \"catchUpTimeoutMillis\" : -1,\n\nChanging Your Replica Set Configuration\n\n|\n\n239",
      "content_length": 862,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "\"getLastErrorModes\" : {\n\n}, \"getLastErrorDefaults\" : { \"w\" : 1, \"wtimeout\" : 0 }, \"replicaSetId\" : ObjectId(\"597a49c67e297327b1e5b116\") } }\n\nEach time you change the configuration, the \"version\" field will increase. It starts at version 1.\n\nYou can also modify existing members, not just add and remove them. To make mod‐ ifications, create the configuration document that you want in the shell and call rs.reconfig(). For example, suppose we have a configuration such as the one shown here:\n\n> rs.config() { \"_id\" : \"testReplSet\", \"version\" : 2, \"members\" : [ { \"_id\" : 0, \"host\" : \"198.51.100.1:27017\" }, { \"_id\" : 1, \"host\" : \"localhost:27018\" }, { \"_id\" : 2, \"host\" : \"localhost:27019\" } ] }\n\nSomeone accidentally added member 0 by IP address, instead of its hostname. To change that, first we load the current configuration in the shell and then we change the relevant fields:\n\n> var config = rs.config() > config.members[0].host = \"localhost:27017\"\n\nNow that the config document is correct, we need to send it to the database using the rs.reconfig() helper:\n\n> rs.reconfig(config)\n\n240\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 1132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "rs.reconfig() is often more useful than rs.add() and rs.remove() for complex operations, such as modifying members’ configurations or adding/removing multiple members at once. You can use it to make any legal configuration change you need: simply create the config document that represents your desired configuration and pass it to rs.reconfig().\n\nHow to Design a Set To plan out your set, there are certain concepts that you must be familiar with. The next chapter goes into more detail about these, but the most important is that replica sets are all about majorities: you need a majority of members to elect a primary, a pri‐ mary can only stay primary as long as it can reach a majority, and a write is safe when it’s been replicated to a majority. This majority is defined to be “more than half of all members in the set,” as shown in Table 10-1.\n\nTable 10-1. What is a majority?\n\nNumber of members in the set Majority of the set 1 2 3 4 5 6 7\n\n1 2 2 3 3 4 4\n\nNote that it doesn’t matter how many members are down or unavailable; majority is based on the set’s configuration.\n\nFor example, suppose that we have a five-member set and three members go down, as shown in Figure 10-1. There are still two members up. These two members cannot reach a majority of the set (at least three members), so they cannot elect a primary. If one of them were primary, it would step down as soon as it noticed that it could not reach a majority. After a few seconds, your set would consist of two secondaries and three unreachable members.\n\nHow to Design a Set\n\n|\n\n241",
      "content_length": 1557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Figure 10-1. With a minority of the set available, all members will be secondaries\n\nMany users find this frustrating: why can’t the two remaining members elect a pri‐ mary? The problem is that it’s possible that the other three members didn’t actually go down, and that it was instead the network that went down, as shown in Figure 10-2. In this case, the three members on the left will elect a primary, since they can reach a majority of the set (three members out of five). In the case of a network partition, we do not want both sides of the partition to elect a primary, because then the set would have two primaries. Both primaries would be writing to the database, and the datasets would diverge. Requiring a majority to elect or stay a primary is a neat way of avoid‐ ing ending up with more than one primary.\n\nFigure 10-2. For the members, a network partition looks identical to servers on the other side of the partition going down\n\nIt is important to configure your set in such a way that you’ll usually be able to have one primary. For example, in the five-member set described here, if members 1, 2, and 3 are in one data center and members 4 and 5 are in another, there should almost always be a majority available in the first data center (it’s more likely to have a net‐ work break between data centers than within them).\n\nThere are a couple of common configurations that are recommended:\n\nA majority of the set in one data center, as in Figure 10-2. This is a good design if you have a primary data center where you always want your replica set’s primary to be located. So long as your primary data center is healthy, you will have a\n\n242\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 1695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "primary. However, if that data center becomes unavailable, your secondary data center will not be able to elect a new primary.\n\nAn equal number of servers in each data center, plus a tie-breaking server in a third location. This is a good design if your data centers are “equal” in preference, since generally servers from either data center will be able to see a majority of the set. However, it involves having three separate locations for servers.\n\nMore complex requirements might require different configurations, but you should keep in mind how your set will acquire a majority under adverse conditions.\n\nAll of these complexities would disappear if MongoDB supported having more than one primary. However, this would bring its own host of complexities. With two pri‐ maries, you would have to handle conflicting writes (e.g., if someone updates a docu‐ ment on one primary and someone deletes it on another primary). There are two popular ways of handling conflicts in systems that support multiple writers: manual reconciliation or having the system arbitrarily pick a “winner.” Neither of these options is a very easy model for developers to code against, seeing as you can’t be sure that the data you’ve written won’t change out from under you. Thus, MongoDB chose to only support having a single primary. This makes development easier but can result in periods when the replica set is read-only.\n\nHow Elections Work When a secondary cannot reach a primary, it will contact all the other members and request that it be elected primary. These other members do several sanity checks: Can they reach a primary that the member seeking election cannot? Is the member seek‐ ing election up to date with replication? Is there any member with a higher priority available that should be elected instead?\n\nIn version 3.2, MongoDB introduced version 1 of the replication protocol. Protocol version 1 is based on the RAFT consensus protocol developed by Diego Ongaro and John Ousterhout at Stanford University. It is best described as RAFT-like and is tail‐ ored to include a number of replication concepts that are specific to MongoDB, such as arbiters, priority, nonvoting members, write concern, etc. Protocol version 1 pro‐ vided the foundation for new features such as a shorter failover time and greatly reduces the time to detect false primary situations. It also prevents double voting through the use of term IDs.\n\nRAFT is a consensus algorithm that is broken into relatively inde‐ pendent subproblems. Consensus is the process through which multiple servers or processes agree on values. RAFT ensures con‐ sensus such that the same series of commands produces the same series of results and arrives at the same series of states across the members of a deployment.\n\nHow to Design a Set\n\n|\n\n243",
      "content_length": 2799,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Replica set members send heartbeats (pings) to each other every two seconds. If a heartbeat does not return from a member within 10 seconds, the other members mark the delinquent member as inaccessible. The election algorithm will make a “best-effort” attempt to have the secondary with the highest priority available call an election. Member priority affects both the timing and the outcome of elections; sec‐ ondaries with higher priority call elections relatively sooner than secondaries with lower priority, and are also more likely to win. However, a lower-priority instance can be elected as primary for brief periods, even if a higher-priority secondary is avail‐ able. Replica set members continue to call elections until the highest-priority member available becomes primary.\n\nTo be elected primary, a member must be up to date with replication, as far as the members it can reach know. All replicated operations are strictly ordered by an ascending identifier, so the candidate must have operations later than or equal to those of any member it can reach.\n\nMember Configuration Options The replica sets we have set up so far have been fairly uniform in that every member has the same configuration as every other member. However, there are many situa‐ tions when you don’t want members to be identical: you might want one member to preferentially be primary or make a member invisible to clients so that no read requests can be routed to it. These and many other configuration options can be specified in the member subdocuments of the replica set configuration. This section outlines the member options that you can set.\n\nPriority Priority is an indication of how strongly this member “wants” to become primary. Its value can range from 0 to 100, and the default is 1. Setting \"priority\" to 0 has a spe‐ cial meaning: members with a priority of 0 can never become primary. These are called passive members.\n\nThe highest-priority member will always be elected primary (so long as it can reach a majority of the set and has the most up-to-date data). For example, suppose you add a member with a priority of 1.5 to the set, like so:\n\n> rs.add({\"host\" : \"server-4:27017\", \"priority\" : 1.5})\n\nAssuming the other members of the set have priority 1, once server-4 caught up with the rest of the set, the current primary would automatically step down and server-4 would elect itself. If server-4 was, for some reason, unable to catch up, the current pri‐ mary would stay primary. Setting priorities will never cause your set to go primary- less. It will also never cause a member that is behind to become primary (until it has caught up).\n\n244\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 2688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "The absolute value of \"priority\" only matters in relation to whether it is greater or less than the other priorities in the set: members with priorities of 100, 1, and 1 will behave the same way as members of another set with priorities 2, 1, and 1.\n\nHidden Members Clients do not route requests to hidden members, and hidden members are not pre‐ ferred as replication sources (although they will be used if more desirable sources are not available). Thus, many people will hide less powerful or backup servers.\n\nFor example, suppose you had a set that looked like this:\n\n> rs.isMaster() { ... \"hosts\" : [ \"server-1:27107\", \"server-2:27017\", \"server-3:27017\" ], ... }\n\nTo hide server-3, you could add the hidden: true field to its configuration. A mem‐ ber must have a priority of 0 to be hidden (you can’t have a hidden primary):\n\n> var config = rs.config() > config.members[2].hidden = true 0 > config.members[2].priority = 0 0 > rs.reconfig(config)\n\nNow running isMaster will show:\n\n> rs.isMaster() { ... \"hosts\" : [ \"server-1:27107\", \"server-2:27017\" ], ... }\n\nrs.status() and rs.config() will still show the member; it only disappears from isMaster. When clients connect to a replica set, they call isMaster to determine the members of the set. Thus, hidden members will never be used for read requests.\n\nTo unhide a member, change the hidden option to false or remove the option entirely.\n\nMember Configuration Options\n\n|\n\n245",
      "content_length": 1432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "Election Arbiters A two-member set has clear disadvantages for majority requirements. However, many people with small deployments do not want to keep three copies of their data, feeling that two is enough and that keeping a third copy is not worth the administrative, operational, and financial costs.\n\nFor these deployments, MongoDB supports a special type of member called an arbiter, whose only purpose is to participate in elections. Arbiters hold no data and aren’t used by clients: they just provide a majority for two-member sets. In general, deployments without arbiters are preferable.\n\nAs arbiters don’t have any of the traditional responsibilities of a mongod server, you can run an arbiter as a lightweight process on a wimpier server than you’d generally use for MongoDB. It’s often a good idea, if possible, to run an arbiter in a separate failure domain from the other members, so that it has an “outside perspective” on the set, as described in the deployment recommendations in “How to Design a Set” on page 241.\n\nYou start up an arbiter in the same way that you start a normal mongod, using the --replSet name option and an empty data directory. You can add it to the set using the rs.addArb() helper:\n\n> rs.addArb(\"server-5:27017\")\n\nEquivalently, you can specify configuration:\n\nthe \"arbiterOnly\" option\n\nin\n\nthe member\n\n> rs.add({\"_id\" : 4, \"host\" : \"server-5:27017\", \"arbiterOnly\" : true})\n\nAn arbiter, once added to the set, is an arbiter forever: you cannot reconfigure an arbiter to become a nonarbiter, or vice versa.\n\nOne other thing that arbiters are good for is breaking ties in larger clusters. If you have an even number of nodes, you may have half the nodes vote for one member and half for another. An arbiter can cast the deciding vote. There are a few things to keep in mind when using arbiters, though; we’ll look at these next.\n\nUse at most one arbiter\n\nNote that, in both of the use cases just described, you need at most one arbiter. You do not need an arbiter if you have an odd number of nodes. A common misconception seems to be that you should add extra arbiters “just in case.” However, it doesn’t help elections go any faster or provide any additional data safety to add extra arbiters.\n\nSuppose you have a three-member set. Two members are required to elect a primary. If you add an arbiter, you’ll have a four-member set, so three members will be\n\n246\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 2438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "required to choose a primary. Thus, your set is potentially less stable: instead of requiring 67% of your set to be up, you’re now requiring 75%.\n\nHaving extra members can also make elections take longer. If you have an even num‐ ber of nodes because you added an arbiter, your arbiters can cause ties, not prevent them.\n\nThe downside to using an arbiter\n\nIf you have a choice between a data node and an arbiter, choose a data node. Using an arbiter instead of a data node in a small set can make some operational tasks more difficult. For example, suppose you are running a replica set with two “normal” mem‐ bers and one arbiter, and one of the data-holding members goes down. If that mem‐ ber is well and truly dead (the data is unrecoverable), you will have to get a copy of the data from the current primary to the new server you’ll be using as a secondary. Copying data can put a lot of stress on a server, and thus slow down your application. (Generally, copying a few gigabytes to a new server is trivial but more than a hundred starts becoming impractical.)\n\nConversely, if you have three data-holding members, there’s more “breathing room” if a server completely dies. You can use the remaining secondary to bootstrap a new server instead of depending on your primary.\n\nIn the two-member-plus-arbiter scenario, the primary is the last remaining good copy of your data and the one trying to handle load from your application while you’re trying to get another copy of your data online.\n\nThus, if possible, use an odd number of “normal” members instead of an arbiter.\n\nIn three-member replica sets with a primary-secondary-arbiter (PSA) architecture or sharded clusters with a three-member PSA shard, there is a known issue with cache pressure increasing if either of the two data-bearing nodes are down and the \"majority\" read concern is enabled. Ideally, you should replace the arbiter with a data-bearing member for these deployments. Alternatively, to prevent storage cache pressure the \"majority\" read concern can be disabled on each of the mongod instances in the deployment or shards.\n\nBuilding Indexes Sometimes a secondary does not need to have the same (or any) indexes that exist on the primary. If you are using a secondary only for backup data or offline batch jobs, you might want to specify \"buildIndexes\" : false in the member’s configuration. This option prevents the secondary from building any indexes.\n\nMember Configuration Options\n\n|\n\n247",
      "content_length": 2467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "This is a permanent setting: members that have \"buildIndexes\" : false specified can never be reconfigured to be “normal” index-building members again. If you want to change a non-index-building member to an index-building one, you must remove it from the set, delete all of its data, add it to the set again, and allow it to resync from scratch.\n\nAs with hidden members, this option requires the member’s priority to be 0.\n\n248\n\n|\n\nChapter 10: Setting Up a Replica Set",
      "content_length": 468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "CHAPTER 11 Components of a Replica Set\n\nThis chapter covers how the pieces of a replica set fit together, including:\n\nHow replica set members replicate new data\n\nHow bringing up new members works\n\nHow elections work\n\nPossible server and network failure scenarios\n\nSyncing Replication is concerned with keeping an identical copy of data on multiple servers. The way MongoDB accomplishes this is by keeping a log of operations, or oplog, con‐ taining every write that a primary performs. This is a capped collection that lives in the local database on the primary. The secondaries query this collection for opera‐ tions to replicate.\n\nEach secondary maintains its own oplog, recording each operation it replicates from the primary. This allows any member to be used as a sync source for any other mem‐ ber, as shown in Figure 11-1. Secondaries fetch operations from the member they are syncing from, apply the operations to their dataset, and then write the operations to their oplog. If applying an operation fails (which should only happen if the underly‐ ing data has been corrupted or in some way differs from the primary’s), the secon‐ dary will exit.\n\n249",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "Figure 11-1. Oplogs keep an ordered list of write operations that have occurred; each member has its own copy of the oplog, which should be identical to the primary’s (mod‐ ulo some lag)\n\nIf a secondary goes down for any reason, when it restarts it will start syncing from the last operation in its oplog. As operations are applied to data and then written to the oplog, the secondary may replay operations that it has already applied to its data. MongoDB is designed to handle this correctly: replaying oplog ops multiple times yields the same result as replaying them once. Each operation in the oplog is idempo‐ tent. That is, oplog operations produce the same results whether applied once or mul‐ tiple times to the target dataset.\n\nBecause the oplog is a fixed size, it can only hold a certain number of operations. In general, the oplog will use space at approximately the same rate as writes come into the system: if you’re writing 1 KB/minute on the primary, your oplog is probably going to fill up at about 1 KB/minute. However, there are a few exceptions: operations that affect multiple documents, such as removes or a multi-updates, will be exploded into many oplog entries. The single operation on the primary will be split into one oplog op per document affected. Thus, if you remove 1,000,000 documents from a collection with db.coll.remove(), it will become 1,000,000 oplog entries removing one document at a time. If you are doing lots of bulk operations, this can fill up your oplog more quickly than you might expect.\n\nIn most cases, the default oplog size is sufficient. If you can predict your replica set’s workload to resemble one of the following patterns, then you might want to create an oplog that is larger than the default. Conversely, if your application predominantly performs reads with a minimal amount of write operations, a smaller oplog may be sufficient. These are the kinds of workloads that might require a larger oplog size:\n\nUpdates to multiple documents at once\n\nThe oplog must translate multi-updates into individual operations in order to maintain idempotency. This can use a great deal of oplog space without a corre‐ sponding increase in data size or disk use.\n\n250\n\n|\n\nChapter 11: Components of a Replica Set",
      "content_length": 2255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Deletions equal the same amount of data as inserts\n\nIf you delete roughly the same amount of data as you insert, the database will not grow significantly in terms of disk use, but the size of the operation log can be quite large.\n\nSignificant number of in-place updates\n\nIf a significant portion of the workload is updates that do not increase the size of the documents, the database records a large number of operations but the quan‐ tity of data on disk does not change.\n\nBefore mongod creates an oplog, you can specify its size with the oplogSizeMB option. However, after you have started a replica set member for the first time, you can only change the size of the oplog using the “Change the Size of the Oplog” procedure.\n\nMongoDB uses two forms of data synchronization: an initial sync to populate new members with the full dataset, and replication to apply ongoing changes to the entire dataset. Let’s take a closer look at each of these.\n\nInitial Sync MongoDB performs an initial sync to copy all the data from one member of the rep‐ lica set to another member. When a member of the set starts up, it will check if it is in a valid state to begin syncing from someone. If it is in a valid state, it will attempt to make a full copy of the data from another member of the set. There are several steps to the process, which you can follow in the mongod’s log.\n\nFirst, MongoDB clones all databases except the local database. The mongod scans every collection in each source database and inserts all the data into its own copies of these collections on the target member. Prior to beginning the clone operations, any existing data on the target member will be dropped.\n\nOnly do an initial sync for a member if you do not want the data in your data directory or have moved it elsewhere, as mongod’s first action is to delete it all.\n\nIn MongoDB 3.4 and later, the initial sync builds all the collection indexes as the documents are copied for each collection (in earlier versions, only the \"_id\" indexes are built during this stage). It also pulls newly added oplog records during the data copy, so you should ensure that the target member has enough disk space in the local database to store these records during this data copy stage.\n\nOnce all the databases are cloned, the mongod uses the oplog from the source to update its dataset to reflect the current state of the replica set, applying all changes to the dataset that occurred while the copy was in progress. These changes might\n\nSyncing\n\n|\n\n251",
      "content_length": 2506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "include any type of write (inserts, updates, and deletes), and this process might mean that mongod has to reclone certain documents that were moved and therefore missed by the cloner.\n\nThis is roughly what the logs will look like if some documents had to be recloned. Depending on the level of traffic and the types of operations that where happening on the sync source, you may or may not have missing objects:\n\nMon Jan 30 15:38:36 [rsSync] oplog sync 1 of 3 Mon Jan 30 15:38:36 [rsBackgroundSync] replSet syncing to: server-1:27017 Mon Jan 30 15:38:37 [rsSyncNotifier] replset setting oplog notifier to server-1:27017 Mon Jan 30 15:38:37 [repl writer worker 2] replication update of non-mod failed: { ts: Timestamp 1352215827000|17, h: -5618036261007523082, v: 2, op: \"u\", ns: \"db1.someColl\", o2: { _id: ObjectId('50992a2a7852201e750012b7') }, o: { $set: { count.0: 2, count.1: 0 } } } Mon Jan 30 15:38:37 [repl writer worker 2] replication info adding missing object Mon Jan 30 15:38:37 [repl writer worker 2] replication missing object not found on source. presumably deleted later in oplog\n\nAt this point, the data should exactly match the dataset as it existed at some point on the primary. The member finishes the initial sync process and transitions to normal syncing, which allows it to become a secondary.\n\nDoing an initial sync is very easy from an operator’s perspective: just start up a mon‐ god with a clean data directory. However, it is often preferable to restore from a backup instead, as covered in Chapter 23. Restoring from a backup is often faster than copying all of your data through mongod.\n\nAlso, cloning can ruin the sync source’s working set. Many deployments end up with a subset of their data that’s frequently accessed and always in memory (because the OS is accessing it often). Performing an initial sync forces the member to page all of its data into memory, evicting the frequently used data. This can slow down a mem‐ ber dramatically as requests that were being handled by data in RAM are suddenly forced to go to disk. However, for small datasets and servers with some breathing room, initial syncing is a good, easy option.\n\nOne of the most common issues people run into with initial sync is it taking too long. In these cases, the new member can “fall off” the end of sync source’s oplog: it gets so far behind the sync source that it can no longer catch up because the sync source’s oplog has overwritten the data the member would need to use to continue replicating.\n\nThere is no way to fix this other than attempting the initial sync at a less busy time or restoring from a backup. The initial sync cannot proceed if the member has fallen off of the sync source’s oplog. “Handling Staleness” on page 253 covers this in more depth.\n\n252\n\n|\n\nChapter 11: Components of a Replica Set",
      "content_length": 2822,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "Replication The second type of synchronization MongoDB performs is replication. Secondary members replicate data continuously after the initial sync. They copy the oplog from their sync source and apply these operations in an asynchronous process. Secondaries may automatically change their sync-from source as needed, in response to changes in the ping time and the state of other members’ replication. There are several rules that govern which members a given node can sync from. For example, replica set members with one vote cannot sync from members with zero votes, and secondaries avoid syncing from delayed members and hidden members. Elections and different classes of replica set members are discussed in later sections.\n\nHandling Staleness If a secondary falls too far behind the actual operations being performed on the sync source, the secondary will go stale. A stale secondary is unable to catch up because every operation in the sync source’s oplog is too far ahead: it would be skipping oper‐ ations if it continued to sync. This could happen if the secondary has had downtime, has more writes than it can handle, or is too busy handling reads.\n\nWhen a secondary goes stale, it will attempt to replicate from each member of the set in turn to see if there’s anyone with a longer oplog that it can bootstrap from. If there is no one with a long-enough oplog, replication on that member will halt and it will need to be fully resynced (or restored from a more recent backup).\n\nTo avoid out-of-sync secondaries, it’s important to have a large oplog so that the pri‐ mary can store a long history of operations. A larger oplog will obviously use more disk space, but in general this is a good tradeoff to make because disk space tends to be cheap and little of the oplog is typically in use, so it doesn’t take up much RAM. A general rule of thumb is that the oplog should provide coverage (replication window) for two to three days’ worth of normal operations. For more information on sizing the oplog, see “Resizing the Oplog” on page 282.\n\nHeartbeats Members need to know about the other members’ states: who’s primary, who they can sync from, and who’s down. To keep an up-to-date view of the set, a member sends out a heartbeat request to every other member of the set every two seconds. A heart‐ beat request is a short message that checks everyone’s state.\n\nOne of the most important functions of heartbeats is to let the primary know if it can reach a majority of the set. If a primary can no longer reach a majority of the servers, it will demote itself and become a secondary (see “How to Design a Set” on page 241).\n\nHeartbeats\n\n|\n\n253",
      "content_length": 2658,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "Member States Members also communicate what state they are in via heartbeats. We’ve already dis‐ cussed two states: primary and secondary. There are several other normal states that you’ll often see members be in:\n\nSTARTUP\n\nThis is the state a member is in when it’s first started, while MongoDB is attempt‐ ing to load its replica set configuration. Once the configuration has been loaded, it transitions to STARTUP2.\n\nSTARTUP2\n\nThis state lasts throughout the initial sync process, which typically takes just a few seconds. The member forks off a couple of threads to handle replication and elections and then transitions into the next state: RECOVERING.\n\nRECOVERING\n\nThis state indicates that the member is operating correctly but is not available for reads. You may see it in a variety of situations.\n\nOn startup, a member has to make a few checks to make sure it’s in a valid state before accepting reads; therefore, all members go through the RECOVERING state briefly on startup before becoming secondaries. A member can also go into this state during long-running operations such as compacting or in response to the replSetMaintenance command.\n\nA member will also go into the RECOVERING state if it has fallen too far behind the other members to catch up. This is, generally, a failure state that requires resyncing the member. The member does not go into an error state at this point because it lives in hope that someone will come online with a long-enough oplog that it can bootstrap itself back to non-staleness.\n\nARBITER\n\nArbiters (see “Election Arbiters” on page 246) have a special state and should always be in this state during normal operation.\n\nThere are also a few states that indicate a problem with the system. These include:\n\nDOWN\n\nIf a member was up but then becomes unreachable, it will enter this state. Note that a member reported as “down” might, in fact, still be up, just unreachable due to network issues.\n\nUNKNOWN\n\nIf a member has never been able to reach another member, it will not know what state it’s in, so it will report it as UNKNOWN. This generally indicates that the\n\n254\n\n|\n\nChapter 11: Components of a Replica Set",
      "content_length": 2155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "unknown member is down or that there are network problems between the two members.\n\nREMOVED\n\nThis is the state of a member that has been removed from the set. If a removed member is added back into the set, it will transition back into its “normal” state.\n\nROLLBACK\n\nThis state is used when a member is rolling back data, as described in “Rollbacks” on page 255. At the end of the rollback process, a server will transition back into the RECOVERING state and then become a secondary.\n\nElections A member will seek election if it cannot reach a primary (and is itself eligible to become primary). A member seeking election will send out a notice to all of the members it can reach. These members may know why this member is an unsuitable primary: it may be behind in replication or there may already be a primary that the member seeking election cannot reach. In these cases, the other members will vote against the candidate.\n\nAssuming that there is no reason to object, the other members will vote for the mem‐ ber seeking election. If the member seeking election receives votes from a majority of the set, the election was successful and the member will transition into PRIMARY state. If it did not receive a majority if votes, it will remain a secondary and may try to become a primary again later. A primary will remain primary until it cannot reach a majority of members, goes down, or is stepped down, or the set is reconfigured.\n\nAssuming that the network is healthy and a majority of the servers are up, elections should be fast. It will take a member up to two seconds to notice that a primary has gone down (due to the heartbeats mentioned earlier) and it will immediately start an election, which should only take a few milliseconds. However, the situation is often nonoptimal: an election may be triggered due to networking issues or overloaded servers responding too slowly. In these cases, an election might take more time—even up to a few minutes.\n\nRollbacks The election process described in the previous section means that if a primary does a write and goes down before the secondaries have a chance to replicate it, the next pri‐ mary elected may not have the write. For example, suppose we have two data centers, one with the primary and a secondary, and the other with three secondaries, as shown in Figure 11-2.\n\nElections\n\n|\n\n255",
      "content_length": 2351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Figure 11-2. A possible two-data-center configuration\n\nSuppose that there is a network partition between the two data centers, as shown in Figure 11-3. The servers in the first data center are up to operation 126, but that data center hasn’t yet replicated to the servers in the other data center.\n\nFigure 11-3. Replication across data centers can be slower than within a single data center\n\nThe servers in the other data center can still reach a majority of the set (three out of five servers). Thus, one of them may be elected primary. This new primary begins taking its own writes, as shown in Figure 11-4.\n\nFigure 11-4. Unreplicated writes won’t match writes on the other side of a network partition\n\n256\n\n|\n\nChapter 11: Components of a Replica Set",
      "content_length": 752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "When the network is repaired, the servers in the first data center will look for opera‐ tion 126 to start syncing from the other servers, but will not be able to find it. When this happens, A and B will begin a process called rollback. Rollback is used to undo ops that were not replicated before failover. The servers with 126 in their oplogs will look back through the oplogs of the servers in the other data center for a common point. They’ll find that operation 125 is the latest operation that matches. Figure 11-5 shows what the oplogs would look like. A apparently crashed before replicating ops 126−128, so these operations are not present on B, which has more recent operations. A will have to roll back these three operations before resuming syncing.\n\nFigure 11-5. Two members with conflicting oplogs—the last common op was 125, so as B has more recent operations A will need to roll back ops 126-128\n\nAt this point, the server will go through the ops it has and write its version of each document affected by those ops to a .bson file in a rollback directory of your data directory. Thus, if (for example) operation 126 was an update, it will write the docu‐ ment updated by 126 to <collectionName>.bson. Then it will copy the version of that document from the current primary.\n\nThe following is a paste of the log entries generated from a typical rollback:\n\nFri Oct 7 06:30:35 [rsSync] replSet syncing to: server-1 Fri Oct 7 06:30:35 [rsSync] replSet our last op time written: Oct 7 06:30:05:3 Fri Oct 7 06:30:35 [rsSync] replset source's GTE: Oct 7 06:30:31:1 Fri Oct 7 06:30:35 [rsSync] replSet rollback 0 Fri Oct 7 06:30:35 [rsSync] replSet ROLLBACK Fri Oct 7 06:30:35 [rsSync] replSet rollback 1 Fri Oct 7 06:30:35 [rsSync] replSet rollback 2 FindCommonPoint Fri Oct 7 06:30:35 [rsSync] replSet info rollback our last optime: Oct 7 06:30:05:3 Fri Oct 7 06:30:35 [rsSync] replSet info rollback their last optime: Oct 7 06:30:31:2 Fri Oct 7 06:30:35 [rsSync] replSet info rollback diff in end of log times: -26 seconds Fri Oct 7 06:30:35 [rsSync] replSet rollback found matching events at Oct 7 06:30:03:4118 Fri Oct 7 06:30:35 [rsSync] replSet rollback findcommonpoint scanned : 6 Fri Oct 7 06:30:35 [rsSync] replSet replSet rollback 3 fixup Fri Oct 7 06:30:35 [rsSync] replSet rollback 3.5 Fri Oct 7 06:30:35 [rsSync] replSet rollback 4 n:3\n\nRollbacks\n\n|\n\n257",
      "content_length": 2375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "Fri Oct 7 06:30:35 [rsSync] replSet minvalid=Oct 7 06:30:31 4e8ed4c7:2 Fri Oct 7 06:30:35 [rsSync] replSet rollback 4.6 Fri Oct 7 06:30:35 [rsSync] replSet rollback 4.7 Fri Oct 7 06:30:35 [rsSync] replSet rollback 5 d:6 u:0 Fri Oct 7 06:30:35 [rsSync] replSet rollback 6 Fri Oct 7 06:30:35 [rsSync] replSet rollback 7 Fri Oct 7 06:30:35 [rsSync] replSet rollback done Fri Oct 7 06:30:35 [rsSync] replSet RECOVERING Fri Oct 7 06:30:36 [rsSync] replSet syncing to: server-1 Fri Oct 7 06:30:36 [rsSync] replSet SECONDARY\n\nThe server begins syncing from another member (server-1, in this case) and realizes that it cannot find its latest operation on the sync source. At that point, it starts the rollback process by going into the ROLLBACK state (replSet ROLLBACK).\n\nAt step 2, it finds the common point between the two oplogs, which was 26 seconds ago. It then begins undoing the operations from the last 26 seconds from its oplog. Once the rollback is complete, it transitions into the RECOVERING state and begins syncing normally again.\n\nTo apply operations that have been rolled back to the current primary, first use mon‐ gorestore to load them into a temporary collection:\n\n$ mongorestore --db stage --collection stuff \\ /data/db/rollback/important.stuff.2018-12-19T18-27-14.0.bson\n\nThen examine the documents (using the shell) and compare them to the current con‐ tents of the collection from whence they came. For example, if someone had created a “normal” index on the rollback member and a unique index on the current primary, you’d want to make sure that there weren’t any duplicates in the rolled-back data and resolve them if there were.\n\nOnce you have a version of the documents that you like in your staging collection, load it into your main collection:\n\n> staging.stuff.find().forEach(function(doc) { ... prod.stuff.insert(doc); ... })\n\nIf you have any insert-only collections, you can directly load the rollback documents into the collection. However, if you are doing updates on the collection you will need to be more careful about how you merge rollback data.\n\nOne often-misused member configuration option is the number of votes each mem‐ ber has. Manipulating the number of votes is almost always not what you want to do and causes a lot of rollbacks (which is why it was not included in the list of member configuration options in the last chapter). Do not change the number of votes unless you are prepared to deal with regular rollbacks.\n\nFor more information on preventing rollbacks, see Chapter 12.\n\n258\n\n|\n\nChapter 11: Components of a Replica Set",
      "content_length": 2572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "When Rollbacks Fail In older versions of MongoDB, it could decide that the rollback was too large to undertake. Since MongoDB version 4.0, there is no limit on the amount of data that can be rolled back. A rollback in versions before 4.0 can fail if there are more than 300 MB of data or about 30 minutes of operations to roll back. In these cases, you must resync the node that is stuck in rollback.\n\nThe most common cause of this is when secondaries are lagging and the primary goes down. If one of the secondaries becomes primary, it will be missing a lot of oper‐ ations from the old primary. The best way to make sure you don’t get a member stuck in rollback is to keep your secondaries as up to date as possible.\n\nRollbacks\n\n|\n\n259",
      "content_length": 737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "CHAPTER 12 Connecting to a Replica Set from Your Application\n\nThis chapter covers how applications interact with replica sets, including:\n\nHow connections and failovers work\n\nWaiting for replication on writes\n\nRouting reads to the correct member\n\nClient−to−Replica Set Connection Behavior MongoDB client libraries (“drivers” in MongoDB parlance) are designed to manage communication with MongoDB servers, regardless of whether the server is a stand‐ alone MongoDB instance or a replica set. For replica sets, by default, drivers will con‐ nect to the primary and route all traffic to it. Your application can perform reads and writes as though it were talking to a standalone server while your replica set quietly keeps hot standbys ready in the background.\n\nConnections to a replica set are similar to connections to a single server. Use the MongoClient class (or equivalent) in your driver and provide a seed list for the driver to connect to. A seed list is simply a list of server addresses. Seeds are members of the replica set your application will read from and write data to. You do not need to list all members in the seed list (although you can). When the driver connects to the seeds, it will discover the other members from them. A connection string usually looks something like this:\n\n\"mongodb://server-1:27017,server-2:27017,server-3:27017\"\n\nSee your driver’s documentation for details.\n\n261",
      "content_length": 1405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "To provide further resilience, you should also use the DNS Seedlist Connection for‐ mat to specify how your applications connect to your replica set. The advantage to using DNS is that servers hosting your MongoDB replica set members can be changed in rotation without needing to reconfigure the clients (specifically, their con‐ nection strings).\n\nAll MongoDB drivers adhere to the server discovery and monitoring (SDAM) spec. They persistently monitor the topology of your replica set to detect any changes in your application’s ability to reach all members of the set. In addition, the drivers monitor the set to maintain information on which member is the primary.\n\nThe purpose of replica sets is to make your data highly available in the face of net‐ work partitions or servers going down. In ordinary circumstances, replica sets respond gracefully to such problems by electing a new primary so that applications can continue to read and write data. If a primary goes down, the driver will automati‐ cally find the new primary (once one is elected) and will route requests to it as soon as possible. However, while there is no reachable primary, your application will be unable to perform writes.\n\nThere may be no primary available for a brief time (during an election) or for an extended period of time (if no reachable member can become primary). By default, the driver will not service any requests—read or write—during this period. If neces‐ sary to your application, you can configure the driver to use secondaries for read requests.\n\nA common desire is to have the driver hide the entire election process (the primary going away and a new primary being elected) from the user. However, no driver han‐ dles failover this way, for a few reasons. First, a driver can only hide a lack of primary for so long. Second, a driver often finds out that the primary went down because an operation failed, which means that the driver doesn’t know whether or not the pri‐ mary processed the operation before going down. This is a fundamental distributed systems problem that is impossible to avoid, so we need a strategy for dealing with it when it emerges. Should we retry the operation on the new primary, if one is elected quickly? Assume it got through on the old primary? Check and see if the new pri‐ mary has the operation?\n\nThe correct strategy, it turns out, is to retry at most one time. Huh? To explain, let’s consider our options. These boil down to the following: don’t retry, give up after retrying some fixed number of times, or retry at most once. We also need to consider the type of error that could be the source of our problem. There are three types of errors we might see in attempting to write to a replica set: a transient network error, a persistent outage (either network or server), or an error caused by a command the server rejects as incorrect (e.g., not authorized). For each type of error, let’s consider our retry options.\n\n262\n\n|\n\nChapter 12: Connecting to a Replica Set from Your Application",
      "content_length": 3023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "For the sake of this discussion, let’s look at the example of a write to simply increment a counter. If our application attempts to increment our counter but gets no response from the server, we don’t know whether the server received the message and per‐ formed the update. So, if we follow a strategy of not retrying this write, for a transient network error, we might undercount. For a persistent outage or a command error not retrying is the correct strategy, because no amount of retrying the write operation will have the desired effect.\n\nIf we follow a strategy of retrying some fixed number of times, for transient network errors, we might overcount (in the case where our first attempt succeeded). For a per‐ sistent outage or command error, retrying multiple times will simply waste cycles.\n\nLet’s look now at the strategy of retrying just once. For a transient network error, we might overcount. For a persistent outage or command error, this is the correct strat‐ egy. However, what if we could ensure that our operations are idempotent? Idempo‐ tent operations are those that have the same outcome whether we do them once or multiple times. With idempotent operations, retrying network errors once has the best chance of correctly dealing with all three types of errors.\n\nAs of MongoDB 3.6, the server and all MongoDB drivers support a retryable writes option. See your driver’s documentation for details on how to use this option. With retryable writes, the driver will automatically follow the retry-at-most-once strategy. Command errors will be returned to the application for client-side handling. Net‐ work errors will be retried once after an appropriate delay that should accommodate a primary election under ordinary circumstances. With retryable writes turned on, the server maintains a unique identifier for each write operation and can therefore determine when the driver is attempting to retry a command that already succeeded. Rather than apply the write again, it will simply return a message indicating the write succeeded and thereby overcome the problem caused by the transient network issue.\n\nWaiting for Replication on Writes Depending on the needs of your application, you might want to require that all writes are replicated to a majority of the replica set before they are acknowledged by the server. In the rare circumstance where the primary of a set goes down and the newly elected primary (formerly a secondary) did not replicate the very last writes to the former primary, those writes will be rolled back when the former primary comes back up. They can be recovered, but it requires manual intervention. For many applica‐ tions, having a small number of writes rolled back is not a problem. In a blog applica‐ tion, for example, there is little real danger in rolling back one or two comments from one reader.\n\nHowever, for other applications, rollback of any writes should be avoided. Suppose your application sends a write to the primary. It receives confirmation that the write\n\nWaiting for Replication on Writes\n\n|\n\n263",
      "content_length": 3063,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "was written, but the primary crashes before any secondaries have had a chance to replicate that write. Your application thinks that it’ll be able to access that write, but the current members of the replica set don’t have a copy of it.\n\nAt some point, a secondary may be elected primary and start taking new writes. When the former primary comes back up, it will discover that it has writes that the current primary does not. To correct this, it will undo any writes that do not match the sequence of operations on the current primary. These operations are not lost, but they are written to special rollback files that have to be manually applied to the cur‐ rent primary. MongoDB cannot automatically apply these writes, since they may con‐ flict with other writes that have happened since the crash. Thus, the writes essentially disappear until an admin gets a chance to apply the rollback files to the current pri‐ mary (see Chapter 11 for more details on rollbacks).\n\nThe requirement of writing to a majority prevents this situation: if the application gets a confirmation that a write succeeded, then the new primary will have to have a copy of the write to be elected (a member must be up to date to be elected primary). If the application does not receive acknowledgment from the server or receives an error, then it will know to try again, because the write was not propagated to a major‐ ity of the set before the primary crashed.\n\nThus, to ensure that writes will be persisted no matter what happens to the set, we must ensure that each write propagates to a majority of the members of the set. We can achieve this using writeConcern.\n\nAs of MongoDB 2.6, writeConcern is integrated with write operations. For example, in JavaScript, we can use writeConcern as follows:\n\ntry { db.products.insertOne( { \"_id\": 10, \"item\": \"envelopes\", \"qty\": 100, type: \"Self-Sealing\" }, { writeConcern: { \"w\" : \"majority\", \"wtimeout\" : 100 } } ); } catch (e) { print (e); }\n\nThe specific syntax in your driver will vary depending on the programming language, but the semantics remain the same. In the example here, we specify a write concern of \"majority\". Upon success, the server will respond with a message such as the following:\n\n{ \"acknowledged\" : true, \"insertedId\" : 10 }\n\nBut the server will not respond until this write operation has replicated to a majority of the members of the replica set. Only then will our application receive acknowledg‐ ment that this write succeeded. If the write does not succeed within the timeout we’ve specified, the server will respond with an error message:\n\n264\n\n|\n\nChapter 12: Connecting to a Replica Set from Your Application",
      "content_length": 2661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "WriteConcernError({ \"code\" : 64, \"errInfo\" : { \"wtimeout\" : true }, \"errmsg\" : \"waiting for replication timed out\" })\n\nWrite concern majority and the replica set election protocol ensure that in the event of a primary election, only secondaries that are up to date with acknowledged writes can be elected primary. In this way, we guarantee that rollback will not happen. With the timeout option, we also have a tunable setting that enables us to detect and flag any long-running writes at the application layer.\n\nOther Options for “w” \"majority\" is not the only writeConcern option. MongoDB also lets you specify an arbitrary number of servers to replicate to by passing \"w\" a number, as shown here:\n\ndb.products.insertOne( { \"_id\": 10, \"item\": \"envelopes\", \"qty\": 100, type: \"Self-Sealing\" }, { writeConcern: { \"w\" : 2, \"wtimeout\" : 100 } } );\n\nThis will wait until two members (the primary and one secondary) have the write.\n\nNote that the \"w\" value includes the primary. If you want the write propagated to n secondaries, you should set \"w\" to n+1 (to include the primary). Setting \"w\" : 1 is the same as not passing the \"w\" option at all because it just checks that the write was successful on the primary.\n\nThe downside to using a literal number is that you have to change your application if your replica set configuration changes.\n\nCustom Replication Guarantees Writing to a majority of a set is considered “safe.” However, some sets may have more complex requirements: you may want to make sure that a write makes it to at least one server in each data center or a majority of the nonhidden nodes. Replica sets allow you to create custom rules that you can pass to \"getLastError\" to guarantee replication to whatever combination of servers you need.\n\nGuaranteeing One Server per Data Center Network issues between data centers are much more common than within data cen‐ ters, and it is more likely for an entire data center to go dark than an equivalent smat‐ tering of servers across multiple data centers. Thus, you might want some data center −specific logic for writes. Guaranteeing a write to every data center before confirming\n\nCustom Replication Guarantees\n\n|\n\n265",
      "content_length": 2180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "success means that, in the case of a write followed by the data center going offline, every other data center will have at least one local copy.\n\nTo set this up, we first classify the members by data center. We do this by adding a \"tags\" field to their replica set configuration:\n\n> var config = rs.config() > config.members[0].tags = {\"dc\" : \"us-east\"} > config.members[1].tags = {\"dc\" : \"us-east\"} > config.members[2].tags = {\"dc\" : \"us-east\"} > config.members[3].tags = {\"dc\" : \"us-east\"} > config.members[4].tags = {\"dc\" : \"us-west\"} > config.members[5].tags = {\"dc\" : \"us-west\"} > config.members[6].tags = {\"dc\" : \"us-west\"}\n\nThe \"tags\" field is an object, and each member can have multiple tags. It might be a “high quality” server in the \"us-east\" data center, for example, in which case we’d want a \"tags\" field such as {\"dc\": \"us-east\", \"quality\" : \"high\"}.\n\nThe second step is to add a rule by creating a \"getLastErrorModes\" field in our rep‐ lica set config. The name \"getLastErrorModes\" is vestigial in the sense that prior to MongoDB 2.6, applications used a method called \"getLastError\" to specify write concern. In replica configs, for \"getLastErrorModes\" each rule is of the form \"name\" : {\"key\" : number}}. \"name\" is the name for the rule, which should describe what the rule does in a way that clients can understand, as they’ll be using this name when they call getLastError. In this example, we might call this rule \"eachDC\" or something more abstract such as \"user-level safe\".\n\nThe \"key\" field is the key field from the tags, so in this example it will be \"dc\". The number is the number of groups that are needed to fulfill this rule. In this case, number is 2 (because we want at least one server from \"us-east\" and one from \"us-west\"). number always means “at least one server from each of number groups.”\n\nWe add \"getLastErrorModes\" to the replica set config as follows and reconfigure to create the rule:\n\n> config.settings = {} > config.settings.getLastErrorModes = [{\"eachDC\" : {\"dc\" : 2}}] > rs.reconfig(config)\n\n\"getLastErrorModes\" lives in the \"settings\" subobject of a replica set config, which contains a few set-level optional settings.\n\nNow we can use this rule for writes:\n\ndb.products.insertOne( { \"_id\": 10, \"item\": \"envelopes\", \"qty\": 100, type: \"Self-Sealing\" }, { writeConcern: { \"w\" : \"eachDC\", wtimeout : 1000 } } );\n\n266\n\n|\n\nChapter 12: Connecting to a Replica Set from Your Application",
      "content_length": 2430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "Note that rules are somewhat abstracted away from the application developer: they don’t have to know which servers are in \"eachDC\" to use the rule, and the rule can change without their application having to change. We could add a data center or change set members and the application would not have to know.\n\nGuaranteeing a Majority of Nonhidden Members Often, hidden members are somewhat second-class citizens: you’re never going to fail over to them and they certainly aren’t taking any reads. Thus, you may only care that nonhidden members received a write and let the hidden members sort it out for themselves.\n\nSuppose we have five members, host0 through host4, host4 being a hidden member. We want to make sure that a majority of the nonhidden members have a write—that is, at least three of host0, host1, host2, and host3. To create a rule for this, first we tag each of the nonhidden members with its own tag:\n\n> var config = rs.config() > config.members[0].tags = [{\"normal\" : \"A\"}] > config.members[1].tags = [{\"normal\" : \"B\"}] > config.members[2].tags = [{\"normal\" : \"C\"}] > config.members[3].tags = [{\"normal\" : \"D\"}]\n\nThe hidden member, host4, is not given a tag.\n\nNow we add a rule for the majority of these servers:\n\n> config.settings.getLastErrorModes = [{\"visibleMajority\" : {\"normal\" : 3}}] > rs.reconfig(config)\n\nFinally, we can use this rule in our application:\n\ndb.products.insertOne( { \"_id\": 10, \"item\": \"envelopes\", \"qty\": 100, type: \"Self-Sealing\" }, { writeConcern: { \"w\" : \"visibleMajority\", wtimeout : 1000 } } );\n\nThis will wait until at least three of the nonhidden members have the write.\n\nCreating Other Guarantees The rules you can create are limitless. Remember that there are two steps to creating a custom replication rule:\n\n1. Tag members by assigning them key/value pairs. The keys describe classifica‐ tions; for example, you might have keys such as \"data_center\" or \"region\" or \"serverQuality\". Values determine which group a server belongs to within a classification. For example, for the key \"data_center\", you might have some servers tagged \"us-east\", some \"us-west\", and others \"aust\".\n\nCustom Replication Guarantees\n\n|\n\n267",
      "content_length": 2169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "2. Create a rule based on the classifications you create. Rules are always of the form {\"name\" : {\"key\" : number}}, where at least one server from number groups must have a write before it has succeeded. For example, you could create a rule {\"twoDCs\" : {\"data_center\" : 2}}, which would mean that at least one server in two of the data centers tagged must confirm a write before it is successful.\n\nThen you can use this rule in getLastErrorModes.\n\nRules are immensely powerful ways to configure replication, although they are com‐ plex to understand and set up. Unless you have fairly involved replication require‐ ments, you should be perfectly safe sticking with \"w\" : \"majority\".\n\nSending Reads to Secondaries By default, drivers will route all requests to the primary. This is generally what you want, but you can configure other options by setting read preferences in your driver. Read preferences let you specify the types of servers queries should be sent to.\n\nSending read requests to secondaries is generally a bad idea. There are some specific situations in which it makes sense, but you should generally send all traffic to the pri‐ mary. If you are considering sending reads to secondaries, make sure to weigh the pros and cons very carefully before allowing it. This section covers why it’s a bad idea and the specific conditions when it makes sense to do so.\n\nConsistency Considerations Applications that require strongly consistent reads should not read from secondaries.\n\nSecondaries should usually be within a few milliseconds of the primary. However, there is no guarantee of this. Sometimes secondaries can fall behind by minutes, hours, or even days due to load, misconfiguration, network errors, or other issues. Client libraries cannot tell how up to date a secondary is, so clients will cheerfully send queries to secondaries that are far behind. Hiding a secondary from client reads can be done but is a manual process. Thus, if your application needs data that is pre‐ dictably up to date, it should not read from secondaries.\n\nIf your application needs to read its own writes (e.g., insert a document and then query for it and find it) you should not send the read to a secondary (unless the write waits for replication to all secondaries using \"w\" as shown earlier). Otherwise, an application may perform a successful write, attempt to read the value, and not be able to find it (because it sent the read to a secondary that hasn’t replicated yet). Clients can issue requests faster than replication can copy operations.\n\nTo always send read requests to the primary, set your read preference to primary (or leave it alone, since primary is the default). If there is no primary, queries will error\n\n268\n\n|\n\nChapter 12: Connecting to a Replica Set from Your Application",
      "content_length": 2794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "out. This means that your application cannot perform queries if the primary goes down. However, it is certainly an acceptable option if your application can deal with downtime during failovers or network partitions or if getting stale data is unacceptable.\n\nLoad Considerations Many users send reads to secondaries to distribute load. For example, if your servers can only handle 10,000 queries a second and you need to handle 30,000, you might set up a couple of secondaries and have them take some of the load. However, this is a dangerous way to scale because it’s easy to accidentally overload your system and dif‐ ficult to recover from once you do.\n\nFor example, suppose that you have the situation just described: 30,000 reads per sec‐ ond. You decide to create a replica set with four members (one of these would be con‐ figured as nonvoting, to prevent ties in elections) to handle this: each secondary is well below its maximum load and the system works perfectly.\n\nUntil one of the secondaries crashes.\n\nNow each of the remaining members are handling 100% of their possible load. If you need to rebuild the member that crashed, it may need to copy data from one of the other servers, overwhelming the remaining servers. Overloading a server often makes it perform slower, lowering the set’s capacity even further and forcing other members to take on more load, causing them to slow down in a death spiral.\n\nOverloading can also cause replication to slow down, making the remaining seconda‐ ries fall behind. Suddenly you have a member down and a member lagging, and everything is too overloaded to have any wiggle room.\n\nIf you have a good idea of how much load a server can take, you might feel like you can plan this out better: use five servers instead of four and the set won’t be overloa‐ ded if one goes down. However, even if you plan it out perfectly (and only lose the number of servers you expected), you still have to fix the situation with the other servers under more stress than they would be otherwise.\n\nA better choice is to use sharding to distribute load. We’ll cover how to set sharding up in Chapter 14.\n\nReasons to Read from Secondaries There are a few cases in which it’s reasonable to send application reads to secondaries. For instance, you may want your application to still be able to perform reads if the primary goes down (and you do not care if those reads are somewhat stale). This is the most common case for distributing reads to secondaries: you’d like a temporary\n\nSending Reads to Secondaries\n\n|\n\n269",
      "content_length": 2546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "read-only mode when your set loses a primary. This read preference is called primary Preferred.\n\nOne common argument for reading from secondaries is to get low-latency reads. You can specify nearest as your read preference to route requests to the lowest-latency member based on average ping time from the driver to the replica set member. If your application needs to access the same document with low latency in multiple data centers, this is the only way to do it. If, however, your documents are more location- based (application servers in this data center need low-latency access to some of your data, or application servers in another data center need low-latency access to other data), this should be done with sharding. Note that you must use sharding if your application requires low-latency reads and low-latency writes: replica sets only allow writes to one location (wherever the primary is).\n\nYou must be willing to sacrifice consistency if you are reading from members that may not have replicated all the writes yet. Alternatively, you could sacrifice write speed if you wanted to wait until writes had been replicated to all members.\n\nIf your application can truly function acceptably with arbitrarily stale data, you can use the secondary or secondaryPreferred read preferences. secondary will always send read requests to a secondary. If there are no secondaries available, this will error out rather than send reads to the primary. It can be used for applications that do not care about stale data and want to use the primary for writes only. If you have any con‐ cerns about staleness of data, this is not recommended.\n\nsecondaryPreferred will send read requests to a secondary if one is available. If no secondaries are available, requests will be sent to the primary.\n\nSometimes, read load is drastically different than write load—i.e., you’re reading entirely different data than you’re writing. You might want dozens of indexes for off‐ line processing that you don’t want to have on the primary. In this case, you might want to set up a secondary with different indexes than the primary. If you’d like to use a secondary for this purpose, you’d probably create a connection directly to it from the driver, instead of using a replica set connection.\n\nConsider which of the options makes sense for your application. You can also com‐ bine options: if some read requests must be from the primary, use primary for those. If you are OK with other reads not having the most up-to-date data, use primaryPre ferred for those. And if certain requests require low latency over consistency, use nearest for those.\n\n270\n\n|\n\nChapter 12: Connecting to a Replica Set from Your Application",
      "content_length": 2699,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "CHAPTER 13 Administration\n\nThis chapter covers replica set administration, including:\n\nPerforming maintenance on individual members\n\nConfiguring sets under a variety of circumstances\n\nGetting information about and resizing your oplog\n\nDoing some more exotic set configurations\n\nConverting from master/slave to a replica set\n\nStarting Members in Standalone Mode A lot of maintenance tasks cannot be performed on secondaries (because they involve writes) and shouldn’t be performed on primaries because of the impact this could have on application performance. Thus, the following sections frequently mention starting up a server in standalone mode. This means restarting the member so that it is a standalone server, not a member of a replica set (temporarily).\n\nTo start up a member in standalone mode, first look at the command-line options used to start it. Suppose they look something like this:\n\n> db.serverCmdLineOpts() { \"argv\" : [ \"mongod\", \"-f\", \"/var/lib/mongod.conf\" ], \"parsed\" : { \"replSet\": \"mySet\", \"port\": \"27017\", \"dbpath\": \"/var/lib/db\" },\n\n271",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "\"ok\" : 1 }\n\nTo perform maintenance on this server we can restart it without the replSet option. This will allow us to read and write to it as a normal standalone mongod. We don’t want the other servers in the set to be able to contact it, so we’ll make it listen on a different port (so that the other members won’t be able to find it). Finally, we want to keep the dbpath the same, as we are presumably starting it up this way to manipulate the server’s data somehow.\n\nFirst, we shut down the server from the mongo shell:\n\n> db.shutdownServer()\n\nThen, in an operating system shell (e.g., bash), we restart mongod on another port and without the replSet parameter:\n\n$ mongod --port 30000 --dbpath /var/lib/db\n\nIt will now be running as a standalone server, listening on port 30000 for connec‐ tions. The other members of the set will attempt to connect to it on port 27017 and assume that it is down.\n\nWhen we have finished performing maintenance on the server, we can shut it down and restart it with its original options. It will automatically sync up with the rest of the set, replicating any operations that it missed while it was “away.”\n\nReplica Set Configuration Replica set configuration is always kept in a document in the local.system.replset col‐ lection. This document is the same on all members of the set. Never update this docu‐ ment using update. Always use an rs helper or the replSetReconfig command.\n\nCreating a Replica Set You create a replica set by starting up the mongods that you want to be members and then passing one of them a configuration through rs.initiate():\n\n> var config = { ... \"_id\" : <setName>, ... \"members\" : [ ... {\"_id\" : 0, \"host\" : <host1>}, ... {\"_id\" : 1, \"host\" : <host2>}, ... {\"_id\" : 2, \"host\" : <host3>} ... ]} > rs.initiate(config)\n\n272\n\n|\n\nChapter 13: Administration",
      "content_length": 1818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "You should always pass a config object to rs.initiate(). If you do not, MongoDB will attempt to automatically generate a config for a one-member replica set; it might not use the hostname that you want or correctly configure the set.\n\nYou only call rs.initiate() on one member of the set. The member that receives the configuration will pass it on to the other members.\n\nChanging Set Members When you add a new set member, it should either have nothing in its data directory —in which case it will perform an initial sync—or have a copy of the data from another member (see Chapter 23 for more information about backing up and restor‐ ing replica set members).\n\nConnect to the primary and add a new member as follows:\n\n> rs.add(\"spock:27017\")\n\nAlternatively, you can specify a more complex member config as a document:\n\n> rs.add({\"host\" : \"spock:27017\", \"priority\" : 0, \"hidden\" : true})\n\nYou can also remove members by their \"host\" field:\n\n> rs.remove(\"spock:27017\")\n\nYou can change a member’s settings by reconfiguring. There are a few restrictions in changing a member’s settings:\n\nYou cannot change a member’s \"_id\".\n\nYou cannot make the member you’re sending the reconfig to (generally the pri‐ mary) priority 0.\n\nYou cannot turn an arbiter into a nonarbiter, or vice versa. • You cannot change a member’s \"buildIndexes\" field from false to true.\n\nNotably, you can change a member’s \"host\" field. Thus, if you incorrectly specify a host (say, if you use a public IP instead of a private one) you can later go back and simply change the config to use the correct IP.\n\nTo change a hostname, you could do something like this:\n\n> var config = rs.config() > config.members[0].host = \"spock:27017\" spock:27017 > rs.reconfig(config)\n\nReplica Set Configuration\n\n|\n\n273",
      "content_length": 1765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "This same strategy applies to changing any other option: fetch the config with rs.con fig(), modify any parts of it that you wish, and reconfigure the set by passing rs.reconfig() the new configuration.\n\nCreating Larger Sets Replica sets are limited to 50 members in total and only 7 voting members. This is to reduce the amount of network traffic required for everyone to heartbeat everyone else and to limit the amount of time elections take.\n\nIf you are creating a replica set that has more than seven members, every additional member must be given zero votes. You can do this by specifying it in the member’s config:\n\n> rs.add({\"_id\" : 7, \"host\" : \"server-7:27017\", \"votes\" : 0})\n\nThis prevents these members from casting positive votes in elections.\n\nForcing Reconfiguration When you permanently lose a majority of a set, you may want to reconfigure the set while it doesn’t have a primary. This is a little tricky, as usually you’d send the reconfig to the primary. In this case, you can force-reconfigure the set by sending a reconfig command to a secondary. Connect to a secondary in the shell and pass it a reconfig with the \"force\" option:\n\n> rs.reconfig(config, {\"force\" : true})\n\nForced reconfigurations follow the same rules as a normal reconfiguration: you must send a valid, well-formed configuration with the correct options. The \"force\" option doesn’t allow invalid configs; it just allows a secondary to accept a reconfig.\n\nForced reconfigurations bump the replica set \"version\" number by a large amount. You may see it jump by tens or hundreds of thousands. This is normal: it is to prevent version number collisions (just in case there’s a reconfig on either side of a network partition).\n\nWhen the secondary receives the reconfig, it will update its configuration and pass the new config along to the other members. The other members of the set will only pick up on a change of config if they recognize the sending server as a member of their current config. Thus, if some of your members have changed hostnames, you should force reconfig from a member that kept its old hostname. If every member has a new hostname, you should shut down each member of the set, start a new one up in standalone mode, change its local.system.replset document manually, and then restart the member.\n\n274\n\n|\n\nChapter 13: Administration",
      "content_length": 2337,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "Manipulating Member State There are several ways to manually change a member’s state for maintenance or in response to load. Note that there is no way to force a member to become primary, however, other than configuring the set appropriately—in this case, by giving the rep‐ lica set member a priority higher than any other member of the set.\n\nTurning Primaries into Secondaries You can demote a primary to a secondary using the stepDown function:\n\n> rs.stepDown()\n\nThis makes the primary step down into SECONDARY state for 60 seconds. If no other primary is elected in that time period, it will be able to attempt a reelection. If you would like it to remain a secondary for a longer or shorter amount of time, you can specify your own number of seconds for it to stay in SECONDARY state:\n\n> rs.stepDown(600) // 10 minutes\n\nPreventing Elections If you need to do some maintenance on the primary but don’t want any of the other eligible members to become primary in the interim, you can force them to stay sec‐ ondaries by running freeze on each of them:\n\n> rs.freeze(10000)\n\nAgain, this takes a number of seconds for the member to remain a secondary.\n\nIf you finish whatever maintenance you’re doing on the primary before this time elapses and want to unfreeze the other members, simply run the command again on each of them, giving a timeout of 0 seconds:\n\n> rs.freeze(0)\n\nAn unfrozen member will be able to hold an election, if it chooses.\n\nYou can also unfreeze primaries that have been stepped down by running rs.freeze(0).\n\nMonitoring Replication It is important to be able to monitor the status of a set: not only that all members are up, but what states they are in and how up to date the replication is. There are several commands you can use to see replica set information. MongoDB hosting services and management tools including Atlas, Cloud Manager, and Ops Manager (see Chap‐\n\nManipulating Member State\n\n|\n\n275",
      "content_length": 1923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "ter 22) also provide mechanisms to monitor replication and dashboards on the key replication metrics.\n\nOften issues with replication are transient: a server could not reach another server, but now it can. The easiest way to see issues like this is to look at the logs. Make sure you know where the logs are being stored (and that they are being stored) and that you can access them.\n\nGetting the Status One of the most useful commands you can run is replSetGetStatus, which gets the current information about every member of the set (from the view of the member you’re running it on). There is a helper for this command in the shell:\n\n> rs.status()\n\n\"set\" : \"replset\", \"date\" : ISODate(\"2019-11-02T20:02:16.543Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : { \"lastCommittedOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"readConcernMajorityOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"appliedOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) } },\n\n\"members\" : [ { \"_id\" : 0, \"name\" : \"m1.example.net:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 269, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1)\n\n276\n\n|\n\nChapter 13: Administration",
      "content_length": 1367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "}, \"optimeDate\" : ISODate(\"2019-11-02T20:02:14Z\"), \"infoMessage\" : \"could not find member to sync from\", \"electionTime\" : Timestamp(1478116933, 1), \"electionDate\" : ISODate(\"2019-11-02T20:02:13Z\"), \"configVersion\" : 1, \"self\" : true }, { \"_id\" : 1, \"name\" : \"m2.example.net:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 14, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2019-11-02T20:02:14Z\"), \"optimeDurableDate\" : ISODate(\"2019-11-02T20:02:14Z\"), \"lastHeartbeat\" : ISODate(\"2019-11-02T20:02:15.618Z\"), \"lastHeartbeatRecv\" : ISODate(\"2019-11-02T20:02:14.866Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"m3.example.net:27017\", \"configVersion\" : 1 }, { \"_id\" : 2, \"name\" : \"m3.example.net:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 14, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2019-11-02T20:02:14Z\"), \"optimeDurableDate\" : ISODate(\"2019-11-02T20:02:14Z\"), \"lastHeartbeat\" : ISODate(\"2019-11-02T20:02:15.619Z\"), \"lastHeartbeatRecv\" : ISODate(\"2019-11-02T20:02:14.787Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"m1.example.net:27018\",\n\nMonitoring Replication\n\n|\n\n277",
      "content_length": 1370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "\"configVersion\" : 1 } ], \"ok\" : 1 }\n\nThese are some of the most useful fields:\n\n\"self\"\n\nThis field is only present in the member rs.status() was run on—in this case, server-2 (m1.example.net:27017).\n\n\"stateStr\"\n\nA string describing the state of the server. See “Member States” on page 254 for descriptions of the various states.\n\n\"uptime\"\n\nThe number of seconds a member has been reachable, or the time since this server was started for the \"self\" member. Thus, server-1 has been up for 269 sec‐ onds, and server-2 and server-3 for 14 seconds.\n\n\"optimeDate\"\n\nThe last optime in each member’s oplog (where that member is synced to). Note that this is the state of each member as reported by the heartbeat, so the optime reported here may be off by a couple of seconds.\n\n\"lastHeartbeat\"\n\nThe time this server last received a heartbeat from the \"self\" member. If there have been network issues or the server has been busy, this may be longer than two seconds ago.\n\n\"pingMs\"\n\nThe running average of how long heartbeats to this server have taken. This is used in determining which member to sync from.\n\n\"errmsg\"\n\nAny status message that the member chose to return in the heartbeat request. These are often merely informational, not error messages. For example, the \"errmsg\" field in server-3 indicates that this server is in the process of initial syncing. The hexadecimal number 507e9a30:851 is the timestamp of the opera‐ tion this member needs to get to to complete the initial sync.\n\nThere are several fields that give overlapping information. \"state\" is the same as \"stateStr\"; it’s simply the internal ID for the state. \"health\" merely reflects whether a given server is reachable (1) or unreachable (0), which is also shown by \"state\" and \"stateStr\" (they’ll be UNKNOWN or DOWN if the server is unreachable). Similarly, \"optime\" and \"optimeDate\" are the same value represented in two ways: one\n\n278\n\n|\n\nChapter 13: Administration",
      "content_length": 1931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "represents milliseconds since the epoch (\"t\" : 135...) and the other is a more human-readable date.\n\nNote that this report is from the point of view of whichever mem‐ ber of the set you run it on: the information it contains may be incorrect or out of date due to network issues.\n\nVisualizing the Replication Graph If you run rs.status() on a secondary, there will be a top-level field called \"syncingTo\". This gives the host that this member is replicating from. By running the replSetGetStatus command on each member of the set, you can figure out the replication graph. For example, assuming server1 was a connection to server1, server2 was a connection to server2, and so on, you might have something like:\n\n> server1.adminCommand({replSetGetStatus: 1})['syncingTo'] server0:27017 > server2.adminCommand({replSetGetStatus: 1})['syncingTo'] server1:27017 > server3.adminCommand({replSetGetStatus: 1})['syncingTo'] server1:27017 > server4.adminCommand({replSetGetStatus: 1})['syncingTo'] server2:27017\n\nThus, server0 is the replication source for server1, server1 is the replication source for server2 and server3, and server2 is the replication source for server4.\n\nMongoDB determines who to sync to based on ping time. When one member heart‐ beats another, it times how long that request takes. MongoDB keeps a running aver‐ age of these times. When a member has to choose another member to sync from, it looks for the one that is closest to it and ahead of it in replication (thus, you cannot end up with a replication cycle: members will only replicate from the primary or sec‐ ondaries that are further ahead).\n\nThis means that if you bring up a new member in a secondary data center, it is more likely to sync from another member in that data center than a member in your pri‐ mary data center (thus minimizing WAN traffic), as shown in Figure 13-1.\n\nHowever, there is a downside to automatic replication chaining: more replication hops means that it takes a bit longer to replicate writes to all servers. For example, let’s say that everything is in one data center but, due to the vagaries of network speeds when you added members, MongoDB ends up replicating in a line, as shown in Figure 13-2.\n\nMonitoring Replication\n\n|\n\n279",
      "content_length": 2237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "Figure 13-1. New secondaries will generally choose to sync from a member in the same data center\n\nFigure 13-2. As replication chains get longer, it takes longer for all members to get a copy of the data\n\nThis is highly unlikely, but not impossible. It is, however, probably undesirable: each secondary in the chain will have to be a bit further behind than the secondary “in front” of it. You can fix this by modifying the replication source for a member using the replSetSyncFrom command (or the rs.syncFrom() helper).\n\nConnect to the secondary whose replication source you want to change and run this command, passing it the server you’d prefer this member to sync from:\n\n> secondary.adminCommand({\"replSetSyncFrom\" : \"server0:27017\"})\n\nIt may take a few seconds to switch sync sources, but if you run rs.status() on that member again, you should see that the \"syncingTo\" field now says \"server0:27017\".\n\nThis member (server4) will now continue replicating from server0 until server0 becomes unavailable or, if it happened to be a secondary, falls significantly behind the other members.\n\nReplication Loops A replication loop is when members end up replicating from one another—for exam‐ ple, A is syncing from B who is syncing from C who is syncing from A. As none of the members in a replication loop can be a primary, the members will not receive any new operations to replicate and will fall behind.\n\n280\n\n|\n\nChapter 13: Administration",
      "content_length": 1441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "Replication loops should be impossible when members choose who to sync from automatically. However, you can force replication loops using the replSetSyncFrom command. Inspect the rs.status() output carefully before manually changing sync targets, and be careful not to create loops. The replSetSyncFrom command will warn you if you do not choose to sync from a member that is strictly ahead, but it will allow it.\n\nDisabling Chaining Chaining is when a secondary syncs from another secondary (instead of the primary). As mentioned earlier, members may decide to sync from other members automati‐ cally. You can disable chaining, forcing everyone to sync from the primary, by chang‐ ing the \"chainingAllowed\" setting to false (if not specified, it defaults to true):\n\n> var config = rs.config() > // create the settings subobject, if it does not already exist > config.settings = config.settings || {} > config.settings.chainingAllowed = false > rs.reconfig(config)\n\nWith \"chainingAllowed\" set to false, all members will sync from the primary. If the primary becomes unavailable, they will fall back to syncing from secondaries.\n\nCalculating Lag One of the most important metrics to track for replication is how well the secondaries are keeping up with the primary. Lag is how far behind a secondary is, which means the difference between the timestamp of the last operation the primary has per‐ formed and the timestamp of the last operation the secondary has applied.\n\nYou can use rs.status() to see a member’s replication state, but you can also get a quick summary by running rs.printReplicationInfo() or rs.printSlaveReplica tionInfo().\n\nrs.printReplicationInfo() gives a summary of the primary’s oplog, including its size and the date range of its operations:\n\n> rs.printReplicationInfo(); configured oplog size: 10.48576MB log length start to end: 3590 secs (1.00hrs) oplog first event time: Tue Apr 10 2018 09:27:57 GMT-0400 (EDT) oplog last event time: Tue Apr 10 2018 10:27:47 GMT-0400 (EDT) now: Tue Apr 10 2018 10:27:47 GMT-0400 (EDT)\n\nIn this example, the oplog is about 10 MB (10 MiB) and is only able to fit about an hour of operations.\n\nMonitoring Replication\n\n|\n\n281",
      "content_length": 2182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "If this were a real deployment, the oplog should probably be larger (see the next sec‐ tion for instructions on changing oplog size). We want the log length to be at least as long as the time it takes to do a full resync. That way, we don’t run into a case where a secondary falls off the end of the oplog before finishing its initial sync.\n\nThe log length is computed by taking the time difference between the first and last operation in the oplog once the oplog has filled up. If the server has just started with nothing in the oplog, then the earliest operation will be relatively recent. In that case, the log length will be small, even though the oplog probably still has free space available. The length is a more useful metric for servers that have been operating long enough to write through their entire oplog at least once.\n\nYou can also use the rs.printSlaveReplicationInfo() function to get the syncedTo value for each member and the time when the last oplog entry was written to each secondary, as shown in the following example:\n\n> rs.printSlaveReplicationInfo(); source: m1.example.net:27017 syncedTo: Tue Apr 10 2018 10:27:47 GMT-0400 (EDT) 0 secs (0 hrs) behind the primary source: m2.example.net:27017 syncedTo: Tue Apr 10 2018 10:27:43 GMT-0400 (EDT) 0 secs (0 hrs) behind the primary source: m3.example.net:27017 syncedTo: Tue Apr 10 2018 10:27:39 GMT-0400 (EDT) 0 secs (0 hrs) behind the primary\n\nRemember that a replica set member’s lag is calculated relative to the primary, not against “wall time.” This usually is irrelevant, but on very low-write systems, this can cause phantom replication lag “spikes.” For example, suppose you do a write once an hour. Right after that write, before it’s replicated, the secondary will look like it’s an hour behind the primary. However, it’ll be able to catch up with that “hour” of opera‐ tions in a few milliseconds. This can sometimes cause confusion when monitoring a low-throughput system.\n\nResizing the Oplog Your primary’s oplog should be thought of as your maintenance window. If your pri‐ mary has an oplog that is an hour long, then you only have one hour to fix anything that goes wrong before your secondaries fall too far behind and must be resynced from scratch. Thus, you generally want to have an oplog that can hold a couple days’ to a week’s worth of data, to give yourself some breathing room if something goes wrong.\n\n282\n\n|\n\nChapter 13: Administration",
      "content_length": 2435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "Unfortunately, there’s no easy way to tell how long your oplog is going to be before it fills up. The WiredTiger storage engine allows online resizing of your oplog while your server is running. You should perform these steps on each secondary replica set member first; once these have been changed, then and only then should you make the changes to your primary. Remember that each server that could become a primary should have a large enough oplog to give you a sane maintenance window.\n\nTo increase the size of your oplog, perform the following steps:\n\n1. Connect to the replica set member. If authentication is enabled, be sure to use a user with privileges that can modify the local database.\n\n2. Verify the current size of the oplog:\n\n> use local > db.oplog.rs.stats(1024*1024).maxSize\n\nThis will display the collection size in megabytes.\n\n3. Change the oplog size of the replica set member:\n\n> db.adminCommand({replSetResizeOplog: 1, size: 16000})\n\nThe following operation changes the oplog size of the replica set member to 16 gigabytes, or 16000 megabytes.\n\n4. Finally, if you have reduced the size of the oplog, you may need to run the com pact to reclaim the disk space allocated. This should not be run against a mem‐ ber while it is a primary. Please see the “Change the Size of the Oplog” tutorial in the MongoDB documentation for more details on this case and on the entire pro‐ cedure.\n\nYou generally should not decrease the size of your oplog: although it may be months long, there is usually ample disk space for it and it does not use up any valuable resources like RAM or CPU.\n\nBuilding Indexes If you send an index build to the primary, the primary will build the index normally and then the secondaries will build the index when they replicate the “build index”\n\nMonitoring Replication\n\n|\n\n283",
      "content_length": 1816,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "operation. Although this is the easiest way to build an index, index builds are resource-intensive operations that can make members unavailable. If all of your sec‐ ondaries start building an index at the same time, almost every member of your set will be offline until the index build completes. This process is only for replica sets; for a sharded cluster, please see the MongoDB documentation tutorial about building indexes on a sharded cluster.\n\nYou must stop all writes to a collection when you are creating a \"unique\" index. If the writes are not stopped, you can end up with inconsistent data across the replica set members.\n\nTherefore, you may want to build an index on one member at a time to minimize the impact on your application. To accomplish this, do the following:\n\n1. Shut down a secondary.\n\n2. Restart it as a standalone server.\n\n3. Build the index on the standalone server.\n\n4. When the index build is complete, restart the server as a member of the replica set. When restarting this member, you need to remove the disableLogicalSes sionCacheRefresh parameter if it is present in your command-line options or configuration file.\n\n5. Repeat steps 1 through 4 for each secondary in the replica set.\n\nYou should now have a set where every member other than the primary has the index built. Now there are two options, and you should choose the one that will impact your production system the least:\n\n1. Build the index on the primary. If you have an “off” time when you have less traf‐ fic, that would probably be a good time to build it. You also might want to modify read preferences to temporarily shunt more load onto secondaries while the build is in progress. The primary will replicate the index build to the secondaries, but they will already have the index so it will be a no-op for them.\n\n2. Step down the primary, then follow steps 2 through 4 of the procedure outlined previously. This requires a failover, but you will have a normally functioning pri‐ mary while the old primary is building its index. After its index build is com‐ plete, you can reintroduce it to the set.\n\nNote that you could also use this technique to build different indexes on a secondary than you have on the rest of the set. This could be useful for offline processing, but\n\n284\n\n|\n\nChapter 13: Administration",
      "content_length": 2312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "make sure a member with different indexes can never become primary: its priority should always be 0.\n\nIf you are building a unique index, make sure that the primary is not inserting dupli‐ cates or that you build the index on the primary first. Otherwise, the primary could be inserting duplicates that would then cause replication errors on secondaries. If this occurs, the secondary will shut itself down. You will have to restart it as a standalone server, remove the unique index, and restart it.\n\nReplication on a Budget If it is difficult to get more than one high-quality server, consider getting a secondary server that is strictly for disaster recovery, with less RAM and CPU, slower disk I/O, etc. The good server will always be your primary and the cheaper server will never handle any client traffic (configure your clients to send all reads to the primary). Here are the options to set for the cheaper box:\n\n\"priority\" : 0\n\nYou do not want this server to ever become primary.\n\n\"hidden\" : true\n\nYou do not want clients ever sending reads to this secondary.\n\n\"buildIndexes\" : false\n\nThis is optional, but it can decrease the load this server has to handle considera‐ bly. If you ever need to restore from this server, you’ll need to rebuild the indexes.\n\n\"votes\" : 0\n\nIf you only have two machines, set \"votes\" on this secondary to 0 so that the primary can stay primary if this machine goes down. If you have a third server (even just your application server), run an arbiter on that instead of setting \"votes\" to 0.\n\nThis will give you the safety and security of having a secondary without having to invest in two high-performance servers.\n\nMonitoring Replication\n\n|\n\n285",
      "content_length": 1684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "PART IV Sharding",
      "content_length": 16,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "CHAPTER 14 Introduction to Sharding\n\nThis chapter covers how to scale with MongoDB. We’ll look at:\n\nWhat sharding is and the components of a cluster\n\nHow to configure sharding\n\nThe basics of how sharding interacts with your application\n\nWhat Is Sharding? Sharding refers to the process of splitting data up across machines; the term partition‐ ing is also sometimes used to describe this concept. By putting a subset of data on each machine, it becomes possible to store more data and handle more load without requiring larger or more powerful machines—just a larger quantity of less-powerful machines. Sharding may be used for other purposes as well, including placing more frequently accessed data on more performant hardware or splitting a dataset based on geography to locate a subset of documents in a collection (e.g., for users based in a particular locale) close to the application servers from which they are most com‐ monly accessed.\n\nManual sharding can be done with almost any database software. With this approach, an application maintains connections to several different database servers, each of which are completely independent. The application manages storing different data on different servers and querying against the appropriate server to get data back. This setup can work well but becomes difficult to maintain when adding or removing nodes from the cluster or in the face of changing data distributions or load patterns.\n\nMongoDB supports autosharding, which tries to both abstract the architecture away from the application and simplify the administration of such a system. MongoDB\n\n289",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "allows your application to ignore the fact that it isn’t talking to a standalone Mon‐ goDB server, to some extent. On the operations side, MongoDB automates balancing data across shards and makes it easier to add and remove capacity.\n\nSharding is the most complex way of configuring MongoDB, both from a develop‐ ment and an operational point of view. There are many components to configure and monitor, and data moves around the cluster automatically. You should be comfortable with standalone servers and replica sets before attempting to deploy or use a sharded cluster. Also, as with replica sets, the recommended means of configuring and deploying sharded clusters is through either MongoDB Ops Manager or MongoDB Atlas. Ops Manager is recommended if you need to maintain control of your com‐ puting infrastructure. MongoDB Atlas is recommended if you can leave the infra‐ structure management to MongoDB (you have the option of running in Amazon AWS, Microsoft Azure, or Google Compute Cloud).\n\nUnderstanding the Components of a Cluster MongoDB’s sharding allows you to create a cluster of many machines (shards) and break up a collection across them, putting a subset of data on each shard. This allows your application to grow beyond the resource limits of a standalone server or replica set.\n\nMany people are confused about the difference between replication and sharding. Remember that replication creates an exact copy of your data on multiple servers, so every server is a mirror image of every other server. Conversely, every shard contains a different subset of data.\n\nOne of the goals of sharding is to make a cluster of 2, 3, 10, or even hundreds of shards look like a single machine to your application. To hide these details from the application, we run one or more routing processes called a mongos in front of the shards. A mongos keeps a “table of contents” that tells it which shard contains which data. Applications can connect to this router and issue requests normally, as shown in Figure 14-1. The router, knowing what data is on which shard, is able to forward the requests to the appropriate shard(s). If there are responses to a request the router col‐ lects them and, if necessary, merges them, and sends them back to the application. As far as the application knows, it’s connected to a standalone mongod, as illustrated in Figure 14-2.\n\n290\n\n|\n\nChapter 14: Introduction to Sharding",
      "content_length": 2413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "Figure 14-1. Sharded client connection\n\nFigure 14-2. Nonsharded client connection\n\nSharding on a Single-Machine Cluster We’ll start by setting up a quick cluster on a single machine. First, start a mongo shell with the --nodb and --norc options:\n\n$ mongo --nodb --norc\n\nTo create a cluster, use the ShardingTest class. Run the following in the mongo shell you just launched:\n\nst = ShardingTest({ name:\"one-min-shards\", chunkSize:1, shards:2, rs:{ nodes:3, oplogSize:10 }, other:{ enableBalancer:true } });\n\nThe chunksize option is covered in Chapter 17. For now, simply set it to 1. As for the other options passed to ShardingTest here, name simply provides a label for our\n\nSharding on a Single-Machine Cluster\n\n|\n\n291",
      "content_length": 719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "sharded cluster, shards specifies that our cluster will be composed of two shards (we do this to keep the resource requirements low for this example), and rs defines each shard as a three-node replica set with an oplogSize of 10 MiB (again, to keep resource shard as a three-node replica set with an oplogSize of 10 MiB (again, to keep resource utilization low). Though it is possible to run one standalone mongod for each shard, it paints a clearer picture of the typical architecture of a sharded clus‐ ter if we create each shard as a replica set. In the last option specified, we are instruct‐ ing ShardingTest to enable the balancer once the cluster is spun up. This will ensure that data is evenly distributed across both shards.\n\nShardingTest is a class designed for internal use by MongoDB Engineering and is therefore undocumented externally. However, because it ships with the MongoDB server, it provides the most straightforward means of experimenting with a sharded cluster. ShardingTest was originally designed to support server test suites and is still used for this purpose. By default it provides a number of conveniences that help in keeping resource utilization as small as possible and in setting up the relatively com‐ plex architecture of a sharded cluster. It makes an assumption about the presence of a /data /db directory on your machine; if ShardingTest fails to run then create this directory and rerun the command again.\n\nWhen you run this command, ShardingTest will do a lot for you automatically. It will create a new cluster with two shards, each of which is a replica set. It will config‐ ure the replica sets and launch each node with the necessary options to establish rep‐ lication protocols. It will launch a mongos to manage requests across the shards so that clients can interact with the cluster as if communicating with a standalone mon‐ god, to some extent. Finally, it will launch an additional replica set for the config servers that maintain the routing table information necessary to ensure queries are directed to the correct shard. Remember that the primary use cases for sharding are to split a dataset to address hardware and cost constraints or to provide better perfor‐ mance to applications (e.g., geographical partitioning). MongoDB sharding provides these capabilities in a way that is seamless to the application in many respects.\n\nOnce ShardingTest has finished setting up your cluster, you will have 10 processes up and running to which you can connect: two replica sets of three nodes each, one config server replica set of three nodes, and one mongos. By default, these processes should begin at port 20000. The mongos should be running at port 20009. Other pro‐ cesses you have running on your local machine and previous calls to ShardingTest can have an effect on which ports ShardingTest uses, but you should not have too much difficulty determining the ports on which your cluster processes are running.\n\nNext, you’ll connect to the mongos to play around with the cluster. Your entire cluster will be dumping its logs to your current shell, so open up a second terminal window and launch another mongo shell:\n\n$ mongo --nodb\n\n292\n\n|\n\nChapter 14: Introduction to Sharding",
      "content_length": 3232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "Use this shell to connect to your cluster’s mongos. Again, your mongos should be run‐ ning on port 20009:\n\n> db = (new Mongo(\"localhost:20009\")).getDB(\"accounts\")\n\nNote that the prompt in your mongo shell should change to reflect that you are con‐ nected to a mongos. Now you are in the situation shown earlier, in Figure 14-1: the shell is the client and is connected to a mongos. You can start passing requests to the mongos and it’ll route them to the shards. You don’t really have to know anything about the shards, like how many there are or what their addresses are. So long as there are some shards out there, you can pass the requests to the mongos and allow it to forward them appropriately.\n\nStart by inserting some data:\n\n> for (var i=0; i<100000; i++) { ... db.users.insert({\"username\" : \"user\"+i, \"created_at\" : new Date()}); ... } > db.users.count() 100000\n\nAs you can see, interacting with mongos works the same way as interacting with a standalone server does.\n\nYou can get an overall view of your cluster by running sh.status(). It will give you a summary of your shards, databases, and collections:\n\n> sh.status() --- Sharding Status --- sharding version: { \"_id\": 1, \"minCompatibleVersion\": 5, \"currentVersion\": 6, \"clusterId\": ObjectId(\"5a4f93d6bcde690005986071\") } shards: { \"_id\" : \"one-min-shards-rs0\", \"host\" : \"one-min-shards-rs0/MBP:20000,MBP:20001,MBP:20002\", \"state\" : 1 } { \"_id\" : \"one-min-shards-rs1\", \"host\" : \"one-min-shards-rs1/MBP:20003,MBP:20004,MBP:20005\", \"state\" : 1 } active mongoses: \"3.6.1\" : 1 autosplit: Currently enabled: no balancer: Currently enabled: no Currently running: no\n\nSharding on a Single-Machine Cluster\n\n|\n\n293",
      "content_length": 1669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: { \"_id\" : \"accounts\", \"primary\" : \"one-min-shards-rs1\", \"partitioned\" : false } { \"_id\" : \"config\", \"primary\" : \"config\", \"partitioned\" : true } config.system.sessions shard key: { \"_id\" : 1 } unique: false balancing: true chunks: one-min-shards-rs0 { \"_id\" : { \"$minKey\" : 1 } } -->> { \"_id\" : { \"$maxKey\" : 1 } } on : one-min-shards-rs0 Timestamp(1, 0)\n\n1\n\nsh is similar to rs, but for sharding: it is a global variable that defines a number of sharding helper functions, which you can see by running sh.help(). As you can see from the sh.status() out‐ put, you have two shards and two databases (config is created automatically).\n\nYour accounts database may have a different primary shard than the one shown here. A primary shard is a “home base” shard that is randomly chosen for each database. All of your data will be on this primary shard. MongoDB cannot automatically dis‐ tribute your data yet because it doesn’t know how (or if) you want it to be distributed. You have to tell it, per collection, how you want it to distribute data.\n\nA primary shard is different from a replica set primary. A primary shard refers to the entire replica set composing a shard. A primary in a replica set is the single server in the set that can take writes.\n\nTo shard a particular collection, first enable sharding on the collection’s database. To do so, run the enableSharding command:\n\n> sh.enableSharding(\"accounts\")\n\nNow sharding is enabled on the accounts database, which allows you to shard collec‐ tions within the database.\n\nWhen you shard a collection, you choose a shard key. This is a field or two that Mon‐ goDB uses to break up data. For example, if you chose to shard on \"username\", Mon‐ goDB would break up the data into ranges of usernames: \"a1-steak-sauce\" through \"defcon\", \"defcon1\" through \"howie1998\", and so on. Choosing a shard key can be\n\n294\n\n|\n\nChapter 14: Introduction to Sharding",
      "content_length": 2017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "thought of as choosing an ordering for the data in the collection. This is a similar concept to indexing, and for good reason: the shard key becomes the most important index on your collection as it gets bigger. To even create a shard key, the field(s) must be indexed.\n\nSo, before enabling sharding, you have to create an index on the key you want to shard by:\n\n> db.users.createIndex({\"username\" : 1})\n\nNow you can shard the collection by \"username\":\n\n> sh.shardCollection(\"accounts.users\", {\"username\" : 1})\n\nAlthough we are choosing a shard key without much thought here, it is an important decision that should be carefully considered in a real system. See Chapter 16 for more advice on choosing a shard key.\n\nIf you wait a few minutes and run sh.status() again, you’ll see that there’s a lot more information displayed than there was before:\n\n> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"5a4f93d6bcde690005986071\") } shards: { \"_id\" : \"one-min-shards-rs0\", \"host\" : \"one-min-shards-rs0/MBP:20000,MBP:20001,MBP:20002\", \"state\" : 1 } { \"_id\" : \"one-min-shards-rs1\", \"host\" : \"one-min-shards-rs1/MBP:20003,MBP:20004,MBP:20005\", \"state\" : 1 } active mongoses: \"3.6.1\" : 1 autosplit: Currently enabled: no balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: 6 : Success databases: { \"_id\" : \"accounts\", \"primary\" : \"one-min-shards-rs1\", \"partitioned\" : true } accounts.users\n\nSharding on a Single-Machine Cluster\n\n|\n\n295",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "shard key: { \"username\" : 1 } unique: false balancing: true chunks: one-min-shards-rs0 6 one-min-shards-rs1 7 { \"username\" : { \"$minKey\" : 1 } } -->> { \"username\" : \"user17256\" } on : one-min-shards-rs0 Timestamp(2, 0) { \"username\" : \"user17256\" } -->> { \"username\" : \"user24515\" } on : one-min-shards-rs0 Timestamp(3, 0) { \"username\" : \"user24515\" } -->> { \"username\" : \"user31775\" } on : one-min-shards-rs0 Timestamp(4, 0) { \"username\" : \"user31775\" } -->> { \"username\" : \"user39034\" } on : one-min-shards-rs0 Timestamp(5, 0) { \"username\" : \"user39034\" } -->> { \"username\" : \"user46294\" } on : one-min-shards-rs0 Timestamp(6, 0) { \"username\" : \"user46294\" } -->> { \"username\" : \"user53553\" } on : one-min-shards-rs0 Timestamp(7, 0) { \"username\" : \"user53553\" } -->> { \"username\" : \"user60812\" } on : one-min-shards-rs1 Timestamp(7, 1) { \"username\" : \"user60812\" } -->> { \"username\" : \"user68072\" } on : one-min-shards-rs1 Timestamp(1, 7) { \"username\" : \"user68072\" } -->> { \"username\" : \"user75331\" } on : one-min-shards-rs1 Timestamp(1, 8) { \"username\" : \"user75331\" } -->> { \"username\" : \"user82591\" } on : one-min-shards-rs1 Timestamp(1, 9) { \"username\" : \"user82591\" } -->> { \"username\" : \"user89851\" } on : one-min-shards-rs1 Timestamp(1, 10) { \"username\" : \"user89851\" } -->> { \"username\" : \"user9711\" } on : one-min-shards-rs1 Timestamp(1, 11) { \"username\" : \"user9711\" } -->> { \"username\" : { \"$maxKey\" : 1 } } on : one-min-shards-rs1 Timestamp(1, 12) { \"_id\" : \"config\", \"primary\" : \"config\", \"partitioned\" : true } config.system.sessions shard key: { \"_id\" : 1 } unique: false balancing: true chunks: one-min-shards-rs0 1 { \"_id\" : { \"$minKey\" : 1 } } -->> { \"_id\" : { \"$maxKey\" : 1 } } on : one-min-shards-rs0 Timestamp(1, 0)\n\nThe collection has been split up into 13 chunks, where each chunk is a subset of your data. These are listed by shard key range (the {\"username\" : minValue} -->> {\"username\" : maxValue} denotes the range of each chunk). Looking at the \"on\" : shard part of the output, you can see that these chunks have been evenly distributed between the shards.\n\nThis process of a collection being split into chunks is shown graphically in Figures 14-3 through 14-5. Before sharding, the collection is essentially a single chunk. Sharding splits it into smaller chunks based on the shard key, as shown in\n\n296\n\n|\n\nChapter 14: Introduction to Sharding",
      "content_length": 2375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "Figure 14-4. These chunks can then be distributed across the cluster, as Figure 14-5 shows.\n\nFigure 14-3. Before a collection is sharded, it can be thought of as a single chunk from the smallest value of the shard key to the largest\n\nFigure 14-4. Sharding splits the collection into many chunks based on shard key ranges\n\nFigure 14-5. Chunks are evenly distributed across the available shards\n\nNotice the keys at the beginning and end of the chunk list: $minKey and $maxKey. $minKey can be thought of as “negative infinity.” It is smaller than any other value in MongoDB. Similarly, $maxKey is like “positive infinity.” It is greater than any other value. Thus, you’ll always see these as the “caps” on your chunk ranges. The values for your shard key will always be between $minKey and $maxKey. These values are actually BSON types and should not be used in your application; they are mainly for internal use. If you wish to refer to them in the shell, use the MinKey and MaxKey constants.\n\nNow that the data is distributed across multiple shards, let’s try doing some queries. First, try a query on a specific username:\n\n> db.users.find({username: \"user12345\"}) { \"_id\" : ObjectId(\"5a4fb11dbb9ce6070f377880\"),\n\nSharding on a Single-Machine Cluster\n\n|\n\n297",
      "content_length": 1257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "\"username\" : \"user12345\", \"created_at\" : ISODate(\"2018-01-05T17:08:45.657Z\") }\n\nAs you can see, querying works normally. However, let’s run an explain to see what MongoDB is doing under the covers:\n\n> db.users.find({username: \"user12345\"}}).explain() { \"queryPlanner\" : { \"mongosPlannerVersion\" : 1, \"winningPlan\" : { \"stage\" : \"SINGLE_SHARD\", \"shards\" : [{ \"shardName\" : \"one-min-shards-rs0\", \"connectionString\" : \"one-min-shards-rs0/MBP:20000,MBP:20001,MBP:20002\", \"serverInfo\" : { \"host\" : \"MBP\", \"port\" : 20000, \"version\" : \"3.6.1\", \"gitVersion\" : \"025d4f4fe61efd1fb6f0005be20cb45a004093d1\" }, \"plannerVersion\" : 1, \"namespace\" : \"accounts.users\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"username\" : { \"$eq\" : \"user12345\" } }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"SHARDING_FILTER\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"username\" : 1 }, \"indexName\" : \"username_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"username\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"username\" : [ \"[\\\"user12345\\\", \\\"user12345\\\"]\"\n\n298\n\n|\n\nChapter 14: Introduction to Sharding",
      "content_length": 1198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "] } } } }, \"rejectedPlans\" : [ ] }] } }, \"ok\" : 1, \"$clusterTime\" : { \"clusterTime\" : Timestamp(1515174248, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } }, \"operationTime\" : Timestamp(1515173700, 201) }\n\nFrom the \"winningPlan\" field in the explain output, we can see that our cluster sat‐ isfied this query using a single shard, one-min-shards-rs0. Based on the output of sh.status() shown earlier, we can see that user12345 does fall within the key range for the first chunk listed for that shard in our cluster.\n\nBecause \"username\" is the shard key, mongos was able to route the query directly to the correct shard. Contrast that with the results for querying for all of the users:\n\n> db.users.find().explain() { \"queryPlanner\":{ \"mongosPlannerVersion\":1, \"winningPlan\":{ \"stage\":\"SHARD_MERGE\", \"shards\":[ { \"shardName\":\"one-min-shards-rs0\", \"connectionString\": \"one-min-shards-rs0/MBP:20000,MBP:20001,MBP:20002\", \"serverInfo\":{ \"host\":\"MBP.fios-router.home\", \"port\":20000, \"version\":\"3.6.1\", \"gitVersion\":\"025d4f4fe61efd1fb6f0005be20cb45a004093d1\" }, \"plannerVersion\":1, \"namespace\":\"accounts.users\", \"indexFilterSet\":false, \"parsedQuery\":{\n\n}, \"winningPlan\":{\n\nSharding on a Single-Machine Cluster\n\n|\n\n299",
      "content_length": 1262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "300\n\n\"stage\":\"SHARDING_FILTER\", \"inputStage\":{ \"stage\":\"COLLSCAN\", \"direction\":\"forward\" } }, \"rejectedPlans\":[\n\n] }, { \"shardName\":\"one-min-shards-rs1\", \"connectionString\": \"one-min-shards-rs1/MBP:20003,MBP:20004,MBP:20005\", \"serverInfo\":{ \"host\":\"MBP.fios-router.home\", \"port\":20003, \"version\":\"3.6.1\", \"gitVersion\":\"025d4f4fe61efd1fb6f0005be20cb45a004093d1\" }, \"plannerVersion\":1, \"namespace\":\"accounts.users\", \"indexFilterSet\":false, \"parsedQuery\":{\n\n}, \"winningPlan\":{ \"stage\":\"SHARDING_FILTER\", \"inputStage\":{ \"stage\":\"COLLSCAN\", \"direction\":\"forward\" } }, \"rejectedPlans\":[\n\n] } ] } }, \"ok\":1, \"$clusterTime\":{ \"clusterTime\":Timestamp(1515174893, 1), \"signature\":{ \"hash\":BinData(0, \"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\":NumberLong(0) } }, \"operationTime\":Timestamp(1515173709, 514) }\n\n|\n\nChapter 14: Introduction to Sharding",
      "content_length": 835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "As you can see from this explain, this query has to visit both shards to find all the data. In general, if we are not using the shard key in the query, mongos will have to send the query to every shard.\n\nQueries that contain the shard key and can be sent to a single shard or a subset of shards are called targeted queries. Queries that must be sent to all shards are called scatter-gather (broadcast) queries: mongos scatters the query to all the shards and then gathers up the results.\n\nOnce you are finished experimenting, shut down the set. Switch back to your original shell and hit Enter a few times to get back to the command line, then run st.stop() to cleanly shut down all of the servers:\n\n> st.stop()\n\nIf you are ever unsure of what an operation will do, it can be helpful to use ShardingTest to spin up a quick local cluster and try it out.\n\nSharding on a Single-Machine Cluster\n\n|\n\n301",
      "content_length": 898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "CHAPTER 15 Configuring Sharding\n\nIn the previous chapter, you set up a “cluster” on one machine. This chapter covers how to set up a more realistic cluster and how each piece fits. In particular, you’ll learn:\n\nHow to set up config servers, shards, and mongos processes\n\nHow to add capacity to a cluster\n\nHow data is stored and distributed\n\nWhen to Shard Deciding when to shard is a balancing act. You generally do not want to shard too early because it adds operational complexity to your deployment and forces you to make design decisions that are difficult to change later. On the other hand, you do not want to wait too long to shard because it is difficult to shard an overloaded sys‐ tem without downtime.\n\nIn general, sharding is used to:\n\nIncrease available RAM\n\nIncrease available disk space\n\nReduce load on a server\n\nRead or write data with greater throughput than a single mongod can handle\n\nThus, good monitoring is important to decide when sharding will be necessary. Care‐ fully measure each of these metrics. Generally people speed toward one of these bot‐ tlenecks much faster than the others, so figure out which one your deployment will\n\n303",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "need to provision for first and make plans well in advance about when and how you plan to convert your replica set.\n\nStarting the Servers The first step in creating a cluster is to start up all of the processes required. As men‐ tioned in the previous chapter, you need to set up the mongos and the shards. There’s also a third component, the config servers, which are an important piece. Config servers are normal mongod servers that store the cluster configuration: which replica sets host the shards, what collections are sharded by, and on which shard each chunk is located. MongoDB 3.2 introduced the use of replica sets as config servers. Replica sets replace the original syncing mechanism used by config servers; the ability to use that mechanism was removed in MongoDB 3.4.\n\nConfig Servers Config servers are the brains of your cluster: they hold all of the metadata about which servers hold what data. Thus, they must be set up first, and the data they hold is extremely important: make sure that they are running with journaling enabled and that their data is stored on nonephemeral drives. In production deployments, your config server replica set should consist of at least three members. Each config server should be on a separate physical machine, preferable geographically distributed.\n\nThe config servers must be started before any of the mongos processes, as mongos pulls its configuration from them. To begin, run the following commands on three separate machines to start your config servers:\n\n$ mongod --configsvr --replSet configRS --bind_ip localhost,198.51.100.51 mongod --dbpath /var/lib/mongodb\n\n$ mongod --configsvr --replSet configRS --bind_ip localhost,198.51.100.52 mongod --dbpath /var/lib/mongodb\n\n$ mongod --configsvr --replSet configRS --bind_ip localhost,198.51.100.53 mongod --dbpath /var/lib/mongodb\n\nThen initiate the config servers as a replica set. To do this, connect a mongo shell to one of the replica set members:\n\n$ mongo --host <hostname> --port <port>\n\nand use the rs.initiate() helper:\n\n> rs.initiate( { _id: \"configRS\", configsvr: true, members: [ { _id : 0, host : \"cfg1.example.net:27019\" },\n\n304\n\n|\n\nChapter 15: Configuring Sharding",
      "content_length": 2184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "{ _id : 1, host : \"cfg2.example.net:27019\" }, { _id : 2, host : \"cfg3.example.net:27019\" } ] } )\n\nHere we’re using configRS as the replica set name. Note that this name appears both on the command line when instantiating each config server and in the call to rs.initiate().\n\nThe --configsvr option indicates to the mongod that you are planning to use it as a config server. On a server running with this option, clients (i.e., other cluster compo‐ nents) cannot write data to any database other than config or admin.\n\nThe admin database contains the collections related to authentication and authoriza‐ tion, as well as the other system.* collections for internal use. The config database con‐ tains the collections that hold the sharded cluster metadata. MongoDB writes data to the config database when the metadata changes, such as after a chunk migration or a chunk split.\n\nWhen writing to config servers, MongoDB uses a writeConcern level of \"majority\". Similarly, when reading from config servers, MongoDB uses a readConcern level of \"majority\". This ensures that sharded cluster metadata will not be committed to the config server replica set until it can’t be rolled back. It also ensures that only metadata that will survive a failure of the config servers will be read. This is necessary to ensure all mongos routers have a consistent view of how data is organized in a sharded cluster.\n\nIn terms of provisioning, config servers should be provisioned adequately in terms of networking and CPU resources. They only hold a table of contents of the data in the cluster so the storage resources required are minimal. They should be deployed on separate hardware to avoid contention for the machine’s resources.\n\nIf all of your config servers are lost, you must dig through the data on your shards to figure out which data is where. This is possible, but slow and unpleasant. Take frequent backups of config server data. Always take a backup of your config servers before perform‐ ing any cluster maintenance.\n\nThe mongos Processes Once you have three config servers running, start a mongos process for your applica‐ tion to connect to. mongos processes need to know where the config servers are, so you must always start mongos with the --configdb option:\n\n$ mongos --configdb \\ configRS/cfg1.example.net:27019, \\\n\nStarting the Servers\n\n|\n\n305",
      "content_length": 2348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "cfg2.example.net:27019,cfg3.example.net:27019 \\ --bind_ip localhost,198.51.100.100 --logpath /var/log/mongos.log\n\nBy default, mongos runs on port 27017. Note that it does not need a data directory (mongos holds no data itself; it loads the cluster configuration from the config servers on startup). Make sure that you set --logpath to save the mongos log somewhere safe.\n\nYou should start a small number of mongos processes and locate them as close to all the shards as possible. This improves performance of queries that need to access mul‐ tiple shards or which perform scatter/gather operations. The minimal setup is at least two mongos processes to ensure high availability. It is possible to run tens or hun‐ dreds of mongos processes but this causes resource contention on the config servers. The recommended approach is to provide a small pool of routers.\n\nAdding a Shard from a Replica Set Finally, you’re ready to add a shard. There are two possibilities: you may have an existing replica set or you may be starting from scratch. We will cover starting from an existing set. If you are starting from scratch, initialize an empty set and follow the steps outlined here.\n\nIf you already have a replica set serving your application, that will become your first shard. To convert it into a shard, you need to make some small configuration modifi‐ cations to the members and then tell the mongos how to find the replica set that will comprise the shard.\n\nFor example, if you have a replica set named rs0 on svr1.example.net, svr2.exam‐ ple.net, and svr3.example.net, you would first connect to one of the members using the mongo shell:\n\n$ mongo srv1.example.net\n\nThen use rs.status() to determine which member is the primary and which are secondaries:\n\n> rs.status() \"set\" : \"rs0\", \"date\" : ISODate(\"2018-11-02T20:02:16.543Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"heartbeatIntervalMillis\" : NumberLong(2000), \"optimes\" : {\n\n\"lastCommittedOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) },\n\n306\n\n|\n\nChapter 15: Configuring Sharding",
      "content_length": 2055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "\"readConcernMajorityOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"appliedOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) } },\n\n\"members\" : [ { \"_id\" : 0, \"name\" : \"svr1.example.net:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 269, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2018-11-02T20:02:14Z\"), \"infoMessage\" : \"could not find member to sync from\", \"electionTime\" : Timestamp(1478116933, 1), \"electionDate\" : ISODate(\"2018-11-02T20:02:13Z\"), \"configVersion\" : 1, \"self\" : true }, { \"_id\" : 1, \"name\" : \"svr2.example.net:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 14, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2018-11-02T20:02:14Z\"), \"optimeDurableDate\" : ISODate(\"2018-11-02T20:02:14Z\"), \"lastHeartbeat\" : ISODate(\"2018-11-02T20:02:15.618Z\"), \"lastHeartbeatRecv\" : ISODate(\"2018-11-02T20:02:14.866Z\"),\n\nStarting the Servers\n\n|\n\n307",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "\"pingMs\" : NumberLong(0), \"syncingTo\" : \"m1.example.net:27017\", \"configVersion\" : 1 }, { \"_id\" : 2, \"name\" : \"svr3.example.net:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 14, \"optime\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1478116934, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2018-11-02T20:02:14Z\"), \"optimeDurableDate\" : ISODate(\"2018-11-02T20:02:14Z\"), \"lastHeartbeat\" : ISODate(\"2018-11-02T20:02:15.619Z\"), \"lastHeartbeatRecv\" : ISODate(\"2018-11-02T20:02:14.787Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"m1.example.net:27017\", \"configVersion\" : 1 } ], \"ok\" : 1 }\n\nBeginning with MongoDB 3.4, for sharded clusters, mongod instances for shards must be configured with the --shardsvr option, either via the configuration file setting sharding.clusterRole or via the command-line option --shardsvr.\n\nYou will need to do this for each of the members of the replica set you are in the pro‐ cess of converting to a shard. You’ll do this by first restarting each secondary in turn with the --shardsvr option, then stepping down the primary and restarting it with the --shardsvr option.\n\nAfter shutting down a secondary, restart it as follows:\n\n$ mongod --replSet \"rs0\" --shardsvr --port 27017 --bind_ip localhost,<ip address of member>\n\nNote that you’ll need to use the correct IP address for each secondary for the --bind_ip parameter.\n\nNow connect a mongo shell to the primary:\n\n$ mongo m1.example.net\n\n308\n\n|\n\nChapter 15: Configuring Sharding",
      "content_length": 1539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "and step it down:\n\n> rs.stepDown()\n\nThen restart the former primary with the --shardsvr option:\n\n$ mongod --replSet \"rs0\" --shardsvr --port 27017 --bind_ip localhost,<ip address of the former primary>\n\nNow you’re ready to add your replica set as a shard. Connect a mongo shell to the admin database of the mongos:\n\n$ mongo mongos1.example.net:27017/admin\n\nAnd add a shard to the cluster using the sh.addShard() method:\n\n> sh.addShard( \"rs0/svr1.example.net:27017,svr2.example.net:27017,svr3.example.net:27017\" )\n\nYou can specify all the members of the set, but you do not have to. mongos will auto‐ matically detect any members that were not included in the seed list. If you run sh.status(), you’ll see that MongoDB soon lists the shard as\n\nrs0/svr1.example.net:27017,svr2.example.net:27017,svr3.example.net:27017\n\nThe set name, rs0, is taken on as an identifier for this shard. If you ever want to remove this shard or migrate data to it, you can use rs0 to describe it. This works bet‐ ter than using a specific server (e.g., svr1.example.net), as replica set membership and status can change over time.\n\nOnce you’ve added the replica set as a shard you can convert your application from connecting to the replica set to connecting to the mongos. When you add the shard, mongos registers that all the databases in the replica set are “owned” by that shard, so it will pass through all queries to your new shard. mongos will also automatically han‐ dle failover for your application as your client library would: it will pass the errors through to you.\n\nTest failing over a shard’s primary in a development environment to ensure that your application handles the errors received from mongos correctly (they should be identi‐ cal to the errors that you receive from talking to the primary directly).\n\nOnce you have added a shard, you must set up all clients to send requests to the mongos instead of contacting the replica set. Shard‐ ing will not function correctly if some clients are still making requests to the replica set directly (not through the mongos). Switch all clients to contacting the mongos immediately after adding the shard and set up a firewall rule to ensure that they are unable to connect directly to the shard.\n\nStarting the Servers\n\n|\n\n309",
      "content_length": 2264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "Prior to MongoDB 3.6 it was possible to create a standalone mongod as a shard. This is no longer an option in versions of MongoDB later than 3.6. All shards must be rep‐ lica sets.\n\nAdding Capacity When you want to add more capacity, you’ll need to add more shards. To add a new, empty shard, create a replica set. Make sure it has a distinct name from any of your other shards. Once it is initialized and has a primary, add it to your cluster by run‐ ning the addShard command through mongos, specifying the new replica set’s name and its hosts as seeds.\n\nIf you have several existing replica sets that are not shards, you can add all of them as new shards in your cluster so long as they do not have any database names in com‐ mon. For example, if you had one replica set with a blog database, one with a calendar database, and one with mail, tel, and music databases, you could add each replica set as a shard and end up with a cluster with three shards and five databases. However, if you had a fourth replica set that also had a database named tel, mongos would refuse to add it to the cluster.\n\nSharding Data MongoDB won’t distribute your data automatically until you tell it how to do so. You must explicitly tell both the database and the collection that you want them to be dis‐ tributed. For example, suppose you wanted to shard the artists collection in the music database on the \"name\" key. First, you’d enable sharding for the database:\n\n> db.enableSharding(\"music\")\n\nSharding a database is always a prerequisite to sharding one of its collections.\n\nOnce you’ve enabled sharding on the database level, you can shard a collection by running sh.shardCollection():\n\n> sh.shardCollection(\"music.artists\", {\"name\" : 1})\n\nNow the artists collection will be sharded by the \"name\" key. If you are sharding an existing collection there must be an index on the \"name\" field; otherwise, the shard Collection call will return an error. If you get an error, create the index (mongos will return the index it suggests as part of the error message) and retry the shardCollec tion command.\n\nIf the collection you are sharding does not yet exist, mongos will automatically create the shard key index for you.\n\nThe shardCollection command splits the collection into chunks, which are the units MongoDB uses to move data around. Once the command returns successfully,\n\n310\n\n|\n\nChapter 15: Configuring Sharding",
      "content_length": 2403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "MongoDB will begin balancing the collection across the shards in your cluster. This process is not instantaneous. For large collections it may take hours to finish this ini‐ tial balancing. This time can be reduced with presplitting where chunks are created on the shards prior to loading the data. Data loaded after this point will be inserted directly to the current shard without requiring additional balancing.\n\nHow MongoDB Tracks Cluster Data Each mongos must always know where to find a document, given its shard key. Theo‐ retically, MongoDB could track where each and every document lived, but this becomes unwieldy for collections with millions or billions of documents. Thus, Mon‐ goDB groups documents into chunks, which are documents in a given range of the shard key. A chunk always lives on a single shard, so MongoDB can keep a small table of chunks mapped to shards.\n\nFor example, if a user collection’s shard key is {\"age\" : 1}, one chunk might be all documents with an \"age\" field between 3 and 17. If mongos gets a query for {\"age\" : 5}, it can route the query to the shard where this chunk lives.\n\nAs writes occur, the number and size of the documents in a chunk might change. Inserts can make a chunk contain more documents, and removes fewer. For example, if we were making a game for children and preteens, our chunk for ages 3−17 might get larger and larger (one would hope). Almost all of our users would be in that chunk and so would be on a single shard, somewhat defeating the point of distribut‐ ing our data. Thus, once a chunk grows to a certain size, MongoDB automatically splits it into two smaller chunks. In this example, the original chunk might be split into one chunk containing documents with ages 3 through 11 and another with ages 12 through 17. Note that these two chunks still cover the entire age range that the original chunk covered: 3−17. As these new chunks grow, they can be split into still smaller chunks until there is a chunk for each age.\n\nYou cannot have chunks with overlapping ranges, like 3−15 and 12−17. If you could, MongoDB would need to check both chunks when attempting to find an age in the overlap, like 14. It is more efficient to only have to look in one place, particularly once chunks begin moving around the cluster.\n\nA document always belongs to one and only one chunk. One consequence of this rule is that you cannot use an array field as your shard key, since MongoDB creates multi‐ ple index entries for arrays. For example, if a document had [5, 26, 83] in its \"age\" field, it would belong in up to three chunks.\n\nHow MongoDB Tracks Cluster Data\n\n|\n\n311",
      "content_length": 2628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "A common misconception is that the data in a chunk is physically grouped on disk. This is incorrect: chunks have no effect on how mongod stores collection data.\n\nChunk Ranges Each chunk is described by the range it contains. A newly sharded collection starts off with a single chunk, and every document lives in this chunk. This chunk’s bounds are negative infinity to infinity, shown as $minKey and $maxKey in the shell.\n\nAs this chunk grows, MongoDB will automatically split it into two chunks, with the range negative infinity to <some value> and <some value> to infinity. <some value> is the same for both chunks: the lower chunk contains everything up to (but not including) <some value>, and the upper chunk contains <some value> and every‐ thing higher.\n\nThis may be more intuitive with an example. Suppose we were sharding by \"age\" as described earlier. All documents with \"age\" between 3 and 17 are contained in one chunk: 3 ≤ \"age\" < 17. When this is split, we end up with two ranges: 3 ≤ \"age\" < 12 in one chunk and 12 ≤ \"age\" < 17 in the other. 12 is called the split point.\n\nChunk information is stored in the config.chunks collection. If you looked at the con‐ tents of that collection, you’d see documents that looked something like this (some fields have been omitted for clarity):\n\n> db.chunks.find(criteria, {\"min\" : 1, \"max\" : 1}) { \"_id\" : \"test.users-age_-100.0\", \"min\" : {\"age\" : -100}, \"max\" : {\"age\" : 23} } { \"_id\" : \"test.users-age_23.0\", \"min\" : {\"age\" : 23}, \"max\" : {\"age\" : 100} } { \"_id\" : \"test.users-age_100.0\", \"min\" : {\"age\" : 100}, \"max\" : {\"age\" : 1000} }\n\nBased on the config.chunks documents shown, here are a few examples of where vari‐ ous documents would live:\n\n312\n\n|\n\nChapter 15: Configuring Sharding",
      "content_length": 1744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "{\"_id\" : 123, \"age\" : 50}\n\nThis document would live in the second chunk, as that chunk contains all docu‐ ments with \"age\" between 23 and 100.\n\n{\"_id\" : 456, \"age\" : 100}\n\nThis document would live in the third chunk, as lower bounds are inclusive. The second chunk contains all documents up to \"age\" : 100, but not any documents where \"age\" equals 100.\n\n{\"_id\" : 789, \"age\" : -101}\n\nThis document would not be in any of these chunks. It would be in some chunk with a range lower than the first chunk’s.\n\nWith a compound shard key, shard ranges work the same way that sorting by the two keys would work. For example, suppose that we had a shard key on {\"username\" : 1, \"age\" : 1}. Then we might have chunk ranges such as:\n\n{ \"_id\" : \"test.users-username_MinKeyage_MinKey\", \"min\" : { \"username\" : { \"$minKey\" : 1 }, \"age\" : { \"$minKey\" : 1 } }, \"max\" : { \"username\" : \"user107487\", \"age\" : 73 } } { \"_id\" : \"test.users-username_\\\"user107487\\\"age_73.0\", \"min\" : { \"username\" : \"user107487\", \"age\" : 73 }, \"max\" : { \"username\" : \"user114978\", \"age\" : 119 } } { \"_id\" : \"test.users-username_\\\"user114978\\\"age_119.0\", \"min\" : { \"username\" : \"user114978\", \"age\" : 119 }, \"max\" : { \"username\" : \"user122468\", \"age\" : 68 } }\n\nHow MongoDB Tracks Cluster Data\n\n|\n\n313",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Thus, mongos can easily find which chunk someone with a given username (or a given username and age) lives in. However, given just an age, mongos would have to check all, or almost all, of the chunks. If we wanted to be able to target queries on age to the right chunk, we’d have to use the “opposite” shard key: {\"age\" : 1, \"user name\" : 1}. This is often a point of confusion: a range over the second half of a shard key will cut across multiple chunks.\n\nSplitting Chunks Each shard primary mongod tracks their current chunks and, once they reach a cer‐ tain threshold, checks if the chunk needs to be split, as shown in Figures 15-1 and 15-2. If the chunk does need to be split, the mongod will request the global chunk size configuration value from the config servers. It will then perform the chunk split and update the metadata on the config servers. New chunk documents are created on the config servers and the old chunk’s range (\"max\") is modified. If the chunk is the top chunk of the shard, then the mongod will request the balancer move this chunk to a different shard. The idea is to prevent a shard from becoming “hot” where the shard key uses a monotonically increasing key.\n\nA shard may not be able to find any split points, though, even for a large chunk, as there are a limited number of ways to legally split a chunk. Any two documents with the same shard key must live in the same chunk, so chunks can only be split between documents where the shard key’s value changes. For example, if the shard key was \"age\", the following chunk could be split at the points where the shard key changed, as indicated:\n\n{\"age\" : 13, \"username\" : \"ian\"} {\"age\" : 13, \"username\" : \"randolph\"} ------------ // split point {\"age\" : 14, \"username\" : \"randolph\"} {\"age\" : 14, \"username\" : \"eric\"} {\"age\" : 14, \"username\" : \"hari\"} {\"age\" : 14, \"username\" : \"mathias\"} ------------ // split point {\"age\" : 15, \"username\" : \"greg\"} {\"age\" : 15, \"username\" : \"andrew\"}\n\nThe primary mongod for the shard only requests that the top chunk for a shard when split be moved to the balancer. The other chunks will remain on the shard unless manually moved.\n\nIf, however, the chunk contained the following documents, it could not be split (unless the application started inserting fractional ages):\n\n{\"age\" : 12, \"username\" : \"kevin\"} {\"age\" : 12, \"username\" : \"spencer\"} {\"age\" : 12, \"username\" : \"alberto\"} {\"age\" : 12, \"username\" : \"tad\"}\n\n314\n\n|\n\nChapter 15: Configuring Sharding",
      "content_length": 2471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "Thus, having a variety of values for your shard key is important. Other important properties will be covered in the next chapter.\n\nIf one of the config servers is down when a mongod tries to do a split, the mongod won’t be able to update the metadata (as shown in Figure 15-3). All config servers must be up and reachable for splits to happen. If the mongod continues to receive write requests for the chunk, it will keep trying to split the chunk and fail. As long as the config servers are not healthy, splits will continue not to work, and all the split attempts can slow down the mongod and the shard involved (which repeats the pro‐ cess shown in Figures 15-1 through 15-3 for each incoming write). This process of mongod repeatedly attempting to split a chunk and being unable to is called a split storm. The only way to prevent split storms is to ensure that your config servers are up and healthy as much of the time as possible.\n\nFigure 15-1. When a client writes to a chunk, the mongod will check its split threshold for the chunk\n\nFigure 15-2. If the split threshold has been reached, the mongod will send a request to the balancer to migrate the top chunk; otherwise the chunk remains on the shard\n\nHow MongoDB Tracks Cluster Data\n\n|\n\n315",
      "content_length": 1250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "Figure 15-3. The mongod chooses a split point and attempts to inform the config server, but cannot reach it; thus, it is still over its split threshold for the chunk and any subse‐ quent writes will trigger this process again\n\nThe Balancer The balancer is responsible for migrating data. It regularly checks for imbalances between shards and, if it finds an imbalance, will begin migrating chunks. In Mon‐ goDB version 3.4+, the balancer is located on the primary member of the config server replica set; prior to this version, each mongos used to play the part of “the bal‐ ancer” occasionally.\n\nThe balancer is a background process on the primary of the config server replica set, which monitors the number of chunks on each shard. It becomes active only when a shard’s number of chunks reaches a specific migration threshold.\n\nIn MongoDB 3.4+, the number of concurrent migrations increased to one migration per shard with a maximum number of concurrent migrations being half the total number of shards. In earlier ver‐ sions only one concurrent migration in total was supported.\n\nAssuming that some collections have hit the threshold, the balancer will begin migrating chunks. It chooses a chunk from the overloaded shard and asks the shard if it should split the chunk before migrating. Once it does any necessary splits, it migrates the chunk(s) to a machine with fewer chunks.\n\nAn application using the cluster does not need be aware that the data is moving: all reads and writes are routed to the old chunk until the move is complete. Once the metadata is updated, any mongos process attempting to access the data in the old location will get an error. These errors should not be visible to the client: the mongos will silently handle the error and retry the operation on the new shard.\n\nThis is a common cause of errors you might see in mongos logs that relate to being “unable to setShardVersion.” When a mongos gets this type of error, it looks up the new location of the data from the config servers, updates its chunk table, and attempts the request again. If it successfully retrieves the data from the new location,\n\n316\n\n|\n\nChapter 15: Configuring Sharding",
      "content_length": 2171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "it will return it to the client as though nothing went wrong (but it will print a message in the log that the error occurred).\n\nIf the mongos is unable to retrieve the new chunk location because the config servers are unavailable, it will return an error to the client. This is another reason why it is important to always have config servers up and healthy.\n\nCollations Collations in MongoDB allow for the specification of language-specific rules for string comparison. Examples of these rules include how lettercase and accent marks are compared. It is possible to shard a collection that is a default collation. There are two requirements: the collection must have an index whose prefix is the shard key, and the index must also have the collation { locale: \"simple\" }.\n\nChange Streams Change Streams allow applications to track real-time changes to the data in the data‐ base. Prior to MongoDB 3.6, this was only possible by tailing the oplog and was a complex error-prone operation. Change streams provide a subscription mechanism for all data changes on a collection, a set of collections, a database, or across a full deployment. The aggregation framework is used by this feature. It allows applications to filter for specific changes or to transform the change notifications received. In a sharded cluster, all change stream operations must be issued against a mongos.\n\nThe changes across a sharded cluster are kept ordered through the use of a global log‐ ical clock. This guarantees the order of changes, and stream notifications can be safely interpreted by the order of their receipt. The mongos needs to check with each shard upon receipt of a change notification, to ensure that no shard has seen more recent changes. The activity level of the cluster and the geographical distribution of the shards can both impact the response time for this checking. The use of notifica‐ tion filters can improve the response time in these situations.\n\nThere are a few notes and caveats when using change streams with a sharded cluster. You open a change stream by issuing an open change stream operation. In sharded deployments, this must be issued against a mongos. If an update operation with multi: true is run against a sharded collection with an open change stream, then it is possible for notifications to be sent for orphaned docu‐ ments. If a shard is removed, it may cause an open change stream cursor to close—furthermore, that cursor may not be fully resumable.\n\nCollations\n\n|\n\n317",
      "content_length": 2493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "CHAPTER 16 Choosing a Shard Key\n\nThe most important task when using sharding is choosing how your data will be dis‐ tributed. To make intelligent choices about this, you have to understand how MongoDB distributes data. This chapter helps you make a good choice of shard key by covering:\n\nHow to decide among multiple possible shard keys\n\nShard keys for several use cases\n\nWhat you can’t use as a shard key\n\nSome alternative strategies if you want to customize how data is distributed\n\nHow to manually shard your data\n\nIt assumes that you understand the basic components of sharding as covered in the previous two chapters.\n\nTaking Stock of Your Usage When you shard a collection you choose a field or two to use to split up the data. This key (or keys) is called a shard key. Once you shard a collection you cannot change your shard key, so it is important to choose correctly.\n\nTo choose a good shard key, you need to understand your workload and how your shard key is going to distribute your application’s requests. This can be difficult to picture, so try to work out some examples—or, even better, try it out on a backup dataset with sample traffic. This section has lots of diagrams and explanations, but there is no substitute for trying it on your own data.\n\nFor each collection that you’re planning to shard, start by answering the following questions:\n\n319",
      "content_length": 1366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "How many shards are you planning to grow to? A three-shard cluster has a great deal more flexibility than a thousand-shard cluster. As a cluster gets larger, you should not plan to fire off queries that can hit all shards, so almost all queries must include the shard key.\n\nAre you sharding to decrease read or write latency? (Latency refers to how long something takes; e.g., a write takes 20 ms, but you need it to take 10 ms.) Decreasing write latency usually involves sending requests to geographically closer or more powerful machines.\n\nAre you sharding to increase read or write throughput? (Throughput refers to how many requests the cluster can handle at the same time; e.g., the cluster can do 1,000 writes in 20 ms, but you need it to do 5,000 writes in 20 ms.) Increasing throughput usually involves adding more parallelization and making sure that requests are distributed evenly across the cluster.\n\nAre you sharding to increase system resources (e.g., give MongoDB more RAM per GB of data)? If so, you want to keep the working set size as small as possible.\n\nUse these answers to evaluate the following shard key descriptions and decide whether the shard key you’re considering would work well in your situation. Does it give you the targeted queries that you need? Does it change the throughput or latency of your system in the ways you need? If you need a compact working set, does it pro‐ vide that?\n\nPicturing Distributions The most common ways people choose to split their data are via ascending, random, and location-based keys. There are other types of keys that could be used, but most use cases fall into one of these categories. The different types of distributions are dis‐ cussed in the following sections.\n\nAscending Shard Keys Ascending shard keys are generally something like a \"date\" field or ObjectId—any‐ thing that steadily increases over time. An autoincrementing primary key is another example of an ascending field, albeit one that doesn’t show up in MongoDB much (unless you’re importing from another database).\n\nSuppose that we shard on an ascending field, like \"_id\" on a collection using ObjectIds. If we shard on \"_id\", then the data will be split into chunks of \"_id\" ranges, as in Figure 16-1. These chunks will be distributed across our sharded cluster of, let’s say, three shards, as shown in Figure 16-2.\n\n320\n\n|\n\nChapter 16: Choosing a Shard Key",
      "content_length": 2392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "Figure 16-1. The collection is split into ranges of ObjectIds; each range is a chunk\n\nSuppose we create a new document. Which chunk will it be in? The answer is the chunk with the range ObjectId(\"5112fae0b4a4b396ff9d0ee5\") through $maxKey. This is called the max chunk, as it is the chunk containing $maxKey.\n\nIf we insert another document, it will also be in the max chunk. In fact, every subse‐ quent insert will be into the max chunk! Every insert’s \"_id\" field will be closer to infinity than the previous one (because ObjectIds are always ascending), so they will all go into the max chunk.\n\nPicturing Distributions\n\n|\n\n321",
      "content_length": 628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "Figure 16-2. Chunks are distributed across shards in a random order\n\nThis has a couple of interesting (and often undesirable) properties. First, all of your writes will be routed to one shard (shard0002, in this case). This chunk will be the only one growing and splitting, as it is the only one that receives inserts. As you insert data, new chunks will “fall off” of this chunk, as shown in Figure 16-3.\n\n322\n\n|\n\nChapter 16: Choosing a Shard Key",
      "content_length": 447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "Figure 16-3. The max chunk continues growing and being split into multiple chunks\n\nThis pattern often makes it more difficult for MongoDB to keep chunks evenly bal‐ anced because all the chunks are being created by one shard. Therefore, MongoDB must constantly move chunks to other shards instead of correcting the small imbal‐ ances that might occur in more evenly distributed systems.\n\nIn MongoDB 4.2, the move of the autosplit functionality to the shard primary mongod added top chunk optimization to address the ascending shard key pattern. The balancer will decide in which other shard to place the top chunk. This helps avoid a situation in which all new chunks are created on just one shard.\n\nRandomly Distributed Shard Keys At the other end of the spectrum are randomly distributed shard keys. Randomly dis‐ tributed keys could be usernames, email addresses, UUIDs, MD5 hashes, or any other key that has no identifiable pattern in your dataset.\n\nSuppose the shard key is a random number between 0 and 1. We’ll end up with a ran‐ dom distribution of chunks on the various shards, as shown in Figure 16-4.\n\nPicturing Distributions\n\n|\n\n323",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "Figure 16-4. As in the previous section, chunks are distributed randomly around the cluster\n\nAs more data is inserted, the data’s random nature means that inserts should hit every chunk fairly evenly. You can prove this to yourself by inserting 10,000 documents and seeing where they end up:\n\n> var servers = {} > var findShard = function (id) { ... var explain = db.random.find({_id:id}).explain(); ... for (var i in explain.shards) { ... var server = explain.shards[i][0];\n\n324\n\n|\n\nChapter 16: Choosing a Shard Key",
      "content_length": 516,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "... if (server.n == 1) { ... if (server.server in servers) { ... servers[server.server]++; ... } else { ... servers[server.server] = 1; ... } ... } ... } ... } > for (var i = 0; i < 10000; i++) { ... var id = ObjectId(); ... db.random.insert({\"_id\" : id, \"x\" : Math.random()}); ... findShard(id); ... } > servers { \"spock:30001\" : 2942, \"spock:30002\" : 4332, \"spock:30000\" : 2726 }\n\nAs writes are randomly distributed, the shards should grow at roughly the same rate, limiting the number of migrates that need to occur.\n\nThe only downside to randomly distributed shard keys is that MongoDB isn’t effi‐ cient at randomly accessing data beyond the size of RAM. However, if you have the capacity or don’t mind the performance hit, random keys nicely distribute load across your cluster.\n\nLocation-Based Shard Keys Location-based shard keys may be things like a user’s IP, latitude and longitude, or address. They’re not necessarily related to a physical location field: the “location” might be a more abstract way that data should be grouped together. In any case, a location-based key is a key where documents with some similarity fall into a range based on this field. This can be handy for both putting data close to its users and keeping related data together on disk. It may also be a legal requirement to remain compliant with GDPR or other similar data privacy legislation. MongoDB uses Zoned Sharding to manage this.\n\nIn MongoDB 4.0.3+, you can define the zones and the zone ranges prior to sharding a collection, which populates chunks for both the zone ranges and for the shard key values as well as performing an initial chunk distribution of these. This greatly reduces the com‐ plexity for sharded zone setup.\n\nPicturing Distributions\n\n|\n\n325",
      "content_length": 1752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "For example, suppose we have a collection of documents that are sharded on IP address. Documents will be organized into chunks based on their IPs and randomly spread across the cluster, as shown in Figure 16-5.\n\nFigure 16-5. A sample distribution of chunks in the IP address collection\n\nIf we wanted certain chunk ranges to be attached to certain shards, we could zone these shards and then assign chunk ranges to each zone. In this example, suppose that we wanted to keep certain IP blocks on certain shards: say, 56.*.*.* (the United States Postal Service’s IP block) on shard0000 and 17.*.*.* (Apple’s IP block) on either shard0000 or shard0002. We do not care where the other IPs live. We could request that the balancer do this by setting up zones:\n\n> sh.addShardToZone(\"shard0000\", \"USPS\") > sh.addShardToZone(\"shard0000\", \"Apple\") > sh.addShardToZone(\"shard0002\", \"Apple\")\n\nNext, we create the rules:\n\n> sh.updateZoneKeyRange(\"test.ips\", {\"ip\" : \"056.000.000.000\"}, ... {\"ip\" : \"057.000.000.000\"}, \"USPS\")\n\nThis attaches all IPs greater than or equal to 56.0.0.0 and less than 57.0.0.0 to the shard zoned as \"USPS\". Next, we add a rule for Apple:\n\n> sh.updateZoneKeyRange(\"test.ips\", {\"ip\" : \"017.000.000.000\"}, ... {\"ip\" : \"018.000.000.000\"}, \"Apple\")\n\nWhen the balancer moves chunks, it will attempt to move chunks with those ranges to those shards. Note that this process is not immediate. Chunks that were not cov‐ ered by a zone key range will be moved around normally. The balancer will continue attempting to distribute chunks evenly among shards.\n\n326\n\n|\n\nChapter 16: Choosing a Shard Key",
      "content_length": 1603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "Shard Key Strategies This section presents a number of shard key options for various types of applications.\n\nHashed Shard Key For loading data as fast as possible, hashed shard keys are the best option. A hashed shard key can make any field randomly distributed, so it is a good choice if you’re going to be using an ascending key in a lot of queries but want writes to be randomly distributed.\n\nThe trade-off is that you can never do a targeted range query with a hashed shard key. If you will not be doing range queries, though, hashed shard keys are a good option.\n\nTo create a hashed shard key, first create a hashed index:\n\n> db.users.createIndex({\"username\" : \"hashed\"})\n\nNext, shard the collection with:\n\n> sh.shardCollection(\"app.users\", {\"username\" : \"hashed\"}) { \"collectionsharded\" : \"app.users\", \"ok\" : 1 }\n\nIf you create a hashed shard key on a nonexistent collection, shardCollection behaves interestingly: it assumes that you want evenly distributed chunks, so it imme‐ diately creates a bunch of empty chunks and distributes them around your cluster. For example, suppose our cluster looked like this before creating the hashed shard key:\n\n> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"version\" : 3 } shards: { \"_id\" : \"shard0000\", \"host\" : \"localhost:30000\" } { \"_id\" : \"shard0001\", \"host\" : \"localhost:30001\" } { \"_id\" : \"shard0002\", \"host\" : \"localhost:30002\" } databases: { \"_id\" : \"admin\", \"partitioned\" : false, \"primary\" : \"config\" } { \"_id\" : \"test\", \"partitioned\" : true, \"primary\" : \"shard0001\" }\n\nImmediately after shardCollection returns there are two chunks on each shard, evenly distributing the key space across the cluster:\n\n> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"version\" : 3 } shards: { \"_id\" : \"shard0000\", \"host\" : \"localhost:30000\" } { \"_id\" : \"shard0001\", \"host\" : \"localhost:30001\" } { \"_id\" : \"shard0002\", \"host\" : \"localhost:30002\" }\n\nShard Key Strategies\n\n|\n\n327",
      "content_length": 1959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "databases: { \"_id\" : \"admin\", \"partitioned\" : false, \"primary\" : \"config\" } { \"_id\" : \"test\", \"partitioned\" : true, \"primary\" : \"shard0001\" } test.foo shard key: { \"username\" : \"hashed\" } chunks: shard0000 2 shard0001 2 shard0002 2 { \"username\" : { \"$MinKey\" : true } } -->> { \"username\" : NumberLong(\"-6148914691236517204\") } on : shard0000 { \"t\" : 3000, \"i\" : 2 } { \"username\" : NumberLong(\"-6148914691236517204\") } -->> { \"username\" : NumberLong(\"-3074457345618258602\") } on : shard0000 { \"t\" : 3000, \"i\" : 3 } { \"username\" : NumberLong(\"-3074457345618258602\") } -->> { \"username\" : NumberLong(0) } on : shard0001 { \"t\" : 3000, \"i\" : 4 } { \"username\" : NumberLong(0) } -->> { \"username\" : NumberLong(\"3074457345618258602\") } on : shard0001 { \"t\" : 3000, \"i\" : 5 } { \"username\" : NumberLong(\"3074457345618258602\") } -->> { \"username\" : NumberLong(\"6148914691236517204\") } on : shard0002 { \"t\" : 3000, \"i\" : 6 } { \"username\" : NumberLong(\"6148914691236517204\") } -->> { \"username\" : { \"$MaxKey\" : true } } on : shard0002 { \"t\" : 3000, \"i\" : 7 }\n\nNote that there are no documents in the collection yet, but when you start inserting them, writes should be evenly distributed across the shards from the get-go. Ordinar‐ ily, you would have to wait for chunks to grow, split, and move to start writing to other shards. With this automatic priming, you’ll immediately have chunk ranges on all shards.\n\nThere are some limitations on what your shard key can be if you’re using a hashed shard key. First, you cannot use the unique option. As with other shard keys, you cannot use array fields. Finally, be aware that floating-point values will be rounded to whole numbers before hashing, so 1 and 1.999999 will both be hashed to the same value.\n\nHashed Shard Keys for GridFS Before attempting to shard GridFS collections, make sure that you understand how GridFS stores data (see Chapter 6 for an explanation).\n\nIn the following explanation, the term “chunks” is overloaded since GridFS splits files into chunks and sharding splits collections into chunks. Thus, the two types of chunks are referred to as “GridFS chunks” and “sharding chunks.”\n\n328\n\n|\n\nChapter 16: Choosing a Shard Key",
      "content_length": 2179,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "GridFS collections are generally excellent candidates for sharding, as they contain massive amounts of file data. However, neither of the indexes that are automatically created on fs.chunks are particularly good shard keys: {\"_id\" : 1} is an ascending key and {\"files_id\" : 1, \"n\" : 1} picks up fs.files’s \"_id\" field, so it is also an ascending key.\n\nHowever, if you create a hashed index on the \"files_id\" field, each file will be ran‐ domly distributed across the cluster, and a file will always be contained in a single chunk. This is the best of both worlds: writes will go to all shards evenly and reading a file’s data will only ever have to hit a single shard.\n\nTo set this up, you must create a new index on {\"files_id\" : \"hashed\"} (as of this writing, mongos cannot use a subset of the compound index as a shard key). Then shard the collection on this field:\n\n> db.fs.chunks.ensureIndex({\"files_id\" : \"hashed\"}) > sh.shardCollection(\"test.fs.chunks\", {\"files_id\" : \"hashed\"}) { \"collectionsharded\" : \"test.fs.chunks\", \"ok\" : 1 }\n\nAs a side note, the fs.files collection may or may not need to be sharded, as it will be much smaller than fs.chunks. You can shard it if you would like, but it is not likely to be necessary.\n\nThe Firehose Strategy If you have some servers that are more powerful than others, you might want to let them handle proportionally more load than your less-powerful servers. For example, suppose you have one shard that can handle 10 times the load of your other machines. Luckily, you have 10 other shards. You could force all inserts to go to the more powerful shard, and then allow the balancer to move older chunks to the other shards. This would give lower-latency writes.\n\nTo use this strategy, we have to pin the highest chunk to the more powerful shard. First, we zone this shard:\n\n> sh.addShardToZone(\"<shard-name>\", \"10x\")\n\nThen we pin the current value of the ascending key through infinity to that shard, so all new writes go to it:\n\n> sh.updateZoneKeyRange(\"<dbName.collName>\", {\"_id\" : ObjectId()}, ... {\"_id\" : MaxKey}, \"10x\")\n\nNow all inserts will be routed to this last chunk, which will always live on the shard zoned \"10x\".\n\nHowever, ranges from now through infinity will be trapped on this shard unless we modify the zone key range. To get around this, we could set up a cron job to update the key range once a day, like this:\n\nShard Key Strategies\n\n|\n\n329",
      "content_length": 2409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "> use config > var zone = db.tags.findOne({\"ns\" : \"<dbName.collName>\", ... \"max\" : {\"<shardKey>\" : MaxKey}}) > zone.min.<shardKey> = ObjectId() > db.tags.save(zone)\n\nThen all of the previous day’s chunks would be able to move to other shards.\n\nAnother downside of this strategy is that it requires some changes to scale. If your most powerful server can no longer handle the number of writes coming in, there is no trivial way to split the load between this server and another.\n\nIf you do not have a high-performance server to firehose into or you are not using zone sharding, do not use an ascending key as the shard key. If you do, all writes will go to a single shard.\n\nMulti-Hotspot Standalone mongod servers are most efficient when doing ascending writes. This con‐ flicts with sharding, in that sharding is most efficient when writes are spread over the cluster. The technique described here basically creates multiple hotspots—optimally several on each shard—so that writes are evenly balanced across the cluster but, within a shard, ascending.\n\nTo accomplish this, we use a compound shard key. The first value in the compound key is a rough, random value with low-ish cardinality. You can picture each value in the first part of the shard key as a chunk, as shown in Figure 16-6. This will eventually work itself out as you insert more data, although it will probably never be divided up this neatly (right on the $minKey lines). However, if you insert enough data, you should eventually have approximately one chunk per random value. As you continue to insert data, you’ll end up with multiple chunks with the same random value, which brings us to the second part of the shard key.\n\n330\n\n|\n\nChapter 16: Choosing a Shard Key",
      "content_length": 1732,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "Figure 16-6. A subset of the chunks: each chunk contains a single state and a range of “_id” values\n\nThe second part of the shard key is an ascending key. This means that within a chunk, values are always increasing, as shown in the sample documents in Figure 16-7. Thus, if you had one chunk per shard, you’d have the perfect setup: ascending writes on every shard, as shown in Figure 16-8. Of course, having n chunks with n hotspots spread across n shards isn’t very extensible: add a new shard and it won’t get any writes because there’s no hotspot chunk to put on it. Thus, you want a few hotspot chunks per shard (to give you room to grow), but not too many. Having a few hotspot chunks will keep the effectiveness of ascending writes, but having, say, a thousand hotspots on a shard will end up being equivalent to random writes.\n\nShard Key Strategies\n\n|\n\n331",
      "content_length": 865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "Figure 16-7. A sample list of inserted documents (note that all “_id” values are increasing)\n\n332\n\n|\n\nChapter 16: Choosing a Shard Key",
      "content_length": 134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "Figure 16-8. The inserted documents, split into chunks (note that, within each chunk, the “_id” values are increasing)\n\nYou can picture this setup as each chunk being a stack of ascending documents. There are multiple stacks on each shard, each ascending until the chunk is split. Once a chunk is split, only one of the new chunks will be a hotspot chunk: the other chunk will essentially be “dead” and never grow again. If the stacks are evenly distributed across the shards, writes will be evenly distributed.\n\nShard Key Strategies\n\n|\n\n333",
      "content_length": 541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "Shard Key Rules and Guidelines There are several practical restrictions to be aware of before choosing a shard key.\n\nDetermining which key to shard on and creating shard keys should be reminiscent of indexing because the two concepts are similar. In fact, often your shard key may just be the index you use most often (or some variation on it).\n\nShard Key Limitations Shard keys cannot be arrays. sh.shardCollection() will fail if any key has an array value, and inserting an array into that field is not allowed.\n\nOnce inserted, a document’s shard key value may be modified unless the shard key field is an immutable _id field. In older versions of MongoDB prior to 4.2, it was not possible to modify a document’s shard key value.\n\nMost special types of indexes cannot be used for shard keys. In particular, you cannot shard on a geospatial index. Using a hashed index for a shard key is allowed, as cov‐ ered previously.\n\nShard Key Cardinality Whether your shard key jumps around or increases steadily, it is important to choose a key with values that will vary. As with indexes, sharding performs better on high- cardinality fields. If, for example, you had a \"logLevel\" key that had only values \"DEBUG\", \"WARN\", or \"ERROR\", MongoDB wouldn’t be able to break up your data into more than three chunks (because there would be only three different values for the shard key). If you have a key with little variation and want to use it as a shard key anyway, you can do so by creating a compound shard key on that key and a key that varies more, like \"logLevel\" and \"timestamp\". It is important that the combination of keys has high cardinality.\n\nControlling Data Distribution Sometimes, automatic data distribution will not fit your requirements. This section gives you some options beyond choosing a shard key and allowing MongoDB to do everything automatically.\n\nAs your cluster gets larger or busier, these solutions become less practical. However, for small clusters, you may want more control.\n\n334\n\n|\n\nChapter 16: Choosing a Shard Key",
      "content_length": 2039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "Using a Cluster for Multiple Databases and Collections MongoDB evenly distributes collections across every shard in your cluster, which works well if you’re storing homogeneous data. However, if you have a log collection that is “lower value” than your other data, you might not want it taking up space on your more expensive servers. Or, if you have one powerful shard, you might want to use it for only a real-time collection and not allow other collections to use it. You can create separate clusters, but you can also give MongoDB specific directions about where you want it to put certain data.\n\nTo set this up, use the sh.addShardToZone() helper in the shell:\n\n> sh.addShardToZone(\"shard0000\", \"high\") > // shard0001 - no zone > // shard0002 - no zone > // shard0003 - no zone > sh.addShardToZone(\"shard0004\", \"low\") > sh.addShardToZone(\"shard0005\", \"low\")\n\nThen you can assign different collections to different shards. For instance, for your super-important real-time collection:\n\n> sh.updateZoneKeyRange(\"super.important\", {\"<shardKey>\" : MinKey}, ... {\"<shardKey>\" : MaxKey}, \"high\")\n\nThis says, “for negative infinity to infinity for this collection, store it on shards tagged \"high\".” This means that no data from the super.important collection will be stored on any other server. Note that this does not affect how other collections are dis‐ tributed: they will still be evenly distributed between this shard and the others.\n\nYou can perform a similar operation to keep the log collection on a low-quality server:\n\n> sh.updateZoneKeyRange(\"some.logs\", {\"<shardKey>\" : MinKey}, ... {\"<shardKey>\" : MaxKey}, \"low\")\n\nThe log collection will now be split evenly between shard0004 and shard0005.\n\nAssigning a zone key range to a collection does not affect it instantly. It is an instruc‐ tion to the balancer stating that, when it runs, these are the viable targets to move the collection to. Thus, if the entire log collection is on shard0002 or evenly distributed among the shards, it will take a little while for all of the chunks to be migrated to shard0004 and shard0005.\n\nAs another example, perhaps you have a collection that you don’t want on the shard zoned \"high\", but you don’t care which other shard it goes on. You can zone all of the non-high-performance shards to create a new grouping. Shards can have as many zones as you need:\n\nControlling Data Distribution\n\n|\n\n335",
      "content_length": 2391,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "> sh.addShardToZone(\"shard0001\", \"whatever\") > sh.addShardToZone(\"shard0002\", \"whatever\") > sh.addShardToZone(\"shard0003\", \"whatever\") > sh.addShardToZone(\"shard0004\", \"whatever\") > sh.addShardToZone(\"shard0005\", \"whatever\")\n\nNow you can specify that you want this collection (call it normal.coll) distributed across these five shards:\n\n> sh.updateZoneKeyRange(\"normal.coll\", {\"<shardKey>\" : MinKey}, ... {\"<shardKey>\" : MaxKey}, \"whatever\")\n\nYou cannot assign collections dynamically—i.e., you can’t say, “when a collection is created, randomly home it to a shard.” How‐ ever, you could have a cron job that went through and did this for you.\n\nIf you make a mistake or change your mind, you can remove a shard from a zone with sh.removeShardFromZone():\n\n> sh.removeShardFromZone(\"shard0005\", \"whatever\")\n\nIf you remove all shards from zones described by a zone key range (e.g., if you remove shard0000 from the zone \"high\"), the balancer won’t distribute the data any‐ where because there aren’t any valid locations listed. All the data will still be readable and writable; it just won’t be able to migrate until you modify your tags or tag ranges.\n\nTo remove a key range from a zone, use sh.removeRangeFromZone(). The following is an example. The range specified must be an exact match to a range previously defined for the namespace some.logs and a given zone:\n\n> sh.removeRangeFromZone(\"some.logs\", {\"<shardKey>\" : MinKey}, ... {\"<shardKey>\" : MaxKey})\n\nManual Sharding Sometimes, for complex requirements or special situations, you may prefer to have complete control over which data is distributed where. You can turn off the balancer if you don’t want data to be automatically distributed and use the moveChunk com‐ mand to manually distribute data.\n\nTo turn off the balancer, connect to a mongos (any mongos is fine) using the mongo shell and disable the balancer using the shell helper sh.stopBalancer():\n\n> sh.stopBalancer()\n\nIf there is currently a migrate in progress, this setting will not take effect until the migrate has completed. However, once any in-flight migrations have finished, the\n\n336\n\n|\n\nChapter 16: Choosing a Shard Key",
      "content_length": 2147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "balancer will stop moving data around. To verify no migrations are in progress after disabling, issue the following in the mongo shell:\n\n> use config > while(sh.isBalancerRunning()) { ... print(\"waiting...\"); ... sleep(1000); ... }\n\nOnce the balancer is off, you can move data around manually (if necessary). First, find out which chunks are where by looking at config.chunks:\n\n> db.chunks.find()\n\nNow, use the moveChunk command to migrate chunks to other shards. Specify the lower bound of the chunk to be migrated and give the name of the shard that you want to move the chunk to:\n\n> sh.moveChunk( ... \"test.manual.stuff\", ... {user_id: NumberLong(\"-1844674407370955160\")}, ... \"test-rs1\")\n\nHowever, unless you are in an exceptional situation, you should use MongoDB’s auto‐ matic sharding instead of doing it manually. If you end up with a hotspot on a shard that you weren’t expecting, you might end up with most of your data on that shard.\n\nIn particular, do not combine setting up unusual distributions manually with running the balancer. If the balancer detects an uneven number of chunks it will simply reshuffle all of your work to get the collection evenly balanced again. If you want uneven distribution of chunks, use the zone sharding technique discussed in “Using a Cluster for Multiple Databases and Collections” on page 335.\n\nControlling Data Distribution\n\n|\n\n337",
      "content_length": 1379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "CHAPTER 17 Sharding Administration\n\nAs with replica sets, you have a number of options for administering sharded clusters. Manual administration is one option. These days it is becoming increasingly com‐ mon to use tools such as Ops Manager and Cloud Manager and the Atlas Database- as-a-Service (DBaaS) offering for all cluster administration. In this chapter, we will demonstrate how to administer a sharded cluster manually, including:\n\nInspecting the cluster’s state: who its members are, where data is held, and what connections are open\n\nAdding, removing, and changing members of a cluster\n\nAdministering data movement and manually moving data\n\nSeeing the Current State There are several helpers available to find out what data is where, what the shards are, and what the cluster is doing.\n\nGetting a Summary with sh.status() sh.status() gives you an overview of your shards, databases, and sharded collec‐ tions. If you have a small number of chunks, it will print a breakdown of which chunks are where as well. Otherwise it will simply give the collection’s shard key and report how many chunks each shard has:\n\n> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5,\n\n339",
      "content_length": 1221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "340\n\n\"currentVersion\" : 6, \"clusterId\" : ObjectId(\"5bdf51ecf8c192ed922f3160\") } shards: { \"_id\" : \"shard01\", \"host\" : \"shard01/localhost:27018,localhost:27019,localhost:27020\", \"state\" : 1 } { \"_id\" : \"shard02\", \"host\" : \"shard02/localhost:27021,localhost:27022,localhost:27023\", \"state\" : 1 } { \"_id\" : \"shard03\", \"host\" : \"shard03/localhost:27024,localhost:27025,localhost:27026\", \"state\" : 1 } active mongoses: \"4.0.3\" : 1 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: 6 : Success databases: { \"_id\" : \"config\", \"primary\" : \"config\", \"partitioned\" : true } config.system.sessions shard key: { \"_id\" : 1 } unique: false balancing: true chunks: shard01 { \"_id\" : { \"$minKey\" : 1 } } -->> { \"_id\" : { \"$maxKey\" : 1 } } on : shard01 Timestamp(1, 0) { \"_id\" : \"video\", \"primary\" : \"shard02\", \"partitioned\" : true, \"version\" : { \"uuid\" : UUID(\"3d83d8b8-9260-4a6f-8d28-c3732d40d961\"), \"lastMod\" : 1 } } video.movies shard key: { \"imdbId\" : \"hashed\" } unique: false balancing: true chunks: shard01 3 shard02 4 shard03 3 { \"imdbId\" : { \"$minKey\" : 1 } } -->> { \"imdbId\" : NumberLong(\"-7262221363006655132\") } on : shard01 Timestamp(2, 0) { \"imdbId\" : NumberLong(\"-7262221363006655132\") } -->> { \"imdbId\" : NumberLong(\"-5315530662268120007\") } on : shard03 Timestamp(3, 0) { \"imdbId\" : NumberLong(\"-5315530662268120007\") } -->>\n\n1\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 1492,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "{ \"imdbId\" : NumberLong(\"-3362204802044524341\") } on : shard03 Timestamp(4, 0) { \"imdbId\" : NumberLong(\"-3362204802044524341\") } -->> { \"imdbId\" : NumberLong(\"-1412311662519947087\") } on : shard01 Timestamp(5, 0) { \"imdbId\" : NumberLong(\"-1412311662519947087\") } -->> { \"imdbId\" : NumberLong(\"524277486033652998\") } on : shard01 Timestamp(6, 0) { \"imdbId\" : NumberLong(\"524277486033652998\") } -->> { \"imdbId\" : NumberLong(\"2484315172280977547\") } on : shard03 Timestamp(7, 0) { \"imdbId\" : NumberLong(\"2484315172280977547\") } -->> { \"imdbId\" : NumberLong(\"4436141279217488250\") } on : shard02 Timestamp(7, 1) { \"imdbId\" : NumberLong(\"4436141279217488250\") } -->> { \"imdbId\" : NumberLong(\"6386258634539951337\") } on : shard02 Timestamp(1, 7) { \"imdbId\" : NumberLong(\"6386258634539951337\") } -->> { \"imdbId\" : NumberLong(\"8345072417171006784\") } on : shard02 Timestamp(1, 8) { \"imdbId\" : NumberLong(\"8345072417171006784\") } -->> { \"imdbId\" : { \"$maxKey\" : 1 } } on : shard02 Timestamp(1, 9)\n\nOnce there are more than a few chunks, sh.status() will summarize the chunk stats instead of printing each chunk. To see all chunks, run sh.status(true) (the true tells sh.status() to be verbose).\n\nAll the information sh.status() shows is gathered from your config database.\n\nSeeing Configuration Information All of the configuration information about your cluster is kept in collections in the config database on the config servers. The shell has several helpers for exposing this information in a more readable way. However, you can always directly query the con‐ fig database for metadata about your cluster.\n\nNever connect directly to your config servers, as you do not want to take the chance of accidentally changing or removing config server data. Instead, connect to the mongos process and use the config database to see its data, as you would for any other database:\n\n> use config\n\nIf you manipulate config data through mongos (instead of connect‐ ing directly to the config servers), mongos will ensure that all of your config servers stay in sync and prevent various dangerous actions like accidentally dropping the config database.\n\nSeeing the Current State\n\n|\n\n341",
      "content_length": 2166,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "In general, you should not directly change any data in the config database (exceptions are noted in the following sections). If you change anything, you will generally have to restart all of your mongos servers to see its effect.\n\nThere are several collections in the config database. This section covers what each one contains and how it can be used.\n\nconfig.shards\n\nThe shards collection keeps track of all the shards in the cluster. A typical document in the shards collection might look something like this:\n\n> db.shards.find() { \"_id\" : \"shard01\", \"host\" : \"shard01/localhost:27018,localhost:27019,localhost:27020\", \"state\" : 1 } { \"_id\" : \"shard02\", \"host\" : \"shard02/localhost:27021,localhost:27022,localhost:27023\", \"state\" : 1 } { \"_id\" : \"shard03\", \"host\" : \"shard03/localhost:27024,localhost:27025,localhost:27026\", \"state\" : 1 }\n\nThe shard’s \"_id\" is picked up from the replica set name, so each replica set in your cluster must have a unique name.\n\nWhen you update your replica set configuration (e.g., adding or removing members), the \"host\" field will be updated automatically.\n\nconfig.databases\n\nThe databases collection keeps track of all of the databases, sharded and not, that the cluster knows about:\n\n> db.databases.find() { \"_id\" : \"video\", \"primary\" : \"shard02\", \"partitioned\" : true, \"version\" : { \"uuid\" : UUID(\"3d83d8b8-9260-4a6f-8d28-c3732d40d961\"), \"lastMod\" : 1 } }\n\nIf enableSharding has been run on a database, \"partitioned\" will be true. The \"pri mary\" is the database’s “home base.” By default, all new collections in that database will be created on the database’s primary shard.\n\nconfig.collections\n\nThe collections collection keeps track of all sharded collections (nonsharded collec‐ tions are not shown). A typical document looks something like this:\n\n> db.collections.find().pretty() { \"_id\" : \"config.system.sessions\",\n\n342\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 1903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "\"lastmodEpoch\" : ObjectId(\"5bdf53122ad9c6907510c22d\"), \"lastmod\" : ISODate(\"1970-02-19T17:02:47.296Z\"), \"dropped\" : false, \"key\" : { \"_id\" : 1 }, \"unique\" : false, \"uuid\" : UUID(\"7584e4cd-fac4-4305-a9d4-bd73e93621bf\") } { \"_id\" : \"video.movies\", \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\"), \"lastmod\" : ISODate(\"1970-02-19T17:02:47.305Z\"), \"dropped\" : false, \"key\" : { \"imdbId\" : \"hashed\" }, \"unique\" : false, \"uuid\" : UUID(\"e6580ffa-fcd3-418f-aa1a-0dfb71bc1c41\") }\n\nThe important fields are:\n\n\"_id\"\n\nThe namespace of the collection.\n\n\"key\"\n\nThe shard key. In this case, it is a hashed shard key on \"imdbId\".\n\n\"unique\"\n\nIndicates that the shard key is not a unique index. By default, the shard key is not unique.\n\nconfig.chunks\n\nThe chunks collection keeps a record of each chunk in all the collections. A typical document in the chunks collection looks something like this:\n\n> db.chunks.find().skip(1).limit(1).pretty() { \"_id\" : \"video.movies-imdbId_MinKey\", \"lastmod\" : Timestamp(2, 0), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\"), \"ns\" : \"video.movies\", \"min\" : { \"imdbId\" : { \"$minKey\" : 1 } }, \"max\" : { \"imdbId\" : NumberLong(\"-7262221363006655132\") }, \"shard\" : \"shard01\", \"history\" : [\n\nSeeing the Current State\n\n|\n\n343",
      "content_length": 1247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "{ \"validAfter\" : Timestamp(1541370579, 3096), \"shard\" : \"shard01\" } ] }\n\nThe most useful fields are:\n\n\"_id\"\n\nThe unique identifier for the chunk. Generally this is the namespace, shard key, and lower chunk boundary.\n\n\"ns\"\n\nThe collection that this chunk is from.\n\n\"min\"\n\nThe smallest value in the chunk’s range (inclusive).\n\n\"max\"\n\nAll values in the chunk are smaller than this value.\n\n\"shard\"\n\nWhich shard the chunk resides on.\n\nThe \"lastmod\" field tracks chunk versioning. For example, if the chunk \"video.movies-imdbId_MinKey\" were split into two chunks, we’d want a way of dis‐ tinguishing the new, smaller \"video.movies-imdbId_MinKey\" chunks from their pre‐ vious incarnation as a single chunk. Thus, the first component of the Timestamp value reflects the number of times a chunk has been migrated to a new shard. The second component of this value reflects the number of splits. The \"lastmodEpoch\" field specifies the collection’s creation epoch. It is used to differentiate requests for the same collection name in the cases where the collection was dropped and immediately recreated.\n\nsh.status() uses the config.chunks collection to gather most of its information.\n\nconfig.changelog\n\nThe changelog collection is useful for keeping track of what a cluster is doing, since it records all of the splits and migrations that have occurred.\n\nSplits are recorded in a document that looks like this:\n\n> db.changelog.find({what: \"split\"}).pretty() { \"_id\" : \"router1-2018-11-05T09:58:58.915-0500-5be05ab2f8c192ed922ffbe7\", \"server\" : \"bob\", \"clientAddr\" : \"127.0.0.1:64621\", \"time\" : ISODate(\"2018-11-05T14:58:58.915Z\"),\n\n344\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 1666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "\"what\" : \"split\", \"ns\" : \"video.movies\", \"details\" : { \"before\" : { \"min\" : { \"imdbId\" : NumberLong(\"2484315172280977547\") }, \"max\" : { \"imdbId\" : NumberLong(\"4436141279217488250\") }, \"lastmod\" : Timestamp(9, 1), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\") }, \"left\" : { \"min\" : { \"imdbId\" : NumberLong(\"2484315172280977547\") }, \"max\" : { \"imdbId\" : NumberLong(\"3459137475094092005\") }, \"lastmod\" : Timestamp(9, 2), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\") }, \"right\" : { \"min\" : { \"imdbId\" : NumberLong(\"3459137475094092005\") }, \"max\" : { \"imdbId\" : NumberLong(\"4436141279217488250\") }, \"lastmod\" : Timestamp(9, 3), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\") } } }\n\nThe \"details\" field gives information about what the original document looked like and what it was split into.\n\nThis output shows what the first chunk split of a collection looks like. Note that the second component of \"lastmod\" for each new chunk was updated so that the values are Timestamp(9, 2) and Timestamp(9, 3), respectively.\n\nMigrations are a bit more complicated and actually create four separate changelog documents: one noting the start of the migrate, one for the “from” shard, one for the “to” shard, and one for the commit that occurs when the migration is finalized. The middle two documents are of interest because these give a breakdown of how long each step in the process took. This can give you an idea of whether it’s the disk, net‐ work, or something else that is causing a bottleneck on migrates.\n\nFor example, the document created by the “from” shard looks like this:\n\nSeeing the Current State\n\n|\n\n345",
      "content_length": 1627,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "> db.changelog.findOne({what: \"moveChunk.to\"}) { \"_id\" : \"router1-2018-11-04T17:29:39.702-0500-5bdf72d32ad9c69075112f08\", \"server\" : \"bob\", \"clientAddr\" : \"\", \"time\" : ISODate(\"2018-11-04T22:29:39.702Z\"), \"what\" : \"moveChunk.to\", \"ns\" : \"video.movies\", \"details\" : { \"min\" : { \"imdbId\" : { \"$minKey\" : 1 } }, \"max\" : { \"imdbId\" : NumberLong(\"-7262221363006655132\") }, \"step 1 of 6\" : 965, \"step 2 of 6\" : 608, \"step 3 of 6\" : 15424, \"step 4 of 6\" : 0, \"step 5 of 6\" : 72, \"step 6 of 6\" : 258, \"note\" : \"success\" } }\n\nEach of the steps listed in \"details\" is timed and the \"stepN of N\" messages show how long each step took, in milliseconds.\n\nWhen the “from” shard receives a moveChunk command from the mongos, it:\n\n1. Checks the command parameters.\n\n2. Confirms with the config servers that it can acquire a distributed lock for the migrate.\n\n3. Tries to contact the “to” shard.\n\n4. Copies the data. This is referred to and logged as “the critical section.”\n\n5. Coordinates with the “to” shard and config servers to confirm the migration.\n\nNote that the “to” and “from” shards must be in close communication starting at \"step4 of 6\": the shards directly talk to one another and the config server to per‐ form the migration. If the “from” server has flaky network connectivity during the final steps, it may end up in a state where it cannot undo the migration and cannot move forward with it. In this case, the mongod will shut down.\n\nThe “to” shard’s changelog document is similar to the “from” shard’s, but the steps are a bit different. It looks like this:\n\n> db.changelog.find({what: \"moveChunk.from\", \"details.max.imdbId\": NumberLong(\"-7262221363006655132\")}).pretty() {\n\n346\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 1720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "\"_id\" : \"router1-2018-11-04T17:29:39.753-0500-5bdf72d321b6e3be02fabf0b\", \"server\" : \"bob\", \"clientAddr\" : \"127.0.0.1:64743\", \"time\" : ISODate(\"2018-11-04T22:29:39.753Z\"), \"what\" : \"moveChunk.from\", \"ns\" : \"video.movies\", \"details\" : { \"min\" : { \"imdbId\" : { \"$minKey\" : 1 } }, \"max\" : { \"imdbId\" : NumberLong(\"-7262221363006655132\") }, \"step 1 of 6\" : 0, \"step 2 of 6\" : 4, \"step 3 of 6\" : 191, \"step 4 of 6\" : 17000, \"step 5 of 6\" : 341, \"step 6 of 6\" : 39, \"to\" : \"shard01\", \"from\" : \"shard02\", \"note\" : \"success\" } }\n\nWhen the “to” shard receives a command from the “from” shard, it:\n\n1. Migrates indexes. If this shard has never held chunks from the migrated collec‐ tion before, it needs to know what fields are indexed. If this isn’t the first time a chunk from this collection is being moved to this shard, then this should be a no-op.\n\n2. Deletes any existing data in the chunk range. There might be data left over from a failed migration or restore procedure that we wouldn’t want to interfere with the current data.\n\n3. Copies all documents in the chunk to the “to” shard.\n\n4. Replays any operations that happened to these documents during the copy (on the “to” shard).\n\n5. Waits for the “to” shard to have replicated the newly migrated data to a majority of servers.\n\n6. Commits the migrate by changing the chunk’s metadata to say that it lives on the “to” shard.\n\nconfig.settings\n\nThis collection contains documents representing the current balancer settings and chunk size. By changing the documents in this collection, you can turn the balancer\n\nSeeing the Current State\n\n|\n\n347",
      "content_length": 1592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "on or off or change the chunk size. Note that you should always connect to mongos, not the config servers directly, to change values in this collection.\n\nTracking Network Connections There are a lot of connections between the components of a cluster. This section cov‐ ers some sharding-specific information (see Chapter 24 for more information on networking).\n\nGetting Connection Statistics The command connPoolStats returns information regarding the open outgoing con‐ nections from the current database instance to other members of the sharded cluster or replica set.\n\nTo avoid interference with any running operations, connPoolStats does not take any locks. As such, the counts may change slightly as connPoolStats gathers information, resulting in slight differences between the hosts and pools connection counts:\n\n> db.adminCommand({\"connPoolStats\": 1}) { \"numClientConnections\" : 10, \"numAScopedConnections\" : 0, \"totalInUse\" : 0, \"totalAvailable\" : 13, \"totalCreated\" : 86, \"totalRefreshing\" : 0, \"pools\" : { \"NetworkInterfaceTL-TaskExecutorPool-0\" : { \"poolInUse\" : 0, \"poolAvailable\" : 2, \"poolCreated\" : 2, \"poolRefreshing\" : 0, \"localhost:27027\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 1, \"refreshing\" : 0 }, \"localhost:27019\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 1, \"refreshing\" : 0 } }, \"NetworkInterfaceTL-ShardRegistry\" : { \"poolInUse\" : 0, \"poolAvailable\" : 1, \"poolCreated\" : 13,\n\n348\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 1457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "\"poolRefreshing\" : 0, \"localhost:27027\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 13, \"refreshing\" : 0 } }, \"global\" : { \"poolInUse\" : 0, \"poolAvailable\" : 10, \"poolCreated\" : 71, \"poolRefreshing\" : 0, \"localhost:27026\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27027\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 1, \"refreshing\" : 0 }, \"localhost:27023\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 7, \"refreshing\" : 0 }, \"localhost:27024\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 6, \"refreshing\" : 0 }, \"localhost:27022\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 9, \"refreshing\" : 0 }, \"localhost:27019\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27021\" : { \"inUse\" : 0, \"available\" : 1,\n\nTracking Network Connections\n\n|\n\n349",
      "content_length": 817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "350\n\n\"created\" : 8, \"refreshing\" : 0 }, \"localhost:27025\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 9, \"refreshing\" : 0 }, \"localhost:27020\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27018\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 7, \"refreshing\" : 0 } } }, \"hosts\" : { \"localhost:27026\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27027\" : { \"inUse\" : 0, \"available\" : 3, \"created\" : 15, \"refreshing\" : 0 }, \"localhost:27023\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 7, \"refreshing\" : 0 }, \"localhost:27024\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 6, \"refreshing\" : 0 }, \"localhost:27022\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 9,\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 772,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "\"refreshing\" : 0 }, \"localhost:27019\" : { \"inUse\" : 0, \"available\" : 2, \"created\" : 9, \"refreshing\" : 0 }, \"localhost:27021\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27025\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 9, \"refreshing\" : 0 }, \"localhost:27020\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 8, \"refreshing\" : 0 }, \"localhost:27018\" : { \"inUse\" : 0, \"available\" : 1, \"created\" : 7, \"refreshing\" : 0 } }, \"replicaSets\" : { \"shard02\" : { \"hosts\" : [ { \"addr\" : \"localhost:27021\", \"ok\" : true, \"ismaster\" : true, \"hidden\" : false, \"secondary\" : false, \"pingTimeMillis\" : 0 }, { \"addr\" : \"localhost:27022\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 },\n\nTracking Network Connections\n\n|\n\n351",
      "content_length": 788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "352\n\n{ \"addr\" : \"localhost:27023\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 } ] }, \"shard03\" : { \"hosts\" : [ { \"addr\" : \"localhost:27024\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 }, { \"addr\" : \"localhost:27025\", \"ok\" : true, \"ismaster\" : true, \"hidden\" : false, \"secondary\" : false, \"pingTimeMillis\" : 0 }, { \"addr\" : \"localhost:27026\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 } ] }, \"configRepl\" : { \"hosts\" : [ { \"addr\" : \"localhost:27027\", \"ok\" : true, \"ismaster\" : true, \"hidden\" : false, \"secondary\" : false, \"pingTimeMillis\" : 0 } ] }, \"shard01\" : { \"hosts\" : [\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "{ \"addr\" : \"localhost:27018\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 }, { \"addr\" : \"localhost:27019\", \"ok\" : true, \"ismaster\" : true, \"hidden\" : false, \"secondary\" : false, \"pingTimeMillis\" : 0 }, { \"addr\" : \"localhost:27020\", \"ok\" : true, \"ismaster\" : false, \"hidden\" : false, \"secondary\" : true, \"pingTimeMillis\" : 0 } ] } }, \"ok\" : 1, \"operationTime\" : Timestamp(1541440424, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541440424, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nIn this output:\n\n\"totalAvailable\" shows the total number of available outgoing connections from the current mongod/mongos instance to other members of the sharded clus‐ ter or replica set.\n\n\"totalCreated\" reports the total number of outgoing connections ever created by the current mongod/mongos instance to other members of the sharded cluster or replica set.\n\n\"totalInUse\" provides the total number of outgoing connections from the cur‐ rent mongod/mongos instance to other members of the sharded cluster or replica set that are currently in use.\n\nTracking Network Connections\n\n|\n\n353",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "\"totalRefreshing\" displays the total number of outgoing connections from the current mongod/mongos instance to other members of the sharded cluster or rep‐ lica set that are currently being refreshed.\n\n\"numClientConnections\" identifies the number of active and stored outgoing synchronous connections from the current mongod/mongos instance to other members of the sharded cluster or replica set. These represent a subset of the connections and \"totalInUse\". by\n\n\"numAScopedConnection\" reports the number of active and stored outgoing scoped synchronous connections from the current mongod/mongos instance to other members of the sharded cluster or replica set. These represent a subset of the connections reported by \"totalAvailable\", \"totalCreated\", and \"totalInUse\".\n\n\"pools\" shows connection statistics (in use/available/created/refreshing) grou‐ ped by the connection pools. A mongod or mongos has two distinct families of outgoing connection pools: — DBClient-based pools (the “write path,” identified by the field name \"global\" in the \"pools\" document)\n\n— NetworkInterfaceTL-based pools (the “read path”)\n\n\"hosts\" shows connection statistics (in use/available/created/refreshing) grou‐ ped by the hosts. It reports on connections between the current mongod/mongos instance and each member of the sharded cluster or replica set.\n\nYou might see connections to other shards in the output of connPoolStats. These indicate that shards are connecting to other shards to migrate data. The primary of one shard will connect directly to the primary of another shard and “suck” its data.\n\nWhen a migrate occurs, a shard sets up a ReplicaSetMonitor (a process that moni‐ tors replica set health) to track the health of the shard on the other side of the migrate. mongod never destroys this monitor, so you may see messages in one replica set’s log about the members of another replica set. This is totally normal and should have no effect on your application.\n\nLimiting the Number of Connections When a client connects to a mongos, the mongos creates a connection to at least one shard to pass along the client’s request. Thus, every client connection into a mongos yields at least one outgoing connection from mongos to the shards.\n\nIf you have many mongos processes, they may create more connections than your shards can handle: by default a mongos will accept up to 65,536 connections (the\n\n354\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 2433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "same as mongod), so if you have 5 mongos processes with 10,000 client connections each, they may be attempting to create 50,000 connections to a shard!\n\nTo prevent this, you can use the --maxConns option to your command-line configura‐ tion for mongos to limit the number of connections it can create. The following for‐ mula can be used to calculate the maximum number of connections a shard can handle from a single mongos:\n\nmaxConns = maxConnsPrimary − (numMembersPerReplicaSet × 3) − (other x 3) / numMongosProcesses\n\nBreaking down the pieces of this formula:\n\nmaxConnsPrimary\n\nThe maximum number of connections on the Primary, typically set to 20,000 to avoid overwhelming the shard with connections from the mongos.\n\n(numMembersPerReplicaSet × 3)\n\nThe primary creates a connection to each secondary and each secondary creates two connections to the primary, for a total of three connections.\n\n(other x 3)\n\nOther is the number of miscellaneous processes that may connect to your mon‐ gods, such as monitoring or backup agents, direct shell connections (for adminis‐ tration), or connections to other shards for migrations.\n\nnumMongosProcesses\n\nThe total number of mongos in the sharded cluster.\n\nNote that --maxConns only prevents mongos from creating more than this many con‐ nections. It doesn’t do anything particularly helpful when this limit is reached: it will simply block requests, waiting for connections to be “freed.” Thus, you must prevent your application from using this many connections, especially as your number of mongos processes grows.\n\nWhen a MongoDB instance exits cleanly it closes all connections before stopping. The members that were connected to it will immediately get socket errors on those connections and be able to refresh them. However, if a MongoDB instance suddenly goes offline due to a power loss, crash, or network problems, it probably won’t cleanly close all of its sockets. In this case, other servers in the cluster may be under the impression that their connection is healthy until they try to perform an operation on it. At that point, they will get an error and refresh the connection (if the member is up again at that point).\n\nThis is a quick process when there are only a few connections. However, when there are thousands of connections that must be refreshed one by one you can get a lot of\n\nTracking Network Connections\n\n|\n\n355",
      "content_length": 2383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "errors because each connection to the downed member must be tried, determined to be bad, and reestablished. There isn’t a particularly good way of preventing this, aside from restarting processes that get bogged down in a reconnection storm.\n\nServer Administration As your cluster grows, you’ll need to add capacity or change configurations. This sec‐ tion covers how to add and remove servers in your cluster.\n\nAdding Servers You can add new mongos processes at any time. Make sure their --configdb option specifies the correct set of config servers and they should be immediately available for clients to connect to.\n\nTo add new shards, use the addShard command as shown in Chapter 15.\n\nChanging Servers in a Shard As you use your sharded cluster, you may want to change the servers in individual shards. To change a shard’s membership, connect directly to the shard’s primary (not through the mongos) and issue a replica set reconfig. The cluster configuration will pick up the change and update config.shards automatically. Do not modify con‐ fig.shards by hand.\n\nThe only exception to this is if you started your cluster with standalone servers as shards, not replica sets.\n\nChanging a shard from a standalone server to a replica set\n\nThe easiest way to do this is to add a new, empty replica set shard and then remove the standalone server shard (as discussed in the next section). Migrations will take care of moving your data to the new shard.\n\nRemoving a Shard In general, shards should not be removed from a cluster. If you are regularly adding and removing shards, you are putting a lot more stress on the system than necessary. If you add too many shards it is better to let your system grow into them, not remove them and add them back later. However, if necessary, you can remove shards.\n\nFirst make sure that the balancer is on. The balancer will be tasked with moving all the data on the shard you want to remove to other shards in a process called draining. To start draining, run the removeShard command. removeShard takes the shard’s name and drains all the chunks on that shard to the other shards:\n\n356\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 2163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "> db.adminCommand({\"removeShard\" : \"shard03\"}) { \"msg\" : \"draining started successfully\", \"state\" : \"started\", \"shard\" : \"shard03\", \"note\" : \"you need to drop or movePrimary these databases\", \"dbsToMove\" : [ ], \"ok\" : 1, \"operationTime\" : Timestamp(1541450091, 2), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450091, 2), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nDraining can take a long time if there are a lot of chunks or large chunks to move. If you have jumbo chunks (see “Jumbo Chunks” on page 364), you may have to tem‐ porarily increase the chunk size to allow draining to move them.\n\nIf you want to keep tabs on how much has been moved, run removeShard again to give you the current status:\n\n> db.adminCommand({\"removeShard\" : \"shard02\"}) { \"msg\" : \"draining ongoing\", \"state\" : \"ongoing\", \"remaining\" : { \"chunks\" : NumberLong(3), \"dbs\" : NumberLong(0) }, \"note\" : \"you need to drop or movePrimary these databases\", \"dbsToMove\" : [ \"video\" ], \"ok\" : 1, \"operationTime\" : Timestamp(1541450139, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450139, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nYou can run removeShard as many times as you want.\n\nServer Administration\n\n|\n\n357",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "Chunks may have to be split to be moved, so you may see the number of chunks increase in the system during the drain. For example, suppose we have a five-shard cluster with the following chunk distributions:\n\ntest-rs0 10 test-rs1 10 test-rs2 10 test-rs3 11 test-rs4 11\n\nThis cluster has a total of 52 chunks. If we remove test-rs3, we might end up with:\n\ntest-rs0 15 test-rs1 15 test-rs2 15 test-rs4 15\n\nThe cluster now has 60 chunks, 18 of which came from shard test-rs3 (11 were there to start and 7 were created from draining splits).\n\nOnce all the chunks have been moved, if there are still databases that have the removed shard as their primary, you’ll need to remove them before the shard can be removed. Each database in a sharded cluster has a primary shard. If the shard you want to remove is also the primary of one of the cluster’s databases, removeShard lists the database in the \"dbsToMove\" field. To finish removing the shard, you must either move the database to a new shard after migrating all data from the shard or drop the database, deleting the associated data files. The output of removeShard will be some‐ thing like:\n\n> db.adminCommand({\"removeShard\" : \"shard02\"}) { \"msg\" : \"draining ongoing\", \"state\" : \"ongoing\", \"remaining\" : { \"chunks\" : NumberLong(3), \"dbs\" : NumberLong(0) }, \"note\" : \"you need to drop or movePrimary these databases\", \"dbsToMove\" : [ \"video\" ], \"ok\" : 1, \"operationTime\" : Timestamp(1541450139, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450139, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\n358\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 1650,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "To finish the remove, move the listed databases with the movePrimary command:\n\n> db.adminCommand({\"movePrimary\" : \"video\", \"to\" : \"shard01\"}) { \"ok\" : 1, \"operationTime\" : Timestamp(1541450554, 12), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450554, 12), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nOnce you have done this, run removeShard one more time:\n\n> db.adminCommand({\"removeShard\" : \"shard02\"}) { \"msg\" : \"removeshard completed successfully\", \"state\" : \"completed\", \"shard\" : \"shard03\", \"ok\" : 1, \"operationTime\" : Timestamp(1541450619, 2), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450619, 2), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nThis is not strictly necessary, but it confirms that you have completed the process. If there are no databases that have this shard as their primary, you will get this response as soon as all chunks have been migrated off the shard.\n\nOnce you have started a shard draining, there is no built-in way to stop it.\n\nBalancing Data In general, MongoDB automatically takes care of balancing data. This section covers how to enable and disable this automatic balancing as well as how to intervene in the balancing process.\n\nBalancing Data\n\n|\n\n359",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "The Balancer Turning off the balancer is a prerequisite to nearly any administrative activity. There is a shell helper to make this easier:\n\n> sh.setBalancerState(false) { \"ok\" : 1, \"operationTime\" : Timestamp(1541450923, 2), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541450923, 2), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nWith the balancer off a new balancing round will not begin, but turning it off will not force an ongoing balancing round to stop immediately—migrations generally cannot stop on a dime. Thus, you should check the config.locks collection to see whether or not a balancing round is still in progress:\n\n> db.locks.find({\"_id\" : \"balancer\"})[\"state\"] 0\n\n0 means the balancer is off.\n\nBalancing puts load on your system: the destination shard must query the source shard for all the documents in a chunk and insert them, and then the source shard must delete them. There are two circumstances in particular where migrations can cause performance problems:\n\n1. Using a hotspot shard key will force constant migrations (as all new chunks will be created on the hotspot). Your system must have the capacity to handle the flow of data coming off of your hotspot shard.\n\n2. Adding a new shard will trigger a stream of migrations as the balancer attempts to populate it.\n\nIf you find that migrations are affecting your application’s performance, you can schedule a window for balancing in the config.settings collection. Run the following update to only allow balancing between 1 p.m. and 4 p.m. First make sure the bal‐ ancer is on, then schedule the window:\n\n> sh.setBalancerState( true ) { \"ok\" : 1, \"operationTime\" : Timestamp(1541451846, 4), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541451846, 4),\n\n360\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 1826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "\"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } > db.settings.update( { _id: \"balancer\" }, { $set: { activeWindow : { start : \"13:00\", stop : \"16:00\" } } }, { upsert: true } ) WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n\nIf you set a balancing window, monitor it closely to ensure that mongos can actually keep your cluster balanced in the time that you have allotted it.\n\nYou must be careful if you plan to combine manual balancing with the automatic bal‐ ancer, since the automatic balancer always determines what to move based on the current state of the set and does not take into account the set’s history. For example, suppose you have shardA and shardB, each holding 500 chunks. shardA is getting a lot of writes, so you turn off the balancer and move 30 of the most active chunks to shardB. If you turn the balancer back on at this point, it will immediately swoop in and move 30 chunks (possibly a different 30) back from shardB to shardA to balance the chunk counts.\n\nTo prevent this, move 30 quiescent chunks from shardB to shardA before starting the balancer. That way there will be no imbalance between the shards and the balancer will be happy to leave things as they are. Alternatively, you could perform 30 splits on shardA’s chunks to even out the chunk counts.\n\nNote that the balancer only uses number of chunks as a metric, not size of data. Mov‐ ing a chunk is called a migration and is how MongoDB balances data across your cluster. Thus, a shard with a few large chunks may end up as the target of a migration from a shard with many small chunks (but a smaller data size).\n\nChanging Chunk Size There can be anywhere from zero to millions of documents in a chunk. Generally, the larger a chunk is, the longer it takes to migrate to another shard. In Chapter 14, we used a chunk size of 1 MB, so that we could see chunk movement easily and quickly. This is generally impractical in a live system; MongoDB would be doing a lot of unnecessary work to keep shards within a few megabytes of each other in size. By default chunks are 64 MB, which generally provides a good balance between ease of migration and migratory churn.\n\nSometimes you may find that migrations are taking too long with 64 MB chunks. To speed them up, you can decrease your chunk size. To do this, connect to mongos through the shell and update the config.settings collection:\n\nBalancing Data\n\n|\n\n361",
      "content_length": 2460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "> db.settings.findOne() { \"_id\" : \"chunksize\", \"value\" : 64 } > db.settings.save({\"_id\" : \"chunksize\", \"value\" : 32}) WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n\nThe previous update would change your chunk size to 32 MB. Existing chunks would not be changed immediately, however; automatic splitting only occurs on insert or update. Thus, if you lower the chunk size, it may take time for all chunks to split to the new size.\n\nSplits cannot be undone. If you increase the chunk size, existing chunks grow only through insertion or updates until they reach the new size. The allowed range of the chunk size is between 1 and 1,024 MB, inclusive.\n\nNote that this is a cluster-wide setting: it affects all collections and databases. Thus, if you need a small chunk size for one collection and a large chunk size for another, you may have to compromise with a chunk size in between the two ideals (or put the col‐ lections in different clusters).\n\nIf MongoDB is doing too many migrations or your documents are large, you may want to increase your chunk size.\n\nMoving Chunks As mentioned earlier, all the data in a chunk lives on a certain shard. If that shard ends up with more chunks than the other shards, MongoDB will move some chunks off it.\n\nYou can manually move chunks using the moveChunk shell helper:\n\n> sh.moveChunk(\"video.movies\", {imdbId: 500000}, \"shard02\") { \"millis\" : 4079, \"ok\" : 1 }\n\nThis would move the chunk containing the document with an \"imdbId\" of 500000 to the shard named shard02. You must use the shard key (\"imdbId\", in this case) to find which chunk to move. Generally, the easiest way to specify a chunk is by its lower bound, although any value in the chunk will work (the upper bound will not, as it is not actually in the chunk). This command will move the chunk before returning, so it may take a while to run. The logs are the best place to see what it is doing if it takes a long time.\n\nIf a chunk is larger than the max chunk size, mongos will refuse to move it:\n\n362\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 2062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "> sh.moveChunk(\"video.movies\", {imdbId: NumberLong(\"8345072417171006784\")}, \"shard02\") { \"cause\" : { \"chunkTooBig\" : true, \"estimatedChunkSize\" : 2214960, \"ok\" : 0, \"errmsg\" : \"chunk too big to move\" }, \"ok\" : 0, \"errmsg\" : \"move failed\" }\n\nIn this case, you must manually split the chunk before moving it, using the splitAt command:\n\n> db.chunks.find({ns: \"video.movies\", \"min.imdbId\": NumberLong(\"6386258634539951337\")}).pretty() { \"_id\" : \"video.movies-imdbId_6386258634539951337\", \"ns\" : \"video.movies\", \"min\" : { \"imdbId\" : NumberLong(\"6386258634539951337\") }, \"max\" : { \"imdbId\" : NumberLong(\"8345072417171006784\") }, \"shard\" : \"shard02\", \"lastmod\" : Timestamp(1, 9), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\"), \"history\" : [ { \"validAfter\" : Timestamp(1541370559, 4), \"shard\" : \"shard02\" } ] } > sh.splitAt(\"video.movies\", {\"imdbId\": NumberLong(\"7000000000000000000\")}) { \"ok\" : 1, \"operationTime\" : Timestamp(1541453304, 1), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541453306, 5), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } > db.chunks.find({ns: \"video.movies\", \"min.imdbId\": NumberLong(\"6386258634539951337\")}).pretty() {\n\nBalancing Data\n\n|\n\n363",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "\"_id\" : \"video.movies-imdbId_6386258634539951337\", \"lastmod\" : Timestamp(15, 2), \"lastmodEpoch\" : ObjectId(\"5bdf72c021b6e3be02fabe0c\"), \"ns\" : \"video.movies\", \"min\" : { \"imdbId\" : NumberLong(\"6386258634539951337\") }, \"max\" : { \"imdbId\" : NumberLong(\"7000000000000000000\") }, \"shard\" : \"shard02\", \"history\" : [ { \"validAfter\" : Timestamp(1541370559, 4), \"shard\" : \"shard02\" } ] }\n\nOnce the chunk has been split into smaller pieces, it should be movable. Alternatively, you can raise the max chunk size and then move it, but you should break up large chunks whenever possible. Sometimes, though, chunks cannot be broken up—we’ll look at this situation next.1\n\nJumbo Chunks Suppose you choose the \"date\" field as your shard key. The \"date\" field in this col‐ lection is a string that looks like \"year/month/day\", which means that mongos can create at most one chunk per day. This works fine for a while, until your application suddenly goes viral and gets a thousand times its typical traffic for one day.\n\nThis day’s chunk is going to be much larger than any other day’s, but it is also com‐ pletely unsplittable because every document has the same value for the shard key.\n\nOnce a chunk is larger than the max chunk size set in config.settings, the balancer will not be allowed to move the chunk. These unsplittable, unmovable chunks are called jumbo chunks and they are inconvenient to deal with.\n\nLet’s take an example. Suppose you have three shards, shard1, shard2, and shard3. If you use the hotspot shard key pattern described in “Ascending Shard Keys” on page 320, all your writes will be going to one shard—say, shard1. The shard primary mon‐ god will request that the balancer move each new top chunk evenly between the other\n\n1 MongoDB 4.4 is planning to add a new parameter (forceJumbo) in the moveChunk function, as well as a new balancer configuration setting attemptToBalanceJumboChunks to address jumbo chunks. The details are in this JIRA ticket describing the work.\n\n364\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 2025,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "shards, but the only chunks that the balancer can move are the nonjumbo chunks, so it will migrate all the small chunks off the hot shard.\n\nNow all the shards will have roughly the same number of chunks, but all of shard2 and shard3’s chunks will be less than 64 MB in size. And if jumbo chunks are being created, more and more of shard1’s chunks will be more than 64 MB in size. Thus, shard1 will fill up a lot faster than the other two shards, even though the number of chunks is perfectly balanced between the three.\n\nThus, one of the indicators that you have jumbo chunk problems is that one shard’s size is growing much faster than the others. You can also look at the output of sh.sta tus() to see if you have jumbo chunks—they will be marked with the jumbo attribute:\n\n> sh.status() ... { \"x\" : -7 } -->> { \"x\" : 5 } on : shard0001 { \"x\" : 5 } -->> { \"x\" : 6 } on : shard0001 jumbo { \"x\" : 6 } -->> { \"x\" : 7 } on : shard0001 jumbo { \"x\" : 7 } -->> { \"x\" : 339 } on : shard0001 ...\n\nYou can use the dataSize command to check chunk sizes. First, use the config.chunks collection to find the chunk ranges:\n\n> use config > var chunks = db.chunks.find({\"ns\" : \"acme.analytics\"}).toArray()\n\nThen use these chunk ranges to find possible jumbo chunks:\n\n> use <dbName> > db.runCommand({\"dataSize\" : \"<dbName.collName>\", ... \"keyPattern\" : {\"date\" : 1}, // shard key ... \"min\" : chunks[0].min, ... \"max\" : chunks[0].max}) { \"size\" : 33567917, \"numObjects\" : 108942, \"millis\" : 634, \"ok\" : 1, \"operationTime\" : Timestamp(1541455552, 10), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1541455552, 10), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } }\n\nBe careful, though—the dataSize command does have to scan the chunk’s data to figure out how big it is. If you can, narrow down your search by using your\n\nBalancing Data\n\n|\n\n365",
      "content_length": 1873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "knowledge of your data: were jumbo chunks created on a certain date? For example, if July 1 was a really busy day, look for chunks with that day in their shard key range.\n\nIf you’re using GridFS and sharding by \"files_id\", you can look at the fs.files collection to find a file’s size.\n\nDistributing jumbo chunks\n\nTo fix a cluster thrown off-balance by jumbo chunks, you must evenly distribute them among the shards.\n\nThis is a complex manual process, but should not cause any downtime (it may cause slowness, as you’ll be migrating a lot of data). In the following description, the shard with the jumbo chunks is referred to as the “from” shard. The shards that the jumbo chunks are migrated to are called the “to” shards. Note that you may have multiple “from” shards that you wish to move chunks off of. Repeat these steps for each:\n\n1. Turn off the balancer. You don’t want the balancer trying to “help” during this process:\n\n> sh.setBalancerState(false)\n\n2. MongoDB will not allow you to move chunks larger than the max chunk size, so temporarily increase the chunk size. Make a note of what your original chunk size is and then change it to something large, like 10000. Chunk size is specified in megabytes: > use config > db.settings.findOne({\"_id\" : \"chunksize\"}) { \"_id\" : \"chunksize\", \"value\" : 64 } > db.settings.save({\"_id\" : \"chunksize\", \"value\" : 10000})\n\n3. Use the moveChunk command to move jumbo chunks off the “from” shard. 4. Run splitChunk on the remaining chunks on the “from” shard until it has roughly the same number of chunks as the “to” shards.\n\n5. Set the chunk size back to its original value:\n\n> db.settings.save({\"_id\" : \"chunksize\", \"value\" : 64})\n\n6. Turn on the balancer:\n\n> sh.setBalancerState(true)\n\n366\n\n|\n\nChapter 17: Sharding Administration",
      "content_length": 1778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "When the balancer is turned on again, it will once again be unable to move the jumbo chunks; they are essentially held in place by their size.\n\nPreventing jumbo chunks\n\nAs the amount of data you are storing grows, the manual process described in the previous section becomes unsustainable. Thus, if you’re having problems with jumbo chunks, you should make it a priority to prevent them from forming.\n\nTo prevent jumbo chunks, modify your shard key to have more granularity. You want almost every document to have a unique value for the shard key, or at least to never have more than the chunk size’s worth of data with a single shard key value.\n\nFor example, if you were using the year/month/day key described earlier, it could quickly be made more fine-grained by adding hours, minutes, and seconds. Similarly, if you’re sharding on something coarse-grained like log level, you can add to your shard key a second field with a lot of granularity, such as an MD5 hash or UUID. Then you can always split a chunk, even if the first field is the same for many documents.\n\nRefreshing Configurations As a final tip, sometimes mongos will not update its configuration correctly from the config servers. If you ever get a configuration that you don’t expect or a mongos seems to be out of date or cannot find data that you know is there, use the flushRouterCon fig command to manually clear all caches:\n\n> db.adminCommand({\"flushRouterConfig\" : 1})\n\nIf flushRouterConfig does not work, restarting all your mongos or mongod processes clears any cached data.\n\nBalancing Data\n\n|\n\n367",
      "content_length": 1573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "PART V Application Administration",
      "content_length": 33,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "CHAPTER 18 Seeing What Your Application Is Doing\n\nOnce you have an application up and running, how do you know what it’s doing? This chapter covers how to figure out what kinds of queries MongoDB is running, how much data is being written, and other details about what it’s actually doing. You’ll learn about:\n\nFinding slow operations and killing them\n\nGetting and interpreting statistics about your collections and databases\n\nUsing command-line tools to give you a picture of what MongoDB is doing\n\nSeeing the Current Operations An easy way to find slow operations is to see what is running. Anything slow is more likely to show up and have been running for longer. It’s not guaranteed, but it’s a good first step to see what might be slowing down an application.\n\nTo see the operations that are running, use the db.currentOp() function:\n\n> db.currentOp() { \"inprog\": [{ \"type\" : \"op\", \"host\" : \"eoinbrazil-laptop-osx:27017\", \"desc\" : \"conn3\", \"connectionId\" : 3, \"client\" : \"127.0.0.1:57181\", \"appName\" : \"MongoDB Shell\", \"clientMetadata\" : { \"application\" : { \"name\" : \"MongoDB Shell\" },\n\n371",
      "content_length": 1095,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "372\n\n\"driver\" : { \"name\" : \"MongoDB Internal Client\", \"version\" : \"4.2.0\" }, \"os\" : { \"type\" : \"Darwin\", \"name\" : \"Mac OS X\", \"architecture\" : \"x86_64\", \"version\" : \"18.7.0\" } }, \"active\" : true, \"currentOpTime\" : \"2019-09-03T23:25:46.380+0100\", \"opid\" : 13594, \"lsid\" : { \"id\" : UUID(\"63b7df66-ca97-41f4-a245-eba825485147\"), \"uid\" : BinData(0,\"47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=\") }, \"secs_running\" : NumberLong(0), \"microsecs_running\" : NumberLong(969), \"op\" : \"insert\", \"ns\" : \"sample_mflix.items\", \"command\" : { \"insert\" : \"items\", \"ordered\" : false, \"lsid\" : { \"id\" : UUID(\"63b7df66-ca97-41f4-a245-eba825485147\") }, \"$readPreference\" : { \"mode\" : \"secondaryPreferred\" }, \"$db\" : \"sample_mflix\" }, \"numYields\" : 0, \"locks\" : { \"ParallelBatchWriterMode\" : \"r\", \"ReplicationStateTransition\" : \"w\", \"Global\" : \"w\", \"Database\" : \"w\", \"Collection\" : \"w\" }, \"waitingForLock\" : false, \"lockStats\" : { \"ParallelBatchWriterMode\" : { \"acquireCount\" : { \"r\" : NumberLong(4) } }, \"ReplicationStateTransition\" : { \"acquireCount\" : { \"w\" : NumberLong(4) }\n\n|\n\nChapter 18: Seeing What Your Application Is Doing",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "}, \"Global\" : { \"acquireCount\" : { \"w\" : NumberLong(4) } }, \"Database\" : { \"acquireCount\" : { \"w\" : NumberLong(4) } }, \"Collection\" : { \"acquireCount\" : { \"w\" : NumberLong(4) } }, \"Mutex\" : { \"acquireCount\" : { \"r\" : NumberLong(196) } } }, \"waitingForFlowControl\" : false, \"flowControlStats\" : { \"acquireCount\" : NumberLong(4) } }], \"ok\": 1 }\n\nThis displays a list of operations that the database is performing. Here are some of the more important fields in the output:\n\n\"opid\"\n\nThe operation’s unique identifier. You can use this number to kill an operation (see “Killing Operations” on page 375).\n\n\"active\"\n\nWhether this operation is running. If this field is false, it means the operation has yielded or is waiting for a lock.\n\n\"secs_running\"\n\nThe duration of this operation in seconds. You can use this to find queries that are taking too long.\n\n\"microsecs_running\"\n\nThe duration of this operation in microseconds. You can use this to find queries that are taking too long.\n\nSeeing the Current Operations\n\n|\n\n373",
      "content_length": 1016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "\"op\"\n\nThe type of operation. This is generally \"query\", \"insert\", \"update\", or \"remove\". Note that database commands are processed as queries.\n\n\"desc\"\n\nAn identifier for the client. This can be correlated with messages in the logs. Every log message related to the connection in our example will be prefixed with [conn3], so you can use this to grep the logs for relevant information.\n\n\"locks\"\n\nA description of the types of locks taken by this operation.\n\n\"waitingForLock\"\n\nWhether this operation is currently blocking, waiting to acquire a lock.\n\n\"numYields\"\n\nThe number of times this operation has yielded, releasing its lock to allow other operations to go. Generally, any operation that searches for documents (queries, updates, and removes) can yield. An operation will only yield if there are other operations enqueued and waiting to take its lock. Basically, if there are no opera‐ tions in the \"waitingForLock\" state, the current operations will not yield.\n\n\"lockstats.timeAcquiringMicros\"\n\nHow long it took this operation to acquire the locks it needed.\n\nYou can filter currentOp to only look for operations fulfilling certain criteria, such as operations on a certain namespace or ones that have been running for a certain length of time. You filter the results by passing in a query argument:\n\n> db.currentOp( { \"active\" : true, \"secs_running\" : { \"$gt\" : 3 }, \"ns\" : /^db1\\./ } )\n\nYou can query on any field in currentOp, using all the normal query operators.\n\nFinding Problematic Operations The most common use for db.currentOp() is looking for slow operations. You can use the filtering technique described in the previous section to find all queries that take longer than a certain amount of time, which may suggest a missing index or improper field filtering.\n\n374\n\n|\n\nChapter 18: Seeing What Your Application Is Doing",
      "content_length": 1835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "Sometimes people will find that unexpected queries are running, generally because there’s an app server running an old or buggy version of the software. The \"client\" field can help you track down where unexpected operations are coming from.\n\nKilling Operations If you find an operation that you want to stop, you can kill it by passing db.killOp() its \"opid\":\n\n> db.killOp(123)\n\nNot all operations can be killed. In general, operations can only be killed when they yield—so updates, finds, and removes can all be killed, but operations holding or waiting for a lock usually cannot be killed.\n\nOnce you have sent a “kill” message to an operation, it will have a \"killed\" field in the db.currentOp() output. However, it won’t actually be dead until it disappears from the list of current operations.\n\nIn MongoDB 4.0, the killOP method was extended to allow it to run on a mongos. It can now kill queries (read operations) that are running across more than one shard in a cluster. In previous versions, this involved manually issuing the kill command across each shard on the respective primary mongod.\n\nFalse Positives If you look for slow operations, you may see some long-running internal operations listed. There are several long-running requests MongoDB may have running, depending on your setup. The most common are the replication thread (which will continue fetching more operations from the sync source for as long as possible) and the writeback listener for sharding. Any long-running query on local.oplog.rs can be ignored, as well as any writebacklistener commands.\n\nIf you kill either of these operations, MongoDB will just restart them. However, you generally should not do that. Killing the replication thread will briefly halt replica‐ tion, and killing the writeback listener may cause mongos to miss legitimate write errors.\n\nPreventing Phantom Operations There is an odd, MongoDB-specific issue that you may run into, particularly if you’re bulk-loading data into a collection. Suppose you have a job that is firing thousands of update operations at MongoDB and MongoDB is grinding to a halt. You quickly stop the job and kill off all the updates that are currently occurring. However, you con‐ tinue to see new updates appearing as soon as you kill the old ones, even though the job is no longer running!\n\nSeeing the Current Operations\n\n|\n\n375",
      "content_length": 2360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "If you are loading data using unacknowledged writes, your application will fire writes at MongoDB, potentially faster than MongoDB can process them. If MongoDB gets backed up, these writes will pile up in the operating system’s socket buffer. When you kill the writes MongoDB is working on, this allows MongoDB to start processing the writes in the buffer. Even if you stop the client from sending writes, any writes that made it into the buffer will get processed by MongoDB, since they’ve already been “received” (just not processed).\n\nThe best way to prevent these phantom writes is to do acknowledged writes: make each write wait until the previous write is complete, not just until the previous write is sitting in a buffer on the database server.\n\nUsing the System Profiler To find slow operations you can use the system profiler, which records operations in a special system.profile collection. The profiler can give you tons of information about operations that are taking a long time, but at a cost: it slows down mongod’s overall performance. Thus, you may only want to turn on the profiler periodically to capture a slice of traffic. If your system is already heavily loaded, you may wish to use another technique described in this chapter to diagnose issues.\n\nBy default, the profiler is off and does not record anything. You can turn it on by run‐ ning db.setProfilingLevel() in the shell:\n\n> db.setProfilingLevel(2) { \"was\" : 0, \"slowms\" : 100, \"ok\" : 1 }\n\nLevel 2 means “profile everything.” Every read and write request received by the data‐ base will be recorded in the system.profile collection of the current database. Profiling is enabled per-database and incurs a heavy performance penalty: every write has to be written an extra time and every read has to take a write lock (because it must write an entry to the system.profile collection). However, it will give you an exhaustive listing of what your system is doing:\n\n> db.foo.insert({x:1}) > db.foo.update({},{$set:{x:2}}) > db.foo.remove() > db.system.profile.find().pretty() { \"op\" : \"insert\", \"ns\" : \"sample_mflix.foo\", \"command\" : { \"insert\" : \"foo\", \"ordered\" : true, \"lsid\" : { \"id\" : UUID(\"63b7df66-ca97-41f4-a245-eba825485147\") }, \"$readPreference\" : {\n\n376\n\n|\n\nChapter 18: Seeing What Your Application Is Doing",
      "content_length": 2294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "\"mode\" : \"secondaryPreferred\" }, \"$db\" : \"sample_mflix\" }, \"ninserted\" : 1, \"keysInserted\" : 1, \"numYield\" : 0, \"locks\" : { ... }, \"flowControl\" : { \"acquireCount\" : NumberLong(3) }, \"responseLength\" : 45, \"protocol\" : \"op_msg\", \"millis\" : 33, \"client\" : \"127.0.0.1\", \"appName\" : \"MongoDB Shell\", \"allUsers\" : [ ], \"user\" : \"\" } { \"op\" : \"update\", \"ns\" : \"sample_mflix.foo\", \"command\" : { \"q\" : {\n\n}, \"u\" : { \"$set\" : { \"x\" : 2 } }, \"multi\" : false, \"upsert\" : false }, \"keysExamined\" : 0, \"docsExamined\" : 1, \"nMatched\" : 1, \"nModified\" : 1, \"numYield\" : 0, \"locks\" : { ... }, \"flowControl\" : { \"acquireCount\" : NumberLong(1) }, \"millis\" : 0, \"planSummary\" : \"COLLSCAN\", \"execStats\" : { ... \"inputStage\" : { ... } }, \"ts\" : ISODate(\"2019-09-03T22:39:33.856Z\"), \"client\" : \"127.0.0.1\",\n\nUsing the System Profiler\n\n|\n\n377",
      "content_length": 820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "\"appName\" : \"MongoDB Shell\", \"allUsers\" : [ ], \"user\" : \"\" } { \"op\" : \"remove\", \"ns\" : \"sample_mflix.foo\", \"command\" : { \"q\" : {\n\n}, \"limit\" : 0 }, \"keysExamined\" : 0, \"docsExamined\" : 1, \"ndeleted\" : 1, \"keysDeleted\" : 1, \"numYield\" : 0, \"locks\" : { ... }, \"flowControl\" : { \"acquireCount\" : NumberLong(1) }, \"millis\" : 0, \"planSummary\" : \"COLLSCAN\", \"execStats\" : { ... \"inputStage\" : { ... } }, \"ts\" : ISODate(\"2019-09-03T22:39:33.858Z\"), \"client\" : \"127.0.0.1\", \"appName\" : \"MongoDB Shell\", \"allUsers\" : [ ], \"user\" : \"\" }\n\nYou can use the \"client\" field to see which users are sending which operations to the database. If you’re using authentication, you can see which user is doing each opera‐ tion, too.\n\nOften, you do not care about most of the operations that your database is doing, just the slow ones. For this, you can set the profiling level to 1. By default, level 1 profiles operations that take longer than 100 ms. You can also specify a second argument, which defines what “slow” means to you. This would record all operations that took longer than 500 ms:\n\n> db.setProfilingLevel(1, 500) { \"was\" : 2, \"slowms\" : 100, \"ok\" : 1 }\n\nTo turn profiling off, set the profiling level to 0:\n\n> db.setProfilingLevel(0) { \"was\" : 1, \"slowms\" : 500, \"ok\" : 1 }\n\n378\n\n|\n\nChapter 18: Seeing What Your Application Is Doing",
      "content_length": 1325,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "It’s generally not a good idea to set slowms to a low value. Even with profiling off, slowms has an effect on mongod: it sets the threshold for printing slow operations in the log. Thus, if you set slowms to 2, every operation that takes longer than 2 ms will show up in the log, even with profiling off. So, if you lower slowms to profile some‐ thing, you might want to raise it again before turning off profiling.\n\nYou can see the current profiling level with db.getProfilingLevel(). The profiling level is not persistent: restarting the database clears the level.\n\nThere are command-line options for configuring the profiling level, namely -- profile level and --slowms time, but bumping up the profiling level is generally a temporary debugging measure, not something you want to add to your configuration long-term.\n\nIn MongoDB 4.2, profiler entries and diagnostic log messages were extended for read/write operations to help improve the identification of slow queries, with the addition of the queryHash and planCacheKey fields. The queryHash string represents a hash of the query shape and is dependent only on the query shape. Each query shape is associated with a queryHash, making it easier to highlight those queries using the same shape. The planCacheKey is the hash of the key for the plan cache entry associated with the query. It includes the details of both the query shape and the cur‐ rently available indexes for the shape. These help you correlate the available informa‐ tion from the profiler to assist with query performance diagnosis.\n\nIf you turn on profiling and the system.profile collection does not already exist, Mon‐ goDB creates a small capped collection for it (a few megabytes in size). If you want to run the profiler for an extended period of time, this may not be enough space for the number of operations you need to record. You can make a larger system.profile collec‐ tion by turning off profiling, dropping the system.profile collection, and creating a new system.profile capped collection that is the size you desire. Then enable profiling on the database.\n\nCalculating Sizes In order to provision the correct amount of disk and RAM, it is useful to know how much space documents, indexes, collections, and databases are taking up. See “Calcu‐ lating the Working Set” on page 429 for information on calculating your working set.\n\nDocuments The easiest way to get the size of a document is to use the shell’s Object.bsonsize() function. Pass in any document to get the size it would be when stored in MongoDB.\n\nFor example, you can see that storing _ids as ObjectIds is more efficient than storing them as strings:\n\nCalculating Sizes\n\n|\n\n379",
      "content_length": 2681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "> Object.bsonsize({_id:ObjectId()}) 22 > // \"\"+ObjectId() converts the ObjectId to a string > Object.bsonsize({_id:\"\"+ObjectId()}) 39\n\nMore practically, you can pass in documents directly from your collections:\n\n> Object.bsonsize(db.users.findOne())\n\nThis shows you exactly how many bytes a document is taking up on disk. However, this does not count padding or indexes, which can often be significant factors in the size of a collection.\n\nCollections For seeing information about a whole collection, there is a stats function:\n\n>db.movies.stats() { \"ns\" : \"sample_mflix.movies\", \"size\" : 65782298, \"count\" : 45993, \"avgObjSize\" : 1430, \"storageSize\" : 45445120, \"capped\" : false, \"wiredTiger\" : { \"metadata\" : { \"formatVersion\" : 1 }, \"creationString\" : \"access_pattern_hint=none,allocation_size=4KB,\\ app_metadata=(formatVersion=1),assert=(commit_timestamp=none,\\ read_timestamp=none),block_allocation=best,block_compressor=\\ snappy,cache_resident=false,checksum=on,colgroups=,collator=,\\ columns=,dictionary=0,encryption=(keyid=,name=),exclusive=\\ false,extractor=,format=btree,huffman_key=,huffman_value=,\\ ignore_in_memory_cache_size=false,immutable=false,internal_item_\\ max=0,internal_key_max=0,internal_key_truncate=true,internal_\\ page_max=4KB,key_format=q,key_gap=10,leaf_item_max=0,leaf_key_\\ max=0,leaf_page_max=32KB,leaf_value_max=64MB,log=(enabled=true),\\ lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,bloom_\\ config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit\\ =0,chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_\\ generation=0,suffix=),merge_max=15,merge_min=0),memory_page_image\\ _max=0,memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,\\ prefix_compression=false,prefix_compression_min=4,source=,split_\\ deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,\\ value_format=u\", \"type\" : \"file\", \"uri\" : \"statistics:table:collection-14--2146526997547809066\", \"LSM\" : { \"bloom filter false positives\" : 0, \"bloom filter hits\" : 0,\n\n380\n\n|\n\nChapter 18: Seeing What Your Application Is Doing",
      "content_length": 2053,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "\"bloom filter misses\" : 0, \"bloom filter pages evicted from cache\" : 0, \"bloom filter pages read into cache\" : 0, \"bloom filters in the LSM tree\" : 0, \"chunks in the LSM tree\" : 0, \"highest merge generation in the LSM tree\" : 0, \"queries that could have benefited from a Bloom filter that did not exist\" : 0, \"sleep for LSM checkpoint throttle\" : 0, \"sleep for LSM merge throttle\" : 0, \"total size of bloom filters\" : 0 }, \"block-manager\" : { \"allocations requiring file extension\" : 0, \"blocks allocated\" : 1358, \"blocks freed\" : 1322, \"checkpoint size\" : 39219200, \"file allocation unit size\" : 4096, \"file bytes available for reuse\" : 6209536, \"file magic number\" : 120897, \"file major version number\" : 1, \"file size in bytes\" : 45445120, \"minor version number\" : 0 }, \"btree\" : { \"btree checkpoint generation\" : 22, \"column-store fixed-size leaf pages\" : 0, \"column-store internal pages\" : 0, \"column-store variable-size RLE encoded values\" : 0, \"column-store variable-size deleted values\" : 0, \"column-store variable-size leaf pages\" : 0, \"fixed-record size\" : 0, \"maximum internal page key size\" : 368, \"maximum internal page size\" : 4096, \"maximum leaf page key size\" : 2867, \"maximum leaf page size\" : 32768, \"maximum leaf page value size\" : 67108864, \"maximum tree depth\" : 0, \"number of key/value pairs\" : 0, \"overflow pages\" : 0, \"pages rewritten by compaction\" : 1312, \"row-store empty values\" : 0, \"row-store internal pages\" : 0, \"row-store leaf pages\" : 0 }, \"cache\" : { \"bytes currently in the cache\" : 40481692, \"bytes dirty in the cache cumulative\" : 40992192, \"bytes read into cache\" : 37064798, \"bytes written from cache\" : 37019396, \"checkpoint blocked page eviction\" : 0, \"data source pages selected for eviction unable to be evicted\" : 32,\n\nCalculating Sizes\n\n|\n\n381",
      "content_length": 1789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "382\n\n\"eviction walk passes of a file\" : 0, \"eviction walk target pages histogram - 0-9\" : 0, \"eviction walk target pages histogram - 10-31\" : 0, \"eviction walk target pages histogram - 128 and higher\" : 0, \"eviction walk target pages histogram - 32-63\" : 0, \"eviction walk target pages histogram - 64-128\" : 0, \"eviction walks abandoned\" : 0, \"eviction walks gave up because they restarted their walk twice\" : 0, \"eviction walks gave up because they saw too many pages and found no candidates\" : 0, \"eviction walks gave up because they saw too many pages and found too few candidates\" : 0, \"eviction walks reached end of tree\" : 0, \"eviction walks started from root of tree\" : 0, \"eviction walks started from saved location in tree\" : 0, \"hazard pointer blocked page eviction\" : 0, \"in-memory page passed criteria to be split\" : 0, \"in-memory page splits\" : 0, \"internal pages evicted\" : 8, \"internal pages split during eviction\" : 0, \"leaf pages split during eviction\" : 0, \"modified pages evicted\" : 1312, \"overflow pages read into cache\" : 0, \"page split during eviction deepened the tree\" : 0, \"page written requiring cache overflow records\" : 0, \"pages read into cache\" : 1330, \"pages read into cache after truncate\" : 0, \"pages read into cache after truncate in prepare state\" : 0, \"pages read into cache requiring cache overflow entries\" : 0, \"pages requested from the cache\" : 3383, \"pages seen by eviction walk\" : 0, \"pages written from cache\" : 1334, \"pages written requiring in-memory restoration\" : 0, \"tracked dirty bytes in the cache\" : 0, \"unmodified pages evicted\" : 8 }, \"cache_walk\" : { \"Average difference between current eviction generation when the page was last considered\" : 0, \"Average on-disk page image size seen\" : 0, \"Average time in cache for pages that have been visited by the eviction server\" : 0, \"Average time in cache for pages that have not been visited by the eviction server\" : 0, \"Clean pages currently in cache\" : 0, \"Current eviction generation\" : 0, \"Dirty pages currently in cache\" : 0, \"Entries in the root page\" : 0, \"Internal pages currently in cache\" : 0, \"Leaf pages currently in cache\" : 0, \"Maximum difference between current eviction generation when the page was last considered\" : 0,\n\n|\n\nChapter 18: Seeing What Your Application Is Doing",
      "content_length": 2289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "\"Maximum page size seen\" : 0, \"Minimum on-disk page image size seen\" : 0, \"Number of pages never visited by eviction server\" : 0, \"On-disk page image sizes smaller than a single allocation unit\" : 0, \"Pages created in memory and never written\" : 0, \"Pages currently queued for eviction\" : 0, \"Pages that could not be queued for eviction\" : 0, \"Refs skipped during cache traversal\" : 0, \"Size of the root page\" : 0, \"Total number of pages currently in cache\" : 0 }, \"compression\" : { \"compressed page maximum internal page size prior to compression\" : 4096, \"compressed page maximum leaf page size prior to compression \" : 131072, \"compressed pages read\" : 1313, \"compressed pages written\" : 1311, \"page written failed to compress\" : 1, \"page written was too small to compress\" : 22 }, \"cursor\" : { \"bulk loaded cursor insert calls\" : 0, \"cache cursors reuse count\" : 0, \"close calls that result in cache\" : 0, \"create calls\" : 1, \"insert calls\" : 0, \"insert key and value bytes\" : 0, \"modify\" : 0, \"modify key and value bytes affected\" : 0, \"modify value bytes modified\" : 0, \"next calls\" : 0, \"open cursor count\" : 0, \"operation restarted\" : 0, \"prev calls\" : 1, \"remove calls\" : 0, \"remove key bytes removed\" : 0, \"reserve calls\" : 0, \"reset calls\" : 2, \"search calls\" : 0, \"search near calls\" : 0, \"truncate calls\" : 0, \"update calls\" : 0, \"update key and value bytes\" : 0, \"update value size change\" : 0 }, \"reconciliation\" : { \"dictionary matches\" : 0, \"fast-path pages deleted\" : 0, \"internal page key bytes discarded using suffix compression\" : 0, \"internal page multi-block writes\" : 0, \"internal-page overflow keys\" : 0,\n\nCalculating Sizes\n\n|\n\n383",
      "content_length": 1656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "\"leaf page key bytes discarded using prefix compression\" : 0, \"leaf page multi-block writes\" : 0, \"leaf-page overflow keys\" : 0, \"maximum blocks required for a page\" : 1, \"overflow values written\" : 0, \"page checksum matches\" : 0, \"page reconciliation calls\" : 1334, \"page reconciliation calls for eviction\" : 1312, \"pages deleted\" : 0 }, \"session\" : { \"object compaction\" : 4 }, \"transaction\" : { \"update conflicts\" : 0 } }, \"nindexes\" : 5, \"indexBuilds\" : [ ], \"totalIndexSize\" : 46292992, \"indexSizes\" : { \"_id_\" : 446464, \"$**_text\" : 44474368, \"genres_1_imdb.rating_1_metacritic_1\" : 724992, \"tomatoes_rating\" : 307200, \"getMovies\" : 339968 }, \"scaleFactor\" : 1, \"ok\" : 1 }\n\nstats starts with the namespace (\"sample_mflix.movies\") and then the count of all documents in the collection. The next couple of fields have to do with the size of the collection. \"size\" is what you’d get if you called Object.bsonsize() on each element in the collection and added up all the sizes: it’s the actual number of bytes in memory the documents in the collection are taking up when uncompressed. Equivalently, if you take the \"avgObjSize\" and multiply it by \"count\", you’ll get \"size\" uncom‐ pressed in memory.\n\nAs mentioned earlier, a total count of the documents’ bytes leaves out the space saved by compressing a collection. \"storageSize\" can be a smaller figure than \"size\", reflecting the space saved by compression.\n\n\"nindexes\" is the number of indexes on the collection. An index is not counted in \"nindexes\" until it finishes being built and cannot be used until it appears in this list. In general, indexes will be a lot larger than the amount of data they store. You can minimize this free space by having right-balanced indexes (as described in “Introduc‐ tion to Compound Indexes” on page 81). Indexes that are randomly distributed will\n\n384\n\n|\n\nChapter 18: Seeing What Your Application Is Doing",
      "content_length": 1898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "generally be approximately 50% free space, whereas ascending-order indexes will be 10% free space.\n\nAs your collections get bigger, it may become difficult to read stats output with sizes in the billions of bytes or beyond. Thus, you can pass in a scaling factor: 1024 for kil‐ obytes, 1024*1024 for megabytes, and so on. For example, this would get the collec‐ tion stats in terabytes:\n\n> db.big.stats(1024*1024*1024*1024)\n\nDatabases Databases have a stats function that’s similar to collections’:\n\n> db.stats() { \"db\" : \"sample_mflix\", \"collections\" : 5, \"views\" : 0, \"objects\" : 98308, \"avgObjSize\" : 819.8680982219148, \"dataSize\" : 80599593, \"storageSize\" : 53620736, \"numExtents\" : 0, \"indexes\" : 12, \"indexSize\" : 47001600, \"scaleFactor\" : 1, \"fsUsedSize\" : 355637043200, \"fsTotalSize\" : 499963174912, \"ok\" : 1 }\n\nFirst, we have the name of the database, the number of collections it contains, and the number of views for the database. \"objects\" is the total count of documents across all collections in this database.\n\nThe bulk of the document contains information about the size of your data. \"fsTotalSize\" should always be the largest: it is the total size of the disk capacity on the filesystem where the MongoDB instance stores data. \"fsUsedSize\" represents the total space used in that filesystem by MongoDB currently. This should correspond to the total space used by all the files in your data directory.\n\nThe next-largest field is generally going to be \"dataSize\", which is the size of the uncompressed data held in this database. This doesn’t match \"storageSize\" because data is typically compressed in WiredTiger. \"indexSize\" is the amount of space all of the indexes for this database take up.\n\ndb.stats() can take a scale argument the same way that the collections’ stats func‐ tion can. If you call db.stats() on a nonexistent database, the values will all be zero.\n\nCalculating Sizes\n\n|\n\n385",
      "content_length": 1912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "Keep in mind that listing databases on a system with a high lock percent can be very slow and block other operations. Avoid doing it, if possible.\n\nUsing mongotop and mongostat MongoDB comes with a few command-line tools that can help you determine what it’s doing by printing stats every few seconds.\n\nmongotop is similar to the top Unix utility: it gives you an overview of which collec‐ tions are busiest. You can also run mongotop --locks to give you locking statistics for each database.\n\nmongostat gives server-wide information. By default, mongostat prints out a list of statistics once per second, although this is configurable by passing a different number of seconds on the command line. Each of the fields gives a count of how many times the activity has happened since the field was last printed:\n\ninsert/query/update/delete/getmore/command\n\nSimple counts of how many of each of these operations there have been.\n\nflushes\n\nHow many times mongod has flushed data to disk.\n\nmapped\n\nThe amount of memory mongod has mapped. This is generally roughly the size of your data directory.\n\nvsize\n\nThe amount of virtual memory mongod is using. This is generally twice the size of your data directory (once for the mapped files, once again for journaling).\n\nres\n\nThe amount of memory mongod is using. This should generally be as close as possible to all the memory on the machine.\n\nlocked db\n\nThe database that spent the most time locked in the last timeslice. This field reports the percent of time the database was locked combined with how long the global lock was held, meaning that this value might be over 100%.\n\nidx miss %\n\nThe percentage of index accesses that had to page fault (because the index entry or section of index being searched was not in memory, so mongod had to go to disk). This is the most confusingly named field in the output.\n\n386\n\n|\n\nChapter 18: Seeing What Your Application Is Doing",
      "content_length": 1909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "qr|qw\n\nThe queue size for reads and writes (i.e., how many reads and writes are block‐ ing, waiting to be processed).\n\nar|aw\n\nHow many active clients there are (i.e., clients currently performing reads and writes).\n\nnetIn\n\nThe number of network bytes in, as counted by MongoDB (not necessarily the same as what the OS would measure).\n\nnetOut\n\nThe number of network bytes out, as counted by MongoDB.\n\nconn\n\nThe number of connections this server has open, both incoming and outgoing.\n\ntime\n\nThe time at which these statistics were taken.\n\nYou can run mongostat on a replica set or sharded cluster. If you use the --discover option, mongostat will try to find all the members of the set or cluster from the mem‐ ber it initially connects to and will print one line per server per second for each. For a large cluster, this can get unmanageable fast, but it can be useful for small clusters and tools that can consume the data and present it in a more readable form.\n\nmongostat is a great way to get a quick snapshot of what your database is doing, but for long-term monitoring a tool like MongoDB Atlas or Ops Manager is preferred (see Chapter 22).\n\nUsing mongotop and mongostat\n\n|\n\n387",
      "content_length": 1183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "CHAPTER 19 An Introduction to MongoDB Security\n\nTo protect your MongoDB cluster and the data it holds, you will want to employ the following security measures:\n\nEnable authorization and enforce authentication\n\nEncrypt communication\n\nEncrypt data\n\nThis chapter demonstrates how to address the first two security measures with a tuto‐ rial on using MongoDB’s support for x.509 to configure authentication and transport layer encryption to ensure secure communications among clients and servers in a MongoDB replica set. We will touch on encrypting data at the storage layer in a later chapter.\n\nMongoDB Authentication and Authorization While authentication and authorization are closely connected, it is important to note that authentication is distinct from authorization. The purpose of authentication is to verify the identity of a user, while authorization determines the verified user’s access to resources and operations.\n\nAuthentication Mechanisms Enabling authorization on a MongoDB cluster enforces authentication and ensures users can only perform actions they are authorized for, as determined by their roles. The Community version of MongoDB provides support for SCRAM (Salted Chal‐ lenge Response Authentication Mechanism) and x.509 certificate authentication. In addition to SCRAM and x.509, MongoDB Enterprise supports Kerberos authentica‐ tion and LDAP proxy authentication. See the documentation for details on the vari‐\n\n389",
      "content_length": 1440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "ous authentication mechanisms that MongoDB supports. In this chapter, we will focus on x.509 authentication. An x.509 digital certificate uses the widely accepted x. 509 public key infrastructure (PKI) standard to verify that a public key belongs to the presenter.\n\nAuthorization When adding a user in MongoDB, you must create the user in a specific database. That database is the authentication database for the user; you can use any database for this purpose. The username and authentication database serves as a unique identifier for a user. However, a user’s privileges are not limited to their authentication data‐ base. When creating a user, you can specify the operations the user may perform on any resources to which they should have access. Resources include the cluster, data‐ bases, and collections.\n\nMongoDB provides a number of built-in roles that grant commonly needed permis‐ sions for database users. These include the following:\n\nread\n\nRead data on all nonsystem collections and on the following system collections: system.indexes, system.js, and system.namespaces.\n\nreadWrite\n\nProvides same privileges as read, plus the ability to modify data on all nonsystem collections and the system.js collection.\n\ndbAdmin\n\nPerform administrative tasks such as schema-related tasks, indexing, and gather‐ ing statistics (does not grant privileges for user and role management).\n\nuserAdmin\n\nCreate and modify roles and users on the current database.\n\ndbOwner\n\nCombines the privileges granted by the readWrite, dbAdmin, and userAdmin roles.\n\nclusterManager\n\nPerform management and monitoring actions on the cluster.\n\nclusterMonitor\n\nProvides read-only access to monitoring tools such as the MongoDB Cloud Man‐ ager and Ops Manager monitoring agent.\n\nhostManager\n\nMonitor and manage servers.\n\n390\n\n|\n\nChapter 19: An Introduction to MongoDB Security",
      "content_length": 1852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "clusterAdmin\n\nCombines the privileges granted by the clusterManager, clusterMonitor, and host‐ Manager roles, plus the dropDatabase action.\n\nbackup\n\nProvides sufficient privileges to use the MongoDB Cloud Manager backup agent or the Ops Manager backup agent, or to use mongodump to back up an entire mongod instance.\n\nrestore\n\nProvides privileges needed to restore data from backups that do not include sys‐ tem.profile collection data.\n\nreadAnyDatabase\n\nProvides same privileges as read on all databases except local and config, plus the listDatabases action on the cluster as a whole.\n\nreadWriteAnyDatabase\n\nProvides same privileges as readWrite on all databases except local and config, plus the listDatabases action on the cluster as a whole.\n\nuserAdminAnyDatabase\n\nProvides same privileges as userAdmin on all databases except local and config (effectively a superuser role).\n\ndbAdminAnyDatabase\n\nProvides same privileges as dbAdmin on all databases except local and config, plus the listDatabases action on the cluster as a whole.\n\nroot\n\nProvides access to the operations and all the resources of the readWriteAnyData‐ base, dbAdminAnyDatabase, userAdminAnyDatabase, clusterAdmin, restore, and backup roles combined.\n\nYou may also create what are known as “user-defined roles,” which are custom roles that group together authorization to perform specific operations and label them with a name so that you may grant this set of permissions to multiple users easily.\n\nA deep dive on built-in roles or user-defined roles is beyond the scope of this chapter. However, this introduction should give you a pretty good idea of what’s possible with MongoDB authorization. For greater detail, please see the authorization section of the MongoDB documentation.\n\nTo ensure that you can add new users as needed, you must first create an admin user. MongoDB does not create a default root or admin user when enabling authentication and authorization, regardless of the authentication mode you are using (x.509 is no exception).\n\nMongoDB Authentication and Authorization\n\n|\n\n391",
      "content_length": 2070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "In MongoDB, authentication and authorization are not enabled by default. You must explicitly enable them by using the --auth option to the mongod command or specify‐ ing a value of \"enabled\" for the security.authorization setting in a MongoDB config file.\n\nTo configure a replica set, first bring it up without authentication and authorization enabled, then create the admin user and the users you’ll need for each client.\n\nUsing x.509 Certificates to Authenticate Both Members and Clients Given that all production MongoDB clusters are composed of multiple members, to secure a cluster, it is essential that all services communicating within the cluster authenticate with one another. Each member of a replica set must authenticate with the others in order to exchange data. Likewise, clients must authenticate with the pri‐ mary and any secondaries that they communicate with.\n\nFor x.509, it’s necessary that a trusted certification authority (CA) sign all certificates. Signing certifies that the named subject of a certificate owns the public key associated with that certificate. A CA acts as a trusted third party to prevent man-in-the-middle attacks.\n\nFigure 19-1 depicts x.509 authentication used to secure a three-member MongoDB replica set. Note the authentication among the client and members of the replica set and the trust relationships with the CA.\n\n392\n\n|\n\nChapter 19: An Introduction to MongoDB Security",
      "content_length": 1420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "Figure 19-1. Overview of the trust hierarchy for X.509 authentication for the three- member replica set used in this chapter\n\nThe members and the client each have their own certificate signed by the CA. For production use, your MongoDB deployment should use valid certificates generated and signed by a single certificate authority. You or your organization can generate and maintain an independent certificate authority, or you can use certificates generated by a third-party TLS/SSL vendor.\n\nWe will refer to certificates used for internal authentication to verify membership in a cluster as member certificates. Both member certificates and client certificates (used to authenticate clients) have a structure resembling the following:\n\nCertificate: Data: Version: 1 (0x0) Serial Number: 1 (0x1) Signature Algorithm: sha256WithRSAEncryption Issuer: C=US, ST=NY, L=New York, O=MongoDB, CN=CA-SIGNER Validity Not Before: Nov 11 22:00:03 2018 GMT Not After : Nov 11 22:00:03 2019 GMT Subject: C=US, ST=NY, L=New York, O=MongoDB, OU=MyServers, CN=server1 Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit)\n\nMongoDB Authentication and Authorization\n\n|\n\n393",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "394\n\nModulus: 00:d3:1c:29:ba:3d:29:44:3b:2b:75:60:95:c8:83: fc:32:1a:fa:29:5c:56:f3:b3:66:88:7f:f9:f9:89: ff:c2:51:b9:ca:1d:4c:d8:b8:5a:fd:76:f5:d3:c9: 95:9c:74:52:e9:8d:5f:2e:6b:ca:f8:6a:16:17:98: dc:aa:bf:34:d0:44:33:33:f3:9d:4b:7e:dd:7a:19: 1b:eb:3b:9e:21:d9:d9:ba:01:9c:8b:16:86:a3:52: a3:e6:e4:5c:f7:0c:ab:7a:1a:be:c6:42:d3:a6:01: 8e:0a:57:b2:cd:5b:28:ee:9d:f5:76:ca:75:7a:c1: 7c:42:d1:2a:7f:17:fe:69:17:49:91:4b:ca:2e:39: b4:a5:e0:03:bf:64:86:ca:15:c7:b2:f7:54:00:f7: 02:fe:cf:3e:12:6b:28:58:1c:35:68:86:3f:63:46: 75:f1:fe:ac:1b:41:91:4f:f2:24:99:54:f2:ed:5b: fd:01:98:65:ac:7a:7a:57:2f:a8:a5:5a:85:72:a6: 9e:fb:44:fb:3b:1c:79:88:3f:60:85:dd:d1:5c:1c: db:62:8c:6a:f7:da:ab:2e:76:ac:af:6d:7d:b1:46: 69:c1:59:db:c6:fb:6f:e1:a3:21:0c:5f:2e:8e:a7: d5:73:87:3e:60:26:75:eb:6f:10:c2:64:1d:a6:19: f3:0b Exponent: 65537 (0x10001) Signature Algorithm: sha256WithRSAEncryption 5d:dd:b2:35:be:27:c2:41:4a:0d:c7:8c:c9:22:05:cd:eb:88: 9d:71:4f:28:c1:79:71:3c:6d:30:19:f4:9c:3d:48:3a:84:d0: 19:00:b1:ec:a9:11:02:c9:a6:9c:74:e7:4e:3c:3a:9f:23:30: 50:5a:d2:47:53:65:06:a7:22:0b:59:71:b0:47:61:62:89:3d: cf:c6:d8:b3:d9:cc:70:20:35:bf:5a:2d:14:51:79:4b:7c:00: 30:39:2d:1d:af:2c:f3:32:fe:c2:c6:a5:b8:93:44:fa:7f:08: 85:f0:01:31:29:00:d4:be:75:7e:0d:f9:1a:f5:e9:75:00:9a: 7b:d0:eb:80:b1:01:00:c0:66:f8:c9:f0:35:6e:13:80:70:08: 5b:95:53:4b:34:ec:48:e3:02:88:5c:cd:a0:6c:b4:bc:65:15: 4d:c8:41:9d:00:f5:e7:f2:d7:f5:67:4a:32:82:2a:04:ae:d7: 25:31:0f:34:e8:63:a5:93:f2:b5:5a:90:71:ed:77:2a:a6:15: eb:fc:c3:ac:ef:55:25:d1:a1:31:7a:2c:80:e3:42:c2:b3:7d: 5e:9a:fc:e4:73:a8:39:50:62:db:b1:85:aa:06:1f:42:27:25: 4b:24:cf:d0:40:ca:51:13:94:97:7f:65:3e:ed:d9:3a:67:08: 79:64:a1:ba -----BEGIN CERTIFICATE----- MIIDODCCAiACAQEwDQYJKoZIhvcNAQELBQAwWTELMAkGA1UEBhMCQ04xCzAJBgNV BAgMAkdEMREwDwYDVQQHDAhTaGVuemhlbjEWMBQGA1UECgwNTW9uZ29EQiBDaGlu YTESMBAGA1UEAwwJQ0EtU0lHTkVSMB4XDTE4MTExMTIyMDAwM1oXDTE5MTExMTIy MDAwM1owazELMAkGA1UEBhMCQ04xCzAJBgNVBAgMAkdEMREwDwYDVQQHDAhTaGVu emhlbjEWMBQGA1UECgwNTW9uZ29EQiBDaGluYTESMBAGA1UECwwJTXlTZXJ2ZXJz MRAwDgYDVQQDDAdzZXJ2ZXIxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC AQEA0xwpuj0pRDsrdWCVyIP8Mhr6KVxW87NmiH/5+Yn/wlG5yh1M2Lha/Xb108mV nHRS6Y1fLmvK+GoWF5jcqr800EQzM/OdS37dehkb6zueIdnZugGcixaGo1Kj5uRc 9wyrehq+xkLTpgGOCleyzVso7p31dsp1esF8QtEqfxf+aRdJkUvKLjm0peADv2SG yhXHsvdUAPcC/s8+EmsoWBw1aIY/Y0Z18f6sG0GRT/IkmVTy7Vv9AZhlrHp6Vy+o pVqFcqae+0T7Oxx5iD9ghd3RXBzbYoxq99qrLnasr219sUZpwVnbxvtv4aMhDF8u jqfVc4c+YCZ1628QwmQdphnzCwIDAQABMA0GCSqGSIb3DQEBCwUAA4IBAQBd3bI1 vifCQUoNx4zJIgXN64idcU8owXlxPG0wGfScPUg6hNAZALHsqRECyaacdOdOPDqf IzBQWtJHU2UGpyILWXGwR2FiiT3Pxtiz2cxwIDW/Wi0UUXlLfAAwOS0dryzzMv7C xqW4k0T6fwiF8AExKQDUvnV+Dfka9el1AJp70OuAsQEAwGb4yfA1bhOAcAhblVNL\n\n|\n\nChapter 19: An Introduction to MongoDB Security",
      "content_length": 2709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "NOxI4wKIXM2gbLS8ZRVNyEGdAPXn8tf1Z0oygioErtclMQ806GOlk/K1WpBx7Xcq phXr/MOs71Ul0aExeiyA40LCs31emvzkc6g5UGLbsYWqBh9CJyVLJM/QQMpRE5SX f2U+7dk6Zwh5ZKG6 -----END CERTIFICATE-----\n\nFor use with x.509 authentication in MongoDB, member certificates must have the following properties:\n\nA single CA must issue all x.509 certificates for the members of the cluster. • The Distinguished Name (DN), found in the subject of the member certificate, must specify a nonempty value for at least one of the following attributes: Orga‐ nization (O), Organizational Unit (OU), or Domain Component (DC).\n\nThe O, OU, and DC attributes must match those from the certificates for the other cluster members.\n\nThe Common Name (CN) or a Subject Alternative Name (SAN) must match the hostname of the server used by the other members of the cluster.\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption In this tutorial we will set up a root CA and an intermediate CA. Best practice recom‐ mends signing the server and client certificates with the intermediate CA.\n\nEstablish a CA Before we can generate signed certificates for the members of our replica set, we must first address the issue of a certificate authority. As mentioned previously, we can either generate and maintain an independent certificate authority or use certificates generated by a third-party TLS/SSL vendor. We will generate our own CA to use for the running example in this chapter. Note that you may access all the code examples in this chapter from the GitHub repository maintained for this book. The examples are drawn from a script you can use to deploy a secure replica set. You’ll see com‐ ments from this script throughout these examples.\n\nGenerate a root CA\n\nTo generate our CA, we will use OpenSSL. To follow along, please make sure you have access to OpenSSL on your local machine.\n\nA root CA is at the top of the certificate chain. This is the ultimate source of trust. Ideally, a third-party CA should be used. However, in the case of an isolated network (typical in a large enterprise environment) or for testing purposes, you’ll need to use a local CA.\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption\n\n|\n\n395",
      "content_length": 2203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "First, we’ll initialize some variables:\n\ndn_prefix=\"/C=US/ST=NY/L=New York/O=MongoDB\" ou_member=\"MyServers\" ou_client=\"MyClients\" mongodb_server_hosts=( \"server1\" \"server2\" \"server3\" ) mongodb_client_hosts=( \"client1\" \"client2\" ) mongodb_port=27017\n\nThen, we’ll create a key pair and store it in the file root-ca.key:\n\n# !!! In production you will want to password-protect the keys # openssl genrsa -aes256 -out root-ca.key 4096 openssl genrsa -out root-ca.key 4096\n\nNext, we’ll create a configuration file to hold our OpenSSL settings that we will use to generate the certificates:\n\n# For the CA policy [ policy_match ] countryName = match stateOrProvinceName = match organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional\n\n[ req ] default_bits = 4096 default_keyfile = server-key.pem default_md = sha256 distinguished_name = req_dn req_extensions = v3_req x509_extensions = v3_ca # The extensions to add to the self-signed cert\n\n[ v3_req ] subjectKeyIdentifier = hash basicConstraints = CA:FALSE keyUsage = critical, digitalSignature, keyEncipherment nsComment = \"OpenSSL Generated Certificate\" extendedKeyUsage = serverAuth, clientAuth\n\n[ req_dn ] countryName = Country Name (2-letter code) countryName_default = US countryName_min = 2 countryName_max = 2\n\nstateOrProvinceName = State or Province Name (full name) stateOrProvinceName_default = NY stateOrProvinceName_max = 64\n\nlocalityName = Locality Name (eg, city)\n\n396\n\n|\n\nChapter 19: An Introduction to MongoDB Security",
      "content_length": 1526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "localityName_default = New York localityName_max = 64\n\norganizationName = Organization Name (eg, company) organizationName_default = MongoDB organizationName_max = 64\n\norganizationalUnitName = Organizational Unit Name (eg, section) organizationalUnitName_default = Education organizationalUnitName_max = 64\n\ncommonName = Common Name (eg, YOUR name) commonName_max = 64\n\n[ v3_ca ] # Extensions for a typical CA\n\nsubjectKeyIdentifier = hash basicConstraints = critical,CA:true authorityKeyIdentifier = keyid:always,issuer:always\n\n# Key usage: this is typical for a CA certificate. However, since it will # prevent it being used as a test self-signed certificate it is best # left out by default. keyUsage = critical,keyCertSign,cRLSign\n\nThen, using the openssl req command, we will create the root certificate. Since the root is the very top of the authority chain, we’ll self-sign this certificate using the pri‐ vate key we created in the previous step (stored in root-ca.key). The -x509 option tells the openssl req command we want to self-sign the certificate using the private key supplied to the -key option. The output is a file called root-ca.crt:\n\nopenssl req -new -x509 -days 1826 -key root-ca.key -out root-ca.crt \\ -config openssl.cnf -subj \"$dn_prefix/CN=ROOTCA\"\n\nIf you take a look at the root-ca.crt file, you’ll find that it contains the public certificate for the root CA. You can verify the contents by taking a look at a human-readable version of the certificate produced by this command:\n\nopenssl x509 -noout -text -in root-ca.crt\n\nThe output from this command will resemble the following:\n\nCertificate: Data: Version: 3 (0x2) Serial Number: 1e:83:0d:9d:43:75:7c:2b:d6:2a:dc:7e:a2:a2:25:af:5d:3b:89:43 Signature Algorithm: sha256WithRSAEncryption Issuer: C = US, ST = NY, L = New York, O = MongoDB, CN = ROOTCA Validity Not Before: Sep 11 21:17:24 2019 GMT\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption\n\n|\n\n397",
      "content_length": 1951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "398\n\nNot After : Sep 10 21:17:24 2024 GMT Subject: C = US, ST = NY, L = New York, O = MongoDB, CN = ROOTCA Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (4096 bit) Modulus: 00:e3:de:05:ae:ba:c9:e0:3f:98:37:18:77:02:35: e7:f6:62:bc:c3:ae:38:81:8d:04:88:da:6c:e0:57: c2:90:86:05:56:7b:d2:74:23:54:f8:ca:02:45:0f: 38:e7:e2:0b:69:ea:f6:c8:13:8f:6c:2d:d6:c1:72: 64:17:83:4e:68:47:cf:de:37:ed:6e:38:b2:ab:3a: e4:45:a8:fa:08:90:a0:f3:0d:3a:14:d8:9a:8d:69: e7:cf:93:1a:71:53:4f:13:29:50:b0:2f:b6:b8:19: 2a:40:21:15:90:43:e7:d8:d8:f3:51:e5:95:58:87: 6c:45:9f:61:fc:b5:97:cf:5b:4e:4a:1f:72:c9:0c: e9:8c:4c:d1:ca:df:b3:a4:da:b4:10:83:81:01:b1: c8:09:22:76:c7:1e:96:c7:e6:56:27:8d:bc:fb:17: ed:d9:23:3f:df:9c:ef:03:20:cc:c3:c4:55:cc:9f: ad:d4:8d:81:95:c3:f1:87:f8:d4:5a:5e:e0:a8:41: 27:c8:0d:52:91:e4:2b:db:25:d6:b7:93:8d:82:33: 7a:a7:b8:e8:cd:a8:e2:94:3d:d6:16:e1:4e:13:63: 3f:77:08:10:cf:23:f6:15:7c:71:24:97:ef:1c:a2: 68:0f:82:e2:f7:24:b3:aa:70:1a:4a:b4:ca:4d:05: 92:5e:47:a2:3d:97:82:f6:d8:c8:04:a7:91:6c:a4: 7d:15:8e:a8:57:70:5d:50:1c:0b:36:ba:78:28:f2: da:5c:ed:4b:ea:60:8c:39:e6:a1:04:26:60:b3:e2: ee:4f:9b:f9:46:3c:7e:df:82:88:29:c2:76:3e:1a: a4:81:87:1f:ce:9e:41:68:de:6c:f3:89:df:ae:02: e7:12:ee:93:20:f1:d2:d6:3d:36:58:ee:71:bf:b3: c5:e7:5a:4b:a0:12:89:ed:f7:cc:ec:34:c7:b2:28: a8:1a:87:c6:8b:5e:d2:c8:25:71:ba:ff:d0:82:1b: 5e:50:a9:8a:c6:0c:ea:4b:17:a6:cc:13:0a:53:36: c6:9d:76:f2:95:cc:ac:b9:64:d5:72:fc:ab:ce:6b: 59:b1:3a:f2:49:2f:2c:09:d0:01:06:e4:f2:49:85: 79:82:e8:c8:bb:1a:ab:70:e3:49:97:9f:84:e0:96: c2:6d:41:ab:59:0c:2e:70:9a:2e:11:c8:83:69:4b: f1:19:97:87:c3:76:0e:bb:b0:2c:92:4a:07:03:6f: 57:bf:a9:ec:19:85:d6:3d:f8:de:03:7f:1b:9a:2f: 6c:02:72:28:b0:69:d5:f9:fb:3d:2e:31:8f:61:50: 59:a6:dd:43:4b:89:e9:68:4b:a6:0d:9b:00:0f:9a: 94:61:71 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Key Identifier: 8B:D6:F8:BD:B7:82:FC:13:BC:61:3F:8B:FA:84:24:3F:A2:14:C8:27 X509v3 Basic Constraints: critical CA:TRUE X509v3 Authority Key Identifier: keyid:8B:D6:F8:BD:B7:82:FC:13:BC:61:3F:8B:FA:84:24:3F:A2:14:C8:27 DirName:/C=US/ST=NY/L=New York/O=MongoDB/CN=ROOTCA serial:1E:83:0D:9D:43:75:7C:2B:D6:2A:DC:7E:A2:A2:25:AF:5D:3B:89:43\n\n|\n\nChapter 19: An Introduction to MongoDB Security",
      "content_length": 2224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "X509v3 Key Usage: critical Certificate Sign, CRL Sign Signature Algorithm: sha256WithRSAEncryption c2:cc:79:40:8b:7b:a1:87:3a:ec:4a:71:9d:ab:69:00:bb:6f: 56:0a:25:3b:8f:bd:ca:4d:4b:c5:27:28:3c:7c:e5:cf:84:ec: 2e:2f:0d:37:35:52:6d:f9:4b:07:fb:9b:da:ea:5b:31:0f:29: 1f:3c:89:6a:10:8e:ae:20:30:8f:a0:cf:f1:0f:41:99:6a:12: 5f:5c:ce:15:d5:f1:c9:0e:24:c4:81:70:df:ad:a0:e1:0a:cc: 52:d4:3e:44:0b:61:48:a9:26:3c:a3:3d:2a:c3:ca:4f:19:60: da:f7:7a:4a:09:9e:26:42:50:05:f8:74:13:4b:0c:78:f1:59: 39:1e:eb:2e:e1:e2:6c:cc:4d:96:95:79:c2:8b:58:41:e8:7a: e6:ad:37:e4:87:d7:ed:bb:7d:fa:47:dd:46:dd:e7:62:5f:e9: fe:17:4b:e3:7a:0e:a1:c5:80:78:39:b7:6c:a6:85:cf:ba:95: d2:8d:09:ab:2d:cb:be:77:9b:3c:22:12:ca:12:86:42:d8:c5: 3c:31:a0:ed:92:bc:7f:3f:91:2d:ec:db:01:bd:26:65:56:12: a3:56:ba:d8:d3:6e:f3:c3:13:84:98:2a:c7:b3:22:05:68:fa: 8e:48:6f:36:8e:3f:e5:4d:88:ef:15:26:4c:b1:d3:7e:25:84: 8c:bd:5b:d2:74:55:cb:b3:fa:45:3f:ee:ef:e6:80:e9:f7:7f: 25:a6:6e:f2:c4:22:f7:b8:40:29:02:f1:5e:ea:8e:df:80:e0: 60:f1:e5:3a:08:81:25:d5:cc:00:8f:5c:ac:a6:02:da:27:c0: cc:4e:d3:f3:14:60:c1:12:3b:21:b4:f7:29:9b:4c:34:39:3c: 2a:d1:4b:86:cc:c7:de:f3:f7:5e:8f:9d:47:2e:3d:fe:e3:49: 70:0e:1c:61:1c:45:a0:5b:d6:48:49:be:6d:f9:3c:49:26:d8: 8b:e6:a1:b2:61:10:fe:0c:e8:44:2c:33:cd:3c:1d:c2:de:c2: 06:98:7c:92:7b:c4:06:a5:1f:02:8a:03:53:ec:bd:b7:fc:31: f3:2a:c1:0e:6a:a5:a8:e4:ea:4d:cc:1d:07:a9:3f:f6:0e:35: 5d:99:31:35:b3:43:90:f3:1c:92:8e:99:15:13:2b:8f:f6:a6: 01:c9:18:05:15:2a:e3:d0:cc:45:66:d3:48:11:a2:b9:b1:20: 59:42:f7:88:15:9f:e0:0c:1d:13:ae:db:09:3d:bf:7a:9d:cf: b2:41:1e:7a:fa:6b:35:20:03:58:a1:6c:02:19:21:5f:25:fc: ba:2f:fc:79:d7:92:e7:37:77:14:10:d9:33:b6:e5:fb:7a:46: ab:d1:86:70:88:92:59:c3\n\nCreate an intermediate CA for signing\n\nNow that we’ve created our root CA, we will create an intermediate CA for signing member and client certificates. An intermediate CA is nothing more than a certificate signed using our root certificate. It is a best practice to use an intermediate CA to sign server (i.e., member) and client certificates. Typically, a CA will use different inter‐ mediate CAs for signing different categories of certificates. If the intermediate CA is compromised and the certificate needs to be revoked, only a portion of the trust tree is affected instead of all certificates signed by the CA, as would be the case if the root CA were used to sign all certificates.\n\n# again, in production you would want to password protect your signing key: # openssl genrsa -aes256 -out signing-ca.key 4096 openssl genrsa -out signing-ca.key 4096\n\nopenssl req -new -key signing-ca.key -out signing-ca.csr \\ -config openssl.cnf -subj \"$dn_prefix/CN=CA-SIGNER\" openssl x509 -req -days 730 -in signing-ca.csr -CA root-ca.crt -CAkey \\\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption\n\n|\n\n399",
      "content_length": 2782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "root-ca.key -set_serial 01 -out signing-ca.crt -extfile openssl.cnf \\ -extensions v3_ca\n\nNote that in the statements above we are using the openssl req command followed by the openssl ca command to sign our signing certificate using our root certificate. The openssl req command creates a signing request and the openssl ca command uses that request as input to create a signed intermediate (signing) certificate.\n\nAs a last step in creating our signing CA, we will concatenate our root certificate (containing our root public key) and signing certificate (containing our signing pub‐ lic key) into a single pem file. This file will be supplied to our mongod or client pro‐ cess later as the value of the --tlsCAFile option.\n\ncat root-ca.crt > root-ca.pem cat signing-ca.crt >> root-ca.pem\n\nWith the root CA and signing CA set up, we are now ready to create the member and client certificates used for authentication in our MongoDB cluster.\n\nGenerate and Sign Member Certificates Member certificates are typically referred to as x.509 server certificates. Use this type of certificate for mongod and mongos processes. Members of a MongoDB cluster use these certificates to verify membership in the cluster. Stated another way, one mongod authenticates itself with other members of a replica set using a server certificate.\n\nTo generate certificates for the members of our replica set, we will use a for loop to generate multiple certificates.\n\n# Pay attention to the OU part of the subject in \"openssl req\" command for host in \"${mongodb_server_hosts[@]}\"; do echo \"Generating key for $host\" openssl genrsa -out ${host}.key 4096\n\nopenssl req -new -key ${host}.key -out ${host}.csr -config openssl.cnf \\ -subj \"$dn_prefix/OU=$ou_member/CN=${host}\" openssl x509 -req -days 365 -in ${host}.csr -CA signing-ca.crt -CAkey \\ signing-ca.key -CAcreateserial -out ${host}.crt -extfile openssl.cnf \\ -extensions v3_req cat ${host}.crt > ${host}.pem cat ${host}.key >> ${host}.pem done\n\nThree steps are involved with each certificate:\n\nUse the openssl genrsa command to create a new key pair.\n\nUse the openssl req command to generate a signing request for the key.\n\nUse the openssl x509 command to sign and output a certificate using the signing CA.\n\n400\n\n|\n\nChapter 19: An Introduction to MongoDB Security",
      "content_length": 2295,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "Notice the variable $ou_member. This signifies the difference between server certifi‐ cates and client certificates. Server and client certificates must differ in the organiza‐ tion part of the Distinguished Names. More specifically, they must differ in at least one of the O, OU, or DC values.\n\nGenerate and Sign Client Certificates Client certificates are used by the mongo shell, MongoDB Compass, MongoDB utilit‐ ies and tools and, of course, by applications using a MongoDB driver. Generating cli‐ ent certificates follows essentially the same process as for member certificates. The one difference is our use of the variable $ou_client. This ensure that the combina‐ tion of the O, OU, and DC values will be different from those of the server certificates generated above.\n\n# Pay attention to the OU part of the subject in \"openssl req\" command for host in \"${mongodb_client_hosts[@]}\"; do echo \"Generating key for $host\" openssl genrsa -out ${host}.key 4096 openssl req -new -key ${host}.key -out ${host}.csr -config openssl.cnf \\ -subj \"$dn_prefix/OU=$ou_client/CN=${host}\" openssl x509 -req -days 365 -in ${host}.csr -CA signing-ca.crt -CAkey \\ signing-ca.key -CAcreateserial -out ${host}.crt -extfile openssl.cnf \\ -extensions v3_req cat ${host}.crt > ${host}.pem cat ${host}.key >> ${host}.pem done\n\nBring Up the Replica Set Without Authentication and Authorization Enabled We can start each member of our replica set without auth enabled as follows. Previ‐ ously, when working with replica sets we’ve not enabled auth so this should look familiar. Here again we are making use of a few variables we defined in “Generate a root CA” on page 395 (or see the full script for this chapter) and a loop to launch each member (mongod) of our replica set.\n\nmport=$mongodb_port for host in \"${mongodb_server_hosts[@]}\"; do echo \"Starting server $host in non-auth mode\" mkdir -p ./db/${host} mongod --replSet set509 --port $mport --dbpath ./db/$host \\ --fork --logpath ./db/${host}.log let \"mport++\" done\n\nOnce each mongod has started, we can then initialize a replica set using these mongods.\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption\n\n|\n\n401",
      "content_length": 2170,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "myhostname=`hostname` cat > init_set.js <<EOF rs.initiate(); mport=$mongodb_port; mport++; rs.add(\"localhost:\" + mport); mport++; rs.add(\"localhost:\" + mport); EOF mongo localhost:$mongodb_port init_set.js\n\nNote that the code above simply constructs a series of commands, stores these com‐ mands in a JavaScript file, and then runs the mongo shell to execute the small script that was created. Together, these commands, when executed in the mongo shell, will connect to the mongod running on port 27017 (value of the $mongodb_port variable set in “Generate a root CA” on page 395), initiate the replica set, and then add each of the other two mongods (on ports 27018 and 27019) to the replica set.\n\nCreate the Admin User Now, we’ll create an admin user based on one of the client certificates we created in “Generate and Sign Client Certificates” on page 401. We will authenticate as this user when connecting from the mongo shell or another client to perform administrative tasks. To authenticate with a client certificate, you must first add the value of the sub‐ ject from the client certificate as a MongoDB user. Each unique x.509 client certificate corresponds to a single MongoDB user; i.e., you cannot use a single client certificate to authenticate more than one MongoDB user. We must add the user in the $external database; i.e., the authentication database is the $external database.\n\nFirst, we’ll get the subject from our client certificate using the openssl x509 command.\n\nopenssl x509 -in client1.pem -inform PEM -subject -nameopt RFC2253 | grep subject\n\nThis should result in the following output:\n\nsubject= CN=client1,OU=MyClients,O=MongoDB,L=New York,ST=NY,C=US\n\nTo create our admin user, we’ll first connect to the primary of our replica set using the mongo shell.\n\nmongo --norc localhost:27017\n\nFrom within the mongo shell, we will issue the following command:\n\ndb.getSiblingDB(\"$external\").runCommand( { createUser: \"CN=client1,OU=MyClients,O=MongoDB,L=New York,ST=NY,C=US\", roles: [ { role: \"readWrite\", db: 'test' }, { role: \"userAdminAnyDatabase\", db: \"admin\" }, { role: \"clusterAdmin\", db:\"admin\"}\n\n402\n\n|\n\nChapter 19: An Introduction to MongoDB Security",
      "content_length": 2178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "], writeConcern: { w: \"majority\" , wtimeout: 5000 } } );\n\nNote the use of the $external database in this command and the fact that we’ve speci‐ fied the subject of our client certificate as the user name.\n\nRestart the Replica Set with Authentication and Authorization Enabled Now that we have an admin user, we can restart the replica set with authentication and authorization enabled and connect as a client. Without a user of any kind, it would be impossible to connect to a replica set with auth enabled.\n\nLet’s stop the replica set in it’s current form (without auth enabled).\n\nkill $(ps -ef | grep mongod | grep set509 | awk '{print $2}')\n\nWe are now ready to restart the replica set with auth enabled. In a production envi‐ ronment, we would copy each of the certificate and key files to their corresponding hosts. Here we’re doing everything on localhost to make things easier. To initiate a secure replica set we will add the following command-line options to each invocation of mongod:\n\n--tlsMode • --clusterAuthMode • --tlsCAFile—root CA file (root-ca.key) • --tlsCertificateKeyFile—certificate file for the mongod • --tlsAllowInvalidHostnames—only used for testing; allows invalid hostnames\n\nHere the file we provide as the value of the tlsCAFile option is used to establish a trust chain. As you recall the root-ca.key file contains the certificate of the root CA as well as the signing CA. By providing this file to the mongod process, we are stating our desire to trust the certificate contained in this file as well as all other certificates signed by these certificates.\n\nOkay, let’s do this.\n\nmport=$mongodb_port for host in \"${mongodb_server_hosts[@]}\"; do echo \"Starting server $host\" mongod --replSet set509 --port $mport --dbpath ./db/$host \\ --tlsMode requireTLS --clusterAuthMode x509 --tlsCAFile root-ca.pem \\ --tlsAllowInvalidHostnames --fork --logpath ./db/${host}.log \\ --tlsCertificateKeyFile ${host}.pem --tlsClusterFile ${host}.pem \\ --bind_ip 127.0.0.1\n\nA Tutorial on MongoDB Authentication and Transport Layer Encryption\n\n|\n\n403",
      "content_length": 2060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "let \"mport++\" done\n\nAnd with that, we have a three-member replica set secured using x.509 certificates for authentication and transport-layer encryption. The only thing left to do is to connect with the mongo shell. We’ll use the client1 certificate to authenticate, because that is the certificate for which we created an admin user.\n\nmongo --norc --tls --tlsCertificateKeyFile client1.pem --tlsCAFile root-ca.pem \\ --tlsAllowInvalidHostnames --authenticationDatabase \"\\$external\" \\ --authenticationMechanism MONGODB-X509\n\nOnce connected, we encourage you to experiment by inserting some data to a collec‐ tion. You should also attempt to connect using any other user (e.g., using the client2.pem). Connections attempts will result in errors like the following.\n\nmongo --norc --tls --tlsCertificateKeyFile client2.pem --tlsCAFile root-ca.pem \\ --tlsAllowInvalidHostnames --authenticationDatabase \"\\$external\" \\ --authenticationMechanism MONGODB-X509 MongoDB shell version v4.2.0 2019-09-11T23:18:31.696+0100 W NETWORK [js] The server certificate does not match the host name. Hostname: 127.0.0.1 does not match 2019-09-11T23:18:31.702+0100 E QUERY [js] Error: Could not find user \"CN=client2,OU=MyClients,O=MongoDB,L=New York,ST=NY,C=US\" for db \"$external\" : connect@src/mongo/shell/mongo.js:341:17 @(connect):3:6 2019-09-11T23:18:31.707+0100 F - [main] exception: connect failed 2019-09-11T23:18:31.707+0100 E - [main] exiting with code 1\n\nIn the tutorial in this chapter, we’ve looked at an example of using x.509 certificates as a basis for authentication and to encrypt communication among clients and members of a replica set. The same procedure works for sharded clusters as well. With respect to securing a MongoDB cluster, please keep the following in mind:\n\nThe directories, root CA and signing CA, as well as the host itself where you gen‐ erate and sign certificates for the member machines or clients, should be pro‐ tected from unauthorized access.\n\nFor simplicity, the root CA and signing CA keys are not password protected in this tutorial. In production it is necessary to use passwords to protect the key from unauthorized use.\n\nWe encourage you to download and experiment with the demo scripts we have pro‐ vided for this chapter in the book’s GitHub repository.\n\n404\n\n|\n\nChapter 19: An Introduction to MongoDB Security",
      "content_length": 2338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "CHAPTER 20 Durability\n\nDurability is a property of database systems that guarantees that write operations that have been committed to the database will survive permanently. For example, if a ticket reservation system reports that your concert seats have been booked, then your seats will remain booked even if some part of the reservation system crashes. For MongoDB, we need to consider durability at the cluster (or more specifically, replica set) level.\n\nIn this chapter, we will cover:\n\nHow MongoDB guarantees durability at the replica set member level through journaling\n\nHow MongoDB guarantees durability at the cluster level using write concern\n\nHow to configure your application and MongoDB cluster to give you the level of durability you need\n\nHow MongoDB guarantees durability at the cluster level using read concern\n\nHow to set the durability level for transactions in replica sets\n\nThroughout this chapter, we will discuss durability in replica sets. A three-member replica set is the most basic cluster recommended for production applications. The discussion here applies to replica sets with more members and to sharded clusters.\n\nDurability at the Member Level Through Journaling To provide durability in the event of a server failure, MongoDB uses a write-ahead log (WAL) called the journal. A WAL is a commonly used technique for durability in database systems. The idea is that we simply write a representation of the changes to be made to the database to a durable medium (i.e., to disk) before applying those\n\n405",
      "content_length": 1533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "changes to the database itself. In many database systems, a WAL is used to provide the atomicity database property as well. However, MongoDB uses other techniques to ensure atomic writes.\n\nBeginning in MongoDB 4.0, as an application performs writes to a replica set, for the data in all replicated collections MongoDB creates journal entries using the same for‐ mat as the oplog.1 As discussed in Chapter 11, MongoDB uses statement-based repli‐ cation based on an operations log, or oplog. The statements in the oplog are a representation of the actual MongoDB changes made to each document affected by a write. Therefore, oplog statements are easy to apply to any member of a replica set regardless of version, hardware, or any other differences between replica set mem‐ bers. In addition, each oplog statement is idempotent, meaning that it can be applied any number of times and the outcome will always be the same change to the database.\n\nLike most databases, MongoDB maintains in-memory views of both the journal and the database data files. By default, it flushes journal entries to disk every 50 milli‐ seconds and flushes database files to disk every 60 seconds. The 60-second interval for flushing data files is called a checkpoint. The journal is used to provide durability for data written since the last checkpoint. With respect to durability concerns, if the server suddenly stops, when it’s restarted the journal can be used to replay any writes that were not flushed to disk before the shutdown.\n\nFor the journal files, MongoDB creates a subdirectory named journal under the dbPath directory. WiredTiger (MongoDB’s default storage engine) journal files have names with the format WiredTigerLog.<sequence>, where <sequence> is a zero- padded number starting from 0000000001. Except for very small log records, Mon‐ goDB compresses the data written to the journal. Journal files have a maximum size limit of approximately 100 MB. Once a journal file exceeds that limit, MongoDB cre‐ ates a new journal file and begins writing new records there. Because journal files are only needed to recover data since the last checkpoint, MongoDB automatically removes “old” journal files—i.e., those written prior to the most recent checkpoint— once a new checkpoint is written.\n\nIf there is a crash (or kill -9), mongod will replay its journal files on startup. By default, the greatest extent of lost writes are those made in the last 100 ms plus the time it takes to flush the journal writes to disk.\n\nIf your application requires a shorter interval for journal flushes, you have two options. One is to change the interval using the --journalCommitInterval option to the mongod command. This option accepts values ranging from 1 to 500 ms. The other option, which we’ll look at in the next section, is to specify in the write concern\n\n1 MongoDB uses a different format for writes to the local database, which stores data used in the replication\n\nprocess and other instance-specific data, but the principles and application are similar.\n\n406\n\n|\n\nChapter 20: Durability",
      "content_length": 3071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "that all writes should journal to disk. Shortening the interval for journaling to disk will negatively impact performance, so you need to be sure of the implications for your applications before changing the journaling default.\n\nDurability at the Cluster Level Using Write Concern With write concern, you can specify what level of acknowledgment your application requires in response to write requests. In a replica set, network partitions, server fail‐ ures, or data center outages may keep writes from being replicated to every member, or even a majority of the members. When a normal state is restored to the replica set, it is possible that writes not replicated to a majority of members will be rolled back. In those situations, clients and the database may have a different view of what data has been committed.\n\nThere are applications for which it might be acceptable in some circumstances to have writes rolled back. For example, it might be okay to roll back a small number of comments in a social application of some kind. MongoDB supports a range of dura‐ bility guarantees at the cluster level to enable application designers to select the dura‐ bility level that works best for their use case.\n\nThe w and wtimeout Options for writeConcern The MongoDB query language supports specifying a write concern for all insert and update methods. As an example, suppose we have an ecommerce application and want to ensure that all orders are durable. Writing an order to the database might look something like the following:\n\ntry { db.products.insertOne( { sku: \"H1100335456\", item: \"Electric Toothbrush Head\", quantity: 3 }, { writeConcern: { w : \"majority\", wtimeout : 100 } } ); } catch (e) { print (e); }\n\nAll insert and update methods take a second parameter, a document. Within that document you can specify a value for writeConcern. In the preceding example, the write concern we have specified indicates that we want to see an acknowledgment from the server that the write completed successfully only if the write was successfully replicated to a majority of the members of our application’s replica set. In addition, the write should return an error if it is not replicated to a majority of replica set mem‐ bers in 100 ms or less. In the case of such an error, MongoDB does not undo success‐ ful data modifications performed before the write concern exceeded the time limit— it will be up to the application to choose how to handle timeouts in such situations. In general, you should configure the wtimeout value so that only in unusual circumstan‐\n\nDurability at the Cluster Level Using Write Concern\n\n|\n\n407",
      "content_length": 2622,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "ces will the application experience timeouts and any actions your application takes in response to a timeout error will ensure the correct state for your data. In most cases, your application should attempt to determine whether the timeout was a result of a transient slowdown in network communications or something more signficant.\n\nAs the value for w in the write concern document, you may specify \"majority\" (as was done in this example). Alternatively, you may specify an integer between zero and the number of members in the replica set. Finally, it is possible to tag replica set members, say to identify those on SSDs versus spinning disks or those used for reporting versus OLTP workloads. You may specify a tag set as the value of w to ensure that writes will only be acknowledged once committed to at least one member of the replica set matching the provided tag set.\n\nThe j (Journaling) Option for writeConcern In addition to providing a value for the w option, you may also request acknowledg‐ ment that the write operation has been written to the journal by using the j option in the write concern document. With a value of true for j, MongoDB acknowledges a successful write only after the requested number of members (the value for w) have written the operation to their on-disk journal. Continuing our example, if we want to ensure all writes are journaled on a majority of members, we can update the code as follows:\n\ntry { db.products.insertOne( { sku: \"H1100335456\", item: \"Electric Toothbrush Head\", quantity: 3 }, { writeConcern: { w : \"majority\", wtimeout : 100, j : true } } ); } catch (e) { print (e); }\n\nWithout waiting for journaling, there is a brief window of about 100 ms on each member when, if the server process or hardware goes down, a write could be lost. However, waiting for journaling before acknowledging writes to members of a replica set does have a performance penalty.\n\nIt is essential that in addressing durability concerns for your applications, you care‐ fully evaluate the requirements your application has and weigh the performance impacts of the durability settings you select.\n\nDurability at a Cluster Level Using Read Concern In MongoDB, read concerns allow for the configuration of when results are read. This can allow clients to see write results before those writes are durable. A read con‐ cern can be used with a write concern to control the level of consistency and\n\n408\n\n|\n\nChapter 20: Durability",
      "content_length": 2454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "availability guarantees made to an application. They should not be confused with read preferences, which deal with where the data is read from; specifically, read preferences determine the data bearing member(s) in the replica set. The default read preferences is to read from the primary.\n\nRead concern determines the consistency and isolation properties of the data being read. The default readConcern is local, which returns data with no guarantees that the data has been written to the majority of the data bearing replica set members. This can result in the data being rolled back in the future. The majority concern returns only durable data (will not be rolled back) that has been acknowledged by the majority of replica set members. In MongoDB 3.4, the linearizable concern was added. It ensures data returned reflects all successful majority-acknowledged writes that have completed prior to the start of the read operation. It may wait for concur‐ rently executing writes to finish before providing results.\n\nIn the same fashion, with write concerns you will need to weight the performance impacts of the read concerns against the durability and isolation guarantees they provide before selecting the appropriate concern for your application.\n\nDurability of Transactions Using a Write Concern In MongoDB, operations on individual documents are atomic. You can use embed‐ ded documents and arrays to express relationships between entities in a single docu‐ ment rather than using a normalized data model splitting entities and relationships across multiple collections. As a result, many applications do not require multi- document transactions.\n\nHowever, for use cases that require atomicity for updates to multiple documents, MongoDB provides the ability to perform multi-document transactions against rep‐ lica sets. Multi-document transactions can be used across multiple operations, docu‐ ments, collections, and databases.\n\nTransactions require that all data changes within the transaction are successful. If any operation fails, the transaction aborts and all data changes are discarded. If all opera‐ tions are successful, all data changes made in the transaction are saved and the writes become visible to future reads.\n\nAs with individual write operations, you may specify a write concern for transactions. You set the write concern at the transaction level, not at the individual operation level. At the time of the commit, transactions use the transaction-level write concern to commit the write operations. Write concerns set for individual operations inside the transaction will be ignored.\n\nYou can set the write concern for the transaction commit at the transaction start. A write concern of 0 is not supported for transactions. If you use a write concern of 1\n\nDurability of Transactions Using a Write Concern\n\n|\n\n409",
      "content_length": 2842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "for a transaction, it can be rolled back if there is a failover. You may use a writeCon cern of \"majority\" to ensure transactions are durable in the face of network and server failures that might force a failover in a replica set. The following provides an example:\n\nfunction updateEmployeeInfo(session) { employeesCollection = session.getDatabase(\"hr\").employees; eventsCollection = session.getDatabase(\"reporting\").events;\n\nsession.startTransaction( {writeConcern: { w: \"majority\" } } );\n\ntry{ employeesCollection.updateOne( { employee: 3 }, { $set: { status: \"Inactive\" } } ); eventsCollection.insertOne( { employee: 3, status: { new: \"Inactive\", old: \"Active\" } } ); } catch (error) { print(\"Caught exception during transaction, aborting.\"); session.abortTransaction(); throw error; }\n\ncommitWithRetry(session); }\n\nWhat MongoDB Does Not Guarantee There are a couple of situations where MongoDB cannot guarantee durability, such as if there are hardware issues or filesystem bugs. In particular, if a hard disk is corrupt, there is nothing MongoDB can do to protect your data.\n\nAlso, different varieties of hardware and software may have different durability guar‐ antees. For example, some cheaper or older hard disks report a write’s success while the write is queued up to be written, not when it has actually been written. MongoDB cannot defend against misreporting at this level: if the system crashes, data may be lost.\n\nBasically, MongoDB is only as safe as the underlying system: if the hardware or file‐ system destroys the data, MongoDB cannot prevent it. Use replication to defend against system issues. If one machine fails, hopefully another will still be functioning correctly.\n\nChecking for Corruption The validate command can be used to check a collection for corruption. To run validate on the movies collection, do:\n\n410\n\n|\n\nChapter 20: Durability",
      "content_length": 1868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "db.movies.validate({full: true}) {\n\n\"ns\" : \"sample_mflix.movies\", \"nInvalidDocuments\" : NumberLong(0), \"nrecords\" : 45993, \"nIndexes\" : 5, \"keysPerIndex\" : {\n\n\"_id_\" : 45993, \"$**_text\" : 3671341, \"genres_1_imdb.rating_1_metacritic_1\" : 94880, \"tomatoes_rating\" : 45993, \"getMovies\" : 45993\n\n}, \"indexDetails\" : {\n\n\"$**_text\" : {\n\n\"valid\" : true\n\n}, \"_id_\" : {\n\n\"valid\" : true\n\n}, \"genres_1_imdb.rating_1_metacritic_1\" : {\n\n\"valid\" : true\n\n}, \"getMovies\" : {\n\n\"valid\" : true\n\n}, \"tomatoes_rating\" : { \"valid\" : true\n\n}\n\n}, \"valid\" : true, \"warnings\" : [ ], \"errors\" : [ ], \"extraIndexEntries\" : [ ], \"missingIndexEntries\" : [ ], \"ok\" : 1\n\n}\n\nThe main field you’re looking for is \"valid\", which will hopefully be true. If it is not, validate will give some details about the corruption it found.\n\nMost of the output from validate describes internal structures of the collection and timestamps used to understand the order of operations across a cluster. These are not particularly useful for debugging. (See Appendix B for more information on collec‐ tion internals.)\n\nYou can only run validate on collections, and it will also check the associated indexes in the field indexDetails. However, this requires a full validate, which is configured with the { full: true } option.\n\nChecking for Corruption\n\n|\n\n411",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "PART VI Server Administration",
      "content_length": 29,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "CHAPTER 21 Setting Up MongoDB in Production\n\nIn Chapter 2, we covered the basics of starting MongoDB. This chapter will go into more detail about which options are important for setting up MongoDB in produc‐ tion, including:\n\nCommonly used options\n\nStarting up and shutting down MongoDB\n\nSecurity-related options\n\nLogging considerations\n\nStarting from the Command Line The MongoDB server is started with the mongod executable. mongod has many config‐ urable startup options; to view all of them, run mongod --help from the command line. A couple of the options are widely used and important to be aware of:\n\n--dbpath\n\nSpecify an alternate directory to use as the data directory; the default is /data/db/ (or, on Windows, \\data\\db\\ on the MongoDB binary’s volume). Each mongod process on a machine needs its own data directory, so if you are running three instances of mongod on one machine, you’ll need three separate data directories. When mongod starts up, it creates a mongod.lock file in its data directory, which prevents any other mongod process from using that directory. If you attempt to start another MongoDB server using the same data directory, it will give an error:\n\nexception in initAndListen: DBPathInUse: Unable to lock the lock file: \\ data/db/mongod.lock (Resource temporarily unavailable). Another mongod instance is already running on the\n\n415",
      "content_length": 1364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "data/db directory, \\ terminating\n\n--port\n\nSpecify the port number for the server to listen on. By default, mongod uses port 27017, which is unlikely to be used by another process (besides other mongod pro‐ cesses). If you would like to run more than one mongod process on a single machine, you’ll need to specify different ports for each one. If you try to start mongod on a port that is already being used, it will give an error:\n\nFailed to set up listener: SocketException: Address already in use.\n\n--fork\n\nOn Unix-based systems, fork the server process, running MongoDB as a daemon.\n\nIf you are starting up mongod for the first time (with an empty data directory), it can take the filesystem a few minutes to allocate database files. The parent pro‐ cess will not return from forking until the preallocation is done and mongod is ready to start accepting connections. Thus, fork may appear to hang. You can tail the log to see what it is doing. You must use --logpath if you specify --fork.\n\n--logpath\n\nSend all output to the specified file rather than outputting on the command line. This will create the file if it does not exist, assuming you have write permissions to the directory. It will also overwrite the log file if it already exists, erasing any older log entries. If you’d like to keep old logs around, use the --logappend option in addition to --logpath (highly recommended).\n\n--directoryperdb\n\nPut each database in its own directory. This allows you to mount different data‐ bases on different disks, if necessary or desired. Common uses for this are putting a local database on its own disk (replication) or moving a database to a different disk if the original one fills up. You could also put databases that handle more load on faster disks and databases with a lower load on slower disks. This basically gives you more flexibility to move things around later.\n\n--config\n\nUse a configuration file for additional options not specified on the command line. This is typically used to make sure options are the same between restarts. See “File-Based Configuration” on page 419 for details.\n\nFor example, to start the server as a daemon listening on port 5586 and sending all output to mongodb.log, we could run this:\n\n$ ./mongod --dbpath data/db --port 5586 --fork --logpath mongodb.log --logappend 2019-09-06T22:52:25.376-0500 I CONTROL [main]\n\n416\n\n|\n\nChapter 21: Setting Up MongoDB in Production",
      "content_length": 2414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "Automatically disabling TLS 1.0, \\ to force-enable TLS 1.0 specify --sslDisabledProtocols 'none' about to fork child process, waiting until server is ready for connections. forked process: 27610 child process started successfully, parent exiting\n\nWhen you first install and start MongoDB, it is a good idea to look at the log. This might be an easy thing to miss, especially if MongoDB is being started from an init script, but the log often contains important warnings that prevent later errors from occurring. If you don’t see any warnings in the MongoDB log on startup, then you are all set. (Startup warnings will also appear on shell startup.)\n\nIf there are any warnings in the startup banner, take note of them. MongoDB will warn you about a variety of issues: that you’re running on a 32-bit machine (which MongoDB is not designed for), that you have NUMA enabled (which can slow your application to a crawl), or that your system does not allow enough open file descrip‐ tors (MongoDB uses a lot of file descriptors).\n\nThe log preamble won’t change when you restart the database, so feel free to run MongoDB from an init script and ignore the logs, once you know what they say. However, it’s a good idea to check again each time you do an install, upgrade, or recover from a crash, just to make sure MongoDB and your system are on the same page.\n\nWhen you start the database, MongoDB will write a document to the local.startup_log collection that describes the version of MongoDB, underlying system, and flags used. We can look at this document using the mongo shell:\n\n> use local switched to db local > db.startup_log.find().sort({startTime: -1}).limit(1).pretty() { \"_id\" : \"server1-1544192927184\", \"hostname\" : \"server1.example.net\", \"startTime\" : ISODate(\"2019-09-06T22:50:47Z\"), \"startTimeLocal\" : \"Fri Sep 6 22:57:47.184\", \"cmdLine\" : { \"net\" : { \"port\" : 5586 }, \"processManagement\" : { \"fork\" : true }, \"storage\" : { \"dbPath\" : \"data/db\" }, \"systemLog\" : { \"destination\" : \"file\", \"logAppend\" : true, \"path\" : \"mongodb.log\" }\n\nStarting from the Command Line\n\n|\n\n417",
      "content_length": 2080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "418\n\n}, \"pid\" : NumberLong(27278), \"buildinfo\" : { \"version\" : \"4.2.0\", \"gitVersion\" : \"a4b751dcf51dd249c5865812b390cfd1c0129c30\", \"modules\" : [ \"enterprise\" ], \"allocator\" : \"system\", \"javascriptEngine\" : \"mozjs\", \"sysInfo\" : \"deprecated\", \"versionArray\" : [ 4, 2, 0, 0 ], \"openssl\" : { \"running\" : \"Apple Secure Transport\" }, \"buildEnvironment\" : { \"distmod\" : \"\", \"distarch\" : \"x86_64\", \"cc\" : \"gcc: Apple LLVM version 8.1.0 (clang-802.0.42)\", \"ccflags\" : \"-mmacosx-version-min=10.10 -fno-omit\\ -frame-pointer -fno-strict-aliasing \\ -ggdb -pthread -Wall -Wsign-compare -Wno-unknown-pragmas \\ -Winvalid-pch -Werror -O2 -Wno-unused\\ -local-typedefs -Wno-unused-function -Wno-unused-private-field \\ -Wno-deprecated-declarations \\ -Wno-tautological-constant-out-of\\ -range-compare -Wno-unused-const-variable -Wno\\ -missing-braces -Wno-inconsistent\\ -missing-override -Wno-potentially-evaluated-expression \\ -Wno-exceptions -fstack-protector\\ -strong -fno-builtin-memcmp\", \"cxx\" : \"g++: Apple LLVM version 8.1.0 (clang-802.0.42)\", \"cxxflags\" : \"-Woverloaded-virtual -Werror=unused-result \\ -Wpessimizing-move -Wredundant-move \\ -Wno-undefined-var-template -stdlib=libc++ \\ -std=c++14\", \"linkflags\" : \"-mmacosx-version-min=10.10 -Wl, \\ -bind_at_load -Wl,-fatal_warnings \\ -fstack-protector-strong \\ -stdlib=libc++\", \"target_arch\" : \"x86_64\", \"target_os\" : \"macOS\" },\n\n|\n\nChapter 21: Setting Up MongoDB in Production",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "\"bits\" : 64, \"debug\" : false, \"maxBsonObjectSize\" : 16777216, \"storageEngines\" : [ \"biggie\", \"devnull\", \"ephemeralForTest\", \"inMemory\", \"queryable_wt\", \"wiredTiger\" ] } }\n\nThis collection can be useful for tracking upgrades and changes in behavior.\n\nFile-Based Configuration MongoDB supports reading configuration information from a file. This can be useful if you have a large set of options you want to use or are automating the task of starting up MongoDB. To tell the server to get options from a configuration file, use the -f or --config flags. For example, run mongod --config ~/.mongodb.conf to use ~/.mongodb.conf as a configuration file.\n\nThe options supported in a configuration file are the same as those accepted at the command line. However, the format is different. As of MongoDB 2.6, MongoDB con‐ figuration files use the YAML format. Here’s an example configuration file:\n\nsystemLog: destination: file path: \"mongod.log\" logAppend: true storage: dbPath: data/db processManagement: fork: true net: port: 5586 ...\n\nThis configuration file specifies the same options we used earlier when starting with regular command-line arguments. Note that these same options are reflected in the startup_log collection document we looked at in the previous section. The only real difference is that the options are specified using JSON rather than YAML.\n\nIn MongoDB 4.2, expansion directives were added to allow the loading of specific configuration file options or loading of the entire configuration file. The advantage of expansion directives is that confidential information, such as passwords and security certificates, does not have to be stored in the config file directly. The --configExpand command-line option enables this feature and must include the expansion directives\n\nStarting from the Command Line\n\n|\n\n419",
      "content_length": 1824,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "you wish to enable. __rest and __exec are the current implementation of the expan‐ sion directives in MongoDB. The __rest expansion directive loads specific configu‐ ration file values or loads the entire configuration file from a REST endpoint. The __exec expansion directive loads specific configuration file values or loads the entire configuration file from a shell or terminal command.\n\nStopping MongoDB Being able to safely stop a running MongoDB server is at least as important as being able to start one. There are a couple of different options for doing this effectively.\n\nThe cleanest way to shut down a running server is to use the shutdown command, {\"shutdown\" : 1}. This is an admin command and must be run on the admin data‐ base. The shell features a helper function to make this easier:\n\n> use admin switched to db admin > db.shutdownServer() server should be down...\n\nWhen run on a primary, the shutdown command steps down the primary and waits for a secondary to catch up before shutting down the server. This minimizes the chance of rollback, but the shutdown isn’t guaranteed to succeed. If there is no secon‐ dary available that can catch up within a few seconds, the shutdown command will fail and the (former) primary will not shut down:\n\n> db.shutdownServer() { \"closest\" : NumberLong(1349465327), \"difference\" : NumberLong(20), \"errmsg\" : \"no secondaries within 10 seconds of my optime\", \"ok\" : 0 }\n\nYou can force the shutdown command to shut down a primary by using the force option:\n\ndb.adminCommand({\"shutdown\" : 1, \"force\" : true})\n\nThis is equivalent to sending a SIGINT or SIGTERM signal (all three of these options result in a clean shutdown, but there may be unreplicated data). If the server is run‐ ning as the foreground process in a terminal, a SIGINT can be sent by pressing Ctrl- C. Otherwise, a command like kill can be used to send the signal. If mongod had 10014 as its PID, the command would be kill -2 10014 (SIGINT) or kill 10014 (SIGTERM).\n\n420\n\n|\n\nChapter 21: Setting Up MongoDB in Production",
      "content_length": 2039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "When mongod receives a SIGINT or SIGTERM, it will do a clean shutdown. This means it will wait for any running operations or file preallocations to finish (this could take a moment), close all open connections, flush all data to disk, and halt.\n\nSecurity Do not set up publicly addressable MongoDB servers. You should restrict access as tightly as possible between the outside world and MongoDB. The best way to do this is to set up firewalls and only allow MongoDB to be reachable on internal network addresses. Chapter 24 covers what connections it’s necessary to allow between MongoDB servers and clients.\n\nBeyond firewalls, there are a few options you can add to your config file to make it more secure:\n\n--bind_ip\n\nSpecify the interfaces that you want MongoDB to listen on. Generally you want this to be an internal IP: something application servers and other members of your cluster can access but that is inaccessible to the outside world. localhost is fine for mongos processes if you’re running the application server on the same machine. For config servers and shards, they’ll need to be addressable from other machines, so stick with non-localhost addresses.\n\nStarting in MongoDB 3.6, mongod and mongos processes bind to localhost by default. When bound only to localhost, mongod and mongos will only accept con‐ nections from clients running on the same machine. This helps limit the expo‐ sure of unsecured MongoDB instances. To bind to other addresses, use the net.bindIp configuration file setting or the --bind_ip command-line option to specify a list of hostnames or IP addresses.\n\n--nounixsocket\n\nDisable listening on the UNIX domain socket. If you’re not planning to connect via filesystem socket, you might as well disallow it. You would only connect via filesystem socket on a machine that is also running an application server: you must be local to use a filesystem socket.\n\n--noscripting\n\nDisable server-side JavaScript execution. Some security issues that have been reported with MongoDB have been JavaScript-related, so it’s generally safer to disallow it, if your application allows.\n\nSecurity\n\n|\n\n421",
      "content_length": 2127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "Several shell helpers assume that JavaScript is available on the server, notably sh.status(). You will see errors if you attempt to run any of these helpers with JavaScript disabled.\n\nData Encryption Data encryption is available in MongoDB Enterprise. These options are not sup‐ ported in the Community version of MongoDB.\n\nThe data encryption process includes the following steps:\n\nGenerate a master key.\n\nGenerate keys for each database.\n\nEncrypt data with the database keys.\n\nEncrypt the database keys with the master key.\n\nWhen using data encryption, all data files are encrypted in the filesystem. Data is only unencrypted in memory and during transmission. To encrypt all of MongoDB’s net‐ work traffic, you can use TLS/SSL. The data encryption options that MongoDB Enterprise users can add to their config files are:\n\n--enableEncryption\n\nEnables encryption in the WiredTiger storage engine. With this option, data stored in memory and on disk will be encrypted. This is sometimes referred to as “encryption at rest.” You must set this to true in order to pass in encryption keys and to configure encryption. This option is false by default.\n\n--encryptionCipherMode\n\nSet the cipher mode for encryption at rest in WiredTiger. There are two modes available: AES256-CBC and AES256-GCM. AES256-CBC is an acronym for 256- bit Advanced Encryption Standard in Cipher Block Chaining Mode. AES256- GCM uses Galois/Counter Mode. Both are standard encryption ciphers. As of MongoDB 4.0, MongoDB Enterprise on Windows no longer supports AES256- GCM.\n\n--encryptionKeyFile\n\nSpecify the path to the local keyfile if you are managing keys using a process other than the Key Management Interoperability Protocol (KMIP).\n\nMongoDB Enterprise also supports key management using KMIP. A discussion of KMIP is beyond the scope of this book. Please see the MongoDB documentation for details on using KMIP with MongoDB.\n\n422\n\n|\n\nChapter 21: Setting Up MongoDB in Production",
      "content_length": 1955,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "SSL Connections As we saw in Chapter 18, MongoDB supports transport encryption using TLS/SSL. This feature is available in all editions of MongoDB. By default, connections to Mon‐ goDB transfer data unencrypted. However, TLS/SSL ensures transport encryption. MongoDB uses native TSL/SSL libraries available on your operating system. Use the option --tlsMode and related options to configure TLS/SSL. Refer to Chapter 18 for more detail, and consult your driver’s documentation on how to create TLS/SSL con‐ nections using your language.\n\nLogging By default, mongod sends its logs to stdout. Most init scripts use the --logpath option to send logs to a file. If you have multiple MongoDB instances on a single machine (say, a mongod and a mongos), make sure that their logs are stored in sepa‐ rate files. Be sure that you know where the logs are and have read access to the files.\n\nMongoDB spits out a lot of log messages, but please do not run with the --quiet option (which suppresses some of them). Leaving the log level at the default is usually perfect: there is enough information for basic debugging (why is this slow, why isn’t this starting up, etc.), but the logs do not take up too much space.\n\nIf you are debugging a specific issue with your application, there are a couple of options for getting more information from the logs. You can change the log level by running the setParameter command, or by setting the log level at startup time by passing it as a string using the --setParameter option.\n\n> db.adminCommand({\"setParameter\" : 1, \"logLevel\" : 3})\n\nYou can also change the log level for a particular component. This is helpful if you are debugging a specific aspect of your application and require more information, but only from that component. In this example, we set the default log verbosity to 1 and the query component verbosity to 2:\n\n> db.adminCommand({\"setParameter\" : 1, logComponentVerbosity: { verbosity: 1, query: { verbosity: 2 }}})\n\nRemember to turn the log level back down to 0 when you’re done debugging, or your logs may be needlessly noisy. You can turn the level all the way up to 5, at which point mongod will print out almost every action it takes, including the contents of every request handled. This can cause a lot of I/O as mongod writes everything to the log file, which can slow down a busy system. Turning on profiling is a better option if you need to see every operation as it’s happening.\n\nBy default, MongoDB logs information about queries that take longer than 100 ms to run. If 100 ms is too short or too long for your application, you can change the thres‐ hold with setProfilingLevel:\n\nLogging\n\n|\n\n423",
      "content_length": 2658,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "> // Only log queries that take longer than 500 ms > db.setProfilingLevel(1, 500) { \"was\" : 0, \"slowms\" : 100, \"ok\" : 1 } > db.setProfilingLevel(0) { \"was\" : 1, \"slowms\" : 500, \"ok\" : 1 }\n\nThe second line will turn off profiling, but the value in milliseconds given in the first line will continue to be used as a threshold for the log (across all databases). You can also set this parameter by restarting MongoDB with the --slowms option.\n\nFinally, set up a cron job that rotates your log every day or week. If MongoDB was started with --logpath, sending the process a SIGUSR1 signal will make it rotate the log. There is also a logRotate command that does the same thing:\n\n> db.adminCommand({\"logRotate\" : 1})\n\nYou cannot rotate logs if MongoDB was not started with --logpath.\n\n424\n\n|\n\nChapter 21: Setting Up MongoDB in Production",
      "content_length": 832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "CHAPTER 22 Monitoring MongoDB\n\nBefore you deploy, it is important to set up some type of monitoring. Monitoring should allow you to track what your server is doing and alert you if something goes wrong. This chapter will cover:\n\nHow to track MongoDB’s memory usage\n\nHow to track application performance metrics\n\nHow to diagnose replication issues\n\nWe’ll use example graphs from MongoDB Ops Manager to demonstrate what to look for when monitoring (see installation instructions for Ops Manager). The monitoring capabilities of MongoDB Atlas (MongoDB’s cloud database service) are very similar. MongoDB also offers a free monitoring service that monitors standalones and replica sets. It keeps the monitoring data for 24 hours after it has been uploaded and pro‐ vides coarse-grained statistics on operation execution times, memory usage, CPU usage, and operation counts.\n\nIf you do not want to use Ops Manager, Atlas, or MongoDB’s free monitoring service, please use some type of monitoring. It will help you detect potential issues before they cause problems and diagnose issues when they occur.\n\nMonitoring Memory Usage Accessing data in memory is fast, and accessing data on disk is slow. Unfortunately, memory is expensive (and disk is cheap), and typically MongoDB uses up memory before any other resource. This section covers how to monitor MongoDB’s interac‐ tions with the CPU, disk, and memory, and what to watch for.\n\n425",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "Introduction to Computer Memory Computers tend to have a small amount of fast-to-access memory and a large amount of slow-to-access disk. When you request a page of data that is stored on disk (and not yet in memory), your system page faults and copies the page from disk into mem‐ ory. It can then access the page in memory extremely quickly. If your program stops regularly using the page and your memory fills up with other pages, the old page will be evicted from memory and only live on disk again.\n\nCopying a page from disk into memory takes a lot longer than reading a page from memory. Thus, the less MongoDB has to copy data from disk, the better. If MongoDB can operate almost entirely in memory, it will be able to access data much faster. Thus, MongoDB’s memory usage is one of the most important stats to track.\n\nTracking Memory Usage MongoDB reports on three “types” of memory in Ops Manager: resident memory, virtual memory, and mapped memory. Resident memory is the memory that Mon‐ goDB explicitly owns in RAM. For example, if you query for a document and it is paged into memory, that page is added to MongoDB’s resident memory.\n\nMongoDB is given an address for that page. This address isn’t the literal address of the page in RAM; it’s a virtual address. MongoDB can pass it to the kernel and the kernel will look up where the page really lives. This way, if the kernel needs to evict the page from memory, MongoDB can still use the address to access it. MongoDB will request the memory from the kernel, the kernel will look at its page cache, see that the page is not there, page fault to copy the page into memory, and return it to MongoDB.\n\nIf your data fits entirely in memory, the resident memory should be approximately the size of your data. When we talk about data being “in memory,” we’re always talk‐ ing about the data being in RAM.\n\nMongoDB’s mapped memory includes all of the data MongoDB has ever accessed (all the pages of data it has addresses for). It will usually be about the size of your dataset.\n\nVirtual memory is an abstraction provided by the operating system that hides the physical storage details from the software process. Each process sees a contiguous address space of memory that it can use. In Ops Manager, the virtual memory use of MongoDB is typically twice the size of the mapped memory.\n\nFigure 22-1 shows the Ops Manager graph for memory information, which describes how much virtual, resident, and mapped memory MongoDB is using. Mapped mem‐ ory is relevant only for older (pre-4.0) deployments using the MMAP storage engine. Now that MongoDB uses the WiredTiger storage engine, you should see zero usage for mapped memory. On a machine dedicated to MongoDB, resident memory should\n\n426\n\n|\n\nChapter 22: Monitoring MongoDB",
      "content_length": 2778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "be a little less than the total memory size (assuming your working set is as large or larger than memory). Resident memory is the statistic that actually tracks how much data is in physical RAM, but by itself this does not tell you much about how MongoDB is using memory.\n\nFigure 22-1. From the top line to the bottom: virtual, resident, and mapped memory\n\nIf your data fits entirely in memory, resident should be approximately the size of your data. When we talk about data being “in memory,” we’re always talking about the data being in RAM.\n\nAs you can see from Figure 22-1, memory metrics tend to be fairly steady, but as your dataset grows virtual memory (top line) will grow with it. Resident memory (middle line) will grow to the size of your available RAM and then hold steady.\n\nTracking Page Faults You can use other statistics to find out how MongoDB is using memory, not just how much of each type it has. One useful stat is the number of page faults, which tells you how often the data MongoDB is looking for is not in RAM. Figures 22-2 and 22-3 are graphs that show page faults over time. Figure 22-3 is page faulting less than Figure 22-2, but by itself this information is not very useful. If the disk in Figure 22-2 can handle that many faults and the application can handle the delay of the disk seeks, there is no particular problem with having so many faults (or more). On the other hand, if your application cannot handle the increased latency of reading data from disk, you have no choice but to store all of your data in memory (or use SSDs).\n\nMonitoring Memory Usage\n\n|\n\n427",
      "content_length": 1597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "Figure 22-2. A system that is page faulting hundreds of times a minute\n\nFigure 22-3. A system that is page faulting a few times a minute\n\nRegardless of how forgiving the application is, page faults become a problem when the disk is overloaded. The amount of load a disk can handle isn’t linear: once a disk begins getting overloaded, each operation must queue for a longer and longer period of time, creating a chain reaction. There is usually a tipping point where disk perfor‐ mance begins degrading quickly. Thus, it is a good idea to stay away from the maxi‐ mum load that your disk can handle.\n\nTrack your page fault numbers over time. If your application is behaving well with a certain number of page faults, you have a baseline for how many page faults the system can handle. If page faults begin to creep up and performance deteriorates, you have a threshold to alert on.\n\nYou can see page fault stats per database by looking at the \"page_faults\" field of serverStatus’s output:\n\n> db.adminCommand({\"serverStatus\": 1})[\"extra_info\"] { \"note\" : \"fields vary by platform\", \"page_faults\" : 50 }\n\n428\n\n|\n\nChapter 22: Monitoring MongoDB",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "\"page_faults\" gives you a count of how many times MongoDB has had to go to disk (since startup).\n\nI/O Wait Page faults in general are closely tied to how long the CPU is idling waiting for the disk, called I/O wait. Some I/O wait is normal; MongoDB has to go to disk some‐ times, and although it tries not to block anything when it does, it cannot completely avoid it. The important thing is that I/O wait is not increasing or near 100%, as shown in Figure 22-4. This indicates that the disk is getting overloaded.\n\nFigure 22-4. I/O wait hovering around 100%\n\nCalculating the Working Set In general, the more data you have in memory, the faster MongoDB will perform. Thus, in order from fastest to slowest, an application could have:\n\n1. The entire dataset in memory. This is nice to have but is often too expensive or infeasible. It may be necessary for applications that depend on fast response times.\n\n2. The working set in memory. This is the most common choice.\n\nYour working set is the data and indexes that your application uses. This may be everything, but generally there’s a core dataset (e.g., the users collection and the last month of activity) that covers 90% of requests. If this working set fits in RAM, MongoDB will generally be fast: it only has to go to disk for a few “unusual” requests.\n\n3. The indexes in memory.\n\n4. The working set of indexes in memory.\n\n5. No useful subset of data in memory. If possible, avoid this. It will be slow.\n\nCalculating the Working Set\n\n|\n\n429",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "You must know what your working set is (and how large it is) to know if you can keep it in memory. The best way to calculate the size of the working set is to track common operations to find out how much your application is reading and writing. For exam‐ ple, suppose your application creates 2 GB of new data per week and 800 MB of that data is regularly accessed. Users tend to access data up to a month old, and data that’s older than that is mostly unused. Your working set size is probably about 3.2 GB (800 MB/week × 4 weeks), plus a fudge factor for indexes, so call it 5 GB.\n\nOne way to think about this is to track data accessed over time, as shown in Figure 22-5. If you choose a cutoff where 90% of your requests fall, like in Figure 22-6, then the data (and indexes) generated in that period of time form your working set. You can measure for that amount of time to figure out how much your dataset grows. Note that this example uses time, but it’s possible that there’s another access pattern that makes more sense for your application (time being the most com‐ mon one).\n\nFigure 22-5. A plot of data accesses by age of data\n\nFigure 22-6. The working set is data used in the requests before the cutoff of “frequent requests” (indicated by the vertical line in the graph)\n\n430\n\n|\n\nChapter 22: Monitoring MongoDB",
      "content_length": 1323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "Some Working Set Examples Suppose that you have a 40 GB working set. A total of 90% of requests hit the work‐ ing set, and 10% hit other data. If you have 500 GB of data and 50 GB of RAM, your working set fits entirely in RAM. Once your application has accessed the data it usu‐ ally accesses (a process called preheating), it should never have to go to disk again for the working set. It then has 10 GB of space available for the 460 GB of less-frequently- accessed data. Obviously, MongoDB will almost always have to go to disk for the non‐ working set data.\n\nOn the other hand, suppose your working set does not fit in RAM—say, if you have only 35 GB of RAM. Then the working set will generally take up most of the RAM. The working set has a higher probability of staying in RAM because it’s accessed more frequently, but at some point the less-frequently-accessed data will have to be paged in, evicting the working set (or other less-frequently-accessed data). Thus, there is a constant churn back and forth from disk: accessing the working set does not have predictable performance anymore.\n\nTracking Performance Performance of queries is often important to track and keep consistent. There are several ways to track if MongoDB is having trouble with the current request load.\n\nCPU can be I/O bound with MongoDB (indicated by a high I/O wait). The Wire‐ dTiger storage engine is multithreaded and can take advantage of additional CPU cores. This can be seen in a higher level of usage across CPU metrics when compared with the older MMAP storage engine. However, if user or system time is approaching 100% (or 100% multiplied by the number of CPUs you have), the most common cause is that you’re missing an index on a frequently used query. It is a good idea to track CPU usage (particularly after deploying a new version of your application) to ensure that all your queries are behaving as they should.\n\nNote that the graph shown in Figure 22-7 is fine: if there is a low number of page faults, I/O wait may be dwarfed by other CPU activities. It is only when the other activities creep up that bad indexes may be a culprit.\n\nTracking Performance\n\n|\n\n431",
      "content_length": 2161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "Figure 22-7. A CPU with minimal I/O wait: the top line is user and the lower line is system; the other stats are very close to 0%\n\nA similar metric is queuing: how many requests are waiting to be processed by Mon‐ goDB. A request is considered queued when it is waiting for the lock it needs to do a read or a write. Figure 22-8 shows a graph of read and write queues over time. No queues are preferred (basically an empty graph), but this graph is nothing to be alarmed about. In a busy system, it isn’t unusual for an operation to have to wait a bit for the correct lock to be available.\n\nFigure 22-8. Read and write queues over time\n\nThe WiredTiger storage engine provides document-level concurrency, which allows for multiple simultaneous writes to the same collection. This has drastically improved the performance of concurrent operations. The ticketing system used controls the number of threads in use to avoid starvation: it issues tickets for read and write oper‐ ations (128 of each, by default), after which point new read or write operations will queue. The wiredTiger.concurrentTransactions.read.available and wired Tiger.concurrentTransactions.write.available fields of serverStatus can be used to track when the number of available tickets reaches zero, indicating the respec‐ tive operations are now queuing up.\n\n432\n\n|\n\nChapter 22: Monitoring MongoDB",
      "content_length": 1368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "You can see if requests are piling up by looking at the number of requests enqueued. Generally, the queue size should be low. A large and ever-present queue is an indica‐ tion that mongod cannot keep up with its load. You should decrease the load on that server as fast as possible.\n\nTracking Free Space One other metric that is basic but important to monitor is disk usage. Sometimes users wait until their disk runs out of space before they think about how they want to handle it. By monitoring your disk usage and tracking free disk space, you can pre‐ dict how long your current drive will be sufficient and plan in advance what to do when it is not.\n\nAs you run out of space, there are several options:\n\nIf you are using sharding, add another shard.\n\nIf you have unused indexes, remove them. These can be identified using the aggregation $indexStats for a specific collection.\n\nIf you have not run a compaction operation, then do so on a secondary to see if it assists. This is normally only useful in cases where a large amount of data or indexes have been removed from a collection and will not be replaced.\n\nShut down each member of the replica set (one at a time) and copy its data to a larger disk, which can then be mounted. Restart the member and proceed to the next.\n\nReplace members of your replica set with members with a larger drive: remove an old member and add a new member, and allow that one to catch up with the rest of the set. Repeat for each member of the set.\n\nIf you are using the directoryperdb option and you have a particularly fast- growing database, move it to its own drive. Then mount the volume as a direc‐ tory in your data directory. This way the rest of your data doesn’t have to be moved.\n\nRegardless of the technique you choose, plan ahead to minimize the impact on your application. You need time to take backups, modify each member of your set in turn, and copy your data from place to place.\n\nMonitoring Replication Replication lag and oplog length are important metrics to track. Lag is when the sec‐ ondaries cannot keep up with the primary. It’s calculated by subtracting the time of the last op applied on a secondary from the time of the last op on the primary. For example, if a secondary just applied an op with the timestamp 3:26:00 p.m. and the\n\nTracking Free Space\n\n|\n\n433",
      "content_length": 2325,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "primary just applied an op with the timestamp 3:29:45 p.m., the secondary is lagging by 3 minutes and 45 seconds. You want lag to be as close to 0 as possible, and it is generally on the order of milliseconds. If a secondary is keeping up with the primary, the replication lag should look something like the graph shown in Figure 22-9: basi‐ cally 0 all the time.\n\nFigure 22-9. A replica set with no lag; this is what you want to see\n\nIf a secondary cannot replicate writes as fast as the primary can write, you’ll start see‐ ing a nonzero lag. The most extreme case of this is when replication is stuck: the sec‐ ondary cannot apply any more operations for some reason. At this point, lag will grow by one second per second, creating the steep slope shown in Figure 22-10. This could be caused by network issues or a missing \"_id\" index, which is required on every collection for replication to function properly.\n\nIf a collection is missing an \"_id\" index, take the server out of the replica set, start it as a standalone server, and build the \"_id\" index. Make sure you create the \"_id\" index as a unique index. Once created, the \"_id\" index cannot be dropped or changed (other than by dropping the whole collection).\n\nIf a system is overloaded, a secondary may gradually fall behind. Some replication will still be happening, so you generally won’t see the characteristic “one second per second” slope in the graph. Still, it’s important to be aware if the secondaries cannot keep up with peak traffic or are gradually falling further behind.\n\n434\n\n|\n\nChapter 22: Monitoring MongoDB",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "Figure 22-10. Replication getting stuck and, just before February 10, beginning to recover; the vertical lines are server restarts\n\nPrimaries do not throttle writes to “help” secondaries catch up, so it’s common for secondaries to fall behind on overloaded systems (particularly as MongoDB tends to prioritize writes over reads, which means replication can be starved on the primary). You can force throttling of the primary to some extent by using \"w\" with your write concern. You also might want to try removing load from the secondary by routing any requests it was handling to another member.\n\nIf you are on an extremely underloaded system, you may see another interesting pat‐ tern: sudden spikes in replication lag, as shown in Figure 22-11. The spikes shown are not actually lag—they are caused by variations in sampling. The mongod is processing one write every couple of minutes. Because lag is measured as the difference between timestamps on the primary and secondary, measuring the timestamp of the secon‐ dary right before a write on the primary makes it look minutes behind. If you increase the write rate, these spikes should disappear.\n\nFigure 22-11. A low-write system can cause “phantom” lag\n\nThe other important replication metric to track is the length of each member’s oplog. Every member that might become primary should have an oplog longer than a day. If\n\nMonitoring Replication\n\n|\n\n435",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "a member may be a sync source for another member, it should have an oplog longer than the time an initial sync takes to complete. Figure 22-12 shows what a standard oplog-length graph looks like. This oplog has an excellent length: 1,111 hours is over a month of data! In general, oplogs should be as long as you can afford the disk space to make them. Given the way they’re used, they take up basically no memory, and a long oplog can mean the difference between a painful ops experience and an easy one.\n\nFigure 22-12. A typical oplog-length graph\n\nFigure 22-13 shows a slightly unusual variation caused by a fairly short oplog and variable traffic. This is still healthy, but the oplog on this machine is probably too short (between 6 and 11 hours of maintenance). The administrator may want to make the oplog longer when they get a chance.\n\nFigure 22-13. Oplog-length graph of an application with daily traffic peaks\n\n436\n\n|\n\nChapter 22: Monitoring MongoDB",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "CHAPTER 23 Making Backups\n\nIt is important to make regular backups of your system. Backups are good protection against most types of failure, and very little can’t be solved by restoring from a clean backup. This chapter covers the common options for making backups:\n\nSingle-server backups, including snapshot backup and restore procedure\n\nSpecial considerations for backing up replica sets\n\nBaking up a sharded cluster\n\nBackups are only useful if you are confident about deploying them in an emergency. Thus, for any backup technique you choose, be sure to practice both making backups and restoring from them until you are comfortable with the restore procedure.\n\nBackup Methods There are a number of options for backing up clusters in MongoDB. MongoDB Atlas, the official MongoDB cloud service, provides both continuous backups and cloud provider snapshots. Continuous backups take incremental backups of data in your cluster, ensuring your backups are typically just a few seconds behind the operating system. Cloud provider snapshots provide localized backup storage using the snap‐ shot functionality of the cluster’s cloud service provider (e.g., Amazon Web Services, Microsoft Azure, or Google Cloud Platform). The best backup solution for the major‐ ity of scenarios is continuous backups.\n\nMongoDB also provides backup capability through Cloud Manager and Ops Man‐ ager. Cloud Manager is a hosted backup, monitoring, and automation service for MongoDB. Ops Manager is an on-premise solution that has similar functionality to Cloud Manager.\n\n437",
      "content_length": 1554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "For individuals and teams managing MongoDB clusters directly, there are several backup strategies. We will outline these strategies in the rest of this chapter.\n\nBacking Up a Server There are a variety of ways to create backups. Regardless of the method, making a backup can cause strain on a system: it generally requires reading all your data into memory. Thus, backups should generally be done on replica set secondaries (as opposed to the primary) or, for standalone servers, at an off time.\n\nThe techniques in this section apply to any mongod, whether a standalone server or a member of a replica set, unless otherwise noted.\n\nFilesystem Snapshot Filesystem snapshots use system-level tools to create copies of the device that holds MongoDB’s data files. These methods complete quickly and work reliably, but require additional system configuration outside of MongoDB.\n\nMongoDB 3.2 added support for volume-level backup of MongoDB instances using the WiredTiger storage engine when those instances’ data files and journal files reside on separate volumes. However, to create a coherent backup, the database must be locked and all writes to the database must be suspended during the backup process.\n\nPrior to MongoDB 3.2, creating volume-level backups of MongoDB instances using WiredTiger required that the data files and journal reside on the same volume.\n\nSnapshots work by creating pointers between the live data and a special snapshot vol‐ ume. These pointers are theoretically equivalent to “hard links.” As the working data diverges from the snapshot, the snapshot process uses a copy-on-write strategy. As a result, the snapshot only stores modified data.\n\nAfter making the snapshot, you mount the snapshot image on your filesystem and copy data from the snapshot. The resulting backup contains a full copy of all data.\n\nThe database must be valid when the snapshot takes place. This means that all writes accepted by the database need to be fully written to disk: either to the journal or to data files. If there are writes that are not on disk when the backup occurs, the backup will not reflect these changes.\n\nFor the WiredTiger storage engine, the data files reflect a consistent state as of the last checkpoint. Checkpoints occur every minute.\n\nSnapshots create an image of an entire disk or volume. Unless you need to back up your entire system, consider isolating your MongoDB data files, journal (if applica‐ ble), and configuration on one logical disk that doesn’t contain any other data.\n\n438\n\n|\n\nChapter 23: Making Backups",
      "content_length": 2546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "Alternatively, store all MongoDB data files on a dedicated device so that you can make backups without duplicating extraneous data.\n\nEnsure that you copy data from snapshots onto other systems. This ensures that data is safe from site failures.\n\nIf your mongod instance has journaling enabled, then you can use any kind of filesys‐ tem or volume/block-level snapshot tool to create backups.\n\nIf you manage your own infrastructure on a Linux-based system, configure your sys‐ tem using the Linux Logical Volume Manager (LVM) to provide your disk packages and provide snapshot capability. LVM allows for the flexible combination and divi‐ sion of physical disk partitions, enabling dynamically resizable filesystems. You can also use LVM-based setups within a cloud/virtualized environment.\n\nIn the initial setup of LVM, first we assign disk partitions to physical volumes (pvcreate), then one or more of these are then assigned to a volume group (vgcreate), and then we create logical volumes (lvcreate) referring to the volume groups. We can build a filesystem on the logical volume (mkfs), which when created can be mounted for use (mount).\n\nSnapshot backup and restore procedure\n\nThis section provides an overview of a simple backup process using LVM on a Linux system. While the tools, commands, and paths may be (slightly) different on your sys‐ tem, the following steps provide a high-level overview of the backup operation.\n\nOnly use the following procedure as a guideline for a backup system and infrastruc‐ ture. Production backup systems must consider a number of application-specific requirements and factors unique to specific environments.\n\nTo create a snapshot with LVM, issue a command as root in the following format:\n\n# lvcreate --size 100M --snapshot --name mdb-snap01 /dev/vg0/mongodb\n\nThis command creates an LVM snapshot (with the --snapshot option) named mdb- snap01 of the mongodb volume in the vg0 volume group, which will be located at /dev/vg0/mdb-snap01. The location and paths to your systems, volume groups, and devices may vary slightly depending on your operating system’s LVM configuration.\n\nThe snapshot has a cap of 100 MB, because of the parameter --size 100M. This size does not reflect the total amount of the data on the disk, but rather the amount of differences between the current state of /dev/vg0/mongodb and the snapshot (/dev/vg0/mdb-snap01).\n\nBacking Up a Server\n\n|\n\n439",
      "content_length": 2415,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "The snapshot will exist when the command returns. You can restore directly from the snapshot at any time, or create a new logical volume and restore from the snapshot to the alternate image.\n\nWhile snapshots are great for creating high-quality backups quickly, they are not ideal as a format for storing backup data. Snapshots typically depend and reside on the same storage infrastructure as the original disk images. Therefore, it’s crucial that you archive these snapshots and store them elsewhere.\n\nAfter creating a snapshot, mount the snapshot and copy the data to separate storage. Alternatively, take a block-level copy of the snapshot image, such as with the follow‐ ing procedure:\n\n# umount /dev/vg0/mdb-snap01\n\n# dd if=/dev/vg0/mdb-snap01 | gzip > mdb-snap01.gz\n\nThis command sequence does the following:\n\nEnsures that the /dev/vg0/mdb-snap01 device is not mounted • Performs a block-level copy of the entire snapshot image using the dd command and compresses the result in a gzipped file in the current working directory\n\nThe dd command will create a large .gz file in your current working directory. Make sure that you run this command in a filesystem that has enough free space.\n\nTo restore a snapshot created with LVM, issue the following sequence of commands:\n\n# lvcreate --size 1G --name mdb-new vg0\n\n# gzip -d -c mdb-snap01.gz | dd of=/dev/vg0/mdb-new\n\n# mount /dev/vg0/mdb-new /srv/mongodb\n\nThis sequence does the following:\n\nCreates a new logical volume named mdb-new, in the /dev/vg0 volume group. The path to the new device will be /dev/vg0/mdb-new. You can use a different name, and change 1G to your desired volume size.\n\nUncompresses and unarchives the mdb-snap01.gz file into the mdb-new disk image.\n\n440\n\n|\n\nChapter 23: Making Backups",
      "content_length": 1760,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "Mounts the mdb-new disk image to the /srv/mongodb directory. Modify the mount point to correspond to your MongoDB data file location or other location as needed.\n\nThe restored snapshot will have a stale mongod.lock file. If you do not remove this file from the snapshot, MongoDB may assume that the stale lock file indicates an unclean shutdown. If you’re running with storage.journal.enabled enabled and you do not use db.fsyncLock(), you do not need to remove the mongod.lock file. If you use db.fsyncLock() you will need to remove the lock.\n\nTo restore a backup without writing to a compressed .gz file, use the following sequence of commands:\n\n# umount /dev/vg0/mdb-snap01\n\n# lvcreate --size 1G --name mdb-new vg0\n\n# dd if=/dev/vg0/mdb-snap01 of=/dev/vg0/mdb-new\n\n# mount /dev/vg0/mdb-new /srv/mongodb\n\nYou can implement off-system backups using the combined process and SSH. This sequence is identical to procedures explained previously, except that it archives and compresses the backup on a remote system using SSH:\n\numount /dev/vg0/mdb-snap01\n\ndd if=/dev/vg0/mdb-snap01 | ssh username@example.com gzip > /opt/backup/ mdb-snap01.gz\n\nlvcreate --size 1G --name mdb-new vg0\n\nssh username@example.com gzip -d -c /opt/backup/mdb-snap01.gz | dd of=/dev/vg0/mdb-new\n\nmount /dev/vg0/mdb-new /srv/mongodb\n\nStarting in MongoDB 3.2, for the purpose of volume-level backup of MongoDB instances using WiredTiger, the data files and the journal are no longer required to reside on a single volume. However, the database must be locked and all writes to the database must be suspended during the backup process to ensure the consistency of the backup.\n\nIf your mongod instance is either running without journaling or has the journal files on a separate volume, you must flush all writes to disk and lock the database to pre‐ vent writes during the backup process. If you have a replica set configuration, then for your backup use a secondary that is not receiving reads (i.e., a hidden member).\n\nBacking Up a Server\n\n|\n\n441",
      "content_length": 2015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "To do this, issue the db.fsyncLock() method in the mongo shell:\n\n> db.fsyncLock();\n\nThen perform the backup operation described previously.\n\nAfter the snapshot completes, unlock the database by issuing the following command in the mongo shell:\n\n> db.fsyncUnlock();\n\nThis process is described more fully in the following section.\n\nCopying Data Files Another way of creating single-server backups is to make a copy of everything in the data directory. Because you cannot copy all of the files at the same moment without filesystem support, you must prevent the data files from changing while you are mak‐ ing the copy. This can be accomplished with a command called fsyncLock:\n\n> db.fsyncLock()\n\nThis command locks the database against any further writes and then flushes all dirty data to disk (fsync), ensuring that the files in the data directory have the latest con‐ sistent information and are not changing.\n\nOnce this command has been run, mongod will enqueue all incoming writes. It will not process any further writes until it has been unlocked. Note that this command stops writes to all databases (not just the one db is connected to).\n\nOnce the fsyncLock command returns, copy all of the files in your data directory to a backup location. On Linux, this can be done with a command such as:\n\n$ cp -R /data/db/* /mnt/external-drive/backup\n\nMake sure that you copy absolutely every file and folder from the data directory to the backup location. Excluding files or directories may make the backup unusable or corrupt.\n\nOnce you have finished copying the data, unlock the database to allow it to take writes again:\n\n> db.fsyncUnlock()\n\nYour database will begin handling writes again normally.\n\nNote that there are some locking issues with authentication and fsyncLock. If you are using authentication, do not close the shell between calling fsyncLock and fsyncUnlock. If you disconnect, you may be unable to reconnect and have to restart mongod. The fsyncLock setting does not persist between restarts; mongod will always start up unlocked.\n\n442\n\n|\n\nChapter 23: Making Backups",
      "content_length": 2081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "As an alternative to fsyncLock, you can instead shut down mongod, copy the files, and then start mongod back up again. Shutting down mongod effectively flushes all changes to disk and prevents new writes from occurring during the backup.\n\nTo restore from the copy of the data directory, ensure that mongod is not running and that the data directory you want to restore into is empty. Copy the backed-up data files to the data directory, and then start mongod. For example, the following com‐ mand would restore the files backed up with the command shown earlier:\n\n$ cp -R /mnt/external-drive/backup/* /data/db/ $ mongod -f mongod.conf\n\nDespite the warnings about partial data directory copies, you can use this method to back up individual databases if you know what to copy and where they are using the --directoryperdb option. To back up an individual database (called, say, myDB), which is only available if you are using the --directoryperdb option, copy the entire myDB directory. Partial data directory copies are only possible with the -- directoryperdb option.\n\nYou can restore specific databases by copying just the files with the correct database name into your data directory. You must be starting from a clean shutdown to restore piecemeal like this. If you had a crash or a hard shutdown, do not attempt to restore a single database from the backup: replace the entire directory and start the mongod to allow the journal files to be replayed.\n\nNever use fsyncLock in conjunction with mongodump (described next). Depending on what else your database is doing, mongodump may hang forever if the database is locked.\n\nUsing mongodump The final way of making a single-server backup is to use mongodump. mongodump is mentioned last because it has some downsides. It is slower (both to get the backup and to restore from it) and it has some issues with replica sets, which are discussed in “Specific Considerations for Replica Sets” on page 446. However, it also has some benefits: it is a good way to back up individual databases, collections, and even sub‐ sets of collections.\n\nmongodump has a variety of options that you can see by running mongodump --help. Here, we will focus on the most useful ones to use for backing up.\n\nTo back up all databases, simply run mongodump. If you are running mongodump on the same machine as the mongod, you can simply specify the port mongod is running on:\n\nBacking Up a Server\n\n|\n\n443",
      "content_length": 2430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "$ mongodump -p 31000\n\nmongodump will create a dump directory in the current directory, which contains a dump of all your data. This dump directory is organized by database and by collection into folders and subfolders. The actual data is stored in .bson files, which merely con‐ tain every document in a collection in BSON, concatenated together. You can exam‐ ine .bson files using the bsondump tool, which comes with MongoDB.\n\nYou do not even need to have a server running to use mongodump. You can use the --dbpath option to specify your data directory, and mongodump will use the data files to copy data:\n\n$ mongodump --dbpath /data/db\n\nYou should not use --dbpath if mongod is running.\n\nOne issue with mongodump is that it is not an instantaneous backup: the system may be taking writes while the backup occurs. Thus, you might end up with a situation where user A begins a backup that causes mongodump to dump the database A, but while this is happening user B drops A. However, mongodump has already dumped it, so you’ll end up with a snapshot of the data that is inconsistent with the state on the original server.\n\nTo avoid this, if you are running mongod with --replSet, you can use mongodump’s --oplog option. This will keep track of all operations that occur on the server while the dump is taking place, so these operations can be replayed when the backup is restored. This gives you a consistent point-in-time snapshot of data from the source server.\n\nIf you pass mongodump a replica set connection string (e.g., \"setName/ seed1,seed2,seed3\"), it will automatically select the primary to dump from. If you want to use a secondary, you can specify a read preference. The read preference can be specified by --uri connection string, by the uri readPreferenceTags option, or by the --readPreference command-line option. For more details on the various settings and options, please see the mongodump MongoDB documentation page.\n\nTo restore from a mongodump backup, use the mongorestore tool:\n\n$ mongorestore -p 31000 --oplogReplay dump/\n\nIf you used the --oplog option to dump the database, you must use the --oplogReplay option with mongorestore to get the point-in-time snapshot.\n\nIf you are replacing data on a running server, you may (or may not) wish to use the --drop option, which drops a collection before restoring it.\n\n444\n\n|\n\nChapter 23: Making Backups",
      "content_length": 2373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "The behavior of mongodump and mongorestore has changed over time. To prevent compatibility issues, try to use the same version of both utilities (you can see their versions by running mongodump --version and mongorestore --version).\n\nFrom MongoDB version 4.2 and up, you cannot use either mongo‐ dump or mongorestore as a strategy for backing up a sharded clus‐ ter. These tools do not maintain the atomicity guarantees of transactions across shards.\n\nMoving collections and databases with mongodump and mongorestore\n\nYou can restore into an entirely different database and collection than you dumped from. This can be useful if different environments use different database names (say, dev and prod) but the same collection names.\n\nTo restore a .bson file into a specific database and collection, specify the targets on the command line:\n\n$ mongorestore --db newDb --collection someOtherColl dump/oldDB/oldColl.bson\n\nIt is also possible to use these tools with SSH to perform data migration without any disk I/O using the archive feature of these tools. This simplifies three stages into one operation, when previously you had to back up to disk, then copy those backup files to a target server, and then run mongorestore on that server to restore the backups:\n\n$ ssh eoin@proxy.server.com mongodump --host source.server.com\\ --archive | ssh eoin@target.server.com mongorestore --archive\n\nCompression can be combined with the archive feature of these tools to further reduce the size of the information sent while performing a data migration. Here is the same SSH data migration example using both the archive and compression features of these tools:\n\n$ ssh eoin@proxy.server.com mongodump --host source.server.com\\ --archive --gzip | ssh eoin@target.server.com mongorestore --archive --gzip\n\nAdministrative complications with unique indexes\n\nIf you have a unique index (other than \"_id\") on any of your collections, you should consider using a different type of backup than mongodump/mongorestore. Unique indexes require that the data does not change in ways that would violate the unique index constraint during the copy. The safest way to ensure this is to choose a method that “freezes” the data, then make a backup as described in either of the previous two sections.\n\nIf you are determined to use mongodump/mongorestore, you may need to preprocess your data when you restore from a backup.\n\nBacking Up a Server\n\n|\n\n445",
      "content_length": 2425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "Specific Considerations for Replica Sets The main additional consideration when backing up a replica set is that as well as the data, you must also capture the state of the replica set to ensure an accurate point-in- time snapshot of your deployment is made.\n\nGenerally, you should make backups from a secondary: this keeps load off of the pri‐ mary, and you can lock a secondary without affecting your application (so long as your application isn’t sending it read requests). You can use any of the three methods outlined previously to back up a replica set member, but a filesystem snapshot or data file copy is recommended. Either of these techniques can be applied to replica set sec‐ ondaries with no modification.\n\nmongodump is not quite as simple to use when replication is enabled. First, if you are using mongodump, you must take your backups using the --oplog option to get a point-in-time snapshot; otherwise the backup’s state won’t match the state of any other members in the cluster. You must also create an oplog when you restore from a mongodump backup, or the restored member will not know where it was synced to.\n\nTo restore a replica set member from a mongodump backup, start the target replica set member as a standalone server with an empty data directory and run mongorestore on it (as described in the previous section) with the --oplogReplay option. Now it should have a complete copy of the data, but it still needs an oplog. Create an oplog using the createCollection command:\n\n> use local > db.createCollection(\"oplog.rs\", {\"capped\" : true, \"size\" : 10000000})\n\nSpecify the size of the collection in bytes. See “Resizing the Oplog” on page 282 for advice on oplog sizing.\n\nNow you need to populate the oplog. The easiest way to do this is to restore the oplog.bson backup file from the dump into the local.oplog.rs collection:\n\n$ mongorestore -d local -c oplog.rs dump/oplog.bson\n\nNote that this is not a dump of the oplog itself (dump/local/oplog.rs.bson), but rather of the oplog operations that occurred during the dump. Once this mongorestore is complete, you can restart this server as a replica set member.\n\nSpecific Considerations for Sharded Clusters The main additional consideration when backing up a sharded cluster using the approaches in this chapter is that you can only back up the pieces when they are active, and sharded clusters are impossible to “perfectly” back up while active: you can’t get a snapshot of the entire state of the cluster at a point in time. However, this limitation is generally sidestepped by the fact that as your cluster gets bigger, it\n\n446\n\n|\n\nChapter 23: Making Backups",
      "content_length": 2640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 469,
      "content": "becomes less and less likely that you’d ever have to restore the whole thing from a backup. Thus, when dealing with a sharded cluster, we focus on backing up pieces: the config servers and the replica sets individually. If you need the ability to back up the whole cluster to a particular point in time or would prefer an automated solution, you can avail yourself of MongoDB’s Cloud Manager or Atlas backup feature.\n\nTurn off the balancer before performing any of these operations on a sharded cluster (either backup or restore). You cannot get a consistent snapshot of the world with chunks flying around. See “Balancing Data” on page 359 for instructions on turning the balancer on and off.\n\nBacking Up and Restoring an Entire Cluster When a cluster is very small or in development, you may want to actually dump and restore the entire thing. You can accomplish this by turning off the balancer and then running mongodump through the mongos. This creates a backup of all of the shards on whatever machine mongodump is running on.\n\nTo restore from this type of backup, run mongorestore connected to a mongos.\n\nAlternatively, after turning off the balancer you can take filesystem or data directory backups of each shard and the config servers. However, you will inevitably get copies from each at slightly different times, which may or may not be a problem. Also, as soon as you turn on the balancer and a migrate occurs, some of the data you backed up from one shard will no longer be there.\n\nBacking Up and Restoring a Single Shard Most often, you’ll only need to restore a single shard in a cluster. If you are not too picky, you can restore from a backup of that shard using one of the single-server methods just described.\n\nThere is one important issue to be aware of, however. Suppose you make a backup of your cluster on Monday. On Thursday, your disk melts down and you have to restore from the backup. In the intervening days, new chunks may have moved to this shard. Your backup of the shard from Monday will not contain these new chunks. You may be able to use a config server backup to figure out where the disappearing chunks lived on Monday, but it is a lot more difficult than simply restoring the shard. In most cases, restoring the shard and losing the data in those chunks is the preferable route.\n\nYou can connect directly to a shard to restore from a backup (instead of going through mongos).\n\nSpecific Considerations for Sharded Clusters\n\n|\n\n447",
      "content_length": 2468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "CHAPTER 24 Deploying MongoDB\n\nThis chapter gives recommendations for setting up a server to go into production. In particular, it covers:\n\nChoosing what hardware to buy and how to set it up\n\nUsing virtualized environments\n\nImportant kernel and disk I/O settings\n\nNetwork setup: who needs to connect to whom\n\nDesigning the System You generally want to optimize for data safety and the quickest access you can afford. This section discusses the best way to accomplish these goals when choosing disks, RAID configuration, CPUs, and other hardware and low-level software components.\n\nChoosing a Storage Medium In order of preference, we would like to store and retrieve data from:\n\n1. RAM\n\n2. SSD\n\n3. Spinning disk\n\nUnfortunately, most people have limited budgets or enough data that storing every‐ thing in RAM is impractical and SSDs are too expensive. Thus, the typical deploy‐ ment is a small amount of RAM (relative to total data size) and a lot of space on a\n\n449",
      "content_length": 965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "spinning disk. If you are in this camp, the important thing is that your working set is smaller than RAM, and you should be ready to scale out if the working set gets bigger.\n\nIf you are able to spend what you like on hardware, buy a lot of RAM and/or SSDs.\n\nReading data from RAM takes a few nanoseconds (say, 100). Conversely, reading from disk takes a few milliseconds (say, 10). It can be hard to picture the difference between these two numbers, so let’s scale them up to more relatable numbers: if accessing RAM took 1 second, accessing the disk would take over a day!\n\n100 nanoseconds × 10,000,000 = 1 second\n\n10 milliseconds × 10,000,000 = 1.16 days\n\nThese are very back-of-the-envelope calculations (your disk might be a bit faster or your RAM a bit slower), but the magnitude of this difference doesn’t change much. Thus, we want to access the disk as seldom as possible.\n\nRecommended RAID Configurations RAID is hardware or software that lets you treat multiple disks as though they were a single disk. It can be used for reliability, performance, or both. A set of disks using RAID is referred to as a RAID array (somewhat redundantly, as RAID stands for redundant array of inexpensive disks).\n\nThere are a number of ways to configure RAID, depending on the features you’re looking for—generally some combination of speed and fault tolerance. These are the most common varieties:\n\nRAID0\n\nStriping disks for improved performance. Each disk holds part of the data, simi‐ lar to MongoDB’s sharding. Because there are multiple underlying disks, lots of data can be written to disk at the same time. This improves throughput on writes. However, if a disk fails and data is lost, there are no copies of it. It also can cause slow reads, as some data volumes may be slower than others.\n\nRAID1\n\nMirroring for improved reliability. An identical copy of the data is written to each member of the array. This has lower performance than RAID0, as a single member with a slow disk can slow down all writes. However, if a disk fails, you will still have a copy of the data on another member of the array.\n\nRAID5\n\nStriping disks, plus keeping an extra piece of data about the other data that’s been stored to prevent data loss on server failure. Basically, RAID5 can handle one\n\n450\n\n|\n\nChapter 24: Deploying MongoDB",
      "content_length": 2313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "disk going down and hide that failure from the user. However, it is slower than any of the other varieties listed here because it needs to calculate this extra piece of information whenever data is written. This is particularly expensive with Mon‐ goDB, as a typical workload does many small writes.\n\nRAID10\n\nA combination of RAID0 and RAID1: data is striped for speed and mirrored for reliability.\n\nWe recommend using RAID10: it is safer than RAID0 and can smooth out perfor‐ mance issues that can occur with RAID1. However, some people feel that RAID1 on top of replica sets is overkill and opt for RAID0. It is a matter of personal preference: how much risk are you willing to trade for performance?\n\nDo not use RAID5: it is very, very slow.\n\nCPU MongoDB historically was very light on CPU, but with the use of the WiredTiger storage engine this is no longer the case. The WiredTiger storage engine is multi‐ threaded and can take advantage of additional CPU cores. You should therefore bal‐ ance your investment between memory and CPU.\n\nWhen choosing between speed and number of cores, go with speed. MongoDB is bet‐ ter at taking advantage of more cycles on a single processor than increased parallelization.\n\nOperating System 64-bit Linux is the operating system MongoDB runs best on. If possible, use some fla‐ vor of that. CentOS and Red Hat Enterprise Linux are probably the most popular choices, but any flavor should work (Ubuntu and Amazon Linux are also common). Be sure to use the most recent stable version of the operating system, because old, buggy packages or kernels can sometimes cause issues.\n\n64-bit Windows is also well supported.\n\nOther flavors of Unix are not as well supported: proceed with caution if you’re using Solaris or one of the BSD variants. Builds for these systems have, at least historically, had a lot of issues. MongoDB explicitly stopped supporting Solaris in August 2017, noting a lack of adoption among users.\n\nOne important note on cross-compatibility: MongoDB uses the same wire protocol and lays out data files identically on all systems, so you can deploy on a combination of operating systems. For example, you could have a mongos process running on Windows and the mongods that are its shards running on Linux. You can also copy data files from Windows to Linux or vice versa with no compatibility issues.\n\nDesigning the System\n\n|\n\n451",
      "content_length": 2384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "Since version 3.4, MongoDB no longer supports 32-bit x86 platforms. Do not run any type of MongoDB server on a 32-bit machine.\n\nMongoDB works with little-endian architectures and one big-endian architecture: IBM’s zSeries. Most drivers support both little- and big-endian systems, so you can run clients on either. However, the server will typically be run on a little-endian machine.\n\nSwap Space You should allocate a small amount of swap in case memory limits are reached to pre‐ vent the kernel from killing MongoDB. It doesn’t usually use any swap space, but in extreme circumstances the WiredTiger storage engine might use some. If this occurs, then you should consider increasing the memory capacity of your machine or review‐ ing your workload to avoid this problematic situation for performance and for stability.\n\nThe majority of memory MongoDB uses is “slippery”: it’ll be flushed to disk and replaced with other memory as soon as the system requests the space for something else. Therefore, database data should never be written to swap space: it’ll be flushed back to disk first.\n\nHowever, occasionally MongoDB will use swap for operations that require ordering data: either building indexes or sorting. It attempts not to use too much memory for these types of operations, but by performing many of them at the same time you may be able to force swapping.\n\nIf your application is managing to make MongoDB use swap space, you should look into redesigning the application or reducing load on the swapping server.\n\nFilesystem For Linux, only the XFS filesystem is recommended for your data volumes with the WiredTiger storage engine. It is possible to use the ext4 filesystem with WiredTiger, but be aware there are known performance issues (specifically, that it may stall on WiredTiger checkpoints).\n\nOn Windows, either NTFS or FAT is fine.\n\nDo not use Network File Storage (NFS) directly mounted for Mon‐ goDB storage. Some client versions lie about flushing, randomly remount and flush the page cache, and do not support exclusive file locking. Using NFS can cause journal corruption and should be avoided at all costs.\n\n452\n\n|\n\nChapter 24: Deploying MongoDB",
      "content_length": 2172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "Virtualization Virtualization is a great way to get cheap hardware and be able to expand fast. How‐ ever, there are some downsides—particularly unpredictable network and disk I/O. This section covers virtualization-specific issues.\n\nMemory Overcommitting The memory overcommit Linux kernel setting controls what happens when processes request too much memory from the operating system. Depending on how it’s set, the kernel may give memory to processes even if that memory is not actually available (in the hopes that it’ll become available by the time the process needs it). That’s called overcommitting: the kernel promises memory that isn’t actually there. This operating system kernel setting does not work well with MongoDB.\n\nThe possible values for vm.overcommit_memory are 0 (the kernel guesses about how much to overcommit); 1 (memory allocation always succeeds); or 2 (don’t commit more virtual address space than swap space plus a fraction of the overcommit ratio). The value 2 is complicated, but it’s the best option available. To set this, run:\n\n$ echo 2 > /proc/sys/vm/overcommit_memory\n\nYou do not need to restart MongoDB after changing this operating system setting.\n\nMystery Memory Sometimes the virtualization layer does not handle memory provisioning correctly. Thus, you may have a virtual machine that claims to have 100 GB of RAM available but only ever allows you to access 60 GB of it. Conversely, we’ve seen people that were supposed to have 20 GB of memory end up being able to fit an entire 100 GB dataset into RAM!\n\nAssuming you don’t end up on the lucky side, there isn’t much you can do. If your operating system readahead is set appropriately and your virtual machine just won’t use all the memory it should, you may just have to switch virtual machines.\n\nHandling Network Disk I/O Issues One of the biggest problems with using virtualized hardware is that you are generally sharing a disk with other tenants, which exacerbates the disk slowness mentioned previously because everyone is competing for disk I/O. Thus, virtualized disks can have very unpredictable performance: they can work fine while your neighbors aren’t busy and suddenly slow down to a crawl if someone else starts hammering the disks.\n\nThe other issue is that this storage is often not physically attached to the machine MongoDB is running on, so even when you have a disk all to yourself I/O will be\n\nVirtualization\n\n|\n\n453",
      "content_length": 2426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": "slower than it would be with a local disk. There is also the unlikely-but-possible sce‐ nario of your MongoDB server losing its network connection to your data.\n\nAmazon has what is probably the most widely used networked block store, called Elastic Block Store (EBS). EBS volumes can be connected to Elastic Compute Cloud (EC2) instances, allowing you to give a machine almost any amount of disk immedi‐ ately. If you are using EC2, you should also enable AWS Enhanced Networking if it’s available for the instance type, as well as disable the dynamic voltage and frequency scaling (DVFS) and CPU power-saving modes plus hyperthreading. On the plus side, EBS makes backups very easy (take a snapshot from a secondary, mount the EBS drive on another instance, and start up mongod). On the downside, you may encounter variable performance.\n\nIf you require more predictable performance, there are a couple of options. One is to host MongoDB on your own servers—that way, you know no one else is slowing things down. However, that’s not an option for a lot of people, so the next best thing is to get an instance in the cloud that guarantees a certain number of I/O Operations Per Second (IOPS). See http://docs.mongodb.org for up-to-date recommendations on hosted offerings.\n\nIf you can’t pursue either of these options and you need more disk I/O than an over‐ loaded EBS volume can sustain, there is a way to hack around it. Basically, what you can do is keep monitoring the volume MongoDB is using. If and when that volume slows down, immediately kill that instance and bring up a new one with a different data volume.\n\nThere are a couple of statistics to watch for:\n\nSpiking I/O utilization (“IO wait” on Cloud Manager/Atlas), for obvious reasons.\n\nPage fault rates spiking. Note that changes in application behavior could also cause working set changes: you should disable this assassination script before deploying new versions of your application.\n\nThe number of lost TCP packets going up (Amazon is particularly bad about this: when performance starts to fall, it drops TCP packets all over the place).\n\nMongoDB’s read and write queues spiking (this can be seen in Cloud Manager/ Atlas or in mongostat’s qr/qw column).\n\nIf your load varies over the day or week, make sure your script takes that into account: you don’t want a rogue cron job killing off all of your instances because of an unusually heavy Monday morning rush.\n\nThis hack relies on you having recent backups or relatively quick-to-sync datasets. If you have each instance holding terabytes of data, you might want to pursue an\n\n454\n\n|\n\nChapter 24: Deploying MongoDB",
      "content_length": 2634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "alternative approach. Also, this is only likely to work: if your new volume is also being hammered, it will be just as slow as the old one.\n\nUsing Non-Networked Disks\n\nThis section uses Amazon-specific vocabulary. However, it may apply to other providers.\n\nEphemeral drives are the actual disks attached to the physical machine your VM is running on. They don’t have a lot of the problems networked storage does. Local disks can still be overloaded by other users on the same box, but with a large box you can be reasonably sure you’re not sharing disks with too many others. Even with a smaller instance, often an ephemeral drive will give better performance than a net‐ worked drive so long as the other tenants aren’t doing tons of IOPS.\n\nThe downside is in the name: these disks are ephemeral. If your EC2 instance goes down, there’s no guarantee you’ll end up on the same box when you restart the instance, and then your data will be gone.\n\nThus, ephemeral drives should be used with care. You should make sure that you do not store any important or unreplicated data on these disks. In particular, do not put the journal on these ephemeral drives, or your database on network storage. In gen‐ eral, think of ephemeral drives as a slow cache rather than a fast disk and use them accordingly.\n\nConfiguring System Settings There are several system settings that can help MongoDB run more smoothly, which are mostly related to disk and memory access. This section covers each of these options and how you should tweak them.\n\nTurning Off NUMA When machines had a single CPU, all RAM was basically the same in terms of access time. As machines started to have more processors, engineers realized that having all memory be equally far from each CPU (as shown in Figure 24-1) was less efficient than having each CPU have some memory that is especially close to it and fast for that particular CPU to access (Figure 24-2). This architecture, where each CPU has its own “local” memory, is called nonuniform memory architecture (NUMA).\n\nConfiguring System Settings\n\n|\n\n455",
      "content_length": 2067,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "Figure 24-1. Uniform memory architecture: all memory has the same access cost for each CPU\n\nFigure 24-2. Nonuniform memory architecture: certain memory is attached to a CPU, giving the CPU faster access to that memory; CPUs can still access other CPUs’ memory, but it is more expensive than accessing their own\n\nFor lots of applications, NUMA works well: the processors often need different data because they’re running different programs. However, this works terribly for data‐ bases in general and MongoDB in particular because databases have such different memory access patterns than other types of applications. MongoDB uses a massive amount of memory and needs to be able to access memory that is “local” to other CPUs. However, the default NUMA settings on many systems make this difficult.\n\nCPUs favor using the memory that is attached to them, and processes tend to favor one CPU over the others. This means that memory often fills up unevenly, potentially leaving you with one processor using 100% of its local memory and the other pro‐ cessors using only a fraction of their memory, as shown in Figure 24-3.\n\nFigure 24-3. Sample memory usage in a NUMA system\n\nIn the scenario in Figure 24-3, suppose CPU1 needs some data that isn’t in memory yet. It must use its local memory for data that doesn’t have a “home” yet, but its local memory is full. Thus, it has to evict some of the data in its local memory to make\n\n456\n\n|\n\nChapter 24: Deploying MongoDB",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "room for the new data, even though there’s plenty of space left in the memory attached to CPU2! This process tends to cause MongoDB to run much slower than expected, as it only has a fraction of the memory available that it should have. Mon‐ goDB vastly prefers semiefficient access to more data over extremely efficient access to less data.\n\nWhen running MongoDB servers and clients on NUMA hardware, you should con‐ figure a memory interleave policy so that the host behaves in a non-NUMA fashion. MongoDB checks NUMA settings on startup when deployed on Linux and Windows machines. If the NUMA configuration may degrade performance, MongoDB prints a warning.\n\nOn Windows, memory interleaving must be enabled through the machine’s BIOS. Consult your system documentation for details.\n\nWhen running MongoDB on Linux, you should disable zone reclaim in the sysctl set‐ tings using one of the following commands:\n\necho 0 | sudo tee /proc/sys/vm/zone_reclaim_mode\n\nsudo sysctl -w vm.zone_reclaim_mode=0\n\nThen, you should use numactl to start your mongod instances, including the config servers, mongos instances, and any clients. If you do not have the numactl command, refer to the documentation for your operating system to install the numactl package.\n\nThe following command demonstrates how to start a MongoDB instance using numactl:\n\nnumactl --interleave=all <path> <options>\n\nThe <path> is the path to the program you are starting and the <options> are any optional arguments to pass to the program.\n\nTo fully disable NUMA behavior, you must perform both operations. For more infor‐ mation, see the documentation.\n\nSetting Readahead Readahead is an optimization where the operating system reads more data from disk than was actually requested. This is useful because most workloads that computers handle are sequential: if you load the first 20 MB of a video, you are probably going to want the next couple of megabytes of it. Thus, the system will read more from disk than you actually request and store it in memory, just in case you need it soon.\n\nFor the WiredTiger storage engine, you should set readahead to between 8 and 32 regardless of the storage media type (spinning disk, SSD, etc.). Setting it higher bene‐ fits sequential I/O operations, but since MongoDB disk access patterns are typically random, a higher readahead value provides limited benefit and may even result in\n\nConfiguring System Settings\n\n|\n\n457",
      "content_length": 2426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "performance degradation. For most workloads, a readahead of between 8 and 32 pro‐ vides optimal MongoDB performance.\n\nIn general, you should set the readahead within this range unless testing shows that a higher value is measurably, repeatably, and reliably beneficial. MongoDB Professional Support can provide advice and guidance on nonzero readahead configurations.\n\nDisabling Transparent Huge Pages (THP) THP causes similar issues to high readahead. Do not use this feature unless:\n\nAll of your data fits into memory.\n\nYou have no plans for it to ever grow beyond memory.\n\nMongoDB needs to page in lots of tiny pieces of memory, so using THP can result in more disk I/O.\n\nSystems move data from disk to memory and back by the page. Pages are generally a couple of kilobytes (x86 defaults to 4,096-byte pages). If a machine has many giga‐ bytes of memory, keeping track of each of these (relatively tiny) pages can be slower than just tracking a few larger-granularity pages. THP is a solution that allows you to have pages that are up to 256 MB (on IA-64 architectures). However, using it means that you are keeping megabytes of data from one section of disk in memory. If your data does not fit in RAM, then swapping in larger pieces from disk will just fill up your memory quickly with data that will need to be swapped out again. Also, flushing any changes to disk will be slower, as the disk must write megabytes of “dirty” data, instead of a few kilobytes.\n\nTHP was actually developed to benefit databases, so this may be surprising to experi‐ enced database admins. However, MongoDB tends to do a lot less sequential disk access than relational databases do.\n\nOn Windows these are called Large Pages, not Huge Pages. Some versions of Windows have this feature enabled by default and some do not, so check and make sure it is turned off.\n\nChoosing a Disk Scheduling Algorithm The disk controller receives requests from the operating system and processes them in an order determined by a scheduling algorithm. Sometimes changing this algo‐ rithm can improve disk performance. For other hardware and workloads, it may not make a difference. The best way to decide which algorithm to use is to test them out\n\n458\n\n|\n\nChapter 24: Deploying MongoDB",
      "content_length": 2251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "yourself on your workload. Deadline and completely fair queueing (CFQ) both tend to be good choices.\n\nThere are a couple of situations where the noop scheduler (a contraction of “no-op”) is the best choice. If you’re in a virtualized environment, use the noop scheduler. This scheduler basically passes the operations through to the underlying disk controller as quickly as possible. It is fastest to do this and let the real disk controller handle any reordering that needs to happen.\n\nSimilarly, on SSDs, the noop scheduler is generally the best choice. SSDs don’t have the same locality issues that spinning disks do.\n\nFinally, if you’re using a RAID controller with caching, use noop. The cache behaves like an SSD and will take care of propagating the writes to the disk efficiently.\n\nIf you are on a physical server that is not virtualized, the operating system should use the deadline scheduler. The deadline scheduler caps maximum latency per request and maintains a reasonable disk throughput that is best for disk-intensive database applications.\n\nYou can change the scheduling algorithm by setting the --elevator option in your boot configuration.\n\nThe option is called \"elevator\" because the scheduler behaves like an elevator, picking up people (I/O requests) from different floors (processes/times) and dropping them off where they want to go in an arguabley optimal way.\n\nOften all of the algorithms work pretty well; you may not see much of a difference between them.\n\nDisabling Access Time Tracking By default, the system tracks when files were last accessed. As the data files used by MongoDB are very high-traffic, you can get a performance boost by disabling this tracking. You can do this on Linux by changing atime to noatime in /etc/fstab:\n\n/dev/sda7 /data xfsf rw,noatime 1 2\n\nYou must remount the device for the changes to take effect.\n\natime is more of an issue on older kernels (e.g., ext3); newer ones use relatime as a default, which is less aggressively updated. Also, be aware that setting noatime can affect other programs using the partition, such as mutt or backup tools.\n\nSimilarly, on Windows you should set the disablelastaccess option. To turn off last access time recording, run:\n\nConfiguring System Settings\n\n|\n\n459",
      "content_length": 2255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "C:\\> fsutil behavior set disablelastaccess 1\n\nYou must reboot for this setting to take effect. Setting this may affect the remote stor‐ age service, but you probably shouldn’t be using a service that automatically moves your data to other disks anyway.\n\nModifying Limits There are two limits that MongoDB tends to blow by: the number of threads a pro‐ cess is allowed to spawn and the number of file descriptors a process is allowed to open. Both of these should generally be set to unlimited.\n\nWhenever a MongoDB server accepts a connection, it spawns a thread to handle all activity on that connection. Therefore, if you have 3,000 connections to the database, the database will have 3,000 threads running (plus a few other threads for non-client- related tasks). Depending on your application server configuration, your client may spawn anywhere from a dozen to thousands of connections to MongoDB.\n\nIf your client will dynamically spawn more child processes as traffic increases (most application servers will do this), it is important to make sure that these child pro‐ cesses are not so numerous that they can max out MongoDB’s limits. For example, if you have 20 application servers, each one of which is allowed to spawn 100 child pro‐ cesses, and each child process can spawn 10 threads that all connect to MongoDB, that could result in the spawning of 20 × 100 × 10 = 20,000 connections at peak traf‐ fic. MongoDB is probably not going to be very happy about spawning tens of thou‐ sands of threads and, if you run out of threads per process, will simply start refusing new connections.\n\nThe other limit to modify is the number of file descriptors MongoDB is allowed to open. Every incoming and outgoing connection uses a file descriptor, so the client connection storm just mentioned would create 20,000 open filehandles.\n\nmongos in particular tends to create connections to many shards. When a client con‐ nects to a mongos and makes a request, the mongos opens connections to any and all shards necessary to fulfill that request. Thus, if a cluster has 100 shards and a client connects to a mongos and tries to query for all of its data, the mongos must open 100 connections: one connection to each shard. This can quickly lead to an explosion in the number of connections, as you can imagine from the previous example. Suppose a liberally configured app server made a hundred connections to a mongos process. This could get translated to 100 inbound connections × 100 shards = 10,000 connec‐ tions to shards! (This assumes a nontargeted query on each connection, which would be a bad design, so this is a somewhat extreme example.)\n\nThus, there are a few adjustments to make. Many people purposefully configure mon‐ gos processes to only allow a certain number of incoming connections by using the maxConns option. This is a good way to enforce that your client is behaving well.\n\n460\n\n|\n\nChapter 24: Deploying MongoDB",
      "content_length": 2932,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "You should also increase the limit on the number of file descriptors, as the default (generally 1,024) is simply too low. Set the max number of file descriptors to unlimi‐ ted or, if you’re nervous about that, 20,000. Each system has a different way of chang‐ ing these limits, but in general, make sure that you change both the hard and soft limits. A hard limit is enforced by the kernel and can only be changed by an adminis‐ trator, whereas a soft limit is user-configurable.\n\nIf the maximum number of connections is left at 1,024, Cloud Manager will warn you by displaying the host in yellow in the host list. If low limits are the issue that trig‐ gered the warning, the Last Ping tab should display a message similar to that shown in Figure 24-4.\n\nFigure 24-4. Cloud Manager low ulimit (file descriptors) setting warning\n\nEven if you have a nonsharded setup and an application that only uses a small num‐ ber of connections, it’s a good idea to increase the hard and soft limits to at least 4,096. That will stop MongoDB from warning you about them and give you some breathing room, just in case.\n\nConfiguring Your Network This section covers which servers should have connectivity to which other servers. Often, for reasons of network security (and sensibility), you may want to limit the connectivity of MongoDB servers. Note that multiserver MongoDB deployments should handle networks being partitioned or down, but it isn’t recommended as a general deployment strategy.\n\nFor a standalone server, clients must be able to make connections to the mongod.\n\nMembers of a replica set must be able to make connections to every other member. Clients must be able to connect to all nonhidden, nonarbiter members. Depending on network configuration, members may also attempt to connect to themselves, so you should allow mongods to create connections to themselves.\n\nSharding is a bit more complicated. There are four components: mongos servers, shards, config servers, and clients. Connectivity can be summarized in the following three points:\n\nA client must be able to connect to a mongos.\n\nA mongos must be able to connect to the shards and config servers.\n\nConfiguring Your Network\n\n|\n\n461",
      "content_length": 2194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "A shard must be able to connect to the other shards and the config servers.\n\nThe full connectivity chart is described in Table 24-1.\n\nTable 24-1. Sharding connectivity\n\nConnectivity\n\nfrom server type\n\nto server type mongos mongos Shard Config server Client\n\nNot required Required Required Not required\n\nShard Config server Not required Not required Not required Required Required Not required Not required Not required\n\nClient Required Not recommended Not recommended Not MongoDB-related\n\nThere are three possible values in the table. “Required” means that connectivity between these two components is required for sharding to work as designed. Mon‐ goDB will attempt to degrade gracefully if it loses these connections due to network issues, but you shouldn’t purposely configure it that way.\n\n“Not required” means that these two elements never talk in the direction specified, so no connectivity is needed.\n\n“Not recommended” means that these two elements should never talk, but due to user error they could. For example, it is recommended that clients only make connec‐ tions to the mongos, not the shards, so that clients do not inadvertently make requests directly to shards. Similarly, clients should not be able to directly access config servers so that they cannot accidentally modify config data.\n\nNote that mongos processes and shards talk to config servers, but config servers don’t make connections to anyone, even one another.\n\nShards must communicate during migrates: shards connect to one another directly to transfer data.\n\nAs mentioned earlier, replica set members that compose shards should be able to con‐ nect to themselves.\n\nSystem Housekeeping This section covers some common issues you should be aware of before deploying.\n\nSynchronizing Clocks In general, it’s safest to have your systems’ clocks within a second of each other. Rep‐ lica sets should be able to handle nearly any clock skew. Sharding can handle some skew (if it gets beyond a few minutes, you’ll start seeing warnings in the logs), but it’s\n\n462\n\n|\n\nChapter 24: Deploying MongoDB",
      "content_length": 2069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "best to minimize it. Having in-sync clocks also makes figuring out what’s happening from logs easier.\n\nYou can keep clocks synchronized using the w32tm tool on Windows and the ntp daemon on Linux.\n\nThe OOM Killer Very occasionally, MongoDB will allocate enough memory that it will be targeted by the out-of-memory (OOM) killer. This particularly tends to happen during index builds, as that is one of the only times when MongoDB’s resident memory should put any strain on the system.\n\nIf your MongoDB process suddenly dies with no errors or exit messages in the logs, check /var/log/messages (or wherever your kernel logs such things) to see if it has any messages about terminating mongod.\n\nIf the kernel has killed MongoDB for memory overuse, you should see something like this in the kernel log:\n\nkernel: Killed process 2771 (mongod) kernel: init invoked oom-killer: gfp_mask=0x201d2, order=0, oomkilladj=0\n\nIf you were running with journaling, you can simply restart mongod at this point. If you were not, restore from a backup or resync the data from a replica.\n\nThe OOM killer gets particularly nervous if you have no swap space and start run‐ ning low on memory, so a good way to prevent it from going on a spree is to config‐ ure a modest amount of swap. As mentioned earlier, MongoDB should never use it, but it makes the OOM killer happy.\n\nIf the OOM killer kills a mongos, you can simply restart it.\n\nTurn Off Periodic Tasks Check that there aren’t any cron jobs, antivirus scanners, or daemons that might peri‐ odically pop to life and steal resources. One culprit we’ve seen is package managers’ automatic update. These programs will come to life, consume a ton of RAM and CPU, and then disappear. This is not something you want running on your produc‐ tion server.\n\nSystem Housekeeping\n\n|\n\n463",
      "content_length": 1807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "APPENDIX A Installing MongoDB\n\nMongoDB binaries are available for Linux, macOS, Windows, and Solaris. This means that, on most platforms, you can download an archive from the MongoDB Download Center page, inflate it, and run the binary.\n\nThe MongoDB server requires a directory it can write database files to and a port it can listen for connections on. This section covers the entire install on the two var‐ iants of system: Windows and everything else (Linux/Unix/macOS).\n\nWhen we speak of “installing MongoDB,” generally what we are talking about is set‐ ting up mongod, the core database server. mongod can be used as a standalone server or as a member of a replica set. Most of the time, this will be the MongoDB process you are using.\n\nChoosing a Version MongoDB uses a fairly simple versioning scheme: even-point releases are stable, and odd-point releases are development versions. For example, anything starting with 4.2 is a stable release, such as 4.2.0, 4.2.1, and 4.2.8. Anything starting with 4.3 is a devel‐ opment release, such as 4.3.0, 4.3.2, or 4.3.12. Let’s take the 4.2/4.3 release as a sample case to demonstrate how the versioning timeline works:\n\n1. MongoDB 4.2.0 is released. This is a major release and will have an extensive changelog.\n\n2. After the developers start working on the milestones for 4.4 (the next major sta‐ ble release), they release 4.3.0. This is the new development branch, which is fairly similar to 4.2.0 but probably with an extra feature or two and maybe some bugs.\n\n465",
      "content_length": 1519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "3. As the developers continue to add features, they will release 4.3.1, 4.3.2, and so on. These releases should not be used in production.\n\n4. Some minor bug fixes may be backported to the 4.2 branch, which will cause releases of 4.2.1, 4.2.2, and so on. Developers are conservative about what is backported; few new features are ever added to a stable release. Generally, only bug fixes are ported.\n\n5. After all of the major milestones have been reached for 4.4.0, 4.3.7 (or whatever the latest development release is) will be turned into 4.4.0-rc0.\n\n6. After extensive testing of 4.4.0-rc0, usually there are a couple minor bugs that need to be fixed. Developers fix these bugs and release 4.4.0-rc1.\n\n7. Developers repeat step 6 until no new bugs are apparent, and then 4.4.0-rc2 (or whatever the latest release ended up being) is renamed 4.4.0.\n\n8. Developers start over from step 1, incrementing all versions by 0.2.\n\nYou can see how close a production release is by browsing the core server roadmap on the MongoDB bug tracker.\n\nIf you are running in production, you should use a stable release. If you are planning to use a development release in production, ask about it first on the mailing list or IRC to get the developers’ advice.\n\nIf you are just starting development on a project, using a development release may be a better choice. By the time you deploy to production, there will probably be a stable release with the features you’re using (MongoDB attempts to stick to a regular cycle of stable releases every 12 months). However, you must balance this against the possi‐ bility that you may run into server bugs, which can be discouraging to a new user.\n\nWindows Install To install MongoDB on Windows, download the Windows .msi from the MongoDB Download Center page. Use the advice in the previous section to choose the correct version of MongoDB. When you click the link, it will download the .msi. Double- click the .msi file icon to launch the installer program.\n\nNow you need to make a directory in which MongoDB can write database files. By default, MongoDB tries to use the \\data\\db directory on the current drive as its data directory (e.g., if you’re running mongod on C: on Windows, it’ll use C:\\Program Files \\MongoDB\\Server\\&<VERSION>\\data). This will be created automatically for you by the installer. If you chose to use a directory other than \\data\\db, you’ll need to specify the path when you start MongoDB, which is covered in a moment.\n\nNow that you have a data directory, open the command prompt (cmd.exe). Navigate to the directory where you unzipped the MongoDB binaries and run the following:\n\n466\n\n| Appendix A: Installing MongoDB",
      "content_length": 2670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 489,
      "content": "$ C:\\Program Files\\MongoDB\\Server\\&<VERSION>\\bin\\mongod.exe\n\nIf you chose a directory other than C:\\Program Files\\MongoDB\\Server\\&<VERSION> \\data, you’ll have to specify it here, with the --dbpath argument:\n\n$ C:\\Program Files\\MongoDB\\Server\\&<VERSION>\\bin\\mongod.exe \\ --dbpath C:\\Documents and Settings\\Username\\My Documents\\db\n\nSee Chapter 21 for more common options, or run mongod.exe --help to see all the options.\n\nInstalling as a Service MongoDB can also be installed as a service on Windows. To do this, simply run it with the full path, escape any spaces, and use the --install option. For example:\n\n$ C:\\Program Files\\MongoDB\\Server\\4.2.0\\bin\\mongod.exe \\ --dbpath \"\\\"C:\\Documents and Settings\\Username\\My Documents\\db\\\"\" \\ --install\n\nIt can then be started and stopped from the Control Panel.\n\nPOSIX (Linux and Mac OS X) Install Choose a version of MongoDB, based on the advice in the section “Choosing a Ver‐ sion” on page 465. Go to the MongoDB Download Center and select the correct ver‐ sion for your OS.\n\nIf you are using a Mac and are running macOS Catalina 10.15+, you should use /System/Volumes/Data/db instead of /data/db. This version made a change that renders the root folder read-only and resets upon reboot, which would result in the loss of your Mon‐ goDB data folder.\n\nYou must create a directory for the database to put its files in. By default the database will use /data/db, although you can specify any other directory. If you create the default directory, make sure it has the correct write permissions. You can create the directory and set the permissions by running the following commands:\n\n$ mkdir -p /data/db $ chown -R $USER:$USER /data/db\n\nmkdir -p creates the directory and all its parents, if necessary (i.e., if the /data direc‐ tory doesn’t exist, it will create the /data directory and then the /data/db directory). chown changes the ownership of /data/db so that your user can write to it. Of course, you can also just create a directory in your home folder and specify that MongoDB should use that when you start the database, to avoid any permissions issues.\n\nInstalling MongoDB\n\n|\n\n467",
      "content_length": 2132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "Decompress the .tar.gz file you downloaded from the MongoDB Download Center:\n\n$ tar zxf mongodb-linux-x86_64-enterprise-rhel62-4.2.0.tgz $ cd mongodb-linux-x86_64-enterprise-rhel62-4.2.0\n\nNow you can start the database:\n\n$ bin/mongod\n\nOr, if you’d like to use an alternate database path, specify it with the --dbpath option:\n\n$ bin/mongod --dbpath ~/db\n\nYou can run mongod.exe --help to see all the possible options.\n\nInstalling from a Package Manager There are also many package managers that can be used to install MongoDB. If you prefer using one of these, there are official packages for Red Hat, Debian, and Ubuntu as well as unofficial packages for many other systems. If you use an unofficial version, make sure it installs a relatively recent version.\n\nOn macOS, there are unofficial packages for Homebrew and MacPorts. To use the MongoDB Homebrew tap, you first install the tap and then install the required ver‐ sion of MongoDB via Homebrew. The following example highlights how to install the latest production version of MongoDB Community Edition. You can add the custom tap in a macOS terminal session using:\n\n$ brew tap mongodb/brew\n\nThen install the latest available production release of MongoDB Community Server (including all command-line tools) using:\n\n$ brew install mongodb-community\n\nIf you go for the MacPorts version, be forewarned: it takes hours to compile all the Boost libraries, which are MongoDB prerequisites. Start the download and leave it overnight.\n\nRegardless of the package manager you use, it is a good idea to figure out where it is putting the MongoDB log files before you have a problem and need to find them. It’s important to make sure they’re being saved properly in advance of any possible issues.\n\n468\n\n| Appendix A: Installing MongoDB",
      "content_length": 1781,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "APPENDIX B MongoDB Internals\n\nIt is not necessary to understand MongoDB’s internals to use it effectively, but they may be of interest to developers who wish to work on tools, contribute, or simply understand what’s happening under the hood. This appendix covers some of the basics. The MongoDB source code is available at https://github.com/mongodb/mongo.\n\nBSON Documents in MongoDB are an abstract concept—the concrete representation of a document varies depending on the driver/language being used. Because documents are used extensively for communication in MongoDB, there also needs to be a repre‐ sentation of documents that is shared by all drivers, tools, and processes in the Mon‐ goDB ecosystem. That representation is called Binary JSON, or BSON (no one knows where the J went).\n\nBSON is a lightweight binary format capable of representing any MongoDB docu‐ ment as a string of bytes. The database understands BSON, and BSON is the format in which documents are saved to disk.\n\nWhen a driver is given a document to insert, use as a query, and so on, it will encode that document to BSON before sending it to the server. Likewise, documents being returned to the client from the server are sent as BSON strings. This BSON data is decoded by the driver to its native document representation before being returned to the client.\n\nThe BSON format has three primary goals:\n\n469",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "Efficiency\n\nBSON is designed to represent data efficiently, without using much extra space. In the worst case BSON is slightly less efficient than JSON, and in the best case (e.g., when storing binary data or large numerics), it is much more efficient.\n\nTraversability\n\nIn some cases, BSON does sacrifice space efficiency to make the format easier to traverse. For example, string values are prefixed with a length rather than relying on a terminator to signify the end of a string. This traversability is useful when the MongoDB server needs to introspect documents.\n\nPerformance\n\nFinally, BSON is designed to be fast to encode to and decode from. It uses C-style representations for types, which are fast to work with in most programming languages.\n\nFor the exact BSON specification, see http://www.bsonspec.org.\n\nWire Protocol Drivers access the MongoDB server using a lightweight TCP/IP wire protocol. The protocol is documented on the MongoDB documentation site but basically consists of a thin wrapper around BSON data. For example, an insert message consists of 20 bytes of header data (which includes a code telling the server to perform an insert and the message length), the collection name to insert into, and a list of BSON docu‐ ments to insert.\n\nData Files Inside the MongoDB data directory, which is /data/db/ by default, a separate file will be stored for each collection and each index. The filenames do not correspond to the names of the collections or indexes, but you can use the stats within the mongo shell to identify the related file for a specific collection. The \"wiredTiger.uri\" field will contain the name of the file to look for in the MongoDB data directory.\n\nUsing stats on the sample_mflix database for the movies collection provides “collection-14--2146526997547809066” as result in the \"wiredTiger.uri\" field:\n\n>db.movies.stats() { \"ns\" : \"sample_mflix.movies\", \"size\" : 65782298, \"count\" : 45993, \"avgObjSize\" : 1430, \"storageSize\" : 45445120, \"capped\" : false,\n\n470\n\n| Appendix B: MongoDB Internals",
      "content_length": 2034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "\"wiredTiger\" : { \"metadata\" : { \"formatVersion\" : 1 }, \"creationString\" : \"access_pattern_hint=none,allocation_size=4KB,\\ app_metadata=(formatVersion=1),assert=(commit_timestamp=none,\\ read_timestamp=none),block_allocation=best,\\ block_compressor=snappy,cache_resident=false,checksum=on,\\ colgroups=,collator=,columns=,dictionary=0,\\ encryption=(keyid=,name=),exclusive=false,extractor=,format=btree,\\ huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,\\ immutable=false,internal_item_max=0,internal_key_max=0,\\ internal_key_truncate=true,internal_page_max=4KB,key_format=q,\\ key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,\\ leaf_value_max=64MB,log=(enabled=true),lsm=(auto_throttle=true,\\ bloom=true,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,\\ bloom_oldest=false,chunk_count_limit=0,chunk_max=5GB,\\ chunk_size=10MB,merge_custom=(prefix=,start_generation=0,suffix=),\\ merge_max=15,merge_min=0),memory_page_image_max=0,\\ memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,\\ prefix_compression=false,prefix_compression_min=4,source=,\\ split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,\\ type=file,value_format=u\", \"type\" : \"file\", \"uri\" : \"statistics:table:collection-14--2146526997547809066\", ... }\n\nThe file’s details can then be verified within the MongoDB data directory:\n\nls -alh collection-14--2146526997547809066.wt -rw------- 1 braz staff 43M 28 Sep 23:33 collection-14--2146526997547809066.wt\n\nIt’s possible to use the aggregation framework to find the URI for each index in a spe‐ cific collection using the following:\n\ndb.movies.aggregate([{ $collStats:{storageStats:{}}}]).next().storageStats.indexDetails { \"_id_\" : { \"metadata\" : { \"formatVersion\" : 8, \"infoObj\" : \"{ \\\"v\\\" : 2, \\\"key\\\" : { \\\"_id\\\" : 1 },\\ \\\"name\\\" : \\\"_id_\\\", \\\"ns\\\" : \\\"sample_mflix.movies\\\" }\" }, \"creationString\" : \"access_pattern_hint=none,allocation_size=4KB,\\ app_metadata=(formatVersion=8,infoObj={ \\\"v\\\" : 2, \\\"key\\\" : \\ { \\\"_id\\\" : 1 },\\\"name\\\" : \\\"_id_\\\", \\\"ns\\\" : \\\"sample_mflix.movies\\\" }),\\ assert=(commit_timestamp=none,read_timestamp=none),block_allocation=best,\\ block_compressor=,cache_resident=false,checksum=on,colgroups=,collator=,\\ columns=,dictionary=0,encryption=(keyid=,name=),exclusive=false,extractor=,\\ format=btree,huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,\\ immutable=false,internal_item_max=0,internal_key_max=0,\\\n\nMongoDB Internals\n\n|\n\n471",
      "content_length": 2423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "internal_key_truncate=true,internal_page_max=16k,key_format=u,key_gap=10,\\ leaf_item_max=0,leaf_key_max=0,leaf_page_max=16k,leaf_value_max=0,\\ log=(enabled=true),lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,\\ bloom_config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit=0,\\ chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_generation=0,\\ suffix=),merge_max=15,merge_min=0),memory_page_image_max=0,\\ memory_page_max=5MB,os_cache_dirty_max=0,os_cache_max=0,\\ prefix_compression=true,prefix_compression_min=4,source=,\\ split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,\\ value_format=u\", \"type\" : \"file\", \"uri\" : \"statistics:table:index-17--2146526997547809066\", ... \"$**_text\" : { ... \"uri\" : \"statistics:table:index-29--2146526997547809066\", ... \"genres_1_imdb.rating_1_metacritic_1\" : { ... \"uri\" : \"statistics:table:index-30--2146526997547809066\", ... }\n\nWiredTiger stores each collection or index in a single arbitrarily large file. The only limits that impact the potential maximum size of this file are filesystem size limits.\n\nWiredTiger writes a new copy of the full document whenever that document is upda‐ ted. The old copy on disk is flagged for reuse and will eventually be overwritten at a future point, typically during the next checkpoint. This recycles the space used within the WiredTiger file. The compact command can be run to move the data within this file to the start, leaving empty space at the end. At regular intervals, WiredTiger removes this excess empty space by truncating the file. At the end of the compaction process, the excess space is returned to the filesystem.\n\nNamespaces Each database is organized into namespaces, which are mapped to WiredTiger files. This abstraction separates the storage engine’s internal details from the MongoDB query layer.\n\nWiredTiger Storage Engine The default storage engine for MongoDB is the WiredTiger storage engine. When the server starts up, it opens the data files and begins the checkpointing and journaling processes. It works in conjunction with the operating system, whose responsibility is focused on paging data in and out as well as flushing data to disk. This storage engine has several important properties:\n\n472\n\n| Appendix B: MongoDB Internals",
      "content_length": 2275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "Compression is on by default for collections and for indexes. The default com‐ pression algorithm is Google’s snappy. Other options include Facebook’s Zstan‐ dard (zstd) and zlib, or indeed no compression. This minimizes storage use in the database at the expense of additional CPU requirements.\n\nDocument-level concurrency allows for updates on different documents from multiple clients in a collection as the same time. WiredTiger uses MultiVersion Concurrency Control (MVCC) to isolate read and write operations to ensure cli‐ ents see a consistent point-in-time view of the data at the start of an operation.\n\nCheckpointing creates a consistent point-in-time snapshot of the data and occurs every 60 seconds. It involves writing all the data in the snapshot to disk and updating the related metadata.\n\nJournaling with checkpointing ensures there is no point in time where data might be lost if there was a failure of a mongod process. WiredTiger uses a write-ahead log (journal) that stores modifications before they are applied.\n\nMongoDB Internals\n\n|\n\n473",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "Symbols $ (dollar sign)\n\n$ operators (see query operators) $$ (variable reference), 181 creating indexes on $**, 148 position operator, 45 querying arrays, 61 reserved character, 8, 10\n\n$indexStats operator, 433 $maxKey, 297, 321 $minKey, 297 --configdb option, 305 . (dot)\n\nin subcollections, 10 reserved character, 8 2d indexes, 133, 144-146 2dsphere indexes, 133 32-bit systems, 417, 452 64-bit systems, 451 \\0 (the null character), 8, 9\n\nA access time tracking, 459 accumulators\n\nfor arrays, 186 in group versus project stage, 186 purpose of, 186 using in project stage, 186, 195\n\nACID (Atomicity, Consistency, Isolation, and\n\nDurability), 200\n\nacknowledged writes, 376 $addToSet operator, 43, 186 admin database\n\ncontents of, 305\n\nIndex\n\nrole of, 11 shutdown command, 420\n\nadmin user, 391, 402 administration, of applications\n\napplication operations, 371-387 durability, 405-411 security considerations, 389-404\n\nadministration, of replica sets\n\nmanipulating member state, 275 monitoring replication, 275-285 replica set configuration, 272-274 replication on a budget, 285 starting members in standalone mode, 271\n\nadministration, of servers backups, 437-447 deployment, 449-463 monitoring, 425-436 production set up, 415-424\n\nadministration, of sharding adding servers, 356-359 balancing data, 359-367 seeing current state, 339-348 tracking network connections, 348-356\n\naggregation framework\n\n$project operator, 169-174 $unwind operator, 174-181 accumulators, 186 aggregate method, 166 array expressions in project stages, 181-185 concept of, 161 expression classes supported, 168 group stage\n\naggregate command, 188\n\n475",
      "content_length": 1628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "aggregating values from multiple docu‐\n\nments, 187\n\ngroup and sort stages, 188 _id field, 192-195 versus project stage, 195\n\nindividual stages, 162 match, project, sort, skip, and limit stages,\n\n163-168\n\npipeline efficiency, 167 presorting, 167 purpose of, 161 repeated stages in, 162 tunables in, 162 writing pipeline results to collections, 198\n\n$all operator, 59 antivirus scanners, 463 Apache Lucene, 146 application design\n\nmanaging consistency, 221 managing schemas, 223 migrating schemas, 222 normalization and denormalization benefits and drawbacks of, 213 cardinality, 216 data representation examples, 212 defined, 211 embedding versus references, 215 social graph data and, 216 update operators, 215 Wil Wheaton effect, 218\n\noptimizations for data manipulations, 219 planning out databases and collections, 220 schema design considerations, 207 schema design patterns, 208-211 when not to use MongoDB, 223\n\napplication operations calculating sizes\n\ncollections, 380 databases, 385 documents, 379 finding problematic, 374 finding slow, 371, 376 genuinely long-running, 375 important fields, 373 killing, 375 preventing phantom, 375 printing stats every few seconds, 386 seeing current, 371\n\napproximation schema design pattern, 210\n\n476\n\n|\n\nIndex\n\narbiters, 246 array expressions, using in project stages,\n\n181-185 array operators\n\nadding elements, 41 adding multiple unique values, 44 positional array modifications, 45 preventing duplicates, 43 removing elements, 44 updates using array filters, 46 using arrays as sets, 43 $arrayElemAt operator, 184 arrays\n\narray type, 18 atomic updates, 19 indexing, 115 manipulating, 41-46 querying, 59-63 uses for, 19 atomicity, 200 attribute schema design pattern, 209 --auth option, 392 authentication, 389 authorization, 390 automatic failover, 236 automatic replication chaining, 279, 281 autosharding, 289, 337 $avg operator, 186 AWS Enhanced Networking, 454\n\nB backup privileges, 391 backups\n\noptions for, 437 replica sets, 446 servers\n\ncopying data files, 442 filesystem snapshots, 438-442 using mongodump, 443 what to backup, 438 balancer (see also chunks)\n\ndraining process, 356-359 firehose strategy and, 329 purpose of, 323 requesting assignments from, 326 role of, 316 turning off, 336, 360\n\nbatch insert, 29 big-endian systems, 452",
      "content_length": 2294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "binary data type, 18 --bind_ip parameter, 308, 421 boolean type, 17 $box operator, 145 broadcast (scatter-gather) queries, 301 BSD variants, 451 BSON (Binary JSON) format, 469 bsondump tool, 444 bucket schema design pattern, 209\n\nC call back API, 200 capacity, adding with shards, 310 capped collections (see also collections)\n\naccess pattern in, 153 benefits and drawbacks of, 219 circular queue-like behavior, 152 creating, 154 inability to change, 154 limiting number of documents in, 154 oplogs, 249 restricted operation, 152 tailable cursors, 154 versus normal, 151\n\ncapped indexes, versus time-to-live indexes,\n\n153 cardinality\n\nin application design, 216 in shard keys, 334 causal consistency, 201 $centerSphere operator, 143, 146 certification authority (CA), 392 chaining, 281 change streams, 317 changelog collecion, 344-348 checkpoints, 406, 473 chunks\n\nallowed size range, 362 basics of, 311 changing chunk size, 361 checking chunk size, 365 chunk ranges, 312 compound shard keys and, 313 defined, 311 jumbo chunks, 364-367 max chunk, 321 moving, 362 seeing all, 341 sharding chunks versus GridFS chunks, 328\n\nsplitting chunks, 314 status overview, 339\n\ncircle, querying for points within, 145 client certificates, 393, 401 client libraries, purpose of, 261 clocks, synchronizing, 462 Cloud Manager, 437 clusterAdmin privileges, 391 clusterManager privileges, 390 clusterMonitor privileges, 390 clusters\n\nbackup options, 437, 446 components of, 290 durability of, 407 sharding on single-machine, 291-301 tracking cluster data, 311-315 using for multiple databases/collections, 335\n\ncode examples, obtaining and using, xvii code type, 18 collations, 317 collection scans, versus indexes, 75 collections\n\nbasics of, 8-10 benefits of separate, 8 calculating size of, 380 capped, 151-155 checking for corruption, 410 chunks and, 312 dealing with inconvenient names, 28 determining busiest, 386 dropping (clearing entire), 34 dynamic schemas, 8 finding random documents in, 69 internal, 9 locking and storage of, 220 moving with mongodump, 445 multiple, 219 naming, 9 planning in application design, 220 querying, 15 sharding, 296 subcollections, 10 time-to-live (TTL), 219 using clusters for multiple, 335 writing aggregation pipeline results to, 198\n\ncollMod command, 223 command line, starting MongoDB from,\n\n415-420\n\ncomments and questions, xviii\n\nIndex\n\n|\n\n477",
      "content_length": 2372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "compact command, 472 comparison operators, 55 comparison order, 68 compound indexes benefits of, 105 best practices, 84, 96 compound unique indexes, 127 defined, 81 geospatial, 144 keys in reverse order, 83 types of queries, 82 using, 85-104\n\navoiding in-memory sort, 100 choosing key directions, 102 covered queries, 103 design goals, 86, 97 implicit indexes, 104 index selectivity, 86 specifying which index to use, 92\n\ncompression, 473 computed schema design pattern, 209 computer memory (see memory) concurrency, 473 conditionals, query, 55 --config, 416 config database\n\ncontents of, 305 options increasing security of, 421 role of, 11 seeing configuration information\n\nconfig.changelog, 344 config.chunks, 343 config.collections, 342 config.databases, 342 config.settings, 347 config.shards, 342 connecting to mongos process, 341\n\nconfig servers\n\nbest practices, 304 initiating as replica sets, 304 purpose of, 304 starting, 304\n\n--configdb option, 356 --configExpand, 419 --configsvr option, 305 connection pools, 221 connPoolStats command, 348 consistency\n\ndefined, 200\n\n478\n\n|\n\nIndex\n\nmanaging in application design, 221\n\ncore API, 200 corruption, checking for, 410 CPU (central processing unit), 451 crashes, recovering from, 355 create command, 154 createCollection command, 154 cron jobs, turning off, 463 CRUD (create, read, update, and delete)\n\nbasic shell operation, 14-16 CRUD API, 33 explain cursor method, 76 optimizations for data manipulation, 219\n\ncursors\n\navoiding large skips, 68 benefits of, 66 creating, 66 cursor hint method, 92 explain cursor method, 76 immortal, 70 iterating through results, 66 limits, skips, and sort options, 67 sending queries to servers, 67 tailable cursors, 154\n\nD daemons, turning off, 463 data\n\nbalancing, 359-367 controlling distribution of, 334-337 copying data files, 442 data files internals, 470 encrypting, 422 importing, 30 munging, 30 optimizations for manipulating, 219 optimizing for safety and access, 449 removing old, 219 restoring deleted, 34 sharding, 310\n\ndata directories, specifying alternate, 415 data types\n\narrays, 19 basic, 16-18 comparison order, 68 dates, 18 embedded documents, 19 _id keys and ObjectIds, 20-22\n\ndatabases",
      "content_length": 2199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "ACID compliant, 200 authentication, 390 basics of, 10 calculating size of, 385 connecting to, 22 document-oriented, 3 durability of, 405 locking statistics on, 386 moving data into and out of, 29\n\n(see also documents)\n\nmoving with mongodump, 445 naming, 10 placing in own directory, 416 planning in application design, 220 reserved names, 11 user permissions, 390\n\n(see also security considerations)\n\nusing clusters for multiple, 335\n\ndataSize command, 365 Date(), 18 dates\n\ncreating new Date objects, 18 date type, 17 time zone information, 19\n\ndb.adminCommand(), 233 db.chunks.find(), 337 db.coll.remove(), 250 db.createCollection(), 223 db.currentOp(), 371, 374 db.fsyncLock(), 442, 442 db.getCollectionNames(), 24 db.getMongo().getDBs(), 24 db.getProfilingLevel(), 379 db.getSisterDB(), 24 db.help command, 23 db.killOp(), 375 db.serverCmdLineOpts(), 271 db.setProfilingLevel(), 376 db.shutdownServer(), 272 db.stats(), 385 db.users.find, 82 dbAdmin privileges, 390 dbAdminAnyDatabase privileges, 391 dbOwner privileges, 390 --dbpath, 415 debugging, 423 deleteMany method, 16, 33 deleteOne method, 16, 33 demormalization, 213\n\n(see also normalization and denormaliza‐\n\ntion) deployment\n\nnetwork configuration, 461 system design, 449-452 system housekeeping, 462 system settings configuration, 455-461\n\nchoosing disk scheduling algorithm, 458 disabling access time tracking, 459 disabling transparent huge pages, 458 modifying limits, 460 setting readahead, 457 turning off NUMA, 455\n\nvirtualization, 453-455 design (see application design) directories\n\nplacing databases in, 416 specifying alternate, 415 --directoryperdb, 416, 433, 443 disk scheduling algorithms, 458 disk usage, tracking, 433 disks, choosing, 449 distinct command, 159 DNS Seedlist Connection format, 228 document versioning schema design pattern,\n\n211\n\ndocument-level concurrency, 473 document-oriented databases, benefits of, 3 documents\n\nadding to collections, 14 basics of, 7 calculating size of, 379 combining multiple into single, 186 deleting from database, 16 embedded, 18 finding random, 69 inserting, 29-33 removing, 33 replacing, 35 returning updated, 49-51 updating, 35-49 updating multiple, 49, 250 _id key autogeneration, 22\n\ndraining process, 356-359 drivers, defined, 261 drop method, 34 dropIndexes command, 129 dump directory, 444 duplicates\n\nIndex\n\n|\n\n479",
      "content_length": 2347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "dropping in indexes, 127 preventing in arrays, 43, 186\n\ndurability\n\nat cluster level using read concern, 408 at cluster level using write concern, 407 at member level through journaling, 405 checking for corruption, 410 defined, 200, 405 of transactions using write concern, 409 situations without guarantee, 410\n\ndynamic schemas, collections, 8 dynamic voltage and frequency scaling (DVFS),\n\n454\n\nE $each modifier, 42 EDITOR variable, 27 Elastic Block Store (EBS), 454 Elastic Compute Cloud (EC2), 454 elections\n\nelection arbiters, 246 how they work, 243 members seeking, 255 preventing, 275 preventing voting, 274 speed of, 255\n\n$elemMatch operator, 62, 64 embedded documents\n\nchanging/updating, 38 embedded document type, 18 indexing, 114 querying, 63 uses for, 19\n\nembedding, versus references, 215 --enableEncryption, 422 enableSharding command, 294 --encryptionCipherMode, 422 --encryptionKeyFile, 422 ephemeral drives, 455 exact phrase searches, 148 executionStats mode, 76 explain command, 76, 87 expressions, using array expression in project\n\nstages, 181-185\n\nextended reference schema design pattern, 210\n\nF files and filesystems\n\ncopying data files, 442\n\n480\n\n|\n\nIndex\n\ndata encryption, 422 file-based configuration, 419 filesystem snapshots, 438-442 recommended filesystem, 452 sending output to, 416 storing files with GridFS, 156-159\n\nfilter expressions, 181 $filter operator, 181 find method\n\narguments to, 53 limitations of, 55 multiple conditions, 54 querying collections, 15 specifying which keys to return, 54\n\nfindAndModify method, 49 findOne method, 15 findOneAndDelete method, 50 findOneAndReplace method, 50 findOneAndUpdate method, 50 firehose strategy, 329 firewalls, 421 $first operator, 186, 197 flushRouterConfig command, 367 force option, 420 --fork, 416 free space, tracking, 433 fs.chunks collection, 158, 329 fs.files collection, 158, 329 fsyncLock command, 442 full validate, 411 full-text indexes\n\nbenefits and drawbacks of, 147 creating, 147 optimizing, 151 phrases and logical ANDs in queries, 148 searching other languages, 151 sorting for relevance, 149 text search, 148 uses for, 146 versus MongoDB Atlas Full-Text Search\n\nIndexes, 146\n\nG $geoIntersects operator, 135, 136, 141 GeoJSON format, 133, 144 $geoNear operator, 136 geospatial indexes\n\n2d indexes, 144-146 compound, 144",
      "content_length": 2319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "geospatial query types, 134 query types and geometries in MongoDB,\n\n136 types of, 133 using, 136-143\n\n2D versus spherical geometry, 136 distortion, 137 exploring data, 139 finding all restaurants in neighborhoods,\n\n142\n\nfinding current neighborhood, 141 finding restaurants within a distance,\n\n143\n\nsearching for restaurants, 138 $geoWithin operator, 135, 136, 143, 145, 146 getCollection function, 28 getLastError function, 265 getLastErrorModes field, 266 GridFS\n\nbenefits and drawbacks of, 156 hashed shard keys for, 328 how it works, 158 keys in, 158 md5 key, 159 metadata location, 158 mongofiles utility, 156 working with from MongoDB drivers, 157\n\n$group operator, 188 $gt operator, 55 $gte operator, 55\n\nH hashed shard keys, 327-329 heartbeat requests, 253 help command, 22 hidden members, 245, 267 hint function, 124 hostManager privileges, 390 hotspot shard keys, 360 hyperthreading, 454\n\nI I/O Operations Per Second (IOPS), 454 I/O wait, 429, 431, 453 _id field, in group stages, 192 _id keys\n\nautogeneration of, 22 basics of, 20 immortal cursors, 70\n\n$in operator, 56, 114 in-place updates, 251 $inc modifier, 37, 39 $inc operator, 215 indexes\n\nbackground indexing, 131 basic operations\n\nchecking build progress, 78 choosing fields to index, 80 compound indexes, 81-104 creating, 78, 129, 131, 134 how MongoDB selects indexes, 84 index cardinality, 116, 208 objects and arrays, 114 query operators, 104-114 benefits and drawbacks of, 80 capped collections, 151-155 changing, 130 compound unique, 127 dropping duplicates in, 127 effectiveness of, 125 example of, 75 explain output\n\nexample of, 120 forcing index use, 124 important fields, 119 types of, 117 uses for, 116\n\nfor full text search, 146-151 geospatial, 133-146 GridFS file storage, 156-159 hashed, 327 hybrid index build, 131 identifying, 130 implicit, 104 metainformation on, 129 multikey, 116 partial, 128 purpose of, 75, 81 removing unneeded, 130 right-balanced, 384 selectivity of, 86, 90 supporting sort operation, 83 time-to-live (TTL), 155 types of, 126-129 unique, 126, 445 versus collection scans, 75 when not to index, 125\n\ninsert method, 33\n\nIndex\n\n|\n\n481",
      "content_length": 2138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "insertMany method, 29 insertOne method, 14, 29 inserts\n\nmultiple documents, 29 ordered versus unordered, 30 single documents, 29 validating, 32\n\ninstallation\n\ndownloading archives, 465 from package managers, 468 POSIX (Linux and Mac OS X), 467 version selection, 465 Windows, 466\n\nisMaster command, 237 isolation, 200\n\nJ joins, relational databases versus MongoDB,\n\n212, 223\n\n--journalCommitInterval, 406 journaling, 405, 473 JSON, MongoDB documents resembling, 16,\n\n469\n\n$jsonSchema operator, 223 jumbo chunks\n\nchecking chunk size, 365 defined, 364 distributing, 366 example of, 364 finding, 365 preventing, 367\n\nK Kerberos authentication, 389 Key Management Interoperability Protocol\n\n(KMIP), 422\n\nkeys\n\nadding, changing, or removing, 39 choosing key directions in compound\n\nindexes, 102\n\ndisallowed characters, 8 in documents, 7 in MongoDB, 17 limitations on, 64 shard keys, 294 specifying query returns, 54\n\nkill command, 420 killOP method, 375\n\n482\n\n|\n\nIndex\n\nL lag, calculating, 281 $last operator, 186, 197 latency\n\ncapping maximum, 459 defined, 320 firehose strategy and, 329 increased by reading from disk, 427 reading from secondaries and, 270\n\nLDAP proxy authentication, 389 limits, modifying, 460 Linux Logical Volume Manager (LVM), 439 little-endian systems, 452 load function, 24 local database, role of, 11 local.oplog.rs, 375 local.startup_log collection, 417 local.system.replset collection, 272 locking statistics, 386 logical sessions, 201 --logpath, 416, 423 logRotate command, 424 logs and logging (see also monitoring)\n\nbest practices, 423 changing log level, 423 checking log at startup, 417 debugging, 423 default options, 423 rotating logs, 424\n\nlong-running requests, 375 $lt operator, 55 $lte operator, 55\n\nM man-in-the-middle attacks, 392 manual sharding, 289, 336 many-to-many relationships, 216 mapped memory, 426 master/slave replication, 238 Math.random() function, 70 max chunk, 321 $max operator, 186 --maxConns option, 355 $maxDistance operator, 143 MaxKey constant, 297 md5 key, 159 members\n\nconfiguration options, 244-248 manipulating state, 275",
      "content_length": 2082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "member certificates, 393, 400 member states, 254 obtaining current status of, 276 starting in standalone mode, 271\n\nmemory\n\ncomputer memory basics, 426 I/O wait, 429 memory overcommitting, 453 mystery memory, 453 tracking memory usage, 426 tracking page faults, 427\n\n$merge operator, 198 $mergeObjects operator, 186 $meta operator, 150 $min operator, 186 MinKey constant, 297 $mod operator, 57 mongo shell\n\n.mongorc.js creation, 25 complex variable editing, 27 help command, 22 inconvenient collection names, 28 launching, 230 manipulating and viewing data in, 14-16 MongoDB client, 14 prompt customization, 26 prompt when connected, 293 purpose of, 13 running, 13 running scripts, 23-25 save shell helper, 48 using, 22 viewing startup log, 417\n\nMongoClient class, 261 mongod\n\nconnecting to, 22 replica set networking considerations, 229 running, 11 startup options, 415-420 stopping, 12\n\nMongoDB\n\nACID compliant transactions, 200 approach to learning, xv basic concepts, 7-11 benefits of, 3-6 consistency models, 222 data types, 16-22 getting and starting, 11 installing, 465-468\n\ninternals, 469-473 logical sessions and causal consistency in,\n\n201\n\nshell basics, 13-16 shell use, 22-28 stopping, 420 when not to use, 223\n\nMongoDB Atlas, 228, 290, 437 mongodump, 443-445 mongofiles utility, 156 mongoimport command-line tool, 30 .mongorc.js files, 25 mongorestore, 445 mongos\n\n--configdb option, 305 locating processes near shards, 306 purpose of, 290\n\nmongostat, 386 mongotop, 386 monitoring (see also logs and logging) calculating the working set, 429 memory usage, 425-429 replication, 433 services available, 425 tracking free space, 433 tracking performance, 431 moveChunk command, 337, 362 multi-hotspot strategy, 330 multiple collections, removing old data with,\n\n219\n\nMultiVersion Concurrency Control (MVCC),\n\n473\n\nmunging data, 30 mystery memory, 453\n\nN namespaces\n\nbasics of, 11, 472 filtering for operations on certain, 374\n\nnaming\n\ncollections, 9 databases, 10 dealing with inconvenient collection names,\n\n28\n\n$ne operator, 43, 55, 104 $near operator, 136 $nearSphere operator, 143 network configuration, 461 networked block stores, 454\n\nIndex\n\n|\n\n483",
      "content_length": 2164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "networking\n\nfor replica sets, 229 tracking network connections, 348\n\nnew Date(), 18 new Mongo(\"hostname\"), 22 new users, adding, 391 $nin operator, 56 --nodb option, 22, 291 non-networked disks, 455 nonuniform memory architecture (NUMA),\n\n455\n\n--norc options, 291 normalization and denormalization benefits and drawbacks of, 213 cardinality, 216 data representation examples, 212 defined, 211 embedding versus references, 215 social graph data and, 216 update operators, 215 Wil Wheaton effect, 218\n\n--noscripting, 421 $not operator, 57, 105 --nounixsocket, 421 null type\n\nquerying on, 57 uses for, 17 number type, 17\n\nO object ID type, 18 Object.bsonsize(), 379 ObjectIDs\n\nbasics of, 20 storing _ids as, 379 objects, indexing, 114 one-to-many relationships, 216 one-to-one relationships, 216 operating system, selecting, 451 operations, killing, 375\n\n(see also application operations)\n\noplogs\n\navoiding out-of-sync secondaries, 253 changing size of, 251 defining size of, 292 purpose of, 249 resizing, 282 size limits, 206, 250 statement-based replication, 406\n\n484\n\n|\n\nIndex\n\nsyncing, 249\n\noplogSizeMB option, 251 Ops Manager, 228, 290, 437 $or operator, 56, 112 $out operator, 198 out-of-memory (OOM) killer, 463 outlier schema design pattern, 209 overcommitting, 453\n\nP page faults, tracking, 427, 431 partialFilterExpression, 126, 128 partitioning, 289 passive members, 244 performance, tracking, 431 (see also monitoring) periodic tasks, turning off, 463 Perl Compatible Regular Expression (PCRE)\n\nlibrary, 58 permissions, 390 phantom operations, 375 ping time, 279 polymorphic schema design pattern, 209 $pop operator, 44 --port, 416 position operator ($), 45 preallocation schema design pattern, 211 primary shards, 294 primary-secondary-arbiter (PSA) architecture,\n\n247\n\nproblematic operations, 374 production set up\n\nchecking log, 417 data encryption, 422 file-based configuration, 419 logging, 423 security, 421\n\n(see also security considerations)\n\nservers, 449 SSL connections, 423 starting from command line, 415-420 stopping MongoDB, 420\n\n--profile level, 379 $project operator, 169-174 public key infrastructure (PKI) standard, 390 publication/subscription systems, 216 $pull operator, 44 $push operator, 41, 186, 195 PyMongo, 157",
      "content_length": 2245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "Q queries\n\n$where queries, 65 across multiple shards, 297 covered queries, 103 criteria for, 55-57 cursors for\n\navoiding large skips, 68 benefits of, 66 creating, 66 immortal cursors, 70 iterating through results, 66 limits, skips, and sorts, 67 sending queries to servers, 67\n\ndot notation and, 63 enabling efficient, 77 equality queries, 82 explain cursor method, 76 find method, 53-55 limitations, 55 multivalue queries, 83 OR queries, 56 overview of, 53 range queries, 55, 62, 82 scatter-gather (broadcast), 301 shape of, 84 targeted, 301 type-specific\n\narrays, 59-63 embedded documents, 63 null, 57 regular expressions, 58\n\nquery conditionals, 55 query documents, 15 query operators\n\ninefficient, 104 OR, 112 ranges, 105 query patterns defined, 77 indexes based on two or more keys, 81\n\nquestions and comments, xviii queueing, 432 --quiet option, 24\n\nR RAFT consensus protocol, 243 RAID (redundant array of independent disk),\n\n450\n\nRAM (random-access memory), 449 random numbers, creating, 70 read concern, 408 read privileges, 390 readahead, 457 readAnyDatabase privileges, 391 readConcern option, 221 reads\n\noptimizing, 219 sending to secondaries, 268-270\n\nreadWrite privileges, 390 readWriteAnyDatabase privileges, 391 reconfig command, 274 regular expressions\n\nprefix expressions, 58 querying with, 18, 58\n\nremove method, 34 removeShard command, 356 replaceOne method, 35 replica sets, components of\n\nbackups, 446 elections, 255 heartbeats, 253 member states, 254 rollbacks, 255-259 syncing, 249-253\n\nreplica sets, configuration of\n\nadding new set members, 273 changing hostnames, 273 changing set members, 273 configuration document, 272 creating larger sets, 274 creating replica sets, 272 forcing reconfiguration, 274 network configuration, 461 removing set members, 273 restrictions on changing, 273\n\nreplica sets, connecting to\n\nclient-to-replica set behavior, 261 custom replication guarantees, 265-268 purpose of replica sets, 262 retry strategy, 262 sending reads to secondaries, 268-270 waiting for replication on writes, 263\n\nreplica sets, setting up\n\nadding new members, 238 benefits of replica sets, 227 best practices, 228 changing configuration, 238-241\n\nIndex\n\n|\n\n485",
      "content_length": 2191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "checking for reconfiguration success, 238 common configurations, 242 configuration document, 230 designing sets, 241-243 how elections work, 243 key concepts, 238 listing and sending members, 230 localhost versus non-localhost servers, 231 majority of the set, 241 member configuration options\n\nbuilding indexes, 247 election arbiters, 246 hidden members, 245 priority, 244 specifying, 244 minority of the set, 242 modifying existing members, 240 networking considerations, 229 number of primaries, 243 observing replication, 233-238 removing members, 238 security considerations, 230 standalone server conversion, 231 test replica sets, 228 viewing status of, 231 replica sets, sharding and\n\nadding shards from replica sets, 306-310 initiating config servers as, 304 role of config servers, 304\n\nReplicaSetMonitor, 354 replication\n\nautomatic replication chaining, 279 budget approach to, 285 custom guarantees, 265-268 master/slave, 238 monitoring\n\nbuilding indexes, 283 calculating lag, 281 disabling chaining, 281 lag and oplog length, 433 obtaining current information, 276 replication graph, 279 replication loops, 280 resizing oplogs, 282 useful fields, 278 using logs for, 275\n\nobserving, 233-238 purpose of, 227, 249 replication protocol, 243\n\n486\n\n|\n\nIndex\n\nreplication thread, 375 statement-based, 406 versus sharding, 290 waiting for on writes, 263\n\nreplSet ROLLBACK, 258 replSetGetStatus command, 276 replSetReconfig command, 272 replSetSyncFrom command, 280 resident memory, 426 restore privileges, 391 retry logic, 200 retry-at-most-once strategy, 263 retryable writes option, 263 right-balanced indexes, 384 rollbacks\n\napplying operations to current primary, 258 avoiding, 263 defined, 257 failed, 259 loading documents into main collection,\n\n258\n\nmanipulating member votes, 258 ROLLBACK state, 258\n\nroot privileges, 391 rs global variable, 233 rs helper functions, 233, 272 rs.add command, 238, 273 rs.config(), 238, 245 rs.help(), 233 rs.initiate(), 230, 272, 304 rs.initiate(config), 233 rs.printReplicationInfo(), 281 rs.printSlaveReplicationInfo(), 281 rs.reconfig(), 240, 274 rs.remove command, 273 rs.status(), 231, 245, 276, 306 rs.syncFrom(), 280\n\nS sanity checks, 243 save shell helper, 48 scatter-gather (broadcast) queries, 301 schemas\n\ndesign considerations, 207 design patterns, 208-211 managing, 223 migrating, 35, 222 trade-off between efficient reads and writes,\n\n219",
      "content_length": 2399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "updating, 37\n\nSCRAM (Salted Challenge Response Authenti‐\n\ncation Mechanism), 389\n\nscripts\n\n.mongorc.js files for frequently loaded, 25 running with mongo shell, 23-25\n\nsecurity considerations\n\nauthentication mechanisms, 389 authorization, 390 config file options, 421 firewalls, 421 replica sets set up, 230 security.authorization setting, 392 tutorial\n\nbringing up replica set, 401 creating admin user, 402 establishing CA, 395-400 generating/signing client certificates, 401 generating/signing member certificates,\n\n400\n\nrestarting replica set, 403\n\nx.509 certificates, 392-395\n\nseed lists, 261 server administration backups, 437-447 deployment, 449-463 in sharded clusters, 356-359 monitoring, 425-436 production set up, 415-424\n\nserver discovery and monitoring (SDAM) spec‐\n\nification, 262\n\nservers\n\nbacking up, 438-445 config servers, 304 connecting to, 22 durability during failures, 405 forking server processes, 416 network configuration, 461 printing statistics about, 386 setting up for production, 449 shutting down, 420 standalone mode, 271, 356\n\n$set modifier, 37 $set operator, 215 $setOnInsert modifier, 48 setParameter command, 423 setProfilingLevel, 423 sh global variable, 294 sh.addShard() method, 309\n\nsh.addShardToZone(), 335 sh.help(), 294 sh.moveChunk(), 337 sh.status(), 294, 339 sh.status(true), 341 sh.stopBalancer(), 336 shard keys, choosing cardinality, 334 controlling data distribution, 334-337 distribution types\n\nascending shard keys, 320 location-based shard keys, 325 possibly types, 320 randomly distributed shard keys, 323\n\nhotspot shard keys, 360 initial steps, 319 limitations on shard keys, 328, 334 rules and guidelines, 334 shard key strategies\n\nfirehose strategy, 329 hashed shard keys, 327 hashed shard keys for GridFS, 328 multi-hotspot strategy, 330\n\nshardCollection command, 327 sharding, administration of\n\nbalancing data\n\nchanging chunk size, 361 jumbo chunks, 364-367 moving chunks, 362 turning balancer off, 360 refreshing configurations, 367 seeing current state\n\nconfiguration information, 341-348 status overview, 339 writeback listener, 375\n\nserver administration adding servers, 356 changing servers in shards, 356 removing shards, 356-359 tracking network connections\n\nconnection statistics, 348-354 limiting number of connections, 354\n\nsharding, basics of\n\nautosharding, 289, 337 benefits of sharding, 289 complexity of sharding, 290 compound shard keys, 313, 330 definition of sharding, 289 goals of sharding, 290\n\nIndex\n\n|\n\n487",
      "content_length": 2486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 510,
      "content": "manual sharding, 289, 336 number of concurrent migrations allowed,\n\n316\n\nprimary shards, 294 primary use case for, 292 processes involved, 304 querying, 297 removing shards, 356-359 sh global variable, 294 shard keys, 294-301\n\n(see also shard keys, choosing) sharding on single-machine clusters,\n\n291-301\n\nsharding versus replication, 290 uses for sharding, 303 when to shard, 303 sharding, configuration of\n\nbackups, 446 balancers, 316 change streams, 317 collations, 317 how MongoDB tracks cluster data, 311-315 network configuration, 461 processes involved, 304 starting the servers\n\nadding capacity, 310 adding shards from replica sets, 306-310 config servers, 304 mongo processes, 305 sharding data, 310 uses for sharding, 303 when to shard, 303 sharding.clusterRole, 308 ShardingTest class, 291 --shardsvr option, 308 shell (see mongo shell) show collections command, 24 show dbs command, 24 shutdown command, 420 SIGINT signal, 420 SIGTERM signal, 420 $size operator, 60, 185 skips\n\navoiding large, 68 finding random documents, 69 paginating results without, 69 skipping query results, 67\n\n$slice modifier, 42 $slice operator, 60, 185\n\n488\n\n|\n\nIndex\n\nslow application operations, 371, 376 --slowms, 379, 424 snapshots, 438-442 social graph data, 216 $sort modifier, 42 space, tracking disk usage, 433 split storms, 315 SSD (solid state drives), 449 SSL connections, 423 st.stop(), 301 staleness, handling, 253 standalone mode, 271, 356 statement-based replication, 406 stats function, 380 stats, printing, 386\n\n(see also application operations)\n\nstdout, 423 storage engine, default, 472 storage medium, choosing, 449 string type, 17 subcollections, 10 subset schema design pattern, 210 $sum operator, 186 swap space, allowance for, 452 syncing\n\ncloning and working sets, 252 handling staleness, 253 initial sync, 251 oplog size, 250 replication, 253 role of oplogs in, 249 system profiler, 376-379 system.indexes collection, 129\n\nT tailable cursors, 154 targeted queries, 301 TCP/IP wire protocol, 470 $text operator, 148 time-to-live (TTL) collections, removing old\n\ndata with, 219\n\ntime-to-live (TTL) indexes\n\ncreating, 155 uses for, 155 versus capped collections, 153\n\nTLS/SSL encryption, 423 --tlsMode, 423 top Unix utility, 386 transactions",
      "content_length": 2252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "ACID definition, 200 defined, 199 MongoDB versions and drivers supporting,\n\n199\n\ntuning transaction limits, 205-206 using, 200-205\n\ntransparent huge pages (THP), 458 transport encryption, 423 tree schema design pattern, 210 two-member-plus-arbiter scenario, 247\n\nU $unset modifier, 38 $unwind operator, 174-181 update operators\n\narray operators, 41-46 decrementing values, 40 idempotent, 215 incrementing values, 37, 39 removing field values, 38 setting field values, 37 uses for, 37\n\nupdateMany method, 47, 49 updateOne method, 16, 47 updates\n\natomic, 35 configuration document, 272 in-place updates, 251 methods available, 35 multiple documents, 49, 250 replacing documents, 35 returning updated documents, 49 using update operators, 37-46 using upserts, 46\n\nupserts, 46 use video command, 24 user-defined roles, 391 userAdmin privileges, 390 userAdminAnyDatabase privileges, 391 users, adding, 391\n\nV validate command, 410 validator option, 223 versioning scheme, 465 virtual memory, 426 virtualization\n\nbenefits and drawbacks of, 453 memory overcommitting, 453 mystery memory, 453 network disk I/O issues, 453\n\nvolume-level backup, 438\n\nW warnings, in startup banner, 417 WGS84 datum, 133 $where clauses, 65 Wil Wheaton effect, 218 wire protocol, 470 WiredTiger storage engine, 472 working set\n\ncalculating size of, 429 examples, 431\n\nwrite concern\n\nassuring propagation to majority of mem‐\n\nbers, 264\n\ndurability of transactions using, 409\n\nwrite-ahead logs (WAL), 405 writebacklistener commands, 375 writeConcern option, 221, 407 writes\n\nacknowledged, 376 optimizing, 219 waiting for replication on, 263\n\nX x.509 certificate authentication\n\nsupport for, 389 using, 392-395 XFS filesystem, 452\n\nIndex\n\n|\n\n489",
      "content_length": 1713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": "About the Authors\n\nShannon Bradshaw is VP of education at MongoDB. Shannon manages the Mon‐ goDB Documentation and MongoDB University teams. These teams develop and maintain the majority of MongoDB learning resources used by the MongoDB com‐ munity. Shannon holds a PhD in computer science from Northwestern University. Prior to MongoDB, Shannon was a computer science professor specializing in infor‐ mation systems and human-information interaction.\n\nEoin Brazil is a senior curriculum engineer at MongoDB. He works on online and instructor-led training products delivered through MongoDB University and previ‐ ously held various positions in the technical services support organization within MongoDB. Eoin holds a PhD and a MSc in computer science from the University of Limerick and a PgDip in technology commercialization from the National University of Ireland, Galway. Prior to MongoDB, he led teams in mobile services and in high- performance computing in the academic research sector.\n\nKristina Chodorow is a software engineer who worked on the MongoDB core for five years. She led MongoDB’s replica set development as well as writing the PHP and Perl drivers. She has given talks on MongoDB at meetups and conferences around the world and maintains a blog on technical topics at http://www.kchodorow.com. She currently works at Google.\n\nColophon\n\nThe animal on the cover of MongoDB: The Definitive Guide, Third Edition, is a mon‐ goose lemur, a member of a highly diverse group of primates endemic to Madagascar. Ancestral lemurs are believed to have inadvertently traveled to Madagascar from Africa (a trip of at least 350 miles) by raft some 65 million years ago. Freed from com‐ petition with other African species (such as monkeys and squirrels), lemurs adapted to fill a wide variety of ecological niches, branching into the almost 100 species known today. These animals’ otherworldly calls, nocturnal activity, and glowing eyes earned them their name, which comes from the lemures (specters) of Roman myth. Malagasy culture also associates lemurs with the supernatural, variously considering them the souls of ancestors, the source of taboo, or spirits bent on revenge. Some vil‐ lages identify a particular species of lemur as the ancestor of their group.\n\nMongoose lemurs (Eulemur mongoz) are medium-sized lemurs, about 12 to 18 inches long and 3 to 4 pounds. The bushy tail adds an additional 16 to 25 inches. Females and young lemurs have white beards, while males have red beards and cheeks. Mon‐ goose lemurs eat fruit and flowers and they act as pollinators for some plants; they are particularly fond of the nectar of the kapok tree. They may also eat leaves and insects.",
      "content_length": 2696,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "Mongoose lemurs inhabit the dry forests of northwestern Madagascar. One of the two species of lemur found outside of Madagascar, they also live in the Comoros Islands (where they are believed to have been introduced by humans). They have the unusual quality of being cathemeral (alternately wakeful during the day and at night), chang‐ ing their activity patterns to suit the wet and dry seasons. Mongoose lemurs are threatened by habitat loss and they are classified as a vulnerable species.\n\nMany of the animals on O’Reilly covers are endangered; all of them are important to the world.\n\nThe cover illustration is by Karen Montgomery, based on a black and white engraving from Lydekker’s Royal Natural History. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 514,
      "content": "There’s much more where this came from.\n\nExperience books, videos, live online training courses, and more from O’Reilly and our 200+ partners—all in one place.\n\nLearn more at oreilly.com/online-learning\n\n5 7 1\n\nc n\n\na d e M y\n\ni\n\nl l i\n\ne R O\n\n’\n\nf o k r a m e d a r t d e r e t s g e r\n\ni\n\na\n\ns\n\ny\n\nl l i\n\ne R O\n\n’\n\nc n\n\na d e M y\n\ni\n\nl l i\n\ne R O 9 1\n\n’\n\n0 2 ©",
      "content_length": 362,
      "extraction_method": "Unstructured"
    }
  ]
}