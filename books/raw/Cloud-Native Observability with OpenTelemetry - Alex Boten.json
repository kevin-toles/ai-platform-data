{
  "metadata": {
    "title": "Cloud-Native Observability with OpenTelemetry - Alex Boten",
    "author": "Alex Boten",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 308,
    "conversion_date": "2025-12-19T17:22:58.865218",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Cloud-Native Observability with OpenTelemetry - Alex Boten.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "The History and Concepts of Observability",
      "start_page": 29,
      "end_page": 49,
      "detection_method": "regex_chapter",
      "content": "Understanding cloud-native applications\n\nThe way applications are built and deployed has drastically changed in the past few years with the\n\nincreased adoption of the internet. An unprecedented increase in demand for services (for example,\n\nstreaming media, social networks, and online shopping) powered by software has raised expectations\n\nfor those services to be readily available. In addition, this increase in demand has fueled the need for\n\ndevelopers to be able to scale their applications quickly. Cloud providers, such as Microsoft, Google,\n\nand Amazon, offer infrastructure to run applications at the click of a button and at a fraction of the\n\ncost, and reduce the risk of deploying servers in traditional data centers. This enables developers to\n\nexperiment more freely and reach a wider audience. Alongside this infrastructure, these cloud\n\nproviders also offer managed services for databases, networking infrastructure, message queues, and\n\nmany other services that, in the past, organizations would control internally.\n\nOne of the advantages these cloud-based providers offer is freeing up organizations to focus on the\n\ncode that matters to their businesses. This replaces costly and time-consuming hardware\n\nimplementations, or operating services they lack expertise in. To take full advantage of cloud\n\nplatforms, developers started looking at how applications that were originally developed as monoliths\n\ncould be re-architected to take advantage of cloud platforms. The following are challenges that could\n\nbe encountered when deploying monoliths to a cloud provider:\n\nScaling a monolith is traditionally done by increasing the number of resources available to the monolith, also known as vertical\n\nscaling. Vertically scaling applications can only go as far as the largest available resource offered by a cloud provider.\n\nImproving the reliability of a monolith means deploying multiple instances to handle multiple failures, thus avoiding downtime.\n\nThis is also known as horizontal scaling. Depending on the size of the monolith, this could quickly ramp up costs. This can also be\n\nwasteful if not all components of the monolith need to be replicated.\n\nThe specific challenges of building applications on cloud platforms have led developers to\n\nincreasingly adopt a service-oriented architecture, or microservice architecture, that organizes\n\napplications as loosely coupled services, each with limited scope. The following figure shows a\n\nmonolith architecture on the left, where all the services in the application are tightly coupled and\n\noperate within the same boundary. In contrast, the microservices architecture on the right shows us\n\nthat the services are loosely coupled, and each service operates independently:\n\nFigure 1.1 – Monolith versus microservices architecture\n\nApplications built using microservices architecture provide developers with the ability to scale only\n\nthe components needed to handle the additional load, meaning horizontal scaling becomes a much\n\nmore attractive option. As it often does, a new architecture comes with its own set of trade-offs and\n\nchallenges. The following are some of the new challenges cloud-native architecture presents that did\n\nnot exist in traditional monolithic systems:\n\nLatency introduced where none existed before, causing applications to fail in unexpected ways.\n\nDependencies can and will fail, so applications must be built defensively to minimize cascading failures.\n\nManaging configuration and secrets across services is difficult.\n\nService orchestration becomes complex.\n\nWith this change in architecture, the scope of each application is reduced significantly, making it\n\neasier to understand the needs of scaling each component. However, the increased number of\n\nindependent services and added complexity also creates challenges for traditional operations (ops)\n\nteams, meaning organizations would also need to adapt.\n\nLooking at the shift to DevOps\n\nThe shift to microservices has, in turn, led to a shift in how development teams are organized. Instead\n\nof a single large team managing a monolithic application, many teams each manage their own\n\nmicroservices. In traditional software development, a software development team would normally\n\nhand off the software once it was deemed complete. The handoff would be to an operations team,\n\nwho would deploy the software and operate it in a production environment. As the number of\n\nservices and teams grew, organizations found themselves growing their operations teams to\n\nunmanageable sizes, and quite often, those teams were still unable to keep up with the demands of\n\nthe changing software.\n\nThis, in turn, led to an explosion of development teams that began the transition from the traditional\n\ndevelopment and operations organization toward the use of new hybrid DevOps teams. Using the\n\nDevOps approach, development teams write, test, build, package, deploy, and operate the code they\n\ndevelop. This ownership of the code through all stages of its life cycle empowers many developers\n\nand organizations to accelerate their feature development. This approach, of course, comes with\n\ndifferent challenges:\n\nIncreased dependencies across development teams mean it's possible that no one has a full picture of the entire application.\n\nKeeping track of changes across an organization can be difficult. This makes the answer to the \"what caused this outage?\"\n\nquestion more challenging to find.\n\nIndividual teams must become familiar with many more tools. This can lead to too much focus on the\n\ntools themselves, rather than on their purpose. The quick adoption of DevOps creates a new problem.\n\nWithout the right amount of visibility across the systems managed by an organization, teams are\n\nstruggling to identify the root causes of issues encountered. This can lead to longer and more frequent\n\noutages, severely impacting the health and happiness of people across organizations. Let's look at\n\nhow the methods of observing systems have evolved to adapt to this changing landscape.\n\nReviewing the history of observability\n\nIn many ways, being able to understand what a computer is doing is both fun and challenging when\n\nworking with software. The ability to understand how systems are behaving has gone through quite a\n\nfew iterations since the early 2000s. Many different markets have been created to solve this need,\n\nsuch as systems monitoring, log management, and application performance monitoring. As is often\n\nthe case, when new challenges come knocking, the doors of opportunity open to those willing to\n\ntackle those challenges. Over the same period, countless vendors and open source projects have\n\nsprung up to help people who are building and operating services in managing their systems. The\n\nterm observability, however, is a recent addition to the software industry and comes from control\n\ntheory.\n\nWikipedia (https://en.wikipedia.org/wiki/Observability) defines observability as:\n\n\"In control theory, observability is a measure of how well internal states of a system can be\n\ninferred from knowledge of its external outputs.\"\n\nObservability is an evolution of its predecessors, built on lessons learned through years of experience\n\nand trial and error. To better understand where observability is today, it's important to understand\n\nwhere some of the methods used today by cloud-native application developers come from, and how\n\nthey have changed over time. We'll start by looking at the following:\n\nCentralized logging\n\nMetrics and dashboards\n\nTracing and analysis\n\nCentralized logging\n\nOne of the first pieces of software a programmer writes when learning a new language is a form of\n\nobservability: \"Hello, World!\". Printing some text to the terminal is usually one of the quickest ways\n\nto provide users with feedback that things are working, and that's why \"Hello, World\" has been a\n\ntradition in computing since the late 1960s.\n\nOne of my favorite methods for debugging is still to add print statements across the code when things\n\naren't working. I've even used this method to troubleshoot an application distributed across multiple\n\nservers before, although I can't say it was my proudest moment, as it caused one of our services to go\n\ndown temporarily because of a typo in an unfamiliar editor. Print statements are great for simple\n\ndebugging, but unfortunately, this only scales so far.\n\nOnce an application is large enough or distributed across enough systems, searching through the logs\n\non individual machines is not practical. Applications can also run on ephemeral machines that may\n\nno longer be present when we need those logs. Combined, all of this created a need to make the logs\n\navailable in a central location for persistent storage and searchability, and thus centralized logging\n\nwas born.\n\nThere are many available vendors that provide a destination for logs, as well as features around\n\nsearching, and alerting based on those logs. There are also many open source projects that have tried\n\nto tackle the challenges of standardizing log formats, providing mechanisms for transport, and storing\n\nthe logs. The following are some of these projects:\n\nFluentd – https://www.fluentd.org\n\nLogstash – https://github.com/elastic/logstash\n\nApache Flume – https://flume.apache.org\n\nCentralized logging additionally provides the opportunity to produce metrics about the data across\n\nthe entire system.\n\nUsing metrics and dashboards\n\nMetrics are possibly the most well-known of the tools available in the observability space. Think of\n\nthe temperature in a thermometer, the speed on the odometer of a car, or the time on a watch. We\n\nhumans love measuring and quantifying things. From the early days of computing, being able to keep\n\ntrack of how resources were utilized was critical in ensuring that multi-user environments provided a\n\ngood user experience for all users of the system.\n\nNowadays, measuring application and system performance via the collection of metrics is common\n\npractice in software development. This data is converted into graphs to generate meaningful\n\nvisualizations for those in charge of monitoring the health of a system.\n\nThese metrics can also be used to configure alerting when certain thresholds have been reached, such\n\nas when an error rate becomes greater than an acceptable percentage. In certain environments,\n\nmetrics are used to automate workflows as a reaction to changes in the system, such as increasing the\n\nnumber of application instances or rolling back a bad deployment. As with logging, over time, many\n\nvendors and projects provided their own solutions to metrics, dashboards, monitoring, and alerting.\n\nSome of the open source projects that focus on metrics are as follows:\n\nPrometheus – https://prometheus.io\n\nStatsD – https://github.com/statsd/statsd\n\nGraphite – https://graphiteapp.org\n\nGrafana – https://github.com/grafana/grafana\n\nLet's now look at tracing and analysis.\n\nApplying tracing and analysis\n\nTracing applications means having the ability to run through the application code and ensure it's\n\ndoing what is expected. This can often, but not always, be achieved in development using a debugger\n\nsuch as GDB (https://www.gnu.org/software/gdb/) or PDB\n\n(https://docs.python.org/3/library/pdb.html) in Python. This becomes impossible when debugging an\n\napplication that is spread across multiple services on different hosts across a network. Researchers at\n\nGoogle published a white paper on a large-scale distributed tracing system built internally: Dapper\n\n(https://research.google/pubs/pub36356/). In this paper, they describe the challenges of distributed\n\nsystems, as well as the approach that was taken to address the problem. This research is the basis of\n\ndistributed tracing as it exists today. After the paper was published, several open source projects\n\nsprung up to provide users with the tools to trace and visualize applications using distributed tracing:\n\nOpenTracing – https://opentracing.io\n\nOpenCensus – https://opencensus.io\n\nZipkin – https://zipkin.io\n\nJaeger – https://www.jaegertracing.io\n\nAs you can imagine, with so many tools, it can be daunting to even know where to begin on the\n\njourney to making a system observable. Users and organizations must spend time and effort upfront\n\nto even get started. This can be challenging when other deadlines are looming. Not only that, but the\n\ntime investment needed to instrument an application can be significant depending on the complexity\n\nof the application, and the return on that investment sometimes isn't made clear until much later. The\n\ntime and money invested, as well as the expertise required, can make it difficult to change from one\n\ntool to another if the initial implementation no longer fits your needs as the system evolves.\n\nSuch a wide array of methods, tools, libraries, and standards has also caused fragmentation in the\n\nindustry and the open source community. This has led to libraries supporting one format or another.\n\nThis leaves it up to the user to fix any gaps within the environments themselves. This also means\n\nthere is effort required to maintain feature parity across different projects. All of this could be\n\naddressed by bringing the people working in these communities together.\n\nWith a better understanding of different tools at the disposal of application developers, their\n\nevolution, and their role, we can start to better appreciate the scope of what OpenTelemetry is trying\n\nto solve.\n\nUnderstanding the history of OpenTelemetry\n\nIn early 2019, the OpenTelemetry project was announced as a merger of two existing open source\n\nprojects: OpenTracing and OpenCensus. Although initially, the goal of this endeavor was to bring\n\nthese two projects together, its ambition to provide an observability framework for cloud-native\n\nsoftware goes much further than that. Since OpenTelemetry combines concepts of both OpenTracing\n\nand OpenCensus, let's first look at each of these projects individually. Please refer to the following\n\nTwitter link, which announced OpenTelemetry by combining both concepts:\n\nhttps://twitter.com/opencensusio/status/1111388599994318848.\n\nFigure 1.2 - Screenshot of the aforementioned tweet\n\nOpenTracing\n\nThe OpenTracing (https://opentracing.io) project, started in 2016, was focused on solving the\n\nproblem of increasing the adoption of distributed tracing as a means for users to better understand\n\ntheir systems. One of the challenges identified by the project was that adoption was difficult because\n\nof cost instrumentation and the lack of consistent quality instrumentation in third-party libraries.\n\nOpenTracing provided a specification for Application Programming Interface (APIs) to address\n\nthis problem. This API could be leveraged independently of the implementation that generated\n\ndistributed traces, therefore allowing application developers and library authors to embed calls to this\n\nAPI in their code. By default, the API would act as a no-op operation, meaning those calls wouldn't\n\ndo anything unless an implementation was configured.\n\nLet's see what this looks like in code. The call to an API to trace a specific piece of code resembles\n\nthe following example. You'll notice the code is accessing a global variable to obtain a Tracer via the\n\nglobal_tracer method. A Tracer in OpenTracing, and in OpenTelemetry (as we'll discuss later in\n\nChapter 2, OpenTelemetry Signals – Tracing, Metrics, and Logging, and Chapter 4, Distributed\n\nTracing – Tracing Code Execution), is a mechanism used to generate trace data. Using a globally\n\nconfigured tracer means that there's no configuration required in this instrumentation code – it can be\n\ndone completely separately. The next line starts aprimary building block, span. We'll discuss this\n\nfurther in Chapter 2, OpenTelemetry Signals – Tracing, Metrics, and Logging, but it is shown here to\n\ngive you an idea of how a Tracer is used in practice:\n\nimport opentracing\n\ntracer = opentracing.global_tracer()\n\nwith tracer.start_active_span('doWork'):\n\n# do work\n\nThe default no-op implementation meant that code could be instrumented without the authors having\n\nto make decisions about how the data would be generated or collected at instrumentation time. It also\n\nmeant that users of instrumented libraries, who didn't want to use distributed tracing in their\n\napplications, could still use the library without incurring a performance penalty by not configuring it.\n\nOn the other hand, users who wanted to configure distributed tracing could choose how this\n\ninformation would be generated. The users of these libraries and applications would choose a Tracer\n\nimplementation and configure it. To comply with the specification, a Tracer implementation only\n\nneeded to adhere to the API defined (https://github.com/opentracing/opentracing-\n\npython/blob/master/opentracing/tracer.py) , which includes the following methods:\n\nStart a new span.\n\nInject an existing span's context into a carrier.\n\nExtract an existing span from a carrier.\n\nAlong with the specification for this API, OpenTracing also provides semantic conventions. These\n\nconventions describe guidelines to improve the quality of the telemetry emitted by instrumenting.\n\nWe'll discuss semantic conventions further when exploring the concepts of OpenTelemetry.\n\nOpenCensus\n\nOpenCensus (https://opencensus.io) started as an internal project at Google, called Census, but was\n\nopen sourced and gained popularity with a wider community in 2017. The project provided libraries\n\nto make the generation and collection of both traces and metrics simpler for application developers. It\n\nalso provided the OpenCensus Collector, an agent run independently that acted as a destination for\n\ntelemetry from applications and could be configured to process the data before sending it along to\n\nbackends for storage and analysis. Telemetry being sent to the collector was transmitted using a wire\n\nformat specified by OpenCensus. The collector was an especially powerful component of\n\nOpenCensus. As shown in Figure 1.3, many applications could be configured to send data to a single\n\ndestination. That destination could then control the flow of the data without having to modify the\n\napplication code any further:\n\nFigure 1.3 – OpenCensus Collector data flow\n\nThe concepts of the API to support distributed tracing in OpenCensus were like those of\n\nOpenTracing's API. In contrast to OpenTracing, however, the project provided a tightly coupled API\n\nand Software Development Kit (SDK), meaning users could use OpenCensus without having to\n\ninstall and configure a separate implementation. Although this simplified the user experience for\n\napplication developers, it also meant that in certain languages, the authors of third-party libraries\n\nwanting to instrument their code would depend on the SDK and all its dependencies. As mentioned\n\nbefore, OpenCensus also provided an API to generate application metrics. It introduced several\n\nconcepts that would become influential in OpenTelemetry:\n\nMeasurement: This is the recorded output of a measure, or a generated metric point.\n\nMeasure: This is a defined metric to be recoded.\n\nAggregation: This describes how the measurements are aggregated.\n\nViews: These combine measures and aggregations to determine how the data should be exported.\n\nTo collect metrics from their applications, developers defined a measure instrument to record\n\nmeasurements, and then configured a view with an aggregation to emit the data to a backend. The\n\nsupported aggregations were count, distribution, sum, and last value.\n\nAs the two projects gained popularity, the pain for users only grew. The existence of both projects\n\nmeant that it was unclear for users what project they should rely on. Using both together was not\n\neasy. One of the core components of distributed tracing is the ability to propagate context between\n\nthe different applications in a distributed system, and this didn't work out of the box between the two\n\nprojects. If a user wanted to collect traces and metrics, they would have to use OpenCensus, but if\n\nthey wanted to use libraries that only supported OpenTracing, then they would have to use both –\n\nOpenTracing for distributed traces, and OpenCensus for metrics. It was a mess, and when there are\n\ntoo many standards, the way to solve all the problems is to invent a new standard!\n\nIt was a mess, and when there are too many standards, the way to solve all the problems is to invent a\n\nnew standard! The following XKCD comic captures the sentiment very aptly:\n\nFigure 1.4 – How standards proliferate comic (credit: XKCD, https://xkcd.com/927/)\n\nSometimes a new standard is a correct solution, especially when that solution:\n\nIs built using the lessons learned from its predecessors\n\nBrings together the communities behind other standards\n\nSupersedes two existing competing standards\n\nThe OpenCensus and OpenTracing organizers worked together to ensure the new standard would\n\nsupport a migration path for existing users of both communities, allowing the projects to eventually\n\nbecome deprecated. This would also make the lives of users easier by offering a single standard to\n\nuse when instrumenting applications. There was no longer any need to guess what project to use!\n\nObservability for cloud-native software\n\nOpenTelemetry aims to standardize how applications are instrumented and how telemetry data is\n\ngenerated, collected, and transmitted. It also aims to give users the tools necessary to correlate that\n\ntelemetry across systems, languages, and applications, to allow them to better understand their\n\nsoftware. One of the initial goals of the project involved ensuring all the functionality that was key to\n\nboth OpenCensus and OpenTracing users would become part of the new project. The focus on pre-\n\nexisting users also leads to the project organizers establishing a migration path to ease the transition\n\nfrom OpenTracing and OpenCensus to OpenTelemetry. To accomplish its lofty goals, OpenTelemetry\n\nprovides the following:\n\nAn open specification\n\nLanguage-specific APIs and SDKs\n\nInstrumentation libraries\n\nSemantic conventions\n\nAn agent to collect telemetry\n\nA protocol to organize, transmit, and receive the data\n\nThe project kicked off with the initial commit on May 1, 2019, and brought together the leaders from\n\nOpenCensus and OpenTracing. The project is governed by a governance committee that holds\n\nelections annually, with elected representatives serving on the committee for two-year terms. The\n\nproject also has a technical committee that oversees the specification, drives project-wide discussion,\n\nand reviews language-specific implementations. In addition, there are various special interest\n\ngroups (SIGs) in the project, focused on features or technologies supported by the project. Each\n\nlanguage implementation has its own SIG with independent maintainers and approvers managing\n\nseparate repositories with tools and processes tailored to the language. The initial work for the project\n\nwas heavily focused on the open specification. This provides guidance for the language-specific\n\nimplementations. Since its first commit, the project has received contributions from over 200\n\norganizations, including observability leaders and cloud providers, as well as end users of\n\nOpenTelemetry. At the time of writing, OpenTelemetry has implementations in 11 languages and 18\n\nspecial interest or working groups.\n\nSince the initial merger of OpenCensus and OpenTracing, communities from additional open source\n\nprojects have participated in OpenTelemetry efforts, including members of the Prometheus and\n\nOpenMetrics projects. Now that we have a better understanding of how OpenTelemetry was brought\n\nto life, let's take a deeper look at the concepts of the project.\n\nUnderstanding the concepts of OpenTelemetry\n\nOpenTelemetry is a large ecosystem. Before diving into the code, having a general understanding of\n\nthe concepts and terminology used in the project will help us. The project is composed of the\n\nfollowing:\n\nSignals\n\nPipelines\n\nResources\n\nContext propagation\n\nLet's look at each of these aspects.\n\nSignals\n\nWith its goal of providing an open specification for encompassing such a wide variety of telemetry\n\ndata, the OpenTelemetry project needed to agree on a term to organize the categories of concern.\n\nEventually, it was decided to call these signals. A signal can be thought of as a standalone component\n\nthat can be configured, providing value on its own. The community decided to align its work into\n\ndeliverables around these signals to deliver value to its users as soon as possible. The alignment of\n\nthe work and separation of concerns in terms of signals has allowed the community to focus its\n\nefforts. The tracing and baggage signals were released in early 2021, soon followed by the metrics\n\nsignal. Each signal in OpenTelemetry comes with the following:\n\nA set of specification documents providing guidance to implementors of the signal\n\nA data model expressing how the signal is to be represented in implementations\n\nAn API that can be used by application and library developers to instrument their code\n\nThe SDK needed to allow users to produce telemetry using the APIs\n\nSemantic conventions that can be used to get consistent, high-quality data\n\nInstrumentation libraries to simplify usage and adoption\n\nThe initial signals defined by OpenTelemetry were tracing, metrics, logging, and baggage. Signals\n\nare a core concept of OpenTelemetry and, as such, we will become quite familiar with them.\n\nSpecification\n\nOne of the most important aspects of OpenTelemetry is ensuring that users can expect a similar\n\nexperience regardless of the language they're using. This is accomplished by defining the standards\n\nfor what is expected of OpenTelemetry-compliant implementations in an open specification. The\n\nprocess used for writing the specification is flexible, but large new features or sections of\n\nfunctionality are often proposed by writing an OpenTelemetry Enhancement Proposal (OTEP).\n\nThe OTEP is submitted for review and is usually provided along with prototype code in multiple\n\nlanguages, to ensure the proposal isn't too language-specific. Once an OTEP is approved and merged,\n\nthe writing of the specification begins. The entire specification lives in a repository on GitHub\n\n(https://github.com/open-telemetry/opentelemetry-specification) and is open for anyone to contribute\n\nor review.\n\nData model\n\nThe data model defines the representation of the components that form a specific signal. It provides\n\nthe specifics of what fields each component must have and describes how all the components interact\n\nwith one another. This piece of the signal definition is particularly important to give clarity as to what\n\nuse cases the APIs and SDKs will support. The data model also explains to developers implementing\n\nthe standard how the data should behave. API\n\nInstrumenting applications can be quite expensive, depending on the size of your code base.\n\nProviding users with an API allows them to go through the process of instrumenting their code in a\n\nway that is vendor-agnostic. The API is decoupled from the code that generates the telemetry,\n\nallowing users the flexibility to swap out the underlying implementations as they see fit. This\n\ninterface can also be relied upon by library and frameworks authors, and only configured to emit\n\ntelemetry data by end users who wish to do so. A user who instruments their code by using the API\n\nand does not configure the SDK will not see any telemetry produced by design. SDK\n\nThe SDK does the bulk of the heavy lifting in OpenTelemetry. It implements the underlying system\n\nthat generates, aggregates, and transmits telemetry data. The SDK provides the controls to configure\n\nhow telemetry should be collected, where it should be transmitted, and how. Configuration of the\n\nSDK is supported via in-code configuration, as well as via environment variables defined in the\n\nspecification. As it is decoupled from the API, using the SDK provided by OpenTelemetry is an\n\noption for users, but it is not required. Users and vendors are free to implement their own SDKs if\n\ndoing so will better fit their needs. Semantic conventions\n\nProducing telemetry can be a daunting task, since you can call anything whatever you wish, but\n\ndoing so would make analyzing this data difficult. For example, if server A labels the duration of an\n\nhttp.server.duration request and server B labels it http.server.request_length, calculating the\n\ntotal duration of a request across both servers requires additional knowledge of this difference, and\n\nlikely additional operations. One way in which OpenTelemetry tries to make this a bit easier is by\n\noffering semantic conventions, or definitions for different types of applications and workloads to\n\nimprove the consistency of telemetry. Some of the types of applications or protocols that are covered\n\nby semantic conventions include the following:\n\nHTTP\n\nDatabase\n\nMessage queues\n\nFunction-as-a-Service (FaaS)\n\nRemote procedure calls (RPC)\n\nProcess metrics\n\nThe full list of semantic conventions is quite extensive and can be found in the specification\n\nrepository. The following figure shows a sample of the semantic convention for tracing database\n\nqueries:\n\nTable 1.1 – Database semantic conventions as defined in the OpenTelemetry specification (https://github.com/open-\n\ntelemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/database.md#connection-\n\nlevel-attributes)\n\nThe consistency of telemetry data reported will ultimately impact the user of that data's ability to use\n\nthis information. Semantic conventions provide both the guidelines of what telemetry should be\n\nreported, as well as how to identify this data. They provide a powerful tool for developers to learn\n\ntheir way around observability.\n\nInstrumentation libraries\n\nTo ensure users can get up and running quickly, instrumentation libraries are made available by\n\nOpenTelemetry SIGs in various languages. These libraries provide instrumentation for popular open\n\nsource projects and frameworks. For example, in Python, the instrumentation libraries include Flask,\n\nRequests, Django, and others. The mechanisms used to implement these libraries are language-\n\nspecific and may be used in combination with auto-instrumentation to provide users with telemetry\n\nwith close to zero code changes required. The instrumentation libraries are supported by the\n\nOpenTelemetry organization and adhere to semantic conventions.\n\nSignals represent the core of the telemetry data that is generated by instrumenting cloud-native\n\napplications. They can be used independently, but the real power of OpenTelemetry is to allow its\n\nusers to correlate data across signals to get a better understanding of their systems. Now that we have\n\na general understanding of what they are, let's look at the other concepts of OpenTelemetry.\n\nPipelines\n\nTo be useful, the telemetry data captured by each signal must eventually be exported to a data store,\n\nwhere storage and analysis can occur. To accomplish this, each signal implementation offers a series\n\nof mechanisms to generate, process, and transmit telemetry. We can think of this as a pipeline, as\n\nrepresented in the following figure:\n\nFigure 1.5 – Telemetry pipeline\n\nThe components in the telemetry pipeline are typically initialized early in the application code to\n\nensure no meaningful telemetry is missed.\n\nIMPORTANT NOTE\n\nIn many languages, the pipeline is configurable via environment variables. This will be explored further in Chapter 7,\n\nInstrumentation Libraries.\n\nOnce configured, the application generally only needs to interact with the generator to record\n\ntelemetry, and the pipeline will take care of collecting and sending the data. Let's look at each\n\ncomponent of the pipeline now.\n\nProviders\n\nThe starting point of the telemetry pipeline is the provider. A provider is a configurable factory that is\n\nused to give application code access to an entity used to generate telemetry data. Although multiple\n\nproviders may be configured within an application, a default global provider may also be made\n\navailable via the SDK. Providers should be configured early in the application code, prior to any\n\ntelemetry data being generated.\n\nTelemetry generators\n\nTo generate telemetry at different points in the code, the telemetry generator instantiated by a\n\nprovider is made available in the SDK. This generator is what most users will interact with through\n\nthe instrumentation of their application and the use of the API. Generators are named differently\n\ndepending on the signal: the tracing signal calls this a tracer, the metrics signal a meter. Their purpose\n\nis generally the same – to generate telemetry data. When instantiating a generator, applications and\n\ninstrumenting libraries must pass a name to the provider. Optionally, users can specify a version\n\nidentifier to the provider as well. This information will be used to provide additional information in\n\nthe telemetry data generated.\n\nProcessors\n\nOnce the telemetry data has been generated, processors provide the ability to further modify the\n\ncontents of the data. Processors may determine the frequency at which data should be processed or\n\nhow the data should be exported. When instantiating a generator, applications and instrumenting\n\nlibraries must pass a name to the provider. Optionally, users can specify a version identifier to the\n\nprovider as well.\n\nExporters\n\nThe last step before telemetry leaves the context of an application is to go through the exporter. The\n\njob of the exporter is to translate the internal data model of OpenTelemetry into the format that best\n\nmatches the configured exporter's understanding. Multiple export formats and protocols are\n\nsupported by the OpenTelemetry project:\n\nOpenTelemetry protocol\n\nConsole\n\nJaeger\n\nZipkin\n\nPrometheus\n\nOpenCensus\n\nThe pipeline allows telemetry data to be produced and emitted. We'll configure pipelines many times\n\nover the following chapters, and we'll see how the flexibility provided by the pipeline accommodates\n\nmany use cases.\n\nResources\n\nAt their most basic, resources can be thought of as a set of attributes that are applied to different\n\nsignals. Conceptually, a resource is used to identify the source of the telemetry data, whether a\n\nmachine, container, or function. This information can be used at the time of analysis to correlate\n\ndifferent events occurring in the same resource. Resource attributes are added to the telemetry data\n\nfrom signals at the export time before the data is emitted to a backend. Resources are typically\n\nconfigured at the start of an application and are associated with the providers. They tend to not\n\nchange throughout the lifetime of the application. Some typical resource attributes would include the\n\nfollowing:\n\nA unique name for the service: service.name\n\nThe version identifier for a service: service.version\n\nThe name of the host where the service is running: host.name\n\nAdditionally, the specification defines resource detectors to further enrich the data. Although\n\nresources can be set manually, resource detectors provide convenient mechanisms to automatically\n\npopulate environment-specific data. For example, the Google Cloud Platform (GCP) resource\n\ndetector (https://www.npmjs.com/package/@opentelemetry/resource-detector-gcp) interacts with the\n\nGoogle API to fill in the following data:\n\nTable 1.2 – GCP resource detector attributes\n\nResources and resource detectors adhere to semantic conventions. Resources are a key component in\n\nmaking telemetry data-rich, meaningful, and consistent across an application. Another important\n\naspect of ensuring the data is meaningful is context propagation.\n\nContext propagation\n\nOne area of observability that is particularly powerful and challenging is context propagation. A core\n\nconcept of distributed tracing, context propagation provides the ability to pass valuable contextual\n\ninformation between services that are separated by a logical boundary. Context propagation is what\n\nallows distributed tracing to tie requests together across multiple systems. OpenTelemetry, as\n\nOpenTracing did before it, has made this a core component of the project. In addition to tracing,\n\ncontext propagation allows for user-defined values (known as baggage) to be propagated. Baggage\n\ncan be used to annotate telemetry across signals.\n\nContext propagation defines a context API as part of the OpenTelemetry specification. This is\n\nindependent of the signals that may use it. Some languages already have built-in context\n\nmechanisms, such as the ContextVar module in Python 3.7+ and the context package in Go. The\n\nspecification recommends that the context API implementations leverage these existing mechanisms.\n\nOpenTelemetry also provides for the interface and implementation of mechanisms required to\n\npropagate context across boundaries. The following abbreviated code shows how two services, A and\n\nB, would use the context API to share context:\n\nfrom opentelemetry.propagate import extract, inject\n\nclass ServiceA:\n\ndef client_request():\n\ninject(headers, context=current_context)\n\n# make a request to ServiceB and pass in headers\n\nclass ServiceB:\n\ndef handle_request():\n\n# receive a request from ServiceA\n\ncontext = extract(headers)\n\nIn Figure 1.6, we can see a comparison between two requests from service A to service B. The top\n\nrequest is made without propagating the context, with the result that service B has neither the trace\n\ninformation nor the baggage that service A does. In the bottom request, this contextual data is\n\ninjected when service A makes a request to service B, and extracted by service B from the incoming\n\nrequest, ensuring service B now has access to the propagated data:\n\nFigure 1.6 – Request between service A and B with and without context propagation\n\nThe propagation of context we have demonstrated allows backends to tie the two sides of the request\n\ntogether, but it also allows service B to make use of the dataset in service A. The challenge with\n\ncontext propagation is that when it isn't working, it's hard to know why. The issue could be that the\n\ncontext isn't being propagated correctly due to configuration issues or possibly a networking\n\nproblem. This is a concept we'll revisit many times throughout the book.\n\nSummary\n\nIn this chapter, we've looked at what observability is, and the challenges it can solve as regards the\n\nuse of cloud-native applications. By exploring the different mechanisms available to generate\n\ntelemetry and improve the observability of applications, we were also able to gain an understanding\n\nof how the observability landscape has evolved, as well as where some challenges remain.\n\nExploring the history behind the OpenTelemetry project gave us an understanding of the origin of the\n\nproject and its goals. We then familiarized ourselves with the components forming tracing, metrics,\n\nlogging signals, and pipelines to give us the terminology and building blocks needed to start\n\nproducing telemetry using OpenTelemetry. This learning will allow us to tackle the first challenge of\n\nobservability – producing high-quality telemetry. Understanding resources and context propagation\n\nwill help us correlate events across services and signals to allow us to tackle the second challenge –\n\nconnecting the data to better understand systems.\n\nLet's now take a closer look at how this all works together in practice. In the next chapter, we will\n\ndive deeper into the concepts of distributed tracing, metrics, logs, and semantic conventions by\n\nlaunching a grocery store application instrumented with OpenTelemetry. We will then explore the\n\ntelemetry generated by this distributed system.\n\nChapter 2: OpenTelemetry Signals – Traces, Metrics, and Logs\n\nLearning how first to instrument an application can be a daunting task. There's a fair amount of\n\nterminology to understand before jumping into the code. I always find that seeing the finish line helps\n\nme get motivated and stay on track. This chapter's goal is to see what telemetry generated by\n\nOpenTelemetry looks like in practice while learning about the theory. In this chapter, we will dive\n\ninto the specifics of the following:\n\nDistributed tracing\n\nMetrics\n\nLogs\n\nProducing consistent quality data with semantic conventions\n\nTo help us get a more practical sense of the terminology and get comfortable with telemetry, we will\n\nlook at the data using various open source tools that can help us to query and visualize telemetry.\n\nTechnical requirements\n\nThis chapter will use an application that is already instrumented with OpenTelemetry, a grocery store,\n\nand several backends to walk through the different concepts of the signals. The environment we will\n\nbe launching relies on Docker Compose. The first step is to install Docker by following the\n\ninstallation instructions at https://docs.docker.com/get-docker/. Ensure Docker is running on your\n\nlocal system by using the following command:\n\n$ docker version\n\nClient:\n\nCloud integration: 1.0.14\n\nVersion: 20.10.6\n\nAPI version: 1.41\n\nGo version: go1.16.3 ...\n\nNext, let's ensure Compose is also installed by running the following command:\n\n$ docker compose version\n\nDocker Compose version 2.0.0-beta.1\n\nIMPORTANT NOTE",
      "page_number": 29
    },
    {
      "number": 2,
      "title": "OpenTelemetry Signals – Traces, Metrics, and Logs",
      "start_page": 50,
      "end_page": 75,
      "detection_method": "regex_chapter",
      "content": "Compose was added to the Docker client in more recent client versions. If the previous command returns an error, follow\n\nthe instructions on the Docker website (https://docs.docker.com/compose/install/) to install Compose. Alternatively, you\n\nmay want to try the docker-compose command to see if you already have an older version installed.\n\nThe following diagram shows an overview of the containers we are launching in the Docker\n\nenvironment to give you an idea of the components involved. The applications on the left are\n\nemitting telemetry processed by the Collector and forwarded to the telemetry backends. The diagram\n\nalso shows the port number exposed by each container for future reference.\n\nFigure 2.1 – Containers within Docker environment\n\nThis chapter briefly introduces the following open source projects that support the storage and\n\nvisualization of OpenTelemetry data:\n\nJaeger (https://www.jaegertracing.io)\n\nPrometheus (https://prometheus.io)\n\nLoki (https://github.com/grafana/loki)\n\nGrafana (https://grafana.com/oss/grafana/)\n\nI strongly recommend visiting the website for each project to gain familiarity with the tools as we\n\nwill use them throughout the chapter. Each of these tools will be revisited in Chapter 10, Configuring\n\nBackends. No prior knowledge of them is required to go through the examples, but they are pretty\n\nhelpful to have in your toolbelt. The configuration files necessary to launch the applications in this\n\nchapter are available in the companion repository (https://github.com/PacktPublishing/Cloud-Native-\n\nObservability) in the chapter2 directory. The following downloads the repository using the git\n\ncommand:\n\n$ git clone https://github.com/PacktPublishing/Cloud-Native-Observability\n\n$ cd Cloud-Native-Observability/chapter02\n\nTo bring up the applications and telemetry backends, run the following command:\n\n$ docker compose up\n\nWe will test the various tools to ensure each one is working as expected and is accessible from your\n\nbrowser. Let's start with Jaeger by accessing the following URL: http://localhost:16686. The\n\nfollowing screenshot shows the interface you should see:\n\nFigure 2.2 – The Jaeger web interface\n\nThe next backend this chapter will use for metrics is Prometheus; let's test the application by visiting\n\nhttp://localhost:9090. The following screenshot is a preview of the Prometheus web interface:\n\nFigure 2.3 – The Prometheus web interface\n\nThe last tool we need to ensure is working in our backend for logs is Loki. We will use Grafana as a\n\ndashboard to visualize the logs being emitted. Begin by visiting http://localhost:3000/explore to\n\nensure Grafana is up; you should be greeted by an interface like the one in Figure 2.4:\n\nFigure 2.4 – The Grafana web interface\n\nThe next application we will check is the OpenTelemetry Collector, which acts as the routing layer\n\nfor all the telemetry produced by the example application. The Collector exposes a health check\n\nendpoint discussed in Chapter 8, OpenTelemetry Collector. For now, it's enough to know that\n\naccessing the endpoint will give us information about the health of the Collector, using the following\n\ncurl command:\n\n$ curl localhost:13133\n\n{\"status\":\"Server available\",\"upSince\":\"2021-10-\n\n03T15:42:02.7345149Z\",\"uptime\":\"9.3414709s\"}\n\nLastly, let's ensure the containers forming the grocery store demo application are running. To do this,\n\nwe use curl again in the following commands to access an endpoint in the applications that returns a\n\nstatus showing the application's health. It's possible to use any other tool capable of making HTTP\n\nrequests, including the browser, to accomplish this. The following checks the status of the grocery\n\nstore:\n\n$ curl localhost:5000/healthcheck\n\n{\n\n\"service\": \"grocery-store\",\n\n\"status\": \"ok\"\n\n}\n\nThe same command can be used to check the status of the inventory application by specifying port\n\n5001:\n\n$ curl localhost:5001/healthcheck\n\n{\n\n\"service\": \"inventory\",\n\n\"status\": \"ok\"\n\n}\n\nThe shopper application represents a client application and does not provide any endpoint to expose\n\nits health status. Instead, we can look at the logs emitted by the application to get a sense of whether\n\nit's doing the right thing or not. The following uses the docker logs command to look at the output\n\nfrom the application. Although it may vary slightly, the output should contain information about the\n\nshopper connecting to the grocery store:\n\n$ docker logs -n 2 shopper\n\nDEBUG:urllib3.connectionpool:http://grocery-store:5000 \"GET /products HTTP/1.1\" 200 107\n\nINFO:shopper:message=\"add orange to cart\"\n\nThe same docker logs command can be used on any of the other containers if you're interested in\n\nseeing more information about them. Once you're done with the chapter, you can clean up all the\n\ncontainers by running stop to terminate the running containers, and rm to delete the containers\n\nthemselves:\n\n$ docker compose stop\n\n$ docker compose rm\n\nAll the examples in this chapter will expect that the Docker Compose environment is already up and\n\nrunning. When in doubt, come back to this technical requirement section to ensure your environment\n\nis still running as expected. Now, let's see what these OpenTelemetry signals are all about, starting\n\nwith traces.\n\nTraces\n\nDistributed tracing is the foundation behind the tracing signal of OpenTelemetry. A distributed trace\n\nis a series of event data generated at various points throughout a system tied together via a unique\n\nidentifier. This identifier is propagated across all components responsible for any operation required\n\nto complete the request, allowing each operation to associate the event data to the originating request.\n\nThe following diagram gives us a simplified example of what a single request may look like when\n\nordering groceries through an app:\n\nFigure 2.5 – Example request through a simplified ordering system\n\nEach trace represents a unique request through a system that can be either synchronous or\n\nasynchronous. Synchronous requests occur in sequence with each unit of work completed before\n\ncontinuing. An example of a synchronous request may be of a client application making a call to a\n\nserver and waiting or blocking until a response is returned before proceeding. In contrast,\n\nasynchronous requests can initiate a series of operations that can occur simultaneously and\n\nindependently. An example of an asynchronous request is a server application submitting messages to\n\na queue or a process that batches operations. Each operation recorded in a trace is represented by a\n\nspan, a single unit of work done in the system. Let's see what the specifics of the data captured in the\n\ntrace look like.\n\nAnatomy of a trace\n\nThe definition of what constitutes a trace has evolved as various systems have been developed to\n\nsupport distributed tracing. The World Wide Web Consortium (W3C), an international group that\n\ncollaborates to move the web forward, assembled a working group in 2017 to produce a definition for\n\ntracing. In February 2020, the first version of the Trace Context specification was completed, with\n\nits details available on the W3C's website (https://www.w3.org/TR/trace-context-1/). OpenTelemetry\n\nfollows the recommendation from the W3C in its definition of the SpanContext, which contains\n\ninformation about the trace and must be propagated throughout the system. The elements of a trace\n\navailable within a span context include the following:\n\nA unique identifier, referred to as a trace ID, identifies the request through the system.\n\nA second identifier, the span ID, is associated with the span that last interacted with the context. This may also be referred to as\n\nthe parent identifier.\n\nTrace flags include additional information about the trace, such as the sampling decision and trace level.\n\nVendor-specific information is carried forward using a Trace state field. This allows individual vendors to propagate information\n\nnecessary for their systems to interpret the tracing data. For example, if a vendor needs an additional identifier to be present in the\n\ntrace information, this identifier could be inserted as vendorA=123456 in the trace state field. Other vendors would add their\n\nown as needed, allowing traces to be shared across vendors.\n\nA span can represent a method call or a subset of the code being called within a method. Multiple\n\nspans within a trace are linked together in a parent-child relationship, with each child span containing\n\ninformation about its parent. The first span in a trace is called the root span and is identified because\n\nit does not have a parent span identifier. The following shows a typical visualization of a trace and\n\nthe spans associated with it. The horizontal axis indicates the duration of the entire trace operation.\n\nThe vertical axis shows the order in which the operations captured by spans took place, starting with\n\nthe first operation at the top:\n\nFigure 2.6 – Visual representation of a trace\n\nLet's look closer at a trace by bringing up a sample generated from the telemetry produced by the\n\ngrocery store application. Access the Jaeger web interface by opening a browser to the following\n\nURL: http://localhost:16686/.\n\nSearch for a trace by selecting a service from the drop-down and clicking the Find Traces button.\n\nThe following screenshot shows the traces found for the shopper service:\n\nFigure 2.7 – Traces search result\n\nTo obtain details about a specific trace, select one of the search results by clicking on the row. The\n\nfollowing screenshot, Figure 2.8, shows the details of the trace generated by a request through the\n\ngrocery store applications. It includes the following:\n\n1. The unique trace ID for this request. In OpenTelemetry, this is represented by a 128-bit integer. It's worth noting that other\n\nsystems may represent this as a 64-bit integer. The integer is encoded into a string containing hexadecimal characters in many\n\nsystems.\n\n2. The start time for the request.\n\n3. The total duration of the request through the system is calculated by subtracting the time the root span is finished from its start\n\ntime.\n\n4. A count of the number of services included in this request.\n\n5. A count of spans recorded in this request is shown in Total Spans.\n\n6. A hierarchical view of the spans in the trace.\n\nFigure 2.8 – A trace in Jaeger\n\nThe preceding screenshot gives us an immediate sense of where time may be spent as the system\n\nprocesses the request. It also provides us with a glimpse into what the underlying code may look like\n\nwithout ever opening an editor. Additional details are captured in spans; let's look at those now.\n\nDetails of a span\n\nAs mentioned previously, the work captured in a trace is broken into separate units or operations,\n\neach represented by a span. The span is a data structure containing the following information:\n\nA unique identifier\n\nA parent span identifier\n\nA name describing the work being recorded\n\nA start and end time\n\nIn OpenTelemetry, a span identifier is represented by a 64-bit integer. The start and end times are\n\nused to calculate the operation's duration. Additionally, spans can contain metadata in the form of\n\nkey-value pairs. In the case of Jaeger and Zipkin, these pairs are referred to as tags, whereas\n\nOpenTelemetry calls them attributes. The goal is to enrich the data provided with the additional\n\ncontext in both cases.\n\nLook for the following details in Figure 2.9, which shows the detailed view of a specific span as\n\nshown in Jaeger:\n\n1. The name identifies the operation represented by this span. In this case, /inventory is the operation's name.\n\n2. SpanID is the unique 64-bit identifier represented in hex-encoded formatting.\n\n3. Start Time is when the operation recorded its start time relative to the start of the request. In the case shown here, the operation\n\nstarted 8.36 milliseconds after the beginning of the request.\n\n4. Duration is the time it took for the operation to complete and is calculated using the start and end times recorded in the span.\n\n5. The Service name identifies the application that triggered the operation and recorded the telemetry.\n\n6. Tags represent additional information about the operation being recorded.\n\n7. Process shows information about the application or process fulfilling the requested operation.\n\nFigure 2.9 – Span details\n\nMany of the tags captured in the span shown previously rely on semantic conventions, which will be\n\ndiscussed further in this chapter.\n\nAdditional considerations\n\nWhen producing distributed traces in a system, it's worth considering the additional visibility's\n\ntradeoffs. Generating tracing information can potentially incur performance overhead at the\n\napplication level. It can result in added latency if tracing information is gathered and transmitted\n\ninline. There is also memory overhead to consider, as collecting information inevitably allocates\n\nresources. These concerns can be largely mitigated using configuration available in OpenTelemetry,\n\nas we'll see in Chapter 4, Distributed Tracing – Tracing Code Execution.\n\nDepending on where the data is sent, additional costs, such as bandwidth or storage, can also become\n\na factor. One of the ways to mitigate these costs is to reduce the amount of data produced by\n\nsampling only a certain amount of the data. We will dive deeper into sampling in Chapter 12,\n\nSampling.\n\nAnother challenging aspect of producing distributed tracing data is ensuring that all the services\n\ncorrectly propagate the context. Failing to propagate the trace ID across the system means that\n\nrequests will be broken into multiple traces, making them difficult to use or not helpful at all.\n\nThe last thing to consider is the effort required to instrument an application correctly. This is a non-\n\ntrivial amount of effort, but as we'll see in future chapters, OpenTelemetry provides instrumentation\n\nlibraries to make this easier.\n\nNow that we have a deeper understanding of traces, let's look at metrics.\n\nMetrics\n\nJust as distributed traces do, metrics provide information about the state of a running system to\n\ndevelopers and operators. The data collected via metrics can be aggregated over time to identify\n\ntrends and patterns in applications graphed through various tools and visualizations. The term metrics\n\nhas a broad range of applications as they can capture low-level system metrics such as CPU cycles,\n\nor higher-level details such as the number of blue sweaters sold today. These examples would be\n\nhelpful to different groups in an organization.\n\nAdditionally, metrics are critical to monitoring the health of an application and deciding when an on-\n\ncall engineer should be alerted. They form the basis of service level indicators (SLIs)\n\n(https://en.wikipedia.org/wiki/Service_level_indicator) that measure the performance of an\n\napplication. These indicators are then used to set service level objectives (SLOs)\n\n(https://en.wikipedia.org/wiki/Service-level_objective) that organizations use to calculate error\n\nbudgets.\n\nIMPORTANT NOTE\n\nSLIs, SLOs, and service level agreements (SLAs) are essential topics in production environments where third-party\n\ndependencies can impact the availability of your service. There are entire books dedicated to the issue that we will not\n\ncover here. The Google site reliability engineering (SRE) book is a great resource for this: https://sre.google/sre-\n\nbook/service-level-objectives/.\n\nThe metrics signal of OpenTelemetry combines various existing open source formats into a unified\n\ndata model. Primarily, it looks to OpenMetrics, StatsD, and Prometheus for existing definitions,\n\nrequirements, and usage, wanting to ensure the use-cases of each of those communities are\n\nunderstood and addressed by the new standard.\n\nAnatomy of a metric\n\nJust about anything can be a metric; record a value at a given time, and you have yourself a metric.\n\nThe common fields a metric contains include the following:\n\nA name identifies the metric being recorded.\n\nA data point value may be an integer or a floating-point value. Note that in the case of a histogram or a summary, there is more\n\nthan one value associated with the metric.\n\nAdditional dimension information about the metric. The representation of these dimensions varies depending on the metrics\n\nbackend. In Prometheus, these dimensions are represented by labels, whereas in StatsD, it is common to add a prefix in the\n\nmetric's name. In OpenTelemetry, dimensions are added to metrics via attributes.\n\nLet's look at data produced by metrics sent from the demo application. Access the Prometheus\n\ninterface via a browser and the following URL: http://localhost:9090. The user interface for\n\nPrometheus allows us to query the time-series database by using the metric's name. The following\n\nscreenshot contains a table showing the value of the request_counter metric. Look for the following\n\ndetails in the resulting table:\n\n1. The name of the metric, in this case, request_counter.\n\n2. The dimensions recorded for this metric are displayed in curly braces as key-value pairs with the key emboldened. In the example\n\nshown, the service_name label caused two different metrics to be recorded, one for the shopper service and another for the\n\nstore service.\n\n3. A reported value, in this example, is an integer. This value may be the last received or a calculated current value depending on the\n\nmetric type.\n\nFigure 2.10 – Table view of metric in Prometheus\n\nThe table view shows the current value as cumulative. An alternative representation of the recorded\n\nmetric is shown in the following figure. As the data received by Prometheus is stored over time, a\n\nline graph can be generated. Click the Graph tab of the interface to see what the data in a chart looks\n\nlike:\n\nFigure 2.11 – Graph view of the same metric in Prometheus\n\nBy looking at the values for the metric over time, we can deduce additional information about the\n\nservice, for example, its start time or trends in its usage. Visualizing metrics also provides\n\nopportunities to identify anomalies.\n\nData point types\n\nA metric is a more generic term that encapsulates different measurements that can be used to\n\nrepresent a wide array of information. As such, the data is captured using various data point types.\n\nThe following diagram compares different kinds of data points that can be captured within a metric:\n\nFigure 2.12 – Comparison of counter, gauge, histogram, and summary data points\n\nEach data point type can be used in different scenarios and has slightly different meanings. It's worth\n\nnoting that even though competing standards provide support for types using the same name, their\n\ndefinition may vary. For example, a counter in StatsD\n\n(https://github.com/statsd/statsd/blob/master/docs/metric_types.md#counting) resets every time the\n\nvalue has been flushed, whereas, in Prometheus\n\n(https://prometheus.io/docs/concepts/metric_types/#counter), it keeps its cumulative value until the\n\nprocess recording the counter is restarted. The following definitions describe how data point types are\n\nrepresented in the OpenTelemetry specification:\n\nA sum measures incremental changes to a recorded value. This incremental change is either monotonic or non-monotonic and\n\nmust be associated with an aggregation temporality. The temporality can be either of the following:\n\n1. Delta aggregation: The reported values contain the change in value from its previous recording.\n\n2. Cumulative aggregation: The value reported includes the previously reported sum in addition to the delta being\n\nreported.\n\nIMPORTANT NOTE\n\nA cumulative sum will reset when an application restarts. This is useful to identify an event in the application but may\n\nbe surprising if it's not accounted for.\n\nThe following diagram shows an example of a sum counter reporting the number of visits over a\n\nperiod of time. The table on the right-hand side shows what values are to be expected depending on\n\nthe type of temporal aggregation chosen:\n\nFigure 2.13 – Sum showing delta and cumulative aggregation values\n\nA sum data point also includes the time window for calculating the sum.\n\nA gauge represents non-monotonic values that only measure the last or current known value at observation. This likely means\n\nsome information is missing, but it may not be relevant. For example, the following diagram represents temperatures recorded at\n\nan hourly interval. More specific data points could provide greater granularity as to the rise and fall of the temperature. These\n\nincremental changes in the temperature may not be required if the goal is to observe trends over weeks or months.\n\nFigure 2.14 – Gauge values recorded\n\nUnlike gauge definitions in other specifications, a gauge in OpenTelemetry is never incremented or\n\ndecremented; it is only ever set to the value being recorded. A timestamp of the observation time\n\nmust be included with the data point.\n\nA histogram data point provides a compressed view into a more significant number of data points by grouping the data into a\n\ndistribution and summarizing the data, rather than reporting individual measurements for every detail represented. The following\n\ndiagram shows sample histogram data points representing a distribution of response durations.\n\nFigure 2.15 – Histogram data points\n\nLike sums, histograms also support a delta or a cumulative aggregation and must contain a time\n\nwindow for the recorded observation. Note that in the case of cumulative aggregation, the data points\n\ncaptured in the distribution will continue to accumulate with each recording.\n\nThe summary data type provides a similar capability to histograms, but it's specifically tailored around providing quantiles of a\n\ndistribution. A quantile, sometimes also referred to as percentile, is a fraction between zero and one, representing a percentage of\n\nthe total number of values recorded that falls under a certain threshold. For example, consider the following 10 response times in\n\nmilliseconds: 1.1, 2.9, 7.5, 8.3, 9, 10, 10, 10, 10, 25. The 0.9-quantile, or the 90th percentile, equals 10 milliseconds.\n\nFigure 2.16 – Summary data points\n\nA summary is somewhat similar to a histogram, where the histogram contains a maximum and a\n\nminimum value; the summary includes a 1.0-quantile and 0.0-quantile to represent the same\n\ninformation. The 0.5-quantile, also known as median, is often expressed in the summary. For a\n\nsummary data point, the quantile calculations happen in the producer of the telemetry, which can\n\nbecome expensive for applications. OpenTelemetry supports summaries to provide interoperability\n\nwith OpenMetrics (https://openmetrics.io) and Prometheus and prefers the usage of a histogram,\n\nwhich moves the calculation of quantiles to the receiver of the telemetry. The following screenshot\n\nshows histogram values recorded by the inventory service for the\n\nhttp_request_duration_milliseconds_bucket metric stored in Prometheus. The data shown\n\nrepresents requests grouped into buckets. Each bucket represents the request duration in milliseconds:\n\nFigure 2.17 – Histogram value in Prometheus\n\nThe count of requests per bucket can then calculate quantiles for further analysis. Now that we're\n\nfamiliar with the different types of metric data points, let's see how metrics can be combined with\n\ntracing to provide additional insights.\n\nExemplars\n\nMetrics are often helpful on their own, but when correlated with tracing information, they provide\n\nmuch more context and depth on the events occurring in a system. Exemplars offer a tool to\n\naccomplish this in OpenTelemetry by enabling a metric to contain information about an active span.\n\nData points defined in OpenTelemetry include an exemplar field as part of their definition. This field\n\ncontains the following:\n\nA trace ID of the current span in progress\n\nThe span ID of the current span in progress\n\nA timestamp of the event measured\n\nA set of attributes associated with the exemplar\n\nThe value being recorded\n\nThe direct correlation that exemplars provide replaces the guesswork that involves cobbling metrics\n\nand traces with timestamps today. Although exemplars are already defined in the stable metrics\n\nsection of the OpenTelemetry protocol, the implementation of exemplars is still under active\n\ndevelopment at the time of writing.\n\nAdditional considerations\n\nA concern that often arises with any telemetry is the importance of managing cardinality. Cardinality\n\nrefers to the uniqueness of a value in a set. While counting cars in a parking lot, the number of wheels\n\nwill likely offer a meager value and low cardinality result as most cars have four wheels. The color,\n\nmake, and model of cars produces higher cardinality. The license plate, or vehicle identification\n\nnumber, results in the highest cardinality, providing the most valuable data to know in an event\n\nconcerning a specific vehicle. For example, if the lights have been left on and the owners should be\n\nnotified, calling out for the person with a four-wheeled car won't work nearly as well as calling for a\n\nspecific license plate. However, the count of cars with specific license plates will always be one,\n\nmaking the counter itself somewhat useless.\n\nOne of the challenges with high-cardinality data is the increased storage cost. Specifically, in the case\n\nof metrics, it's possible to significantly increase the number of metrics being produced and stored by\n\nadding a single attribute or label. Suppose an application creating a counter for each request\n\nprocessed uses a unique identifier as the metric's name. In that case, the producer or receiver may\n\ntranslate this into a unique time series for each request. This results in a sudden and unexpected\n\nincrease in load in the system. This is sometimes referred to as cardinality explosion.\n\nWhen choosing attributes associated with produced metrics, it's essential to consider the scale of the\n\nservices and infrastructure producing the telemetry. Some questions to keep in mind are as follows:\n\nWill scaling components of the system increase the number of metrics in a way that is understood? When a system scales, the last\n\nthing anyone wants is for an unexpected spike in metrics to cause outages.\n\nAre any attributes specific to instances of an application? This could cause problems in the case of a crashing application.\n\nUsing labels with finite and knowable values (for example, countries rather than street names) may\n\nbe preferable depending on how the data is stored. When choosing a solution, understanding the\n\nstorage model and limits of the telemetry backend must also be considered.\n\nLogs\n\nAlthough logs have evolved, what constitutes a log is quite broad. Also known as log files, a log is a\n\nrecord of events written to output. Traditionally, logs would be written to a file on disk, searching\n\nthrough as needed. A more recent practice is to emit logs to remote services using the network. This\n\nprovides long-term storage for the data in a location and improves searchability and aggregation.\n\nAnatomy of a log\n\nMany applications define their formats for what constitutes a log. There are several existing standard\n\nformats. An example includes the Common Log Format often used by web servers. It's challenging to\n\nidentify commonalities across formats, but at the very least, a log should consist of the following:\n\nA timestamp recording the time of the event\n\nThe message or payload representing the event\n\nThis message can take many forms and include various application-specific information. In the case\n\nof structured logging, the log is formatted as a series of key-value pairs to simplify identifying the\n\ndifferent fields contained within the log. Other formats record logs in a specific order with a\n\nseparating character instead. The following shows an example log emitted by the standard formatter\n\nin Flask, a Python web framework that shows the following:\n\nA timestamp is enclosed in square brackets.\n\nA space-delimited set of elements forms the message logged, including the client IP, the HTTP method used to make a request, the\n\nrequest's path, the protocol version, and the response code:\n\n172.20.0.9 - - [11/Oct/2021 18:50:25] \"GET /inventory HTTP/1.1\" 200 -\n\nThe previous sample is an example of the Common Log Format mentioned earlier. The same log may\n\nlook something like this as a structured log encoded as JSON:\n\n{\n\n\"host\": \"172.20.0.9\",\n\n\"date\": \"11/Oct/2021 18:50:25\",\n\n\"method\": \"GET\",\n\n\"path\": \"/inventory\",\n\n\"protocol\": \"HTTP/1.1\",\n\n\"status\": 200\n\n}\n\nAs you can see with structured logs, identifying the information is more intuitive if you're not already\n\nfamiliar with the type of logs produced. Let's see what logs our demo application produces by\n\nlooking at the Grafana interface, at http://localhost:3000/explore.\n\nThis brings us to the explore view, which allows us to search through telemetry generated by the\n\ndemo application. Ensure that Loki is selected from the data source drop-down in the top left corner.\n\nFilter the logs using the {job=\"shopper\"} query to retrieve all the logs generated by the shopper\n\napplication. The following screenshot shows a log emitted to the Loki backend, which contains the\n\nfollowing:\n\n1. The name of the application is under the job label.\n\n2. A timestamp of the log is shown both as a timestamp and as a nanosecond value.\n\n3. The body of the logged event.\n\n4. Additional labels and values associated with the event.\n\nFigure 2.18 – Log shown in Grafana\n\nNow that we can search for logs, let's see how we can combine the information provided by logs with\n\nother signals via correlation to give us more context.\n\nCorrelating logs\n\nIn the same way that information provided by metrics can be augmented by combining them with\n\nother signals, logs too can provide more context by embedding tracing information. As we'll see in\n\nChapter 6, Logging - Capturing Events, one of the goals of the logging signal in OpenTelemetry is to\n\nprovide correlation capability to already existing logging libraries. Logs recorded via OpenTelemetry\n\ncontain the trace ID and span ID for any span active at the time of the event. The following\n\nscreenshot shows the details of a log record containing the traceID and spanID attributes:\n\nFigure 2.19 – Log containing trace ID\n\nUsing these attributes can then reveal the specific request that triggered this event. The following\n\nscreenshot demonstrates what the corresponding trace looks like in Jaeger. If you'd like to try for\n\nyourself, copy the traceID attribute into the Lookup by Trace ID field to search for the trace:\n\nFigure 2.20 – Corresponding trace in Jaeger\n\nThe correlation demonstrated in the previous example makes exploring events faster and less error-\n\nprone. As we will see in Chapter 6, Logging - Capturing Events, the OpenTelemetry specification\n\nprovides recommendations for what information should be included in logs being emitted. It also\n\nprovides guidelines for how existing formats can map their values with OpenTelemetry.\n\nAdditional considerations\n\nThe free form of traditional logs makes them incredibly convenient to use without considering their\n\nstructure. If you want to add any data to the logs, just call a function and print anything you'd like;\n\nit'll be great. However, this can pose some challenges. One of these challenges is the opportunity for\n\nleaking potentially private information into the logs and transmitting it to a centralized logging\n\nplatform. This problem applies to all telemetry, but it's particularly easy to do with logs. This is\n\nespecially true when logs contain debugging information, which may include data structures with\n\npasswords fields or private keys. It's good to review any logging calls in the code to ensure the\n\nlogged data does not contain information that should not be logged.\n\nLogs can also be overly verbose, which can cause unexpected volumes to be generated. This may\n\nmake sifting through the logs for useful information difficult, if not impossible, depending on the size\n\nof the environment. It can also lead to unanticipated costs when using centralized logging platforms.\n\nSpecific libraries or frameworks generate much debugging information. Ensuring the correct severity\n\nlevel is configured goes a long towards addressing this concern. However, it's hard to predict just\n\nhow much data will be needed upfront. On more than one occasion, I've responded to alerts in the\n\nmiddle of the night, wishing for a more verbose log level to be configured.\n\nSemantic conventions\n\nHigh-quality telemetry allows the data consumer to find answers to questions when needed.\n\nSometimes critical operations can lack instrumentation causing blind spots in the observability of a\n\nsystem. Other times, the processes are instrumented, but the data is not rich enough to be helpful. The\n\nOpenTelemetry project attempts to solve this through semantic conventions defined in the\n\nspecification. These conventions cover the following:\n\nAttributes that should be present for traces, metrics, and logs.\n\nResource attribute definitions for various types of workloads, including hosts, containers, and functions. The resource attributes\n\ndescribed by the specification also include characteristics specific to multiple popular cloud platforms.\n\nRecommendations for what telemetry should be emitted by components participating in various scenarios such as messaging\n\nsystems, client-server applications, and database interactions.\n\nThese semantic conventions help ensure that the data generated when following the OpenTelemetry\n\nspecification is consistent. This simplifies the work of folks instrumenting applications or libraries by\n\nproviding guidelines for what should be instrumented and how. It also means that anyone analyzing\n\ntelemetry produced by standard-compliant code can understand the meaning of the data by\n\nreferencing the specification for additional information.\n\nFollowing semantic conventions recommendations from a specification in a Markdown document\n\ncan be challenging when writing code. Thankfully, OpenTelemetry also provides some tools to help. Adopting semantic conventions\n\nSemantic conventions are great, but it makes sense to turn the recommendations into code to make it\n\npractical for developers to use them. The OpenTelemetry specification repository provides a folder\n\nthat contains the semantic conventions described as YAML for this specific reason\n\n(https://github.com/open-telemetry/opentelemetry-specification/tree/main/semantic_conventions).\n\nThese are combined with the semantic conventions generator (https://github.com/open-\n\ntelemetry/build-tools/blob/v0.7.0/semantic-conventions/) to produce code in various languages. This\n\ncode is shipped as independent libraries in some languages, helping guide developers. We will\n\nrepeatedly rely upon the semantic conventions package in Python in further chapters as we\n\ninstrument application code.\n\nSchema URL\n\nA challenge of semantic conventions is that as telemetry and observability evolve, so will the\n\nterminology used to describe events that we want to observe. An example of this happened when the\n\ndb.hbase.namespace and db.cassandra.keyspace keys were renamed to use db.name instead. Such a\n\nchange would cause problems for anyone already using this field as part of their analysis, or even\n\nalerting. To ensure the semantic conventions can evolve as needed while remaining backward-\n\ncompatible with existing instrumentation, the OpenTelemetry community introduced the schema\n\nURL.\n\nIMPORTANT NOTE\n\nThe OpenTelemetry community understands the importance of backward compatibility in instrumentation code. Going back\n\nand re-instrumenting an application because of a new version of a telemetry library is a pain. As such, a significant amount\n\nof effort has gone into ensuring that components defined in OpenTelemetry remain interoperable with previous versions.\n\nThe project defines its versioning and stability guarantees as part of the specification (https://github.com/open-\n\ntelemetry/opentelemetry-specification/blob/main/specification/versioning-and-stability.md).\n\nThe schema URL is a field added to the telemetry generated for logs, metrics, resources, and traces\n\ntying the emitted telemetry to a version of the semantic conventions. This field allows the producers\n\nand consumers of telemetry to understand how to interpret the data. The schema also provides\n\ninstructions for converting data from one version to another, as per the following example:\n\n1.8.0 schema\n\nfile_format: 1.0.0\n\nschema_url: https://opentelemetry.io/schemas/1.8.0\n\nversions:\n\n1.8.0:\n\nspans:\n\nchanges:\n\nrename_attributes:\n\nattribute_map:\n\ndb.cassandra.keyspace: db.name\n\ndb.hbase.namespace: db.name\n\n1.7.0:\n\n1.6.1:\n\nContinuing with the previous example, imagine a producer of Cassandra telemetry is emitting\n\ndb.cassandra.keyspace as the name for a Cassandra database and specifying the schema as 1.7.0. It\n\nsends the data to a backend that implements schema 1.8.0. By reading the schema URL and\n\nimplementing the appropriate translation, the backend can produce telemetry in its expected version,\n\nwhich is powerful! Schemas decouple systems involved in telemetry, providing them with the\n\nflexibility to evolve independently.\n\nSummary\n\nThis chapter allowed us to learn or review some concepts that will assist us when instrumenting\n\napplications using OpenTelemetry. We looked at the building blocks of distributed tracing, which will\n\ncome in handy when we go through instrumenting our first application with OpenTelemetry in\n\nChapter 4, Distributed Tracing – Tracing Code Execution. We also started analyzing tracing data\n\nusing tools that developers and operators make use of every day.\n\nWe then switched to the metrics signal; first, looking at the minimal contents of a metric, then\n\ncomparing different data types commonly used to produce metrics and their structures. Discussing\n\nexemplars gave us a brief introduction to how correlating metrics with traces can create a more\n\ncomplete picture of what is happening within a system by combining telemetry across signals.\n\nLooking at log formats and searching through logs to find information about the demo application\n\nallowed us to get familiar with yet another tool available in the observability practitioner's toolbelt.\n\nLastly, by leveraging semantic conventions defined in OpenTelemetry, we can begin to produce\n\nconsistent, high-quality data. Following these conventions removes the painful task of naming things,\n\nwhich everyone in the software industry agrees is hard for producers of telemetry. Additionally, these\n\nconventions remove the guesswork when interpreting the data.\n\nKnowing the theory and concepts behind instrumentation and telemetry is excellent to provide us\n\nwith the tools to do all the instrumentation work ourselves. Still, what if I were to tell you it may not\n\nbe necessary to instrument every call in every library manually? The next chapter will cover how\n\nauto-instrumentation looks to help developers in their quest for better visibility into their systems.\n\nChapter 3: Auto-Instrumentation\n\nThe purpose of telemetry is to give people information about systems. This data is used to make\n\ninformed decisions about ways to improve software and prevent disasters from occurring. In the case\n\nof an outage, analytics tools can help us investigate the root cause of the interruption by interpreting\n\ntelemetry. Once the event has been resolved, the recorded traces, metrics, and logs can be correlated\n\nretroactively to gain a complete picture of what happened. In all these cases, the knowledge that's\n\ngained from telemetry assists in solving problems, be it future, present, or past, in applications within\n\nan organization. Being able to see the code is very rarely the bread and butter of an organization,\n\nwhich sometimes makes conversations about investing in observability difficult. Decision-makers\n\nmust constantly make tradeoffs regarding where to invest. The upfront cost of instrumenting code can\n\nbe a deterrent to even getting started, especially if a solution is complicated to implement and will\n\nfail to deliver any value for a long time. Auto-instrumentation looks to alleviate some of the burdens\n\nof instrumenting code manually.\n\nIn this chapter, we will cover the following topics:\n\nWhat is auto-instrumentation?\n\nBytecode manipulation\n\nRuntime hooks and monkey patching\n\nWe will look at some example code in Java and Python, as well as the emitted telemetry, to\n\nunderstand the power of auto-instrumentation. Let's get started!\n\nTechnical requirements\n\nThe application in this chapter simulates the broken telephone game. If you're not familiar with this\n\ngame, it is played by having one person think of a phrase and whisper it to the second player. The\n\nsecond player listens to the best of their ability and whispers it to the third player; this continues until\n\nthe last player shares the message they received with the rest of the group.\n\nEach application represents a player, with the first one printing out the message it is sending, then\n\nplacing the message in a request object that's sent to the next application. The last application in the\n\ngame will print out the message it receives. The following diagram shows the data flow of requests\n\nand responses through the system:\n\nFigure 3.1 – Architectural diagram of the example application The communication between each service is done via\n\ngRPC (https://grpc.io), a remote procedure call system developed by Google. For the sake of this chapter, it is enough\n\nto know that the applications do the following:\n\nShare a common understanding of the data structure of a service and a message via the protocol buffer's definition file.\n\nSend data to each other using the protocol.\n\nThe telemetry that's emitted by each application is sent to the OpenTelemetry Collector via the\n\nOpenTelemetry exporter that's configured in each service. The collector then forwards it to Jaeger,\n\nwhich we'll use to visualize tracing information collected.\n\nThe examples in this chapter are provided within Docker containers to make launching them easier;\n\nthis also means you don't need to install separate runtime languages and libraries on your system. If\n\nyou went through the Docker setup steps in the previous chapter, you can skip ahead to Step 3:\n\n1. Ensure Docker is installed on your system by following the instructions on the Docker website (https://docs.docker.com/get-\n\ndocker/). The following command shows the version of Docker that's running on your local system: $ docker version\n\nClient:\n\nCloud integration: 1.0.14\n\nVersion: 20.10.6\n\nAPI version: 1.41\n\nGo version: go1.16.3 ...\n\n2. Verify that docker compose is installed on your system using the following command. If it is not installed, follow the\n\ndirections on the Docker website (https://docs.docker.com/compose/install/) to install it: $ docker compose version\n\nDocker Compose version 2.0.0-beta.1\n\n3. Download a copy of the companion repository from GitHub and launch the Docker environment that's available in the\n\nchapter3 directory: $ git clone https://github.com/PacktPublishing/Cloud-Native-Observability\n\n$ cd Cloud-Native-Observability/chapter03\n\n$ docker compose up",
      "page_number": 50
    },
    {
      "number": 3,
      "title": "Auto-Instrumentation",
      "start_page": 76,
      "end_page": 88,
      "detection_method": "regex_chapter",
      "content": "The applications that form the demo system for this chapter are written in JavaScript, Python, Go,\n\nand Java. The code for the application in each language that will be shown in this chapter is also\n\navailable in this book's GitHub repository, in the chapter3 directory; each language is in a separate\n\nfolder. We will look through some of the code in this chapter, but not all of it.\n\nLastly, although it is not a requirement for this chapter, if you're interested in exploring the trace\n\ninformation that's emitted from the demo application, the best way to see it is through the Jaeger web\n\ninterface. The Docker compose environment launches Jaeger along with the demo app, so you can\n\nverify that it is up and running by launching a web browser and visiting http://localhost:16686.\n\nWhat is auto-instrumentation?\n\nIn the very early days of the OpenTelemetry project, a proposal was created to support producing\n\ntelemetry without manual instrumentation. As we mentioned earlier in this book, OpenTelemetry uses\n\nOpenTelemetry Enhancement Proposals or OTEPs to propose significant changes or new work\n\nbefore producing a specification. One of the very first OTEPs to be produced by the project\n\n(https://github.com/open-telemetry/oteps/blob/main/text/0001-telemetry-without-manual-\n\ninstrumentation.md) described the need to support users that wanted to produce telemetry without\n\nhaving to modify the code to do so: Cross-language requirements for automated approaches to\n\nextracting portable telemetry data with zero source code modification. – OpenTelemetry\n\nEnhancement Proposal #0001\n\nBeing able to get started with OpenTelemetry with very little effort for new users was very much a\n\ngoal from the start of the project. The hope was to address one of the pain points of producing\n\ntelemetry: the cost of manual instrumentation.\n\nChallenges of manual instrumentation\n\nInstrumenting an application can be a difficult task. This is especially true for someone who hasn't\n\ndone so before. Instrumenting applications is a skill that takes time and practice to perfect. Some of\n\nthe things that can be challenging when instrumenting code are as follows:\n\nThe libraries and APIs that are provided by telemetry frameworks can be hard to learn how to use. With auto-instrumentation,\n\nusers do not have to learn how to use the libraries and APIs directly; instead, they rely on a simplified user experience that can be\n\ntuned via configuration.\n\nInstrumenting applications can be tricky. This can be especially true for legacy applications where the original author of the code\n\nis no longer around. By reducing the amount of code that needs to be modified, auto-instrumentation reduces the surface of the\n\nchanges that need to be made and minimizes the risks involved.\n\nKnowing what to instrument and how it should be done takes practice. The authors of auto-instrumentation tooling and libraries\n\nensure that the telemetry that's produced by auto-instrumentation follows the semantic conventions defined by OpenTelemetry.\n\nAdditionally, it's not uncommon for systems to contain applications written in different languages.\n\nThis adds to the complexity of manually instrumenting code as it requires developers to learn how to\n\ninstrument in multiple languages. Auto-instrumentation provides the necessary tooling to minimize\n\nthe effort here, as the goal of the OpenTelemetry project is to support the same configuration across\n\nlanguages. This means that, in theory, the auto-instrumentation experience will be fairly consistent. I\n\nsay fairly here because the libraries and tools are still changing, so some inconsistencies are being\n\nworked through in the project.\n\nComponents of auto-instrumentation\n\nIn terms of OpenTelemetry, auto-instrumentation is made up of two parts. The first part is composed\n\nof instrumentation libraries. These libraries are provided and supported by members of the\n\nOpenTelemetry community, who use the OpenTelemetry API to instrument popular third-party\n\nlibraries and frameworks in each language. The following table lists some of the instrumentation\n\nlibraries that are provided by OpenTelemetry in various languages at the time of writing:\n\nFigure 3.2 – Some of the available instrumentation libraries in OpenTelemetry Most of these instrumentation libraries\n\nare specific to a particular third-party library of a language. For example, the Boto instrumentation library instruments\n\nmethod calls that are specific to the Boto library. However, there are cases where multiple instrumentation libraries\n\ncould be used to instrument the same thing. An example of this is the Requests and urllib3 instrumentation\n\nlibraries, which would, in theory, instrument the same thing since Requests is built on top of urllib3. When choosing\n\ninstrumentation libraries, a good rule of thumb is to find the library that is the most specific to your use case. If more\n\nthan one fits, inspect the data that's emitted by each library to find the one that fits your needs.\n\nThe details of how exactly instrumentation libraries are implemented vary from language to language\n\nand sometimes even from library to library. Some of these details will become clearer as we progress\n\nthrough this chapter and look at some of the mechanisms that are used in Java and Python libraries.\n\nAs we mentioned at the beginning of this section, two components form auto-instrumentation. The\n\nsecond component is a mechanism that's provided by OpenTelemetry to allow users to automatically\n\ninvoke the instrumentation libraries without additional work on the part of the user. This mechanism\n\nis sometimes called an agent or a runner. In practice, the purpose of this tool is to configure\n\nOpenTelemetry and load the instrumentation libraries that can be used to then generate telemetry.\n\nIMPORTANT NOTE\n\nAuto-instrumentation is still being actively developed and the OpenTelemetry specification around auto-instrumentation, its\n\nimplementation, and how configuration should be specified is still in development. The adoption in different languages is,\n\nat the time of writing, in various stages. For the examples in this chapter, the Python and Java examples use full auto-\n\ninstrumentation with both instrumentation libraries and an agent. The JavaScript and Go code only leverage\n\ninstrumentation libraries.\n\nLimits of auto-instrumentation\n\nAuto-instrumentation is a good place to start the journey of instrumenting an application and gaining\n\nmore visibility into its inner workings. However, there are some limitations as to what can be\n\nachieved with automatic instrumentation, all of which we should take into consideration.\n\nThe first limitation may seem obvious, but it is that auto-instrumentation cannot instrument\n\napplication-specific code. As such, the instrumentation that's produced via auto-instrumentation is\n\nalways going to be missing some critical information about your application. For example, consider\n\nthe following simplified code example of a client application making a web request via the\n\ninstrumented requests HTTP library: def do_something_important():\n\n# doing many important things\n\ndef client_request():\n\ndo_something_important()\n\nrequests.get(\"https://webserver\")\n\nIf auto-instrumentation were exclusively used to instrument the previous example, telemetry would\n\nbe generated for the call that was made via the library, but no information would be captured about\n\nwhat happened when the do_something_important function was called. This would likely be\n\nundesirable as it could leave many questions unanswered.\n\nAnother limitation of auto-instrumentation is that it may instrument things you're not interested in.\n\nThis may result in the same network call being recorded multiple times, or generated data that you're\n\nnot interested in using. An effort is being made in OpenTelemetry to support configuration to give\n\nusers fine-grained control over how telemetry is generated via instrumentation libraries.\n\nWith this in mind, let's learn how auto-instrumentation is implemented in Java.\n\nBytecode manipulation\n\nThe Java implementation of auto-instrumentation for OpenTelemetry leverages the Java\n\nInstrumentation API to instrument code\n\n(https://docs.oracle.com/javase/8/docs/api/java/lang/instrument/Instrumentation.html). This API is\n\ndefined as part of the Java language and can be used by anyone interested in collecting information\n\nabout an application.\n\nOpenTelemetry Java agent\n\nThe OpenTelemetry Java agent is distributed to users via a single Java archive (JAR) file, which can\n\nbe downloaded from the opentelemetry-java-instrumentation repository (https://github.com/open-\n\ntelemetry/opentelemetry-java-instrumentation/releases). The JAR contains the following\n\ncomponents:\n\nThe javaagent module. This is called by the Java Instrumentation API.\n\nInstrumenting libraries for various frameworks and third-party libraries.\n\nThe tooling to initialize and configure the OpenTelemetry components. These will be used to produce telemetry and deliver it to\n\nits destination.\n\nThe JAR is invoked by passing it to the Java runtime via the -javaagent command-line option. The\n\nJava OpenTelemetry agent supports configuration via command-line arguments, also known in Java\n\nas system properties. The following command is an example of how the agent can be used in\n\npractice: java -javaagent:/app/opentelemetry-javaagent.jar \\\n\nDotel.resource.attributes=service.name=broken-telephone-java\\ -Dotel.traces.exporter=otlp\n\n\\\n\njar broken-telephone.jar\n\nNote that the preceding command is also how the demo application is launched inside the container.\n\nUsing the Java agent to load the OpenTelemetry agent gives the library a chance to modify the\n\nbytecode before any other code is executed. The following diagram shows some of the components\n\nthat are involved in the initialization process when the OpenTelemetry agent is used.\n\nOpenTelemetryAgent starts the process, while OpenTelemetryInstaller uses the configuration\n\nprovided at invocation time to configure the emitters of telemetry. Meanwhile, AgentInstaller loads\n\nByte Buddy, an open source library for modifying Java code at runtime, which is used to instrument\n\nthe code via bytecode injection:\n\nFigure 3.3 – OpenTelemetry Java agent loading order\n\nAgentInstaller also loads all the third-party instrumentation libraries that are available in the\n\nOpenTelemetry agent.\n\nIMPORTANT NOTE\n\nThe mechanics of bytecode injection are outside the scope of this book. For the sake of this chapter, it's enough to know\n\nthat the Java agent injects the instrumentation code at runtime. If you're interested in learning more, I recommend\n\nspending some time browsing the Byte Buddy site: https://bytebuddy.net/#/.\n\nThe following code shows the Java code that handles gRPC requests for the broken telephone server.\n\nThe specifics of the code are not overly important here, but pay attention to any instrumentation code\n\nyou can see: BrokenTelephoneServer.java\n\nstatic class BrokenTelephoneImpl extends BrokenTelephoneGrpc.BrokenTelephoneImplBase {\n\n@Override\n\npublic void saySomething(Brokentelephone.BrokenTelephoneRequest req,\n\nStreamObserver<Brokentelephone.BrokenTelephoneResponse> responseObserver) {\n\nBrokentelephone.BrokenTelephoneResponse reply =\n\nBrokentelephone.BrokenTelephoneResponse.newBuilder() .setMessage(\"Hello \" +\n\nreq.getMessage()).build(); responseObserver.onNext(reply);\n\nresponseObserver.onCompleted();\n\n}\n\n}\n\nAs you can see, there is no mention of OpenTelemetry anywhere in the code. The real magic happens\n\nwhen the agent is called at runtime and instruments the application via bytecode injection, as we'll\n\nsee shortly. With this, we now have an idea of how auto-instrumentation works in Java. Now, let's\n\ncompare this to the Python implementation.\n\nRuntime hooks and monkey patching\n\nIn Python, unlike in Java, where a single archive contains everything that's needed to support auto-\n\ninstrumentation, the implementation relies on several separate components that must be discussed to\n\nhelp us fully understand how auto-instrumentation works.\n\nInstrumenting libraries\n\nInstrumentation libraries in Python rely on one of two mechanisms to instrument third-party libraries:\n\nEvent hooks are exposed by the libraries being instrumented, allowing the instrumenting libraries to register and produce\n\ntelemetry as events occur.\n\nAny intercepting calls to libraries are instrumented and are replaced at runtime via a technique known as monkey patching\n\n(https://en.wikipedia.org/wiki/Monkey_patch). The instrumenting library receives the original call, produces telemetry data, and\n\nthen calls the underlying library.\n\nMonkey patching is like bytecode injection in that the applications make calls to libraries without\n\nsuspecting that those calls have been replaced along the way. The following diagram shows how the\n\nopentelemetry-instrumentation-redis monkey patch calls redis.Redis.execute_command to produce\n\ntelemetry data before calling the underlying library:\n\nFigure 3.4 – Monkey-catched call to the Redis library\n\nEach instrumentation library adheres to an interface to register and deregister itself. At the time of\n\nwriting, in Python, unlike in Java, the different instrumentation libraries are packaged independently.\n\nThis has the advantage of reducing the number of dependencies that are required to install the\n\ninstrumentation libraries. However, it does have the disadvantage of requiring users to know what\n\npackages they will need to install. There are a few ways to work around this, which we'll explore in\n\nChapter 7, Instrumentation Libraries.\n\nThe Instrumentor interface\n\nTo ensure a consistent experience for the users of instrumentation libraries, as well as ensuring the\n\ndevelopers of those libraries know what needs to be implemented, the OpenTelemetry Python\n\ncommunity has defined the Instrumentor (https://opentelemetry-python-\n\ncontrib.readthedocs.io/en/latest/instrumentation/base/instrumentor.html) interface. This interface\n\nrequires library authors to provide implementations for the following methods:\n\n_instrument: This method contains any initialization logic for the instrumenting library. This is where monkey patching or\n\nregistering for event hooks takes place.\n\n_uninstrument: This method provides the logic to deregister the library from event hooks or remove any monkey patching.\n\nThis may also contain any additional cleanup operations.\n\ninstrumentation_dependencies: This method returns a list of the library and the versions that the instrumentation\n\nlibrary supports.\n\nIn addition to fulfilling the Instrumentor interface, if an instrumentation library wishes to be available\n\nfor auto-instrumentation, it must register itself via an entry point. An entry point is a Python\n\nmechanism that allows modules to make themselves discoverable by registering a class or method via\n\na string at installation time.\n\nIMPORTANT NOTE\n\nAdditional information on entry points is available in the official Python documentation:\n\nhttps://packaging.python.org/specifications/entry-points/.\n\nOther Python code can then load this code by doing a lookup for an entry point by name and\n\nexecuting it.\n\nWrapper script\n\nFor those mechanisms to be triggered, the Python implementation ships a script that can be called to\n\nwrap any Python application. The opentelemetry-instrument script finds all the instrumentations that\n\nhave been installed in an environment by loading the entry points registered under the\n\nopentelemetry_instrumentor name.\n\nThe following diagram shows two different instrumentation library packages, opentelemetry-\n\ninstrumentation-foo and opentelemetry-instrumentation-bar, registering a separate Python class in\n\nthe opentelemetry_instrumentor entry point's catalog. This catalog is globally available within the\n\nPython environment and when opentelemetry-instrument is invoked, it searches that catalog and\n\nloads any instrumentation that's been registered by calling the instrument method:\n\nFigure 3.5 – Package registration\n\nThe opentelemetry-instrument script is made available via the opentelemetry-instrumentation\n\nPython package. The following code shows the gRPC server implemented in Python. As you can see,\n\nas with the previous example, it does not mention OpenTelemetry: brokentelephone.py\n\n#!/usr/bin/env python3\n\nfrom concurrent import futures\n\nimport grpc\n\nimport brokentelephone_pb2\n\nimport brokentelephone_pb2_grpc\n\nclass Player(brokentelephone_pb2_grpc.BrokenTelephoneServicer): def SaySomething(self,\n\nrequest, context):\n\nreturn brokentelephone_pb2.BrokenTelephoneResponse(\n\nmessage=\"Hello, %s!\" % request.message\n\n)\n\ndef serve():\n\nserver = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n\nbrokentelephone_pb2_grpc.add_BrokenTelephoneServicer_to_server(Player(), server)\n\nserver.add_insecure_port(\"[::]:50051\")\n\nserver.start()\n\nserver.wait_for_termination()\n\nif __name__ == \"__main__\":\n\nserve()\n\nAs we saw in the Java code example, the preceding code is strictly application code – there's no\n\ninstrumentation in sight. The following command shows an example of how auto-instrumentation is\n\ninvoked in Python: opentelemetry-instrument ./broken_telephone.py\n\nThe following screenshot shows a trace that's been generated by our sample application. As we can\n\nsee, the originating request was made by the brokentelephone-js service to Python, Go, and finally\n\nthe Java application. The trace information was generated by the gRPC instrumentation library in\n\neach of those languages:\n\nFigure 3.6 – Sample trace generated automatically across all broken telephone services If you'd like to see a trace for\n\nyourself, the demo application should allow you to do so. Just browse to the Jaeger interface at http://localhost:16686\n\nand search for a trace, as we did in Chapter 2, OpenTelemetry Signals – Traces, Metrics, and Logs. The generated\n\ntrace can give us a glimpse into the data flow through our entire sample application. Although the broken telephone is\n\nsomewhat trivial, you can imagine how this information would be useful for mapping information across a distributed\n\nsystem. With very little effort, we're able to see where time is spent in our system.\n\nSummary\n\nWith auto-instrumentation, it's possible to reduce the time that's required to instrument an existing\n\napplication. Reducing the friction to get started with telemetry gives users a chance to try it before\n\ninvesting significant amounts of time in manual instrumentation. And although the data that's\n\ngenerated via auto-instrumentation is likely not enough to get to the bottom of issues in complex\n\nsystems, it's a solid starting point. Auto-instrumentation can also be quite useful when you're\n\ninstrumenting an unfamiliar system.\n\nThe use of instrumentation libraries allows users to gain insight into what the libraries they're using\n\nare doing, without having to learn the ins and outs of them. The OpenTelemetry libraries that are\n\navailable at the time of writing can be used to instrument existing code by following the online\n\ndocumentation that's been made available by each language. As we'll learn in Chapter 7,\n\nInstrumentation Libraries, using these libraries can be tremendously useful in reducing the code\n\nthat's needed to instrument applications.\n\nIn this chapter, we compared two different implementations of auto-instrumentation by looking at the\n\nJava implementation, which utilizes bytecode injection, and the Python implementation, which uses\n\nruntime hooks and monkey patching. In each case, the implementation leverages features of the\n\nlanguage that allows the implementation to inject telemetry at appropriate times in the code's\n\nexecution. Before diving into auto-instrumentation, however, it is useful to understand how each\n\nsignal can be leveraged independently, starting with distributed tracing. We will do this in the next\n\nchapter.\n\nSection 2: Instrumenting an Application\n\nIn this part, you will walk through instrumenting an application by using the signals offered by\n\nOpenTelemetry: distributed tracing, metrics, and logging.\n\nThis part of the book comprises the following chapters:\n\nChapter 4, Distributed Tracing – Tracing Code Execution\n\nChapter 5, Metrics – Recording Measurements\n\nChapter 6, Logging – Capturing Events\n\nChapter 7, Instrumentation Libraries\n\nChapter 4: Distributed Tracing – Tracing Code Execution\n\nSo, now that we have an understanding of the concepts of OpenTelemetry and are familiar with the\n\ndifferent signals it covers, it's time to start instrumenting application code. In Chapter 2,\n\nOpenTelemetry Signals – Tracing, Metrics, and Logging, we covered the terminology and concepts of\n\nthose signals by looking at a system that was instrumented with OpenTelemetry. Now, it's time to get\n\nhands-on with some code to start generating telemetry ourselves, and to do this, we're going to first\n\nlook at implementing a tracing signal.\n\nIn this chapter, we will cover the following topics:\n\nConfiguring OpenTelemetry\n\nGenerating tracing data\n\nEnriching the data with attributes, events, and links\n\nAdding error handling information\n\nBy the end of this chapter, you'll have instrumented several applications with OpenTelemetry and be\n\nable to trace how those applications are connected via distributed tracing. This will start giving you a\n\nsense of how distributed tracing can be used in your own applications going forward.\n\nTechnical requirements\n\nAt the time of writing, OpenTelemetry for Python supports Python 3.6+. All Python examples in this\n\nbook will use Python 3.8, which can be downloaded and installed by following the instructions at\n\nhttps://docs.python.org/3/using/index.html. The following command can verify which version of\n\nPython is installed. It's possible for multiple versions to be installed simultaneously on a single\n\nsystem, which is why both python and python3 are shown here: $ python --version\n\n$ python3 --version\n\nIt is recommended to use a virtual environment to run the examples in this book\n\n(https://docs.python.org/3/library/venv.html). A virtual environment in Python allows you to install\n\npackages in isolation from the rest of the system, meaning that if anything goes wrong, you can\n\nalways delete the virtual environment and start a fresh one. The following commands will create a\n\nnew virtual environment in a folder called cloud_native_observability: $ mkdir\n\ncloud_native_observability\n\n$ python3 -m venv cloud_native_observability\n\n$ source cloud_native_observability/bin/activate\n\nThe example code in this chapter will rely on a few different third-party libraries – Flask and\n\nRequest. The following command will install all the required packages for this chapter using the\n\npackage installation for Python, pip: $ pip install flask requests\n\nNow that we have a virtual environment configured and the libraries needed, we will install the\n\nnecessary Python packages to use OpenTelemetry. The main libraries we'll need for this section are\n\nthe API and SDK packages: $ pip install opentelemetry-api opentelemetry-sdk\n\nThe pip freeze command lists all the installed packages in this Python environment; we can use it to\n\nconfirm whether the correct packages are installed: $ pip freeze | grep opentelemetry\n\nopentelemetry-api==1.3.0\n\nopentelemetry-sdk==1.3.0\n\nopentelemetry-semantic-conventions==0.22b0\n\nThe version of the packages installed in your environment may differ, as the OpenTelemetry project\n\nis still very much under active development, and releases are pretty frequent. It's important to\n\nremember this as we work through the examples, as some methods may be slightly different, or the\n\noutput may vary.\n\nIMPORTANT NOTE\n\nThe OpenTelemetry APIs should not change unless a major version is released.\n\nConfiguring the tracing pipeline\n\nWith the packages now installed, we're ready to take our first step to generate distributed traces with\n\nOpenTelemetry – configuring the tracing pipeline. The tracing pipeline is what allows the tracing\n\ndata we explored in Chapter 2, OpenTelemetry Signals – Traces, Metrics, and Logs, to be generated\n\nwhen the OpenTelemetry API calls are made. The pipeline also defines where and how the data will\n\nbe emitted. Without a tracing pipeline, a no-op implementation is used by the API, meaning the code\n\nwill not generate distributed traces. The tracing pipeline configures the following:\n\nTracerProvider to determine how spans should be generated\n\nA Resource object, which identifies the source of the spans\n\nSpanProcessor to describe how spans will be exported\n\nSpanExporter to describe where the spans will be exported\n\nThe following code imports the TracerProvider, ConsoleSpanExporter, and SimpleSpanProcessor\n\nclasses from the SDK to configure a tracer provider. In this example, ConsoleSpanExporter will be\n\nused to output traces from the application to the console. The last step to configure the tracer provider",
      "page_number": 76
    },
    {
      "number": 4,
      "title": "Distributed Tracing – Tracing Code Execution",
      "start_page": 89,
      "end_page": 128,
      "detection_method": "regex_chapter",
      "content": "is to call the set_tracer_provider method, which will set the global tracer provider to the provider\n\nwe instantiated. The code will be placed in the configure_tracer method, which will be called before\n\nwe do anything else in the code: shopper.py\n\n#!/usr/bin/env python3\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor def\n\nconfigure_tracer():\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = SimpleSpanProcessor(exporter)\n\nprovider = TracerProvider()\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nif __name__ == \"__main__\":\n\nconfigure_tracer()\n\nThroughout this chapter, as we iterate over the application and add more code, each time we do so,\n\nwe will test the code and inspect its output using the following command, unless specified otherwise:\n\n$ python ./shopper.py\n\nRunning this command for the initial code will not output anything. This allows us to confirm that the\n\nmodules have been found and imported correctly, and that the code doesn't have any errors in it.\n\nIMPORTANT NOTE\n\nA common mistake when first configuring TracerProvider is to forget to set the global TracerProvider, causing the\n\nAPI to use a default no-op implementation of TracerProvider. This default is configured intentionally for the use case\n\nwhere a user does not wish to enable tracing within their application.\n\nAlthough it may not seem like much, configuring TracerProvider for an application is a critical first\n\nstep before we can start collecting distributed traces. It's a bit like gathering all the ingredients before\n\nbaking a cake, so let's get baking!\n\nGetting a tracer\n\nWith the tracing pipeline configured, we can now obtain the generator for our tracing data, Tracer.\n\nThe TracerProvider interface defines a single method to allow us to obtain a tracer, get_tracer. This\n\nmethod requires a name argument and, optionally, a version argument, which should reflect the name\n\nand version of the instrumenting module. This information is valuable for users to quickly identify\n\nwhat the source of the tracing data is. An example shown in Figure 4.1 shows how the values passed\n\ninto get_tracer will vary, depending on where the call is made. Inside the library calls to requests\n\nand Flask, the name and version will reflect those libraries, whereas in the shopper and\n\ngrocery_store modules, the name and version will reflect those modules.\n\nFigure 4.1 – The tracer name and version configuration at different stages of an application To get the first tracer, the\n\nfollowing code will be added to shopper.py immediately at the end of configure_tracer to return a tracer from the\n\nmethod: shopper.py\n\ndef configure_tracer():\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = SimpleSpanProcessor(exporter)\n\nprovider = TracerProvider()\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(\"shopper.py\", \"0.0.1\")\n\nif __name__ == \"__main__\":\n\ntracer = configure_tracer()\n\nIt's important to remember to make the name and version meaningful; the name should be unique\n\nwithin the scope of the application it instruments. The instrumentation scope could be a package,\n\nmodule, or even a class. It's finally time to start using this tracer and trace the application! There are\n\nseveral ways to create a span in OpenTelemetry; let's explore them now.\n\nGenerating tracing data\n\nIt's finally time to start generating telemetry from the application! There are several ways to create a\n\nspan in OpenTelemetry; the first one we'll use is to call start_span on the tracer instance we\n\nobtained previously. This will create a span object, using the only required string argument as the\n\nname of the span. The span object is the building block of distributed tracing and is intended to\n\nrepresent a unique unit of work in our application. In the following example, we will create a new\n\nSpan object before calling a method that will do some work. Since our application is a shopper, the\n\nfirst thing the shopper will do is browse the store. In order for the tracing data to be useful, it's\n\nimportant to use a meaningful name in the creation of the span. Once browse has returned, we will\n\ncall end on the span object to signal that the work is complete: shopper.py\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nif __name__ == \"__main__\":\n\ntracer = configure_tracer()\n\nspan = tracer.start_span(\"visit store\")\n\nbrowse()\n\nspan.end()\n\nRunning the code will output our first trace to the console. The ConsoleSpanExporter automatically\n\noutputs the data as formatted JSON to make it easier to read: shopper.py output\n\nvisiting the grocery store\n\n{\n\n\"name\": \"visit store\",\n\n\"context\": {\n\n\"trace_id\": \"0x4c6fd97f286439b1a4bb109f12bf2095\", \"span_id\": \"0x6ea2219c865f6c4b\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": null,\n\n\"start_time\": \"2021-06-26T20:26:47.176169Z\", \"end_time\": \"2021-06-26T20:26:47.176194Z\",\n\n\"status\": {\n\n\"status_code\": \"UNSET\"\n\n},\n\n\"attributes\": {},\n\n\"events\": [],\n\n\"links\": [],\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"service.name\": \"unknown_service\"\n\n}\n\n}\n\nSome of the data worth noting in the preceding output is as follows:\n\nThe name field of the span we provided.\n\nThe automatically generated trace and span identifiers – trace_id and span_id.\n\nThe start_time and end_time timestamps, which can be used to calculate the duration of an operation being traced.\n\nThe parent_id identifier is not set. This identifies the span created as the beginning of a trace, otherwise known as root\n\nspan.\n\nThe status_code field of the span is set to UNSET by default.\n\nWith the preceding span information from the JSON output, we now have the first piece of data about\n\nthe work that our application is doing. One of the most critical pieces of information generated in this\n\ndata is trace_id. This trace identifier is a 128-bit integer that allows operations to be tied together in\n\na distributed trace and represents the single request through the entire system. span_id is a 64-bit\n\ninteger used to identify the specific unit of work in the request and also relationships between\n\ndifferent operations. In this next code example, we'll add another operation to our trace to see how\n\nthis identifier works, but we'll need to take a brief detour to look at the Context API before continuing\n\ntoo far.\n\nIMPORTANT NOTE\n\nThe examples in this chapter will only use ConsoleSpanExporter. We will explore additional exporters in Chapter 8,\n\nThe OpenTelemetry Collector, and Chapter 10, Configuring a Backend, when we look at the OpenTelemetry Collector and\n\ndifferent backends.\n\nThe Context API\n\nIn order to tie spans together, we'll need to activate our spans before starting new ones. Activating a\n\nspan in OpenTelemetry is synonymous with setting the span in the current context object. The\n\nContext object is a mechanism used across signals to share data about the application either in-\n\nprocess or across API boundaries via propagation. No matter where you are in the application code,\n\nit's possible to get the current span by using the Context API. The Context object can be thought of as\n\nan immutable data store with a consistent API across implementations. In Python, the implementation\n\nrelies on ContextVars, as previously discussed in Chapter 1, The History and Concepts of\n\nObservability, but not all languages have the notion of a context built into the language itself. The\n\nContext API ensures that users will have a consistent experience when using OpenTelemetry. The\n\nAPI definition for interacting with the context is fairly minimal:\n\nget_value: Retrieves a value for a given key from the context. The only required argument is a key and, optionally, a\n\ncontext argument. If no context is passed in, the value returned will be pulled from the global context.\n\nset_value: Stores a value for a certain key in the context. The method receives a key, value, and optionally, a context argument\n\nto set the value into. As mentioned before, the context is immutable, so the return value is a new Context object with the new\n\nvalue set.\n\nattach: Calling attach associates the current execution with a specified context. In other words, it sets the current context to\n\nthe context passed in as an argument. The return value is a unique token, which is used by the detach method described next.\n\ndetach: To return the context to its previous state, this method receives a token that was obtained by attaching to another\n\ncontext. Upon calling it, the context that was current at the time attach was called is restored.\n\nDon't worry if the description doesn't quite make sense yet; the next example will help clarify things.\n\nIn the following code, we activate the span by setting it in the context via the set_span_in_context\n\nmethod, which, under the hood, calls the current context's set_value method. The return value of this\n\ncall is a new immutable context object, which we can then attach to before starting the second span:\n\nshopper.py\n\nfrom opentelemetry import context, trace\n\nif __name__ == \"__main__\":\n\ntracer = configure_tracer()\n\nspan = tracer.start_span(\"visit store\")\n\nctx = trace.set_span_in_context(span)\n\ntoken = context.attach(ctx)\n\nspan2 = tracer.start_span(\"browse\")\n\nbrowse()\n\nspan2.end()\n\ncontext.detach(token)\n\nspan.end()\n\nRunning the application and looking at the output once again, we can now see that the trace_id value\n\nfor both spans is the same. We can also see that the browse span has a parent_id field that matches\n\nspan_id of the visit store span: shopper.py output\n\nvisiting the grocery store\n\n{\n\n\"name\": \"browse\",\n\n\"context\": {\n\n\"trace_id\": \"0x03c197ae7424cc492ab1c92112490be1\", \"span_id\": \"0xb7396b0e6ccab2fd\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": \"0x8dd8c60c67518a8d\", }\n\n{\n\n\"name\": \"visit store\",\n\n\"context\": {\n\n\"trace_id\": \"0x03c197ae7424cc492ab1c92112490be1\", \"span_id\": \"0x8dd8c60c67518a8d\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": null,\n\n}\n\nStarting and ending spans manually can be useful in many cases, but as demonstrated by the previous\n\ncode, managing the context manually can be somewhat cumbersome. More often than not, it is easier\n\nin Python to use a context manager to wrap the work we want to trace. The start_as_current_span\n\nconvenience method allows us to do exactly this by creating a new Span object, setting it as the\n\ncurrent span in a context, and calling the attach method. Additionally, it will automatically end the\n\nspan once the context has been exited. The following code shows us how we can simplify the\n\nprevious code we wrote: shopper.py\n\nif __name__ == \"__main__\":\n\ntracer = configure_tracer()\n\nwith tracer.start_as_current_span(\"visit store\"):\n\nwith tracer.start_as_current_span(\"browse\"):\n\nbrowse()\n\nThis method simplifies the code quite a bit. The automatic management of the context can be used to\n\nquickly create hierarchies of spans. In the following code, we will add one new method and one more\n\nspan. We'll then run the code to observe how each span will use the previous span in the context as\n\nthe new span's parent: shopper.py\n\ndef add_item_to_cart(item):\n\nprint(\"add {} to cart\".format(item))\n\nif __name__ == \"__main__\":\n\ntracer = configure_tracer()\n\nwith tracer.start_as_current_span(\"visit store\"):\n\nwith tracer.start_as_current_span(\"browse\"):\n\nbrowse()\n\nwith tracer.start_as_current_span(\"add item to cart\"):\n\nadd_item_to_cart(\"orange\")\n\nRunning the shopper application, we're starting to see what is appearing to be more and more like a\n\nreal trace. Looking at the output from the new code, we can see three different operations captured.\n\nThe order in which output appears in your terminal may vary; we will review operations in the same\n\norder in which they appear in the code. The first operation to look at is visit store, as mentioned\n\npreviously; the root span can be identified by the parent_id field being null: shopper.py output\n\n{\n\n\"name\": \"visit store\",\n\n\"context\": {\n\n\"trace_id\": \"0x9251fa73b421a143a7654afb048a4fc7\", \"span_id\": \"0x08c9bf4cccd7ba5d\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": null,\n\n\"start_time\": \"2021-06-26T21:43:20.441933Z\",\n\n\"end_time\": \"2021-06-26T21:43:20.442222Z\",\n\n\"status\": {\n\n\"status_code\": \"UNSET\"\n\n},\n\n\"attributes\": {},\n\n\"events\": [],\n\n\"links\": [],\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"service.name\": \"unknown_service\"\n\n}\n\n}\n\nThe next operation to review in the output is the browse span. Note that the span's parent_id\n\nidentifier is equal to the span_id identifier of the visit store span. trace_id also matches, which\n\nindicates that the spans are connected in the same trace: shopper.py output\n\n{\n\n\"name\": \"browse\",\n\n\"context\": {\n\n\"trace_id\": \"0x9251fa73b421a143a7654afb048a4fc7\", \"span_id\": \"0xa77587668be46030\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": \"0x08c9bf4cccd7ba5d\", \"start_time\": \"2021-06-26T21:43:20.442091Z\",\n\n\"end_time\": \"2021-06-26T21:43:20.442212Z\",\n\n\"status\": {\n\n\"status_code\": \"UNSET\"\n\n},\n\n\"attributes\": {},\n\n\"events\": [],\n\n\"links\": [],\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"service.name\": \"unknown_service\"\n\n}\n\n}\n\nThe last span to review is the add item to cart span. As with the previous span, its trace_id\n\nidentifier will also match the previous spans. In this case, the parent_id identifier of the add item to\n\ncart span now matches the span_id identifier of the browse span: shopper.py output\n\n{\n\n\"name\": \"add item to cart\",\n\n\"context\": {\n\n\"trace_id\": \"0x9251fa73b421a143a7654afb048a4fc7\", \"span_id\": \"0x6470521265d80512\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": \"0xa77587668be46030\", \"start_time\": \"2021-06-26T21:43:20.442169Z\",\n\n\"end_time\": \"2021-06-26T21:43:20.442191Z\",\n\n\"status\": {\n\n\"status_code\": \"UNSET\"\n\n},\n\n\"attributes\": {},\n\n\"events\": [],\n\n\"links\": [],\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"service.name\": \"unknown_service\"\n\n}\n\n}\n\nNot too bad – the code looks much simpler than the previous example, and we can already see how\n\neasy it is to trace code in applications. The last method we can use to start a span is by using a\n\ndecorator. A decorator is a convenient way to instrument code without having to add any tracing\n\nspecific information to the code itself. This makes the code a bit cleaner.\n\nIMPORTANT NOTE\n\nUsing the decorator means you will need to keep an instance of a tracer initialized and available globally for the decorators\n\nto be able to use it.\n\nRefactoring the shopper.py code, we will move the instantiation of the tracer out of the main method\n\nand add decorators to each of the methods we've defined previously. Note that the code in main is\n\nsimplified significantly: shopper.py\n\ntracer = configure_tracer()\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nadd_item_to_cart(\"orange\")\n\n@tracer.start_as_current_span(\"add item to cart\")\n\ndef add_item_to_cart(item):\n\nprint(\"add {} to cart\".format(item))\n\n@tracer.start_as_current_span(\"visit store\")\n\ndef visit_store():\n\nbrowse()\n\nif __name__ == \"__main__\":\n\nvisit_store()\n\nRun the program once again; the spans will be printed as they were before. The output will not have\n\nchanged with this refactor, but the code looks much cleaner. As with the previous example, context\n\nmanagement is handled for us, so we don't need to worry about interacting with the Context API.\n\nReading the code is much simpler with decorators, and it's also easy for someone new to the code to\n\nimplement new methods with the same pattern when adding code to the application.\n\nSpan processors\n\nA quick note about the span processor used in the code so far – the initial configuration of the tracing\n\npipeline used SimpleSpanProcessor. This does all of its processing in line with the export happening\n\nas soon as the span ends. This means that every span added to the code will add latency in the\n\napplication, which is generally not what we want. This may be the right choice in some cases – for\n\nexample, if it's impossible to guarantee that threads other than the main thread will finish before a\n\nprogram is interrupted. However, it's generally recommended that span processing happens out of\n\nband from the main thread. An alternative to SimpleSpanProcessor is BatchSpanProcessor. Figure 4.2\n\nshows how the execution of the program is interrupted by SimpleSpanProcessor to export a span,\n\nwhereas with BatchSpanProcessor, another thread handles the export operation:\n\nFigure 4.2 – SimpleSpanProcessor versus BatchSpanProcessor\n\nAs the name suggests, BatchSpanProcessor groups the export of spans. It does this by launching a\n\nseparate thread that exports spans on a schedule or when there's a certain number of items in the\n\nqueue. This prevents adding unnecessary latency to the normal application code paths. Configuring\n\nBatchSpanProcessor is done much like SimpleSpanProcessor. For the remainder of the examples in\n\nthis chapter, we will use this new processor. The following refactor updates the imports and code in\n\nthe tracer configuration to use BatchSpanProcessor: shopper.py\n\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter def\n\nconfigure_tracer():\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter) provider = TracerProvider()\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(\"shopper.py\", \"0.0.1\")\n\nRun the application now to confirm that the program still works and that the output is the same with\n\nthe new span processor in place. Although it may not look like much has changed, if you look closely\n\nat the start_time and end_time fields of each span produced, the duration of each span has changed.\n\nFigure 4.3 shows a chart comparing output from running the program with each type of span\n\nprocessor. The duration of the visit store span is significantly shorter using BatchSpanProcessor\n\nbecause the processing of each span is happening asynchronously:\n\nFigure 4.3 – Span durations using SimpleSpanProcessor and BatchSpanProcessor Even though microseconds may\n\nnot seem like much in our example, this type of performance impact is critical to systems in production.\n\nBatchSpanProcessor is a much better choice for running real-world applications. Now, we have a better sense of\n\nhow to generate tracing data via the API, but the data we've produced so far could be improved. It doesn't have nearly\n\nenough details to make it truly useful, so let's tackle that next.\n\nEnriching the data\n\nYou may have noticed in output from the previous examples that each span emitted contains a\n\nresource attribute. The resource attribute provides an immutable set of attributes, representing the\n\nentity producing the telemetry. resource attributes are not specific to tracing. Any signal that emits\n\ntelemetry leverages resource attributes by adding them to the data produced at export time. As\n\ncovered in Chapter 1, The History and Concepts of Observability, the resource in an application is\n\nassociated with the telemetry generator, which, in the case of tracing, is TracerProvider. The\n\nresource attribute on the span output we've seen so far is automatically provided by the SDK with\n\nsome information about the SDK itself, as well as a default service.name. The service name is used\n\nby many backends to identify the services sending traces to them; however, as you can see, the\n\ndefault value of unknown_service is not a super useful name. Let's fix this. The following code will\n\ncreate a new Resource object with a service name and version number, and then pass it in as an\n\nargument to the TracerProvider constructor: shopper.py\n\nfrom opentelemetry.sdk.resources import Resource\n\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter def\n\nconfigure_tracer():\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter)\n\nresource = Resource.create(\n\n{\n\n\"service.name\": \"shopper\",\n\n\"service.version\": \"0.1.2\",\n\n}\n\n)\n\nprovider = TracerProvider(resource=resource) provider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(\"shopper.py\", \"0.0.1\")\n\nThe output from the application will now include the information added in the resource attribute\n\nalong with the automatically populated data, as shown here: \"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"service.name\": \"shopper\",\n\n\"service.version\": \"0.1.2\"\n\n}\n\nThis is much more useful than unknown_service; however, we now have the name and version\n\nhardcoded in two places. Even worse, the names and versions don't match. Let's fix this before going\n\nfurther by refactoring the configure_tracer method to expect the name and version arguments, as\n\nfollows: shopper.py\n\ndef configure_tracer(name, version): exporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter)\n\nresource = Resource.create(\n\n{\n\n\"service.name\": name,\n\n\"service.version\": version,\n\n}\n\n)\n\nprovider = TracerProvider(resource=resource)\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(name, version) tracer = configure_tracer(\"shopper\", \"0.1.2\") After\n\nrunning the application, the output should remain the same as it was before the change.\n\nThe code is now less error-prone, as we only have one place to set the service name and\n\nversion information, and configure_tracer can be reused to configure OpenTelemetry for\n\ndifferent applications, which will come in handy shortly.\n\nSome additional information you may want to populate in the resource includes things such as the\n\nhostname or, in the case of a dynamic runtime environment, an instance identifier of some sort. The\n\nOpenTelemetry SDK provides an interface to provide some of the details about a resource\n\nautomatically; this is known as the ResourceDetector interface.\n\nResourceDetector\n\nAs its name suggests, the purpose of the ResourceDetector attribute is to detect information that will\n\nautomatically be populated into a resource. A resource detector is a great way to extract information\n\nabout a platform running an application, and there are already existing detectors for some popular\n\ncloud providers. This information can be a useful way to group applications by region or host when\n\ntrying to pinpoint application performance issues. The interface for ResourceDetector specifies a\n\nsingle method to implement, detect, which returns a resource. Let's implement a ResourceDetector\n\ninterface that we can reuse in all the services of the grocery store. This detector will automatically fill\n\nin the hostname and IP address of the machine running the code; to accomplish this, Python's socket\n\nlibrary will come in handy. Place the following code in a new file in the same directory as\n\nshopper.py: local_machine_resource_detector.py\n\nimport socket\n\nfrom opentelemetry.sdk.resources import Resource, ResourceDetector\n\nclass LocalMachineResourceDetector(ResourceDetector):\n\ndef detect(self):\n\nhostname = socket.gethostname()\n\nip_address = socket.gethostbyname(hostname)\n\nreturn Resource.create(\n\n{\n\n\"net.host.name\": hostname,\n\n\"net.host.ip\": ip_address,\n\n}\n\n)\n\nTo make use of this new module, let's import it into the shopper application. The code in\n\nconfigure_tracer will also be updated to call this new ResourceDetector first, before adding the\n\nservice name and version information. As mentioned earlier, a resource is immutable, meaning that\n\nthere's no method to call to update a specific resource. Adding new attributes to the resource\n\ngenerated by our resource detector is done via a call to a resource's merge method. merge creates a\n\nnew resource from the caller's attributes and then updates that new resource to include all the\n\nattributes of the resource passed in as an argument. The following update to the code imports the\n\nmodule we just created, and creates a new resource by calling LocalMachineResourceDetector and\n\ncalls merge to ensure that our previous resource information is not lost: shopper.py\n\nfrom opentelemetry.sdk.resources import Resource\n\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor from\n\nlocal_machine_resource_detector import LocalMachineResourceDetector\n\ndef configure_tracer(name, version):\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = SimpleSpanProcessor(exporter)\n\nlocal_resource = LocalMachineResourceDetector().detect()\n\nresource = local_resource.merge(\n\nResource.create(\n\n{\n\n\"service.name\": name,\n\n\"service.version\": version,\n\n}\n\n)\n\n)\n\nprovider = TracerProvider(resource=resource)\n\nreturn trace.get_tracer(name, version)\n\nThe output from running the code will now contain all the resources seen in the previous example,\n\nbut it will also include the information generated by LocalMachineResourceDetector: \"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"net.host.name\": \"myhost.local\",\n\n\"net.host.ip\": \"192.168.128.47\",\n\n\"service.name\": \"shopper\",\n\n\"service.version\": \"0.1.2\"\n\n}\n\nIMPORTANT NOTE\n\nIf the same resource attribute is included in both the caller and the resource passed into merge, the attributes of the\n\nargument resource will override the caller. For example, if resource_one has an attribute of foo=one and\n\nresource_two has an attribute of foo=two, the resulting resource from calling\n\nresource_one.merge(resource_two) will have an attribute of foo=two.\n\nFeel free to play around with ResourceDetector and see what other useful information you can add\n\nabout your machine. Try adding some environment variables or the version of Python running on\n\nyour system; this can be valuable when troubleshooting applications!\n\nSpan attributes\n\nLooking through the tracing data being emitted, we can start to get an idea of what is happening in\n\nthe code we're writing. Now, let's figure out what data we should add about our shopper to make this\n\ntrace even more useful. As the shopper application will be used as an HTTP client, we can take a look\n\nat the semantic conventions available in the specification to inspire us; Figure 4.4\n\n(https://github.com/open-telemetry/opentelemetry-\n\nspecification/blob/main/specification/trace/semantic_conventions/http.md#http-client-server-\n\nexample) shows us the span attributes to add if we want to adhere to OpenTelemetry's semantic\n\nconventions, as well as some sample values:\n\nFigure 4.4 – The HTTP client span attributes semantic conventions\n\nA valid attribute must be either a string, a 64-bit integer, a float, or a Boolean. An attribute can also\n\nbe an array of any of those values, but it must be a homogenous array, meaning the elements of the\n\narray must be a single type.\n\nIMPORTANT NOTE\n\nNull or None values are not encouraged in attributes, as the handling of null values in backends may differ and thus\n\ncreate unexpected behavior.\n\nIn the next example, we will update the browse method to include the recommended attributes for a\n\nclient application. Since we're using decorators here, we'll need to get the current span by calling the\n\nget_current_span method. Once we have the span, we can call the set_attribute method, which\n\nrequires two arguments – the key to set and the value. Since we have not yet started the server, we'll\n\nset a placeholder value for http.url and net.peer.ip: shopper.py\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nspan = trace.get_current_span()\n\nspan.set_attribute(\"http.method\", \"GET\")\n\nspan.set_attribute(\"http.flavor\", \"1.1\")\n\nspan.set_attribute(\"http.url\", \"http://localhost:5000\")\n\nspan.set_attribute(\"net.peer.ip\", \"127.0.0.1\")\n\nLooking at the output from running the program, we will expect to see the attributes added to the\n\nbrowse span; let's take a look: \"name\": \"browse\",\n\n\"attributes\": {\n\n\"http.method\": \"GET\",\n\n\"http.flavor\": \"1.1\",\n\n\"http.url\": \"http://localhost:5000\",\n\n\"net.peer.ip\": \"127.0.0.1\"\n\n},\n\nExcellent! The data is there. It's a bit inconvenient to make independent calls to a method when\n\nwanting to set multiple attributes; thankfully, there's a convenient method to address this. The code\n\ncan be simplified by making a single call to set_attributes and passing in a dictionary with the\n\nsame values: shopper.py\n\nspan.set_attributes(\n\n{\n\n\"http.method\": \"GET\",\n\n\"http.flavor\": \"1.1\",\n\n\"http.url\": \"http://localhost:5000\",\n\n\"net.peer.ip\": \"127.0.0.1\",\n\n}\n\n)\n\nSetting so many attributes, it can be easy for a typo to sneak in. This would, at best, be caught during\n\na review but, at worst, could mean missing some critical data. Imagine a scenario where some\n\nalerting is configured to rely on the url and flavor attributes, but somewhere along the way, flavor is\n\nspelled as flavour. The correctness of the tracing data is critical, and to make setting these attributes\n\nmore easy, a semantic conventions package provides constants that can be used instead of hardcoding\n\ncommon keys and values. The following is a refactor of the code to make use of the opentelemetry-\n\nsemantic-conventions package: shopper.py\n\nfrom opentelemetry.semconv.trace import HttpFlavorValues, SpanAttributes\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_METHOD: \"GET\", SpanAttributes.HTTP_FLAVOR:\n\nHttpFlavorValues.HTTP_1_1.value, SpanAttributes.HTTP_URL: \"http://localhost:5000\",\n\nSpanAttributes.NET_PEER_IP: \"127.0.0.1\", }\n\n)\n\nOf course, using semantic conventions alone may not give us enough information about the specifics\n\nof the application. One of the powers of attributes is to add meaningful data about the transaction\n\nbeing traced to allow us to understand what happened. One aspect of the shopper application that will\n\nlikely be unique once we start processing real data is information about the items and quantities\n\nadded to the cart. The following code adds attributes to the span to record that information:\n\nshopper.py\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_METHOD: \"GET\",\n\nSpanAttributes.HTTP_FLAVOR: str(HttpFlavorValues.HTTP_1_1),\n\nSpanAttributes.HTTP_URL: \"http://localhost:5000\",\n\nSpanAttributes.NET_PEER_IP: \"127.0.0.1\",\n\n}\n\n)\n\nadd_item_to_cart(\"orange\", 5)\n\n@tracer.start_as_current_span(\"add item to cart\")\n\ndef add_item_to_cart(item, quantity):\n\nspan = trace.get_current_span()\n\nspan.set_attributes({\n\n\"item\": item,\n\n\"quantity\": quantity,\n\n})\n\nprint(\"add {} to cart\".format(item))\n\nThe topic of span attributes will be revisited when we introduce the server later in this chapter.\n\nAttributes are also a key component of other signals, so we'll come back to them throughout the\n\nbook. One last thing to be aware of when thinking of attributes, and really any data being recorded in\n\ntraces, is to be cognizant of Personally Identifiable Information (PII). Whenever possible, save\n\nyourself the trouble and remove all PII from the telemetry. We'll cover more on this topic in Chapter\n\n8, OpenTelemetry Collector.\n\nSpanKind\n\nAnother piece of information that is useful about a span is SpanKind. SpanKind is a qualifier that\n\ncategorizes the span and provides additional information about the relationship between spans in a\n\ntrace. The following categories for span kinds are defined in OpenTelemetry:\n\nINTERNAL: This indicates that the span represents an operation that is internal to an application, meaning that this specific\n\noperation has no external dependencies or relationships. This is the default value for a span when not set.\n\nCLIENT: This identifies the span as an operation making a request to a remote service, which should be identified as a server\n\nspan. The request made by this operation is synchronous, and the client should wait for a response from the server.\n\nSERVER: This indicates that the span is an operation responding to a synchronous request from a client span. In a client/server,\n\nthe client is identified as the parent span to the server, as it is the originator of the request.\n\nPRODUCER: This identifies the operation as an originator of an asynchronous request. Unlike in the case of the client span, the\n\nproducer is not expecting a response from the consumer of the asynchronous request.\n\nCONSUMER: This identifies the operation as a consumer of an asynchronous request from a producer.\n\nAs you may have noticed so far, all the spans that we've created have been identified as internal. The\n\nfollowing information can be found throughout the output we've generated until now: \"kind\":\n\n\"SpanKind.INTERNAL\"\n\nNow is a good time to start making the shopper application a bit more realistic by adding some calls\n\nto a grocery store server. Knowing that this will be a client using HTTP requests to retrieve data from\n\nthe server, we will set SpanKind to CLIENT on the operation that makes a call to the server. On the\n\nreceiving side, we will set SpanKind on the operation that is responding to the request to SERVER. The\n\nway to set kind is by passing the kind argument when creating the span. The following code adds a\n\nweb request from the client to the server in the browse method. The HTTP request will be facilitated\n\nby using the requests (https://docs.python-requests.org/) library. The request to the server will be\n\nwrapped by a context manager, which starts a new span named web request with the kind set to\n\nCLIENT: shopper.py\n\nimport requests\n\nfrom common import configure_tracer\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nwith tracer.start_as_current_span(\n\n\"web request\", kind=trace.SpanKind.CLIENT\n\n) as span:\n\nurl = \"http://localhost:5000\"\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_METHOD: \"GET\",\n\nSpanAttributes.HTTP_FLAVOR: str(HttpFlavorValues.HTTP_1_1),\n\nSpanAttributes.HTTP_URL: url,\n\nSpanAttributes.NET_PEER_IP: \"127.0.0.1\",\n\n}\n\n)\n\nresp = requests.get(url)\n\nspan.set_attribute(SpanAttributes.HTTP_STATUS_CODE, resp.status_code)\n\nSo far, all the code written was done on the client side; let's talk about the server side. Before starting\n\non the server, in order to reduce the duplication of code, configure_tracer has been moved into a\n\nseparate common.py module and placed in the same directory as the rest of the code. In this refactor,\n\nwe've also updated the previously hardcoded service.name and service.version attribute keys to use\n\nvalues from the semantic conventions package: common.py\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.sdk.resources import Resource\n\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter from\n\nopentelemetry.semconv.resource import ResourceAttributes\n\nfrom local_machine_resource_detector import LocalMachineResourceDetector\n\ndef configure_tracer(name, version):\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter)\n\nlocal_resource = LocalMachineResourceDetector().detect()\n\nresource = local_resource.merge(\n\nResource.create(\n\n{\n\nResourceAttributes.SERVICE_NAME: name,\n\nResourceAttributes.SERVICE_VERSION: version,\n\n}\n\n)\n\n)\n\nprovider = TracerProvider(resource=resource)\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(name, version)\n\nThis code can now be used in both shopper.py and the new server code in grocery_store.py to\n\ninstantiate a tracer. The server code uses Flask (https://flask.palletsprojects.com/en/1.1.x/) to provide\n\nan API, and the initial code for the application will implement a single route handler. We won't dive\n\ntoo deeply into the nuts and bolts of how Flask works in this book. For the purpose of our\n\napplication, it's enough to know that the response handler can be configured with a path via the route\n\ndecorator and that the run method launches a web server. An additional decorator to create a span on\n\nthe handler sets the kind to SERVER, as it is the operation that is responding to the CLIENT span\n\ninstrumented previously. Note that in the code, there are also several attributes being set following\n\nthe semantic conventions; the Flask library conveniently makes most of the information available\n\nquite easily: grocery_store.py\n\nfrom flask import Flask, request\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.semconv.trace import HttpFlavorValues, SpanAttributes\n\nfrom opentelemetry.trace import SpanKind\n\nfrom common import configure_tracer\n\ntracer = configure_tracer(\"0.1.2\", \"grocery-store\")\n\napp = Flask(__name__)\n\n@app.route(\"/\")\n\n@tracer.start_as_current_span(\"welcome\", kind=SpanKind.SERVER)\n\ndef welcome():\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_FLAVOR: request.environ.get(\"SERVER_PROTOCOL\"),\n\nSpanAttributes.HTTP_METHOD: request.method,\n\nSpanAttributes.HTTP_USER_AGENT: str(request.user_agent),\n\nSpanAttributes.HTTP_HOST: request.host,\n\nSpanAttributes.HTTP_SCHEME: request.scheme,\n\nSpanAttributes.HTTP_TARGET: request.path,\n\nSpanAttributes.HTTP_CLIENT_IP: request.remote_addr,\n\n}\n\n)\n\nreturn \"Welcome to the grocery store!\"\n\nif __name__ == \"__main__\":\n\napp.run()\n\nOf course, to see the traces we must first run the application; to get the server running, use the\n\nfollowing command: python grocery_store.py\n\nIf another application is already running on the default port that Flask uses, 5000, you may encounter\n\nthe Address already in use error. Ensure only one instance of the server is running at any given\n\ntime.\n\nIMPORTANT NOTE\n\nIt's possible to run the server with debug mode enabled to have it automatically updated every time the code changes.\n\nThis is convenient when doing rapid development but should never be left enabled outside of development. Debug mode\n\nalso causes problems with auto-instrumentation, as we discussed in Chapter 3, Auto-Instrumentation. Enabling debug\n\nmode is accomplished by calling the run method as follows: run(debug=True).\n\nIn any future examples, the server will always need to be run before the shopper; otherwise, the client\n\napplication will throw HTTP connection exceptions. I find it particularly helpful to use a terminal\n\nthat supports split screens to have both the client and the server running side by side. Let's run both\n\napplications now and inspect the output data emitted. The server operation named / will be identified\n\nas a SERVER span: grocery_store.py output\n\n{\n\n\"name\": \"/\",\n\n\"context\": {\n\n\"trace_id\": \"0xe7f562a98f81a36ba81aaf1e239dd718\",\n\n\"span_id\": \"0x51daed87f12f5bc0\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.SERVER\", \"parent_id\": null,\n\n}\n\nOn the client side, the operation named web request will be identified as a CLIENT span: shopper.py\n\noutput\n\n{\n\n\"name\": \"web request\",\n\n\"context\": {\n\n\"trace_id\": \"0xc2747c6a8c7f7e12618bf69d7d71a1c8\",\n\n\"span_id\": \"0x88b7afb56d248244\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.CLIENT\", \"parent_id\": \"0xe756587bc381338c\",\n\n}\n\nThis new data is starting to help define the ties between different services and describe the\n\nrelationships between the components of the system, which is great. By exploring the tracing data\n\nalone, we can start getting a clearer idea of the role that each application plays. Oddly enough\n\nthough, the data we're currently generating doesn't appear to be fully connected yet. The trace_id\n\nidentifier between the client and the server doesn't match, and moreover, the SERVER span doesn't\n\ncontain parent_id; it seems we forgot about propagation!\n\nPropagating context\n\nGetting the information from one service to another across the network boundary requires some\n\nadditional work, namely, propagating the context. Without this context propagation, each service will\n\ngenerate a new trace independently, which means that the backend will not be able to tie the services\n\ntogether at analysis time. As shown in Figure 4.5, a trace without propagation between services is\n\nmissing the link between services, which means the traces will be more difficult to correlate:\n\nFigure 4.5 – Traces with and without propagation\n\nSpecifically, the data needed to propagate the context between services is span_context. This\n\nincludes four key pieces of information:\n\nspan_id: The identifier of the current span\n\ntrace_id: The identifier of the current trace\n\ntrace_flags: The additional configuration flags available to control tracing levels and sampling, as per the W3C Trace\n\nContext specification (https://www.w3.org/TR/trace-context/#trace-flags)\n\ntrace_state: A set of vendor-specific identification data as per the W3C Trace Context specification\n\n(https://www.w3.org/TR/trace-context/#tracestate-header)\n\nThe span_context information is used anytime a new span is started. trace_id is set as the current\n\nnew span's trace ID, and span_id will be used as the new span's parent ID. When a new span is\n\nstarted in a different service, if the context isn't propagated correctly, the new span has no information\n\nfrom which to pull the data it needs. Context must be serialized and injected across boundaries into a\n\ncarrier for propagation to occur. On the receiving end, the context must be extracted from the carrier\n\nand deserialized. The carrier medium used to transport the context, in the case of our application, is\n\nHTTP headers. OpenTelemetry's Propagators API provides the methods we'll use in the next\n\nexample. On the client side, we'll call the inject method to set span_context in a dictionary that will\n\nbe passed into the HTTP request as headers: shopper.py\n\nfrom opentelemetry.propagate import inject\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nwith tracer.start_as_current_span(\n\n\"web request\", kind=trace.SpanKind.CLIENT\n\n) as span:\n\nheaders = {}\n\ninject(headers)\n\nresp = requests.get(url, headers=headers)\n\nOn the server side, it is a little more complicated, as we need to ensure the context is extracted before\n\nthe decorator instantiates the span in the request handler. Conveniently, Flask has a mechanism\n\navailable via decorators to call methods before and after a request is handled. This allows us to\n\nextract the context from the request headers and attach to the context before the request handler is\n\ncalled. The call to attach will return a token that will be stored in the context of the request. Once the\n\nrequest has been processed, the call to detach restores the previous context: grocery_store.py\n\nfrom opentelemetry import context\n\nfrom opentelemetry.propagate import extract\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest.environ[\"context_token\"] = token\n\n@app.teardown_request\n\ndef teardown_request_func(err):\n\ntoken = request.environ.get(\"context_token\", None)\n\nif token:\n\ncontext.detach(token)\n\nTesting the new code will show that the context is now propagated; remember to restart the server as\n\nwell as run the client. Take a look at the following output, paying special attention to trace_id and\n\nspan_id: shopper.py output\n\n{\n\n\"name\": \"web request\",\n\n\"context\": {\n\n\"trace_id\": \"0x1fe2dc4e2e750e4598463749300277ed\", \"span_id\": \"0x5771b0a074e00a5b\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.CLIENT\",\n\n}\n\nIf everything went according to plan, the client and the server should be part of the same trace. The\n\noutput on the server side shows the span containing a parent_id field which matches the client's\n\nspan_id field. As well, note the trace_id field which matches on both sides of the request:\n\ngrocery_store.py output\n\n{\n\n\"name\": \"/\",\n\n\"context\": {\n\n\"trace_id\": \"0x1fe2dc4e2e750e4598463749300277ed\", \"span_id\": \"0x26f143d0f8a9c0bd\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.SERVER\",\n\n\"parent_id\": \"0x5771b0a074e00a5b\", }\n\nNow that the services are connected, let's explore propagation a bit further!\n\nAdditional propagator formats\n\nThe propagation we've used so far in the example is the W3C Trace Context propagation format.\n\nThe Trace Context format is fairly recent and is by no means the only propagation format out there.\n\nThere are additional propagators available to use with OpenTelemetry to support interoperability with\n\nother tracing standards. Additional propagators supported by OpenTelemetry and available at the time\n\nof writing include B3, Jaeger, and ot-trace. Currently supported propagators implement a\n\nTextMapPropagator interface with an inject method and an extract method. A propagator is\n\nconfigured globally using the set_global_textmap method. The following code shows an example of\n\nconfiguring a B3MultiFormat propagator for an application. This propagator can be found by\n\ninstalling the opentelemetry-propagator-b3 package: from opentelemetry.propagators.b3 import\n\nB3MultiFormat\n\nfrom opentelemetry.propagate import set_global_textmap\n\nset_global_textmap(B3MultiFormat())\n\nIMPORTANT NOTE\n\nTroubleshooting propagation issues can be difficult and time-consuming. Services can easily be misconfigured to\n\npropagate data using different formats and doing so will result in propagation not working at all.\n\nIf you decided to use the previous code in either the shopper or the grocery store applications but not\n\nboth, you may have noticed propagation breaking. It's not uncommon for applications in the wild to\n\nhave different propagation formats configured. Thankfully, it's possible to configure multiple\n\npropagators simultaneously in OpenTelemetry by using a composite propagator.\n\nComposite propagator\n\nA composite propagator allows users to configure multiple propagators from different cross-cutting\n\nconcerns. In its current implementation in many languages, the composite propagator can support\n\nmultiple propagators for the same signal. This functionality provides backward compatibility with\n\nolder systems while being future-proof. CompositePropagator has the same interface as any\n\npropagator but supports passing in a list of propagators at initialization. This list is then iterated\n\nthrough at injection and extraction time. This next example introduces one additional service, a\n\nlegacy inventory system that is configured to use B3 propagation. Figure 4.6 shows the flow of the\n\nrequest from the shopper, through the store, and to the inventory system that we will be adding in the\n\nnext example:\n\nFigure 4.6 – A request to the legacy inventory system\n\nSince the grocery store needs to propagate requests using both W3C Trace Context and B3, we'll\n\nneed to update the code to configure CompositePropagator to support this. The first thing to do before\n\ndiving into the code is to ensure that the B3 propagator package is installed: pip install\n\nopentelemetry-propagator-b3\n\nFor the sake of simplifying the server code, the following code shows a new method being added to\n\ncommon.py to set span attributes in a server handler. This new method,\n\nset_span_attributes_from_flask, can be used both in legacy_inventory.py (as we'll see shortly) and\n\nin grocery_store.py: common.py\n\nfrom flask import request\n\nfrom opentelemetry.semconv.trace import SpanAttributes\n\ndef set_span_attributes_from_flask():\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_FLAVOR: request.environ.get(\"SERVER_PROTOCOL\"),\n\nSpanAttributes.HTTP_METHOD: request.method,\n\nSpanAttributes.HTTP_USER_AGENT: str(request.user_agent),\n\nSpanAttributes.HTTP_HOST: request.host,\n\nSpanAttributes.HTTP_SCHEME: request.scheme,\n\nSpanAttributes.HTTP_TARGET: request.path,\n\nSpanAttributes.HTTP_CLIENT_IP: request.remote_addr,\n\n}\n\n)\n\nThe legacy_inventory.py service code is another Flask server application with a single handler that,\n\nfor now, returns a hardcoded list of items and quantities encoded using JSON. The following code is\n\nvery similar to the grocery store code. The configuration for both the Flask app and OpenTelemetry\n\nshould be familiar, the significant difference being how we configure OpenTelemetry in this new\n\nservice is the propagator, by calling set_global_textmap. It's also important to remember to set the\n\nport number to a different value than the default Flask port by passing an argument to app.run;\n\notherwise, we will run into a socket error when trying to run both grocery_store.py and\n\nlegacy_inventory.py: legacy_inventory.py\n\nfrom flask import Flask, jsonify, request\n\nfrom opentelemetry import context\n\nfrom opentelemetry.propagate import extract, set_global_textmap\n\nfrom opentelemetry.propagators.b3 import B3MultiFormat\n\nfrom opentelemetry.trace import SpanKind\n\nfrom common import configure_tracer, set_span_attributes_from_flask\n\ntracer = configure_tracer(\"legacy-inventory\", \"0.9.1\") app = Flask(__name__)\n\nset_global_textmap(B3MultiFormat())\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest.environ[\"context_token\"] = token\n\n@app.teardown_request\n\ndef teardown_request_func(err):\n\ntoken = request.environ.get(\"context_token\", None)\n\nif token:\n\ncontext.detach(token)\n\n@app.route(\"/inventory\")\n\n@tracer.start_as_current_span(\"/inventory\", kind=SpanKind.SERVER) def inventory():\n\nset_span_attributes_from_flask()\n\nproducts = [\n\n{\"name\": \"oranges\", \"quantity\": \"10\"}, {\"name\": \"apples\", \"quantity\": \"20\"}, ]\n\nreturn jsonify(products)\n\nif __name__ == \"__main__\":\n\napp.run(debug=True, port=5001)\n\nIn the grocery store application, we will configure CompositePropagator to support both the W3C\n\nTrace Context and B3 formats. Add the following code to grocery_store.py: grocery_store.py\n\nfrom opentelemetry.propagate import extract, inject, set_global_textmap\n\nfrom opentelemetry.propagators.b3 import B3MultiFormat\n\nfrom opentelemetry.propagators.composite import CompositePropagator\n\nfrom opentelemetry.trace.propagation import tracecontext\n\nset_global_textmap(CompositePropagator([tracecontext.TraceContextTextMapPropagator(),\n\nB3MultiFormat()])) Additionally, the following handler will be added to the store, which\n\nwill make a request to the legacy inventory service. The key thing to remember here is to\n\nensure the context is present in the headers by calling inject and that the headers are\n\npassed into the request: grocery_store.py\n\nimport requests\n\nfrom common import set_span_attributes_from_flask\n\n...\n\n@app.route(\"/\")\n\n@tracer.start_as_current_span(\"welcome\", kind=SpanKind.SERVER)\n\ndef welcome():\n\nset_span_attributes_from_flask()\n\nreturn \"Welcome to the grocery store!\"\n\n@app.route(\"/products\")\n\n@tracer.start_as_current_span(\"/products\", kind=SpanKind.SERVER) def products():\n\nset_span_attributes_from_flask()\n\nwith tracer.start_as_current_span(\"inventory request\") as span: url =\n\n\"http://localhost:5001/inventory\"\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_METHOD: \"GET\",\n\nSpanAttributes.HTTP_FLAVOR: str(HttpFlavorValues.HTTP_1_1),\n\nSpanAttributes.HTTP_URL: url,\n\nSpanAttributes.NET_PEER_IP: \"127.0.0.1\",\n\n}\n\n)\n\nheaders = {}\n\ninject(headers)\n\nresp = requests.get(url, headers=headers)\n\nreturn resp.text\n\nThe last change we need before trying this out is an update to the shopper application's browse\n\nmethod to send a request to the new endpoint: shopper.py\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nwith tracer.start_as_current_span(\n\n\"web request\", kind=trace.SpanKind.CLIENT\n\n) as span:\n\nurl = \"http://localhost:5000/products\"\n\nNow, we have a third application to launch; the following commands need to be run from separate\n\nterminal windows, and remember to ensure no other applications are running on ports 5000 and 5001\n\nto avoid socket errors: $ python ./legacy_inventory.py\n\n$ python ./grocery_store.py\n\n$ python ./shopper.py\n\nOnce the legacy inventory server is up and running, making a request from the shopper should yield\n\nsome exciting results. In the output, we'll be looking for trace_id to be consistent across all three\n\nservices, and, as in the previous example of propagation, parent_id of the server span should match\n\nspan_id of the corresponding client request span: shopper.py output\n\n\"name\": \"web request\",\n\n\"context\": {\n\n\"trace_id\": \"0xb2a655bfd008007711903d8a72130813\", \"span_id\": \"0x3c183afa2640a2bb\", },\n\nThe following output from the grocery store includes two spans. The span named /products\n\nrepresents the request received from the client, and if the context is successfully extracted, trace_id\n\nwill match the previous output. The second span is the request to the inventory service:\n\ngrocery_store.py output\n\n\"name\": \"/products\",\n\n\"context\": {\n\n\"trace_id\": \"0xb2a655bfd008007711903d8a72130813\", \"span_id\": \"0x77883e3459f83fb6\", },\n\n\"parent_id\": \"0x3c183afa2640a2bb\", ----\n\n\"name\": \"inventory request\",\n\n\"context\": {\n\n\"trace_id\": \"0xb2a655bfd008007711903d8a72130813\", \"span_id\": \"0x8137dbaaa3f40062\", },\n\n\"parent_id\": \"0x77883e3459f83fb6\", The last output is from the inventory service. Remember\n\nthat this service is using a different propagator format. If the propagation was\n\nconfigured correctly, trace_id should remain consistent with the other two services, and\n\nparent_id should reflect that the parent operation is the inventory request span:\n\nlegacy_inventory.py output\n\n\"name\": \"/inventory\",\n\n\"context\": {\n\n\"trace_id\": \"0xb2a655bfd008007711903d8a72130813\", \"span_id\": \"0x3306b21b8000912b\", },\n\n\"parent_id\": \"0x8137dbaaa3f40062\", This was a lot of work, but once you get propagation\n\nconfigured and working across a system, it's rare that you'll need to go back and make\n\nchanges to it. It's a set-it-and-forget-it type of operation. If you happen to be working\n\nwith a brand-new code base, choose a single propagation format and stick to it; it will\n\nsave you a lot of headaches. We've now grasped one of the most important concepts in\n\ndistributed tracing, the propagation of span context across systems. Let's take a look at\n\nwhere else propagation can help us.\n\nIMPORTANT NOTE\n\nA possible alternative when working with large code bases and multiple propagator formats is to always configure all\n\navailable propagation formats. This may seem like overkill, but sometimes, it makes sense to prioritize interoperability over\n\nsaving a few bytes.\n\nRecording events, exceptions, and status\n\nQuickly identifying when an issue arises is a key aspect of distributed tracing. As demonstrated in\n\nFigure 4.7 with the Jaeger interface, in many backends, traces that contain errors are highlighted to\n\nmake them easy to find for users of data:\n\nFigure 4.7 – The trace view in Jaeger\n\nIn the following sections, we will explore the facilities that OpenTelemetry provides to capture\n\nevents, record exceptions, and set the status of a span.\n\nEvents\n\nIn addition to attributes, an event provides the facility to record data about a span that occurs at a\n\nspecific time. Events are similar to logs in OpenTracing in that they contain a timestamp and can\n\ncontain a list of attributes or key/value pairs. An event is added via an add_event method on the span,\n\nwhich accepts a name argument and, optionally, a timestamp and a list of attributes, as shown in the\n\nfollowing code: shopper.py\n\nspan.add_event(\"about to send a request\")\n\nresp = requests.get(url, headers=headers)\n\nspan.add_event(\"request sent\", attributes={\"url\": url}, timestamp=0)\n\nAs you'll see in the following output, the list of events is kept in the order in which they are added;\n\nthey are not ordered by the timestamps they are recorded with: shopper.py output\n\n\"events\": [\n\n{\n\n\"name\": \"about to send a request\",\n\n\"timestamp\": \"2021-07-12T06:38:49.793903Z\",\n\n\"attributes\": {}\n\n},\n\n{\n\n\"name\": \"request sent\",\n\n\"timestamp\": \"1970-01-01T00:00:00.000000Z\",\n\n\"attributes\": {\n\n\"url\": \"http://localhost:5000/products\"\n\n}\n\n}\n\n],\n\nEvents differ from attributes in that they have a time dimension to them, which can be helpful to\n\nbetter understand the sequence of things inside a span. There are also events that have a special\n\nmeaning, as we'll see with exceptions.\n\nExceptions\n\nIn OpenTelemetry, the concepts of exceptions and the status of a span are intentionally kept separate.\n\nA span may contain many exceptions, but these exceptions don't necessarily mean that the status of\n\nthe span should be set as an error. For example, a user may want to record exceptions when a request\n\nis made to a specific service, but there may be retry logic that will cause the operation to eventually\n\nsucceed anyway. Recording those exceptions may be useful to identify areas of the code that can be\n\nimproved. The initial definition of an exception in the OpenTelemetry specification is that an\n\nexception is as follows:\n\nRecorded as an event\n\nThe specific name exception\n\nContains the minimum of either exception.type or an exception.message attribute\n\nThe following code records an exception if a request to the grocery store fails by creating one such\n\nevent. Let's add a try/except block in the browse method to capture the exception and change url to\n\nmake the request intentionally fail: shopper.py\n\ntry:\n\nurl = \"invalid_url\"\n\nresp = requests.get(url, headers=headers)\n\nspan.add_event(\n\n\"request sent\",\n\nattributes={\"url\": url},\n\ntimestamp=0,\n\n)\n\nspan.set_attribute(\n\nSpanAttributes.HTTP_STATUS_CODE,\n\nresp.status_code\n\n)\n\nexcept Exception as err:\n\nattributes = {\n\nSpanAttributes.EXCEPTION_MESSAGE: str(err),\n\n}\n\nspan.add_event(\"exception\", attributes=attributes)\n\nRunning the code will produce an exception that will be caught. This exception will then be recoded\n\nas an event and added to the tracing data emitted at the console: shopper.py output\n\n\"events\": [\n\n{\n\n\"name\": \"exception\",\n\n\"timestamp\": \"2021-07-10T04:13:05.287376Z\",\n\n\"attributes\": {\n\n\"exception.message\": \"Invalid URL 'invalid_url': No schema supplied. Perhaps you meant\n\nhttp://invalid_url?\"\n\n}\n\n}\n\n]\n\nAlthough this provides us with more information, it's not practical to have to write so many lines of\n\ncode every time we want to record an exception. Thankfully, the OpenTelemetry specification has\n\ndefined a span method in the API to address this. The following code replaces the code in the except\n\nblock of the previous example to use the record_exception method on the span, instead of manually\n\ncreating an event. Semantically, these are equivalent, but the method is much more convenient. The\n\nmethod accepts an exception as its first argument and supports optional parameters to pass in\n\nadditional event attributes, as well as a timestamp: shopper.py\n\ntry:\n\nurl = \"invalid_url\"\n\nresp = requests.get(url, headers=headers)\n\n...\n\nexcept Exception as err:\n\nspan.record_exception(err)\n\nNext time the code is run, the exception event is automatically generated for us. Taking a closer look\n\nat the output, it's even more useful than before, as we now see the following:\n\nThe message populated as we did before\n\nThe exception type\n\nA stack trace capturing exactly where in the code the exception was raised\n\nThis allows us to immediately find the problematic code and resolve the issue:\n\nshopper.py output\n\n\"events\": [\n\n{\n\n\"name\": \"exception\",\n\n\"timestamp\": \"2021-07-10T04:17:07.328665Z\",\n\n\"attributes\": {\n\n\"exception.type\": \"MissingSchema\",\n\n\"exception.message\": \"Invalid URL 'invalid_url': No schema supplied. Perhaps you meant\n\nhttp://invalid_url?\", \"exception.stacktrace\": \"Traceback (most recent call last):\\n File\n\n\\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/opentelemetry/trace/__init__.py\\\", line 522, in use_span\\n yield span\\n File\n\n\\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/opentelemetry/sdk/trace/__init__.py\\\", line 879, in start_as_current_span\\n yield\n\nspan_context\\n File \\\"/Users/alex/dev/cloud-native-observability/chapter4/./shopper.py\\\",\n\nline 110, in browse\\n resp = requests.get(\\\"invalid_url\\\", headers=headers)\\n File\n\n\\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/api.py\\\", line 76, in get\\n return request('get', url, params=params,\n\n**kwargs)\\n File \\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/api.py\\\", line 61, in request\\n return session.request(method=method,\n\nurl=url, **kwargs)\\n File \\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/sessions.py\\\", line 528, in request\\n prep = self.prepare_request(req)\\n\n\nFile \\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/sessions.py\\\", line 456, in prepare_request\\n p.prepare(\\n File\n\n\\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/models.py\\\", line 316, in prepare\\n self.prepare_url(url, params)\\n File\n\n\\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/models.py\\\", line 390, in prepare_url\\n raise\n\nMissingSchema(error)\\nrequests.exceptions.MissingSchema: Invalid URL 'invalid_url': No\n\nschema supplied. Perhaps you meant http://invalid_url?\\n\", \"exception.escaped\": \"False\"\n\n}\n\n}\n\n],\n\nThis type of detail about exceptions in a system is incredibly valuable when debugging, especially\n\nwhen the events may have occurred minutes, hours, or even days ago. It's worth noting that the\n\nformat of the stack trace is language-specific, as described in Figure 4.8 (https://github.com/open-\n\ntelemetry/opentelemetry-\n\nspecification/blob/main/specification/trace/semantic_conventions/exceptions.md#stacktrace-\n\nrepresentation):\n\nFigure 4.8 – The stack trace format per language\n\nAdditionally, the Python SDK also automatically captures uncaught exceptions and adds an exception\n\nevent to the span that is active when the exception occurs. We can update the code we just wrote in\n\nthe previous example to remove the try/except block, leaving the invalid URL. The following code\n\nhas the same effect as calling record_exception directly: shopper.py\n\nresp = requests.get(\"invalid_url\", headers=headers)\n\nRecording exceptions in spans is valuable, but in the event that it is preferable not to do so, it's\n\npossible to set an optional flag when creating a span to disable the functionality. You can try it in the\n\nprevious example by setting the record_exception optional argument, as follows: shopper.py\n\nwith tracer.start_as_current_span(\n\n\"web request\", kind=trace.SpanKind.CLIENT, record_exception=False\n\n) as span:\n\nNow that we understand how exceptions are recorded, let's further investigate how or even if these\n\nexceptions connect to the status of a span.\n\nStatus\n\nAs mentioned previously in this chapter, the span status has significant benefits to users. Quickly\n\nbeing able to filter through traces based on the span status makes things much easier for operators.\n\nThe status is composed of a status code and, optionally, a description. There are currently three\n\nsupported span status codes:\n\nUNSET\n\nOK\n\nERROR\n\nThe default status code on any new span is UNSET. This default behavior ensures that when a span\n\nstatus code is set to OK, it has been done intentionally. An earlier version of the specification defaulted\n\na span status code to OK, which left room for misinterpretations – was the span really OK or did the\n\ncode return before an error status code was set? The decision to set the span status is really up to the\n\napplication developer or operators of the service. The interface to set a status on a span receives a\n\nStatus object, which is composed of StatusCode and a description string. This next example sets the\n\nspan status code to OK based on the response from the web request. Note that we're using a feature of\n\nthe Requests library's Response object to return True if the HTTP status code on the response is\n\nbetween 200 and 400: shopper.py\n\nfrom opentelemetry.trace import Status, StatusCode\n\ndef browse():\n\nwith tracer.start_as_current_span(\n\n\"web request\", kind=trace.SpanKind.CLIENT, record_exception=False ) as span:\n\nurl = \"http://localhost:5000/products\"\n\nresp = requests.get(url, headers=headers)\n\nif resp:\n\nspan.set_status(Status(StatusCode.OK))\n\nelse:\n\nspan.set_status(\n\nStatus(StatusCode.ERROR, \"status code: {}\".format(resp.status_code))\n\n)\n\nWith the code in place, test the application first with the http://localhost:5000/products URL to see\n\nthe following output when a valid URL is used: shopper.py output with valid URL\n\n\"status\": {\n\n\"status_code\": \"OK\"\n\n}\n\nNow, update the URL to an invalid endpoint such as http://localhost:5000/invalid to see the following\n\noutput when the response contains an error code: shopper.py output with invalid URL\n\n\"status\": {\n\n\"status_code\": \"ERROR\",\n\n\"description\": \"status code: 404\"\n\n}\n\nIMPORTANT NOTE\n\nThe description field will only be used if the status code is set to ERROR; it is ignored otherwise.\n\nAnother thing to note about status codes is that, as per semantic convention, instrumentation libraries\n\nshould not change the status code to OK unless they are providing a configuration option to do this.\n\nThis is to prevent having an instrumentation library unexpectedly change the outcome of the span.\n\nThey are, however, encouraged to set the status code to ERROR when errors defined in the semantic\n\nconvention for the type of instrumentation library are encountered.\n\nAs with recording exceptions, it's also possible to configure spans to automatically set the status\n\nwhen an exception occurs. This is accomplished via a set_status_on_exception argument, available\n\nwhen starting a span: shopper.py\n\nwith tracer.start_as_current_span(\n\n\"web request\",\n\nkind=trace.SpanKind.CLIENT,\n\nset_status_on_exception=True,\n\n) as span:\n\nPlay around with the code and see what the status output is when using this setting. Although it may\n\nseem like a lot of work, handling errors and setting the status on spans meaningfully will make a\n\nworld of difference at analysis time. Not only that, but having to work through the different scenarios\n\nin the code at instrumentation time is a forcing function to really ensure a solid understanding of what\n\nthe code is expected to do. And when things go wrong, as they will, having this data will make a\n\nworld of difference.\n\nSummary\n\nAnd just like that, you've explored many important concepts of the tracing signal in OpenTelemetry!\n\nThere was quite a bit to grasp in this chapter, but hopefully, the concepts we've been exploring so far\n\nare starting to make more sense now that there's some code behind them. With this knowledge, you\n\nnow know how to configure different components of the OpenTelemetry tracing pipeline to obtain a\n\ntracer and export data to the console. You also have the ability to start spans in various ways,\n\ndepending on your application's needs. We then spent some time improving the data emitted by\n\nenriching it using attributes, resources, and resource detectors. Last but not least, we took a look at\n\nthe important topic of events, status, and exceptions to capture some important information about\n\nerrors when they happen in code.\n\nOur understanding of the Context API will allow us to share information across our application, and\n\nknowing how to use the Propagation API will allow us to ensure that information is shared across\n\napplication boundaries.\n\nAlthough you probably have many more questions, you now know enough to look through some\n\nexisting applications or plan ahead for instrumenting new applications through distributed tracing. As\n\nsome of the components we've explored in this chapter are similar across signals, many of the\n\nconcepts that may not quite make sense yet will become clearer as we take a look at the next chapter,\n\nwhich looks at metrics. Let's go measure some things!\n\nChapter 5: Metrics – Recording Measurements\n\nTracing code execution throughout a system is one way to capture information about what is\n\nhappening in an application, but what if we're looking to measure something that would be better\n\nserved by a more lightweight option than a trace? Now that we've learned how to generate distributed\n\ntraces using OpenTelemetry, it's time to look at the next signal: metrics. As we did in Chapter 4,\n\nDistributed Tracing – Tracing Code Execution, we will first look at configuring the OpenTelemetry\n\npipeline to produce metrics. Then, we'll continue to improve the telemetry emitted by the grocery\n\nstore application by using the instruments OpenTelemetry puts at our disposal. In this chapter, we\n\nwill do the following:\n\nConfigure OpenTelemetry to collect, aggregate, and export metrics to the terminal.\n\nGenerate metrics using the different instruments available.\n\nUse metrics to gain a better understanding of the grocery store application.\n\nAugmenting the grocery store application will allow us to put the different instruments into practice\n\nto grasp better how each instrument can be used to record measurements. As we explore other metrics\n\nthat are useful to produce for cloud-native applications, we will seek to understand some of the\n\nquestions we may answer using each instrument.\n\nTechnical requirements\n\nAs with the examples in the previous chapter, the code is written using Python 3.8, but\n\nOpenTelemetry Python supports Python 3.6+ at the time of writing. Ensure you have a compatible\n\nversion installed on your system following the instructions at\n\nhttps://docs.python.org/3/using/index.html. To verify that a compatible version is installed on your\n\nsystem, run the following commands: $ python --version\n\n$ python3 --version\n\nOn many systems, both python and python3 point to the same installation, but this is not always the\n\ncase, so it's good to be aware of this if one points to an unsupported version. In all examples, running\n\napplications in Python will call the python command, but they can also be run via the python3\n\ncommand, depending on your system.\n\nThe first few examples in this chapter will show a standalone example exploring how to configure\n\nOpenTelemetry to produce metrics. The code will require the OpenTelemetry API and SDK\n\npackages, which we'll install via the following pip command: $ pip install opentelemetry-api==1.10.0\n\n\\\n\nopentelemetry-sdk==1.10.0 \\\n\nopentelemetry-propagator-b3==1.10.0\n\nAdditionally, we will use the Prometheus exporter to demonstrate a pull-based exporter to emit\n\nmetrics. This exporter can be installed via pip as well: $ pip install opentelemetry-exporter-\n\nprometheus==0.29b0\n\nFor the later examples involving the grocery store application, you can download the sample from\n\nChapter 4, Distributed Tracing – Tracing Code Execution, and add the code along with the examples.\n\nThe following git command will clone the companion repository: $ git clone\n\nhttps://github.com/PacktPublishing/Cloud-Native-Observability\n\nThe chapter04 directory in the repository contains the code for the grocery store. The complete\n\nexample, including all the code in the examples from this chapter, is available in the chapter05\n\ndirectory. I recommend adding the code following the examples and using the complete example\n\ncode as a reference if you get into trouble. Also, if you haven't read Chapter 4, Distributed Tracing –\n\nTracing Code Execution, it may be helpful to skim through the details of how the grocery store\n\napplication is built in that chapter to get your bearings.\n\nThe grocery store depends on the Requests library (https://docs.python-requests.org/) to make web\n\nrequests at various points and the Flask library (https://flask.palletsprojects.com) to provide a\n\nlightweight web server for some of the services. Both libraries can be installed via the following pip\n\ncommand: $ pip install flask requests\n\nAdditionally, the chapter will utilize a third-party open source tool (https://github.com/rakyll/hey) to\n\ngenerate some load on the web application. The tool can be downloaded from the repository. The\n\nfollowing commands download the macOS binary and rename it to hey using curl with the -o flag,\n\nthen ensure the binary is executable using chmod: $ curl -o hey https://hey-release.s3.us-east-\n\n2.amazonaws.com/hey_darwin_amd64\n\n$ chmod +x ./hey\n\nIf you have a different load generation tool you're familiar with, and there are many, feel free to use\n\nthat instead if you prefer. This should be everything we need to start; let's start measuring!\n\nConfiguring the metrics pipeline\n\nThe metrics signal was designed to be conceptually similar to the tracing signal. The metrics pipeline\n\nconsists of the following:\n\nA MeterProvider to determine how metrics should be generated and provide access to a meter.\n\nThe meter is used to create instruments, which are used to record measurements.",
      "page_number": 89
    },
    {
      "number": 5,
      "title": "Metrics – Recording Measurements",
      "start_page": 129,
      "end_page": 163,
      "detection_method": "regex_chapter",
      "content": "Views allow the application developer to filter and process metrics produced by the software development kit (SDK).\n\nA MetricReader, which collects metrics being recorded.\n\nThe MetricExporter provides a mechanism to translate metrics into an output format for various protocols.\n\nThere are quite a few components, and a picture always helps me grasp concepts more quickly. The\n\nfollowing figure shows us the different elements in the pipeline:\n\nFigure 5.1 – Metrics pipeline\n\nMeterProvider can be associated with a resource to identify the source of metrics produced. We'll see\n\nshortly how we can reuse the LocalMachineResourceDetector we created in Chapter 4, Distributed\n\nTracing – Tracing Code Execution, with metrics. For now, the first example instantiates\n\nMeterProvider with an empty resource. The code then calls the set_meter_provider global method to\n\nset the MeterProvider for the entire application.\n\nAdd the following code to a new file named metrics.py. Later in the chapter, we will refactor the\n\ncode to add a MeterProvider to the grocery store, but to get started, the simpler, the better.\n\nmetrics.py\n\nfrom opentelemetry._metrics import set_meter_provider\n\nfrom opentelemetry.sdk._metrics import MeterProvider\n\nfrom opentelemetry.sdk.resources import Resource\n\ndef configure_meter_provider():\n\nprovider = MeterProvider(resource=Resource.create())\n\nset_meter_provider(provider)\n\nif __name__ == \"__main__\":\n\nconfigure_meter_provider()\n\nRun the code with the following command to ensure it runs without any errors:\n\npython ./metrics.py\n\nNo errors and no output? Well done, you're on the right track!\n\nIMPORTANT NOTE\n\nThe previous code shows that the metric modules are located at _metrics. This will change to metrics once the\n\npackages have been marked stable. Depending on when you're reading this, it may have already happened.\n\nNext, we'll need to configure an exporter to tell our application what to do with metrics once they're\n\ngenerated. The OpenTelemetry SDK contains ConsoleMetricExporter that emits metrics to the\n\nconsole, useful when getting started and debugging. PeriodicExportingMetricReader can be\n\nconfigured to periodically export metrics. The following code configures both components and adds\n\nthe reader to the MeterProvider. The code sets the export interval to 5000 milliseconds, or 5 seconds,\n\noverriding the default of 60 seconds: metrics.py\n\nfrom opentelemetry._metrics import set_meter_provider\n\nfrom opentelemetry.sdk._metrics import MeterProvider\n\nfrom opentelemetry.sdk.resources import Resource\n\nfrom opentelemetry.sdk._metrics.export import (\n\nConsoleMetricExporter,\n\nPeriodicExportingMetricReader,\n\n)\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nprovider = MeterProvider(metric_readers=[reader], resource=Resource.create())\n\nset_meter_provider(provider)\n\nif __name__ == \"__main__\":\n\nconfigure_meter_provider()\n\nRun the code once more. The expectation is that the output from running the code will still not show\n\nanything. The only reason to run the code is to ensure our dependencies are fulfilled, and there are no\n\ntypos.\n\nIMPORTANT NOTE\n\nLike TracerProvider, MeterProvider uses a default no-op implementation in the API. This allows developers to\n\ninstrument code without worrying about the details of how metrics will be generated. It does mean that unless we\n\nremember to set the global MeterProvider to use MeterProvider from the SDK package, any calls made to the API\n\nto generate metrics will result in no metrics being generated. This is one of the most common gotchas for folks working\n\nwith OpenTelemetry.\n\nWe're almost ready to start producing metrics with an exporter, a metric reader, and a MeterProvider\n\nconfigured. The next step is getting a meter.\n\nObtaining a meter\n\nWith MeterProvider globally configured, we can use a global method to obtain a meter. As mentioned\n\nearlier, the meter will be used to create instruments, which will be used throughout the application\n\ncode to record measurements. The meter receives the following arguments at creation time:\n\nThe name of the application or library generating metrics\n\nAn optional version identifies the version of the application or library producing the telemetry\n\nAn optional schema_url to describe the data generated\n\nIMPORTANT NOTE\n\nThe schema URL was introduced in OpenTelemetry as part of the OpenTelemetry Enhancement Proposal 152\n\n(https://github.com/open-telemetry/oteps/blob/main/text/0152-telemetry-schemas.md). The goal of schemas is to\n\nprovide OpenTelemetry instrumented applications a way to signal to external systems consuming the telemetry what\n\nthe semantic versioning of the data produced will look like. Schema URL parameters are optional but recommended\n\nfor all producers of telemetry: meters, tracers, and log emitters.\n\nThis information is used to identify the application or library producing the metrics. For example,\n\napplication A making a web request via the requests library may contain more than one meter:\n\nThe first meter is created by application A with a name identifying it with the version number matching the application.\n\nA second meter is created by the requests instrumentation library with the name opentelemetry-instrumentation-\n\nrequests and the instrumentation library version.\n\nThe urllib instrumentation library creates the third meter with the name opentelemetry-instrumentation-urllib,\n\na library utilized by the requests library.\n\nHaving a name and a version identifier is critical in differentiating the source of the metrics. As we'll\n\nsee later in the chapter, when we look at the Views section, this identifying information can also be\n\nused to filter out the telemetry we're not interested in. The following code uses the\n\nget_meter_provider global API method to access the global MeterProvider we configured earlier,\n\nand then calls get_meter with a name, version, and schema_url parameter: metrics.py\n\nfrom opentelemetry._metrics import get_meter_provider, set_meter_provider ...\n\nif __name__ == \"__main__\":\n\nconfigure_meter_provider()\n\nmeter = get_meter_provider().get_meter(\n\nname=\"metric-example\",\n\nversion=\"0.1.2\",\n\nschema_url=\" https://opentelemetry.io/schemas/1.9.0\",\n\n)\n\nIn OpenTelemetry, instruments used to record measurements are associated with a single meter and\n\nmust have unique names within the context of that meter.\n\nPush-based and pull-based exporting\n\nOpenTelemetry supports two methods for exporting metrics data to external systems: push-based and\n\npull-based. A push-based exporter sends measurements from the application to a destination at a\n\nregular interval on a trigger. This trigger could be a maximum number of metrics to transfer or a\n\nschedule. The push-based method will be familiar to users of StatsD\n\n(https://github.com/statsd/statsd), where a network daemon opens a port and listens for metrics to be\n\nsent to it. Similarly, the ConsoleSpanExporter for the tracing signal in Chapter 4, Distributed Tracing\n\n– Tracing Code Execution, is a push-based exporter.\n\nOn the other hand, a pull-based exporter exposes an endpoint pulled from or scraped by an external\n\nsystem. Most commonly, a pull-based exporter exposes this information via a web endpoint or a local\n\nsocket; this is the method popularized by Prometheus (https://prometheus.io). The following diagram\n\nshows the data flow comparison between a push and a pull model:\n\nFigure 5.2 – Push versus pull-based reporting\n\nNotice the direction of the arrow showing the interaction between the exporter and an external\n\nsystem. When configuring a pull-based exporter, remember that system permissions may need to be\n\nconfigured to allow an application to open a new port for incoming requests. One such pull-based\n\nexporter defined in the OpenTelemetry specification is the Prometheus exporter.\n\nThe pipeline configuration for a pull exporter is slightly less complex. The metric reader interface can\n\nbe used as a single point to collect and expose metrics in the Prometheus format. The following code\n\nshows how to expose a Prometheus endpoint on port 8000 using the start_http_server method from\n\nthe Prometheus client library. It then configures PrometheusMetricReader with a prefix parameter to\n\nprovide a namespace for all metrics generated by our application. Finally, the code adds a call waiting\n\nfor input from the user before exiting; this gives us a chance to see the exposed metrics before the\n\napplication exits: from opentelemetry.exporter.prometheus import PrometheusMetricReader\n\nfrom prometheus_client import start_http_server\n\ndef configure_meter_provider():\n\nstart_http_server(port=8000, addr=\"localhost\")\n\nreader = PrometheusMetricReader(prefix=\"MetricExample\")\n\nprovider = MeterProvider(metric_readers=[reader], resource=Resource.create())\n\nset_meter_provider(provider)\n\nif __name__ == \"__main__\":\n\n...\n\ninput(\"Press any key to exit...\")\n\nIf you run the application now, you can use a browser to see the Prometheus formatted data available\n\nby visiting http://localhost:8000. Alternatively, you can use the curl command to see the output\n\ndata in the terminal as per the following example: $ curl http://localhost:8000\n\n# HELP python_gc_objects_collected_total Objects collected during gc\n\n# TYPE python_gc_objects_collected_total counter\n\npython_gc_objects_collected_total{generation=\"0\"} 1057.0\n\npython_gc_objects_collected_total{generation=\"1\"} 49.0\n\npython_gc_objects_collected_total{generation=\"2\"} 0.0\n\n# HELP python_gc_objects_uncollectable_total Uncollectable object found during GC\n\n# TYPE python_gc_objects_uncollectable_total counter\n\npython_gc_objects_uncollectable_total{generation=\"0\"} 0.0\n\npython_gc_objects_uncollectable_total{generation=\"1\"} 0.0\n\npython_gc_objects_uncollectable_total{generation=\"2\"} 0.0\n\n# HELP python_gc_collections_total Number of times this generation was collected\n\n# TYPE python_gc_collections_total counter\n\npython_gc_collections_total{generation=\"0\"} 55.0\n\npython_gc_collections_total{generation=\"1\"} 4.0\n\npython_gc_collections_total{generation=\"2\"} 0.0\n\n# HELP python_info Python platform information\n\n# TYPE python_info gauge\n\npython_info{implementation=\"CPython\",major=\"3\",minor=\"8\",patchlevel=\"0\",version=\"3.9.0\"}\n\n1.0\n\nThe Prometheus client library generates the previous data; note that there are no OpenTelemetry\n\nmetrics generated by our application, which makes sense since we haven't generated anything yet!\n\nWe'll get to that next. We'll see in Chapter 11, Diagnosing Problems, how to integrate OpenTelemetry\n\nwith a Prometheus backend. For the sake of simplicity, the remainder of the examples in this chapter\n\nwill be using the push-based ConsoleMetricExporter configured earlier. If you're more familiar with\n\nPrometheus, please use this configuration instead.\n\nChoosing the right OpenTelemetry instrument\n\nWe're now ready to generate metrics from our application. If you recall, in tracing, the tracer\n\nproduces spans, which are used to create distributed traces. By contrast, the meter does not generate\n\nmetrics; an instrument does. The meter's role is to produce instruments. OpenTelemetry offers many\n\ndifferent instruments to record measurements. The following figure shows a list of all the instruments\n\navailable:\n\nFigure 5.3 – OpenTelemetry instruments\n\nEach instrument has a specific purpose, and the correct instrument depends on the following:\n\nThe type of measurement being recorded\n\nWhether the measurement must be done synchronously\n\nWhether the values being recorded are monotonic or not\n\nFor synchronous instruments, a method is called on the instrument when it is time for a\n\nmeasurement to be recorded. For asynchronous instruments, a callback method is configured at the\n\ninstrument's creation time.\n\nEach instrument has a name and kind property. Additionally, a unit and a description may be specified.\n\nCounter\n\nA counter is a commonly available instrument across metric ecosystems and implementations over\n\nthe years, although its definition across systems varies. In OpenTelemetry, a counter is an increasing\n\nmonotonic instrument, only supporting non-negative value increases. The following diagram shows a\n\nsample graph representing a monotonic counter:\n\nFigure 5.4 – Increasing monotonic counter graph\n\nA counter can be used to represent the following:\n\nNumber of requests received\n\nCount of orders processed\n\nCPU time utilization\n\nThe following code instantiates a counter to keep a tally of the number of items sold in the grocery\n\nstore. The code uses the add method to increment the counter and passes the locale of the customer as\n\nan attribute: metrics.py\n\nif __name__ == \"__main__\":\n\n...\n\ncounter = meter.create_counter(\n\n\"items_sold\",\n\nunit=\"items\",\n\ndescription=\"Total items sold\"\n\n)\n\ncounter.add(6, {\"locale\": \"fr-FR\", \"country\": \"CA\"})\n\ncounter.add(1, {\"locale\": \"es-ES\"})\n\nRunning the code outputs the counter with all its attributes:\n\noutput\n\n{\"attributes\": {\"locale\": \"fr-FR\", \"country\": \"CA\"}, \"description\": \"Total items sold\",\n\n\"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"items_sold\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\": 1646535699616146000,\n\n\"time_unix_nano\": 1646535699616215000, \"value\": 7, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": true}}\n\n{\"attributes\": {\"locale\": \"es-ES\"}, \"description\": \"Total items sold\",\n\n\"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"items_sold\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\": 1646535699616215001,\n\n\"time_unix_nano\": 1646535699616237000, \"value\": 0, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": true}}\n\nNote that the attributes themselves do not influence the value of the counter. They are only\n\naugmenting the telemetry with additional dimensions about the transaction. A monotonic instrument\n\nlike the counter cannot receive a negative value. The following code tries to add a negative value: if\n\n__name__ == \"__main__\":\n\n...\n\ncounter.add(6, {\"locale\": \"fr-FR\", \"country\": \"CA\"}) counter.add(-1, {\"unicorn\": 1})\n\nThis code results in the following warning, which provides the developer with a helpful hint:\n\noutput\n\nAdd amount must be non-negative on Counter items_sold.\n\nKnowing to use the right instrument can help avoid generating unexpected data. It's also good to\n\nconsider adding validation to the data being passed into instruments when unsure of the data source.\n\nAsynchronous counter\n\nThe asynchronous counter can be used as a counter. Its only difference is that it is used\n\nasynchronously. Asynchronous counters can represent data that is only ever-increasing, and that may\n\nbe too costly to report synchronously or is more appropriate to record on set intervals. Some\n\nexamples of this would be reporting the following:\n\nCPU time utilized by a process\n\nTotal network bytes transferred\n\nThe following code shows us how to create an asynchronous counter using the\n\nasync_counter_callback callback method, which will be called every time\n\nPeriodExportingMetricReader executes. To ensure the instrument has a chance to record a few\n\nmeasurements, we've added sleep in the code as well to pause the code before exiting: metrics.py\n\nimport time\n\nfrom opentelemetry._metrics.measurement import Measurement\n\ndef async_counter_callback():\n\nyield Measurement(10)\n\nif __name__ == \"__main__\":\n\n...\n\n# async counter\n\nmeter.create_observable_counter(\n\nname=\"major_page_faults\",\n\ncallback=async_counter_callback,\n\ndescription=\"page faults requiring I/O\",\n\nunit=\"fault\",\n\n)\n\ntime.sleep(10)\n\nIf you haven't commented out the output from the instrument, you should see the output from both\n\ncounters now. The following output omits the previous example's output for brevity: output\n\n{\"attributes\": \"\", \"description\": \"page faults requiring I/O\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"major_page_faults\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language':\n\n'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0',\n\n'service.name': 'unknown_service'}, maxlen=None)\", \"unit\": \"fault\", \"point\":\n\n{\"start_time_unix_nano\": 1646538230507539000, \"time_unix_nano\": 1646538230507614000,\n\n\"value\": 10, \"aggregation_temporality\": 2, \"is_monotonic\": true}}\n\n{\"attributes\": \"\", \"description\": \"page faults requiring I/O\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"major_page_faults\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language':\n\n'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0',\n\n'service.name': 'unknown_service'}, maxlen=None)\", \"unit\": \"fault\", \"point\":\n\n{\"start_time_unix_nano\": 1646538230507539000, \"time_unix_nano\": 1646538235507059000,\n\n\"value\": 20, \"aggregation_temporality\": 2, \"is_monotonic\": true}}\n\nThese counters are great for ever-increasing values, but measurements go up and down sometimes.\n\nLet's see what OpenTelemetry has in store for that.\n\nAn up/down counter\n\nThe following instrument is very similar to the counter. As you may have guessed from its name, the\n\ndifference between the counter and the up/down counter is that the latter can record values that go\n\nup and down; it is non-monotonic. The following diagram shows us what a graph representing a non-\n\nmonotonic counter may look like:\n\nFigure 5.5 – Non-monotonic counter graph\n\nCreating an UpDownCounter instrument is done via the create_up_down_counter method. Increment\n\nand decrement operations are done via the single add method with either positive or negative values:\n\nmetrics.py\n\nif __name__ == \"__main__\":\n\n...\n\ninventory_counter = meter.create_up_down_counter(\n\nname=\"inventory\",\n\nunit=\"items\",\n\ndescription=\"Number of items in inventory\",\n\n)\n\ninventory_counter.add(20)\n\ninventory_counter.add(-5)\n\nThe previous example's output will be as follows:\n\noutput\n\n{\"attributes\": \"\", \"description\": \"Number of items in inventory\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"inventory\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\":\n\n1646538574503018000, \"time_unix_nano\": 1646538574503083000, \"value\": 15,\n\n\"aggregation_temporality\": 2, \"is_monotonic\": false}}\n\nNote the previous example only emits a single metric. This is expected as the two recordings were\n\naggregated into a single value for the period reported.\n\nAsynchronous up/down counter\n\nAs you may imagine, as the counter has an asynchronous counterpart, so does UpDownCounter. The\n\nasynchronous up/down counter allows us to increment or decrement a value on a set interval. As\n\nyou will see shortly, it is pretty similar in nature to the asynchronous gauge. The main difference\n\nbetween the two is that the asynchronous up/down counter should be used when the values being\n\nrecorded are additive in nature, meaning the measurements can be added across dimensions. Some\n\nexamples of metrics that could be recorded via this instrument are as follows:\n\nChanges in the number of customers in a store\n\nNet revenue for an organization across business units\n\nThe following creates an asynchronous up/down counter to keep track of the current number of\n\ncustomers in a store. Note that, unlike its synchronous counterpart, the value recorded in the\n\nasynchronous up/down counter is an absolute value, not a delta. As per the previous asynchronous\n\nexample, an async_updowncounter_callback callback method does the work of reporting the measure:\n\nmetrics.py\n\ndef async_updowncounter_callback():\n\nyield Measurement(20, {\"locale\": \"en-US\"})\n\nyield Measurement(10, {\"locale\": \"fr-CA\"})\n\nif __name__ == \"__main__\":\n\n...\n\nupcounter_counter = meter.create_observable_up_down_counter(\n\nname=\"customer_in_store\",\n\ncallback=async_updowncounter_callback,\n\nunit=\"persons\",\n\ndescription=\"Keeps a count of customers in the store\"\n\n)\n\nThe output will start to look familiar based on the previous examples we've already run through:\n\noutput\n\n{\"attributes\": {\"locale\": \"en-US\"}, \"description\": \"Keeps a count of customers in the\n\nstore\", \"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"customer_in_store\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"persons\", \"point\": {\"start_time_unix_nano\": 1647735390164970000,\n\n\"time_unix_nano\": 1647735390164986000, \"value\": 20, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": false}}\n\n{\"attributes\": {\"locale\": \"fr-CA\"}, \"description\": \"Keeps a count of customers in the\n\nstore\", \"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"customer_in_store\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"persons\", \"point\": {\"start_time_unix_nano\": 1647735390164980000,\n\n\"time_unix_nano\": 1647735390165009000, \"value\": 10, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": false}}\n\nCounters and up/down counters are suitable for many data types, but not all. Let's see what other\n\ninstruments allow us to measure.\n\nHistogram\n\nA histogram instrument is useful when comparing the frequency distribution of values across large\n\ndata sets. Histograms use buckets to group the data they represent and effectively identify outliers or\n\nanomalies. Some examples of data representable by histograms are as follows:\n\nResponse times for requests to a service\n\nThe height of individuals\n\nFigure 5.6 shows a sample histogram chart representing the response time for requests. It looks like a\n\nbar chart, but it differs in that each bar represents a bucket containing a range for the values it\n\ncontains. The y axis represents the count of elements in each bucket:\n\nFigure 5.6 – Histogram graph\n\nTo capture information in a histogram, the buckets specified must be able to contain all the values it\n\nexpects to record. For example, take a histogram containing two buckets with explicit upper bounds\n\nof 0 ms and 10 ms. Any measurement greater than 10 ms bound would be excluded from the\n\nhistogram. Both Prometheus and OpenTelemetry address this by capturing any value beyond the\n\nmaximum upper boundary in an additional bucket. The histograms we'll explore in this chapter all\n\nuse explicit boundaries, but OpenTelemetry also provides experimental support for exponential\n\nhistograms (https://github.com/open-telemetry/opentelemetry-\n\nspecification/blob/main/specification/metrics/datamodel.md#exponentialhistogram).\n\nHistograms can be, and are often, used to calculate percentiles. The following code creates a\n\nhistogram via the create_histogram method. The method used to produce a metric with a histogram\n\nis named record: metrics.py\n\nif __name__ == \"__main__\":\n\n...\n\nhistogram = meter.create_histogram(\n\n\"response_times\",\n\nunit=\"ms\",\n\ndescription=\"Response times for all requests\",\n\n)\n\nhistogram.record(96)\n\nhistogram.record(9)\n\nIn this example, we record two measurements that fall into separate buckets. Notice how they appear\n\nin the output: output\n\n{\"attributes\": \"\", \"description\": \"Response times for all requests\",\n\n\"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"response_times\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\": 1646539219677439000,\n\n\"time_unix_nano\": 1646539219677522000, \"bucket_counts\": [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n\n\"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0, 500.0, 1000.0], \"sum\":\n\n105, \"aggregation_temporality\": 2}}\n\nAs with the counter and up/down counter, the histogram is synchronous.\n\nAsynchronous gauge\n\nThe last instrument defined by OpenTelemetry is the asynchronous gauge. This instrument can be\n\nused to record measurements that are non-additive in nature; in other words, which wouldn't make\n\nsense to sum together. An asynchronous gauge can represent the following:\n\nThe average memory consumption of a system\n\nThe temperature of a data center\n\nThe following code uses Python's built-in resource module to measure the maximum resident set size\n\n(https://en.wikipedia.org/wiki/Resident_set_size). This value is set in async_gauge_callback, which is\n\nused as the callback for the gauge we're creating: metrics.py\n\nimport resource\n\ndef async_gauge_callback():\n\nrss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n\nyield Measurement(rss, {})\n\nif __name__ == \"__main__\":\n\n...\n\nmeter.create_observable_gauge(\n\nname=\"maxrss\",\n\nunit=\"bytes\",\n\ncallback=async_gauge_callback,\n\ndescription=\"Max resident set size\",\n\n)\n\ntime.sleep(10)\n\nRunning the code will show us memory consumption information about our application using\n\nOpenTelemetry: output\n\n{\"attributes\": \"\", \"description\": \"Max resident set size\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"maxrss\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\", \"unit\": \"bytes\", \"point\": {\"time_unix_nano\":\n\n1646539432021601000, \"value\": 18341888}}\n\n{\"attributes\": \"\", \"description\": \"Max resident set size\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"maxrss\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\", \"unit\": \"bytes\", \"point\": {\"time_unix_nano\":\n\n1646539437018742000, \"value\": 19558400}}\n\nExcellent, we now know about the instruments and have started generating a steady metrics stream.\n\nThe last topic about instruments to be covered is duplicate instruments.\n\nDuplicate instruments\n\nDuplicate instrument registration conflicts arise if more than one instrument is created within a single\n\nmeter with the same name. This can potentially produce semantic errors in the data, as many\n\ntelemetry backends uniquely identify metrics via their names. Conflicting instruments may be\n\nintentional when two separate code paths need to report the same metric, or, when multiple\n\ndevelopers want to record different metrics but accidentally use the same name; naming things is\n\nhard. There are a few ways the OpenTelemetry SDK handles conflicting instruments:\n\nIf the conflicting instruments are identical, the values recorded by these instruments are aggregated. The data generated appears as\n\nthough a single instrument produced them.\n\nIf the instruments are not identical, but the conflict can be resolved via View configuration, the user will not be warned. As we'll\n\nsee next, views provide a mechanism to produce unique metric streams, differentiating the instruments.\n\nIf the instruments are not identical and their conflicts are not resolved via views, a warning is emitted, and their data is generated\n\nwithout modification.\n\nIndividual meters act as a namespace, meaning two meters can separately create identical instruments\n\nwithout any issues. Using a unique namespace for each meter ensures that application developers can\n\ncreate instruments that make sense for their applications without running the risk of interfering with\n\nother metrics generated by underlying libraries. This will also make searching for metrics easier once\n\nexported outside the application. Let's see how we can shape the metrics stream to fit our needs with\n\nviews.\n\nCustomizing metric outputs with views\n\nSome applications may produce more metrics than an application developer is interested in. You may\n\nhave noticed this with the example code for instruments; as we added more examples, it became\n\ndifficult to find the metrics we were interested in. Recall the example mentioned earlier in this\n\nchapter: application A represents a client library making web requests that could produce metrics via\n\nthree different meters. If each of those meters keeps a request counter, duplicate data is highly likely\n\nto be generated. Duplicated data may not be a problem on a small scale, but when scaling services up\n\nto handling thousands and millions of requests, unnecessary metrics can become quite expensive.\n\nThankfully, views provide a way for users of OpenTelemetry to configure the SDK only to generate\n\nthe metrics they want. In addition to providing a mechanism to filter metrics, views can also\n\nconfigure aggregation or be used to add a new dimension to metrics.\n\nFiltering\n\nThe first aspect of interest is the ability to customize which metrics will be processed. To select\n\ninstruments, the following criteria can be applied to a view:\n\ninstrument_name: The name of the instrument\n\ninstrument_type: The type of the instrument\n\nmeter_name: The name of the meter\n\nmeter_version: The version of the meter\n\nmeter_schema: The schema URL of the meter\n\nThe SDK provides a default view as a catch-all for any instruments not matched by configured views.\n\nIMPORTANT NOTE\n\nThe code in this chapter uses version 1.10.0 which supports the parameter enable_default_view to modify to disable the\n\ndefault view. This has changed in version 1.11.0 with the following change: https://github.com/open-\n\ntelemetry/opentelemetry-python/pull/2547. If you are using a newer version, you will need to configure a wildcard view with\n\na DropAggregation, refer to the official documentation (https://opentelemetry-\n\npython.readthedocs.io/en/latest/sdk/metrics.html) for more information.\n\nThe following code selects the inventory instrument we created in an earlier example. Views are\n\nadded to the MeterProvider as an argument to the constructor.\n\nAnother argument is added disabling the default view:\n\nmetrics.py\n\nfrom opentelemetry.sdk._metrics.view import View\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nview = View(instrument_name=\"inventory\")\n\nprovider = MeterProvider(\n\nmetric_readers=[reader],\n\nresource=Resource.create(),\n\nviews=[view],\n\nenable_default_view=False,\n\n)\n\nThe resulting output shows a metric stream limited to a single instrument:\n\noutput\n\n{\"attributes\": {\"locale\": \"fr-FR\", \"country\": \"CA\"}, \"description\": \"total items sold\",\n\n\"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"sold\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\": 1647800250023129000,\n\n\"time_unix_nano\": 1647800250023292000, \"value\": 6, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": true}}\n\n{\"attributes\": {\"locale\": \"es-ES\"}, \"description\": \"total items sold\",\n\n\"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"sold\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\": 1647800250023138000,\n\n\"time_unix_nano\": 1647800250023312000, \"value\": 1, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": true}}\n\nThe views parameter accepts a list, making adding multiple views trivial. This provides a great deal\n\nof flexibility and control for users. An instrument must match all arguments passed into the View\n\nconstructor. Let's update the previous example and see what happens when we try to create a view by\n\nselecting an instrument of the Counter type with the name inventory: metrics.py\n\nfrom opentelemetry._metrics.instrument import Counter\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nview = View(instrument_name=\"inventory\", instrument_type=Counter)\n\nprovider = MeterProvider(\n\nmetric_readers=[reader],\n\nresource=Resource.create(),\n\nviews=[view],\n\nenable_default_view=False,\n\n)\n\nAs you may already suspect, these criteria will not match any instruments, and no data will be\n\nproduced by running the code.\n\nIMPORTANT NOTE\n\nAll criteria specified when selecting instruments are optional. However, if no optional argument is specified, the code will\n\nraise an exception as per the OpenTelemetry specification.\n\nUsing views to filter instruments based on instrument or meter identification is a great way to reduce\n\nthe noise and cost of generating too many metrics.\n\nDimensions\n\nIn addition to selecting instruments, it's also possible to configure a view to only report specific\n\ndimensions. A dimension in this context is an attribute associated with the metric. For example, a\n\ncustomer counter may record information about customers as per Figure 5.7. Each attribute\n\nassociated with the counter, such as the country the customer is visiting from or the locale their\n\nbrowser is set to, offers another dimension to the metric recorded during their visit:\n\nFigure 5.7 – Additional dimensions for a counter\n\nDimensions can be used to aggregate data in meaningful ways; continuing with the previous table,\n\nwe can obtain the following information:\n\nThree customers visited our store.\n\nTwo customers visited from Canada and one from France.\n\nTwo had browsers configured to French (fr-FR), and one to English (en-US).\n\nViews allow us to customize the output from our metrics stream. Using the attributes_keys\n\nargument, we specify the dimensions we want to see in a particular view. The following configures a\n\nview to match the Counter instruments and to discard any attributes other than locale: metrics.py\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nview = View(instrument_type=Counter, attribute_keys=[\"locale\"])\n\n...\n\nYou may remember that in the code we wrote earlier when configuring instruments, the items_sold\n\ncounter generated two metrics. The first contained country and locale attributes; the second\n\ncontained the locale attribute. The configuration in this view will produce a metric stream discarding\n\nall attributes not specified via attribute_keys: output\n\n{\"attributes\": {\"locale\": \"fr-FR\"}, \"description\": \"Total items sold\", ...\n\n{\"attributes\": {\"locale\": \"es-ES\"}, \"description\": \"Total items sold\", ...\n\nNote that when using attribute_keys, all metrics not containing the specified attributes will be\n\naggregated. This is because by removing the attributes, the view effectively transforms the metrics, as\n\nper the following table:\n\nFigure 5.8 – Effect of attribute keys on counter operations\n\nAn example of where this may be useful is separating requests containing errors from those that do\n\nnot, or grouping requests by status code.\n\nIn addition to customizing the metric stream attributes, views can also alter their name or description.\n\nThe following renames the metric generated and updates its description. Additionally, it removes all\n\nattributes from the metric stream: metrics.py\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nview = View(\n\ninstrument_type=Counter,\n\nattribute_keys=[],\n\nname=\"sold\",\n\ndescription=\"total items sold\",\n\n)\n\n...\n\nThe output now shows us a single aggregated metric that is more meaningful to us:\n\noutput\n\n{\"attributes\": \"\", \"description\": \"total items sold\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"sold\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\":\n\n1646593079208078000, \"time_unix_nano\": 1646593079208238000, \"value\": 7,\n\n\"aggregation_temporality\": 2, \"is_monotonic\": true}}\n\nCustomizing views allow us to focus further on the output of the metrics generated. Let's see how we\n\ncan combine the metrics with aggregators.\n\nAggregation\n\nThe last configuration of views we will investigate is aggregation. The aggregation option gives the\n\nview the ability to change the default aggregation used by an instrument to one of the following\n\nmethods:\n\nSumAggregation: Add the instrument's measurements and set the current value as the sum. The monotonicity and temporality\n\nfor the sum are derived from the instrument.\n\nLastValueAggregation: Record the last measurement and its timestamp as the current value of this view.\n\nExplicitBucketHistogramAggregation: Use a histogram where the boundaries can be set via configuration. Additional\n\noptions for this aggregation are boundaries for the buckets of the histogram and record_min_max to record the minimum\n\nand maximum values.\n\nThe following table, Figure 5.9, shows us the default aggregation for each instrument:\n\nFigure 5.9 – Default aggregation per instrument\n\nAggregating data in the SDK allows us to reduce the number of data points transmitted. However,\n\nthis means the data available at query time is less granular, limiting the user's ability to query it.\n\nKeeping this in mind, let's look at configuring the aggregation for one of our counter instruments to\n\nsee how this works. The following code updates the view configured earlier to use\n\nLastValueAggregation instead of the SumAggregation default: metrics.py\n\nfrom opentelemetry.sdk._metrics.aggregation import LastValueAggregation\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nview = View(\n\ninstrument_type=Counter,\n\nattribute_keys=[],\n\nname=\"sold\",\n\ndescription=\"total items sold\",\n\naggregation=LastValueAggregation(),\n\n)\n\nYou'll notice in the output now that instead of reporting the sum of all measurements (7) for the\n\ncounter, only the last value (1) recorded is produced: output\n\n{\"attributes\": \"\", \"description\": \"total items sold\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"sold\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\", \"unit\": \"items\", \"point\": {\"time_unix_nano\":\n\n1646594506458381000, \"value\": 1}}\n\nAlthough it's essential to have the ability to configure aggregation, the default aggregation may well\n\nserve your purpose most of the time.\n\nIMPORTANT NOTE\n\nAs mentioned earlier, sum aggregation derives the temporality of the sum reported from its instrument. This temporality\n\ncan be either cumulative or delta. This determines whether the reported metrics are to be interpreted as always starting at\n\nthe same time, therefore, reporting a cumulative metric, or if the metrics reported represent a moving start time, and the\n\nreported values contain the delta from the previous report. For more information about temporality, refer to the\n\nOpenTelemetry specification found at https://github.com/open-telemetry/opentelemetry-\n\nspecification/blob/main/specification/metrics/datamodel.md#temporality.\n\nThe grocery store\n\nIt's time to go back to the example application from Chapter 4, Distributed Tracing –Tracing Code\n\nExecution, to get some practical experience of all the knowledge we've gained so far. Let's start by\n\nadding a method to retrieve a meter that will resemble configure_tracer from the previous chapter.\n\nThis method will be named configure_meter and will contain the configuration code from an\n\nexample earlier in this chapter. One main difference is the addition of a resource that uses\n\nLocalMachineResourceDetector, as we already defined in this module. Add the following code to the\n\ncommon.py module: common.py\n\nfrom opentelemetry._metrics import get_meter_provider, set_meter_provider\n\nfrom opentelemetry.sdk._metrics import MeterProvider\n\nfrom opentelemetry.sdk._metrics.export import (\n\nConsoleMetricExporter,\n\nPeriodicExportingMetricReader,\n\n)\n\ndef configure_meter(name, version):\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nlocal_resource = LocalMachineResourceDetector().detect()\n\nresource = local_resource.merge(\n\nResource.create(\n\n{\n\nResourceAttributes.SERVICE_NAME: name,\n\nResourceAttributes.SERVICE_VERSION: version,\n\n}\n\n)\n\n)\n\nprovider = MeterProvider(metric_readers=[reader], resource=resource)\n\nset_meter_provider(provider)\n\nschema_url = \"https://opentelemetry.io/schemas/1.9.0\"\n\nreturn get_meter_provider().get_meter(\n\nname=name,\n\nversion=version,\n\nschema_url=schema_url,\n\n)\n\nNow, update shopper.py to call this method and set the return value to a global variable named meter\n\nthat we'll use throughout the application: shopper.py\n\nfrom common import configure_tracer, configure_meter\n\ntracer = configure_tracer(\"shopper\", \"0.1.2\")\n\nmeter = configure_meter(\"shopper\", \"0.1.2\")\n\nWe will be adding this line to grocery_store.py and legacy_inventory.py in the following examples,\n\nbut you may choose to do so now. Now, to start the applications and ensure the code works as it\n\nshould, launch the three applications in separate terminals using the following commands in the order\n\npresented: $ python legacy_inventory.py\n\n$ python grocery_store.py\n\n$ python shopper.py\n\nThe execution of shopper.py should return right away. If no errors were printed out because of\n\nrunning those commands, we're off to a good start and are getting closer to adding metrics to our\n\napplications!\n\nNumber of requests\n\nWhen considering what metrics are essential to get insights about an application, it can be\n\noverwhelming to think of all the things we could measure. A good place is to start is with the golden\n\nsignals as documented in the Google Site Reliability Engineering (SRE) book,\n\nhttps://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals.\n\nMeasuring the traffic to our application is an easy place to start by counting the number of requests it\n\nreceives. It can help answer questions such as the following:\n\nWhat is the traffic pattern for our application?\n\nIs the application capable of handling the traffic we expected?\n\nHow successful is the application?\n\nIn future chapters, we'll investigate how this metric can be used to determine if the application should\n\nbe scaled automatically. A metric such as the total number of requests a service can handle is likely a\n\nnumber that would be revealed during benchmarking.\n\nThe following code calls configure_meter and creates a counter via the create_counter method to\n\nkeep track of the incoming requests to the server application. The request_counter value is\n\nincremented before the request is processed: grocery_store.py\n\nfrom common import configure_meter, configure_tracer, set_span_attributes_from_flask\n\ntracer = configure_tracer(\"grocery-store\", \"0.1.2\")\n\nmeter = configure_meter(\"grocery-store\", \"0.1.2\")\n\nrequest_counter = meter.create_counter(\n\nname=\"requests\",\n\nunit=\"request\",\n\ndescription=\"Total number of requests\",\n\n)\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest_counter.add(1)\n\nrequest.environ[\"context_token\"] = token\n\nThe updated grocery store code should reload automatically, but restart the grocery store application\n\nif it does not. Once the updated code is running, make the following three requests to the store by\n\nusing curl: $ curl localhost:5000\n\n$ curl localhost:5000/products\n\n$ curl localhost:5000/none-existent-url\n\nThis should give us output similar to the abbreviated output. Pay attention to the increasing value\n\nfield, which increases by one with each visit: 127.0.0.1 - - [06/Mar/2022 11:44:41] \"GET /\n\nHTTP/1.1\" 200 -\n\n{\"attributes\": \"\", \"description\": \"Total number of requests\", ... \"point\":\n\n{\"start_time_unix_nano\": 1646595826470792000, \"time_unix_nano\": 1646595833190445000,\n\n\"value\": 1, \"aggregation_temporality\": 2, \"is_monotonic\": true}}\n\n127.0.0.1 - - [06/Mar/2022 11:44:46] \"GET /products HTTP/1.1\" 200 -\n\n{\"attributes\": \"\", \"description\": \"Total number of requests\", ... \"point\":\n\n{\"start_time_unix_nano\": 1646595826470792000, \"time_unix_nano\": 1646595883232762000,\n\n\"value\": 2, \"aggregation_temporality\": 2, \"is_monotonic\": true}}\n\n127.0.0.1 - - [06/Mar/2022 11:44:47] \"GET /none-existent-url HTTP/1.1\" 404 -\n\n{\"attributes\": \"\", \"description\": \"Total number of requests\", ... \"point\":\n\n{\"start_time_unix_nano\": 1646595826470792000, \"time_unix_nano\": 1646595888236270000,\n\n\"value\": 3, \"aggregation_temporality\": 2, \"is_monotonic\": true}}\n\nIn addition to counting the total number of requests, it's helpful to have a way to track the different\n\nresponse codes. In the previous example, if you look at the output, you'll notice the last response's\n\nstatus code indicated a 404 error, which would be helpful to identify differently from other responses.\n\nKeeping a separate counter would allow us to calculate an error rate that could infer the service's\n\nhealth. Alternatively, using attributes can accomplish this, as well. The following moves the code to\n\nincrement the counter where the response status code is available. This code is then recorded as an\n\nattribute on the metric: grocery_store.py\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest.environ[\"context_token\"] = token\n\n@app.after_request\n\ndef after_request_func(response):\n\nrequest_counter.add(1, {\"code\": response.status_code})\n\nreturn response\n\nTo trigger the new code, use the following curl command:\n\n$ curl localhost:5000/none-existent-url\n\nThe result includes the status code attribute:\n\noutput\n\n{\"attributes\": {\"code\": 404}, \"description\": \"Total number of requests\",\n\n\"instrumentation_info\": \"InstrumentationInfo(grocery-store, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"requests\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'net.host.name': 'host',\n\n'net.host.ip': '127.0.0.1', 'service.name': 'grocery-store', 'service.version': '0.1.2'},\n\nmaxlen=None)\", \"unit\": \"request\", \"point\": {\"start_time_unix_nano\": 1646598200103414000,\n\n\"time_unix_nano\": 1646598203067451000, \"value\": 1, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": true}}\n\nSend a few more requests through to obtain different status codes. You can start seeing how this\n\ninformation can calculate error rates. The name given to metrics is significant.\n\nIMPORTANT NOTE\n\nIt's not possible to generate telemetry where there is no instrumentation. However, it is possible to filter out undesired\n\ntelemetry using the configuration in the SDK and the OpenTelemetry collector. Remember this when instrumenting code.\n\nWe'll visit how the collector can filter telemetry in Chapter 8, OpenTelemetry Collector, and Chapter 9, Deploying the\n\nCollector.\n\nThe data has shown us how to use a counter to produce meaningful data enriched with attributes. The\n\nvalue of this data will become even more apparent once we look at analysis tools in Chapter 10,\n\nConfiguring Backends.\n\nRequest duration\n\nThe next metric to produce is request duration. The goal of understanding the request duration across\n\na system is to be able to answer questions such as the following:\n\nHow long did the request take?\n\nHow much time did each service add to the total duration of the request?\n\nWhat is the experience for users?\n\nRequest duration is an interesting metric to understand the health of a service and can often be the\n\nsymptom of an underlying issue. Collecting the duration is best done via a histogram, which can\n\nprovide us with the organization and visualization necessary to understand the distribution across\n\nmany requests. In the following example, we are interested in measuring the duration of operations\n\nwithin each service. We are also interested in capturing the duration of upstream requests and the\n\nnetwork latency costs across each service in our distributed application. Figure 5.10 shows how this\n\nwill be measured:\n\nFigure 5.10 – Measuring request duration\n\nWe can use the different measurements across the entire request to understand where time is spent.\n\nThis could help differentiate network issues from application issues. For example, if a request from\n\nshopper.py to grocery_store.py takes 100 ms, but the operation within grocery_store.py takes less\n\nthan 1 ms, we know that the additional 99 ms were spent outside the application code.\n\nIMPORTANT NOTE\n\nWhen a network is involved, unexpected latency can always exist. This common fallacy of cloud-native applications must\n\nbe accounted for when designing applications. Investment in network engineering and deploying applications within closer\n\nphysical proximity significantly reduces latency.\n\nIn the following example, the upstream_duration_histo histogram is configured to record the\n\nduration of requests from shopper.py to grocery_store.py. An additional histogram,\n\ntotal_duration_histo, is created to capture the duration of the entire operation within the shopper\n\napplication. The period is calculated using the time_ns method from the time library, which returns\n\nthe current time in nanoseconds, which we convert to milliseconds: shopper.py\n\nimport time\n\ntotal_duration_histo = meter.create_histogram(\n\nname=\"duration\",\n\ndescription=\"request duration\",\n\nunit=\"ms\",\n\n)\n\nupstream_duration_histo = meter.create_histogram(\n\nname=\"upstream_request_duration\",\n\ndescription=\"duration of upstream requests\",\n\nunit=\"ms\",\n\n)\n\ndef browse():\n\n...\n\nstart = time.time_ns()\n\nresp = requests.get(url, headers=headers)\n\nduration = (time.time_ns() - start)/1e6\n\nupstream_duration_histo.record(duration)\n\n...\n\ndef visit_store():\n\nstart = time.time_ns()\n\nbrowse()\n\nduration = (time.time_ns() - start)/1e6\n\ntotal_duration_histo.record(duration)\n\nThe next step is to configure a histogram in grocery_store.py to record upstream requests and\n\noperation durations. For brevity, I will omit the instantiation of the two histograms to the following\n\ncode, as the code is identical to the previous example. The following uses methods decorated with\n\nFlask's before_request and after_request to calculate the beginning and end of the entire operation.\n\nWe also need to calculate the upstream request that occurs in the products method: grocery_store.py\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest_counter.add(1, {})\n\nrequest.environ[\"context_token\"] = token\n\nrequest.environ[\"start_time\"] = time.time_ns()\n\n@app.after_request\n\ndef after_request_func(response):\n\nrequest_counter.add(1, {\"code\": response.status_code})\n\nduration = (time.time_ns() - request.environ[\"start_time\"]) / 1e6\n\ntotal_duration_histo.record(duration)\n\nreturn response\n\n@app.route(\"/products\")\n\n@tracer.start_as_current_span(\"/products\", kind=SpanKind.SERVER)\n\ndef products():\n\n...\n\ninject(headers)\n\nstart = time.time_ns()\n\nresp = requests.get(url, headers=headers)\n\nduration = (time.time_ns() - start) / 1e6\n\nupstream_duration_histo.record(duration)\n\nLastly, for this example, let's add duration calculation for legacy_inventory.py. The code will be\n\nmore straightforward since this service has no upstream requests yet, thus, we'll only need to define a\n\nsingle histogram: legacy_inventory.py\n\nfrom flask import request\n\nimport time\n\ntotal_duration_histo = meter.create_histogram(\n\nname=\"duration\",\n\ndescription=\"request duration\",\n\nunit=\"ms\",\n\n)\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest.environ[\"start_time\"] = time.time_ns()\n\n@app.after_request\n\ndef after_request_func(response):\n\nduration = (time.time_ns() - request.environ[\"start_time\"]) / 1e6\n\ntotal_duration_histo.record(duration)\n\nreturn response\n\nNow that we have all these histograms in place, we can finally look at the duration of our requests.\n\nThe following output combines the output from all three applications to give us a complete picture of\n\nthe time spent across the system. Pay close attention to the sum value recorded for each histogram. As\n\nwe're only sending one request through, the sum equates the value for that single request: output\n\n{\"attributes\": \"\", \"description\": \"duration of upstream requests\", \"instrumentation_info\":\n\n\"InstrumentationInfo(shopper, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\", \"name\":\n\n\"upstream_request_duration\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\":\n\n1646626129420576000, \"time_unix_nano\": 1646626129420946000, \"bucket_counts\": [0, 0, 0, 0,\n\n0, 0, 0, 0, 0, 0, 1], \"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0,\n\n500.0, 1000.0], \"sum\": 18.981, \"aggregation_temporality\": 2}}\n\n{\"attributes\": \"\", \"description\": \"request duration\", \"instrumentation_info\":\n\n\"InstrumentationInfo(shopper, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\", \"name\":\n\n\"duration\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\": 1646626129420775000,\n\n\"time_unix_nano\": 1646626129420980000, \"bucket_counts\": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n\n\"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0, 500.0, 1000.0], \"sum\":\n\n19.354, \"aggregation_temporality\": 2}}\n\n{\"attributes\": \"\", \"description\": \"request duration\", \"instrumentation_info\":\n\n\"InstrumentationInfo(grocery-store, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"duration\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\": 1646626129419257000,\n\n\"time_unix_nano\": 1646626133006672000, \"bucket_counts\": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n\n\"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0, 500.0, 1000.0], \"sum\":\n\n10.852, \"aggregation_temporality\": 2}}\n\n{\"attributes\": \"\", \"description\": \"duration of upstream requests\", \"instrumentation_info\":\n\n\"InstrumentationInfo(grocery-store, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"upstream_request_duration\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\":\n\n1646626129419136000, \"time_unix_nano\": 1646626135619575000, \"bucket_counts\": [0, 0, 0, 1,\n\n0, 0, 0, 0, 0, 0, 0], \"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0,\n\n500.0, 1000.0], \"sum\": 10.36, \"aggregation_temporality\": 2}}\n\n{\"attributes\": \"\", \"description\": \"request duration\", \"instrumentation_info\":\n\n\"InstrumentationInfo(legacy-inventory, 0.9.1, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"duration\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\": 1646626129417730000,\n\n\"time_unix_nano\": 1646626134436096000, \"bucket_counts\": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n\n\"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0, 500.0, 1000.0], \"sum\":\n\n0.494, \"aggregation_temporality\": 2}}\n\nThe difference in upstream_request_duration and duration sums for each application gives us the\n\nduration of the operation within each application. Looking closely at the data produced, we can see a\n\nsignificant portion of the request, 93% in this case, is spent communicating between applications.\n\nIf you're looking at this and wondering, Couldn't distributed tracing calculate the duration of the\n\nrequest and latency instead?, you're right. This type of information is also available via distributed\n\ntracing, so long as all the operations along the way are instrumented.\n\nConcurrent requests\n\nAnother critical metric is the concurrent number of requests an application is processing at any given\n\ntime. This helps answer the following:\n\nIs the application a bottleneck for a system?\n\nCan the application handle a surge in requests?\n\nNormally, this value is obtained by calculating a rate of the number of requests per second via the\n\ncounter added earlier. However, since we need practice with instruments and have yet to send our\n\ndata to a backend that allows for analysis, we'll record it manually.\n\nIt's possible to use several instruments to capture this. For the sake of this example, we will use an\n\nup/down counter, but we could have also used a gauge as well. We will increment the up/down\n\ncounter every time a new request begins and decrement it after each request: grocery_store.py\n\nconcurrent_counter = meter.create_up_down_counter(\n\nname=\"concurrent_requests\",\n\nunit=\"request\",\n\ndescription=\"Total number of concurrent requests\",\n\n)\n\n@app.before_request\n\ndef before_request_func():\n\n...\n\nconcurrent_counter.add(1)\n\n@app.after_request\n\ndef after_request_func(err):\n\n...\n\nconcurrent_counter.add(-1)\n\nTo ensure we can see multiple users connected simultaneously, we will use a different tool than\n\nshopper.py, which we've used for this far. The hey load generation program allows us to generate\n\nhundreds of requests in parallel, enabling us to see the up/down counter in action. Run the program\n\nnow with the following command to generate 300 requests with a maximum concurrency of 10: $ hey\n\nn 3000 -c 10 http://localhost:5000/products\n\nThat command should have created enough parallel connections. Let's look at the metrics generated;\n\nwe should expect to see the recorded value going up as the number of concurrent requests increases,\n\nand then going back down: output\n\n{\"attributes\": \"\", \"description\": \"Total number of concurrent requests\",\n\n\"instrumentation_info\": \"InstrumentationInfo(grocery-store, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"concurrent_requests\", \"unit\":\n\n\"request\", \"point\": {\"start_time_unix_nano\": 1646627738799214000, \"time_unix_nano\":\n\n1646627769865503000, \"value\": 10, \"aggregation_temporality\": 2, \"is_monotonic\": false}}\n\n{\"attributes\": \"\", \"description\": \"Total number of concurrent requests\",\n\n\"instrumentation_info\": \"InstrumentationInfo(grocery-store, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"concurrent_requests\", \"unit\":\n\n\"request\", \"point\": {\"start_time_unix_nano\": 1646627738799214000, \"time_unix_nano\":\n\n1646627774867317000, \"value\": 0, \"aggregation_temporality\": 2, \"is_monotonic\": false}}\n\nWe will come back to using this tool later, but it's worth keeping around if you want to test the\n\nperformance of your applications. We will be looking at some additional tools to generate load in\n\nChapter 11, Diagnosing Problems. Try pushing the load higher to see if you can cause the application\n\nto fail altogether by increasing the number of requests or concurrency.\n\nResource consumption\n\nThe following metrics we will capture from our applications are runtime performance metrics.\n\nCapturing the performance metrics of an application can help us answer questions such as the\n\nfollowing:\n\nHow many resources does my application need?\n\nWhat budget will I need to run this service for the next 6 months?\n\nThis often helps guide decisions of what resources will be needed as the business needs change.\n\nQuite often, application performance metrics, such as memory, CPU, and network consumption,\n\nindicate where time could be spent reducing the cost of an application.\n\nIMPORTANT NOTE\n\nIn the following example, we will focus specifically on runtime application metrics. These do not include system-level\n\nmetrics. There is an essential distinction between the two. Runtime application metrics should be recorded by each\n\napplication individually. On the other hand, system-level metrics should only be recorded once for the entire system.\n\nReporting system-level metrics from multiple applications running on the same system is problematic. This will cause\n\nsystem performance metrics to be duplicated, which will require de-duplication either at transport or at analysis time.\n\nAnother problem is that querying the system for metrics is expensive, and doing so multiple times places an unnecessary\n\nburden on the system.\n\nWhen looking for runtime metrics, there are many metrics to choose from. Let's record the memory\n\nconsumption that we will measure using an asynchronous gauge. One of the tools available to\n\nprovide a way to measure memory statistics in Python comes with the standard library. The resource\n\npackage (https://docs.python.org/3/library/resource.html) provides usage information about our\n\nprocess. Additional third-party libraries are available, such as psutil (https://psutil.readthedocs.io/),\n\nwhich provides even more information about the resource utilization of your process. It's an excellent\n\npackage for collecting information about CPU, disk, and network usage.\n\nAs the implementation for capturing this metric will be the same across all the applications in the\n\nsystem, the code for the callback will be placed in common.py. The following creates a\n\nrecord_max_rss_callback method to record the maximum resident set size for the application. It also\n\ndefines a convenience method called start_recording_memory_metrics, which creates the\n\nasynchronous gauge. Add these methods to common.py now: common.py\n\nimport resource\n\nfrom opentelemetry._metrics.measurement import Measurement\n\ndef record_max_rss_callback():\n\nyield Measurement(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n\ndef start_recording_memory_metrics(meter):\n\nmeter.create_observable_gauge(\n\ncallback=record_max_rss_callback,\n\nname=\"maxrss\",\n\nunit=\"bytes\",\n\ndescription=\"Max resident set size\",\n\n)\n\nNext, add a call to start_recording_memory_metrics in each application in our system. Add the\n\nfollowing code to shopper.py, legacy_inventory.py, and grocery_store.py: shopper.py\n\nfrom common import start_recording_memory_metrics\n\nif __name__ == \"__main__\":\n\nstart_recording_memory_metrics(meter)\n\nAfter adding this code to each application and ensuring they have been reloaded, each should start\n\nreporting the following values: output\n\n{\"attributes\": \"\", \"description\": \"Max resident set size\", \"instrumentation_info\":\n\n\"InstrumentationInfo(legacy-inventory, 0.9.1, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"maxrss\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'net.host.name':\n\n'host', 'net.host.ip': '10.0.0.141', 'service.name': 'legacy-inventory',\n\n'service.version': '0.9.1'}, maxlen=None)\", \"unit\": \"bytes\", \"point\": {\"time_unix_nano\":\n\n1646637404789912000, \"value\": 33083392}}\n\nAnd just like that, we have memory telemetry about our applications. I urge you to add additional\n\nusage metrics to the application and look at the psutil library mentioned earlier to expand the\n\ntelemetry of your services. The metrics we added to the grocery store are by no means exhaustive.\n\nInstrumenting the code and gaining familiarity with instruments gives us a starting point from which\n\nto work.\n\nSummary\n\nWe've covered much ground in this chapter about the metrics signal. We started by familiarizing\n\nourselves with the different components and terminology of the metrics pipeline and how to\n\nconfigure them. We then looked at all the ins and outs of the individual instruments available to\n\nrecord measurements and used each one to record sample metrics.\n\nUsing views, we learned to aggregate, filter, and customize the metric streams being emitted by our\n\napplication to fit our specific needs. This will be handy when we start leveraging instrumentation\n\nlibraries. Finally, we returned to the grocery store to get hands-on experience with instrumenting an\n\nexisting application and collecting real-world metrics.\n\nMetrics is a deep topic that goes well beyond what has been covered in this chapter, but hopefully,\n\nwhat you've learned thus far is enough to start considering how OpenTelemetry can be used in your\n\ncode. The next chapter will look at the third and final signal we will cover in this book – logging.\n\nChapter 6: Logging – Capturing Events\n\nMetrics and traces go a long way in helping understand the behaviors and intricacies of cloud-native\n\napplications. Sometimes though, it's useful to log additional information that can be used at debug\n\ntime. Logging gives us the ability to record information in a way that is perhaps more flexible and\n\nfreeform than either tracing or metrics. That flexibility is both wonderful and terrible. It allows logs\n\nto be customized to fit whatever need arises using natural language, which often, but not always,\n\nmakes it easier to interpret by the reader. But the flexibility is often abused, resulting in a mess of\n\nlogs that are hard to search through and even harder to aggregate in any meaningful way. This chapter\n\nwill take a look at how OpenTelemetry tackles the challenges of logging and how it can be used to\n\nimprove the telemetry generated by an application. We will cover the following topics:\n\nConfiguring OpenTelemetry to export logs\n\nProducing logs via the OpenTelemetry API and a standard logging library\n\nThe logging signal in practice within the context of the grocery store application\n\nAlong the way, we will learn about standard logging in Python as well as logging with Flask, giving\n\nus a chance to use an instrumentation library as well. But first, let's ensure we have everything we\n\nneed set up.\n\nTechnical requirements\n\nIf you've already completed Chapter 4, Distributed Tracing, or Chapter 5, Metrics - Recording\n\nMeasurements, the setup here will be quite familiar. Ensure the version of Python in your\n\nenvironment is at least Python 3.6 by running the following commands: $ python --version\n\n$ python3 --version\n\nThis chapter will rely on the OpenTelemetry API and SDK packages that are installable via pip with\n\nthe following command. The examples in this chapter are using the version 1.9.0 opentelemetry-api\n\nand opentelemetry-sdk packages: $ pip install opentelemetry-api \\\n\nopentelemetry-sdk \\\n\nopentelemetry-propagator-b3\n\nIMPORTANT NOTE\n\nThe OpenTelemetry examples in this chapter rely on an experimental release of the logging signal for OpenTelemetry. This\n\nmeans it's possible that by the time you're reading this, the updated packages have moved methods to different packages.\n\nThe release notes available for each release should help you identify where the packages have moved to\n\n(https://github.com/open-telemetry/opentelemetry-python/releases).\n\nAdditionally, in this chapter, we will use an instrumentation package made available by the\n\nOpenTelemetry Python community. This instrumentation will assist us when adding logging\n\ninformation to Flask applications that are part of the grocery store. Install the package via pip with\n\nthe following command: $ pip install opentelemetry-instrumentation-wsgi\n\nThe code for this chapter is available in the companion repository. The following uses git to copy the\n\nrepository locally: $ git clone https://github.com/PacktPublishing/Cloud-Native-Observability The\n\ncompleted code for the examples in this chapter is available in the chapter06 directory. If you're\n\ninterested in writing the code yourself, I suggest you start by copying the code in the chapter04\n\ndirectory and following along.\n\nLastly, we will need to install the libraries that the grocery store relies on. This can be done via the\n\nfollowing pip command: $ pip install flask requests\n\nWe're now ready to start logging!\n\nConfiguring OpenTelemetry logging\n\nUnlike with the two signals we covered in Chapter 4, Distributed Tracing, and Chapter 5, Metrics -\n\nRecording Measurements, the logging signal in OpenTelemetry does not concern itself with\n\nstandardizing a logging interface. Many languages already have an established logging API, and a\n\ndecision early on in OpenTelemetry was made to leverage those pre-existing tools. Although\n\nOpenTelemetry provides an API capable of producing logging, which we'll use shortly, the signal is\n\nintent on hooking into the existing logging facilities. Its focus is to augment the logs produced and\n\nprovide a mechanism to correlate those logs with other signals. Figure 6.1 shows us the components\n\nof the logging pipeline:",
      "page_number": 129
    },
    {
      "number": 6,
      "title": "Logging – Capturing Events",
      "start_page": 164,
      "end_page": 180,
      "detection_method": "regex_chapter",
      "content": "Figure 6.1 – The logging pipeline\n\nThese components combine to produce log records and emit them to external systems. The logging\n\npipeline is comprised of the following:\n\nA LogEmitterProvider, which provides a mechanism to instantiate one or more log emitters\n\nThe LogEmitter, which produces LogRecord data\n\nThe LogProcessor, which consumes log records and passes them on to a LogExporter for sending the data to a backend\n\nFirst, as with all the other OpenTelemetry signals, we must configure the provider. The following\n\ncode instantiates a LogEmitterProvider from the SDK, passes in a resource via the resource\n\nargument, and then sets the global log emitter via the set_log_emitter_provider method: logs.py\n\nfrom opentelemetry.sdk._logs import LogEmitterProvider, set_log_emitter_provider from\n\nopentelemetry.sdk.resources import Resource\n\ndef configure_log_emitter_provider():\n\nprovider = LogEmitterProvider(resource=Resource.create())\n\nset_log_emitter_provider(provider)\n\nConfiguring LogEmitter alone won't allow us to produce telemetry. We'll need a log processor and an\n\nexporter to go a step further. Let's add BatchLogProcessor, which, as the name suggests, batches the\n\nprocessing of log records. We will also use a ConsoleLogExporter to output logging information to\n\nthe console: logs.py\n\nfrom opentelemetry.sdk._logs.export import ConsoleLogExporter, BatchLogProcessor\n\nfrom opentelemetry.sdk._logs import LogEmitterProvider, set_log_emitter_provider from\n\nopentelemetry.sdk.resources import Resource\n\ndef configure_log_emitter_provider():\n\nprovider = LogEmitterProvider(resource=Resource.create())\n\nset_log_emitter_provider(provider)\n\nexporter = ConsoleLogExporter()\n\nprovider.add_log_processor(BatchLogProcessor(exporter))\n\nWith OpenTelemetry configured, we're now ready to start instrumenting our logs.\n\nProducing logs\n\nFollowing the pattern from previous signals, we should be ready to get an instance of a log producer\n\nand start logging, right? Well, not quite – let's find out why.\n\nUsing LogEmitter\n\nUsing the same method that we used for metrics and tracing, we can now obtain LogEmitter, which\n\nwill allow us to use the OpenTelemetry API to start producing logs. The following code shows us\n\nhow to accomplish this using the get_log_emitter method: logs.py\n\nfrom opentelemetry.sdk._logs import (\n\nLogEmitterProvider,\n\nget_log_emitter_provider,\n\nset_log_emitter_provider,\n\n)\n\nif __name__ == \"__main__\":\n\nconfigure_log_emitter_provider()\n\nlog_emitter = get_log_emitter_provider().get_log_emitter(\n\n\"shopper\",\n\n\"0.1.2\",\n\n)\n\nWith LogEmitter in hand, we're now ready to generate LogRecord. The LogRecord contains the\n\nfollowing information:\n\ntimestamp: A time associated with the log record in nanoseconds.\n\ntrace_id: A hex-encoded identifier of the trace to correlate with the log record. There will be more on this, the span identifier,\n\nand trace flags shortly.\n\nspan_id: A hex-encoded identifier of the span to correlate with the log record.\n\ntrace_flags: Trace flags associated with the trace active when the log record was produced.\n\nseverity_text: A string representation of the severity level.\n\nseverity_number: A numeric value of the severity level.\n\nbody: The contents of the log message being recorded.\n\nresource: The resource associated with the producer of the log record.\n\nattributes: Additional information associated with the log record in the form of key-value pairs.\n\nEach one of those fields can be passed as an argument to the constructor; note that all those fields are\n\noptional. The following creates LogRecord with some minimal information and calls emit to produce a\n\nlog entry: logs.py\n\nimport time\n\nfrom opentelemetry.sdk._logs import (\n\nLogEmitterProvider,\n\nLogRecord,\n\nget_log_emitter_provider,\n\nset_log_emitter_provider,\n\n)\n\nif __name__ == \"__main__\":\n\nconfigure_log_emitter_provider()\n\nlog_emitter = get_log_emitter_provider().get_log_emitter(\n\n\"shopper\",\n\n\"0.1.2\",\n\n)\n\nlog_emitter.emit(\n\nLogRecord(\n\ntimestamp=time.time_ns(),\n\nbody=\"first log line\",\n\n)\n\n)\n\nAfter all this work, we can finally see a log line! Run the code, and the output should look something\n\nlike this: output\n\n{\"body\": \"first log line\", \"name\": null, \"severity_number\": \"None\", \"severity_text\": null,\n\n\"attributes\": null, \"timestamp\": 1630814115049294000, \"trace_id\": \"\", \"span_id\": \"\",\n\n\"trace_flags\": null, \"resource\": \"\"}\n\nAs you can see, there's a lot of information missing to give us a full picture of what was happening.\n\nOne of the most important pieces of information associated with a log entry is the severity level. The\n\nOpenTelemetry specification defines 24 different log levels categorized in 6 severity groups, as\n\nshown in the following figure:\n\nFigure 6.2 – Log severity levels defined by OpenTelemetry When defining the severity level, all log levels above that\n\nnumber are reported.\n\nLet's ensure the log record we generate sets a meaningful severity level: logs.py\n\nfrom opentelemetry.sdk._logs.severity import SeverityNumber\n\nif __name__ == \"__main__\":\n\n...\n\nlog_emitter.emit(\n\nLogRecord(\n\ntimestamp=time.time_ns(),\n\nbody=\"first log line\",\n\nseverity_number=SeverityNumber.INFO,\n\n)\n\n)\n\nThere – now at least readers of those logs should be able to know how important those log lines are.\n\nRun the code and look for the severity number in the output: output\n\n{\"body\": \"first log line\", \"name\": null, \"severity_number\": \"<SeverityNumber.INFO: 9>\",\n\n\"severity_text\": null, \"attributes\": null, \"timestamp\": 1630814944956950000, \"trace_id\":\n\n\"\", \"span_id\": \"\", \"trace_flags\": null, \"resource\": \"\"}\n\nAs mentioned earlier in this chapter, one of the goals of the OpenTelemetry logging signal is to\n\nremain interoperable with existing logging APIs. Looking at how much work we just did to get a log\n\nline with minimal information, it really seems like there should be a better way, and there is!\n\nThe standard logging library\n\nWhat if we tried using the standard logging library available in Python to interact with\n\nOpenTelemetry instead? The logging library has been part of the standard Python library since\n\nversion 2.3 and is used by many popular frameworks, such as Django and Flask.\n\nIMPORTANT NOTE\n\nThe standard logging module in Python is quite powerful and flexible. If you're not familiar with it, it may take some time to\n\nget used to it. I recommend reading the Python docs available on python.org here:\n\nhttps://docs.python.org/3/library/logging.html.\n\nThe Python implementation of the OpenTelemetry signal provides an additional component to use,\n\nOTLPHandler. The following figure shows where OTLPHandler fits in with the rest of the logging\n\npipeline:\n\nFigure 6.3 – OTLPHandler uses LogEmitter to produce logs OTLPHandler extends the standard logging library's\n\nlogging.Handler class and uses the configured LogEmitter to produce log records.\n\nIMPORTANT NOTE:\n\nThe OTLPHandler was renamed LoggingHandler in releases of the opentelemetry-sdk package newer than 1.10.0. Be\n\nsure to update any references to it in the examples if you've installed a newer version.\n\nThe following code block first imports the logging module. Then, using the getLogger method, a\n\nstandard Logger object is obtained. This is the object we will use anytime a log line is needed from\n\nthe application. Finally, OTLPHandler is added to logger, and a warning message is logged: logs.py\n\nimport logging\n\nfrom opentelemetry.sdk._logs import (\n\nLogEmitterProvider,\n\nLogRecord,\n\nOTLPHandler,\n\nget_log_emitter_provider,\n\nset_log_emitter_provider,\n\n)\n\nif __name__ == \"__main__\":\n\n...\n\nlogger = logging.getLogger(__file__)\n\nhandler = OTLPHandler()\n\nlogger.addHandler(handler)\n\nlogger.warning(\"second log line\")\n\nLet's see how the information generated differs from the previous example; many of the fields are\n\nautomatically filled in for us:\n\nThe timestamp is set to the current time.\n\nThe severity number and text are set based on the method used to record a log – in this case, the warning method sets the log\n\nseverity to WARN.\n\nTrace and span information is set by pulling information from the current context. As our example does not include starting a\n\ntrace, we should expect the values in these fields to be invalid.\n\nResource data is set via the log emitter provider.\n\nThis provides us with a significant improvement in the data generated.\n\noutput\n\n{\"body\": \"second log line\", \"name\": null, \"severity_number\": \"<SeverityNumber.WARN: 13>\",\n\n\"severity_text\": \"WARNING\", \"attributes\": {}, \"timestamp\": 1630810960785737984,\n\n\"trace_id\": \"0x00000000000000000000000000000000\", \"span_id\": \"0x0000000000000000\",\n\n\"trace_flags\": 0, \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name':\n\n'unknown_service'}, maxlen=None\"}\n\nNot only does this output contain richer data, but we also didn't need to work nearly as hard to obtain\n\nit, and we used a standard library to generate the logs. The attributes field doesn't appear to contain\n\nanything useful yet – let's fix that. OTLPHandler creates the attribute dictionary by looking at any\n\nextra attributes defined in the standard LogRecord. The following code passes an extra argument at\n\nlogging time: logs.py\n\nif __name__ == \"__main__\":\n\n...\n\nlogger.warning(\"second log line\", extra={\"key1\": \"val1\"}) As with other attribute\n\ndictionaries we may have encountered previously, they should contain information relevant\n\nto the specific event being logged. The output should now show us the additional\n\nattributes: output\n\n{\"body\": \"second log line\", \"name\": null, \"severity_number\": \"<SeverityNumber.WARN: 13>\",\n\n\"severity_text\": \"WARNING\", \"attributes\": {\"key1\": \"val1\"}, \"timestamp\":\n\n1630946024854904064, \"trace_id\": \"0x00000000000000000000000000000000\", \"span_id\":\n\n\"0x0000000000000000\", \"trace_flags\": 0, \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name': 'unknown_service'},\n\nmaxlen=None\"}\n\nLet's produce one last example with the standard logger and update the previous code to record a log\n\nusing the info method. This should give us the same severity as the example where we used the log\n\nemitter directly: logs.py\n\nimport logging\n\nif __name__ == \"__main__\":\n\n...\n\nlogger.info(\"second log line\") Run the code again to see the result. If you're no longer\n\nseeing a log with the second log line as its body and are perplexed, don't worry – you're\n\nnot alone. This is due to a feature of the standard logging library. The Python logging\n\nmodule creates a root logger, which is used anytime a more specific logger isn't\n\nconfigured. By default, the root logger is configured to only log messages with a severity\n\nof a warning or higher. Any logger instantiated via getLogger inherits that severity,\n\nwhich explains why our info level messages are not displayed. Our example can be fixed by\n\ncalling setLevel for the logger we are using in our program: logs.py\n\nif __name__ == \"__main__\":\n\n...\n\nlogger = logging.getLogger(__file__)\n\nlogger.setLevel(logging.DEBUG)\n\nhandler = OTLPHandler()\n\nlogger.addHandler(handler)\n\nlogger.info(\"second log line\")\n\nThe output should now contain the log line as we expected:\n\noutput\n\n{\"body\": \"second log line\", \"name\": null, \"severity_number\": \"<SeverityNumber.INFO: 9>\",\n\n\"severity_text\": \"INFO\", \"attributes\": {}, \"timestamp\": 1630857128712922112, \"trace_id\":\n\n\"0x00000000000000000000000000000000\", \"span_id\": \"0x0000000000000000\", \"trace_flags\": 0,\n\n\"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.9.0, 'service.name': 'unknown_service'},\n\nmaxlen=None)\"}\n\nAn alternative way to configure the log level of the root logger is to use the basicConfig method of\n\nthe logging module. This allows you to configure the severity level, formatting, and so on\n\n(https://docs.python.org/3/library/logging.html#logging.basicConfig). Another benefit of using the\n\nexisting logging library means that with a little bit of additional configuration, any existing\n\napplication should be able to leverage OpenTelemetry logging. Speaking of an existing application,\n\nlet's return to the grocery store.\n\nA logging signal in practice\n\nGetting familiar with the logging signal theory is great; now it's time to put it into practice. Before\n\nusing OpenTelemetry logging in the grocery store, let's take a minute to move the configuration code\n\ninto the common.py module: common.py\n\nimport logging\n\nfrom opentelemetry.sdk._logs.export import ConsoleLogExporter, BatchLogProcessor from\n\nopentelemetry.sdk._logs import (\n\nLogEmitterProvider,\n\nOTLPHandler,\n\nset_log_emitter_provider,\n\n)\n\ndef configure_logger(name, version):\n\nprovider = LogEmitterProvider(resource=Resource.create())\n\nset_log_emitter_provider(provider)\n\nexporter = ConsoleLogExporter()\n\nprovider.add_log_processor(BatchLogProcessor(exporter))\n\nlogger = logging.getLogger(name)\n\nlogger.setLevel(logging.DEBUG)\n\nhandler = OTLPHandler()\n\nlogger.addHandler(handler)\n\nreturn logger\n\nWith the code in place, we can now obtain a logger in the same fashion as we obtained a tracer and a\n\nmeter previously. The following code updates the shopper application to instantiate a logger via\n\nconfigure_logger. Additionally, let's update the add_item_to_cart method to use logger.info rather\n\nthan print: shopper.py\n\nfrom common import configure_tracer, configure_meter, configure_logger\n\ntracer = configure_tracer(\"shopper\", \"0.1.2\") meter = configure_meter(\"shopper\", \"0.1.2\")\n\nlogger = configure_logger(\"shopper\", \"0.1.2\")\n\n@tracer.start_as_current_span(\"add item to cart\") def add_item_to_cart(item, quantity):\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\n\"item\": item,\n\n\"quantity\": quantity,\n\n}\n\n)\n\nlogger.info(\"add {} to cart\".format(item)) Use the following commands in separate\n\nterminals to launch the grocery store, the legacy inventory, and finally, the shopper\n\napplications: $ python legacy_inventory.py\n\n$ python grocery_store.py\n\n$ python shopper.py\n\nPay special attention to output running from the previous command; it should include similar output,\n\nconfirming that our configuration is correct: output\n\n{\"body\": \"add orange to cart\", \"name\": null, \"severity_number\": \"<SeverityNumber.INFO:\n\n9>\", \"severity_text\": \"INFO\", \"attributes\": {}, \"timestamp\": 1630859469283874048,\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x0fc5e89573d7f794\",\n\n\"trace_flags\": 1, \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\"}\n\nThis is a great starting point; let's see how we can correlate the information from this log line with the\n\ninformation from our traces.\n\nDistributed tracing and logs\n\nWe saw earlier in this chapter that the LogRecord class contains fields for span and trace identifiers as\n\nwell as trace flags. The intention behind this is to allow logs to be correlated with specific traces and\n\nspans, permitting the end user to gain a better understanding of what their application is doing when\n\nit's running in production. So often, the process of correlating telemetry involves searching tirelessly\n\nthrough events using a timestamp as a mechanism to match up different sources of information. This\n\nis not always practical for the following reasons:\n\nMany operations happen concurrently on the same system, making it difficult to know which operation caused the event.\n\nThe difficulty caused by operations happening simultaneously is exacerbated in distributed systems as even more operations are\n\noccurring.\n\nThe clocks across different systems may, and often do, drift. This drift causes timestamps to not match.\n\nA mechanism developed to address this has been to produce a unique event identifier for each event\n\nand add this identifier to all logs recorded. One challenge of this is ensuring that this information is\n\nthen propagated across the entire system; this is exactly what the trace identifier in OpenTelemetry\n\ndoes. As shown in Figure 6.4, the trace and span identifiers can pinpoint the specific operation that\n\ntriggers a log to be recorded:\n\nFigure 6.4 – Log and trace correlation\n\nReturning to the output from the previous example, the following shows the logging output as well as\n\na snippet of the tracing output containing the name of the operations and their identifiers. See\n\nwhether you can determine from the output which operation triggered the log record: output\n\n{\"body\": \"add orange to cart\", \"name\": null, \"severity_number\": \"<SeverityNumber.INFO:\n\n9>\", \"severity_text\": \"INFO\", \"attributes\": {}, \"timestamp\": 1630859469283874048,\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x0fc5e89573d7f794\",\n\n\"trace_flags\": 1, \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\"}\n\n{\n\n\"name\": \"web request\",\n\n\"context\": {\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x6e4e03cacd3411b5\",\n\n},\n\n}\n\n{\n\n\"name\": \"add item to cart\",\n\n\"context\": {\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x0fc5e89573d7f794\",\n\n},\n\n}\n\n{\n\n\"name\": \"browse\",\n\n\"context\": {\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x5a2262c9dd473b40\",\n\n},\n\n}\n\n{\n\n\"name\": \"visit store\",\n\n\"context\": {\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x504caee882574a9e\",\n\n},\n\n}\n\nIf you've guessed that the log line was generated by the add item to cart operation, you've guessed\n\ncorrectly. Although this particular example is simple since you're already familiar with the code itself,\n\nyou can imagine how valuable this information can be to troubleshoot an unfamiliar system.\n\nEquipped with the information provided by the distributed trace associated with the log record, you're\n\nempowered to jump into the source code and debug an issue faster. Let's see how we can use\n\nOpenTelemetry logging with the other applications in our system.\n\nOpenTelemetry logging with Flask\n\nAs covered previously in the chapter, many frameworks, including Flask, use the standard logging\n\nlibrary in Python. This makes configuring OpenTelemetry for the grocery store similar to how any\n\nchanges to logging in Flask would be done. The following code imports and uses configure_logger\n\nto set up the logging pipeline. Next, we use the logging module's dictConfig method to add\n\nOTLPHandler to the root logger, and configure the severity level to DEBUG to ensure all our logs are\n\noutput. In a production setting, you will likely want to make this option configurable rather than\n\nhardcode it to debug level to save costs: grocery_store.py\n\nfrom logging.config import dictConfig\n\nfrom common import (\n\nconfigure_meter,\n\nconfigure_tracer,\n\nconfigure_logger,\n\nset_span_attributes_from_flask,\n\nstart_recording_memory_metrics,\n\n)\n\ntracer = configure_tracer(\"grocery-store\", \"0.1.2\") meter = configure_meter(\"grocery-\n\nstore\", \"0.1.2\") logger = configure_logger(\"grocery-store\", \"0.1.2\")\n\ndictConfig(\n\n{\n\n\"version\": 1,\n\n\"handlers\": {\n\n\"otlp\": {\n\n\"class\": \"opentelemetry.sdk._logs.OTLPHandler\",\n\n}\n\n},\n\n\"root\": {\"level\": \"DEBUG\", \"handlers\": [\"otlp\"]},\n\n}\n\n)\n\napp = Flask(__name__)\n\nEnsure some requests are sent to the grocery store either by running shopper.py or via curl and see\n\nwhat the output from the server looks like now. The following output shows it before the change on\n\nthe first line and after the change on the second line: output\n\n127.0.0.1 - - [05/Sep/2021 10:58:28] \"GET /products HTTP/1.1\" 200 -\n\n{\"body\": \"127.0.0.1 - - [05/Sep/2021 10:58:48] \\\"GET /products HTTP/1.1\\\" 200 -\", \"name\":\n\nnull, \"severity_number\": \"<SeverityNumber.INFO: 9>\", \"severity_text\": \"INFO\",\n\n\"attributes\": {}, \"timestamp\": 1630864728996940032, \"trace_id\":\n\n\"0x00000000000000000000000000000000\", \"span_id\": \"0x0000000000000000\", \"trace_flags\": 0,\n\n\"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\"}\n\nWe can see the original message is now recorded as the body of the message, and all the additional\n\ninformation is also presented. Although, if we look closely, we can see that the span_id, trace_id,\n\nand trace_flags information is missing. It looks like the context for our request is lost somewhere\n\nalong the way, so let's fix that. What is confusing about this is that we already have hooks defined to\n\nhandle before_request and teardown_request, which, in theory, should ensure that the trace\n\ninformation is available. However, the log record we see is generated by Flask's built-in web server\n\n(wsgi), not the Flask application, and is triggered after the original request has been completed as far\n\nas Flask knows. We can address this by creating middleware ourselves, but thankfully, we don't have\n\nto.\n\nLogging with WSGI middleware\n\nThe OpenTelemetry community publishes a package that provides support for instrumenting an\n\napplication that uses a wsgi-compatible implementation, such as the built-in Flask server. The\n\nopentelemetry-instrumentation-wsgi package provides the middleware that hooks into the\n\nappropriate mechanisms to make trace information for the duration of the request. The following\n\ncode imports the middleware and updates the Flask app to use it: grocery_store.py\n\nfrom opentelemetry.instrumentation.wsgi import OpenTelemetryMiddleware\n\n...\n\napp = Flask(__name__)\n\napp.wsgi_app = OpenTelemetryMiddleware(app.wsgi_app)\n\nWith the middleware in place, a new request to our application should allow us to see the span_id,\n\ntrace_id, and trace_flags components that we expect: output\n\n{\"body\": \"127.0.0.1 - - [05/Sep/2021 11:39:36] \\\"GET /products HTTP/1.1\\\" 200 -\", \"name\":\n\nnull, \"severity_number\": \"<SeverityNumber.INFO: 9>\", \"severity_text\": \"INFO\",\n\n\"attributes\": {}, \"timestamp\": 1630867176948227072, \"trace_id\":\n\n\"0xf999a4164ac2f20c20549f19abd4b434\", \"span_id\": \"0xed5d3071ece38633\", \"trace_flags\": 1,\n\n\"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\"}\n\nWe will look at how this works in more detail in Chapter 7, Instrumentation Libraries, and see how\n\nwe can simplify the application code using instrumentation libraries. For the purpose of this example,\n\nit's enough to know that the middleware enables us to see the tracing information in the log we are\n\nrecording.\n\nResource correlation\n\nAnother piece of data that OpenTelemetry logging uses when augmenting telemetry is the resource\n\nattribute. As you may remember from previous chapters, the resource describes the source of the\n\ntelemetry. This will allow us to correlate events occurring across separate signals for the same\n\nresource. In Chapter 4, Distributed Tracing, we defined a LocalMachineResourceDetector class that\n\nproduces an OpenTelemetry resource that includes information about the local machine. Let's update\n\nthe code in configure_logger that instantiates the LogEmitterProvider to use this resource, rather\n\nthan create an empty resource: common.py\n\ndef configure_logger(name, version):\n\nlocal_resource = LocalMachineResourceDetector().detect()\n\nresource = local_resource.merge(\n\nResource.create(\n\n{\n\nResourceAttributes.SERVICE_NAME: name,\n\nResourceAttributes.SERVICE_VERSION: version,\n\n}\n\n)\n\n)\n\nprovider = LogEmitterProvider(resource=resource) set_log_emitter_provider(provider)\n\n...\n\nWith the change in place, run shopper.py once again to see that the log record now contains more\n\nmeaningful data about the source of the log entry: {\"body\": \"add orange to cart\", \"name\": null,\n\n\"severity_number\": \"<SeverityNumber.INFO: 9>\", \"severity_text\": \"INFO\", \"attributes\": {},\n\n\"timestamp\": 1630949852869427968, \"trace_id\": \"0x2ff0e5c9886f2672c3af4468483d341d\",\n\n\"span_id\": \"0x40d72ae565b4c19a\", \"trace_flags\": 1, \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry',\n\n'telemetry.sdk.version': '1.9.0', 'net.host.name': 'MacBook-Pro.local', 'net.host.ip': '127.0.0.1',\n\n'service.name': 'shopper', 'service.version': '0.1.2'}, maxlen=None)\"}\n\nLooking at the previous output, we now know the name and version of the service. We also have\n\nvaluable information about the machine that generated this information. In a distributed system, this\n\ninformation can be used in combination with metrics generated by the same resource to identify\n\nproblems with a specific system, compute node, environment, or even region.\n\nSummary\n\nWith the knowledge of this chapter ingrained in our minds, we have now covered the core signals\n\nthat OpenTelemetry helps produce. Understanding how to produce telemetry by manually\n\ninstrumenting code is a building block on the road to improving observability. Without telemetry, the\n\njob of understanding what a system is doing is much more difficult.\n\nIn this chapter, we learned about the purpose of the logging implementation in OpenTelemetry, as\n\nwell as how it is intended to co-exist with existing logging implementations. After configuring the\n\nlogging pipeline, we learned how to use the OpenTelemetry API to produce logs and compared doing\n\nso with using a standard logging API. Returning to the grocery store, we explored how logging can\n\nbe correlated with traces and metrics. This allowed us to understand how we may be able to leverage\n\nOpenTelemetry logging within existing applications to improve our ability to use log statements\n\nwhen debugging applications.\n\nFinally, we scratched the surface of how instrumentation libraries can help to make the production of\n\ntelemetry easier. We will take an in-depth look at this in the next chapter, dedicated to simplifying the\n\ngrocery store application by leveraging existing instrumentation libraries.\n\nChapter 7: Instrumentation Libraries\n\nUnderstanding the ins and outs of the OpenTelemetry API is quite helpful for manually instrumenting\n\ncode. But what if we could save ourselves some of that work and still have visibility into what our\n\ncode is doing? As covered in Chapter 3, Auto-Instrumentation, one of the initial objectives of\n\nOpenTelemetry is providing developers with tools to instrument their applications at a minimal cost.\n\nInstrumentation libraries combined with auto-instrumentation enable users to start with\n\nOpenTelemetry without learning the APIs, and leverage the community's efforts and expertise.\n\nThis chapter will investigate the components of auto-instrumentation, how they can be configured,\n\nand how they interact with instrumentation libraries. Diving deeper into the implementation details of\n\ninstrumentation libraries will allow us to understand precisely how telemetry data is produced.\n\nAlthough telemetry created automatically may seem like magic, we'll seek to unveil the mechanics\n\nbehind this illusion. The chapter covers the following main topics:\n\nAuto-instrumentation configuration and its components\n\nThe Requests library instrumentor\n\nAutomatic configuration\n\nRevisiting the grocery store\n\nThe Flask library instrumentor\n\nFinding instrumentation libraries\n\nWith this information, we will revisit some of our existing code in the grocery store to simplify our\n\ncode and manage and improve the generated telemetry. Along the way, we will look at the specifics\n\nof existing third-party libraries supported by the OpenTelemetry project. Let's start with setting up\n\nour environment.\n\nTechnical requirements\n\nThe examples in this chapter are provided in this book's companion repository, found here:\n\nhttps://github.com/PacktPublishing/Cloud-Native-Observability. The source code can be downloaded\n\nvia git as per the following command: $ git clone https://github.com/PacktPublishing/Cloud-Native-\n\nObservability $ cd Cloud-Native-Observability/chapter07\n\nThe completed examples from this chapter are in the chapter7 directory. If you'd prefer the refactor\n\nalong, copy the code from chapter6 as a starting point. Next, we'll need to ensure the version of\n\nPython on your system is at least 3.6. You can verify it with the following commands: $ python --\n\nversion\n\nPython 3.8.9\n\n$ python3 --version\n\nPython 3.8.9\n\nThis chapter will use the same opentelemetry-api, opentelemetry-sdk, and opentelemetry-\n\npropagator-b3 packages we installed in previous chapters. In addition, we will use the\n\nopentelemetry-instrumentation and opentelemetry-distro packages. Install the packages via pip\n\nnow: $ pip install opentelemetry-api \\\n\nopentelemetry-sdk \\\n\nopentelemetry-instrumentation \\\n\nopentelemetry-propagator-b3 \\\n\nopentelemetry-distro\n\nWe will need to install additional packages libraries used by our applications: the Flask and Requests\n\nlibraries. Lastly, we will install the instrumentation libraries that automatically instrument the calls\n\nfor those libraries. The standard naming convention for instrumentation libraries in OpenTelemetry is\n\nto prefix the library's name being instrumented with opentelemetry-instrumentation-. Use pip to\n\ninstall those packages now: $ pip install flask \\\n\nopentelemetry-instrumentation-flask \\\n\nrequests \\\n\nopentelemetry-instrumentation-requests\n\nEnsure all the required packages have been installed by looking at the output from pip freeze, which\n\nlists all the packages installed: $ pip freeze | grep opentelemetry\n\nopentelemetry-api==1.9.0\n\nopentelemetry-distro==0.28b0\n\nopentelemetry-instrumentation==0.28b0\n\nopentelemetry-instrumentation-flask==0.28b0\n\nopentelemetry-instrumentation-requests==0.28b0\n\nopentelemetry-instrumentation-wsgi==0.28b0\n\nopentelemetry-propagator-b3==1.9.0\n\nopentelemetry-proto==1.9.0\n\nopentelemetry-sdk==1.9.0\n\nopentelemetry-semantic-conventions==0.28b0\n\nopentelemetry-util-http==0.28b0",
      "page_number": 164
    },
    {
      "number": 7,
      "title": "Instrumentation Libraries",
      "start_page": 181,
      "end_page": 209,
      "detection_method": "regex_chapter",
      "content": "Throughout the chapter, we will rely on two scripts made available by the opentelemetry-\n\ninstrumentation package: opentelemetry-instrument and opentelemetry-bootstrap. Ensure these\n\nscripts are available in your path with the following commands: $ opentelemetry-instrument --help\n\nusage: opentelemetry-instrument [-h]...\n\n$ opentelemetry-bootstrap --help\n\nusage: opentelemetry-bootstrap [-h]...\n\nNow that we have all the packages installed and the code available, let's see how auto-\n\ninstrumentation works in practice.\n\nAuto-instrumentation configuration\n\nSince auto-instrumentation aims to get started as quickly as possible, let's see how fast we can\n\ngenerate telemetry with as little code as possible. The following code makes a web request to\n\nhttps://www.cloudnativeobservability.com and prints the HTTP response code: http_request.py\n\nimport requests\n\nurl = \"https://www.cloudnativeobservability.com\"\n\nresp = requests.get(url)\n\nprint(resp.status_code)\n\nWhen running the code, assuming network connectivity is available and the URL we're requesting\n\nconnects us to a server that is operating normally, we should see 200 printed out: $ python\n\nhttp_request.py\n\n200\n\nGreat, the program works; now it's time to instrument it. The following command uses the\n\nopentelemetry-instrument application to wrap the application we created. We will look more closely\n\nat the command and its options shortly. For now, run the command: $ opentelemetry-instrument --\n\ntraces_exporter console \\\n\n--metrics_exporter console \\\n\n--logs_exporter console \\\n\npython http_request.py\n\nIf everything went according to plan, we should now see the following output, which contains\n\ntelemetry: output\n\n200\n\n{\n\n\"name\": \"HTTP GET\",\n\n\"context\": {\n\n\"trace_id\": \"0x953ca1322b930819077a921a838df0cd\", \"span_id\": \"0x5b3b72c9c836178a\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.CLIENT\",\n\n\"parent_id\": null,\n\n\"start_time\": \"2021-11-25T17:38:21.331540Z\", \"end_time\": \"2021-11-25T17:38:22.033434Z\",\n\n\"status\": {\n\n\"status_code\": \"UNSET\"\n\n},\n\n\"attributes\": {\n\n\"http.method\": \"GET\",\n\n\"http.url\": \"https://www.cloudnativeobservability.com\", \"http.status_code\": 200\n\n},\n\n\"events\": [],\n\n\"links\": [],\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.9.0\",\n\n\"telemetry.auto.version\": \"0.28b0\",\n\n\"service.name\": \"unknown_service\"\n\n}\n\n}\n\nOkay, that's exciting, but what just happened? Figure 7.1 shows how the opentelemetry-instrument\n\ncommand is instrumenting the code for our web request by doing the following:\n\n1. Loading the configuration options defined by the OpenTelemetryDistro class, which is part of the opentelemetry-\n\ndistro package.\n\n2. Automatically configuring the telemetry pipelines for traces, metrics, and logs via OpenTelemetryConfigurator. The\n\ndetails of how this configuration is set will become clearer shortly.\n\n3. Iterating through instrumentor classes registered via entry points in the Python environment under\n\nopentelemetry_instrumentor to find available instrumentation libraries. In doing so, it finds and loads the\n\nRequestsInstrumentor class defined in the opentelemetry-instrumentation-requests package.\n\n4. With the instrumentation library loaded, the call to get is now processed by the requests instrumentation library, which\n\ncreates a span before calling the original get method.\n\nThe preceding steps are depicted in the following diagram:\n\nFigure 7.1 – opentelemetry-instrument\n\nThe configuration of the telemetry pipeline involves a few different mechanisms loaded via entry\n\npoints at various times before the application code is executed. Thinking back to Chapter 3, Auto-\n\nInstrumentation, we introduced entry points (https://packaging.python.org/specifications/entry-\n\npoints/) as a mechanism that allows Python packages to register classes or methods globally. The\n\ncombination of entry points, interfaces, and options to choose from can make the configuration\n\nprocess a bit complex to understand.\n\nOpenTelemetry distribution\n\nThe first step in the configuration process is loading classes registered under the\n\nopentelemetry_distro entry point. This entry point is reserved for classes adhering to the BaseDistro\n\ninterface, and its purpose is to allow implementors to set configuration options at the earliest possible\n\ntime. The term distro is short for distribution, a concept that is still being officially defined in\n\nOpenTelemetry. Essentially, a distro is a way for users to customize OpenTelemetry to fit their needs,\n\nallowing them to reduce the complexity of deploying and using OpenTelemetry. For example, the\n\ndefault configuration for OpenTelemetry Python is to configure an OpenTelemetry protocol exporter\n\nfor all signals. This is accomplished via the OpenTelemetryDistro class mentioned previously. The\n\nfollowing code shows us how the OpenTelemetryDistro class configures the default exporter by\n\nsetting environment variables: OpenTelemetryDistro class\n\nclass OpenTelemetryDistro(BaseDistro):\n\n\"\"\"\n\nThe OpenTelemetry provided Distro configures a default\n\nconfiguration out of the box.\n\n\"\"\"\n\ndef _configure(self, **kwargs):\n\nos.environ.setdefault(OTEL_TRACES_EXPORTER, \"otlp_proto_grpc\")\n\nos.environ.setdefault(OTEL_METRICS_EXPORTER, \"otlp_proto_grpc\")\n\nos.environ.setdefault(OTEL_LOGS_EXPORTER, \"otlp_proto_grpc\") As a user, you could create\n\nyour distribution to preconfigure all the specific parameters needed to tailor auto-\n\ninstrumentation for your environment: for example, protocol, destination, and transport\n\noptions. A list of open source examples extending the BaseDistro interface can be found\n\nhere: https://github.com/PacktPublishing/Cloud-Native-\n\nObservability/tree/main/chapter7#opentelemetry-distro-implementations. With those options\n\nconfigured, you can then provide an entry point to your implementation of the BaseDistro\n\ninterface, package it up, and add this new package as a dependency in your applications.\n\nTherefore, the distribution makes deploying a consistent configuration across a\n\ndistributed system easier.\n\nOpenTelemetry configurator\n\nThe next piece of the configuration puzzle is what is currently known in OpenTelemetry Python as\n\nthe configurator. The purpose of the configurator is to load all the components defined in the\n\nconfiguration specified by the distro. Another way is to think of the distro as the co-pilot, deciding\n\nwhere the car needs to go, and the configurator as the driver. The configurator is an extensible and\n\ndeclarative interface for configuring OpenTelemetry. It is loaded by auto-instrumentation via, and\n\nyou may have guessed it, an entry point. The opentelemetry_configurator entry point is reserved for\n\nclasses adhering to the _BaseConfigurator interface, whose sole purpose is to prepare the logs,\n\nmetrics, and traces pipelines to produce telemetry.\n\nIMPORTANT NOTE\n\nAs you may have noticed, the _BaseConfigurator class is preceded by an underscore. This is done intentionally for\n\nclasses that are not officially part of the supported OpenTelemetry API in Python and warrant extra caution. Methods and\n\nclasses that are not supported formally can and often do change with new releases.\n\nThe implementation of the _BaseConfigurator interface loaded in the previous example, the\n\nOpenTelemetryConfigurator class configures a telemetry pipeline for each signal using components\n\nfrom the standard opentelemetry-sdk package. As a user, this configurator is precisely what you want\n\nmost of the time. However, if a user wishes to provide an alternative SDK, it would be possible to\n\nprovide their configurator implementation to use this SDK instead.\n\nThis covers the two main entry points used by auto-instrumentation. We will continue discussing\n\nadditional entry points throughout this chapter. As a reference, the following table captures the entry\n\npoints used by OpenTelemetry Python along with the interface each entry point expects. The table\n\nalso shows us a brief description of what each entry point is used for:\n\nFigure 7.2 – Entry points used in OpenTelemetry Python\n\nSimilar to OpenTelemetryDistro, the OpenTelemetryConfigurator class and its parent use environment\n\nvariables to achieve its goal of configuring OpenTelemetry for the end use.\n\nEnvironment variables\n\nTo provide additional flexibility to users, OpenTelemetry supports the configuration of many of its\n\ncomponents across all languages via environment variables. These variables are defined in the\n\nOpenTelemetry specification, ensuring each compliant language implementation understands them.\n\nThis allows users to re-use the same configuration options across any language they choose. I\n\nrecommend reading the complete list of options available in the specification repository found here:\n\nhttps://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/sdk-\n\nenvironment-variables.md.\n\nWe will look more closely at specific variables as we refactor the grocery store further in this chapter.\n\nMany, but not all, of the environment variables used by auto-instrumentation are part of the\n\nspecification linked previously. This is because the implementation details of each language may\n\nrequire additional variables not relevant to others. Language-specific environment variables are\n\nsupported in the following format: OTEL_{LANGUAGE}_{FEATURE}\n\nAs we'll see shortly, Python-specific options are prefixed with OTEL_PYTHON_. Any option with this\n\nprefix will only be found in Python, and the naming convention helps set that expectation with users.\n\nCommand-line options\n\nThe last tool available to configure OpenTelemetry without editing the application code is the use of\n\ncommand-line arguments, which can be set when invoking opentelemetry-instrument. Recall the\n\ncommand we used to call in the earlier example: $ opentelemetry-instrument --traces_exporter\n\nconsole \\\n\n--metrics_exporter console \\\n\n--logs_exporter console \\\n\npython http_request.py\n\nThis command used command-line arguments to override the traces, metrics, and logs exporters to\n\nuse the console exporter instead of the configured default. All options available via command line\n\ncan be listed using the --help flag when invoking opentelemetry-instrument. These options are the\n\nsame as those available through environment variables, with a slightly easier name for convenience.\n\nThe name of the command-line argument is the name of the environment variable in lowercase\n\nwithout the OTEL_ or OTEL_PYTHON prefix. The following table shows a few examples:\n\nFigure 7.3 – Environment variable to command-line argument translations With that, we've covered how auto-\n\ninstrumentation configures OpenTelemetry to generate the telemetry we saw. But what about the instrumented call?\n\nLet's see how the Requests library instrumentation works.\n\nRequests library instrumentor\n\nThe Instrumentor interface provides instrumentation libraries with the minimum requirements a\n\nlibrary must provide to support auto-instrumentation. Implementors must provide an implementation\n\nfor _instrument and _uninstrument, that's all. The instrumentation implementation details vary from\n\none library to another depending on whether the library offers any event or callback mechanisms for\n\ninstrumentation. In the case of the Requests library, the opentelemetry-instrumentation-requests\n\nlibrary relies on monkey patching the Session.request and Session.send methods from the requests\n\nlibrary. This instrumentation library does the following:\n\n1. Provides a wrapper method for the library calls that it instruments, and intercepts calls through those wrappers\n\n2. Upon invocation, creates a new span by calling the start_as_current_span method of the OpenTelemetry API, ensuring\n\nthe span name follows semantic conventions\n\n3. Injects the context information into the request headers via the context API's attach method to ensure the tracing data is\n\npropagated to the request's destination\n\n4. Reads the response and sets the status code accordingly via the span's set_status method Important Note\n\nInstrumentation libraries must check if the span will be recorded before adding additional attributes to avoid potentially\n\ncostly operations. This is done to minimize the instrumentation's impact on existing applications when it is not in use.\n\nAdditional implementation details can be found in the opentelemetry-python-contrib repository:\n\nhttps://github.com/open-telemetry/opentelemetry-python-\n\ncontrib/blob/main/instrumentation/opentelemetry-instrumentation-\n\nrequests/src/opentelemetry/instrumentation/requests/__init__.py. The code may inspire you to write\n\nand contribute an instrumentation library of your own.\n\nAdditional configuration options\n\nThe Requests instrumentation library supports the following additional configurable options:\n\nspan_callback: A callback mechanism to inject additional information into a span is available via this parameter. For\n\nexample, this allows users to inject additional information from the response into the span.\n\nname_callback: The default name of a span created by the requests instrumentation library is in the HTTP {method}\n\nformat. The name_callback parameter allows users to customize the name of the span as needed.\n\nexcluded_urls: There are HTTP destinations for which capturing telemetry may not be desirable, a typical case being\n\nrequests made to a health check endpoint. The excluded_urls parameter supports configuring a comma-separated list of\n\nURLs exempt from telemetry. This parameter is also configurable via the OTEL_PYTHON_REQUESTS_EXCLUDED_URLS\n\nenvironment variable and is available for use with auto-instrumentation.\n\nAs you may have noted by reading the description of each configuration option, not all these options\n\nare available for configuration via auto-instrumentation. It's possible to use instrumentation libraries\n\nwithout auto-instrumentation. Let's see how.\n\nManual invocation\n\nThe following code updates the previous example to configure a tracer and instrument the\n\nrequests.get call via the instrumentation library: http_request.py\n\nimport requests\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom opentelemetry.sdk.trace.export import (\n\nBatchSpanProcessor,\n\nConsoleSpanExporter,\n\n)\n\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\n\ndef configure_tracer():\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter)\n\nprovider = TracerProvider()\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nconfigure_tracer()\n\nRequestsInstrumentor().instrument()\n\nurl = \"https://www.cloudnativeobservability.com\"\n\nresp = requests.get(url)\n\nprint(resp.status_code)\n\nThis is quite a bit of additional code. Since we're no longer relying on auto-instrumentation, we must\n\nconfigure the tracing pipeline manually. Running this code without invoking opentelemetry-\n\ninstrument looks like this: $ python http_request.py\n\nThis should yield very similar telemetry to what we saw earlier. The following shows an excerpt of\n\nthat output: output\n\n200\n\n{\n\n\"name\": \"HTTP GET\",\n\n\"context\": {\n\n\"trace_id\": \"0xc2ee1f399911a10d361231a46c6fec1b\", ...\n\nWe can further customize the telemetry produced by configuring additional options we discussed\n\npreviously. The following code example will customize the name of the span and add other attributes\n\nto the data generated. It does so by doing the following:\n\nAdding a rename_span method to replace the HTTP prefix in the name\n\nAdding the add_response_attribute method to append header information from the response object as a span attribute\n\nUpdating the call to instrument to utilize the new functionality\n\nhttp_request.py\n\ndef rename_span(method, url):\n\nreturn f\"Web Request {method}\"\n\ndef add_response_attributes(span, response):\n\nspan.set_attribute(\"http.response.headers\", str(response.headers))\n\nconfigure_tracer()\n\nRequestsInstrumentor().instrument(\n\nname_callback=rename_span,\n\nspan_callback=add_response_attributes,\n\n)\n\nRunning the updated code should give us the slightly updated telemetry as per the following\n\nabbreviated sample output: output\n\n200\n\n{\n\n\"name\": \"Web Request GET\",\n\n\"attributes\": {\n\n\"http.method\": \"GET\",\n\n\"http.url\": \"https://www.cloudnativeobservability.com\", \"http.status_code\": 200,\n\n\"http.response.headers\": \"{'Connection': 'keep-alive', 'Content-Length': '1864', 'Server':\n\n'GitHub.com'\n\n...\n\nWith this, we've now seen how to leverage the Requests instrumentation library without using auto-\n\ninstrumentation. The added flexibility of the features not available through auto-instrumentation is\n\nnice, but configuring pipelines is tedious. Thankfully, it's possible to get the best of both worlds by\n\nusing auto-instrumentation and configuring the instrumentor manually. Update the example to\n\nremove all the configuration code. The following is all that should be left: http_request.py\n\nimport requests\n\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor def\n\nrename_span(method, url):\n\nreturn f\"Web Request {method}\"\n\ndef add_response_attributes(span, response):\n\nspan.set_attribute(\"http.response.headers\", str(response.headers))\n\nRequestsInstrumentor().instrument(\n\nname_callback=rename_span,\n\nspan_callback=add_response_attributes,\n\n)\n\nresp = requests.get(\"https://www.cloudnativeobservability.com\") print(resp.status_code)\n\nRun the new code via the following command we used earlier in the chapter: $ opentelemetry-\n\ninstrument --traces_exporter console \\\n\n--metrics_exporter console \\\n\n--logs_exporter console \\\n\npython http_request.py\n\nLooking at the output, it's clear that something didn't go as planned. The following warning appears\n\nat the top of the output: Attempting to instrument while already instrumented\n\nAdditionally, if you look through the telemetry generated, the span name is back to its original value,\n\nand the response headers attribute is missing. Recall that the opentelemetry-instrument script\n\niterates through all the installed instrumentors before calling the application code. This means that by\n\nthe time our application code is executed, the Request instrumentor has already instrumented the\n\nRequests library.\n\nDouble instrumentation\n\nMany instrumentation libraries have a safeguard in place to prevent double instrumentation. Double\n\ninstrumentation in most cases would mean that every piece of telemetry generated is recorded twice.\n\nThis causes all sorts of problems, from potential added performance costs to making telemetry\n\nanalysis difficult.\n\nWe can ensure that the library isn't instrumented first to mitigate this issue. Add the following method\n\ncall to your code: http_request.py\n\nimport requests\n\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor ...\n\nRequestsInstrumentor().uninstrument()\n\nRequestsInstrumentor().instrument(\n\nname_callback=rename_span,\n\nspan_callback=add_response_attributes,\n\n)\n\nRunning this code once more shows us that the warning is gone and that the telemetry contains the\n\ncustomization we expected. All this with much simpler code. Great! Let's see now how we can apply\n\nthis to the grocery store.\n\nAutomatic configuration\n\nWe added new instrumentation in the past three chapters and watched how we could generate more\n\ninformation each time we instrumented the code. We will now see how we can continue to provide\n\nthe same level of telemetry but simplify our lives by removing some of the code. The first code we\n\nwill be removing is the configuration code we extracted into the common.py module. If you recall from\n\nprevious chapters, the purpose of the configure_tracer, configure_meter, and configure_logger\n\nmethods, which we will review in detail shortly, is to do the following:\n\nConfigure the emitter of telemetry.\n\nConfigure the destination and mechanism to output the telemetry.\n\nAdd resource information to identify our service.\n\nAs we saw earlier in this chapter, the opentelemetry-instrument script enables us to remove the code\n\ndoing the configuration by interpreting environment variables or command-line arguments that will\n\ndo the same thing. We will review the configuration code for each signal and look at the flags that\n\ncan be used to replace the code with environment variables. One of the configurations common to all\n\nsignals is the resource information; let's start there.\n\nConfiguring resource attributes\n\nA resource provides information about the source of the telemetry. If you look through the common.py\n\ncode, you may recall that each method used to configure a signal also called methods to configure the\n\nresource. The code looks something like the following: common.py\n\nlocal_resource = LocalMachineResourceDetector().detect()\n\nresource = local_resource.merge(\n\nResource.create(\n\n{\n\nResourceAttributes.SERVICE_NAME: name,\n\nResourceAttributes.SERVICE_VERSION: version,\n\n}\n\n)\n\n)\n\nThe code uses a resource detector to fill in the hostname and IP address automatically. A current\n\nlimitation of auto-instrumentation in Python is the lack of support for configuring resource detectors.\n\nThankfully, since the functionality of our resource detector is somewhat limited, it's possible to\n\nreplace it, as we'll see shortly.\n\nThe code also adds a service name and version information to our resource. Resource attributes can\n\nbe configured for auto-instrumentation through one of the following options:\n\nFigure 7.4 – Resource configuration\n\nNote that the command-line arguments are shown here for reference only. For the remainder of the\n\nchapter, the commands used to run applications will use environment variables. The format of the\n\nparameters used for both methods is interchangeable. However, the OpenTelemetry specification only\n\nofficially supports environment variables. These are consistent across implementations.\n\nThe following shows how using only environment variables to configure resources can produce the\n\nsame result as the previous code. The example uses the hostname system utility to retrieve the name\n\nof the current host and ipconfig to retrieve the IP address. The invocation for these tools may vary\n\ndepending on your system: $ OTEL_RESOURCE_ATTRIBUTES=\"service.name=chap7-Requests-\n\napp, service.version=0.1.2,\n\nnet.host.name='hostname',\n\nnet.host.ip='ipconfig getifaddr en0'\" \\\n\nopentelemetry-instrument --traces_exporter console \\\n\n--metrics_exporter console \\\n\n--logs_exporter console \\\n\npython http_request.py\n\nThe resource information in the output from this command now includes the following details: output\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.9.0\",\n\n\"service.name\": \"chap7-Requests-app\",\n\n\"service.version\": \"0.1.2\",\n\n\"net.host.name\": \"cloud\",\n\n\"net.host.ip\": \"10.0.0.141\",\n\n\"telemetry.auto.version\": \"0.28b0\"\n\n}\n\nWe can now start configuring signals with resource attributes out of the way.\n\nConfiguring traces\n\nThe following code shows the configure_tracer method used to configure the tracing pipeline. Note\n\nthat the code no longer contains resource configuration as we've already taken care of that:\n\ncommon.py\n\ndef configure_tracer(name, version):\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter)\n\nprovider = TracerProvider()\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(name, version)\n\nThe main components to configure for tracing to emit telemetry are as follows:\n\nTracerProvider\n\nSpanProcessor\n\nSpanExporter\n\nIt's possible to set both TracerProvider and SpanExporter via environment variables. This is not the\n\ncase for SpanProcessor. The OpenTelemetry SDK for Python defaults to using BatchSpanProcessor\n\nwhen auto-instrumentation is used in combination with the opentelemetry-distro package. Options\n\nfor configuring BatchSpanProcessor are available via environment variables.\n\nIMPORTANT NOTE\n\nBatchSpanProcessor will satisfy most use cases. However, if your application requires an alternative SpanProcessor\n\nimplementation, it can be specified via a custom OpenTelemetry distribution package. Custom span processors can filter\n\nor enhance data before it is exported.\n\nAnother component we haven't talked about much yet is the sampler, which we'll cover in Chapter\n\n12, Sampling. For now, it's enough to know that the sampler is also configurable via environment\n\nvariables.\n\nThe following table shows the options for configuring the tracing pipeline. The acronym BSP stands\n\nfor BatchSpanProcessor:\n\nFigure 7.5 – Tracing configuration\n\nAs we continue adding configuration options, the command used to launch the application can get\n\nquite unruly. To alleviate this, I recommend exporting each variable as we go along. The following\n\nexports the OTEL_RESOURCE_ATTRIBUTES variable we previously set: $ export\n\nOTEL_RESOURCE_ATTRIBUTES=\"service.name=chap7-Requests-app, service.version=0.1.2,\n\nnet.host.name='hostname', net.host.ip='ipconfig getifaddr en0'\"\n\nWe've already configured the exporter via command-line arguments in previous examples. The\n\nfollowing shows us configuring the exporter and provider via environment variables. The console\n\nand sdk strings correspond to the name of the entry point for the ConsoleSpanExporter and the\n\nOpenTelemetry SDK TracerProvider classes: $ OTEL_TRACES_EXPORTER=console \\\n\nOTEL_PYTHON_TRACER_PROVIDER=sdk \\\n\nopentelemetry-instrument --metrics_exporter console \\\n\n--logs_exporter console \\\n\npython http_request.py\n\nReading the output from the previous command is uneventful as it is just setting the same\n\nconfiguration in another way. However, we can now move on to metrics with this configuration in\n\nplace.\n\nConfiguring metrics\n\nThe configuration for metrics is similar to the configuration for tracing, as we can see from the\n\nfollowing code for the configure_meter method: common.py\n\ndef configure_meter(name, version):\n\nexporter = ConsoleMetricExporter()\n\nprovider = MeterProvider()\n\nset_meter_provider(provider)\n\nreturn get_meter_provider().get_meter(\n\nname=name,\n\nversion=version,\n\n)\n\nAt the time of writing, the specification for metrics is reaching stability. As such, the support for\n\nauto-instrumentation and configuration will likely solidify over the coming months. For now, this\n\nsection will focus on the options that are available and not likely to change, which covers the\n\nfollowing:\n\nMeterProvider\n\nMetricExporter\n\nThe following table shows the options available to configure the metrics pipeline:\n\nFigure 7.6 – Metrics configuration\n\nThe following command is provided as a reference for configuring MeterProvider and\n\nMetricsExporter via environment variables: $ OTEL_METRICS_EXPORTER=console \\\n\nOTEL_PYTHON_METER_PROVIDER=sdk \\\n\nopentelemetry-instrument --logs_exporter console \\\n\npython http_request.py\n\nNote that running the previous command as is results in an error as it does not configure the tracing\n\nsignal. Any signal not explicitly configured defaults to using the OpenTelemetry Protocol (OTLP)\n\nexporter, which we've not installed in this environment. As the application does not currently produce\n\nmetrics, we wouldn't expect to see any changes in the telemetry emitted.\n\nConfiguring logs\n\nThe configure_logger method configures the following OpenTelemetry components:\n\nLogEmitterProvider\n\nLogProcessor\n\nLogExporter\n\ncommon.py\n\ndef configure_logger(name, version):\n\nprovider = LogEmitterProvider()\n\nset_log_emitter_provider(provider)\n\nexporter = ConsoleLogExporter()\n\nprovider.add_log_processor(BatchLogProcessor(exporter))\n\nlogger = logging.getLogger(name)\n\nlogger.setLevel(logging.DEBUG)\n\nhandler = OTLPHandler()\n\nlogger.addHandler(handler)\n\nreturn logger\n\nAs with metrics, the configuration and auto-instrumentation for the logging signal are still currently\n\nunder development. The following table can be used as a reference for the environment variables and\n\ncommand-line arguments available to configure logging at the time of writing:\n\nFigure 7.7 – Logging configuration\n\nAs with the tracing configuration's span processor, there isn't currently a mechanism for configuring\n\nthe log processor via auto-instrumentation. This can change in the future. Using those options, we\n\nknow how to configure the last signal for auto-instrumentation: $\n\nOTEL_LOGS_EXPORTER=console \\\n\nOTEL_PYTHON_LOG_EMITTER_PROVIDER=sdk \\\n\nopentelemetry-instrument python http_request.py\n\nWe're almost ready to revisit the grocery store code with the signals and resources configured. The\n\nlast thing left to configure is propagation.\n\nConfiguring propagation\n\nContext propagation provides the ability to share context information across distributed systems. This\n\ncan be accomplished via various mechanisms, as we discovered in Chapter 4, Distributed Tracing –\n\nTracing Code Execution. To ensure applications can interoperate with any of the propagation formats,\n\nOpenTelemetry supports configuring propagators via the following environment variable:\n\nFigure 7.8 – Propagator configuration\n\nLater in this chapter, an application will need to configure the B3 and TraceContext propagators.\n\nOpenTelemetry makes it possible to configure multiple propagators by specifying a comma-separated\n\nlist. As mentioned earlier, with so many configuration options, using environment variables can\n\nbecome hard to manage. An effort is underway to add support for configuration files to\n\nOpenTelemetry, but the timeline on when that will be available is still in flux.\n\nRecall the code we instrumented in the last three chapters. Let's go through it now and leverage\n\nconfiguration and the instrumentation libraries wherever possible.\n\nRevisiting the grocery store\n\nIt's finally time to use all this new knowledge about auto-instrumentation to clean up the grocery\n\nstore application. This section will showcase the simplified code that continues to produce the\n\ntelemetry we've come to expect over the last few chapters. The custom decorators have been\n\nremoved, as has the code configuring the tracer provider, meter provider, and log emitter provider.\n\nAll we're left with now is the application code.\n\nLegacy inventory\n\nThe legacy inventory service is a great place to start. It is a small Flask application with a single\n\nendpoint. The Flask instrumentor, installed at the beginning of the chapter via the opentelemetry-\n\ninstrumentation-flask package, will replace the manual instrumentation code we previously added.\n\nThe following code instantiates the Flask app and provides the /inventory endpoint:\n\nlegacy_inventory.py\n\n#!/usr/bin/env python3\n\nfrom flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route(\"/inventory\")\n\ndef inventory():\n\nproducts = [\n\n{\"name\": \"oranges\", \"quantity\": \"10\"}, {\"name\": \"apples\", \"quantity\": \"20\"}, ]\n\nreturn jsonify(products)\n\nif __name__ == \"__main__\":\n\napp.run(port=5001)\n\nIf you remember from previous chapters, this service was configured to use the B3 format propagator.\n\nThis will be reflected in the configuration options we pass in when starting the service via auto-\n\ninstrumentation: $ OTEL_RESOURCE_ATTRIBUTES=\"service.name=legacy-inventory,\n\nservice.version=0.9.1,\n\nnet.host.name='hostname',\n\nnet.host.ip='ipconfig getifaddr en0'\" \\\n\nOTEL_TRACES_EXPORTER=console \\\n\nOTEL_PYTHON_TRACER_PROVIDER=sdk \\\n\nOTEL_METRICS_EXPORTER=console \\\n\nOTEL_PYTHON_METER_PROVIDER=sdk \\\n\nOTEL_LOGS_EXPORTER=console \\\n\nOTEL_PYTHON_LOG_EMITTER_PROVIDER=sdk \\\n\nOTEL_PROPAGATORS=b3 \\\n\nopentelemetry-instrument python legacy_inventory.py\n\nWith this service running, let's look at the next one.\n\nGrocery store\n\nThe next service to revisit is the grocery store. This service is also a Flask application and will\n\nleverage the same instrumentation library. In addition, it will use the Requests instrumentor to add\n\ntelemetry to the calls it makes to the legacy inventory. The code looks like this: grocery_store.py\n\n#!/usr/bin/env python3\n\nfrom logging.config import dictConfig\n\nimport requests\n\nfrom flask import Flask\n\nfrom opentelemetry.instrumentation.wsgi import OpenTelemetryMiddleware dictConfig(\n\n{\n\n\"version\": 1,\n\n\"handlers\": {\n\n\"otlp\": {\n\n\"class\": \"opentelemetry.sdk._logs.OTLPHandler\", }\n\n},\n\n\"root\": {\"level\": \"DEBUG\", \"handlers\": [\"otlp\"]}, }\n\n)\n\napp = Flask(__name__)\n\napp.wsgi_app = OpenTelemetryMiddleware(app.wsgi_app)\n\n@app.route(\"/\")\n\ndef welcome():\n\nreturn \"Welcome to the grocery store!\"\n\n@app.route(\"/products\")\n\ndef products():\n\nurl = \"http://localhost:5001/inventory\"\n\nresp = requests.get(url)\n\nreturn resp.text\n\nif __name__ == \"__main__\":\n\napp.run(port=5000)\n\nRunning the application will look very similar to running the legacy inventory with only a few\n\ndifferent parameters:\n\nservice.name and service.version will be updated to reflect the different applications.\n\nThe propagators will be configured to use both B3 and TraceContext formats, making it possible for context to be propagated\n\nfrom the shopper through to the legacy inventory.\n\nIn a separate terminal window, with the legacy inventory service still running, run the following to\n\nstart the grocery store: $ OTEL_RESOURCE_ATTRIBUTES=\"service.name=grocery-store,\n\nservice.version=0.1.2,\n\nnet.host.name='hostname',\n\nnet.host.ip='ipconfig getifaddr en0'\" \\\n\nOTEL_TRACES_EXPORTER=console \\\n\nOTEL_PYTHON_TRACER_PROVIDER=sdk \\\n\nOTEL_METRICS_EXPORTER=console \\\n\nOTEL_PYTHON_METER_PROVIDER=sdk \\\n\nOTEL_LOGS_EXPORTER=console \\\n\nOTEL_PYTHON_LOG_EMITTER_PROVIDER=sdk \\\n\nOTEL_PROPAGATORS=b3,tracecontext \\\n\nopentelemetry-instrument python grocery_store.py\n\nThe grocery store is up and running. Now we just need to generate some requests via the shopper\n\nservice.\n\nShopper\n\nFinally, the shopper application initiates the request through the system. The RequestsInstrumentor\n\ninstruments web requests to the grocery store. Of course, the backend requests don't tell the whole\n\nstory about what goes on inside the shopper application.\n\nAs discussed in Chapter 3, Auto-Instrumentation, auto-instrumentation can be pretty valuable. In rare\n\ncases, it can even be enough to cover most of the functionality within an application. Applications\n\nfocused on Create, Read, Update, and Delete operations (https://en.wikipedia.org/wiki/CRUD) may\n\nnot contain enough business logic to warrant manual instrumentation. Operators of applications\n\nrelying heavily on instrumented libraries may also gain enough visibility from auto-instrumentation.\n\nHowever, you'll want to add additional details about your code in most scenarios. For those cases, it's\n\ncrucial to combine auto-instrumentation with manual instrumentation. Such is the case for the last\n\napplication in our system. The following code shows us the simplified version of the shopper service.\n\nAs you can see from the code, there is still manual instrumentation code, but no configuration to be\n\nseen, as this is all managed by auto-instrumentation. Additionally, you'll note that the get call from\n\nthe requests module no longer requires manual instrumentation: shopper.py\n\n#!/usr/bin/env python3\n\nimport logging\n\nimport requests\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.sdk._logs import OTLPHandler\n\ntracer = trace.get_tracer(\"shopper\", \"0.1.2\") logger = logging.getLogger(\"shopper\")\n\nlogger.setLevel(logging.DEBUG)\n\nlogger.addHandler(OTLPHandler())\n\n@tracer.start_as_current_span(\"add item to cart\")\n\ndef add_item_to_cart(item, quantity):\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\n\"item\": item,\n\n\"quantity\": quantity,\n\n}\n\n)\n\nlogger.info(\"add {} to cart\".format(item))\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nresp = requests.get(\"http://localhost:5000/products\") add_item_to_cart(\"orange\", 5)\n\n@tracer.start_as_current_span(\"visit store\")\n\ndef visit_store():\n\nbrowse()\n\nif __name__ == \"__main__\":\n\nvisit_store()\n\nIt's time to generate some telemetry! Open a third terminal and launch the shopper application with\n\nthe following command: $ OTEL_RESOURCE_ATTRIBUTES=\"service.name=shopper,\n\nservice.version=0.1.3,\n\nnet.host.name='hostname',\n\nnet.host.ip='ipconfig getifaddr en0'\" \\\n\nOTEL_TRACES_EXPORTER=console \\\n\nOTEL_PYTHON_TRACER_PROVIDER=sdk \\\n\nOTEL_METRICS_EXPORTER=console \\\n\nOTEL_PYTHON_METER_PROVIDER=sdk \\\n\nOTEL_LOGS_EXPORTER=console \\\n\nOTEL_PYTHON_LOG_EMITTER_PROVIDER=sdk \\\n\nopentelemetry-instrument python shopper.py\n\nThis command should have generated telemetry from all three applications visible in the individual\n\nterminal windows.\n\nIMPORTANT NOTE\n\nSince the metrics and logging signals are under active development, the instrumentation libraries we use in this chapter\n\nonly support tracing. Therefore, we will focus on the tracing data being emitted for the time being. It's possible that by the\n\ntime you're reading this, those libraries also emit logs and metrics.\n\nWe will not go through it in detail since the tracing data being emitted is similar to the data we've\n\nalready inspected for the grocery store. Looking through the distributed trace generated, we can see\n\nthe following:\n\nSpans generated for each application; the service.name and service.version resource attributes should reflect this.\n\nThe trace ID has been propagated correctly across application boundaries. Check the trace_id field across all three terminals to\n\nconfirm.\n\nThe Requests and Flask instrumentation libraries have automatically populated attributes.\n\nThe following diagram offers a visualization of the spans generated across the system. Spans are\n\nidentified as having been automatically generated (A) or manually generated (M).\n\nFigure 7.9 – Tracing information generated\n\nThis is one of the most exciting aspects of OpenTelemetry. We have telemetry generated by two\n\napplications that contain no instrumentation code. The developers of those applications don't need to\n\nlearn about OpenTelemetry for their applications to produce information about their service, which\n\ncan be helpful to diagnose issues in the future. Getting started has never been easier. Let's take a\n\nquick look at how the Flask instrumentation works.\n\nFlask library instrumentor\n\nLike the Requests library, the Flask instrumentation library contains an implementation of the\n\nBaseInstrumentor interface. The code is available in the OpenTelemetry Python contrib repository at\n\nhttps://github.com/open-telemetry/opentelemetry-python-\n\ncontrib/blob/main/instrumentation/opentelemetry-instrumentation-\n\nflask/src/opentelemetry/instrumentation/flask/__init__.py. The implementation leverages a few\n\ndifferent aspects of the Flask library to achieve instrumentation. It wraps the original Flask app and\n\nregisters a callback via the before_request method. It then provides a middleware to execute\n\ninstrumentation code at response time. This allows the instrumentation to capture the beginning and\n\nthe end of requests through the library.\n\nAdditional configuration options\n\nThe following options are available to configure FlaskInstrumentor further:\n\nexcluded_urls: Supports a comma-separated list of regular expressions for excluding specific URLs from producing\n\ntelemetry. This option is also configurable with auto-instrumentation via the OTEL_PYTHON_FLASK_EXCLUDED_URLS\n\nenvironment variable.\n\nrequest_hook: A method to be executed before every Request received by the Flask application.\n\nresponse_hook: Similar to the request_hook argument, the response_hook allows a user to configure a method to be\n\nperformed before a response is returned to the caller.\n\nIMPORTANT NOTE\n\nWhen using the Flask instrumentation library with auto-instrumentation, it's essential to know that the debug mode\n\nmay cause issues. By default, the debug mode uses a reloader, which causes the auto-instrumentation to fail. For\n\nmore information on disabling the reloader, see the OpenTelemetry Python documentation: https://opentelemetry-\n\npython.readthedocs.io/en/latest/examples/auto-instrumentation/README.html#instrumentation-while-debugging.\n\nThe Requests and Flask instrumentation libraries are just two of many instrumentation libraries\n\navailable for Python developers.\n\nFinding instrumentation libraries\n\nA challenge with instrumentation libraries is keeping track of which libraries are available across\n\ndifferent languages. The libraries available for Python currently live in the opentelemetry-collector-\n\ncontrib repository (https://github.com/open-telemetry/opentelemetry-python-contrib), but that may\n\nnot always be the case.\n\nOpenTelemetry registry\n\nThe official OpenTelemetry website provides a searchable registry (https://opentelemetry.io/registry/)\n\nthat includes packages across languages. This information for this registry is stored in a GitHub\n\nrepository, which can be updated via pull Requests.\n\nopentelemetry-bootstrap\n\nTo make getting started even more accessible, the OpenTelemetry Python community maintains the\n\nopentelemetry-bootstrap tool, installed via the opentelemetry-instrumentation package. This tool\n\nlooks at all installed packages in an environment and lists instrumentation libraries for that\n\nenvironment. It's possible to use the command also to install instrumentation libraries. The following\n\ncommand shows us how to use opentelemetry-bootstrap to list packages: $ opentelemetry-bootstrap\n\nopentelemetry-instrumentation-logging==0.28b0\n\nopentelemetry-instrumentation-urllib==0.28b0\n\nopentelemetry-instrumentation-wsgi==0.28b0\n\nopentelemetry-instrumentation-flask==0.28b0\n\nopentelemetry-instrumentation-jinja2==0.28b0\n\nopentelemetry-instrumentation-requests==0.28b0\n\nopentelemetry-instrumentation-urllib3==0.28b0\n\nLooking through that list, there are a few additional packages that we may want to install now that we\n\nknow about them. Conveniently, the -a install option installs all the listed packages.\n\nSummary\n\nInstrumentation libraries for third-party libraries are an excellent way for users to use OpenTelemetry\n\nwith little to no effort. Additionally, instrumentation libraries don't require users to wait for third-\n\nparty libraries to support OpenTelemetry directly. This helps reduce the burden on the maintainers of\n\nthose third-party libraries by not asking them to support APIs, which are still evolving.\n\nThis chapter allowed us to understand how auto-instrumentation leverages instrumentation libraries\n\nto simplify the user experience of adopting OpenTelemetry. By inspecting all the components that\n\ncombine to make it possible to simplify the code needed to configure telemetry pipelines, we were\n\nable to produce telemetry with little to no instrumentation code.\n\nRevisiting the grocery store then allowed us to compare the telemetry generated by auto-instrumented\n\ncode with manual instrumentation. Along the way, we took a closer look at how different\n\ninstrumentations are implemented and their configurable options.\n\nAlthough instrumentation libraries make it possible for users to start using OpenTelemetry today,\n\nthey require the installation of another library within environments, taking on additional\n\ndependencies. As instrumentation libraries have only just started maturing, this may cause users to\n\nhesitate to adopt them. Ideally, as OpenTelemetry adoption increases and its API reaches stability\n\nacross signals, third-party library maintainers will start instrumenting the libraries themselves with\n\nOpenTelemetry, removing the need for an additional library. This has already begun with some\n\nframeworks, such as Spring in Java and .NET Core libraries.\n\nWith the knowledge of OpenTelemetry signals, instrumentation libraries, and auto-instrumentation in\n\nour toolbelt, we will now focus on what to do with the telemetry data we're producing. The following\n\nfew chapters will focus on collecting, transmitting, and analyzing OpenTelemetry data. First, all this\n\ndata must go somewhere, and the OpenTelemetry Collector is a perfect destination. This will be the\n\ntopic of the next chapter.\n\nSection 3: Using Telemetry Data\n\nIn this part, you will learn how to deploy the OpenTelemetry Collector in conjunction with various\n\nbackends to visualize the telemetry data as well as identify issues with their cloud-native\n\napplications.\n\nThis part of the book comprises the following chapters:\n\nChapter 8, OpenTelemetry Collector\n\nChapter 9, Deploying the Collector\n\nChapter 10, Configuring Backends\n\nChapter 11, Diagnosing Problems\n\nChapter 12, Sampling\n\nChapter 8: OpenTelemetry Collector\n\nSo, now that we've learned how to use OpenTelemetry to generate traces, metrics, and logs, we want\n\nto do something with all this telemetry data. To make the most of this data, we will need to be able to\n\nstore and visualize it because, let's be honest – reading telemetry data from the console isn't going to\n\ncut it. As we'll discuss in Chapter 10, Configuring Backends, many destinations can be used for\n\ntelemetry data. To send telemetry to a backend, the telemetry pipeline for metrics, traces, and logs\n\nneeds to be configured to use an exporter that's specific to that signal and the backend. For example,\n\nif you wanted to send traces to Zipkin, metrics to Prometheus, and logs to Elasticsearch, each would\n\nneed to be configured in the appropriate application code. Configuring this across dozens of services\n\nwritten in different languages adds to the complexity of managing the code. But now, imagine\n\ndeciding that one of the backends must be changed because it no longer suits the needs of your\n\nbusiness. Although it may not seem like a lot of work on a small scale, in a distributed system with\n\napplications that have been produced over many years by various engineers, the amount of effort to\n\nupdate, test, and deploy all that code could be quite significant, not to mention risky.\n\nWouldn't it be great if there were a way to configure an exporter once, and then use only\n\nconfiguration files to modify the destination of the data? There is – it's called OpenTelemetry\n\nCollector and this is what we'll be exploring in this chapter.\n\nIn this chapter, we will cover the following topics:\n\nThe purpose of OpenTelemetry Collector\n\nUnderstanding the components of OpenTelemetry Collector\n\nTransporting telemetry via OTLP\n\nUsing OpenTelemetry Collector\n\nLet's start by ensuring we have all the tools in place to work with the collector.\n\nTechnical requirements\n\nThis chapter will introduce OpenTelemetry Collector as a standalone binary, which can be\n\ndownloaded from https://github.com/open-telemetry/opentelemetry-collector-\n\nreleases/releases/tag/v0.43.0. It's also possible to build the collector from the source, but this will not\n\nbe covered in this chapter. The following commands will download the binary that's been compiled\n\nfor macOS on Intel processors, extract the otelcol file, and ensure the binary can be executed: $\n\nwget -O otelcol.tar.gz https://github.com/open-telemetry/opentelemetry-collector-\n\nreleases/releases/download/v0.43.0/otelcol_0.43.0_darwin_amd64.tar.gz $ tar -xzf otelcol.tar.gz\n\notelcol\n\n$ chmod +x ./otelcol\n\n$ ./otelcol --version\n\notelcol version 0.43.0\n\nWith the correct binary downloaded, let's ensure that the collector can start by using the following\n\ncommand. It is expected that the process will exit: $ ./otelcol\n\nError: failed to get config: invalid configuration: no enabled receivers specified in\n\nconfig 2022/02/13 11:52:47 collector server run finished with error: failed to get config:\n\ninvalid configuration: no enabled receivers specified in config Important Note\n\nThe OpenTelemetry Collector project produces a different binary for various operating systems (Windows, Linux, and\n\nmacOS) and architectures. You must download the correct one for your environment.\n\nThe configuration for the collector is written in YAML format (https://en.wikipedia.org/wiki/YAML),\n\nbut we'll try to steer clear of most of the traps of YAML by providing complete configuration\n\nexamples. The collector is written in Go, so this chapter includes code snippets in Go. Each piece of\n\ncode will be thoroughly explained, but don't worry if the details of the language escape you – the\n\nconcept of the code is what we'll be focusing on. To send data to OpenTelemetry Collector from\n\nPython applications, we'll need to install the OTLP exporter, which can be done via pip: $ pip install\n\nopentelemetry-exporter-otlp \\\n\nopentelemetry-propagator-b3 \\\n\nopentelemetry-instrumentation-wsgi \\\n\nflask \\\n\nrequests\n\nIMPORTANT NOTE\n\nThe opentelemetry-exporter-otlp package itself does not contain any exporter code. It uses dependencies to pull\n\nin a different package for each different encoding and transport option that's supported by OTLP We will discuss these\n\nlater in this chapter.\n\nThe completed code and configuration for this chapter is available in this book's GitHub repository in\n\nthe chapter08 directory: $ git clone https://github.com/PacktPublishing/Cloud-Native-Observability $\n\ncd Cloud-Native-Observability/chapter08\n\nAs with the previous chapters, the code in these examples builds on top of the previous chapters. If\n\nyou'd like to follow along with the code changes, copy the code from the chapter06 folder. Now, let's\n\ndive in and figure out what this collector is all about, and why you should care about it.",
      "page_number": 181
    },
    {
      "number": 8,
      "title": "OpenTelemetry Collector",
      "start_page": 210,
      "end_page": 238,
      "detection_method": "regex_chapter",
      "content": "The purpose of OpenTelemetry Collector\n\nIn essence, OpenTelemetry Collector is a process that receives telemetry in various formats,\n\nprocesses it, and then exports it to one or more destinations. The collector acts as a broker between\n\nthe source of the telemetry, applications, or nodes, for example, and the backend that will ultimately\n\nstore the data for analysis. The following diagram shows where the collector would be deployed in an\n\nenvironment containing various components:\n\nFigure 8.1 – Architecture diagram of an environment with a collector Deploying a component such as OpenTelemetry\n\nCollector is not free as it requires additional resources to be spent on running, operating, and monitoring it. The\n\nfollowing are some reasons why deploying a collector may be helpful:\n\nYou can decouple the source of the telemetry data from its destination. This means that developers can configure a single\n\ndestination for the telemetry data in application code and allow the operators of the collector to determine where that data will go\n\nas needed, without having to modify the existing code.\n\nYou can provide a single destination for many data types. The collector can be configured to receive traces, metrics, and logs in\n\nmany different formats, such as OTLP Jaeger, Zipkin, Prometheus, StatsD, and many more.\n\nYou can reduce latency when sending data to a backend. This mitigates unexpected side effects from occurring when an event\n\ncauses a backend to be unresponsive. A collector deployment can also be horizontally scaled to increase capacity as required.\n\nYou can modify telemetry data to address compliance and security concerns. Data can be filtered by the collector via processors\n\nbased on the criteria defined in the configuration. Doing so can stop data leakage and prevent information that shouldn't be\n\nincluded in the telemetry data from ever being stored in a backend.\n\nWe will discuss deployment scenarios for the collector in Chapter 9, Deploying the Collector. For\n\nnow, let's focus on the architecture and components that provide the functionality of the collector.\n\nUnderstanding the components of OpenTelemetry Collector\n\nThe collector allows users to configure pipelines for each signal separately by combining any number\n\nof receivers, processors, and exporters as shown in the following diagram. This gives the collector a\n\nlot of flexibility in how and where it can be used:\n\nFigure 8.2 – Dataflow through the collector\n\nThe initial implementation of the collector was a fork of the OpenCensus Service\n\n(https://opencensus.io/service/), which served a similar purpose in the OpenCensus ecosystem. The\n\ncollector supports many open protocols out of the box for inputs and outputs, which we'll explore in\n\nmore detail as we take a closer look at each component. Each component in the collector implements\n\nthe Component interface, which is fairly minimal, as shown in the following code: type Component\n\ninterface {\n\nStart(ctx context.Context, host Host) error\n\nShutdown(ctx context.Context) error\n\n}\n\nThis interface makes it easy for implementors to add additional components to the collector, making\n\nit very extensible. Let's look at each component in more detail.\n\nReceivers\n\nThe first component in a pipeline is the receiver, a component that receives data in various supported\n\nformats and converts this data into an internal data format within the collector. Typically, a receiver\n\nregisters a listener that exposes a port in the collector for the protocols it supports. For example, the\n\nJaeger receiver supports the following protocols:\n\nThrift Binary on port 6832\n\nThrift Compact on port 6831\n\nThrift HTTP on port 14268\n\ngRPC on port 14250\n\nIMPORTANT NOTE\n\nDefault port values can be overridden via configuration, as we'll see later in this chapter.\n\nIt's possible to enable multiple protocols for the same receiver so that each of the protocols listed\n\npreviously will listen on different ports by default. The following table shows the supported receiver\n\nformats for each signal type:\n\nFigure 8.3 – Receiver formats per signal\n\nNote that all the receivers shown here are receivers that support data in a specific format. However,\n\nan exception is the host metrics receiver, which will be discussed later in this chapter. Receivers can\n\nbe reused across multiple pipelines and it's possible to configure multiple receivers for the same\n\npipeline. The following configuration example enables the OTLP gRPC receiver and the Jaeger Thrift\n\nBinary receiver. Then, it configures three separate pipelines named traces/otlp, traces/jaeger, and\n\ntraces/both, which use those receivers: receivers:\n\notlp:\n\nprotocols:\n\ngrpc:\n\njaeger:\n\nprotocols:\n\nthrift_binary:\n\nservice:\n\npipelines:\n\ntraces/otlp:\n\nreceivers: [otlp]\n\ntraces/jaeger:\n\nreceivers: [jaeger]\n\ntraces/both:\n\nreceivers: [otlp, jaeger]\n\nOne scenario where it would be beneficial to create separate pipelines for different receivers is if\n\nadditional processing needs to occur on the data from one pipeline but not the other. As with the\n\ncomponent interface, the interface for receivers is kept minimal, as shown in the following code. The\n\nTracesReceiver, MetricsReceiver, and LogsReceiver receivers all embed the same Receiver\n\ninterface, which embeds the Component interface we saw previously: type Receiver interface {\n\nComponent\n\n}\n\ntype TracesReceiver interface {\n\nReceiver\n\n}\n\nThe simplicity of the interface makes it easy to implement additional receivers as needed. As we\n\nmentioned previously, the main task of a receiver is to translate data that's being received into various\n\nformats, but what about the host metrics receiver? Host metrics receiver\n\nThe host metrics receiver can be configured to collect metrics about the host running the collector. It\n\ncan be configured to scrape metrics for the CPU, disk, memory, and various other system-level\n\ndetails. The following example shows how the hostmetrics receiver can be configured to scrape\n\nload, memory, and network information from a host every 10 seconds (10s): receivers:\n\nhostmetrics:\n\ncollection_interval: 10s\n\nscrapers:\n\nload:\n\nmemory:\n\nnetwork:\n\nservice:\n\npipelines:\n\nmetrics:\n\nreceivers: [hostmetrics]\n\nThe receiver supports additional configuration so that you can include or exclude specific devices or\n\nmetrics. Configuring this receiver can help you monitor the performance of the host without running\n\nadditional processes to do so. Once the telemetry data has been received through a receiver, it can be\n\nprocessed further via processors.\n\nProcessors\n\nIt can be beneficial to perform some additional tasks, such as filtering unwanted telemetry or\n\ninjecting additional attributes, on the data before passing it to the exporter. This is the job of the\n\nprocessor. Unlike receivers and exporters, the capabilities of processors vary significantly from one\n\nprocessor to another. It's also worth noting that the order of the components in the configuration\n\nmatters for processors, as the data is passed serially from one processor to another. In addition to\n\nembedding the component interface, the processor interface also embeds a consumer interface that\n\nmatches the signal that's being processed, as shown in the following code snippet. The purpose of the\n\nconsumer interface is to provide a function that consumes the signal, such as ConsumeMetrics. It also\n\nprovides information about whether the processor will modify the data it processes via the\n\nMutatesData capability: type Capabilities struct {\n\nMutatesData bool\n\n}\n\ntype baseConsumer interface {\n\nCapabilities() Capabilities\n\n}\n\ntype Metrics interface {\n\nbaseConsumer\n\nConsumeMetrics(ctx context.Context, md pdata.Metrics) error }\n\ntype MetricsProcessor interface {\n\nProcessor\n\nconsumer.Metrics\n\n}\n\nThe following example configures an attributes processor called attributes/add-key to insert an\n\nattribute with the example-key key and sets its value to first. The second attributes processor,\n\nattributes/update-key, updates the value of the example-key attribute to the second value. The traces\n\npipeline is then configured to add the attribute and update its value: processors:\n\nattributes/add-key:\n\nactions:\n\nkey: example-key\n\naction: insert\n\nvalue: first\n\nattributes/update-key:\n\nactions:\n\nkey: example-key\n\naction: update\n\nvalue: second\n\nservice:\n\npipelines:\n\ntraces:\n\nprocessors: [attributes/add-key, attributes/update-key]\n\nThe output that's expected from this configuration is that all the spans that are emitted have an\n\nexample-key attribute set to a value of second. Since the order of the processors matters, inverting the\n\nprocessors in the preceding example would set the value to first. The previous example is a bit silly\n\nsince it doesn't make a lot of sense to configure multiple attributes processors in that manner, but it\n\nillustrates that ordering the processors matters. Let's see what a more realistic example may look like.\n\nThe following configuration copies a value from one attribute with the old-key key into another one\n\nwith the new-key key before deleting the old-key attribute: processors:\n\nattributes/copy-and-delete:\n\nactions:\n\nkey: new-key\n\naction: upsert\n\nfrom_attribute: old-key\n\nkey: old-key\n\naction: delete\n\nservice:\n\npipelines:\n\ntraces:\n\nprocessors: [attributes/copy-and-delete]\n\nA configuration like the previous one could be used to migrate values or consolidate data coming in\n\nfrom multiple systems, where different names are used to represent the same data. As we mentioned\n\nearlier, processors cover a range of functionality. The following table lists the current processors, as\n\nwell as the signals they process:\n\nFigure 8.4 – Processors per signal\n\nSome of these processors will be familiar to you if you've already used an OpenTelemetry SDK. It's\n\nworth taking a moment to explore these processors further.\n\nAttributes processor\n\nAs we discussed earlier, the attributes processor can be used to modify telemetry data attributes. It\n\nsupports the following operations:\n\ndelete: This deletes an attribute for a specified key.\n\nextract: This uses a regular expression to extract values from the specified attribute and upsert new attributes resulting from\n\nthat extraction.\n\nhash: This computes a SHA-1 hash of the value for an existing attribute and updates the value of that attribute to the computed\n\nhash.\n\ninsert: This inserts an attribute for a specified key when it does not exist. It does nothing if the attribute exists.\n\nupdate: This updates an existing attribute with a specified value. It does nothing if the attribute does not exist.\n\nupsert: This combines the functionality of insert and update. If an attribute does not exist, it will insert it with the\n\nspecified value; otherwise, it will update the attribute with the value.\n\nThe attributes processor, along with the span processor, which we'll see shortly, allows you to include\n\nor exclude spans based on match_type, which can either be an exact match configured as strict or a\n\nregular expression configured with regexp. The matching is applied to one or more of the configured\n\nfields: services, span_names, or attributes. The following example includes spans for the super-\n\nsecret and secret services: processors:\n\nattributes/include-secret:\n\ninclude:\n\nmatch_type: strict\n\nservices: [\"super-secret\", \"secret\"]\n\nactions:\n\nkey: secret-attr\n\naction: delete\n\nThe attributes processor can be quite useful when you're scrubbing personally identifiable\n\ninformation (PII) or other sensitive information. A common way sensitive information makes its\n\nway into telemetry data is via debug logs that capture private variables it shouldn't have, or by user\n\ninformation, passwords, or private keys being recorded in metadata. Data leaks often happen\n\naccidentally and are much more frequent than you'd think.\n\nIMPORTANT NOTE\n\nIt's possible to configure both an include and exclude rule at the same time. If that is the case, include is checked\n\nbefore exclude.\n\nFilter processor\n\nThe filter processor allows you to include or exclude telemetry data based on the configured criteria.\n\nThis processor, like the attributes and span processors, can be configured to match names with either\n\nstrict or regexp matching. It's also possible to use an expression that matches attributes as well as\n\nnames. Further scoping on the filter can be achieved by specifying resource_attributes. In terms of\n\nits implementation, at the time of writing, the filter processor only supports filtering for metrics,\n\nthough additional signal support has been requested by the community.\n\nProbabilistic sampling processor\n\nAlthough sampling is a topic that we'll cover in more detail in Chapter 12, Sampling, it's important to\n\nknow that the collector provides a sampling processor for traces known as the probabilistic\n\nsampling processor. It can be used to reduce the number of traces that are exported from the\n\ncollector by specifying a sampling percentage, which determines what percentage of traces should be\n\nkept. The hash_seed parameter is used to determine how the collector should hash the trace IDs to\n\ndetermine which traces to process: processors:\n\nprobabilistic_sampler:\n\nsampling_percentage: 20\n\nhash_seed: 12345\n\nThe hash_seed configuration parameter becomes especially important when multiple collectors are\n\nconnected. For example, imagine that a collector (A) has been configured to send its data to another\n\ncollector (B) before sending the data to a backend. With both A and B configured using the previous\n\nexample, if 100 traces are sent through the two collectors, a total of 20 of those will be sent through\n\nto the backend. If, on the other hand, the two collectors use a different hash_seed, collector A will\n\nsend 20 traces to collector B, and collector B will sample 20% of those, resulting in four traces being\n\nsent to the backend. Either case is valid, but it's important to understand the difference.\n\nIMPORTANT NOTE\n\nThe probabilistic sampling processor prioritizes the sampling priority attribute before the trace ID hashing if the attribute is\n\npresent. This attribute is defined in the semantic conventions and was originally defined in OpenTracing. More information\n\non this will be provided in Chapter 12, Sampling, but for now, it's just good to be aware of it. Resource processor\n\nThe resource processor lets users modify attributes, just like the attributes processor. However,\n\ninstead of updating attributes on individual spans, metrics, or logs, the resource processor updates the\n\nattributes of the resource associated with the telemetry data. The options that are available for\n\nconfiguring the resource processor are the same as for the attributes processor. This can be seen in the\n\nfollowing example, which uses upsert for the deployment.environment attribute and renames the\n\nruntime attribute to container.runtime using the insert and delete actions: processors:\n\nresource:\n\nattributes:\n\nkey: deployment.environment\n\nvalue: staging\n\naction: upsert\n\nkey: container.runtime\n\nfrom_attribute: runtime\n\naction: insert\n\nkey: runtime\n\naction: delete\n\nNow, let's discuss the span processor. Span processor\n\nIt may be useful to manipulate the names of spans or attributes of spans based on their names. This is\n\nthe job of the span processor. It can extract attributes from a span and update its name based on\n\nthose attributes. Alternatively, it can take the span's name and expand it to individual attributes\n\nassociated with the span. The following example shows how to rename a span based on the\n\nmessaging.system and messaging.operation attributes, which will be separated by the : character.\n\nThe second configuration of the span processor shows how to extract the storeId and orderId\n\nattributes from the span's name: processors:\n\nspan/rename:\n\nname:\n\nfrom_attributes: [\"messaging.system\", \"messaging.operation\"]\n\nseparator: \":\"\n\nspan/create-attributes:\n\nname:\n\nto_attributes:\n\nrules:\n\n^\\/stores\\/(?P<storeId>.*)\\/.*$\n\n^.*\\/orders/(?P<orderId>.*)\\/.*$\n\nAs we mentioned previously, the span processor also supports the include and exclude\n\nconfigurations to help you filter spans. Not all processors are used to modify the telemetry data; some\n\nchange the behavior of the collector itself.\n\nBatch processor\n\nThe batch processor helps you batch data to increase the efficiency of transmitting the data. It can be\n\nconfigured both to send batches based on batch size and a schedule. The following code configures a\n\nbatch processor to send data every 10s or every 10000 records and limits the size of the batch to 11000\n\nrecords: processors:\n\nbatch:\n\ntimeout: 10s # default 200ms\n\nsend_batch_size: 10000 # default 8192\n\nsend_batch_max_size: 11000 # default 0 – no limit\n\nIt is recommended to configure a batch processor for all the pipelines to optimize the throughput of\n\nthe collector.\n\nMemory limiter processor\n\nTo ensure the collector is conscious of resource consumption, the memory limiter processor lets\n\nusers control the amount of memory the collector consumes. This helps ensure the collector does as\n\nmuch as it can to avoid running out of memory. Limits can be specified either via fixed mebibyte\n\nvalues or percentages that are calculated based on the total available memory. If both are specified,\n\nthe fixed values take precedence. The memory limiter enforces both soft and hard limits, with the\n\ndifference defined by a spike limit configuration. It is recommended to use the ballast extension\n\nalongside the memory limiter. The ballast extension allows the collector to pre-allocate memory to\n\nimprove the stability of the heap. The recommended size for the ballast is between one-third to one-\n\nhalf of the total memory for the collector. The following code configures the memory limiter to use\n\nup to 250 Mib of the memory configured via limit_mib, with 50 Mib as the difference between the\n\nsoft and hard limits, which is configured via spike_limit_mib: processors:\n\nmemory_limiter:\n\ncheck_interval: 5s\n\nlimit_mib: 250\n\nspike_limit_mib: 50\n\nextensions:\n\nmemory_ballast:\n\nsize_mib: 125\n\nThe memory limiter processor, along with the batch processor, are both recommended if you wish to\n\noptimize the performance of the collector.\n\nIMPORTANT NOTE\n\nWhen the processor exceeds soft limits, it returns errors and starts dropping data. If it exceeds hard limits, it will also force\n\ngarbage collection to free memory.\n\nThe memory limiter should be the first processor you configure in the pipeline. This ensures that\n\nwhen the memory threshold is exceeded, the errors that are returned are propagated to the receivers.\n\nThis allows the receivers to send appropriate error codes back to the client, who can then throttle the\n\nrequests they are sending. Now that we understand how to process our telemetry data to fit our needs,\n\nlet's learn how to use the collector to export all this data.\n\nExporters\n\nThe last component of the pipeline is the exporter. The role of the exporter in the collector pipeline is\n\nfairly similar to its role in the SDK, as we explored in previous chapters. The exporter takes the data\n\nin its internal collector format, marshals it into the output format, and sends it to one or more\n\nconfigured destinations. The interface for the exporter is very similar to the processor interface as it is\n\nalso a consumer, separated again by a signal. The following code shows us the LogsExporter\n\ninterface, which embeds the interfaces we explored earlier: type LogsExporter interface {\n\nExporter\n\nconsumer.Logs\n\n}\n\nMultiple exporters of the same type can be configured for different destinations as necessary. It's also\n\npossible to configure multiple exporters for the same pipeline to output the data to multiple locations.\n\nThe following code configures a jaeger exporter, which is used for exporting traces, and an otlp\n\nexporter, which will be used for both traces and metrics: exporters:\n\njaeger:\n\nendpoint: jaeger:14250\n\notlp:\n\nendpoint: otelcol:4317\n\nservice:\n\npipelines:\n\ntraces:\n\nexporters: [jaeger, otlp]\n\nmetrics:\n\nexporters: [otlp]\n\nSeveral other formats are supported by exporters. The following table lists the available exporters, as\n\nwell as the signals that each supports:\n\nFigure 8.5 – Exporters per signal\n\nNote that in addition to exporting data across different signals to destinations that can be reached over\n\na network, it's also possible to export telemetry data locally to the console via the logs exporter or as\n\nJSON to a file via the file exporter. Receivers, processors, and exporters cover the components in the\n\npipeline, but there is yet more to cover about the collector.\n\nExtensions\n\nAlthough most of the functionality of the collector revolves around the telemetry pipelines, there is\n\nadditional functionality that is made available via extensions. Extensions provide you with another\n\nway to extend the collector. The following extensions are currently available:\n\nballast: This allows users to configure a memory ballast for the collector to improve the overall stability and performance of\n\nthe collector.\n\nhealth_check: This makes an endpoint available for checking the health of the collector. This can be useful for service\n\ndiscovery or orchestration of the collector.\n\npprof: This enables the Go performance profiler, which can be used to identify performance issues within the collector.\n\nzpages: This enables an endpoint in the collector that provides debugging information about the components in the collector.\n\nThus far, all the components we've explored are part of the core collector distribution and are built\n\ninto the binary we'll be using in our examples later in this chapter. However, those are far from the\n\nonly components that are available.\n\nAdditional components\n\nAs you can imagine, providing this much functionality in an application can become quite complex.\n\nTo reduce the complexity of the collector's core functionality without impeding progress and\n\nenthusiasm in the community, the main collector repository contains components that are defined as\n\npart of the OpenTelemetry specification. With all the flexibility the collector provides, many\n\nindividuals and organizations are contributing additional receivers, processors, and exporters. These\n\ncan be found in the opentelemetry-collector-contrib repository at https://github.com/open-\n\ntelemetry/opentelemetry-collector-contrib. As the code in this repository is changing rapidly, we\n\nwon't be going over the components available there, but I strongly suggest browsing through the\n\nrepository to get an idea of what is available.\n\nBefore learning how to use the collector and configuring an application to send data to it, it's\n\nimportant to understand a little bit more about the preferred protocol to receive and export data via\n\nthe collector. This is known as OTLP.\n\nTransporting telemetry via OTLP\n\nWe've mentioned OTLP multiple times in this chapter and this book, so let's look at what it is. To\n\nensure that telemetry data is transmitted as efficiently and reliably as possible, OpenTelemetry has\n\ndefined OTLP. The protocol itself is defined via protocol buffer\n\n(https://developers.google.com/protocol-buffers) definition files. This means that any client or server\n\nthat's interested in sending or receiving OTLP only has to implement these definitions to support it.\n\nOTLP is the recommended protocol of OpenTelemetry for transmitting telemetry data and is\n\nsupported as a core component of the collector.\n\nIMPORTANT NOTE\n\nProtocol buffers or protobufs are a language and platform-agnostic mechanism for serializing data that was originally\n\nintended for gRPC. Libraries are provided to generate the code from the protobuf definition files in a variety of languages.\n\nThis is a much deeper topic than we will have time for in this book, so if you're interested in reading the protocol files, I\n\nstrongly recommended learning more about protocol buffers – they're pretty cool! The Google developer site that was\n\nlinked previously is a great resource to get started.\n\nThe definition for OTLP (https://github.com/open-telemetry/opentelemetry-proto) is divided into\n\nmultiple sections to cover the different signals. Each component of the protocol provides backward\n\ncompatibility guaranteed via its maturity level, which allows adopters to get a sense of how often\n\nthey should expect breaking changes. An alpha level makes no guarantees around breaking changes\n\nwhile a stable level guarantees backward-incompatible changes will be introduced no more\n\nfrequently than every 12 months. The maturity level of each component is available in the project's\n\nREADME.md file and the current state, at the time of writing, can be seen in the following table. It's very\n\nlikely to change by the time you're reading this as progress is being made quite rapidly!\n\nFigure 8.6 – Maturity level of OTLP components\n\nTaking a closer look at the preceding table (https://github.com/open-telemetry/opentelemetry-\n\nproto#maturity-level), note that it includes a different section for protobuf and JSON encoding. Let's\n\ntalk about why that is.\n\nEncodings and protocols\n\nThe specification defines the encodings and protocols that are supported by OTLP. Initially, the\n\nfollowing three combinations are supported:\n\nprotobufs over gRPC\n\nprotobufs over HTTP\n\nJSON over HTTP\n\nDepending on the requirements of your application or the infrastructure that will be used to deploy\n\nyour code, certain restrictions may guide the decision of which encoding or protocol to choose. For\n\nexample, users may be deploying applications in an environment that doesn't support gRPC. This was\n\ntrue for a long time with serverless Python environments in various cloud providers. Similarly, gRPC\n\nwas not supported in the browser, meaning users of OpenTelemetry for JavaScript cannot use gRPC\n\nwhen instrumenting a browser application. Another tradeoff that may cause users to choose one\n\npackage over another is the impact of serializing and deserializing data using JSON, which can have\n\nsome serious performance implications in certain languages compared to using protobufs. The\n\ndifferent combinations of encodings and protocols exist to provide additional flexibility for users,\n\ndepending on the requirements of their environments.\n\nOne of the requirements for any OpenTelemetry language implementation is to support at least one of\n\nthese formats before marking a signal as generally available. This ensures that users can use OTLP to\n\nexport data across their entire system, from application instrumentation to the backend.\n\nAdditional design considerations\n\nBackpressure can happen when clients are generating telemetry data faster than the recipients can\n\nreceive it. To address this, the specification for OTLP also defines how clients should handle\n\nresponses from servers to manage backpressure when receiving systems become overloaded. Another\n\ndesign goal of the protocol is to ensure it is load balancer-friendly so that you can horizontally scale\n\nvarious components that could be involved in handling telemetry data using OTLP. Equipped with\n\nthis knowledge of the protocol, let's start sending data to the collector.\n\nUsing OpenTelemetry Collector\n\nNow that we're familiar with the core components of OpenTelemetry Collector and OTLP, it's time to\n\nstart using the collector with the grocery store. The following diagram gives us an idea of how\n\ntelemetry data is currently configured and where we are trying to go with this chapter:\n\nFigure 8.7 – Before and after diagrams of exporting telemetry data At the beginning of this chapter, we installed the\n\nOTLP exporters for Python via the opentelemetry-exporter-otlp package. This, in turn, installed the packages\n\nthat are available for each protocol and encoding:\n\nopentelemetry-exporter-otlp-proto-grpc\n\nopentelemetry-exporter-otlp-proto-http\n\nopentelemetry-exporter-otlp-json-http\n\nThe package that includes all the protocols and the encoding is a convenient way to start, but once\n\nyou're familiar with the requirements for your environment, you'll want to choose a specific encoding\n\nand protocol to reduce dependencies.\n\nConfiguring the exporter\n\nThe following examples will leverage the otlp-proto-grpc package, which includes the exporter\n\nclasses we'll use to export telemetry – OTLPSpanExporter, OTLPMetricExporter, and OTLPLogExporter.\n\nThe code builds on the example applications from Chapter 6, Logging —Capturing Events, by\n\nupdating the common.py module to use the OTLP exporters instead of the control exporters, which\n\nwe've used so far: common.py\n\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n\nfrom opentelemetry.exporter.otlp.proto.grpc._metric_exporter import OTLPMetricExporter\n\nfrom opentelemetry.exporter.otlp.proto.grpc._log_exporter import OTLPLogExporter\n\ndef configure_tracer(name, version):\n\n...\n\nexporter = OTLPSpanExporter()\n\n...\n\ndef configure_meter(name, version):\n\n...\n\nexporter = OTLPMetricExporter()\n\n...\n\ndef configure_logger(name, version):\n\n...\n\nexporter = OTLPLogExporter()\n\n...\n\nBy default, as per the specification, the exporters will be configured to send data to a collector\n\nrunning on localhost:4317.\n\nConfiguring the collector\n\nThe following collector configuration sets up the otlp receiver, which will be used to receive\n\ntelemetry data from our application. Additionally, it configures the logging exporter to output useful\n\ninformation to the console: config/collector/config.yml receivers:\n\notlp:\n\nprotocols:\n\ngrpc:\n\nexporters:\n\nlogging:\n\nservice:\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nexporters: [logging]\n\nmetrics:\n\nreceivers: [otlp]\n\nexporters: [logging]\n\nlogs:\n\nreceivers: [otlp]\n\nexporters: [logging]\n\nIMPORTANT NOTE\n\nIn the following examples, each time config.yml is updated, the collector must be restarted for the changes to take\n\neffect.\n\nIt's time to see whether the collector and the application can communicate. First, start the collector\n\nusing the following command from the terminal: ./otelcol --config ./config/collector/config.yml\n\nIf everything is going according to plan, the process should be up and running, and the output from it\n\nshould list the components that have been loaded. It should also contain a message similar to the\n\nfollowing: collector output\n\n2021-05-30T16:19:03.088-0700 info service/application.go:197 Everything is ready. Begin\n\nrunning and processing data.\n\nNext, we need to run the application code in a separate terminal. First, launch the legacy inventory,\n\nfollowed by the grocery store, and then the shopper application. Note that legacy_inventory.py and\n\ngrocery_store.py will remain running for the remainder of this chapter as we will not make any\n\nfurther changes to them: python legacy_inventory.py\n\npython grocery_store.py\n\npython shopper.py\n\nPay close attention to the output from the terminal running the collector. You should see some output\n\ndescribing the traces, metrics, and logs that have been processed by the collector. The following code\n\ngives you an idea of what to look for: collector output\n\n2022-02-13T14:35:47.101-0800 INFO loggingexporter/logging_exporter.go:69 LogsExporter\n\n{\"#logs\": 1}\n\n2022-02-13T14:35:47.110-0800 INFO loggingexporter/logging_exporter.go:40 TracesExporter\n\n{\"#spans\": 4}\n\n2022-02-13T14:35:49.858-0800 INFO loggingexporter/logging_exporter.go:40 TracesExporter\n\n{\"#spans\": 1}\n\n2022-02-13T14:35:50.533-0800 INFO loggingexporter/logging_exporter.go:40 TracesExporter\n\n{\"#spans\": 3}\n\n2022-02-13T14:35:50.535-0800 INFO loggingexporter/logging_exporter.go:69 LogsExporter\n\n{\"#logs\": 2}\n\nExcellent – let's do some more fun things with the collector by adding some processors to our\n\nconfiguration! If you look closely at the preceding output, you'll notice that TracesExporter is\n\nmentioned in three separate instances. Since each of our applications is sending telemetry data, the\n\nexporter is called with the new data. The batch processor can improve it's efficiency here by waiting a\n\nwhile and sending a single batch containing all the telemetry data simultaneously. The following code\n\nconfigures the batch processor with a timeout of 10 seconds (10s), so the processor will wait up until\n\nthat time to send a batch. Then, we can add this processor to each pipeline:\n\nconfig/collector/config.yml processors:\n\nbatch:\n\ntimeout: 10s\n\n...\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nprocessors: [batch]\n\nexporters: [logging]\n\nmetrics:\n\nreceivers: [otlp]\n\nprocessors: [batch]\n\nexporters: [logging]\n\nlogs:\n\nreceivers: [otlp]\n\nprocessors: [batch]\n\nexporters: [logging]\n\nTry running the shopper application once again. This time, the output from the collector should show\n\na single line including the sum of all the spans we saw earlier: collector output\n\n2022-02-13T14:40:07.360-0800 INFO loggingexporter/logging_exporter.go:69 LogsExporter\n\n{\"#logs\": 2}\n\n2022-02-13T14:40:07.360-0800 INFO loggingexporter/logging_exporter.go:40 TracesExporter\n\n{\"#spans\": 8}\n\nIf you run the shopper application a few times, you'll notice a 10-second delay in the collector\n\noutputting information about the telemetry data that's been generated. This is the batch processor at\n\nwork. Let's make the logging output slightly more useful by updating the logging exporter\n\nconfiguration: config/collector/config.yml exporters:\n\nlogging:\n\nloglevel: debug\n\nRestarting the collector and running the shopper application again will output the full telemetry data\n\nthat's been received. What should appear is a verbose list of all the telemetry data the collector is\n\nreceiving. Look specifically for the span named add item to cart as we'll be modifying it in the next\n\nfew examples: collector output\n\nSpan #0\n\nTrace ID : 1592a37b7513b73eaefabde700f4ae9b\n\nParent ID : 2411c263df768eb5\n\nID : 8e6f5cdb56d6448d\n\nName : HTTP GET\n\nKind : SPAN_KIND_SERVER\n\nStart time : 2022-02-13 22:41:42.673298 +0000 UTC\n\nEnd time : 2022-02-13 22:41:42.677336 +0000 UTC\n\nStatus code : STATUS_CODE_UNSET\n\nStatus message :\n\nAttributes:\n\n> http.method: STRING(GET)\n\n> http.server_name: STRING(127.0.0.1)\n\n> http.scheme: STRING(http)\n\n> net.host.port: INT(5000)\n\n> http.host: STRING(localhost:5000)\n\n> http.target: STRING(/products)\n\n> net.peer.ip: STRING(127.0.0.1)\n\nSo far, our telemetry data is being emitted to a collector from three different applications. Now, we\n\ncan see all the telemetry data on the terminal running the collector. Let's take this a step further and\n\nmodify this telemetry data via some processors.\n\nModifying spans\n\nOne of the great features of the collector is its ability to operate on telemetry data from a central\n\nlocation. The following example demonstrates some of the power behind the processors. The\n\nfollowing configuration uses two different processors to augment the span we mentioned previously.\n\nFirst, the attributes processor will add an attribute to identify a location attribute. Next, the span\n\nprocessor will use the attributes from the span to rename the span so that it includes the location,\n\nitem, and quantity attributes. The new processors must also be added to the traces pipeline's\n\nprocessors array: config/collector/config.yml processors:\n\nattributes/add-location:\n\nactions:\n\nkey: location\n\naction: insert\n\nvalue: europe\n\nspan/rename:\n\nname:\n\nfrom_attributes: [location, item, quantity]\n\nseparator: \":\"\n\n...\n\npipelines:\n\ntraces:\n\nprocessors: [batch, attributes/add-location, span/rename]\n\nIMPORTANT NOTE\n\nRemember that the order of the processors matters. In this case, the reverse order wouldn't work as the location attribute\n\nwould not be populated.\n\nRun the shopper and look at the output from the collector to see the effect of the new processors. The\n\nnew exported span contains a location attribute with the europe value, which we configured. Its\n\nname has also been updated to location:item:quantity: collector output\n\nSpan #1\n\nTrace ID : 47dac26efa8de0ca1e202b6d64fd319c\n\nParent ID : ee10984575037d4a\n\nID : a4f42124645c4d3b\n\nName : europe:orange:5\n\nKind : SPAN_KIND_INTERNAL\n\nStart time : 2022-02-13 22:44:57.072143 +0000 UTC\n\nEnd time : 2022-02-13 22:44:57.07751 +0000 UTC Status code : STATUS_CODE_UNSET\n\nStatus message :\n\nAttributes:\n\n> item: STRING(orange)\n\n> quantity: INT(5)\n\n> location: STRING(europe)\n\nThis isn't bad for 10 lines of configuration! The final example will explore the hostmetrics receiver\n\nand how to configure the filter processor for metrics.\n\nFiltering metrics\n\nSo far, we've looked at how to modify spans, but what about metrics? As we discussed previously, the\n\nhostmetrics receiver captures metrics about the localhost. Let's see it in action. The following\n\nexample configures the host metrics receiver to scrape memory and network information every 10\n\nseconds: config/collector/config.yml receivers:\n\nhostmetrics:\n\ncollection_intervals: 10s\n\nscrapers:\n\nmemory:\n\nnetwork:\n\n...\n\nservice:\n\npipelines:\n\nmetrics:\n\nreceivers: [otlp, hostmetrics]\n\nAfter configuring this receiver, just restart the collector – you should see metrics in the collector\n\noutput, without running shopper.py. The output will include memory and network metrics: collector\n\noutput\n\nInstrumentationLibraryMetrics #0\n\nInstrumentationLibrary\n\nMetric #0\n\nDescriptor:\n\n> Name: system.memory.usage\n\n> Description: Bytes of memory in use.\n\n> Unit: By\n\n> DataType: IntSum\n\n> IsMonotonic: false\n\n> AggregationTemporality: AGGREGATION_TEMPORALITY_CUMULATIVE\n\nIntDataPoints #0\n\nData point labels:\n\n> state: used\n\nStartTimestamp: 1970-01-01 00:00:00 +0000 UTC\n\nTimestamp: 2022-02-13 22:48:16.999087 +0000 UTC\n\nValue: 10880851968\n\nMetric #1\n\nDescriptor:\n\n> Name: system.network.packets\n\n> Description: The number of packets transferred.\n\n> Unit: {packets}\n\n> DataType: IntSum\n\n> IsMonotonic: true\n\n> AggregationTemporality: AGGREGATION_TEMPORALITY_CUMULATIVE\n\nIntDataPoints #0\n\nData point labels:\n\n> device: lo0\n\n> direction: transmit\n\nStartTimestamp: 1970-01-01 00:00:00 +0000 UTC\n\nTimestamp: 2022-02-13 22:48:16.999087 +0000 UTC\n\nValue: 120456\n\nWell done – the collector is now generating metrics for you! Depending on the type of system you're\n\nrunning the collector on, you may have many network interfaces available that are generating a lot of\n\nmetrics. Let's update the configuration to scrape metrics for a single interface to reduce some of the\n\nnoise. On my host, I will use lo0 as the interface: config/collector/config.yml receivers:\n\nhostmetrics:\n\ncollection_intervals: 10s\n\nscrapers:\n\nmemory:\n\nnetwork:\n\ninclude:\n\nmatch_type: strict\n\ninterfaces: [lo0]\n\nIMPORTANT NOTE\n\nNetwork interface names vary based on the operating system being used. Some common interface names are lo0, eth0,\n\nen0, and wlan0. If you're unsure, look for the device label in the previous output, which should show you some of the\n\ninterfaces that are available on your system.\n\nThe output will be significantly reduced, but there are still many network metrics to sift through.\n\nsystem.network.connections is quite noisy as it collects data points for each tcp state. Let's take this\n\none step further and use the filter processor to exclude system.network.connections:\n\nconfig/collector/config.yml processors:\n\nfilter/network-connections:\n\nmetrics:\n\nexclude:\n\nmatch_type: strict\n\nmetric_names:\n\nsystem.network.connections\n\n...\n\npipelines:\n\nmetrics:\n\nreceivers: [hostmetrics]\n\nprocessors: [batch, filter/network-connections]\n\nRestarting the collector one last time will yield a much easier-to-read output. Of course, there are\n\nmany more scenarios to experiment with when it comes to the collector and its components, but this\n\ngives you a good idea of how to get started. I recommend spending some time experimenting with\n\ndifferent configurations and processors to get comfortable with it. And with that, we now have an\n\nunderstanding of one of the most critical components of OpenTelemetry – the collector.\n\nSummary\n\nIn this chapter, you learned about the fundamentals of OpenTelemetry Collector and its components.\n\nYou now know what role receivers, processors, exporters, and extensions play in the collector and\n\nknow about the specifics of individual processors.\n\nAdditionally, we looked at the definition of the OTLP, its benefits, and the design decisions behind\n\ncreating the protocol. Equipped with this knowledge, we configured OpenTelemetry Collector for the\n\nfirst time and updated the grocery store to emit data to it. Using a variety of processors, we\n\nmanipulated the data the collector was receiving to get a working understanding of how to harness\n\nthe power of the collector.\n\nThe next chapter will expand on this knowledge and take the collector from a component that's used\n\nin development to a core component of your infrastructure. We'll explore how to deploy the collector\n\nin a variety of scenarios to make the most of it.\n\nChapter 9: Deploying the Collector\n\nNow that we've learned about the ins and outs of the collector, it's time to look at how we can use it\n\nin production. This chapter will explain how the flexibility of the collector can help us to deploy it in\n\na variety of scenarios. Using Docker, Kubernetes, and Helm, we will learn how to use the\n\nOpenTelemetry collector in combination with the grocery store application from earlier chapters.\n\nThis will give us the necessary knowledge to start using the collector in our cloud-native\n\nenvironment.\n\nIn this chapter, we will focus on the following main topics:\n\nUsing the collector as a sidecar to collect application telemetry\n\nDeploying the collector as an agent to collect system-level telemetry\n\nConfiguring the collector as a gateway\n\nAlong the way, we'll look at some strategies for scaling the collector. Additionally, we'll spend some\n\nmore time with the processors that we looked at in Chapter 8, OpenTelemetry Collector. Unlike the\n\nprevious chapters, which focused on OpenTelemetry components, this chapter is all about using\n\nthem. As such, it will introduce a number of tools that you might encounter when working with\n\ncloud-native infrastructure.\n\nTechnical requirements\n\nThis chapter will cover a few different tools that we can use to deploy the collector. We will be using\n\ncontainers to run the sample application and collector; all the examples are available from the public\n\nDocker container registry (https://hub.docker.com). Although we won't dive too deeply into what\n\ncontainers are, just know that containers provide a convenient way to build, package, and deploy self-\n\ncontained applications that are immutable. For us to run containers locally, we will use Docker, just\n\nas we did in Chapter 2, OpenTelemetry Signals - Traces, Metrics and Logs. The following is a list of\n\nthe technical requirements for this chapter:\n\nIf you don't already have Docker installed on your machine, follow the instructions available at https://docs.docker.com/get-\n\ndocker/ to get started on Windows, macOS, and Linux. Once you have it installed, run the following command from a Terminal. If\n\neverything is working correctly, there should be no errors reported: $ docker ps\n\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n\nShortly, we will be required to run a command using the kubectl Kubernetes command-line tool. This tool interacts with the\n\nKubernetes API, which we'll do continuously throughout the chapter to access our applications once they're running in the cluster.\n\nDepending on your environment, you might already have a copy of this tool installed. Check whether that is the case by running\n\nthe following command: $ kubectl version\n\nClient Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\"...\n\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.7\"...\n\nIf the output from running the previous command shows command not found, go through the\n\ninstallation steps documented on the Kubernetes website at https://kubernetes.io/docs/tasks/tools/.\n\nIn addition to Docker, we will also use Kubernetes (https://kubernetes.io) throughout this chapter. This is because it is one of the\n\nleading open source tools used in cloud-native infrastructure. Kubernetes will provide the container orchestration for our\n\nexamples and the collector. It's worth noting that Kubernetes is not the only container orchestration solution that is available;\n\nhowever, it is one of the more popular ones. There are many different tools available to set up a local Kubernetes cluster. For\n\ninstance, I'll use kind to set up my cluster, which runs a local cluster inside Docker. If you already have access to a cluster, then\n\ngreat! You're good to go. Otherwise, head over to https://kind.sigs.k8s.io/docs/user/quick-start/ and follow the installation\n\ninstructions for your platform. Once kind is installed, run the following command to start a cluster: $ kind create cluster\n\nCreating cluster \"kind\" ...\n\n✓ Ensuring node image (kindest/node:v1.19.1)\n\n✓ Preparing nodes\n\n✓ Writing configuration\n\n✓ Starting control-plane\n\n✓ Installing CNI\n\n✓ Installing StorageClass\n\nSet kubectl context to \"kind-kind\"\n\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nThe previous command should get the cluster started for you. Getting a cluster up and running is\n\ncrucial to use the examples in the rest of this chapter. If you're running into issues while setting up a\n\nlocal cluster with kind, you might want to investigate one of the following alternatives: A. Minikube:\n\nhttps://minikube.sigs.k8s.io/docs/start/\n\nB. K3s: https://k3s.io\n\nC. Docker Desktop: https://docs.docker.com/desktop/kubernetes/\n\nHow the cluster is run isn't going to be important; having a cluster is what really matters.\n\nAdditionally, if running a local cluster isn't feasible, you might want to look at some hosted options:\n\nA. Google Kubernetes Engine: https://cloud.google.com/kubernetes-engine\n\nB. Amazon Elastic Kubernetes Service: https://aws.amazon.com/eks/\n\nC. Azure Kubernetes Service: https://azure.microsoft.com/en-us/services/kubernetes-service/\n\nYou should know that there are always costs associated with using a hosted Kubernetes cluster.\n\nNow, check the state of the cluster using kubectl, which we installed earlier. Run the following command to check whether the\n\ncluster is ready: $kubectl cluster-info --context kind-kind\n\nKubernetes master is running at https://127.0.0.1:62708\n\nKubeDNS is running at https://127.0.0.1:62708/api/v1/namespaces/kube-\n\nsystem/services/kube-dns:dns/proxy\n\nGood job at getting this far! I know there are a lot of tools to install, but it'll be worth it! The last tool that we'll use throughout this\n\nchapter is Helm. This is a package manager for applications running in Kubernetes. Helm will allow us to install applications in\n\nour cluster by using the YAML configuration it calls charts; these provide the default configuration for many applications that are\n\navailable to deploy in Kubernetes. The instructions for installing Helm are available from the Helm website at\n\nhttps://helm.sh/docs/intro/install/. Once again, to ensure the tool is working and correctly configured in your path, run the\n\nfollowing command: helm version\n\nThe full configuration for all the examples in this chapter is available in the companion repository at\n\nhttps://github.com/PacktPublishing/Cloud-Native-Observability. Please feel free to look in the\n\nchapter9 folder if any of the examples give you trouble. Great! Now that the hard part is done, let's\n\nget to the fun stuff and start deploying OpenTelemetry collectors in our cluster!\n\nCollecting application telemetry\n\nPreviously, we looked at how to use the collector running as a local binary. This can be useful for\n\ndevelopment and testing, but it's not how the collector would be deployed in a production\n\nenvironment. Before going further, here are some Kubernetes concepts that we will be using in this\n\nchapter:\n\nPod: This is a container or a group of containers that form an application.\n\nSidecar: This is a container that is deployed alongside application containers but isn't tightly coupled with the application in the\n\npod.\n\nNode: This is a representation of a Kubernetes worker; it could be a physical host or a virtual machine.\n\nDaemonSet: This is a pod template specification to ensure a pod is deployed to the configured nodes.\n\nIMPORTANT NOTE\n\nThe concepts of Kubernetes form a much deeper topic than we have time for in this book. For our examples, we will\n\nonly cover the bare minimum that is necessary for this chapter. There is a lot more to cover and, thankfully, many",
      "page_number": 210
    },
    {
      "number": 9,
      "title": "Deploying the Collector",
      "start_page": 239,
      "end_page": 254,
      "detection_method": "regex_chapter",
      "content": "resources are available on the internet regarding this vast topic.\n\nFigure 9.1 shows three different deployment scenarios that can be used to deploy the OpenTelemetry\n\ncollector in a production environment, which, in this case, is a Kubernetes cluster:\n\nThe first deployment (1) is alongside the application containers within the same pod. This deployment is commonly referred to as\n\na sidecar deployment.\n\nThe second deployment (2) shows the collector running as a container on the same node as the application pod. This agent\n\ndeployment represents a DaemonSet deployment, which means that the collector container will be present in every node in the\n\nKubernetes cluster.\n\nThe third deployment (3) is shown running the collector as a gateway. In practice, the containers in the collector service will run\n\non Kubernetes nodes, which may or may not be the same as the ones running the application pod.\n\nAdditionally, the following diagram shows the flow for the telemetry data from one collector to\n\nanother, which we will configure in this chapter:\n\nFigure 9.1 – The three deployment options for the collector In this chapter, we will work through each scenario, starting\n\nwith collecting application telemetry. We can do this by deploying the collector as close to the application as possible\n\nwithin the same pod. When emitting telemetry from an application, often, it's useful to offload the data as quickly as\n\npossible to reduce the resource impact on the application. This allows the application to spend most of its time on what\n\nit is meant to do, that is, manage the workloads it was created to manage. To ensure the lowest possible latency while\n\ntransmitting telemetry, let's look at deploying the collector as close as possible to the application, as a sidecar.\n\nDeploying the sidecar\n\nTo reduce that latency and the complexity of collecting telemetry, deploying the collector as a loosely\n\ncoupled container within the same pod as the application makes the most sense. This ensures the\n\nfollowing:\n\nThe application will always have a consistent destination to send its telemetry to since applications within the same pod can\n\ncommunicate with each other via localhost.\n\nThe latency between the application and the collector will not affect the application. This allows the application to offload its\n\ntelemetry as quickly as possible, preventing unexpected memory loss or CPU pressure for high-throughput applications.\n\nLet's look at how this is done. First, consider the following configuration, which includes the\n\nshopper, the grocery store, and the inventory applications. These have been containerized to allow us\n\nto deploy them via Kubernetes. In addition to this, the pod configuration contains a collector\n\ncontainer. The most important thing to note in the configuration for our use case is the containers\n\nsection, which defines the four containers that make up the application via name and image containers.\n\nCreate a YAML file that includes the following configuration: config/collector/sidecar.yml\n\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\nname: cloud-native-example\n\nlabels:\n\napp: example\n\nspec:\n\nreplicas: 1\n\nselector:\n\nmatchLabels:\n\napp: example\n\ntemplate:\n\nmetadata:\n\nlabels:\n\napp: example\n\nspec:\n\ncontainers:\n\nname: legacy-inventory\n\nimage: codeboten/legacy-inventory:chapter9\n\nname: grocery-store\n\nimage: codeboten/grocery-store:chapter9\n\nname: shopper\n\nimage: codeboten/shopper:chapter9\n\nname: collector\n\nimage: otel/opentelemetry-collector:0.43.0\n\nThe default configuration for the collector container configures an OTLP receiver, which you'll\n\nremember from Chapter 8, OpenTelemetry Collector. Additionally, it configures a logging exporter.\n\nWe will modify this configuration later in this chapter; however, for now, the default is good enough.\n\nLet's apply the previous configuration to our cluster by running the following command. This uses\n\nthe configuration to pull the container images from the Docker repository and creates the deployment\n\nand pod running the application: $ kubectl apply -f config/collector/sidecar.yml\n\ndeployment.apps/cloud-native-example created\n\nWe can ensure the pod is up and running with the following command, which gives us details about\n\nthe pod along with the containers that are running within it: $ kubectl describe pod -l app=example\n\nWe should be able to view all the details about the pod we configured:\n\nkubectl describe output Name: cloud-native-example-6bdfd8b6d6-cfhc7\n\nNamespace: default\n\nPriority: 0 ...\n\nWith the pod running, we should now be able to look at the logs of the collector sidecar and observe\n\nthe telemetry flowing. The following command lets us view the logs from any container within the\n\npod. The container can be specified via the -c flag followed by the name of the container in question.\n\nThe -f flag can be used to tail the logs. You can use the same command to observe the output of the\n\nother containers by changing the -c flag to the name of different containers: kubectl logs -l\n\napp=example -f -c collector\n\nThe output of the previous command will contain telemetry from the various applications in the\n\ngrocery store example. It should look similar to the following: kubectl logs output\n\nSpan #6\n\nTrace ID : 2ca9779b6ad6d5b1a067dd83ea0942d4\n\nParent ID : 09b499899194ba83\n\nID : c8a1d75232eaf376\n\nName : inventory request\n\nKind : SPAN_KIND_INTERNAL\n\nStart time : 2021-06-19 22:38:53.3719469 +0000 UTC\n\nEnd time : 2021-06-19 22:38:53.3759594 +0000 UTC\n\nStatus code : STATUS_CODE_UNSET\n\nStatus message :\n\nAttributes:\n\n> http.method: STRING(GET)\n\n> http.flavor: STRING(HttpFlavorValues.HTTP_1_1) -> http.url:\n\nSTRING(http://localhost:5001/inventory) -> net.peer.ip: STRING(127.0.0.1)\n\nNow we have a pod with a collector sidecar collecting telemetry! We will come back to make\n\nchanges to this pod shortly, but first, let's look at the next deployment scenario.\n\nSystem-level telemetry\n\nAs discussed in Chapter 8, OpenTelemetry Collector, the OpenTelemetry collector can be configured\n\nto collect metrics about the system it's running on. Often, this can be helpful when you wish to\n\nidentify resource constraints on nodes, which is a fairly common problem. Additionally, the collector\n\ncan be configured to forward data. So, it might be beneficial to deploy a collector on each host or\n\nnode in your environment to provide an aggregation point for all the applications running on that\n\nnode. As shown in the following diagram, deploying a collector as an agent can reduce the number of\n\nconnections needed to send telemetry from each node:\n\nFigure 9.2 – Backend connections from nodes with and without an agent This can become a significant processing\n\nbottleneck if, for example, the backend requires secure connections to be established with some level of frequency and\n\nif many applications are running per node.\n\nDeploying the agent\n\nThe preferred way to deploy the collector as an agent is by using Helm Charts, which is provided by\n\nthe OpenTelemetry project. You can find this at https://github.com/open-telemetry/opentelemetry-\n\nhelm-charts. The first step to install a Helm chart is to tell Helm where it should look for the chart\n\nusing the following command. This adds the open-telemetry repository to Helm: $ helm repo add\n\nopen-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts \"open-telemetry\" has\n\nbeen added to your repositories Then, we can launch the collector service using the following\n\ncommand. This will install the opentelemetry-collector Helm chart, using all the default options: $\n\nhelm install otel-collector open-telemetry/opentelemetry-collector Let's check to see what happened\n\nin our Kubernetes cluster because of the previous command. The collector chart should have\n\ndeployed the collector using DaemonSet. As mentioned earlier in the chapter, a DaemonSet is a way to\n\ndeploy an instance of a pod on all nodes in Kubernetes. The following command lists all deployed\n\nDaemonSet deployments in our cluster, and you can view the resulting output as follows: $ kubectl get\n\nDaemonSet\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\n\notel-collector-opentelemetry-collector-agent 1 1 1 1 1 <none> 3m25s Note that the results\n\nmight be different depending on how many nodes your cluster has; mine has a single node.\n\nNext, let's examine the pods created using the following command: $ kubectl get Pod\n\nNAME READY STATUS RESTARTS AGE\n\notel-collector-opentelemetry-collector-agent-hhgkk 1/1 Running 0 4m39s With the collector\n\nrunning as an agent on the node, let's learn about how to forward all the data from the\n\ncollector sidecar to the agent.\n\nConnecting the sidecar and the agent\n\nIt's time to update the sidecar collector configuration to use an OTLP exporter to export data. This\n\ncan be accomplished using a ConfigMap, which gives us the ability to have Kubernetes create a file\n\nthat will be mounted as a volume inside the container. For brevity, the details of ConfigMap and the\n\nvolumes in Kubernetes will not be described here. Add the following ConfigMap object to the top of\n\nthe sidecar configuration file: config/collector/sidecar.yml apiVersion: v1\n\nkind: ConfigMap\n\nmetadata:\n\nname: otel-sidecar-conf\n\nlabels:\n\napp: opentelemetry\n\ncomponent: otel-sidecar-conf\n\ndata:\n\notel-sidecar-config: |\n\nreceivers:\n\notlp:\n\nprotocols:\n\ngrpc:\n\nhttp:\n\nexporters:\n\notlp:\n\nendpoint: \"$NODE_IP:4317\"\n\ntls:\n\ninsecure: true\n\nservice:\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nexporters: [otlp]\n\nmetrics:\n\nreceivers: [otlp]\n\nexporters: [otlp]\n\nlogs:\n\nreceivers: [otlp]\n\nexporters: [otlp]\n\nThe preceding configuration might remind you of the collector-specific configuration we explored in\n\nChapter 8, OpenTelemetry Collector. It is worth noting that we will be using the NODE_IP\n\nenvironment variable in the configuration of the endpoint for the OTLP exporter.\n\nFollowing this, we need to update the containers section further down. This is so that we can use the\n\notel-sidecar-conf ConfigMap and tell the collector container to pass the configuration file at start\n\ntime via the command option. The following configuration also exposes the node's IP address as an\n\nenvironment variable named NODE_IP: config/collector/sidecar.yml apiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\nname: cloud-native-example\n\nlabels:\n\napp: example\n\nspec:\n\nreplicas: 1\n\nselector:\n\nmatchLabels:\n\napp: example\n\ntemplate:\n\nmetadata:\n\nlabels:\n\napp: example\n\nspec:\n\ncontainers:\n\nname: legacy-inventory\n\nimage: codeboten/legacy-inventory:latest\n\nname: grocery-store\n\nimage: codeboten/grocery-store:latest\n\nname: shopper\n\nimage: codeboten/shopper:latest\n\nname: collector\n\nimage: otel/opentelemetry-collector:0.27.0\n\ncommand:\n\n\"/otelcol\"\n\n\"--config=/conf/otel-sidecar-config.yaml\"\n\nvolumeMounts:\n\nname: otel-sidecar-config-vol\n\nmountPath: /conf\n\nenv:\n\nname: NODE_IP\n\nvalueFrom:\n\nfieldRef:\n\nfieldPath: status.hostIP\n\nvolumes:\n\nconfigMap:\n\nname: otel-sidecar-conf\n\nitems:\n\nkey: otel-sidecar-config\n\npath: otel-sidecar-config.yaml\n\nname: otel-sidecar-config-vol\n\nFor this new configuration to take effect, we'll go ahead and apply the configuration with the\n\nfollowing command: $ kubectl apply -f config/collector/sidecar.yml\n\nLooking at the logs for the agent, we can now observe that telemetry is being processed by the\n\ncollector: kubectl logs -l component=agent-collector -f\n\n2021-06-26T22:57:50.719Z INFO loggingexporter/logging_exporter.go:327 TracesExporter\n\n{\"#spans\": 20}\n\n2021-06-26T22:57:50.919Z INFO loggingexporter/logging_exporter.go:327 TracesExporter\n\n{\"#spans\": 10}\n\n2021-06-26T22:57:53.726Z INFO loggingexporter/logging_exporter.go:375 MetricsExporter\n\n{\"#metrics\": 22}\n\n2021-06-26T22:57:54.730Z INFO loggingexporter/logging_exporter.go:327 TracesExporter\n\n{\"#spans\": 5}\n\nWhile we're here, we might as well take some time to augment the telemetry processed by the\n\ncollector. We can do this by applying some of the lessons we learned in Chapter 8, OpenTelemetry\n\nCollector. Let's configure a processor to provide more visibility inside our infrastructure.\n\nAdding resource attributes\n\nOne of the great things about the agent collector is being able to ensure information about the node\n\nit's running on is present across all the telemetry processed by the agent. Helm allows us to override\n\nthe default configuration via YAML; the following can be used in conjunction with the Helm chart to\n\nconfigure a resource attributes processor to inject information into our telemetry. It does the\n\nfollowing:\n\nIt makes an environment variable named NODE_NAME available for use by the resource attributes processor.\n\nIt sets the loglevel parameter of the logging exporter to debug. This allows us to observe the data being emitted by the\n\ncollector in more detail.\n\nIt configures a resource attributes processor to inject the NODE_NAME environment variable into an attribute with the\n\nk8s.node.name key. Additionally, it adds the processor to the pipelines for logs, metrics, and traces.\n\nCreate a new config/collector/config.yml configuration file that contains the following\n\nconfiguration. We'll use this to update the Helm chart: config/collector/config.yml extraEnvs:\n\nname: NODE_NAME\n\nvalueFrom:\n\nfieldRef:\n\nfieldPath: spec.nodeName\n\nconfig:\n\nexporters:\n\nlogging:\n\nloglevel: debug\n\nagentCollector:\n\nenabled: true\n\nconfigOverride:\n\nprocessors:\n\nresource:\n\nattributes:\n\nkey: k8s.node.name\n\nvalue: ${NODE_NAME}\n\naction: upsert\n\nservice:\n\npipelines:\n\nmetrics:\n\nprocessors: [batch, memory_limiter, resource]\n\ntraces:\n\nprocessors: [batch, memory_limiter, resource]\n\nlogs:\n\nprocessors: [batch, memory_limiter, resource]\n\nApply the preceding configuration via Helm using the following command: $ helm upgrade otel-\n\ncollector open-telemetry/opentelemetry-collector -f ./config/collector/config.yml Release \"otel-\n\ncollector\" has been upgraded. Happy Helming!\n\nNAME: otel-collector\n\nLAST DEPLOYED: Sun Sep 19 13:22:10 2021\n\nLooking at the logs from the agent, we should observe that the telemetry contains the attributes we\n\nadded earlier: $ kubectl logs -l component=agent-collector -f\n\nNow, we have the collector sidecar sending data to the agent, and the agent is adding attributes via a\n\nprocessor: kubectl logs output\n\n2021-09-19T20:30:20.888Z DEBUG loggingexporter/logging_exporter.go:366 ResourceSpans #0\n\nResource labels:\n\n> telemetry.sdk.language: STRING(python)\n\n> telemetry.sdk.name: STRING(opentelemetry)\n\n> telemetry.sdk.version: STRING(1.3.0)\n\n> net.host.name: STRING(cloud-native-example-5d57799766-w8rjp) -> net.host.ip:\n\nSTRING(10.244.0.5)\n\n> service.name: STRING(grocery-store)\n\n> service.version: STRING(0.1.2)\n\n> k8s.node.name: STRING(kind-control-plane)\n\nInstrumentationLibrarySpans #0\n\nInstrumentationLibrary 0.1.2 grocery-store\n\nIMPORTANT NOTE\n\nYou might find it confusing that the previous example is not configuring receivers and exporters for the telemetry pipelines.\n\nThis is because the values we pass into Helm only override some of the default configurations in the chart. Since we only\n\nneeded to override the processors, the exporters and receivers continued to use the defaults that had already been\n\nconfigured. If you'd like to look at all the configured defaults, I suggest you refer to the repository at\n\nhttps://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-collector/values.yaml.\n\nHaving this single point to aggregate and add information to telemetry could be used to simplify our\n\napplication code. If you recall, in Chapter 4, Distributed Tracing – Tracing Code Execution, we\n\ncreated a custom ResourceDetector parameter to add net.host.name and net.host.ip attributes to all\n\napplications. That code could be removed in favor of injecting the same data via the collector. This\n\nmeans that now, any application could get these attributes without the complexity of utilizing custom\n\ncode. Next, let's look at standalone service deployment.\n\nCollector as a gateway\n\nThe last scenario we'll cover is how to deploy the collector as a standalone service, also known as a\n\ngateway. In this mode, the collector can provide a horizontally scalable service to do additional\n\nprocessing on the telemetry before sending it to a backend. Horizontal scaling means that if the\n\nservice comes under too much pressure, we can launch additional instances of it, which, in this case,\n\nis the collector, to manage the increasing load. Additionally, the standalone service can provide a\n\ncentral location for the configuring, sampling, and scrubbing of the telemetry. From a security\n\nstandpoint, it might also be preferable to have a single service sending traffic outside of your\n\nnetwork. This is because it simplifies the rules that need to be configured and reduces the risk and\n\nblast radius of vulnerabilities.\n\nIMPORTANT NOTE\n\nIf your backend is deployed within your network, it's possible that a standalone service for the collector will be overkill, as\n\nyou might be happier sending telemetry directly to the backend and saving yourself the trouble of operating an additional\n\nservice in your infrastructure.\n\nConveniently, the same Helm chart we used earlier to deploy the collector as an agent can also be\n\nused to configure the gateway. This also provides us with an opportunity to configure the agent to\n\nexport its data to the standalone collector, and therefore, we can feed two birds with one scone by\n\ndoing both at the same time. Depending on your Kubernetes cluster, the default value of 2Gi might\n\nprevent the service from starting as it did in the case of my kind cluster. The following section can be\n\nappended to the bottom of the configuration file from the previous example to enable\n\nstandaloneCollector and limit its memory consumption to 512Mi: config/collector/config.yml\n\nstandaloneCollector:\n\nenabled: true\n\nresources:\n\nlimits:\n\ncpu: 1\n\nmemory: 512Mi\n\nApply the update to the Helm chart by running the following command again: $ helm upgrade otel-\n\ncollector open-telemetry/opentelemetry-collector -f ./config/collector/config.yml A nice feature of the\n\nOpenTelemetry collector Helm chart is that if both agentCollector and standaloneCollector are\n\nconfigured, an OTLP exporter is automatically configured on the agent to forward traffic on the\n\nstandalone collector. The following code depicts a snippet of the Helm chart template to give us an\n\nidea of how that will be configured: config.tpl\n\n{{- if .Values.standaloneCollector.enabled }}\n\nexporters:\n\notlp:\n\nendpoint: {{ include \"opentelemetry-collector.fullname\" . }}:4317\n\ninsecure: true\n\n{{- end }}\n\nIt's time to examine the logs from the new service to check whether the data is reaching the\n\nstandalone collector. The following command should be familiar now; make sure that you use the\n\nstandalone-collector label when filtering the logs: $ kubectl logs -l component=standalone-\n\ncollector -f Now the output from the logs shows us the same logs that we observed from the agent\n\ncollector earlier, being processed by the standalone collector: kubectl logs output\n\nMetric #11\n\nDescriptor:\n\n> Name: otelcol_processor_accepted_spans\n\n> Description: Number of spans successfully pushed into the next component in the\n\npipeline.\n\n> Unit:\n\n> DataType: DoubleSum\n\n> IsMonotonic: true\n\n> AggregationTemporality: AGGREGATION_TEMPORALITY_CUMULATIVE\n\nDoubleDataPoints #0\n\nData point labels:\n\n> processor: memory_limiter\n\n> service_instance_id: b208628b-7b0f-4275-9ea8-a5c445582cbc StartTime:\n\n1632083630725000000\n\nTimestamp: 1632083730725000000\n\nValue: 718.000000\n\nIf you run kubectl logs with the agent-collector label, you'll find that because the agent collector is\n\nnow using the otlp exporter instead of the logging exporter, it no longer emits logs.\n\nAutoscaling\n\nUnlike the sidecar, which relied on an application pod, or the agent deployment, which relied on\n\nindividual nodes to scale, the standalone service can be automatically scaled based on CPU and\n\nmemory constraints. It does this using a Kubernetes feature known as HorizontalPodAutocaling,\n\nwhich can be configured via the following: autoscaling:\n\nenabled: false\n\nminReplicas: 1\n\nmaxReplicas: 10\n\ntargetCPUUtilizationPercentage: 80\n\ntargetMemoryUtilizationPercentage: 80\n\nDepending on the needs of your environment, combining autoscaling with a load balancer might be\n\nworth pursuing to provide a high level of reliability and capacity for the service.\n\nOpenTelemetry Operator\n\nAnother option for managing the OpenTelemetry collector in a Kubernetes environment is the\n\nOpenTelemetry operator (https://github.com/open-telemetry/opentelemetry-operator). If you're\n\nalready familiar with using operators, they reduce the complexity of deploying and maintaining\n\ncomponents in the Kubernetes landscape. In addition to managing the deployment of the collector,\n\nthe OpenTelemetry operator provides support for auto-instrumenting applications.\n\nSummary\n\nWe've only just scratched the surface of how to run the collector in production by looking at very\n\nspecific use cases. However, you can start thinking about how to apply the lessons you have learned\n\nfrom this chapter to your environments. Whether it be using Kubernetes, bare metal, or another form\n\nof hybrid cloud environment, the same principles we explored in this chapter regarding how to best\n\ncollect telemetry will apply. Collecting telemetry from an application should always be done with\n\nminimal impact on the application itself. The sidecar deployment mode provides a collection point as\n\nclose as possible to the application without adding any dependency to the application itself.\n\nThe deployment of the collector as an agent gives us the ability to collect information about the\n\nworker running our applications, which could also allow us to monitor the health of the resources in\n\nour cluster. Additionally, this serves as a convenient point to augment the telemetry from applications\n\nwith resource-specific attributes, which can be leveraged at analysis time. Finally, deploying the\n\ncollector as a gateway allowed us to start thinking about how to deploy and scale a service to collect\n\ntelemetry within our networks.\n\nThis chapter also gave us a chance to become familiar with some of the tools that OpenTelemetry\n\nprovides to infrastructure engineers to manage the collector. We experimented with the\n\nOpenTelemetry collector container alongside the Helm charts provided by the project. Now that we\n\nhave our environment deployed and primed to send data to a backend, in the next chapter, we'll take a\n\nlook at options for open source backends.\n\nChapter 10: Configuring Backends\n\nSo far, what we've been learning about has focused on the tools that are used to generate telemetry\n\ndata. Although producing telemetry data is an essential aspect of making a system observable, it\n\nwould be difficult to argue that the data we've generated in the past few chapters has made our system\n\nobservable. After all, reading hundreds of lines of output in a console is hardly a practical tool for\n\nanalysis. Data analysis is an essential aspect of observability that we have only briefly discussed thus\n\nfar. This chapter is all about the tools we can use to analyze our applications' telemetry.\n\nWe are going to cover the following topics:\n\nOpen source telemetry backends to analyze traces, metrics, and logs\n\nConsiderations for running analysis systems in production\n\nThroughout this chapter, we will visualize the data we've generated and start thinking about using it\n\nin real life. There is a large selection of analysis tools to choose from, but this chapter will only focus\n\non a select few. It's worth noting that many commercial products (https://opentelemetry.io/vendors/)\n\nsupport OpenTelemetry; this chapter will focus solely on open source projects. This chapter will also\n\nskim the surface of the knowledge that you will need to run these telemetry backends in production.\n\nTechnical requirements\n\nThis chapter will use Python code to directly configure and use backends from a test application. To\n\nensure your environment is set up correctly, run the following commands and ensure Python 3.6 or\n\ngreater is installed on your system: $ python --version\n\nPython 3.8.9\n\n$ python3 --version\n\nPython 3.8.9\n\nIf you do not have Python 3.6+ installed, go to the Python website\n\n(https://www.python.org/downloads/) for instructions on installing the latest version.\n\nTo test out some of the exporters we'll be using in the chapter, install the following OpenTelemetry\n\npackages via pip: $ pip install opentelemetry-distro \\\n\nopentelemetry-exporter-jaeger \\\n\nopentelemetry-exporter-zipkin\n\nAdditionally, we will use Docker (https://docs.docker.com/get-docker/) to deploy backends. The\n\nfollowing code will ensure Docker is up and running in your environment: $ docker version\n\nClient:\n\nCloud integration: 1.0.14\n\nVersion: 20.10.6\n\nAPI version: 1.41\n\nGo version: go1.16.3 ...\n\nTo launch the backends, we will use Docker Compose once again. Ensure Compose is available by\n\nrunning the following commands: $ docker compose version\n\nDocker Compose version 2.0.0-beta.1\n\nNow, download the code and configuration for this chapter from this book's GitHub repository: $ git\n\nclone https://github.com/PacktPublishing/Cloud-Native-Observability $ cd Cloud-Native-\n\nObservability/chapter10\n\nWith the code downloaded, we're ready to launch the backends using Compose: $ docker compose up\n\nThe following diagram shows the architecture of the environment that we'll be deploying. Initially,\n\nthe example for this chapter will connect to the backends directly. After that, we will send data to the\n\nOpenTelemetry Collector which we'll connect to the telemetry backends. Grafanais connected to\n\nJaeger, Zipkin, Loki, and Prometheus, as we will discuss later in this chapter.\n\nFigure 10.1 – Backend deployment in Docker The configuration files for all this chapter's examples can be found in the\n\nconfig directory. Let's dive in!\n\nBackend options for analyzing telemetry data\n\nThe world of observability contains an abundance of tools to provide you with insights into what\n\nsystems are doing. Within OpenTelemetry, a backend is the destination of the telemetry data and is\n\nwhere it is stored and analyzed. All the telemetry backends that we will explore in this chapter\n\nprovide the following:\n\nA destination for the telemetry data. This is usually in the form of a network endpoint, but not always.\n\nStorage for the telemetry data. The retention period that's supported by the storage is determined by the size of the storage and the\n\namount of data being stored.\n\nVisualization tooling for the data. All the tools we'll use provide a web interface for displaying and querying telemetry data.\n\nIn OpenTelemetry, applications connect to backends via exporters, two of which we've already\n\nconfigured: the console exporter and the OTLP exporter. Each application can be configured to send\n\ndata directly via an exporter that's been implemented specifically for that backend. The following\n\ntable shows a current list of officially supported exporters for the backends by the OpenTelemetry\n\nspecification, along with their status in the Python implementation:\n\nFigure 10.2 – Status of the exporters in Python for officially supported backends Each language that implements the\n\nOpenTelemetry specification must provide an exporter for these backends. Additional information about the support for\n\neach exporter in different languages can be found in the specification repository: https://github.com/open-\n\ntelemetry/opentelemetry-specification/blob/main/spec-compliance-matrix.md#exporters.\n\nTracing\n\nStarting with the tracing signal, let's look at some options for visualizing traces. As we work through\n\ndifferent backends, we'll see how it's possible to use other methods to configure a backend, starting\n\nwith auto-instrumentation. The following code makes a series of calls to create a table and insert\n\nsome data into a local database using SQLite (https://www.sqlite.org/index.html) while logging some\n\ninformation along the way: sqlite_example.py import logging\n\nimport os\n\nimport sqlite3\n\nlogging.basicConfig(level=logging.DEBUG)\n\nlogger = logging.getLogger(__name__)",
      "page_number": 239
    },
    {
      "number": 10,
      "title": "Configuring Backends",
      "start_page": 255,
      "end_page": 273,
      "detection_method": "regex_chapter",
      "content": "logger.info(\"creating database\")\n\ncon = sqlite3.connect(\"example.db\") cur = con.cursor()\n\nlogger.info(\"adding table\")\n\ncur.execute(\n\n\"\"\"CREATE TABLE clouds\n\n(category text, description text)\"\"\"\n\n)\n\nlogger.info(\"inserting values\")\n\ncur.execute(\"INSERT INTO clouds VALUES ('stratus','grey')\") con.commit()\n\ncon.close()\n\nlogger.info(\"deleting database\")\n\nos.remove(\"example.db\")\n\nRun the preceding code to ensure everything is working as expected by running the following\n\ncommand: $ python sqlite_example.py\n\nINFO:__main__:creating database\n\nINFO:__main__:adding table\n\nINFO:__main__:inserting values\n\nINFO:__main__:deleting database\n\nNow that we have some working code, let's ensure we can produce telemetry data by utilizing auto-\n\ninstrumentation. As you may recall from Chapter 7, Instrumentation Libraries, Python provides the\n\nopentelemetry-bootstrap script to detect and install instrumentation libraries for us automatically.\n\nThe library we're using in our code, sqlite3, has a supported instrumentation library that we can\n\ninstall with the following command: $ opentelemetry-bootstrap -a install\n\nCollecting opentelemetry-instrumentation-sqlite3==0.26b1\n\n...\n\nThe output from the preceding command will produce some logging information that's generated by\n\ninstalling the packages through pip. If the output doesn't quite match mine, opentelemetry-bootstrap\n\nlikely found additional packages to install for your environment.\n\nUsing opentelemetry-instrument, let's ensure that telemetry data is generated by configuring our\n\ntrusty console exporter: $ OTEL_RESOURCE_ATTRIBUTES=service.name=sqlite_example \\\n\nOTEL_TRACES_EXPORTER=console \\\n\nopentelemetry-instrument python sqlite_example.py The output should now contain tracing\n\ninformation that's similar to the following abbreviated output: output\n\nINFO:__main__:creating database\n\nINFO:__main__:adding table\n\nINFO:__main__:inserting values\n\nINFO:__main__:deleting database\n\n{\n\n\"name\": \"CREATE\",\n\n\"context\": {\n\n\"trace_id\": \"0xf98afa4316b3ac52633270b1e0534ffe\", \"span_id\": \"0xb52fb818cb0823da\",\n\n\"trace_state\": \"[]\"\n\n},\n\n...\n\nNow, we're ready to look at our first telemetry backend by using a working example that utilizes\n\ninstrumentation to produce telemetry data. Zipkin\n\nOne of the original backends for distributed tracing, Zipkin (https://zipkin.io) was developed and\n\nopen sourced by Twitter in 2012. The project was made available for anyone to use under the Apache\n\n2.0 license, and its community is actively maintaining and developing the project. Its core\n\ncomponents are as follows:\n\nA collector to receive and index traces.\n\nA storage component, which provides a pluggable interface for storing data in various databases. The three storage options that\n\nare supported by Zipkin natively are Cassandra, Elasticsearch, and MySQL.\n\nA query service or API, which can be used to retrieve data from storage.\n\nAs we'll see shortly, there's a web UI, which gives users visualization and querying capabilities.\n\nThe easiest way to send data from the sample application to Zipkin is by changing the\n\nOTEL_TRACES_EXPORTER environment variable, as per the following command: $\n\nOTEL_RESOURCE_ATTRIBUTES=service.name=sqlite_example \\\n\nOTEL_TRACES_EXPORTER=zipkin \\\n\nopentelemetry-instrument python sqlite_example.py Setting the environment variable to\n\nzipkin tells auto-instrumentation to load ZipkinExporter, which is defined in the\n\nopentelemetry-exporter-zipkin-proto-http package. This connects to Zipkin via HTTP over\n\nport 9411. Launch a browser and access the Zipkin web UI via http://localhost:9411/zipkin.\n\nFigure 10.3 – Zipkin UI landing page\n\nSearch for traces by clicking the Run Query button. The results should show two traces; clicking on\n\nthe details of one of these will bring up additional span information. This includes the attributes that\n\nare automatically populated by the instrumentation library, which are labeled as Tags in the Zipkin\n\ninterface.\n\nFigure 10.4 – Trace details view\n\nThe interface for querying lets you search for traces by trace ID, service name, duration, or tag,\n\namong other filters. It's also possible to filter traces by specifying a time window for the query. One\n\nlast feature of Zipkin we will inspect requires multiple services to produce traces. As it happens, we\n\nhave the grocery store already making telemetry data in our Docker environment; all we need to do is\n\nconfigure it to send data to Zipkin. Since the grocery store has already been configured to send data\n\nto the OpenTelemetry Collector, we'll update the collector's configuration to send data to Zipkin. Add\n\nthe following configuration to enable the Zipkin exporter for the Collector:\n\nconfig/collector/config.yml receivers:\n\notlp:\n\nprotocols:\n\ngrpc:\n\nexporters:\n\nlogging:\n\nloglevel: debug\n\nzipkin:\n\nendpoint: http://zipkin:9411/api/v2/spans\n\nservice:\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nexporters: [logging, zipkin]\n\nmetrics:\n\nreceivers: [otlp]\n\nexporters: [logging]\n\nlogs:\n\nreceivers: [otlp]\n\nexporters: [logging]\n\nFor the configuration changes to take effect, the OpenTelemetry Collector container must be\n\nrestarted. In terminal, use the following command from the chapter10 directory: $ docker compose\n\nrestart opentelemetry-collector An alternative would be to relaunch the entire Docker Compose\n\nenvironment, but restarting just the opentelemetry-collector container is more expedient.\n\nIMPORTANT NOTE\n\nTrying to run the restart command from other directories will result in an error while trying to find a suitable\n\nconfiguration.\n\nLooking at the Zipkin interface again, searching for traces yields much more interesting results when\n\nthe traces link spans across services. Try running some queries by searching for specific names or\n\ntags and see interesting ways to peruse the data. One more feature worth noting is the dependency\n\ngraph, as shown in the following screenshot. It provides a service diagram that connects the\n\ncomponents of the grocery store.\n\nFigure 10.5 – Zipkin dependencies service diagram The dependencies service diagram can often be helpful if you wish\n\nto get a quick overview of a system and understand the flow of information between components. Let's see how this\n\ncompares with another tracing backend.\n\nJaeger\n\nInitially developed by engineers at Uber, Jaeger (https://www.jaegertracing.io) was open sourced in\n\n2015. It became a part of the Cloud Native Computing Foundation (CNCF), the same organization\n\nthat oversees OpenTelemetry, in 2017. The Jaeger project provides the following:\n\nAn agent that runs as close to the application as possible, often on the same host or inside the same pod.\n\nA collector to receive distributed traces that, depending on your deployment, talks directly to a datastore or Kafka for buffering.\n\nAn ingester that is (optionally) deployed. Its purpose is to read Kafka data and output it to a datastore.\n\nA query service that fetches data and provides a web UI for users to view it.\n\nReturning to the sample SQLite application for a moment, the following code uses in-code\n\nconfiguration to configure OpenTelemetry with JaegerExporter. It would be easy to update the\n\nOTEL_TRACES_EXPORTER variable to jaeger instead of zipkin and run opentelemetry-instrument to\n\naccomplish the same thing. Still, auto-instrumentation may not always be possible for an application.\n\nKnowing how to configure these exporters manually will surely come in handy someday.\n\nThe code in the following example adds the familiar configuration of the tracing pipeline. The\n\nfollowing are a couple of things to note:\n\nJaegerExporter has been configured to use a secure connection by default. We must pass in the insecure argument to change\n\nthis.\n\nThe code manually invokes SQLite3Instrumentor to trace calls via the sqlite3 library.\n\nAdd the following code to the top of the SQLite example code we created previously:\n\nsqlite_example.py ...\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.exporter.jaeger.proto.grpc import JaegerExporter from\n\nopentelemetry.instrumentation.sqlite3 import SQLite3Instrumentor from\n\nopentelemetry.sdk.trace import TracerProvider from opentelemetry.sdk.resources import\n\nResource from opentelemetry.sdk.trace.export import BatchSpanProcessor def\n\nconfigure_opentelemetry():\n\nSQLite3Instrumentor().instrument()\n\nexporter = JaegerExporter(insecure=True)\n\nprovider = TracerProvider(\n\nresource=Resource.create({\"service.name\": \"sqlite_example\"}) )\n\nprovider.add_span_processor(BatchSpanProcessor(exporter))\n\ntrace.set_tracer_provider(provider)\n\nconfigure_opentelemetry()\n\n...\n\nRunning the application with the following command will send data to Jaeger: $ python\n\nsqlite_example.py\n\nAccess the Jaeger interface by browsing to http://localhost:16686/. Upon arriving on the landing\n\npage, searching for traces should yield results similar to what's shown in the following screenshot.\n\nNote that in Jaeger, you'll need to select a service from the dropdown on the left-hand side before you\n\ncan find traces.\n\nFigure 10.6 – Jaeger search results\n\nLooking through the details for each trace, we can see that the same information we previously found\n\nin Zipkin can be seen in Jaeger, although organized slightly differently. Next, let's update the\n\nCollector file's configuration to send traces from the grocery store to Jaeger. Add the following\n\njaeger section under the exporters definition in the Collector configuration file:\n\nconfig/collector/config.yml ...\n\nexporters:\n\n...\n\njaeger:\n\nendpoint: jaeger:14250\n\ntls:\n\ninsecure: true\n\nservice:\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nexporters: [logging, zipkin, jaeger]\n\n...\n\nRestart the Collector container to reload the updated configuration: $ docker compose restart\n\nopentelemetry-collector The Jaeger web UI starts becoming more interesting when more data comes\n\nin. For example, note the scatter plot displayed previously in the search results; it's an excellent way\n\nto identify outliers. The chart supports clicking on individual traces to bring up additional details.\n\nFigure 10.7 – Scatter plot of trace durations Like Zipkin, Jaeger visualizes the relationship between services via the\n\nSystem Architecture diagram. An exciting feature that Jaeger delivers is that you can compare traces by selecting\n\ntraces of interest from the search results and clicking the Compare Traces button. The following screenshot shows a\n\ncomparison between two traces for the same operation. In one instance, the grocery store failed to connect to the\n\nlegacy inventory service, resulting in an error and a missing span.\n\nFigure 10.8 – Trace comparison diagram\n\nThis visual representation of the trace comparison can help us quickly identify a difference between a\n\ntypical trace and one where an error occurred, zoning in on where the change was made.\n\nMetrics\n\nAs of November 2021, Prometheus is the only officially supported exporter for the metrics signal.\n\nOfficial support for StatsD in the specification was requested some time ago\n\n(https://github.com/open-telemetry/opentelemetry-specification/issues/374), but the lack of a\n\nspecification for StatsD has stopped OpenTelemetry from making it a requirement.\n\nPrometheus\n\nA project initially developed in 2012 by engineers at SoundCloud, Prometheus\n\n(https://prometheus.io) is a dominant open source metrics system. Its support for multi-dimensional\n\ndata and first-class support for alerting quickly made it a favorite of DevOps practitioners. Initially,\n\nPrometheus used a pull model only. Applications that wanted to store metrics exposed them via a\n\nnetwork endpoint that had been scraped by the Prometheus server. Prometheus now supports the push\n\nmodel via Prometheus Remote Write, allowing producers to send data to a remote server. The\n\ncomponents of interest to us currently are as follows:\n\nThe Prometheus server collects data from scrape targets and stores it in its time-series database (TSDB).\n\nThe Prometheus Query Language (PromQL) for searching and aggregating metrics.\n\nVisualization for metrics data via the Prometheus web UI.\n\nAs the current implementation of the Prometheus exporter for Python is still in development, in this\n\nsection, we will focus on the data that's produced by the grocery store, which is sent through the\n\nCollector. The implementation of the Prometheus exporter in the Collector is also in development at\n\nthe time of writing, but it is further along. The following configuration can be added to the Collector's\n\nconfiguration to send metrics to Prometheus: config/collector/config.yml exporters:\n\n...\n\nprometheus:\n\nendpoint: 0.0.0.0:8889\n\nresource_to_telemetry_conversion:\n\nenabled: true\n\nservice:\n\npipelines:\n\n...\n\nmetrics:\n\nreceivers: [otlp]\n\nexporters: [logging, prometheus]\n\n...\n\nReload the Collector with the following command:\n\n$ docker compose restart opentelemetry-collector Bring up the Prometheus web interface by\n\npointing your browser to http://localhost:9090. Using PromQL, the following query will\n\nreturn all the metrics that have been produced by the OpenTelemetry Collector:\n\n{job=\"opentelemetry-collector\"}\n\nThis can be seen in the following screenshot:\n\nFigure 10.9 – PromQL query results\n\nThe pull model makes horizontally scaling Prometheus an easy aspect that makes it a good option for\n\nmany environments. There are, of course, challenges with running Prometheus at scale, like any other\n\nbackend. Unfortunately, we don't have the space to dive into data availability across regions and\n\nlong-term storage, to name just a few challenges. Like Jaeger and OpenTelemetry, Prometheus is also\n\na project under the governance of the CNCF.\n\nLogging\n\nEven with no officially supported backends at the time of writing, it's helpful to have a way to query\n\nlogs that doesn't require looking at files on disk directly or paying for a service to get started. The\n\ntools that we've discussed in this section have exporters available in the OpenTelemetry Collector but\n\nmay not necessarily have exporters implemented in other languages.\n\nLoki\n\nA project started by Grafana Labs in 2018, Loki is a log aggregation system that's designed to be easy\n\nto scale and operate. Its design is inspired by Prometheus and is composed of the following\n\ncomponents:\n\nA distributor that validates and pre-processes incoming logging data before sending it off to the ingester\n\nAn ingester that writes data to storage and provides a read endpoint for in-memory data\n\nA ruler that interprets configurable rules and triggers actions based on them\n\nA querier that performs queries for both the ingester and storage\n\nA query frontend that acts as a proxy for optimizing requests that are made to the querier\n\nThese components can be run in a single deployment or as a separate service to make it easy to\n\ndeploy them in whichever mode makes the most sense. The OpenTelemetry Collector provides an\n\nexporter for Loki, which can be configured as per the following code snippet. The configuration of\n\nthe Loki exporter supports relabeling attributes and resource attributes before sending the data. In the\n\nfollowing example, the service.name resource attribute has been relabeled job:\n\nconfig/collector/config.yml exporters:\n\n…\n\nloki:\n\nendpoint: http://loki:3100/loki/api/v1/push\n\nlabels:\n\nresource:\n\nservice.nam\": \"job\"\n\nservice:\n\npipelines:\n\n...\n\nlogs:\n\nreceivers: [otlp]\n\nexporters: [logging, loki]\n\n...\n\nOnce more, restart the Collector to reload the configuration and start sending data to Loki: $ docker\n\ncompose restart opentelemetry-collector Now, it's time to review this logging data. You may have\n\nnoticed that the components we mentioned earlier for Loki lack an interface for visualizing the data.\n\nThat's because the interface of choice for Loki is Grafana, which is a separate project altogether.\n\nGrafana\n\nGrafana (https://grafana.com/grafana/) is an open source tool that's been developed since 2014 by\n\nGrafana Labs to allow users to visualize and query telemetry data. Grafana enables users to configure\n\ndata sources that support various formats for traces, metrics, and logs. This includes Zipkin, Jaeger,\n\nPrometheus, and Loki.\n\nLet's see how we can access the logs we sent to our Loki backend. Access the Explore section of the\n\nGrafana interface via a browser by going to http://localhost:3000/explore. In the query field, enter\n\n{job=~\"grocery-store|inventory|shopper\"}. This will bring up all the logs for all the grocery store\n\ncomponents.\n\nFigure 10.10 – Logs search results\n\nGrafana allows users to create dashboards and alerts for the data that's received via its data sources.\n\nSince it's possible to view data from all signals, it's also possible to see data across all signals within a\n\nsingle dashboard. An example of such a dashboard has been preconfigured in the development\n\nenvironment and is accessible via the following URL: http://localhost:3000/d/otel/opentelemetry.\n\nFigure 10.11 – Dashboard combining data across signals There are many more capabilities to explore regarding each\n\nof the tools discussed in this chapter. A vast amount of information on all the features and configuration options is\n\navailable on the website associated with each project. I strongly recommend spending some time familiarizing yourself\n\nwith these tools.\n\nRunning in production\n\nUsing analysis tools in development is one thing; running them in production is another. Running a\n\nsingle container on one machine is not an acceptable strategy for operating a service that provides\n\ninformation that's critical to an organization. It's worth considering the challenges of scaling\n\ntelemetry backends to meet the demands of the real world. The following subsections highlight areas\n\nthat require further reading before you run any of the backends mentioned earlier in production.\n\nHigh availability\n\nThe availability of telemetry backends is likely not as critical to end users as that of the applications\n\nthey are used to monitor. However, having an outage and realizing that the data that's required to\n\ninvestigate is unavailable or missing during the outage causes problems. If an application promises an\n\nuptime of 99.99%, the telemetry backend must be available to account for those guarantees. Some\n\naspects to consider when thinking of the high availability in the context of a telemetry backend are as\n\nfollows:\n\nEnsuring the telemetry receivers are available to senders. This can be accomplished by placing a load balancer between the\n\nsenders and the receivers.\n\nConsidering how the backends will be upgraded and how to minimize the impact on the applications being observed.\n\nUnderstanding the expectations for being able to query the data.\n\nDeciding how much of the data needs to be replicated to mitigate the risks of catastrophic failure.\n\nAdditionally, geo-distributed environments must consider how the applications will behave if a\n\nbackend is deployed in distant regions. Many of the backends we've discussed provide\n\nrecommendations for deploying the backend in a mode that supports high availability.\n\nScalability\n\nThe telemetry backend must be able to grow alongside the applications they support. Whether that's\n\nby adding more instances or increasing the number of resources that are given to the backend,\n\nknowing what the tools support can help you decide which backend to use. Some questions that are\n\nworth asking are as follows:\n\nCan the components of the backend be scaled independently?\n\nWill scaling the backend require vertical scaling or horizontal scaling?\n\nHow far will the solution scale? Is there a hard limit somewhere along the way?\n\nWhen we think about scalability, it's essential to understand the limitations of the tools we're working\n\nwith, even if we never come close to using them to their full extent.\n\nData retention\n\nA key challenge in telemetry is the volume of data that's being produced. It's easy to lean toward\n\nstoring every detail forever, as it is hard to predict when the data may become necessary. It's a bit like\n\nholding on to all those old cables and connectors for hardware that hasn't existed since the late 90s;\n\nyou never know when it will come in handy!\n\nThe problem with storing all the data forever is that it becomes costly at scale. On the other hand, the\n\ncost tends to cause engineers to lean in the opposite direction too much, where we log or record so\n\nlittle that it becomes hard to find anything of value. Some options to think about are as follows:\n\nIdentify an acceptable data retention period for the quantity of data that's being produced. This will likely change as teams become\n\nbetter at identifying issues within shorter periods.\n\nIf long-term data storage is desirable, use lower-cost storage to reduce operational costs. This may result in longer query times,\n\nbut the data will still be available.\n\nTune a sensible sampling option for the different signals. More on this will be covered in Chapter 12, Sampling.\n\nAt a minimum, data retention should cover periods when engineers are expected to be away. For\n\nexample, if no one is watching systems during a 2-day weekend, data should be retained for 3 or\n\nmore days. Otherwise, events that occur during the weekend will be impossible to investigate.\n\nWhatever you decide regarding the retention method, there are plenty of ways to fine-tune it over\n\ntime. It's also critical for teams across the organization to be aware of what this data retention is.\n\nPrivacy regulations\n\nDepending on the contents of the telemetry data that's produced by applications, the requirements for\n\nwhere and how the data can be stored vary. For example, regulations such as the General Data\n\nProtection Regulation (GDPR) recommend personally identifiable data to be pseudonymized to\n\nensure nobody can be associated with the data without additional processing. Depending on the\n\nrequirements in your environment and the telemetry data that's being produced, we have to take the\n\nfollowing into account about the data:\n\nThe data may need to remain within a specific country or region.\n\nThe data may need to be processed further before being stored. This could mean many things, from the data being encrypted to\n\nscrubbing it of personally identifiable information or pseudonymization.\n\nThe data may need access control and auditing capabilities.\n\nUsing the OpenTelemetry Collector as a receiver of telemetry data before sending the data to\n\ntelemetry backends can alleviate concerns around data privacy. Various processors in the Collector\n\ncan be configured to facilitate the scrubbing of sensitive information.\n\nSummary\n\nOne of the many jobs of software engineers today includes evaluating the new technology and tools\n\nthat are available to determine whether these tools would improve their ability to accomplish their\n\ngoals. Leveraging auto-instrumentation, in-code configuration, and the OpenTelemetry Collector, we\n\nquickly sent data from one backend to another to help us compare these tools.\n\nAll the tools we've discussed in this chapter take much more than a few pages to become familiar\n\nwith. Entire books have been written about running these in production, and the skills to do so well at\n\nscale require practice and experience. Understanding some areas that need additional thinking when\n\nthose tools are deployed allows us to uncover some of the unknowns.\n\nLooking through the different tools and starting to see how each one provides functionality to\n\nvisualize the data gave us a sense of how telemetry data can be used to start answering questions\n\nabout our systems. In the next chapter, we will focus on how these visualizations can identify specific\n\nproblems.\n\nChapter 11: Diagnosing Problems\n\nFinally, after instrumenting application code, configuring a collector to transmit the data, and setting\n\nup a backend to receive the telemetry, we have all the pieces in place to observe a system. But what\n\ndoes that mean? How can we detect abnormalities in a system with all these tools? That's what this\n\nchapter is all about. This chapter aims to look through the lens of an analyst and see what the shape\n\nof the data looks like as events occur in a system. To do this, we'll look at the following areas:\n\nHow leaning on chaos engineering can provide the framework for running experiments in a system\n\nCommon scenarios of issues that can arise in distributed systems\n\nTools that allow us to introduce failures into our system\n\nAs we go through each scenario, we'll describe the experiment, propose a hypothesis, and use\n\ntelemetry to verify whether our expectations match what the data shows us. We will use the data and\n\nbecome more familiar with analysis tools to help us understand how we may answer questions about\n\nour systems in production. As always, let's start by setting up our environment first.\n\nTechnical requirements\n\nThe examples in this chapter will use the grocery store application we've used and revisited\n\nthroughout the book. Since the chapter's goal is to analyze telemetry and not specifically look at how\n\nthis telemetry is produced, the application code will not be the focus of the chapter. Instead of\n\nrunning the code as separate applications, we will use it as Docker (https://docs.docker.com/get-\n\ndocker/) containers and run it via Compose. Ensure Docker is installed with the following command:\n\n$ docker version\n\nClient:\n\nCloud integration: 1.0.14\n\nVersion: 20.10.6\n\nAPI version: 1.41\n\nGo version: go1.16.3 ...\n\nThe following command will ensure Docker Compose is also installed:\n\n$ docker compose version\n\nDocker Compose version 2.0.0-beta.1\n\nThe book's companion repository (https://github.com/PacktPublishing/Cloud-Native-Observability)\n\ncontains the Docker Compose configuration file, as well as the configuration required to run the\n\nvarious containers. Download the companion repository via Git:\n\n$ git clone https://github.com/PacktPublishing/Cloud-Native-Observability\n\n$ cd Cloud-Native-Observability/chapter11\n\nWith the configuration in place, start the environment via the following:\n\n$ docker compose up\n\nThroughout the chapter, as we conduct experiments, know that it is always possible to reset the\n\npristine Docker environment by removing the containers entirely with the following commands:\n\n$ docker compose stop\n\n$ docker compose rm\n\nAll the tools needed to run various experiments have already been installed inside the grocery store\n\napplication containers, meaning there are no additional tools to install. The commands will be\n\nexecuted via docker exec and run within the container.\n\nIntroducing a little chaos\n\nIn normal circumstances, the real world is unpredictable enough that intentionally introducing\n\nproblems may seem unnecessary. Accidental configuration changes, sharks chewing through\n\nundersea cables, and power outages affecting data centers are just a few events that have caused\n\nlarge-scale issues across the world. In distributed systems, in particular, dependencies can cause\n\nfailures that may be difficult to account for during normal development.\n\nPutting applications through various stress, load, functional, and integration tests before they are\n\ndeployed to production can help predict their behavior to a large extent. However, some\n\ncircumstances may be hard to reproduce outside of a production environment. A practice known as\n\nchaos engineering (https://principlesofchaos.org) allows engineers to learn and explore the behavior\n\nof a system. This is done by intentionally introducing new conditions into the system through\n\nexperiments. The goal of these experiments is to ensure that systems are robust enough to withstand\n\nfailures in production.\n\nIMPORTANT NOTE\n\nAlthough chaos engineers run experiments in production, it's essential to understand that one of the principles of chaos\n\nengineering is not to cause unnecessary pain to users, meaning experiments must be controlled and limited in scope. In\n\nother words, despite its name, chaos engineering isn't just going around a data center and unplugging cables haphazardly.\n\nThe cycle for producing experiments goes as follows:\n\n1. It begins with a system under a known good state or steady state.\n\n2. A hypothesis is then proposed to explain the experiment's impact on the system's state.\n\n3. The proposed experiment is run on the system.\n\n4. Verification of the impact on the system takes place, validating that the prediction matches the hypothesis. The verification step\n\nprovides an opportunity to identify unexpected side effects of the experiment. If something behaved precisely as expected, great!\n\nIf it acted worse than expected, why? If it behaved better than expected, what happened? It's essential to understand what\n\nhappened, especially if the results were better than expected. It's too easy to look at a favorable outcome and move right along\n\nwithout taking the time to understand why it happened.\n\n5. Once verification is complete, improvements to the system are made, and the cycle begins anew. Ideally, running these\n\nexperiments can be automated once the results on the system are satisfactory to guard against future regressions.\n\nFigure 11.1 – Chaos engineering life cycle\n\nThe hypothesis step is crucial because if the proposed experiment will have disastrous repercussions\n\non the system, it may be worth rethinking the experiment.\n\nFor example, a hypothesis that turning off all production servers simultaneously will cause a\n\ncomplete system failure doesn't need to be validated. Additionally, this would guarantee the creation\n\nof unnecessary pain for all users of the system. There isn't much to learn from running this in\n\nproduction, except seeing how quickly users send angry tweets.\n\nAn alternative experiment that may be worthwhile is to shut down an availability zone or a region of\n\na system to ensure load balancing works. This would provide an opportunity to learn how production\n\ntraffic would be handled in the case of such a failure, validating that those systems in place to\n\nmanage such a failure are doing their jobs. Of course, if no such mechanisms are in place, it's not\n\nworth experimenting either, as this would have the same impact as shutting down all the servers for\n\nall users in that zone or region.",
      "page_number": 255
    },
    {
      "number": 11,
      "title": "Diagnosing Problems",
      "start_page": 274,
      "end_page": 290,
      "detection_method": "regex_chapter",
      "content": "This chapter will take a page out of the chaos engineering book and introduce various failure modes\n\ninto the grocery store system. We will propose a hypothesis and run experiments to validate the\n\nassumptions. Using the telemetry produced by the store, we will validate that the system behaved as\n\nexpected. This will allow us to use the telemetry produced to understand how we can answer\n\nquestions about our system via this information. Let's explore a problem that impacts all networked\n\napplications: latency.\n\nExperiment #1 – increased latency\n\nLatency is the delay introduced when a call is made and the response is returned to the originating\n\ncaller. It can inject itself into many aspects of a system. This is especially true in a distributed system\n\nwhere latency can be found anywhere one service calls out to another. The following diagram shows\n\nan example of how latency can be calculated between two services. Service A calls a remote service\n\n(B), the request duration is 25 ms, but a large portion of that time is spent transferring data to and\n\nfrom service B, with only 5 ms spent executing code.\n\nFigure 11.2 – Latency incurred by calling a remote service\n\nIf the services are collocated, the latency is usually negligible and can often be ignored. However,\n\nlatency must be accounted for when services communicate over a network. This is something to think\n\nabout at development time. It can be caused by factors such as the following:\n\nThe physical distance between the servers hosting services. As even the speed of light requires time to travel distance, the greater\n\nthe distance between services, the greater the latency.\n\nA busy network. If a network reaches the limits of how much data it can transfer, it may throttle the data transmitted.\n\nProblems in any applications or systems connecting the services. Load balancers and DNS services are just two examples of the\n\nservices needed to connect two services.\n\nExperiment\n\nThe first experiment we'll run is to increase the latency in the network interface of the grocery store.\n\nThe experiment uses a Linux utility to manipulate the configuration on the network interface: Traffic\n\nControl (https://en.wikipedia.org/wiki/Tc_(Linux)). Traffic Control, or tc, is a powerful utility that\n\ncan simulate a host of scenarios, including packet loss, increased latency, or throughput limits. In this\n\nexperiment, tc will add a delay to inbound and outbound traffic, as shown in Figure 11.3:\n\nFigure 11.3 – Experiment #1 will add latency to the network interface\n\nHypothesis\n\nIncreasing the latency to the grocery store network interface will incur the following:\n\nA reduction in the total number of requests processed\n\nAn increase in the request duration time\n\nUse the following Docker command to introduce the latency. This uses the tc utility inside the\n\ngrocery store container to add a 1s delay to all traffic received and sent through interface eth0:\n\n$ docker exec grocery-store tc qdisc add dev eth0 root netem delay 1s Verify\n\nTo observe the metrics and traces generated, access the Application Metrics dashboard in Grafana\n\nvia the following URL: http://localhost:3000/d/apps/application-metrics. You'll immediately notice a\n\ndrop in the Request count time series and an increase in Request duration time quantiles. As time\n\npasses, you'll also start seeing the Request duration distribution histogram change to show an\n\nincreasing number of requests falling into buckets with longer durations that are as per the following\n\nscreenshot:\n\nFigure 11.4 – Request metrics for shopper, grocery-store, and inventory services\n\nNote that although the drop in request count is the same across the inventory and grocery store\n\nservices, the duration of the request for the inventory service remains unchanged. This is a great\n\nstarting point, but it would be ideal to identify precisely where this jump in the request duration\n\noccurred.\n\nIMPORTANT NOTE\n\nAs discussed earlier in this book, the correlation between metrics and traces provided by exemplars could help us drill\n\ndown more quickly by giving us specific traces to investigate from the metrics. However, since the implementation of\n\nexemplar support in OpenTelemetry is still under development at the time of writing, the example in this chapter does not\n\ntake advantage of it. I hope that by the time you're reading this, exemplar support is implemented across many languages\n\nin OpenTelemetry.\n\nLet's look at the tracing data in Jaeger available at http://localhost:16686. From the metrics, we\n\nalready know that the issue appears to be isolated to the grocery store service. Sure enough, searching\n\nfor traces for that service yields the following chart:\n\nFigure 11.5 – Increased duration results in Jaeger\n\nIt's clear from this chart that something happened. The following screenshot shows us two traces; at\n\nthe top is a trace from before we introduced the latency; at the bottom is a trace from after. Although\n\nthe two look similar, looking at the duration of the spans named web request and /products, it's clear\n\nthat those operations are taking far longer at the bottom than at the top.\n\nFigure 11.6 – Trace comparison before and after latency was introduced\n\nAs hypothesized, the total number of requests processed by the grocery store dropped due to the\n\nsimulation. This, in turn, reduced the number of calls to the inventory service. The total duration of\n\nthe request as observed by the shopper client increased significantly.\n\nRemove the delay to see how the system recovers. The following command removes the delay\n\nintroduced earlier:\n\n$ docker exec grocery-store tc qdisc del dev eth0 root netem delay 1s\n\nLatency is only one of the aspects of networks that can cause problems for applications. Traffic\n\nControl's network emulator (https://man7.org/linux/man-pages/man8/tc-netem.8.html) functionality\n\ncan simulate many other symptoms, such as packet loss and rate-limiting, or even the re-ordering of\n\npackets. If you're keen on playing with networks, it can be a lot of fun to simulate different scenarios.\n\nHowever, the network isn't the only thing that can cause problems for systems.\n\nExperiment #2 – resource pressure\n\nAlthough cloud providers make provisioning new computing resources more accessible than ever\n\nbefore, even computing in the cloud is still bound by the physical constraints of hardware running\n\napplications. Memory, processors, hard drives, and networks all have their limits. Many factors can\n\ncontribute to resource exhaustion:\n\nMisconfigured or misbehaving applications. Crashing and restarting in a fast loop, failing to free memory, or making requests over\n\nthe network too aggressively can all contribute to a load on resources.\n\nAn increase or spike in requests being processed by the service. This could be good news; the service is more popular than ever!\n\nOr it could be bad news, the result of a denial-of-service attack. Either way, more data to process means more resources are\n\nrequired.\n\nShared resources cause resource starvation. This problem is sometimes referred to as the noisy neighbor problem, where resources\n\nare consumed by another tenant of the physical hardware where a service is running.\n\nAutoscaling or dynamic resource allocation helps alleviate resource pressures to some degree by\n\nallowing users to configure thresholds at which new resources should be made available to the\n\nsystem. To know how these thresholds should be configured, it's valuable to experiment with how\n\napplications behave under limited resources. Experiment\n\nWe'll investigate how telemetry can help identify resource pressures in the following scenario. The\n\ngrocery store container is constrained to 50 M of memory via its Docker Compose configuration.\n\nMemory pressure will be applied to the container via stress.\n\nThe Unix stress utility (https://www.unix.com/man-page/debian/1/STRESS/) spins workers that\n\nproduce loads on systems. It creates memory, CPU, and I/O pressures by calling system functions in\n\na loop; malloc/free, sqrt, and sync, depending on which resource is being pressured.\n\nFigure 11.7 – Experiment #2 will apply memory pressure to the container\n\nHypothesis\n\nAs resources are consumed by stress, we expect the following to happen:\n\nThe grocery store processes fewer requests as it cannot obtain the resources to process requests.\n\nLatency increases across the system, as requests will take longer to process through the grocery store.\n\nMetrics collected from the grocery store container should quickly identify the increased resource pressure.\n\nThe following introduces memory pressure by adding workers that consume a total of 40 M of\n\nmemory to the grocery store container via stress for 30 minutes:\n\n$ docker exec grocery-store stress --vm 20 --vm-bytes 2M --timeout 30m\n\nstress: info: [20] dispatching hogs: 0 cpu, 0 io, 10 vm, 0 hdd Verify\n\nWith the pressure in place, let's see whether the telemetry matches what we expected. Looking at the\n\napplication metrics, we can see an almost immediate increase in request duration as per the following\n\nscreenshot. The request count is also slightly impacted simultaneously.\n\nFigure 11.8 – Application metrics\n\nWhat else can we learn about the event? Searching through traces, an increase in duration similar to\n\nwhat occurred during the first experiment is shown:\n\nFigure 11.9 – Trace duration increased\n\nLooking in more detail at individual traces, we can identify which paths through the code cause this\n\nincrease. Not surprisingly, the allocating memory span, which locates an operation performing a\n\nmemory allocation, is now significantly longer, with its time jumping from 2.48 ms to 49.76 ms:\n\nFigure 11.10 – Trace comparison before and after the memory increase\n\nThere is a second dashboard worth investigating at this time, the Container metrics dashboard\n\n(http://localhost:3000/d/containers/container-metrics). This dashboard shows the CPU, memory, and\n\nnetwork metrics collected directly from Docker by the collector's Docker stats receiver\n\n(https://github.com/open-telemetry/opentelemetry-collector-\n\ncontrib/tree/main/receiver/dockerstatsreceiver). Reviewing the following charts, it's evident that\n\nresource utilization increased significantly in one container:\n\nFigure 11.11 – Container metrics for CPU, memory, and network\n\nFrom the data, it's evident that something happened to the container running the grocery store. The\n\nduration of requests through the system increased, as did the metrics showing memory and CPU\n\nutilization for the container. Identifying resource pressure in this testing environment isn't particularly\n\ninteresting beyond this.\n\nRecall that OpenTelemetry specifies resource attributes for all signals, meaning that if multiple\n\nservices are running on the same resource, host, container, or node, it would be possible to correlate\n\ninformation about those services using this resource information, meaning that if we were running\n\nmultiple applications on the same host, and one of them triggered memory pressure, it would be\n\npossible to verify its impact on other services within the same host by utilizing its resource attributes\n\nas an identifier when querying telemetry.\n\nResource information can help answer questions when, for example, a host has lost power, and there\n\nis a need to identify all services impacted by this event quickly. Another way to use this information\n\nis when two completely unrelated services are experiencing problems simultaneously. If those two\n\nservices operate on the same node, resource information will help connect the dots.\n\nExperiment #3 – unexpected shutdown\n\nIf a service exits in a forest of microservices and no one is around to observe it, does it make a\n\nsound? With the right telemetry and alert configuration in place, it certainly does. Philosophical\n\nquestions aside, services shutting down happens all the time. Ensuring that services can manage this\n\nevent gracefully is vital in dynamic environments where applications come and go as needed.\n\nExpected and unexpected shutdowns or restarts can be caused by any number of reasons. Some\n\ncommon ones are as follows:\n\nAn uncaught exception in the code causes the application to crash and exit.\n\nResources consumed by a service pass a certain threshold, causing an application to be terminated by a resource manager.\n\nA job completes its task, exiting intentionally as it terminates.\n\nExperiment\n\nThis last experiment will simulate a service exiting unexpectedly in our system to give us an idea of\n\nwhat to look for when identifying this type of failure. Using the docker kill command, the inventory\n\nservice will be shut down unexpectedly, leaving the rest of the services to respond to this failure and\n\nreport this issue.\n\nFigure 11.12 – Experiment #3 will terminate the inventory service\n\nIn a production environment, issues arising from this scenario would be mitigated by having multiple\n\ninstances of the inventory service running behind some load balancing, be it a load balancer or DNS\n\nload balancing. This would result in traffic being redirected away from the failed instance and over to\n\nthe others still in operation. For our experiment, however, a single instance is running, causing a\n\ncomplete failure of the service.\n\nHypothesis\n\nShutting down the inventory service will result in the following:\n\nAll metrics from the inventory container will stop reporting.\n\nErrors will be recorded by the grocery store; this should be visible through the request count per status code reporting status code\n\n500.\n\nLogs should report errors from the shopper container.\n\nUsing the following command, send a signal to shut down the inventory service. Note that docker\n\nkill sends the container a kill signal, whereas docker stop would send a term signal. We use kill\n\nhere to prevent the service from shutting down cleanly:\n\n$ docker kill inventory Verify\n\nWith the inventory service stopped, let's head over to the application metrics dashboard one last time\n\nto see what happened. The request count graph shows a rapid increase in requests whose response\n\ncode is 500, representing an internal server error.\n\nFigure 11.13 – The request counter shows an increase in errors\n\nOne signal we've yet to use in this chapter is logging. Look for the Logs panel at the bottom of the\n\napplication metrics dashboard to find all the logs emitted by our system. Specifically, look for an\n\nentry reporting a failed request to the grocery store such as the following, which is produced by the\n\nshopper application:\n\nFigure 11.14 – Log entry being recorded\n\nExpanding the log entry shows details about the event that caused an error. Unfortunately, the\n\nmessage request to grocery store failed isn't particularly helpful here, although notice that there\n\nis a TraceID field in the data shown. This field is adjacent to a link. Clicking on the link will take us\n\nto the corresponding trace in Jaeger, which shows us the following:\n\nFigure 11.15 – Trace confirms the grocery store is unable to contact the inventory\n\nThe trace provides more context as to what error caused it to fail, which is helpful. An exception with\n\nthe message recorded in the span provides ample details about the legacy-inventory service\n\nappearing to be missing. Lastly, the container metrics dashboard will confirm the inventory container\n\nstopped reporting metrics as per the following screenshot:\n\nFigure 11.16 – Inventory container stopped reporting metrics\n\nRestore the stopped container via the docker start command and observe as the error rate drops and\n\ntraffic is returned to normal:\n\n$ docker start inventory\n\nThere are many more scenarios that we could investigate in this chapter. However, we only have\n\nlimited time to cover these. From message queues filling up to caching problems, the world is full of\n\nproblems just waiting to be uncovered.\n\nUsing telemetry first to answer questions\n\nThese experiments are a great way to gain familiarity with telemetry. Still, it feels like cheating to\n\nknow what caused a change before referring to the telemetry to investigate a problem. A more\n\ncommon way to use telemetry is to look at it when a problem occurs without intentionally causing it.\n\nUsually, this happens when deploying new code in a system.\n\nCode changes are deployed to many services in a distributed system several times a day. This makes\n\nit challenging to figure out which change is responsible for a regression. The complexity of\n\nidentifying problematic code is compounded by the updates being deployed by different teams.\n\nUpdate the image configuration for the shopper, grocery-store, and legacy-inventory services in the\n\nDocker Compose configuration to use the following:\n\ndocker-compose.yml\n\nshopper:\n\nimage: codeboten/shopper:chapter11-example1\n\n...\n\ngrocery-store:\n\nimage: codeboten/grocery-store:chapter11-example1\n\n...\n\nlegacy-inventory:\n\nimage: codeboten/legacy-inventory:chapter11-example1\n\nUpdate the containers by running the following command in a separate terminal:\n\n$ docker compose up -d legacy-inventory grocery-store shopper\n\nWas the deployment of the new code a success? Did we make things better or worse? Let's look at\n\nwhat the data shows us. Starting with the application metrics dashboard, it doesn't look promising.\n\nRequest duration has spiked upward, and requests per second dropped significantly.\n\nFigure 11.17 – Application metrics of the deployment\n\nIt appears to be impacting both the inventory service and grocery store service, which would indicate\n\nsomething may have gone wrong in the latest deployment of the inventory service. Looking at traces,\n\nsearching for all traces shows the same increase in request duration as the graphs from the metrics.\n\nSelecting a trace and looking through the details points to a likely culprit:\n\nFigure 11.18 – A suspicious span named sleepy service\n\nIt appears the addition of an operation called sleepy service is causing all sorts of problems in the\n\nlatest deployment! With this information, we can revert the change and resolve the issue.\n\nIn addition to the previous scenario, four additional scenarios are available through published\n\ncontainers to practice your observation skills. They have unoriginal tags: chapter11-example2,\n\nchapter11-example3, chapter11-example4, and chapter11-example5. I recommend trying them all\n\nbefore looking through the scenarios folder in the companion repository to see whether you can\n\nidentify the deployed problem!\n\nSummary\n\nLearning to navigate telemetry data produced by systems comfortably takes time. Even with years of\n\nexperience, the most knowledgeable engineers can still be puzzled by unexpected changes in\n\nobservability data. The more time spent getting comfortable with the tools, the quicker it will be to\n\nget to the bottom of just what caused changes in behavior.\n\nThe tools and techniques described in this chapter can be used repeatedly to better understand exactly\n\nwhat a system is doing. With chaos engineering practices, we can improve the resilience of our\n\nsystems by identifying areas that can be improved upon under controlled circumstances. By\n\nmethodically experimenting and observing the results from our hypotheses, we can measure the\n\nimprovements as we're making them.\n\nMany tools are available for experimenting and simulating failures; learning how to use these tools\n\ncan be a powerful addition to any engineer's toolset. As we worked our way through the vast amount\n\nof data produced by our instrumented system, it's clear that having a way to correlate data across\n\nsignals is critical in quickly moving through the data.\n\nIt's also clear that generating more data is not always a good thing, as it is possible to become\n\noverwhelmed quickly or overwhelm backends. The last chapter looks at how sampling can help\n\nreduce the volume of data.\n\nChapter 12: Sampling\n\nOne of the challenges of telemetry, in general, is managing the quantity of data that can be produced\n\nby instrumentation. This can be problematic at the time of generation if the tools producing telemetry\n\nconsume too many resources. It can also be costly to transfer the data across various points of the\n\nnetwork. And, of course, the more data is produced, the more storage it consumes, and the more\n\nresources are required to sift through it at the time of analysis. The last topic we'll discuss in this\n\nbook focuses on how we can reduce the amount of data produced by instrumentation while retaining\n\nthe value and fidelity of the data. To achieve this, we will be looking at sampling. Although primarily\n\na concern of tracing, sampling has an impact across metrics and logs as well, which we'll learn about\n\nthroughout this chapter. We'll look at the following areas:\n\nConcepts of sampling, including sampling strategies, across the different signals of OpenTelemetry\n\nHow to configure sampling at the application level via the OpenTelemetry Software Development Kit (SDK)\n\nUsing the OpenTelemetry collector to sample data\n\nAlong the way, we'll look at some common pitfalls of sampling to learn how they can best be\n\navoided. Let's start with the technical requirements for the chapter.\n\nTechnical requirements\n\nAll the code for the examples in the chapter is available in the companion repository, which can be\n\ndownloaded using git with the following command. The examples are under the chapter12 directory:\n\n$ git clone https://github.com/PacktPublishing/Cloud-Native-Observability $ cd Cloud-Native-\n\nObservability/chapter12\n\nThe first example in the chapter consists of an example application that uses the OpenTelemetry\n\nPython SDK to configure a sampler. To run the code, we'll need Python 3.6 or greater installed: $\n\npython --version\n\nPython 3.8.9\n\n$ python3 --version\n\nPython 3.8.9\n\nIf Python is not installed on your system, or the installed version of Python is less than the supported\n\nversion, follow the instructions from the Python website (https://www.python.org/downloads/) to\n\ninstall a compatible version.\n\nNext, install the following OpenTelemetry packages via pip. Note that through dependency\n\nrequirements, additional packages will automatically be installed: $ pip install opentelemetry-distro \\\n\nopentelemetry-exporter-otlp\n\n$ pip freeze | grep opentelemetry\n\nopentelemetry-api==1.8.0\n\nopentelemetry-distro==0.27b0\n\nopentelemetry-exporter-otlp==1.8.0\n\nopentelemetry-exporter-otlp-proto-grpc==1.8.0\n\nopentelemetry-exporter-otlp-proto-http==1.8.0\n\nopentelemetry-instrumentation==0.27b0\n\nopentelemetry-proto==1.8.0\n\nopentelemetry-sdk==1.8.0\n\nThe second example will use the OpenTelemetry Collector, which can be downloaded from GitHub\n\ndirectly. The example will focus on the tail sampling processor, which currently resides in the\n\nopentelemetry-collector-contrib repository. The version used in this chapter can be found at the\n\nfollowing location: https://github.com/open-telemetry/opentelemetry-collector-\n\nreleases/releases/tag/v0.43.0. Download a binary that matches your current system from the available\n\nreleases. For example, the following command downloads the macOS for AMD64-compatible binary.\n\nIt also ensures the executable flag is set and runs the binary to check that things are working: $ wget -\n\nO otelcol.tar.gz https://github.com/open-telemetry/opentelemetry-collector-\n\nreleases/releases/download/v0.43.0/otelcol-contrib_0.43.0_darwin_amd64.tar.gz $ tar -xzf\n\notelcol.tar.gz otelcol-contrib\n\n$ chmod +x ./otelcol-contrib\n\n$ ./otelcol-contrib --version\n\notelcol-contrib version 0.43.0\n\nIf a package matching your environment isn't available, you can compile the collector manually. The\n\nsource is available on GitHub: https://github.com/open-telemetry/opentelemetry-collector-contrib.\n\nWith this in place, let's get started with sampling!\n\nConcepts of sampling across signals\n\nA method often used in the domain of research, the process of sampling selects a subset of data points\n\nacross a larger dataset to reduce the amount of data to be analyzed. This can be done because either\n\nanalyzing the entire dataset would be impossible, or unnecessary to achieve the research goal, or\n\nbecause it would be impractical to do so. For example, if we wanted to record how many doors on\n\naverage each car in a store parking lot has, it may be possible to go through the entire parking lot and\n\nrecord the data in its entirety. However, if the parking lot contains 20,000 cars, it may be best to\n\nselect a sample of those cars, say 2,000, and analyze that instead. There are many sampling methods\n\nused to ensure that a representational subset of the data is selected, to ensure the meaning of the data\n\nis not lost because of the sampling.\n\nMethods for sampling can be grouped as either of the following:\n\nProbabilistic (https://en.wikipedia.org/wiki/Probability_sampling): The probability of sampling is a known quantity, and that\n\nquantity is applied across all the data points in the dataset. Returning to the parking lot example, a probabilistic strategy would be\n\nto sample 10% of all cars. To accomplish this, we could record the data for every tenth car parked. In small datasets, probabilistic\n\nsampling is less effective as the variability between data points is higher.\n\nNon-probabilistic (https://en.wikipedia.org/wiki/Nonprobability_sampling): The selection of data is based on specific\n\ncharacteristics of the data. An example of this may be to choose the 2,000 cars closest to the store out of convenience. This\n\nintroduces bias into the selection process. The parking area located closest to the store may include designated spots or even spots\n\nreserved for smaller cars, therefore impacting the results.\n\nTraces\n\nSpecifically, sampling in the context of OpenTelemetry really means deciding what to do with spans\n\nthat form a particular trace. Spans in a trace are either processed or dropped, depending on the\n\nconfiguration of the sampler. Various components of OpenTelemetry are involved in carrying the\n\ndecision throughout the system:\n\nA Sampler is the starting point, allowing users to select a sampling level. Several samplers are defined in the OpenTelemetry\n\nspecification, more on this shortly.\n\nThe TracerProvider class receives a sampler as a configuration parameter. This ensures that all traces produced by the\n\nTracer provided by a specific TracerProvider are sampled consistently.\n\nOnce a trace is created, a decision is made on whether to sample the trace. This decision is stored in the SpanContext\n\nassociated with all spans in this trace. The sampling decision is propagated to all the services participating in the distributed trace\n\nvia the Propagator configured.\n\nFinally, once a span has ended, the SpanProcessor applies the sampling decision. It passes the spans for all sampled traces to\n\nthe SpanExporter. Traces that are not sampled are not exported.\n\nMetrics\n\nFor certain types of data, sampling just doesn't work. Sampling in the case of metrics may severely\n\nalter the data, rendering it effectively useless. For example, imagine recording data for each incoming\n\nrequest to a service, incrementing a counter by one with each request. Sampling this data would mean",
      "page_number": 274
    },
    {
      "number": 12,
      "title": "Sampling",
      "start_page": 291,
      "end_page": 308,
      "detection_method": "regex_chapter",
      "content": "that any increment that is not sampled would result in unaccounted requests. Values recorded as a\n\nresult would lose the meaning of the original data.\n\nA single metric data point is smaller than a single trace. This means that typically, managing metrics\n\ndata creates less overhead to process and store. I say typically here because this depends on many\n\nfactors, such as the dimensions of the data and the frequency at which data points are collected.\n\nReducing the amount of data produced by the metrics signal focuses on aggregating the data, which\n\nreduces the number of data points transmitted. It does this by combining data points rather than\n\nselecting specific points and discarding others. There is, however, one aspect of metrics where\n\nsampling comes into play: exemplars. If you recall from Chapter 2, OpenTelemetry Signals – Traces,\n\nMetrics, and Logs, exemplars are data points that allow metrics to be correlated with traces. There is\n\nno need to produce exemplars that reference unsampled traces. The details of how exemplars and\n\ntheir sampling should be configured are still being discussed in the OpenTelemetry specification as of\n\nDecember 2021. It is good to be aware that this will be a feature of OpenTelemetry in the near future.\n\nLogs\n\nAt the time of writing, there is no specification in OpenTelemetry around if or how the logging signal\n\nshould be sampled. The following shows a couple of ways that are currently being considered:\n\nOpenTelemetry provides the ability for logs to be correlated with traces. As such, it may make sense to provide a configuration\n\noption to only emit log records that are correlated with sampled traces.\n\nLog records could be sampled in the same way that traces can be configured via a sampler, to only emit a fraction of the total logs\n\n(https://github.com/open-telemetry/opentelemetry-specification/issues/2237).\n\nAn alternative to sampling for logging is aggregation. Log records that contain the same message\n\ncould be aggregated and transmitted as a single record, which could include a counter of repeated\n\nevents. As these options are purely speculative, we won't focus any additional efforts on sampling\n\nand logging in this chapter.\n\nBefore diving into the code and what samplers are available, let's get familiar with some of the\n\nsampling strategies available.\n\nSampling strategies\n\nWhen deciding on how to best configure sampling for a distributed system, the strategy selected\n\noften depends on the environment. Depending on the strategy chosen, the sampling decision is made\n\nat different points in the system, as shown in the following diagram:\n\nFigure 12.1 – Different points at which sampling decisions can take place The previous diagram shows where the\n\ndecisions to sample are made, but before choosing a strategy, we must understand what they are and when they are\n\nappropriate.\n\nHead sampling\n\nThe quickest way to decide about a trace is to decide at the very beginning whether to drop it or not;\n\nthis is known as head sampling. The application that creates the first span in a trace, the root span,\n\ndecides whether to sample the trace or not, and propagates that decision via the context to every\n\nsubsequent service called. This signals to all other participants in the trace whether they should be\n\nsending this span to a backend.\n\nHead sampling reduces the overhead for the entire system, as each application can discard\n\nunnecessary spans without computing a sampling decision. It also reduces the amount of data\n\ntransmitted, which can have a significant impact on network costs.\n\nAlthough it is the most efficient way to sample data, deciding at the beginning of the trace whether it\n\nshould be sampled or not doesn't always work. As we'll see shortly, when exploring the different\n\nsamplers available, it's possible for applications to configure sampling differently from one another.\n\nThis could cause applications to not respect the decision made by the root span, causing broken traces\n\nto be received by the backend. Figure 12.2 shows five applications interacting and combining into a\n\ndistributed system producing spans. It highlights what would happen if two applications, B and C,\n\nwere configured to sample a trace, but the other applications in the system were not:\n\nFigure 12.2 – Inconsistent sampling configuration The backend would receive four spans and some context about the\n\nsystem but would be missing four additional spans and quite a bit of information.\n\nIMPORTANT NOTE\n\nInconsistent sampler configuration is a problem that affects all sampling strategies. Configuring multiple applications in a\n\ndistributed system introduces the possibility of inconsistencies. Using a consistent sampling configuration across\n\napplications is critical.\n\nMaking a sampling decision at the very beginning of a trace can also cause valuable information to\n\nbe missed. Continuing with the example from the previous diagram, if an error occurs in application\n\nD, but the sampling decision made by application A discards the trace, that error would not be\n\nreported to the backend. An inherent problem with head sampling is that the decision is made before\n\nall the information is available.\n\nTail sampling\n\nIf making the decision at the beginning of a trace is problematic because of a lack of information,\n\nwhat about making the decision at the end of a trace? Tail sampling is another common strategy that\n\nwaits until a trace is complete before making a sampling decision. This allows the sampler to perform\n\nsome analysis on the trace to detect potentially anomalous or interesting occurrences.\n\nWith tail sampling, all the applications in a distributed system must produce and transmit the\n\ntelemetry to a destination that decides to sample the data or not. This can become costly for large\n\ndistributed systems. Depending on where the tail sampling is performed, this option may cause\n\nsignificant amounts of data to be produced and transferred over the network, which could have little\n\nvalue.\n\nAdditionally, to make sampling decisions, the sampler must buffer in memory or store the data for the\n\nentire trace until it is ready to decide. This will inevitably lead to an increase in memory and storage\n\nconsumed, depending on the size and duration of traces. As mitigation around memory concerns, a\n\nmaximum trace duration can be configured in tail sampling. However, this leads to data gaps for any\n\ntraces that never finish within that set time. This is problematic as those traces can help identify\n\nproblems within a system.\n\nProbability sampling\n\nAs discussed earlier in the chapter, probability sampling ensures that data is selected randomly,\n\nremoving bias from the data sampled. Probability sampling is somewhat different from head and tail\n\nsampling, as it is both a configuration that can be applied to those other strategies and a strategy in\n\nitself. The sampling decision can be made by each component in the system individually, so long as\n\nthe components share the same algorithm for applying the probability. In OpenTelemetry, the\n\nTraceIdRatioBased sampler (https://github.com/open-telemetry/opentelemetry-\n\nspecification/blob/main/specification/trace/sdk.md#traceidratiobased) combined with the standard\n\nrandom trace ID generator provides a mechanism for probability sampling. The decision to sample is\n\ncalculated by applying a configurable ratio to a hash of the trace ID. Since the trace ID is propagated\n\nacross the system, all components configured with the same ratio and the TraceIdRatioBased sampler\n\ncan apply the same logic at decision time independently:\n\nFigure 12.3 – Probabilistic sampling decisions can be applied at every step of the system There are other sampling\n\nstrategies available, but these are the ones we'll concern ourselves with for the remainder of this chapter.\n\nSamplers available\n\nThere are a few different options when choosing a sampler. The following options are defined in the\n\nOpenTelemetry specification and are available in all implementations:\n\nAlways on: As the name suggests, the always_on sampler samples all traces.\n\nAlways off: This sampler does not sample any traces.\n\nTrace ID ratio: The trace ID ratio sampler, as discussed earlier, is a type of probability sampler available in OpenTelemetry.\n\nParent-based: The parent-based sampler is a sampler that supports the head sampling strategy. The parent-based sampler can be\n\nconfigured with always on, always_off, or with a trace ID ratio decision as a fallback, when a sampling decision has not\n\nalready been made for a trace.\n\nUsing the OpenTelemetry Python SDK will give us a chance to put these samplers to use.\n\nSampling at the application level via the SDK\n\nAllowing applications to decide what to sample, provides a great amount of flexibility to application\n\ndevelopers and operators, as these applications are the source of the tracing data. Samplers can be\n\nconfigured in OpenTelemetry as a property of the tracer provider. In the following code, a\n\nconfigure_tracer method configures the OpenTelemetry tracing pipeline and receives Sampler as a\n\nmethod argument. This method is used to obtain three different tracers, each with its own sampling\n\nconfiguration:\n\nALWAYS_ON: A sampler that always samples.\n\nALWAYS_OFF: A sampler that never samples.\n\nTraceIdRatioBased: A probability sampler, which in the example is configured to sample traces 50% of the time.\n\nThe code then produces a separate trace using each tracer to demonstrate how sampling impacts the\n\noutput generated by ConsoleSpanExporter: sample.py\n\nfrom opentelemetry.sdk.trace import TracerProvider from opentelemetry.sdk.trace.export\n\nimport BatchSpanProcessor, ConsoleSpanExporter from opentelemetry.sdk.trace.sampling\n\nimport ALWAYS_OFF, ALWAYS_ON, TraceIdRatioBased def configure_tracer(sampler):\n\nprovider = TracerProvider(sampler=sampler)\n\nprovider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter())) return\n\nprovider.get_tracer(__name__)\n\nalways_on_tracer = configure_tracer(ALWAYS_ON) always_off_tracer =\n\nconfigure_tracer(ALWAYS_OFF) ratio_tracer = configure_tracer(TraceIdRatioBased(0.5)) with\n\nalways_on_tracer.start_as_current_span(\"always-on\") as span: span.set_attribute(\"sample\",\n\n\"always sampled\") with always_off_tracer.start_as_current_span(\"always-off\") as span:\n\nspan.set_attribute(\"sample\", \"never sampled\") with\n\nratio_tracer.start_as_current_span(\"ratio\") as span: span.set_attribute(\"sample\",\n\n\"sometimes sampled\") Run the code using the following command:\n\n$ python sample.py\n\nThe output should do one of the following:\n\nContain a trace with a span named always-on.\n\nNot contain a trace with a span named always-off.\n\nMaybe contain a trace with a span named ratio. You may need to run the code a few times to get this trace to produce output.\n\nThe following sample output is abbreviated to only show the name of the span and significant\n\nattributes: output\n\n{\n\n\"name\": \"ratio\",\n\n\"attributes\": {\n\n\"sample\": \"sometimes sampled\"\n\n},\n\n}\n\n{\n\n\"name\": \"always-on\",\n\n\"attributes\": {\n\n\"sample\": \"always sampled\"\n\n},\n\n}\n\nNote that although the example configures three different samplers, a real-world application would\n\nonly ever use one sampler. An exception to this is a single application containing multiple services\n\nwith separate sampling requirements.\n\nNOTE\n\nIn addition to configuring a sampler via code, it's also possible to configure it via the OTEL_TRACES_SAMPLER and\n\nOTEL_TRACES_SAMPLER_ARG environment variables.\n\nUsing application configuration allows us to use head sampling, but individual applications don't\n\nhave the information needed to make tail sampling decisions. For that, we need to go further down\n\nthe pipeline.\n\nUsing the OpenTelemetry Collector to sample data\n\nConfiguring the application to sample traces is great, but what if we wanted to use tail sampling\n\ninstead? The OpenTelemetry Collector provides a natural point where sampling can be performed.\n\nToday, it supports both tail sampling and probabilistic sampling via processors. As we've already\n\ndiscussed the probabilistic sampling processor in Chapter 8, The OpenTelemetry Collector, we'll\n\nfocus this section on the tail sampling processor.\n\nTail sampling processor\n\nIn addition to supporting the configuration of sampling via specifying a probabilistic sampling\n\npercentage, the tail sampling processor can make sampling decisions based on a variety of\n\ncharacteristics of a trace. It can choose to sample based on one of the following:\n\nOverall trace duration\n\nSpan attributes' values\n\nStatus code of a span\n\nTo accomplish this, the tail sampling processor supports the configuration of policies to sample\n\ntraces. To better understand how tail sampling can impact the tracing data produced by configuring a\n\nvariety of policies in the collector, let's look at the following code snippet, which configures a\n\ncollector with the following:\n\nThe OpenTelemetry protocol listener, which will receive the telemetry from an example application\n\nA logging exporter to allow us to see the tracing data in the terminal\n\nThe tail sampling processor with a policy to always sample all traces\n\nThe following code snippet contains the elements of the previous list: config/collector/config.yml\n\nreceivers:\n\notlp:\n\nprotocols:\n\ngrpc:\n\nexporters:\n\nlogging:\n\nloglevel: debug\n\nprocessors:\n\ntail_sampling:\n\ndecision_wait: 5s\n\npolicies: [{ name: always, type: always_sample }]\n\nservice:\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nprocessors: [tail_sampling]\n\nexporters: [logging]\n\nStart the collector using the following command, which includes the configuration previously shown:\n\n$ ./otelcol-contrib --config ./config/collector/config.yml Next, the ensuing code is an application that\n\nwill send multiple traces to the collector to demonstrate some of the capabilities of the tail sampling\n\nprocessor: multiple_traces.py import time\n\nfrom opentelemetry import trace\n\ntracer = trace.get_tracer_provider().get_tracer(__name__) with\n\ntracer.start_as_current_span(\"slow-span\"): time.sleep(1)\n\nfor i in range(0, 20):\n\nwith tracer.start_as_current_span(\"fast-span\"): pass\n\nOpen a new terminal and start the program using OpenTelemetry auto-instrumentation, as per the\n\nfollowing command: $ opentelemetry-instrument python multiple_traces.py Looking through the\n\noutput in the collector terminal, you should see a total of 21 traces being emitted. Let's now update\n\nthe collector configuration to only sample 10% of all traces. This can be configured via a policy, as\n\nper the following: config/collector/config.yml processors:\n\ntail_sampling:\n\ndecision_wait: 5s\n\npolicies:\n\n[\n\n{\n\nname: probability,\n\ntype: probabilistic,\n\nprobabilistic: { sampling_percentage: 10 },\n\n},\n\n]\n\nRestart the collector and run multiple_traces.py once more to see the effects of applying the new\n\npolicy. The results should show roughly 10% of traces, which in this case would be about two traces.\n\nI say roughly here because the configuration relies on probabilistic sampling using the trace\n\nidentifier. Since the trace ID is randomly generated, there is some variance in the results with such a\n\nsmall sample set. Run the command a few times if needed to see the sampling policy in action: output\n\nSpan #0\n\nTrace ID : 9581c95ae58bc8368050728f50c32f73\n\nParent ID :\n\nID : b9c3fb8838eb0f33\n\nName : fast-span\n\nKind : SPAN_KIND_INTERNAL\n\nStart time : 2021-12-28 21:29:01.144907 +0000 UTC\n\nEnd time : 2021-12-28 21:29:01.144922 +0000 UTC\n\nStatus code : STATUS_CODE_UNSET\n\nStatus message :\n\nSpan #0\n\nTrace ID : 2a8950f2365e515324c62dfdc23735ba Parent ID :\n\nID : c5217fb16c4d90ff\n\nName : fast-span\n\nKind : SPAN_KIND_INTERNAL\n\nStart time : 2021-12-28 21:29:01.14498 +0000 UTC\n\nEnd time : 2021-12-28 21:29:01.144996 +0000 UTC\n\nStatus code : STATUS_CODE_UNSET\n\nStatus message :\n\nNote that in the previous output, only the spans named fast-span were emitted. It's unfortunate,\n\nbecause the information about slow-span may be more useful to us. It's additionally possible to\n\nconfigure the tail sampling processor to combine policies to create more complex sampling decisions.\n\nFor example, you may want to continue capturing only 10% of all traces but always capture traces\n\nrepresenting operations that took longer than 1 second to complete. In this case, the following\n\ncombination of a latency-based policy with a probabilistic policy would make this possible:\n\nconfig/collector/config.yml processors:\n\ntail_sampling:\n\ndecision_wait: 5s\n\npolicies:\n\n[\n\n{\n\nname: probability,\n\ntype: probabilistic,\n\nprobabilistic: { sampling_percentage: 10 },\n\n},\n\n{ name: slow, type: latency, latency: { threshold_ms: 1000 } }, ]\n\nRestart the collector one last time and run the example code. You'll notice that both a percentage of\n\ntraces and the trace containing slow-span are visible in the output from the collector. There are other\n\ncharacteristics that can be configured, but this gives you an idea of how the tail sampling processor\n\nworks. Another example is to base the sampling decision on the status code, which is a convenient\n\nway to capture errors in a system. Another yet is to sample custom attributes, which could be used to\n\nscope the sampling to specific systems.\n\nIMPORTANT NOTE\n\nChoosing to sample traces on known characteristics introduces bias in the selection of spans that could inadvertently hide\n\nuseful telemetry. Tread carefully when configuring sampling to use non-probabilistic data as it may exclude more\n\ninformation than you'd like. Combining probabilistic and non-probabilistic sampling, as in the previous example, allows us\n\nto work around this limitation.\n\nSummary\n\nUnderstanding the different options for sampling provides us with the ability to manage the amount\n\nof data produced by our applications. Knowing the trade-offs of different sampling strategies and\n\nsome of the methods available helps decrease the level of noise in a busy environment.\n\nThe OpenTelemetry configuration and samplers available to configure sampling at the application\n\nlevel can help reduce the load and cost upfront in systems via head sampling. Configuring tail\n\nsampling at collection time provides the added benefit of making a more informed decision on what\n\nto keep or discard. This benefit comes at the added cost of having to run a collection point with\n\nsufficient resources to buffer the data until a decision can be reached.\n\nUltimately, the decisions made when configuring sampling will impact what data is available to\n\nobserve what is happening in a system. Sample too little and you may miss important events. Sample\n\ntoo much and the cost of producing telemetry for a system may be too high or the data too noisy to\n\nsearch through. Sample only for known issues and you may miss the opportunity to find\n\nabnormalities you didn't even know about.\n\nDuring development, sampling 100% of the data makes sense as the volume is low. In production, a\n\nmuch smaller percentage of data, under 10%, is often representative of the data as a whole.\n\nThe information in this chapter has given us an understanding of the concepts of sampling. It has also\n\ngiven us an idea of the trade-offs in choosing different sampling strategies. In the end, choosing the\n\nright strategy requires experimenting and tweaking as we learn more about our systems.\n\nPackt.com\n\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as\n\nindustry leading tools to help you plan your personal development and advance your career. For more\n\ninformation, please visit our website.\n\nWhy subscribe?\n\nSpend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\nFully searchable for easy access to vital information\n\nCopy and paste, print, and bookmark content\n\nDid you know that Packt offers eBook versions of every book published, with PDF and ePub files\n\navailable? You can upgrade to the eBook version at packt.com and as a print book customer, you are\n\nentitled to a discount on the eBook copy. Get in touch with us at customercare@packtpub.com for\n\nmore details.\n\nAt www.packt.com, you can also read a collection of free technical articles, sign up for a range of\n\nfree newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\n\nOther Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other books by Packt:\n\nCloud Native with Kubernetes\n\nAlexander Raul\n\nISBN: 9781838823078\n\nSet up Kubernetes and configure its authentication\n\nDeploy your applications to Kubernetes\n\nConfigure and provide storage to Kubernetes applications\n\nExpose Kubernetes applications outside the cluster\n\nControl where and how applications are run on Kubernetes\n\nSet up observability for Kubernetes\n\nBuild a continuous integration and continuous deployment (CI/CD) pipeline for Kubernetes\n\nExtend Kubernetes with service meshes, serverless, and more\n\nThe Kubernetes Bible\n\nNassim Kebbani, Piotr Tylenda, Russ McKendrick\n\nISBN: 9781838827694\n\nManage containerized applications with Kubernetes\n\nUnderstand Kubernetes architecture and the responsibilities of each component\n\nSet up Kubernetes on Amazon Elastic Kubernetes Service, Google Kubernetes Engine, and Microsoft Azure Kubernetes Service\n\nDeploy cloud applications such as Prometheus and Elasticsearch using Helm charts\n\nDiscover advanced techniques for Pod scheduling and auto-scaling the cluster\n\nUnderstand possible approaches to traffic routing in Kubernetes\n\nPackt is searching for authors like you\n\nIf you're interested in becoming an author for Packt, please visit authors.packtpub.com and apply\n\ntoday. We have worked with thousands of developers and tech professionals, just like you, to help\n\nthem share their insight with the global tech community. You can make a general application, apply\n\nfor a specific hot topic that we are recruiting an author for, or submit your own idea.\n\nShare Your Thoughts\n\nNow you've finished Cloud-Native Observability with OpenTelemetry, we'd love to hear your\n\nthoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon\n\nreview page for this book and share your feedback or leave a review on the site that you purchased it\n\nfrom.\n\nYour review is important to us and the tech community and will help us make sure we're delivering\n\nexcellent quality content.",
      "page_number": 291
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "BIRMINGHAM—MUMBAI\n\nCloud-Native Observability with OpenTelemetry\n\nCopyright © 2022 Packt Publishing\n\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or\n\ntransmitted in any form or by any means, without the prior written permission of the publisher,\n\nexcept in the case of brief quotations embedded in critical articles or reviews.\n\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information\n\npresented. However, the information contained in this book is sold without warranty, either express or\n\nimplied. Neither the author, nor Packt Publishing or its dealers and distributors, will be held liable for\n\nany damages caused or alleged to have been caused directly or indirectly by this book.\n\nPackt Publishing has endeavored to provide trademark information about all of the companies and\n\nproducts mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot\n\nguarantee the accuracy of this information.\n\nGroup Product Manager: Rahul Nair\n\nPublishing Product Manager: Shrilekha Malpani\n\nSenior Editor: Arun Nadar\n\nContent Development Editor: Sujata Tripathi\n\nTechnical Editor: Rajat Sharma\n\nCopy Editor: Safis Editing\n\nProject Coordinator: Shagun Saini\n\nProofreader: Safis Editing\n\nIndexer: Pratik Shirodkar",
      "content_length": 1323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Production Designer: Ponraj Dhandapani\n\nMarketing Coordinator: Nimisha Dua\n\nFirst published: April 2022\n\nProduction reference: 1140422\n\nPublished by Packt Publishing Ltd.\n\nLivery Place\n\n35 Livery Street\n\nBirmingham\n\nB3 2PB, UK.\n\nISBN 978-1-80107-770-5\n\nwww.packt.com",
      "content_length": 266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "To my mother, sister, and father. Thank you for teaching me to persevere in the face of adversity,\n\nalways be curious, and work hard.\n\nForeword\n\nIt has never been a better time to be a software engineer.\n\nAs engineers, we are motivated by impact and efficiency—and who can argue that both are not\n\nskyrocketing, particularly in comparison with time spent and energy invested?\n\nThese days, you can build out a scalable, elastic, distributed system to serve your code to millions of\n\nusers per day with a few clicks—without ever having to personally understand much about\n\noperations or architecture. You can write lambda functions or serverless code, hit save, and begin\n\nserving them to users immediately.\n\nIt feels like having superpowers, especially for those of us who remember the laborious times before.\n\nEvery year brings more powerful APIs and higher-level abstractions – many, many infinitely complex\n\nsystems that \"just work\" at the click of a button or the press of a key.\n\nBut when it doesn't \"just work,\" it has gotten harder than ever to untangle the reasons and understand\n\nwhy.\n\nSuperpowers don't come for free it turns out. The winds of change may be sweeping us all briskly out\n\ntoward a sea of ever-expanding options, infinite flexibility, automated resiliency, and even cost-\n\neffectiveness, but these glories have come at the price of complexity—skyrocketing, relentlessly\n\ncompounding complexity and the cognitive overload that comes with it.\n\nSystems no longer fail in predictable ways. Static dashboards are no longer a viable tool for\n\nunderstanding your systems. And though better tools will help, digging ourselves out of this hole is\n\nnot merely an issue of switching from one tool to another. We need to rethink the way software gets\n\nbuilt, shipped, and maintained, to be production-focused from day 1.\n\nFor far too long now, we have been building and shipping software in the dark. Software engineers\n\nact like all they need to do is write tests and make sure their code passes. While tests are important,\n\nall they can really do is validate the logic of your code and increase your confidence that you have\n\nnot introduced any serious regressions. Operations engineers, meanwhile, rely on monitoring checks,\n\nbut those are a blunt tool at best. Most bugs will never rise to the criticality of a paging alert, which\n\nmeans that as a system gets more mature and sophisticated, most issues will have to be found and\n\nreported by your users.",
      "content_length": 2468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "And this isn't just a problem of bugs, firefighting, or outages. This is about understanding your\n\nsoftware in the wild—as your users run your code on your infrastructure, at a given time. Production\n\nremains far too much of a black box for too many people, who are then forced to try and reason about\n\nit by reading lines of code and using elaborate mental models.\n\nBecause we've all been shipping code blindly, all this time, we ship changes we don't fully\n\nunderstand to a production system that is a hairball of changes we've never truly understood. We've\n\nbeen shipping blindly for years and years now, leaving SRE teams and ops teams to poke at the black\n\nboxes and try to clean up the mess—all the while still blindfolded. The fact that anything has ever\n\nworked is a testament to the creativity and dedication of these teams.\n\nA funny thing starts happening when people begin instrumenting their code for observability and\n\ninspecting it in production—regularly, after every deployment, as a habit. You find bugs everywhere,\n\nbugs you never knew existed. It's like picking up a rock and watching all the little nasties lurking\n\nunderneath scuttle away from the light.\n\nWith monitoring tools and aggregates, we were always able to see that errors existed, but we had no\n\nway of correlating them to an event or figuring out what was different about the erroring requests.\n\nNow, all of a sudden, we are able to look at an error spike and say, \"Ah! All of these errors are for\n\nrequests coming from clients running app version 1.63, calling the /export endpoint, querying the\n\nprimaries for mysql-shard3, shard5, and shard7, with a payload of over 10 KB, and timing out after\n\n15 seconds.\" Or we can pull up a trace and see that one of the erroring requests was issuing thousands\n\nof serial database queries in a row. So many gnarly bugs and opaque behaviors become shallow once\n\nyou can visualize them. It's the most satisfying experience in the world.\n\nBut yes, you do have to instrument your code. (Auto-instrumentation is about as effective as\n\nautomated code commenting.) So let's talk about that.\n\nI can hear you now—\"Ugh, instrumentation!\" Most people would rather get bitten by a rattlesnake\n\nthan refactor their logging and instrumentation code. I know this, and so does every vendor under the\n\nsun. This is why even legacy logging companies are practically printing money. Once they get your\n\ndata flowing in, it takes an act of God to move it or turn it off.\n\nThis is a big part of the reason we, as an industry, are so behind when it comes to public, reusable\n\nstandards and tooling for instrumentation and observability, which is why I am so delighted to\n\nparticipate in the push for OpenTelemetry. Yes, it's in the clumsy toddler years of technological\n\nadvancement. But it will get better. It has gotten better. I was cynical about OTel in the early days,\n\nbut the community excitement and uptake have exceeded my expectations at every step. As well it\n\nshould. Because the promise of OpenTelemetry is that you may need to instrument your code once,\n\nbut only once. And then you can move from vendor to vendor without re-instrumenting.",
      "content_length": 3154,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "This means vendors will have to compete for your business on features, usability, and cost-\n\neffectiveness, instead of vendor lock-in. OTel has the potential to finally break this stranglehold—to\n\nmake it so you only instrument once, and you can move from vendor to vendor with just a few lines\n\nof configuration changes. This is brilliant—this changes everything. This is one battle you should\n\nabsolutely join and fight.\n\nSoftware systems aren't going to get simpler anytime soon. Yet the job of developing and maintaining\n\nsoftware may paradoxically be poised to get faster and easier, by forcing us to finally adopt better\n\nreal-time instrumentation and telemetry. Going from monitoring to observability is like the difference\n\nbetween visual flight rating (VFR) and instrument flight rating (IFR) for pilots. Yeah, learning to\n\nfly (or code) by instrumentation feels a little strange at first, but once you master it, you can fly so\n\nmuch faster, farther, and more safely than ever before.\n\nIt's not just about observability. There are lots of dovetailing trends in tech right now—feature flags,\n\nchaos engineering, progressive deployment, and so on—all of which center production, and focus on\n\nshrinking the distance and tightening the feedback loops between dev and prod. Together they deliver\n\ncompounding benefits that help teams move swiftly and safely, devoting more of their time to solving\n\nnew and interesting puzzles that move the business forward, and less time to toil and yak shaving.\n\nIt's not just about observability... but it starts with observability. The ability to see what is happening\n\nis the most important feedback loop of all.\n\nAnd observability starts with instrumentation.\n\nSo, here we go.\n\nCharity Majors\n\nCTO, Honeycomb\n\nContributors\n\nAbout the author\n\nAlex Boten is a senior staff software engineer at Lightstep and has spent the last 10 years helping\n\norganizations adapt to a cloud-native landscape. From building core network infrastructure to mobile\n\nclient applications and everything in between, Alex has first-hand knowledge of how complex\n\ntroubleshooting distributed applications is.\n\nThis led him to the domain of observability and contributing to open source projects in the space. A\n\ncontributor, approver, and maintainer in several aspects of OpenTelemetry, Alex has helped evolve\n\nthe project from its early days in 2019 into the massive community effort that it is today.",
      "content_length": 2422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "More than anything, Alex loves making sense of the technology around us and sharing his learnings\n\nwith others.\n\nAbout the reviewer\n\nYuri Grinshteyn strongly believes that reliability is a key feature of any service and works to\n\nadvocate for site reliability engineering principles and practices. He graduated from Tufts University\n\nwith a degree in computer engineering and has worked in monitoring, diagnostics, observability, and\n\nreliability throughout his career. Currently, he is a site reliability engineer at Google Cloud, where he\n\nworks with customers to help them achieve appropriate reliability for their services; previously, he\n\nworked at Oracle, Compuware, Hitachi Consulting, and Empirix. You can find his work on YouTube,\n\nMedium, and GitHub. He and his family live just outside of San Francisco and love taking advantage\n\nof everything California has to offer.",
      "content_length": 879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Table of Contents\n\nPreface",
      "content_length": 26,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Section 1: The Basics\n\nChapter 1: The History and Concepts of Observability\n\nUnderstanding cloud-native applications\n\nLooking at the shift to DevOps\n\nReviewing the history of observability\n\nCentralized logging\n\nUsing metrics and dashboards\n\nApplying tracing and analysis\n\nUnderstanding the history of OpenTelemetry\n\nOpenTracing\n\nOpenCensus\n\nObservability for cloud-native software\n\nUnderstanding the concepts of OpenTelemetry\n\nSignals\n\nPipelines\n\nResources\n\nContext propagation\n\nSummary",
      "content_length": 486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Chapter 2: OpenTelemetry Signals – Traces, Metrics, and Logs\n\nTechnical requirements\n\nTraces\n\nAnatomy of a trace\n\nDetails of a span\n\nAdditional considerations\n\nMetrics\n\nAnatomy of a metric\n\nData point types\n\nExemplars\n\nAdditional considerations\n\nLogs\n\nAnatomy of a log\n\nCorrelating logs\n\nAdditional considerations\n\nSemantic conventions\n\nSummary",
      "content_length": 344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Chapter 3: Auto-Instrumentation\n\nTechnical requirements\n\nWhat is auto-instrumentation?\n\nChallenges of manual instrumentation\n\nComponents of auto-instrumentation\n\nLimits of auto-instrumentation\n\nBytecode manipulation\n\nOpenTelemetry Java agent\n\nRuntime hooks and monkey patching\n\nInstrumenting libraries\n\nThe Instrumentor interface\n\nWrapper script\n\nSummary",
      "content_length": 354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Section 2: Instrumenting an Application\n\nChapter 4: Distributed Tracing – Tracing Code Execution\n\nTechnical requirements\n\nConfiguring the tracing pipeline\n\nGetting a tracer\n\nGenerating tracing data\n\nThe Context API\n\nSpan processors\n\nEnriching the data\n\nResourceDetector\n\nSpan attributes\n\nSpanKind\n\nPropagating context\n\nAdditional propagator formats\n\nComposite propagator\n\nRecording events, exceptions, and status\n\nEvents\n\nExceptions\n\nStatus\n\nSummary",
      "content_length": 449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Chapter 5: Metrics – Recording Measurements\n\nTechnical requirements\n\nConfiguring the metrics pipeline\n\nObtaining a meter\n\nPush-based and pull-based exporting\n\nChoosing the right OpenTelemetry instrument\n\nCounter\n\nAsynchronous counter\n\nAn up/down counter\n\nAsynchronous up/down counter\n\nHistogram\n\nAsynchronous gauge\n\nDuplicate instruments\n\nCustomizing metric outputs with views\n\nFiltering\n\nDimensions\n\nAggregation\n\nThe grocery store\n\nNumber of requests\n\nRequest duration\n\nConcurrent requests\n\nResource consumption\n\nSummary",
      "content_length": 521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Chapter 6: Logging – Capturing Events\n\nTechnical requirements\n\nConfiguring OpenTelemetry logging\n\nProducing logs\n\nUsing LogEmitter\n\nThe standard logging library\n\nA logging signal in practice\n\nDistributed tracing and logs\n\nOpenTelemetry logging with Flask\n\nLogging with WSGI middleware\n\nResource correlation\n\nSummary",
      "content_length": 315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Chapter 7: Instrumentation Libraries\n\nTechnical requirements\n\nAuto-instrumentation configuration\n\nOpenTelemetry distribution\n\nOpenTelemetry configurator\n\nEnvironment variables\n\nCommand-line options\n\nRequests library instrumentor\n\nAdditional configuration options\n\nManual invocation\n\nDouble instrumentation\n\nAutomatic configuration\n\nConfiguring resource attributes\n\nConfiguring traces\n\nConfiguring metrics\n\nConfiguring logs\n\nConfiguring propagation\n\nRevisiting the grocery store\n\nLegacy inventory\n\nGrocery store\n\nShopper\n\nFlask library instrumentor\n\nAdditional configuration options\n\nFinding instrumentation libraries\n\nOpenTelemetry registry\n\nopentelemetry-bootstrap\n\nSummary",
      "content_length": 674,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Section 3: Using Telemetry Data\n\nChapter 8: OpenTelemetry Collector\n\nTechnical requirements\n\nThe purpose of OpenTelemetry Collector\n\nUnderstanding the components of OpenTelemetry Collector\n\nReceivers\n\nProcessors\n\nExporters\n\nExtensions\n\nAdditional components\n\nTransporting telemetry via OTLP\n\nEncodings and protocols\n\nAdditional design considerations\n\nUsing OpenTelemetry Collector\n\nConfiguring the exporter\n\nConfiguring the collector\n\nModifying spans\n\nFiltering metrics\n\nSummary",
      "content_length": 478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Chapter 9: Deploying the Collector\n\nTechnical requirements\n\nCollecting application telemetry\n\nDeploying the sidecar\n\nSystem-level telemetry\n\nDeploying the agent\n\nConnecting the sidecar and the agent\n\nAdding resource attributes\n\nCollector as a gateway\n\nAutoscaling\n\nOpenTelemetry Operator\n\nSummary",
      "content_length": 296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Chapter 10: Configuring Backends\n\nTechnical requirements\n\nBackend options for analyzing telemetry data\n\nTracing\n\nMetrics\n\nLogging\n\nRunning in production\n\nHigh availability\n\nScalability\n\nData retention\n\nPrivacy regulations\n\nSummary",
      "content_length": 230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Chapter 11: Diagnosing Problems\n\nTechnical requirements\n\nIntroducing a little chaos\n\nExperiment #1 – increased latency\n\nExperiment #2 – resource pressure\n\nExperiment #3 – unexpected shutdown\n\nUsing telemetry first to answer questions\n\nSummary",
      "content_length": 242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Chapter 12: Sampling\n\nTechnical requirements\n\nConcepts of sampling across signals\n\nTraces\n\nMetrics\n\nLogs\n\nSampling strategies\n\nSamplers available\n\nSampling at the application level via the SDK\n\nUsing the OpenTelemetry Collector to sample data\n\nTail sampling processor\n\nSummary\n\nOther Books You May Enjoy",
      "content_length": 303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Preface\n\nCloud-Native Observability with OpenTelemetry is a guide to helping you look for answers to\n\nquestions about your applications. This book teaches you how to produce telemetry from your\n\napplications using an open standard to retain control of data. OpenTelemetry provides the tools\n\nnecessary for you to gain visibility into the performance of your services. It allows you to instrument\n\nyour application code through vendor-neutral APIs, libraries and tools.\n\nBy reading Cloud-Native Observability with OpenTelemetry, you'll learn about the concepts and\n\nsignals of OpenTelemetry - traces, metrics, and logs. You'll practice producing telemetry for these\n\nsignals by configuring and instrumenting a distributed cloud-native application using the\n\nOpenTelemetry API. The book also guides you through deploying the collector, as well as telemetry\n\nbackends necessary to help you understand what to do with the data once it's emitted. You'll look at\n\nvarious examples of how to identify application performance issues through telemetry. By analyzing\n\ntelemetry, you'll also be able to better understand how an observable application can improve the\n\nsoftware development life cycle.\n\nBy the end of this book, you'll be well-versed with OpenTelemetry, be able to instrument services\n\nusing the OpenTelemetry API to produce distributed traces, metrics and logs, and more.",
      "content_length": 1376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Who this book is for\n\nThis book is for software engineers and systems operators looking to better understand their\n\ninfrastructure, services, and applications by using telemetry data like never before. Working\n\nknowledge of Python programming is assumed for the example applications you'll build and\n\ninstrument using the OpenTelemetry API and SDK. Some familiarity with Go programming, Linux,\n\nDocker, and Kubernetes is preferable to help you set up additional components in various examples\n\nthroughout the book.",
      "content_length": 514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "What this book covers\n\nChapter 1, The History and Concepts of Observability, provides an overview of the evolution of\n\nobservability. It describes the challenges and fragmentation that ultimately created the need for an\n\nopen standard. It provides an overview of both OpenTracing and OpenCensus. The last section of this\n\nchapter dives into OpenTelemetry, how it started, and how it got to where it is today. This concepts\n\nchapter will provide you with an overview of different concepts vital in understanding\n\nOpenTelemetry. This chapter introduces signals before going over the pipeline for generating,\n\nprocessing, and exporting telemetry. The resources section will describe the purpose of resources.\n\nChapter 2, OpenTelemetry Signals – Traces, Metrics, and Logs, describes the different signals that\n\ncomprise OpenTelemetry: traces, metrics, and logs. It begins by giving you an understanding of the\n\nconcepts of distributed tracing by defining spans, traces, and context propagation. The following\n\nsection explores metrics by looking at the different measurements and the instruments\n\nOpenTelemetry offers to capture this information. The section on logging describes how logging fits\n\nin with the other signals in OpenTelemetry. Semantic conventions are also covered in this chapter to\n\nunderstand their significance and role in each signal.\n\nChapter 3, Auto-Instrumentation, explains the challenges of manual instrumentation and how the\n\nOpenTelemetry project sets out to solve those challenges. After that, you will dive into the mechanics\n\nof auto instrumentation in different languages.\n\nChapter 4, Distributed Tracing – Tracing Code Execution, begins by introducing the grocery store\n\napplication we will instrument throughout the book. You will then start using the OpenTelemetry\n\nAPIs to configure a tracing pipeline and its various components: the tracer provider, span processor,\n\nand exporter. After obtaining a tracer, you will instrument code to generate traces. The remainder of\n\nthe chapter will discuss augmenting that tracing data with attributes, events, links, statuses, and\n\nexceptions.\n\nChapter 5, Metrics – Recording Measurements, teaches you how to capture application metrics. You\n\nwill begin by configuring the components of a metrics pipeline: the meter provider, meter, and\n\nexporter. The chapter then describes the instruments available in OpenTelemetry to collect metrics\n\nbefore using each one in the context of the grocery store application.\n\nChapter 6, Logging – Capturing Events, covers logging, the last of the core signals of\n\nOpenTelemetry discussed in this book. The chapter walks you through configuring the components of\n\nthe logging pipeline to emit telemetry. You will then use existing logging libraries to enhance logged\n\nevents through correlation with OpenTelemetry.",
      "content_length": 2819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Chapter 7, Instrumentation Libraries, teaches you how to use instrumentation libraries to instrument\n\nthe grocery store application automatically after learning how to do so manually. Using auto\n\ninstrumentation and environment variables supported by OpenTelemetry, this chapter shows you how\n\nto obtain telemetry from your code quickly.\n\nChapter 8, OpenTelemetry Collector, explores another core component that OpenTelemetry provides:\n\nthe OpenTelemetry Collector. The Collector allows users to collect and aggregate data before\n\ntransmitting it to various backends. This chapter describes the concepts present in the Collector,\n\npresents its use cases, and explains the challenges it solves. After learning about the OpenTelemetry\n\nProtocol (OTLP), you will modify the grocery store application to emit telemetry to the collector via\n\nOTLP.\n\nChapter 9, Deploying the Collector, puts the OpenTelemetry Collector to work in a Kubernetes\n\nenvironment in various deployment scenarios. You will use Kubernetes to deploy the Collector as a\n\nsidecar, agent, and gateway to collect application-level and system-level telemetry.\n\nChapter 10, Configuring Backends, teaches you about various open source telemetry backend options\n\nto store and visualize data. This chapter explores using OpenTelemetry with Zipkin, Jaeger,\n\nPrometheus, and Loki utilizing a local environment. You will configure exporters in application code\n\nand the OpenTelemetry Collector to emit data to these backends. After instrumenting and collecting\n\nall the telemetry from applications, it's finally time to start using this information to identify issues in\n\na system.\n\nChapter 11, Diagnosing Problems, dives into techniques used to correlate data across the different\n\nOpenTelemetry signals to identify the root cause of common problems in production effectively. This\n\nchapter introduces you to chaos engineering and tools to generate synthetic loads and service\n\ninterruptions to produce different scenarios.\n\nChapter 12, Sampling, explains the concept of sampling and how it applies to distributed tracing.\n\nHead, tail, and probability sampling strategies are introduced in this chapter. You will configure\n\nsampling using the OpenTelemetry APIs and the OpenTelemetry Collector, comparing the results of\n\ndifferent sampling configurations.\n\nTo get the most out of this book\n\nThe examples in this book were developed on macOS x86-64 using versions of Python ranging from\n\n3.6 to 3.9. The latest version of OpenTelemetry for Python tested is version 1.10.0, which includes\n\nexperimental support for both metrics and logging. It's likely that the API will change in subsequent\n\nreleases, so be aware of the version installed as you go through the examples. Consult the changelog",
      "content_length": 2747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "of the OpenTelemetry Python repository (https://github.com/open-telemetry/opentelemetry-\n\npython/blob/main/CHANGELOG.md) for the latest updates.\n\nMany examples in the book rely on Docker and Docker Compose to deploy environments locally. As\n\nof January 2022, the license for Docker Desktop still allows users to install it for free for personal\n\nuse, education, and non-commercial open source projects. If the licensing prevents you from using\n\nDocker Desktop, there are alternatives available.\n\nIf you are using the digital version of this book, we advise you to type the code yourself or\n\naccess the code from the book's GitHub repository (a link is available in the next section).\n\nDoing so will help you avoid any potential errors related to the copying and pasting of code.\n\nDownload the example code files\n\nYou can download the example code files for this book from GitHub at\n\nhttps://github.com/PacktPublishing/Cloud-Native-Observability. If there's an update to the code, it\n\nwill be updated in the GitHub repository.\n\nWe also have other code bundles from our rich catalog of books and videos available at\n\nhttps://github.com/PacktPublishing/. Check them out!\n\nDownload the color images\n\nWe also provide a PDF file that has color images of the screenshots and diagrams used in this book.\n\nYou can download it here: https://static.packt-cdn.com/downloads/9781801077705_ColorImages.pdf.\n\nConventions used",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "There are a number of text conventions used throughout this book.\n\nCode in text: Indicates code words in text, database table names, folder names, filenames, file\n\nextensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: \"The\n\ncode then calls the global set_meter_provider method to set the meter provider for the entire\n\napplication.\"\n\nA block of code is set as follows:\n\nfrom opentelemetry._metrics import set_meter_provider\n\nfrom opentelemetry.sdk._metrics import MeterProvider\n\nfrom opentelemetry.sdk.resources import Resource\n\ndef configure_meter_provider():\n\nprovider = MeterProvider(resource=Resource.create())\n\nset_meter_provider(provider)\n\nif __name__ == \"__main__\":\n\nconfigure_meter_provider()\n\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines or items\n\nare set in bold:\n\nfrom opentelemetry._metrics import get_meter_provider, set_meter_provider\n\n...\n\nif __name__ == \"__main__\":\n\nconfigure_meter_provider()\n\nmeter = get_meter_provider().get_meter(\n\nname=\"metric-example\",\n\nversion=\"0.1.2\",\n\nschema_url=\" https://opentelemetry.io/schemas/1.9.0\",\n\n)\n\nAny command-line input or output is written as follows:\n\n$ git clone https://github.com/PacktPublishing/Cloud-Native-Observability\n\n$ cd Cloud-Native-Observability/chapter7\n\nBold: Indicates a new term, an important word, or words that you see onscreen. For instance, words\n\nin menus or dialog boxes appear in bold. Here is an example: \"Search for traces by clicking the Run\n\nQuery button.\"\n\nTIPS OR IMPORTANT NOTES",
      "content_length": 1548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Appear like this.\n\nGet in touch\n\nFeedback from our readers is always welcome.\n\nGeneral feedback: If you have questions about any aspect of this book, email us at\n\ncustomercare@packtpub.com and mention the book title in the subject of your message.\n\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do\n\nhappen. If you have found a mistake in this book, we would be grateful if you would report this to us.\n\nPlease visit www.packtpub.com/support/errata and fill in the form.\n\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would be\n\ngrateful if you would provide us with the location address or website name. Please contact us at\n\ncopyright@packt.com with a link to the material.\n\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you\n\nare interested in either writing or contributing to a book, please visit authors.packtpub.com.\n\nShare Your Thoughts\n\nOnce you've read Cloud-Native Observability with OpenTelemetry, we'd love to hear your thoughts!\n\nPlease click here to go straight to the Amazon review page for this book and share your feedback.\n\nYour review is important to us and the tech community and will help us make sure we're delivering\n\nexcellent quality content.",
      "content_length": 1304,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Section 1: The Basics\n\nIn this part, you will learn about the origin of OpenTelemetry and why it was needed. We will then\n\ndive into the various components and concepts of OpenTelemetry.\n\nThis part of the book comprises the following chapters:\n\nChapter 1, The History and Concepts of Observability\n\nChapter 2, OpenTelemetry Signals: Traces, Metrics, and Logs\n\nChapter 3, Auto-Instrumentation",
      "content_length": 391,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Chapter 1: The History and Concepts of Observability\n\nThe term observability has only been around in the software industry for a short time, but the\n\nconcepts and goals it represents have been around for much longer. Indeed, ever since the earliest\n\ndays of computing, programmers have been trying to answer the question: is the system doing what I\n\nthink it should be?\n\nFor some, observability consists of buying a one-size-fits-all solution that includes logs, metrics, and\n\ntraces, then configuring some off-the-shelf integrations and calling it a day. These tools can be used\n\nto increase visibility into a piece of software's behavior by providing mechanisms to produce and\n\ncollect telemetry. The following are some examples of telemetry that can be added to a system:\n\nKeeping a count of the number of requests received\n\nAdding a log entry when an event occurs\n\nRecording a value for current memory consumption on a machine\n\nTracing a request from a client all the way to a backend service\n\nHowever, producing high-quality telemetry is only one part of the observability challenge. The other\n\npart is ensuring that events occurring across the different types of telemetry can be correlated in\n\nmeaningful ways during analysis. The goal of observability is to answer questions that you may have\n\nabout the system:\n\nIf a problem occurred in production, what evidence would you have to be able to identify it?\n\nWhy is this service suddenly overwhelmed when it was fine just a minute ago?\n\nIf a specific condition from a client triggers an anomaly in some underlying service, would you know it without customers or\n\nsupport calling you?\n\nThese are some of the questions that the domain of observability can help answer. Observability is\n\nabout empowering the people who build and operate distributed applications to understand their\n\ncode's behavior while running in production. In this chapter, we will explore the following:\n\nUnderstanding cloud-native applications\n\nLooking at the shift to DevOps\n\nReviewing the history of observability\n\nUnderstanding the history of OpenTelemetry\n\nUnderstanding the concepts of OpenTelemetry\n\nBefore we begin looking at the history of observability, it's important to understand the changes in the\n\nsoftware industry that have led to the need for observability in the first place. Let's start with the shift\n\nto the cloud.",
      "content_length": 2362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Understanding cloud-native applications\n\nThe way applications are built and deployed has drastically changed in the past few years with the\n\nincreased adoption of the internet. An unprecedented increase in demand for services (for example,\n\nstreaming media, social networks, and online shopping) powered by software has raised expectations\n\nfor those services to be readily available. In addition, this increase in demand has fueled the need for\n\ndevelopers to be able to scale their applications quickly. Cloud providers, such as Microsoft, Google,\n\nand Amazon, offer infrastructure to run applications at the click of a button and at a fraction of the\n\ncost, and reduce the risk of deploying servers in traditional data centers. This enables developers to\n\nexperiment more freely and reach a wider audience. Alongside this infrastructure, these cloud\n\nproviders also offer managed services for databases, networking infrastructure, message queues, and\n\nmany other services that, in the past, organizations would control internally.\n\nOne of the advantages these cloud-based providers offer is freeing up organizations to focus on the\n\ncode that matters to their businesses. This replaces costly and time-consuming hardware\n\nimplementations, or operating services they lack expertise in. To take full advantage of cloud\n\nplatforms, developers started looking at how applications that were originally developed as monoliths\n\ncould be re-architected to take advantage of cloud platforms. The following are challenges that could\n\nbe encountered when deploying monoliths to a cloud provider:\n\nScaling a monolith is traditionally done by increasing the number of resources available to the monolith, also known as vertical\n\nscaling. Vertically scaling applications can only go as far as the largest available resource offered by a cloud provider.\n\nImproving the reliability of a monolith means deploying multiple instances to handle multiple failures, thus avoiding downtime.\n\nThis is also known as horizontal scaling. Depending on the size of the monolith, this could quickly ramp up costs. This can also be\n\nwasteful if not all components of the monolith need to be replicated.\n\nThe specific challenges of building applications on cloud platforms have led developers to\n\nincreasingly adopt a service-oriented architecture, or microservice architecture, that organizes\n\napplications as loosely coupled services, each with limited scope. The following figure shows a\n\nmonolith architecture on the left, where all the services in the application are tightly coupled and\n\noperate within the same boundary. In contrast, the microservices architecture on the right shows us\n\nthat the services are loosely coupled, and each service operates independently:",
      "content_length": 2745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Figure 1.1 – Monolith versus microservices architecture\n\nApplications built using microservices architecture provide developers with the ability to scale only\n\nthe components needed to handle the additional load, meaning horizontal scaling becomes a much\n\nmore attractive option. As it often does, a new architecture comes with its own set of trade-offs and\n\nchallenges. The following are some of the new challenges cloud-native architecture presents that did\n\nnot exist in traditional monolithic systems:\n\nLatency introduced where none existed before, causing applications to fail in unexpected ways.\n\nDependencies can and will fail, so applications must be built defensively to minimize cascading failures.\n\nManaging configuration and secrets across services is difficult.\n\nService orchestration becomes complex.\n\nWith this change in architecture, the scope of each application is reduced significantly, making it\n\neasier to understand the needs of scaling each component. However, the increased number of\n\nindependent services and added complexity also creates challenges for traditional operations (ops)\n\nteams, meaning organizations would also need to adapt.\n\nLooking at the shift to DevOps\n\nThe shift to microservices has, in turn, led to a shift in how development teams are organized. Instead\n\nof a single large team managing a monolithic application, many teams each manage their own\n\nmicroservices. In traditional software development, a software development team would normally\n\nhand off the software once it was deemed complete. The handoff would be to an operations team,\n\nwho would deploy the software and operate it in a production environment. As the number of\n\nservices and teams grew, organizations found themselves growing their operations teams to",
      "content_length": 1767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "unmanageable sizes, and quite often, those teams were still unable to keep up with the demands of\n\nthe changing software.\n\nThis, in turn, led to an explosion of development teams that began the transition from the traditional\n\ndevelopment and operations organization toward the use of new hybrid DevOps teams. Using the\n\nDevOps approach, development teams write, test, build, package, deploy, and operate the code they\n\ndevelop. This ownership of the code through all stages of its life cycle empowers many developers\n\nand organizations to accelerate their feature development. This approach, of course, comes with\n\ndifferent challenges:\n\nIncreased dependencies across development teams mean it's possible that no one has a full picture of the entire application.\n\nKeeping track of changes across an organization can be difficult. This makes the answer to the \"what caused this outage?\"\n\nquestion more challenging to find.\n\nIndividual teams must become familiar with many more tools. This can lead to too much focus on the\n\ntools themselves, rather than on their purpose. The quick adoption of DevOps creates a new problem.\n\nWithout the right amount of visibility across the systems managed by an organization, teams are\n\nstruggling to identify the root causes of issues encountered. This can lead to longer and more frequent\n\noutages, severely impacting the health and happiness of people across organizations. Let's look at\n\nhow the methods of observing systems have evolved to adapt to this changing landscape.\n\nReviewing the history of observability\n\nIn many ways, being able to understand what a computer is doing is both fun and challenging when\n\nworking with software. The ability to understand how systems are behaving has gone through quite a\n\nfew iterations since the early 2000s. Many different markets have been created to solve this need,\n\nsuch as systems monitoring, log management, and application performance monitoring. As is often\n\nthe case, when new challenges come knocking, the doors of opportunity open to those willing to\n\ntackle those challenges. Over the same period, countless vendors and open source projects have\n\nsprung up to help people who are building and operating services in managing their systems. The\n\nterm observability, however, is a recent addition to the software industry and comes from control\n\ntheory.\n\nWikipedia (https://en.wikipedia.org/wiki/Observability) defines observability as:",
      "content_length": 2428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "\"In control theory, observability is a measure of how well internal states of a system can be\n\ninferred from knowledge of its external outputs.\"\n\nObservability is an evolution of its predecessors, built on lessons learned through years of experience\n\nand trial and error. To better understand where observability is today, it's important to understand\n\nwhere some of the methods used today by cloud-native application developers come from, and how\n\nthey have changed over time. We'll start by looking at the following:\n\nCentralized logging\n\nMetrics and dashboards\n\nTracing and analysis\n\nCentralized logging\n\nOne of the first pieces of software a programmer writes when learning a new language is a form of\n\nobservability: \"Hello, World!\". Printing some text to the terminal is usually one of the quickest ways\n\nto provide users with feedback that things are working, and that's why \"Hello, World\" has been a\n\ntradition in computing since the late 1960s.\n\nOne of my favorite methods for debugging is still to add print statements across the code when things\n\naren't working. I've even used this method to troubleshoot an application distributed across multiple\n\nservers before, although I can't say it was my proudest moment, as it caused one of our services to go\n\ndown temporarily because of a typo in an unfamiliar editor. Print statements are great for simple\n\ndebugging, but unfortunately, this only scales so far.\n\nOnce an application is large enough or distributed across enough systems, searching through the logs\n\non individual machines is not practical. Applications can also run on ephemeral machines that may\n\nno longer be present when we need those logs. Combined, all of this created a need to make the logs\n\navailable in a central location for persistent storage and searchability, and thus centralized logging\n\nwas born.\n\nThere are many available vendors that provide a destination for logs, as well as features around\n\nsearching, and alerting based on those logs. There are also many open source projects that have tried\n\nto tackle the challenges of standardizing log formats, providing mechanisms for transport, and storing\n\nthe logs. The following are some of these projects:\n\nFluentd – https://www.fluentd.org\n\nLogstash – https://github.com/elastic/logstash\n\nApache Flume – https://flume.apache.org",
      "content_length": 2317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Centralized logging additionally provides the opportunity to produce metrics about the data across\n\nthe entire system.\n\nUsing metrics and dashboards\n\nMetrics are possibly the most well-known of the tools available in the observability space. Think of\n\nthe temperature in a thermometer, the speed on the odometer of a car, or the time on a watch. We\n\nhumans love measuring and quantifying things. From the early days of computing, being able to keep\n\ntrack of how resources were utilized was critical in ensuring that multi-user environments provided a\n\ngood user experience for all users of the system.\n\nNowadays, measuring application and system performance via the collection of metrics is common\n\npractice in software development. This data is converted into graphs to generate meaningful\n\nvisualizations for those in charge of monitoring the health of a system.\n\nThese metrics can also be used to configure alerting when certain thresholds have been reached, such\n\nas when an error rate becomes greater than an acceptable percentage. In certain environments,\n\nmetrics are used to automate workflows as a reaction to changes in the system, such as increasing the\n\nnumber of application instances or rolling back a bad deployment. As with logging, over time, many\n\nvendors and projects provided their own solutions to metrics, dashboards, monitoring, and alerting.\n\nSome of the open source projects that focus on metrics are as follows:\n\nPrometheus – https://prometheus.io\n\nStatsD – https://github.com/statsd/statsd\n\nGraphite – https://graphiteapp.org\n\nGrafana – https://github.com/grafana/grafana\n\nLet's now look at tracing and analysis.\n\nApplying tracing and analysis\n\nTracing applications means having the ability to run through the application code and ensure it's\n\ndoing what is expected. This can often, but not always, be achieved in development using a debugger\n\nsuch as GDB (https://www.gnu.org/software/gdb/) or PDB\n\n(https://docs.python.org/3/library/pdb.html) in Python. This becomes impossible when debugging an\n\napplication that is spread across multiple services on different hosts across a network. Researchers at\n\nGoogle published a white paper on a large-scale distributed tracing system built internally: Dapper\n\n(https://research.google/pubs/pub36356/). In this paper, they describe the challenges of distributed",
      "content_length": 2334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "systems, as well as the approach that was taken to address the problem. This research is the basis of\n\ndistributed tracing as it exists today. After the paper was published, several open source projects\n\nsprung up to provide users with the tools to trace and visualize applications using distributed tracing:\n\nOpenTracing – https://opentracing.io\n\nOpenCensus – https://opencensus.io\n\nZipkin – https://zipkin.io\n\nJaeger – https://www.jaegertracing.io\n\nAs you can imagine, with so many tools, it can be daunting to even know where to begin on the\n\njourney to making a system observable. Users and organizations must spend time and effort upfront\n\nto even get started. This can be challenging when other deadlines are looming. Not only that, but the\n\ntime investment needed to instrument an application can be significant depending on the complexity\n\nof the application, and the return on that investment sometimes isn't made clear until much later. The\n\ntime and money invested, as well as the expertise required, can make it difficult to change from one\n\ntool to another if the initial implementation no longer fits your needs as the system evolves.\n\nSuch a wide array of methods, tools, libraries, and standards has also caused fragmentation in the\n\nindustry and the open source community. This has led to libraries supporting one format or another.\n\nThis leaves it up to the user to fix any gaps within the environments themselves. This also means\n\nthere is effort required to maintain feature parity across different projects. All of this could be\n\naddressed by bringing the people working in these communities together.\n\nWith a better understanding of different tools at the disposal of application developers, their\n\nevolution, and their role, we can start to better appreciate the scope of what OpenTelemetry is trying\n\nto solve.\n\nUnderstanding the history of OpenTelemetry\n\nIn early 2019, the OpenTelemetry project was announced as a merger of two existing open source\n\nprojects: OpenTracing and OpenCensus. Although initially, the goal of this endeavor was to bring\n\nthese two projects together, its ambition to provide an observability framework for cloud-native\n\nsoftware goes much further than that. Since OpenTelemetry combines concepts of both OpenTracing\n\nand OpenCensus, let's first look at each of these projects individually. Please refer to the following\n\nTwitter link, which announced OpenTelemetry by combining both concepts:\n\nhttps://twitter.com/opencensusio/status/1111388599994318848.",
      "content_length": 2506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Figure 1.2 - Screenshot of the aforementioned tweet\n\nOpenTracing\n\nThe OpenTracing (https://opentracing.io) project, started in 2016, was focused on solving the\n\nproblem of increasing the adoption of distributed tracing as a means for users to better understand\n\ntheir systems. One of the challenges identified by the project was that adoption was difficult because\n\nof cost instrumentation and the lack of consistent quality instrumentation in third-party libraries.\n\nOpenTracing provided a specification for Application Programming Interface (APIs) to address\n\nthis problem. This API could be leveraged independently of the implementation that generated\n\ndistributed traces, therefore allowing application developers and library authors to embed calls to this\n\nAPI in their code. By default, the API would act as a no-op operation, meaning those calls wouldn't\n\ndo anything unless an implementation was configured.\n\nLet's see what this looks like in code. The call to an API to trace a specific piece of code resembles\n\nthe following example. You'll notice the code is accessing a global variable to obtain a Tracer via the\n\nglobal_tracer method. A Tracer in OpenTracing, and in OpenTelemetry (as we'll discuss later in\n\nChapter 2, OpenTelemetry Signals – Tracing, Metrics, and Logging, and Chapter 4, Distributed\n\nTracing – Tracing Code Execution), is a mechanism used to generate trace data. Using a globally\n\nconfigured tracer means that there's no configuration required in this instrumentation code – it can be\n\ndone completely separately. The next line starts aprimary building block, span. We'll discuss this",
      "content_length": 1616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "further in Chapter 2, OpenTelemetry Signals – Tracing, Metrics, and Logging, but it is shown here to\n\ngive you an idea of how a Tracer is used in practice:\n\nimport opentracing\n\ntracer = opentracing.global_tracer()\n\nwith tracer.start_active_span('doWork'):\n\n# do work\n\nThe default no-op implementation meant that code could be instrumented without the authors having\n\nto make decisions about how the data would be generated or collected at instrumentation time. It also\n\nmeant that users of instrumented libraries, who didn't want to use distributed tracing in their\n\napplications, could still use the library without incurring a performance penalty by not configuring it.\n\nOn the other hand, users who wanted to configure distributed tracing could choose how this\n\ninformation would be generated. The users of these libraries and applications would choose a Tracer\n\nimplementation and configure it. To comply with the specification, a Tracer implementation only\n\nneeded to adhere to the API defined (https://github.com/opentracing/opentracing-\n\npython/blob/master/opentracing/tracer.py) , which includes the following methods:\n\nStart a new span.\n\nInject an existing span's context into a carrier.\n\nExtract an existing span from a carrier.\n\nAlong with the specification for this API, OpenTracing also provides semantic conventions. These\n\nconventions describe guidelines to improve the quality of the telemetry emitted by instrumenting.\n\nWe'll discuss semantic conventions further when exploring the concepts of OpenTelemetry.\n\nOpenCensus\n\nOpenCensus (https://opencensus.io) started as an internal project at Google, called Census, but was\n\nopen sourced and gained popularity with a wider community in 2017. The project provided libraries\n\nto make the generation and collection of both traces and metrics simpler for application developers. It\n\nalso provided the OpenCensus Collector, an agent run independently that acted as a destination for\n\ntelemetry from applications and could be configured to process the data before sending it along to\n\nbackends for storage and analysis. Telemetry being sent to the collector was transmitted using a wire\n\nformat specified by OpenCensus. The collector was an especially powerful component of\n\nOpenCensus. As shown in Figure 1.3, many applications could be configured to send data to a single\n\ndestination. That destination could then control the flow of the data without having to modify the\n\napplication code any further:",
      "content_length": 2463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Figure 1.3 – OpenCensus Collector data flow\n\nThe concepts of the API to support distributed tracing in OpenCensus were like those of\n\nOpenTracing's API. In contrast to OpenTracing, however, the project provided a tightly coupled API\n\nand Software Development Kit (SDK), meaning users could use OpenCensus without having to\n\ninstall and configure a separate implementation. Although this simplified the user experience for\n\napplication developers, it also meant that in certain languages, the authors of third-party libraries\n\nwanting to instrument their code would depend on the SDK and all its dependencies. As mentioned\n\nbefore, OpenCensus also provided an API to generate application metrics. It introduced several\n\nconcepts that would become influential in OpenTelemetry:\n\nMeasurement: This is the recorded output of a measure, or a generated metric point.\n\nMeasure: This is a defined metric to be recoded.\n\nAggregation: This describes how the measurements are aggregated.\n\nViews: These combine measures and aggregations to determine how the data should be exported.\n\nTo collect metrics from their applications, developers defined a measure instrument to record\n\nmeasurements, and then configured a view with an aggregation to emit the data to a backend. The\n\nsupported aggregations were count, distribution, sum, and last value.",
      "content_length": 1333,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "As the two projects gained popularity, the pain for users only grew. The existence of both projects\n\nmeant that it was unclear for users what project they should rely on. Using both together was not\n\neasy. One of the core components of distributed tracing is the ability to propagate context between\n\nthe different applications in a distributed system, and this didn't work out of the box between the two\n\nprojects. If a user wanted to collect traces and metrics, they would have to use OpenCensus, but if\n\nthey wanted to use libraries that only supported OpenTracing, then they would have to use both –\n\nOpenTracing for distributed traces, and OpenCensus for metrics. It was a mess, and when there are\n\ntoo many standards, the way to solve all the problems is to invent a new standard!\n\nIt was a mess, and when there are too many standards, the way to solve all the problems is to invent a\n\nnew standard! The following XKCD comic captures the sentiment very aptly:\n\nFigure 1.4 – How standards proliferate comic (credit: XKCD, https://xkcd.com/927/)\n\nSometimes a new standard is a correct solution, especially when that solution:\n\nIs built using the lessons learned from its predecessors\n\nBrings together the communities behind other standards\n\nSupersedes two existing competing standards\n\nThe OpenCensus and OpenTracing organizers worked together to ensure the new standard would\n\nsupport a migration path for existing users of both communities, allowing the projects to eventually\n\nbecome deprecated. This would also make the lives of users easier by offering a single standard to\n\nuse when instrumenting applications. There was no longer any need to guess what project to use!\n\nObservability for cloud-native software",
      "content_length": 1720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "OpenTelemetry aims to standardize how applications are instrumented and how telemetry data is\n\ngenerated, collected, and transmitted. It also aims to give users the tools necessary to correlate that\n\ntelemetry across systems, languages, and applications, to allow them to better understand their\n\nsoftware. One of the initial goals of the project involved ensuring all the functionality that was key to\n\nboth OpenCensus and OpenTracing users would become part of the new project. The focus on pre-\n\nexisting users also leads to the project organizers establishing a migration path to ease the transition\n\nfrom OpenTracing and OpenCensus to OpenTelemetry. To accomplish its lofty goals, OpenTelemetry\n\nprovides the following:\n\nAn open specification\n\nLanguage-specific APIs and SDKs\n\nInstrumentation libraries\n\nSemantic conventions\n\nAn agent to collect telemetry\n\nA protocol to organize, transmit, and receive the data\n\nThe project kicked off with the initial commit on May 1, 2019, and brought together the leaders from\n\nOpenCensus and OpenTracing. The project is governed by a governance committee that holds\n\nelections annually, with elected representatives serving on the committee for two-year terms. The\n\nproject also has a technical committee that oversees the specification, drives project-wide discussion,\n\nand reviews language-specific implementations. In addition, there are various special interest\n\ngroups (SIGs) in the project, focused on features or technologies supported by the project. Each\n\nlanguage implementation has its own SIG with independent maintainers and approvers managing\n\nseparate repositories with tools and processes tailored to the language. The initial work for the project\n\nwas heavily focused on the open specification. This provides guidance for the language-specific\n\nimplementations. Since its first commit, the project has received contributions from over 200\n\norganizations, including observability leaders and cloud providers, as well as end users of\n\nOpenTelemetry. At the time of writing, OpenTelemetry has implementations in 11 languages and 18\n\nspecial interest or working groups.\n\nSince the initial merger of OpenCensus and OpenTracing, communities from additional open source\n\nprojects have participated in OpenTelemetry efforts, including members of the Prometheus and\n\nOpenMetrics projects. Now that we have a better understanding of how OpenTelemetry was brought\n\nto life, let's take a deeper look at the concepts of the project.\n\nUnderstanding the concepts of OpenTelemetry",
      "content_length": 2524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "OpenTelemetry is a large ecosystem. Before diving into the code, having a general understanding of\n\nthe concepts and terminology used in the project will help us. The project is composed of the\n\nfollowing:\n\nSignals\n\nPipelines\n\nResources\n\nContext propagation\n\nLet's look at each of these aspects.\n\nSignals\n\nWith its goal of providing an open specification for encompassing such a wide variety of telemetry\n\ndata, the OpenTelemetry project needed to agree on a term to organize the categories of concern.\n\nEventually, it was decided to call these signals. A signal can be thought of as a standalone component\n\nthat can be configured, providing value on its own. The community decided to align its work into\n\ndeliverables around these signals to deliver value to its users as soon as possible. The alignment of\n\nthe work and separation of concerns in terms of signals has allowed the community to focus its\n\nefforts. The tracing and baggage signals were released in early 2021, soon followed by the metrics\n\nsignal. Each signal in OpenTelemetry comes with the following:\n\nA set of specification documents providing guidance to implementors of the signal\n\nA data model expressing how the signal is to be represented in implementations\n\nAn API that can be used by application and library developers to instrument their code\n\nThe SDK needed to allow users to produce telemetry using the APIs\n\nSemantic conventions that can be used to get consistent, high-quality data\n\nInstrumentation libraries to simplify usage and adoption\n\nThe initial signals defined by OpenTelemetry were tracing, metrics, logging, and baggage. Signals\n\nare a core concept of OpenTelemetry and, as such, we will become quite familiar with them.\n\nSpecification\n\nOne of the most important aspects of OpenTelemetry is ensuring that users can expect a similar\n\nexperience regardless of the language they're using. This is accomplished by defining the standards\n\nfor what is expected of OpenTelemetry-compliant implementations in an open specification. The\n\nprocess used for writing the specification is flexible, but large new features or sections of\n\nfunctionality are often proposed by writing an OpenTelemetry Enhancement Proposal (OTEP).\n\nThe OTEP is submitted for review and is usually provided along with prototype code in multiple",
      "content_length": 2299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "languages, to ensure the proposal isn't too language-specific. Once an OTEP is approved and merged,\n\nthe writing of the specification begins. The entire specification lives in a repository on GitHub\n\n(https://github.com/open-telemetry/opentelemetry-specification) and is open for anyone to contribute\n\nor review.\n\nData model\n\nThe data model defines the representation of the components that form a specific signal. It provides\n\nthe specifics of what fields each component must have and describes how all the components interact\n\nwith one another. This piece of the signal definition is particularly important to give clarity as to what\n\nuse cases the APIs and SDKs will support. The data model also explains to developers implementing\n\nthe standard how the data should behave. API\n\nInstrumenting applications can be quite expensive, depending on the size of your code base.\n\nProviding users with an API allows them to go through the process of instrumenting their code in a\n\nway that is vendor-agnostic. The API is decoupled from the code that generates the telemetry,\n\nallowing users the flexibility to swap out the underlying implementations as they see fit. This\n\ninterface can also be relied upon by library and frameworks authors, and only configured to emit\n\ntelemetry data by end users who wish to do so. A user who instruments their code by using the API\n\nand does not configure the SDK will not see any telemetry produced by design. SDK\n\nThe SDK does the bulk of the heavy lifting in OpenTelemetry. It implements the underlying system\n\nthat generates, aggregates, and transmits telemetry data. The SDK provides the controls to configure\n\nhow telemetry should be collected, where it should be transmitted, and how. Configuration of the\n\nSDK is supported via in-code configuration, as well as via environment variables defined in the\n\nspecification. As it is decoupled from the API, using the SDK provided by OpenTelemetry is an\n\noption for users, but it is not required. Users and vendors are free to implement their own SDKs if\n\ndoing so will better fit their needs. Semantic conventions\n\nProducing telemetry can be a daunting task, since you can call anything whatever you wish, but\n\ndoing so would make analyzing this data difficult. For example, if server A labels the duration of an\n\nhttp.server.duration request and server B labels it http.server.request_length, calculating the\n\ntotal duration of a request across both servers requires additional knowledge of this difference, and\n\nlikely additional operations. One way in which OpenTelemetry tries to make this a bit easier is by\n\noffering semantic conventions, or definitions for different types of applications and workloads to",
      "content_length": 2695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "improve the consistency of telemetry. Some of the types of applications or protocols that are covered\n\nby semantic conventions include the following:\n\nHTTP\n\nDatabase\n\nMessage queues\n\nFunction-as-a-Service (FaaS)\n\nRemote procedure calls (RPC)\n\nProcess metrics\n\nThe full list of semantic conventions is quite extensive and can be found in the specification\n\nrepository. The following figure shows a sample of the semantic convention for tracing database\n\nqueries:\n\nTable 1.1 – Database semantic conventions as defined in the OpenTelemetry specification (https://github.com/open-\n\ntelemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/database.md#connection-\n\nlevel-attributes)\n\nThe consistency of telemetry data reported will ultimately impact the user of that data's ability to use\n\nthis information. Semantic conventions provide both the guidelines of what telemetry should be\n\nreported, as well as how to identify this data. They provide a powerful tool for developers to learn\n\ntheir way around observability.\n\nInstrumentation libraries\n\nTo ensure users can get up and running quickly, instrumentation libraries are made available by\n\nOpenTelemetry SIGs in various languages. These libraries provide instrumentation for popular open",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "source projects and frameworks. For example, in Python, the instrumentation libraries include Flask,\n\nRequests, Django, and others. The mechanisms used to implement these libraries are language-\n\nspecific and may be used in combination with auto-instrumentation to provide users with telemetry\n\nwith close to zero code changes required. The instrumentation libraries are supported by the\n\nOpenTelemetry organization and adhere to semantic conventions.\n\nSignals represent the core of the telemetry data that is generated by instrumenting cloud-native\n\napplications. They can be used independently, but the real power of OpenTelemetry is to allow its\n\nusers to correlate data across signals to get a better understanding of their systems. Now that we have\n\na general understanding of what they are, let's look at the other concepts of OpenTelemetry.\n\nPipelines\n\nTo be useful, the telemetry data captured by each signal must eventually be exported to a data store,\n\nwhere storage and analysis can occur. To accomplish this, each signal implementation offers a series\n\nof mechanisms to generate, process, and transmit telemetry. We can think of this as a pipeline, as\n\nrepresented in the following figure:",
      "content_length": 1201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Figure 1.5 – Telemetry pipeline\n\nThe components in the telemetry pipeline are typically initialized early in the application code to\n\nensure no meaningful telemetry is missed.\n\nIMPORTANT NOTE\n\nIn many languages, the pipeline is configurable via environment variables. This will be explored further in Chapter 7,\n\nInstrumentation Libraries.\n\nOnce configured, the application generally only needs to interact with the generator to record\n\ntelemetry, and the pipeline will take care of collecting and sending the data. Let's look at each\n\ncomponent of the pipeline now.\n\nProviders\n\nThe starting point of the telemetry pipeline is the provider. A provider is a configurable factory that is\n\nused to give application code access to an entity used to generate telemetry data. Although multiple\n\nproviders may be configured within an application, a default global provider may also be made\n\navailable via the SDK. Providers should be configured early in the application code, prior to any\n\ntelemetry data being generated.\n\nTelemetry generators\n\nTo generate telemetry at different points in the code, the telemetry generator instantiated by a\n\nprovider is made available in the SDK. This generator is what most users will interact with through\n\nthe instrumentation of their application and the use of the API. Generators are named differently\n\ndepending on the signal: the tracing signal calls this a tracer, the metrics signal a meter. Their purpose\n\nis generally the same – to generate telemetry data. When instantiating a generator, applications and\n\ninstrumenting libraries must pass a name to the provider. Optionally, users can specify a version\n\nidentifier to the provider as well. This information will be used to provide additional information in\n\nthe telemetry data generated.\n\nProcessors\n\nOnce the telemetry data has been generated, processors provide the ability to further modify the\n\ncontents of the data. Processors may determine the frequency at which data should be processed or\n\nhow the data should be exported. When instantiating a generator, applications and instrumenting\n\nlibraries must pass a name to the provider. Optionally, users can specify a version identifier to the\n\nprovider as well.\n\nExporters\n\nThe last step before telemetry leaves the context of an application is to go through the exporter. The\n\njob of the exporter is to translate the internal data model of OpenTelemetry into the format that best",
      "content_length": 2425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "matches the configured exporter's understanding. Multiple export formats and protocols are\n\nsupported by the OpenTelemetry project:\n\nOpenTelemetry protocol\n\nConsole\n\nJaeger\n\nZipkin\n\nPrometheus\n\nOpenCensus\n\nThe pipeline allows telemetry data to be produced and emitted. We'll configure pipelines many times\n\nover the following chapters, and we'll see how the flexibility provided by the pipeline accommodates\n\nmany use cases.\n\nResources\n\nAt their most basic, resources can be thought of as a set of attributes that are applied to different\n\nsignals. Conceptually, a resource is used to identify the source of the telemetry data, whether a\n\nmachine, container, or function. This information can be used at the time of analysis to correlate\n\ndifferent events occurring in the same resource. Resource attributes are added to the telemetry data\n\nfrom signals at the export time before the data is emitted to a backend. Resources are typically\n\nconfigured at the start of an application and are associated with the providers. They tend to not\n\nchange throughout the lifetime of the application. Some typical resource attributes would include the\n\nfollowing:\n\nA unique name for the service: service.name\n\nThe version identifier for a service: service.version\n\nThe name of the host where the service is running: host.name\n\nAdditionally, the specification defines resource detectors to further enrich the data. Although\n\nresources can be set manually, resource detectors provide convenient mechanisms to automatically\n\npopulate environment-specific data. For example, the Google Cloud Platform (GCP) resource\n\ndetector (https://www.npmjs.com/package/@opentelemetry/resource-detector-gcp) interacts with the\n\nGoogle API to fill in the following data:",
      "content_length": 1740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Table 1.2 – GCP resource detector attributes\n\nResources and resource detectors adhere to semantic conventions. Resources are a key component in\n\nmaking telemetry data-rich, meaningful, and consistent across an application. Another important\n\naspect of ensuring the data is meaningful is context propagation.\n\nContext propagation\n\nOne area of observability that is particularly powerful and challenging is context propagation. A core\n\nconcept of distributed tracing, context propagation provides the ability to pass valuable contextual\n\ninformation between services that are separated by a logical boundary. Context propagation is what\n\nallows distributed tracing to tie requests together across multiple systems. OpenTelemetry, as\n\nOpenTracing did before it, has made this a core component of the project. In addition to tracing,\n\ncontext propagation allows for user-defined values (known as baggage) to be propagated. Baggage\n\ncan be used to annotate telemetry across signals.\n\nContext propagation defines a context API as part of the OpenTelemetry specification. This is\n\nindependent of the signals that may use it. Some languages already have built-in context\n\nmechanisms, such as the ContextVar module in Python 3.7+ and the context package in Go. The\n\nspecification recommends that the context API implementations leverage these existing mechanisms.\n\nOpenTelemetry also provides for the interface and implementation of mechanisms required to\n\npropagate context across boundaries. The following abbreviated code shows how two services, A and\n\nB, would use the context API to share context:\n\nfrom opentelemetry.propagate import extract, inject\n\nclass ServiceA:\n\ndef client_request():\n\ninject(headers, context=current_context)\n\n# make a request to ServiceB and pass in headers\n\nclass ServiceB:\n\ndef handle_request():\n\n# receive a request from ServiceA",
      "content_length": 1853,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "context = extract(headers)\n\nIn Figure 1.6, we can see a comparison between two requests from service A to service B. The top\n\nrequest is made without propagating the context, with the result that service B has neither the trace\n\ninformation nor the baggage that service A does. In the bottom request, this contextual data is\n\ninjected when service A makes a request to service B, and extracted by service B from the incoming\n\nrequest, ensuring service B now has access to the propagated data:\n\nFigure 1.6 – Request between service A and B with and without context propagation\n\nThe propagation of context we have demonstrated allows backends to tie the two sides of the request\n\ntogether, but it also allows service B to make use of the dataset in service A. The challenge with\n\ncontext propagation is that when it isn't working, it's hard to know why. The issue could be that the\n\ncontext isn't being propagated correctly due to configuration issues or possibly a networking\n\nproblem. This is a concept we'll revisit many times throughout the book.",
      "content_length": 1048,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "Summary\n\nIn this chapter, we've looked at what observability is, and the challenges it can solve as regards the\n\nuse of cloud-native applications. By exploring the different mechanisms available to generate\n\ntelemetry and improve the observability of applications, we were also able to gain an understanding\n\nof how the observability landscape has evolved, as well as where some challenges remain.\n\nExploring the history behind the OpenTelemetry project gave us an understanding of the origin of the\n\nproject and its goals. We then familiarized ourselves with the components forming tracing, metrics,\n\nlogging signals, and pipelines to give us the terminology and building blocks needed to start\n\nproducing telemetry using OpenTelemetry. This learning will allow us to tackle the first challenge of\n\nobservability – producing high-quality telemetry. Understanding resources and context propagation\n\nwill help us correlate events across services and signals to allow us to tackle the second challenge –\n\nconnecting the data to better understand systems.\n\nLet's now take a closer look at how this all works together in practice. In the next chapter, we will\n\ndive deeper into the concepts of distributed tracing, metrics, logs, and semantic conventions by\n\nlaunching a grocery store application instrumented with OpenTelemetry. We will then explore the\n\ntelemetry generated by this distributed system.",
      "content_length": 1399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Chapter 2: OpenTelemetry Signals – Traces, Metrics, and Logs\n\nLearning how first to instrument an application can be a daunting task. There's a fair amount of\n\nterminology to understand before jumping into the code. I always find that seeing the finish line helps\n\nme get motivated and stay on track. This chapter's goal is to see what telemetry generated by\n\nOpenTelemetry looks like in practice while learning about the theory. In this chapter, we will dive\n\ninto the specifics of the following:\n\nDistributed tracing\n\nMetrics\n\nLogs\n\nProducing consistent quality data with semantic conventions\n\nTo help us get a more practical sense of the terminology and get comfortable with telemetry, we will\n\nlook at the data using various open source tools that can help us to query and visualize telemetry.\n\nTechnical requirements\n\nThis chapter will use an application that is already instrumented with OpenTelemetry, a grocery store,\n\nand several backends to walk through the different concepts of the signals. The environment we will\n\nbe launching relies on Docker Compose. The first step is to install Docker by following the\n\ninstallation instructions at https://docs.docker.com/get-docker/. Ensure Docker is running on your\n\nlocal system by using the following command:\n\n$ docker version\n\nClient:\n\nCloud integration: 1.0.14\n\nVersion: 20.10.6\n\nAPI version: 1.41\n\nGo version: go1.16.3 ...\n\nNext, let's ensure Compose is also installed by running the following command:\n\n$ docker compose version\n\nDocker Compose version 2.0.0-beta.1\n\nIMPORTANT NOTE",
      "content_length": 1541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Compose was added to the Docker client in more recent client versions. If the previous command returns an error, follow\n\nthe instructions on the Docker website (https://docs.docker.com/compose/install/) to install Compose. Alternatively, you\n\nmay want to try the docker-compose command to see if you already have an older version installed.\n\nThe following diagram shows an overview of the containers we are launching in the Docker\n\nenvironment to give you an idea of the components involved. The applications on the left are\n\nemitting telemetry processed by the Collector and forwarded to the telemetry backends. The diagram\n\nalso shows the port number exposed by each container for future reference.\n\nFigure 2.1 – Containers within Docker environment\n\nThis chapter briefly introduces the following open source projects that support the storage and\n\nvisualization of OpenTelemetry data:\n\nJaeger (https://www.jaegertracing.io)\n\nPrometheus (https://prometheus.io)\n\nLoki (https://github.com/grafana/loki)\n\nGrafana (https://grafana.com/oss/grafana/)\n\nI strongly recommend visiting the website for each project to gain familiarity with the tools as we\n\nwill use them throughout the chapter. Each of these tools will be revisited in Chapter 10, Configuring\n\nBackends. No prior knowledge of them is required to go through the examples, but they are pretty\n\nhelpful to have in your toolbelt. The configuration files necessary to launch the applications in this",
      "content_length": 1452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "chapter are available in the companion repository (https://github.com/PacktPublishing/Cloud-Native-\n\nObservability) in the chapter2 directory. The following downloads the repository using the git\n\ncommand:\n\n$ git clone https://github.com/PacktPublishing/Cloud-Native-Observability\n\n$ cd Cloud-Native-Observability/chapter02\n\nTo bring up the applications and telemetry backends, run the following command:\n\n$ docker compose up\n\nWe will test the various tools to ensure each one is working as expected and is accessible from your\n\nbrowser. Let's start with Jaeger by accessing the following URL: http://localhost:16686. The\n\nfollowing screenshot shows the interface you should see:\n\nFigure 2.2 – The Jaeger web interface\n\nThe next backend this chapter will use for metrics is Prometheus; let's test the application by visiting\n\nhttp://localhost:9090. The following screenshot is a preview of the Prometheus web interface:",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Figure 2.3 – The Prometheus web interface\n\nThe last tool we need to ensure is working in our backend for logs is Loki. We will use Grafana as a\n\ndashboard to visualize the logs being emitted. Begin by visiting http://localhost:3000/explore to\n\nensure Grafana is up; you should be greeted by an interface like the one in Figure 2.4:\n\nFigure 2.4 – The Grafana web interface\n\nThe next application we will check is the OpenTelemetry Collector, which acts as the routing layer\n\nfor all the telemetry produced by the example application. The Collector exposes a health check\n\nendpoint discussed in Chapter 8, OpenTelemetry Collector. For now, it's enough to know that\n\naccessing the endpoint will give us information about the health of the Collector, using the following\n\ncurl command:\n\n$ curl localhost:13133\n\n{\"status\":\"Server available\",\"upSince\":\"2021-10-\n\n03T15:42:02.7345149Z\",\"uptime\":\"9.3414709s\"}",
      "content_length": 900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Lastly, let's ensure the containers forming the grocery store demo application are running. To do this,\n\nwe use curl again in the following commands to access an endpoint in the applications that returns a\n\nstatus showing the application's health. It's possible to use any other tool capable of making HTTP\n\nrequests, including the browser, to accomplish this. The following checks the status of the grocery\n\nstore:\n\n$ curl localhost:5000/healthcheck\n\n{\n\n\"service\": \"grocery-store\",\n\n\"status\": \"ok\"\n\n}\n\nThe same command can be used to check the status of the inventory application by specifying port\n\n5001:\n\n$ curl localhost:5001/healthcheck\n\n{\n\n\"service\": \"inventory\",\n\n\"status\": \"ok\"\n\n}\n\nThe shopper application represents a client application and does not provide any endpoint to expose\n\nits health status. Instead, we can look at the logs emitted by the application to get a sense of whether\n\nit's doing the right thing or not. The following uses the docker logs command to look at the output\n\nfrom the application. Although it may vary slightly, the output should contain information about the\n\nshopper connecting to the grocery store:\n\n$ docker logs -n 2 shopper\n\nDEBUG:urllib3.connectionpool:http://grocery-store:5000 \"GET /products HTTP/1.1\" 200 107\n\nINFO:shopper:message=\"add orange to cart\"\n\nThe same docker logs command can be used on any of the other containers if you're interested in\n\nseeing more information about them. Once you're done with the chapter, you can clean up all the\n\ncontainers by running stop to terminate the running containers, and rm to delete the containers\n\nthemselves:\n\n$ docker compose stop\n\n$ docker compose rm\n\nAll the examples in this chapter will expect that the Docker Compose environment is already up and\n\nrunning. When in doubt, come back to this technical requirement section to ensure your environment",
      "content_length": 1848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "is still running as expected. Now, let's see what these OpenTelemetry signals are all about, starting\n\nwith traces.\n\nTraces\n\nDistributed tracing is the foundation behind the tracing signal of OpenTelemetry. A distributed trace\n\nis a series of event data generated at various points throughout a system tied together via a unique\n\nidentifier. This identifier is propagated across all components responsible for any operation required\n\nto complete the request, allowing each operation to associate the event data to the originating request.\n\nThe following diagram gives us a simplified example of what a single request may look like when\n\nordering groceries through an app:\n\nFigure 2.5 – Example request through a simplified ordering system\n\nEach trace represents a unique request through a system that can be either synchronous or\n\nasynchronous. Synchronous requests occur in sequence with each unit of work completed before\n\ncontinuing. An example of a synchronous request may be of a client application making a call to a\n\nserver and waiting or blocking until a response is returned before proceeding. In contrast,\n\nasynchronous requests can initiate a series of operations that can occur simultaneously and\n\nindependently. An example of an asynchronous request is a server application submitting messages to\n\na queue or a process that batches operations. Each operation recorded in a trace is represented by a",
      "content_length": 1411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "span, a single unit of work done in the system. Let's see what the specifics of the data captured in the\n\ntrace look like.\n\nAnatomy of a trace\n\nThe definition of what constitutes a trace has evolved as various systems have been developed to\n\nsupport distributed tracing. The World Wide Web Consortium (W3C), an international group that\n\ncollaborates to move the web forward, assembled a working group in 2017 to produce a definition for\n\ntracing. In February 2020, the first version of the Trace Context specification was completed, with\n\nits details available on the W3C's website (https://www.w3.org/TR/trace-context-1/). OpenTelemetry\n\nfollows the recommendation from the W3C in its definition of the SpanContext, which contains\n\ninformation about the trace and must be propagated throughout the system. The elements of a trace\n\navailable within a span context include the following:\n\nA unique identifier, referred to as a trace ID, identifies the request through the system.\n\nA second identifier, the span ID, is associated with the span that last interacted with the context. This may also be referred to as\n\nthe parent identifier.\n\nTrace flags include additional information about the trace, such as the sampling decision and trace level.\n\nVendor-specific information is carried forward using a Trace state field. This allows individual vendors to propagate information\n\nnecessary for their systems to interpret the tracing data. For example, if a vendor needs an additional identifier to be present in the\n\ntrace information, this identifier could be inserted as vendorA=123456 in the trace state field. Other vendors would add their\n\nown as needed, allowing traces to be shared across vendors.\n\nA span can represent a method call or a subset of the code being called within a method. Multiple\n\nspans within a trace are linked together in a parent-child relationship, with each child span containing\n\ninformation about its parent. The first span in a trace is called the root span and is identified because\n\nit does not have a parent span identifier. The following shows a typical visualization of a trace and\n\nthe spans associated with it. The horizontal axis indicates the duration of the entire trace operation.\n\nThe vertical axis shows the order in which the operations captured by spans took place, starting with\n\nthe first operation at the top:",
      "content_length": 2357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Figure 2.6 – Visual representation of a trace\n\nLet's look closer at a trace by bringing up a sample generated from the telemetry produced by the\n\ngrocery store application. Access the Jaeger web interface by opening a browser to the following\n\nURL: http://localhost:16686/.\n\nSearch for a trace by selecting a service from the drop-down and clicking the Find Traces button.\n\nThe following screenshot shows the traces found for the shopper service:\n\nFigure 2.7 – Traces search result\n\nTo obtain details about a specific trace, select one of the search results by clicking on the row. The\n\nfollowing screenshot, Figure 2.8, shows the details of the trace generated by a request through the",
      "content_length": 686,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "grocery store applications. It includes the following:\n\n1. The unique trace ID for this request. In OpenTelemetry, this is represented by a 128-bit integer. It's worth noting that other\n\nsystems may represent this as a 64-bit integer. The integer is encoded into a string containing hexadecimal characters in many\n\nsystems.\n\n2. The start time for the request.\n\n3. The total duration of the request through the system is calculated by subtracting the time the root span is finished from its start\n\ntime.\n\n4. A count of the number of services included in this request.\n\n5. A count of spans recorded in this request is shown in Total Spans.\n\n6. A hierarchical view of the spans in the trace.\n\nFigure 2.8 – A trace in Jaeger\n\nThe preceding screenshot gives us an immediate sense of where time may be spent as the system\n\nprocesses the request. It also provides us with a glimpse into what the underlying code may look like\n\nwithout ever opening an editor. Additional details are captured in spans; let's look at those now.\n\nDetails of a span\n\nAs mentioned previously, the work captured in a trace is broken into separate units or operations,\n\neach represented by a span. The span is a data structure containing the following information:\n\nA unique identifier\n\nA parent span identifier\n\nA name describing the work being recorded\n\nA start and end time",
      "content_length": 1345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "In OpenTelemetry, a span identifier is represented by a 64-bit integer. The start and end times are\n\nused to calculate the operation's duration. Additionally, spans can contain metadata in the form of\n\nkey-value pairs. In the case of Jaeger and Zipkin, these pairs are referred to as tags, whereas\n\nOpenTelemetry calls them attributes. The goal is to enrich the data provided with the additional\n\ncontext in both cases.\n\nLook for the following details in Figure 2.9, which shows the detailed view of a specific span as\n\nshown in Jaeger:\n\n1. The name identifies the operation represented by this span. In this case, /inventory is the operation's name.\n\n2. SpanID is the unique 64-bit identifier represented in hex-encoded formatting.\n\n3. Start Time is when the operation recorded its start time relative to the start of the request. In the case shown here, the operation\n\nstarted 8.36 milliseconds after the beginning of the request.\n\n4. Duration is the time it took for the operation to complete and is calculated using the start and end times recorded in the span.\n\n5. The Service name identifies the application that triggered the operation and recorded the telemetry.\n\n6. Tags represent additional information about the operation being recorded.\n\n7. Process shows information about the application or process fulfilling the requested operation.\n\nFigure 2.9 – Span details\n\nMany of the tags captured in the span shown previously rely on semantic conventions, which will be\n\ndiscussed further in this chapter.\n\nAdditional considerations\n\nWhen producing distributed traces in a system, it's worth considering the additional visibility's\n\ntradeoffs. Generating tracing information can potentially incur performance overhead at the\n\napplication level. It can result in added latency if tracing information is gathered and transmitted\n\ninline. There is also memory overhead to consider, as collecting information inevitably allocates\n\nresources. These concerns can be largely mitigated using configuration available in OpenTelemetry,\n\nas we'll see in Chapter 4, Distributed Tracing – Tracing Code Execution.\n\nDepending on where the data is sent, additional costs, such as bandwidth or storage, can also become\n\na factor. One of the ways to mitigate these costs is to reduce the amount of data produced by",
      "content_length": 2301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "sampling only a certain amount of the data. We will dive deeper into sampling in Chapter 12,\n\nSampling.\n\nAnother challenging aspect of producing distributed tracing data is ensuring that all the services\n\ncorrectly propagate the context. Failing to propagate the trace ID across the system means that\n\nrequests will be broken into multiple traces, making them difficult to use or not helpful at all.\n\nThe last thing to consider is the effort required to instrument an application correctly. This is a non-\n\ntrivial amount of effort, but as we'll see in future chapters, OpenTelemetry provides instrumentation\n\nlibraries to make this easier.\n\nNow that we have a deeper understanding of traces, let's look at metrics.\n\nMetrics\n\nJust as distributed traces do, metrics provide information about the state of a running system to\n\ndevelopers and operators. The data collected via metrics can be aggregated over time to identify\n\ntrends and patterns in applications graphed through various tools and visualizations. The term metrics\n\nhas a broad range of applications as they can capture low-level system metrics such as CPU cycles,\n\nor higher-level details such as the number of blue sweaters sold today. These examples would be\n\nhelpful to different groups in an organization.\n\nAdditionally, metrics are critical to monitoring the health of an application and deciding when an on-\n\ncall engineer should be alerted. They form the basis of service level indicators (SLIs)\n\n(https://en.wikipedia.org/wiki/Service_level_indicator) that measure the performance of an\n\napplication. These indicators are then used to set service level objectives (SLOs)\n\n(https://en.wikipedia.org/wiki/Service-level_objective) that organizations use to calculate error\n\nbudgets.\n\nIMPORTANT NOTE\n\nSLIs, SLOs, and service level agreements (SLAs) are essential topics in production environments where third-party\n\ndependencies can impact the availability of your service. There are entire books dedicated to the issue that we will not\n\ncover here. The Google site reliability engineering (SRE) book is a great resource for this: https://sre.google/sre-\n\nbook/service-level-objectives/.\n\nThe metrics signal of OpenTelemetry combines various existing open source formats into a unified\n\ndata model. Primarily, it looks to OpenMetrics, StatsD, and Prometheus for existing definitions,\n\nrequirements, and usage, wanting to ensure the use-cases of each of those communities are\n\nunderstood and addressed by the new standard.",
      "content_length": 2487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Anatomy of a metric\n\nJust about anything can be a metric; record a value at a given time, and you have yourself a metric.\n\nThe common fields a metric contains include the following:\n\nA name identifies the metric being recorded.\n\nA data point value may be an integer or a floating-point value. Note that in the case of a histogram or a summary, there is more\n\nthan one value associated with the metric.\n\nAdditional dimension information about the metric. The representation of these dimensions varies depending on the metrics\n\nbackend. In Prometheus, these dimensions are represented by labels, whereas in StatsD, it is common to add a prefix in the\n\nmetric's name. In OpenTelemetry, dimensions are added to metrics via attributes.\n\nLet's look at data produced by metrics sent from the demo application. Access the Prometheus\n\ninterface via a browser and the following URL: http://localhost:9090. The user interface for\n\nPrometheus allows us to query the time-series database by using the metric's name. The following\n\nscreenshot contains a table showing the value of the request_counter metric. Look for the following\n\ndetails in the resulting table:\n\n1. The name of the metric, in this case, request_counter.\n\n2. The dimensions recorded for this metric are displayed in curly braces as key-value pairs with the key emboldened. In the example\n\nshown, the service_name label caused two different metrics to be recorded, one for the shopper service and another for the\n\nstore service.\n\n3. A reported value, in this example, is an integer. This value may be the last received or a calculated current value depending on the\n\nmetric type.\n\nFigure 2.10 – Table view of metric in Prometheus\n\nThe table view shows the current value as cumulative. An alternative representation of the recorded\n\nmetric is shown in the following figure. As the data received by Prometheus is stored over time, a\n\nline graph can be generated. Click the Graph tab of the interface to see what the data in a chart looks\n\nlike:",
      "content_length": 1996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Figure 2.11 – Graph view of the same metric in Prometheus\n\nBy looking at the values for the metric over time, we can deduce additional information about the\n\nservice, for example, its start time or trends in its usage. Visualizing metrics also provides\n\nopportunities to identify anomalies.\n\nData point types\n\nA metric is a more generic term that encapsulates different measurements that can be used to\n\nrepresent a wide array of information. As such, the data is captured using various data point types.\n\nThe following diagram compares different kinds of data points that can be captured within a metric:",
      "content_length": 605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Figure 2.12 – Comparison of counter, gauge, histogram, and summary data points\n\nEach data point type can be used in different scenarios and has slightly different meanings. It's worth\n\nnoting that even though competing standards provide support for types using the same name, their\n\ndefinition may vary. For example, a counter in StatsD\n\n(https://github.com/statsd/statsd/blob/master/docs/metric_types.md#counting) resets every time the\n\nvalue has been flushed, whereas, in Prometheus\n\n(https://prometheus.io/docs/concepts/metric_types/#counter), it keeps its cumulative value until the\n\nprocess recording the counter is restarted. The following definitions describe how data point types are\n\nrepresented in the OpenTelemetry specification:\n\nA sum measures incremental changes to a recorded value. This incremental change is either monotonic or non-monotonic and\n\nmust be associated with an aggregation temporality. The temporality can be either of the following:\n\n1. Delta aggregation: The reported values contain the change in value from its previous recording.\n\n2. Cumulative aggregation: The value reported includes the previously reported sum in addition to the delta being\n\nreported.\n\nIMPORTANT NOTE\n\nA cumulative sum will reset when an application restarts. This is useful to identify an event in the application but may\n\nbe surprising if it's not accounted for.\n\nThe following diagram shows an example of a sum counter reporting the number of visits over a\n\nperiod of time. The table on the right-hand side shows what values are to be expected depending on\n\nthe type of temporal aggregation chosen:\n\nFigure 2.13 – Sum showing delta and cumulative aggregation values",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "A sum data point also includes the time window for calculating the sum.\n\nA gauge represents non-monotonic values that only measure the last or current known value at observation. This likely means\n\nsome information is missing, but it may not be relevant. For example, the following diagram represents temperatures recorded at\n\nan hourly interval. More specific data points could provide greater granularity as to the rise and fall of the temperature. These\n\nincremental changes in the temperature may not be required if the goal is to observe trends over weeks or months.\n\nFigure 2.14 – Gauge values recorded\n\nUnlike gauge definitions in other specifications, a gauge in OpenTelemetry is never incremented or\n\ndecremented; it is only ever set to the value being recorded. A timestamp of the observation time\n\nmust be included with the data point.\n\nA histogram data point provides a compressed view into a more significant number of data points by grouping the data into a\n\ndistribution and summarizing the data, rather than reporting individual measurements for every detail represented. The following\n\ndiagram shows sample histogram data points representing a distribution of response durations.\n\nFigure 2.15 – Histogram data points\n\nLike sums, histograms also support a delta or a cumulative aggregation and must contain a time\n\nwindow for the recorded observation. Note that in the case of cumulative aggregation, the data points\n\ncaptured in the distribution will continue to accumulate with each recording.",
      "content_length": 1511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "The summary data type provides a similar capability to histograms, but it's specifically tailored around providing quantiles of a\n\ndistribution. A quantile, sometimes also referred to as percentile, is a fraction between zero and one, representing a percentage of\n\nthe total number of values recorded that falls under a certain threshold. For example, consider the following 10 response times in\n\nmilliseconds: 1.1, 2.9, 7.5, 8.3, 9, 10, 10, 10, 10, 25. The 0.9-quantile, or the 90th percentile, equals 10 milliseconds.\n\nFigure 2.16 – Summary data points\n\nA summary is somewhat similar to a histogram, where the histogram contains a maximum and a\n\nminimum value; the summary includes a 1.0-quantile and 0.0-quantile to represent the same\n\ninformation. The 0.5-quantile, also known as median, is often expressed in the summary. For a\n\nsummary data point, the quantile calculations happen in the producer of the telemetry, which can\n\nbecome expensive for applications. OpenTelemetry supports summaries to provide interoperability\n\nwith OpenMetrics (https://openmetrics.io) and Prometheus and prefers the usage of a histogram,\n\nwhich moves the calculation of quantiles to the receiver of the telemetry. The following screenshot\n\nshows histogram values recorded by the inventory service for the\n\nhttp_request_duration_milliseconds_bucket metric stored in Prometheus. The data shown\n\nrepresents requests grouped into buckets. Each bucket represents the request duration in milliseconds:",
      "content_length": 1481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Figure 2.17 – Histogram value in Prometheus\n\nThe count of requests per bucket can then calculate quantiles for further analysis. Now that we're\n\nfamiliar with the different types of metric data points, let's see how metrics can be combined with\n\ntracing to provide additional insights.\n\nExemplars\n\nMetrics are often helpful on their own, but when correlated with tracing information, they provide\n\nmuch more context and depth on the events occurring in a system. Exemplars offer a tool to\n\naccomplish this in OpenTelemetry by enabling a metric to contain information about an active span.\n\nData points defined in OpenTelemetry include an exemplar field as part of their definition. This field\n\ncontains the following:\n\nA trace ID of the current span in progress\n\nThe span ID of the current span in progress\n\nA timestamp of the event measured\n\nA set of attributes associated with the exemplar\n\nThe value being recorded\n\nThe direct correlation that exemplars provide replaces the guesswork that involves cobbling metrics\n\nand traces with timestamps today. Although exemplars are already defined in the stable metrics\n\nsection of the OpenTelemetry protocol, the implementation of exemplars is still under active\n\ndevelopment at the time of writing.",
      "content_length": 1245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Additional considerations\n\nA concern that often arises with any telemetry is the importance of managing cardinality. Cardinality\n\nrefers to the uniqueness of a value in a set. While counting cars in a parking lot, the number of wheels\n\nwill likely offer a meager value and low cardinality result as most cars have four wheels. The color,\n\nmake, and model of cars produces higher cardinality. The license plate, or vehicle identification\n\nnumber, results in the highest cardinality, providing the most valuable data to know in an event\n\nconcerning a specific vehicle. For example, if the lights have been left on and the owners should be\n\nnotified, calling out for the person with a four-wheeled car won't work nearly as well as calling for a\n\nspecific license plate. However, the count of cars with specific license plates will always be one,\n\nmaking the counter itself somewhat useless.\n\nOne of the challenges with high-cardinality data is the increased storage cost. Specifically, in the case\n\nof metrics, it's possible to significantly increase the number of metrics being produced and stored by\n\nadding a single attribute or label. Suppose an application creating a counter for each request\n\nprocessed uses a unique identifier as the metric's name. In that case, the producer or receiver may\n\ntranslate this into a unique time series for each request. This results in a sudden and unexpected\n\nincrease in load in the system. This is sometimes referred to as cardinality explosion.\n\nWhen choosing attributes associated with produced metrics, it's essential to consider the scale of the\n\nservices and infrastructure producing the telemetry. Some questions to keep in mind are as follows:\n\nWill scaling components of the system increase the number of metrics in a way that is understood? When a system scales, the last\n\nthing anyone wants is for an unexpected spike in metrics to cause outages.\n\nAre any attributes specific to instances of an application? This could cause problems in the case of a crashing application.\n\nUsing labels with finite and knowable values (for example, countries rather than street names) may\n\nbe preferable depending on how the data is stored. When choosing a solution, understanding the\n\nstorage model and limits of the telemetry backend must also be considered.\n\nLogs\n\nAlthough logs have evolved, what constitutes a log is quite broad. Also known as log files, a log is a\n\nrecord of events written to output. Traditionally, logs would be written to a file on disk, searching\n\nthrough as needed. A more recent practice is to emit logs to remote services using the network. This\n\nprovides long-term storage for the data in a location and improves searchability and aggregation.\n\nAnatomy of a log",
      "content_length": 2725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Many applications define their formats for what constitutes a log. There are several existing standard\n\nformats. An example includes the Common Log Format often used by web servers. It's challenging to\n\nidentify commonalities across formats, but at the very least, a log should consist of the following:\n\nA timestamp recording the time of the event\n\nThe message or payload representing the event\n\nThis message can take many forms and include various application-specific information. In the case\n\nof structured logging, the log is formatted as a series of key-value pairs to simplify identifying the\n\ndifferent fields contained within the log. Other formats record logs in a specific order with a\n\nseparating character instead. The following shows an example log emitted by the standard formatter\n\nin Flask, a Python web framework that shows the following:\n\nA timestamp is enclosed in square brackets.\n\nA space-delimited set of elements forms the message logged, including the client IP, the HTTP method used to make a request, the\n\nrequest's path, the protocol version, and the response code:\n\n172.20.0.9 - - [11/Oct/2021 18:50:25] \"GET /inventory HTTP/1.1\" 200 -\n\nThe previous sample is an example of the Common Log Format mentioned earlier. The same log may\n\nlook something like this as a structured log encoded as JSON:\n\n{\n\n\"host\": \"172.20.0.9\",\n\n\"date\": \"11/Oct/2021 18:50:25\",\n\n\"method\": \"GET\",\n\n\"path\": \"/inventory\",\n\n\"protocol\": \"HTTP/1.1\",\n\n\"status\": 200\n\n}\n\nAs you can see with structured logs, identifying the information is more intuitive if you're not already\n\nfamiliar with the type of logs produced. Let's see what logs our demo application produces by\n\nlooking at the Grafana interface, at http://localhost:3000/explore.\n\nThis brings us to the explore view, which allows us to search through telemetry generated by the\n\ndemo application. Ensure that Loki is selected from the data source drop-down in the top left corner.\n\nFilter the logs using the {job=\"shopper\"} query to retrieve all the logs generated by the shopper\n\napplication. The following screenshot shows a log emitted to the Loki backend, which contains the\n\nfollowing:\n\n1. The name of the application is under the job label.",
      "content_length": 2203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "2. A timestamp of the log is shown both as a timestamp and as a nanosecond value.\n\n3. The body of the logged event.\n\n4. Additional labels and values associated with the event.\n\nFigure 2.18 – Log shown in Grafana\n\nNow that we can search for logs, let's see how we can combine the information provided by logs with\n\nother signals via correlation to give us more context.\n\nCorrelating logs\n\nIn the same way that information provided by metrics can be augmented by combining them with\n\nother signals, logs too can provide more context by embedding tracing information. As we'll see in\n\nChapter 6, Logging - Capturing Events, one of the goals of the logging signal in OpenTelemetry is to\n\nprovide correlation capability to already existing logging libraries. Logs recorded via OpenTelemetry\n\ncontain the trace ID and span ID for any span active at the time of the event. The following\n\nscreenshot shows the details of a log record containing the traceID and spanID attributes:",
      "content_length": 971,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Figure 2.19 – Log containing trace ID\n\nUsing these attributes can then reveal the specific request that triggered this event. The following\n\nscreenshot demonstrates what the corresponding trace looks like in Jaeger. If you'd like to try for\n\nyourself, copy the traceID attribute into the Lookup by Trace ID field to search for the trace:\n\nFigure 2.20 – Corresponding trace in Jaeger",
      "content_length": 382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "The correlation demonstrated in the previous example makes exploring events faster and less error-\n\nprone. As we will see in Chapter 6, Logging - Capturing Events, the OpenTelemetry specification\n\nprovides recommendations for what information should be included in logs being emitted. It also\n\nprovides guidelines for how existing formats can map their values with OpenTelemetry.\n\nAdditional considerations\n\nThe free form of traditional logs makes them incredibly convenient to use without considering their\n\nstructure. If you want to add any data to the logs, just call a function and print anything you'd like;\n\nit'll be great. However, this can pose some challenges. One of these challenges is the opportunity for\n\nleaking potentially private information into the logs and transmitting it to a centralized logging\n\nplatform. This problem applies to all telemetry, but it's particularly easy to do with logs. This is\n\nespecially true when logs contain debugging information, which may include data structures with\n\npasswords fields or private keys. It's good to review any logging calls in the code to ensure the\n\nlogged data does not contain information that should not be logged.\n\nLogs can also be overly verbose, which can cause unexpected volumes to be generated. This may\n\nmake sifting through the logs for useful information difficult, if not impossible, depending on the size\n\nof the environment. It can also lead to unanticipated costs when using centralized logging platforms.\n\nSpecific libraries or frameworks generate much debugging information. Ensuring the correct severity\n\nlevel is configured goes a long towards addressing this concern. However, it's hard to predict just\n\nhow much data will be needed upfront. On more than one occasion, I've responded to alerts in the\n\nmiddle of the night, wishing for a more verbose log level to be configured.\n\nSemantic conventions\n\nHigh-quality telemetry allows the data consumer to find answers to questions when needed.\n\nSometimes critical operations can lack instrumentation causing blind spots in the observability of a\n\nsystem. Other times, the processes are instrumented, but the data is not rich enough to be helpful. The\n\nOpenTelemetry project attempts to solve this through semantic conventions defined in the\n\nspecification. These conventions cover the following:\n\nAttributes that should be present for traces, metrics, and logs.\n\nResource attribute definitions for various types of workloads, including hosts, containers, and functions. The resource attributes\n\ndescribed by the specification also include characteristics specific to multiple popular cloud platforms.\n\nRecommendations for what telemetry should be emitted by components participating in various scenarios such as messaging\n\nsystems, client-server applications, and database interactions.",
      "content_length": 2820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "These semantic conventions help ensure that the data generated when following the OpenTelemetry\n\nspecification is consistent. This simplifies the work of folks instrumenting applications or libraries by\n\nproviding guidelines for what should be instrumented and how. It also means that anyone analyzing\n\ntelemetry produced by standard-compliant code can understand the meaning of the data by\n\nreferencing the specification for additional information.\n\nFollowing semantic conventions recommendations from a specification in a Markdown document\n\ncan be challenging when writing code. Thankfully, OpenTelemetry also provides some tools to help. Adopting semantic conventions\n\nSemantic conventions are great, but it makes sense to turn the recommendations into code to make it\n\npractical for developers to use them. The OpenTelemetry specification repository provides a folder\n\nthat contains the semantic conventions described as YAML for this specific reason\n\n(https://github.com/open-telemetry/opentelemetry-specification/tree/main/semantic_conventions).\n\nThese are combined with the semantic conventions generator (https://github.com/open-\n\ntelemetry/build-tools/blob/v0.7.0/semantic-conventions/) to produce code in various languages. This\n\ncode is shipped as independent libraries in some languages, helping guide developers. We will\n\nrepeatedly rely upon the semantic conventions package in Python in further chapters as we\n\ninstrument application code.\n\nSchema URL\n\nA challenge of semantic conventions is that as telemetry and observability evolve, so will the\n\nterminology used to describe events that we want to observe. An example of this happened when the\n\ndb.hbase.namespace and db.cassandra.keyspace keys were renamed to use db.name instead. Such a\n\nchange would cause problems for anyone already using this field as part of their analysis, or even\n\nalerting. To ensure the semantic conventions can evolve as needed while remaining backward-\n\ncompatible with existing instrumentation, the OpenTelemetry community introduced the schema\n\nURL.\n\nIMPORTANT NOTE\n\nThe OpenTelemetry community understands the importance of backward compatibility in instrumentation code. Going back\n\nand re-instrumenting an application because of a new version of a telemetry library is a pain. As such, a significant amount\n\nof effort has gone into ensuring that components defined in OpenTelemetry remain interoperable with previous versions.\n\nThe project defines its versioning and stability guarantees as part of the specification (https://github.com/open-\n\ntelemetry/opentelemetry-specification/blob/main/specification/versioning-and-stability.md).\n\nThe schema URL is a field added to the telemetry generated for logs, metrics, resources, and traces\n\ntying the emitted telemetry to a version of the semantic conventions. This field allows the producers\n\nand consumers of telemetry to understand how to interpret the data. The schema also provides\n\ninstructions for converting data from one version to another, as per the following example:",
      "content_length": 3028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "1.8.0 schema\n\nfile_format: 1.0.0\n\nschema_url: https://opentelemetry.io/schemas/1.8.0\n\nversions:\n\n1.8.0:\n\nspans:\n\nchanges:\n\nrename_attributes:\n\nattribute_map:\n\ndb.cassandra.keyspace: db.name\n\ndb.hbase.namespace: db.name\n\n1.7.0:\n\n1.6.1:\n\nContinuing with the previous example, imagine a producer of Cassandra telemetry is emitting\n\ndb.cassandra.keyspace as the name for a Cassandra database and specifying the schema as 1.7.0. It\n\nsends the data to a backend that implements schema 1.8.0. By reading the schema URL and\n\nimplementing the appropriate translation, the backend can produce telemetry in its expected version,\n\nwhich is powerful! Schemas decouple systems involved in telemetry, providing them with the\n\nflexibility to evolve independently.\n\nSummary\n\nThis chapter allowed us to learn or review some concepts that will assist us when instrumenting\n\napplications using OpenTelemetry. We looked at the building blocks of distributed tracing, which will\n\ncome in handy when we go through instrumenting our first application with OpenTelemetry in\n\nChapter 4, Distributed Tracing – Tracing Code Execution. We also started analyzing tracing data\n\nusing tools that developers and operators make use of every day.\n\nWe then switched to the metrics signal; first, looking at the minimal contents of a metric, then\n\ncomparing different data types commonly used to produce metrics and their structures. Discussing\n\nexemplars gave us a brief introduction to how correlating metrics with traces can create a more\n\ncomplete picture of what is happening within a system by combining telemetry across signals.\n\nLooking at log formats and searching through logs to find information about the demo application\n\nallowed us to get familiar with yet another tool available in the observability practitioner's toolbelt.",
      "content_length": 1802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Lastly, by leveraging semantic conventions defined in OpenTelemetry, we can begin to produce\n\nconsistent, high-quality data. Following these conventions removes the painful task of naming things,\n\nwhich everyone in the software industry agrees is hard for producers of telemetry. Additionally, these\n\nconventions remove the guesswork when interpreting the data.\n\nKnowing the theory and concepts behind instrumentation and telemetry is excellent to provide us\n\nwith the tools to do all the instrumentation work ourselves. Still, what if I were to tell you it may not\n\nbe necessary to instrument every call in every library manually? The next chapter will cover how\n\nauto-instrumentation looks to help developers in their quest for better visibility into their systems.",
      "content_length": 767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "Chapter 3: Auto-Instrumentation\n\nThe purpose of telemetry is to give people information about systems. This data is used to make\n\ninformed decisions about ways to improve software and prevent disasters from occurring. In the case\n\nof an outage, analytics tools can help us investigate the root cause of the interruption by interpreting\n\ntelemetry. Once the event has been resolved, the recorded traces, metrics, and logs can be correlated\n\nretroactively to gain a complete picture of what happened. In all these cases, the knowledge that's\n\ngained from telemetry assists in solving problems, be it future, present, or past, in applications within\n\nan organization. Being able to see the code is very rarely the bread and butter of an organization,\n\nwhich sometimes makes conversations about investing in observability difficult. Decision-makers\n\nmust constantly make tradeoffs regarding where to invest. The upfront cost of instrumenting code can\n\nbe a deterrent to even getting started, especially if a solution is complicated to implement and will\n\nfail to deliver any value for a long time. Auto-instrumentation looks to alleviate some of the burdens\n\nof instrumenting code manually.\n\nIn this chapter, we will cover the following topics:\n\nWhat is auto-instrumentation?\n\nBytecode manipulation\n\nRuntime hooks and monkey patching\n\nWe will look at some example code in Java and Python, as well as the emitted telemetry, to\n\nunderstand the power of auto-instrumentation. Let's get started!\n\nTechnical requirements\n\nThe application in this chapter simulates the broken telephone game. If you're not familiar with this\n\ngame, it is played by having one person think of a phrase and whisper it to the second player. The\n\nsecond player listens to the best of their ability and whispers it to the third player; this continues until\n\nthe last player shares the message they received with the rest of the group.\n\nEach application represents a player, with the first one printing out the message it is sending, then\n\nplacing the message in a request object that's sent to the next application. The last application in the\n\ngame will print out the message it receives. The following diagram shows the data flow of requests\n\nand responses through the system:",
      "content_length": 2246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Figure 3.1 – Architectural diagram of the example application The communication between each service is done via\n\ngRPC (https://grpc.io), a remote procedure call system developed by Google. For the sake of this chapter, it is enough\n\nto know that the applications do the following:\n\nShare a common understanding of the data structure of a service and a message via the protocol buffer's definition file.\n\nSend data to each other using the protocol.\n\nThe telemetry that's emitted by each application is sent to the OpenTelemetry Collector via the\n\nOpenTelemetry exporter that's configured in each service. The collector then forwards it to Jaeger,\n\nwhich we'll use to visualize tracing information collected.\n\nThe examples in this chapter are provided within Docker containers to make launching them easier;\n\nthis also means you don't need to install separate runtime languages and libraries on your system. If\n\nyou went through the Docker setup steps in the previous chapter, you can skip ahead to Step 3:\n\n1. Ensure Docker is installed on your system by following the instructions on the Docker website (https://docs.docker.com/get-\n\ndocker/). The following command shows the version of Docker that's running on your local system: $ docker version\n\nClient:\n\nCloud integration: 1.0.14\n\nVersion: 20.10.6\n\nAPI version: 1.41\n\nGo version: go1.16.3 ...\n\n2. Verify that docker compose is installed on your system using the following command. If it is not installed, follow the\n\ndirections on the Docker website (https://docs.docker.com/compose/install/) to install it: $ docker compose version\n\nDocker Compose version 2.0.0-beta.1\n\n3. Download a copy of the companion repository from GitHub and launch the Docker environment that's available in the\n\nchapter3 directory: $ git clone https://github.com/PacktPublishing/Cloud-Native-Observability\n\n$ cd Cloud-Native-Observability/chapter03\n\n$ docker compose up",
      "content_length": 1901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "The applications that form the demo system for this chapter are written in JavaScript, Python, Go,\n\nand Java. The code for the application in each language that will be shown in this chapter is also\n\navailable in this book's GitHub repository, in the chapter3 directory; each language is in a separate\n\nfolder. We will look through some of the code in this chapter, but not all of it.\n\nLastly, although it is not a requirement for this chapter, if you're interested in exploring the trace\n\ninformation that's emitted from the demo application, the best way to see it is through the Jaeger web\n\ninterface. The Docker compose environment launches Jaeger along with the demo app, so you can\n\nverify that it is up and running by launching a web browser and visiting http://localhost:16686.\n\nWhat is auto-instrumentation?\n\nIn the very early days of the OpenTelemetry project, a proposal was created to support producing\n\ntelemetry without manual instrumentation. As we mentioned earlier in this book, OpenTelemetry uses\n\nOpenTelemetry Enhancement Proposals or OTEPs to propose significant changes or new work\n\nbefore producing a specification. One of the very first OTEPs to be produced by the project\n\n(https://github.com/open-telemetry/oteps/blob/main/text/0001-telemetry-without-manual-\n\ninstrumentation.md) described the need to support users that wanted to produce telemetry without\n\nhaving to modify the code to do so: Cross-language requirements for automated approaches to\n\nextracting portable telemetry data with zero source code modification. – OpenTelemetry\n\nEnhancement Proposal #0001\n\nBeing able to get started with OpenTelemetry with very little effort for new users was very much a\n\ngoal from the start of the project. The hope was to address one of the pain points of producing\n\ntelemetry: the cost of manual instrumentation.\n\nChallenges of manual instrumentation\n\nInstrumenting an application can be a difficult task. This is especially true for someone who hasn't\n\ndone so before. Instrumenting applications is a skill that takes time and practice to perfect. Some of\n\nthe things that can be challenging when instrumenting code are as follows:\n\nThe libraries and APIs that are provided by telemetry frameworks can be hard to learn how to use. With auto-instrumentation,\n\nusers do not have to learn how to use the libraries and APIs directly; instead, they rely on a simplified user experience that can be\n\ntuned via configuration.\n\nInstrumenting applications can be tricky. This can be especially true for legacy applications where the original author of the code\n\nis no longer around. By reducing the amount of code that needs to be modified, auto-instrumentation reduces the surface of the\n\nchanges that need to be made and minimizes the risks involved.",
      "content_length": 2768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Knowing what to instrument and how it should be done takes practice. The authors of auto-instrumentation tooling and libraries\n\nensure that the telemetry that's produced by auto-instrumentation follows the semantic conventions defined by OpenTelemetry.\n\nAdditionally, it's not uncommon for systems to contain applications written in different languages.\n\nThis adds to the complexity of manually instrumenting code as it requires developers to learn how to\n\ninstrument in multiple languages. Auto-instrumentation provides the necessary tooling to minimize\n\nthe effort here, as the goal of the OpenTelemetry project is to support the same configuration across\n\nlanguages. This means that, in theory, the auto-instrumentation experience will be fairly consistent. I\n\nsay fairly here because the libraries and tools are still changing, so some inconsistencies are being\n\nworked through in the project.\n\nComponents of auto-instrumentation\n\nIn terms of OpenTelemetry, auto-instrumentation is made up of two parts. The first part is composed\n\nof instrumentation libraries. These libraries are provided and supported by members of the\n\nOpenTelemetry community, who use the OpenTelemetry API to instrument popular third-party\n\nlibraries and frameworks in each language. The following table lists some of the instrumentation\n\nlibraries that are provided by OpenTelemetry in various languages at the time of writing:\n\nFigure 3.2 – Some of the available instrumentation libraries in OpenTelemetry Most of these instrumentation libraries\n\nare specific to a particular third-party library of a language. For example, the Boto instrumentation library instruments\n\nmethod calls that are specific to the Boto library. However, there are cases where multiple instrumentation libraries\n\ncould be used to instrument the same thing. An example of this is the Requests and urllib3 instrumentation",
      "content_length": 1874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "libraries, which would, in theory, instrument the same thing since Requests is built on top of urllib3. When choosing\n\ninstrumentation libraries, a good rule of thumb is to find the library that is the most specific to your use case. If more\n\nthan one fits, inspect the data that's emitted by each library to find the one that fits your needs.\n\nThe details of how exactly instrumentation libraries are implemented vary from language to language\n\nand sometimes even from library to library. Some of these details will become clearer as we progress\n\nthrough this chapter and look at some of the mechanisms that are used in Java and Python libraries.\n\nAs we mentioned at the beginning of this section, two components form auto-instrumentation. The\n\nsecond component is a mechanism that's provided by OpenTelemetry to allow users to automatically\n\ninvoke the instrumentation libraries without additional work on the part of the user. This mechanism\n\nis sometimes called an agent or a runner. In practice, the purpose of this tool is to configure\n\nOpenTelemetry and load the instrumentation libraries that can be used to then generate telemetry.\n\nIMPORTANT NOTE\n\nAuto-instrumentation is still being actively developed and the OpenTelemetry specification around auto-instrumentation, its\n\nimplementation, and how configuration should be specified is still in development. The adoption in different languages is,\n\nat the time of writing, in various stages. For the examples in this chapter, the Python and Java examples use full auto-\n\ninstrumentation with both instrumentation libraries and an agent. The JavaScript and Go code only leverage\n\ninstrumentation libraries.\n\nLimits of auto-instrumentation\n\nAuto-instrumentation is a good place to start the journey of instrumenting an application and gaining\n\nmore visibility into its inner workings. However, there are some limitations as to what can be\n\nachieved with automatic instrumentation, all of which we should take into consideration.\n\nThe first limitation may seem obvious, but it is that auto-instrumentation cannot instrument\n\napplication-specific code. As such, the instrumentation that's produced via auto-instrumentation is\n\nalways going to be missing some critical information about your application. For example, consider\n\nthe following simplified code example of a client application making a web request via the\n\ninstrumented requests HTTP library: def do_something_important():\n\n# doing many important things\n\ndef client_request():\n\ndo_something_important()\n\nrequests.get(\"https://webserver\")\n\nIf auto-instrumentation were exclusively used to instrument the previous example, telemetry would\n\nbe generated for the call that was made via the library, but no information would be captured about",
      "content_length": 2753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "what happened when the do_something_important function was called. This would likely be\n\nundesirable as it could leave many questions unanswered.\n\nAnother limitation of auto-instrumentation is that it may instrument things you're not interested in.\n\nThis may result in the same network call being recorded multiple times, or generated data that you're\n\nnot interested in using. An effort is being made in OpenTelemetry to support configuration to give\n\nusers fine-grained control over how telemetry is generated via instrumentation libraries.\n\nWith this in mind, let's learn how auto-instrumentation is implemented in Java.\n\nBytecode manipulation\n\nThe Java implementation of auto-instrumentation for OpenTelemetry leverages the Java\n\nInstrumentation API to instrument code\n\n(https://docs.oracle.com/javase/8/docs/api/java/lang/instrument/Instrumentation.html). This API is\n\ndefined as part of the Java language and can be used by anyone interested in collecting information\n\nabout an application.\n\nOpenTelemetry Java agent\n\nThe OpenTelemetry Java agent is distributed to users via a single Java archive (JAR) file, which can\n\nbe downloaded from the opentelemetry-java-instrumentation repository (https://github.com/open-\n\ntelemetry/opentelemetry-java-instrumentation/releases). The JAR contains the following\n\ncomponents:\n\nThe javaagent module. This is called by the Java Instrumentation API.\n\nInstrumenting libraries for various frameworks and third-party libraries.\n\nThe tooling to initialize and configure the OpenTelemetry components. These will be used to produce telemetry and deliver it to\n\nits destination.\n\nThe JAR is invoked by passing it to the Java runtime via the -javaagent command-line option. The\n\nJava OpenTelemetry agent supports configuration via command-line arguments, also known in Java\n\nas system properties. The following command is an example of how the agent can be used in\n\npractice: java -javaagent:/app/opentelemetry-javaagent.jar \\\n\nDotel.resource.attributes=service.name=broken-telephone-java\\ -Dotel.traces.exporter=otlp\n\n\\\n\njar broken-telephone.jar\n\nNote that the preceding command is also how the demo application is launched inside the container.\n\nUsing the Java agent to load the OpenTelemetry agent gives the library a chance to modify the",
      "content_length": 2276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "bytecode before any other code is executed. The following diagram shows some of the components\n\nthat are involved in the initialization process when the OpenTelemetry agent is used.\n\nOpenTelemetryAgent starts the process, while OpenTelemetryInstaller uses the configuration\n\nprovided at invocation time to configure the emitters of telemetry. Meanwhile, AgentInstaller loads\n\nByte Buddy, an open source library for modifying Java code at runtime, which is used to instrument\n\nthe code via bytecode injection:\n\nFigure 3.3 – OpenTelemetry Java agent loading order\n\nAgentInstaller also loads all the third-party instrumentation libraries that are available in the\n\nOpenTelemetry agent.\n\nIMPORTANT NOTE\n\nThe mechanics of bytecode injection are outside the scope of this book. For the sake of this chapter, it's enough to know\n\nthat the Java agent injects the instrumentation code at runtime. If you're interested in learning more, I recommend\n\nspending some time browsing the Byte Buddy site: https://bytebuddy.net/#/.\n\nThe following code shows the Java code that handles gRPC requests for the broken telephone server.\n\nThe specifics of the code are not overly important here, but pay attention to any instrumentation code\n\nyou can see: BrokenTelephoneServer.java\n\nstatic class BrokenTelephoneImpl extends BrokenTelephoneGrpc.BrokenTelephoneImplBase {\n\n@Override\n\npublic void saySomething(Brokentelephone.BrokenTelephoneRequest req,\n\nStreamObserver<Brokentelephone.BrokenTelephoneResponse> responseObserver) {\n\nBrokentelephone.BrokenTelephoneResponse reply =\n\nBrokentelephone.BrokenTelephoneResponse.newBuilder() .setMessage(\"Hello \" +\n\nreq.getMessage()).build(); responseObserver.onNext(reply);\n\nresponseObserver.onCompleted();",
      "content_length": 1724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "}\n\n}\n\nAs you can see, there is no mention of OpenTelemetry anywhere in the code. The real magic happens\n\nwhen the agent is called at runtime and instruments the application via bytecode injection, as we'll\n\nsee shortly. With this, we now have an idea of how auto-instrumentation works in Java. Now, let's\n\ncompare this to the Python implementation.\n\nRuntime hooks and monkey patching\n\nIn Python, unlike in Java, where a single archive contains everything that's needed to support auto-\n\ninstrumentation, the implementation relies on several separate components that must be discussed to\n\nhelp us fully understand how auto-instrumentation works.\n\nInstrumenting libraries\n\nInstrumentation libraries in Python rely on one of two mechanisms to instrument third-party libraries:\n\nEvent hooks are exposed by the libraries being instrumented, allowing the instrumenting libraries to register and produce\n\ntelemetry as events occur.\n\nAny intercepting calls to libraries are instrumented and are replaced at runtime via a technique known as monkey patching\n\n(https://en.wikipedia.org/wiki/Monkey_patch). The instrumenting library receives the original call, produces telemetry data, and\n\nthen calls the underlying library.\n\nMonkey patching is like bytecode injection in that the applications make calls to libraries without\n\nsuspecting that those calls have been replaced along the way. The following diagram shows how the\n\nopentelemetry-instrumentation-redis monkey patch calls redis.Redis.execute_command to produce\n\ntelemetry data before calling the underlying library:\n\nFigure 3.4 – Monkey-catched call to the Redis library",
      "content_length": 1618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Each instrumentation library adheres to an interface to register and deregister itself. At the time of\n\nwriting, in Python, unlike in Java, the different instrumentation libraries are packaged independently.\n\nThis has the advantage of reducing the number of dependencies that are required to install the\n\ninstrumentation libraries. However, it does have the disadvantage of requiring users to know what\n\npackages they will need to install. There are a few ways to work around this, which we'll explore in\n\nChapter 7, Instrumentation Libraries.\n\nThe Instrumentor interface\n\nTo ensure a consistent experience for the users of instrumentation libraries, as well as ensuring the\n\ndevelopers of those libraries know what needs to be implemented, the OpenTelemetry Python\n\ncommunity has defined the Instrumentor (https://opentelemetry-python-\n\ncontrib.readthedocs.io/en/latest/instrumentation/base/instrumentor.html) interface. This interface\n\nrequires library authors to provide implementations for the following methods:\n\n_instrument: This method contains any initialization logic for the instrumenting library. This is where monkey patching or\n\nregistering for event hooks takes place.\n\n_uninstrument: This method provides the logic to deregister the library from event hooks or remove any monkey patching.\n\nThis may also contain any additional cleanup operations.\n\ninstrumentation_dependencies: This method returns a list of the library and the versions that the instrumentation\n\nlibrary supports.\n\nIn addition to fulfilling the Instrumentor interface, if an instrumentation library wishes to be available\n\nfor auto-instrumentation, it must register itself via an entry point. An entry point is a Python\n\nmechanism that allows modules to make themselves discoverable by registering a class or method via\n\na string at installation time.\n\nIMPORTANT NOTE\n\nAdditional information on entry points is available in the official Python documentation:\n\nhttps://packaging.python.org/specifications/entry-points/.\n\nOther Python code can then load this code by doing a lookup for an entry point by name and\n\nexecuting it.\n\nWrapper script\n\nFor those mechanisms to be triggered, the Python implementation ships a script that can be called to\n\nwrap any Python application. The opentelemetry-instrument script finds all the instrumentations that",
      "content_length": 2327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "have been installed in an environment by loading the entry points registered under the\n\nopentelemetry_instrumentor name.\n\nThe following diagram shows two different instrumentation library packages, opentelemetry-\n\ninstrumentation-foo and opentelemetry-instrumentation-bar, registering a separate Python class in\n\nthe opentelemetry_instrumentor entry point's catalog. This catalog is globally available within the\n\nPython environment and when opentelemetry-instrument is invoked, it searches that catalog and\n\nloads any instrumentation that's been registered by calling the instrument method:\n\nFigure 3.5 – Package registration\n\nThe opentelemetry-instrument script is made available via the opentelemetry-instrumentation\n\nPython package. The following code shows the gRPC server implemented in Python. As you can see,\n\nas with the previous example, it does not mention OpenTelemetry: brokentelephone.py\n\n#!/usr/bin/env python3\n\nfrom concurrent import futures\n\nimport grpc\n\nimport brokentelephone_pb2\n\nimport brokentelephone_pb2_grpc\n\nclass Player(brokentelephone_pb2_grpc.BrokenTelephoneServicer): def SaySomething(self,\n\nrequest, context):\n\nreturn brokentelephone_pb2.BrokenTelephoneResponse(\n\nmessage=\"Hello, %s!\" % request.message\n\n)\n\ndef serve():",
      "content_length": 1249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n\nbrokentelephone_pb2_grpc.add_BrokenTelephoneServicer_to_server(Player(), server)\n\nserver.add_insecure_port(\"[::]:50051\")\n\nserver.start()\n\nserver.wait_for_termination()\n\nif __name__ == \"__main__\":\n\nserve()\n\nAs we saw in the Java code example, the preceding code is strictly application code – there's no\n\ninstrumentation in sight. The following command shows an example of how auto-instrumentation is\n\ninvoked in Python: opentelemetry-instrument ./broken_telephone.py\n\nThe following screenshot shows a trace that's been generated by our sample application. As we can\n\nsee, the originating request was made by the brokentelephone-js service to Python, Go, and finally\n\nthe Java application. The trace information was generated by the gRPC instrumentation library in\n\neach of those languages:\n\nFigure 3.6 – Sample trace generated automatically across all broken telephone services If you'd like to see a trace for\n\nyourself, the demo application should allow you to do so. Just browse to the Jaeger interface at http://localhost:16686\n\nand search for a trace, as we did in Chapter 2, OpenTelemetry Signals – Traces, Metrics, and Logs. The generated\n\ntrace can give us a glimpse into the data flow through our entire sample application. Although the broken telephone is\n\nsomewhat trivial, you can imagine how this information would be useful for mapping information across a distributed\n\nsystem. With very little effort, we're able to see where time is spent in our system.\n\nSummary\n\nWith auto-instrumentation, it's possible to reduce the time that's required to instrument an existing\n\napplication. Reducing the friction to get started with telemetry gives users a chance to try it before",
      "content_length": 1751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "investing significant amounts of time in manual instrumentation. And although the data that's\n\ngenerated via auto-instrumentation is likely not enough to get to the bottom of issues in complex\n\nsystems, it's a solid starting point. Auto-instrumentation can also be quite useful when you're\n\ninstrumenting an unfamiliar system.\n\nThe use of instrumentation libraries allows users to gain insight into what the libraries they're using\n\nare doing, without having to learn the ins and outs of them. The OpenTelemetry libraries that are\n\navailable at the time of writing can be used to instrument existing code by following the online\n\ndocumentation that's been made available by each language. As we'll learn in Chapter 7,\n\nInstrumentation Libraries, using these libraries can be tremendously useful in reducing the code\n\nthat's needed to instrument applications.\n\nIn this chapter, we compared two different implementations of auto-instrumentation by looking at the\n\nJava implementation, which utilizes bytecode injection, and the Python implementation, which uses\n\nruntime hooks and monkey patching. In each case, the implementation leverages features of the\n\nlanguage that allows the implementation to inject telemetry at appropriate times in the code's\n\nexecution. Before diving into auto-instrumentation, however, it is useful to understand how each\n\nsignal can be leveraged independently, starting with distributed tracing. We will do this in the next\n\nchapter.",
      "content_length": 1461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Section 2: Instrumenting an Application\n\nIn this part, you will walk through instrumenting an application by using the signals offered by\n\nOpenTelemetry: distributed tracing, metrics, and logging.\n\nThis part of the book comprises the following chapters:\n\nChapter 4, Distributed Tracing – Tracing Code Execution\n\nChapter 5, Metrics – Recording Measurements\n\nChapter 6, Logging – Capturing Events\n\nChapter 7, Instrumentation Libraries",
      "content_length": 432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Chapter 4: Distributed Tracing – Tracing Code Execution\n\nSo, now that we have an understanding of the concepts of OpenTelemetry and are familiar with the\n\ndifferent signals it covers, it's time to start instrumenting application code. In Chapter 2,\n\nOpenTelemetry Signals – Tracing, Metrics, and Logging, we covered the terminology and concepts of\n\nthose signals by looking at a system that was instrumented with OpenTelemetry. Now, it's time to get\n\nhands-on with some code to start generating telemetry ourselves, and to do this, we're going to first\n\nlook at implementing a tracing signal.\n\nIn this chapter, we will cover the following topics:\n\nConfiguring OpenTelemetry\n\nGenerating tracing data\n\nEnriching the data with attributes, events, and links\n\nAdding error handling information\n\nBy the end of this chapter, you'll have instrumented several applications with OpenTelemetry and be\n\nable to trace how those applications are connected via distributed tracing. This will start giving you a\n\nsense of how distributed tracing can be used in your own applications going forward.\n\nTechnical requirements\n\nAt the time of writing, OpenTelemetry for Python supports Python 3.6+. All Python examples in this\n\nbook will use Python 3.8, which can be downloaded and installed by following the instructions at\n\nhttps://docs.python.org/3/using/index.html. The following command can verify which version of\n\nPython is installed. It's possible for multiple versions to be installed simultaneously on a single\n\nsystem, which is why both python and python3 are shown here: $ python --version\n\n$ python3 --version\n\nIt is recommended to use a virtual environment to run the examples in this book\n\n(https://docs.python.org/3/library/venv.html). A virtual environment in Python allows you to install\n\npackages in isolation from the rest of the system, meaning that if anything goes wrong, you can\n\nalways delete the virtual environment and start a fresh one. The following commands will create a\n\nnew virtual environment in a folder called cloud_native_observability: $ mkdir\n\ncloud_native_observability\n\n$ python3 -m venv cloud_native_observability\n\n$ source cloud_native_observability/bin/activate",
      "content_length": 2184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "The example code in this chapter will rely on a few different third-party libraries – Flask and\n\nRequest. The following command will install all the required packages for this chapter using the\n\npackage installation for Python, pip: $ pip install flask requests\n\nNow that we have a virtual environment configured and the libraries needed, we will install the\n\nnecessary Python packages to use OpenTelemetry. The main libraries we'll need for this section are\n\nthe API and SDK packages: $ pip install opentelemetry-api opentelemetry-sdk\n\nThe pip freeze command lists all the installed packages in this Python environment; we can use it to\n\nconfirm whether the correct packages are installed: $ pip freeze | grep opentelemetry\n\nopentelemetry-api==1.3.0\n\nopentelemetry-sdk==1.3.0\n\nopentelemetry-semantic-conventions==0.22b0\n\nThe version of the packages installed in your environment may differ, as the OpenTelemetry project\n\nis still very much under active development, and releases are pretty frequent. It's important to\n\nremember this as we work through the examples, as some methods may be slightly different, or the\n\noutput may vary.\n\nIMPORTANT NOTE\n\nThe OpenTelemetry APIs should not change unless a major version is released.\n\nConfiguring the tracing pipeline\n\nWith the packages now installed, we're ready to take our first step to generate distributed traces with\n\nOpenTelemetry – configuring the tracing pipeline. The tracing pipeline is what allows the tracing\n\ndata we explored in Chapter 2, OpenTelemetry Signals – Traces, Metrics, and Logs, to be generated\n\nwhen the OpenTelemetry API calls are made. The pipeline also defines where and how the data will\n\nbe emitted. Without a tracing pipeline, a no-op implementation is used by the API, meaning the code\n\nwill not generate distributed traces. The tracing pipeline configures the following:\n\nTracerProvider to determine how spans should be generated\n\nA Resource object, which identifies the source of the spans\n\nSpanProcessor to describe how spans will be exported\n\nSpanExporter to describe where the spans will be exported\n\nThe following code imports the TracerProvider, ConsoleSpanExporter, and SimpleSpanProcessor\n\nclasses from the SDK to configure a tracer provider. In this example, ConsoleSpanExporter will be\n\nused to output traces from the application to the console. The last step to configure the tracer provider",
      "content_length": 2382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "is to call the set_tracer_provider method, which will set the global tracer provider to the provider\n\nwe instantiated. The code will be placed in the configure_tracer method, which will be called before\n\nwe do anything else in the code: shopper.py\n\n#!/usr/bin/env python3\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor def\n\nconfigure_tracer():\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = SimpleSpanProcessor(exporter)\n\nprovider = TracerProvider()\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nif __name__ == \"__main__\":\n\nconfigure_tracer()\n\nThroughout this chapter, as we iterate over the application and add more code, each time we do so,\n\nwe will test the code and inspect its output using the following command, unless specified otherwise:\n\n$ python ./shopper.py\n\nRunning this command for the initial code will not output anything. This allows us to confirm that the\n\nmodules have been found and imported correctly, and that the code doesn't have any errors in it.\n\nIMPORTANT NOTE\n\nA common mistake when first configuring TracerProvider is to forget to set the global TracerProvider, causing the\n\nAPI to use a default no-op implementation of TracerProvider. This default is configured intentionally for the use case\n\nwhere a user does not wish to enable tracing within their application.\n\nAlthough it may not seem like much, configuring TracerProvider for an application is a critical first\n\nstep before we can start collecting distributed traces. It's a bit like gathering all the ingredients before\n\nbaking a cake, so let's get baking!\n\nGetting a tracer\n\nWith the tracing pipeline configured, we can now obtain the generator for our tracing data, Tracer.\n\nThe TracerProvider interface defines a single method to allow us to obtain a tracer, get_tracer. This\n\nmethod requires a name argument and, optionally, a version argument, which should reflect the name\n\nand version of the instrumenting module. This information is valuable for users to quickly identify",
      "content_length": 2131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "what the source of the tracing data is. An example shown in Figure 4.1 shows how the values passed\n\ninto get_tracer will vary, depending on where the call is made. Inside the library calls to requests\n\nand Flask, the name and version will reflect those libraries, whereas in the shopper and\n\ngrocery_store modules, the name and version will reflect those modules.\n\nFigure 4.1 – The tracer name and version configuration at different stages of an application To get the first tracer, the\n\nfollowing code will be added to shopper.py immediately at the end of configure_tracer to return a tracer from the\n\nmethod: shopper.py\n\ndef configure_tracer():\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = SimpleSpanProcessor(exporter)\n\nprovider = TracerProvider()\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(\"shopper.py\", \"0.0.1\")\n\nif __name__ == \"__main__\":\n\ntracer = configure_tracer()\n\nIt's important to remember to make the name and version meaningful; the name should be unique\n\nwithin the scope of the application it instruments. The instrumentation scope could be a package,\n\nmodule, or even a class. It's finally time to start using this tracer and trace the application! There are\n\nseveral ways to create a span in OpenTelemetry; let's explore them now.\n\nGenerating tracing data\n\nIt's finally time to start generating telemetry from the application! There are several ways to create a\n\nspan in OpenTelemetry; the first one we'll use is to call start_span on the tracer instance we",
      "content_length": 1546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "obtained previously. This will create a span object, using the only required string argument as the\n\nname of the span. The span object is the building block of distributed tracing and is intended to\n\nrepresent a unique unit of work in our application. In the following example, we will create a new\n\nSpan object before calling a method that will do some work. Since our application is a shopper, the\n\nfirst thing the shopper will do is browse the store. In order for the tracing data to be useful, it's\n\nimportant to use a meaningful name in the creation of the span. Once browse has returned, we will\n\ncall end on the span object to signal that the work is complete: shopper.py\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nif __name__ == \"__main__\":\n\ntracer = configure_tracer()\n\nspan = tracer.start_span(\"visit store\")\n\nbrowse()\n\nspan.end()\n\nRunning the code will output our first trace to the console. The ConsoleSpanExporter automatically\n\noutputs the data as formatted JSON to make it easier to read: shopper.py output\n\nvisiting the grocery store\n\n{\n\n\"name\": \"visit store\",\n\n\"context\": {\n\n\"trace_id\": \"0x4c6fd97f286439b1a4bb109f12bf2095\", \"span_id\": \"0x6ea2219c865f6c4b\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": null,\n\n\"start_time\": \"2021-06-26T20:26:47.176169Z\", \"end_time\": \"2021-06-26T20:26:47.176194Z\",\n\n\"status\": {\n\n\"status_code\": \"UNSET\"\n\n},\n\n\"attributes\": {},\n\n\"events\": [],\n\n\"links\": [],\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",",
      "content_length": 1489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"service.name\": \"unknown_service\"\n\n}\n\n}\n\nSome of the data worth noting in the preceding output is as follows:\n\nThe name field of the span we provided.\n\nThe automatically generated trace and span identifiers – trace_id and span_id.\n\nThe start_time and end_time timestamps, which can be used to calculate the duration of an operation being traced.\n\nThe parent_id identifier is not set. This identifies the span created as the beginning of a trace, otherwise known as root\n\nspan.\n\nThe status_code field of the span is set to UNSET by default.\n\nWith the preceding span information from the JSON output, we now have the first piece of data about\n\nthe work that our application is doing. One of the most critical pieces of information generated in this\n\ndata is trace_id. This trace identifier is a 128-bit integer that allows operations to be tied together in\n\na distributed trace and represents the single request through the entire system. span_id is a 64-bit\n\ninteger used to identify the specific unit of work in the request and also relationships between\n\ndifferent operations. In this next code example, we'll add another operation to our trace to see how\n\nthis identifier works, but we'll need to take a brief detour to look at the Context API before continuing\n\ntoo far.\n\nIMPORTANT NOTE\n\nThe examples in this chapter will only use ConsoleSpanExporter. We will explore additional exporters in Chapter 8,\n\nThe OpenTelemetry Collector, and Chapter 10, Configuring a Backend, when we look at the OpenTelemetry Collector and\n\ndifferent backends.\n\nThe Context API\n\nIn order to tie spans together, we'll need to activate our spans before starting new ones. Activating a\n\nspan in OpenTelemetry is synonymous with setting the span in the current context object. The\n\nContext object is a mechanism used across signals to share data about the application either in-\n\nprocess or across API boundaries via propagation. No matter where you are in the application code,\n\nit's possible to get the current span by using the Context API. The Context object can be thought of as\n\nan immutable data store with a consistent API across implementations. In Python, the implementation\n\nrelies on ContextVars, as previously discussed in Chapter 1, The History and Concepts of",
      "content_length": 2328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Observability, but not all languages have the notion of a context built into the language itself. The\n\nContext API ensures that users will have a consistent experience when using OpenTelemetry. The\n\nAPI definition for interacting with the context is fairly minimal:\n\nget_value: Retrieves a value for a given key from the context. The only required argument is a key and, optionally, a\n\ncontext argument. If no context is passed in, the value returned will be pulled from the global context.\n\nset_value: Stores a value for a certain key in the context. The method receives a key, value, and optionally, a context argument\n\nto set the value into. As mentioned before, the context is immutable, so the return value is a new Context object with the new\n\nvalue set.\n\nattach: Calling attach associates the current execution with a specified context. In other words, it sets the current context to\n\nthe context passed in as an argument. The return value is a unique token, which is used by the detach method described next.\n\ndetach: To return the context to its previous state, this method receives a token that was obtained by attaching to another\n\ncontext. Upon calling it, the context that was current at the time attach was called is restored.\n\nDon't worry if the description doesn't quite make sense yet; the next example will help clarify things.\n\nIn the following code, we activate the span by setting it in the context via the set_span_in_context\n\nmethod, which, under the hood, calls the current context's set_value method. The return value of this\n\ncall is a new immutable context object, which we can then attach to before starting the second span:\n\nshopper.py\n\nfrom opentelemetry import context, trace\n\nif __name__ == \"__main__\":\n\ntracer = configure_tracer()\n\nspan = tracer.start_span(\"visit store\")\n\nctx = trace.set_span_in_context(span)\n\ntoken = context.attach(ctx)\n\nspan2 = tracer.start_span(\"browse\")\n\nbrowse()\n\nspan2.end()\n\ncontext.detach(token)\n\nspan.end()\n\nRunning the application and looking at the output once again, we can now see that the trace_id value\n\nfor both spans is the same. We can also see that the browse span has a parent_id field that matches\n\nspan_id of the visit store span: shopper.py output\n\nvisiting the grocery store\n\n{\n\n\"name\": \"browse\",",
      "content_length": 2272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "\"context\": {\n\n\"trace_id\": \"0x03c197ae7424cc492ab1c92112490be1\", \"span_id\": \"0xb7396b0e6ccab2fd\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": \"0x8dd8c60c67518a8d\", }\n\n{\n\n\"name\": \"visit store\",\n\n\"context\": {\n\n\"trace_id\": \"0x03c197ae7424cc492ab1c92112490be1\", \"span_id\": \"0x8dd8c60c67518a8d\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": null,\n\n}\n\nStarting and ending spans manually can be useful in many cases, but as demonstrated by the previous\n\ncode, managing the context manually can be somewhat cumbersome. More often than not, it is easier\n\nin Python to use a context manager to wrap the work we want to trace. The start_as_current_span\n\nconvenience method allows us to do exactly this by creating a new Span object, setting it as the\n\ncurrent span in a context, and calling the attach method. Additionally, it will automatically end the\n\nspan once the context has been exited. The following code shows us how we can simplify the\n\nprevious code we wrote: shopper.py\n\nif __name__ == \"__main__\":\n\ntracer = configure_tracer()\n\nwith tracer.start_as_current_span(\"visit store\"):\n\nwith tracer.start_as_current_span(\"browse\"):\n\nbrowse()\n\nThis method simplifies the code quite a bit. The automatic management of the context can be used to\n\nquickly create hierarchies of spans. In the following code, we will add one new method and one more\n\nspan. We'll then run the code to observe how each span will use the previous span in the context as\n\nthe new span's parent: shopper.py\n\ndef add_item_to_cart(item):\n\nprint(\"add {} to cart\".format(item))\n\nif __name__ == \"__main__\":",
      "content_length": 1615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "tracer = configure_tracer()\n\nwith tracer.start_as_current_span(\"visit store\"):\n\nwith tracer.start_as_current_span(\"browse\"):\n\nbrowse()\n\nwith tracer.start_as_current_span(\"add item to cart\"):\n\nadd_item_to_cart(\"orange\")\n\nRunning the shopper application, we're starting to see what is appearing to be more and more like a\n\nreal trace. Looking at the output from the new code, we can see three different operations captured.\n\nThe order in which output appears in your terminal may vary; we will review operations in the same\n\norder in which they appear in the code. The first operation to look at is visit store, as mentioned\n\npreviously; the root span can be identified by the parent_id field being null: shopper.py output\n\n{\n\n\"name\": \"visit store\",\n\n\"context\": {\n\n\"trace_id\": \"0x9251fa73b421a143a7654afb048a4fc7\", \"span_id\": \"0x08c9bf4cccd7ba5d\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": null,\n\n\"start_time\": \"2021-06-26T21:43:20.441933Z\",\n\n\"end_time\": \"2021-06-26T21:43:20.442222Z\",\n\n\"status\": {\n\n\"status_code\": \"UNSET\"\n\n},\n\n\"attributes\": {},\n\n\"events\": [],\n\n\"links\": [],\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"service.name\": \"unknown_service\"\n\n}\n\n}",
      "content_length": 1267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "The next operation to review in the output is the browse span. Note that the span's parent_id\n\nidentifier is equal to the span_id identifier of the visit store span. trace_id also matches, which\n\nindicates that the spans are connected in the same trace: shopper.py output\n\n{\n\n\"name\": \"browse\",\n\n\"context\": {\n\n\"trace_id\": \"0x9251fa73b421a143a7654afb048a4fc7\", \"span_id\": \"0xa77587668be46030\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": \"0x08c9bf4cccd7ba5d\", \"start_time\": \"2021-06-26T21:43:20.442091Z\",\n\n\"end_time\": \"2021-06-26T21:43:20.442212Z\",\n\n\"status\": {\n\n\"status_code\": \"UNSET\"\n\n},\n\n\"attributes\": {},\n\n\"events\": [],\n\n\"links\": [],\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"service.name\": \"unknown_service\"\n\n}\n\n}\n\nThe last span to review is the add item to cart span. As with the previous span, its trace_id\n\nidentifier will also match the previous spans. In this case, the parent_id identifier of the add item to\n\ncart span now matches the span_id identifier of the browse span: shopper.py output\n\n{\n\n\"name\": \"add item to cart\",\n\n\"context\": {\n\n\"trace_id\": \"0x9251fa73b421a143a7654afb048a4fc7\", \"span_id\": \"0x6470521265d80512\",\n\n\"trace_state\": \"[]\"\n\n},",
      "content_length": 1268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "\"kind\": \"SpanKind.INTERNAL\",\n\n\"parent_id\": \"0xa77587668be46030\", \"start_time\": \"2021-06-26T21:43:20.442169Z\",\n\n\"end_time\": \"2021-06-26T21:43:20.442191Z\",\n\n\"status\": {\n\n\"status_code\": \"UNSET\"\n\n},\n\n\"attributes\": {},\n\n\"events\": [],\n\n\"links\": [],\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"service.name\": \"unknown_service\"\n\n}\n\n}\n\nNot too bad – the code looks much simpler than the previous example, and we can already see how\n\neasy it is to trace code in applications. The last method we can use to start a span is by using a\n\ndecorator. A decorator is a convenient way to instrument code without having to add any tracing\n\nspecific information to the code itself. This makes the code a bit cleaner.\n\nIMPORTANT NOTE\n\nUsing the decorator means you will need to keep an instance of a tracer initialized and available globally for the decorators\n\nto be able to use it.\n\nRefactoring the shopper.py code, we will move the instantiation of the tracer out of the main method\n\nand add decorators to each of the methods we've defined previously. Note that the code in main is\n\nsimplified significantly: shopper.py\n\ntracer = configure_tracer()\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nadd_item_to_cart(\"orange\")\n\n@tracer.start_as_current_span(\"add item to cart\")\n\ndef add_item_to_cart(item):\n\nprint(\"add {} to cart\".format(item))",
      "content_length": 1454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "@tracer.start_as_current_span(\"visit store\")\n\ndef visit_store():\n\nbrowse()\n\nif __name__ == \"__main__\":\n\nvisit_store()\n\nRun the program once again; the spans will be printed as they were before. The output will not have\n\nchanged with this refactor, but the code looks much cleaner. As with the previous example, context\n\nmanagement is handled for us, so we don't need to worry about interacting with the Context API.\n\nReading the code is much simpler with decorators, and it's also easy for someone new to the code to\n\nimplement new methods with the same pattern when adding code to the application.\n\nSpan processors\n\nA quick note about the span processor used in the code so far – the initial configuration of the tracing\n\npipeline used SimpleSpanProcessor. This does all of its processing in line with the export happening\n\nas soon as the span ends. This means that every span added to the code will add latency in the\n\napplication, which is generally not what we want. This may be the right choice in some cases – for\n\nexample, if it's impossible to guarantee that threads other than the main thread will finish before a\n\nprogram is interrupted. However, it's generally recommended that span processing happens out of\n\nband from the main thread. An alternative to SimpleSpanProcessor is BatchSpanProcessor. Figure 4.2\n\nshows how the execution of the program is interrupted by SimpleSpanProcessor to export a span,\n\nwhereas with BatchSpanProcessor, another thread handles the export operation:\n\nFigure 4.2 – SimpleSpanProcessor versus BatchSpanProcessor",
      "content_length": 1554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "As the name suggests, BatchSpanProcessor groups the export of spans. It does this by launching a\n\nseparate thread that exports spans on a schedule or when there's a certain number of items in the\n\nqueue. This prevents adding unnecessary latency to the normal application code paths. Configuring\n\nBatchSpanProcessor is done much like SimpleSpanProcessor. For the remainder of the examples in\n\nthis chapter, we will use this new processor. The following refactor updates the imports and code in\n\nthe tracer configuration to use BatchSpanProcessor: shopper.py\n\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter def\n\nconfigure_tracer():\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter) provider = TracerProvider()\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(\"shopper.py\", \"0.0.1\")\n\nRun the application now to confirm that the program still works and that the output is the same with\n\nthe new span processor in place. Although it may not look like much has changed, if you look closely\n\nat the start_time and end_time fields of each span produced, the duration of each span has changed.\n\nFigure 4.3 shows a chart comparing output from running the program with each type of span\n\nprocessor. The duration of the visit store span is significantly shorter using BatchSpanProcessor\n\nbecause the processing of each span is happening asynchronously:\n\nFigure 4.3 – Span durations using SimpleSpanProcessor and BatchSpanProcessor Even though microseconds may\n\nnot seem like much in our example, this type of performance impact is critical to systems in production.\n\nBatchSpanProcessor is a much better choice for running real-world applications. Now, we have a better sense of\n\nhow to generate tracing data via the API, but the data we've produced so far could be improved. It doesn't have nearly\n\nenough details to make it truly useful, so let's tackle that next.\n\nEnriching the data",
      "content_length": 1998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "You may have noticed in output from the previous examples that each span emitted contains a\n\nresource attribute. The resource attribute provides an immutable set of attributes, representing the\n\nentity producing the telemetry. resource attributes are not specific to tracing. Any signal that emits\n\ntelemetry leverages resource attributes by adding them to the data produced at export time. As\n\ncovered in Chapter 1, The History and Concepts of Observability, the resource in an application is\n\nassociated with the telemetry generator, which, in the case of tracing, is TracerProvider. The\n\nresource attribute on the span output we've seen so far is automatically provided by the SDK with\n\nsome information about the SDK itself, as well as a default service.name. The service name is used\n\nby many backends to identify the services sending traces to them; however, as you can see, the\n\ndefault value of unknown_service is not a super useful name. Let's fix this. The following code will\n\ncreate a new Resource object with a service name and version number, and then pass it in as an\n\nargument to the TracerProvider constructor: shopper.py\n\nfrom opentelemetry.sdk.resources import Resource\n\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter def\n\nconfigure_tracer():\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter)\n\nresource = Resource.create(\n\n{\n\n\"service.name\": \"shopper\",\n\n\"service.version\": \"0.1.2\",\n\n}\n\n)\n\nprovider = TracerProvider(resource=resource) provider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(\"shopper.py\", \"0.0.1\")\n\nThe output from the application will now include the information added in the resource attribute\n\nalong with the automatically populated data, as shown here: \"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"service.name\": \"shopper\",\n\n\"service.version\": \"0.1.2\"",
      "content_length": 2041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "}\n\nThis is much more useful than unknown_service; however, we now have the name and version\n\nhardcoded in two places. Even worse, the names and versions don't match. Let's fix this before going\n\nfurther by refactoring the configure_tracer method to expect the name and version arguments, as\n\nfollows: shopper.py\n\ndef configure_tracer(name, version): exporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter)\n\nresource = Resource.create(\n\n{\n\n\"service.name\": name,\n\n\"service.version\": version,\n\n}\n\n)\n\nprovider = TracerProvider(resource=resource)\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(name, version) tracer = configure_tracer(\"shopper\", \"0.1.2\") After\n\nrunning the application, the output should remain the same as it was before the change.\n\nThe code is now less error-prone, as we only have one place to set the service name and\n\nversion information, and configure_tracer can be reused to configure OpenTelemetry for\n\ndifferent applications, which will come in handy shortly.\n\nSome additional information you may want to populate in the resource includes things such as the\n\nhostname or, in the case of a dynamic runtime environment, an instance identifier of some sort. The\n\nOpenTelemetry SDK provides an interface to provide some of the details about a resource\n\nautomatically; this is known as the ResourceDetector interface.\n\nResourceDetector\n\nAs its name suggests, the purpose of the ResourceDetector attribute is to detect information that will\n\nautomatically be populated into a resource. A resource detector is a great way to extract information\n\nabout a platform running an application, and there are already existing detectors for some popular\n\ncloud providers. This information can be a useful way to group applications by region or host when\n\ntrying to pinpoint application performance issues. The interface for ResourceDetector specifies a\n\nsingle method to implement, detect, which returns a resource. Let's implement a ResourceDetector\n\ninterface that we can reuse in all the services of the grocery store. This detector will automatically fill",
      "content_length": 2148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "in the hostname and IP address of the machine running the code; to accomplish this, Python's socket\n\nlibrary will come in handy. Place the following code in a new file in the same directory as\n\nshopper.py: local_machine_resource_detector.py\n\nimport socket\n\nfrom opentelemetry.sdk.resources import Resource, ResourceDetector\n\nclass LocalMachineResourceDetector(ResourceDetector):\n\ndef detect(self):\n\nhostname = socket.gethostname()\n\nip_address = socket.gethostbyname(hostname)\n\nreturn Resource.create(\n\n{\n\n\"net.host.name\": hostname,\n\n\"net.host.ip\": ip_address,\n\n}\n\n)\n\nTo make use of this new module, let's import it into the shopper application. The code in\n\nconfigure_tracer will also be updated to call this new ResourceDetector first, before adding the\n\nservice name and version information. As mentioned earlier, a resource is immutable, meaning that\n\nthere's no method to call to update a specific resource. Adding new attributes to the resource\n\ngenerated by our resource detector is done via a call to a resource's merge method. merge creates a\n\nnew resource from the caller's attributes and then updates that new resource to include all the\n\nattributes of the resource passed in as an argument. The following update to the code imports the\n\nmodule we just created, and creates a new resource by calling LocalMachineResourceDetector and\n\ncalls merge to ensure that our previous resource information is not lost: shopper.py\n\nfrom opentelemetry.sdk.resources import Resource\n\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor from\n\nlocal_machine_resource_detector import LocalMachineResourceDetector\n\ndef configure_tracer(name, version):\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = SimpleSpanProcessor(exporter)\n\nlocal_resource = LocalMachineResourceDetector().detect()\n\nresource = local_resource.merge(\n\nResource.create(",
      "content_length": 1919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "{\n\n\"service.name\": name,\n\n\"service.version\": version,\n\n}\n\n)\n\n)\n\nprovider = TracerProvider(resource=resource)\n\nreturn trace.get_tracer(name, version)\n\nThe output from running the code will now contain all the resources seen in the previous example,\n\nbut it will also include the information generated by LocalMachineResourceDetector: \"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.3.0\",\n\n\"net.host.name\": \"myhost.local\",\n\n\"net.host.ip\": \"192.168.128.47\",\n\n\"service.name\": \"shopper\",\n\n\"service.version\": \"0.1.2\"\n\n}\n\nIMPORTANT NOTE\n\nIf the same resource attribute is included in both the caller and the resource passed into merge, the attributes of the\n\nargument resource will override the caller. For example, if resource_one has an attribute of foo=one and\n\nresource_two has an attribute of foo=two, the resulting resource from calling\n\nresource_one.merge(resource_two) will have an attribute of foo=two.\n\nFeel free to play around with ResourceDetector and see what other useful information you can add\n\nabout your machine. Try adding some environment variables or the version of Python running on\n\nyour system; this can be valuable when troubleshooting applications!\n\nSpan attributes\n\nLooking through the tracing data being emitted, we can start to get an idea of what is happening in\n\nthe code we're writing. Now, let's figure out what data we should add about our shopper to make this\n\ntrace even more useful. As the shopper application will be used as an HTTP client, we can take a look\n\nat the semantic conventions available in the specification to inspire us; Figure 4.4\n\n(https://github.com/open-telemetry/opentelemetry-",
      "content_length": 1699,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "specification/blob/main/specification/trace/semantic_conventions/http.md#http-client-server-\n\nexample) shows us the span attributes to add if we want to adhere to OpenTelemetry's semantic\n\nconventions, as well as some sample values:\n\nFigure 4.4 – The HTTP client span attributes semantic conventions\n\nA valid attribute must be either a string, a 64-bit integer, a float, or a Boolean. An attribute can also\n\nbe an array of any of those values, but it must be a homogenous array, meaning the elements of the\n\narray must be a single type.\n\nIMPORTANT NOTE\n\nNull or None values are not encouraged in attributes, as the handling of null values in backends may differ and thus\n\ncreate unexpected behavior.\n\nIn the next example, we will update the browse method to include the recommended attributes for a\n\nclient application. Since we're using decorators here, we'll need to get the current span by calling the\n\nget_current_span method. Once we have the span, we can call the set_attribute method, which\n\nrequires two arguments – the key to set and the value. Since we have not yet started the server, we'll\n\nset a placeholder value for http.url and net.peer.ip: shopper.py\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nspan = trace.get_current_span()\n\nspan.set_attribute(\"http.method\", \"GET\")\n\nspan.set_attribute(\"http.flavor\", \"1.1\")\n\nspan.set_attribute(\"http.url\", \"http://localhost:5000\")\n\nspan.set_attribute(\"net.peer.ip\", \"127.0.0.1\")\n\nLooking at the output from running the program, we will expect to see the attributes added to the\n\nbrowse span; let's take a look: \"name\": \"browse\",",
      "content_length": 1632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "\"attributes\": {\n\n\"http.method\": \"GET\",\n\n\"http.flavor\": \"1.1\",\n\n\"http.url\": \"http://localhost:5000\",\n\n\"net.peer.ip\": \"127.0.0.1\"\n\n},\n\nExcellent! The data is there. It's a bit inconvenient to make independent calls to a method when\n\nwanting to set multiple attributes; thankfully, there's a convenient method to address this. The code\n\ncan be simplified by making a single call to set_attributes and passing in a dictionary with the\n\nsame values: shopper.py\n\nspan.set_attributes(\n\n{\n\n\"http.method\": \"GET\",\n\n\"http.flavor\": \"1.1\",\n\n\"http.url\": \"http://localhost:5000\",\n\n\"net.peer.ip\": \"127.0.0.1\",\n\n}\n\n)\n\nSetting so many attributes, it can be easy for a typo to sneak in. This would, at best, be caught during\n\na review but, at worst, could mean missing some critical data. Imagine a scenario where some\n\nalerting is configured to rely on the url and flavor attributes, but somewhere along the way, flavor is\n\nspelled as flavour. The correctness of the tracing data is critical, and to make setting these attributes\n\nmore easy, a semantic conventions package provides constants that can be used instead of hardcoding\n\ncommon keys and values. The following is a refactor of the code to make use of the opentelemetry-\n\nsemantic-conventions package: shopper.py\n\nfrom opentelemetry.semconv.trace import HttpFlavorValues, SpanAttributes\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_METHOD: \"GET\", SpanAttributes.HTTP_FLAVOR:\n\nHttpFlavorValues.HTTP_1_1.value, SpanAttributes.HTTP_URL: \"http://localhost:5000\",",
      "content_length": 1626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "SpanAttributes.NET_PEER_IP: \"127.0.0.1\", }\n\n)\n\nOf course, using semantic conventions alone may not give us enough information about the specifics\n\nof the application. One of the powers of attributes is to add meaningful data about the transaction\n\nbeing traced to allow us to understand what happened. One aspect of the shopper application that will\n\nlikely be unique once we start processing real data is information about the items and quantities\n\nadded to the cart. The following code adds attributes to the span to record that information:\n\nshopper.py\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_METHOD: \"GET\",\n\nSpanAttributes.HTTP_FLAVOR: str(HttpFlavorValues.HTTP_1_1),\n\nSpanAttributes.HTTP_URL: \"http://localhost:5000\",\n\nSpanAttributes.NET_PEER_IP: \"127.0.0.1\",\n\n}\n\n)\n\nadd_item_to_cart(\"orange\", 5)\n\n@tracer.start_as_current_span(\"add item to cart\")\n\ndef add_item_to_cart(item, quantity):\n\nspan = trace.get_current_span()\n\nspan.set_attributes({\n\n\"item\": item,\n\n\"quantity\": quantity,\n\n})\n\nprint(\"add {} to cart\".format(item))\n\nThe topic of span attributes will be revisited when we introduce the server later in this chapter.\n\nAttributes are also a key component of other signals, so we'll come back to them throughout the\n\nbook. One last thing to be aware of when thinking of attributes, and really any data being recorded in\n\ntraces, is to be cognizant of Personally Identifiable Information (PII). Whenever possible, save",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "yourself the trouble and remove all PII from the telemetry. We'll cover more on this topic in Chapter\n\n8, OpenTelemetry Collector.\n\nSpanKind\n\nAnother piece of information that is useful about a span is SpanKind. SpanKind is a qualifier that\n\ncategorizes the span and provides additional information about the relationship between spans in a\n\ntrace. The following categories for span kinds are defined in OpenTelemetry:\n\nINTERNAL: This indicates that the span represents an operation that is internal to an application, meaning that this specific\n\noperation has no external dependencies or relationships. This is the default value for a span when not set.\n\nCLIENT: This identifies the span as an operation making a request to a remote service, which should be identified as a server\n\nspan. The request made by this operation is synchronous, and the client should wait for a response from the server.\n\nSERVER: This indicates that the span is an operation responding to a synchronous request from a client span. In a client/server,\n\nthe client is identified as the parent span to the server, as it is the originator of the request.\n\nPRODUCER: This identifies the operation as an originator of an asynchronous request. Unlike in the case of the client span, the\n\nproducer is not expecting a response from the consumer of the asynchronous request.\n\nCONSUMER: This identifies the operation as a consumer of an asynchronous request from a producer.\n\nAs you may have noticed so far, all the spans that we've created have been identified as internal. The\n\nfollowing information can be found throughout the output we've generated until now: \"kind\":\n\n\"SpanKind.INTERNAL\"\n\nNow is a good time to start making the shopper application a bit more realistic by adding some calls\n\nto a grocery store server. Knowing that this will be a client using HTTP requests to retrieve data from\n\nthe server, we will set SpanKind to CLIENT on the operation that makes a call to the server. On the\n\nreceiving side, we will set SpanKind on the operation that is responding to the request to SERVER. The\n\nway to set kind is by passing the kind argument when creating the span. The following code adds a\n\nweb request from the client to the server in the browse method. The HTTP request will be facilitated\n\nby using the requests (https://docs.python-requests.org/) library. The request to the server will be\n\nwrapped by a context manager, which starts a new span named web request with the kind set to\n\nCLIENT: shopper.py\n\nimport requests\n\nfrom common import configure_tracer\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nwith tracer.start_as_current_span(",
      "content_length": 2671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "\"web request\", kind=trace.SpanKind.CLIENT\n\n) as span:\n\nurl = \"http://localhost:5000\"\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_METHOD: \"GET\",\n\nSpanAttributes.HTTP_FLAVOR: str(HttpFlavorValues.HTTP_1_1),\n\nSpanAttributes.HTTP_URL: url,\n\nSpanAttributes.NET_PEER_IP: \"127.0.0.1\",\n\n}\n\n)\n\nresp = requests.get(url)\n\nspan.set_attribute(SpanAttributes.HTTP_STATUS_CODE, resp.status_code)\n\nSo far, all the code written was done on the client side; let's talk about the server side. Before starting\n\non the server, in order to reduce the duplication of code, configure_tracer has been moved into a\n\nseparate common.py module and placed in the same directory as the rest of the code. In this refactor,\n\nwe've also updated the previously hardcoded service.name and service.version attribute keys to use\n\nvalues from the semantic conventions package: common.py\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.sdk.resources import Resource\n\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter from\n\nopentelemetry.semconv.resource import ResourceAttributes\n\nfrom local_machine_resource_detector import LocalMachineResourceDetector\n\ndef configure_tracer(name, version):\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter)\n\nlocal_resource = LocalMachineResourceDetector().detect()\n\nresource = local_resource.merge(\n\nResource.create(\n\n{\n\nResourceAttributes.SERVICE_NAME: name,\n\nResourceAttributes.SERVICE_VERSION: version,\n\n}",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": ")\n\n)\n\nprovider = TracerProvider(resource=resource)\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(name, version)\n\nThis code can now be used in both shopper.py and the new server code in grocery_store.py to\n\ninstantiate a tracer. The server code uses Flask (https://flask.palletsprojects.com/en/1.1.x/) to provide\n\nan API, and the initial code for the application will implement a single route handler. We won't dive\n\ntoo deeply into the nuts and bolts of how Flask works in this book. For the purpose of our\n\napplication, it's enough to know that the response handler can be configured with a path via the route\n\ndecorator and that the run method launches a web server. An additional decorator to create a span on\n\nthe handler sets the kind to SERVER, as it is the operation that is responding to the CLIENT span\n\ninstrumented previously. Note that in the code, there are also several attributes being set following\n\nthe semantic conventions; the Flask library conveniently makes most of the information available\n\nquite easily: grocery_store.py\n\nfrom flask import Flask, request\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.semconv.trace import HttpFlavorValues, SpanAttributes\n\nfrom opentelemetry.trace import SpanKind\n\nfrom common import configure_tracer\n\ntracer = configure_tracer(\"0.1.2\", \"grocery-store\")\n\napp = Flask(__name__)\n\n@app.route(\"/\")\n\n@tracer.start_as_current_span(\"welcome\", kind=SpanKind.SERVER)\n\ndef welcome():\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_FLAVOR: request.environ.get(\"SERVER_PROTOCOL\"),\n\nSpanAttributes.HTTP_METHOD: request.method,\n\nSpanAttributes.HTTP_USER_AGENT: str(request.user_agent),\n\nSpanAttributes.HTTP_HOST: request.host,\n\nSpanAttributes.HTTP_SCHEME: request.scheme,",
      "content_length": 1814,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "SpanAttributes.HTTP_TARGET: request.path,\n\nSpanAttributes.HTTP_CLIENT_IP: request.remote_addr,\n\n}\n\n)\n\nreturn \"Welcome to the grocery store!\"\n\nif __name__ == \"__main__\":\n\napp.run()\n\nOf course, to see the traces we must first run the application; to get the server running, use the\n\nfollowing command: python grocery_store.py\n\nIf another application is already running on the default port that Flask uses, 5000, you may encounter\n\nthe Address already in use error. Ensure only one instance of the server is running at any given\n\ntime.\n\nIMPORTANT NOTE\n\nIt's possible to run the server with debug mode enabled to have it automatically updated every time the code changes.\n\nThis is convenient when doing rapid development but should never be left enabled outside of development. Debug mode\n\nalso causes problems with auto-instrumentation, as we discussed in Chapter 3, Auto-Instrumentation. Enabling debug\n\nmode is accomplished by calling the run method as follows: run(debug=True).\n\nIn any future examples, the server will always need to be run before the shopper; otherwise, the client\n\napplication will throw HTTP connection exceptions. I find it particularly helpful to use a terminal\n\nthat supports split screens to have both the client and the server running side by side. Let's run both\n\napplications now and inspect the output data emitted. The server operation named / will be identified\n\nas a SERVER span: grocery_store.py output\n\n{\n\n\"name\": \"/\",\n\n\"context\": {\n\n\"trace_id\": \"0xe7f562a98f81a36ba81aaf1e239dd718\",\n\n\"span_id\": \"0x51daed87f12f5bc0\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.SERVER\", \"parent_id\": null,\n\n}\n\nOn the client side, the operation named web request will be identified as a CLIENT span: shopper.py\n\noutput\n\n{",
      "content_length": 1737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "\"name\": \"web request\",\n\n\"context\": {\n\n\"trace_id\": \"0xc2747c6a8c7f7e12618bf69d7d71a1c8\",\n\n\"span_id\": \"0x88b7afb56d248244\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.CLIENT\", \"parent_id\": \"0xe756587bc381338c\",\n\n}\n\nThis new data is starting to help define the ties between different services and describe the\n\nrelationships between the components of the system, which is great. By exploring the tracing data\n\nalone, we can start getting a clearer idea of the role that each application plays. Oddly enough\n\nthough, the data we're currently generating doesn't appear to be fully connected yet. The trace_id\n\nidentifier between the client and the server doesn't match, and moreover, the SERVER span doesn't\n\ncontain parent_id; it seems we forgot about propagation!\n\nPropagating context\n\nGetting the information from one service to another across the network boundary requires some\n\nadditional work, namely, propagating the context. Without this context propagation, each service will\n\ngenerate a new trace independently, which means that the backend will not be able to tie the services\n\ntogether at analysis time. As shown in Figure 4.5, a trace without propagation between services is\n\nmissing the link between services, which means the traces will be more difficult to correlate:",
      "content_length": 1279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Figure 4.5 – Traces with and without propagation\n\nSpecifically, the data needed to propagate the context between services is span_context. This\n\nincludes four key pieces of information:\n\nspan_id: The identifier of the current span\n\ntrace_id: The identifier of the current trace\n\ntrace_flags: The additional configuration flags available to control tracing levels and sampling, as per the W3C Trace\n\nContext specification (https://www.w3.org/TR/trace-context/#trace-flags)\n\ntrace_state: A set of vendor-specific identification data as per the W3C Trace Context specification\n\n(https://www.w3.org/TR/trace-context/#tracestate-header)\n\nThe span_context information is used anytime a new span is started. trace_id is set as the current\n\nnew span's trace ID, and span_id will be used as the new span's parent ID. When a new span is\n\nstarted in a different service, if the context isn't propagated correctly, the new span has no information\n\nfrom which to pull the data it needs. Context must be serialized and injected across boundaries into a\n\ncarrier for propagation to occur. On the receiving end, the context must be extracted from the carrier\n\nand deserialized. The carrier medium used to transport the context, in the case of our application, is\n\nHTTP headers. OpenTelemetry's Propagators API provides the methods we'll use in the next\n\nexample. On the client side, we'll call the inject method to set span_context in a dictionary that will\n\nbe passed into the HTTP request as headers: shopper.py\n\nfrom opentelemetry.propagate import inject\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nprint(\"visiting the grocery store\")\n\nwith tracer.start_as_current_span(\n\n\"web request\", kind=trace.SpanKind.CLIENT\n\n) as span:\n\nheaders = {}\n\ninject(headers)\n\nresp = requests.get(url, headers=headers)\n\nOn the server side, it is a little more complicated, as we need to ensure the context is extracted before\n\nthe decorator instantiates the span in the request handler. Conveniently, Flask has a mechanism\n\navailable via decorators to call methods before and after a request is handled. This allows us to\n\nextract the context from the request headers and attach to the context before the request handler is\n\ncalled. The call to attach will return a token that will be stored in the context of the request. Once the\n\nrequest has been processed, the call to detach restores the previous context: grocery_store.py",
      "content_length": 2408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "from opentelemetry import context\n\nfrom opentelemetry.propagate import extract\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest.environ[\"context_token\"] = token\n\n@app.teardown_request\n\ndef teardown_request_func(err):\n\ntoken = request.environ.get(\"context_token\", None)\n\nif token:\n\ncontext.detach(token)\n\nTesting the new code will show that the context is now propagated; remember to restart the server as\n\nwell as run the client. Take a look at the following output, paying special attention to trace_id and\n\nspan_id: shopper.py output\n\n{\n\n\"name\": \"web request\",\n\n\"context\": {\n\n\"trace_id\": \"0x1fe2dc4e2e750e4598463749300277ed\", \"span_id\": \"0x5771b0a074e00a5b\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.CLIENT\",\n\n}\n\nIf everything went according to plan, the client and the server should be part of the same trace. The\n\noutput on the server side shows the span containing a parent_id field which matches the client's\n\nspan_id field. As well, note the trace_id field which matches on both sides of the request:\n\ngrocery_store.py output\n\n{\n\n\"name\": \"/\",\n\n\"context\": {\n\n\"trace_id\": \"0x1fe2dc4e2e750e4598463749300277ed\", \"span_id\": \"0x26f143d0f8a9c0bd\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.SERVER\",\n\n\"parent_id\": \"0x5771b0a074e00a5b\", }",
      "content_length": 1300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Now that the services are connected, let's explore propagation a bit further!\n\nAdditional propagator formats\n\nThe propagation we've used so far in the example is the W3C Trace Context propagation format.\n\nThe Trace Context format is fairly recent and is by no means the only propagation format out there.\n\nThere are additional propagators available to use with OpenTelemetry to support interoperability with\n\nother tracing standards. Additional propagators supported by OpenTelemetry and available at the time\n\nof writing include B3, Jaeger, and ot-trace. Currently supported propagators implement a\n\nTextMapPropagator interface with an inject method and an extract method. A propagator is\n\nconfigured globally using the set_global_textmap method. The following code shows an example of\n\nconfiguring a B3MultiFormat propagator for an application. This propagator can be found by\n\ninstalling the opentelemetry-propagator-b3 package: from opentelemetry.propagators.b3 import\n\nB3MultiFormat\n\nfrom opentelemetry.propagate import set_global_textmap\n\nset_global_textmap(B3MultiFormat())\n\nIMPORTANT NOTE\n\nTroubleshooting propagation issues can be difficult and time-consuming. Services can easily be misconfigured to\n\npropagate data using different formats and doing so will result in propagation not working at all.\n\nIf you decided to use the previous code in either the shopper or the grocery store applications but not\n\nboth, you may have noticed propagation breaking. It's not uncommon for applications in the wild to\n\nhave different propagation formats configured. Thankfully, it's possible to configure multiple\n\npropagators simultaneously in OpenTelemetry by using a composite propagator.\n\nComposite propagator\n\nA composite propagator allows users to configure multiple propagators from different cross-cutting\n\nconcerns. In its current implementation in many languages, the composite propagator can support\n\nmultiple propagators for the same signal. This functionality provides backward compatibility with\n\nolder systems while being future-proof. CompositePropagator has the same interface as any\n\npropagator but supports passing in a list of propagators at initialization. This list is then iterated\n\nthrough at injection and extraction time. This next example introduces one additional service, a\n\nlegacy inventory system that is configured to use B3 propagation. Figure 4.6 shows the flow of the\n\nrequest from the shopper, through the store, and to the inventory system that we will be adding in the\n\nnext example:",
      "content_length": 2518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Figure 4.6 – A request to the legacy inventory system\n\nSince the grocery store needs to propagate requests using both W3C Trace Context and B3, we'll\n\nneed to update the code to configure CompositePropagator to support this. The first thing to do before\n\ndiving into the code is to ensure that the B3 propagator package is installed: pip install\n\nopentelemetry-propagator-b3\n\nFor the sake of simplifying the server code, the following code shows a new method being added to\n\ncommon.py to set span attributes in a server handler. This new method,\n\nset_span_attributes_from_flask, can be used both in legacy_inventory.py (as we'll see shortly) and\n\nin grocery_store.py: common.py\n\nfrom flask import request\n\nfrom opentelemetry.semconv.trace import SpanAttributes\n\ndef set_span_attributes_from_flask():\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_FLAVOR: request.environ.get(\"SERVER_PROTOCOL\"),\n\nSpanAttributes.HTTP_METHOD: request.method,\n\nSpanAttributes.HTTP_USER_AGENT: str(request.user_agent),\n\nSpanAttributes.HTTP_HOST: request.host,\n\nSpanAttributes.HTTP_SCHEME: request.scheme,\n\nSpanAttributes.HTTP_TARGET: request.path,\n\nSpanAttributes.HTTP_CLIENT_IP: request.remote_addr,\n\n}\n\n)\n\nThe legacy_inventory.py service code is another Flask server application with a single handler that,\n\nfor now, returns a hardcoded list of items and quantities encoded using JSON. The following code is",
      "content_length": 1420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "very similar to the grocery store code. The configuration for both the Flask app and OpenTelemetry\n\nshould be familiar, the significant difference being how we configure OpenTelemetry in this new\n\nservice is the propagator, by calling set_global_textmap. It's also important to remember to set the\n\nport number to a different value than the default Flask port by passing an argument to app.run;\n\notherwise, we will run into a socket error when trying to run both grocery_store.py and\n\nlegacy_inventory.py: legacy_inventory.py\n\nfrom flask import Flask, jsonify, request\n\nfrom opentelemetry import context\n\nfrom opentelemetry.propagate import extract, set_global_textmap\n\nfrom opentelemetry.propagators.b3 import B3MultiFormat\n\nfrom opentelemetry.trace import SpanKind\n\nfrom common import configure_tracer, set_span_attributes_from_flask\n\ntracer = configure_tracer(\"legacy-inventory\", \"0.9.1\") app = Flask(__name__)\n\nset_global_textmap(B3MultiFormat())\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest.environ[\"context_token\"] = token\n\n@app.teardown_request\n\ndef teardown_request_func(err):\n\ntoken = request.environ.get(\"context_token\", None)\n\nif token:\n\ncontext.detach(token)\n\n@app.route(\"/inventory\")\n\n@tracer.start_as_current_span(\"/inventory\", kind=SpanKind.SERVER) def inventory():\n\nset_span_attributes_from_flask()\n\nproducts = [\n\n{\"name\": \"oranges\", \"quantity\": \"10\"}, {\"name\": \"apples\", \"quantity\": \"20\"}, ]\n\nreturn jsonify(products)\n\nif __name__ == \"__main__\":\n\napp.run(debug=True, port=5001)\n\nIn the grocery store application, we will configure CompositePropagator to support both the W3C\n\nTrace Context and B3 formats. Add the following code to grocery_store.py: grocery_store.py\n\nfrom opentelemetry.propagate import extract, inject, set_global_textmap",
      "content_length": 1819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "from opentelemetry.propagators.b3 import B3MultiFormat\n\nfrom opentelemetry.propagators.composite import CompositePropagator\n\nfrom opentelemetry.trace.propagation import tracecontext\n\nset_global_textmap(CompositePropagator([tracecontext.TraceContextTextMapPropagator(),\n\nB3MultiFormat()])) Additionally, the following handler will be added to the store, which\n\nwill make a request to the legacy inventory service. The key thing to remember here is to\n\nensure the context is present in the headers by calling inject and that the headers are\n\npassed into the request: grocery_store.py\n\nimport requests\n\nfrom common import set_span_attributes_from_flask\n\n...\n\n@app.route(\"/\")\n\n@tracer.start_as_current_span(\"welcome\", kind=SpanKind.SERVER)\n\ndef welcome():\n\nset_span_attributes_from_flask()\n\nreturn \"Welcome to the grocery store!\"\n\n@app.route(\"/products\")\n\n@tracer.start_as_current_span(\"/products\", kind=SpanKind.SERVER) def products():\n\nset_span_attributes_from_flask()\n\nwith tracer.start_as_current_span(\"inventory request\") as span: url =\n\n\"http://localhost:5001/inventory\"\n\nspan.set_attributes(\n\n{\n\nSpanAttributes.HTTP_METHOD: \"GET\",\n\nSpanAttributes.HTTP_FLAVOR: str(HttpFlavorValues.HTTP_1_1),\n\nSpanAttributes.HTTP_URL: url,\n\nSpanAttributes.NET_PEER_IP: \"127.0.0.1\",\n\n}\n\n)\n\nheaders = {}\n\ninject(headers)\n\nresp = requests.get(url, headers=headers)\n\nreturn resp.text\n\nThe last change we need before trying this out is an update to the shopper application's browse\n\nmethod to send a request to the new endpoint: shopper.py",
      "content_length": 1520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "def browse():\n\nprint(\"visiting the grocery store\")\n\nwith tracer.start_as_current_span(\n\n\"web request\", kind=trace.SpanKind.CLIENT\n\n) as span:\n\nurl = \"http://localhost:5000/products\"\n\nNow, we have a third application to launch; the following commands need to be run from separate\n\nterminal windows, and remember to ensure no other applications are running on ports 5000 and 5001\n\nto avoid socket errors: $ python ./legacy_inventory.py\n\n$ python ./grocery_store.py\n\n$ python ./shopper.py\n\nOnce the legacy inventory server is up and running, making a request from the shopper should yield\n\nsome exciting results. In the output, we'll be looking for trace_id to be consistent across all three\n\nservices, and, as in the previous example of propagation, parent_id of the server span should match\n\nspan_id of the corresponding client request span: shopper.py output\n\n\"name\": \"web request\",\n\n\"context\": {\n\n\"trace_id\": \"0xb2a655bfd008007711903d8a72130813\", \"span_id\": \"0x3c183afa2640a2bb\", },\n\nThe following output from the grocery store includes two spans. The span named /products\n\nrepresents the request received from the client, and if the context is successfully extracted, trace_id\n\nwill match the previous output. The second span is the request to the inventory service:\n\ngrocery_store.py output\n\n\"name\": \"/products\",\n\n\"context\": {\n\n\"trace_id\": \"0xb2a655bfd008007711903d8a72130813\", \"span_id\": \"0x77883e3459f83fb6\", },\n\n\"parent_id\": \"0x3c183afa2640a2bb\", ----\n\n\"name\": \"inventory request\",\n\n\"context\": {\n\n\"trace_id\": \"0xb2a655bfd008007711903d8a72130813\", \"span_id\": \"0x8137dbaaa3f40062\", },\n\n\"parent_id\": \"0x77883e3459f83fb6\", The last output is from the inventory service. Remember\n\nthat this service is using a different propagator format. If the propagation was\n\nconfigured correctly, trace_id should remain consistent with the other two services, and\n\nparent_id should reflect that the parent operation is the inventory request span:\n\nlegacy_inventory.py output\n\n\"name\": \"/inventory\",",
      "content_length": 1986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "\"context\": {\n\n\"trace_id\": \"0xb2a655bfd008007711903d8a72130813\", \"span_id\": \"0x3306b21b8000912b\", },\n\n\"parent_id\": \"0x8137dbaaa3f40062\", This was a lot of work, but once you get propagation\n\nconfigured and working across a system, it's rare that you'll need to go back and make\n\nchanges to it. It's a set-it-and-forget-it type of operation. If you happen to be working\n\nwith a brand-new code base, choose a single propagation format and stick to it; it will\n\nsave you a lot of headaches. We've now grasped one of the most important concepts in\n\ndistributed tracing, the propagation of span context across systems. Let's take a look at\n\nwhere else propagation can help us.\n\nIMPORTANT NOTE\n\nA possible alternative when working with large code bases and multiple propagator formats is to always configure all\n\navailable propagation formats. This may seem like overkill, but sometimes, it makes sense to prioritize interoperability over\n\nsaving a few bytes.\n\nRecording events, exceptions, and status\n\nQuickly identifying when an issue arises is a key aspect of distributed tracing. As demonstrated in\n\nFigure 4.7 with the Jaeger interface, in many backends, traces that contain errors are highlighted to\n\nmake them easy to find for users of data:\n\nFigure 4.7 – The trace view in Jaeger\n\nIn the following sections, we will explore the facilities that OpenTelemetry provides to capture\n\nevents, record exceptions, and set the status of a span.\n\nEvents",
      "content_length": 1444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "In addition to attributes, an event provides the facility to record data about a span that occurs at a\n\nspecific time. Events are similar to logs in OpenTracing in that they contain a timestamp and can\n\ncontain a list of attributes or key/value pairs. An event is added via an add_event method on the span,\n\nwhich accepts a name argument and, optionally, a timestamp and a list of attributes, as shown in the\n\nfollowing code: shopper.py\n\nspan.add_event(\"about to send a request\")\n\nresp = requests.get(url, headers=headers)\n\nspan.add_event(\"request sent\", attributes={\"url\": url}, timestamp=0)\n\nAs you'll see in the following output, the list of events is kept in the order in which they are added;\n\nthey are not ordered by the timestamps they are recorded with: shopper.py output\n\n\"events\": [\n\n{\n\n\"name\": \"about to send a request\",\n\n\"timestamp\": \"2021-07-12T06:38:49.793903Z\",\n\n\"attributes\": {}\n\n},\n\n{\n\n\"name\": \"request sent\",\n\n\"timestamp\": \"1970-01-01T00:00:00.000000Z\",\n\n\"attributes\": {\n\n\"url\": \"http://localhost:5000/products\"\n\n}\n\n}\n\n],\n\nEvents differ from attributes in that they have a time dimension to them, which can be helpful to\n\nbetter understand the sequence of things inside a span. There are also events that have a special\n\nmeaning, as we'll see with exceptions.\n\nExceptions\n\nIn OpenTelemetry, the concepts of exceptions and the status of a span are intentionally kept separate.\n\nA span may contain many exceptions, but these exceptions don't necessarily mean that the status of\n\nthe span should be set as an error. For example, a user may want to record exceptions when a request\n\nis made to a specific service, but there may be retry logic that will cause the operation to eventually",
      "content_length": 1700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "succeed anyway. Recording those exceptions may be useful to identify areas of the code that can be\n\nimproved. The initial definition of an exception in the OpenTelemetry specification is that an\n\nexception is as follows:\n\nRecorded as an event\n\nThe specific name exception\n\nContains the minimum of either exception.type or an exception.message attribute\n\nThe following code records an exception if a request to the grocery store fails by creating one such\n\nevent. Let's add a try/except block in the browse method to capture the exception and change url to\n\nmake the request intentionally fail: shopper.py\n\ntry:\n\nurl = \"invalid_url\"\n\nresp = requests.get(url, headers=headers)\n\nspan.add_event(\n\n\"request sent\",\n\nattributes={\"url\": url},\n\ntimestamp=0,\n\n)\n\nspan.set_attribute(\n\nSpanAttributes.HTTP_STATUS_CODE,\n\nresp.status_code\n\n)\n\nexcept Exception as err:\n\nattributes = {\n\nSpanAttributes.EXCEPTION_MESSAGE: str(err),\n\n}\n\nspan.add_event(\"exception\", attributes=attributes)\n\nRunning the code will produce an exception that will be caught. This exception will then be recoded\n\nas an event and added to the tracing data emitted at the console: shopper.py output\n\n\"events\": [\n\n{\n\n\"name\": \"exception\",\n\n\"timestamp\": \"2021-07-10T04:13:05.287376Z\",\n\n\"attributes\": {",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "\"exception.message\": \"Invalid URL 'invalid_url': No schema supplied. Perhaps you meant\n\nhttp://invalid_url?\"\n\n}\n\n}\n\n]\n\nAlthough this provides us with more information, it's not practical to have to write so many lines of\n\ncode every time we want to record an exception. Thankfully, the OpenTelemetry specification has\n\ndefined a span method in the API to address this. The following code replaces the code in the except\n\nblock of the previous example to use the record_exception method on the span, instead of manually\n\ncreating an event. Semantically, these are equivalent, but the method is much more convenient. The\n\nmethod accepts an exception as its first argument and supports optional parameters to pass in\n\nadditional event attributes, as well as a timestamp: shopper.py\n\ntry:\n\nurl = \"invalid_url\"\n\nresp = requests.get(url, headers=headers)\n\n...\n\nexcept Exception as err:\n\nspan.record_exception(err)\n\nNext time the code is run, the exception event is automatically generated for us. Taking a closer look\n\nat the output, it's even more useful than before, as we now see the following:\n\nThe message populated as we did before\n\nThe exception type\n\nA stack trace capturing exactly where in the code the exception was raised\n\nThis allows us to immediately find the problematic code and resolve the issue:\n\nshopper.py output\n\n\"events\": [\n\n{\n\n\"name\": \"exception\",\n\n\"timestamp\": \"2021-07-10T04:17:07.328665Z\",\n\n\"attributes\": {\n\n\"exception.type\": \"MissingSchema\",\n\n\"exception.message\": \"Invalid URL 'invalid_url': No schema supplied. Perhaps you meant\n\nhttp://invalid_url?\", \"exception.stacktrace\": \"Traceback (most recent call last):\\n File\n\n\\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-",
      "content_length": 1706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "packages/opentelemetry/trace/__init__.py\\\", line 522, in use_span\\n yield span\\n File\n\n\\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/opentelemetry/sdk/trace/__init__.py\\\", line 879, in start_as_current_span\\n yield\n\nspan_context\\n File \\\"/Users/alex/dev/cloud-native-observability/chapter4/./shopper.py\\\",\n\nline 110, in browse\\n resp = requests.get(\\\"invalid_url\\\", headers=headers)\\n File\n\n\\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/api.py\\\", line 76, in get\\n return request('get', url, params=params,\n\n**kwargs)\\n File \\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/api.py\\\", line 61, in request\\n return session.request(method=method,\n\nurl=url, **kwargs)\\n File \\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/sessions.py\\\", line 528, in request\\n prep = self.prepare_request(req)\\n\n\nFile \\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/sessions.py\\\", line 456, in prepare_request\\n p.prepare(\\n File\n\n\\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/models.py\\\", line 316, in prepare\\n self.prepare_url(url, params)\\n File\n\n\\\"/Users/alex/dev/cloud_native_observability/lib/python3.8/site-\n\npackages/requests/models.py\\\", line 390, in prepare_url\\n raise\n\nMissingSchema(error)\\nrequests.exceptions.MissingSchema: Invalid URL 'invalid_url': No\n\nschema supplied. Perhaps you meant http://invalid_url?\\n\", \"exception.escaped\": \"False\"\n\n}\n\n}\n\n],\n\nThis type of detail about exceptions in a system is incredibly valuable when debugging, especially\n\nwhen the events may have occurred minutes, hours, or even days ago. It's worth noting that the\n\nformat of the stack trace is language-specific, as described in Figure 4.8 (https://github.com/open-\n\ntelemetry/opentelemetry-\n\nspecification/blob/main/specification/trace/semantic_conventions/exceptions.md#stacktrace-\n\nrepresentation):\n\nFigure 4.8 – The stack trace format per language",
      "content_length": 2043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Additionally, the Python SDK also automatically captures uncaught exceptions and adds an exception\n\nevent to the span that is active when the exception occurs. We can update the code we just wrote in\n\nthe previous example to remove the try/except block, leaving the invalid URL. The following code\n\nhas the same effect as calling record_exception directly: shopper.py\n\nresp = requests.get(\"invalid_url\", headers=headers)\n\nRecording exceptions in spans is valuable, but in the event that it is preferable not to do so, it's\n\npossible to set an optional flag when creating a span to disable the functionality. You can try it in the\n\nprevious example by setting the record_exception optional argument, as follows: shopper.py\n\nwith tracer.start_as_current_span(\n\n\"web request\", kind=trace.SpanKind.CLIENT, record_exception=False\n\n) as span:\n\nNow that we understand how exceptions are recorded, let's further investigate how or even if these\n\nexceptions connect to the status of a span.\n\nStatus\n\nAs mentioned previously in this chapter, the span status has significant benefits to users. Quickly\n\nbeing able to filter through traces based on the span status makes things much easier for operators.\n\nThe status is composed of a status code and, optionally, a description. There are currently three\n\nsupported span status codes:\n\nUNSET\n\nOK\n\nERROR\n\nThe default status code on any new span is UNSET. This default behavior ensures that when a span\n\nstatus code is set to OK, it has been done intentionally. An earlier version of the specification defaulted\n\na span status code to OK, which left room for misinterpretations – was the span really OK or did the\n\ncode return before an error status code was set? The decision to set the span status is really up to the\n\napplication developer or operators of the service. The interface to set a status on a span receives a\n\nStatus object, which is composed of StatusCode and a description string. This next example sets the\n\nspan status code to OK based on the response from the web request. Note that we're using a feature of\n\nthe Requests library's Response object to return True if the HTTP status code on the response is\n\nbetween 200 and 400: shopper.py\n\nfrom opentelemetry.trace import Status, StatusCode\n\ndef browse():",
      "content_length": 2259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "with tracer.start_as_current_span(\n\n\"web request\", kind=trace.SpanKind.CLIENT, record_exception=False ) as span:\n\nurl = \"http://localhost:5000/products\"\n\nresp = requests.get(url, headers=headers)\n\nif resp:\n\nspan.set_status(Status(StatusCode.OK))\n\nelse:\n\nspan.set_status(\n\nStatus(StatusCode.ERROR, \"status code: {}\".format(resp.status_code))\n\n)\n\nWith the code in place, test the application first with the http://localhost:5000/products URL to see\n\nthe following output when a valid URL is used: shopper.py output with valid URL\n\n\"status\": {\n\n\"status_code\": \"OK\"\n\n}\n\nNow, update the URL to an invalid endpoint such as http://localhost:5000/invalid to see the following\n\noutput when the response contains an error code: shopper.py output with invalid URL\n\n\"status\": {\n\n\"status_code\": \"ERROR\",\n\n\"description\": \"status code: 404\"\n\n}\n\nIMPORTANT NOTE\n\nThe description field will only be used if the status code is set to ERROR; it is ignored otherwise.\n\nAnother thing to note about status codes is that, as per semantic convention, instrumentation libraries\n\nshould not change the status code to OK unless they are providing a configuration option to do this.\n\nThis is to prevent having an instrumentation library unexpectedly change the outcome of the span.\n\nThey are, however, encouraged to set the status code to ERROR when errors defined in the semantic\n\nconvention for the type of instrumentation library are encountered.\n\nAs with recording exceptions, it's also possible to configure spans to automatically set the status\n\nwhen an exception occurs. This is accomplished via a set_status_on_exception argument, available\n\nwhen starting a span: shopper.py\n\nwith tracer.start_as_current_span(\n\n\"web request\",",
      "content_length": 1705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "kind=trace.SpanKind.CLIENT,\n\nset_status_on_exception=True,\n\n) as span:\n\nPlay around with the code and see what the status output is when using this setting. Although it may\n\nseem like a lot of work, handling errors and setting the status on spans meaningfully will make a\n\nworld of difference at analysis time. Not only that, but having to work through the different scenarios\n\nin the code at instrumentation time is a forcing function to really ensure a solid understanding of what\n\nthe code is expected to do. And when things go wrong, as they will, having this data will make a\n\nworld of difference.\n\nSummary\n\nAnd just like that, you've explored many important concepts of the tracing signal in OpenTelemetry!\n\nThere was quite a bit to grasp in this chapter, but hopefully, the concepts we've been exploring so far\n\nare starting to make more sense now that there's some code behind them. With this knowledge, you\n\nnow know how to configure different components of the OpenTelemetry tracing pipeline to obtain a\n\ntracer and export data to the console. You also have the ability to start spans in various ways,\n\ndepending on your application's needs. We then spent some time improving the data emitted by\n\nenriching it using attributes, resources, and resource detectors. Last but not least, we took a look at\n\nthe important topic of events, status, and exceptions to capture some important information about\n\nerrors when they happen in code.\n\nOur understanding of the Context API will allow us to share information across our application, and\n\nknowing how to use the Propagation API will allow us to ensure that information is shared across\n\napplication boundaries.\n\nAlthough you probably have many more questions, you now know enough to look through some\n\nexisting applications or plan ahead for instrumenting new applications through distributed tracing. As\n\nsome of the components we've explored in this chapter are similar across signals, many of the\n\nconcepts that may not quite make sense yet will become clearer as we take a look at the next chapter,\n\nwhich looks at metrics. Let's go measure some things!",
      "content_length": 2114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Chapter 5: Metrics – Recording Measurements\n\nTracing code execution throughout a system is one way to capture information about what is\n\nhappening in an application, but what if we're looking to measure something that would be better\n\nserved by a more lightweight option than a trace? Now that we've learned how to generate distributed\n\ntraces using OpenTelemetry, it's time to look at the next signal: metrics. As we did in Chapter 4,\n\nDistributed Tracing – Tracing Code Execution, we will first look at configuring the OpenTelemetry\n\npipeline to produce metrics. Then, we'll continue to improve the telemetry emitted by the grocery\n\nstore application by using the instruments OpenTelemetry puts at our disposal. In this chapter, we\n\nwill do the following:\n\nConfigure OpenTelemetry to collect, aggregate, and export metrics to the terminal.\n\nGenerate metrics using the different instruments available.\n\nUse metrics to gain a better understanding of the grocery store application.\n\nAugmenting the grocery store application will allow us to put the different instruments into practice\n\nto grasp better how each instrument can be used to record measurements. As we explore other metrics\n\nthat are useful to produce for cloud-native applications, we will seek to understand some of the\n\nquestions we may answer using each instrument.\n\nTechnical requirements\n\nAs with the examples in the previous chapter, the code is written using Python 3.8, but\n\nOpenTelemetry Python supports Python 3.6+ at the time of writing. Ensure you have a compatible\n\nversion installed on your system following the instructions at\n\nhttps://docs.python.org/3/using/index.html. To verify that a compatible version is installed on your\n\nsystem, run the following commands: $ python --version\n\n$ python3 --version\n\nOn many systems, both python and python3 point to the same installation, but this is not always the\n\ncase, so it's good to be aware of this if one points to an unsupported version. In all examples, running\n\napplications in Python will call the python command, but they can also be run via the python3\n\ncommand, depending on your system.\n\nThe first few examples in this chapter will show a standalone example exploring how to configure\n\nOpenTelemetry to produce metrics. The code will require the OpenTelemetry API and SDK\n\npackages, which we'll install via the following pip command: $ pip install opentelemetry-api==1.10.0\n\n\\",
      "content_length": 2410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "opentelemetry-sdk==1.10.0 \\\n\nopentelemetry-propagator-b3==1.10.0\n\nAdditionally, we will use the Prometheus exporter to demonstrate a pull-based exporter to emit\n\nmetrics. This exporter can be installed via pip as well: $ pip install opentelemetry-exporter-\n\nprometheus==0.29b0\n\nFor the later examples involving the grocery store application, you can download the sample from\n\nChapter 4, Distributed Tracing – Tracing Code Execution, and add the code along with the examples.\n\nThe following git command will clone the companion repository: $ git clone\n\nhttps://github.com/PacktPublishing/Cloud-Native-Observability\n\nThe chapter04 directory in the repository contains the code for the grocery store. The complete\n\nexample, including all the code in the examples from this chapter, is available in the chapter05\n\ndirectory. I recommend adding the code following the examples and using the complete example\n\ncode as a reference if you get into trouble. Also, if you haven't read Chapter 4, Distributed Tracing –\n\nTracing Code Execution, it may be helpful to skim through the details of how the grocery store\n\napplication is built in that chapter to get your bearings.\n\nThe grocery store depends on the Requests library (https://docs.python-requests.org/) to make web\n\nrequests at various points and the Flask library (https://flask.palletsprojects.com) to provide a\n\nlightweight web server for some of the services. Both libraries can be installed via the following pip\n\ncommand: $ pip install flask requests\n\nAdditionally, the chapter will utilize a third-party open source tool (https://github.com/rakyll/hey) to\n\ngenerate some load on the web application. The tool can be downloaded from the repository. The\n\nfollowing commands download the macOS binary and rename it to hey using curl with the -o flag,\n\nthen ensure the binary is executable using chmod: $ curl -o hey https://hey-release.s3.us-east-\n\n2.amazonaws.com/hey_darwin_amd64\n\n$ chmod +x ./hey\n\nIf you have a different load generation tool you're familiar with, and there are many, feel free to use\n\nthat instead if you prefer. This should be everything we need to start; let's start measuring!\n\nConfiguring the metrics pipeline\n\nThe metrics signal was designed to be conceptually similar to the tracing signal. The metrics pipeline\n\nconsists of the following:\n\nA MeterProvider to determine how metrics should be generated and provide access to a meter.\n\nThe meter is used to create instruments, which are used to record measurements.",
      "content_length": 2492,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Views allow the application developer to filter and process metrics produced by the software development kit (SDK).\n\nA MetricReader, which collects metrics being recorded.\n\nThe MetricExporter provides a mechanism to translate metrics into an output format for various protocols.\n\nThere are quite a few components, and a picture always helps me grasp concepts more quickly. The\n\nfollowing figure shows us the different elements in the pipeline:\n\nFigure 5.1 – Metrics pipeline\n\nMeterProvider can be associated with a resource to identify the source of metrics produced. We'll see\n\nshortly how we can reuse the LocalMachineResourceDetector we created in Chapter 4, Distributed\n\nTracing – Tracing Code Execution, with metrics. For now, the first example instantiates\n\nMeterProvider with an empty resource. The code then calls the set_meter_provider global method to\n\nset the MeterProvider for the entire application.\n\nAdd the following code to a new file named metrics.py. Later in the chapter, we will refactor the\n\ncode to add a MeterProvider to the grocery store, but to get started, the simpler, the better.\n\nmetrics.py\n\nfrom opentelemetry._metrics import set_meter_provider\n\nfrom opentelemetry.sdk._metrics import MeterProvider\n\nfrom opentelemetry.sdk.resources import Resource\n\ndef configure_meter_provider():\n\nprovider = MeterProvider(resource=Resource.create())\n\nset_meter_provider(provider)",
      "content_length": 1395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "if __name__ == \"__main__\":\n\nconfigure_meter_provider()\n\nRun the code with the following command to ensure it runs without any errors:\n\npython ./metrics.py\n\nNo errors and no output? Well done, you're on the right track!\n\nIMPORTANT NOTE\n\nThe previous code shows that the metric modules are located at _metrics. This will change to metrics once the\n\npackages have been marked stable. Depending on when you're reading this, it may have already happened.\n\nNext, we'll need to configure an exporter to tell our application what to do with metrics once they're\n\ngenerated. The OpenTelemetry SDK contains ConsoleMetricExporter that emits metrics to the\n\nconsole, useful when getting started and debugging. PeriodicExportingMetricReader can be\n\nconfigured to periodically export metrics. The following code configures both components and adds\n\nthe reader to the MeterProvider. The code sets the export interval to 5000 milliseconds, or 5 seconds,\n\noverriding the default of 60 seconds: metrics.py\n\nfrom opentelemetry._metrics import set_meter_provider\n\nfrom opentelemetry.sdk._metrics import MeterProvider\n\nfrom opentelemetry.sdk.resources import Resource\n\nfrom opentelemetry.sdk._metrics.export import (\n\nConsoleMetricExporter,\n\nPeriodicExportingMetricReader,\n\n)\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nprovider = MeterProvider(metric_readers=[reader], resource=Resource.create())\n\nset_meter_provider(provider)\n\nif __name__ == \"__main__\":\n\nconfigure_meter_provider()\n\nRun the code once more. The expectation is that the output from running the code will still not show\n\nanything. The only reason to run the code is to ensure our dependencies are fulfilled, and there are no\n\ntypos.\n\nIMPORTANT NOTE",
      "content_length": 1798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Like TracerProvider, MeterProvider uses a default no-op implementation in the API. This allows developers to\n\ninstrument code without worrying about the details of how metrics will be generated. It does mean that unless we\n\nremember to set the global MeterProvider to use MeterProvider from the SDK package, any calls made to the API\n\nto generate metrics will result in no metrics being generated. This is one of the most common gotchas for folks working\n\nwith OpenTelemetry.\n\nWe're almost ready to start producing metrics with an exporter, a metric reader, and a MeterProvider\n\nconfigured. The next step is getting a meter.\n\nObtaining a meter\n\nWith MeterProvider globally configured, we can use a global method to obtain a meter. As mentioned\n\nearlier, the meter will be used to create instruments, which will be used throughout the application\n\ncode to record measurements. The meter receives the following arguments at creation time:\n\nThe name of the application or library generating metrics\n\nAn optional version identifies the version of the application or library producing the telemetry\n\nAn optional schema_url to describe the data generated\n\nIMPORTANT NOTE\n\nThe schema URL was introduced in OpenTelemetry as part of the OpenTelemetry Enhancement Proposal 152\n\n(https://github.com/open-telemetry/oteps/blob/main/text/0152-telemetry-schemas.md). The goal of schemas is to\n\nprovide OpenTelemetry instrumented applications a way to signal to external systems consuming the telemetry what\n\nthe semantic versioning of the data produced will look like. Schema URL parameters are optional but recommended\n\nfor all producers of telemetry: meters, tracers, and log emitters.\n\nThis information is used to identify the application or library producing the metrics. For example,\n\napplication A making a web request via the requests library may contain more than one meter:\n\nThe first meter is created by application A with a name identifying it with the version number matching the application.\n\nA second meter is created by the requests instrumentation library with the name opentelemetry-instrumentation-\n\nrequests and the instrumentation library version.\n\nThe urllib instrumentation library creates the third meter with the name opentelemetry-instrumentation-urllib,\n\na library utilized by the requests library.\n\nHaving a name and a version identifier is critical in differentiating the source of the metrics. As we'll\n\nsee later in the chapter, when we look at the Views section, this identifying information can also be\n\nused to filter out the telemetry we're not interested in. The following code uses the\n\nget_meter_provider global API method to access the global MeterProvider we configured earlier,\n\nand then calls get_meter with a name, version, and schema_url parameter: metrics.py\n\nfrom opentelemetry._metrics import get_meter_provider, set_meter_provider ...\n\nif __name__ == \"__main__\":",
      "content_length": 2894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "configure_meter_provider()\n\nmeter = get_meter_provider().get_meter(\n\nname=\"metric-example\",\n\nversion=\"0.1.2\",\n\nschema_url=\" https://opentelemetry.io/schemas/1.9.0\",\n\n)\n\nIn OpenTelemetry, instruments used to record measurements are associated with a single meter and\n\nmust have unique names within the context of that meter.\n\nPush-based and pull-based exporting\n\nOpenTelemetry supports two methods for exporting metrics data to external systems: push-based and\n\npull-based. A push-based exporter sends measurements from the application to a destination at a\n\nregular interval on a trigger. This trigger could be a maximum number of metrics to transfer or a\n\nschedule. The push-based method will be familiar to users of StatsD\n\n(https://github.com/statsd/statsd), where a network daemon opens a port and listens for metrics to be\n\nsent to it. Similarly, the ConsoleSpanExporter for the tracing signal in Chapter 4, Distributed Tracing\n\n– Tracing Code Execution, is a push-based exporter.\n\nOn the other hand, a pull-based exporter exposes an endpoint pulled from or scraped by an external\n\nsystem. Most commonly, a pull-based exporter exposes this information via a web endpoint or a local\n\nsocket; this is the method popularized by Prometheus (https://prometheus.io). The following diagram\n\nshows the data flow comparison between a push and a pull model:\n\nFigure 5.2 – Push versus pull-based reporting",
      "content_length": 1399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Notice the direction of the arrow showing the interaction between the exporter and an external\n\nsystem. When configuring a pull-based exporter, remember that system permissions may need to be\n\nconfigured to allow an application to open a new port for incoming requests. One such pull-based\n\nexporter defined in the OpenTelemetry specification is the Prometheus exporter.\n\nThe pipeline configuration for a pull exporter is slightly less complex. The metric reader interface can\n\nbe used as a single point to collect and expose metrics in the Prometheus format. The following code\n\nshows how to expose a Prometheus endpoint on port 8000 using the start_http_server method from\n\nthe Prometheus client library. It then configures PrometheusMetricReader with a prefix parameter to\n\nprovide a namespace for all metrics generated by our application. Finally, the code adds a call waiting\n\nfor input from the user before exiting; this gives us a chance to see the exposed metrics before the\n\napplication exits: from opentelemetry.exporter.prometheus import PrometheusMetricReader\n\nfrom prometheus_client import start_http_server\n\ndef configure_meter_provider():\n\nstart_http_server(port=8000, addr=\"localhost\")\n\nreader = PrometheusMetricReader(prefix=\"MetricExample\")\n\nprovider = MeterProvider(metric_readers=[reader], resource=Resource.create())\n\nset_meter_provider(provider)\n\nif __name__ == \"__main__\":\n\n...\n\ninput(\"Press any key to exit...\")\n\nIf you run the application now, you can use a browser to see the Prometheus formatted data available\n\nby visiting http://localhost:8000. Alternatively, you can use the curl command to see the output\n\ndata in the terminal as per the following example: $ curl http://localhost:8000\n\n# HELP python_gc_objects_collected_total Objects collected during gc\n\n# TYPE python_gc_objects_collected_total counter\n\npython_gc_objects_collected_total{generation=\"0\"} 1057.0\n\npython_gc_objects_collected_total{generation=\"1\"} 49.0\n\npython_gc_objects_collected_total{generation=\"2\"} 0.0\n\n# HELP python_gc_objects_uncollectable_total Uncollectable object found during GC\n\n# TYPE python_gc_objects_uncollectable_total counter\n\npython_gc_objects_uncollectable_total{generation=\"0\"} 0.0\n\npython_gc_objects_uncollectable_total{generation=\"1\"} 0.0\n\npython_gc_objects_uncollectable_total{generation=\"2\"} 0.0\n\n# HELP python_gc_collections_total Number of times this generation was collected",
      "content_length": 2401,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "# TYPE python_gc_collections_total counter\n\npython_gc_collections_total{generation=\"0\"} 55.0\n\npython_gc_collections_total{generation=\"1\"} 4.0\n\npython_gc_collections_total{generation=\"2\"} 0.0\n\n# HELP python_info Python platform information\n\n# TYPE python_info gauge\n\npython_info{implementation=\"CPython\",major=\"3\",minor=\"8\",patchlevel=\"0\",version=\"3.9.0\"}\n\n1.0\n\nThe Prometheus client library generates the previous data; note that there are no OpenTelemetry\n\nmetrics generated by our application, which makes sense since we haven't generated anything yet!\n\nWe'll get to that next. We'll see in Chapter 11, Diagnosing Problems, how to integrate OpenTelemetry\n\nwith a Prometheus backend. For the sake of simplicity, the remainder of the examples in this chapter\n\nwill be using the push-based ConsoleMetricExporter configured earlier. If you're more familiar with\n\nPrometheus, please use this configuration instead.\n\nChoosing the right OpenTelemetry instrument\n\nWe're now ready to generate metrics from our application. If you recall, in tracing, the tracer\n\nproduces spans, which are used to create distributed traces. By contrast, the meter does not generate\n\nmetrics; an instrument does. The meter's role is to produce instruments. OpenTelemetry offers many\n\ndifferent instruments to record measurements. The following figure shows a list of all the instruments\n\navailable:\n\nFigure 5.3 – OpenTelemetry instruments\n\nEach instrument has a specific purpose, and the correct instrument depends on the following:\n\nThe type of measurement being recorded\n\nWhether the measurement must be done synchronously",
      "content_length": 1598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "Whether the values being recorded are monotonic or not\n\nFor synchronous instruments, a method is called on the instrument when it is time for a\n\nmeasurement to be recorded. For asynchronous instruments, a callback method is configured at the\n\ninstrument's creation time.\n\nEach instrument has a name and kind property. Additionally, a unit and a description may be specified.\n\nCounter\n\nA counter is a commonly available instrument across metric ecosystems and implementations over\n\nthe years, although its definition across systems varies. In OpenTelemetry, a counter is an increasing\n\nmonotonic instrument, only supporting non-negative value increases. The following diagram shows a\n\nsample graph representing a monotonic counter:\n\nFigure 5.4 – Increasing monotonic counter graph\n\nA counter can be used to represent the following:\n\nNumber of requests received\n\nCount of orders processed\n\nCPU time utilization\n\nThe following code instantiates a counter to keep a tally of the number of items sold in the grocery\n\nstore. The code uses the add method to increment the counter and passes the locale of the customer as\n\nan attribute: metrics.py\n\nif __name__ == \"__main__\":\n\n...",
      "content_length": 1172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "counter = meter.create_counter(\n\n\"items_sold\",\n\nunit=\"items\",\n\ndescription=\"Total items sold\"\n\n)\n\ncounter.add(6, {\"locale\": \"fr-FR\", \"country\": \"CA\"})\n\ncounter.add(1, {\"locale\": \"es-ES\"})\n\nRunning the code outputs the counter with all its attributes:\n\noutput\n\n{\"attributes\": {\"locale\": \"fr-FR\", \"country\": \"CA\"}, \"description\": \"Total items sold\",\n\n\"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"items_sold\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\": 1646535699616146000,\n\n\"time_unix_nano\": 1646535699616215000, \"value\": 7, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": true}}\n\n{\"attributes\": {\"locale\": \"es-ES\"}, \"description\": \"Total items sold\",\n\n\"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"items_sold\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\": 1646535699616215001,\n\n\"time_unix_nano\": 1646535699616237000, \"value\": 0, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": true}}\n\nNote that the attributes themselves do not influence the value of the counter. They are only\n\naugmenting the telemetry with additional dimensions about the transaction. A monotonic instrument\n\nlike the counter cannot receive a negative value. The following code tries to add a negative value: if\n\n__name__ == \"__main__\":\n\n...\n\ncounter.add(6, {\"locale\": \"fr-FR\", \"country\": \"CA\"}) counter.add(-1, {\"unicorn\": 1})\n\nThis code results in the following warning, which provides the developer with a helpful hint:\n\noutput\n\nAdd amount must be non-negative on Counter items_sold.\n\nKnowing to use the right instrument can help avoid generating unexpected data. It's also good to\n\nconsider adding validation to the data being passed into instruments when unsure of the data source.",
      "content_length": 2207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "Asynchronous counter\n\nThe asynchronous counter can be used as a counter. Its only difference is that it is used\n\nasynchronously. Asynchronous counters can represent data that is only ever-increasing, and that may\n\nbe too costly to report synchronously or is more appropriate to record on set intervals. Some\n\nexamples of this would be reporting the following:\n\nCPU time utilized by a process\n\nTotal network bytes transferred\n\nThe following code shows us how to create an asynchronous counter using the\n\nasync_counter_callback callback method, which will be called every time\n\nPeriodExportingMetricReader executes. To ensure the instrument has a chance to record a few\n\nmeasurements, we've added sleep in the code as well to pause the code before exiting: metrics.py\n\nimport time\n\nfrom opentelemetry._metrics.measurement import Measurement\n\ndef async_counter_callback():\n\nyield Measurement(10)\n\nif __name__ == \"__main__\":\n\n...\n\n# async counter\n\nmeter.create_observable_counter(\n\nname=\"major_page_faults\",\n\ncallback=async_counter_callback,\n\ndescription=\"page faults requiring I/O\",\n\nunit=\"fault\",\n\n)\n\ntime.sleep(10)\n\nIf you haven't commented out the output from the instrument, you should see the output from both\n\ncounters now. The following output omits the previous example's output for brevity: output\n\n{\"attributes\": \"\", \"description\": \"page faults requiring I/O\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"major_page_faults\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language':\n\n'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0',\n\n'service.name': 'unknown_service'}, maxlen=None)\", \"unit\": \"fault\", \"point\":\n\n{\"start_time_unix_nano\": 1646538230507539000, \"time_unix_nano\": 1646538230507614000,\n\n\"value\": 10, \"aggregation_temporality\": 2, \"is_monotonic\": true}}",
      "content_length": 1882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "{\"attributes\": \"\", \"description\": \"page faults requiring I/O\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"major_page_faults\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language':\n\n'python', 'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0',\n\n'service.name': 'unknown_service'}, maxlen=None)\", \"unit\": \"fault\", \"point\":\n\n{\"start_time_unix_nano\": 1646538230507539000, \"time_unix_nano\": 1646538235507059000,\n\n\"value\": 20, \"aggregation_temporality\": 2, \"is_monotonic\": true}}\n\nThese counters are great for ever-increasing values, but measurements go up and down sometimes.\n\nLet's see what OpenTelemetry has in store for that.\n\nAn up/down counter\n\nThe following instrument is very similar to the counter. As you may have guessed from its name, the\n\ndifference between the counter and the up/down counter is that the latter can record values that go\n\nup and down; it is non-monotonic. The following diagram shows us what a graph representing a non-\n\nmonotonic counter may look like:\n\nFigure 5.5 – Non-monotonic counter graph\n\nCreating an UpDownCounter instrument is done via the create_up_down_counter method. Increment\n\nand decrement operations are done via the single add method with either positive or negative values:\n\nmetrics.py\n\nif __name__ == \"__main__\":\n\n...\n\ninventory_counter = meter.create_up_down_counter(",
      "content_length": 1417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "name=\"inventory\",\n\nunit=\"items\",\n\ndescription=\"Number of items in inventory\",\n\n)\n\ninventory_counter.add(20)\n\ninventory_counter.add(-5)\n\nThe previous example's output will be as follows:\n\noutput\n\n{\"attributes\": \"\", \"description\": \"Number of items in inventory\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"inventory\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\":\n\n1646538574503018000, \"time_unix_nano\": 1646538574503083000, \"value\": 15,\n\n\"aggregation_temporality\": 2, \"is_monotonic\": false}}\n\nNote the previous example only emits a single metric. This is expected as the two recordings were\n\naggregated into a single value for the period reported.\n\nAsynchronous up/down counter\n\nAs you may imagine, as the counter has an asynchronous counterpart, so does UpDownCounter. The\n\nasynchronous up/down counter allows us to increment or decrement a value on a set interval. As\n\nyou will see shortly, it is pretty similar in nature to the asynchronous gauge. The main difference\n\nbetween the two is that the asynchronous up/down counter should be used when the values being\n\nrecorded are additive in nature, meaning the measurements can be added across dimensions. Some\n\nexamples of metrics that could be recorded via this instrument are as follows:\n\nChanges in the number of customers in a store\n\nNet revenue for an organization across business units\n\nThe following creates an asynchronous up/down counter to keep track of the current number of\n\ncustomers in a store. Note that, unlike its synchronous counterpart, the value recorded in the\n\nasynchronous up/down counter is an absolute value, not a delta. As per the previous asynchronous\n\nexample, an async_updowncounter_callback callback method does the work of reporting the measure:\n\nmetrics.py\n\ndef async_updowncounter_callback():\n\nyield Measurement(20, {\"locale\": \"en-US\"})",
      "content_length": 2099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "yield Measurement(10, {\"locale\": \"fr-CA\"})\n\nif __name__ == \"__main__\":\n\n...\n\nupcounter_counter = meter.create_observable_up_down_counter(\n\nname=\"customer_in_store\",\n\ncallback=async_updowncounter_callback,\n\nunit=\"persons\",\n\ndescription=\"Keeps a count of customers in the store\"\n\n)\n\nThe output will start to look familiar based on the previous examples we've already run through:\n\noutput\n\n{\"attributes\": {\"locale\": \"en-US\"}, \"description\": \"Keeps a count of customers in the\n\nstore\", \"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"customer_in_store\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"persons\", \"point\": {\"start_time_unix_nano\": 1647735390164970000,\n\n\"time_unix_nano\": 1647735390164986000, \"value\": 20, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": false}}\n\n{\"attributes\": {\"locale\": \"fr-CA\"}, \"description\": \"Keeps a count of customers in the\n\nstore\", \"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"customer_in_store\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"persons\", \"point\": {\"start_time_unix_nano\": 1647735390164980000,\n\n\"time_unix_nano\": 1647735390165009000, \"value\": 10, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": false}}\n\nCounters and up/down counters are suitable for many data types, but not all. Let's see what other\n\ninstruments allow us to measure.\n\nHistogram\n\nA histogram instrument is useful when comparing the frequency distribution of values across large\n\ndata sets. Histograms use buckets to group the data they represent and effectively identify outliers or\n\nanomalies. Some examples of data representable by histograms are as follows:\n\nResponse times for requests to a service\n\nThe height of individuals",
      "content_length": 2108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Figure 5.6 shows a sample histogram chart representing the response time for requests. It looks like a\n\nbar chart, but it differs in that each bar represents a bucket containing a range for the values it\n\ncontains. The y axis represents the count of elements in each bucket:\n\nFigure 5.6 – Histogram graph\n\nTo capture information in a histogram, the buckets specified must be able to contain all the values it\n\nexpects to record. For example, take a histogram containing two buckets with explicit upper bounds\n\nof 0 ms and 10 ms. Any measurement greater than 10 ms bound would be excluded from the\n\nhistogram. Both Prometheus and OpenTelemetry address this by capturing any value beyond the\n\nmaximum upper boundary in an additional bucket. The histograms we'll explore in this chapter all\n\nuse explicit boundaries, but OpenTelemetry also provides experimental support for exponential\n\nhistograms (https://github.com/open-telemetry/opentelemetry-\n\nspecification/blob/main/specification/metrics/datamodel.md#exponentialhistogram).\n\nHistograms can be, and are often, used to calculate percentiles. The following code creates a\n\nhistogram via the create_histogram method. The method used to produce a metric with a histogram\n\nis named record: metrics.py\n\nif __name__ == \"__main__\":\n\n...\n\nhistogram = meter.create_histogram(\n\n\"response_times\",\n\nunit=\"ms\",\n\ndescription=\"Response times for all requests\",\n\n)",
      "content_length": 1400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "histogram.record(96)\n\nhistogram.record(9)\n\nIn this example, we record two measurements that fall into separate buckets. Notice how they appear\n\nin the output: output\n\n{\"attributes\": \"\", \"description\": \"Response times for all requests\",\n\n\"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"response_times\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\": 1646539219677439000,\n\n\"time_unix_nano\": 1646539219677522000, \"bucket_counts\": [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n\n\"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0, 500.0, 1000.0], \"sum\":\n\n105, \"aggregation_temporality\": 2}}\n\nAs with the counter and up/down counter, the histogram is synchronous.\n\nAsynchronous gauge\n\nThe last instrument defined by OpenTelemetry is the asynchronous gauge. This instrument can be\n\nused to record measurements that are non-additive in nature; in other words, which wouldn't make\n\nsense to sum together. An asynchronous gauge can represent the following:\n\nThe average memory consumption of a system\n\nThe temperature of a data center\n\nThe following code uses Python's built-in resource module to measure the maximum resident set size\n\n(https://en.wikipedia.org/wiki/Resident_set_size). This value is set in async_gauge_callback, which is\n\nused as the callback for the gauge we're creating: metrics.py\n\nimport resource\n\ndef async_gauge_callback():\n\nrss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n\nyield Measurement(rss, {})\n\nif __name__ == \"__main__\":\n\n...\n\nmeter.create_observable_gauge(\n\nname=\"maxrss\",\n\nunit=\"bytes\",\n\ncallback=async_gauge_callback,\n\ndescription=\"Max resident set size\",",
      "content_length": 1863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": ")\n\ntime.sleep(10)\n\nRunning the code will show us memory consumption information about our application using\n\nOpenTelemetry: output\n\n{\"attributes\": \"\", \"description\": \"Max resident set size\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"maxrss\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\", \"unit\": \"bytes\", \"point\": {\"time_unix_nano\":\n\n1646539432021601000, \"value\": 18341888}}\n\n{\"attributes\": \"\", \"description\": \"Max resident set size\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"maxrss\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\", \"unit\": \"bytes\", \"point\": {\"time_unix_nano\":\n\n1646539437018742000, \"value\": 19558400}}\n\nExcellent, we now know about the instruments and have started generating a steady metrics stream.\n\nThe last topic about instruments to be covered is duplicate instruments.\n\nDuplicate instruments\n\nDuplicate instrument registration conflicts arise if more than one instrument is created within a single\n\nmeter with the same name. This can potentially produce semantic errors in the data, as many\n\ntelemetry backends uniquely identify metrics via their names. Conflicting instruments may be\n\nintentional when two separate code paths need to report the same metric, or, when multiple\n\ndevelopers want to record different metrics but accidentally use the same name; naming things is\n\nhard. There are a few ways the OpenTelemetry SDK handles conflicting instruments:\n\nIf the conflicting instruments are identical, the values recorded by these instruments are aggregated. The data generated appears as\n\nthough a single instrument produced them.\n\nIf the instruments are not identical, but the conflict can be resolved via View configuration, the user will not be warned. As we'll\n\nsee next, views provide a mechanism to produce unique metric streams, differentiating the instruments.\n\nIf the instruments are not identical and their conflicts are not resolved via views, a warning is emitted, and their data is generated\n\nwithout modification.\n\nIndividual meters act as a namespace, meaning two meters can separately create identical instruments\n\nwithout any issues. Using a unique namespace for each meter ensures that application developers can\n\ncreate instruments that make sense for their applications without running the risk of interfering with\n\nother metrics generated by underlying libraries. This will also make searching for metrics easier once",
      "content_length": 2821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "exported outside the application. Let's see how we can shape the metrics stream to fit our needs with\n\nviews.\n\nCustomizing metric outputs with views\n\nSome applications may produce more metrics than an application developer is interested in. You may\n\nhave noticed this with the example code for instruments; as we added more examples, it became\n\ndifficult to find the metrics we were interested in. Recall the example mentioned earlier in this\n\nchapter: application A represents a client library making web requests that could produce metrics via\n\nthree different meters. If each of those meters keeps a request counter, duplicate data is highly likely\n\nto be generated. Duplicated data may not be a problem on a small scale, but when scaling services up\n\nto handling thousands and millions of requests, unnecessary metrics can become quite expensive.\n\nThankfully, views provide a way for users of OpenTelemetry to configure the SDK only to generate\n\nthe metrics they want. In addition to providing a mechanism to filter metrics, views can also\n\nconfigure aggregation or be used to add a new dimension to metrics.\n\nFiltering\n\nThe first aspect of interest is the ability to customize which metrics will be processed. To select\n\ninstruments, the following criteria can be applied to a view:\n\ninstrument_name: The name of the instrument\n\ninstrument_type: The type of the instrument\n\nmeter_name: The name of the meter\n\nmeter_version: The version of the meter\n\nmeter_schema: The schema URL of the meter\n\nThe SDK provides a default view as a catch-all for any instruments not matched by configured views.\n\nIMPORTANT NOTE\n\nThe code in this chapter uses version 1.10.0 which supports the parameter enable_default_view to modify to disable the\n\ndefault view. This has changed in version 1.11.0 with the following change: https://github.com/open-\n\ntelemetry/opentelemetry-python/pull/2547. If you are using a newer version, you will need to configure a wildcard view with\n\na DropAggregation, refer to the official documentation (https://opentelemetry-\n\npython.readthedocs.io/en/latest/sdk/metrics.html) for more information.\n\nThe following code selects the inventory instrument we created in an earlier example. Views are\n\nadded to the MeterProvider as an argument to the constructor.\n\nAnother argument is added disabling the default view:",
      "content_length": 2328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "metrics.py\n\nfrom opentelemetry.sdk._metrics.view import View\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nview = View(instrument_name=\"inventory\")\n\nprovider = MeterProvider(\n\nmetric_readers=[reader],\n\nresource=Resource.create(),\n\nviews=[view],\n\nenable_default_view=False,\n\n)\n\nThe resulting output shows a metric stream limited to a single instrument:\n\noutput\n\n{\"attributes\": {\"locale\": \"fr-FR\", \"country\": \"CA\"}, \"description\": \"total items sold\",\n\n\"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"sold\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\": 1647800250023129000,\n\n\"time_unix_nano\": 1647800250023292000, \"value\": 6, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": true}}\n\n{\"attributes\": {\"locale\": \"es-ES\"}, \"description\": \"total items sold\",\n\n\"instrumentation_info\": \"InstrumentationInfo(metric-example, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"sold\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\": 1647800250023138000,\n\n\"time_unix_nano\": 1647800250023312000, \"value\": 1, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": true}}\n\nThe views parameter accepts a list, making adding multiple views trivial. This provides a great deal\n\nof flexibility and control for users. An instrument must match all arguments passed into the View\n\nconstructor. Let's update the previous example and see what happens when we try to create a view by\n\nselecting an instrument of the Counter type with the name inventory: metrics.py\n\nfrom opentelemetry._metrics.instrument import Counter\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()",
      "content_length": 2134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "reader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nview = View(instrument_name=\"inventory\", instrument_type=Counter)\n\nprovider = MeterProvider(\n\nmetric_readers=[reader],\n\nresource=Resource.create(),\n\nviews=[view],\n\nenable_default_view=False,\n\n)\n\nAs you may already suspect, these criteria will not match any instruments, and no data will be\n\nproduced by running the code.\n\nIMPORTANT NOTE\n\nAll criteria specified when selecting instruments are optional. However, if no optional argument is specified, the code will\n\nraise an exception as per the OpenTelemetry specification.\n\nUsing views to filter instruments based on instrument or meter identification is a great way to reduce\n\nthe noise and cost of generating too many metrics.\n\nDimensions\n\nIn addition to selecting instruments, it's also possible to configure a view to only report specific\n\ndimensions. A dimension in this context is an attribute associated with the metric. For example, a\n\ncustomer counter may record information about customers as per Figure 5.7. Each attribute\n\nassociated with the counter, such as the country the customer is visiting from or the locale their\n\nbrowser is set to, offers another dimension to the metric recorded during their visit:\n\nFigure 5.7 – Additional dimensions for a counter\n\nDimensions can be used to aggregate data in meaningful ways; continuing with the previous table,\n\nwe can obtain the following information:",
      "content_length": 1440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Three customers visited our store.\n\nTwo customers visited from Canada and one from France.\n\nTwo had browsers configured to French (fr-FR), and one to English (en-US).\n\nViews allow us to customize the output from our metrics stream. Using the attributes_keys\n\nargument, we specify the dimensions we want to see in a particular view. The following configures a\n\nview to match the Counter instruments and to discard any attributes other than locale: metrics.py\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nview = View(instrument_type=Counter, attribute_keys=[\"locale\"])\n\n...\n\nYou may remember that in the code we wrote earlier when configuring instruments, the items_sold\n\ncounter generated two metrics. The first contained country and locale attributes; the second\n\ncontained the locale attribute. The configuration in this view will produce a metric stream discarding\n\nall attributes not specified via attribute_keys: output\n\n{\"attributes\": {\"locale\": \"fr-FR\"}, \"description\": \"Total items sold\", ...\n\n{\"attributes\": {\"locale\": \"es-ES\"}, \"description\": \"Total items sold\", ...\n\nNote that when using attribute_keys, all metrics not containing the specified attributes will be\n\naggregated. This is because by removing the attributes, the view effectively transforms the metrics, as\n\nper the following table:\n\nFigure 5.8 – Effect of attribute keys on counter operations\n\nAn example of where this may be useful is separating requests containing errors from those that do\n\nnot, or grouping requests by status code.\n\nIn addition to customizing the metric stream attributes, views can also alter their name or description.\n\nThe following renames the metric generated and updates its description. Additionally, it removes all",
      "content_length": 1820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "attributes from the metric stream: metrics.py\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nview = View(\n\ninstrument_type=Counter,\n\nattribute_keys=[],\n\nname=\"sold\",\n\ndescription=\"total items sold\",\n\n)\n\n...\n\nThe output now shows us a single aggregated metric that is more meaningful to us:\n\noutput\n\n{\"attributes\": \"\", \"description\": \"total items sold\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"sold\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\", \"unit\": \"items\", \"point\": {\"start_time_unix_nano\":\n\n1646593079208078000, \"time_unix_nano\": 1646593079208238000, \"value\": 7,\n\n\"aggregation_temporality\": 2, \"is_monotonic\": true}}\n\nCustomizing views allow us to focus further on the output of the metrics generated. Let's see how we\n\ncan combine the metrics with aggregators.\n\nAggregation\n\nThe last configuration of views we will investigate is aggregation. The aggregation option gives the\n\nview the ability to change the default aggregation used by an instrument to one of the following\n\nmethods:\n\nSumAggregation: Add the instrument's measurements and set the current value as the sum. The monotonicity and temporality\n\nfor the sum are derived from the instrument.\n\nLastValueAggregation: Record the last measurement and its timestamp as the current value of this view.\n\nExplicitBucketHistogramAggregation: Use a histogram where the boundaries can be set via configuration. Additional\n\noptions for this aggregation are boundaries for the buckets of the histogram and record_min_max to record the minimum\n\nand maximum values.\n\nThe following table, Figure 5.9, shows us the default aggregation for each instrument:",
      "content_length": 1938,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Figure 5.9 – Default aggregation per instrument\n\nAggregating data in the SDK allows us to reduce the number of data points transmitted. However,\n\nthis means the data available at query time is less granular, limiting the user's ability to query it.\n\nKeeping this in mind, let's look at configuring the aggregation for one of our counter instruments to\n\nsee how this works. The following code updates the view configured earlier to use\n\nLastValueAggregation instead of the SumAggregation default: metrics.py\n\nfrom opentelemetry.sdk._metrics.aggregation import LastValueAggregation\n\ndef configure_meter_provider():\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nview = View(\n\ninstrument_type=Counter,\n\nattribute_keys=[],\n\nname=\"sold\",\n\ndescription=\"total items sold\",\n\naggregation=LastValueAggregation(),\n\n)\n\nYou'll notice in the output now that instead of reporting the sum of all measurements (7) for the\n\ncounter, only the last value (1) recorded is produced: output\n\n{\"attributes\": \"\", \"description\": \"total items sold\", \"instrumentation_info\":\n\n\"InstrumentationInfo(metric-example, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"sold\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\", \"unit\": \"items\", \"point\": {\"time_unix_nano\":\n\n1646594506458381000, \"value\": 1}}",
      "content_length": 1494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Although it's essential to have the ability to configure aggregation, the default aggregation may well\n\nserve your purpose most of the time.\n\nIMPORTANT NOTE\n\nAs mentioned earlier, sum aggregation derives the temporality of the sum reported from its instrument. This temporality\n\ncan be either cumulative or delta. This determines whether the reported metrics are to be interpreted as always starting at\n\nthe same time, therefore, reporting a cumulative metric, or if the metrics reported represent a moving start time, and the\n\nreported values contain the delta from the previous report. For more information about temporality, refer to the\n\nOpenTelemetry specification found at https://github.com/open-telemetry/opentelemetry-\n\nspecification/blob/main/specification/metrics/datamodel.md#temporality.\n\nThe grocery store\n\nIt's time to go back to the example application from Chapter 4, Distributed Tracing –Tracing Code\n\nExecution, to get some practical experience of all the knowledge we've gained so far. Let's start by\n\nadding a method to retrieve a meter that will resemble configure_tracer from the previous chapter.\n\nThis method will be named configure_meter and will contain the configuration code from an\n\nexample earlier in this chapter. One main difference is the addition of a resource that uses\n\nLocalMachineResourceDetector, as we already defined in this module. Add the following code to the\n\ncommon.py module: common.py\n\nfrom opentelemetry._metrics import get_meter_provider, set_meter_provider\n\nfrom opentelemetry.sdk._metrics import MeterProvider\n\nfrom opentelemetry.sdk._metrics.export import (\n\nConsoleMetricExporter,\n\nPeriodicExportingMetricReader,\n\n)\n\ndef configure_meter(name, version):\n\nexporter = ConsoleMetricExporter()\n\nreader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)\n\nlocal_resource = LocalMachineResourceDetector().detect()\n\nresource = local_resource.merge(\n\nResource.create(\n\n{\n\nResourceAttributes.SERVICE_NAME: name,\n\nResourceAttributes.SERVICE_VERSION: version,\n\n}\n\n)",
      "content_length": 2027,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": ")\n\nprovider = MeterProvider(metric_readers=[reader], resource=resource)\n\nset_meter_provider(provider)\n\nschema_url = \"https://opentelemetry.io/schemas/1.9.0\"\n\nreturn get_meter_provider().get_meter(\n\nname=name,\n\nversion=version,\n\nschema_url=schema_url,\n\n)\n\nNow, update shopper.py to call this method and set the return value to a global variable named meter\n\nthat we'll use throughout the application: shopper.py\n\nfrom common import configure_tracer, configure_meter\n\ntracer = configure_tracer(\"shopper\", \"0.1.2\")\n\nmeter = configure_meter(\"shopper\", \"0.1.2\")\n\nWe will be adding this line to grocery_store.py and legacy_inventory.py in the following examples,\n\nbut you may choose to do so now. Now, to start the applications and ensure the code works as it\n\nshould, launch the three applications in separate terminals using the following commands in the order\n\npresented: $ python legacy_inventory.py\n\n$ python grocery_store.py\n\n$ python shopper.py\n\nThe execution of shopper.py should return right away. If no errors were printed out because of\n\nrunning those commands, we're off to a good start and are getting closer to adding metrics to our\n\napplications!\n\nNumber of requests\n\nWhen considering what metrics are essential to get insights about an application, it can be\n\noverwhelming to think of all the things we could measure. A good place is to start is with the golden\n\nsignals as documented in the Google Site Reliability Engineering (SRE) book,\n\nhttps://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals.\n\nMeasuring the traffic to our application is an easy place to start by counting the number of requests it\n\nreceives. It can help answer questions such as the following:\n\nWhat is the traffic pattern for our application?\n\nIs the application capable of handling the traffic we expected?",
      "content_length": 1826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "How successful is the application?\n\nIn future chapters, we'll investigate how this metric can be used to determine if the application should\n\nbe scaled automatically. A metric such as the total number of requests a service can handle is likely a\n\nnumber that would be revealed during benchmarking.\n\nThe following code calls configure_meter and creates a counter via the create_counter method to\n\nkeep track of the incoming requests to the server application. The request_counter value is\n\nincremented before the request is processed: grocery_store.py\n\nfrom common import configure_meter, configure_tracer, set_span_attributes_from_flask\n\ntracer = configure_tracer(\"grocery-store\", \"0.1.2\")\n\nmeter = configure_meter(\"grocery-store\", \"0.1.2\")\n\nrequest_counter = meter.create_counter(\n\nname=\"requests\",\n\nunit=\"request\",\n\ndescription=\"Total number of requests\",\n\n)\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest_counter.add(1)\n\nrequest.environ[\"context_token\"] = token\n\nThe updated grocery store code should reload automatically, but restart the grocery store application\n\nif it does not. Once the updated code is running, make the following three requests to the store by\n\nusing curl: $ curl localhost:5000\n\n$ curl localhost:5000/products\n\n$ curl localhost:5000/none-existent-url\n\nThis should give us output similar to the abbreviated output. Pay attention to the increasing value\n\nfield, which increases by one with each visit: 127.0.0.1 - - [06/Mar/2022 11:44:41] \"GET /\n\nHTTP/1.1\" 200 -\n\n{\"attributes\": \"\", \"description\": \"Total number of requests\", ... \"point\":\n\n{\"start_time_unix_nano\": 1646595826470792000, \"time_unix_nano\": 1646595833190445000,\n\n\"value\": 1, \"aggregation_temporality\": 2, \"is_monotonic\": true}}\n\n127.0.0.1 - - [06/Mar/2022 11:44:46] \"GET /products HTTP/1.1\" 200 -\n\n{\"attributes\": \"\", \"description\": \"Total number of requests\", ... \"point\":\n\n{\"start_time_unix_nano\": 1646595826470792000, \"time_unix_nano\": 1646595883232762000,\n\n\"value\": 2, \"aggregation_temporality\": 2, \"is_monotonic\": true}}",
      "content_length": 2072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "127.0.0.1 - - [06/Mar/2022 11:44:47] \"GET /none-existent-url HTTP/1.1\" 404 -\n\n{\"attributes\": \"\", \"description\": \"Total number of requests\", ... \"point\":\n\n{\"start_time_unix_nano\": 1646595826470792000, \"time_unix_nano\": 1646595888236270000,\n\n\"value\": 3, \"aggregation_temporality\": 2, \"is_monotonic\": true}}\n\nIn addition to counting the total number of requests, it's helpful to have a way to track the different\n\nresponse codes. In the previous example, if you look at the output, you'll notice the last response's\n\nstatus code indicated a 404 error, which would be helpful to identify differently from other responses.\n\nKeeping a separate counter would allow us to calculate an error rate that could infer the service's\n\nhealth. Alternatively, using attributes can accomplish this, as well. The following moves the code to\n\nincrement the counter where the response status code is available. This code is then recorded as an\n\nattribute on the metric: grocery_store.py\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest.environ[\"context_token\"] = token\n\n@app.after_request\n\ndef after_request_func(response):\n\nrequest_counter.add(1, {\"code\": response.status_code})\n\nreturn response\n\nTo trigger the new code, use the following curl command:\n\n$ curl localhost:5000/none-existent-url\n\nThe result includes the status code attribute:\n\noutput\n\n{\"attributes\": {\"code\": 404}, \"description\": \"Total number of requests\",\n\n\"instrumentation_info\": \"InstrumentationInfo(grocery-store, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"requests\", \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'net.host.name': 'host',\n\n'net.host.ip': '127.0.0.1', 'service.name': 'grocery-store', 'service.version': '0.1.2'},\n\nmaxlen=None)\", \"unit\": \"request\", \"point\": {\"start_time_unix_nano\": 1646598200103414000,\n\n\"time_unix_nano\": 1646598203067451000, \"value\": 1, \"aggregation_temporality\": 2,\n\n\"is_monotonic\": true}}\n\nSend a few more requests through to obtain different status codes. You can start seeing how this\n\ninformation can calculate error rates. The name given to metrics is significant.\n\nIMPORTANT NOTE",
      "content_length": 2246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "It's not possible to generate telemetry where there is no instrumentation. However, it is possible to filter out undesired\n\ntelemetry using the configuration in the SDK and the OpenTelemetry collector. Remember this when instrumenting code.\n\nWe'll visit how the collector can filter telemetry in Chapter 8, OpenTelemetry Collector, and Chapter 9, Deploying the\n\nCollector.\n\nThe data has shown us how to use a counter to produce meaningful data enriched with attributes. The\n\nvalue of this data will become even more apparent once we look at analysis tools in Chapter 10,\n\nConfiguring Backends.\n\nRequest duration\n\nThe next metric to produce is request duration. The goal of understanding the request duration across\n\na system is to be able to answer questions such as the following:\n\nHow long did the request take?\n\nHow much time did each service add to the total duration of the request?\n\nWhat is the experience for users?\n\nRequest duration is an interesting metric to understand the health of a service and can often be the\n\nsymptom of an underlying issue. Collecting the duration is best done via a histogram, which can\n\nprovide us with the organization and visualization necessary to understand the distribution across\n\nmany requests. In the following example, we are interested in measuring the duration of operations\n\nwithin each service. We are also interested in capturing the duration of upstream requests and the\n\nnetwork latency costs across each service in our distributed application. Figure 5.10 shows how this\n\nwill be measured:\n\nFigure 5.10 – Measuring request duration\n\nWe can use the different measurements across the entire request to understand where time is spent.\n\nThis could help differentiate network issues from application issues. For example, if a request from\n\nshopper.py to grocery_store.py takes 100 ms, but the operation within grocery_store.py takes less\n\nthan 1 ms, we know that the additional 99 ms were spent outside the application code.",
      "content_length": 1972,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "IMPORTANT NOTE\n\nWhen a network is involved, unexpected latency can always exist. This common fallacy of cloud-native applications must\n\nbe accounted for when designing applications. Investment in network engineering and deploying applications within closer\n\nphysical proximity significantly reduces latency.\n\nIn the following example, the upstream_duration_histo histogram is configured to record the\n\nduration of requests from shopper.py to grocery_store.py. An additional histogram,\n\ntotal_duration_histo, is created to capture the duration of the entire operation within the shopper\n\napplication. The period is calculated using the time_ns method from the time library, which returns\n\nthe current time in nanoseconds, which we convert to milliseconds: shopper.py\n\nimport time\n\ntotal_duration_histo = meter.create_histogram(\n\nname=\"duration\",\n\ndescription=\"request duration\",\n\nunit=\"ms\",\n\n)\n\nupstream_duration_histo = meter.create_histogram(\n\nname=\"upstream_request_duration\",\n\ndescription=\"duration of upstream requests\",\n\nunit=\"ms\",\n\n)\n\ndef browse():\n\n...\n\nstart = time.time_ns()\n\nresp = requests.get(url, headers=headers)\n\nduration = (time.time_ns() - start)/1e6\n\nupstream_duration_histo.record(duration)\n\n...\n\ndef visit_store():\n\nstart = time.time_ns()\n\nbrowse()\n\nduration = (time.time_ns() - start)/1e6\n\ntotal_duration_histo.record(duration)\n\nThe next step is to configure a histogram in grocery_store.py to record upstream requests and\n\noperation durations. For brevity, I will omit the instantiation of the two histograms to the following",
      "content_length": 1547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "code, as the code is identical to the previous example. The following uses methods decorated with\n\nFlask's before_request and after_request to calculate the beginning and end of the entire operation.\n\nWe also need to calculate the upstream request that occurs in the products method: grocery_store.py\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest_counter.add(1, {})\n\nrequest.environ[\"context_token\"] = token\n\nrequest.environ[\"start_time\"] = time.time_ns()\n\n@app.after_request\n\ndef after_request_func(response):\n\nrequest_counter.add(1, {\"code\": response.status_code})\n\nduration = (time.time_ns() - request.environ[\"start_time\"]) / 1e6\n\ntotal_duration_histo.record(duration)\n\nreturn response\n\n@app.route(\"/products\")\n\n@tracer.start_as_current_span(\"/products\", kind=SpanKind.SERVER)\n\ndef products():\n\n...\n\ninject(headers)\n\nstart = time.time_ns()\n\nresp = requests.get(url, headers=headers)\n\nduration = (time.time_ns() - start) / 1e6\n\nupstream_duration_histo.record(duration)\n\nLastly, for this example, let's add duration calculation for legacy_inventory.py. The code will be\n\nmore straightforward since this service has no upstream requests yet, thus, we'll only need to define a\n\nsingle histogram: legacy_inventory.py\n\nfrom flask import request\n\nimport time\n\ntotal_duration_histo = meter.create_histogram(\n\nname=\"duration\",\n\ndescription=\"request duration\",\n\nunit=\"ms\",",
      "content_length": 1428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": ")\n\n@app.before_request\n\ndef before_request_func():\n\ntoken = context.attach(extract(request.headers))\n\nrequest.environ[\"start_time\"] = time.time_ns()\n\n@app.after_request\n\ndef after_request_func(response):\n\nduration = (time.time_ns() - request.environ[\"start_time\"]) / 1e6\n\ntotal_duration_histo.record(duration)\n\nreturn response\n\nNow that we have all these histograms in place, we can finally look at the duration of our requests.\n\nThe following output combines the output from all three applications to give us a complete picture of\n\nthe time spent across the system. Pay close attention to the sum value recorded for each histogram. As\n\nwe're only sending one request through, the sum equates the value for that single request: output\n\n{\"attributes\": \"\", \"description\": \"duration of upstream requests\", \"instrumentation_info\":\n\n\"InstrumentationInfo(shopper, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\", \"name\":\n\n\"upstream_request_duration\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\":\n\n1646626129420576000, \"time_unix_nano\": 1646626129420946000, \"bucket_counts\": [0, 0, 0, 0,\n\n0, 0, 0, 0, 0, 0, 1], \"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0,\n\n500.0, 1000.0], \"sum\": 18.981, \"aggregation_temporality\": 2}}\n\n{\"attributes\": \"\", \"description\": \"request duration\", \"instrumentation_info\":\n\n\"InstrumentationInfo(shopper, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\", \"name\":\n\n\"duration\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\": 1646626129420775000,\n\n\"time_unix_nano\": 1646626129420980000, \"bucket_counts\": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n\n\"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0, 500.0, 1000.0], \"sum\":\n\n19.354, \"aggregation_temporality\": 2}}\n\n{\"attributes\": \"\", \"description\": \"request duration\", \"instrumentation_info\":\n\n\"InstrumentationInfo(grocery-store, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"duration\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\": 1646626129419257000,\n\n\"time_unix_nano\": 1646626133006672000, \"bucket_counts\": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n\n\"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0, 500.0, 1000.0], \"sum\":\n\n10.852, \"aggregation_temporality\": 2}}\n\n{\"attributes\": \"\", \"description\": \"duration of upstream requests\", \"instrumentation_info\":\n\n\"InstrumentationInfo(grocery-store, 0.1.2, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"upstream_request_duration\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\":\n\n1646626129419136000, \"time_unix_nano\": 1646626135619575000, \"bucket_counts\": [0, 0, 0, 1,\n\n0, 0, 0, 0, 0, 0, 0], \"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0,\n\n500.0, 1000.0], \"sum\": 10.36, \"aggregation_temporality\": 2}}\n\n{\"attributes\": \"\", \"description\": \"request duration\", \"instrumentation_info\":\n\n\"InstrumentationInfo(legacy-inventory, 0.9.1, https://opentelemetry.io/schemas/1.9.0)\",",
      "content_length": 2865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "\"name\": \"duration\", \"unit\": \"ms\", \"point\": {\"start_time_unix_nano\": 1646626129417730000,\n\n\"time_unix_nano\": 1646626134436096000, \"bucket_counts\": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n\n\"explicit_bounds\": [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0, 500.0, 1000.0], \"sum\":\n\n0.494, \"aggregation_temporality\": 2}}\n\nThe difference in upstream_request_duration and duration sums for each application gives us the\n\nduration of the operation within each application. Looking closely at the data produced, we can see a\n\nsignificant portion of the request, 93% in this case, is spent communicating between applications.\n\nIf you're looking at this and wondering, Couldn't distributed tracing calculate the duration of the\n\nrequest and latency instead?, you're right. This type of information is also available via distributed\n\ntracing, so long as all the operations along the way are instrumented.\n\nConcurrent requests\n\nAnother critical metric is the concurrent number of requests an application is processing at any given\n\ntime. This helps answer the following:\n\nIs the application a bottleneck for a system?\n\nCan the application handle a surge in requests?\n\nNormally, this value is obtained by calculating a rate of the number of requests per second via the\n\ncounter added earlier. However, since we need practice with instruments and have yet to send our\n\ndata to a backend that allows for analysis, we'll record it manually.\n\nIt's possible to use several instruments to capture this. For the sake of this example, we will use an\n\nup/down counter, but we could have also used a gauge as well. We will increment the up/down\n\ncounter every time a new request begins and decrement it after each request: grocery_store.py\n\nconcurrent_counter = meter.create_up_down_counter(\n\nname=\"concurrent_requests\",\n\nunit=\"request\",\n\ndescription=\"Total number of concurrent requests\",\n\n)\n\n@app.before_request\n\ndef before_request_func():\n\n...\n\nconcurrent_counter.add(1)\n\n@app.after_request\n\ndef after_request_func(err):",
      "content_length": 1995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "...\n\nconcurrent_counter.add(-1)\n\nTo ensure we can see multiple users connected simultaneously, we will use a different tool than\n\nshopper.py, which we've used for this far. The hey load generation program allows us to generate\n\nhundreds of requests in parallel, enabling us to see the up/down counter in action. Run the program\n\nnow with the following command to generate 300 requests with a maximum concurrency of 10: $ hey\n\nn 3000 -c 10 http://localhost:5000/products\n\nThat command should have created enough parallel connections. Let's look at the metrics generated;\n\nwe should expect to see the recorded value going up as the number of concurrent requests increases,\n\nand then going back down: output\n\n{\"attributes\": \"\", \"description\": \"Total number of concurrent requests\",\n\n\"instrumentation_info\": \"InstrumentationInfo(grocery-store, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"concurrent_requests\", \"unit\":\n\n\"request\", \"point\": {\"start_time_unix_nano\": 1646627738799214000, \"time_unix_nano\":\n\n1646627769865503000, \"value\": 10, \"aggregation_temporality\": 2, \"is_monotonic\": false}}\n\n{\"attributes\": \"\", \"description\": \"Total number of concurrent requests\",\n\n\"instrumentation_info\": \"InstrumentationInfo(grocery-store, 0.1.2,\n\nhttps://opentelemetry.io/schemas/1.9.0)\", \"name\": \"concurrent_requests\", \"unit\":\n\n\"request\", \"point\": {\"start_time_unix_nano\": 1646627738799214000, \"time_unix_nano\":\n\n1646627774867317000, \"value\": 0, \"aggregation_temporality\": 2, \"is_monotonic\": false}}\n\nWe will come back to using this tool later, but it's worth keeping around if you want to test the\n\nperformance of your applications. We will be looking at some additional tools to generate load in\n\nChapter 11, Diagnosing Problems. Try pushing the load higher to see if you can cause the application\n\nto fail altogether by increasing the number of requests or concurrency.\n\nResource consumption\n\nThe following metrics we will capture from our applications are runtime performance metrics.\n\nCapturing the performance metrics of an application can help us answer questions such as the\n\nfollowing:\n\nHow many resources does my application need?\n\nWhat budget will I need to run this service for the next 6 months?\n\nThis often helps guide decisions of what resources will be needed as the business needs change.\n\nQuite often, application performance metrics, such as memory, CPU, and network consumption,\n\nindicate where time could be spent reducing the cost of an application.\n\nIMPORTANT NOTE",
      "content_length": 2487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "In the following example, we will focus specifically on runtime application metrics. These do not include system-level\n\nmetrics. There is an essential distinction between the two. Runtime application metrics should be recorded by each\n\napplication individually. On the other hand, system-level metrics should only be recorded once for the entire system.\n\nReporting system-level metrics from multiple applications running on the same system is problematic. This will cause\n\nsystem performance metrics to be duplicated, which will require de-duplication either at transport or at analysis time.\n\nAnother problem is that querying the system for metrics is expensive, and doing so multiple times places an unnecessary\n\nburden on the system.\n\nWhen looking for runtime metrics, there are many metrics to choose from. Let's record the memory\n\nconsumption that we will measure using an asynchronous gauge. One of the tools available to\n\nprovide a way to measure memory statistics in Python comes with the standard library. The resource\n\npackage (https://docs.python.org/3/library/resource.html) provides usage information about our\n\nprocess. Additional third-party libraries are available, such as psutil (https://psutil.readthedocs.io/),\n\nwhich provides even more information about the resource utilization of your process. It's an excellent\n\npackage for collecting information about CPU, disk, and network usage.\n\nAs the implementation for capturing this metric will be the same across all the applications in the\n\nsystem, the code for the callback will be placed in common.py. The following creates a\n\nrecord_max_rss_callback method to record the maximum resident set size for the application. It also\n\ndefines a convenience method called start_recording_memory_metrics, which creates the\n\nasynchronous gauge. Add these methods to common.py now: common.py\n\nimport resource\n\nfrom opentelemetry._metrics.measurement import Measurement\n\ndef record_max_rss_callback():\n\nyield Measurement(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n\ndef start_recording_memory_metrics(meter):\n\nmeter.create_observable_gauge(\n\ncallback=record_max_rss_callback,\n\nname=\"maxrss\",\n\nunit=\"bytes\",\n\ndescription=\"Max resident set size\",\n\n)\n\nNext, add a call to start_recording_memory_metrics in each application in our system. Add the\n\nfollowing code to shopper.py, legacy_inventory.py, and grocery_store.py: shopper.py\n\nfrom common import start_recording_memory_metrics\n\nif __name__ == \"__main__\":\n\nstart_recording_memory_metrics(meter)",
      "content_length": 2511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "After adding this code to each application and ensuring they have been reloaded, each should start\n\nreporting the following values: output\n\n{\"attributes\": \"\", \"description\": \"Max resident set size\", \"instrumentation_info\":\n\n\"InstrumentationInfo(legacy-inventory, 0.9.1, https://opentelemetry.io/schemas/1.9.0)\",\n\n\"name\": \"maxrss\", \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.10.0', 'net.host.name':\n\n'host', 'net.host.ip': '10.0.0.141', 'service.name': 'legacy-inventory',\n\n'service.version': '0.9.1'}, maxlen=None)\", \"unit\": \"bytes\", \"point\": {\"time_unix_nano\":\n\n1646637404789912000, \"value\": 33083392}}\n\nAnd just like that, we have memory telemetry about our applications. I urge you to add additional\n\nusage metrics to the application and look at the psutil library mentioned earlier to expand the\n\ntelemetry of your services. The metrics we added to the grocery store are by no means exhaustive.\n\nInstrumenting the code and gaining familiarity with instruments gives us a starting point from which\n\nto work.\n\nSummary\n\nWe've covered much ground in this chapter about the metrics signal. We started by familiarizing\n\nourselves with the different components and terminology of the metrics pipeline and how to\n\nconfigure them. We then looked at all the ins and outs of the individual instruments available to\n\nrecord measurements and used each one to record sample metrics.\n\nUsing views, we learned to aggregate, filter, and customize the metric streams being emitted by our\n\napplication to fit our specific needs. This will be handy when we start leveraging instrumentation\n\nlibraries. Finally, we returned to the grocery store to get hands-on experience with instrumenting an\n\nexisting application and collecting real-world metrics.\n\nMetrics is a deep topic that goes well beyond what has been covered in this chapter, but hopefully,\n\nwhat you've learned thus far is enough to start considering how OpenTelemetry can be used in your\n\ncode. The next chapter will look at the third and final signal we will cover in this book – logging.",
      "content_length": 2128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Chapter 6: Logging – Capturing Events\n\nMetrics and traces go a long way in helping understand the behaviors and intricacies of cloud-native\n\napplications. Sometimes though, it's useful to log additional information that can be used at debug\n\ntime. Logging gives us the ability to record information in a way that is perhaps more flexible and\n\nfreeform than either tracing or metrics. That flexibility is both wonderful and terrible. It allows logs\n\nto be customized to fit whatever need arises using natural language, which often, but not always,\n\nmakes it easier to interpret by the reader. But the flexibility is often abused, resulting in a mess of\n\nlogs that are hard to search through and even harder to aggregate in any meaningful way. This chapter\n\nwill take a look at how OpenTelemetry tackles the challenges of logging and how it can be used to\n\nimprove the telemetry generated by an application. We will cover the following topics:\n\nConfiguring OpenTelemetry to export logs\n\nProducing logs via the OpenTelemetry API and a standard logging library\n\nThe logging signal in practice within the context of the grocery store application\n\nAlong the way, we will learn about standard logging in Python as well as logging with Flask, giving\n\nus a chance to use an instrumentation library as well. But first, let's ensure we have everything we\n\nneed set up.\n\nTechnical requirements\n\nIf you've already completed Chapter 4, Distributed Tracing, or Chapter 5, Metrics - Recording\n\nMeasurements, the setup here will be quite familiar. Ensure the version of Python in your\n\nenvironment is at least Python 3.6 by running the following commands: $ python --version\n\n$ python3 --version\n\nThis chapter will rely on the OpenTelemetry API and SDK packages that are installable via pip with\n\nthe following command. The examples in this chapter are using the version 1.9.0 opentelemetry-api\n\nand opentelemetry-sdk packages: $ pip install opentelemetry-api \\\n\nopentelemetry-sdk \\\n\nopentelemetry-propagator-b3\n\nIMPORTANT NOTE\n\nThe OpenTelemetry examples in this chapter rely on an experimental release of the logging signal for OpenTelemetry. This\n\nmeans it's possible that by the time you're reading this, the updated packages have moved methods to different packages.\n\nThe release notes available for each release should help you identify where the packages have moved to\n\n(https://github.com/open-telemetry/opentelemetry-python/releases).",
      "content_length": 2426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "Additionally, in this chapter, we will use an instrumentation package made available by the\n\nOpenTelemetry Python community. This instrumentation will assist us when adding logging\n\ninformation to Flask applications that are part of the grocery store. Install the package via pip with\n\nthe following command: $ pip install opentelemetry-instrumentation-wsgi\n\nThe code for this chapter is available in the companion repository. The following uses git to copy the\n\nrepository locally: $ git clone https://github.com/PacktPublishing/Cloud-Native-Observability The\n\ncompleted code for the examples in this chapter is available in the chapter06 directory. If you're\n\ninterested in writing the code yourself, I suggest you start by copying the code in the chapter04\n\ndirectory and following along.\n\nLastly, we will need to install the libraries that the grocery store relies on. This can be done via the\n\nfollowing pip command: $ pip install flask requests\n\nWe're now ready to start logging!\n\nConfiguring OpenTelemetry logging\n\nUnlike with the two signals we covered in Chapter 4, Distributed Tracing, and Chapter 5, Metrics -\n\nRecording Measurements, the logging signal in OpenTelemetry does not concern itself with\n\nstandardizing a logging interface. Many languages already have an established logging API, and a\n\ndecision early on in OpenTelemetry was made to leverage those pre-existing tools. Although\n\nOpenTelemetry provides an API capable of producing logging, which we'll use shortly, the signal is\n\nintent on hooking into the existing logging facilities. Its focus is to augment the logs produced and\n\nprovide a mechanism to correlate those logs with other signals. Figure 6.1 shows us the components\n\nof the logging pipeline:",
      "content_length": 1729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Figure 6.1 – The logging pipeline\n\nThese components combine to produce log records and emit them to external systems. The logging\n\npipeline is comprised of the following:\n\nA LogEmitterProvider, which provides a mechanism to instantiate one or more log emitters\n\nThe LogEmitter, which produces LogRecord data\n\nThe LogProcessor, which consumes log records and passes them on to a LogExporter for sending the data to a backend\n\nFirst, as with all the other OpenTelemetry signals, we must configure the provider. The following\n\ncode instantiates a LogEmitterProvider from the SDK, passes in a resource via the resource\n\nargument, and then sets the global log emitter via the set_log_emitter_provider method: logs.py\n\nfrom opentelemetry.sdk._logs import LogEmitterProvider, set_log_emitter_provider from\n\nopentelemetry.sdk.resources import Resource\n\ndef configure_log_emitter_provider():\n\nprovider = LogEmitterProvider(resource=Resource.create())\n\nset_log_emitter_provider(provider)\n\nConfiguring LogEmitter alone won't allow us to produce telemetry. We'll need a log processor and an\n\nexporter to go a step further. Let's add BatchLogProcessor, which, as the name suggests, batches the\n\nprocessing of log records. We will also use a ConsoleLogExporter to output logging information to\n\nthe console: logs.py",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "from opentelemetry.sdk._logs.export import ConsoleLogExporter, BatchLogProcessor\n\nfrom opentelemetry.sdk._logs import LogEmitterProvider, set_log_emitter_provider from\n\nopentelemetry.sdk.resources import Resource\n\ndef configure_log_emitter_provider():\n\nprovider = LogEmitterProvider(resource=Resource.create())\n\nset_log_emitter_provider(provider)\n\nexporter = ConsoleLogExporter()\n\nprovider.add_log_processor(BatchLogProcessor(exporter))\n\nWith OpenTelemetry configured, we're now ready to start instrumenting our logs.\n\nProducing logs\n\nFollowing the pattern from previous signals, we should be ready to get an instance of a log producer\n\nand start logging, right? Well, not quite – let's find out why.\n\nUsing LogEmitter\n\nUsing the same method that we used for metrics and tracing, we can now obtain LogEmitter, which\n\nwill allow us to use the OpenTelemetry API to start producing logs. The following code shows us\n\nhow to accomplish this using the get_log_emitter method: logs.py\n\nfrom opentelemetry.sdk._logs import (\n\nLogEmitterProvider,\n\nget_log_emitter_provider,\n\nset_log_emitter_provider,\n\n)\n\nif __name__ == \"__main__\":\n\nconfigure_log_emitter_provider()\n\nlog_emitter = get_log_emitter_provider().get_log_emitter(\n\n\"shopper\",\n\n\"0.1.2\",\n\n)\n\nWith LogEmitter in hand, we're now ready to generate LogRecord. The LogRecord contains the\n\nfollowing information:\n\ntimestamp: A time associated with the log record in nanoseconds.",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "trace_id: A hex-encoded identifier of the trace to correlate with the log record. There will be more on this, the span identifier,\n\nand trace flags shortly.\n\nspan_id: A hex-encoded identifier of the span to correlate with the log record.\n\ntrace_flags: Trace flags associated with the trace active when the log record was produced.\n\nseverity_text: A string representation of the severity level.\n\nseverity_number: A numeric value of the severity level.\n\nbody: The contents of the log message being recorded.\n\nresource: The resource associated with the producer of the log record.\n\nattributes: Additional information associated with the log record in the form of key-value pairs.\n\nEach one of those fields can be passed as an argument to the constructor; note that all those fields are\n\noptional. The following creates LogRecord with some minimal information and calls emit to produce a\n\nlog entry: logs.py\n\nimport time\n\nfrom opentelemetry.sdk._logs import (\n\nLogEmitterProvider,\n\nLogRecord,\n\nget_log_emitter_provider,\n\nset_log_emitter_provider,\n\n)\n\nif __name__ == \"__main__\":\n\nconfigure_log_emitter_provider()\n\nlog_emitter = get_log_emitter_provider().get_log_emitter(\n\n\"shopper\",\n\n\"0.1.2\",\n\n)\n\nlog_emitter.emit(\n\nLogRecord(\n\ntimestamp=time.time_ns(),\n\nbody=\"first log line\",\n\n)\n\n)\n\nAfter all this work, we can finally see a log line! Run the code, and the output should look something\n\nlike this: output",
      "content_length": 1402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "{\"body\": \"first log line\", \"name\": null, \"severity_number\": \"None\", \"severity_text\": null,\n\n\"attributes\": null, \"timestamp\": 1630814115049294000, \"trace_id\": \"\", \"span_id\": \"\",\n\n\"trace_flags\": null, \"resource\": \"\"}\n\nAs you can see, there's a lot of information missing to give us a full picture of what was happening.\n\nOne of the most important pieces of information associated with a log entry is the severity level. The\n\nOpenTelemetry specification defines 24 different log levels categorized in 6 severity groups, as\n\nshown in the following figure:\n\nFigure 6.2 – Log severity levels defined by OpenTelemetry When defining the severity level, all log levels above that\n\nnumber are reported.\n\nLet's ensure the log record we generate sets a meaningful severity level: logs.py\n\nfrom opentelemetry.sdk._logs.severity import SeverityNumber\n\nif __name__ == \"__main__\":\n\n...\n\nlog_emitter.emit(\n\nLogRecord(\n\ntimestamp=time.time_ns(),\n\nbody=\"first log line\",\n\nseverity_number=SeverityNumber.INFO,\n\n)\n\n)\n\nThere – now at least readers of those logs should be able to know how important those log lines are.\n\nRun the code and look for the severity number in the output: output\n\n{\"body\": \"first log line\", \"name\": null, \"severity_number\": \"<SeverityNumber.INFO: 9>\",\n\n\"severity_text\": null, \"attributes\": null, \"timestamp\": 1630814944956950000, \"trace_id\":\n\n\"\", \"span_id\": \"\", \"trace_flags\": null, \"resource\": \"\"}\n\nAs mentioned earlier in this chapter, one of the goals of the OpenTelemetry logging signal is to\n\nremain interoperable with existing logging APIs. Looking at how much work we just did to get a log",
      "content_length": 1600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "line with minimal information, it really seems like there should be a better way, and there is!\n\nThe standard logging library\n\nWhat if we tried using the standard logging library available in Python to interact with\n\nOpenTelemetry instead? The logging library has been part of the standard Python library since\n\nversion 2.3 and is used by many popular frameworks, such as Django and Flask.\n\nIMPORTANT NOTE\n\nThe standard logging module in Python is quite powerful and flexible. If you're not familiar with it, it may take some time to\n\nget used to it. I recommend reading the Python docs available on python.org here:\n\nhttps://docs.python.org/3/library/logging.html.\n\nThe Python implementation of the OpenTelemetry signal provides an additional component to use,\n\nOTLPHandler. The following figure shows where OTLPHandler fits in with the rest of the logging\n\npipeline:\n\nFigure 6.3 – OTLPHandler uses LogEmitter to produce logs OTLPHandler extends the standard logging library's\n\nlogging.Handler class and uses the configured LogEmitter to produce log records.\n\nIMPORTANT NOTE:\n\nThe OTLPHandler was renamed LoggingHandler in releases of the opentelemetry-sdk package newer than 1.10.0. Be\n\nsure to update any references to it in the examples if you've installed a newer version.",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "The following code block first imports the logging module. Then, using the getLogger method, a\n\nstandard Logger object is obtained. This is the object we will use anytime a log line is needed from\n\nthe application. Finally, OTLPHandler is added to logger, and a warning message is logged: logs.py\n\nimport logging\n\nfrom opentelemetry.sdk._logs import (\n\nLogEmitterProvider,\n\nLogRecord,\n\nOTLPHandler,\n\nget_log_emitter_provider,\n\nset_log_emitter_provider,\n\n)\n\nif __name__ == \"__main__\":\n\n...\n\nlogger = logging.getLogger(__file__)\n\nhandler = OTLPHandler()\n\nlogger.addHandler(handler)\n\nlogger.warning(\"second log line\")\n\nLet's see how the information generated differs from the previous example; many of the fields are\n\nautomatically filled in for us:\n\nThe timestamp is set to the current time.\n\nThe severity number and text are set based on the method used to record a log – in this case, the warning method sets the log\n\nseverity to WARN.\n\nTrace and span information is set by pulling information from the current context. As our example does not include starting a\n\ntrace, we should expect the values in these fields to be invalid.\n\nResource data is set via the log emitter provider.\n\nThis provides us with a significant improvement in the data generated.\n\noutput\n\n{\"body\": \"second log line\", \"name\": null, \"severity_number\": \"<SeverityNumber.WARN: 13>\",\n\n\"severity_text\": \"WARNING\", \"attributes\": {}, \"timestamp\": 1630810960785737984,\n\n\"trace_id\": \"0x00000000000000000000000000000000\", \"span_id\": \"0x0000000000000000\",\n\n\"trace_flags\": 0, \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name':\n\n'unknown_service'}, maxlen=None\"}\n\nNot only does this output contain richer data, but we also didn't need to work nearly as hard to obtain\n\nit, and we used a standard library to generate the logs. The attributes field doesn't appear to contain",
      "content_length": 1939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "anything useful yet – let's fix that. OTLPHandler creates the attribute dictionary by looking at any\n\nextra attributes defined in the standard LogRecord. The following code passes an extra argument at\n\nlogging time: logs.py\n\nif __name__ == \"__main__\":\n\n...\n\nlogger.warning(\"second log line\", extra={\"key1\": \"val1\"}) As with other attribute\n\ndictionaries we may have encountered previously, they should contain information relevant\n\nto the specific event being logged. The output should now show us the additional\n\nattributes: output\n\n{\"body\": \"second log line\", \"name\": null, \"severity_number\": \"<SeverityNumber.WARN: 13>\",\n\n\"severity_text\": \"WARNING\", \"attributes\": {\"key1\": \"val1\"}, \"timestamp\":\n\n1630946024854904064, \"trace_id\": \"0x00000000000000000000000000000000\", \"span_id\":\n\n\"0x0000000000000000\", \"trace_flags\": 0, \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name': 'unknown_service'},\n\nmaxlen=None\"}\n\nLet's produce one last example with the standard logger and update the previous code to record a log\n\nusing the info method. This should give us the same severity as the example where we used the log\n\nemitter directly: logs.py\n\nimport logging\n\nif __name__ == \"__main__\":\n\n...\n\nlogger.info(\"second log line\") Run the code again to see the result. If you're no longer\n\nseeing a log with the second log line as its body and are perplexed, don't worry – you're\n\nnot alone. This is due to a feature of the standard logging library. The Python logging\n\nmodule creates a root logger, which is used anytime a more specific logger isn't\n\nconfigured. By default, the root logger is configured to only log messages with a severity\n\nof a warning or higher. Any logger instantiated via getLogger inherits that severity,\n\nwhich explains why our info level messages are not displayed. Our example can be fixed by\n\ncalling setLevel for the logger we are using in our program: logs.py\n\nif __name__ == \"__main__\":\n\n...\n\nlogger = logging.getLogger(__file__)\n\nlogger.setLevel(logging.DEBUG)\n\nhandler = OTLPHandler()\n\nlogger.addHandler(handler)\n\nlogger.info(\"second log line\")\n\nThe output should now contain the log line as we expected:",
      "content_length": 2232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "output\n\n{\"body\": \"second log line\", \"name\": null, \"severity_number\": \"<SeverityNumber.INFO: 9>\",\n\n\"severity_text\": \"INFO\", \"attributes\": {}, \"timestamp\": 1630857128712922112, \"trace_id\":\n\n\"0x00000000000000000000000000000000\", \"span_id\": \"0x0000000000000000\", \"trace_flags\": 0,\n\n\"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.9.0, 'service.name': 'unknown_service'},\n\nmaxlen=None)\"}\n\nAn alternative way to configure the log level of the root logger is to use the basicConfig method of\n\nthe logging module. This allows you to configure the severity level, formatting, and so on\n\n(https://docs.python.org/3/library/logging.html#logging.basicConfig). Another benefit of using the\n\nexisting logging library means that with a little bit of additional configuration, any existing\n\napplication should be able to leverage OpenTelemetry logging. Speaking of an existing application,\n\nlet's return to the grocery store.\n\nA logging signal in practice\n\nGetting familiar with the logging signal theory is great; now it's time to put it into practice. Before\n\nusing OpenTelemetry logging in the grocery store, let's take a minute to move the configuration code\n\ninto the common.py module: common.py\n\nimport logging\n\nfrom opentelemetry.sdk._logs.export import ConsoleLogExporter, BatchLogProcessor from\n\nopentelemetry.sdk._logs import (\n\nLogEmitterProvider,\n\nOTLPHandler,\n\nset_log_emitter_provider,\n\n)\n\ndef configure_logger(name, version):\n\nprovider = LogEmitterProvider(resource=Resource.create())\n\nset_log_emitter_provider(provider)\n\nexporter = ConsoleLogExporter()\n\nprovider.add_log_processor(BatchLogProcessor(exporter))\n\nlogger = logging.getLogger(name)\n\nlogger.setLevel(logging.DEBUG)\n\nhandler = OTLPHandler()\n\nlogger.addHandler(handler)\n\nreturn logger",
      "content_length": 1831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "With the code in place, we can now obtain a logger in the same fashion as we obtained a tracer and a\n\nmeter previously. The following code updates the shopper application to instantiate a logger via\n\nconfigure_logger. Additionally, let's update the add_item_to_cart method to use logger.info rather\n\nthan print: shopper.py\n\nfrom common import configure_tracer, configure_meter, configure_logger\n\ntracer = configure_tracer(\"shopper\", \"0.1.2\") meter = configure_meter(\"shopper\", \"0.1.2\")\n\nlogger = configure_logger(\"shopper\", \"0.1.2\")\n\n@tracer.start_as_current_span(\"add item to cart\") def add_item_to_cart(item, quantity):\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\n\"item\": item,\n\n\"quantity\": quantity,\n\n}\n\n)\n\nlogger.info(\"add {} to cart\".format(item)) Use the following commands in separate\n\nterminals to launch the grocery store, the legacy inventory, and finally, the shopper\n\napplications: $ python legacy_inventory.py\n\n$ python grocery_store.py\n\n$ python shopper.py\n\nPay special attention to output running from the previous command; it should include similar output,\n\nconfirming that our configuration is correct: output\n\n{\"body\": \"add orange to cart\", \"name\": null, \"severity_number\": \"<SeverityNumber.INFO:\n\n9>\", \"severity_text\": \"INFO\", \"attributes\": {}, \"timestamp\": 1630859469283874048,\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x0fc5e89573d7f794\",\n\n\"trace_flags\": 1, \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',\n\n'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\"}\n\nThis is a great starting point; let's see how we can correlate the information from this log line with the\n\ninformation from our traces.\n\nDistributed tracing and logs\n\nWe saw earlier in this chapter that the LogRecord class contains fields for span and trace identifiers as\n\nwell as trace flags. The intention behind this is to allow logs to be correlated with specific traces and\n\nspans, permitting the end user to gain a better understanding of what their application is doing when",
      "content_length": 2094,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "it's running in production. So often, the process of correlating telemetry involves searching tirelessly\n\nthrough events using a timestamp as a mechanism to match up different sources of information. This\n\nis not always practical for the following reasons:\n\nMany operations happen concurrently on the same system, making it difficult to know which operation caused the event.\n\nThe difficulty caused by operations happening simultaneously is exacerbated in distributed systems as even more operations are\n\noccurring.\n\nThe clocks across different systems may, and often do, drift. This drift causes timestamps to not match.\n\nA mechanism developed to address this has been to produce a unique event identifier for each event\n\nand add this identifier to all logs recorded. One challenge of this is ensuring that this information is\n\nthen propagated across the entire system; this is exactly what the trace identifier in OpenTelemetry\n\ndoes. As shown in Figure 6.4, the trace and span identifiers can pinpoint the specific operation that\n\ntriggers a log to be recorded:\n\nFigure 6.4 – Log and trace correlation\n\nReturning to the output from the previous example, the following shows the logging output as well as\n\na snippet of the tracing output containing the name of the operations and their identifiers. See\n\nwhether you can determine from the output which operation triggered the log record: output\n\n{\"body\": \"add orange to cart\", \"name\": null, \"severity_number\": \"<SeverityNumber.INFO:\n\n9>\", \"severity_text\": \"INFO\", \"attributes\": {}, \"timestamp\": 1630859469283874048,\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x0fc5e89573d7f794\",\n\n\"trace_flags\": 1, \"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python',",
      "content_length": 1738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "'telemetry.sdk.name': 'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name':\n\n'unknown_service'}, maxlen=None)\"}\n\n{\n\n\"name\": \"web request\",\n\n\"context\": {\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x6e4e03cacd3411b5\",\n\n},\n\n}\n\n{\n\n\"name\": \"add item to cart\",\n\n\"context\": {\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x0fc5e89573d7f794\",\n\n},\n\n}\n\n{\n\n\"name\": \"browse\",\n\n\"context\": {\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x5a2262c9dd473b40\",\n\n},\n\n}\n\n{\n\n\"name\": \"visit store\",\n\n\"context\": {\n\n\"trace_id\": \"0x67a8df13b8d5678912a8101bb5724fa4\", \"span_id\": \"0x504caee882574a9e\",\n\n},\n\n}\n\nIf you've guessed that the log line was generated by the add item to cart operation, you've guessed\n\ncorrectly. Although this particular example is simple since you're already familiar with the code itself,\n\nyou can imagine how valuable this information can be to troubleshoot an unfamiliar system.\n\nEquipped with the information provided by the distributed trace associated with the log record, you're\n\nempowered to jump into the source code and debug an issue faster. Let's see how we can use\n\nOpenTelemetry logging with the other applications in our system.\n\nOpenTelemetry logging with Flask",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "As covered previously in the chapter, many frameworks, including Flask, use the standard logging\n\nlibrary in Python. This makes configuring OpenTelemetry for the grocery store similar to how any\n\nchanges to logging in Flask would be done. The following code imports and uses configure_logger\n\nto set up the logging pipeline. Next, we use the logging module's dictConfig method to add\n\nOTLPHandler to the root logger, and configure the severity level to DEBUG to ensure all our logs are\n\noutput. In a production setting, you will likely want to make this option configurable rather than\n\nhardcode it to debug level to save costs: grocery_store.py\n\nfrom logging.config import dictConfig\n\nfrom common import (\n\nconfigure_meter,\n\nconfigure_tracer,\n\nconfigure_logger,\n\nset_span_attributes_from_flask,\n\nstart_recording_memory_metrics,\n\n)\n\ntracer = configure_tracer(\"grocery-store\", \"0.1.2\") meter = configure_meter(\"grocery-\n\nstore\", \"0.1.2\") logger = configure_logger(\"grocery-store\", \"0.1.2\")\n\ndictConfig(\n\n{\n\n\"version\": 1,\n\n\"handlers\": {\n\n\"otlp\": {\n\n\"class\": \"opentelemetry.sdk._logs.OTLPHandler\",\n\n}\n\n},\n\n\"root\": {\"level\": \"DEBUG\", \"handlers\": [\"otlp\"]},\n\n}\n\n)\n\napp = Flask(__name__)\n\nEnsure some requests are sent to the grocery store either by running shopper.py or via curl and see\n\nwhat the output from the server looks like now. The following output shows it before the change on\n\nthe first line and after the change on the second line: output\n\n127.0.0.1 - - [05/Sep/2021 10:58:28] \"GET /products HTTP/1.1\" 200 -",
      "content_length": 1515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "{\"body\": \"127.0.0.1 - - [05/Sep/2021 10:58:48] \\\"GET /products HTTP/1.1\\\" 200 -\", \"name\":\n\nnull, \"severity_number\": \"<SeverityNumber.INFO: 9>\", \"severity_text\": \"INFO\",\n\n\"attributes\": {}, \"timestamp\": 1630864728996940032, \"trace_id\":\n\n\"0x00000000000000000000000000000000\", \"span_id\": \"0x0000000000000000\", \"trace_flags\": 0,\n\n\"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\"}\n\nWe can see the original message is now recorded as the body of the message, and all the additional\n\ninformation is also presented. Although, if we look closely, we can see that the span_id, trace_id,\n\nand trace_flags information is missing. It looks like the context for our request is lost somewhere\n\nalong the way, so let's fix that. What is confusing about this is that we already have hooks defined to\n\nhandle before_request and teardown_request, which, in theory, should ensure that the trace\n\ninformation is available. However, the log record we see is generated by Flask's built-in web server\n\n(wsgi), not the Flask application, and is triggered after the original request has been completed as far\n\nas Flask knows. We can address this by creating middleware ourselves, but thankfully, we don't have\n\nto.\n\nLogging with WSGI middleware\n\nThe OpenTelemetry community publishes a package that provides support for instrumenting an\n\napplication that uses a wsgi-compatible implementation, such as the built-in Flask server. The\n\nopentelemetry-instrumentation-wsgi package provides the middleware that hooks into the\n\nappropriate mechanisms to make trace information for the duration of the request. The following\n\ncode imports the middleware and updates the Flask app to use it: grocery_store.py\n\nfrom opentelemetry.instrumentation.wsgi import OpenTelemetryMiddleware\n\n...\n\napp = Flask(__name__)\n\napp.wsgi_app = OpenTelemetryMiddleware(app.wsgi_app)\n\nWith the middleware in place, a new request to our application should allow us to see the span_id,\n\ntrace_id, and trace_flags components that we expect: output\n\n{\"body\": \"127.0.0.1 - - [05/Sep/2021 11:39:36] \\\"GET /products HTTP/1.1\\\" 200 -\", \"name\":\n\nnull, \"severity_number\": \"<SeverityNumber.INFO: 9>\", \"severity_text\": \"INFO\",\n\n\"attributes\": {}, \"timestamp\": 1630867176948227072, \"trace_id\":\n\n\"0xf999a4164ac2f20c20549f19abd4b434\", \"span_id\": \"0xed5d3071ece38633\", \"trace_flags\": 1,\n\n\"resource\": \"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name':\n\n'opentelemetry', 'telemetry.sdk.version': '1.9.0', 'service.name': 'unknown_service'},\n\nmaxlen=None)\"}",
      "content_length": 2652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "We will look at how this works in more detail in Chapter 7, Instrumentation Libraries, and see how\n\nwe can simplify the application code using instrumentation libraries. For the purpose of this example,\n\nit's enough to know that the middleware enables us to see the tracing information in the log we are\n\nrecording.\n\nResource correlation\n\nAnother piece of data that OpenTelemetry logging uses when augmenting telemetry is the resource\n\nattribute. As you may remember from previous chapters, the resource describes the source of the\n\ntelemetry. This will allow us to correlate events occurring across separate signals for the same\n\nresource. In Chapter 4, Distributed Tracing, we defined a LocalMachineResourceDetector class that\n\nproduces an OpenTelemetry resource that includes information about the local machine. Let's update\n\nthe code in configure_logger that instantiates the LogEmitterProvider to use this resource, rather\n\nthan create an empty resource: common.py\n\ndef configure_logger(name, version):\n\nlocal_resource = LocalMachineResourceDetector().detect()\n\nresource = local_resource.merge(\n\nResource.create(\n\n{\n\nResourceAttributes.SERVICE_NAME: name,\n\nResourceAttributes.SERVICE_VERSION: version,\n\n}\n\n)\n\n)\n\nprovider = LogEmitterProvider(resource=resource) set_log_emitter_provider(provider)\n\n...\n\nWith the change in place, run shopper.py once again to see that the log record now contains more\n\nmeaningful data about the source of the log entry: {\"body\": \"add orange to cart\", \"name\": null,\n\n\"severity_number\": \"<SeverityNumber.INFO: 9>\", \"severity_text\": \"INFO\", \"attributes\": {},\n\n\"timestamp\": 1630949852869427968, \"trace_id\": \"0x2ff0e5c9886f2672c3af4468483d341d\",\n\n\"span_id\": \"0x40d72ae565b4c19a\", \"trace_flags\": 1, \"resource\":\n\n\"BoundedAttributes({'telemetry.sdk.language': 'python', 'telemetry.sdk.name': 'opentelemetry',\n\n'telemetry.sdk.version': '1.9.0', 'net.host.name': 'MacBook-Pro.local', 'net.host.ip': '127.0.0.1',\n\n'service.name': 'shopper', 'service.version': '0.1.2'}, maxlen=None)\"}",
      "content_length": 2010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Looking at the previous output, we now know the name and version of the service. We also have\n\nvaluable information about the machine that generated this information. In a distributed system, this\n\ninformation can be used in combination with metrics generated by the same resource to identify\n\nproblems with a specific system, compute node, environment, or even region.\n\nSummary\n\nWith the knowledge of this chapter ingrained in our minds, we have now covered the core signals\n\nthat OpenTelemetry helps produce. Understanding how to produce telemetry by manually\n\ninstrumenting code is a building block on the road to improving observability. Without telemetry, the\n\njob of understanding what a system is doing is much more difficult.\n\nIn this chapter, we learned about the purpose of the logging implementation in OpenTelemetry, as\n\nwell as how it is intended to co-exist with existing logging implementations. After configuring the\n\nlogging pipeline, we learned how to use the OpenTelemetry API to produce logs and compared doing\n\nso with using a standard logging API. Returning to the grocery store, we explored how logging can\n\nbe correlated with traces and metrics. This allowed us to understand how we may be able to leverage\n\nOpenTelemetry logging within existing applications to improve our ability to use log statements\n\nwhen debugging applications.\n\nFinally, we scratched the surface of how instrumentation libraries can help to make the production of\n\ntelemetry easier. We will take an in-depth look at this in the next chapter, dedicated to simplifying the\n\ngrocery store application by leveraging existing instrumentation libraries.",
      "content_length": 1644,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "Chapter 7: Instrumentation Libraries\n\nUnderstanding the ins and outs of the OpenTelemetry API is quite helpful for manually instrumenting\n\ncode. But what if we could save ourselves some of that work and still have visibility into what our\n\ncode is doing? As covered in Chapter 3, Auto-Instrumentation, one of the initial objectives of\n\nOpenTelemetry is providing developers with tools to instrument their applications at a minimal cost.\n\nInstrumentation libraries combined with auto-instrumentation enable users to start with\n\nOpenTelemetry without learning the APIs, and leverage the community's efforts and expertise.\n\nThis chapter will investigate the components of auto-instrumentation, how they can be configured,\n\nand how they interact with instrumentation libraries. Diving deeper into the implementation details of\n\ninstrumentation libraries will allow us to understand precisely how telemetry data is produced.\n\nAlthough telemetry created automatically may seem like magic, we'll seek to unveil the mechanics\n\nbehind this illusion. The chapter covers the following main topics:\n\nAuto-instrumentation configuration and its components\n\nThe Requests library instrumentor\n\nAutomatic configuration\n\nRevisiting the grocery store\n\nThe Flask library instrumentor\n\nFinding instrumentation libraries\n\nWith this information, we will revisit some of our existing code in the grocery store to simplify our\n\ncode and manage and improve the generated telemetry. Along the way, we will look at the specifics\n\nof existing third-party libraries supported by the OpenTelemetry project. Let's start with setting up\n\nour environment.\n\nTechnical requirements\n\nThe examples in this chapter are provided in this book's companion repository, found here:\n\nhttps://github.com/PacktPublishing/Cloud-Native-Observability. The source code can be downloaded\n\nvia git as per the following command: $ git clone https://github.com/PacktPublishing/Cloud-Native-\n\nObservability $ cd Cloud-Native-Observability/chapter07\n\nThe completed examples from this chapter are in the chapter7 directory. If you'd prefer the refactor\n\nalong, copy the code from chapter6 as a starting point. Next, we'll need to ensure the version of\n\nPython on your system is at least 3.6. You can verify it with the following commands: $ python --\n\nversion",
      "content_length": 2301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Python 3.8.9\n\n$ python3 --version\n\nPython 3.8.9\n\nThis chapter will use the same opentelemetry-api, opentelemetry-sdk, and opentelemetry-\n\npropagator-b3 packages we installed in previous chapters. In addition, we will use the\n\nopentelemetry-instrumentation and opentelemetry-distro packages. Install the packages via pip\n\nnow: $ pip install opentelemetry-api \\\n\nopentelemetry-sdk \\\n\nopentelemetry-instrumentation \\\n\nopentelemetry-propagator-b3 \\\n\nopentelemetry-distro\n\nWe will need to install additional packages libraries used by our applications: the Flask and Requests\n\nlibraries. Lastly, we will install the instrumentation libraries that automatically instrument the calls\n\nfor those libraries. The standard naming convention for instrumentation libraries in OpenTelemetry is\n\nto prefix the library's name being instrumented with opentelemetry-instrumentation-. Use pip to\n\ninstall those packages now: $ pip install flask \\\n\nopentelemetry-instrumentation-flask \\\n\nrequests \\\n\nopentelemetry-instrumentation-requests\n\nEnsure all the required packages have been installed by looking at the output from pip freeze, which\n\nlists all the packages installed: $ pip freeze | grep opentelemetry\n\nopentelemetry-api==1.9.0\n\nopentelemetry-distro==0.28b0\n\nopentelemetry-instrumentation==0.28b0\n\nopentelemetry-instrumentation-flask==0.28b0\n\nopentelemetry-instrumentation-requests==0.28b0\n\nopentelemetry-instrumentation-wsgi==0.28b0\n\nopentelemetry-propagator-b3==1.9.0\n\nopentelemetry-proto==1.9.0\n\nopentelemetry-sdk==1.9.0\n\nopentelemetry-semantic-conventions==0.28b0\n\nopentelemetry-util-http==0.28b0",
      "content_length": 1588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Throughout the chapter, we will rely on two scripts made available by the opentelemetry-\n\ninstrumentation package: opentelemetry-instrument and opentelemetry-bootstrap. Ensure these\n\nscripts are available in your path with the following commands: $ opentelemetry-instrument --help\n\nusage: opentelemetry-instrument [-h]...\n\n$ opentelemetry-bootstrap --help\n\nusage: opentelemetry-bootstrap [-h]...\n\nNow that we have all the packages installed and the code available, let's see how auto-\n\ninstrumentation works in practice.\n\nAuto-instrumentation configuration\n\nSince auto-instrumentation aims to get started as quickly as possible, let's see how fast we can\n\ngenerate telemetry with as little code as possible. The following code makes a web request to\n\nhttps://www.cloudnativeobservability.com and prints the HTTP response code: http_request.py\n\nimport requests\n\nurl = \"https://www.cloudnativeobservability.com\"\n\nresp = requests.get(url)\n\nprint(resp.status_code)\n\nWhen running the code, assuming network connectivity is available and the URL we're requesting\n\nconnects us to a server that is operating normally, we should see 200 printed out: $ python\n\nhttp_request.py\n\n200\n\nGreat, the program works; now it's time to instrument it. The following command uses the\n\nopentelemetry-instrument application to wrap the application we created. We will look more closely\n\nat the command and its options shortly. For now, run the command: $ opentelemetry-instrument --\n\ntraces_exporter console \\\n\n--metrics_exporter console \\\n\n--logs_exporter console \\\n\npython http_request.py\n\nIf everything went according to plan, we should now see the following output, which contains\n\ntelemetry: output\n\n200\n\n{\n\n\"name\": \"HTTP GET\",",
      "content_length": 1708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "\"context\": {\n\n\"trace_id\": \"0x953ca1322b930819077a921a838df0cd\", \"span_id\": \"0x5b3b72c9c836178a\",\n\n\"trace_state\": \"[]\"\n\n},\n\n\"kind\": \"SpanKind.CLIENT\",\n\n\"parent_id\": null,\n\n\"start_time\": \"2021-11-25T17:38:21.331540Z\", \"end_time\": \"2021-11-25T17:38:22.033434Z\",\n\n\"status\": {\n\n\"status_code\": \"UNSET\"\n\n},\n\n\"attributes\": {\n\n\"http.method\": \"GET\",\n\n\"http.url\": \"https://www.cloudnativeobservability.com\", \"http.status_code\": 200\n\n},\n\n\"events\": [],\n\n\"links\": [],\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.9.0\",\n\n\"telemetry.auto.version\": \"0.28b0\",\n\n\"service.name\": \"unknown_service\"\n\n}\n\n}\n\nOkay, that's exciting, but what just happened? Figure 7.1 shows how the opentelemetry-instrument\n\ncommand is instrumenting the code for our web request by doing the following:\n\n1. Loading the configuration options defined by the OpenTelemetryDistro class, which is part of the opentelemetry-\n\ndistro package.\n\n2. Automatically configuring the telemetry pipelines for traces, metrics, and logs via OpenTelemetryConfigurator. The\n\ndetails of how this configuration is set will become clearer shortly.\n\n3. Iterating through instrumentor classes registered via entry points in the Python environment under\n\nopentelemetry_instrumentor to find available instrumentation libraries. In doing so, it finds and loads the\n\nRequestsInstrumentor class defined in the opentelemetry-instrumentation-requests package.\n\n4. With the instrumentation library loaded, the call to get is now processed by the requests instrumentation library, which\n\ncreates a span before calling the original get method.",
      "content_length": 1642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "The preceding steps are depicted in the following diagram:\n\nFigure 7.1 – opentelemetry-instrument\n\nThe configuration of the telemetry pipeline involves a few different mechanisms loaded via entry\n\npoints at various times before the application code is executed. Thinking back to Chapter 3, Auto-\n\nInstrumentation, we introduced entry points (https://packaging.python.org/specifications/entry-\n\npoints/) as a mechanism that allows Python packages to register classes or methods globally. The\n\ncombination of entry points, interfaces, and options to choose from can make the configuration\n\nprocess a bit complex to understand.\n\nOpenTelemetry distribution\n\nThe first step in the configuration process is loading classes registered under the\n\nopentelemetry_distro entry point. This entry point is reserved for classes adhering to the BaseDistro",
      "content_length": 840,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "interface, and its purpose is to allow implementors to set configuration options at the earliest possible\n\ntime. The term distro is short for distribution, a concept that is still being officially defined in\n\nOpenTelemetry. Essentially, a distro is a way for users to customize OpenTelemetry to fit their needs,\n\nallowing them to reduce the complexity of deploying and using OpenTelemetry. For example, the\n\ndefault configuration for OpenTelemetry Python is to configure an OpenTelemetry protocol exporter\n\nfor all signals. This is accomplished via the OpenTelemetryDistro class mentioned previously. The\n\nfollowing code shows us how the OpenTelemetryDistro class configures the default exporter by\n\nsetting environment variables: OpenTelemetryDistro class\n\nclass OpenTelemetryDistro(BaseDistro):\n\n\"\"\"\n\nThe OpenTelemetry provided Distro configures a default\n\nconfiguration out of the box.\n\n\"\"\"\n\ndef _configure(self, **kwargs):\n\nos.environ.setdefault(OTEL_TRACES_EXPORTER, \"otlp_proto_grpc\")\n\nos.environ.setdefault(OTEL_METRICS_EXPORTER, \"otlp_proto_grpc\")\n\nos.environ.setdefault(OTEL_LOGS_EXPORTER, \"otlp_proto_grpc\") As a user, you could create\n\nyour distribution to preconfigure all the specific parameters needed to tailor auto-\n\ninstrumentation for your environment: for example, protocol, destination, and transport\n\noptions. A list of open source examples extending the BaseDistro interface can be found\n\nhere: https://github.com/PacktPublishing/Cloud-Native-\n\nObservability/tree/main/chapter7#opentelemetry-distro-implementations. With those options\n\nconfigured, you can then provide an entry point to your implementation of the BaseDistro\n\ninterface, package it up, and add this new package as a dependency in your applications.\n\nTherefore, the distribution makes deploying a consistent configuration across a\n\ndistributed system easier.\n\nOpenTelemetry configurator\n\nThe next piece of the configuration puzzle is what is currently known in OpenTelemetry Python as\n\nthe configurator. The purpose of the configurator is to load all the components defined in the\n\nconfiguration specified by the distro. Another way is to think of the distro as the co-pilot, deciding\n\nwhere the car needs to go, and the configurator as the driver. The configurator is an extensible and\n\ndeclarative interface for configuring OpenTelemetry. It is loaded by auto-instrumentation via, and\n\nyou may have guessed it, an entry point. The opentelemetry_configurator entry point is reserved for\n\nclasses adhering to the _BaseConfigurator interface, whose sole purpose is to prepare the logs,\n\nmetrics, and traces pipelines to produce telemetry.\n\nIMPORTANT NOTE",
      "content_length": 2640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "As you may have noticed, the _BaseConfigurator class is preceded by an underscore. This is done intentionally for\n\nclasses that are not officially part of the supported OpenTelemetry API in Python and warrant extra caution. Methods and\n\nclasses that are not supported formally can and often do change with new releases.\n\nThe implementation of the _BaseConfigurator interface loaded in the previous example, the\n\nOpenTelemetryConfigurator class configures a telemetry pipeline for each signal using components\n\nfrom the standard opentelemetry-sdk package. As a user, this configurator is precisely what you want\n\nmost of the time. However, if a user wishes to provide an alternative SDK, it would be possible to\n\nprovide their configurator implementation to use this SDK instead.\n\nThis covers the two main entry points used by auto-instrumentation. We will continue discussing\n\nadditional entry points throughout this chapter. As a reference, the following table captures the entry\n\npoints used by OpenTelemetry Python along with the interface each entry point expects. The table\n\nalso shows us a brief description of what each entry point is used for:\n\nFigure 7.2 – Entry points used in OpenTelemetry Python\n\nSimilar to OpenTelemetryDistro, the OpenTelemetryConfigurator class and its parent use environment\n\nvariables to achieve its goal of configuring OpenTelemetry for the end use.",
      "content_length": 1384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Environment variables\n\nTo provide additional flexibility to users, OpenTelemetry supports the configuration of many of its\n\ncomponents across all languages via environment variables. These variables are defined in the\n\nOpenTelemetry specification, ensuring each compliant language implementation understands them.\n\nThis allows users to re-use the same configuration options across any language they choose. I\n\nrecommend reading the complete list of options available in the specification repository found here:\n\nhttps://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/sdk-\n\nenvironment-variables.md.\n\nWe will look more closely at specific variables as we refactor the grocery store further in this chapter.\n\nMany, but not all, of the environment variables used by auto-instrumentation are part of the\n\nspecification linked previously. This is because the implementation details of each language may\n\nrequire additional variables not relevant to others. Language-specific environment variables are\n\nsupported in the following format: OTEL_{LANGUAGE}_{FEATURE}\n\nAs we'll see shortly, Python-specific options are prefixed with OTEL_PYTHON_. Any option with this\n\nprefix will only be found in Python, and the naming convention helps set that expectation with users.\n\nCommand-line options\n\nThe last tool available to configure OpenTelemetry without editing the application code is the use of\n\ncommand-line arguments, which can be set when invoking opentelemetry-instrument. Recall the\n\ncommand we used to call in the earlier example: $ opentelemetry-instrument --traces_exporter\n\nconsole \\\n\n--metrics_exporter console \\\n\n--logs_exporter console \\\n\npython http_request.py\n\nThis command used command-line arguments to override the traces, metrics, and logs exporters to\n\nuse the console exporter instead of the configured default. All options available via command line\n\ncan be listed using the --help flag when invoking opentelemetry-instrument. These options are the\n\nsame as those available through environment variables, with a slightly easier name for convenience.\n\nThe name of the command-line argument is the name of the environment variable in lowercase\n\nwithout the OTEL_ or OTEL_PYTHON prefix. The following table shows a few examples:",
      "content_length": 2268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Figure 7.3 – Environment variable to command-line argument translations With that, we've covered how auto-\n\ninstrumentation configures OpenTelemetry to generate the telemetry we saw. But what about the instrumented call?\n\nLet's see how the Requests library instrumentation works.\n\nRequests library instrumentor\n\nThe Instrumentor interface provides instrumentation libraries with the minimum requirements a\n\nlibrary must provide to support auto-instrumentation. Implementors must provide an implementation\n\nfor _instrument and _uninstrument, that's all. The instrumentation implementation details vary from\n\none library to another depending on whether the library offers any event or callback mechanisms for\n\ninstrumentation. In the case of the Requests library, the opentelemetry-instrumentation-requests\n\nlibrary relies on monkey patching the Session.request and Session.send methods from the requests\n\nlibrary. This instrumentation library does the following:\n\n1. Provides a wrapper method for the library calls that it instruments, and intercepts calls through those wrappers\n\n2. Upon invocation, creates a new span by calling the start_as_current_span method of the OpenTelemetry API, ensuring\n\nthe span name follows semantic conventions\n\n3. Injects the context information into the request headers via the context API's attach method to ensure the tracing data is\n\npropagated to the request's destination\n\n4. Reads the response and sets the status code accordingly via the span's set_status method Important Note\n\nInstrumentation libraries must check if the span will be recorded before adding additional attributes to avoid potentially\n\ncostly operations. This is done to minimize the instrumentation's impact on existing applications when it is not in use.\n\nAdditional implementation details can be found in the opentelemetry-python-contrib repository:\n\nhttps://github.com/open-telemetry/opentelemetry-python-\n\ncontrib/blob/main/instrumentation/opentelemetry-instrumentation-\n\nrequests/src/opentelemetry/instrumentation/requests/__init__.py. The code may inspire you to write\n\nand contribute an instrumentation library of your own.\n\nAdditional configuration options\n\nThe Requests instrumentation library supports the following additional configurable options:",
      "content_length": 2266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "span_callback: A callback mechanism to inject additional information into a span is available via this parameter. For\n\nexample, this allows users to inject additional information from the response into the span.\n\nname_callback: The default name of a span created by the requests instrumentation library is in the HTTP {method}\n\nformat. The name_callback parameter allows users to customize the name of the span as needed.\n\nexcluded_urls: There are HTTP destinations for which capturing telemetry may not be desirable, a typical case being\n\nrequests made to a health check endpoint. The excluded_urls parameter supports configuring a comma-separated list of\n\nURLs exempt from telemetry. This parameter is also configurable via the OTEL_PYTHON_REQUESTS_EXCLUDED_URLS\n\nenvironment variable and is available for use with auto-instrumentation.\n\nAs you may have noted by reading the description of each configuration option, not all these options\n\nare available for configuration via auto-instrumentation. It's possible to use instrumentation libraries\n\nwithout auto-instrumentation. Let's see how.\n\nManual invocation\n\nThe following code updates the previous example to configure a tracer and instrument the\n\nrequests.get call via the instrumentation library: http_request.py\n\nimport requests\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom opentelemetry.sdk.trace.export import (\n\nBatchSpanProcessor,\n\nConsoleSpanExporter,\n\n)\n\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\n\ndef configure_tracer():\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter)\n\nprovider = TracerProvider()\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nconfigure_tracer()\n\nRequestsInstrumentor().instrument()\n\nurl = \"https://www.cloudnativeobservability.com\"\n\nresp = requests.get(url)\n\nprint(resp.status_code)",
      "content_length": 1911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "This is quite a bit of additional code. Since we're no longer relying on auto-instrumentation, we must\n\nconfigure the tracing pipeline manually. Running this code without invoking opentelemetry-\n\ninstrument looks like this: $ python http_request.py\n\nThis should yield very similar telemetry to what we saw earlier. The following shows an excerpt of\n\nthat output: output\n\n200\n\n{\n\n\"name\": \"HTTP GET\",\n\n\"context\": {\n\n\"trace_id\": \"0xc2ee1f399911a10d361231a46c6fec1b\", ...\n\nWe can further customize the telemetry produced by configuring additional options we discussed\n\npreviously. The following code example will customize the name of the span and add other attributes\n\nto the data generated. It does so by doing the following:\n\nAdding a rename_span method to replace the HTTP prefix in the name\n\nAdding the add_response_attribute method to append header information from the response object as a span attribute\n\nUpdating the call to instrument to utilize the new functionality\n\nhttp_request.py\n\ndef rename_span(method, url):\n\nreturn f\"Web Request {method}\"\n\ndef add_response_attributes(span, response):\n\nspan.set_attribute(\"http.response.headers\", str(response.headers))\n\nconfigure_tracer()\n\nRequestsInstrumentor().instrument(\n\nname_callback=rename_span,\n\nspan_callback=add_response_attributes,\n\n)\n\nRunning the updated code should give us the slightly updated telemetry as per the following\n\nabbreviated sample output: output\n\n200\n\n{\n\n\"name\": \"Web Request GET\",\n\n\"attributes\": {\n\n\"http.method\": \"GET\",",
      "content_length": 1498,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "\"http.url\": \"https://www.cloudnativeobservability.com\", \"http.status_code\": 200,\n\n\"http.response.headers\": \"{'Connection': 'keep-alive', 'Content-Length': '1864', 'Server':\n\n'GitHub.com'\n\n...\n\nWith this, we've now seen how to leverage the Requests instrumentation library without using auto-\n\ninstrumentation. The added flexibility of the features not available through auto-instrumentation is\n\nnice, but configuring pipelines is tedious. Thankfully, it's possible to get the best of both worlds by\n\nusing auto-instrumentation and configuring the instrumentor manually. Update the example to\n\nremove all the configuration code. The following is all that should be left: http_request.py\n\nimport requests\n\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor def\n\nrename_span(method, url):\n\nreturn f\"Web Request {method}\"\n\ndef add_response_attributes(span, response):\n\nspan.set_attribute(\"http.response.headers\", str(response.headers))\n\nRequestsInstrumentor().instrument(\n\nname_callback=rename_span,\n\nspan_callback=add_response_attributes,\n\n)\n\nresp = requests.get(\"https://www.cloudnativeobservability.com\") print(resp.status_code)\n\nRun the new code via the following command we used earlier in the chapter: $ opentelemetry-\n\ninstrument --traces_exporter console \\\n\n--metrics_exporter console \\\n\n--logs_exporter console \\\n\npython http_request.py\n\nLooking at the output, it's clear that something didn't go as planned. The following warning appears\n\nat the top of the output: Attempting to instrument while already instrumented\n\nAdditionally, if you look through the telemetry generated, the span name is back to its original value,\n\nand the response headers attribute is missing. Recall that the opentelemetry-instrument script\n\niterates through all the installed instrumentors before calling the application code. This means that by\n\nthe time our application code is executed, the Request instrumentor has already instrumented the\n\nRequests library.\n\nDouble instrumentation",
      "content_length": 1991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Many instrumentation libraries have a safeguard in place to prevent double instrumentation. Double\n\ninstrumentation in most cases would mean that every piece of telemetry generated is recorded twice.\n\nThis causes all sorts of problems, from potential added performance costs to making telemetry\n\nanalysis difficult.\n\nWe can ensure that the library isn't instrumented first to mitigate this issue. Add the following method\n\ncall to your code: http_request.py\n\nimport requests\n\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor ...\n\nRequestsInstrumentor().uninstrument()\n\nRequestsInstrumentor().instrument(\n\nname_callback=rename_span,\n\nspan_callback=add_response_attributes,\n\n)\n\nRunning this code once more shows us that the warning is gone and that the telemetry contains the\n\ncustomization we expected. All this with much simpler code. Great! Let's see now how we can apply\n\nthis to the grocery store.\n\nAutomatic configuration\n\nWe added new instrumentation in the past three chapters and watched how we could generate more\n\ninformation each time we instrumented the code. We will now see how we can continue to provide\n\nthe same level of telemetry but simplify our lives by removing some of the code. The first code we\n\nwill be removing is the configuration code we extracted into the common.py module. If you recall from\n\nprevious chapters, the purpose of the configure_tracer, configure_meter, and configure_logger\n\nmethods, which we will review in detail shortly, is to do the following:\n\nConfigure the emitter of telemetry.\n\nConfigure the destination and mechanism to output the telemetry.\n\nAdd resource information to identify our service.\n\nAs we saw earlier in this chapter, the opentelemetry-instrument script enables us to remove the code\n\ndoing the configuration by interpreting environment variables or command-line arguments that will\n\ndo the same thing. We will review the configuration code for each signal and look at the flags that\n\ncan be used to replace the code with environment variables. One of the configurations common to all\n\nsignals is the resource information; let's start there.",
      "content_length": 2126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "Configuring resource attributes\n\nA resource provides information about the source of the telemetry. If you look through the common.py\n\ncode, you may recall that each method used to configure a signal also called methods to configure the\n\nresource. The code looks something like the following: common.py\n\nlocal_resource = LocalMachineResourceDetector().detect()\n\nresource = local_resource.merge(\n\nResource.create(\n\n{\n\nResourceAttributes.SERVICE_NAME: name,\n\nResourceAttributes.SERVICE_VERSION: version,\n\n}\n\n)\n\n)\n\nThe code uses a resource detector to fill in the hostname and IP address automatically. A current\n\nlimitation of auto-instrumentation in Python is the lack of support for configuring resource detectors.\n\nThankfully, since the functionality of our resource detector is somewhat limited, it's possible to\n\nreplace it, as we'll see shortly.\n\nThe code also adds a service name and version information to our resource. Resource attributes can\n\nbe configured for auto-instrumentation through one of the following options:\n\nFigure 7.4 – Resource configuration\n\nNote that the command-line arguments are shown here for reference only. For the remainder of the\n\nchapter, the commands used to run applications will use environment variables. The format of the\n\nparameters used for both methods is interchangeable. However, the OpenTelemetry specification only\n\nofficially supports environment variables. These are consistent across implementations.\n\nThe following shows how using only environment variables to configure resources can produce the\n\nsame result as the previous code. The example uses the hostname system utility to retrieve the name\n\nof the current host and ipconfig to retrieve the IP address. The invocation for these tools may vary\n\ndepending on your system: $ OTEL_RESOURCE_ATTRIBUTES=\"service.name=chap7-Requests-\n\napp, service.version=0.1.2,",
      "content_length": 1862,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "net.host.name='hostname',\n\nnet.host.ip='ipconfig getifaddr en0'\" \\\n\nopentelemetry-instrument --traces_exporter console \\\n\n--metrics_exporter console \\\n\n--logs_exporter console \\\n\npython http_request.py\n\nThe resource information in the output from this command now includes the following details: output\n\n\"resource\": {\n\n\"telemetry.sdk.language\": \"python\",\n\n\"telemetry.sdk.name\": \"opentelemetry\",\n\n\"telemetry.sdk.version\": \"1.9.0\",\n\n\"service.name\": \"chap7-Requests-app\",\n\n\"service.version\": \"0.1.2\",\n\n\"net.host.name\": \"cloud\",\n\n\"net.host.ip\": \"10.0.0.141\",\n\n\"telemetry.auto.version\": \"0.28b0\"\n\n}\n\nWe can now start configuring signals with resource attributes out of the way.\n\nConfiguring traces\n\nThe following code shows the configure_tracer method used to configure the tracing pipeline. Note\n\nthat the code no longer contains resource configuration as we've already taken care of that:\n\ncommon.py\n\ndef configure_tracer(name, version):\n\nexporter = ConsoleSpanExporter()\n\nspan_processor = BatchSpanProcessor(exporter)\n\nprovider = TracerProvider()\n\nprovider.add_span_processor(span_processor)\n\ntrace.set_tracer_provider(provider)\n\nreturn trace.get_tracer(name, version)\n\nThe main components to configure for tracing to emit telemetry are as follows:\n\nTracerProvider",
      "content_length": 1262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "SpanProcessor\n\nSpanExporter\n\nIt's possible to set both TracerProvider and SpanExporter via environment variables. This is not the\n\ncase for SpanProcessor. The OpenTelemetry SDK for Python defaults to using BatchSpanProcessor\n\nwhen auto-instrumentation is used in combination with the opentelemetry-distro package. Options\n\nfor configuring BatchSpanProcessor are available via environment variables.\n\nIMPORTANT NOTE\n\nBatchSpanProcessor will satisfy most use cases. However, if your application requires an alternative SpanProcessor\n\nimplementation, it can be specified via a custom OpenTelemetry distribution package. Custom span processors can filter\n\nor enhance data before it is exported.\n\nAnother component we haven't talked about much yet is the sampler, which we'll cover in Chapter\n\n12, Sampling. For now, it's enough to know that the sampler is also configurable via environment\n\nvariables.\n\nThe following table shows the options for configuring the tracing pipeline. The acronym BSP stands\n\nfor BatchSpanProcessor:\n\nFigure 7.5 – Tracing configuration\n\nAs we continue adding configuration options, the command used to launch the application can get\n\nquite unruly. To alleviate this, I recommend exporting each variable as we go along. The following\n\nexports the OTEL_RESOURCE_ATTRIBUTES variable we previously set: $ export\n\nOTEL_RESOURCE_ATTRIBUTES=\"service.name=chap7-Requests-app, service.version=0.1.2,\n\nnet.host.name='hostname', net.host.ip='ipconfig getifaddr en0'\"",
      "content_length": 1478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "We've already configured the exporter via command-line arguments in previous examples. The\n\nfollowing shows us configuring the exporter and provider via environment variables. The console\n\nand sdk strings correspond to the name of the entry point for the ConsoleSpanExporter and the\n\nOpenTelemetry SDK TracerProvider classes: $ OTEL_TRACES_EXPORTER=console \\\n\nOTEL_PYTHON_TRACER_PROVIDER=sdk \\\n\nopentelemetry-instrument --metrics_exporter console \\\n\n--logs_exporter console \\\n\npython http_request.py\n\nReading the output from the previous command is uneventful as it is just setting the same\n\nconfiguration in another way. However, we can now move on to metrics with this configuration in\n\nplace.\n\nConfiguring metrics\n\nThe configuration for metrics is similar to the configuration for tracing, as we can see from the\n\nfollowing code for the configure_meter method: common.py\n\ndef configure_meter(name, version):\n\nexporter = ConsoleMetricExporter()\n\nprovider = MeterProvider()\n\nset_meter_provider(provider)\n\nreturn get_meter_provider().get_meter(\n\nname=name,\n\nversion=version,\n\n)\n\nAt the time of writing, the specification for metrics is reaching stability. As such, the support for\n\nauto-instrumentation and configuration will likely solidify over the coming months. For now, this\n\nsection will focus on the options that are available and not likely to change, which covers the\n\nfollowing:\n\nMeterProvider\n\nMetricExporter\n\nThe following table shows the options available to configure the metrics pipeline:",
      "content_length": 1503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "Figure 7.6 – Metrics configuration\n\nThe following command is provided as a reference for configuring MeterProvider and\n\nMetricsExporter via environment variables: $ OTEL_METRICS_EXPORTER=console \\\n\nOTEL_PYTHON_METER_PROVIDER=sdk \\\n\nopentelemetry-instrument --logs_exporter console \\\n\npython http_request.py\n\nNote that running the previous command as is results in an error as it does not configure the tracing\n\nsignal. Any signal not explicitly configured defaults to using the OpenTelemetry Protocol (OTLP)\n\nexporter, which we've not installed in this environment. As the application does not currently produce\n\nmetrics, we wouldn't expect to see any changes in the telemetry emitted.\n\nConfiguring logs\n\nThe configure_logger method configures the following OpenTelemetry components:\n\nLogEmitterProvider\n\nLogProcessor\n\nLogExporter\n\ncommon.py\n\ndef configure_logger(name, version):\n\nprovider = LogEmitterProvider()\n\nset_log_emitter_provider(provider)\n\nexporter = ConsoleLogExporter()\n\nprovider.add_log_processor(BatchLogProcessor(exporter))\n\nlogger = logging.getLogger(name)\n\nlogger.setLevel(logging.DEBUG)\n\nhandler = OTLPHandler()\n\nlogger.addHandler(handler)\n\nreturn logger",
      "content_length": 1172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "As with metrics, the configuration and auto-instrumentation for the logging signal are still currently\n\nunder development. The following table can be used as a reference for the environment variables and\n\ncommand-line arguments available to configure logging at the time of writing:\n\nFigure 7.7 – Logging configuration\n\nAs with the tracing configuration's span processor, there isn't currently a mechanism for configuring\n\nthe log processor via auto-instrumentation. This can change in the future. Using those options, we\n\nknow how to configure the last signal for auto-instrumentation: $\n\nOTEL_LOGS_EXPORTER=console \\\n\nOTEL_PYTHON_LOG_EMITTER_PROVIDER=sdk \\\n\nopentelemetry-instrument python http_request.py\n\nWe're almost ready to revisit the grocery store code with the signals and resources configured. The\n\nlast thing left to configure is propagation.\n\nConfiguring propagation\n\nContext propagation provides the ability to share context information across distributed systems. This\n\ncan be accomplished via various mechanisms, as we discovered in Chapter 4, Distributed Tracing –\n\nTracing Code Execution. To ensure applications can interoperate with any of the propagation formats,\n\nOpenTelemetry supports configuring propagators via the following environment variable:\n\nFigure 7.8 – Propagator configuration\n\nLater in this chapter, an application will need to configure the B3 and TraceContext propagators.\n\nOpenTelemetry makes it possible to configure multiple propagators by specifying a comma-separated\n\nlist. As mentioned earlier, with so many configuration options, using environment variables can\n\nbecome hard to manage. An effort is underway to add support for configuration files to\n\nOpenTelemetry, but the timeline on when that will be available is still in flux.",
      "content_length": 1775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Recall the code we instrumented in the last three chapters. Let's go through it now and leverage\n\nconfiguration and the instrumentation libraries wherever possible.\n\nRevisiting the grocery store\n\nIt's finally time to use all this new knowledge about auto-instrumentation to clean up the grocery\n\nstore application. This section will showcase the simplified code that continues to produce the\n\ntelemetry we've come to expect over the last few chapters. The custom decorators have been\n\nremoved, as has the code configuring the tracer provider, meter provider, and log emitter provider.\n\nAll we're left with now is the application code.\n\nLegacy inventory\n\nThe legacy inventory service is a great place to start. It is a small Flask application with a single\n\nendpoint. The Flask instrumentor, installed at the beginning of the chapter via the opentelemetry-\n\ninstrumentation-flask package, will replace the manual instrumentation code we previously added.\n\nThe following code instantiates the Flask app and provides the /inventory endpoint:\n\nlegacy_inventory.py\n\n#!/usr/bin/env python3\n\nfrom flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route(\"/inventory\")\n\ndef inventory():\n\nproducts = [\n\n{\"name\": \"oranges\", \"quantity\": \"10\"}, {\"name\": \"apples\", \"quantity\": \"20\"}, ]\n\nreturn jsonify(products)\n\nif __name__ == \"__main__\":\n\napp.run(port=5001)\n\nIf you remember from previous chapters, this service was configured to use the B3 format propagator.\n\nThis will be reflected in the configuration options we pass in when starting the service via auto-\n\ninstrumentation: $ OTEL_RESOURCE_ATTRIBUTES=\"service.name=legacy-inventory,\n\nservice.version=0.9.1,\n\nnet.host.name='hostname',\n\nnet.host.ip='ipconfig getifaddr en0'\" \\\n\nOTEL_TRACES_EXPORTER=console \\",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "OTEL_PYTHON_TRACER_PROVIDER=sdk \\\n\nOTEL_METRICS_EXPORTER=console \\\n\nOTEL_PYTHON_METER_PROVIDER=sdk \\\n\nOTEL_LOGS_EXPORTER=console \\\n\nOTEL_PYTHON_LOG_EMITTER_PROVIDER=sdk \\\n\nOTEL_PROPAGATORS=b3 \\\n\nopentelemetry-instrument python legacy_inventory.py\n\nWith this service running, let's look at the next one.\n\nGrocery store\n\nThe next service to revisit is the grocery store. This service is also a Flask application and will\n\nleverage the same instrumentation library. In addition, it will use the Requests instrumentor to add\n\ntelemetry to the calls it makes to the legacy inventory. The code looks like this: grocery_store.py\n\n#!/usr/bin/env python3\n\nfrom logging.config import dictConfig\n\nimport requests\n\nfrom flask import Flask\n\nfrom opentelemetry.instrumentation.wsgi import OpenTelemetryMiddleware dictConfig(\n\n{\n\n\"version\": 1,\n\n\"handlers\": {\n\n\"otlp\": {\n\n\"class\": \"opentelemetry.sdk._logs.OTLPHandler\", }\n\n},\n\n\"root\": {\"level\": \"DEBUG\", \"handlers\": [\"otlp\"]}, }\n\n)\n\napp = Flask(__name__)\n\napp.wsgi_app = OpenTelemetryMiddleware(app.wsgi_app)\n\n@app.route(\"/\")\n\ndef welcome():\n\nreturn \"Welcome to the grocery store!\"\n\n@app.route(\"/products\")\n\ndef products():",
      "content_length": 1157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "url = \"http://localhost:5001/inventory\"\n\nresp = requests.get(url)\n\nreturn resp.text\n\nif __name__ == \"__main__\":\n\napp.run(port=5000)\n\nRunning the application will look very similar to running the legacy inventory with only a few\n\ndifferent parameters:\n\nservice.name and service.version will be updated to reflect the different applications.\n\nThe propagators will be configured to use both B3 and TraceContext formats, making it possible for context to be propagated\n\nfrom the shopper through to the legacy inventory.\n\nIn a separate terminal window, with the legacy inventory service still running, run the following to\n\nstart the grocery store: $ OTEL_RESOURCE_ATTRIBUTES=\"service.name=grocery-store,\n\nservice.version=0.1.2,\n\nnet.host.name='hostname',\n\nnet.host.ip='ipconfig getifaddr en0'\" \\\n\nOTEL_TRACES_EXPORTER=console \\\n\nOTEL_PYTHON_TRACER_PROVIDER=sdk \\\n\nOTEL_METRICS_EXPORTER=console \\\n\nOTEL_PYTHON_METER_PROVIDER=sdk \\\n\nOTEL_LOGS_EXPORTER=console \\\n\nOTEL_PYTHON_LOG_EMITTER_PROVIDER=sdk \\\n\nOTEL_PROPAGATORS=b3,tracecontext \\\n\nopentelemetry-instrument python grocery_store.py\n\nThe grocery store is up and running. Now we just need to generate some requests via the shopper\n\nservice.\n\nShopper\n\nFinally, the shopper application initiates the request through the system. The RequestsInstrumentor\n\ninstruments web requests to the grocery store. Of course, the backend requests don't tell the whole\n\nstory about what goes on inside the shopper application.\n\nAs discussed in Chapter 3, Auto-Instrumentation, auto-instrumentation can be pretty valuable. In rare\n\ncases, it can even be enough to cover most of the functionality within an application. Applications\n\nfocused on Create, Read, Update, and Delete operations (https://en.wikipedia.org/wiki/CRUD) may",
      "content_length": 1758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "not contain enough business logic to warrant manual instrumentation. Operators of applications\n\nrelying heavily on instrumented libraries may also gain enough visibility from auto-instrumentation.\n\nHowever, you'll want to add additional details about your code in most scenarios. For those cases, it's\n\ncrucial to combine auto-instrumentation with manual instrumentation. Such is the case for the last\n\napplication in our system. The following code shows us the simplified version of the shopper service.\n\nAs you can see from the code, there is still manual instrumentation code, but no configuration to be\n\nseen, as this is all managed by auto-instrumentation. Additionally, you'll note that the get call from\n\nthe requests module no longer requires manual instrumentation: shopper.py\n\n#!/usr/bin/env python3\n\nimport logging\n\nimport requests\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.sdk._logs import OTLPHandler\n\ntracer = trace.get_tracer(\"shopper\", \"0.1.2\") logger = logging.getLogger(\"shopper\")\n\nlogger.setLevel(logging.DEBUG)\n\nlogger.addHandler(OTLPHandler())\n\n@tracer.start_as_current_span(\"add item to cart\")\n\ndef add_item_to_cart(item, quantity):\n\nspan = trace.get_current_span()\n\nspan.set_attributes(\n\n{\n\n\"item\": item,\n\n\"quantity\": quantity,\n\n}\n\n)\n\nlogger.info(\"add {} to cart\".format(item))\n\n@tracer.start_as_current_span(\"browse\")\n\ndef browse():\n\nresp = requests.get(\"http://localhost:5000/products\") add_item_to_cart(\"orange\", 5)\n\n@tracer.start_as_current_span(\"visit store\")\n\ndef visit_store():\n\nbrowse()\n\nif __name__ == \"__main__\":\n\nvisit_store()",
      "content_length": 1571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "It's time to generate some telemetry! Open a third terminal and launch the shopper application with\n\nthe following command: $ OTEL_RESOURCE_ATTRIBUTES=\"service.name=shopper,\n\nservice.version=0.1.3,\n\nnet.host.name='hostname',\n\nnet.host.ip='ipconfig getifaddr en0'\" \\\n\nOTEL_TRACES_EXPORTER=console \\\n\nOTEL_PYTHON_TRACER_PROVIDER=sdk \\\n\nOTEL_METRICS_EXPORTER=console \\\n\nOTEL_PYTHON_METER_PROVIDER=sdk \\\n\nOTEL_LOGS_EXPORTER=console \\\n\nOTEL_PYTHON_LOG_EMITTER_PROVIDER=sdk \\\n\nopentelemetry-instrument python shopper.py\n\nThis command should have generated telemetry from all three applications visible in the individual\n\nterminal windows.\n\nIMPORTANT NOTE\n\nSince the metrics and logging signals are under active development, the instrumentation libraries we use in this chapter\n\nonly support tracing. Therefore, we will focus on the tracing data being emitted for the time being. It's possible that by the\n\ntime you're reading this, those libraries also emit logs and metrics.\n\nWe will not go through it in detail since the tracing data being emitted is similar to the data we've\n\nalready inspected for the grocery store. Looking through the distributed trace generated, we can see\n\nthe following:\n\nSpans generated for each application; the service.name and service.version resource attributes should reflect this.\n\nThe trace ID has been propagated correctly across application boundaries. Check the trace_id field across all three terminals to\n\nconfirm.\n\nThe Requests and Flask instrumentation libraries have automatically populated attributes.\n\nThe following diagram offers a visualization of the spans generated across the system. Spans are\n\nidentified as having been automatically generated (A) or manually generated (M).",
      "content_length": 1718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Figure 7.9 – Tracing information generated\n\nThis is one of the most exciting aspects of OpenTelemetry. We have telemetry generated by two\n\napplications that contain no instrumentation code. The developers of those applications don't need to\n\nlearn about OpenTelemetry for their applications to produce information about their service, which\n\ncan be helpful to diagnose issues in the future. Getting started has never been easier. Let's take a\n\nquick look at how the Flask instrumentation works.\n\nFlask library instrumentor\n\nLike the Requests library, the Flask instrumentation library contains an implementation of the\n\nBaseInstrumentor interface. The code is available in the OpenTelemetry Python contrib repository at\n\nhttps://github.com/open-telemetry/opentelemetry-python-\n\ncontrib/blob/main/instrumentation/opentelemetry-instrumentation-",
      "content_length": 842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "flask/src/opentelemetry/instrumentation/flask/__init__.py. The implementation leverages a few\n\ndifferent aspects of the Flask library to achieve instrumentation. It wraps the original Flask app and\n\nregisters a callback via the before_request method. It then provides a middleware to execute\n\ninstrumentation code at response time. This allows the instrumentation to capture the beginning and\n\nthe end of requests through the library.\n\nAdditional configuration options\n\nThe following options are available to configure FlaskInstrumentor further:\n\nexcluded_urls: Supports a comma-separated list of regular expressions for excluding specific URLs from producing\n\ntelemetry. This option is also configurable with auto-instrumentation via the OTEL_PYTHON_FLASK_EXCLUDED_URLS\n\nenvironment variable.\n\nrequest_hook: A method to be executed before every Request received by the Flask application.\n\nresponse_hook: Similar to the request_hook argument, the response_hook allows a user to configure a method to be\n\nperformed before a response is returned to the caller.\n\nIMPORTANT NOTE\n\nWhen using the Flask instrumentation library with auto-instrumentation, it's essential to know that the debug mode\n\nmay cause issues. By default, the debug mode uses a reloader, which causes the auto-instrumentation to fail. For\n\nmore information on disabling the reloader, see the OpenTelemetry Python documentation: https://opentelemetry-\n\npython.readthedocs.io/en/latest/examples/auto-instrumentation/README.html#instrumentation-while-debugging.\n\nThe Requests and Flask instrumentation libraries are just two of many instrumentation libraries\n\navailable for Python developers.\n\nFinding instrumentation libraries\n\nA challenge with instrumentation libraries is keeping track of which libraries are available across\n\ndifferent languages. The libraries available for Python currently live in the opentelemetry-collector-\n\ncontrib repository (https://github.com/open-telemetry/opentelemetry-python-contrib), but that may\n\nnot always be the case.\n\nOpenTelemetry registry\n\nThe official OpenTelemetry website provides a searchable registry (https://opentelemetry.io/registry/)\n\nthat includes packages across languages. This information for this registry is stored in a GitHub\n\nrepository, which can be updated via pull Requests.",
      "content_length": 2299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "opentelemetry-bootstrap\n\nTo make getting started even more accessible, the OpenTelemetry Python community maintains the\n\nopentelemetry-bootstrap tool, installed via the opentelemetry-instrumentation package. This tool\n\nlooks at all installed packages in an environment and lists instrumentation libraries for that\n\nenvironment. It's possible to use the command also to install instrumentation libraries. The following\n\ncommand shows us how to use opentelemetry-bootstrap to list packages: $ opentelemetry-bootstrap\n\nopentelemetry-instrumentation-logging==0.28b0\n\nopentelemetry-instrumentation-urllib==0.28b0\n\nopentelemetry-instrumentation-wsgi==0.28b0\n\nopentelemetry-instrumentation-flask==0.28b0\n\nopentelemetry-instrumentation-jinja2==0.28b0\n\nopentelemetry-instrumentation-requests==0.28b0\n\nopentelemetry-instrumentation-urllib3==0.28b0\n\nLooking through that list, there are a few additional packages that we may want to install now that we\n\nknow about them. Conveniently, the -a install option installs all the listed packages.\n\nSummary\n\nInstrumentation libraries for third-party libraries are an excellent way for users to use OpenTelemetry\n\nwith little to no effort. Additionally, instrumentation libraries don't require users to wait for third-\n\nparty libraries to support OpenTelemetry directly. This helps reduce the burden on the maintainers of\n\nthose third-party libraries by not asking them to support APIs, which are still evolving.\n\nThis chapter allowed us to understand how auto-instrumentation leverages instrumentation libraries\n\nto simplify the user experience of adopting OpenTelemetry. By inspecting all the components that\n\ncombine to make it possible to simplify the code needed to configure telemetry pipelines, we were\n\nable to produce telemetry with little to no instrumentation code.\n\nRevisiting the grocery store then allowed us to compare the telemetry generated by auto-instrumented\n\ncode with manual instrumentation. Along the way, we took a closer look at how different\n\ninstrumentations are implemented and their configurable options.\n\nAlthough instrumentation libraries make it possible for users to start using OpenTelemetry today,\n\nthey require the installation of another library within environments, taking on additional\n\ndependencies. As instrumentation libraries have only just started maturing, this may cause users to\n\nhesitate to adopt them. Ideally, as OpenTelemetry adoption increases and its API reaches stability\n\nacross signals, third-party library maintainers will start instrumenting the libraries themselves with",
      "content_length": 2560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "OpenTelemetry, removing the need for an additional library. This has already begun with some\n\nframeworks, such as Spring in Java and .NET Core libraries.\n\nWith the knowledge of OpenTelemetry signals, instrumentation libraries, and auto-instrumentation in\n\nour toolbelt, we will now focus on what to do with the telemetry data we're producing. The following\n\nfew chapters will focus on collecting, transmitting, and analyzing OpenTelemetry data. First, all this\n\ndata must go somewhere, and the OpenTelemetry Collector is a perfect destination. This will be the\n\ntopic of the next chapter.",
      "content_length": 588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Section 3: Using Telemetry Data\n\nIn this part, you will learn how to deploy the OpenTelemetry Collector in conjunction with various\n\nbackends to visualize the telemetry data as well as identify issues with their cloud-native\n\napplications.\n\nThis part of the book comprises the following chapters:\n\nChapter 8, OpenTelemetry Collector\n\nChapter 9, Deploying the Collector\n\nChapter 10, Configuring Backends\n\nChapter 11, Diagnosing Problems\n\nChapter 12, Sampling",
      "content_length": 457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "Chapter 8: OpenTelemetry Collector\n\nSo, now that we've learned how to use OpenTelemetry to generate traces, metrics, and logs, we want\n\nto do something with all this telemetry data. To make the most of this data, we will need to be able to\n\nstore and visualize it because, let's be honest – reading telemetry data from the console isn't going to\n\ncut it. As we'll discuss in Chapter 10, Configuring Backends, many destinations can be used for\n\ntelemetry data. To send telemetry to a backend, the telemetry pipeline for metrics, traces, and logs\n\nneeds to be configured to use an exporter that's specific to that signal and the backend. For example,\n\nif you wanted to send traces to Zipkin, metrics to Prometheus, and logs to Elasticsearch, each would\n\nneed to be configured in the appropriate application code. Configuring this across dozens of services\n\nwritten in different languages adds to the complexity of managing the code. But now, imagine\n\ndeciding that one of the backends must be changed because it no longer suits the needs of your\n\nbusiness. Although it may not seem like a lot of work on a small scale, in a distributed system with\n\napplications that have been produced over many years by various engineers, the amount of effort to\n\nupdate, test, and deploy all that code could be quite significant, not to mention risky.\n\nWouldn't it be great if there were a way to configure an exporter once, and then use only\n\nconfiguration files to modify the destination of the data? There is – it's called OpenTelemetry\n\nCollector and this is what we'll be exploring in this chapter.\n\nIn this chapter, we will cover the following topics:\n\nThe purpose of OpenTelemetry Collector\n\nUnderstanding the components of OpenTelemetry Collector\n\nTransporting telemetry via OTLP\n\nUsing OpenTelemetry Collector\n\nLet's start by ensuring we have all the tools in place to work with the collector.\n\nTechnical requirements\n\nThis chapter will introduce OpenTelemetry Collector as a standalone binary, which can be\n\ndownloaded from https://github.com/open-telemetry/opentelemetry-collector-\n\nreleases/releases/tag/v0.43.0. It's also possible to build the collector from the source, but this will not\n\nbe covered in this chapter. The following commands will download the binary that's been compiled\n\nfor macOS on Intel processors, extract the otelcol file, and ensure the binary can be executed: $\n\nwget -O otelcol.tar.gz https://github.com/open-telemetry/opentelemetry-collector-",
      "content_length": 2465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "releases/releases/download/v0.43.0/otelcol_0.43.0_darwin_amd64.tar.gz $ tar -xzf otelcol.tar.gz\n\notelcol\n\n$ chmod +x ./otelcol\n\n$ ./otelcol --version\n\notelcol version 0.43.0\n\nWith the correct binary downloaded, let's ensure that the collector can start by using the following\n\ncommand. It is expected that the process will exit: $ ./otelcol\n\nError: failed to get config: invalid configuration: no enabled receivers specified in\n\nconfig 2022/02/13 11:52:47 collector server run finished with error: failed to get config:\n\ninvalid configuration: no enabled receivers specified in config Important Note\n\nThe OpenTelemetry Collector project produces a different binary for various operating systems (Windows, Linux, and\n\nmacOS) and architectures. You must download the correct one for your environment.\n\nThe configuration for the collector is written in YAML format (https://en.wikipedia.org/wiki/YAML),\n\nbut we'll try to steer clear of most of the traps of YAML by providing complete configuration\n\nexamples. The collector is written in Go, so this chapter includes code snippets in Go. Each piece of\n\ncode will be thoroughly explained, but don't worry if the details of the language escape you – the\n\nconcept of the code is what we'll be focusing on. To send data to OpenTelemetry Collector from\n\nPython applications, we'll need to install the OTLP exporter, which can be done via pip: $ pip install\n\nopentelemetry-exporter-otlp \\\n\nopentelemetry-propagator-b3 \\\n\nopentelemetry-instrumentation-wsgi \\\n\nflask \\\n\nrequests\n\nIMPORTANT NOTE\n\nThe opentelemetry-exporter-otlp package itself does not contain any exporter code. It uses dependencies to pull\n\nin a different package for each different encoding and transport option that's supported by OTLP We will discuss these\n\nlater in this chapter.\n\nThe completed code and configuration for this chapter is available in this book's GitHub repository in\n\nthe chapter08 directory: $ git clone https://github.com/PacktPublishing/Cloud-Native-Observability $\n\ncd Cloud-Native-Observability/chapter08\n\nAs with the previous chapters, the code in these examples builds on top of the previous chapters. If\n\nyou'd like to follow along with the code changes, copy the code from the chapter06 folder. Now, let's\n\ndive in and figure out what this collector is all about, and why you should care about it.",
      "content_length": 2333,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "The purpose of OpenTelemetry Collector\n\nIn essence, OpenTelemetry Collector is a process that receives telemetry in various formats,\n\nprocesses it, and then exports it to one or more destinations. The collector acts as a broker between\n\nthe source of the telemetry, applications, or nodes, for example, and the backend that will ultimately\n\nstore the data for analysis. The following diagram shows where the collector would be deployed in an\n\nenvironment containing various components:\n\nFigure 8.1 – Architecture diagram of an environment with a collector Deploying a component such as OpenTelemetry\n\nCollector is not free as it requires additional resources to be spent on running, operating, and monitoring it. The\n\nfollowing are some reasons why deploying a collector may be helpful:\n\nYou can decouple the source of the telemetry data from its destination. This means that developers can configure a single\n\ndestination for the telemetry data in application code and allow the operators of the collector to determine where that data will go\n\nas needed, without having to modify the existing code.\n\nYou can provide a single destination for many data types. The collector can be configured to receive traces, metrics, and logs in\n\nmany different formats, such as OTLP Jaeger, Zipkin, Prometheus, StatsD, and many more.\n\nYou can reduce latency when sending data to a backend. This mitigates unexpected side effects from occurring when an event\n\ncauses a backend to be unresponsive. A collector deployment can also be horizontally scaled to increase capacity as required.\n\nYou can modify telemetry data to address compliance and security concerns. Data can be filtered by the collector via processors\n\nbased on the criteria defined in the configuration. Doing so can stop data leakage and prevent information that shouldn't be\n\nincluded in the telemetry data from ever being stored in a backend.",
      "content_length": 1894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "We will discuss deployment scenarios for the collector in Chapter 9, Deploying the Collector. For\n\nnow, let's focus on the architecture and components that provide the functionality of the collector.\n\nUnderstanding the components of OpenTelemetry Collector\n\nThe collector allows users to configure pipelines for each signal separately by combining any number\n\nof receivers, processors, and exporters as shown in the following diagram. This gives the collector a\n\nlot of flexibility in how and where it can be used:\n\nFigure 8.2 – Dataflow through the collector\n\nThe initial implementation of the collector was a fork of the OpenCensus Service\n\n(https://opencensus.io/service/), which served a similar purpose in the OpenCensus ecosystem. The\n\ncollector supports many open protocols out of the box for inputs and outputs, which we'll explore in\n\nmore detail as we take a closer look at each component. Each component in the collector implements\n\nthe Component interface, which is fairly minimal, as shown in the following code: type Component\n\ninterface {\n\nStart(ctx context.Context, host Host) error\n\nShutdown(ctx context.Context) error\n\n}\n\nThis interface makes it easy for implementors to add additional components to the collector, making\n\nit very extensible. Let's look at each component in more detail.\n\nReceivers\n\nThe first component in a pipeline is the receiver, a component that receives data in various supported\n\nformats and converts this data into an internal data format within the collector. Typically, a receiver",
      "content_length": 1525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "registers a listener that exposes a port in the collector for the protocols it supports. For example, the\n\nJaeger receiver supports the following protocols:\n\nThrift Binary on port 6832\n\nThrift Compact on port 6831\n\nThrift HTTP on port 14268\n\ngRPC on port 14250\n\nIMPORTANT NOTE\n\nDefault port values can be overridden via configuration, as we'll see later in this chapter.\n\nIt's possible to enable multiple protocols for the same receiver so that each of the protocols listed\n\npreviously will listen on different ports by default. The following table shows the supported receiver\n\nformats for each signal type:\n\nFigure 8.3 – Receiver formats per signal\n\nNote that all the receivers shown here are receivers that support data in a specific format. However,\n\nan exception is the host metrics receiver, which will be discussed later in this chapter. Receivers can\n\nbe reused across multiple pipelines and it's possible to configure multiple receivers for the same",
      "content_length": 958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "pipeline. The following configuration example enables the OTLP gRPC receiver and the Jaeger Thrift\n\nBinary receiver. Then, it configures three separate pipelines named traces/otlp, traces/jaeger, and\n\ntraces/both, which use those receivers: receivers:\n\notlp:\n\nprotocols:\n\ngrpc:\n\njaeger:\n\nprotocols:\n\nthrift_binary:\n\nservice:\n\npipelines:\n\ntraces/otlp:\n\nreceivers: [otlp]\n\ntraces/jaeger:\n\nreceivers: [jaeger]\n\ntraces/both:\n\nreceivers: [otlp, jaeger]\n\nOne scenario where it would be beneficial to create separate pipelines for different receivers is if\n\nadditional processing needs to occur on the data from one pipeline but not the other. As with the\n\ncomponent interface, the interface for receivers is kept minimal, as shown in the following code. The\n\nTracesReceiver, MetricsReceiver, and LogsReceiver receivers all embed the same Receiver\n\ninterface, which embeds the Component interface we saw previously: type Receiver interface {\n\nComponent\n\n}\n\ntype TracesReceiver interface {\n\nReceiver\n\n}\n\nThe simplicity of the interface makes it easy to implement additional receivers as needed. As we\n\nmentioned previously, the main task of a receiver is to translate data that's being received into various\n\nformats, but what about the host metrics receiver? Host metrics receiver\n\nThe host metrics receiver can be configured to collect metrics about the host running the collector. It\n\ncan be configured to scrape metrics for the CPU, disk, memory, and various other system-level",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "details. The following example shows how the hostmetrics receiver can be configured to scrape\n\nload, memory, and network information from a host every 10 seconds (10s): receivers:\n\nhostmetrics:\n\ncollection_interval: 10s\n\nscrapers:\n\nload:\n\nmemory:\n\nnetwork:\n\nservice:\n\npipelines:\n\nmetrics:\n\nreceivers: [hostmetrics]\n\nThe receiver supports additional configuration so that you can include or exclude specific devices or\n\nmetrics. Configuring this receiver can help you monitor the performance of the host without running\n\nadditional processes to do so. Once the telemetry data has been received through a receiver, it can be\n\nprocessed further via processors.\n\nProcessors\n\nIt can be beneficial to perform some additional tasks, such as filtering unwanted telemetry or\n\ninjecting additional attributes, on the data before passing it to the exporter. This is the job of the\n\nprocessor. Unlike receivers and exporters, the capabilities of processors vary significantly from one\n\nprocessor to another. It's also worth noting that the order of the components in the configuration\n\nmatters for processors, as the data is passed serially from one processor to another. In addition to\n\nembedding the component interface, the processor interface also embeds a consumer interface that\n\nmatches the signal that's being processed, as shown in the following code snippet. The purpose of the\n\nconsumer interface is to provide a function that consumes the signal, such as ConsumeMetrics. It also\n\nprovides information about whether the processor will modify the data it processes via the\n\nMutatesData capability: type Capabilities struct {\n\nMutatesData bool\n\n}\n\ntype baseConsumer interface {\n\nCapabilities() Capabilities\n\n}",
      "content_length": 1706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "type Metrics interface {\n\nbaseConsumer\n\nConsumeMetrics(ctx context.Context, md pdata.Metrics) error }\n\ntype MetricsProcessor interface {\n\nProcessor\n\nconsumer.Metrics\n\n}\n\nThe following example configures an attributes processor called attributes/add-key to insert an\n\nattribute with the example-key key and sets its value to first. The second attributes processor,\n\nattributes/update-key, updates the value of the example-key attribute to the second value. The traces\n\npipeline is then configured to add the attribute and update its value: processors:\n\nattributes/add-key:\n\nactions:\n\nkey: example-key\n\naction: insert\n\nvalue: first\n\nattributes/update-key:\n\nactions:\n\nkey: example-key\n\naction: update\n\nvalue: second\n\nservice:\n\npipelines:\n\ntraces:\n\nprocessors: [attributes/add-key, attributes/update-key]\n\nThe output that's expected from this configuration is that all the spans that are emitted have an\n\nexample-key attribute set to a value of second. Since the order of the processors matters, inverting the\n\nprocessors in the preceding example would set the value to first. The previous example is a bit silly\n\nsince it doesn't make a lot of sense to configure multiple attributes processors in that manner, but it\n\nillustrates that ordering the processors matters. Let's see what a more realistic example may look like.\n\nThe following configuration copies a value from one attribute with the old-key key into another one\n\nwith the new-key key before deleting the old-key attribute: processors:\n\nattributes/copy-and-delete:\n\nactions:",
      "content_length": 1532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "key: new-key\n\naction: upsert\n\nfrom_attribute: old-key\n\nkey: old-key\n\naction: delete\n\nservice:\n\npipelines:\n\ntraces:\n\nprocessors: [attributes/copy-and-delete]\n\nA configuration like the previous one could be used to migrate values or consolidate data coming in\n\nfrom multiple systems, where different names are used to represent the same data. As we mentioned\n\nearlier, processors cover a range of functionality. The following table lists the current processors, as\n\nwell as the signals they process:\n\nFigure 8.4 – Processors per signal",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "Some of these processors will be familiar to you if you've already used an OpenTelemetry SDK. It's\n\nworth taking a moment to explore these processors further.\n\nAttributes processor\n\nAs we discussed earlier, the attributes processor can be used to modify telemetry data attributes. It\n\nsupports the following operations:\n\ndelete: This deletes an attribute for a specified key.\n\nextract: This uses a regular expression to extract values from the specified attribute and upsert new attributes resulting from\n\nthat extraction.\n\nhash: This computes a SHA-1 hash of the value for an existing attribute and updates the value of that attribute to the computed\n\nhash.\n\ninsert: This inserts an attribute for a specified key when it does not exist. It does nothing if the attribute exists.\n\nupdate: This updates an existing attribute with a specified value. It does nothing if the attribute does not exist.\n\nupsert: This combines the functionality of insert and update. If an attribute does not exist, it will insert it with the\n\nspecified value; otherwise, it will update the attribute with the value.\n\nThe attributes processor, along with the span processor, which we'll see shortly, allows you to include\n\nor exclude spans based on match_type, which can either be an exact match configured as strict or a\n\nregular expression configured with regexp. The matching is applied to one or more of the configured\n\nfields: services, span_names, or attributes. The following example includes spans for the super-\n\nsecret and secret services: processors:\n\nattributes/include-secret:\n\ninclude:\n\nmatch_type: strict\n\nservices: [\"super-secret\", \"secret\"]\n\nactions:\n\nkey: secret-attr\n\naction: delete\n\nThe attributes processor can be quite useful when you're scrubbing personally identifiable\n\ninformation (PII) or other sensitive information. A common way sensitive information makes its\n\nway into telemetry data is via debug logs that capture private variables it shouldn't have, or by user\n\ninformation, passwords, or private keys being recorded in metadata. Data leaks often happen\n\naccidentally and are much more frequent than you'd think.\n\nIMPORTANT NOTE\n\nIt's possible to configure both an include and exclude rule at the same time. If that is the case, include is checked\n\nbefore exclude.",
      "content_length": 2272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Filter processor\n\nThe filter processor allows you to include or exclude telemetry data based on the configured criteria.\n\nThis processor, like the attributes and span processors, can be configured to match names with either\n\nstrict or regexp matching. It's also possible to use an expression that matches attributes as well as\n\nnames. Further scoping on the filter can be achieved by specifying resource_attributes. In terms of\n\nits implementation, at the time of writing, the filter processor only supports filtering for metrics,\n\nthough additional signal support has been requested by the community.\n\nProbabilistic sampling processor\n\nAlthough sampling is a topic that we'll cover in more detail in Chapter 12, Sampling, it's important to\n\nknow that the collector provides a sampling processor for traces known as the probabilistic\n\nsampling processor. It can be used to reduce the number of traces that are exported from the\n\ncollector by specifying a sampling percentage, which determines what percentage of traces should be\n\nkept. The hash_seed parameter is used to determine how the collector should hash the trace IDs to\n\ndetermine which traces to process: processors:\n\nprobabilistic_sampler:\n\nsampling_percentage: 20\n\nhash_seed: 12345\n\nThe hash_seed configuration parameter becomes especially important when multiple collectors are\n\nconnected. For example, imagine that a collector (A) has been configured to send its data to another\n\ncollector (B) before sending the data to a backend. With both A and B configured using the previous\n\nexample, if 100 traces are sent through the two collectors, a total of 20 of those will be sent through\n\nto the backend. If, on the other hand, the two collectors use a different hash_seed, collector A will\n\nsend 20 traces to collector B, and collector B will sample 20% of those, resulting in four traces being\n\nsent to the backend. Either case is valid, but it's important to understand the difference.\n\nIMPORTANT NOTE\n\nThe probabilistic sampling processor prioritizes the sampling priority attribute before the trace ID hashing if the attribute is\n\npresent. This attribute is defined in the semantic conventions and was originally defined in OpenTracing. More information\n\non this will be provided in Chapter 12, Sampling, but for now, it's just good to be aware of it. Resource processor\n\nThe resource processor lets users modify attributes, just like the attributes processor. However,\n\ninstead of updating attributes on individual spans, metrics, or logs, the resource processor updates the\n\nattributes of the resource associated with the telemetry data. The options that are available for\n\nconfiguring the resource processor are the same as for the attributes processor. This can be seen in the",
      "content_length": 2745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "following example, which uses upsert for the deployment.environment attribute and renames the\n\nruntime attribute to container.runtime using the insert and delete actions: processors:\n\nresource:\n\nattributes:\n\nkey: deployment.environment\n\nvalue: staging\n\naction: upsert\n\nkey: container.runtime\n\nfrom_attribute: runtime\n\naction: insert\n\nkey: runtime\n\naction: delete\n\nNow, let's discuss the span processor. Span processor\n\nIt may be useful to manipulate the names of spans or attributes of spans based on their names. This is\n\nthe job of the span processor. It can extract attributes from a span and update its name based on\n\nthose attributes. Alternatively, it can take the span's name and expand it to individual attributes\n\nassociated with the span. The following example shows how to rename a span based on the\n\nmessaging.system and messaging.operation attributes, which will be separated by the : character.\n\nThe second configuration of the span processor shows how to extract the storeId and orderId\n\nattributes from the span's name: processors:\n\nspan/rename:\n\nname:\n\nfrom_attributes: [\"messaging.system\", \"messaging.operation\"]\n\nseparator: \":\"\n\nspan/create-attributes:\n\nname:\n\nto_attributes:\n\nrules:\n\n^\\/stores\\/(?P<storeId>.*)\\/.*$\n\n^.*\\/orders/(?P<orderId>.*)\\/.*$\n\nAs we mentioned previously, the span processor also supports the include and exclude\n\nconfigurations to help you filter spans. Not all processors are used to modify the telemetry data; some",
      "content_length": 1460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "change the behavior of the collector itself.\n\nBatch processor\n\nThe batch processor helps you batch data to increase the efficiency of transmitting the data. It can be\n\nconfigured both to send batches based on batch size and a schedule. The following code configures a\n\nbatch processor to send data every 10s or every 10000 records and limits the size of the batch to 11000\n\nrecords: processors:\n\nbatch:\n\ntimeout: 10s # default 200ms\n\nsend_batch_size: 10000 # default 8192\n\nsend_batch_max_size: 11000 # default 0 – no limit\n\nIt is recommended to configure a batch processor for all the pipelines to optimize the throughput of\n\nthe collector.\n\nMemory limiter processor\n\nTo ensure the collector is conscious of resource consumption, the memory limiter processor lets\n\nusers control the amount of memory the collector consumes. This helps ensure the collector does as\n\nmuch as it can to avoid running out of memory. Limits can be specified either via fixed mebibyte\n\nvalues or percentages that are calculated based on the total available memory. If both are specified,\n\nthe fixed values take precedence. The memory limiter enforces both soft and hard limits, with the\n\ndifference defined by a spike limit configuration. It is recommended to use the ballast extension\n\nalongside the memory limiter. The ballast extension allows the collector to pre-allocate memory to\n\nimprove the stability of the heap. The recommended size for the ballast is between one-third to one-\n\nhalf of the total memory for the collector. The following code configures the memory limiter to use\n\nup to 250 Mib of the memory configured via limit_mib, with 50 Mib as the difference between the\n\nsoft and hard limits, which is configured via spike_limit_mib: processors:\n\nmemory_limiter:\n\ncheck_interval: 5s\n\nlimit_mib: 250\n\nspike_limit_mib: 50\n\nextensions:\n\nmemory_ballast:\n\nsize_mib: 125\n\nThe memory limiter processor, along with the batch processor, are both recommended if you wish to\n\noptimize the performance of the collector.",
      "content_length": 2000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "IMPORTANT NOTE\n\nWhen the processor exceeds soft limits, it returns errors and starts dropping data. If it exceeds hard limits, it will also force\n\ngarbage collection to free memory.\n\nThe memory limiter should be the first processor you configure in the pipeline. This ensures that\n\nwhen the memory threshold is exceeded, the errors that are returned are propagated to the receivers.\n\nThis allows the receivers to send appropriate error codes back to the client, who can then throttle the\n\nrequests they are sending. Now that we understand how to process our telemetry data to fit our needs,\n\nlet's learn how to use the collector to export all this data.\n\nExporters\n\nThe last component of the pipeline is the exporter. The role of the exporter in the collector pipeline is\n\nfairly similar to its role in the SDK, as we explored in previous chapters. The exporter takes the data\n\nin its internal collector format, marshals it into the output format, and sends it to one or more\n\nconfigured destinations. The interface for the exporter is very similar to the processor interface as it is\n\nalso a consumer, separated again by a signal. The following code shows us the LogsExporter\n\ninterface, which embeds the interfaces we explored earlier: type LogsExporter interface {\n\nExporter\n\nconsumer.Logs\n\n}\n\nMultiple exporters of the same type can be configured for different destinations as necessary. It's also\n\npossible to configure multiple exporters for the same pipeline to output the data to multiple locations.\n\nThe following code configures a jaeger exporter, which is used for exporting traces, and an otlp\n\nexporter, which will be used for both traces and metrics: exporters:\n\njaeger:\n\nendpoint: jaeger:14250\n\notlp:\n\nendpoint: otelcol:4317\n\nservice:\n\npipelines:\n\ntraces:\n\nexporters: [jaeger, otlp]\n\nmetrics:\n\nexporters: [otlp]",
      "content_length": 1826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Several other formats are supported by exporters. The following table lists the available exporters, as\n\nwell as the signals that each supports:\n\nFigure 8.5 – Exporters per signal\n\nNote that in addition to exporting data across different signals to destinations that can be reached over\n\na network, it's also possible to export telemetry data locally to the console via the logs exporter or as\n\nJSON to a file via the file exporter. Receivers, processors, and exporters cover the components in the\n\npipeline, but there is yet more to cover about the collector.\n\nExtensions\n\nAlthough most of the functionality of the collector revolves around the telemetry pipelines, there is\n\nadditional functionality that is made available via extensions. Extensions provide you with another\n\nway to extend the collector. The following extensions are currently available:",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "ballast: This allows users to configure a memory ballast for the collector to improve the overall stability and performance of\n\nthe collector.\n\nhealth_check: This makes an endpoint available for checking the health of the collector. This can be useful for service\n\ndiscovery or orchestration of the collector.\n\npprof: This enables the Go performance profiler, which can be used to identify performance issues within the collector.\n\nzpages: This enables an endpoint in the collector that provides debugging information about the components in the collector.\n\nThus far, all the components we've explored are part of the core collector distribution and are built\n\ninto the binary we'll be using in our examples later in this chapter. However, those are far from the\n\nonly components that are available.\n\nAdditional components\n\nAs you can imagine, providing this much functionality in an application can become quite complex.\n\nTo reduce the complexity of the collector's core functionality without impeding progress and\n\nenthusiasm in the community, the main collector repository contains components that are defined as\n\npart of the OpenTelemetry specification. With all the flexibility the collector provides, many\n\nindividuals and organizations are contributing additional receivers, processors, and exporters. These\n\ncan be found in the opentelemetry-collector-contrib repository at https://github.com/open-\n\ntelemetry/opentelemetry-collector-contrib. As the code in this repository is changing rapidly, we\n\nwon't be going over the components available there, but I strongly suggest browsing through the\n\nrepository to get an idea of what is available.\n\nBefore learning how to use the collector and configuring an application to send data to it, it's\n\nimportant to understand a little bit more about the preferred protocol to receive and export data via\n\nthe collector. This is known as OTLP.\n\nTransporting telemetry via OTLP\n\nWe've mentioned OTLP multiple times in this chapter and this book, so let's look at what it is. To\n\nensure that telemetry data is transmitted as efficiently and reliably as possible, OpenTelemetry has\n\ndefined OTLP. The protocol itself is defined via protocol buffer\n\n(https://developers.google.com/protocol-buffers) definition files. This means that any client or server\n\nthat's interested in sending or receiving OTLP only has to implement these definitions to support it.\n\nOTLP is the recommended protocol of OpenTelemetry for transmitting telemetry data and is\n\nsupported as a core component of the collector.\n\nIMPORTANT NOTE",
      "content_length": 2555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Protocol buffers or protobufs are a language and platform-agnostic mechanism for serializing data that was originally\n\nintended for gRPC. Libraries are provided to generate the code from the protobuf definition files in a variety of languages.\n\nThis is a much deeper topic than we will have time for in this book, so if you're interested in reading the protocol files, I\n\nstrongly recommended learning more about protocol buffers – they're pretty cool! The Google developer site that was\n\nlinked previously is a great resource to get started.\n\nThe definition for OTLP (https://github.com/open-telemetry/opentelemetry-proto) is divided into\n\nmultiple sections to cover the different signals. Each component of the protocol provides backward\n\ncompatibility guaranteed via its maturity level, which allows adopters to get a sense of how often\n\nthey should expect breaking changes. An alpha level makes no guarantees around breaking changes\n\nwhile a stable level guarantees backward-incompatible changes will be introduced no more\n\nfrequently than every 12 months. The maturity level of each component is available in the project's\n\nREADME.md file and the current state, at the time of writing, can be seen in the following table. It's very\n\nlikely to change by the time you're reading this as progress is being made quite rapidly!",
      "content_length": 1327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Figure 8.6 – Maturity level of OTLP components\n\nTaking a closer look at the preceding table (https://github.com/open-telemetry/opentelemetry-\n\nproto#maturity-level), note that it includes a different section for protobuf and JSON encoding. Let's\n\ntalk about why that is.\n\nEncodings and protocols\n\nThe specification defines the encodings and protocols that are supported by OTLP. Initially, the\n\nfollowing three combinations are supported:\n\nprotobufs over gRPC\n\nprotobufs over HTTP\n\nJSON over HTTP\n\nDepending on the requirements of your application or the infrastructure that will be used to deploy\n\nyour code, certain restrictions may guide the decision of which encoding or protocol to choose. For\n\nexample, users may be deploying applications in an environment that doesn't support gRPC. This was\n\ntrue for a long time with serverless Python environments in various cloud providers. Similarly, gRPC\n\nwas not supported in the browser, meaning users of OpenTelemetry for JavaScript cannot use gRPC\n\nwhen instrumenting a browser application. Another tradeoff that may cause users to choose one\n\npackage over another is the impact of serializing and deserializing data using JSON, which can have\n\nsome serious performance implications in certain languages compared to using protobufs. The\n\ndifferent combinations of encodings and protocols exist to provide additional flexibility for users,\n\ndepending on the requirements of their environments.\n\nOne of the requirements for any OpenTelemetry language implementation is to support at least one of\n\nthese formats before marking a signal as generally available. This ensures that users can use OTLP to\n\nexport data across their entire system, from application instrumentation to the backend.\n\nAdditional design considerations\n\nBackpressure can happen when clients are generating telemetry data faster than the recipients can\n\nreceive it. To address this, the specification for OTLP also defines how clients should handle\n\nresponses from servers to manage backpressure when receiving systems become overloaded. Another\n\ndesign goal of the protocol is to ensure it is load balancer-friendly so that you can horizontally scale\n\nvarious components that could be involved in handling telemetry data using OTLP. Equipped with\n\nthis knowledge of the protocol, let's start sending data to the collector.",
      "content_length": 2340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "Using OpenTelemetry Collector\n\nNow that we're familiar with the core components of OpenTelemetry Collector and OTLP, it's time to\n\nstart using the collector with the grocery store. The following diagram gives us an idea of how\n\ntelemetry data is currently configured and where we are trying to go with this chapter:\n\nFigure 8.7 – Before and after diagrams of exporting telemetry data At the beginning of this chapter, we installed the\n\nOTLP exporters for Python via the opentelemetry-exporter-otlp package. This, in turn, installed the packages\n\nthat are available for each protocol and encoding:\n\nopentelemetry-exporter-otlp-proto-grpc\n\nopentelemetry-exporter-otlp-proto-http\n\nopentelemetry-exporter-otlp-json-http\n\nThe package that includes all the protocols and the encoding is a convenient way to start, but once\n\nyou're familiar with the requirements for your environment, you'll want to choose a specific encoding\n\nand protocol to reduce dependencies.\n\nConfiguring the exporter\n\nThe following examples will leverage the otlp-proto-grpc package, which includes the exporter\n\nclasses we'll use to export telemetry – OTLPSpanExporter, OTLPMetricExporter, and OTLPLogExporter.\n\nThe code builds on the example applications from Chapter 6, Logging —Capturing Events, by\n\nupdating the common.py module to use the OTLP exporters instead of the control exporters, which\n\nwe've used so far: common.py\n\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n\nfrom opentelemetry.exporter.otlp.proto.grpc._metric_exporter import OTLPMetricExporter",
      "content_length": 1568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "from opentelemetry.exporter.otlp.proto.grpc._log_exporter import OTLPLogExporter\n\ndef configure_tracer(name, version):\n\n...\n\nexporter = OTLPSpanExporter()\n\n...\n\ndef configure_meter(name, version):\n\n...\n\nexporter = OTLPMetricExporter()\n\n...\n\ndef configure_logger(name, version):\n\n...\n\nexporter = OTLPLogExporter()\n\n...\n\nBy default, as per the specification, the exporters will be configured to send data to a collector\n\nrunning on localhost:4317.\n\nConfiguring the collector\n\nThe following collector configuration sets up the otlp receiver, which will be used to receive\n\ntelemetry data from our application. Additionally, it configures the logging exporter to output useful\n\ninformation to the console: config/collector/config.yml receivers:\n\notlp:\n\nprotocols:\n\ngrpc:\n\nexporters:\n\nlogging:\n\nservice:\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nexporters: [logging]\n\nmetrics:\n\nreceivers: [otlp]\n\nexporters: [logging]",
      "content_length": 911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "logs:\n\nreceivers: [otlp]\n\nexporters: [logging]\n\nIMPORTANT NOTE\n\nIn the following examples, each time config.yml is updated, the collector must be restarted for the changes to take\n\neffect.\n\nIt's time to see whether the collector and the application can communicate. First, start the collector\n\nusing the following command from the terminal: ./otelcol --config ./config/collector/config.yml\n\nIf everything is going according to plan, the process should be up and running, and the output from it\n\nshould list the components that have been loaded. It should also contain a message similar to the\n\nfollowing: collector output\n\n2021-05-30T16:19:03.088-0700 info service/application.go:197 Everything is ready. Begin\n\nrunning and processing data.\n\nNext, we need to run the application code in a separate terminal. First, launch the legacy inventory,\n\nfollowed by the grocery store, and then the shopper application. Note that legacy_inventory.py and\n\ngrocery_store.py will remain running for the remainder of this chapter as we will not make any\n\nfurther changes to them: python legacy_inventory.py\n\npython grocery_store.py\n\npython shopper.py\n\nPay close attention to the output from the terminal running the collector. You should see some output\n\ndescribing the traces, metrics, and logs that have been processed by the collector. The following code\n\ngives you an idea of what to look for: collector output\n\n2022-02-13T14:35:47.101-0800 INFO loggingexporter/logging_exporter.go:69 LogsExporter\n\n{\"#logs\": 1}\n\n2022-02-13T14:35:47.110-0800 INFO loggingexporter/logging_exporter.go:40 TracesExporter\n\n{\"#spans\": 4}\n\n2022-02-13T14:35:49.858-0800 INFO loggingexporter/logging_exporter.go:40 TracesExporter\n\n{\"#spans\": 1}\n\n2022-02-13T14:35:50.533-0800 INFO loggingexporter/logging_exporter.go:40 TracesExporter\n\n{\"#spans\": 3}\n\n2022-02-13T14:35:50.535-0800 INFO loggingexporter/logging_exporter.go:69 LogsExporter\n\n{\"#logs\": 2}\n\nExcellent – let's do some more fun things with the collector by adding some processors to our\n\nconfiguration! If you look closely at the preceding output, you'll notice that TracesExporter is\n\nmentioned in three separate instances. Since each of our applications is sending telemetry data, the",
      "content_length": 2209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "exporter is called with the new data. The batch processor can improve it's efficiency here by waiting a\n\nwhile and sending a single batch containing all the telemetry data simultaneously. The following code\n\nconfigures the batch processor with a timeout of 10 seconds (10s), so the processor will wait up until\n\nthat time to send a batch. Then, we can add this processor to each pipeline:\n\nconfig/collector/config.yml processors:\n\nbatch:\n\ntimeout: 10s\n\n...\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nprocessors: [batch]\n\nexporters: [logging]\n\nmetrics:\n\nreceivers: [otlp]\n\nprocessors: [batch]\n\nexporters: [logging]\n\nlogs:\n\nreceivers: [otlp]\n\nprocessors: [batch]\n\nexporters: [logging]\n\nTry running the shopper application once again. This time, the output from the collector should show\n\na single line including the sum of all the spans we saw earlier: collector output\n\n2022-02-13T14:40:07.360-0800 INFO loggingexporter/logging_exporter.go:69 LogsExporter\n\n{\"#logs\": 2}\n\n2022-02-13T14:40:07.360-0800 INFO loggingexporter/logging_exporter.go:40 TracesExporter\n\n{\"#spans\": 8}\n\nIf you run the shopper application a few times, you'll notice a 10-second delay in the collector\n\noutputting information about the telemetry data that's been generated. This is the batch processor at\n\nwork. Let's make the logging output slightly more useful by updating the logging exporter\n\nconfiguration: config/collector/config.yml exporters:\n\nlogging:\n\nloglevel: debug",
      "content_length": 1444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "Restarting the collector and running the shopper application again will output the full telemetry data\n\nthat's been received. What should appear is a verbose list of all the telemetry data the collector is\n\nreceiving. Look specifically for the span named add item to cart as we'll be modifying it in the next\n\nfew examples: collector output\n\nSpan #0\n\nTrace ID : 1592a37b7513b73eaefabde700f4ae9b\n\nParent ID : 2411c263df768eb5\n\nID : 8e6f5cdb56d6448d\n\nName : HTTP GET\n\nKind : SPAN_KIND_SERVER\n\nStart time : 2022-02-13 22:41:42.673298 +0000 UTC\n\nEnd time : 2022-02-13 22:41:42.677336 +0000 UTC\n\nStatus code : STATUS_CODE_UNSET\n\nStatus message :\n\nAttributes:\n\n> http.method: STRING(GET)\n\n> http.server_name: STRING(127.0.0.1)\n\n> http.scheme: STRING(http)\n\n> net.host.port: INT(5000)\n\n> http.host: STRING(localhost:5000)\n\n> http.target: STRING(/products)\n\n> net.peer.ip: STRING(127.0.0.1)\n\nSo far, our telemetry data is being emitted to a collector from three different applications. Now, we\n\ncan see all the telemetry data on the terminal running the collector. Let's take this a step further and\n\nmodify this telemetry data via some processors.\n\nModifying spans\n\nOne of the great features of the collector is its ability to operate on telemetry data from a central\n\nlocation. The following example demonstrates some of the power behind the processors. The\n\nfollowing configuration uses two different processors to augment the span we mentioned previously.\n\nFirst, the attributes processor will add an attribute to identify a location attribute. Next, the span\n\nprocessor will use the attributes from the span to rename the span so that it includes the location,",
      "content_length": 1657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "item, and quantity attributes. The new processors must also be added to the traces pipeline's\n\nprocessors array: config/collector/config.yml processors:\n\nattributes/add-location:\n\nactions:\n\nkey: location\n\naction: insert\n\nvalue: europe\n\nspan/rename:\n\nname:\n\nfrom_attributes: [location, item, quantity]\n\nseparator: \":\"\n\n...\n\npipelines:\n\ntraces:\n\nprocessors: [batch, attributes/add-location, span/rename]\n\nIMPORTANT NOTE\n\nRemember that the order of the processors matters. In this case, the reverse order wouldn't work as the location attribute\n\nwould not be populated.\n\nRun the shopper and look at the output from the collector to see the effect of the new processors. The\n\nnew exported span contains a location attribute with the europe value, which we configured. Its\n\nname has also been updated to location:item:quantity: collector output\n\nSpan #1\n\nTrace ID : 47dac26efa8de0ca1e202b6d64fd319c\n\nParent ID : ee10984575037d4a\n\nID : a4f42124645c4d3b\n\nName : europe:orange:5\n\nKind : SPAN_KIND_INTERNAL\n\nStart time : 2022-02-13 22:44:57.072143 +0000 UTC\n\nEnd time : 2022-02-13 22:44:57.07751 +0000 UTC Status code : STATUS_CODE_UNSET\n\nStatus message :\n\nAttributes:\n\n> item: STRING(orange)",
      "content_length": 1183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "> quantity: INT(5)\n\n> location: STRING(europe)\n\nThis isn't bad for 10 lines of configuration! The final example will explore the hostmetrics receiver\n\nand how to configure the filter processor for metrics.\n\nFiltering metrics\n\nSo far, we've looked at how to modify spans, but what about metrics? As we discussed previously, the\n\nhostmetrics receiver captures metrics about the localhost. Let's see it in action. The following\n\nexample configures the host metrics receiver to scrape memory and network information every 10\n\nseconds: config/collector/config.yml receivers:\n\nhostmetrics:\n\ncollection_intervals: 10s\n\nscrapers:\n\nmemory:\n\nnetwork:\n\n...\n\nservice:\n\npipelines:\n\nmetrics:\n\nreceivers: [otlp, hostmetrics]\n\nAfter configuring this receiver, just restart the collector – you should see metrics in the collector\n\noutput, without running shopper.py. The output will include memory and network metrics: collector\n\noutput\n\nInstrumentationLibraryMetrics #0\n\nInstrumentationLibrary\n\nMetric #0\n\nDescriptor:\n\n> Name: system.memory.usage\n\n> Description: Bytes of memory in use.\n\n> Unit: By\n\n> DataType: IntSum\n\n> IsMonotonic: false\n\n> AggregationTemporality: AGGREGATION_TEMPORALITY_CUMULATIVE",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "IntDataPoints #0\n\nData point labels:\n\n> state: used\n\nStartTimestamp: 1970-01-01 00:00:00 +0000 UTC\n\nTimestamp: 2022-02-13 22:48:16.999087 +0000 UTC\n\nValue: 10880851968\n\nMetric #1\n\nDescriptor:\n\n> Name: system.network.packets\n\n> Description: The number of packets transferred.\n\n> Unit: {packets}\n\n> DataType: IntSum\n\n> IsMonotonic: true\n\n> AggregationTemporality: AGGREGATION_TEMPORALITY_CUMULATIVE\n\nIntDataPoints #0\n\nData point labels:\n\n> device: lo0\n\n> direction: transmit\n\nStartTimestamp: 1970-01-01 00:00:00 +0000 UTC\n\nTimestamp: 2022-02-13 22:48:16.999087 +0000 UTC\n\nValue: 120456\n\nWell done – the collector is now generating metrics for you! Depending on the type of system you're\n\nrunning the collector on, you may have many network interfaces available that are generating a lot of\n\nmetrics. Let's update the configuration to scrape metrics for a single interface to reduce some of the\n\nnoise. On my host, I will use lo0 as the interface: config/collector/config.yml receivers:\n\nhostmetrics:\n\ncollection_intervals: 10s\n\nscrapers:\n\nmemory:\n\nnetwork:\n\ninclude:\n\nmatch_type: strict\n\ninterfaces: [lo0]",
      "content_length": 1103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "IMPORTANT NOTE\n\nNetwork interface names vary based on the operating system being used. Some common interface names are lo0, eth0,\n\nen0, and wlan0. If you're unsure, look for the device label in the previous output, which should show you some of the\n\ninterfaces that are available on your system.\n\nThe output will be significantly reduced, but there are still many network metrics to sift through.\n\nsystem.network.connections is quite noisy as it collects data points for each tcp state. Let's take this\n\none step further and use the filter processor to exclude system.network.connections:\n\nconfig/collector/config.yml processors:\n\nfilter/network-connections:\n\nmetrics:\n\nexclude:\n\nmatch_type: strict\n\nmetric_names:\n\nsystem.network.connections\n\n...\n\npipelines:\n\nmetrics:\n\nreceivers: [hostmetrics]\n\nprocessors: [batch, filter/network-connections]\n\nRestarting the collector one last time will yield a much easier-to-read output. Of course, there are\n\nmany more scenarios to experiment with when it comes to the collector and its components, but this\n\ngives you a good idea of how to get started. I recommend spending some time experimenting with\n\ndifferent configurations and processors to get comfortable with it. And with that, we now have an\n\nunderstanding of one of the most critical components of OpenTelemetry – the collector.\n\nSummary\n\nIn this chapter, you learned about the fundamentals of OpenTelemetry Collector and its components.\n\nYou now know what role receivers, processors, exporters, and extensions play in the collector and\n\nknow about the specifics of individual processors.\n\nAdditionally, we looked at the definition of the OTLP, its benefits, and the design decisions behind\n\ncreating the protocol. Equipped with this knowledge, we configured OpenTelemetry Collector for the\n\nfirst time and updated the grocery store to emit data to it. Using a variety of processors, we\n\nmanipulated the data the collector was receiving to get a working understanding of how to harness\n\nthe power of the collector.",
      "content_length": 2014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "The next chapter will expand on this knowledge and take the collector from a component that's used\n\nin development to a core component of your infrastructure. We'll explore how to deploy the collector\n\nin a variety of scenarios to make the most of it.",
      "content_length": 251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "Chapter 9: Deploying the Collector\n\nNow that we've learned about the ins and outs of the collector, it's time to look at how we can use it\n\nin production. This chapter will explain how the flexibility of the collector can help us to deploy it in\n\na variety of scenarios. Using Docker, Kubernetes, and Helm, we will learn how to use the\n\nOpenTelemetry collector in combination with the grocery store application from earlier chapters.\n\nThis will give us the necessary knowledge to start using the collector in our cloud-native\n\nenvironment.\n\nIn this chapter, we will focus on the following main topics:\n\nUsing the collector as a sidecar to collect application telemetry\n\nDeploying the collector as an agent to collect system-level telemetry\n\nConfiguring the collector as a gateway\n\nAlong the way, we'll look at some strategies for scaling the collector. Additionally, we'll spend some\n\nmore time with the processors that we looked at in Chapter 8, OpenTelemetry Collector. Unlike the\n\nprevious chapters, which focused on OpenTelemetry components, this chapter is all about using\n\nthem. As such, it will introduce a number of tools that you might encounter when working with\n\ncloud-native infrastructure.\n\nTechnical requirements\n\nThis chapter will cover a few different tools that we can use to deploy the collector. We will be using\n\ncontainers to run the sample application and collector; all the examples are available from the public\n\nDocker container registry (https://hub.docker.com). Although we won't dive too deeply into what\n\ncontainers are, just know that containers provide a convenient way to build, package, and deploy self-\n\ncontained applications that are immutable. For us to run containers locally, we will use Docker, just\n\nas we did in Chapter 2, OpenTelemetry Signals - Traces, Metrics and Logs. The following is a list of\n\nthe technical requirements for this chapter:\n\nIf you don't already have Docker installed on your machine, follow the instructions available at https://docs.docker.com/get-\n\ndocker/ to get started on Windows, macOS, and Linux. Once you have it installed, run the following command from a Terminal. If\n\neverything is working correctly, there should be no errors reported: $ docker ps\n\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n\nShortly, we will be required to run a command using the kubectl Kubernetes command-line tool. This tool interacts with the\n\nKubernetes API, which we'll do continuously throughout the chapter to access our applications once they're running in the cluster.\n\nDepending on your environment, you might already have a copy of this tool installed. Check whether that is the case by running\n\nthe following command: $ kubectl version",
      "content_length": 2705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\"...\n\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.7\"...\n\nIf the output from running the previous command shows command not found, go through the\n\ninstallation steps documented on the Kubernetes website at https://kubernetes.io/docs/tasks/tools/.\n\nIn addition to Docker, we will also use Kubernetes (https://kubernetes.io) throughout this chapter. This is because it is one of the\n\nleading open source tools used in cloud-native infrastructure. Kubernetes will provide the container orchestration for our\n\nexamples and the collector. It's worth noting that Kubernetes is not the only container orchestration solution that is available;\n\nhowever, it is one of the more popular ones. There are many different tools available to set up a local Kubernetes cluster. For\n\ninstance, I'll use kind to set up my cluster, which runs a local cluster inside Docker. If you already have access to a cluster, then\n\ngreat! You're good to go. Otherwise, head over to https://kind.sigs.k8s.io/docs/user/quick-start/ and follow the installation\n\ninstructions for your platform. Once kind is installed, run the following command to start a cluster: $ kind create cluster\n\nCreating cluster \"kind\" ...\n\n✓ Ensuring node image (kindest/node:v1.19.1)\n\n✓ Preparing nodes\n\n✓ Writing configuration\n\n✓ Starting control-plane\n\n✓ Installing CNI\n\n✓ Installing StorageClass\n\nSet kubectl context to \"kind-kind\"\n\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nThe previous command should get the cluster started for you. Getting a cluster up and running is\n\ncrucial to use the examples in the rest of this chapter. If you're running into issues while setting up a\n\nlocal cluster with kind, you might want to investigate one of the following alternatives: A. Minikube:\n\nhttps://minikube.sigs.k8s.io/docs/start/\n\nB. K3s: https://k3s.io\n\nC. Docker Desktop: https://docs.docker.com/desktop/kubernetes/",
      "content_length": 1990,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "How the cluster is run isn't going to be important; having a cluster is what really matters.\n\nAdditionally, if running a local cluster isn't feasible, you might want to look at some hosted options:\n\nA. Google Kubernetes Engine: https://cloud.google.com/kubernetes-engine\n\nB. Amazon Elastic Kubernetes Service: https://aws.amazon.com/eks/\n\nC. Azure Kubernetes Service: https://azure.microsoft.com/en-us/services/kubernetes-service/\n\nYou should know that there are always costs associated with using a hosted Kubernetes cluster.\n\nNow, check the state of the cluster using kubectl, which we installed earlier. Run the following command to check whether the\n\ncluster is ready: $kubectl cluster-info --context kind-kind\n\nKubernetes master is running at https://127.0.0.1:62708\n\nKubeDNS is running at https://127.0.0.1:62708/api/v1/namespaces/kube-\n\nsystem/services/kube-dns:dns/proxy\n\nGood job at getting this far! I know there are a lot of tools to install, but it'll be worth it! The last tool that we'll use throughout this\n\nchapter is Helm. This is a package manager for applications running in Kubernetes. Helm will allow us to install applications in\n\nour cluster by using the YAML configuration it calls charts; these provide the default configuration for many applications that are\n\navailable to deploy in Kubernetes. The instructions for installing Helm are available from the Helm website at\n\nhttps://helm.sh/docs/intro/install/. Once again, to ensure the tool is working and correctly configured in your path, run the\n\nfollowing command: helm version\n\nThe full configuration for all the examples in this chapter is available in the companion repository at\n\nhttps://github.com/PacktPublishing/Cloud-Native-Observability. Please feel free to look in the\n\nchapter9 folder if any of the examples give you trouble. Great! Now that the hard part is done, let's\n\nget to the fun stuff and start deploying OpenTelemetry collectors in our cluster!\n\nCollecting application telemetry\n\nPreviously, we looked at how to use the collector running as a local binary. This can be useful for\n\ndevelopment and testing, but it's not how the collector would be deployed in a production\n\nenvironment. Before going further, here are some Kubernetes concepts that we will be using in this\n\nchapter:\n\nPod: This is a container or a group of containers that form an application.\n\nSidecar: This is a container that is deployed alongside application containers but isn't tightly coupled with the application in the\n\npod.\n\nNode: This is a representation of a Kubernetes worker; it could be a physical host or a virtual machine.\n\nDaemonSet: This is a pod template specification to ensure a pod is deployed to the configured nodes.\n\nIMPORTANT NOTE\n\nThe concepts of Kubernetes form a much deeper topic than we have time for in this book. For our examples, we will\n\nonly cover the bare minimum that is necessary for this chapter. There is a lot more to cover and, thankfully, many",
      "content_length": 2951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "resources are available on the internet regarding this vast topic.\n\nFigure 9.1 shows three different deployment scenarios that can be used to deploy the OpenTelemetry\n\ncollector in a production environment, which, in this case, is a Kubernetes cluster:\n\nThe first deployment (1) is alongside the application containers within the same pod. This deployment is commonly referred to as\n\na sidecar deployment.\n\nThe second deployment (2) shows the collector running as a container on the same node as the application pod. This agent\n\ndeployment represents a DaemonSet deployment, which means that the collector container will be present in every node in the\n\nKubernetes cluster.\n\nThe third deployment (3) is shown running the collector as a gateway. In practice, the containers in the collector service will run\n\non Kubernetes nodes, which may or may not be the same as the ones running the application pod.\n\nAdditionally, the following diagram shows the flow for the telemetry data from one collector to\n\nanother, which we will configure in this chapter:\n\nFigure 9.1 – The three deployment options for the collector In this chapter, we will work through each scenario, starting\n\nwith collecting application telemetry. We can do this by deploying the collector as close to the application as possible\n\nwithin the same pod. When emitting telemetry from an application, often, it's useful to offload the data as quickly as\n\npossible to reduce the resource impact on the application. This allows the application to spend most of its time on what\n\nit is meant to do, that is, manage the workloads it was created to manage. To ensure the lowest possible latency while\n\ntransmitting telemetry, let's look at deploying the collector as close as possible to the application, as a sidecar.",
      "content_length": 1775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "Deploying the sidecar\n\nTo reduce that latency and the complexity of collecting telemetry, deploying the collector as a loosely\n\ncoupled container within the same pod as the application makes the most sense. This ensures the\n\nfollowing:\n\nThe application will always have a consistent destination to send its telemetry to since applications within the same pod can\n\ncommunicate with each other via localhost.\n\nThe latency between the application and the collector will not affect the application. This allows the application to offload its\n\ntelemetry as quickly as possible, preventing unexpected memory loss or CPU pressure for high-throughput applications.\n\nLet's look at how this is done. First, consider the following configuration, which includes the\n\nshopper, the grocery store, and the inventory applications. These have been containerized to allow us\n\nto deploy them via Kubernetes. In addition to this, the pod configuration contains a collector\n\ncontainer. The most important thing to note in the configuration for our use case is the containers\n\nsection, which defines the four containers that make up the application via name and image containers.\n\nCreate a YAML file that includes the following configuration: config/collector/sidecar.yml\n\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\nname: cloud-native-example\n\nlabels:\n\napp: example\n\nspec:\n\nreplicas: 1\n\nselector:\n\nmatchLabels:\n\napp: example\n\ntemplate:\n\nmetadata:\n\nlabels:\n\napp: example\n\nspec:\n\ncontainers:\n\nname: legacy-inventory\n\nimage: codeboten/legacy-inventory:chapter9\n\nname: grocery-store",
      "content_length": 1563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "image: codeboten/grocery-store:chapter9\n\nname: shopper\n\nimage: codeboten/shopper:chapter9\n\nname: collector\n\nimage: otel/opentelemetry-collector:0.43.0\n\nThe default configuration for the collector container configures an OTLP receiver, which you'll\n\nremember from Chapter 8, OpenTelemetry Collector. Additionally, it configures a logging exporter.\n\nWe will modify this configuration later in this chapter; however, for now, the default is good enough.\n\nLet's apply the previous configuration to our cluster by running the following command. This uses\n\nthe configuration to pull the container images from the Docker repository and creates the deployment\n\nand pod running the application: $ kubectl apply -f config/collector/sidecar.yml\n\ndeployment.apps/cloud-native-example created\n\nWe can ensure the pod is up and running with the following command, which gives us details about\n\nthe pod along with the containers that are running within it: $ kubectl describe pod -l app=example\n\nWe should be able to view all the details about the pod we configured:\n\nkubectl describe output Name: cloud-native-example-6bdfd8b6d6-cfhc7\n\nNamespace: default\n\nPriority: 0 ...\n\nWith the pod running, we should now be able to look at the logs of the collector sidecar and observe\n\nthe telemetry flowing. The following command lets us view the logs from any container within the\n\npod. The container can be specified via the -c flag followed by the name of the container in question.\n\nThe -f flag can be used to tail the logs. You can use the same command to observe the output of the\n\nother containers by changing the -c flag to the name of different containers: kubectl logs -l\n\napp=example -f -c collector\n\nThe output of the previous command will contain telemetry from the various applications in the\n\ngrocery store example. It should look similar to the following: kubectl logs output\n\nSpan #6\n\nTrace ID : 2ca9779b6ad6d5b1a067dd83ea0942d4\n\nParent ID : 09b499899194ba83\n\nID : c8a1d75232eaf376\n\nName : inventory request\n\nKind : SPAN_KIND_INTERNAL\n\nStart time : 2021-06-19 22:38:53.3719469 +0000 UTC",
      "content_length": 2078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "End time : 2021-06-19 22:38:53.3759594 +0000 UTC\n\nStatus code : STATUS_CODE_UNSET\n\nStatus message :\n\nAttributes:\n\n> http.method: STRING(GET)\n\n> http.flavor: STRING(HttpFlavorValues.HTTP_1_1) -> http.url:\n\nSTRING(http://localhost:5001/inventory) -> net.peer.ip: STRING(127.0.0.1)\n\nNow we have a pod with a collector sidecar collecting telemetry! We will come back to make\n\nchanges to this pod shortly, but first, let's look at the next deployment scenario.\n\nSystem-level telemetry\n\nAs discussed in Chapter 8, OpenTelemetry Collector, the OpenTelemetry collector can be configured\n\nto collect metrics about the system it's running on. Often, this can be helpful when you wish to\n\nidentify resource constraints on nodes, which is a fairly common problem. Additionally, the collector\n\ncan be configured to forward data. So, it might be beneficial to deploy a collector on each host or\n\nnode in your environment to provide an aggregation point for all the applications running on that\n\nnode. As shown in the following diagram, deploying a collector as an agent can reduce the number of\n\nconnections needed to send telemetry from each node:\n\nFigure 9.2 – Backend connections from nodes with and without an agent This can become a significant processing\n\nbottleneck if, for example, the backend requires secure connections to be established with some level of frequency and\n\nif many applications are running per node.",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "Deploying the agent\n\nThe preferred way to deploy the collector as an agent is by using Helm Charts, which is provided by\n\nthe OpenTelemetry project. You can find this at https://github.com/open-telemetry/opentelemetry-\n\nhelm-charts. The first step to install a Helm chart is to tell Helm where it should look for the chart\n\nusing the following command. This adds the open-telemetry repository to Helm: $ helm repo add\n\nopen-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts \"open-telemetry\" has\n\nbeen added to your repositories Then, we can launch the collector service using the following\n\ncommand. This will install the opentelemetry-collector Helm chart, using all the default options: $\n\nhelm install otel-collector open-telemetry/opentelemetry-collector Let's check to see what happened\n\nin our Kubernetes cluster because of the previous command. The collector chart should have\n\ndeployed the collector using DaemonSet. As mentioned earlier in the chapter, a DaemonSet is a way to\n\ndeploy an instance of a pod on all nodes in Kubernetes. The following command lists all deployed\n\nDaemonSet deployments in our cluster, and you can view the resulting output as follows: $ kubectl get\n\nDaemonSet\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\n\notel-collector-opentelemetry-collector-agent 1 1 1 1 1 <none> 3m25s Note that the results\n\nmight be different depending on how many nodes your cluster has; mine has a single node.\n\nNext, let's examine the pods created using the following command: $ kubectl get Pod\n\nNAME READY STATUS RESTARTS AGE\n\notel-collector-opentelemetry-collector-agent-hhgkk 1/1 Running 0 4m39s With the collector\n\nrunning as an agent on the node, let's learn about how to forward all the data from the\n\ncollector sidecar to the agent.\n\nConnecting the sidecar and the agent\n\nIt's time to update the sidecar collector configuration to use an OTLP exporter to export data. This\n\ncan be accomplished using a ConfigMap, which gives us the ability to have Kubernetes create a file\n\nthat will be mounted as a volume inside the container. For brevity, the details of ConfigMap and the\n\nvolumes in Kubernetes will not be described here. Add the following ConfigMap object to the top of\n\nthe sidecar configuration file: config/collector/sidecar.yml apiVersion: v1\n\nkind: ConfigMap\n\nmetadata:\n\nname: otel-sidecar-conf\n\nlabels:\n\napp: opentelemetry",
      "content_length": 2395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "component: otel-sidecar-conf\n\ndata:\n\notel-sidecar-config: |\n\nreceivers:\n\notlp:\n\nprotocols:\n\ngrpc:\n\nhttp:\n\nexporters:\n\notlp:\n\nendpoint: \"$NODE_IP:4317\"\n\ntls:\n\ninsecure: true\n\nservice:\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nexporters: [otlp]\n\nmetrics:\n\nreceivers: [otlp]\n\nexporters: [otlp]\n\nlogs:\n\nreceivers: [otlp]\n\nexporters: [otlp]\n\nThe preceding configuration might remind you of the collector-specific configuration we explored in\n\nChapter 8, OpenTelemetry Collector. It is worth noting that we will be using the NODE_IP\n\nenvironment variable in the configuration of the endpoint for the OTLP exporter.\n\nFollowing this, we need to update the containers section further down. This is so that we can use the\n\notel-sidecar-conf ConfigMap and tell the collector container to pass the configuration file at start\n\ntime via the command option. The following configuration also exposes the node's IP address as an\n\nenvironment variable named NODE_IP: config/collector/sidecar.yml apiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\nname: cloud-native-example",
      "content_length": 1054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "labels:\n\napp: example\n\nspec:\n\nreplicas: 1\n\nselector:\n\nmatchLabels:\n\napp: example\n\ntemplate:\n\nmetadata:\n\nlabels:\n\napp: example\n\nspec:\n\ncontainers:\n\nname: legacy-inventory\n\nimage: codeboten/legacy-inventory:latest\n\nname: grocery-store\n\nimage: codeboten/grocery-store:latest\n\nname: shopper\n\nimage: codeboten/shopper:latest\n\nname: collector\n\nimage: otel/opentelemetry-collector:0.27.0\n\ncommand:\n\n\"/otelcol\"\n\n\"--config=/conf/otel-sidecar-config.yaml\"\n\nvolumeMounts:\n\nname: otel-sidecar-config-vol\n\nmountPath: /conf\n\nenv:\n\nname: NODE_IP\n\nvalueFrom:\n\nfieldRef:\n\nfieldPath: status.hostIP\n\nvolumes:\n\nconfigMap:",
      "content_length": 601,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "name: otel-sidecar-conf\n\nitems:\n\nkey: otel-sidecar-config\n\npath: otel-sidecar-config.yaml\n\nname: otel-sidecar-config-vol\n\nFor this new configuration to take effect, we'll go ahead and apply the configuration with the\n\nfollowing command: $ kubectl apply -f config/collector/sidecar.yml\n\nLooking at the logs for the agent, we can now observe that telemetry is being processed by the\n\ncollector: kubectl logs -l component=agent-collector -f\n\n2021-06-26T22:57:50.719Z INFO loggingexporter/logging_exporter.go:327 TracesExporter\n\n{\"#spans\": 20}\n\n2021-06-26T22:57:50.919Z INFO loggingexporter/logging_exporter.go:327 TracesExporter\n\n{\"#spans\": 10}\n\n2021-06-26T22:57:53.726Z INFO loggingexporter/logging_exporter.go:375 MetricsExporter\n\n{\"#metrics\": 22}\n\n2021-06-26T22:57:54.730Z INFO loggingexporter/logging_exporter.go:327 TracesExporter\n\n{\"#spans\": 5}\n\nWhile we're here, we might as well take some time to augment the telemetry processed by the\n\ncollector. We can do this by applying some of the lessons we learned in Chapter 8, OpenTelemetry\n\nCollector. Let's configure a processor to provide more visibility inside our infrastructure.\n\nAdding resource attributes\n\nOne of the great things about the agent collector is being able to ensure information about the node\n\nit's running on is present across all the telemetry processed by the agent. Helm allows us to override\n\nthe default configuration via YAML; the following can be used in conjunction with the Helm chart to\n\nconfigure a resource attributes processor to inject information into our telemetry. It does the\n\nfollowing:\n\nIt makes an environment variable named NODE_NAME available for use by the resource attributes processor.\n\nIt sets the loglevel parameter of the logging exporter to debug. This allows us to observe the data being emitted by the\n\ncollector in more detail.\n\nIt configures a resource attributes processor to inject the NODE_NAME environment variable into an attribute with the\n\nk8s.node.name key. Additionally, it adds the processor to the pipelines for logs, metrics, and traces.\n\nCreate a new config/collector/config.yml configuration file that contains the following\n\nconfiguration. We'll use this to update the Helm chart: config/collector/config.yml extraEnvs:",
      "content_length": 2239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "name: NODE_NAME\n\nvalueFrom:\n\nfieldRef:\n\nfieldPath: spec.nodeName\n\nconfig:\n\nexporters:\n\nlogging:\n\nloglevel: debug\n\nagentCollector:\n\nenabled: true\n\nconfigOverride:\n\nprocessors:\n\nresource:\n\nattributes:\n\nkey: k8s.node.name\n\nvalue: ${NODE_NAME}\n\naction: upsert\n\nservice:\n\npipelines:\n\nmetrics:\n\nprocessors: [batch, memory_limiter, resource]\n\ntraces:\n\nprocessors: [batch, memory_limiter, resource]\n\nlogs:\n\nprocessors: [batch, memory_limiter, resource]\n\nApply the preceding configuration via Helm using the following command: $ helm upgrade otel-\n\ncollector open-telemetry/opentelemetry-collector -f ./config/collector/config.yml Release \"otel-\n\ncollector\" has been upgraded. Happy Helming!\n\nNAME: otel-collector\n\nLAST DEPLOYED: Sun Sep 19 13:22:10 2021\n\nLooking at the logs from the agent, we should observe that the telemetry contains the attributes we\n\nadded earlier: $ kubectl logs -l component=agent-collector -f",
      "content_length": 909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Now, we have the collector sidecar sending data to the agent, and the agent is adding attributes via a\n\nprocessor: kubectl logs output\n\n2021-09-19T20:30:20.888Z DEBUG loggingexporter/logging_exporter.go:366 ResourceSpans #0\n\nResource labels:\n\n> telemetry.sdk.language: STRING(python)\n\n> telemetry.sdk.name: STRING(opentelemetry)\n\n> telemetry.sdk.version: STRING(1.3.0)\n\n> net.host.name: STRING(cloud-native-example-5d57799766-w8rjp) -> net.host.ip:\n\nSTRING(10.244.0.5)\n\n> service.name: STRING(grocery-store)\n\n> service.version: STRING(0.1.2)\n\n> k8s.node.name: STRING(kind-control-plane)\n\nInstrumentationLibrarySpans #0\n\nInstrumentationLibrary 0.1.2 grocery-store\n\nIMPORTANT NOTE\n\nYou might find it confusing that the previous example is not configuring receivers and exporters for the telemetry pipelines.\n\nThis is because the values we pass into Helm only override some of the default configurations in the chart. Since we only\n\nneeded to override the processors, the exporters and receivers continued to use the defaults that had already been\n\nconfigured. If you'd like to look at all the configured defaults, I suggest you refer to the repository at\n\nhttps://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-collector/values.yaml.\n\nHaving this single point to aggregate and add information to telemetry could be used to simplify our\n\napplication code. If you recall, in Chapter 4, Distributed Tracing – Tracing Code Execution, we\n\ncreated a custom ResourceDetector parameter to add net.host.name and net.host.ip attributes to all\n\napplications. That code could be removed in favor of injecting the same data via the collector. This\n\nmeans that now, any application could get these attributes without the complexity of utilizing custom\n\ncode. Next, let's look at standalone service deployment.\n\nCollector as a gateway\n\nThe last scenario we'll cover is how to deploy the collector as a standalone service, also known as a\n\ngateway. In this mode, the collector can provide a horizontally scalable service to do additional\n\nprocessing on the telemetry before sending it to a backend. Horizontal scaling means that if the\n\nservice comes under too much pressure, we can launch additional instances of it, which, in this case,\n\nis the collector, to manage the increasing load. Additionally, the standalone service can provide a\n\ncentral location for the configuring, sampling, and scrubbing of the telemetry. From a security\n\nstandpoint, it might also be preferable to have a single service sending traffic outside of your",
      "content_length": 2552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "network. This is because it simplifies the rules that need to be configured and reduces the risk and\n\nblast radius of vulnerabilities.\n\nIMPORTANT NOTE\n\nIf your backend is deployed within your network, it's possible that a standalone service for the collector will be overkill, as\n\nyou might be happier sending telemetry directly to the backend and saving yourself the trouble of operating an additional\n\nservice in your infrastructure.\n\nConveniently, the same Helm chart we used earlier to deploy the collector as an agent can also be\n\nused to configure the gateway. This also provides us with an opportunity to configure the agent to\n\nexport its data to the standalone collector, and therefore, we can feed two birds with one scone by\n\ndoing both at the same time. Depending on your Kubernetes cluster, the default value of 2Gi might\n\nprevent the service from starting as it did in the case of my kind cluster. The following section can be\n\nappended to the bottom of the configuration file from the previous example to enable\n\nstandaloneCollector and limit its memory consumption to 512Mi: config/collector/config.yml\n\nstandaloneCollector:\n\nenabled: true\n\nresources:\n\nlimits:\n\ncpu: 1\n\nmemory: 512Mi\n\nApply the update to the Helm chart by running the following command again: $ helm upgrade otel-\n\ncollector open-telemetry/opentelemetry-collector -f ./config/collector/config.yml A nice feature of the\n\nOpenTelemetry collector Helm chart is that if both agentCollector and standaloneCollector are\n\nconfigured, an OTLP exporter is automatically configured on the agent to forward traffic on the\n\nstandalone collector. The following code depicts a snippet of the Helm chart template to give us an\n\nidea of how that will be configured: config.tpl\n\n{{- if .Values.standaloneCollector.enabled }}\n\nexporters:\n\notlp:\n\nendpoint: {{ include \"opentelemetry-collector.fullname\" . }}:4317\n\ninsecure: true\n\n{{- end }}\n\nIt's time to examine the logs from the new service to check whether the data is reaching the\n\nstandalone collector. The following command should be familiar now; make sure that you use the\n\nstandalone-collector label when filtering the logs: $ kubectl logs -l component=standalone-",
      "content_length": 2187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "collector -f Now the output from the logs shows us the same logs that we observed from the agent\n\ncollector earlier, being processed by the standalone collector: kubectl logs output\n\nMetric #11\n\nDescriptor:\n\n> Name: otelcol_processor_accepted_spans\n\n> Description: Number of spans successfully pushed into the next component in the\n\npipeline.\n\n> Unit:\n\n> DataType: DoubleSum\n\n> IsMonotonic: true\n\n> AggregationTemporality: AGGREGATION_TEMPORALITY_CUMULATIVE\n\nDoubleDataPoints #0\n\nData point labels:\n\n> processor: memory_limiter\n\n> service_instance_id: b208628b-7b0f-4275-9ea8-a5c445582cbc StartTime:\n\n1632083630725000000\n\nTimestamp: 1632083730725000000\n\nValue: 718.000000\n\nIf you run kubectl logs with the agent-collector label, you'll find that because the agent collector is\n\nnow using the otlp exporter instead of the logging exporter, it no longer emits logs.\n\nAutoscaling\n\nUnlike the sidecar, which relied on an application pod, or the agent deployment, which relied on\n\nindividual nodes to scale, the standalone service can be automatically scaled based on CPU and\n\nmemory constraints. It does this using a Kubernetes feature known as HorizontalPodAutocaling,\n\nwhich can be configured via the following: autoscaling:\n\nenabled: false\n\nminReplicas: 1\n\nmaxReplicas: 10\n\ntargetCPUUtilizationPercentage: 80\n\ntargetMemoryUtilizationPercentage: 80\n\nDepending on the needs of your environment, combining autoscaling with a load balancer might be\n\nworth pursuing to provide a high level of reliability and capacity for the service.",
      "content_length": 1528,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "OpenTelemetry Operator\n\nAnother option for managing the OpenTelemetry collector in a Kubernetes environment is the\n\nOpenTelemetry operator (https://github.com/open-telemetry/opentelemetry-operator). If you're\n\nalready familiar with using operators, they reduce the complexity of deploying and maintaining\n\ncomponents in the Kubernetes landscape. In addition to managing the deployment of the collector,\n\nthe OpenTelemetry operator provides support for auto-instrumenting applications.\n\nSummary\n\nWe've only just scratched the surface of how to run the collector in production by looking at very\n\nspecific use cases. However, you can start thinking about how to apply the lessons you have learned\n\nfrom this chapter to your environments. Whether it be using Kubernetes, bare metal, or another form\n\nof hybrid cloud environment, the same principles we explored in this chapter regarding how to best\n\ncollect telemetry will apply. Collecting telemetry from an application should always be done with\n\nminimal impact on the application itself. The sidecar deployment mode provides a collection point as\n\nclose as possible to the application without adding any dependency to the application itself.\n\nThe deployment of the collector as an agent gives us the ability to collect information about the\n\nworker running our applications, which could also allow us to monitor the health of the resources in\n\nour cluster. Additionally, this serves as a convenient point to augment the telemetry from applications\n\nwith resource-specific attributes, which can be leveraged at analysis time. Finally, deploying the\n\ncollector as a gateway allowed us to start thinking about how to deploy and scale a service to collect\n\ntelemetry within our networks.\n\nThis chapter also gave us a chance to become familiar with some of the tools that OpenTelemetry\n\nprovides to infrastructure engineers to manage the collector. We experimented with the\n\nOpenTelemetry collector container alongside the Helm charts provided by the project. Now that we\n\nhave our environment deployed and primed to send data to a backend, in the next chapter, we'll take a\n\nlook at options for open source backends.",
      "content_length": 2162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Chapter 10: Configuring Backends\n\nSo far, what we've been learning about has focused on the tools that are used to generate telemetry\n\ndata. Although producing telemetry data is an essential aspect of making a system observable, it\n\nwould be difficult to argue that the data we've generated in the past few chapters has made our system\n\nobservable. After all, reading hundreds of lines of output in a console is hardly a practical tool for\n\nanalysis. Data analysis is an essential aspect of observability that we have only briefly discussed thus\n\nfar. This chapter is all about the tools we can use to analyze our applications' telemetry.\n\nWe are going to cover the following topics:\n\nOpen source telemetry backends to analyze traces, metrics, and logs\n\nConsiderations for running analysis systems in production\n\nThroughout this chapter, we will visualize the data we've generated and start thinking about using it\n\nin real life. There is a large selection of analysis tools to choose from, but this chapter will only focus\n\non a select few. It's worth noting that many commercial products (https://opentelemetry.io/vendors/)\n\nsupport OpenTelemetry; this chapter will focus solely on open source projects. This chapter will also\n\nskim the surface of the knowledge that you will need to run these telemetry backends in production.\n\nTechnical requirements\n\nThis chapter will use Python code to directly configure and use backends from a test application. To\n\nensure your environment is set up correctly, run the following commands and ensure Python 3.6 or\n\ngreater is installed on your system: $ python --version\n\nPython 3.8.9\n\n$ python3 --version\n\nPython 3.8.9\n\nIf you do not have Python 3.6+ installed, go to the Python website\n\n(https://www.python.org/downloads/) for instructions on installing the latest version.\n\nTo test out some of the exporters we'll be using in the chapter, install the following OpenTelemetry\n\npackages via pip: $ pip install opentelemetry-distro \\\n\nopentelemetry-exporter-jaeger \\\n\nopentelemetry-exporter-zipkin\n\nAdditionally, we will use Docker (https://docs.docker.com/get-docker/) to deploy backends. The\n\nfollowing code will ensure Docker is up and running in your environment: $ docker version",
      "content_length": 2224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "Client:\n\nCloud integration: 1.0.14\n\nVersion: 20.10.6\n\nAPI version: 1.41\n\nGo version: go1.16.3 ...\n\nTo launch the backends, we will use Docker Compose once again. Ensure Compose is available by\n\nrunning the following commands: $ docker compose version\n\nDocker Compose version 2.0.0-beta.1\n\nNow, download the code and configuration for this chapter from this book's GitHub repository: $ git\n\nclone https://github.com/PacktPublishing/Cloud-Native-Observability $ cd Cloud-Native-\n\nObservability/chapter10\n\nWith the code downloaded, we're ready to launch the backends using Compose: $ docker compose up\n\nThe following diagram shows the architecture of the environment that we'll be deploying. Initially,\n\nthe example for this chapter will connect to the backends directly. After that, we will send data to the\n\nOpenTelemetry Collector which we'll connect to the telemetry backends. Grafanais connected to\n\nJaeger, Zipkin, Loki, and Prometheus, as we will discuss later in this chapter.\n\nFigure 10.1 – Backend deployment in Docker The configuration files for all this chapter's examples can be found in the\n\nconfig directory. Let's dive in!\n\nBackend options for analyzing telemetry data",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "The world of observability contains an abundance of tools to provide you with insights into what\n\nsystems are doing. Within OpenTelemetry, a backend is the destination of the telemetry data and is\n\nwhere it is stored and analyzed. All the telemetry backends that we will explore in this chapter\n\nprovide the following:\n\nA destination for the telemetry data. This is usually in the form of a network endpoint, but not always.\n\nStorage for the telemetry data. The retention period that's supported by the storage is determined by the size of the storage and the\n\namount of data being stored.\n\nVisualization tooling for the data. All the tools we'll use provide a web interface for displaying and querying telemetry data.\n\nIn OpenTelemetry, applications connect to backends via exporters, two of which we've already\n\nconfigured: the console exporter and the OTLP exporter. Each application can be configured to send\n\ndata directly via an exporter that's been implemented specifically for that backend. The following\n\ntable shows a current list of officially supported exporters for the backends by the OpenTelemetry\n\nspecification, along with their status in the Python implementation:\n\nFigure 10.2 – Status of the exporters in Python for officially supported backends Each language that implements the\n\nOpenTelemetry specification must provide an exporter for these backends. Additional information about the support for\n\neach exporter in different languages can be found in the specification repository: https://github.com/open-\n\ntelemetry/opentelemetry-specification/blob/main/spec-compliance-matrix.md#exporters.\n\nTracing\n\nStarting with the tracing signal, let's look at some options for visualizing traces. As we work through\n\ndifferent backends, we'll see how it's possible to use other methods to configure a backend, starting\n\nwith auto-instrumentation. The following code makes a series of calls to create a table and insert\n\nsome data into a local database using SQLite (https://www.sqlite.org/index.html) while logging some\n\ninformation along the way: sqlite_example.py import logging\n\nimport os\n\nimport sqlite3\n\nlogging.basicConfig(level=logging.DEBUG)\n\nlogger = logging.getLogger(__name__)",
      "content_length": 2199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "logger.info(\"creating database\")\n\ncon = sqlite3.connect(\"example.db\") cur = con.cursor()\n\nlogger.info(\"adding table\")\n\ncur.execute(\n\n\"\"\"CREATE TABLE clouds\n\n(category text, description text)\"\"\"\n\n)\n\nlogger.info(\"inserting values\")\n\ncur.execute(\"INSERT INTO clouds VALUES ('stratus','grey')\") con.commit()\n\ncon.close()\n\nlogger.info(\"deleting database\")\n\nos.remove(\"example.db\")\n\nRun the preceding code to ensure everything is working as expected by running the following\n\ncommand: $ python sqlite_example.py\n\nINFO:__main__:creating database\n\nINFO:__main__:adding table\n\nINFO:__main__:inserting values\n\nINFO:__main__:deleting database\n\nNow that we have some working code, let's ensure we can produce telemetry data by utilizing auto-\n\ninstrumentation. As you may recall from Chapter 7, Instrumentation Libraries, Python provides the\n\nopentelemetry-bootstrap script to detect and install instrumentation libraries for us automatically.\n\nThe library we're using in our code, sqlite3, has a supported instrumentation library that we can\n\ninstall with the following command: $ opentelemetry-bootstrap -a install\n\nCollecting opentelemetry-instrumentation-sqlite3==0.26b1\n\n...\n\nThe output from the preceding command will produce some logging information that's generated by\n\ninstalling the packages through pip. If the output doesn't quite match mine, opentelemetry-bootstrap\n\nlikely found additional packages to install for your environment.\n\nUsing opentelemetry-instrument, let's ensure that telemetry data is generated by configuring our\n\ntrusty console exporter: $ OTEL_RESOURCE_ATTRIBUTES=service.name=sqlite_example \\\n\nOTEL_TRACES_EXPORTER=console \\\n\nopentelemetry-instrument python sqlite_example.py The output should now contain tracing\n\ninformation that's similar to the following abbreviated output: output\n\nINFO:__main__:creating database",
      "content_length": 1840,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "INFO:__main__:adding table\n\nINFO:__main__:inserting values\n\nINFO:__main__:deleting database\n\n{\n\n\"name\": \"CREATE\",\n\n\"context\": {\n\n\"trace_id\": \"0xf98afa4316b3ac52633270b1e0534ffe\", \"span_id\": \"0xb52fb818cb0823da\",\n\n\"trace_state\": \"[]\"\n\n},\n\n...\n\nNow, we're ready to look at our first telemetry backend by using a working example that utilizes\n\ninstrumentation to produce telemetry data. Zipkin\n\nOne of the original backends for distributed tracing, Zipkin (https://zipkin.io) was developed and\n\nopen sourced by Twitter in 2012. The project was made available for anyone to use under the Apache\n\n2.0 license, and its community is actively maintaining and developing the project. Its core\n\ncomponents are as follows:\n\nA collector to receive and index traces.\n\nA storage component, which provides a pluggable interface for storing data in various databases. The three storage options that\n\nare supported by Zipkin natively are Cassandra, Elasticsearch, and MySQL.\n\nA query service or API, which can be used to retrieve data from storage.\n\nAs we'll see shortly, there's a web UI, which gives users visualization and querying capabilities.\n\nThe easiest way to send data from the sample application to Zipkin is by changing the\n\nOTEL_TRACES_EXPORTER environment variable, as per the following command: $\n\nOTEL_RESOURCE_ATTRIBUTES=service.name=sqlite_example \\\n\nOTEL_TRACES_EXPORTER=zipkin \\\n\nopentelemetry-instrument python sqlite_example.py Setting the environment variable to\n\nzipkin tells auto-instrumentation to load ZipkinExporter, which is defined in the\n\nopentelemetry-exporter-zipkin-proto-http package. This connects to Zipkin via HTTP over\n\nport 9411. Launch a browser and access the Zipkin web UI via http://localhost:9411/zipkin.",
      "content_length": 1732,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Figure 10.3 – Zipkin UI landing page\n\nSearch for traces by clicking the Run Query button. The results should show two traces; clicking on\n\nthe details of one of these will bring up additional span information. This includes the attributes that\n\nare automatically populated by the instrumentation library, which are labeled as Tags in the Zipkin\n\ninterface.\n\nFigure 10.4 – Trace details view\n\nThe interface for querying lets you search for traces by trace ID, service name, duration, or tag,\n\namong other filters. It's also possible to filter traces by specifying a time window for the query. One\n\nlast feature of Zipkin we will inspect requires multiple services to produce traces. As it happens, we",
      "content_length": 699,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "have the grocery store already making telemetry data in our Docker environment; all we need to do is\n\nconfigure it to send data to Zipkin. Since the grocery store has already been configured to send data\n\nto the OpenTelemetry Collector, we'll update the collector's configuration to send data to Zipkin. Add\n\nthe following configuration to enable the Zipkin exporter for the Collector:\n\nconfig/collector/config.yml receivers:\n\notlp:\n\nprotocols:\n\ngrpc:\n\nexporters:\n\nlogging:\n\nloglevel: debug\n\nzipkin:\n\nendpoint: http://zipkin:9411/api/v2/spans\n\nservice:\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nexporters: [logging, zipkin]\n\nmetrics:\n\nreceivers: [otlp]\n\nexporters: [logging]\n\nlogs:\n\nreceivers: [otlp]\n\nexporters: [logging]\n\nFor the configuration changes to take effect, the OpenTelemetry Collector container must be\n\nrestarted. In terminal, use the following command from the chapter10 directory: $ docker compose\n\nrestart opentelemetry-collector An alternative would be to relaunch the entire Docker Compose\n\nenvironment, but restarting just the opentelemetry-collector container is more expedient.\n\nIMPORTANT NOTE\n\nTrying to run the restart command from other directories will result in an error while trying to find a suitable\n\nconfiguration.\n\nLooking at the Zipkin interface again, searching for traces yields much more interesting results when\n\nthe traces link spans across services. Try running some queries by searching for specific names or",
      "content_length": 1447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "tags and see interesting ways to peruse the data. One more feature worth noting is the dependency\n\ngraph, as shown in the following screenshot. It provides a service diagram that connects the\n\ncomponents of the grocery store.\n\nFigure 10.5 – Zipkin dependencies service diagram The dependencies service diagram can often be helpful if you wish\n\nto get a quick overview of a system and understand the flow of information between components. Let's see how this\n\ncompares with another tracing backend.\n\nJaeger\n\nInitially developed by engineers at Uber, Jaeger (https://www.jaegertracing.io) was open sourced in\n\n2015. It became a part of the Cloud Native Computing Foundation (CNCF), the same organization\n\nthat oversees OpenTelemetry, in 2017. The Jaeger project provides the following:\n\nAn agent that runs as close to the application as possible, often on the same host or inside the same pod.\n\nA collector to receive distributed traces that, depending on your deployment, talks directly to a datastore or Kafka for buffering.\n\nAn ingester that is (optionally) deployed. Its purpose is to read Kafka data and output it to a datastore.\n\nA query service that fetches data and provides a web UI for users to view it.\n\nReturning to the sample SQLite application for a moment, the following code uses in-code\n\nconfiguration to configure OpenTelemetry with JaegerExporter. It would be easy to update the\n\nOTEL_TRACES_EXPORTER variable to jaeger instead of zipkin and run opentelemetry-instrument to\n\naccomplish the same thing. Still, auto-instrumentation may not always be possible for an application.\n\nKnowing how to configure these exporters manually will surely come in handy someday.",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "The code in the following example adds the familiar configuration of the tracing pipeline. The\n\nfollowing are a couple of things to note:\n\nJaegerExporter has been configured to use a secure connection by default. We must pass in the insecure argument to change\n\nthis.\n\nThe code manually invokes SQLite3Instrumentor to trace calls via the sqlite3 library.\n\nAdd the following code to the top of the SQLite example code we created previously:\n\nsqlite_example.py ...\n\nfrom opentelemetry import trace\n\nfrom opentelemetry.exporter.jaeger.proto.grpc import JaegerExporter from\n\nopentelemetry.instrumentation.sqlite3 import SQLite3Instrumentor from\n\nopentelemetry.sdk.trace import TracerProvider from opentelemetry.sdk.resources import\n\nResource from opentelemetry.sdk.trace.export import BatchSpanProcessor def\n\nconfigure_opentelemetry():\n\nSQLite3Instrumentor().instrument()\n\nexporter = JaegerExporter(insecure=True)\n\nprovider = TracerProvider(\n\nresource=Resource.create({\"service.name\": \"sqlite_example\"}) )\n\nprovider.add_span_processor(BatchSpanProcessor(exporter))\n\ntrace.set_tracer_provider(provider)\n\nconfigure_opentelemetry()\n\n...\n\nRunning the application with the following command will send data to Jaeger: $ python\n\nsqlite_example.py\n\nAccess the Jaeger interface by browsing to http://localhost:16686/. Upon arriving on the landing\n\npage, searching for traces should yield results similar to what's shown in the following screenshot.\n\nNote that in Jaeger, you'll need to select a service from the dropdown on the left-hand side before you\n\ncan find traces.",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Figure 10.6 – Jaeger search results\n\nLooking through the details for each trace, we can see that the same information we previously found\n\nin Zipkin can be seen in Jaeger, although organized slightly differently. Next, let's update the\n\nCollector file's configuration to send traces from the grocery store to Jaeger. Add the following\n\njaeger section under the exporters definition in the Collector configuration file:\n\nconfig/collector/config.yml ...\n\nexporters:\n\n...\n\njaeger:\n\nendpoint: jaeger:14250\n\ntls:\n\ninsecure: true\n\nservice:\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nexporters: [logging, zipkin, jaeger]",
      "content_length": 611,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "...\n\nRestart the Collector container to reload the updated configuration: $ docker compose restart\n\nopentelemetry-collector The Jaeger web UI starts becoming more interesting when more data comes\n\nin. For example, note the scatter plot displayed previously in the search results; it's an excellent way\n\nto identify outliers. The chart supports clicking on individual traces to bring up additional details.\n\nFigure 10.7 – Scatter plot of trace durations Like Zipkin, Jaeger visualizes the relationship between services via the\n\nSystem Architecture diagram. An exciting feature that Jaeger delivers is that you can compare traces by selecting\n\ntraces of interest from the search results and clicking the Compare Traces button. The following screenshot shows a\n\ncomparison between two traces for the same operation. In one instance, the grocery store failed to connect to the\n\nlegacy inventory service, resulting in an error and a missing span.",
      "content_length": 941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Figure 10.8 – Trace comparison diagram\n\nThis visual representation of the trace comparison can help us quickly identify a difference between a\n\ntypical trace and one where an error occurred, zoning in on where the change was made.\n\nMetrics\n\nAs of November 2021, Prometheus is the only officially supported exporter for the metrics signal.\n\nOfficial support for StatsD in the specification was requested some time ago\n\n(https://github.com/open-telemetry/opentelemetry-specification/issues/374), but the lack of a\n\nspecification for StatsD has stopped OpenTelemetry from making it a requirement.\n\nPrometheus\n\nA project initially developed in 2012 by engineers at SoundCloud, Prometheus\n\n(https://prometheus.io) is a dominant open source metrics system. Its support for multi-dimensional\n\ndata and first-class support for alerting quickly made it a favorite of DevOps practitioners. Initially,\n\nPrometheus used a pull model only. Applications that wanted to store metrics exposed them via a\n\nnetwork endpoint that had been scraped by the Prometheus server. Prometheus now supports the push\n\nmodel via Prometheus Remote Write, allowing producers to send data to a remote server. The\n\ncomponents of interest to us currently are as follows:\n\nThe Prometheus server collects data from scrape targets and stores it in its time-series database (TSDB).\n\nThe Prometheus Query Language (PromQL) for searching and aggregating metrics.\n\nVisualization for metrics data via the Prometheus web UI.\n\nAs the current implementation of the Prometheus exporter for Python is still in development, in this\n\nsection, we will focus on the data that's produced by the grocery store, which is sent through the\n\nCollector. The implementation of the Prometheus exporter in the Collector is also in development at\n\nthe time of writing, but it is further along. The following configuration can be added to the Collector's\n\nconfiguration to send metrics to Prometheus: config/collector/config.yml exporters:\n\n...\n\nprometheus:\n\nendpoint: 0.0.0.0:8889\n\nresource_to_telemetry_conversion:\n\nenabled: true\n\nservice:\n\npipelines:\n\n...",
      "content_length": 2093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "metrics:\n\nreceivers: [otlp]\n\nexporters: [logging, prometheus]\n\n...\n\nReload the Collector with the following command:\n\n$ docker compose restart opentelemetry-collector Bring up the Prometheus web interface by\n\npointing your browser to http://localhost:9090. Using PromQL, the following query will\n\nreturn all the metrics that have been produced by the OpenTelemetry Collector:\n\n{job=\"opentelemetry-collector\"}\n\nThis can be seen in the following screenshot:\n\nFigure 10.9 – PromQL query results\n\nThe pull model makes horizontally scaling Prometheus an easy aspect that makes it a good option for\n\nmany environments. There are, of course, challenges with running Prometheus at scale, like any other\n\nbackend. Unfortunately, we don't have the space to dive into data availability across regions and\n\nlong-term storage, to name just a few challenges. Like Jaeger and OpenTelemetry, Prometheus is also\n\na project under the governance of the CNCF.\n\nLogging\n\nEven with no officially supported backends at the time of writing, it's helpful to have a way to query\n\nlogs that doesn't require looking at files on disk directly or paying for a service to get started. The",
      "content_length": 1157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "tools that we've discussed in this section have exporters available in the OpenTelemetry Collector but\n\nmay not necessarily have exporters implemented in other languages.\n\nLoki\n\nA project started by Grafana Labs in 2018, Loki is a log aggregation system that's designed to be easy\n\nto scale and operate. Its design is inspired by Prometheus and is composed of the following\n\ncomponents:\n\nA distributor that validates and pre-processes incoming logging data before sending it off to the ingester\n\nAn ingester that writes data to storage and provides a read endpoint for in-memory data\n\nA ruler that interprets configurable rules and triggers actions based on them\n\nA querier that performs queries for both the ingester and storage\n\nA query frontend that acts as a proxy for optimizing requests that are made to the querier\n\nThese components can be run in a single deployment or as a separate service to make it easy to\n\ndeploy them in whichever mode makes the most sense. The OpenTelemetry Collector provides an\n\nexporter for Loki, which can be configured as per the following code snippet. The configuration of\n\nthe Loki exporter supports relabeling attributes and resource attributes before sending the data. In the\n\nfollowing example, the service.name resource attribute has been relabeled job:\n\nconfig/collector/config.yml exporters:\n\n…\n\nloki:\n\nendpoint: http://loki:3100/loki/api/v1/push\n\nlabels:\n\nresource:\n\nservice.nam\": \"job\"\n\nservice:\n\npipelines:\n\n...\n\nlogs:\n\nreceivers: [otlp]\n\nexporters: [logging, loki]\n\n...\n\nOnce more, restart the Collector to reload the configuration and start sending data to Loki: $ docker\n\ncompose restart opentelemetry-collector Now, it's time to review this logging data. You may have",
      "content_length": 1719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "noticed that the components we mentioned earlier for Loki lack an interface for visualizing the data.\n\nThat's because the interface of choice for Loki is Grafana, which is a separate project altogether.\n\nGrafana\n\nGrafana (https://grafana.com/grafana/) is an open source tool that's been developed since 2014 by\n\nGrafana Labs to allow users to visualize and query telemetry data. Grafana enables users to configure\n\ndata sources that support various formats for traces, metrics, and logs. This includes Zipkin, Jaeger,\n\nPrometheus, and Loki.\n\nLet's see how we can access the logs we sent to our Loki backend. Access the Explore section of the\n\nGrafana interface via a browser by going to http://localhost:3000/explore. In the query field, enter\n\n{job=~\"grocery-store|inventory|shopper\"}. This will bring up all the logs for all the grocery store\n\ncomponents.\n\nFigure 10.10 – Logs search results\n\nGrafana allows users to create dashboards and alerts for the data that's received via its data sources.\n\nSince it's possible to view data from all signals, it's also possible to see data across all signals within a\n\nsingle dashboard. An example of such a dashboard has been preconfigured in the development\n\nenvironment and is accessible via the following URL: http://localhost:3000/d/otel/opentelemetry.",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "Figure 10.11 – Dashboard combining data across signals There are many more capabilities to explore regarding each\n\nof the tools discussed in this chapter. A vast amount of information on all the features and configuration options is\n\navailable on the website associated with each project. I strongly recommend spending some time familiarizing yourself\n\nwith these tools.\n\nRunning in production\n\nUsing analysis tools in development is one thing; running them in production is another. Running a\n\nsingle container on one machine is not an acceptable strategy for operating a service that provides\n\ninformation that's critical to an organization. It's worth considering the challenges of scaling\n\ntelemetry backends to meet the demands of the real world. The following subsections highlight areas\n\nthat require further reading before you run any of the backends mentioned earlier in production.\n\nHigh availability",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "The availability of telemetry backends is likely not as critical to end users as that of the applications\n\nthey are used to monitor. However, having an outage and realizing that the data that's required to\n\ninvestigate is unavailable or missing during the outage causes problems. If an application promises an\n\nuptime of 99.99%, the telemetry backend must be available to account for those guarantees. Some\n\naspects to consider when thinking of the high availability in the context of a telemetry backend are as\n\nfollows:\n\nEnsuring the telemetry receivers are available to senders. This can be accomplished by placing a load balancer between the\n\nsenders and the receivers.\n\nConsidering how the backends will be upgraded and how to minimize the impact on the applications being observed.\n\nUnderstanding the expectations for being able to query the data.\n\nDeciding how much of the data needs to be replicated to mitigate the risks of catastrophic failure.\n\nAdditionally, geo-distributed environments must consider how the applications will behave if a\n\nbackend is deployed in distant regions. Many of the backends we've discussed provide\n\nrecommendations for deploying the backend in a mode that supports high availability.\n\nScalability\n\nThe telemetry backend must be able to grow alongside the applications they support. Whether that's\n\nby adding more instances or increasing the number of resources that are given to the backend,\n\nknowing what the tools support can help you decide which backend to use. Some questions that are\n\nworth asking are as follows:\n\nCan the components of the backend be scaled independently?\n\nWill scaling the backend require vertical scaling or horizontal scaling?\n\nHow far will the solution scale? Is there a hard limit somewhere along the way?\n\nWhen we think about scalability, it's essential to understand the limitations of the tools we're working\n\nwith, even if we never come close to using them to their full extent.\n\nData retention\n\nA key challenge in telemetry is the volume of data that's being produced. It's easy to lean toward\n\nstoring every detail forever, as it is hard to predict when the data may become necessary. It's a bit like\n\nholding on to all those old cables and connectors for hardware that hasn't existed since the late 90s;\n\nyou never know when it will come in handy!",
      "content_length": 2322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "The problem with storing all the data forever is that it becomes costly at scale. On the other hand, the\n\ncost tends to cause engineers to lean in the opposite direction too much, where we log or record so\n\nlittle that it becomes hard to find anything of value. Some options to think about are as follows:\n\nIdentify an acceptable data retention period for the quantity of data that's being produced. This will likely change as teams become\n\nbetter at identifying issues within shorter periods.\n\nIf long-term data storage is desirable, use lower-cost storage to reduce operational costs. This may result in longer query times,\n\nbut the data will still be available.\n\nTune a sensible sampling option for the different signals. More on this will be covered in Chapter 12, Sampling.\n\nAt a minimum, data retention should cover periods when engineers are expected to be away. For\n\nexample, if no one is watching systems during a 2-day weekend, data should be retained for 3 or\n\nmore days. Otherwise, events that occur during the weekend will be impossible to investigate.\n\nWhatever you decide regarding the retention method, there are plenty of ways to fine-tune it over\n\ntime. It's also critical for teams across the organization to be aware of what this data retention is.\n\nPrivacy regulations\n\nDepending on the contents of the telemetry data that's produced by applications, the requirements for\n\nwhere and how the data can be stored vary. For example, regulations such as the General Data\n\nProtection Regulation (GDPR) recommend personally identifiable data to be pseudonymized to\n\nensure nobody can be associated with the data without additional processing. Depending on the\n\nrequirements in your environment and the telemetry data that's being produced, we have to take the\n\nfollowing into account about the data:\n\nThe data may need to remain within a specific country or region.\n\nThe data may need to be processed further before being stored. This could mean many things, from the data being encrypted to\n\nscrubbing it of personally identifiable information or pseudonymization.\n\nThe data may need access control and auditing capabilities.\n\nUsing the OpenTelemetry Collector as a receiver of telemetry data before sending the data to\n\ntelemetry backends can alleviate concerns around data privacy. Various processors in the Collector\n\ncan be configured to facilitate the scrubbing of sensitive information.\n\nSummary\n\nOne of the many jobs of software engineers today includes evaluating the new technology and tools\n\nthat are available to determine whether these tools would improve their ability to accomplish their",
      "content_length": 2616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "goals. Leveraging auto-instrumentation, in-code configuration, and the OpenTelemetry Collector, we\n\nquickly sent data from one backend to another to help us compare these tools.\n\nAll the tools we've discussed in this chapter take much more than a few pages to become familiar\n\nwith. Entire books have been written about running these in production, and the skills to do so well at\n\nscale require practice and experience. Understanding some areas that need additional thinking when\n\nthose tools are deployed allows us to uncover some of the unknowns.\n\nLooking through the different tools and starting to see how each one provides functionality to\n\nvisualize the data gave us a sense of how telemetry data can be used to start answering questions\n\nabout our systems. In the next chapter, we will focus on how these visualizations can identify specific\n\nproblems.",
      "content_length": 860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "Chapter 11: Diagnosing Problems\n\nFinally, after instrumenting application code, configuring a collector to transmit the data, and setting\n\nup a backend to receive the telemetry, we have all the pieces in place to observe a system. But what\n\ndoes that mean? How can we detect abnormalities in a system with all these tools? That's what this\n\nchapter is all about. This chapter aims to look through the lens of an analyst and see what the shape\n\nof the data looks like as events occur in a system. To do this, we'll look at the following areas:\n\nHow leaning on chaos engineering can provide the framework for running experiments in a system\n\nCommon scenarios of issues that can arise in distributed systems\n\nTools that allow us to introduce failures into our system\n\nAs we go through each scenario, we'll describe the experiment, propose a hypothesis, and use\n\ntelemetry to verify whether our expectations match what the data shows us. We will use the data and\n\nbecome more familiar with analysis tools to help us understand how we may answer questions about\n\nour systems in production. As always, let's start by setting up our environment first.\n\nTechnical requirements\n\nThe examples in this chapter will use the grocery store application we've used and revisited\n\nthroughout the book. Since the chapter's goal is to analyze telemetry and not specifically look at how\n\nthis telemetry is produced, the application code will not be the focus of the chapter. Instead of\n\nrunning the code as separate applications, we will use it as Docker (https://docs.docker.com/get-\n\ndocker/) containers and run it via Compose. Ensure Docker is installed with the following command:\n\n$ docker version\n\nClient:\n\nCloud integration: 1.0.14\n\nVersion: 20.10.6\n\nAPI version: 1.41\n\nGo version: go1.16.3 ...\n\nThe following command will ensure Docker Compose is also installed:\n\n$ docker compose version\n\nDocker Compose version 2.0.0-beta.1\n\nThe book's companion repository (https://github.com/PacktPublishing/Cloud-Native-Observability)\n\ncontains the Docker Compose configuration file, as well as the configuration required to run the",
      "content_length": 2108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "various containers. Download the companion repository via Git:\n\n$ git clone https://github.com/PacktPublishing/Cloud-Native-Observability\n\n$ cd Cloud-Native-Observability/chapter11\n\nWith the configuration in place, start the environment via the following:\n\n$ docker compose up\n\nThroughout the chapter, as we conduct experiments, know that it is always possible to reset the\n\npristine Docker environment by removing the containers entirely with the following commands:\n\n$ docker compose stop\n\n$ docker compose rm\n\nAll the tools needed to run various experiments have already been installed inside the grocery store\n\napplication containers, meaning there are no additional tools to install. The commands will be\n\nexecuted via docker exec and run within the container.\n\nIntroducing a little chaos\n\nIn normal circumstances, the real world is unpredictable enough that intentionally introducing\n\nproblems may seem unnecessary. Accidental configuration changes, sharks chewing through\n\nundersea cables, and power outages affecting data centers are just a few events that have caused\n\nlarge-scale issues across the world. In distributed systems, in particular, dependencies can cause\n\nfailures that may be difficult to account for during normal development.\n\nPutting applications through various stress, load, functional, and integration tests before they are\n\ndeployed to production can help predict their behavior to a large extent. However, some\n\ncircumstances may be hard to reproduce outside of a production environment. A practice known as\n\nchaos engineering (https://principlesofchaos.org) allows engineers to learn and explore the behavior\n\nof a system. This is done by intentionally introducing new conditions into the system through\n\nexperiments. The goal of these experiments is to ensure that systems are robust enough to withstand\n\nfailures in production.\n\nIMPORTANT NOTE\n\nAlthough chaos engineers run experiments in production, it's essential to understand that one of the principles of chaos\n\nengineering is not to cause unnecessary pain to users, meaning experiments must be controlled and limited in scope. In\n\nother words, despite its name, chaos engineering isn't just going around a data center and unplugging cables haphazardly.\n\nThe cycle for producing experiments goes as follows:\n\n1. It begins with a system under a known good state or steady state.\n\n2. A hypothesis is then proposed to explain the experiment's impact on the system's state.",
      "content_length": 2458,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "3. The proposed experiment is run on the system.\n\n4. Verification of the impact on the system takes place, validating that the prediction matches the hypothesis. The verification step\n\nprovides an opportunity to identify unexpected side effects of the experiment. If something behaved precisely as expected, great!\n\nIf it acted worse than expected, why? If it behaved better than expected, what happened? It's essential to understand what\n\nhappened, especially if the results were better than expected. It's too easy to look at a favorable outcome and move right along\n\nwithout taking the time to understand why it happened.\n\n5. Once verification is complete, improvements to the system are made, and the cycle begins anew. Ideally, running these\n\nexperiments can be automated once the results on the system are satisfactory to guard against future regressions.\n\nFigure 11.1 – Chaos engineering life cycle\n\nThe hypothesis step is crucial because if the proposed experiment will have disastrous repercussions\n\non the system, it may be worth rethinking the experiment.\n\nFor example, a hypothesis that turning off all production servers simultaneously will cause a\n\ncomplete system failure doesn't need to be validated. Additionally, this would guarantee the creation\n\nof unnecessary pain for all users of the system. There isn't much to learn from running this in\n\nproduction, except seeing how quickly users send angry tweets.\n\nAn alternative experiment that may be worthwhile is to shut down an availability zone or a region of\n\na system to ensure load balancing works. This would provide an opportunity to learn how production\n\ntraffic would be handled in the case of such a failure, validating that those systems in place to\n\nmanage such a failure are doing their jobs. Of course, if no such mechanisms are in place, it's not\n\nworth experimenting either, as this would have the same impact as shutting down all the servers for\n\nall users in that zone or region.",
      "content_length": 1963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "This chapter will take a page out of the chaos engineering book and introduce various failure modes\n\ninto the grocery store system. We will propose a hypothesis and run experiments to validate the\n\nassumptions. Using the telemetry produced by the store, we will validate that the system behaved as\n\nexpected. This will allow us to use the telemetry produced to understand how we can answer\n\nquestions about our system via this information. Let's explore a problem that impacts all networked\n\napplications: latency.\n\nExperiment #1 – increased latency\n\nLatency is the delay introduced when a call is made and the response is returned to the originating\n\ncaller. It can inject itself into many aspects of a system. This is especially true in a distributed system\n\nwhere latency can be found anywhere one service calls out to another. The following diagram shows\n\nan example of how latency can be calculated between two services. Service A calls a remote service\n\n(B), the request duration is 25 ms, but a large portion of that time is spent transferring data to and\n\nfrom service B, with only 5 ms spent executing code.\n\nFigure 11.2 – Latency incurred by calling a remote service\n\nIf the services are collocated, the latency is usually negligible and can often be ignored. However,\n\nlatency must be accounted for when services communicate over a network. This is something to think\n\nabout at development time. It can be caused by factors such as the following:\n\nThe physical distance between the servers hosting services. As even the speed of light requires time to travel distance, the greater\n\nthe distance between services, the greater the latency.\n\nA busy network. If a network reaches the limits of how much data it can transfer, it may throttle the data transmitted.\n\nProblems in any applications or systems connecting the services. Load balancers and DNS services are just two examples of the\n\nservices needed to connect two services.\n\nExperiment\n\nThe first experiment we'll run is to increase the latency in the network interface of the grocery store.\n\nThe experiment uses a Linux utility to manipulate the configuration on the network interface: Traffic\n\nControl (https://en.wikipedia.org/wiki/Tc_(Linux)). Traffic Control, or tc, is a powerful utility that\n\ncan simulate a host of scenarios, including packet loss, increased latency, or throughput limits. In this\n\nexperiment, tc will add a delay to inbound and outbound traffic, as shown in Figure 11.3:",
      "content_length": 2461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Figure 11.3 – Experiment #1 will add latency to the network interface\n\nHypothesis\n\nIncreasing the latency to the grocery store network interface will incur the following:\n\nA reduction in the total number of requests processed\n\nAn increase in the request duration time\n\nUse the following Docker command to introduce the latency. This uses the tc utility inside the\n\ngrocery store container to add a 1s delay to all traffic received and sent through interface eth0:\n\n$ docker exec grocery-store tc qdisc add dev eth0 root netem delay 1s Verify\n\nTo observe the metrics and traces generated, access the Application Metrics dashboard in Grafana\n\nvia the following URL: http://localhost:3000/d/apps/application-metrics. You'll immediately notice a\n\ndrop in the Request count time series and an increase in Request duration time quantiles. As time\n\npasses, you'll also start seeing the Request duration distribution histogram change to show an\n\nincreasing number of requests falling into buckets with longer durations that are as per the following\n\nscreenshot:\n\nFigure 11.4 – Request metrics for shopper, grocery-store, and inventory services",
      "content_length": 1135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "Note that although the drop in request count is the same across the inventory and grocery store\n\nservices, the duration of the request for the inventory service remains unchanged. This is a great\n\nstarting point, but it would be ideal to identify precisely where this jump in the request duration\n\noccurred.\n\nIMPORTANT NOTE\n\nAs discussed earlier in this book, the correlation between metrics and traces provided by exemplars could help us drill\n\ndown more quickly by giving us specific traces to investigate from the metrics. However, since the implementation of\n\nexemplar support in OpenTelemetry is still under development at the time of writing, the example in this chapter does not\n\ntake advantage of it. I hope that by the time you're reading this, exemplar support is implemented across many languages\n\nin OpenTelemetry.\n\nLet's look at the tracing data in Jaeger available at http://localhost:16686. From the metrics, we\n\nalready know that the issue appears to be isolated to the grocery store service. Sure enough, searching\n\nfor traces for that service yields the following chart:\n\nFigure 11.5 – Increased duration results in Jaeger\n\nIt's clear from this chart that something happened. The following screenshot shows us two traces; at\n\nthe top is a trace from before we introduced the latency; at the bottom is a trace from after. Although\n\nthe two look similar, looking at the duration of the spans named web request and /products, it's clear\n\nthat those operations are taking far longer at the bottom than at the top.",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "Figure 11.6 – Trace comparison before and after latency was introduced\n\nAs hypothesized, the total number of requests processed by the grocery store dropped due to the\n\nsimulation. This, in turn, reduced the number of calls to the inventory service. The total duration of\n\nthe request as observed by the shopper client increased significantly.\n\nRemove the delay to see how the system recovers. The following command removes the delay\n\nintroduced earlier:\n\n$ docker exec grocery-store tc qdisc del dev eth0 root netem delay 1s\n\nLatency is only one of the aspects of networks that can cause problems for applications. Traffic\n\nControl's network emulator (https://man7.org/linux/man-pages/man8/tc-netem.8.html) functionality\n\ncan simulate many other symptoms, such as packet loss and rate-limiting, or even the re-ordering of\n\npackets. If you're keen on playing with networks, it can be a lot of fun to simulate different scenarios.\n\nHowever, the network isn't the only thing that can cause problems for systems.\n\nExperiment #2 – resource pressure",
      "content_length": 1044,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "Although cloud providers make provisioning new computing resources more accessible than ever\n\nbefore, even computing in the cloud is still bound by the physical constraints of hardware running\n\napplications. Memory, processors, hard drives, and networks all have their limits. Many factors can\n\ncontribute to resource exhaustion:\n\nMisconfigured or misbehaving applications. Crashing and restarting in a fast loop, failing to free memory, or making requests over\n\nthe network too aggressively can all contribute to a load on resources.\n\nAn increase or spike in requests being processed by the service. This could be good news; the service is more popular than ever!\n\nOr it could be bad news, the result of a denial-of-service attack. Either way, more data to process means more resources are\n\nrequired.\n\nShared resources cause resource starvation. This problem is sometimes referred to as the noisy neighbor problem, where resources\n\nare consumed by another tenant of the physical hardware where a service is running.\n\nAutoscaling or dynamic resource allocation helps alleviate resource pressures to some degree by\n\nallowing users to configure thresholds at which new resources should be made available to the\n\nsystem. To know how these thresholds should be configured, it's valuable to experiment with how\n\napplications behave under limited resources. Experiment\n\nWe'll investigate how telemetry can help identify resource pressures in the following scenario. The\n\ngrocery store container is constrained to 50 M of memory via its Docker Compose configuration.\n\nMemory pressure will be applied to the container via stress.\n\nThe Unix stress utility (https://www.unix.com/man-page/debian/1/STRESS/) spins workers that\n\nproduce loads on systems. It creates memory, CPU, and I/O pressures by calling system functions in\n\na loop; malloc/free, sqrt, and sync, depending on which resource is being pressured.\n\nFigure 11.7 – Experiment #2 will apply memory pressure to the container\n\nHypothesis\n\nAs resources are consumed by stress, we expect the following to happen:\n\nThe grocery store processes fewer requests as it cannot obtain the resources to process requests.\n\nLatency increases across the system, as requests will take longer to process through the grocery store.\n\nMetrics collected from the grocery store container should quickly identify the increased resource pressure.\n\nThe following introduces memory pressure by adding workers that consume a total of 40 M of\n\nmemory to the grocery store container via stress for 30 minutes:",
      "content_length": 2529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "$ docker exec grocery-store stress --vm 20 --vm-bytes 2M --timeout 30m\n\nstress: info: [20] dispatching hogs: 0 cpu, 0 io, 10 vm, 0 hdd Verify\n\nWith the pressure in place, let's see whether the telemetry matches what we expected. Looking at the\n\napplication metrics, we can see an almost immediate increase in request duration as per the following\n\nscreenshot. The request count is also slightly impacted simultaneously.\n\nFigure 11.8 – Application metrics\n\nWhat else can we learn about the event? Searching through traces, an increase in duration similar to\n\nwhat occurred during the first experiment is shown:\n\nFigure 11.9 – Trace duration increased\n\nLooking in more detail at individual traces, we can identify which paths through the code cause this\n\nincrease. Not surprisingly, the allocating memory span, which locates an operation performing a\n\nmemory allocation, is now significantly longer, with its time jumping from 2.48 ms to 49.76 ms:",
      "content_length": 945,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "Figure 11.10 – Trace comparison before and after the memory increase\n\nThere is a second dashboard worth investigating at this time, the Container metrics dashboard\n\n(http://localhost:3000/d/containers/container-metrics). This dashboard shows the CPU, memory, and\n\nnetwork metrics collected directly from Docker by the collector's Docker stats receiver\n\n(https://github.com/open-telemetry/opentelemetry-collector-\n\ncontrib/tree/main/receiver/dockerstatsreceiver). Reviewing the following charts, it's evident that\n\nresource utilization increased significantly in one container:",
      "content_length": 576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "Figure 11.11 – Container metrics for CPU, memory, and network\n\nFrom the data, it's evident that something happened to the container running the grocery store. The\n\nduration of requests through the system increased, as did the metrics showing memory and CPU\n\nutilization for the container. Identifying resource pressure in this testing environment isn't particularly\n\ninteresting beyond this.\n\nRecall that OpenTelemetry specifies resource attributes for all signals, meaning that if multiple\n\nservices are running on the same resource, host, container, or node, it would be possible to correlate\n\ninformation about those services using this resource information, meaning that if we were running\n\nmultiple applications on the same host, and one of them triggered memory pressure, it would be\n\npossible to verify its impact on other services within the same host by utilizing its resource attributes\n\nas an identifier when querying telemetry.\n\nResource information can help answer questions when, for example, a host has lost power, and there\n\nis a need to identify all services impacted by this event quickly. Another way to use this information\n\nis when two completely unrelated services are experiencing problems simultaneously. If those two\n\nservices operate on the same node, resource information will help connect the dots.\n\nExperiment #3 – unexpected shutdown\n\nIf a service exits in a forest of microservices and no one is around to observe it, does it make a\n\nsound? With the right telemetry and alert configuration in place, it certainly does. Philosophical\n\nquestions aside, services shutting down happens all the time. Ensuring that services can manage this\n\nevent gracefully is vital in dynamic environments where applications come and go as needed.",
      "content_length": 1758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "Expected and unexpected shutdowns or restarts can be caused by any number of reasons. Some\n\ncommon ones are as follows:\n\nAn uncaught exception in the code causes the application to crash and exit.\n\nResources consumed by a service pass a certain threshold, causing an application to be terminated by a resource manager.\n\nA job completes its task, exiting intentionally as it terminates.\n\nExperiment\n\nThis last experiment will simulate a service exiting unexpectedly in our system to give us an idea of\n\nwhat to look for when identifying this type of failure. Using the docker kill command, the inventory\n\nservice will be shut down unexpectedly, leaving the rest of the services to respond to this failure and\n\nreport this issue.\n\nFigure 11.12 – Experiment #3 will terminate the inventory service\n\nIn a production environment, issues arising from this scenario would be mitigated by having multiple\n\ninstances of the inventory service running behind some load balancing, be it a load balancer or DNS\n\nload balancing. This would result in traffic being redirected away from the failed instance and over to\n\nthe others still in operation. For our experiment, however, a single instance is running, causing a\n\ncomplete failure of the service.\n\nHypothesis\n\nShutting down the inventory service will result in the following:\n\nAll metrics from the inventory container will stop reporting.\n\nErrors will be recorded by the grocery store; this should be visible through the request count per status code reporting status code\n\n500.\n\nLogs should report errors from the shopper container.\n\nUsing the following command, send a signal to shut down the inventory service. Note that docker\n\nkill sends the container a kill signal, whereas docker stop would send a term signal. We use kill\n\nhere to prevent the service from shutting down cleanly:\n\n$ docker kill inventory Verify\n\nWith the inventory service stopped, let's head over to the application metrics dashboard one last time\n\nto see what happened. The request count graph shows a rapid increase in requests whose response",
      "content_length": 2060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "code is 500, representing an internal server error.\n\nFigure 11.13 – The request counter shows an increase in errors\n\nOne signal we've yet to use in this chapter is logging. Look for the Logs panel at the bottom of the\n\napplication metrics dashboard to find all the logs emitted by our system. Specifically, look for an\n\nentry reporting a failed request to the grocery store such as the following, which is produced by the\n\nshopper application:\n\nFigure 11.14 – Log entry being recorded\n\nExpanding the log entry shows details about the event that caused an error. Unfortunately, the\n\nmessage request to grocery store failed isn't particularly helpful here, although notice that there\n\nis a TraceID field in the data shown. This field is adjacent to a link. Clicking on the link will take us\n\nto the corresponding trace in Jaeger, which shows us the following:",
      "content_length": 857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Figure 11.15 – Trace confirms the grocery store is unable to contact the inventory\n\nThe trace provides more context as to what error caused it to fail, which is helpful. An exception with\n\nthe message recorded in the span provides ample details about the legacy-inventory service\n\nappearing to be missing. Lastly, the container metrics dashboard will confirm the inventory container\n\nstopped reporting metrics as per the following screenshot:\n\nFigure 11.16 – Inventory container stopped reporting metrics\n\nRestore the stopped container via the docker start command and observe as the error rate drops and\n\ntraffic is returned to normal:\n\n$ docker start inventory\n\nThere are many more scenarios that we could investigate in this chapter. However, we only have\n\nlimited time to cover these. From message queues filling up to caching problems, the world is full of\n\nproblems just waiting to be uncovered.\n\nUsing telemetry first to answer questions\n\nThese experiments are a great way to gain familiarity with telemetry. Still, it feels like cheating to\n\nknow what caused a change before referring to the telemetry to investigate a problem. A more\n\ncommon way to use telemetry is to look at it when a problem occurs without intentionally causing it.\n\nUsually, this happens when deploying new code in a system.\n\nCode changes are deployed to many services in a distributed system several times a day. This makes\n\nit challenging to figure out which change is responsible for a regression. The complexity of\n\nidentifying problematic code is compounded by the updates being deployed by different teams.",
      "content_length": 1592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Update the image configuration for the shopper, grocery-store, and legacy-inventory services in the\n\nDocker Compose configuration to use the following:\n\ndocker-compose.yml\n\nshopper:\n\nimage: codeboten/shopper:chapter11-example1\n\n...\n\ngrocery-store:\n\nimage: codeboten/grocery-store:chapter11-example1\n\n...\n\nlegacy-inventory:\n\nimage: codeboten/legacy-inventory:chapter11-example1\n\nUpdate the containers by running the following command in a separate terminal:\n\n$ docker compose up -d legacy-inventory grocery-store shopper\n\nWas the deployment of the new code a success? Did we make things better or worse? Let's look at\n\nwhat the data shows us. Starting with the application metrics dashboard, it doesn't look promising.\n\nRequest duration has spiked upward, and requests per second dropped significantly.\n\nFigure 11.17 – Application metrics of the deployment\n\nIt appears to be impacting both the inventory service and grocery store service, which would indicate\n\nsomething may have gone wrong in the latest deployment of the inventory service. Looking at traces,",
      "content_length": 1059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "searching for all traces shows the same increase in request duration as the graphs from the metrics.\n\nSelecting a trace and looking through the details points to a likely culprit:\n\nFigure 11.18 – A suspicious span named sleepy service\n\nIt appears the addition of an operation called sleepy service is causing all sorts of problems in the\n\nlatest deployment! With this information, we can revert the change and resolve the issue.\n\nIn addition to the previous scenario, four additional scenarios are available through published\n\ncontainers to practice your observation skills. They have unoriginal tags: chapter11-example2,\n\nchapter11-example3, chapter11-example4, and chapter11-example5. I recommend trying them all\n\nbefore looking through the scenarios folder in the companion repository to see whether you can\n\nidentify the deployed problem!\n\nSummary\n\nLearning to navigate telemetry data produced by systems comfortably takes time. Even with years of\n\nexperience, the most knowledgeable engineers can still be puzzled by unexpected changes in\n\nobservability data. The more time spent getting comfortable with the tools, the quicker it will be to\n\nget to the bottom of just what caused changes in behavior.\n\nThe tools and techniques described in this chapter can be used repeatedly to better understand exactly\n\nwhat a system is doing. With chaos engineering practices, we can improve the resilience of our\n\nsystems by identifying areas that can be improved upon under controlled circumstances. By\n\nmethodically experimenting and observing the results from our hypotheses, we can measure the\n\nimprovements as we're making them.\n\nMany tools are available for experimenting and simulating failures; learning how to use these tools\n\ncan be a powerful addition to any engineer's toolset. As we worked our way through the vast amount\n\nof data produced by our instrumented system, it's clear that having a way to correlate data across\n\nsignals is critical in quickly moving through the data.",
      "content_length": 1985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "It's also clear that generating more data is not always a good thing, as it is possible to become\n\noverwhelmed quickly or overwhelm backends. The last chapter looks at how sampling can help\n\nreduce the volume of data.",
      "content_length": 217,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "Chapter 12: Sampling\n\nOne of the challenges of telemetry, in general, is managing the quantity of data that can be produced\n\nby instrumentation. This can be problematic at the time of generation if the tools producing telemetry\n\nconsume too many resources. It can also be costly to transfer the data across various points of the\n\nnetwork. And, of course, the more data is produced, the more storage it consumes, and the more\n\nresources are required to sift through it at the time of analysis. The last topic we'll discuss in this\n\nbook focuses on how we can reduce the amount of data produced by instrumentation while retaining\n\nthe value and fidelity of the data. To achieve this, we will be looking at sampling. Although primarily\n\na concern of tracing, sampling has an impact across metrics and logs as well, which we'll learn about\n\nthroughout this chapter. We'll look at the following areas:\n\nConcepts of sampling, including sampling strategies, across the different signals of OpenTelemetry\n\nHow to configure sampling at the application level via the OpenTelemetry Software Development Kit (SDK)\n\nUsing the OpenTelemetry collector to sample data\n\nAlong the way, we'll look at some common pitfalls of sampling to learn how they can best be\n\navoided. Let's start with the technical requirements for the chapter.\n\nTechnical requirements\n\nAll the code for the examples in the chapter is available in the companion repository, which can be\n\ndownloaded using git with the following command. The examples are under the chapter12 directory:\n\n$ git clone https://github.com/PacktPublishing/Cloud-Native-Observability $ cd Cloud-Native-\n\nObservability/chapter12\n\nThe first example in the chapter consists of an example application that uses the OpenTelemetry\n\nPython SDK to configure a sampler. To run the code, we'll need Python 3.6 or greater installed: $\n\npython --version\n\nPython 3.8.9\n\n$ python3 --version\n\nPython 3.8.9\n\nIf Python is not installed on your system, or the installed version of Python is less than the supported\n\nversion, follow the instructions from the Python website (https://www.python.org/downloads/) to\n\ninstall a compatible version.",
      "content_length": 2154,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Next, install the following OpenTelemetry packages via pip. Note that through dependency\n\nrequirements, additional packages will automatically be installed: $ pip install opentelemetry-distro \\\n\nopentelemetry-exporter-otlp\n\n$ pip freeze | grep opentelemetry\n\nopentelemetry-api==1.8.0\n\nopentelemetry-distro==0.27b0\n\nopentelemetry-exporter-otlp==1.8.0\n\nopentelemetry-exporter-otlp-proto-grpc==1.8.0\n\nopentelemetry-exporter-otlp-proto-http==1.8.0\n\nopentelemetry-instrumentation==0.27b0\n\nopentelemetry-proto==1.8.0\n\nopentelemetry-sdk==1.8.0\n\nThe second example will use the OpenTelemetry Collector, which can be downloaded from GitHub\n\ndirectly. The example will focus on the tail sampling processor, which currently resides in the\n\nopentelemetry-collector-contrib repository. The version used in this chapter can be found at the\n\nfollowing location: https://github.com/open-telemetry/opentelemetry-collector-\n\nreleases/releases/tag/v0.43.0. Download a binary that matches your current system from the available\n\nreleases. For example, the following command downloads the macOS for AMD64-compatible binary.\n\nIt also ensures the executable flag is set and runs the binary to check that things are working: $ wget -\n\nO otelcol.tar.gz https://github.com/open-telemetry/opentelemetry-collector-\n\nreleases/releases/download/v0.43.0/otelcol-contrib_0.43.0_darwin_amd64.tar.gz $ tar -xzf\n\notelcol.tar.gz otelcol-contrib\n\n$ chmod +x ./otelcol-contrib\n\n$ ./otelcol-contrib --version\n\notelcol-contrib version 0.43.0\n\nIf a package matching your environment isn't available, you can compile the collector manually. The\n\nsource is available on GitHub: https://github.com/open-telemetry/opentelemetry-collector-contrib.\n\nWith this in place, let's get started with sampling!\n\nConcepts of sampling across signals\n\nA method often used in the domain of research, the process of sampling selects a subset of data points\n\nacross a larger dataset to reduce the amount of data to be analyzed. This can be done because either\n\nanalyzing the entire dataset would be impossible, or unnecessary to achieve the research goal, or",
      "content_length": 2097,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "because it would be impractical to do so. For example, if we wanted to record how many doors on\n\naverage each car in a store parking lot has, it may be possible to go through the entire parking lot and\n\nrecord the data in its entirety. However, if the parking lot contains 20,000 cars, it may be best to\n\nselect a sample of those cars, say 2,000, and analyze that instead. There are many sampling methods\n\nused to ensure that a representational subset of the data is selected, to ensure the meaning of the data\n\nis not lost because of the sampling.\n\nMethods for sampling can be grouped as either of the following:\n\nProbabilistic (https://en.wikipedia.org/wiki/Probability_sampling): The probability of sampling is a known quantity, and that\n\nquantity is applied across all the data points in the dataset. Returning to the parking lot example, a probabilistic strategy would be\n\nto sample 10% of all cars. To accomplish this, we could record the data for every tenth car parked. In small datasets, probabilistic\n\nsampling is less effective as the variability between data points is higher.\n\nNon-probabilistic (https://en.wikipedia.org/wiki/Nonprobability_sampling): The selection of data is based on specific\n\ncharacteristics of the data. An example of this may be to choose the 2,000 cars closest to the store out of convenience. This\n\nintroduces bias into the selection process. The parking area located closest to the store may include designated spots or even spots\n\nreserved for smaller cars, therefore impacting the results.\n\nTraces\n\nSpecifically, sampling in the context of OpenTelemetry really means deciding what to do with spans\n\nthat form a particular trace. Spans in a trace are either processed or dropped, depending on the\n\nconfiguration of the sampler. Various components of OpenTelemetry are involved in carrying the\n\ndecision throughout the system:\n\nA Sampler is the starting point, allowing users to select a sampling level. Several samplers are defined in the OpenTelemetry\n\nspecification, more on this shortly.\n\nThe TracerProvider class receives a sampler as a configuration parameter. This ensures that all traces produced by the\n\nTracer provided by a specific TracerProvider are sampled consistently.\n\nOnce a trace is created, a decision is made on whether to sample the trace. This decision is stored in the SpanContext\n\nassociated with all spans in this trace. The sampling decision is propagated to all the services participating in the distributed trace\n\nvia the Propagator configured.\n\nFinally, once a span has ended, the SpanProcessor applies the sampling decision. It passes the spans for all sampled traces to\n\nthe SpanExporter. Traces that are not sampled are not exported.\n\nMetrics\n\nFor certain types of data, sampling just doesn't work. Sampling in the case of metrics may severely\n\nalter the data, rendering it effectively useless. For example, imagine recording data for each incoming\n\nrequest to a service, incrementing a counter by one with each request. Sampling this data would mean",
      "content_length": 3020,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "that any increment that is not sampled would result in unaccounted requests. Values recorded as a\n\nresult would lose the meaning of the original data.\n\nA single metric data point is smaller than a single trace. This means that typically, managing metrics\n\ndata creates less overhead to process and store. I say typically here because this depends on many\n\nfactors, such as the dimensions of the data and the frequency at which data points are collected.\n\nReducing the amount of data produced by the metrics signal focuses on aggregating the data, which\n\nreduces the number of data points transmitted. It does this by combining data points rather than\n\nselecting specific points and discarding others. There is, however, one aspect of metrics where\n\nsampling comes into play: exemplars. If you recall from Chapter 2, OpenTelemetry Signals – Traces,\n\nMetrics, and Logs, exemplars are data points that allow metrics to be correlated with traces. There is\n\nno need to produce exemplars that reference unsampled traces. The details of how exemplars and\n\ntheir sampling should be configured are still being discussed in the OpenTelemetry specification as of\n\nDecember 2021. It is good to be aware that this will be a feature of OpenTelemetry in the near future.\n\nLogs\n\nAt the time of writing, there is no specification in OpenTelemetry around if or how the logging signal\n\nshould be sampled. The following shows a couple of ways that are currently being considered:\n\nOpenTelemetry provides the ability for logs to be correlated with traces. As such, it may make sense to provide a configuration\n\noption to only emit log records that are correlated with sampled traces.\n\nLog records could be sampled in the same way that traces can be configured via a sampler, to only emit a fraction of the total logs\n\n(https://github.com/open-telemetry/opentelemetry-specification/issues/2237).\n\nAn alternative to sampling for logging is aggregation. Log records that contain the same message\n\ncould be aggregated and transmitted as a single record, which could include a counter of repeated\n\nevents. As these options are purely speculative, we won't focus any additional efforts on sampling\n\nand logging in this chapter.\n\nBefore diving into the code and what samplers are available, let's get familiar with some of the\n\nsampling strategies available.\n\nSampling strategies\n\nWhen deciding on how to best configure sampling for a distributed system, the strategy selected\n\noften depends on the environment. Depending on the strategy chosen, the sampling decision is made\n\nat different points in the system, as shown in the following diagram:",
      "content_length": 2618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "Figure 12.1 – Different points at which sampling decisions can take place The previous diagram shows where the\n\ndecisions to sample are made, but before choosing a strategy, we must understand what they are and when they are\n\nappropriate.\n\nHead sampling\n\nThe quickest way to decide about a trace is to decide at the very beginning whether to drop it or not;\n\nthis is known as head sampling. The application that creates the first span in a trace, the root span,\n\ndecides whether to sample the trace or not, and propagates that decision via the context to every\n\nsubsequent service called. This signals to all other participants in the trace whether they should be\n\nsending this span to a backend.\n\nHead sampling reduces the overhead for the entire system, as each application can discard\n\nunnecessary spans without computing a sampling decision. It also reduces the amount of data\n\ntransmitted, which can have a significant impact on network costs.\n\nAlthough it is the most efficient way to sample data, deciding at the beginning of the trace whether it\n\nshould be sampled or not doesn't always work. As we'll see shortly, when exploring the different\n\nsamplers available, it's possible for applications to configure sampling differently from one another.\n\nThis could cause applications to not respect the decision made by the root span, causing broken traces\n\nto be received by the backend. Figure 12.2 shows five applications interacting and combining into a\n\ndistributed system producing spans. It highlights what would happen if two applications, B and C,\n\nwere configured to sample a trace, but the other applications in the system were not:",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "Figure 12.2 – Inconsistent sampling configuration The backend would receive four spans and some context about the\n\nsystem but would be missing four additional spans and quite a bit of information.\n\nIMPORTANT NOTE\n\nInconsistent sampler configuration is a problem that affects all sampling strategies. Configuring multiple applications in a\n\ndistributed system introduces the possibility of inconsistencies. Using a consistent sampling configuration across\n\napplications is critical.\n\nMaking a sampling decision at the very beginning of a trace can also cause valuable information to\n\nbe missed. Continuing with the example from the previous diagram, if an error occurs in application\n\nD, but the sampling decision made by application A discards the trace, that error would not be\n\nreported to the backend. An inherent problem with head sampling is that the decision is made before\n\nall the information is available.\n\nTail sampling\n\nIf making the decision at the beginning of a trace is problematic because of a lack of information,\n\nwhat about making the decision at the end of a trace? Tail sampling is another common strategy that",
      "content_length": 1131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "waits until a trace is complete before making a sampling decision. This allows the sampler to perform\n\nsome analysis on the trace to detect potentially anomalous or interesting occurrences.\n\nWith tail sampling, all the applications in a distributed system must produce and transmit the\n\ntelemetry to a destination that decides to sample the data or not. This can become costly for large\n\ndistributed systems. Depending on where the tail sampling is performed, this option may cause\n\nsignificant amounts of data to be produced and transferred over the network, which could have little\n\nvalue.\n\nAdditionally, to make sampling decisions, the sampler must buffer in memory or store the data for the\n\nentire trace until it is ready to decide. This will inevitably lead to an increase in memory and storage\n\nconsumed, depending on the size and duration of traces. As mitigation around memory concerns, a\n\nmaximum trace duration can be configured in tail sampling. However, this leads to data gaps for any\n\ntraces that never finish within that set time. This is problematic as those traces can help identify\n\nproblems within a system.\n\nProbability sampling\n\nAs discussed earlier in the chapter, probability sampling ensures that data is selected randomly,\n\nremoving bias from the data sampled. Probability sampling is somewhat different from head and tail\n\nsampling, as it is both a configuration that can be applied to those other strategies and a strategy in\n\nitself. The sampling decision can be made by each component in the system individually, so long as\n\nthe components share the same algorithm for applying the probability. In OpenTelemetry, the\n\nTraceIdRatioBased sampler (https://github.com/open-telemetry/opentelemetry-\n\nspecification/blob/main/specification/trace/sdk.md#traceidratiobased) combined with the standard\n\nrandom trace ID generator provides a mechanism for probability sampling. The decision to sample is\n\ncalculated by applying a configurable ratio to a hash of the trace ID. Since the trace ID is propagated\n\nacross the system, all components configured with the same ratio and the TraceIdRatioBased sampler\n\ncan apply the same logic at decision time independently:",
      "content_length": 2184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Figure 12.3 – Probabilistic sampling decisions can be applied at every step of the system There are other sampling\n\nstrategies available, but these are the ones we'll concern ourselves with for the remainder of this chapter.\n\nSamplers available\n\nThere are a few different options when choosing a sampler. The following options are defined in the\n\nOpenTelemetry specification and are available in all implementations:\n\nAlways on: As the name suggests, the always_on sampler samples all traces.\n\nAlways off: This sampler does not sample any traces.\n\nTrace ID ratio: The trace ID ratio sampler, as discussed earlier, is a type of probability sampler available in OpenTelemetry.\n\nParent-based: The parent-based sampler is a sampler that supports the head sampling strategy. The parent-based sampler can be\n\nconfigured with always on, always_off, or with a trace ID ratio decision as a fallback, when a sampling decision has not\n\nalready been made for a trace.\n\nUsing the OpenTelemetry Python SDK will give us a chance to put these samplers to use.\n\nSampling at the application level via the SDK\n\nAllowing applications to decide what to sample, provides a great amount of flexibility to application\n\ndevelopers and operators, as these applications are the source of the tracing data. Samplers can be\n\nconfigured in OpenTelemetry as a property of the tracer provider. In the following code, a\n\nconfigure_tracer method configures the OpenTelemetry tracing pipeline and receives Sampler as a",
      "content_length": 1483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "method argument. This method is used to obtain three different tracers, each with its own sampling\n\nconfiguration:\n\nALWAYS_ON: A sampler that always samples.\n\nALWAYS_OFF: A sampler that never samples.\n\nTraceIdRatioBased: A probability sampler, which in the example is configured to sample traces 50% of the time.\n\nThe code then produces a separate trace using each tracer to demonstrate how sampling impacts the\n\noutput generated by ConsoleSpanExporter: sample.py\n\nfrom opentelemetry.sdk.trace import TracerProvider from opentelemetry.sdk.trace.export\n\nimport BatchSpanProcessor, ConsoleSpanExporter from opentelemetry.sdk.trace.sampling\n\nimport ALWAYS_OFF, ALWAYS_ON, TraceIdRatioBased def configure_tracer(sampler):\n\nprovider = TracerProvider(sampler=sampler)\n\nprovider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter())) return\n\nprovider.get_tracer(__name__)\n\nalways_on_tracer = configure_tracer(ALWAYS_ON) always_off_tracer =\n\nconfigure_tracer(ALWAYS_OFF) ratio_tracer = configure_tracer(TraceIdRatioBased(0.5)) with\n\nalways_on_tracer.start_as_current_span(\"always-on\") as span: span.set_attribute(\"sample\",\n\n\"always sampled\") with always_off_tracer.start_as_current_span(\"always-off\") as span:\n\nspan.set_attribute(\"sample\", \"never sampled\") with\n\nratio_tracer.start_as_current_span(\"ratio\") as span: span.set_attribute(\"sample\",\n\n\"sometimes sampled\") Run the code using the following command:\n\n$ python sample.py\n\nThe output should do one of the following:\n\nContain a trace with a span named always-on.\n\nNot contain a trace with a span named always-off.\n\nMaybe contain a trace with a span named ratio. You may need to run the code a few times to get this trace to produce output.\n\nThe following sample output is abbreviated to only show the name of the span and significant\n\nattributes: output\n\n{\n\n\"name\": \"ratio\",\n\n\"attributes\": {\n\n\"sample\": \"sometimes sampled\"\n\n},\n\n}\n\n{\n\n\"name\": \"always-on\",",
      "content_length": 1909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "\"attributes\": {\n\n\"sample\": \"always sampled\"\n\n},\n\n}\n\nNote that although the example configures three different samplers, a real-world application would\n\nonly ever use one sampler. An exception to this is a single application containing multiple services\n\nwith separate sampling requirements.\n\nNOTE\n\nIn addition to configuring a sampler via code, it's also possible to configure it via the OTEL_TRACES_SAMPLER and\n\nOTEL_TRACES_SAMPLER_ARG environment variables.\n\nUsing application configuration allows us to use head sampling, but individual applications don't\n\nhave the information needed to make tail sampling decisions. For that, we need to go further down\n\nthe pipeline.\n\nUsing the OpenTelemetry Collector to sample data\n\nConfiguring the application to sample traces is great, but what if we wanted to use tail sampling\n\ninstead? The OpenTelemetry Collector provides a natural point where sampling can be performed.\n\nToday, it supports both tail sampling and probabilistic sampling via processors. As we've already\n\ndiscussed the probabilistic sampling processor in Chapter 8, The OpenTelemetry Collector, we'll\n\nfocus this section on the tail sampling processor.\n\nTail sampling processor\n\nIn addition to supporting the configuration of sampling via specifying a probabilistic sampling\n\npercentage, the tail sampling processor can make sampling decisions based on a variety of\n\ncharacteristics of a trace. It can choose to sample based on one of the following:\n\nOverall trace duration\n\nSpan attributes' values\n\nStatus code of a span\n\nTo accomplish this, the tail sampling processor supports the configuration of policies to sample\n\ntraces. To better understand how tail sampling can impact the tracing data produced by configuring a\n\nvariety of policies in the collector, let's look at the following code snippet, which configures a\n\ncollector with the following:",
      "content_length": 1865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "The OpenTelemetry protocol listener, which will receive the telemetry from an example application\n\nA logging exporter to allow us to see the tracing data in the terminal\n\nThe tail sampling processor with a policy to always sample all traces\n\nThe following code snippet contains the elements of the previous list: config/collector/config.yml\n\nreceivers:\n\notlp:\n\nprotocols:\n\ngrpc:\n\nexporters:\n\nlogging:\n\nloglevel: debug\n\nprocessors:\n\ntail_sampling:\n\ndecision_wait: 5s\n\npolicies: [{ name: always, type: always_sample }]\n\nservice:\n\npipelines:\n\ntraces:\n\nreceivers: [otlp]\n\nprocessors: [tail_sampling]\n\nexporters: [logging]\n\nStart the collector using the following command, which includes the configuration previously shown:\n\n$ ./otelcol-contrib --config ./config/collector/config.yml Next, the ensuing code is an application that\n\nwill send multiple traces to the collector to demonstrate some of the capabilities of the tail sampling\n\nprocessor: multiple_traces.py import time\n\nfrom opentelemetry import trace\n\ntracer = trace.get_tracer_provider().get_tracer(__name__) with\n\ntracer.start_as_current_span(\"slow-span\"): time.sleep(1)\n\nfor i in range(0, 20):\n\nwith tracer.start_as_current_span(\"fast-span\"): pass\n\nOpen a new terminal and start the program using OpenTelemetry auto-instrumentation, as per the\n\nfollowing command: $ opentelemetry-instrument python multiple_traces.py Looking through the\n\noutput in the collector terminal, you should see a total of 21 traces being emitted. Let's now update",
      "content_length": 1497,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "the collector configuration to only sample 10% of all traces. This can be configured via a policy, as\n\nper the following: config/collector/config.yml processors:\n\ntail_sampling:\n\ndecision_wait: 5s\n\npolicies:\n\n[\n\n{\n\nname: probability,\n\ntype: probabilistic,\n\nprobabilistic: { sampling_percentage: 10 },\n\n},\n\n]\n\nRestart the collector and run multiple_traces.py once more to see the effects of applying the new\n\npolicy. The results should show roughly 10% of traces, which in this case would be about two traces.\n\nI say roughly here because the configuration relies on probabilistic sampling using the trace\n\nidentifier. Since the trace ID is randomly generated, there is some variance in the results with such a\n\nsmall sample set. Run the command a few times if needed to see the sampling policy in action: output\n\nSpan #0\n\nTrace ID : 9581c95ae58bc8368050728f50c32f73\n\nParent ID :\n\nID : b9c3fb8838eb0f33\n\nName : fast-span\n\nKind : SPAN_KIND_INTERNAL\n\nStart time : 2021-12-28 21:29:01.144907 +0000 UTC\n\nEnd time : 2021-12-28 21:29:01.144922 +0000 UTC\n\nStatus code : STATUS_CODE_UNSET\n\nStatus message :\n\nSpan #0\n\nTrace ID : 2a8950f2365e515324c62dfdc23735ba Parent ID :\n\nID : c5217fb16c4d90ff\n\nName : fast-span\n\nKind : SPAN_KIND_INTERNAL\n\nStart time : 2021-12-28 21:29:01.14498 +0000 UTC\n\nEnd time : 2021-12-28 21:29:01.144996 +0000 UTC",
      "content_length": 1329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "Status code : STATUS_CODE_UNSET\n\nStatus message :\n\nNote that in the previous output, only the spans named fast-span were emitted. It's unfortunate,\n\nbecause the information about slow-span may be more useful to us. It's additionally possible to\n\nconfigure the tail sampling processor to combine policies to create more complex sampling decisions.\n\nFor example, you may want to continue capturing only 10% of all traces but always capture traces\n\nrepresenting operations that took longer than 1 second to complete. In this case, the following\n\ncombination of a latency-based policy with a probabilistic policy would make this possible:\n\nconfig/collector/config.yml processors:\n\ntail_sampling:\n\ndecision_wait: 5s\n\npolicies:\n\n[\n\n{\n\nname: probability,\n\ntype: probabilistic,\n\nprobabilistic: { sampling_percentage: 10 },\n\n},\n\n{ name: slow, type: latency, latency: { threshold_ms: 1000 } }, ]\n\nRestart the collector one last time and run the example code. You'll notice that both a percentage of\n\ntraces and the trace containing slow-span are visible in the output from the collector. There are other\n\ncharacteristics that can be configured, but this gives you an idea of how the tail sampling processor\n\nworks. Another example is to base the sampling decision on the status code, which is a convenient\n\nway to capture errors in a system. Another yet is to sample custom attributes, which could be used to\n\nscope the sampling to specific systems.\n\nIMPORTANT NOTE\n\nChoosing to sample traces on known characteristics introduces bias in the selection of spans that could inadvertently hide\n\nuseful telemetry. Tread carefully when configuring sampling to use non-probabilistic data as it may exclude more\n\ninformation than you'd like. Combining probabilistic and non-probabilistic sampling, as in the previous example, allows us\n\nto work around this limitation.\n\nSummary\n\nUnderstanding the different options for sampling provides us with the ability to manage the amount\n\nof data produced by our applications. Knowing the trade-offs of different sampling strategies and",
      "content_length": 2058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "some of the methods available helps decrease the level of noise in a busy environment.\n\nThe OpenTelemetry configuration and samplers available to configure sampling at the application\n\nlevel can help reduce the load and cost upfront in systems via head sampling. Configuring tail\n\nsampling at collection time provides the added benefit of making a more informed decision on what\n\nto keep or discard. This benefit comes at the added cost of having to run a collection point with\n\nsufficient resources to buffer the data until a decision can be reached.\n\nUltimately, the decisions made when configuring sampling will impact what data is available to\n\nobserve what is happening in a system. Sample too little and you may miss important events. Sample\n\ntoo much and the cost of producing telemetry for a system may be too high or the data too noisy to\n\nsearch through. Sample only for known issues and you may miss the opportunity to find\n\nabnormalities you didn't even know about.\n\nDuring development, sampling 100% of the data makes sense as the volume is low. In production, a\n\nmuch smaller percentage of data, under 10%, is often representative of the data as a whole.\n\nThe information in this chapter has given us an understanding of the concepts of sampling. It has also\n\ngiven us an idea of the trade-offs in choosing different sampling strategies. In the end, choosing the\n\nright strategy requires experimenting and tweaking as we learn more about our systems.",
      "content_length": 1464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "Packt.com\n\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as\n\nindustry leading tools to help you plan your personal development and advance your career. For more\n\ninformation, please visit our website.\n\nWhy subscribe?\n\nSpend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\nFully searchable for easy access to vital information\n\nCopy and paste, print, and bookmark content\n\nDid you know that Packt offers eBook versions of every book published, with PDF and ePub files\n\navailable? You can upgrade to the eBook version at packt.com and as a print book customer, you are\n\nentitled to a discount on the eBook copy. Get in touch with us at customercare@packtpub.com for\n\nmore details.\n\nAt www.packt.com, you can also read a collection of free technical articles, sign up for a range of\n\nfree newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\n\nOther Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other books by Packt:",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "Cloud Native with Kubernetes\n\nAlexander Raul\n\nISBN: 9781838823078\n\nSet up Kubernetes and configure its authentication\n\nDeploy your applications to Kubernetes",
      "content_length": 157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "Configure and provide storage to Kubernetes applications\n\nExpose Kubernetes applications outside the cluster\n\nControl where and how applications are run on Kubernetes\n\nSet up observability for Kubernetes\n\nBuild a continuous integration and continuous deployment (CI/CD) pipeline for Kubernetes\n\nExtend Kubernetes with service meshes, serverless, and more",
      "content_length": 354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "The Kubernetes Bible\n\nNassim Kebbani, Piotr Tylenda, Russ McKendrick\n\nISBN: 9781838827694\n\nManage containerized applications with Kubernetes\n\nUnderstand Kubernetes architecture and the responsibilities of each component\n\nSet up Kubernetes on Amazon Elastic Kubernetes Service, Google Kubernetes Engine, and Microsoft Azure Kubernetes Service\n\nDeploy cloud applications such as Prometheus and Elasticsearch using Helm charts\n\nDiscover advanced techniques for Pod scheduling and auto-scaling the cluster\n\nUnderstand possible approaches to traffic routing in Kubernetes\n\nPackt is searching for authors like you\n\nIf you're interested in becoming an author for Packt, please visit authors.packtpub.com and apply\n\ntoday. We have worked with thousands of developers and tech professionals, just like you, to help\n\nthem share their insight with the global tech community. You can make a general application, apply\n\nfor a specific hot topic that we are recruiting an author for, or submit your own idea.\n\nShare Your Thoughts\n\nNow you've finished Cloud-Native Observability with OpenTelemetry, we'd love to hear your\n\nthoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon\n\nreview page for this book and share your feedback or leave a review on the site that you purchased it\n\nfrom.\n\nYour review is important to us and the tech community and will help us make sure we're delivering\n\nexcellent quality content.",
      "content_length": 1440,
      "extraction_method": "Unstructured"
    }
  ]
}