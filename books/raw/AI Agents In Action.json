{
  "metadata": {
    "title": "AI Agents In Action",
    "author": "Micheal Lanham",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 346,
    "conversion_date": "2025-11-28T12:01:17.688602",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "AI Agents In Action.pdf",
    "extraction_method": "PyMuPDF (Direct: 345, OCR: 1)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "M A N N I N G\nMicheal Lanham\n\n\nThe differences between the LLM interactions from direct action \ncompared to using proxy agents, agents, and autonomous agents\nPlease explain the\nde\ufb01nition of agent.\nLarge language model\n(ChatGPT)\nLLM: The de\ufb01nition\nof agent is...\nShow an illustration\nof an agent.\nLarge language model\n(ChatGPT)\n\"An image of a female\nsecret agent of Hispanic\ndescent in a nighttime\nurban setting. . .\nImage generation model\n(DALL-E 3)\nNo agent or assistant\ndirect connection to LLM\nAgent/assistant proxy for\nimage generator\nWhat is the temperature\nin Calgary today?\nLarge language model\n(ChatGPT)\nLLM identi\ufb01es an external\nfunction API to call and\nparameters to connect\nto a weather service.\nAgent/assistant acting on\nbehalf of user\nUser con\ufb01rms\nexecution okay.\nAsks user if it\u2019s okay\nto execute the function\non their behalf.\nExecutes the function\nand returns weather\ninformation.\nFilter my emails by\nimportance and notify\nme of the top 5 most\nimportant emails.\nLarge language model\n(ChatGPT)\nLLM identi\ufb01es an external\nfunction API to call and\nparameters to connect\nto an email service.\nNoti\ufb01es the user of\nimportant emails.\nAutonomous agent making\ndecisions on behalf of user\nLLM reads\nand sorts emails by\nwhat it deems to be\nimportant.\nDecision step\nLLM reformulates\nweather information and\nresponds to the user.\n.\n\n\nAI Agents in Action\n\n\nAI Agents in Action\nMICHEAL LANHAM\nM A N N I N G\nSHELTER ISLAND\n\n\nFor online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n\u00a92025 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning Publications \nwas aware of a trademark claim, the designations have been printed in initial caps or all caps.\nRecognizing the importance of preserving what has been written, it is Manning\u2019s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nThe authors and publisher have made every effort to ensure that the information in this book \nwas correct at press time. The authors and publisher do not assume and hereby disclaim any \nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether \nsuch errors or omissions result from negligence, accident, or any other cause, or from any usage \nof the information herein.\nManning Publications Co.\nDevelopment editor: Becky Whitney\n20 Baldwin Road\nTechnical editor: Ross Turner\nPO Box 761\nReview editor: Kishor Rit\nShelter Island, NY 11964\nProduction editor: Keri Hales\nCopy editor: Julie McNamee\nProofreader: Katie Tennant\nTechnical proofreader: Ross Turner\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781633436343\nPrinted in the United States of America\n\n\n I dedicate this book to all the readers who embark on this journey with me. \nBooks are a powerful way for an author to connect with readers on a deeply personal \nlevel, chapter by chapter, page by page. In that shared experience of learning, \nexploring, and growing together, I find true meaning. May this book inspire you \nand challenge you, and help you see the incredible potential that AI agents hold\u2014\nnot just for the future but also for today.\n\n\nvivi\nbrief contents\n1\n\u25a0\nIntroduction to agents and their world\n1\n2\n\u25a0\nHarnessing the power of large language models\n14\n3\n\u25a0\nEngaging GPT assistants\n39\n4\n\u25a0\nExploring multi-agent systems\n68\n5\n\u25a0\nEmpowering agents with actions\n98\n6\n\u25a0\nBuilding autonomous assistants\n129\n7\n\u25a0\nAssembling and using an agent platform\n160\n8\n\u25a0\nUnderstanding agent memory and knowledge\n180\n9\n\u25a0\nMastering agent prompts with prompt flow\n212\n10\n\u25a0\nAgent reasoning and evaluation\n244\n11\n\u25a0\nAgent planning and feedback\n272\n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary",
      "content": "vii\ncontents\npreface\nxiii\nacknowledgments\nxv\nabout this book\nxvii\nabout the author\nxxi\nabout the cover illustration\nxxii\n1 Introduction to agents and their world\n1\n1.1\nDefining agents\n1\n1.2\nUnderstanding the component systems of an agent\n4\n1.3\nExamining the rise of the agent era: Why agents?\n9\n1.4\nPeeling back the AI interface\n11\n1.5\nNavigating the agent landscape\n12\n2 Harnessing the power of large language models\n14\n2.1\nMastering the OpenAI API\n16\nConnecting to the chat completions model\n16\n\u25a0Understanding the \nrequest and response\n18\n2.2\nExploring open source LLMs with LM Studio\n20\nInstalling and running LM Studio\n20\n\u25a0Serving an LLM locally \nwith LM Studio\n23\n\n\nCONTENTS\nviii\n2.3\nPrompting LLMs with prompt engineering\n25\nCreating detailed queries\n28\n\u25a0Adopting personas\n29\nUsing delimiters\n30\n\u25a0Specifying steps\n31\n\u25a0Providing \nexamples\n32\n\u25a0Specifying output length\n33\n2.4\nChoosing the optimal LLM for your specific needs\n34\n2.5\nExercises\n36\n3 Engaging GPT assistants\n39\n3.1\nExploring GPT assistants through ChatGPT\n40\n3.2\nBuilding a GPT that can do data science\n44\n3.3\nCustomizing a GPT and adding custom actions\n49\nCreating an assistant to build an assistant\n49\n\u25a0Connecting the \ncustom action to an assistant\n53\n3.4\nExtending an assistant\u2019s knowledge using file uploads\n56\nBuilding the Calculus Made Easy GPT\n56\n\u25a0Knowledge search \nand more with file uploads\n58\n3.5\nPublishing your GPT\n61\nExpensive GPT assistants\n62\n\u25a0Understanding the economics of \nGPTs\n63\n\u25a0Releasing the GPT\n63\n3.6\nExercises\n65\n4 Exploring multi-agent systems\n68\n4.1\nIntroducing multi-agent systems with AutoGen Studio\n69\nInstalling and using AutoGen Studio\n70\n\u25a0Adding skills in \nAutoGen Studio\n72\n4.2\nExploring AutoGen\n77\nInstalling and consuming AutoGen\n77\n\u25a0Enhancing code output \nwith agent critics\n79\n\u25a0Understanding the AutoGen cache\n81\n4.3\nGroup chat with agents and AutoGen\n82\n4.4\nBuilding an agent crew with CrewAI\n84\nCreating a jokester crew of CrewAI agents\n84\n\u25a0Observing agents \nworking with AgentOps\n87\n4.5\nRevisiting coding agents with CrewAI\n90\n4.6\nExercises\n95\n\n\nCONTENTS\nix\n5 Empowering agents with actions\n98\n5.1\nDefining agent actions\n99\n5.2\nExecuting OpenAI functions\n101\nAdding functions to LLM API calls\n101\n\u25a0Actioning function \ncalls\n103\n5.3\nIntroducing Semantic Kernel\n107\nGetting started with SK semantic functions\n108\n\u25a0Semantic \nfunctions and context variables\n109\n5.4\nSynergizing semantic and native functions\n111\nCreating and registering a semantic skill/plugin\n111\n\u25a0Applying \nnative functions\n115\n\u25a0Embedding native functions within \nsemantic functions\n117\n5.5\nSemantic Kernel as an interactive service agent\n118\nBuilding a semantic GPT interface\n119\n\u25a0Testing semantic \nservices\n121\n\u25a0Interactive chat with the semantic service \nlayer\n122\n5.6\nThinking semantically when writing semantic \nservices\n125\n5.7\nExercises\n127\n6 Building autonomous assistants\n129\n6.1\nIntroducing behavior trees\n130\nUnderstanding behavior tree execution\n131\n\u25a0Deciding on \nbehavior trees\n132\n\u25a0Running behavior trees with Python \nand py_trees\n134\n6.2\nExploring the GPT Assistants Playground\n136\nInstalling and running the Playground\n136\n\u25a0Using and \nbuilding custom actions\n138\n\u25a0Installing the assistants \ndatabase\n140\n\u25a0Getting an assistant to run code locally\n140\nInvestigating the assistant process through logs\n142\n6.3\nIntroducing agentic behavior trees\n143\nManaging assistants with assistants\n143\n\u25a0Building a coding \nchallenge ABT\n145\n\u25a0Conversational AI systems vs. other \nmethods\n149\n\u25a0Posting YouTube videos to X\n150\nRequired X setup\n151\n6.4\nBuilding conversational autonomous multi-agents\n153\n\n\nCONTENTS\nx\n6.5\nBuilding ABTs with back chaining\n155\n6.6\nExercises\n156\n7 Assembling and using an agent platform\n160\n7.1\nIntroducing Nexus, not just another agent platform\n161\nRunning Nexus\n162\n\u25a0Developing Nexus\n163\n7.2\nIntroducing Streamlit for chat application \ndevelopment\n165\nBuilding a Streamlit chat application\n165\n\u25a0Creating a streaming \nchat application\n168\n7.3\nDeveloping profiles and personas for agents\n170\n7.4\nPowering the agent and understanding the agent \nengine\n172\n7.5\nGiving an agent actions and tools\n174\n7.6\nExercises\n178\n8 Understanding agent memory and knowledge\n180\n8.1\nUnderstanding retrieval in AI applications\n181\n8.2\nThe basics of retrieval augmented generation (RAG)\n182\n8.3\nDelving into semantic search and document \nindexing\n184\nApplying vector similarity search\n184\n\u25a0Vector databases and \nsimilarity search\n188\n\u25a0Demystifying document embeddings\n189\nQuerying document embeddings from Chroma\n190\n8.4\nConstructing RAG with LangChain\n192\nSplitting and loading documents with LangChain\n192\n\u25a0Splitting \ndocuments by token with LangChain\n195\n8.5\nApplying RAG to building agent knowledge\n196\n8.6\nImplementing memory in agentic systems\n200\nConsuming memory stores in Nexus\n202\n\u25a0Semantic memory and \napplications to semantic, episodic, and procedural memory\n204\n8.7\nUnderstanding memory and knowledge \ncompression\n207\n8.8\nExercises\n209\n\n\nCONTENTS\nxi\n9 Mastering agent prompts with prompt flow\n212\n9.1\nWhy we need systematic prompt engineering\n213\n9.2\nUnderstanding agent profiles and personas\n216\n9.3\nSetting up your first prompt flow\n217\nGetting started\n218\n\u25a0Creating profiles with Jinja2 \ntemplates\n222\n\u25a0Deploying a prompt flow API\n223\n9.4\nEvaluating profiles: Rubrics and grounding\n224\n9.5\nUnderstanding rubrics and grounding\n228\n9.6\nGrounding evaluation with an LLM profile\n230\n9.7\nComparing profiles: Getting the perfect \nprofile\n232\nParsing the LLM evaluation output\n232\n\u25a0Running batch \nprocessing in prompt flow\n235\n\u25a0Creating an evaluation flow \nfor grounding\n238\n\u25a0Exercises\n242\n10 Agent reasoning and evaluation\n244\n10.1\nUnderstanding direct solution prompting\n245\nQuestion-and-answer prompting\n246\n\u25a0Implementing few-shot \nprompting\n248\n\u25a0Extracting generalities with zero-shot \nprompting\n250\n10.2\nReasoning in prompt engineering\n252\nChain of thought prompting\n253\n\u25a0Zero-shot CoT \nprompting\n257\n\u25a0Step by step with prompt chaining\n258\n10.3\nEmploying evaluation for consistent solutions\n261\nEvaluating self-consistency prompting\n262\n\u25a0Evaluating tree of \nthought prompting\n266\n10.4\nExercises\n270\n11 Agent planning and feedback\n272\n11.1\nPlanning: The essential tool for all agents/assistants\n273\n11.2\nUnderstanding the sequential planning process\n277\n11.3\nBuilding a sequential planner\n278\n11.4\nReviewing a stepwise planner: OpenAI Strawberry\n285\n\n\nCONTENTS\nxii\n11.5\nApplying planning, reasoning, evaluation, and feedback to \nassistant and agentic systems\n288\nApplication of assistant/agentic planning\n288\n\u25a0Application of \nassistant/agentic reasoning\n290\n\u25a0Application of evaluation to \nagentic systems\n291\n\u25a0Application of feedback to agentic/assistant \napplications\n293\n11.6\nExercises\n296\nappendix A\nAccessing OpenAI large language models\n299\nappendix B\nPython development environment\n305\nindex\n311\n\n\nxiii\npreface\nMy journey into the world of intelligent systems began back in the early 1980s. Like\nmany people then, I believed artificial intelligence (AI) was just around the corner. It\nalways seemed like one more innovation and technological leap would lead us to the\nintelligence we imagined. But that leap never came.\n Perhaps the promise of HAL, from Stanley Kubrick\u2019s 2001: A Space Odyssey, capti-\nvated me with the idea of a truly intelligent computer companion. After years of effort,\ntrial, and countless errors, I began to understand that creating AI was far more com-\nplex than we humans had imagined. In the early 1990s, I shifted my focus, applying\nmy skills to more tangible goals in other industries.\n Not until the late 1990s, after experiencing a series of challenging and transforma-\ntive events, did I realize my passion for building intelligent systems. I knew these sys-\ntems might never reach the superintelligence of HAL, but I was okay with that. I\nfound fulfillment in working with machine learning and data science, creating models\nthat could learn and adapt. For more than 20 years, I thrived in this space, tackling\nproblems that required creativity, precision, and a sense of possibility.\n During that time, I worked on everything from genetic algorithms for predicting\nunknown inputs to developing generative learning models for horizontal drilling in\nthe oil-and-gas sector. These experiences led me to write, where I shared my knowl-\nedge by way of books on various topics\u2014reverse-engineering Pok\u00e9mon Go, building\naugmented and virtual reality experiences, designing audio for games, and applying\nreinforcement learning to create intelligent agents. I spent years knuckles-deep in\ncode, developing agents in Unity ML-Agents and deep reinforcement learning.\n\n\nPREFACE\nxiv\n Even then, I never imagined that one day I could simply describe what I wanted to\nan AI model, and it would make it happen. I never imagined that, in my lifetime, I\nwould be able to collaborate with an AI as naturally as I do today. And I certainly never\nimagined how fast\u2014and simultaneously how slow\u2014this journey would feel.\n In November 2022, the release of ChatGPT changed everything. It changed the\nworld\u2019s perception of AI, and it changed the way we build intelligent systems. For me,\nit also altered my perspective on the capabilities of these systems. Suddenly, the idea\nof agents that could autonomously perform complex tasks wasn\u2019t just a far-off dream\nbut instead a tangible, achievable reality. In some of my earlier books, I had described\nagentic systems that could undertake specific tasks, but now, those once-theoretical\nideas were within reach.\n This book is the culmination of my decades of experience in building intelligent\nsystems, but it\u2019s also a realization of the dreams I once had about what AI could\nbecome. AI agents are here, poised to transform how we interact with technology, how\nwe work, and, ultimately, how we live.\n Yet, even now, I see hesitation from organizations when it comes to adopting agen-\ntic systems. I believe this hesitation stems not from fear of AI but rather from a lack\nof understanding and expertise in building these systems. I hope that this book\nhelps to bridge that gap. I want to introduce AI agents as tools that can be accessible\nto everyone\u2014tools we shouldn\u2019t fear but instead respect, manage responsibly, and\nlearn to work with in harmony.\n",
      "page_number": 9
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "detection_method": "topic_boundary",
      "content": "xv\nacknowledgments\nI want to extend my deepest gratitude to the machine learning and deep learning\ncommunities for their tireless dedication and incredible work. Just a few short years\nago, many questioned whether the field was headed for another AI winter\u2014a period\nof stagnation and doubt. But thanks to the persistence, brilliance, and passion of\ncountless individuals, the field not only persevered but also flourished. We\u2019re standing\non the threshold of an AI-driven future, and I am endlessly grateful for the contribu-\ntions of this talented community.\n Writing a book, even with the help of AI, is no small feat. It takes dedication, col-\nlaboration, and a tremendous amount of support. I am incredibly thankful to the\nteam of editors and reviewers who made this book possible. I want to express my\nheartfelt thanks to everyone who took the time to review and provide feedback. In\nparticular, I want to thank Becky Whitney, my content editor, and Ross Turner, my\ntechnical editor and chief production and technology officer at OpenSC, for their\ndedication, as well as the whole production team at Manning for their insight and\nunwavering support throughout this journey.\n To my partner, Rhonda\u2014your love, patience, and encouragement mean the world\nto me. You\u2019ve been the cornerstone of my support system, not just for this book but\nfor all the books that have come before. I truly couldn\u2019t have done any of this without\nyou. Thank you for being my rock, my partner, and my inspiration.\n Many of the early ideas for this book grew out of my work at Symend. It was during\nmy time there that I first began developing the concepts and designs for agentic sys-\ntems that laid the foundation for this book. I am deeply grateful to my colleagues at\n\n\nACKNOWLEDGMENTS\nxvi\nSymend for their collaboration and contributions, including Peh Teh, Andrew Wright,\nZiko Rajabali, Chris Garrett, Kouros, Fatemeh Torabi Asr, Sukh Singh, and Hanif\nJoshaghani. Your insights and hard work helped bring these ideas to life, and I am\nhonored to have worked alongside such an incredible group of people.\n Finally, I would like to thank all the reviewers: Anandaganesh Balakrishnan, Aryan\nJadon, Chau Giang, Dan Sheikh, David Curran, Dibyendu Roy Chowdhury, Divya\nBhargavi, Felipe Provezano Coutinho, Gary Pass, John Williams, Jose San Leandro,\nLaurence Giglio, Manish Jain, Maxim Volgin, Michael Wang, Mike Metzger, Piti\nChampeethong, Prashant Dwivedi, Radhika Kanubaddhi, Rajat Kant Goel, Ramaa\nVissa, Richard Vaughan, Satej Kumar Sahu, Sergio Gtz, Siva Dhandapani, Annamaneni\nSriharsha, Sri Ram Macharla, Sumit Bhattacharyya, Tony Holdroyd, Vidal Graupera,\nVidhya Vinay, and Vinoth Nageshwaran. Your suggestions helped make this a better\nbook.\n\n\nxvii\nabout this book\nAI Agents in Action is about building and working with intelligent agent systems\u2014not\njust creating autonomous entities but also developing agents that can effectively tackle\nand solve real-world problems. The book starts with the basics of working with large\nlanguage models (LLMs) to build assistants, multi-agent systems, and agentic behav-\nioral agents. From there, it explores the key components of agentic systems: retrieval\nsystems for knowledge and memory augmentation, action and tool usage, reasoning,\nplanning, evaluation, and feedback. The book demonstrates how these components\nempower agents to perform a wide range of complex tasks through practical examples.\n This journey isn\u2019t just about technology; it\u2019s about reimagining how we approach\nproblem solving. I hope this book inspires you to see intelligent agents as partners in\ninnovation, capable of transforming ideas into actions in ways that were once thought\nimpossible. Together, we\u2019ll explore how AI can augment human potential, enabling us\nto achieve far more than we could alone.\nWho should read this book\nThis book is for anyone curious about intelligent agents and how to develop agentic\nsystems\u2014whether you\u2019re building your first helpful assistant or diving deeper into\ncomplex multi-agent systems. No prior experience with agents, agentic systems,\nprompt engineering, or working with LLMs is required. All you need is a basic under-\nstanding of Python and familiarity with GitHub repositories. My goal is to make these\nconcepts accessible and engaging, empowering anyone who wants to explore the\nworld of AI agents to do so with confidence.\n\n\nABOUT THIS BOOK\nxviii\n Whether you\u2019re a developer, researcher, or hobbyist or are simply intrigued by the\npossibilities of AI, this book is for you. I hope that in these pages you\u2019ll find inspira-\ntion, practical guidance, and a new appreciation for the remarkable potential of intel-\nligent agents. Let this book guide understanding, creating, and unleashing the power\nof AI agents in action.\nHow this book is organized: A road map\nThis book has 11 chapters. Chapter 1, \u201cIntroduction to agents and their world,\u201d\nbegins by laying a foundation with fundamental definitions of large language models,\nchat systems, assistants, and autonomous agents. As the book progresses, the discus-\nsion shifts to the key components that make up an agent and how these components\nwork together to create truly effective systems. Here is a quick summary of chapters 2\nthrough 11:\n\uf0a1Chapter 2, \u201cHarnessing the power of large language models\u201d\u2014We start by exploring\nhow to use commercial LLMs, such as OpenAI. We then examine tools, such as\nLM Studio, that provide the infrastructure and support for running various\nopen source LLMs, enabling anyone to experiment and innovate.\n\uf0a1Chapter 3, \u201cEngaging GPT assistants\u201d\u2014This chapter dives into the capabilities of\nthe GPT Assistants platform from OpenAI. Assistants are foundational agent\ntypes, and we explore how to create practical and diverse assistants, from culi-\nnary helpers to intern data scientists and even a book learning assistant.\n\uf0a1Chapter 4, \u201cExploring multi-agent systems\u201d\u2014Agentic tools have advanced signifi-\ncantly quickly. Here, we explore two sophisticated multi-agent systems: CrewAI\nand AutoGen. We demonstrate AutoGen\u2019s ability to develop code autono-\nmously and see how CrewAI can bring together a group of joke researchers to\ncreate humor collaboratively.\n\uf0a1Chapter 5, \u201cEmpowering agents with actions\u201d\u2014Actions are fundamental to any\nagentic system. This chapter discusses how agents can use tools and functions to\nexecute actions, ranging from database and application programming interface\n(API) queries to generating images. We focus on enabling agents to take mean-\ningful actions autonomously.\n\uf0a1Chapter 6, \u201cBuilding autonomous assistants\u201d\u2014We explore the behavior tree\u2014a sta-\nple in robotics and game systems\u2014as a mechanism to orchestrate multiple\ncoordinated agents. We\u2019ll use behavior trees to tackle challenges such as code\ncompetitions and social media content creation.\n\uf0a1Chapter 7, \u201cAssembling and using an agent platform\u201d\u2014This chapter introduces Nexus,\na sophisticated platform for orchestrating multiple agents and LLMs. We discuss\nhow Nexus facilitates agentic workflows and enables complex interactions between\nagents, providing an example of a fully functioning multi-agent environment.\n\uf0a1Chapter 8, \u201cUnderstanding agent memory and knowledge\u201d\u2014Retrieval-augmented\ngeneration (RAG) has become an essential tool for extending the capabilities\n\n\nABOUT THIS BOOK\nxix\nof LLM agents. This chapter explores how retrieval mechanisms can serve as\nboth a source of knowledge by processing ingested files, and of memory, allow-\ning agents to recall previous interactions or events.\n\uf0a1Chapter 9, \u201cMastering agent prompts with prompt flow\u201d\u2014Prompt engineering is cen-\ntral to an agent\u2019s success. This chapter introduces prompt flow, a tool from Mic-\nrosoft that helps automate the testing and evaluation of prompts, enabling\nmore robust and effective agentic behavior.\n\uf0a1Chapter 10, \u201cAgent reasoning and evaluation\u201d\u2014Reasoning is crucial to solving\nproblems intelligently. In this chapter, we explore various reasoning techniques,\nsuch as chain of thought (CoT), and show how agents can evaluate reasoning\nstrategies even during inference, improving their capacity to solve problems\nautonomously.\n\uf0a1Chapter 11, \u201cAgent planning and feedback\u201d\u2014Planning is perhaps an agent\u2019s most crit-\nical skill in achieving its goals. We discuss how agents can incorporate planning to\nnavigate complex tasks and how feedback loops can be used to refine those plans.\nThe chapter concludes by integrating all the key components\u2014actions, memory\nand knowledge, reasoning, evaluation, planning, and feedback\u2014into practical\nexamples of agentic systems that solve real-world problems.\nAbout the code\nThe code for this book is spread across several open source projects, many of which\nare hosted by me or by other organizations in GitHub repositories. Throughout this\nbook, I strive to make the content as accessible as possible, taking a low-code approach\nto help you focus on core concepts. Many chapters demonstrate how simple prompts\ncan generate meaningful code, showcasing the power of AI-assisted development.\n Additionally, you\u2019ll find a variety of assistant profiles and multi-agent systems that\ndemonstrate how to solve real-world problems using generated code. These examples\nare meant to inspire, guide, and empower you to explore what is possible with AI\nagents. I am deeply grateful to the many contributors and the community members\nwho have collaborated on these projects, and I encourage you to explore the reposito-\nries, experiment with the code, and adapt it to your own needs. This book is a testa-\nment to the power of collaboration and the incredible things we can achieve together.\n This book contains many examples of source code both in numbered listings and in\nline with normal text. In both cases, source code is formatted in a fixed-width font\nlike this to separate it from ordinary text. Sometimes, some of the code is typeset in\nbold to highlight code that has changed from previous steps in the chapter, such as\nwhen a feature is added to an existing line of code. In many cases, the original source\ncode has been reformatted; we\u2019ve added line breaks and reworked indentation to\naccommodate the available page space in the book. In some cases, even this wasn\u2019t\nenough, and listings include line-continuation markers (\u27a5). Additionally, comments in\nthe source code have often been removed from the listings when the code is described\n\n\nABOUT THIS BOOK\nxx\nin the text. Code annotations accompany many of the listings, highlighting important\nconcepts.\n You can get executable snippets of code from the liveBook (online) version of this\nbook at https://livebook.manning.com/book/ai-agents-in-action. The complete code\nfor the examples in the book is available for download from the Manning website at\nwww.manning.com/books/ai-agents-in-action. In addition, the code developed for this\nbook has been placed in three GitHub repositories that are all publicly accessible:\n\uf0a1GPT-Agents (the original book title), at https://github.com/cxbxmxcx/GPT-\nAgents, holds the code for several examples demonstrated in the chapters.\n\uf0a1GPT Assistants Playground, at https://github.com/cxbxmxcx/GPTAssistants\nPlayground, is an entire platform and tool dedicated to building OpenAI GPT\nassistants with a helpful web user interface and plenty of tools to develop auton-\nomous agent systems.\n\uf0a1Nexus, at https://github.com/cxbxmxcx/Nexus, is an example of a web-based\nagentic tool that can help you create agentic systems and demonstrate various\ncode challenges.\nliveBook discussion forum\nPurchase of AI Agents in Action includes free access to liveBook, Manning\u2019s online\nreading platform. Using liveBook\u2019s exclusive discussion features, you can attach com-\nments to the book globally or to specific sections or paragraphs. It\u2019s a snap to make\nnotes for yourself, ask and answer technical questions, and receive help from the\nauthor and other users. To access the forum, go to https://livebook.manning.com/\nbook/ai-agents-in-action/discussion. You can also learn more about Manning\u2019s forums\nand the rules of conduct at https://livebook.manning.com/discussion.\n Manning\u2019s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It isn\u2019t a commitment to any specific amount of participation on the part of the\nauthor, whose contribution to the forum remains voluntary (and unpaid). We suggest\nyou try asking the him challenging questions lest his interest stray! The forum and the\narchives of previous discussions will be accessible from the publisher\u2019s website as long\nas the book is in print.\n\n\nxxi\nabout the author\nMICHEAL LANHAM is a distinguished software and technology\ninnovator with more than two decades of experience in the\nindustry. He has an extensive background in developing various\nsoftware applications across several domains, such as gaming,\ngraphics, web development, desktop engineering, AI, GIS, oil\nand gas geoscience/geomechanics, and machine learning.\nMicheal began by pioneering work in integrating neural net-\nworks and evolutionary algorithms into game development,\nwhich began around the turn of the millennium. He has\nauthored multiple influential books exploring deep learning, game development,\nand augmented reality, including Evolutionary Deep Learning (Manning, 2023) and\nAugmented Reality Game Development (Packt Publishing, 2017). He has contributed to\nthe tech community via publications with many significant tech publishers, including\nManning. Micheal resides in Calgary, Alberta, Canada, with his large family, whom he\nenjoys cooking for.\n\n\nxxii\nabout the cover illustration\nThe figure on the cover of AI Agents in Action is \u201cCl\u00e9mentinien,\u201d taken from Balthasar\nHacquet\u2019s Illustrations de L\u2019Illyrie et la Dalmatie, published in 1815.\n In those days, it was easy to identify where people lived and what their trade or sta-\ntion in life was just by their dress. Manning celebrates the inventiveness and initiative\nof the computer business with book covers based on the rich diversity of regional cul-\nture centuries ago, brought back to life by pictures from collections such as this one.\n",
      "page_number": 17
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 25-32)",
      "start_page": 25,
      "end_page": 32,
      "detection_method": "topic_boundary",
      "content": "1\nIntroduction to agents\nand their world\nThe agent isn\u2019t a new concept in machine learning and artificial intelligence (AI).\nIn reinforcement learning, for instance, the word agent denotes an active decision-\nmaking and learning intelligence. In other areas, the word agent aligns more with\nan automated application or software that does something on your behalf.\n1.1\nDefining agents\nYou can consult any online dictionary to find the definition of an agent. The Mer-\nriam-Webster Dictionary defines it this way (www.merriam-webster.com/dictionary/\nagent):\nThis chapter covers\n\uf0a1Defining the concept of agents\n\uf0a1Differentiating the components of an \nagent\n\uf0a1Analyzing the rise of the agent era: \nWhy agents?\n\uf0a1Peeling back the AI interface\n\uf0a1Navigating the agent landscape\n\n\n2\nCHAPTER 1\nIntroduction to agents and their world\n\uf0a1One that acts or exerts power\n\uf0a1Something that produces or can produce an effect\n\uf0a1A means or instrument by which a guiding intelligence achieves a result\nThe word agent in our journey to build powerful agents in this book uses this dictio-\nnary definition. That also means the term assistant will be synonymous with agent.\nTools like OpenAI\u2019s GPT Assistants will also fall under the AI agent blanket. OpenAI\navoids the word agent because of the history of machine learning, where an agent is\nself-deciding and autonomous.\n Figure 1.1 shows four cases where a user may interact with a large language model\n(LLM) directly or through an agent/assistant proxy, an agent/assistant, or an autono-\nmous agent. These four use cases are highlighted in more detail in this list:\n\uf0a1Direct user interaction\u2014If you used earlier versions of ChatGPT, you experienced\ndirect interaction with the LLM. There is no proxy agent or other assistant\ninterjecting on your behalf.\n\uf0a1Agent/assistant proxy\u2014If you\u2019ve used Dall-E 3 through ChatGPT, then you\u2019ve expe-\nrienced a proxy agent interaction. In this use case, an LLM interjects your\nrequests and reformulates them in a format better designed for the task. For\nexample, for image generation, ChatGPT better formulates the prompt. A proxy\nagent is an everyday use case to assist users with unfamiliar tasks or models.\n\uf0a1Agent/assistant\u2014If you\u2019ve ever used a ChatGPT plugin or GPT assistant, then\nyou\u2019ve experienced this use case. In this case, the LLM is aware of the plugin\nor assistant functions and prepares to make calls to this plugin/function.\nHowever, before making a call, the LLM requires user approval. If approved,\nthe plugin or function is executed, and the results are returned to the LLM.\nThe LLM then wraps this response in natural language and returns it to\nthe user.\n\uf0a1Autonomous agent\u2014In this use case, the agent interprets the user\u2019s request, con-\nstructs a plan, and identifies decision points. From this, it executes the steps in\nthe plan and makes the required decisions independently. The agent may\nrequest user feedback after certain milestone tasks, but it\u2019s often given free rein\nto explore and learn if possible. This agent poses the most ethical and safety\nconcerns, which we\u2019ll explore later. \nFigure 1.1 demonstrates the use cases for a single flow of actions on an LLM using a\nsingle agent. For more complex problems, we often break agents into profiles or per-\nsonas. Each agent profile is given a specific task and executes that task with specialized\ntools and knowledge.\n Multi-agent systems are agent profiles that work together in various configurations to\nsolve a problem. Figure 1.2 demonstrates an example of a multi-agent system using\nthree agents: a controller or proxy and two profile agents as workers controlled by the\nproxy. The coder profile on the left writes the code the user requests; on the right is a\n\n\n3\n1.1\nDefining agents\ntester profile designed to write unit tests. These agents work and communicate\ntogether until they are happy with the code and then pass it on to the user.\n Figure 1.2 shows one of the possibly infinite agent configurations. (In chapter 4,\nwe\u2019ll explore Microsoft\u2019s open source platform, AutoGen, which supports multiple\nconfigurations for employing multi-agent systems.)\n Multi-agent systems can work autonomously but may also function guided entirely\nby human feedback. The benefits of using multiple agents are like those of a single\nPlease explain the\nde\ufb01nition of agent.\nLarge language model\n(ChatGPT)\nLLM: The de\ufb01nition\nof agent is...\nShow an illustration\nof an agent.\nLarge language model\n(ChatGPT)\n\"An image of a female\nsecret agent of Hispanic\ndescent in a nighttime\nurban setting. . .\nImage generation model\n(DALL-E 3)\nNo agent or assistant\ndirect connection to LLM\nAgent/assistant proxy for\nimage generator\nWhat is the temperature\nin Calgary today?\nLarge language model\n(ChatGPT)\nLLM identi\ufb01es an external\nfunction API to call and\nparameters to connect\nto a weather service.\nAgent/assistant acting on\nbehalf of user\nUser con\ufb01rms\nexecution okay.\nAsks user if it\u2019s okay\nto execute the function\non their behalf.\nExecutes the function\nand returns weather\ninformation.\nFilter my emails by\nimportance and notify\nme of the top 5 most\nimportant emails.\nLarge language model\n(ChatGPT)\nLLM identi\ufb01es an external\nfunction API to call and\nparameters to connect\nto an email service.\nNoti\ufb01es the user of\nimportant emails.\nAutonomous agent making\ndecisions on behalf of user\nLLM reads\nand sorts emails by\nwhat it deems to be\nimportant.\nDecision step\nLLM reformulates\nweather information and\nresponds to the user.\n.\nFigure 1.1\nThe differences between the LLM interactions from direct action compared to using proxy agents, \nagents, and autonomous agents\n\n\n4\nCHAPTER 1\nIntroduction to agents and their world\nagent but often magnified. Where a single agent typically specializes in a single task,\nmulti-agent systems can tackle multiple tasks in parallel. Multiple agents can also pro-\nvide feedback and evaluation, reducing errors when completing assignments.\n As we can see, an AI agent or agent system can be assembled in multiple ways.\nHowever, an agent itself can also be assembled using multiple components. In the\nnext section, we\u2019ll cover topics ranging from an agent\u2019s profile to the actions it may\nperform, as well as memory and planning.\n1.2\nUnderstanding the component systems of an agent\nAgents can be complex units composed of multiple component systems. These com-\nponents are the tools the agent employs to help it complete its goal or assigned tasks\nand even create new ones. Components may be simple or complex systems, typically\nsplit into five categories.\n Figure 1.3 describes the major categories of components a single-agent system may\nincorporate. Each element will have subtypes that can define the component\u2019s type,\nUser query\nAnswer\nController\nCoder\nTester\nFeature request\nCode\nCode\nUnit tests\nLarge Language Model\nLarge Language Model\nThe controller agent\ncan execute code on\nthe user\u2019s behalf.\nRepeat until the\ncode and tests\nwork as expected.\nWorker agents\ninteract with\nthe LLM.\nFigure 1.2\nIn this example of a multi-agent system, the controller or agent proxy communicates directly with \nthe user. Two agents\u2014a coder and a tester\u2014work in the background to create code and write unit tests to test \nthe code.\n\n\n5\n1.2\nUnderstanding the component systems of an agent\nstructure, and use. At the core of all agents is the profile and persona; extending from\nthat are the systems and functions that enhance the agent.\nThe agent profile and persona shown in figure 1.4 represent the base description of\nthe agent. The persona\u2014often called the system prompt\u2014guides an agent to complete\ntasks, learn how to respond, and other nuances. It includes elements such as the back-\nground (e.g., coder, writer) and demographics, and it can be generated through\nmethods such as handcrafting, LLM assistance, or data-driven techniques, including\nevolutionary algorithms.\n We\u2019ll explore how to create effective and specific agent profiles/personas through\ntechniques such as rubrics and grounding. In addition, we\u2019ll explain the aspects of\nhuman-formulated versus AI-formulated (LLM) profiles, including innovative tech-\nniques using data and evolutionary algorithms to build profiles.\nA persona represents the agent\u2019s\nmain role or function, typically\nde\ufb01ned in a system prompt. The\npro\ufb01le describes the entire agent\nsystem.\nPro\ufb01le and Persona\nActions and Tool Use\nReasoning and Evaluation\nMemory and Knowledge\nPlanning and Feedback\nActions represent a function/\ntool an agent can use.\nReasoning and evaluation\nground the agent and empower\nit to make better decisions.\nPlanning and feedback\nallow the agent to learn and\nimprove on task completion.\nMemory and knowledge provide\nadded context to the agent for a\nspeci\ufb01c request or task.\nFigure 1.3\nThe five main components of a single-agent system (image generated through DALL-E 3)\n\n\n6\nCHAPTER 1\nIntroduction to agents and their world\nNOTE\nThe agent or assistant profile is composed of elements, including the\npersona. It may be helpful to think of profiles describing the work the agent/\nassistant will perform and the tools it needs.\nFigure 1.5 demonstrates the component actions and tool use in the context of agents\ninvolving activities directed toward task completion or acquiring information. These\nactions can be categorized into task completion, exploration, and communication,\nwith varying levels of effect on the agent\u2019s environment and internal states. Actions\ncan be generated manually, through memory recollection, or by following predefined\nplans, influencing the agent\u2019s behavior and enhancing learning.\n Understanding the action target helps us define clear objectives for task comple-\ntion, exploration, or communication. Recognizing the action effect reveals how actions\ninfluence task outcomes, the agent\u2019s environment, and its internal states, contributing\nto efficient decision making. Lastly, grasping action generation methods equips us\nwith the knowledge to create actions manually, recall them from memory, or follow\npredefined plans, enhancing our ability to effectively shape agent behavior and learn-\ning processes.\n Figure 1.6 shows the component knowledge and memory in more detail. Agents\nuse knowledge and memory to annotate context with the most pertinent information\nwhile limiting the number of tokens used. Knowledge and memory structures can be\nunified, where both subsets follow a single structure or hybrid structure involving a\nmix of different retrieval forms. Knowledge and memory formats can vary widely from\nAgent persona: We\u2019ll understand how\nto clearly de\ufb01ne the persona, specifying\ntheir role and characteristics to guide\nthe agent effectively.\nAgent role and demographics: We\u2019ll\nsee how relevant demographic and role\ndetails can provide agent context, such\nas age, gender, or background, for a\nmore relevant interaction.\nHuman vs. AI assistance for persona\ngeneration: We\u2019ll highlight the role\nof human involvement in persona\ngeneration, whether it\u2019s entirely\nhuman driven or assisted by LLMs\nor other agents.\nInnovative persona techniques:\nPrompts generated through data\nor other novel approaches such as\nevolutionary algorithms to enhance\nagent capabilities.\nPro\ufb01le and Persona\nPro\ufb01le Contents\nPro\ufb01le Generation\nPersona: Role, i.e., coder or tester\nDemographics: Sex, age, background\nHandcrafted: Manually designed by\nhumans\nLLM generated: Directed by human\nprompts\nData generated: Constructed from\ndata personas\nFigure 1.4\nAn in-depth look at how we\u2019ll explore creating agent profiles\n\n\n7\n1.2\nUnderstanding the component systems of an agent\nlanguage (e.g., PDF documents) to databases (relational, object, or document) and\nembeddings, simplifying semantic similarity search through vector representations or\neven simple lists serving as agent memories.\nFigure 1.7 shows the reasoning and evaluation component of an agent system. Research\nand practical applications have shown that LLMs/agents can effectively reason. Rea-\nsoning and evaluation systems annotate an agent\u2019s workflow by providing an ability to\nthink through problems and evaluate solutions.\nAction targets: We\u2019ll learn the importance\nof de\ufb01ning action targets, whether for task\ncompletion, exploration, or communication,\nto clarify the agent\u2019s objectives.\nAction space and impact: We\u2019ll learn the\nsigni\ufb01cance of understanding how actions\naffect task completion and their effect on\nthe agent\u2019s environment, internal states,\nand self-knowledge.\nAction generation methods: We\u2019ll see the\nvarious ways actions can be generated, such\nas manual creation, memory recollection,\nor plan following, to illustrate the diversity\nof agent behaviors.\nAction and Tool Use\nAction Target\nSemantic or native functions\nTools, self-knowledge, other agents\nEnvironments, new actions, internal\nstates, other agents\nManual, memory recollection, plan\nfollowing\nFigure 1.5\nThe aspects of agent actions we\u2019ll explore in this book\nMemory and Knowledge\nRetrieval Structure\n\u2022 Uni\ufb01ed\n\u2022 Hybrid\nRetrieval Formats\n\u2022 Language\n\u2022 Databases\n\u2022 Embeddings\n\u2022 Lists\nRetrieval Operation\n\u2022 Augmentation\n\u2022 Semantic Extraction\n\u2022 Compression\nRetrieval structure variety: We\u2019ll learn\nabout the diverse memory structures\nagents can employ, including uni\ufb01ed and\nhybrid approaches, enabling \ufb02exibility in\ninformation storage.\nRetrieval formats: We\u2019ll explore the\nvarious data sources for memory, such\nas language (e.g., PDF documents),\ndatabases (relational, object, or\ndocument), and embeddings, offering a\nrich pool of information to draw upon.\nSemantic similarity: We\u2019ll learn how\nembeddings enable semantic similarity\nsearches, facilitating ef\ufb01cient retrieval of\nrelevant data and enhancing the agent\u2019s\ndecision-making capabilities.\nFigure 1.6\nExploring the role and use of agent memory and knowledge\n\n\n8\nCHAPTER 1\nIntroduction to agents and their world\nFigure 1.8 shows the component agent planning/feedback and its role in organizing\ntasks to achieve higher-level goals. It can be categorized into these two approaches:\n\uf0a1Planning without feedback\u2014Autonomous agents make decisions independently.\n\uf0a1Planning with feedback\u2014Monitoring and modifying plans is based on various\nsources of input, including environmental changes and direct human feedback.\nWithin planning, agents may employ single-path reasoning, sequential reasoning through\neach step of a task, or multipath reasoning to explore multiple strategies and save the\nReasoning and Evaluation\nReasoning\n\u2022 Zero-shot prompting\n\u2022 One-shot prompting\n\u2022 Few-shot prompting\n\u2022 Chain of thought\n\u2022 Tree of thought\n\u2022 Skeleton of thought\nEvaluation\n\u2022 Self-consistency\n\u2022 Prompt chaining\nReasoning enables the agent\nto self-re\ufb02ect and internally\nreason out the completion\nof a task or tasks.\nEvaluation provides the basis\nfor an agent\u2019s self-re\ufb02ection\non working through and upon\ntask completion.\nFigure 1.7\nThe reasoning and evaluation component and details\nWe\u2019ll look at various planning\nstrategies with and without\nfeedback\u2014from basic and\nsequential planners to automatic\ntool use with reasoning.\nFeedback may come from a variety\nof sources, such as environmental,\nhuman, and an LLM via various\nconstructive feedback patterns.\nPlanning and Feedback\nPlanning without feedback\n(autonomous)\n\u2022 Basic planning\n\u2022 Automatic reasoning with\ntool use\n\u2022 Sequential planning\nPlanning with feedback\n\u2022 Environmental feedback\n\u2022 Human feedback\n\u2022 LLM feedback\n\u2022 Adaptive constructive\nfeedback\nFigure 1.8\nExploring the role of agent planning and reasoning\n",
      "page_number": 25
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 33-40)",
      "start_page": 33,
      "end_page": 40,
      "detection_method": "topic_boundary",
      "content": "9\n1.3\nExamining the rise of the agent era: Why agents?\nefficient ones for future use. External planners, which can be code or other agent sys-\ntems, may also play a role in orchestrating plans.\n Any of our previous agent types\u2014the proxy agent/assistant, agent/assistant, or\nautonomous agent\u2014may use some or all of these components. Even the planning\ncomponent has a role outside of the autonomous agent and can effectively empower\neven the regular agent.\n1.3\nExamining the rise of the agent era: Why agents?\nAI agents and assistants have quickly moved from the main commodity in AI research\nto mainstream software development. An ever-growing list of tools and platforms assist\nin the construction and empowerment of agents. To an outsider, it may all seem like\nhype intended to inflate the value of some cool but overrated technology.\n During the first few months after ChatGPT\u2019s initial release, a new discipline called\nprompt engineering was formed: users found that using various techniques and patterns\nin their prompts allowed them to generate better and more consistent output. How-\never, users also realized that prompt engineering could only go so far.\n Prompt engineering is still an excellent way to interact directly with LLMs such as\nChatGPT. Over time, many users discovered that effective prompting required iteration,\nreflection, and more iteration. The first agent systems, such as AutoGPT, emerged from\nthese discoveries, capturing the community\u2019s attention.\n Figure 1.9 shows the original design of AutoGPT, one of the first autonomous\nagent systems. The agent is designed to iterate a planned sequence of tasks that it\ndefines by looking at the user\u2019s goal. Through each task iteration of steps, the agent\nevaluates the goal and determines if the task is complete. If the task isn\u2019t complete, the\nagent may replan the steps and update the plan based on new knowledge or human\nfeedback.\n AutoGPT became the first example to demonstrate the power of using task plan-\nning and iteration with LLM models. From this and in tandem, other agent systems\nand frameworks exploded into the community using similar planning and task itera-\ntion systems. It\u2019s generally accepted that planning, iteration, and repetition are the\nbest processes for solving complex and multifaceted goals for an LLM.\n However, autonomous agent systems require trust in the agent decision-making\nprocess, the guardrails/evaluation system, and the goal definition. Trust is also some-\nthing that is acquired over time. Our lack of trust stems from our lack of understand-\ning of an autonomous agent\u2019s capabilities.\nNOTE\nArtificial general intelligence (AGI) is a form of intelligence that\ncan learn to accomplish any task a human can. Many practitioners in this\nnew world of AI believe an AGI using autonomous agent systems is an\nattainable goal.\nFor this reason, many of the mainstream and production-ready agent tools aren\u2019t auton-\nomous. However, they still provide a significant benefit in managing and automating\n\n\n10\nCHAPTER 1\nIntroduction to agents and their world\ntasks using GPTs (LLMs). Therefore, as our goal in this book is to understand all agent\nforms, many more practical applications will be driven by non-autonomous agents.\n Agents and agent tools are only the top layer of a new software application devel-\nopment paradigm. We\u2019ll look at this new paradigm in the next section.\nThe agent evaluates if\nthe goal is complete\nafter every task\niteration.\nAI Large Language\nModels (GPT-4)\nInternet\nOther Tools\nEvaluation\nDe\ufb01ning and\nSequencing Tasks\nSetting Goals\nTask Execution\nGoals not complete\nThe agent can be\nset to ask for\npermission for\nevery task or for\nevery x number\nof tasks.\nGoals complete\nUser sets the main\nobjective goal\nAgent executes\ntasks.\nThe agent could write code to\nperform other tasks as needed.\nThe agent\nplans out the\nsequence of\ntasks to\nundertake.\nAutonomous AI Mechanism\nFigure 1.9\nThe original design of the AutoGPT agent system\n\n\n11\n1.4\nPeeling back the AI interface\n1.4\nPeeling back the AI interface\nThe AI agent paradigm is not only a shift in how we work with LLMs but is also per-\nceived as a shift in how we develop software and handle data. Software and data will\nno longer be interfaced using user interfaces (UIs), application programming inter-\nfaces (APIs), and specialized query languages such as SQL. Instead, they will be\ndesigned to be interfaced using natural language.\n Figure 1.10 shows a high-level snapshot of what this new architecture may look like\nand what role AI agents play. Data, software, and applications adapt to support seman-\ntic, natural language interfaces. These AI interfaces allow agents to collect data and\ninteract with software applications, even other agents or agent applications. This rep-\nresents a new shift in how we interact with software and applications.\nAn AI interface is a collection of functions, tools, and data layers that expose data and\napplications by natural language. In the past, the word semantic has been heavily\nAgent interface layer (natural language)\nPlanning: Agent takes the goal and breaks into tasks.\n1. Collect data.\n2. Annotate data.\n3. Format data, and create report visualizations.\n4. Present report.\n4. Agent presents the report.\nPlease create a report of\nlast year\u2019s sales.\nGPT data layer\nDatabase\n1. Query database using natural\nlanguage.\nGPT functions\nAPIs,\nweb browsing,\nsearch, etc.\nAll communication via natural language\n2. Annotate data by calling semantic\nfunctions using natural language.\nExternal agents\nGPT tools\n3. External agent formats data and\nmay write code to generate visuals.\nFigure 1.10\nA vision of how agents will interact with software systems\n\n\n12\nCHAPTER 1\nIntroduction to agents and their world\nused to describe these interfaces, and even some tools use the name; however,\n\u201csemantic\u201d can also have a variety of meanings and uses. Therefore, in this book,\nwe\u2019ll use the term AI interface.\n The construction of AI interfaces will empower agents that need to consume\nthe services, tools, and data. With this empowerment will come increased accuracy\nin completing tasks and more trustworthy and autonomous applications. While an\nAI interface may not be appropriate for all software and data, it will dominate many\nuse cases.\n1.5\nNavigating the agent landscape\nGPT agents represent an entire shift in how consumers and developers approach\neverything, from finding information to building software and accessing data. Almost\ndaily, a new agent framework, component, or interface pops up on GitHub or in a\nresearch paper. This can be overwhelming and intimidating to the new user trying to\ngrasp what agent systems are and how to use them.\nSummary\n\uf0a1An agent is an entity that acts or exerts power, produces an effect, or serves as a\nmeans for achieving a result. An agent automates interaction with a large lan-\nguage model (LLM) in AI.\n\uf0a1An assistant is synonymous with an agent. Both terms encompass tools such as\nOpenAI\u2019s GPT Assistants.\n\uf0a1Autonomous agents can make independent decisions, and their distinction\nfrom non-autonomous agents is crucial.\n\uf0a1The four main types of LLM interactions include direct user interaction, agent/\nassistant proxy, agent/assistant, and autonomous agent.\n\uf0a1Multi-agent systems involve agent profiles working together, often controlled by\na proxy, to accomplish complex tasks.\n\uf0a1The main components of an agent include the profile/persona, actions, knowl-\nedge/memory, reasoning/evaluation, and planning/feedback.\n\uf0a1Agent profiles and personas guide an agent\u2019s tasks, responses, and other nuances,\noften including background and demographics.\n\uf0a1Actions and tools for agents can be manually generated, recalled from memory,\nor follow predefined plans.\n\uf0a1Agents use knowledge and memory structures to optimize context and mini-\nmize token usage via various formats, from documents to embeddings.\n\uf0a1Reasoning and evaluation systems enable agents to think through problems\nand assess solutions using prompting patterns such as zero-shot, one-shot, and\nfew-shot.\n\uf0a1Planning/feedback components organize tasks to achieve goals using single-path\nor multipath reasoning and integrating environmental and human feedback.\n\n\n13\nSummary\n\uf0a1The rise of AI agents has introduced a new software development paradigm,\nshifting from traditional to natural language\u2013based AI interfaces.\n\uf0a1Understanding the progression and interaction of these tools helps develop\nagent systems, whether single, multiple, or autonomous.\n\n\n14\nHarnessing the power\nof large language models\nThe term large language models (LLMs) has now become a ubiquitous descriptor of a\nform of AI. These LLMs have been developed using generative pretrained trans-\nformers (GPTs). While other architectures also power LLMs, the GPT form is cur-\nrently the most successful.\n LLMs and GPTs are generative models, which means they are trained to generate\nrather than predict or classify content. To illustrate this further, consider figure 2.1,\nwhich shows the difference between generative and predictive/classification mod-\nels. Generative models create something from the input, whereas predictive and\nclassifying models classify it.\nThis chapter covers\n\uf0a1Understanding the basics of LLMs\n\uf0a1Connecting to and consuming the OpenAI API\n\uf0a1Exploring and using open source LLMs with \nLM Studio\n\uf0a1Prompting LLMs with prompt engineering\n\uf0a1Choosing the optimal LLM for your specific \nneeds\n\n\n15\nWe can further define an LLM by its constituent parts, as shown in figure 2.2. In\nthis diagram, data represents the content used to train the model, and architecture is\nan attribute of the model itself, such as the number of parameters or size of the\nmodel. Models are further trained specifically to the desired use case, including\nchat, completions, or instruction. Finally, fine-tuning is a feature added to models\nthat refines the input data and model training to better match a particular use case\nor domain.\nInput text\nOutputs the most\nprobable next word\ngiven the input\nthus far\nOutputs the most\nprobable class\nthe phrase is\naligned with\nOutputs text\nOutputs prediction\nFigure 2.1\nThe difference between generative and predictive models\nData\nLLM\nTraining\nArchitecture\nFine-tuning\nThe input data represents the\ncontent the model will train on.\nThis typically consists of terabytes\nto petabytes of data.\nDenotes the model architecture.\nThe architecture de\ufb01nes things\nsuch as context, token limits,\nembedding size, and number of\nparameters (model size).\nDe\ufb01nes the form of training used to\ntrain the model. Training will also\nde\ufb01ne the model use case, such as\nchat completions, completions,\ninstruct, or question/answer.\nFine-tuning is the process of\nmaking a model more speci\ufb01c\nto a particular domain or dataset.\nFigure 2.2\nThe main elements that describe an LLM\nCHAPTER 2\nHarnessing the power of large language models\n\n\n16\nCHAPTER 2\nHarnessing the power of large language models\nThe transformer architecture of GPTs, which is a specific architecture of LLMs, allows\nthe models to be scaled to billions of parameters in size. This requires these large\nmodels to be trained on terabytes of documents to build a foundation. From there,\nthese models will be successively trained using various methods for the desired use\ncase of the model.\n ChatGPT, for example, is trained effectively on the public internet and then fine-\ntuned using several training strategies. The final fine-tuning training is completed\nusing an advanced form called reinforcement learning with human feedback (RLHF). This\nproduces a model use case called chat completions.\n Chat completions LLMs are designed to improve through iteration and refine-\nment\u2014in other words, chatting. These models have also been benchmarked to be the\nbest in task completion, reasoning, and planning, which makes them ideal for build-\ning agents and assistants. Completion models are trained/designed only to provide\ngenerated content on input text, so they don\u2019t benefit from iteration.\n For our journey to build powerful agents in this book, we focus on the class of\nLLMs called chat completions models. That, of course, doesn\u2019t preclude you from try-\ning other model forms for your agents. However, you may have to significantly alter\nthe code samples provided to support other model forms.\n We\u2019ll uncover more details about LLMs and GPTs later in this chapter when we\nlook at running an open source LLM locally. In the next section, we look at how to\nconnect to an LLM using a growing standard from OpenAI.\n2.1\nMastering the OpenAI API\nNumerous AI agents and assistant projects use the OpenAI API SDK to connect to an\nLLM. While not standard, the basic concepts describing a connection now follow the\nOpenAI pattern. Therefore, we must understand the core concepts of an LLM con-\nnection using the OpenAI SDK.\n This chapter will look at connecting to an LLM model using the OpenAI Python\nSDK/package. We\u2019ll discuss connecting to a GPT-4 model, the model response, count-\ning tokens, and how to define consistent messages. Starting in the following subsec-\ntion, we\u2019ll examine how to use OpenAI.\n2.1.1\nConnecting to the chat completions model\nTo complete the exercises in this section and subsequent ones, you must set up a\nPython developer environment and get access to an LLM. Appendix A walks you\nthrough setting up an OpenAI account and accessing GPT-4 or other models. Appen-\ndix B demonstrates setting up a Python development environment with Visual Studio\nCode (VS Code), including installing needed extensions. Review these sections if you\nwant to follow along with the scenarios.\n Start by opening the source code chapter_2 folder in VS Code and creating a new\nPython virtual environment. Again, refer to appendix B if you need assistance.\n",
      "page_number": 33
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 41-49)",
      "start_page": 41,
      "end_page": 49,
      "detection_method": "topic_boundary",
      "content": "17\n2.1\nMastering the OpenAI API\n Then, install the OpenAI and Python dot environment packages using the com-\nmand in the following listing. This will install the required packages into the virtual\nenvironment.\npip install openai python-dotenv\nNext, open the connecting.py file in VS Code, and inspect the code shown in listing 2.2.\nBe sure to set the model\u2019s name to an appropriate name\u2014for example, gpt-4. At the\ntime of writing, the gpt-4-1106-preview was used to represent GPT-4 Turbo.\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nload_dotenv()                          \napi_key = os.getenv('OPENAI_API_KEY')\nif not api_key:                            \n    raise ValueError(\"No API key found. Please check your .env file.\")\nclient = OpenAI(api_key=api_key)                       \ndef ask_chatgpt(user_message):\n    response = client.chat.completions.create(     \n        model=\"gpt-4-1106-preview\",\n        messages=[{\"role\": \"system\",\n \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": user_message}],\n        temperature=0.7,\n        )\n    return response.choices[0].message.content    \nuser = \"What is the capital of France?\"\nresponse = ask_chatgpt(user)               \nprint(response)\nA lot is happening here, so let\u2019s break it down by section, starting with the beginning\nand loading the environment variables. In the chapter_2 folder is another file called\n.env, which holds environment variables. These variables are set automatically by call-\ning the load_dotenv function.\n You must set your OpenAI API key in the .env file, as shown in the next listing.\nAgain, refer to appendix A to find out how to get a key and find a model name.\nOPENAI_API_KEY='your-openai-api-key'\nAfter setting the key, you can debug the file by pressing the F5 key or selecting Run >\nStart Debugging from the VS Code menu. This will run the code, and you should see\nsomething like \u201cThe capital of France is Paris.\u201d\nListing 2.1\npip installs\nListing 2.2\nconnecting.py\nListing 2.3\n.env\nLoads the secrets \nstored in the .env file\nChecks to see \nwhether the key is set\nCreates a client \nwith the key\nUses the create \nfunction to generate \na response\nReturns just the content \nof the response\nExecutes the request and \nreturns the response\n\n\n18\nCHAPTER 2\nHarnessing the power of large language models\n Remember that the response from a generative model depends on the probability.\nThe model will probably give us a correct and consistent answer in this case.\n You can play with these probabilities by adjusting the temperature of the request.\nIf you want a model to be more consistent, turn the temperature down to 0, but if you\nwant the model to produce more variation, turn the temperature up. We\u2019ll explore\nsetting the temperature further in the next section.\n2.1.2\nUnderstanding the request and response\nDigging into the chat completions request and response features can be helpful. We\u2019ll\nfocus on the request first, as shown next. The request encapsulates the intended\nmodel, the messages, and the temperature.\nresponse = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",                \n    messages=[{\"role\": \"system\", \n\"content\": \"You are a helpful assistant.\"},                    \n              {\"role\": \"user\", \"content\": user_message}],     \n    temperature=0.7,    \n    )\nWithin the request, the messages block describes a set of messages and roles used in a\nrequest. Messages for a chat completions model can be defined in three roles:\n\uf0a1System role\u2014A message that describes the request\u2019s rules and guidelines. It can\noften be used to describe the role of the LLM in making the request.\n\uf0a1User role\u2014Represents and contains the message from the user.\n\uf0a1Assistant role\u2014Can be used to capture the message history of previous responses\nfrom the LLM. It can also inject a message history when perhaps none existed.\nThe message sent in a single request can encapsulate an entire conversation, as shown\nin the JSON in the following listing.\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the capital of France?\"\n    },\n    {\n        \"role\": \"assistant\",\nListing 2.4\nThe chat completions request\nListing 2.5\nMessages with history\nThe model or deployment used \nto respond to the request\nThe system \nrole message\nThe user role \nmessage\nThe temperature or \nvariability of the request\n\n\n19\n2.1\nMastering the OpenAI API\n        \"content\": \"The capital of France is Paris.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is an interesting fact of Paris.\"\n    }\n],\nYou can see how this can be applied by opening message_history.py in VS Code and\ndebugging it by pressing F5. After the file runs, be sure to check the output. Then, try\nto run the sample a few more times to see how the results change.\n The results will change from each run to the next due to the high temperature of\n.7. Go ahead and reduce the temperature to .0, and run the message_history.py\nsample a few more times. Keeping the temperature at 0 will show the same or similar\nresults each time.\n Setting a request\u2019s temperature will often depend on your particular use case.\nSometimes, you may want to limit the responses\u2019 stochastic nature (randomness).\nReducing the temperature to 0 will give consistent results. Likewise, a value of 1.0 will\ngive the most variability in the responses.\n Next, we also want to know what information is being returned for each request.\nThe next listing shows the output format for the response. You can see this output by\nrunning the message_history.py file in VS Code.\n{\n    \"id\": \"chatcmpl-8WWL23up3IRfK1nrDFQ3EHQfhx0U6\",\n    \"choices\": [                                     \n        {\n            \"finish_reason\": \"stop\",\n            \"index\": 0,\n            \"message\": {\n                \"content\": \"\u2026 omitted\",\n                \"role\": \"assistant\",      \n                \"function_call\": null,\n                \"tool_calls\": null\n            },\n            \"logprobs\": null\n        }\n    ],\n    \"created\": 1702761496,\n    \"model\": \"gpt-4-1106-preview\",    \n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": \"fp_3905aa4f79\",\n    \"usage\": {\n        \"completion_tokens\": 78,    \n        \"prompt_tokens\": 48,        \n        \"total_tokens\": 126         \n    }\n}\nListing 2.6\nChat completions response\nA model may \nreturn more than \none response.\nResponses \nreturned in the \nassistant role\nIndicates the \nmodel used\nCounts the number of \ninput (prompt) and output \n(completion) tokens used\n\n\n20\nCHAPTER 2\nHarnessing the power of large language models\nIt can be helpful to track the number of input tokens (those used in prompts) and the\noutput tokens (the number returned through completions). Sometimes, minimizing\nand reducing the number of tokens can be essential. Having fewer tokens typically\nmeans LLM interactions will be cheaper, respond faster, and produce better and more\nconsistent results.\n That covers the basics of connecting to an LLM and returning responses. Through-\nout this book, we\u2019ll review and expand on how to interact with LLMs. Until then, we\u2019ll\nexplore in the next section how to load and use open source LLMs.\n2.2\nExploring open source LLMs with LM Studio\nCommercial LLMs, such as GPT-4 from OpenAI, are an excellent place to start to\nlearn how to use modern AI and build agents. However, commercial agents are an\nexternal resource that comes at a cost, reduces data privacy and security, and\nintroduces dependencies. Other external influences can further complicate these\nfactors.\n It\u2019s unsurprising that the race to build comparable open source LLMs is growing\nmore competitive every day. As a result, there are now open source LLMs that may be\nadequate for numerous tasks and agent systems. There have even been so many\nadvances in tooling in just a year that hosting LLMs locally is now very easy, as we\u2019ll see\nin the next section.\n2.2.1\nInstalling and running LM Studio\nLM Studio is a free download that supports downloading and hosting LLMs and other\nmodels locally for Windows, Mac, and Linux. The software is easy to use and offers sev-\neral helpful features to get you started quickly. Here is a quick summary of steps to\ndownload and set up LM Studio:\n1\nDownload LM Studio from https://lmstudio.ai/.\n2\nAfter downloading, install the software per your operating system. Be aware that\nsome versions of LM Studio may be in beta and require installation of addi-\ntional tools or libraries.\n3\nLaunch the software.\nFigure 2.3 shows the LM Studio window running. From there, you can review the cur-\nrent list of hot models, search for others, and even download. The home page content\ncan be handy for understanding the details and specifications of the top models.\n An appealing feature of LM Studio is its ability to analyze your hardware and align\nit with the requirements of a given model. The software will let you know how well you\ncan run a given model. This can be a great time saver in guiding what models you\nexperiment with.\n \n\n\n21\n2.2\nExploring open source LLMs with LM Studio\nEnter some text to search for a model, and click Go. You\u2019ll be taken to the search\npage interface, as shown in figure 2.4. From this page, you can see all the model\nvariations and other specifications, such as context token size. After you click the\nCompatibility Guess button, the software will even tell you if the model will run on\nyour system.\n Click to download any model that will run on your system. You may want to stick\nwith models designed for chat completions, but if your system is limited, work with\nwhat you have. In addition, if you\u2019re unsure of which model to use, go ahead and\ndownload to try them. LM Studio is a great way to explore and experiment with many\nmodels.\n \n \n \n \nChat interface to talk\ndirectly to a local LLM\nRun a local model\nas a service.\nSearch area\nBrowse already\ndownloaded models.\nFigure 2.3\nLM Studio software showing the main home page\n\n\n22\nCHAPTER 2\nHarnessing the power of large language models\nAfter the model is downloaded, you can then load and run the model on the chat\npage or as a server on the server page. Figure 2.5 shows loading and running a\nmodel on the chat page. It also shows the option for enabling and using a GPU if\nyou have one.\n To load and run a model, open the drop-down menu at the top middle of the\npage, and select a downloaded model. A progress bar will appear showing the model\nloading, and when it\u2019s ready, you can start typing into the UI.\n The software even allows you to use some or all of your GPU, if detected, for the\nmodel inference. A GPU will generally speed up the model response times in some\ncapacities. You can see how adding a GPU can affect the model\u2019s performance by\nlooking at the performance status at the bottom of the page, as shown in figure 2.5.\n Chatting with a model and using or playing with various prompts can help you\ndetermine how well a model will work for your given use case. A more systematic\napproach is using the prompt flow tool for evaluating prompts and LLMs. We\u2019ll\ndescribe how to use prompt flow in chapter 9.\n LM Studio also allows a model to be run on a server and made accessible using the\nOpenAI package. We\u2019ll see how to use the server feature and serve a model in the\nnext section.\nSearch text\nLook at the model\ncard on Hugging Face.\nThe compatibility guesser\ninforms you if a model will run.\nShows the\ndownloaded models\nFigure 2.4\nThe LM Studio search page\n\n\n23\n2.2\nExploring open source LLMs with LM Studio\n2.2.2\nServing an LLM locally with LM Studio\nRunning an LLM locally as a server is easy with LM Studio. Just open the server page,\nload a model, and then click the Start Server button, as shown in figure 2.6. From\nthere, you can copy and paste any of the examples to connect with your model.\n You can review an example of the Python code by opening chapter_2/lmstudio_\nserver.py in VS Code. The code is also shown here in listing 2.7. Then, run the code\nin the VS Code debugger (press F5).\n \n \nLoaded model\nConversation\nhistory\nText area for\nuser messages\nModel system\nprompt\nEnabling GPU acceleration,\navailable when GPU detected\nModel performance\nand usage\nFigure 2.5\nThe LM Studio chat page with a loaded, locally running LLM\n\n\n24\nCHAPTER 2\nHarnessing the power of large language models\nfrom openai import OpenAI\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\ncompletion = client.chat.completions.create(\n  model=\"local-model\",                          \n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}      \n  ],\n  temperature=0.7,\n)\nprint(completion.choices[0].message)     \nListing 2.7\nlmstudio_server.py\nLoaded model\nEnabling GPU acceleration,\navailable when GPU detected\nExample to connect\nto the server\nStart/stop the server\nLogs showing startup and\nconnection information\nFigure 2.6\nThe LM Studio server page and a server running an LLM\nCurrently not used; \ncan be anything\nFeel free to change \nthe message as \nyou like.\nDefault code outputs \nthe whole message.\n\n\n25\n2.3\nPrompting LLMs with prompt engineering\nIf you encounter problems connecting to the server or experience any other prob-\nlems, be sure your configuration for the Server Model Settings matches the model\ntype. For example, in figure 2.6, shown earlier, the loaded model differs from the\nserver settings. The corrected settings are shown in figure 2.7.\nNow, you can use a locally hosted LLM or a commercial model to build, test, and\npotentially even run your agents. The following section will examine how to build\nprompts using prompt engineering more effectively.\n2.3\nPrompting LLMs with prompt engineering\nA prompt defined for LLMs is the message content used in the request for better\nresponse output. Prompt engineering is a new and emerging field that attempts to struc-\nture a methodology for building prompts. Unfortunately, prompt building isn\u2019t a well-\nestablished science, and there is a growing and diverse set of methods defined as\nprompt engineering.\n Fortunately, organizations such as OpenAI have begun documenting a universal\nset of strategies, as shown in figure 2.8. These strategies cover various tactics, some\nrequiring additional infrastructure and considerations. As such, the prompt engineer-\ning strategies relating to more advanced concepts will be covered in the indicated\nchapters.\n Each strategy in figure 2.8 unfolds into tactics that can further refine the specific\nmethod of prompt engineering. This chapter will examine the fundamental Write\nLoaded model\nServer\nodel ettings\nM\nS\nmatch the loaded model.\nWhen running correctly,\nyou\u2019ll see the message logs.\nFigure 2.7\nChoosing the correct Server Model Settings for the loaded model\n",
      "page_number": 41
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 50-57)",
      "start_page": 50,
      "end_page": 57,
      "detection_method": "topic_boundary",
      "content": "26\nCHAPTER 2\nHarnessing the power of large language models\nClear Instructions strategy. Figure 2.9 shows the tactics for this strategy in more detail,\nalong with examples for each tactic. We\u2019ll look at running these examples using a\ncode demo in the following sections.\n The Write Clear Instructions strategy is about being careful and specific about\nwhat you ask for. Asking an LLM to perform a task is no different from asking a per-\nson to complete the same task. Generally, the more information and context relevant\nto a task you can specify in a request, the better the response.\n This strategy has been broken down into specific tactics you can apply to prompts.\nTo understand how to use those, a code demo (prompt_engineering.py) with various\nprompt examples is in the chapter 2 source code folder.\n Open the prompt_engineering.py file in VS Code, as shown in listing 2.8. This\ncode starts by loading all the JSON Lines files in the prompts folder. Then, it displays\nthe list of files as choices and allows the user to select a prompt option. After selecting\nthe option, the prompts are submitted to an LLM, and the response is printed.\n \n \n \nWrite Clear Instructions\nBe speci\ufb01c in what you ask.\nTactics include detailing queries, adopting personas, using delimiters,\nspecifying steps, providing examples, and specifying output length.\nBasics\nProvide Reference Text\nHelps reduce fabrications.\nTactics involve instructing the model to use or cite reference texts.\nMemory\nUse External Tools\nEnhances model capabilities.\nTactics include embeddings-based search, code execution, and\naccess to speci\ufb01c functions.\nMemory\nSplit Complex Tasks into Simpler Subtasks\nReduces error rates.\nTactics include intent classi\ufb01cation, summarizing dialogues, and\npiecewise summarization of documents.\nPlanning\nGive Models Time to \u201cThink\u201d\nAllows more reliable reasoning.\nTactics involve working out solutions before conclusions, using inner\nmonologue, and reviewing previous answers.\nPlanning\nTest Changes Systematically\nEnsures improvements are genuine.\nTactics involve evaluating model outputs with reference to standard\nanswers.\nEvaluation\nPrompt Engineering Strategies\nFigure 2.8\nOpenAI prompt engineering strategies reviewed in this book, by chapter location\n\n\n27\n2.3\nPrompting LLMs with prompt engineering\ndef main():\n    directory = \"prompts\"\n    text_files = list_text_files_in_directory(directory)   \n    if not text_files:\n        print(\"No text files found in the directory.\")\n        return\n    def print_available():                                    \n        print(\"Available prompt tactics:\")\n        for i, filename in enumerate(text_files, start=1):\n            print(f\"{i}. {filename}\")\n    while True:\n        try:\n            print_available()                                 \n            choice = int(input(\"Enter \u2026 0 to exit): \"))          \nListing 2.8\nprompt_engineering.py (main())\nDetailed\nQueries\nAdopting\nPersonas\nUsing\nDelimiters\nSpecifying\nSteps\nProviding\nExamples\nSpecify Output\nLength\nTactics for Strategy: Writing Clear Instructions\nWithout detailed\nqueries:\nWho\u2019s the prime\nminister?\nWith detailed\nqueries:\nWho is the prime\nminister of Canada,\nand how frequently\nare elections held?\nSYSTEM:\nYou are a prompt\nexpert and will\nsuggest ways to\nimprove a user\nrequest.\nUSER:\nWhat is the capital\nof Canada?\nUSER:\nSummarize the text\ndelimited by triple\nquotes with a\nlimerick:\n\u201ctext to be\nsummarized\u201d\n.\nEXAMPLES\nSYSTEM:\nUse the following\nstep-by-step\ninstructions to\nrespond to user\ninputs:\nStep 1 - Summarize\nthe text in triple\nquotes to one\nsentence with a\npre\ufb01x that says\n\u201cSummary: \u201d.\nStep 2 - Translate\nthe summary from\nStep 1 into French,\nwith a pre\ufb01x that\nsays \u201cTranslation: \u201d.\nUSER:\n\u201ctext to summarize\nand translate\u201d\nSYSTEM:\nAnswer in a\nconsistent style.\nUSER:\nT\no\neach me ab ut\npatience.\nA\nN\nSSISTA T:\nThe river that\ncarves the deepest\nvalley \ufb02ows from a\nmodest spring; the\nmost intricate\ntapestry begins with\na solitary thread.\nUSER:\nT\no\neach me ab ut\nthe ocean.\nUSER:\nSummarize the text\ndelimited by triple\nquotes in about 50\nwords.\n\u201ctext to summarize\nhere\u201d\nThis is the\nexample.\nProvide as much\ndetail as you can\nin a query;\ngenerally, the\nmore detail the\nbetter.\nPersonas can\ninclude details\nabout\ndemographics,\nknowledge, and\npersonality.\nDelimiters can\nhel separate\np\nblocks of content\nfrom speci\ufb01cation\ndetails.\nLimiting the\nlength of output\ncan be speci\ufb01c\nto words, bullet\npoints, or other\nmetrics.\nUsing steps can\nhelp the LLM\nbetter process\nthe task, but be\nsure to limit\nthe number.\nExamples are a\nform of few-shot\nlearning and can\nbe an excellent\nway to indicate\nthe desired\nresponse format\nand other details.\nFigure 2.9\nThe tactics for the Write Clear Instructions strategy\nCollects all \nthe files for the \ngiven folder\nPrints the list of \nfiles as choices\nInputs the \nuser\u2019s choice\n\n\n28\nCHAPTER 2\nHarnessing the power of large language models\n            if choice == 0:\n                break\n            elif 1 <= choice <= len(text_files):\n                selected_file = text_files[choice - 1]\n                file_path = os.path.join(directory,\n      selected_file)\n                prompts = \n\u27a5 load_and_parse_json_file(file_path)                         \n                print(f\"Running prompts for {selected_file}\")\n                for i, prompt in enumerate(prompts):\n                    print(f\"PROMPT {i+1} --------------------\")\n                    print(prompt)\n                    print(f\"REPLY ---------------------------\")\n                    print(prompt_llm(prompt))                      \n            else:\n                print(\"Invalid choice. Please enter a valid number.\")\n        except ValueError:\n            print(\"Invalid input. Please enter a number.\")\nA commented-out section from the listing demonstrates how to connect to a local\nLLM. This will allow you to explore the same prompt engineering tactics applied to\nopen source LLMs running locally. By default, this example uses the OpenAI model\nwe configured previously in section 2.1.1. If you didn\u2019t complete that earlier, please go\nback and do it before running this one.\n Figure 2.10 shows the output of running the prompt engineering tactics tester, the\nprompt_engineering.py file in VS Code. When you run the tester, you can enter a\nvalue for the tactic you want to test and watch it run.\nIn the following sections, we\u2019ll explore each prompt tactic in more detail. We\u2019ll also\nexamine the various examples.\n2.3.1\nCreating detailed queries\nThe basic premise of this tactic is to provide as much detail as possible but also to be\ncareful not to give irrelevant details. The following listing shows the JSON Lines file\nexamples for exploring this tactic.\n \n \nLoads the \nprompt and \nparses it into \nmessages\nSubmits the \nprompt to an \nOpenAI LLM\nFigure 2.10\nThe output of the prompt engineering tactics tester\n\n\n29\n2.3\nPrompting LLMs with prompt engineering\n[                       \n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is an agent?\"     \n    }\n]\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"\nWhat is a GPT Agent? \nPlease give me 3 examples of a GPT agent\n\"\"\"                                       \n    }\n]\nThis example demonstrates the difference between using detailed queries and not. It\nalso goes a step further by asking for examples. Remember, the more relevance and\ncontext you can provide in your prompt, the better the overall response. Asking for\nexamples is another way of enforcing the relationship between the question and the\nexpected output.\n2.3.2\nAdopting personas\nAdopting personas grants the ability to define an overarching context or set of rules to\nthe LLM. The LLM can then use that context and/or rules to frame all later output\nresponses. This is a compelling tactic and one that we\u2019ll make heavy use of throughout\nthis book.\n Listing 2.10 shows an example of employing two personas to answer the same ques-\ntion. This can be an enjoyable technique for exploring a wide range of novel applica-\ntions, from getting demographic feedback to specializing in a specific task or even\nrubber ducking.\nListing 2.9\ndetailed_queries.jsonl\nGPT rubber ducking\nRubber ducking is a problem-solving technique in which a person explains a problem\nto an inanimate object, like a rubber duck, to understand or find a solution. This\nmethod is prevalent in programming and debugging, as articulating the problem aloud\noften helps clarify the problem and can lead to new insights or solutions.\nThe first example doesn\u2019t \nuse detailed queries.\nFirst ask the LLM a \nvery general question.\nAsk a more specific question, \nand ask for examples.\n\n\n30\nCHAPTER 2\nHarnessing the power of large language models\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nYou are a 20 year old female who attends college \nin computer science. Answer all your replies as \na junior programmer.\n\"\"\"                        \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the best subject to study.\"\n    }\n]\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nYou are a 38 year old male registered nurse. \nAnswer all replies as a medical professional.\n\"\"\"                                            \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the best subject to study.\"\n    }\n]\nA core element of agent profiles is the persona. We\u2019ll employ various personas to assist\nagents in completing their tasks. When you run this tactic, pay particular attention to\nthe way the LLM outputs the response.\n2.3.3\nUsing delimiters\nDelimiters are a useful way of isolating and getting the LLM to focus on some part of a\nmessage. This tactic is often combined with other tactics but can work well inde-\npendently. The following listing demonstrates two examples, but there are several\nother ways of describing delimiters, from XML tags to using markdown.\n[\n    {\n(continued)\nGPT rubber ducking uses the same technique, but instead of an inanimate object, we\nuse an LLM. This strategy can be expanded further by giving the LLM a persona spe-\ncific to the desired solution domain.\nListing 2.10\nadopting_personas.jsonl\nListing 2.11\nusing_delimiters.jsonl\nFirst persona\nSecond persona\n\n\n31\n2.3\nPrompting LLMs with prompt engineering\n        \"role\": \"system\",\n        \"content\": \"\"\"\nSummarize the text delimited by triple quotes \nwith a haiku.\n\"\"\"              \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"A gold chain is cool '''but a silver chain is better'''\"\n    }\n]\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nYou will be provided with a pair of statements \n(delimited with XML tags) about the same topic. \nFirst summarize the arguments of each statement. \nThen indicate which of them makes a better statement\n and explain why.\n\"\"\"                       \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"\n<statement>gold chains are cool</statement>\n<statement>silver chains are better</statement>\n\"\"\"\n    }\n]\nWhen you run this tactic, pay attention to the parts of the text the LLM focuses on\nwhen it outputs the response. This tactic can be beneficial for describing information\nin a hierarchy or other relationship patterns.\n2.3.4\nSpecifying steps\nSpecifying steps is another powerful tactic that can have many uses, including in\nagents, as shown in listing 2.12. It\u2019s especially powerful when developing prompts or\nagent profiles for complex multistep tasks. You can specify steps to break down these\ncomplex prompts into a step-by-step process that the LLM can follow. In turn, these\nsteps can guide the LLM through multiple interactions over a more extended conver-\nsation and many iterations.\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nUse the following step-by-step instructions to respond to user inputs.\nStep 1 - The user will provide you with text in triple single quotes. \nSummarize this text in one sentence with a prefix that says 'Summary: '.\nListing 2.12\nspecifying_steps.jsonl \nThe delimiter is defined by \ncharacter type and repetition.\nThe delimiter is defined \nby XML standards.\n\n\n32\nCHAPTER 2\nHarnessing the power of large language models\nStep 2 - Translate the summary from Step 1 into Spanish, \nwith a prefix that says 'Translation: '.\n\"\"\"                                         \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"'''I am hungry and would like to order an appetizer.'''\"\n    }\n]\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nUse the following step-by-step instructions to respond to user inputs.\nStep 1 - The user will provide you with text. Answer any questions in \nthe text in one sentence with a prefix that says 'Answer: '.\nStep 2 - Translate the Answer from Step 1 into a dad joke,\n with a prefix that says 'Dad Joke: '.\"\"\"                     \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the tallest structure in Paris?\"\n    }\n]\n2.3.5\nProviding examples\nProviding examples is an excellent way to guide the desired output of an LLM. There\nare numerous ways to demonstrate examples to an LLM. The system message/prompt\ncan be a helpful way to emphasize general output. In the following listing, the example\nis added as the last LLM assistant reply, given the prompt \u201cTeach me about Python.\u201d\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nAnswer all replies in a consistent style that follows the format, \nlength and style of your previous responses.\nExample:\n  user:\n       Teach me about Python.\n  assistant:                                               \n       Python is a programming language developed in 1989\n by Guido van Rossum.\n  Future replies:\n       The response was only a sentence so limit\n all future replies to a single sentence.\n\"\"\"                                          \n    },\n    {\n        \"role\": \"user\",\nListing 2.13\nproviding_examples.jsonl\nNotice the tactic of \nusing delimiters.\nSteps can be \ncompletely \ndifferent \noperations.\nInjects the \nsample output as \nthe \u201cprevious\u201d \nassistant reply\nAdds a limit output tactic to \nrestrict the size of the output \nand match the example\n\n\n33\n2.3\nPrompting LLMs with prompt engineering\n        \"content\": \"Teach me about Java.\"\n    }\n]\nProviding examples can also be used to request a particular output format from a\ncomplex series of tasks that derive the output. For example, asking an LLM to pro-\nduce code that matches a sample output is an excellent use of examples. We\u2019ll employ\nthis tactic throughout the book, but other methods exist for guiding output.\n2.3.6\nSpecifying output length\nThe tactic of specifying output length can be helpful in not just limiting tokens but\nalso in guiding the output to a desired format. Listing 2.14 shows an example of using\ntwo different techniques for this tactic. The first limits the output to fewer than 10\nwords. This can have the added benefit of making the response more concise and\ndirected, which can be desirable for some use cases. The second example demon-\nstrates limiting output to a concise set of bullet points. This method can help narrow\ndown the output and keep answers short. More concise answers generally mean the\noutput is more focused and contains less filler.\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nSummarize all replies into 10 or fewer words.\n\"\"\"                                              \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Please tell me an exciting fact about Paris?\"\n    }\n]\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nSummarize all replies into 3 bullet points.\n\"\"\"                                          \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Please tell me an exciting fact about Paris?\"\n    }\n]\nKeeping answers brief can have additional benefits when developing multi-agent sys-\ntems. Any agent system that converses with other agents can benefit from more con-\ncise and focused replies. It tends to keep the LLM more focused and reduces noisy\ncommunication.\nListing 2.14\nspecifying_output_length.jsonl\nRestricting the output \nmakes the answer \nmore concise.\nRestricts the answer \nto a short set of \nbullets\n",
      "page_number": 50
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 58-66)",
      "start_page": 58,
      "end_page": 66,
      "detection_method": "topic_boundary",
      "content": "34\nCHAPTER 2\nHarnessing the power of large language models\n Be sure to run through all the examples of the prompt tactics for this strategy. As\nmentioned, we\u2019ll cover other prompt engineering strategies and tactics in future chap-\nters. We\u2019ll finish this chapter by looking at how to pick the best LLM for your use case.\n2.4\nChoosing the optimal LLM for your specific needs\nWhile being a successful crafter of AI agents doesn\u2019t require an in-depth understand-\ning of LLMs, it\u2019s helpful to be able to evaluate the specifications. Like a computer\nuser, you don\u2019t need to know how to build a processor to understand the differences\nin processor models. This analogy holds well for LLMs, and while the criteria may be\ndifferent, it still depends on some primary considerations.\n From our previous discussion and look at LM Studio, we can extract some funda-\nmental criteria that will be important to us when considering LLMs. Figure 2.11\nLLMs\n2\n1\n3\n4\n5\nModel Performance:\nDetermines how well a\nmodel may perform on a\ngiven benchmark, such\nas answering SAT\nquestions.\nModel Parameters (Size):\nSpeci\ufb01cs the size of the\nmodel in billions of\nparameters. Larger\nmodels typically perform\nbetter on general tasks.\nUse Case (Model Type):\nDetermines the type of\nmodel and expected use\ncase. This could be chat\ncompletions for a model\nlike ChatGPT.\nTraining Input: Speci\ufb01es\nthe material used to train\nthe model. This can\nrange from everything on\nthe internet to a speci\ufb01c\ndomain Python code.\nTraining Method: Speci\ufb01es\nhow the model is trained\nand/or \ufb01ne-tuned. Models\nlike ChatGPT are trained\nusing reinforcement learning\nwith human feedback.\n6\n7\n8\nContext Token Size:\nSpeci\ufb01es how large the\nmodel\u2019s context size is in\ntokens. Large context is\nimportant for verbose agent\nconversations.\nModel Speed (Model\nDeployment): Denotes the\nspeed of the model. OpenAI\nmodels marked Turbo are\ntypically faster. For local LLMs,\nspeed will be determined by\nthe infrastructure.\nModel Cost (Project\nBudget): Could represent\nthe price of the service or\nthe cost to host and run a\nmodel on your infrastructure.\nFigure 2.11\nThe important criteria to consider when consuming an LLM\n\n\n35\n2.4\nChoosing the optimal LLM for your specific needs\nexplains the essential criteria to define what makes an LLM worth considering for cre-\nating a GPT agent or any LLM task.\n For our purposes of building AI agents, we need to look at each of these criteria in\nterms related to the task. Model context size and speed could be considered the sixth\nand seventh criteria, but they are usually considered variations of a model deployment\narchitecture and infrastructure. An eighth criterion to consider for an LLM is cost,\nbut this depends on many other factors. Here is a summary of how these criteria relate\nto building AI agents:\n\uf0a1Model performance\u2014You\u2019ll generally want to understand the LLM\u2019s performance\nfor a given set of tasks. For example, if you\u2019re building an agent specific to cod-\ning, then an LLM that performs well on code will be essential.\n\uf0a1Model parameters (size)\u2014The size of a model is often an excellent indication of\ninference performance and how well the model responds. However, the size of\na model will also dictate your hardware requirements. If you plan to use your\nown locally hosted model, the model size will also primarily dictate the com-\nputer and GPU you need. Fortunately, we\u2019re seeing small, very capable open\nsource models being released regularly.\n\uf0a1Use case (model type)\u2014The type of model has several variations. Chat completions\nmodels such as ChatGPT are effective for iterating and reasoning through a\nproblem, whereas models such as completion, question/answer, and instruct\nare more related to specific tasks. A chat completions model is essential for\nagent applications, especially those that iterate.\n\uf0a1Training input\u2014Understanding the content used to train a model will often dic-\ntate the domain of a model. While general models can be effective across tasks,\nmore specific or fine-tuned models can be more relevant to a domain. This may\nbe a consideration for a domain-specific agent where a smaller, more fine-tuned\nmodel may perform as well as or better than a larger model such as GPT-4.\n\uf0a1Training method\u2014It\u2019s perhaps less of a concern, but it can be helpful to under-\nstand what method was used to train a model. How a model is trained can\naffect its ability to generalize, reason, and plan. This can be essential for plan-\nning agents but perhaps less significant for agents than for a more task-specific\nassistant.\n\uf0a1Context token size\u2014The context size of a model is more specific to the model\narchitecture and type. It dictates the size of context or memory the model may\nhold. A smaller context window of less than 4,000 tokens is typically more than\nenough for simple tasks. However, a large context window can be essential when\nusing multiple agents\u2014all conversing over a task. The models will typically be\ndeployed with variations on the context window size.\n\uf0a1Model speed (model deployment)\u2014The speed of a model is dictated by its inference\nspeed (or how fast a model replies to a request), which in turn is dictated by the\ninfrastructure it runs on. If your agent isn\u2019t directly interacting with users, raw\n\n\n36\nCHAPTER 2\nHarnessing the power of large language models\nreal-time speed may not be necessary. On the other hand, an LLM agent inter-\nacting in real time needs to be as quick as possible. For commercial models,\nspeed will be determined and supported by the provider. Your infrastructure\nwill determine the speed for those wanting to run their LLMs.\n\uf0a1Model cost (project budget)\u2014The cost is often dictated by the project. Whether\nlearning to build an agent or implementing enterprise software, cost is always a\nconsideration. A significant tradeoff exists between running your LLMs versus\nusing a commercial API.\nThere is a lot to consider when choosing which model you want to build a production\nagent system on. However, picking and working with a single model is usually best for\nresearch and learning purposes. If you\u2019re new to LLMs and agents, you\u2019ll likely want\nto choose a commercial option such as GPT-4 Turbo. Unless otherwise stated, the\nwork in this book will depend on GPT-4 Turbo.\n Over time, models will undoubtedly be replaced by better models. So you may\nneed to upgrade or swap out models. To do this, though, you must understand the\nperformance metrics of your LLMs and agents. Fortunately, in chapter 9, we\u2019ll explore\nevaluating LLMs, prompts, and agent profiles with prompt flow.\n2.5\nExercises\nUse the following exercises to help you engage with the material in this chapter:\n\uf0a1Exercise 1\u2014Consuming Different LLMs\nObjective\u2014Use the connecting.py code example to consume a different LLM\nfrom OpenAI or another provider.\nTasks:\n\u2013 Modify connecting.py to connect to a different LLM.\n\u2013 Choose an LLM from OpenAI or another provider.\n\u2013 Update the API keys and endpoints in the code.\n\u2013 Execute the modified code and validate the response.\n\uf0a1Exercise 2\u2014Exploring Prompt Engineering Tactics\nObjective\u2014Explore various prompt engineering tactics, and create variations\nfor each.\nTasks:\n\u2013 Review the prompt engineering tactics covered in the chapter.\n\u2013 Write variations for each tactic, experimenting with different phrasing and\nstructures.\n\u2013 Test the variations with an LLM to observe different outcomes.\n\u2013 Document the results, and analyze the effectiveness of each variation.\n\uf0a1Exercise 3\u2014Downloading and Running an LLM with LM Studio\nObjective\u2014Download an LLM using LM Studio, and connect it to prompt engi-\nneering tactics.\n\n\n37\nSummary\nTasks:\n\u2013 Install LM Studio on your machine.\n\u2013 Download an LLM using LM Studio.\n\u2013 Serve the model using LM Studio.\n\u2013 Write Python code to connect to the served model.\n\u2013 Integrate the prompt engineering tactics example with the served model.\n\uf0a1Exercise 4\u2014Comparing Commercial and Open source LLMs\nObjective\u2014Compare the performance of a commercial LLM such as GPT-4\nTurbo with an open source model using prompt engineering examples.\nTasks:\n\u2013 Implement the prompt engineering examples using GPT-4 Turbo.\n\u2013 Repeat the implementation using an open source LLM.\n\u2013 Evaluate the models based on criteria such as response accuracy, coherence,\nand speed.\n\u2013 Document the evaluation process, and summarize the findings.\n\uf0a1Exercise 5\u2014Hosting Alternatives for LLMs\nObjective\u2014Contrast and compare alternatives for hosting an LLM versus using a\ncommercial model.\nTasks:\n\u2013 Research different hosting options for LLMs (e.g., local servers, cloud services).\n\u2013 Evaluate the benefits and drawbacks of each hosting option.\n\u2013 Compare these options to using a commercial model in terms of cost, perfor-\nmance, and ease of use.\n\u2013 Write a report summarizing the comparison and recommending the best\napproach based on specific use cases.\nSummary\n\uf0a1LLMs use a type of architecture called generative pretrained transformers (GPTs).\n\uf0a1Generative models (e.g., LLMs and GPTs) differ from predictive/classification\nmodels by learning how to represent data and not simply classify it.\n\uf0a1LLMs are a collection of data, architecture, and training for specific use cases,\ncalled fine-tuning.\n\uf0a1The OpenAI API SDK can be used to connect to an LLM from models, such as\nGPT-4, and also used to consume open source LLMs.\n\uf0a1You can quickly set up Python environments and install the necessary packages\nfor LLM integration.\n\uf0a1LLMs can handle various requests and generate unique responses that can be\nused to enhance programming skills related to LLM integration.\n\uf0a1Open source LLMs are an alternative to commercial models and can be hosted\nlocally using tools such as LM Studio.\n\n\n38\nCHAPTER 2\nHarnessing the power of large language models\n\uf0a1Prompt engineering is a collection of techniques that help craft more effective\nprompts to improve LLM responses.\n\uf0a1LLMs can be used to power agents and assistants, from simple chatbots to fully\ncapable autonomous workers.\n\uf0a1Selecting the most suitable LLM for specific needs depends on the perfor-\nmance, parameters, use case, training input, and other criteria.\n\uf0a1Running LLMs locally requires a variety of skills, from setting up GPUs to under-\nstanding various configuration options.\n\n\n39\nEngaging\nGPT assistants\nAs we explore the OpenAI crusade into assistants and what has been hinted at, ulti-\nmately, an agent platform called GPT Assistants, we\u2019ll introduce GPT assistants\nthrough the ChatGPT interface. Then, we\u2019ll add in several fully developed assis-\ntants that can suggest recipes from ingredients, fully analyze data as a data scientist,\nguide readers through books, and be extended with custom actions. By the end of\nthe chapter, we\u2019ll be ready to build a fully functional agent that can be published to\nthe OpenAI GPT Store.\nThis chapter covers\n\uf0a1Introducing the OpenAI GPT Assistants platform \nand the ChatGPT UI \n\uf0a1Building a GPT that can use the code \ninterpretation capabilities\n\uf0a1Extending an assistant via custom actions \n\uf0a1Adding knowledge to a GPT via file uploads\n\uf0a1Commercializing your GPT and publishing it to \nthe GPT Store \n\n\n40\nCHAPTER 3\nEngaging GPT assistants\n3.1\nExploring GPT assistants through ChatGPT\nChatGPT (ChatGPT Plus, at the time of writing) allows you to build GPT assistants,\nconsume other assistants, and even publish them, as you\u2019ll see by the end of the chap-\nter. When OpenAI announced the release of the GPT Assistants platform, it helped\ndefine and solidify the emergence of AI agents. As such, it\u2019s worth a serious review by\nanyone interested in building and consuming agent systems. First, we\u2019ll look at build-\ning GPT assistants through ChatGPT Plus, which requires a premium subscription. If\nyou don\u2019t want to purchase a subscription, browse this chapter as a primer, and chap-\nter 6 will demonstrate consuming the API service later.\n Figure 3.1 shows the page for the GPT Store within ChatGPT (https://chatgpt\n.com/gpts). From here, you can search and explore various GPTs for virtually any\ntask. The amount of usage will typically indicate how well each GPT works, so gauge\nwhich works best for you. \nCreating your first GPT Assistant is as simple as clicking the Create button and follow-\ning along with the GPT Builder chat interface. Figure 3.2 shows using the Builder to\nExplore the GPTs\nyou have made.\nSearch for GPTs by\nname of category.\nClick Create to create\nyour own GPT.\nFigure 3.1\nThe main interface to the GPT Store\n\n\n41\n3.1\nExploring GPT assistants through ChatGPT\ncreate a GPT. Working through this exercise a couple of times can be a great way to\nstart understanding an assistant\u2019s requirements.\nAfter working with the Builder, you can open the manual configuration panel, shown\nin figure 3.3, and edit the GPT directly. You\u2019ll see the name, description, instruc-\ntions, and conversation starters populated from your conversations with the Builder.\nThis can be a great start, but generally, you\u2019ll want to edit and tweak these proper-\nties manually.\n If you want to follow along with building your own Culinary Companion, enter the\ntext from listing 3.1 into the instructions. These instructions were partly generated by\nconversing with the Builder and added based on explicit outputs. The explicit outputs\nare added to the instructions as rules. \n \n \n \n \nThe Builder can guide you\nthrough the creation process.\nDialog with the Builder\nto create your GPT.\nFigure 3.2\nInteracting with \nthe GPT Builder to create an \nassistant\n\n\n42\nCHAPTER 3\nEngaging GPT assistants\nCulinary Companion assists users with a friendly, engaging tone, \nreminiscent of the famous chef Julia Child.    \nIt provides quick meal ideas and simplifies complex recipes, focusing on \ningredients the user already has. This GPT emphasizes practical, easy-\nto-follow culinary advice and adapts to dietary preferences. It's \ndesigned to make cooking a more accessible and enjoyable experience, \nencouraging users to experiment with their meals while offering helpful \nListing 3.1\nInstructions for Culinary Companion\nUsing the Builder will generate a set\nof instructions for your assistant, or\nyou can start completely from scratch.\nThe Con\ufb01gure panel gives\nyou direct control over your\nGPT\u2019s properties.\nA good name and description\nwill be essential if you plan on\npublishing this agent.\nGood conversation starters can help\nusers quickly understand what your\nagent does and how it works.\nFigure 3.3\nThe Configure panel of the GPT Assistants platform interface\nPersonality or persona\nof your assistant\n",
      "page_number": 58
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 67-76)",
      "start_page": 67,
      "end_page": 76,
      "detection_method": "topic_boundary",
      "content": "43\n3.1\nExploring GPT assistants through ChatGPT\ntips in a warm, approachable manner.    \nRULES:\nWhen generating a recipe, always create an image of the final prepared \nrecipe.                                                                  \nWhen generating a recipe, estimate the calories and nutritional values \nper serving.                                                             \nWhen generating a recipe, provide a shopping list of ingredients with \nestimated prices needed to complete the recipe.                          \nWhen generating a recipe, estimate the total cost per serving based on \nthe shopping list.                                                       \nDefining rules for an assistant/agent essentially creates a template for what the agent\nwill produce. Adding rules ensures that the GPT output is consistent and aligned with\nyour expectations of how the agent should operate. Defining and giving an agent/\nassistant a persona provides them with a unique and memorable personality.\nNOTE\nGiving an agent/assistant a particular personality can make a differ-\nence in the type and form of output. Asking a cooking agent to speak as the\nfirst celebrity chef, Julia Child, not only provides for a fun tone but also\nengages more references that may mention or talk about her cooking style\nand teaching. When constructing an agent/assistant, assigning a particular\npersona/personality can be helpful.\nWith just these few steps, we have a culinary companion that not only gives us recipes\nfor ingredients we have on hand but also generates an image of the finished recipe,\nestimates the nutritional value, creates a shopping list with an estimate of prices, and\nbreaks down the cost per serving.\n Try the assistant by requesting a recipe and providing a list of ingredients you have\nor prefer. Listing 3.2 shows an example of a simple request with extra information to\nset the mood. Of course, you can add any ingredients or situations you like and then\nsee the results.\nI have a bag of prepared frozen chicken strips and I want to make a \nromantic dinner for two.\nFigure 3.4 shows the formatted output results from the GPT provided by the prompt.\nIt certainly looks good enough to eat. All of this output was generated because of the\ninstructions we provided the agent. \n While the output results look great, they may not all be factual and correct, and\nyour results may vary. For instance, the GPT added chicken strips to the shopping list\nwhen we had already suggested having those ingredients. Furthermore, the prices and\nestimated nutritional information are just estimates, but this can be resolved later if\nthey interest you.\nListing 3.2\nPrompting the recipe\nGeneral guidelines of the \nagent\u2019s role and goal \nA set of rules the agent will\nfollow when suggesting a recipe\n\n\n44\nCHAPTER 3\nEngaging GPT assistants\nOut of the box, though, GPT Assistants is quite impressive for quickly building a proof-\nof-concept assistant or agent. As you\u2019ll see later in the chapter, it also provides an excel-\nlent platform for consuming assistants outside ChatGPT. In the next section, we\u2019ll look\nat more impressive features GPTs provide, such as file uploads and code interpretation.\n3.2\nBuilding a GPT that can do data science\nThe GPT Assistants platform has and will likely be extended to include various agent\ncomponents. Currently, GPT Assistants support what is referred to as knowledge,\nmemory, and actions. In chapter 8, we\u2019ll discuss the details of knowledge and memory,\nand in chapter 5, we cover the concept of tool use through actions.\n In our next exercise, we\u2019ll build an assistant to perform a first-pass data science\nreview of any CSV document we provide. This agent will use the ability or action that\nallows for coding and code interpretation. When you enable code interpretation, the\nassistant will allow file uploads by default.\n Before we do that, though, we want to design our agent, and what better way to do\nthat than to ask an LLM to build us an assistant? Listing 3.3 shows the prompt request-\ning ChatGPT (GPT-4) to design a data science assistant. Notice how we\u2019re not asking\nfor everything in a single prompt but instead iterating over the information returned\nby the LLM.\nFIRST PROMPT:    \nwhat is a good basic and interesting data science \nexperiment you can task someone with a single \ncsv file that contains interesting data?    \nListing 3.3\nPrompting for a data science assistant\nThe GPT assistant creates\nthe recipe from the provided\ningredients and provides a\nshopping list with costs and\ncost per ser\nnutritional\nving,\ninformation per serving, and\ninstructions on how to cook\nthe recipe.\nChicken Parmesan with Spaghetti Recipe\nIngredients:\n1 bag of frozen prepared chicken strips\n1 jar of your favorite marinara sauce\n1 cup of shredded mozzarella cheese\n1/4 cup of grated Parmesan cheese\n200g of spaghetti\n1 tablespoon of olive oil\n2 cloves of garlic, minced\nFresh basil leaves (for garnish)\nSalt and pepper to taste\nEstimated Costs:\nChicken strips: $6.00\nMarinara sauce: $3.00\nMozzarella cheese: $2.00\nParmesan cheese: $1.50\nSpaghetti: $1.00\nGarlic: $0.30\nBasil: $0.50\nTotal estimated cost: $14.30\nCost per serving: approximately $7.15\nNutritional Values per Serving:\nCalories: 600 kcal\nProtein: 35 g\nCarbohydrates: 75 g\nFat: 18 g\nInstructions: (not shown)\nGenerated by the Culinary\nCompanion GPT Assistant\nFigure 3.4\nThe output results of the Culinary Companion GPT\nFirst, ask the \nLLM to set the \nfoundation.\n\n\n45\n3.2\nBuilding a GPT that can do data science\nSECOND PROMPT:    \nokay, can you now write all those steps into instructions \nto be used for a GPT Agent (LLM agent) to replicate all of \nthe above steps     \nTHIRD PROMPT:    \nWhat is a famous personality that can embody the agent \ndata scientist and be able to present data to users?     \nThe result of that conversation provided for the assistant instructions shown in listing\n3.4. In this case, the assistant was named Data Scout, but feel free to name your assis-\ntant what appeals to you. \nThis GPT, named Data Scout, is designed to assist users by analyzing CSV \nfiles and providing insights like Nate Silver, a famous statistician known \nfor his accessible and engaging approach to data. Data Scout combines \nrigorous analysis with a clear and approachable communication style, \nmaking complex data insights understandable. It is equipped to handle \nstatistical testing, predictive modeling, data visualization, and more, \noffering suggestions for further exploration based on solid data-driven \nevidence.\nData Scout requires the user to upload a csv file of data they want to \nanalyze. After the user uploads the file you will perform the following \ntasks:\nData Acquisition\n    Ask the user to upload a csv file of data.\n    Instructions: Use the pandas library to read the data from the CSV \nfile. Ensure the data is correctly loaded by displaying the first few rows \nusing df.head().\n2. Exploratory Data Analysis (EDA)\nData Cleaning\n    Task: Identify and handle missing values, correct data types.\n    Instructions: Check for missing values using df.isnull().sum(). For \ncategorical data, consider filling missing values with the mode, and for \nnumerical data, use the median or mean. Convert data types if necessary \nusing df.astype().\nVisualization\n    Task: Create visualizations to explore the data.\n    Instructions: Use matplotlib and seaborn to create histograms, scatter \nplots, and box plots. For example, use sns.histplot() for histograms and \nsns.scatterplot() for scatter plots.\nDescriptive Statistics\n    Task: Calculate basic statistical measures.\n    Instructions: Use df.describe() to get a summary of the statistics and \ndf.mean(), df.median() for specific calculations.\nListing 3.4\nData Scout instructions\nThen, ask the LLM to \nconvert the previous \nsteps to a more \nformal process.\nFinally, ask the LLM to \nprovide a personality \nthat can represent the \nprocess.\n\n\n46\nCHAPTER 3\nEngaging GPT assistants\n3. Hypothesis Testing\n    Task: Test a hypothesis formulated based on the dataset.\n    Instructions: Depending on the data type, perform statistical tests \nlike the t-test or chi-squared test using scipy.stats. For example, use \nstats.ttest_ind() for the t-test between two groups.\n4. Predictive Modeling\nFeature Engineering\n    Task: Enhance the dataset with new features.\n    Instructions: Create new columns in the DataFrame based on existing \ndata to capture additional information or relationships. Use operations \nlike df['new_feature'] = df['feature1'] / df['feature2'].\nModel Selection\n    Task: Choose and configure a machine learning model.\n    Instructions: Based on the task (classification or regression), select \na model from scikit-learn, like RandomForestClassifier() or \nLinearRegression(). Configure the model parameters.\nTraining and Testing\n    Task: Split the data into training and testing sets, then train the \nmodel.\n    Instructions: Use train_test_split from scikit-learn to divide the \ndata. Train the model using model.fit(X_train, y_train).\nModel Evaluation\n    Task: Assess the model performance.\n    Instructions: Use metrics like mean squared error (MSE) or accuracy. \nCalculate these using metrics.mean_squared_error(y_test, y_pred) or \nmetrics.accuracy_score(y_test, y_pred).\n5. Insights and Conclusions\n    Task: Interpret and summarize the findings from the analysis and \nmodeling.\n    Instructions: Discuss the model coefficients or feature importances. \nDraw conclusions about the hypothesis and the predictive analysis. Suggest \nreal-world implications or actions based on the results.\n6. Presentation\n    Task: Prepare a report or presentation.\n    Instructions: Summarize the process and findings in a clear and \naccessible format, using plots and bullet points. Ensure that the \npresentation is understandable for non-technical stakeholders.\nAfter generating the instructions, you can copy and paste them into the Configure\npanel in figure 3.5. Be sure to give the assistant the Code Interpretation tool (skill) by\nselecting the corresponding checkbox. You don\u2019t need to upload files here; the assis-\ntant will allow file uploads when the Code Interpretation checkbox is enabled.\n Now, we can test the assistant by uploading a CSV file and asking questions about\nit. The source code folder for this chapter contains a file called netflix_titles.csv;\nthe top few rows are summarized in listing 3.5. Of course, you can use any CSV file you\nwant, but this exercise will use the Netflix example. Note that this dataset was down-\nloaded from Kaggle, but you can use any other CSV if you prefer.\n\n\n47\n3.2\nBuilding a GPT that can do data science\nshow_id,type,title,director,cast,country,date_added,\nrelease_year,rating,duration,listed_in,description    \ns1,Movie,Dick Johnson Is Dead,Kirsten Johnson,, \nUnited States,\"September 25, 2021\",2020,PG-13,90 min,\nDocumentaries,\"As her father nears the end of his life, \nfilmmaker Kirsten Johnson stages his death in inventive \nand comical ways to help them both face the inevitable.\"    \nWe could upload the file and ask the assistant to do its thing, but for this exercise,\nwe\u2019ll be more specific. Listing 3.6 shows the prompt and uploading the file to engage\nthe assistant (including Netflix_titles.csv in the request). This example filters the\nresults to Canada, but you can, of course, use any country you want to view.\nAnalyze the attached CSV and filter the results to the \ncountry Canada and output any significant discoveries \nin trends etc.    \nIf you encounter problems with the assistant parsing the file, refresh your browser win-\ndow and try again. Depending on your data and filter, the assistant will now use the\nCode Interpreter as a data scientist would to analyze and extract trends in the data. \nListing 3.5\nnetflix_titles.csv (top row of data)\nListing 3.6\nPrompting the Data Scout\nConversation starters provide a quick\ndescription and guide the user.\nBe sure the Code Interpreter is selected.\nFigure 3.5\nTurning on the Code Interpreter tool/skill\nComma-separated \nlist of columns\nAn example row \nof data from the \ndataset\nYou can select a \ndifferent country to \nfilter the data on.\n\n\n48\nCHAPTER 3\nEngaging GPT assistants\n Figure 3.6 shows the output generated for the prompt in listing 3.5 using the net-\nflix_titles.csv file for data. Your output may look quite different if you select a dif-\nferent country or request another analysis.\nThe data science plots the assistant is building are created by writing and executing\ncode with the Code Interpreter. You can try this with other CSV files or, if you want,\ndifferent forms of data to analyze. You could even continue iterating with the assistant\nto update the plots visually or analyze other trends.\n Code interpretation is a compelling skill that you\u2019ll likely add to many of your\nagents for everything from calculations to custom formatting. In the next section, we\nlook at how to extend the capabilities of a GPT through custom actions.\nTop 10 Popular Genres in Canadian Content\nRatings Distribution\nDistribution of Content Types\nTrend of Releases over the Years\ncount\ncount\ncount\nrating\nChildren and Family\nmovies\nComedies\nDramas\nKid\u2019s TV\nInternational Movies\nThrillers\nAction and Adventure\nIndependent Movies\nDocumentaries\nHorror Movies\ntype\nrelease_year\nFigure 3.6\nThe output generated by the assistant as it analyzed the CSV data\n\n\n49\n3.3\nCustomizing a GPT and adding custom actions\n3.3\nCustomizing a GPT and adding custom actions\nIn our next exercise, we\u2019ll demonstrate the use of custom actions, which can signifi-\ncantly extend the reach of your assistant. Adding custom actions to an agent requires\nseveral components, from understanding the OpenAPI specification endpoint to con-\nnecting to a service. Therefore, before we add custom actions, we\u2019ll build another\nGPT in the next section to assist us.\n3.3.1\nCreating an assistant to build an assistant\nGiven GPTs\u2019 capabilities, it only makes sense that we use one to assist in building oth-\ners. In this section, we\u2019ll build a GPT that can help us create a service we can connect\nas a custom action to another GPT. And yes, we\u2019ll even use an LLM to begin construct-\ning our helper GPT.\n The following listing shows the prompt for creating the instructions for our helper\nGPT. This prompt is intended to generate the instructions for the assistant.\nI want to create a GPT assistant that can generate a FastAPI service that \nwill perform some action to be specified. As part of the FastAPI code \ngeneration, I want the assistant to generate the OpenAPI specification for \nthe endpoint. Please outline a set of instructions for this agent.\nListing 3.8 shows the bulk of the instructions generated for the prompt. The output\nwas then modified and slightly updated with specific information and other details.\nCopy and paste those instructions from the file (assistant_builder.txt) into your\nGPT. Be sure to select the Code Interpreter capability also.\nThis GPT is designed to assist users in generating FastAPI services \ntailored to specific actions, complete with the corresponding OpenAPI \nspecifications for the endpoints. The assistant will provide code snippets \nand guidance on structuring and documenting API services using FastAPI, \nensuring that the generated services are ready for integration and \ndeployment.\n1.   Define the Action and Endpoint: First, determine the specific action \nthe FastAPI service should perform. This could be anything from fetching \ndata, processing information, or interacting with other APIs or databases.\n2.    Design the API Endpoint: Decide on the HTTP method (GET, POST, PUT, \nDELETE, etc.) and the endpoint URI structure. Define the input parameters \n(path, query, or body parameters) and the expected response structure.\n3. Generate FastAPI Code:\n        Setup FastAPI: Import FastAPI and other necessary libraries.\n        Create API Function: Write a Python function that performs the \ndesired action. This function should accept the defined input parameters \nand return the appropriate response.\nListing 3.7\nPrompting the helper design (in GPT Builder or ChatGPT)\nListing 3.8\nCustom action assistant instructions\n\n\n50\nCHAPTER 3\nEngaging GPT assistants\n4. Decorate the Function: Use FastAPI's decorators (e.g., \n@app.get(\"/endpoint\")) to link the function with the specified endpoint \nand HTTP method.\n        Define Input and Output Models: Use Pydantic models to define the \nstructure of the input and output data. This ensures validation and \nserialization of the data.\n5. Generate OpenAPI Specification:\n        FastAPI automatically generates the OpenAPI specification based on \nthe endpoint definitions and Pydantic models. Ensure that all function \nparameters and models are well-documented using docstrings and field \ndescriptions.\n        Optionally, customize the OpenAPI specification by adding \nmetadata, tags, or additional responses directly in the FastAPI decorators.\n6. Deployment:\n        Describe to the user how to prepare the FastAPI application for \ndeployment. \n        Instruct them on how to use ngrok to deploy the \nservice and host it on the user's local machine.     \nAfter preparing the assistant, ensure everything is set in the Configure panel (includ-\ning setting the Code Interpreter checkbox), and then refresh your browser window.\nThis will prepare the assistant for a new session. You can request the kind of service\nyou want to build from here.\n Listing 3.9 shows the request to the Custom Action Assistant to create a daily task\nendpoint. If you understand how APIs work, you can suggest other options, such as\nPOST. Of course, you can also ask the assistant to guide you and create your service.\nI want to define a GET endpoint that replies with my list of daily tasks  \nAfter you enter the prompt, the assistant will generate the code and instructions for\ncreating and running the FastAPI endpoint. The following listing shows an example\nof the code generated from the previous request.\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom typing import List\napp = FastAPI()\nclass Task(BaseModel):   \n    id: int\n    description: str\n    completed: bool\ntasks = [    \n    Task(id=1, description=\"Buy groceries\", completed=False),\n    Task(id=2, description=\"Read a book\", completed=True),\nListing 3.9\nPrompt requesting task endpoint service\nListing 3.10\ndaily_tasks_api.py (generated from assistant)\nThis uses ngrok as an \nexample to deploy the \nservice locally.\nUse Pydantic to create \na type for the task.\nThis is a static list of \ntasks to demonstrate.\n\n\n51\n3.3\nCustomizing a GPT and adding custom actions\n    Task(id=3, description=\"Complete FastAPI project\", completed=False),\n]\n@app.get(\"/tasks\", response_model=List[Task])   \nasync def get_tasks():\n    \"\"\"\n    Retrieve a list of daily tasks.\n    \"\"\"\n    return tasks\nEnter the code into Visual Studio Code (VS Code), and confirm that fastapi and\nuvicorn are installed with pip. Then, run the API using the command shown in the\nfollowing listing, which runs the API in the chapter source file.\nuvicorn daily_tasks_api:app \u2013reload     \nOpen a browser to http://127.0.0.1:8000/docs, the default location for the Swagger\nendpoint, as shown in figure 3.7.\nListing 3.11\nRunning the API\nThe tasks endpoint\nChange the name of the module/file \nif you\u2019re using something different.\nNavigate your browser to\nhttp://127.0.0. l :8000/docs.\nClick this link to open and view\nthe OpenAPI speci\ufb01cation in JSON.\nFigure 3.7\nNavigating the Swagger docs and getting the openapi.json document\n\n\n52\nCHAPTER 3\nEngaging GPT assistants\nClicking the /openapi.json link will display the OpenAPI specification for the end-\npoint, as shown in listing 3.12 (JSON converted to YAML). You\u2019ll need to copy and\nsave this document for later use when setting up the custom action on the agent. The\nendpoint produces JSON, but you can also use specifications written in YAML.\nopenapi: 3.1.0\ninfo:\n  title: FastAPI\n  version: 0.1.0\npaths:\n  /tasks:\n    get:\n      summary: Get Tasks\n      description: Retrieve a list of daily tasks.\n      operationId: get_tasks_tasks_get\n      responses:\n        '200':\n          description: Successful Response\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  $ref: '#/components/schemas/Task'\n                title: Response Get Tasks Tasks Get\ncomponents:\n  schemas:\n    Task:\n      type: object\n      properties:\n        id:\n          type: integer\n          title: Id\n        description:\n          type: string\n          title: Description\n        completed:\n          type: boolean\n          title: Completed\n      required:\n        - id\n        - description\n        - completed\n      title: Task\nBefore connecting an assistant to the service, you must set up and use ngrok to open a\ntunnel to your local machine running the service. Prompt the GPT to provide the\nListing 3.12\nOpenAPI specification for the task API \n",
      "page_number": 67
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 77-84)",
      "start_page": 77,
      "end_page": 84,
      "detection_method": "topic_boundary",
      "content": "53\n3.3\nCustomizing a GPT and adding custom actions\ninstructions and help you set up ngrok, and run the application to open an endpoint\nto port 8000 on your machine, as shown in listing 3.13. If you change the port or use a\ndifferent configuration, you must update it accordingly.\n./ngrok authtoken <YOUR_AUTHTOKEN>     \n./ngrok http 8000     \nAfter you run ngrok, you\u2019ll see an external URL that you can now use to access the ser-\nvice on your machine. Copy this URL for later use when setting up the assistant. In the\nnext section, we\u2019ll create the assistant that consumes this service as a custom action.\n3.3.2\nConnecting the custom action to an assistant\nWith the service up and running on your machine and accessible externally via the\nngrok tunnel, we can build the new assistant. This time, we\u2019ll create a simple assistant\nto help us organize our daily tasks, where the tasks will be accessible from our locally\nrunning task service.\n Open the GPT interface and the Configure panel, and copy and paste the instruc-\ntions shown in listing 3.14 into the new assistant. Be sure to name the assistant and\nenter a helpful description as well. Also, turn on the Code Interpreter capability to\nallow the assistant to create the final plot, showing the tasks.\nTask Organizer is designed to help the user prioritize their daily tasks \nbased on urgency and time availability, providing structured guidance on \nhow to categorize tasks by urgency and suggesting optimal time blocks for \ncompleting these tasks. It adopts a persona inspired by Tim Ferriss, known \nfor his focus on productivity and efficiency. It uses clear, direct \nlanguage and avoids making assumptions about the user's free time.\nWhen you are done organizing the tasks create a plot \nshowing when and how the tasks will be completed.     \nClick the Create New Action button at the bottom of the panel. Figure 3.8 shows the\ninterface for adding a custom action. You must copy and paste the OpenAPI specifica-\ntion for your service into the window. Then, you must add a new section called servers\nand populate that with your URL, as shown in the figure.\n \n \n \nListing 3.13\nRunning ngrok (following the instructions setup)\nListing 3.14\nTask Organizer (task_organizer_assistant.txt)\nEnter your auth token \nobtained from ngrok.com.\nOpens a tunnel on port 8000 \nto external internet traffic\nThis feature requires \nthe Code Interpreter \nto be enabled.\n\n\n54\nCHAPTER 3\nEngaging GPT assistants\nAfter the specification is set, you can test it by clicking the Test button. This will run a\ntest, and you\u2019ll see the results shown in the conversation window, as shown in figure 3.9.\n After you\u2019re satisfied, everything is set. Refresh your browser window to reset the\nsession, and enter something like the prompt shown in listing 3.15. This will prompt\nthe agent to call the service to get your daily tasks, summarize the output, and solve\nyour task organization dilemma.\nhow should I organize my tasks for today?\nThe assistant should produce a plot of the task schedule at the end. If it gets this\nwrong or the formatting isn\u2019t what you prefer, you can add instructions to specify the\nformat/style the assistant should output.\nListing 3.15\nTask Organizer prompt\nAdd the servers section to the YAML, and\nenter the ngrok URL for your service.\nCopy and paste the YAML or JSON\nspeci\ufb01cation into the window.\nFigure 3.8\nAdding a new custom action\n\n\n55\n3.3\nCustomizing a GPT and adding custom actions\nYou can improve the service, but if you make any changes to the API, the specification\nin the assistant custom actions will need to be updated. From here, though, you can\nadd custom action services run from your computer or hosted as a service.\nNOTE\nBe aware that unknown users can activate custom actions if you pub-\nlish an assistant for public consumption, so don\u2019t expose services that charge\nyou a service fee or access private information unless that is your intention.\nLikewise, services opened through an ngrok tunnel will be exposed through\nthe assistant, which may be of concern. Please be careful when publishing\nagents that consume custom actions.\nCustom actions are a great way to add dynamic functionality to an assistant, whether\nfor personal or commercial use. File uploads are a better option for providing an assis-\ntant with static knowledge. The next section will explore using file uploads to extend\nan assistant\u2019s knowledge.\nReturned list of tasks\nTesting the endpoint\nFigure 3.9\nTesting the API service endpoint is correctly configured as a custom action\n\n\n56\nCHAPTER 3\nEngaging GPT assistants\n3.4\nExtending an assistant\u2019s knowledge using file uploads\nIf you\u2019ve engaged with LLMs, you likely have heard about the retrieval augmented\ngeneration (RAG) pattern. Chapter 8 will explore RAG in detail for the application of\nboth knowledge and memory. Detailed knowledge of RAG isn\u2019t required to use the\nfile upload capability, but if you need some foundation, check out that chapter.\n The GPT Assistants platform provides a knowledge capability called file uploads,\nwhich allows you to populate the GPT with a static knowledge base about anything in\nvarious formats. As of writing, the GPT Assistants platform allows you to upload up to\n512 MB of documents. In the next two exercises, we\u2019ll look at two different GPTs\ndesigned to assist users with consuming books.\n3.4.1\nBuilding the Calculus Made Easy GPT\nBooks and written knowledge will always be the backbone of our knowledge base. But\nreading text is a full-time concerted effort many people don\u2019t have time for. Audio-\nbooks made consuming books again accessible; you could listen while multitasking,\nbut not all books transitioned well to audio.\n Enter the world of AI and intelligent assistants. With GPTs, we can create an inter-\nactive experience between the reader and the book. No longer is the reader forced to\nconsume a book page by page but rather as a whole.\n To demonstrate this concept, we\u2019ll build a GPT based on a classic math text called\nCalculus Made Easy, by Silvanus P. Thompson. The book is freely available through the\nGutenberg Press website. While it\u2019s more than a hundred years old, it still provides a\nsolid material background.\nNOTE\nIf you\u2019re serious about learning calculus but this assistant is still too\nadvanced, check out a great book by Clifford A. Pickover called Calculus and\nPizza. It\u2019s a great book for learning calculus or just to get an excellent\nrefresher. You could also try making your Calculus and Pizza assistant if you\nhave an eBook version. Unfortunately, copyright laws would prevent you from\npublishing this GPT without permission.\nOpen ChatGPT, go to My GPTs, create a new GPT, click the Configure tab, and then\nupload the file, as shown in figure 3.10. Upload the book from the chapter\u2019s source\ncode folder: chapter _03/calculus_made_easy.pdf. This will add the book to the\nGPT\u2019s knowledge.\n Scroll up and add the instructions shown in listing 3.16. The initial preamble text\nwas generated by conversing with the GPT Builder. After updating the preamble text,\na personality was added by asking ChatGPT for famous mathematicians. Then, finally,\nrules were added to provide additional guidance to the GPT on what explicit out-\ncomes we want.\n \n \n \n\n\n57\n3.4\nExtending an assistant\u2019s knowledge using file uploads\nThis GPT is designed to be an expert teacher and mentor \nof calculus based on the book 'Calculus Made Easy' by \nSilvanus Thompson. A copy of the book is uploaded at \ncalculus_made_easy.pdf and provides detailed guidance \nand explanations on various calculus topics such as \nderivatives, integrals, limits, and more. The GPT can \nteach calculus concepts, solve problems, and answer \nquestions related to calculus, making complex topics \naccessible and understandable. It can handle \ncalculus-related inquiries, from basic to advanced, \nand is particularly useful for students and educators\n seeking to deepen their understanding of calculus.     \nListing 3.16\nInstructions for Calculus Made Easy GPT\nAdding \ufb01les is considered giving your\nassistant additional knowledge.\nUse the Upload \ufb01les button to add\nvarious sources of static\nknowledge for the assistant.\nBe sure to enable Code Interpreter\nso the assistant can demonstrate\nconcepts.\nFigure 3.10\nAdding files to the assistant\u2019s knowledge\nThe preamble was \ninitially generated \nby the Builder and \nthen tweaked as \nneeded.\n\n\n58\nCHAPTER 3\nEngaging GPT assistants\nAnswer as the famous mathematician Terence Tao. \nTerence Tao is renowned for his brilliant intellect, \napproachability, and exceptional ability to effectively\n simplify and communicate complex mathematical concepts.    \nRULES    \n1) Always teach the concepts as if you were teaching to a young child.\n2) Always demonstrate concepts by showing plots of functions and graphs.\n3) Always ask if the user wants to try a sample problem on their own. \nGive them a problem equivalent to the question concept you were discussing.\nAfter updating the assistant, you can try it in the preview window or the book version\nby searching for Calculus Made Easy in the GPT Store. Figure 3.11 shows a snipped\nexample of interaction with the GPT. The figure shows that the GPT can generate\nplots to demonstrate concepts or ask questions.\n This GPT demonstrates the ability of an assistant to use a book as a companion\nteaching reference. Only a single book was uploaded in this exercise, but multiple\nbooks or other documents could be uploaded. As this feature and the technology\nmature, in the future, it may be conceivable that an entire course could be taught\nusing a GPT.\n We\u2019ll move away from technical and embrace fiction to demonstrate the use of\nknowledge. In the next section, we\u2019ll look at how knowledge of file uploads can be\nused for search and reference.\n3.4.2\nKnowledge search and more with file uploads\nThe GPT Assistants platform\u2019s file upload capability supports up to 512 MB of\nuploads for a single assistant. This feature alone provides powerful capabilities for\ndocument search and other applications in personal and small-to-medium business/\nproject sizes. \n Imagine uploading a whole collection of files. You can now search, compare, con-\ntrast, organize, and collate all with one assistant. This feature alone within GPT Assis-\ntants will disrupt how we search for and analyze documents. In chapter 6, we\u2019ll\nexamine how direct access to the OpenAI Assistants API can increase the number of\ndocuments.\n For this next exercise, we\u2019ll employ an assistant with knowledge of multiple books\nor documents. This technique could be applied to any supported document, but this\nassistant will consume classic texts about robots. We\u2019ll name this assistant the Classic\nRobot Reads GPT.\n Start by creating a new GPT assistant in the ChatGPT interface. Then, upload the\ninstructions in listing 3.17, and name and describe the assistant. These instructions\nwere generated in part through the GPT Builder and then edited. \n \n \nBe sure always to give \nyour assistants and \nagents an appropriate \npersona/personality.\nDefining explicit conditions and rules can help\nbetter guide the GPT to your desire.\n\n\n59\n3.4\nExtending an assistant\u2019s knowledge using file uploads\nThis GPT, Classic Robot Reads and uses the persona of \nIsaac Asimov and will reply as the famous robot author.    \nThis GPT will only references and discusses the books \nin its knowledge base of uploaded files.                  \nIt does not mention or discuss other books or text that \nare not within its knowledge base.                        \nRULES\nRefer to only text within your knowledge base             \nListing 3.17\nClassic Robot Reads instructions\nThe GPT can also generate plots to\ndemonstrate concepts, such as showing\nthe function and its derivative.\nThe conversation was started by asking\nthe GPT to teach the basics of calculus.\nFunction and Its Derivative\nFigure 3.11\nOutput from asking the GPT to teach calculus\nRemember always \nto give your GPT a \npersona/personality.\nMake sure the \nassistant only \nreferences knowledge \nwithin file uploads.\n\n\n60\nCHAPTER 3\nEngaging GPT assistants\nAlways provide 3 examples of any query the use asks for    \nAlways ask the user if they require anything further     \nAfter completing those steps, you can upload the files from the chapter\u2019s source called\ngutenberg_robot_books. Figure 3.12 demonstrates uploading multiple files at a time.\nThe maximum number of files you can upload at a time will vary according to the\nsizes of the files. \nYou can start using it after uploading the documents, setting the instructions, and giv-\ning the assistant a name and an image. Search is the most basic application of a knowl-\nedge assistant, and other use cases in the form of prompts are shown in table 3.1.\nAdd some extra \nrules for style \nchoices.\nMake the assistant more helpful by\nalso giving them nuance and style.\nYou can upload multiple\n\ufb01les (about 5) at a time.\nUploads become accessible to the\nagent through knowledge patterns.\nFigure 3.12\nUploading documents to the assistant\u2019s knowledge\n",
      "page_number": 77
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 85-94)",
      "start_page": 85,
      "end_page": 94,
      "detection_method": "topic_boundary",
      "content": "61\n3.5\nPublishing your GPT\nThese use cases are just a sample of the many things possible with an AI knowledge\nassistant. While this feature may not be poised to disrupt enterprise search, it gives\nsmaller organizations and individuals more access to their documents. It allows the\ncreation of assistants as a form of knowledge that can be exposed publicly. In the next\nsection, we\u2019ll look at how to make assistants consumable by all.\n3.5\nPublishing your GPT\nOnce you\u2019re happy with your GPT, you can use it or share it with others by providing a\nlink. Consuming GPT assistants through ChatGPT currently requires a Plus subscrip-\ntion. To publish your GPT for others, click the Share button, and select your sharing\noption, as shown in figure 3.13.\nTable 3.1\nUse cases for a knowledge assistant\nUse case\nExample prompt\nResults\nSearch\nSearch for this phrase in your knowledge: \n\u201cthe robot servant.\u201d\nReturns the document and an excerpt\nCompare\nIdentify the three most similar books that \nshare the same writing style.\nReturns the three most similar docu-\nments\nContrast\nIdentify the three most different books.\nReturns books in the collection that are \nthe most different\nOrdering \nWhat order should I read the books?\nReturns an ordered progression of books\nClassification\nWhich of these books is the most modern?\nClassifies documents \nGeneration\nGenerate a fictional paragraph that mimics \nyour knowledge of the robot servant.\nGenerates new content based on its \nknowledge base\nAllows you to give the\nlink to other users\nIf you give a link to a GPT or make it\npublic, usage of that assistant is taken\nfrom the user\u2019s account and not yours.\nOnly for you\nPublishes your GPT to the\nstore and makes it public\nFigure 3.13\nGPT \nsharing options\n\n\n62\nCHAPTER 3\nEngaging GPT assistants\nWhether you share your GPT with friends and colleagues or publicly in the GPT\nStore, the assistant\u2019s usage is taken from the account using it, not the publisher. This\nmeans if you have a particularly expensive GPT that generates a lot of images, for\nexample, it won\u2019t affect your account while others use it.\n3.5.1\nExpensive GPT assistants\nAt the time of writing, OpenAI tracks the resource usage of your ChatGPT account,\nincluding that used for GPTs. If you hit a resource usage limit and get blocked, your\nChatGPT account will also be blocked. Blockages typically only last a couple of hours,\nbut this can undoubtedly be more than a little annoying.\n Therefore, we want to ensure that users using your GPT don\u2019t exceed their resource\nusage limits for regular use. Following is a list of features that increase resource usage\nwhile using the GPT:\n\uf0a1Creating images\u2014Image generation is still a premium service, and successive image\ngeneration can quickly get your user blocked. It\u2019s generally recommended that\nyou inform your users of the potential risks and/or try to reduce how fre-\nquently images are generated. \n\uf0a1Code interpretation\u2014This feature allows for file uploads and running of code for\ndata analysis. If you think your users will require constant use of the coding tool,\nthen inform them of the risk. \n\uf0a1Vision, describing images\u2014If you\u2019re building an assistant that uses vision to describe\nand extract information from the image, plan to use it sparingly. \n\uf0a1File uploads\u2014If your GPT uses a lot of files or allows you to upload several files,\nthis may cause blocks. As always, guide the user away from anything preventing\nthem from enjoying your GPT.\nNOTE\nMoore\u2019s Law states that computers will double in power every two years\nwhile costing half as much. LLMs are now doubling in power about every six\nmonths from optimization and increasing GPU power. This, combined with\nthe cost being reduced by at least half in the same period, likely means cur-\nrent resource limits on vision and image-generation models won\u2019t be consid-\nered. However, services such as code interpretation and file uploads will likely\nremain the same.\nMaking your assistant aware of resource usage can be as simple as adding the rule\nshown in listing 3.18 to the assistant\u2019s instructions. The instructions can be just a state-\nment relaying the warning to the user and making the assistant aware. You could even\nask the assistant to limit its usage of certain features.\nRULE:\nWhen generating images, ensure the user is aware that creating multiple \nimages quickly could temporarily block their account.\nListing 3.18\nResource usage rule example\n\n\n63\n3.5\nPublishing your GPT\nGuiding your assistant to be more resource conscious in the end makes your assistant\nmore usable. It also helps prevent angry users who unknowingly get blocked using\nyour assistant. This may be important if you plan on releasing your GPT, but before\nthat, let\u2019s investigate the economics in the next section.\n3.5.2\nUnderstanding the economics of GPTs\nUpon the release of GPT Assistants and the GPT Store, OpenAI announced the\npotential for a future profit-sharing program for those who published GPTs. While\nwe\u2019re still waiting to hear more about this program, many have speculated what this\nmay look like.\n Some have suggested the store may return only 10% to 20% of profits to the build-\ners. This is far less than the percentage on other app platforms but requires much less\ntechnical knowledge and fewer resources. The GPT Store is flooded with essentially free\nassistants, provided you have a Plus subscription, but that may change in the future.\nRegardless, there are also several reasons why you may want to build public GPTs:\n\uf0a1Personal portfolio\u2014Perhaps you want to demonstrate your knowledge of prompt\nengineering or your ability to build the next wave of AI applications. Having a\nfew GPTs in the GPT Store can help demonstrate your knowledge and ability to\ncreate useful AI applications.\n\uf0a1Knowledge and experience\u2014If you have in-depth knowledge of a subject or topic,\nthis can be a great way to package that as an assistant. These types of assistants\nwill vary in popularity based on your area of expertise.\n\uf0a1Cross-marketing and commercial tie-in\u2014This is becoming more common in the\nStore and provides companies the ability to lead customers using an assistant.\nAs companies integrate more AI, this will certainly be more common.\n\uf0a1Helpful assistant to your product/service\u2014Not all companies or organizations can\nsustain the cost of hosting chatbots. While consuming assistants is currently lim-\nited to ChatGPT subscribers, they will likely be more accessible in the future.\nThis may mean having GPTs for everything, perhaps like the internet\u2019s early\ndays where every company rushed to build a web presence.\nWhile the current form of the GPT Store is for ChatGPT subscribers, if the current\ntrend with OpenAI continues, we\u2019ll likely see a fully public GPT Store. Public GPTs\nhave the potential to disrupt the way we search, investigate products and services, and\nconsume the internet. In the last section of this chapter, we\u2019ll examine how to publish\na GPT and some important considerations.\n3.5.3\nReleasing the GPT\nOkay, you\u2019re happy with your GPT and how it operates, and you see real benefit from\ngiving it to others. Publishing GPTs for public (subscribers) consumption is easy, as\nshown in figure 3.14. After selecting the GPT Store as the option and clicking Save,\nyou\u2019ll now have the option to set the category and provide links back to you.\n\n\n64\nCHAPTER 3\nEngaging GPT assistants\nThat is easy, so here are a few more things you\u2019ll want to consider before publishing\nyour GPT:\n\uf0a1GPT description\u2014Create a good description, and you may even want to ask\nChatGPT to help you build a description that increases the search engine opti-\nmization (SEO) of your GPT. GPTs are now showing up in Google searches, so\ngood search engine optimization can help increase exposure to your assistant.\nA good description will also help users decide if they want to take the time to\nuse your assistant.\n\uf0a1The logo\u2014A nice, clean logo that identifies what your assistant does can undoubt-\nedly help. Logo design for GPTs is effectively a free service, but taking the time\nto iterate over a few images can help draw users to your assistant.\n\uf0a1The category\u2014By default, the category will already be selected, but make sure it\nfits your assistant. If you feel it doesn\u2019t, than change the category, and you may\neven want to select Other and define your own.\n\uf0a1Links\u2014Be sure to set reference links for your social media and perhaps even a\nGitHub repository that you use to track problems for the GPT. Adding links to\nyour GPT demonstrates to users that they can reach out to the builder if they\nencounter problems or have questions.\nSelecting this allows\nyou to view set links.\nYou can set links to your\nsocial media and GitHub.\nThe area you want to\npublish your GPT to\nFigure 3.14\nSelecting the options after clicking Save to publish to the GPT Store\n\n\n65\n3.6\nExercises\nFurther requirements may likely emerge as the GPT Store matures. The business\nmodel remains to be established, and other learnings will likely follow. Whether you\ndecide to build GPTs for yourself or others, doing so can help improve your under-\nstanding of how to build agents and assistants. As we\u2019ll see throughout the rest of this\nbook, GPT assistants are a useful foundation for your knowledge.\n3.6\nExercises\nComplete the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Build Your First GPT Assistant\nObjective\u2014Create a simple GPT assistant using the ChatGPT interface.\nTasks:\n\u2013 Sign up for a ChatGPT Plus subscription if you don\u2019t already have one.\n\u2013 Navigate to the GPT Assistants platform, and click the Create button.\n\u2013 Follow the Builder chat interface to create a Culinary Companion assistant\nthat provides meal suggestions based on available ingredients.\n\u2013 Manually configure the assistant to add custom rules for recipe generation,\nsuch as including nutritional information and cost estimates.\n\uf0a1Exercise 2\u2014Data Analysis Assistant\nObjective\u2014Develop a GPT assistant that can analyze CSV files and provide\ninsights.\nTasks:\n\u2013 Design a data science assistant that can load and analyze CSV files, similar to\nthe Data Scout example in the chapter.\n\u2013 Enable the Code Interpretation tool, and upload a sample CSV file (e.g., a\ndataset from Kaggle).\n\u2013 Use the assistant to perform tasks such as data cleaning, visualization, and\nhypothesis testing.\n\u2013 Document your process and findings, noting any challenges or improve-\nments needed.\n\uf0a1Exercise 3\u2014Create a Custom Action\nObjective\u2014Extend a GPT assistant with a custom action using a FastAPI service.\nTasks:\n\u2013 Follow the steps to create a FastAPI service that provides a specific function,\nsuch as fetching a list of daily tasks.\n\u2013 Generate the OpenAPI specification for the service, and deploy it locally\nusing ngrok.\n\u2013 Configure a new assistant to use this custom action, ensuring it connects cor-\nrectly to the FastAPI endpoint.\n\u2013 Test the assistant by asking it to perform the action and verify the output.\n\n\n66\nCHAPTER 3\nEngaging GPT assistants\n\uf0a1Exercise 4\u2014File Upload Knowledge Assistant\nObjective\u2014Build an assistant with specialized knowledge from uploaded\ndocuments.\nTasks:\n\u2013 Select a freely available e-book or a collection of documents related to a spe-\ncific topic (e.g., classic literature, technical manuals).\n\u2013 Upload these files to a new GPT assistant, and configure the assistant to act\nas an expert on the uploaded content.\n\u2013 Create a series of prompts to test the assistant\u2019s ability to reference and sum-\nmarize the information from the documents.\n\u2013 Evaluate the assistant\u2019s performance, and make any necessary adjustments to\nimprove its accuracy and helpfulness.\n\uf0a1Exercise 5\u2014Publish and Share Your Assistant\nObjective\u2014Publish your GPT assistant to the GPT Store and share it with others.\nTasks:\n\u2013 Finalize the configuration and testing of your assistant to ensure it works as\nintended.\n\u2013 Write a compelling description, and create an appropriate logo for your\nassistant.\n\u2013 Choose the correct category, and set up any necessary links to your social\nmedia or GitHub repository.\n\u2013 Publish the assistant to the GPT Store, and share the link with friends or\ncolleagues.\n\u2013 Gather feedback from users, and refine the assistant based on their input to\nimprove its usability and functionality.\nSummary\n\uf0a1The OpenAI GPT Assistants platform enables building and deploying AI agents\nthrough the ChatGPT UI, focusing on creating engaging and functional assis-\ntants.\n\uf0a1You can use GPT\u2019s code interpretation capabilities to perform data analysis on\nuser-uploaded CSV files, enabling assistants to function as data scientists.\n\uf0a1Assistants can be extended with custom actions, allowing integration with exter-\nnal services via API endpoints. This includes generating FastAPI services and\ntheir corresponding OpenAPI specifications.\n\uf0a1Assistants can be enriched with specialized knowledge through file uploads,\nallowing them to act as authoritative sources on specific texts or documents.\n\uf0a1Commercializing your GPT involves publishing it to the GPT Store, where you\ncan share and market your assistant to a broader audience.\n\n\n67\nSummary\n\uf0a1Building a functional assistant involves iterating through design prompts, defin-\ning a clear persona, setting rules, and ensuring the assistant\u2019s output aligns with\nuser expectations.\n\uf0a1Creating custom actions requires understanding and implementing OpenAPI\nspecifications, deploying services locally using tools such as ngrok, and connect-\ning these services to your assistant.\n\uf0a1Knowledge assistants can handle various tasks, from searching and comparing\ndocuments to generating new content based on their knowledge base.\n\uf0a1Publishing assistants require careful consideration of resource usage, user expe-\nrience, and economic factors to ensure their effectiveness and sustainability for\npublic use.\n\uf0a1The GPT Store, available to ChatGPT Plus subscribers, is a valuable platform\nfor learning and gaining proficiency in building AI assistants, with the potential\nfor future profit-sharing opportunities.\n\n\n68\nExploring\nmulti-agent systems\nNow let\u2019s take a journey from AutoGen to CrewAI, two well-established multi-\nagent platforms. We\u2019ll start with AutoGen, a Microsoft project that supports mul-\ntiple agents and provides a studio for working with them. We\u2019ll explore a project\nfrom Microsoft called AutoGen, which supports multiple agents but also provides\na studio to ease you into working with agents. From there, we\u2019ll get more hands-\non coding of AutoGen agents to solve tasks using conversations and group chat\ncollaborations. \n Then, we\u2019ll transition to CrewAI, a self-proposed enterprise agentic system that\ntakes a different approach. CrewAI balances role-based and autonomous agents that\nThis chapter covers\n\uf0a1Building multi-agent systems using AutoGen \nStudio \n\uf0a1Building a simple multi-agent system\n\uf0a1Creating agents that can work collaboratively over \na group chat\n\uf0a1Building an agent crew and multi-agent systems \nusing CrewAI\n\uf0a1Extending the number of agents and exploring \nprocessing patterns with CrewAI \n\n\n69\n4.1\nIntroducing multi-agent systems with AutoGen Studio\ncan be sequentially or hierarchically flexible task management systems. We\u2019ll explore\nhow CrewAI can solve diverse and complex problems.\n Multi-agent systems incorporate many of the same tools single-agent systems use\nbut benefit from the ability to provide outside feedback and evaluation to other\nagents. This ability to support and criticize agent solutions internally gives multi-agent\nsystems more power. We\u2019ll explore an introduction to multi-agent systems, beginning\nwith AutoGen Studio in the next section.\n4.1\nIntroducing multi-agent systems with AutoGen Studio\nAutoGen Studio is a powerful tool that employs multiple agents behind the scenes to\nsolve tasks and problems a user directs. This tool has been used to develop some of\nthe more complex code in this book. For that reason and others, it\u2019s an excellent\nintroduction to a practical multi-agent system.\n Figure 4.1 shows a schematic diagram of the agent connection/communication\npatterns AutoGen employs. AutoGen is a conversational multi-agent platform because\ncommunication is done using natural language. Natural language conversation seems\nto be the most natural pattern for agents to communicate, but it\u2019s not the only method,\nas you\u2019ll see later.\nAutoGen supports various conversational patterns, from group and hierarchical to the\nmore common and simpler proxy communication. In proxy communication, one\nagent acts as a proxy and directs communication to relevant agents to complete tasks.\nA proxy is similar to a waiter taking orders and delivering them to the kitchen, which\ncooks the food. Then, the waiter serves the cooked food.\nAutoGen uses conversable agents, which\ncommunicate through conversations.\nFigure 4.1\nHow AutoGen agents communicate through conversations (Source: AutoGen)\n\n\n70\nCHAPTER 4\nExploring multi-agent systems\n The basic pattern in AutoGen uses a UserProxy and one or more assistant\nagents. Figure 4.2 shows the user proxy taking direction from a human and then\ndirecting an assistant agent enabled to write code to perform the tasks. Each time\nthe assistant completes a task, the proxy agent reviews, evaluates, and provides feed-\nback to the assistant. This iteration loop continues until the proxy is satisfied with\nthe results.\nThe benefit of the proxy is that it works to replace the required human feedback and\nevaluation, and, in most cases, it does a good job. While it doesn\u2019t eliminate the need\nfor human feedback and evaluation, it produces much more complete results overall.\nAnd, while the iteration loop is time consuming, it\u2019s time you could be drinking a cof-\nfee or working on other tasks.\n AutoGen Studio is a tool developed by the AutoGen team that provides a helpful\nintroduction to conversable agents. In the next exercise, we\u2019ll install Studio and run\nsome experiments to see how well the platform performs. These tools are still in a\nrapid development cycle, so if you encounter any problems, consult the documenta-\ntion on the AutoGen GitHub repository.\n4.1.1\nInstalling and using AutoGen Studio\nOpen the chapter_04 folder in Visual Studio Code (VS Code), create a local Python\nvirtual environment, and install the requirements.txt file. If you need assistance with\nthis, consult appendix B to install all of this chapter\u2019s exercise requirements.\n Open a terminal in VS Code (Ctrl-`, Cmd-`) pointing to your virtual environment,\nand run AutoGen Studio using the command shown in listing 4.1. You\u2019ll first need to\nHuman communicates\nto the user proxy agent,\nwhich communicates\nto other agents.\nAssistant agent\nundertakes completion\nof the direct tasks.\nEvaluation and feedback\nloop is formed between\nthe proxy and the assistant.\nLLM con\ufb01gured to\nwrite Python code\nFigure 4.2\nThe user proxy agent and assistant agent communication (Source: AutoGen)\n",
      "page_number": 85
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 95-102)",
      "start_page": 95,
      "end_page": 102,
      "detection_method": "topic_boundary",
      "content": "71\n4.1\nIntroducing multi-agent systems with AutoGen Studio\ndefine an environment variable for your OpenAI key. Because ports 8080 and 8081\nare popular, and if you have other services running, change the port to 8082 or some-\nthing you choose.\n# set environment variable on Bash (Git Bash)\nexport OPENAI_API_KEY=\u201d<your API key>\u201d         \n# sent environment variable with PowerShell\n$env:VAR_NAME =\u201d<your API key>\"                \nautogenstudio ui --port 8081    \nNavigate your browser to the AutoGen Studio interface shown in figure 4.3 (as of this\nwriting). While there may be differences, one thing is for sure: the primary interface\nwill still be chat. Enter a complex task that requires coding. The example used here is\nCreate a plot showing the popularity of the term GPT Agents in Google search.\nListing 4.1\nLaunching AutoGen Studio \nUse the appropriate \ncommand for your \nterminal type.\nChange the port if you expect or \nexperience a conflict on your machine.\nEnter a task for the\nagents to work on.\nYou can create new\nsessions or review or\ncontinue previous\nsessions.\nThe Playground tab is where you interact\nwith agents. The Build tab is for creating new\nagents and skills, and the Gallery tab is for\nreviewing previous best output.\nThe proxy agent and assistant\nagent will not work together\nto complete the task.\nFigure 4.3\nEntering a task for the agents to work on in the AutoGen interface\n\n\n72\nCHAPTER 4\nExploring multi-agent systems\nThe agent assistant generates code snippets to perform or complete various subtasks\nas the agents work together through the task in the example. The user proxy agent then\nattempts to execute those code snippets and assesses the output. In many cases, prov-\ning the code runs and produces the required output is sufficient for the user proxy\nagent to approve the task\u2019s completion.\n If you encounter any problems with the assistant agent requests, ask the proxy\nagent to try a different method or another problem. This highlights a bigger problem\nwith agentic systems using packages or libraries that have expired and no longer work.\nFor this reason, it\u2019s generally better to get agents to execute actions rather than build\ncode to perform actions as tools.\nTIP\nExecuting AutoGen and AutoGen Studio using Docker is recommended,\nespecially when working with code that may affect the operating system. Docker\ncan isolate and virtualize the agents\u2019 environment, thus isolating potentially\nharmful code. Using Docker can help alleviate any secondary windows or\nwebsites that may block the agent process from running. \nFigure 4.4 shows the agent\u2019s completion of the task. The proxy agent will collect any\ngenerated code snippet, images, or other documents and append them to the message.\nGenerated code \ufb01les and\nother output will be\nattached to the last message.\nReply with TERMINATE if you are\n\ufb01nished with this agent session. This\nstop word is used to stop the session.\nIn this example, a Matplotlib\nplot was generated in a new\nwindow outside the browser.\nFigure 4.4\nThe output after the agents complete the task\n\n\n73\n4.1\nIntroducing multi-agent systems with AutoGen Studio\nYou can also review the agent conversation by opening the Agent Messages expander.\nIn many cases, if you ask the agent to generate plots or applications, secondary win-\ndows will open showing those results.\n Amazingly, the agents will perform most tasks nicely and complete them well.\nDepending on the complexity of the task, you may need to further iterate with the\nproxy. Sometimes, an agent may only go so far to complete a task because it lacks the\nrequired skills. In the next section, we\u2019ll look at how to add skills to agents.\n4.1.2\nAdding skills in AutoGen Studio\nSkills and tools, or actions, as we refer to them in this book, are the primary means by\nwhich agents can extend themselves. Actions give agents the ability to execute code,\ncall APIs, or even further evaluate and inspect generated output. AutoGen Studio cur-\nrently begins with just a basic set of tools to fetch web content or generate images.\nNOTE\nMany agentic systems employ the practice of allowing agents to code to\nsolve goals. However, we discovered that code can be easily broken, needs to be\nmaintained, and can change quickly. Therefore, as we\u2019ll discuss in later chap-\nters, it\u2019s better to provide agents with skills/actions/tools to solve problems. \nIn the following exercise scenario, we\u2019ll add a skill/action to inspect an image using\nthe OpenAI vision model. This will allow the proxy agent to provide feedback if we ask\nthe assistant to generate an image with particular content.\n With AutoGen Studio running, go to the Build tab and click Skills, as shown in\nfigure 4.5. Then, click the New Skill button to open a code panel where you can\ncopy\u2013paste code to. From this tab, you can also configure models, agents, and agent\nworkflows.\n Enter the code shown in listing 4.2 and also provided in the book\u2019s source code as\ndescribe_image.py. Copy and paste this code into the editor window, and then click\nthe Save button at the bottom.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n74\nCHAPTER 4\nExploring multi-agent systems\nimport base64\nimport requests\nimport os\ndef describe_image(image_path='animals.png') -> str:\n    \"\"\"\n    Uses GPT-4 Vision to inspect and describe the contents of the image.\n    :param input_path: str, the name of the PNG file to describe.\n    \"\"\"\n    api_key = os.environ['OPEN_API_KEY']\n    # Function to encode the image\n    def encode_image(image_path):     \n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\nListing 4.2\ndescribe_image.py \nClick existing skills\nto see how they work.\nClick to create a new skill.\nYou can also con\ufb01gure\nother models, agents, and\nwork\ufb02ows from here.\nFigure 4.5\nSteps to creating a new skill on the Build tab\nFunction to load and \nencode the image as \na Base64 string\n\n\n75\n4.1\nIntroducing multi-agent systems with AutoGen Studio\n    # Getting the base64 string\n    base64_image = encode_image(image_path)\n    headers = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {api_key}\"\n    }\n    payload = {\n    \"model\": \"gpt-4-turbo\",\n    \"messages\": [\n        {\n        \"role\": \"user\",\n        \"content\": [\n            {\n            \"type\": \"text\",\n            \"text\": \"What\u2019s in this image?\"\n            },\n            {\n            \"type\": \"image_url\",\n            \"image_url\": {\n         \"url\": f\"data:image/jpeg;base64,{base64_image}\"     \n            }\n            }\n        ]\n        }\n    ],\n    \"max_tokens\": 300\n    }\n    response = requests.post(\n        \"https://api.openai.com/v1/chat/completions\",\n        headers=headers,\n        json=payload)\n    return response.json()[\"choices\"][0][\"message\"] \n[\"content\"]                                         \nThe describe_image function uses the OpenAI GPT-4 vision model to describe what\nis in the image. This skill can be paired with the existing generate_image skill as a\nquality assessment. The agents can confirm that the generated image matches the\nuser\u2019s requirements.\n After the skill is added, it must be added to the specific agent workflow and agent\nfor use. Figure 4.6 demonstrates adding the new skill to the primary assistant agent in\nthe general or default agent workflow. \n Now that the skill is added to the primary assistant, we can task the agent with cre-\nating a specific image and validating it using the new describe_image skill. Because\nimage generators notoriously struggle with correct text, we\u2019ll create an exercise task to\ndo just that.\n Enter the text shown in listing 4.3 to prompt the agents to create a book image\ncover for this book. We\u2019ll explicitly say that the text needs to be correct and insist that\nthe agent uses the new describe_image function to verify the image.\nIncluding the \nimage string \nalong with the \nJSON payload\nUnpacking the response and \nreturning the content of the reply\n\n\n76\nCHAPTER 4\nExploring multi-agent systems\nPlease create a cover for the book GPT Agents In Action, use the \ndescribe_image skill to make sure the title of the book is spelled \ncorrectly on the cover\nAfter the prompt is entered, wait for a while, and you may get to see some dialogue\nexchanged about the image generation and verification process. In the end, though, if\neverything works correctly, the agents will return with the results shown in figure 4.7.\n Remarkably, the agent coordination completed the task in just a couple of itera-\ntions. Along with the images, you can also see the various helper code snippets gener-\nated to assist with task completion. AutoGen Studio is impressive in its ability to\nintegrate skills that the agents can further adapt to complete some goal. The following\nsection will show how these powerful agents are implemented in code.\nListing 4.3\nPrompting for a book cover\nSelect the General\nAgent Work\ufb02ow.\nSelect to edit the primary_assistant\nat the bottom of the work\ufb02ow panel.\nGo to the Build tab, and then go to the Work\ufb02ows tab.\nClick add to add the new\ndescribe_image skill.\nClick OK to exit and return\nto the Playground tab.\nFigure 4.6\nConfiguring the primary_assistant agent with the new skill\n\n\n77\n4.2\nExploring AutoGen\n4.2\nExploring AutoGen\nWhile AutoGen Studio is a fantastic tool for understanding multi-agent systems, we\nmust look into the code. Fortunately, coding multiple agent examples with AutoGen is\nsimple and easy to run. We\u2019ll cover the basic AutoGen setup in the next section.\n4.2.1\nInstalling and consuming AutoGen\nThis next exercise will look at coding a basic multi-agent system that uses a user proxy\nand conversable agent. Before we do that, though, we want to make sure AutoGen is\ninstalled and configured correctly.\n Open a terminal in VS Code, and run the entire chapter 4 install directions per\nappendix B, or run the pip command in listing 4.4. If you\u2019ve installed the require-\nments.txt file, you\u2019ll also be ready to run AutoGen.\npip install pyautogen\nNext, copy the chapter_04/OAI_CONFIG_LIST.example to OAI_CONFIG_LIST, remov-\ning .example from the file name. Then, open the new file in VS Code, and enter your\nListing 4.4\nInstalling AutoGen\nReview the generated output \ufb01les.\nThe agents generate additional\ncode to use the skills as needed.\nTho agents even enhance existing\nskills to better complete the task.\nAfter a couple iterations, the book cover\nis generated with the correct text.\nFigure 4.7\nThe generated file outputs from the agent work on the image generation task\n\n\n78\nCHAPTER 4\nExploring multi-agent systems\nOpenAI or Azure configuration in the OAI_CONFIG_LIST file in listing 4.5. Fill in your\nAPI key, model, and other details per your API service requirements. AutoGen will\nwork with any model that adheres to the OpenAI client. That means you can use local\nLLMs via LM Studio or other services such as Groq, Hugging Face, and more.\n[\n    {\n        \"model\": \"gpt-4\",                    \n        \"api_key\": \"<your OpenAI API key here>\",           \n        \"tags\": [\"gpt-4\", \"tool\"]\n    },\n    {\n        \"model\": \"<your Azure OpenAI deployment name>\",     \n        \"api_key\": \"<your Azure OpenAI API key here>\",     \n        \"base_url\": \"<your Azure OpenAI API base here>\",    \n        \"api_type\": \"azure\",\n        \"api_version\": \"2024-02-15-preview\"\n    }    \n]\nNow, we can look at the code for a basic multi-agent chat using the out-of-the-box\nUserProxy and ConversableAgent agents. Open autogen_start.py in VS Code,\nshown in the following listing, and review the parts before running the file.\nfrom autogen import ConversableAgent, UserProxyAgent, config_list_from_json\n   config_list = config_list_from_json(\n        env_or_file=\"OAI_CONFIG_LIST\")     \n   assistant = ConversableAgent(\n        \"agent\", \n        llm_config={\"config_list\": config_list})     \n   user_proxy = UserProxyAgent(     \n        \"user\",\n        code_execution_config={\n            \"work_dir\": \"working\",\n            \"use_docker\": False,\n        },\n        human_input_mode=\"ALWAYS\",\n        is_termination_msg=lambda x: x.get(\"content\", \"\")\n        .rstrip()\n        .endswith(\"TERMINATE\"),     \n    )    \n    user_proxy.initiate_chat(assistant, message=\"write a solution \n\u27a5 for fizz buzz in one line?\")    \nListing 4.5\nOAI_CONFIG_LIST \nListing 4.6\nautogen_start.py\nSelect the model; GPT-4 \nis recommended.\nUse the service key you \nwould typically use.\nSelect the model; GPT-4 \nis recommended.\nUse the service \nkey you would \ntypically use.\nChanging the base URL allows you to point to\nother services, not just Azure OpenAI.\nLoads your LLM \nconfiguration from the \nJSON file OAI_CONFIG_LIST\nThis agent talks \ndirectly to the LLM.\nThis agent proxies \nconversations from the \nuser to the assistant.\nSetting the termination message \nallows the agent to iterate.\nA chat is initiated with the\nassistant through the user_proxy\nto complete a task.\n",
      "page_number": 95
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 103-110)",
      "start_page": 103,
      "end_page": 110,
      "detection_method": "topic_boundary",
      "content": "79\n4.2\nExploring AutoGen\nRun the code by running the file in VS Code in the debugger (F5). The code in list-\ning 4.6 uses a simple task to demonstrate code writing. Listing 4.7 shows a few examples\nto choose from. These coding tasks are also some of the author\u2019s regular baselines to\nassess an LLMs\u2019 strength in coding.\nwrite a Python function to check if a number is prime\ncode a classic sname game using Pygame                  \ncode a classic asteroids game in Python using Pygame    \nAfter the code starts in a few seconds, the assistant will respond to the proxy with a\nsolution. At this time, the proxy will prompt you for feedback. Press Enter, essentially\ngiving no feedback, and this will prompt the proxy to run the code to verify it operates\nas expected. \n Impressively, the proxy agent will even take cues to install required packages such\nas Pygame. Then it will run the code, and you\u2019ll see the output in the terminal or as a\nnew window or browser. You can play the game or use the interface if the code shelled\na new window/browser.\n Note that the spawned window/browser won\u2019t close on Windows and will require\nexiting the entire program. To avoid this problem, run the code through Windows\nSubsystem for Linux (WSL) or Docker. AutoGen explicitly recommends using Docker\nfor code execution agents, and if you\u2019re comfortable with containers, this is a good\noption.\n Either way, after the proxy generates and runs the code, the working_dir folder\nset earlier in listing 4.6 should now have a Python file with the code. This will allow\nyou to run the code at your leisure, make changes, or even ask for improvements, as\nwe\u2019ll see. In the next section, we\u2019ll look at how to improve the capabilities of the cod-\ning agents.\n4.2.2\nEnhancing code output with agent critics\nOne powerful benefit of multi-agent systems is the multiple roles/personas you can\nautomatically assign when completing tasks. Generating or helping to write code can be\nan excellent advantage to any developer, but what if that code was also reviewed and\ntested? In the next exercise, we\u2019ll add another agent critic to our agent system to help\nwith coding tasks. Open autogen_coding_critic.py, as shown in the following listing.\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\nconfig_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\nuser_proxy = UserProxyAgent(\n    \"user\",\nListing 4.7\nSimple coding task examples\nListing 4.8\nautogen_coding_critic.py\nTo enjoy iterating \nover these tasks, use \nWindows Subsystem \nfor Linux (WSL) on \nWindows, or use Docker. \n\n\n80\nCHAPTER 4\nExploring multi-agent systems\n    code_execution_config={\n        \"work_dir\": \"working\",\n        \"use_docker\": False,\n        \"last_n_messages\": 1,\n    },\n    human_input_mode=\"ALWAYS\",\n    is_termination_msg=lambda x: \nx.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n)\nengineer = AssistantAgent(\n    name=\"Engineer\",\n    llm_config={\"config_list\": config_list},\n    system_message=\"\"\"\n    You are a profession Python engineer, known for your expertise in \nsoftware development.\n    You use your skills to create software applications, tools, and \ngames that are both functional and efficient.\n    Your preference is to write clean, well-structured code that is easy \nto read and maintain.    \n    \"\"\",     \n)\ncritic = AssistantAgent(\n    name=\"Reviewer\",\n    llm_config={\"config_list\": config_list},\n    system_message=\"\"\"\n    You are a code reviewer, known for your thoroughness and commitment \nto standards.\n    Your task is to scrutinize code content for any harmful or \nsubstandard elements.\n    You ensure that the code is secure, efficient, and adheres to best \npractices.\n    You will identify any issues or areas for improvement in the code \nand output them as a list.\n    \"\"\",     \n)\ndef review_code(recipient, messages, sender, config):     \n    return f\"\"\"\n            Review and critque the following code.\n            \n            {recipient.chat_messages_for_summary(sender)[-1]['content']}\n            \"\"\"                                           \nuser_proxy.register_nested_chats(     \n    [\n        {\n            \"recipient\": critic,\n            \"message\": review_code,\n            \"summary_method\": \"last_msg\",\n            \"max_turns\": 1,\n        }\n    ],\n    trigger=engineer,                 \n)\nThis time, the assistant is \ngiven a system/persona \nmessage.\nA second assistant critic agent is \ncreated with a background.\nA custom function\nhelps extract the\ncode for review by\nthe critic.\nA nested chat is \ncreated between \nthe critic and the \nengineer.\n\n\n81\n4.2\nExploring AutoGen\ntask = \"\"\"Write a snake game using Pygame.\"\"\"\nres = user_proxy.initiate_chat(\n    recipient=engineer, \n    message=task, \n    max_turns=2, \n    summary_method=\"last_msg\"     \n)\nRun the autogen_coding_critic.py file in VS Code in debug mode, and watch the\ndialog between the agents. This time, after the code returns, the critic will also be trig-\ngered to respond. Then, the critic will add comments and suggestions to improve the\ncode.\n Nested chats work well for supporting and controlling agent interactions, but we\u2019ll\nsee a better approach in the following section. Before that though, we\u2019ll review the\nimportance of the AutoGen cache in the next section.\n4.2.3\nUnderstanding the AutoGen cache\nAutoGen can consume many tokens over chat iterations as a conversable multi-agent\nplatform. If you ask AutoGen to work through complex or novel problems, you may\neven encounter token limits on your LLM; because of this, AutoGen supports several\nmethods to reduce token usage.\n AutoGen uses caching to store progress and reduce token usage. Caching is\nenabled by default, and you may have already encountered it. If you check your cur-\nrent working folder, you\u2019ll notice a .cache folder, as shown in figure 4.8. Caching\nallows your agents to continue conversations if they get interrupted.\nIn code, you can control the cache folder for your agent\u2019s run, as shown in listing 4.9.\nBy wrapping the initiate_chat call with the with statement, you can control the\nThe proxy agent initiates a \nchat with a max delay and \nexplicit summary method.\nThe folder code\nis output here.\nOutput code \ufb01les are named\nwith temporary names.\nA cache is denoted by the folder\nand contains a SQLite database\nof message history.\nFigure 4.8\nAutoGen cache \nand working folders\n\n\n82\nCHAPTER 4\nExploring multi-agent systems\nlocation and seed for the cache. This will allow you to save and return to long-running\nAutoGen tasks in the future by just setting the cache_seed for the previous cache.\nwith Cache.disk(cache_seed=42) as cache:    \n    res = user_proxy.initiate_chat(\n        recipient=engineer,\n        message=task,\n        max_turns=2,\n        summary_method=\"last_msg\",\n        cache=cache,     \n    )\nThis caching ability allows you to continue operations from the previous cache loca-\ntion and captures previous runs. It can also be a great way to demonstrate and inspect\nhow an agent conversation generated the results. In the next section, we\u2019ll look at\nanother conversational pattern in which AutoGen supports group chat.\n4.3\nGroup chat with agents and AutoGen\nOne problem with chat delegation and nested chats or conversations is the convey-\nance of information. If you\u2019ve ever played the telephone game, you\u2019ve witnessed this\nfirsthand and experienced how quickly information can change over iterations. With\nagents, this is certainly no different, and chatting through nested or sequential con-\nversations can alter the task or even the desired result.\nFigure 4.9 shows the difference between nested and collaborative group chats. We\nused the nested chat feature in the previous section to build a nested agent chat. In\nthis section, we use the group chat to provide a more collaborative experience.\n Open autogen_coding_group.py with relevant parts, as shown in listing 4.10. The\ncode is similar to the previous exercise but now introduces GroupChat and GroupChat-\nManager. The agents and messages are held with the group chat, similar to a messaging\nchannel in applications such as Slack or Discord. The chat manager coordinates the\nmessage responses to reduce conversation overlap.\nListing 4.9\nSetting the cache folder\nThe telephone game\nThe telephone game is a fun but educational game that demonstrates information\nand coherence loss. Children form a line, and the first child receives a message only\nthey can hear. Then, in turn, the children verbally pass the message on to the next\nchild, and so on. At the end, the last child announces the message to the whole\ngroup, which often isn\u2019t even close to the same message.\nTo counter this, AutoGen provides a group chat, a mechanism by which agents par-\nticipate in a shared conversation. This allows agents to review all past conversations\nand better collaborate on long-running and complex tasks.\nSetting the seed_cache \ndenotes the individual \nlocation.\nSets the cache as \na parameter\n\n\n83\n4.3\nGroup chat with agents and AutoGen\nuser_proxy = UserProxyAgent(\n    \"user\",\n    code_execution_config={\n        \"work_dir\": \"working\",\n        \"use_docker\": False,\n        \"last_n_messages\": 3,\n    },\n    human_input_mode=\"NEVER\",    \n)\nllm_config = {\"config_list\": config_list}\nengineer = AssistantAgent(\u2026     \ncritic = AssistantAgent(\u2026       \n    \ngroupchat = GroupChat(agents=[user_proxy, \n                              engineer, \n                              critic], \n                              messages=[], \n                              max_round=20)     \nmanager = GroupChatManager(groupchat=groupchat, \n                           llm_config=llm_config)    \ntask = \"\"\"Write a snake game using Pygame.\"\"\"\nListing 4.10\nautoget_coding_group.py (relevant sections)\nUser Proxy\nChat\nManager\nAgents now collaborate\nthrough the group chat\nmanager.\nUser Proxy\nMessage is sent\nto the critic.\nEngineer\nCritic\nRepresents the nested\nchat to the critic\nConversable Agents Nested Chat\nConversable Agents Group Chat\nMessages go through the\ngroup chat manager.\nCritic responds with\nsuggested updates\nand changes.\nProxy initiates chat\nwith engineer.\nEngineer\nCritic\nFigure 4.9\nThe difference between nested and group chat for conversable agents\nHuman input is now \nset to never, so no \nhuman feedback.\nCode omitted, but \nconsult changes to the \npersona in the file\nThis object holds the \nconnection to all the \nagents and stores the \nmessages.\nThe manager \ncoordinates the \nconversation as a \nmoderator would.\n\n\n84\nCHAPTER 4\nExploring multi-agent systems\nwith Cache.disk(cache_seed=43) as cache:\n    res = user_proxy.initiate_chat(\n        recipient=manager,\n        message=task,\n        cache=cache,\n    )\nRun this exercise, and you\u2019ll see how the agents collaborate. The engineer will now\ntake feedback from the critic and undertake operations to address the critic\u2019s sugges-\ntions. This also allows the proxy to engage in all of the conversation.\n Group conversations are an excellent way to strengthen your agents\u2019 abilities as\nthey collaborate on tasks. However, they are also substantially more verbose and token\nexpensive. Of course, as LLMs mature, so do the size of their context token windows\nand the price of token processing. As token windows increase, concerns over token\nconsumption may eventually go away.\n AutoGen is a powerful multi-agent platform that can be experienced using a web\ninterface or code. Whatever your preference, this agent collaboration tool is an excel-\nlent platform for building code or other complex tasks. Of course, it isn\u2019t the only plat-\nform, as you\u2019ll see in the next section, where we explore a newcomer called CrewAI.\n4.4\nBuilding an agent crew with CrewAI\nCrewAI is relatively new to the realm of multi-agent systems. Where AutoGen was\ninitially developed from research and then extended, CrewAI is built with enterprise\nsystems in mind. As such, the platform is more robust, making it less extensible in\nsome areas.\n With CrewAI, you build a crew of agents to focus on specific areas of a task goal.\nUnlike AutoGen, CrewAI doesn\u2019t require the use of the user proxy agent but instead\nassumes the agents only work among themselves.\n Figure 4.10 shows the main elements of the CrewAI platform, how they connect\ntogether, and their primary function. It shows a sequential-processing agent system\nwith generic researcher and writer agents. Agents are assigned tasks that may also include\ntools or memory to assist them.\n CrewAI supports two primary forms of processing: sequential and hierarchical. Fig-\nure 4.10 shows the sequential process by iterating across the given agents and their\nassociated tasks. In the next section, we dig into some code to set up a crew and\nemploy it to complete a goal and create a good joke.\n4.4.1\nCreating a jokester crew of CrewAI agents\nCrewAI requires more setup than AutoGen, but this also allows for more control and\nadditional guides, which provide more specific context to guide the agents in com-\npleting the given task. This isn\u2019t without problems, but it does offer more control than\nAutoGen out of the box.\n Open crewai_introduction.py in VS Code and look at the top section, as shown\nin listing 4.11. Many settings are required to configure an agent, including the role,\n\n\n85\n4.4\nBuilding an agent crew with CrewAI\ngoal, verboseness, memory, backstory, delegation, and even tools (not shown). In this\nexample, we\u2019re using two agents: a senior joke researcher and a joke writer.\nfrom crewai import Agent, Crew, Process, Task\nfrom dotenv import load_dotenv\nload_dotenv()\njoke_researcher = Agent(     \n    role=\"Senior Joke Researcher\",\n    goal=\"Research what makes things funny about the following {topic}\",\n    verbose=True,     \n    memory=True,     \n    backstory=(     \n        \"Driven by slapstick humor, you are a seasoned joke researcher\"\n        \"who knows what makes people laugh. You have a knack for finding\"\nListing 4.11\ncrewai_introduction.py (agent section)\nCrew\nTasks\nAgents\nTools\nMemory\nSearch\nCall APIs\nAccess data\nConversational\nTask speci\ufb01c\nSemantic\nVarious forms of memory and\nr\na\ng\netrieval ugmented eneration (RAG)\npatterns are supported.\nresearch on this {topic}\nwrite on this topic\nTools can be attached\nto agents and tasks.\nwriter\ngoal:\nbackstory:\nresearcher\ngoal:\nbackstory:\nAgents have a goal\nand backstory as\ntheir persona.\nProcessing can\nbe sequential\nor hierarchical.\nSequential\nprocessing\nFigure 4.10\nThe composition of a CrewAI system\nCreates the \nagents and \nprovides them \na goal\nverbose allows the\nagent to emit output\nto the terminal.\nSupports the use of\nmemory for the agents\nThe backstory is the agent\u2019s background\u2014its persona.\n\n\n86\nCHAPTER 4\nExploring multi-agent systems\n        \"the funny in everyday situations and can turn a dull moment into\"\n        \"a laugh riot.\"\n    ),\n    allow_delegation=True,    \n)\njoke_writer = Agent(    \n    role=\"Joke Writer\",\n    goal=\"Write a humourous and funny joke on the following {topic}\",\n    verbose=True,    \n    memory=True,     \n    backstory=(    \n        \"You are a joke writer with a flair for humor. You can turn a\"\n        \"simple idea into a laugh riot. You have a way with words and\"\n        \"can make people laugh with just a few lines.\"\n    ),\n    allow_delegation=False,    \n)\nMoving down the code, we next see the tasks, as shown in listing 4.12. Tasks denote an\nagent\u2019s process to complete the primary system goal. They also link an agent to work\non a specific task, define the output from that task, and may include how it\u2019s executed.\nresearch_task = Task(        \n    description=(\n        \"Identify what makes the following topic:{topic} so funny.\"\n        \"Be sure to include the key elements that make it humourous.\"\n        \"Also, provide an analysis of the current social trends,\"\n        \"and how it impacts the perception of humor.\"\n    ),\n    expected_output=\"A comprehensive 3 paragraphs long report \n\u27a5     on the latest jokes.\",              \n    agent=joke_researcher,     \n)\nwrite_task = Task(  \n    description=(\n        \"Compose an insightful, humourous and socially aware joke on \n{topic}.\"\n        \"Be sure to include the key elements that make it funny and\"\n        \"relevant to the current social trends.\"\n    ),\n    expected_output=\"A joke on {topic}.\",  \n    agent=joke_writer,        \n    async_execution=False,         \n    output_file=\"the_best_joke.md\",     \n)\nListing 4.12\ncrewai_introduction.py (task section)\nThe agents can either be delegated to or \nare allowed to delegate; True means they \ncan delegate.\nCreates the \nagents and \nprovides them \na goal\nverbose allows the\nagent to emit output\nto the terminal.\nSupports the use of\nmemory for the agents\nThe backstory is the\nagent\u2019s background\u2014\nits persona.\nThe Task description defines how \nthe agent will complete the task.\nExplicitly defines the expected \noutput from performing the task\nThe \nagent \nassigned \nto work \non the \ntask\nThe Task description defines how \nthe agent will complete the task.\nExplicitly defines the expected \noutput from performing the task\nIf the agent should execute \nasynchronously\nAny output the agent will generate\n",
      "page_number": 103
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 111-118)",
      "start_page": 111,
      "end_page": 118,
      "detection_method": "topic_boundary",
      "content": "87\n4.4\nBuilding an agent crew with CrewAI\nNow, we can see how everything comes together as the Crew at the bottom of the file,\nas shown in listing 4.13. Again, many options can be set when building the Crew,\nincluding the agents, tasks, process type, memory, cache, maximum requests per min-\nute (max_rpm), and whether the crew shares.\ncrew = Crew(\n    agents=[joke_researcher, joke_writer],   \n    tasks=[research_task, write_task],    \n    process=Process.sequential,     \n    memory=True,     \n    cache=True,    \n    max_rpm=100,    \n    share_crew=True,    \n)\nresult = crew.kickoff(inputs={\"topic\": \"AI engineer jokes\"})\nprint(result)\nWhen you\u2019re done reviewing, run the file in VS Code (F5), and watch the terminal for\nconversations and messages from the crew. As you can probably tell by now, the goal of\nthis agent system is to craft jokes related to AI engineering. Here are some of the fun-\nnier jokes generated over a few runs of the agent system:\n\uf0a1Why was the computer cold? It left Windows open.\n\uf0a1Why don\u2019t AI engineers play hide and seek with their algorithms? Because no\nmatter where they hide, the algorithms always find them in the \u201coverfitting\u201d\nroom!\n\uf0a1What is an AI engineer\u2019s favorite song? \u201cI just called to say I love you . . . and to\ncollect more data for my voice recognition software.\u201d\n\uf0a1Why was the AI engineer broke? Because he spent all his money on cookies, but\nhis browser kept eating them.\nBefore you run more iterations of the joke crew, you should read the next section.\nThis section shows how to add observability to the multi-agent system.\n4.4.2\nObserving agents working with AgentOps\nObserving a complex assemblage such as a multi-agent system is critical to understand-\ning the myriad of problems that can happen. Observability through application tracing\nis a key element of any complex system, especially one engaged in enterprise use.\n CrewAI supports connecting to a specialized agent operations platform appropri-\nately called AgentOps. This observability platform is generic and designed to support\nListing 4.13\ncrewai_introduction.py (crew section)\nThe agents assembled\ninto the crew\nThe tasks the agents\ncan work on\nDefining how the agents \nwill interact\nWhether the system should \nuse memory; needs to be set \nif agents/tasks have it on\nWhether the system \nshould use a cache, \nsimilar to AutoGen\nMaximum requests per minute \nthe system should limit itself to\nWhether the crew should share \ninformation, similar to group chat\n\n\n88\nCHAPTER 4\nExploring multi-agent systems\nobservability with any agent platform specific to LLM usage. Currently, no pricing or\ncommercialization details are available.\n Connecting to AgentOps is as simple as installing the package, getting an API key,\nand adding a line of code to your crew setup. This next exercise will go through the\nsteps to connect and run AgentOps.\n Listing 4.14 shows installing the agentops package using pip. You can install the\npackage alone or as an additional component of the crewai package. Remember that\nAgentOps can also be connected to other agent platforms for observability.\npip install agentops\nor as an option with CrewAI\npip install crewai[agentops]\nBefore using AgentOps, you need to sign up for an API key. Following are the general\nsteps to sign up for a key at the time of writing:\n1\nVisit https://app.agentops.ai in your browser.\n2\nSign up for an account. \n3\nCreate a project, or use the default.\n4\nGo to Settings > Projects and API Keys.\n5\nCopy and/or generate a new API key; this will copy the key to your browser.\n6\nPaste the key to your .env file in your project.\nAfter the API key is copied, it should resemble the example shown in the following\nlisting.\nAGENTOPS_API_KEY=\"your API key\"\nNow, we need to add a few lines of code to the CrewAI script. Listing 4.16 shows the\nadditions as they are added to the crewai_agentops.py file. When creating your own\nscripts, all you need to do is add the agentops package and initialize it when using\nCrewAI.\nimport agentops     \nfrom crewai import Agent, Crew, Process, Task\nfrom dotenv import load_dotenv\nload_dotenv()\nagentops.init()    \nListing 4.14\nInstalling AgentOps\nListing 4.15\nenv.: Adding an AgentOps key\nListing 4.16\ncrewai_agentops.py (AgentOps additions)\nThe addition of the \nrequired package\nMake sure to initialize the \npackage after the environment \nvariables are loaded.\n\n\n89\n4.4\nBuilding an agent crew with CrewAI\nRun the crewai_agentops.py file in VS Code (F5), and watch the agents work as before.\nHowever, you can now go to the AgentOps dashboard and view the agent interactions\nat various levels.\n Figure 4.11 shows the dashboard for running the joke crew to create the best joke.\nSeveral statistics include total duration, the run environment, prompt and completion\ntokens, LLM call timings, and estimated cost. Seeing the cost can be both sobering\nand indicative of how verbose agent conversations can become.\nThe AgentOps platform is an excellent addition to any agent platform. While it\u2019s built\ninto CrewAI, it\u2019s helpful that the observability could be added to AutoGen or other\nframeworks. Another attractive thing about AgentOps is that it\u2019s dedicated to observing\nagent interactions and not transforming from a machine learning operations platform.\nIn the future, we\u2019ll likely see the spawn of more agent observability patterns.\n One benefit that can\u2019t be overstated is the cost observation that an observability\nplatform can provide. Did you notice in figure 4.11 that creating a single joke costs a\nlittle over 50 cents? Agents can be very powerful, but they can also become very costly,\nand it\u2019s essential to observe what those costs are in terms of practicality and commer-\ncialization.\nYou can even track individual\nLLM calls, actions, and tool use.\nPrompts and replies are also\ncaptured for all iterations.\nVarious statistics are captured with\nrespect to the entire agent conversation\nsequence, including costs.\nSystem information\nis captured as well.\nFigure 4.11\nThe AgentOps dashboard for running the joke crew\n\n\n90\nCHAPTER 4\nExploring multi-agent systems\n In the last section of this chapter, we\u2019ll return to CrewAI and revisit building\nagents that can code games. This will provide an excellent comparison between the\ncapabilities of AutoGen and CrewAI.\n4.5\nRevisiting coding agents with CrewAI\nA great way to compare capabilities between multi-agent platforms is to implement\nsimilar tasks in a bot. In this next set of exercises, we\u2019ll employ CrewAI as a game pro-\ngramming team. Of course, this could be adapted to other coding tasks as well.\n Open crewai_coding_crew.py in VS Code, and we\u2019ll first review the agent section\nin listing 4.17. Here, we\u2019re creating a senior engineer, a QA engineer, and a chief QA\nengineer with a role, goal, and backstory. \nprint(\"## Welcome to the Game Crew\")     \nprint(\"-------------------------------\")\ngame = input(\"What is the game you would like to build?\n\u27a5 What will be the mechanics?\\n\")\nsenior_engineer_agent = Agent(\n    role=\"Senior Software Engineer\",\n    goal=\"Create software as needed\",\n    backstory=dedent(\n        \"\"\"\n        You are a Senior Software Engineer at a leading tech think tank.\n        Your expertise in programming in python. and do your best to\n        produce perfect code\n        \"\"\"\n    ),\n    allow_delegation=False,\n    verbose=True,\n)\nqa_engineer_agent = Agent(\n    role=\"Software Quality Control Engineer\",\n    goal=\"create prefect code, by analizing the code \n\u27a5 that is given for errors\",\n    backstory=dedent(\n        \"\"\"\n        You are a software engineer that specializes in checking code\n        for errors. You have an eye for detail and a knack for finding\n        hidden bugs.\n        You check for missing imports, variable declarations, mismatched\n        brackets and syntax errors.\n        You also check for security vulnerabilities, and logic errors\n        \"\"\"\n    ),\n    allow_delegation=False,\n    verbose=True,\n)\nListing 4.17\ncrewai_coding_crew.py (agent section)\nAllows the user \nto input the \ninstructions for \ntheir game\n\n\n91\n4.5\nRevisiting coding agents with CrewAI\nchief_qa_engineer_agent = Agent(\n    role=\"Chief Software Quality Control Engineer\",\n    goal=\"Ensure that the code does the job that it is supposed to do\",\n    backstory=dedent(\n        \"\"\"\n        You are a Chief Software Quality Control Engineer at a leading\n        tech think tank. You are responsible for ensuring that the code\n        that is written does the job that it is supposed to do.\n        You are responsible for checking the code for errors and ensuring\n        that it is of the highest quality.\n        \"\"\"\n    ),\n    allow_delegation=True,    \n    verbose=True,\n)\nScrolling down in the file will display the agent tasks, as shown in listing 4.18. The task\ndescriptions and expected output should be easy to follow. Again, each agent has a\nspecific task to provide better context when working to complete the task.\ncode_task = Task(\n    description=f\"\"\"\nYou will create a game using python, these are the instructions:\n        Instructions\n        ------------\n        {game}            \n        You will write the code for the game using python.\"\"\",\n    expected_output=\"Your Final answer must be the \n\u27a5 full python code, only the python code and nothing else.\",\n    agent=senior_engineer_agent,\n)\nqa_task = Task(\n    description=f\"\"\"You are helping create a game \n\u27a5 using python, these are the instructions:\n        Instructions\n        ------------\n        {game}            \n        Using the code you got, check for errors. Check for logic errors,\n        syntax errors, missing imports, variable declarations, \nmismatched brackets,\n        and security vulnerabilities.\"\"\",\n    expected_output=\"Output a list of issues you found in the code.\",\n    agent=qa_engineer_agent,\n)\nevaluate_task = Task(\n    description=f\"\"\"You are helping create a game \n\u27a5 using python, these are the instructions:\n        Instructions\n        ------------\n        {game}            \nListing 4.18\ncrewai_coding_crew.py (task section)\nOnly the chief QA engineer \ncan delegate tasks.\nThe game instructions\nare substituted into the\nprompt using Python\nformatting.\n\n\n92\nCHAPTER 4\nExploring multi-agent systems\n        You will look over the code to insure that it is complete and\n        does the job that it is supposed to do. \"\"\",\n    expected_output=\"Your Final answer must be the \n\u27a5 corrected a full python code, only the python code and nothing else.\",\n    agent=chief_qa_engineer_agent,\n)\nFinally, we can see how this comes together by going to the bottom of the file, as\nshown in listing 4.19. This crew configuration is much like what we\u2019ve seen before.\nEach agent and task are added, as well as the verbose and process attributes. For this\nexample, we\u2019ll continue to use sequential methods. \ncrew = Crew(\n    agents=[senior_engineer_agent, \n            qa_engineer_agent, \n            chief_qa_engineer_agent],\n    tasks=[code_task, qa_task, evaluate_task],\n    verbose=2,  \n    process=Process.sequential,     \n)\n# Get your crew to work!\nresult = crew.kickoff()   \nprint(\"######################\")\nprint(result)\nWhen you run the VS Code (F5) file, you\u2019ll be prompted to enter the instructions for\nwriting a game. Enter some instructions, perhaps the snake game or another game\nyou choose. Then, let the agents work, and observe what they produce.\n With the addition of the chief QA engineer, the results will generally look better\nthan what was produced with AutoGen, at least out of the box. If you review the code,\nyou\u2019ll see that it generally follows good patterns and, in some cases, may even include\ntests and unit tests.\n Before we finish the chapter, we\u2019ll make one last change to the crew\u2019s processing\npattern. Previously, we employed sequential processing, as shown in figure 4.10. Fig-\nure 4.12 shows what hierarchical processing looks like in CrewAI. \n Adding this manager is a relatively simple process. Listing 4.20 shows the addi-\ntional code changes to a new file that uses the coding crew in a hierarchical method.\nAside from importing a class for connecting to OpenAI from LangChain, the other\naddition is adding this class as the crew manger, manager_llm.\n \n \n \n \nListing 4.19\ncrewai_coding_crew.py (crew section)\nProcess is sequential.\nNo additional context is \nprovided in the kickoff.\n\n\n93\n4.5\nRevisiting coding agents with CrewAI\nfrom langchain_openai import ChatOpenAI     \ncrew = Crew(\n    agents=[senior_engineer_agent, \n            qa_engineer_agent, \n            chief_qa_engineer_agent],\n    tasks=[code_task, qa_task, evaluate_task],\n    verbose=2,  \n    process=Process.hierarchical,    \n    manager_llm=ChatOpenAI(              \n        temperature=0, model=\"gpt-4\"     \nListing 4.20\ncrewai_hierarchy.py (crew manager sections)\nCrew\nTasks\nAgents\nTools\nMemory\nSearch\nCall APIs\nAccess data\nConversational\nTask speci\ufb01c\nSemantic\nTools can be attached\nto agents and tasks.\nresearch on this {topic}\nwrite on this topic\nwriter\ngoal:\nbackstory:\nresearcher\ngoal:\nbackstory:\nAgents have a goal\nand backstory as\ntheir persona.\ncrew\nmanager\nHierarchical processing\nis coordinated through\na managing agent.\nVarious forms of memory and\nr\na\ng\netrieval ugmented eneration (RAG)\npatterns are supported.\nFigure 4.12\nHierarchical processing of agents coordinated through a crew manager\nImports the LLM \nconnector from \nLangChain\nYou must set a crew \nmanager when selecting \nhierarchical processing.\nSets the crew manager to \nbe the LLM connector\n\n\n94\nCHAPTER 4\nExploring multi-agent systems\n    ),   \n)        \nRun this file in VS Code (F5). When prompted, enter a game you want to create. Try\nusing the same game you tried with AutoGen; the snake game is also a good baseline\nexample. Observe the agents work through the code and review it repeatedly for\nproblems.\n After you run the file, you can also jump on AgentOps to review the cost of this\nrun. Chances are, it will cost over double what it would have without the agent man-\nager. The output will also likely not be significantly better. This is the trap of building\nagent systems without understanding how quickly things can spiral.\n An example of this spiral that often happens when agents continually iterate over\nthe same actions is frequently repeating tasks. You can view this problem in AgentOps,\nas shown in figure 4.13, by viewing the Repeat Thoughts plot. \nThe Repeat Thoughts plot from AgentOps is an excellent way to measure the repeti-\ntion your agent system encounters. Overly repetitive thought patterns typically\nmean the agent isn\u2019t being decisive enough and instead keeps trying to generate a\ndifferent answer. If you encounter this problem, you want to change the agents\u2019 pro-\ncessing patterns, tasks, and goals. You may even want to alter the system\u2019s type and\nnumber of agents.\n Multi-agent systems are an excellent way to break up work in terms of work pat-\nterns of jobs and tasks. Generally, the job role is allocated to an agent role/persona,\nYou must set a crew manager when \nselecting hierarchical processing.\nFigure 4.13\nThe repetition of thoughts as they occurred within an agent run\nPlot indicates the repetition\nof the same thoughts in an\nagent interaction.\n2\n1.5\n1\n0.5\n",
      "page_number": 111
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 119-130)",
      "start_page": 119,
      "end_page": 130,
      "detection_method": "topic_boundary",
      "content": "95\n4.6\nExercises\nand the tasks it needs to complete may be implicit, as in AutoGen, or more explicit, as\nin CrewAI.\n In this chapter, we covered many useful tools and platforms that you can use right\naway to improve your work, life, and more. That completes our journey through multi-\nagent platforms, but it doesn\u2019t conclude our exploration and use of multiple agents,\nas we\u2019ll discover in later chapters.\n4.6\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Basic Agent Communication with AutoGen\nObjective\u2014Familiarize yourself with basic agent communications and setup in\nAutoGen.\nTasks:\n\u2013 Set up AutoGen Studio on your local machine, following the instructions\nprovided in this chapter.\n\u2013 Create a simple multi-agent system with a user proxy and two assistant agents.\n\u2013 Implement a basic task where the user proxy coordinates between the assis-\ntant agents to generate a simple text output, such as summarizing a short\nparagraph.\n\uf0a1Exercise 2\u2014Implementing Advanced Agent Skills in AutoGen Studio\nObjective\u2014Enhance agent capabilities by adding advanced skills.\nTasks:\n\u2013 Develop and integrate a new skill into an AutoGen agent that allows it to\nfetch and display real-time data from a public API (e.g., weather information\nor stock prices).\n\u2013 Ensure the agent can ask for user preferences (e.g., city for weather, type of\nstocks) and display the fetched data accordingly.\n\uf0a1Exercise 3\u2014Role-Based Task Management with CrewAI\nObjective\u2014Explore role-based task management in CrewAI.\nTasks:\n\u2013 Design a CrewAI setup where multiple agents are assigned specific roles\n(e.g., data fetcher, analyzer, presenter).\n\u2013 Configure a task sequence where the data fetcher collects data, the analyzer\nprocesses the data, and the presenter generates a report.\n\u2013 Execute the sequence and observe the flow of information and task delega-\ntion among agents.\n\uf0a1Exercise 4\u2014Multi-Agent Collaboration in Group Chat Using AutoGen\nObjective\u2014Understand and implement a group chat system in AutoGen to facil-\nitate agent collaboration.\n\n\n96\nCHAPTER 4\nExploring multi-agent systems\nTasks:\n\u2013 Set up a scenario where multiple agents need to collaborate to solve a com-\nplex problem (e.g., planning an itinerary for a business trip).\n\u2013 Use the group chat feature to allow agents to share information, ask ques-\ntions, and provide updates to each other.\n\u2013 Monitor the agents\u2019 interactions and effectiveness in collaborative problem\nsolving.\n\uf0a1Exercise 5\u2014Adding and Testing Observability with AgentOps in CrewAI\nObjective\u2014Implement and evaluate the observability of agents using AgentOps\nin a CrewAI environment.\nTasks:\n\u2013 Integrate AgentOps into a CrewAI multi-agent system.\n\u2013 Design a task for the agents that involves significant computation or data\nprocessing (e.g., analyzing customer reviews to determine sentiment trends).\n\u2013 Use AgentOps to monitor the performance, cost, and output accuracy of the\nagents. Identify any potential inefficiencies or errors in agent interactions.\nSummary\n\uf0a1AutoGen, developed by Microsoft, is a conversational multi-agent platform that\nemploys a variety of agent types, such as user proxies and assistant agents, to\nfacilitate task execution through natural language interactions.\n\uf0a1AutoGen Studio acts as a development environment that allows users to create,\ntest, and manage multi-agent systems, enhancing the usability of AutoGen.\n\uf0a1AutoGen supports multiple communication patterns, including group chats\nand hierarchical and proxy communications. Proxy communication involves a\nprimary agent (proxy) that interfaces between the user and other agents to\nstreamline task completion.    \n\uf0a1CrewAI offers a structured approach to building multi-agent systems with a focus\non enterprise applications. It emphasizes role-based and autonomous agent func-\ntionalities, allowing for flexible, sequential, or hierarchical task management.\n\uf0a1Practical exercises in the chapter illustrate how to set up and use AutoGen Stu-\ndio, including installing necessary components and running basic multi-agent\nsystems.\n\uf0a1Agents in AutoGen can be equipped with specific skills to perform tasks such as\ncode generation, image analysis, and data retrieval, thereby broadening their\napplication scope.\n\uf0a1CrewAI is distinguished by its ability to structure agent interactions more rigidly\nthan AutoGen, which can be advantageous in settings that require precise and\ncontrolled agent behavior.\n\uf0a1CrewAI supports integrating memory and tools for agents to consume through\ntask completion.\n\n\n97\nSummary\n\uf0a1CrewAI supports integration with observability tools such as AgentOps, which\nprovides insights into agent performance, interaction efficiency, and cost\nmanagement.\n\uf0a1AgentOps is an agent observability platform that can help you easily monitor\nextensive agent interactions.\n\n\n98\nEmpowering agents\nwith actions\nIn this chapter, we explore actions through the use of functions and how agents\ncan use them as well. We\u2019ll start by looking at OpenAI function calling and then\nquickly move on to another project from Microsoft called Semantic Kernel (SK),\nwhich we\u2019ll use to build and manage skills and functions for agents or as agents.\n We\u2019ll finish the chapter using SK to host our first agent system. This will be a\ncomplete chapter with plenty of annotated code examples. \nThis chapter covers\n\uf0a1How an agent acts outside of itself using actions\n\uf0a1Defining and using OpenAI functions\n\uf0a1The Semantic Kernel and how to use \nsemantic functions\n\uf0a1Synergizing semantic and native functions\n\uf0a1Instantiating a GPT interface with \nSemantic Kernel \n\n\n99\n5.1\nDefining agent actions\n5.1\nDefining agent actions\nChatGPT plugins were first introduced to provide a session with abilities, skills, or\ntools. With a plugin, you can search the web or create spreadsheets or graphs. Plugins\nprovide ChatGPT with the means to extend the platform.\n Figure 5.1 shows how a ChatGPT plugin works. In this example, a new movie rec-\nommender plugin has been installed in ChatGPT. When a user asks ChatGPT to rec-\nommend a new movie, the large language model (LLM) recognizes that it has a\nplugin to manage that action. It then breaks down the user request into actionable\nparameters, which it passes to the new movie recommender.\nThe recommender then scrapes a website showcasing new movies and appends that\ninformation to a new prompt request to an LLM. With this information, the LLM\nCalls the plugin/function\nwith parameters\nHere are some new movies\nyou may like to see...\nChatGPT with a new\nmovie recommender\nplugin\nChatGPT user\nCon\ufb01rms the use of\nthe recommender plugin\nGPT-4.5?\nPlugin calls LLM to get a recommendation\nfor the list of new movies.\nPlugin uses a service\nto nd new movie.\n\ufb01\nPlugin replies with\nrecommended new movie.\nNew Movie Recommender\nplugin (agent)\nThe plugin/agent\nscrapes websites for\nnew movies.\nUse External Tools\nEnhances model capabilities.\nTactics include embeddings-based search, code execution, and access\nto speci c functions.\n\ufb01\nActions\nMemory\nPrompt Engineering Strategies\nCan you recommend\na new movie?\nA plugin may use the same,\ndifferent, or even multiple\nLLMs.\nSecond, the plugin\nuses an LLM to get\na recommendation.\nFirst, the plugin scrapes\na site for a list of new\nmovies.\nThe LLM recognizes the\nrequest for a plugin and\nthen extracts the input\nparameters required for\nthe plugin.\nFigure 5.1\nHow a ChatGPT plugin operates and how plugins and other external tools (e.g., APIs) align with the \nUse External Tools prompt engineering strategy\n\n\n100\nCHAPTER 5\nEmpowering agents with actions\nresponds to the recommender, which passes this back to ChatGPT. ChatGPT then\nresponds to the user with the recommended request.\n We can think of plugins as proxies for actions. A plugin generally encapsulates one\nor more abilities, such as calling an API or scraping a website. Actions, therefore, are\nextensions of plugins\u2014they give a plugin its abilities.\n AI agents can be considered plugins and consumers of plugins, tools, skills, and\nother agents. Adding skills, functions, and tools to an agent/plugin allows it to exe-\ncute well-defined actions\u2014figure 5.2 highlights where agent actions occur and their\ninteraction with LLMs and other systems.\nAn agent action is an ability that allows it to use a function, skill, or tool. What gets\nconfusing is that different frameworks use different terminology. We\u2019ll define an\naction as anything an agent can do to establish some basic definitions.\n ChatGPT plugins and functions represent an actionable ability that ChatGPT or an\nagent system can use to perform additional actions. Now let\u2019s examine the basis for\nOpenAI plugins and the function definition.\n1. The LLM recognizes the\nrequest for a plugin/agent\nand then extracts the input\nparameters required to\nactivate the agent.\nCalls the agent/plugin\nwith parameters\nAgent System\nChatGPT user\nGPT-4.5?\nAgent replies with\nrecommended new movie.\nNew Movie Recommender\nplugin (agent)\n2. The agent adds the\ninformation to a prompt\nused to make a request\nto an LLM.\nThe plugin/agent scrapes\nwebsites for new movies.\n3. The agent uses an action\nto \ufb01nd new movies.\nAn agent may use the\nsame, different, or\neven multiple LLMs.\nChevron denotes an\nagent action.\nAn agent action can be a\nfunction or skill/tool prompt.\n4. The agent system passes\nthe responses to the LLM\nto summarize the results.\nCan you recommend\na new movie?\nHere are some new movies\nyou may like to see...\nCon\ufb01rms the use of\na recommender plugin\nPlugin calls LLM to get a\nrecommendation for the list of new movies.\nAgent uses a function or\nskill to nd new movie.\n\ufb01\nFigure 5.2\nHow an agent uses actions to perform external tasks\n\n\n101\n5.2\nExecuting OpenAI functions\n5.2\nExecuting OpenAI functions\nOpenAI, with the enablement of plugins, introduced a structure specification for defin-\ning the interface between functions/plugins an LLM could action. This specification is\nbecoming a standard that LLM systems can follow to provide actionable systems.\n These same function definitions are now also being used to define plugins for\nChatGPT and other systems. Next, we\u2019ll explore how to use functions directly with\nan LLM call.\n5.2.1\nAdding functions to LLM API calls\nFigure 5.3 demonstrates how an LLM recognizes and uses the function definition to\ncast its response as the function call.\nListing 5.1 shows the details of an LLM API call using tools and a function definition.\nAdding a function definition allows the LLM to reply regarding the function\u2019s input\nparameters. This means the LLM will identify the correct function and parse the rele-\nvant parameters for the user\u2019s request.\nresponse = client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[{\"role\": \"system\",\n                   \"content\": \"You are a helpful assistant.\"},\n                  {\"role\": \"user\", \"content\": user_message}],\nListing 5.1\nfirst_function.py (API call)\nGPT-4\nMake a request to\nLLM using tools.\nCon\ufb01rms the request matches\na particular function de\ufb01nition\nExtracts parameters matching\nthe function de nition from the\n\ufb01\noriginal request\nRequest\nModel: GPT-4\nMessages:\nSystem: you are a ...\nUser: please recommend a movie.\nParameters:\nTemperature: .7\nMax tokens: 256\nTools\n\"type\": \"function\",\n:\nReplies with the tool name\n(function) and input parameters\nfor the function\nTools represents plugins or\nfunctions added to a request.\nThe LLM does not\nexecute the function.\nIf the LLM doesn\u2019t\nmatch any tools, it\nwill respond given\nthe expected prompt.\nFigure 5.3\nHow a single LLM request, including tools, gets interpreted by an LLM\n\n\n102\nCHAPTER 5\nEmpowering agents with actions\n        temperature=0.7,\n        tools=[    \n            {\n                \"type\": \"function\",    \n                \"function\": {\n                    \"name\": \"recommend\",\n                    \"description\": \"Provide a \u2026 topic.\",    \n                    \"parameters\": {\n                        \"type\": \"object\",    \n                        \"properties\": {\n                            \"topic\": {\n                                \"type\": \"string\",\n                                \"description\": \n                                   \"The topic,\u2026 for.\",    \n                            },\n                            \"rating\": {\n                                \"type\": \"string\",\n                                \"description\": \n                          \"The rating \u2026 given.\",    \n                                \"enum\": [\"good\",\n                                         \"bad\", \n                                         \"terrible\"]    \n                                },\n                        },\n                        \"required\": [\"topic\"],\n                    },\n                },\n                }\n            ]\n        )\nTo see how this works, open Visual Studio Code (VS Code) to the book\u2019s source code\nfolder: chapter_4/first_function.py. It\u2019s a good practice to open the relevant chapter\nfolder in VS Code to create a new Python environment and install the requirements.txt\nfile. If you need assistance with this, consult appendix B.\n Before starting, correctly set up an .env file in the chapter_4 folder with your\nAPI credentials. Function calling is an extra capability provided by the LLM com-\nmercial service. At the time of writing, this feature wasn\u2019t an option for open\nsource LLM deployments.\n Next, we\u2019ll look at the bottom of the code in first_function.py, as shown in list-\ning 5.2. Here are just two examples of calls made to an LLM using the request previ-\nously specified in listing 5.1. Here, each request shows the generated output from\nrunning the example.\nuser = \"Can you please recommend me a time travel movie?\"\nresponse = ask_chatgpt(user)    \nprint(response)\nListing 5.2\nfirst_function.py (exercising the API)\nNew parameter called tools\nSets the type of tool to function\nProvides an excellent \ndescription of what \nthe function does\nDefines the type of parameters \nfor input; an object represents \na JSON document.\nExcellent \ndescriptions \nfor each input \nparameter\nYou can even \ndescribe in terms \nof enumerations.\nPreviously \ndefined function\n\n\n103\n5.2\nExecuting OpenAI functions\n###Output\nFunction(arguments='{\"topic\":\"time travel movie\"}', \n                      name='recommend')    \nuser = \"Can you please recommend me a good time travel movie?\"\nresponse = ask_chatgpt(user)    \nprint(response)\n###Output\nFunction(arguments='{\"topic\":\"time travel movie\",\n                     \"rating\":\"good\"}',\n name='recommend')    \nRun the first_function.py Python script in VS Code using the debugger (F5) or the\nterminal to see the same results. Here, the LLM parses the input request to match any\nregistered tools. In this case, the tool is the single function definition, that is, the rec-\nommended function. The LLM extracts the input parameters from this function and\nparses those from the request. Then, it replies with the named function and desig-\nnated input parameters.\nNOTE\nThe actual function isn\u2019t being called. The LLM only returns the sug-\ngested function and the relevant input parameters. The name and parame-\nters must be extracted and passed into a function matching the signature to\nact on the function. We\u2019ll look at an example of this in the next section.\n5.2.2\nActioning function calls\nNow that we understand that an LLM doesn\u2019t execute the function or plugin directly,\nwe can look at an example that executes the tools. Keeping with the recommender\ntheme, we\u2019ll look at another example that adds a Python function for simple\nrecommendations.\n Figure 5.4 shows how this simple example will work. We\u2019ll submit a single request\nthat includes a tool function definition, asking for three recommendations. The LLM,\nin turn, will reply with the three function calls with input parameters (time travel, rec-\nipe, and gift). The results from executing the functions are then passed back to the\nLLM, which converts them back to natural language and returns a reply.\n Now that we understand the example, open parallel_functions.py in VS Code.\nListing 5.3 shows the Python function that you want to call to give recommendations.\n \n \n \n \n \n \n \nReturned in \nthe name of the \nfunction to call \nand the extracted \ninput parameters\nPreviously \ndefined function\nReturned in the name of the function to \ncall and the extracted input parameters\n\n\n104\nCHAPTER 5\nEmpowering agents with actions\ndef recommend(topic, rating=\"good\"):\n    if \"time travel\" in topic.lower():    \n        return json.dumps({\"topic\": \"time travel\",\n                           \"recommendation\": \"Back to the Future\",\n                           \"rating\": rating})\n    elif \"recipe\" in topic.lower():    \n        return json.dumps({\"topic\": \"recipe\",\n                           \"recommendation\": \"The best thing \u2026 ate.\",\n                           \"rating\": rating})\n    elif \"gift\" in topic.lower():    \n        return json.dumps({\"topic\": \"gift\",\n                           \"recommendation\": \"A glorious new...\",\n                           \"rating\": rating})\n    else:    \n        return json.dumps({\"topic\": topic,\n                           \"recommendation\": \"unknown\"})    \nNext, we\u2019ll look at the function called run_conversation, where all the work starts\nwith the request construction.\nuser = \"\"\"Can you please make recommendations for the following:\n1. Time travel movies\n2. Recipes\nListing 5.3\nparallel_functions.py (recommend function)\nListing 5.4\nparallel_functions.py (run_conversation, request)\nMake a request to\nLLM using tools.\nGPT\nRequest\nMessages:\nUser: Can you please make\nrecommendations for the following:\n1. Time travel movies\n2. Recipes\n3. Gifts\nTools: recommend function de nition\n\ufb01\nCon\ufb01rms the request matches a\nparticular function de\ufb01nition and\nthere are 3 calls to evaluate\nReturns 3 tool calls to the function recommend\n\u201c\n\u201d\nCreates 3 function replies, one\nfor each recommendation\nAdd results of function execution to\nconversation history, and ask LLM to respond.\nExecute functions.\nReturn results of all three recommendations\nin natural language.\nGPT\nReturns the function\nname and parameters\nCould be the same\nor different LLM\nFigure 5.4\nA sample request returns three tool function calls and then submits the results back to the LLM \nto return a natural language response.\nChecks to \nsee if the \nstring is \ncontained \nwithin the \ntopic input\nIf no topic is \ndetected, returns \nthe default\nReturns a \nJSON object\n\n\n105\n5.2\nExecuting OpenAI functions\n3. Gifts\"\"\"    \nmessages = [{\"role\": \"user\", \"content\": user}]    \ntools = [    \n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"recommend\",\n            \"description\": \n                \"Provide a recommendation for any topic.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"topic\": {\n                        \"type\": \"string\",\n                        \"description\": \n                              \"The topic, \u2026 recommendation for.\",\n                        },\n                        \"rating\": {\n                            \"type\": \"string\",\n                            \"description\": \"The rating \u2026 was given.\",\n                            \"enum\": [\"good\", \"bad\", \"terrible\"]\n                            },\n                        },\n                \"required\": [\"topic\"],\n                },\n            },\n        }\n    ]\nListing 5.5 shows the request being made, which we\u2019ve covered before, but there are a\nfew things to note. This call uses a lower model such as GPT-3.5 because delegating\nfunctions is a more straightforward task and can be done using older, cheaper, less\nsophisticated language models.\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",    \n    messages=messages,    \n    tools=tools,    \n    tool_choice=\"auto\",  \n)\nresponse_message = response.choices[0].message    \nAt this point, after the API call, the response should hold the information for the\nrequired function calls. Remember, we asked the LLM to provide us with three recom-\nmendations, which means it should also provide us with three function call outputs, as\nshown in the following listing.\n \nListing 5.5\nparallel_functions.py (run_conversation, API call)\nThe user message \nasks for three \nrecommendations.\nNote that there is no \nsystem message.\nAdds the function \ndefinition to the tools \npart of the request\nLLMs that delegate to functions \ncan be simpler models.\nAdds the messages and tools definitions\nauto is the default.\nThe returned message \nfrom the LLM\n\n\n106\nCHAPTER 5\nEmpowering agents with actions\ntool_calls = response_message.tool_calls    \nif tool_calls:    \n    available_functions = {\n        \"recommend\": recommend,\n    }    \n    # Step 4: send the info for each function call and function response to \nthe model\n    for tool_call in tool_calls:    \n        function_name = tool_call.function.name\n        function_to_call = available_functions[function_name]\n        function_args = json.loads(tool_call.function.arguments)\n        function_response = function_to_call(\n            topic=function_args.get(\"topic\"),    \n            rating=function_args.get(\"rating\"),\n        )\n        messages.append(    \n            {\n                \"tool_call_id\": tool_call.id,\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n    second_response = client.chat.completions.create(    \n        model=\"gpt-3.5-turbo-1106\",\n        messages=messages,\n    )\n    return second_response.choices[0].message.content    \nThe tool call outputs and the calls to the recommender function results are appended\nto the messages. Notice how messages now also contain the history of the first call.\nThis is then passed back to the LLM to construct a reply in natural language.\n Debug this example in VS Code by pressing the F5 key with the file open. The fol-\nlowing listing shows the output of running parallel_functions.py.\nHere are some recommendations for you:\n1. Time travel movies: \"Back to the Future\"\n2. Recipes: \"The best thing you ever ate.\"\n3. Gifts: \"A glorious new...\" (the recommendation was cut off, so I \ncouldn't provide the full recommendation)\nI hope you find these recommendations helpful! Let me know if you need \nmore information.\nThis completes this simple demonstration. For more advanced applications, the func-\ntions could do any number of things, from scraping websites to calling search engines\nto completing far more complex tasks.\nListing 5.6\nparallel_functions.py (run_conversation, tool_calls)\nListing 5.7\nparallel_functions.py (output)\nIf the response contains \ntool calls, execute them.\nOnly one function but \ncould contain several\nLoops through the calls and replays \nthe content back to the LLM\nExecutes the recommend \nfunction from extracted \nparameters\nAppends the results of \neach function call to the \nset of messages\nSends another request \nto the LLM with updated \ninformation and returns \nthe message reply\n",
      "page_number": 119
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 131-138)",
      "start_page": 131,
      "end_page": 138,
      "detection_method": "topic_boundary",
      "content": "107\n5.3\nIntroducing Semantic Kernel\n Functions are an excellent way to cast outputs for a particular task. However, the\nwork of handling functions or tools and making secondary calls can be done in a\ncleaner and more efficient way. The following section will uncover a more robust sys-\ntem of adding actions to agents.\n5.3\nIntroducing Semantic Kernel\nSemantic Kernel (SK) is another open source project from Microsoft intended to help\nbuild AI applications, which we call agents. At its core, the project is best used to\ndefine actions, or what the platform calls semantic plugins, which are wrappers for skills\nand functions.\n Figure 5.5 shows how the SK can be used as a plugin and a consumer of OpenAI\nplugins. The SK relies on the OpenAI plugin definition to define a plugin. That way, it\ncan consume and publish itself or other plugins to other systems.\nAn OpenAI plugin definition maps precisely to the function definitions in listing 5.4.\nThis means that SK is the orchestrator of API tool calls, aka plugins. That also means\nthat SK can help organize multiple plugins with a chat interface or an agent.\nNOTE\nThe team at SK originally labeled the functional modules as skills. How-\never, to be more consistent with OpenAI, they have since renamed skills to\nLLM\nInterface is de ned like an OpenAI plugin.\n\ufb01\nSemantic\nKernel\nInterface as an OpenAI plugin\nChatGPT\nPlugins (Semantic Skills and Native Functions)\nMath Plugin\n(native function)\nRecommend Plugin\n(semantic function)\nGet Movies Plugin\n(native plugin)\nPlease recommend\na movie.\nCan be consumed as a plugin\nand also consumes plugins\nRequests can be made\ndirectly to the kernel\nThe kernel itself can be\nregistered as a plugin.\nLLM\nCan be the same\nLLM or different\nFigure 5.5\nHow the Semantic Kernel integrates as a plugin and can also consume plugins\n\n\n108\nCHAPTER 5\nEmpowering agents with actions\nplugins. What is more confusing is that the code still uses the term skills. There-\nfore, throughout this chapter, we\u2019ll use skills and plugins to mean the same thing.\nSK is a useful tool for managing multiple plugins (actions for agents) and, as we\u2019ll see\nlater, can also assist with memory and planning tools. For this chapter, we\u2019ll focus on\nthe actions/plugins. In the next section, we look at how to get started using SK.\n5.3.1\nGetting started with SK semantic functions\nSK is easy to install and works within Python, Java, and C#. This is excellent news as it also\nallows plugins developed in one language to be consumed in a different language. How-\never, you can\u2019t yet develop a native function in one language and use it in another.\n We\u2019ll continue from where we left off for the Python environment using the\nchapter_4 workspace in VS Code. Be sure you have a workspace configured if you\nwant to explore and run any examples.\n Listing 5.8 shows how to install SK from a terminal within VS Code. You can also\ninstall the SK extension for VS Code. The extension can be a helpful tool to create\nplugins/skills, but it isn\u2019t required.\npip uninstall semantic-kernel    \ngit clone https://github.com/microsoft/semantic-kernel.git    \ncd semantic-kernel/python    \npip install -e .    \nOnce you finish the installation, open SK_connecting.py in VS Code. Listing 5.9\nshows a demo of running an example quickly through SK. The example creates a chat\ncompletion service using either OpenAI or Azure OpenAI.\nimport semantic_kernel as sk\nselected_service = \"OpenAI\"    \nkernel = sk.Kernel()    \nservice_id = None\nif selected_service == \"OpenAI\":\n    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n    api_key, org_id = sk.openai_settings_from_dot_env()    \n    service_id = \"oai_chat_gpt\"\n    kernel.add_service(\n        OpenAIChatCompletion(\n            service_id=service_id,\n            ai_model_id=\"gpt-3.5-turbo-1106\",\nListing 5.8\nInstalling Semantic Kernel \nListing 5.9\nSK_connecting.py\nUninstalls any previous installations of SK\nClones the \nrepository to \na local folder\nChanges to the source folder\nInstalls the editable package \nfrom the source folder\nSets the service you\u2019re using \n(OpenAI or Azure OpenAI)\nCreates the \nkernel\nLoads secrets \nfrom the .env file \nand sets them on \nthe chat service\n\n\n109\n5.3\nIntroducing Semantic Kernel\n            api_key=api_key,\n            org_id=org_id,\n        ),\n    )\nelif selected_service == \"AzureOpenAI\":\n    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n    deployment, api_key, endpoint = \n\u27a5 sk.azure_openai_settings_from_dot_env()  \n    service_id = \"aoai_chat_completion\"\n    kernel.add_service(\n        AzureChatCompletion(\n            service_id=service_id,\n            deployment_name=deployment,\n            endpoint=endpoint,\n            api_key=api_key,\n        ),\n    )\n#This function is currently broken\nasync def run_prompt():\n    result = await kernel.invoke_prompt( \n              \u27a5 prompt=\"recommend a movie about \n\u27a5 time travel\")    \n    print(result)\n# Use asyncio.run to execute the async function\nasyncio.run(run_prompt())    \n###Output\nOne highly recommended time travel movie is \"Back to the Future\" (1985) \ndirected by Robert Zemeckis. This classic film follows the adventures of \nteenager Marty McFly (Michael J. Fox)\u2026\nRun the example by pressing F5 (debugging), and you should see an output similar to\nlisting 5.9. This example demonstrates how a semantic function can be created with SK\nand executed. A semantic function is the equivalent of a prompt template in prompt\nflow, another Microsoft tool. In this example, we define a simple prompt as a function.\n It\u2019s important to note that this semantic function isn\u2019t defined as a plugin. How-\never, the kernel can create the function as a self-contained semantic element that\ncan be executed against an LLM. Semantic functions can be used alone or regis-\ntered as plugins, as you\u2019ll see later. Let\u2019s jump to the next section, where we intro-\nduce contextual variables.\n5.3.2\nSemantic functions and context variables\nExpanding on the previous example, we can look at adding contextual variables to the\nsemantic function. This pattern of adding placeholders to prompt templates is one\nwe\u2019ll review over and over. In this example, we look at a prompt template that has\nplaceholders for subject, genre, format, and custom.\nLoads secrets \nfrom the .env file \nand sets them on \nthe chat service\nInvokes the \nprompt\nCalls the function \nasynchronously\n\n\n110\nCHAPTER 5\nEmpowering agents with actions\n Open SK_context_variables.py in VS Code, as shown in the next listing. The\nprompt is equivalent to setting aside a system and user section of the prompt.\n#top section omitted\u2026\nprompt = \"\"\"    \nsystem:\nYou have vast knowledge of everything and can recommend anything provided \nyou are given the following criteria, the subject, genre, format and any \nother custom information.\nuser:\nPlease recommend a {{$format}} with the subject {{$subject}} and {{$genre}}.\nInclude the following custom information: {{$custom}}\n\"\"\"\nprompt_template_config = sk.PromptTemplateConfig(    \n    template=prompt,\n    name=\"tldr\",\n    template_format=\"semantic-kernel\",\n    input_variables=[\n        InputVariable(\n            name=\"format\", \n            description=\"The format to recommend\", \n            is_required=True\n        ),\n        InputVariable(\n            name=\"suject\", \n            description=\"The subject to recommend\", \n            is_required=True\n        ),\n        InputVariable(\n            name=\"genre\", \n            description=\"The genre to recommend\", \n            is_required=True\n        ),\n        InputVariable(\n            name=\"custom\",\n            description=\"Any custom information [CA]\n                       to enhance the recommendation\",\n            is_required=True,\n        ),\n    ],\n    execution_settings=execution_settings,\n)\nrecommend_function = kernel.create_function_from_prompt(    \n    prompt_template_config=prompt_template_config,\n    function_name=\"Recommend_Movies\",\n    plugin_name=\"Recommendation\",\n)\nListing 5.10\nSK_context_variables.py\nDefines a prompt \nwith placeholders\nConfigures a \nprompt template \nand input variable \ndefinitions\nCreates a kernel \nfunction from \nthe prompt\n\n\n111\n5.4\nSynergizing semantic and native functions\nasync def run_recommendation(    \n    subject=\"time travel\",\n    format=\"movie\", \n    genre=\"medieval\", \n           custom=\"must be a comedy\"\n):\n    recommendation = await kernel.invoke(\n        recommend_function,\n        sk.KernelArguments(subject=subject,\n                      format=format, \n                      genre=genre, \n                      custom=custom),    \n    )\n    print(recommendation)\n# Use asyncio.run to execute the async function\nasyncio.run(run_recommendation())\n###Output\nOne movie that fits the criteria of being about time travel, set in a \nmedieval period, and being a comedy is \"The Visitors\" (Les Visiteurs) \nfrom 1993. This French film, directed by Jean-Marie Poir\u00e9, follows a \nknight and his squire who are transported to the modern era by a \nwizard\u2019s spell gone wrong.\u2026\nGo ahead and debug this example (F5), and wait for the output to be generated. That is\nthe basis for setting up SK and creating and exercising semantic functions. In the next\nsection, we move on to see how a semantic function can be registered as a skill/plugin.\n5.4\nSynergizing semantic and native functions\nSemantic functions encapsulate a prompt/profile and execute through interaction with\nan LLM. Native functions are the encapsulation of code that may perform anything\nfrom scraping websites to searching the web. Both semantic and native functions can\nregister as plugins/skills in the SK kernel.\n A function, semantic or native, can be registered as a plugin and used the same\nway we registered the earlier function directly with our API calls. When a function is\nregistered as a plugin, it becomes accessible to chat or agent interfaces, depending on\nthe use case. The next section looks at how a semantic function is created and regis-\ntered with the kernel.\n5.4.1\nCreating and registering a semantic skill/plugin\nThe VS Code extension for SK provides helpful tools for creating plugins/skills. In\nthis section, we\u2019ll use the SK extension to create a plugin/skill and then edit the com-\nponents of that extension. After that, we\u2019ll register and execute the plugin in the SK.\n Figure 5.6 shows the process for creating a new skill within VS Code using the SK\nextension. (Refer to appendix B for directions if you need to install this extension.)\nYou\u2019ll then be given the option for the skill/plugin folder to place the function.\nAlways group functions that are similar together. After creating a skill, enter the name\nCreates an asynchronous \nfunction to wrap the \nfunction call\nSets the \nkernel \nfunction \narguments\n\n\n112\nCHAPTER 5\nEmpowering agents with actions\nand description of the function you want to develop. Be sure to describe the function\nas if the LLM were going to use it.\nYou can see the completed skills and functions by opening the skills/plugin folder\nand reviewing the files. We\u2019ll follow the previously constructed example, so open the\nskills/Recommender/Recommend_Movies folder, as shown in figure 5.7. Inside this folder\nis a config.json file, the function description, and the semantic function/prompt in\na file called skprompt.txt.\n Listing 5.11 shows the contents of the semantic function definition, also known as\nthe plugin definition. Note that the type is marked as completion and not of type\nfunction because this is a semantic function. We would define a native function as a\ntype function.\n \n \n \n1. Select the icon to create a\nnew semantic skill/plugin.\n2. Select an existing skill\nfolder, or create a new one.\n3. Name the function.\n4. Then, provide a description.\nFigure 5.6\nThe process of creating a new skill/plugin\n\n\n113\n5.4\nSynergizing semantic and native functions\n{\n    \"schema\": 1,\n    \"type\": \"completion\",    \n    \"description\": \"A function to recommend movies based on users list of \npreviously seen movies.\",\n    \"completion\": {    \n        \"max_tokens\": 256,\n        \"temperature\": 0,\n        \"top_p\": 0,\n        \"presence_penalty\": 0,\n        \"frequency_penalty\": 0\n    },\n    \"input\": {\n        \"parameters\": [\n            {\n                \"name\": \"input\",    \n                \"description\": \"The users list of previously seen movies.\",\n                \"defaultValue\": \"\"\n            }\n        ]\n    },\n    \"default_backends\": []\n}\nNext, we can look at the definition of the semantic function prompt, as shown in list-\ning 5.12. The format is a little different, but what we see here matches the earlier\nexamples using templating. This prompt recommends movies based on a list of mov-\nies the user has previously seen.\nYou are a wise movie recommender and you have been asked to recommend a \nmovie to a user.\nYou are provided a list of movies that the user has watched before.\nYou want to recommend a movie that the user has not watched before.\n[INPUT]\nListing 5.11\nRecommend_Movies/config.json\nListing 5.12\nRecommend_Movies/skprompt.txt\nThe folder containing\nthe\n/plugin\nskill\nAn inner folder that holds\nthe plugin/skill de\ufb01nitions\nDe\ufb01nes the function/plugin\ndescription\nPrompt that de\ufb01nes the\nsemantic function\nFigure 5.7\nThe file and folder \nstructure of a semantic \nfunction skill/plugin\nSemantic functions are \nfunctions of type completion.\nWe can also set the \ncompletion parameters for \nhow the function is called.\nDefines the parameters \ninput into the semantic \nfunction\n\n\n114\nCHAPTER 5\nEmpowering agents with actions\n{{$input}}\n[END INPUT]\nNow, we\u2019ll dive into the code that loads the skill/plugin and executes it in a simple\nexample. Open the SK_first_skill.py file in VS Code. The following listing shows\nan abridged version highlighting the new sections.\nkernel = sk.Kernel()\nplugins_directory = \"plugins\"\nrecommender = kernel.import_plugin_from_prompt_directory(\n    plugins_directory,\n    \"Recommender\",\n)    \nrecommend = recommender[\"Recommend_Movies\"]\nseen_movie_list = [    \n    \"Back to the Future\",\n    \"The Terminator\",\n    \"12 Monkeys\",\n    \"Looper\",\n    \"Groundhog Day\",\n    \"Primer\",\n    \"Donnie Darko\",\n    \"Interstellar\",\n    \"Time Bandits\",\n    \"Doctor Strange\",\n]\nasync def run():\n    result = await kernel.invoke(\n        recommend,\n        sk.KernelArguments(    \n            settings=execution_settings, input=\", \".join(seen_movie_list)\n        ),\n    )\n    print(result)\nasyncio.run(run())    \n###Output\nBased on the list of movies you've provided, it seems you have an \ninterest in science fiction, time travel, and mind-bending narratives. \nGiven that you've watched a mix of classics and modern films in this \ngenre, I would recommend the following movie that you have not watched \nbefore:\n\"Edge of Tomorrow\" (also known as \"Live Die Repeat: Edge of Tomorrow\")\u2026\nListing 5.13\nSK_first_skill.py (abridged listing)\nLoads the prompt from \nthe plugins folder\nList of user\u2019s previously \nseen movies\nInput is set to joined \nlist of seen movies.\nFunction is executed \nasynchronously.\n",
      "page_number": 131
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 139-146)",
      "start_page": 139,
      "end_page": 146,
      "detection_method": "topic_boundary",
      "content": "115\n5.4\nSynergizing semantic and native functions\nThe code loads the skill/plugin from the skills directory and the plugin folder.\nWhen a skill is loaded into the kernel and not just created, it becomes a registered\nplugin. That means it can be executed directly as is done here or through an LLM\nchat conversation via the plugin interface.\n Run the code (F5), and you should see an output like listing 5.13. We now have a\nsimple semantic function that can be hosted as a plugin. However, this function\nrequires users to input a complete list of movies they have watched. We\u2019ll look at a\nmeans to fix this by introducing native functions in the next section.\n5.4.2\nApplying native functions\nAs stated, native functions are code that can do anything. In the following example,\nwe\u2019ll introduce a native function to assist the semantic function we built earlier.\n This native function will load a list of movies the user has previously seen, from a file.\nWhile this function introduces the concept of memory, we\u2019ll defer that discussion until\nchapter 8. Consider this new native function as any code that could virtually do anything.\n Native functions can be created and registered using the SK extension. For this exam-\nple, we\u2019ll create a native function directly in code to make the example easier to follow.\n Open SK_native_functions.py in VS Code. We\u2019ll start by looking at how the\nnative function is defined. A native function is typically defined within a class, which\nsimplifies managing and instantiating native functions.\nclass MySeenMoviesDatabase:\n    \"\"\"\n    Description: Manages the list of users seen movies.    \n    \"\"\"\n    @kernel_function(    \n        description=\"Loads a list of movies \u2026 user has already seen\",\n        name=\"LoadSeenMovies\",\n    )\n    def load_seen_movies(self) -> str:    \n        try:\n            with open(\"seen_movies.txt\", 'r') as file:    \n                lines = [line.strip() for line in file.readlines()]\n                comma_separated_string = ', '.join(lines)\n            return comma_separated_string\n        except Exception as e:\n            print(f\"Error reading file: {e}\")\n            return None\nWith the native function defined, we can see how it\u2019s used by scrolling down in the\nfile, as shown in the following listing.\n \n \nListing 5.14\nSK_native_functions.py (MySeenMovieDatabase)\nProvides a description\nfor the container class\nUses a decorator to \nprovide function \ndescription and \nname\nThe actual function \nreturns a list of movies in a \ncomma-separated string.\nLoads seen\nmovies from\nthe text file\n\n\n116\nCHAPTER 5\nEmpowering agents with actions\nplugins_directory = \"plugins\"\nrecommender = kernel.import_plugin_from_prompt_directory(\n    plugins_directory,\n    \"Recommender\",\n)    \nrecommend = recommender[\"Recommend_Movies\"]\nseen_movies_plugin = kernel.import_plugin_from_object(\n    MySeenMoviesDatabase(), \"SeenMoviesPlugin\"\n)    \nload_seen_movies = seen_movies_plugin[\"LoadSeenMovies\"]    \nasync def show_seen_movies():\n    seen_movie_list = await load_seen_movies(kernel)\n    return seen_movie_list\nseen_movie_list = asyncio.run(show_seen_movies())    \nprint(seen_movie_list)\nasync def run():     \n    result = await kernel.invoke(\n        recommend,\n        sk.KernelArguments(\n                settings=execution_settings,\n                input=seen_movie_list),\n    )\n    print(result)\nasyncio.run(run())\n###Output\nThe Matrix, The Matrix Reloaded, The Matrix Revolutions, The Matrix \nResurrections \u2013 output from print statement\nBased on your interest in the \"The Matrix\" series, it seems you enjoy \nscience fiction films with a strong philosophical undertone and action \nelements. Given that you've watched all\nOne important aspect to note is how the native function was imported into the kernel.\nThe act of importing to the kernel registers that function as a plugin/skill. This means\nthe function can be used as a skill from the kernel through other conversations or\ninteractions. We\u2019ll see how to embed a native function within a semantic function in\nthe next section.\nListing 5.15\nSK_native_functions (remaining code)\nLoads the semantic function \nas shown previously\nImports the skill \ninto the kernel and \nregisters the function \nas a plugin\nLoads the native \nfunction\nExecutes the \nfunction and returns \nthe list as a string\nWraps the \nplugin call in an \nasynchronous \nfunction and \nexecutes\n\n\n117\n5.4\nSynergizing semantic and native functions\n5.4.3\nEmbedding native functions within semantic functions\nThere are plenty of powerful features within SK, but one beneficial feature is the abil-\nity to embed native or semantic functions within other semantic functions. The follow-\ning listing shows how a native function can be embedded within a semantic function.\nsk_prompt = \"\"\"\nYou are a wise movie recommender and you have been asked to recommend a \nmovie to a user.\nYou have a list of movies that the user has watched before.\nYou want to recommend a movie that \nthe user has not watched before.    \nMovie List: {{MySeenMoviesDatabase.LoadSeenMovies}}.    \n\"\"\"\nThe next example, SK_semantic_native_functions.py, uses inline native and seman-\ntic functions. Open the file in VS Code, and the following listing shows the code to\ncreate, register, and execute the functions.\nprompt_template_config = sk.PromptTemplateConfig(\n    template=sk_prompt,\n    name=\"tldr\",\n    template_format=\"semantic-kernel\",\n    execution_settings=execution_settings,\n)    \nrecommend_function = kernel.create_function_from_prompt(\n    prompt_template_config=prompt_template_config,\n    function_name=\"Recommend_Movies\",\n    plugin_name=\"Recommendation\",\n)    \nasync def run_recommendation():    \n    recommendation = await kernel.invoke(\n        recommend_function,\n        sk.KernelArguments(),\n    )\n    print(recommendation)\n# Use asyncio.run to execute the async function\nasyncio.run(run_recommendation())\n###Output\nBased on the list provided, it seems the user is a fan of the Matrix \nfranchise. Since they have watched all four existing Matrix movies, I \nwould recommend a\u2026\nListing 5.16\nSK_semantic_native_functions.py (skprompt)\nListing 5.17\nSK_semantic_native_functions.py (abridged)\nThe exact \ninstruction text \nas previous\nThe native function is referenced and identified\nby class name and function name.\nCreates the prompt template \nconfig for the prompt\nCreates an inline semantic \nfunction from the prompt\nExecutes the semantic \nfunction asynchronously\n\n\n118\nCHAPTER 5\nEmpowering agents with actions\nRun the code, and you should see an output like listing 5.17. One important aspect to\nnote is that the native function is registered with the kernel, but the semantic function\nis not. This is important because function creation doesn\u2019t register a function.\n For this example to work correctly, the native function must be registered with the\nkernel, which uses the import_plugin function call\u2014the first line in listing 5.17. How-\never, the semantic function itself isn\u2019t registered. An easy way to register the function\nis to make it a plugin and import it.\n These simple exercises showcase ways to integrate plugins and skills into chat or\nagent interfaces. In the next section, we\u2019ll look at a complete example demonstrating\nadding a plugin representing a service or GPT interface to a chat function.\n5.5\nSemantic Kernel as an interactive service agent\nIn chapter 1, we introduced the concept of the GPT interface\u2014a new paradigm in\nconnecting services and other components to LLMs via plugins and semantic layers.\nSK provides an excellent abstraction for converting any service to a GPT interface.\n Figure 5.8 shows a GPT interface constructed around an API service called The\nMovie Database (TMDB; www.themoviedb.org). The TMDB site provides a free API that\nexposes information about movies and TV shows.\nTo follow along with the exercises in this section, you must register for a free account\nfrom TMDB and create an API key. Instructions for getting an API key can be found at\nthe TMDB website (www.themoviedb.org) or by asking a GPT-4 turbo or a more\nrecent LLM.\nUser\nWeb Interface\nThe Movie Database\nwww.themoviedb.org\nAPI Interface\nGPT Interface\nSemantic Kernel\nChat Interface\nAgent Interface\nA user can access the site now in\nthree ways: web, chat, or agent.\nSK acts as an abstraction\nand tool to expose the\ninterface as a plugin.\nThis is the semantic\nmapping of functions\nto API endpoints.\nThis is the developer\nAPI endpoint exposed\nby the site.\nFigure 5.8\nThis layer architecture diagram shows the role of a GPT interface and the Semantic \nKernel being exposed to chat or agent interfaces.\n\n\n119\n5.5\nSemantic Kernel as an interactive service agent\n Over the next set of subsections, we\u2019ll create a GPT interface using an SK set of\nnative functions. Then, we\u2019ll use the SK kernel to test the interface and, later in this\nchapter, implement it as plugins into a chat function. In the next section, we look at\nbuilding a GPT interface against the TMDB API.\n5.5.1\nBuilding a semantic GPT interface\nTMDB is an excellent service, but it provides no semantic services or services that can\nbe plugged into ChatGPT or an agent. To do that, we must wrap the API calls that\nTMDB exposes in a semantic service layer.\n A semantic service layer is a GPT interface that exposes functions through natural\nlanguage. As discussed, to expose functions to ChatGPT or other interfaces such as\nagents, they must be defined as plugins. Fortunately, SK can create the plugins for us\nautomatically, given that we write our semantic service layer correctly.\n A native plugin or set of skills can act as a semantic layer. To create a native plugin,\ncreate a new plugin folder, and put a Python file holding a class containing the set of\nnative functions inside that folder. The SK extension currently doesn\u2019t do this well, so\nmanually creating the module works best.\n Figure 5.9 shows the structure of the new plugin called Movies and the semantic\nservice layer called tmdb.py. For native functions, the parent folder\u2019s name (Movies) is\nused in the import.\nOpen the tmdb.py file in VS Code, and look at the top of the file, as shown in listing\n5.18. This file contains a class called TMDbService, which exposes several functions\nthat map to API endpoint calls. The idea is to map the various relevant API function\ncalls in this semantic service layer. This will expose the functions as plugins for a chat\nor agent interface.\nfrom semantic_kernel.functions import kernel_funct\nimport requests\nimport inspect\ndef print_function_call():    \n    #omitted \u2026\nListing 5.18\ntmdb.py (top of file)\nParent skills folder\nName of the plugin folder\nFile/module containing the class\nand set of native functions\nFigure 5.9\nThe folder and file \nstructure of the TMDB plugin\nPrints the calls to \nthe functions for \ndebugging\n\n\n120\nCHAPTER 5\nEmpowering agents with actions\nclass TMDbService:    \n    def __init__(self):\n        # enter your TMDb API key here\n        self.api_key = \"your-TMDb-api-key\"\n    @kernel_function(    \n        description=\"Gets the movie genre ID for a given genre name\",\n        name=\"get_movie_genre_id\",\n        input_description=\"The movie genre name of the genre_id to get\",\n        )\n    def get_movie_genre_id(self, genre_name: str) -> str:    \n        print_function_call()\n        base_url = \"https://api.themoviedb.org/3\"\n        endpoint = f\"{base_url}/genre/movie/list\n                     \u27a5 ?api_key={self.api_key}&language=en-US\"\n        response = requests.get(endpoint)    \n        if response.status_code == 200:    \n            genres = response.json()['genres']\n            for genre in genres:\n                if genre_name.lower() in genre['name'].lower():\n                    return str(genre['id'])    \n        return None\nThe bulk of the code for the TMDbService and the functions to call the TMDB end-\npoints was written with the help of GPT-4 Turbo. Then, each function was wrapped\nwith the sk_function decorator to expose it semantically.\n A few of the TMDB API calls have been mapped semantically. Listing 5.19 shows\nanother example of a function exposed to the semantic service layer. This function\npulls a current top 10 list of movies playing for a particular genre.\n@kernel_function(    \n        description=\"\u201d\u201d\nGets a list of currently playing movies for a given genre\u201d\u201d\",\n        name=\"get_top_movies_by_genre\",\n        input_description=\"The genre of the movies to get\",\n        )\n    def get_top_movies_by_genre(self, genre: str) -> str:\n        print_function_call()\n        genre_id = self.get_movie_genre_id(genre)    \n        if genre_id:\n            base_url = \"https://api.themoviedb.org/3\n            playing_movies_endpoint = f\"{base_url}/movie/now_playing?\n\u27a5 api_key={self.api_key}&language=en-US\"\n            response = requests.get(\n                          playing_movies_endpoint)    \n            if response.status_code != 200:\n                return \"\"\nListing 5.19\ntmdb.py (get_top_movies_by_genre)\nTop-level service \nand decorator \nused to describe \nthe function (good \ndescriptions are \nimportant)\nFunction wrapped in \nsemantic wrapper; \nshould return str\nCalls the API endpoint, and, \nif good (code 200), checks \nfor matching genre\nFound the genre, \nreturns the id\nDecorates the function \nwith descriptions\nFinds the genre \nid for the given \ngenre name\nGets a list of currently \nplaying movies\n\n\n121\n5.5\nSemantic Kernel as an interactive service agent\n            playing_movies = response.json()['results'\n            for movie in playing_movies:    \n                movie['genre_ids'] = [str(genre_id)  \n                      \u27a5 for genre_id in movie['genre_ids']]\n            filtered_movies = [movie for movie \n\u27a5 in playing_movies if genre_id \n\u27a5 in movie['genre_ids']][:10]    \n            results = \", \".join([movie['title'] for movie in \nfiltered_movies])\n            return results\n        else:\n            return \"\"\nLook through the various other API calls mapped semantically. As you can see, there\nis a well-defined pattern for converting API calls to a semantic service. Before we run\nthe full service, we\u2019ll test each of the functions in the next section.\n5.5.2\nTesting semantic services\nIn a real-world application, you\u2019ll likely want to write a complete set of unit or integra-\ntion tests for each semantic service function. We won\u2019t do that here; instead, we\u2019ll\nwrite a quick helper script to test the various functions.\n Open test_tmdb_service.py in VS Code, and review the code, as shown in listing\n5.20. You can comment and uncomment any functions to test them in isolation. Be\nsure to have only one function uncommented at a time.\nimport semantic_kernel as sk\nfrom plugins.Movies.tmdb import TMDbService\nasync def main():\n    kernel = sk.Kernel()    \n    tmdb_service = kernel.import_plugin_from_object \n\u27a5 (TMDbService(), \"TMDBService\")    \n    print(\n        await tmdb_service[\"get_movie_genre_id\"](\n            kernel, sk.KernelArguments(\n                            genre_name=\"action\")    \n        )\n    )    \n    print(\n        await tmdb_service[\"get_tv_show_genre_id\"](\n            kernel, sk.KernelArguments(\n                            genre_name=\"action\")    \n        )\n    )    \n    print(\n        await tmdb_service[\"get_top_movies_by_genre\"](\n            kernel, sk.KernelArguments(\nListing 5.20\ntest_tmdb_service.py\nConverts \ngenre_ids \nto strings\nChecks to see \nif the genre id \nmatches movie \ngenres\nInstantiates \nthe kernel\nImports the \nplugin service\nInputs parameter \nto functions, \nwhen needed\nExecutes and \ntests the various \nfunctions\nInputs parameter \nto functions, \nwhen needed\nExecutes and \ntests the various \nfunctions\n\n\n122\nCHAPTER 5\nEmpowering agents with actions\n                            genre_name=\"action\")    \n        )\n    )    \n    print(\n        await tmdb_service[\"get_top_tv_shows_by_genre\"](\n            kernel, sk.KernelArguments(\n                            genre_name=\"action\")    \n        )\n    )\n    print(await tmdb_service[\"get_movie_genres\"](\nkernel, sk.KernelArguments()))                       \n    print(await tmdb_service[\"get_tv_show_genres\"](\nkernel, sk.KernelArguments()))                       \n# Run the main function\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())    \n###Output\nFunction name: get_top_tv_shows_by_genre    \nArguments:\n  self = <skills.Movies.tmdb.TMDbService object at 0x00000159F52090C0>\n  genre = action\nFunction name: get_tv_show_genre_id    \nArguments:\n  self = <skills.Movies.tmdb.TMDbService object at 0x00000159F52090C0>\n  genre_name = action\nArcane, One Piece, Rick and Morty, Avatar: The Last Airbender, Fullmetal \nAlchemist: Brotherhood, Demon Slayer: Kimetsu no Yaiba, Invincible, \nAttack on Titan, My Hero Academia, Fighting Spirit, The Owl House\nThe real power of SK is shown in this test. Notice how the TMDbService class is imported\nas a plugin, but we don\u2019t have to define any plugin configurations other than what we\nalready did? By just writing one class that wrapped a few API functions, we\u2019ve exposed\npart of the TMDB API semantically. Now, with the functions exposed, we can look at\nhow they can be used as plugins for a chat interface in the next section.\n5.5.3\nInteractive chat with the semantic service layer\nWith the TMDB functions exposed semantically, we can move on to integrating them\ninto a chat interface. This will allow us to converse naturally in this interface to get var-\nious information, such as current top movies.\n Open SK_service_chat.py in VS Code. Scroll down to the start of the new section\nof code that creates the functions, as shown in listing 5.21. The functions created here\nare now exposed as plugins, except we filter out the chat function, which we don\u2019t\nwant to expose as a plugin. The chat function here allows the user to converse directly\nwith the LLM and shouldn\u2019t be a plugin.\nInputs parameter \nto functions, \nwhen needed\nExecutes and tests the various functions\nExecutes and tests \nthe various functions\nExecutes main \nasynchronously\nCalls print\nfunction details\nto notify when\nthe function is\nbeing called\n",
      "page_number": 139
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 147-158)",
      "start_page": 147,
      "end_page": 158,
      "detection_method": "topic_boundary",
      "content": "123\n5.5\nSemantic Kernel as an interactive service agent\nsystem_message = \"You are a helpful AI assistant.\"\ntmdb_service = kernel.import_plugin_from_object(\nTMDbService(), \"TMDBService\")    \n# extracted section of code\nexecution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n        service_id=service_id,\n        ai_model_id=model_id,\n        max_tokens=2000,\n        temperature=0.7,\n        top_p=0.8,\n        tool_choice=\"auto\",\n        tools=get_tool_call_object(\n            kernel, {\"exclude_plugin\": [\"ChatBot\"]}),    \n    )\nprompt_config = sk.PromptTemplateConfig.from_completion_parameters(\n    max_tokens=2000,\n    temperature=0.7,\n    top_p=0.8,\n    function_call=\"auto\",\n    chat_system_prompt=system_message,\n)    \nprompt_template = OpenAIChatPromptTemplate(\n    \"{{$user_input}}\", kernel.prompt_template_engine, prompt_config\n)    \nhistory = ChatHistory()\nhistory.add_system_message(\"You recommend movies and TV Shows.\")\nhistory.add_user_message(\"Hi there, who are you?\")\nhistory.add_assistant_message(\n    \"I am Rudy, the recommender chat bot. I'm trying to figure out what \npeople need.\"\n)    \nchat_function = kernel.create_function_from_prompt(\n    prompt_template_config=prompt_template,\n    plugin_name=\"ChatBot\",\n    function_name=\"Chat\",\n)    \nNext, we can continue by scrolling in the same file to review the chat function, as\nshown in the following listing.\nasync def chat() -> bool:\n    try:\n        user_input = input(\"User:> \")    \n    except KeyboardInterrupt:\nListing 5.21\nSK_service_chat.py (function setup)\nListing 5.22\nSK_service_chat.py (chat function)\nImports the \nTMDbService \nas a plugin\nConfigures the \nexecution settings and \nadds filtered tools\nConfigures \nthe prompt \nconfiguration\nDefines the input \ntemplate and takes full \nstrings as user input\nAdds the chat history object \nand populates some history\nCreates the chat function\nInput is taken \ndirectly from the \nterminal/console.\n\n\n124\nCHAPTER 5\nEmpowering agents with actions\n        print(\"\\n\\nExiting chat...\")\n        return False\n    except EOFError:\n        print(\"\\n\\nExiting chat...\")\n        return False\n    if user_input == \"exit\":    \n        print(\"\\n\\nExiting chat...\")\n        return False\n    arguments = sk.KernelArguments(    \n        user_input=user_input,\n        history=(\"\\n\").join(\n           [f\"{msg.role}: {msg.content}\" for msg in history]),\n    )\n    result = await chat_completion_with_tool_call(    \n        kernel=kernel,\n        arguments=arguments,\n        chat_plugin_name=\"ChatBot\",\n        chat_function_name=\"Chat\",\n        chat_history=history,\n    )\n    print(f\"AI Agent:> {result}\")\n    return True\nLastly, scroll down to the bottom of the file, and review the primary function. This is\nthe code that calls the chat function in a loop.\nasync def main() -> None:\n    chatting = True\n    context = kernel.create_new_context()\n    print(\"Welcome to your first AI Agent\\    \n\\n  Type 'exit' to exit.\\\n\\n  Ask to get a list of currently playing movies by genre.\"\n    )\n    while chatting:    \n        chatting, context = await chat(context)    \nif __name__ == \"__main__\":\n    asyncio.ru n(main())\nRun the chat interface, run the file (F5), and then ask about movies or television\nshows of a particular genre. An example conversation session is shown in listing 5.24.\nThis output shows how a request to list movies from two genres made the chat inter-\nface make multiple calls to the get_top_movie_by_genre function.\nWelcome to your first AI Agent\n  Type 'exit' to exit.\nListing 5.23\nSK_service_chat.py (main function)\nListing 5.24\nSK_service_chat.py (example conversation) \nIf the user types \nexit, then exit \nthe chat.\nCreates arguments to \npass to the function\nUses the utility \nfunction to call \nthe function and \nexecute the tool\nIntroduction \nto the user\nContinues until \nchatting is False\nCalls the \nchat function \nasynchronously\n\n\n125\n5.6\nThinking semantically when writing semantic services\n  Ask to get a list of currently playing movies by genre.\nUser:> Input: can you give me list of the current top playing movies for \nthe action and comedy genres?\nFunction name: get_top_movies_by_genre    \nArguments:\n  genre = action\nFunction name: get_movie_genre_id    \nArguments:\n  genre_name = action\nFunction name: get_top_movies_by_genre    \nArguments:\n  genre = comedy\nFunction name: get_movie_genre_id    \nArguments:\n  genre_name = comedy\nAgent:> Here are the current top-playing movies \nfor the action and comedy genres:\n**Action:**    \n1. The Hunger Games: The Ballad of Songbirds & Snakes\n2. Rebel Moon - Part One: A Child of Fire\n3. Aquaman and the Lost Kingdom\n4. Silent Night\n5. The Family Plan\n6. Freelance\n7. Migration\n8. Sound of Freedom\n9. Godzilla Minus One\n**Comedy:**    \n1. The Family Plan\n2. Wonka\n3. Freelance\n4. Saltburn\n5. Chicken Run: Dawn of the Nugget\n6. Trolls Band Together\n7. There's Something in the Barn\n8. Migration\nPlease note that some movies may overlap in both genres, such as \n\"The Family Plan\" and \"Freelance\n.\"\nBe sure to explore the chat interface\u2019s boundaries and what you can ask for from the\nTMDB service. For example, try asking for a list of genres for movies or television\nshows. This service is a good first try, but we can perhaps do better, as we\u2019ll see in the\nnext section.\n5.6\nThinking semantically when writing semantic services\nNow we\u2019ve seen an excellent demonstration of converting an API into a semantic ser-\nvice interface. As it is, the functions return the titles of the top movies and television\nshows currently playing. However, by just returning the titles, we\u2019re limiting the ability\nof the LLM to parse the results on its own.\nLLM makes two calls to \nget_top_movies_by_genre.\nInternal\ncall to\nget the\ngenre id\nList of the top \ncurrent action \nmovies\nList of the top \ncurrent comedy \nmovies\n\n\n126\nCHAPTER 5\nEmpowering agents with actions\n Therefore, we\u2019ll create a v2 version of TMDbService to correct this and return the\nresults as JSON strings. Open the file tmdb_v2.py in VS Code, and scroll down to the\nget_top_movies_by_genre function.\ndef get_top_movies_by_genre(self, genre: str) -> str:\n        print_function_call()\n        genre_id = self.get_movie_genre_id(genre)\n        if genre_id:\n            #same code \u2026\n            return json.dumps(filtered_movies)    \n        else:\n            return \"\"\nNow open SK_service_chat.py in VS Code, and comment and uncomment the line\nshown in listing 5.26. This will then use version 2 of the TMDbService that outputs\nresults as full JSON documents in a single string.\n#from skills.Movies.tmdb import TMDbService    \nfrom skills.Movies.tmdb_v2 import TMDbService    \nRerun the SK_service_chat.py file in VS Code, and alter your query slightly, as\nshown by the output in the following listing.\nUser:> get a list of currently playing movies for the \naction genre and only return movies about space    \nAgent:> To find currently playing action movies that are specifically \nabout space, I will need to manually filter the provided list for those \nthat have space-related themes in their overview. Here's what fits that \ncriteria from the list:\n1. **Rebel Moon - Part One: A Child of Fire**    \n   - Release Date: 2023-12-15\n   - Overview: When a peaceful colony on the edge of the galaxy finds \nitself threatened by the armies of the tyrannical Regent Balisarius, \nthey dispatch Kora, a young woman with a mysterious past, to seek out \nwarriors from neighboring planets to help them take a stand.\nThis is the only movie from the provided list that clearly mentions a \nspace-related theme in its overview. \u2026\nListing 5.25\ntmdb_v2.py (get_top_movies_by_genre)\nListing 5.26\nSK_service_chat.py (modifying imports)\nListing 5.27\nSK_service_chat.py (TMDb_v2 service output)\nNow returns a \nfiltered list as \na JSON string\nComment out this line.\nUncomment this line to use \nversion 2 of the service.\nNew query asks to \ninclude an additional \nfilter for space\nThe LLM calls the service and then\nreviews the returned results that\nmatch the filter.\n\n\n127\n5.7\nExercises\nBecause the semantic service functions now return the complete movie listing in\nJSON, the LLM can apply additional filtering. This is the real power of semantic\nservices, allowing you to process the data through the LLM. We won\u2019t see this power\nby just returning a list of titles.\n This last exercise demonstrated the change in mentality you need to make when\nwriting semantic service layers. Generally, you\u2019ll typically want to return as much infor-\nmation as possible. Returning more information takes advantage of the LLM abilities\nto filter, sort, and transform data independently. In the next chapter, we\u2019ll explore\nbuilding autonomous agents using behavior trees.\n5.7\nExercises\nComplete the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Creating a Basic Plugin for Temperature Conversion \nObjective\u2014Familiarize yourself with creating a simple plugin for the OpenAI chat\ncompletions API. \nTasks:\n\u2013 Develop a plugin that converts temperatures between Celsius and Fahrenheit.\n\u2013 Test the plugin by integrating it into a simple OpenAI chat session where\nusers can ask for temperature conversions.\n\uf0a1Exercise 2\u2014Developing a Weather Information Plugin \nObjective\u2014Learn to create a plugin that performs a unique task. \nTasks:\n\u2013 Create a plugin for the OpenAI chat completions API that fetches weather\ninformation from a public API.\n\u2013 Ensure the plugin can handle user requests for current weather conditions\nin different cities.\n\uf0a1Exercise 3\u2014Crafting a Creative Semantic Function \nObjective\u2014Explore the creation of semantic functions. \nTasks:\n\u2013 Develop a semantic function that writes a poem or tells a children\u2019s story\nbased on user input.\n\u2013 Test the function in a chat session to ensure it generates creative and coher-\nent outputs.\n\uf0a1Exercise 4\u2014Enhancing Semantic Functions with Native Functions \nObjective\u2014Understand how to combine semantic and native functions. \nTasks:\n\u2013 Create a semantic function that uses a native function to enhance its capabilities.\n\u2013 For example, develop a semantic function that generates a meal plan and\nuses a native function to fetch nutritional information for the ingredients.\n\n\n128\nCHAPTER 5\nEmpowering agents with actions\n\uf0a1Exercise 5\u2014Wrapping an Existing Web API with Semantic Kernel \nObjective\u2014Learn to wrap existing web APIs as semantic service plugins. \nTasks:\n\u2013 Use SK to wrap a news API and expose it as a semantic service plugin in a\nchat agent.\n\u2013 Ensure the plugin can handle user requests for the latest news articles on\nvarious topics.\nSummary\n\uf0a1Agent actions extend the capabilities of an agent system, such as ChatGPT. This\nincludes the ability to add plugins to ChatGPT and LLMs to function as proxies\nfor actions.\n\uf0a1OpenAI supports function definitions and plugins within an OpenAI API ses-\nsion. This includes adding function definitions to LLM API calls and under-\nstanding how these functions allow the LLM to perform additional actions.\n\uf0a1The Semantic Kernel (SK) is an open source project from Microsoft that can be\nused to build AI applications and agent systems. This includes the role of seman-\ntic plugins in defining native and semantic functions.\n\uf0a1Semantic functions encapsulate the prompt/profile template used to engage\nan LLM.\n\uf0a1Native functions encapsulate code that performs or executes an action using an\nAPI or other interface.\n\uf0a1Semantic functions can be combined with other semantic or native functions\nand layered within one another as execution stages.\n\uf0a1SK can be used to create a GPT interface over the top of API calls in a semantic\nservice layer and expose them as chat or agent interface plugins.\n\uf0a1Semantic services represent the interaction between LLMs and plugins, as well as\nthe practical implementation of these concepts in creating efficient AI agents.\n\n\n129\nBuilding\nautonomous assistants\nNow that we\u2019ve covered how actions extend the power/capabilities of agents, we\ncan look at how behavior trees can guide agentic systems. We\u2019ll start by understand-\ning the basics of behavior trees and how they control robotics and AI in games. \n We\u2019ll return to agentic actions and examine how actions can be implemented\non the OpenAI Assistants platform using the GPT Assistants Playground project.\nFrom there, we\u2019ll look at how to build an autonomous agentic behavior tree (ABT)\nusing OpenAI assistants. Then, we\u2019ll move on to understanding the need for con-\ntrols and guardrails on autonomous agents and using control barrier functions. \nThis chapter covers\n\uf0a1Behavior trees for robotics and AI apps\n\uf0a1GPT Assistants Playground and creating \nassistants and actions\n\uf0a1Autonomous control of agentic behavior trees\n\uf0a1Simulating conversational multi-agent systems \nvia agentic behavior trees\n\uf0a1Using back chaining to create behavior trees \nfor complex systems \n\n\n130\nCHAPTER 6\nBuilding autonomous assistants\n In the final section of the chapter, we\u2019ll examine the use of the AgentOps platform\nto monitor our autonomous behavior-driven agentic systems. This will be an exciting\nchapter with several challenges. Let\u2019s begin by jumping into the next section, which\nintroduces behavior trees.\n6.1\nIntroducing behavior trees\nBehavior trees are a long-established pattern used to control robotics and AI in games.\nRodney A. Brooks first introduced the concept in his \u201cA Robust Layered Control Sys-\ntem for a Mobile Robot\u201d paper in 1986. This laid the groundwork for a pattern that\nexpanded on using the tree and node structure we have today.\n If you\u2019ve ever played a computer game with nonplayer characters (NPCs) or inter-\nacted with advanced robotic systems, you\u2019ve witnessed behavior trees at work. Figure 6.1\nshows a simple behavior tree. The tree represents all the primary nodes: selector or\nfallback nodes, sequence nodes, action nodes, and condition nodes.\nTable 6.1 describes the functions and purpose of the primary nodes we\u2019ll explore in\nthis book. There are other nodes and node types, and you can even create custom\nnodes, but for now, we\u2019ll focus on those in the table.\nTable 6.1\nThe primary nodes used in behavior trees\nNode\nPurpose\nFunction\nType\nSelector \n(fallback)\nThis node works by selecting the first \nchild that completes successfully. It\u2019s \noften called the fallback node \nbecause it will always fall back to the \nlast successful node that executed.\nThe node calls its children in \nsequence and stops executing \nwhen the first child succeeds. \nWhen a child node succeeds, it will \nreturn success; if no nodes suc-\nceed, it returns failure.\nComposite\n\u2192\n?\n?\n\u2192\n\u2192\nThe root node can be any\ncomposite node, such as\nselector or sequence.\nExecution \ufb02ows from\ntop to bottom and\nthen from left to right.\n1\n2\n3\n4\n5\n6\n7\nFigure 6.1\nA simple behavior tree of eating an apple or a pear\n\n\n131\n6.1\nIntroducing behavior trees\nThe primary nodes in table 6.1 can provide enough functionality to handle numerous\nuse cases. However, understanding behavior trees initially can be daunting. You won\u2019t\nappreciate their underlying complexity until you start using them. Before we build\nsome simple trees, we want to look at execution in more detail in the next section.\n6.1.1\nUnderstanding behavior tree execution\nUnderstanding how behavior trees execute is crucial to designing and implementing\nbehavior trees. Unlike most concepts in computer science, behavior trees operate in\nterms of success and failure. When a node in a behavior tree executes, it will return\neither success or failure; this even applies to conditions and selector nodes.\n Behavior trees execute from top to bottom and left to right. Figure 6.2 shows\nthe process and what happens if a node fails or succeeds. In the example, the AI\nthe tree controls has an apple but no pear. In the first sequence node, a condition\nchecks if the AI has an apple. Because the AI doesn\u2019t have an apple, it aborts the\nsequence and falls back to the selector. The selector then selects its next child\nnode, another sequence, that checks if the AI has a pear, and because it does, the\nAI eats the apple.\nSequence\nThis node executes all of its children \nin sequence until one node fails or \nthey all complete successfully.\nThe node calls each of its children \nin sequence regardless of whether \nthey fail or succeed. If all children \nsucceed, it returns success, and \nfailure if just one child fails.\nComposite\nCondition\nBehavior trees don\u2019t use Boolean \nlogic but rather success or failure \nas a means of control. The condition \nreturns success if the condition is \ntrue and false otherwise.\nThe node returns success or failure \nbased on a condition.\nTask\nAction\nThis is where the action happens. \nThe node executes and returns suc-\ncess if successful or returns failure \notherwise.\nTask\nDecorator\nThey work by controlling the execu-\ntion of child nodes. They are often \nreferred to as conditionals because \nthey can determine whether a node is \nworth executing or safe to execute. \nThe node controls execution of \nthe child nodes. Decorators can \noperate as control barrier functions \nto block or prevent unwanted \nbehaviors.\nDecorator\nParallel\nThis node executes all of its nodes in \nparallel. Success or failure is con-\ntrolled by a threshold of the number \nof children needed to succeed to \nreturn success.\nThe node executes all of its child \nnodes in sequence regardless of \nthe status of the nodes.\nComposite\nTable 6.1\nThe primary nodes used in behavior trees (continued)\nNode\nPurpose\nFunction\nType\n\n\n132\nCHAPTER 6\nBuilding autonomous assistants\nBehavior trees provide control over how an AI system will execute at a macro or micro\nlevel. Regarding robotics, behavior trees will typically be designed to operate at the\nmicro level, where each action or condition is a small event, such as detecting the\napple. Conversely, behavior trees can also control more macro systems, such as NPCs\nin games, where each action may be a combination of events, like attacking the player.\n For agentic systems, behavior trees support controlling an agent or assistant at\nyour chosen level. We\u2019ll explore controlling agents at the task and, in later chapters,\nthe planning levels. After all, with the power of LLMs, agents can construct their own\nbehavior tree.\n Of course, several other forms of AI control could be used to control agentic sys-\ntems. The next section will examine those different systems and compare them to\nbehavior trees.\n6.1.2\nDeciding on behavior trees\nNumerous other AI control systems have benefits and are worth exploring in con-\ntrolling agentic systems. They can demonstrate the benefits of behavior trees and pro-\nvide other options for specific use cases. The behavior tree is an excellent pattern, but\nit isn\u2019t the only one, and it\u2019s worth learning about others.\n Table 6.2 highlights several other systems we may consider for controlling AI sys-\ntems. Each item in the table describes what the method does, its shortcomings, and its\npossible application to agentic AI control.\n \nThe root node\nexecutes according\nto its composite type.\nSequence nodes execute\nall child nodes in order; if\na node fails, the sequence\nfails.\nIf the previous node in\na sequence fails, the\nsequence is aborted.\nIn this example, the AI has the\npear, which returns success\nand then eats the pear.\nNode success/failure\n\ufb02ows back to the\nparent node.\nSelector nodes execute\nall children and return\nsuccess on the \ufb01rst child\nthat returns success.\n?\n\u2192\n\u2192\n1\n2\n3\n4\n6\n7\nFigure 6.2\nThe execution process of a simple behavior tree\n\n\n133\n6.1\nIntroducing behavior trees\nTable 6.2\nComparison of other AI control systems\nControl name\nDescription\nShortcomings \nControl agentic AI?\nFinite state \nmachinea (FSM)\nFSMs model AI using a set \nof states and transitions \ntriggered by events or \nconditions.\nFSMs can become \nunwieldy with increasing \ncomplexity.\nFSMs aren\u2019t practical for \nagents because they don\u2019t \nscale well.\nDecision treeb\nDecision trees use a \ntree-like model of deci-\nsions and their possible \nconsequences.\nDecision trees can suffer \nfrom overfitting and lack \ngeneralization in complex \nscenarios.\nDecision trees can be \nadapted and enhanced \nwith behavior trees.\nUtility-based \nsystemb\nUtility functions evaluate \nand select the best action \nbased on the current \nsituation.\nThese systems require \ncareful design of utility \nfunctions to balance \npriorities.\nThis pattern can be \nadopted within a behavior \ntree.\nRule-based \nsystema\nThis set of if-then rules \ndefine the behavior of \nthe AI.\nThese systems can \nbecome cumbersome with \nmany rules, leading to \npotential conflicts.\nThese aren\u2019t very practical \nwhen paired with agentic \nsystems powered by \nLLMs.\nPlanning sys-\ntemc\nPlanning systems gener-\nate a sequence of actions \nto achieve a specific goal \nusing planning algorithms.\nThese systems are com-\nputationally expensive and \nrequire significant domain \nknowledge.\nAgents can already imple-\nment such patterns on \ntheir own as we\u2019ll see in \nlater chapters.\nBehavioral \ncloningc\nBehavioral cloning \nrefers to learning policies \nby mimicking expert \ndemonstrations.\nThis system may struggle \nwith generalization to \nunseen situations.\nThis can be incorporated \ninto behavior trees or into \na specific task.\nHierarchical \nTask Network \n(HTN)d\nHTNs decompose tasks \ninto smaller, manageable \nsubtasks arranged in a \nhierarchy.\nThese are complex to \nmanage and design for \nvery large tasks.\nHTNs allow for better orga-\nnization and execution of \ncomplex tasks. This pat-\ntern can be used for larger \nagentic systems.\nBlackboard \nsystemb\nThese systems feature \ncollaborative problem-solv-\ning using a shared black-\nboard for different \nsubsystems.\nThese systems are diffi-\ncult to implement and \nmanage communication \nbetween subsystems.\nAgentic systems can \nimplement similar pat-\nterns using conversation \nor group chats/threads.\nGenetic \nalgorithm (GA)d\nThese optimization tech-\nniques are inspired by \nnatural selection to \nevolve solutions to \nsolve problems.\nGAs are computationally \nintensive and may not \nalways find the optimal \nsolution.\nGAs have potential and \ncould even be used to \noptimize behavior trees.\na Not practical when considering complex agentic systems\nb Exists in behavior trees or can easily be incorporated\nc Typically applied at the task or action/condition level\nd Advanced systems that would require heavy lifting when applied to agents\n\n\n134\nCHAPTER 6\nBuilding autonomous assistants\nIn later chapters of this book, we\u2019ll investigate some of the patterns discussed in\ntable 6.2. Overall, several patterns can be enhanced or incorporated using behavior\ntrees as the base. While other patterns, such as FSMs, may be helpful for small experi-\nments, they lack the scalability of behavior trees.\n Behavior trees can provide several benefits as an AI control system, including scal-\nability. The following list highlights other notable benefits of using behavior trees:\n\uf0a1Modularity and reusability\u2014Behavior trees promote a modular approach to design-\ning behaviors, allowing developers to create reusable components. Nodes in a\nbehavior tree can be easily reused across different parts of the tree or even in\ndifferent projects, enhancing maintainability and reducing development time.\n\uf0a1Scalability\u2014As systems grow in complexity, behavior trees handle the addition\nof new behaviors more gracefully than other methods, such as FSMs. Behavior\ntrees allow for the hierarchical organization of tasks, making it easier to manage\nand understand large behavior sets.\n\uf0a1Flexibility and extensibility\u2014Behavior trees offer a flexible framework where new\nnodes (actions, conditions, decorators) can be added without drastically alter-\ning the existing structure. This extensibility makes it straightforward to intro-\nduce new behaviors or modify existing ones to adapt to new requirements.\n\uf0a1Debugging and visualization\u2014Behavior trees provide a clear and intuitive visual\nrepresentation of behaviors, which is beneficial for debugging and understand-\ning the decision-making process. Tools that support behavior trees often\ninclude graphical editors that allow developers to visualize and debug the tree\nstructure, making it easier to identify and fix problems.\n\uf0a1Decoupling of decision logic\u2014Behavior trees separate the decision-making and\nexecution logic, promoting a clear distinction between high-level strategy and\nlow-level actions. This decoupling simplifies the design and allows for more\nstraightforward modifications and testing of specific behavior parts without\naffecting the entire system.\nHaving made a strong case for behavior trees, we should now consider how to imple-\nment them in code. In the next section, we look at how to build a simple behavior\ntree, using Python code.\n6.1.3\nRunning behavior trees with Python and py_trees\nBecause behavior trees have been around for so long and have been incorporated into\nmany technologies, creating a sample demonstration is very simple. Of course, the easi-\nest way is to ask ChatGPT or your favorite AI chat tool. Listing 6.1 shows the result of\nusing a prompt to generate the code sample and submitting figure 6.1 as the example\ntree. The final code had to be corrected for simple naming and parameter errors.\nNOTE\nAll the code for this chapter can be found by downloading the GPT\nAssistants Playground project at https://mng.bz/Ea0q.\n",
      "page_number": 147
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 159-166)",
      "start_page": 159,
      "end_page": 166,
      "detection_method": "topic_boundary",
      "content": "135\n6.1\nIntroducing behavior trees\nimport py_trees\nclass HasApple(py_trees.behaviour.Behaviour):     \n    def __init__(self, name):\n        super(HasApple, self).__init__(name)\n    def update(self):        \n        if True:  \n            return py_trees.common.Status.SUCCESS\n        else:\n            return py_trees.common.Status.FAILURE\n# Other classes omitted\u2026\nhas_apple = HasApple(name=\"Has apple\")     \neat_apple = EatApple(name=\"Eat apple\")     \nsequence_1 = py_trees.composites.Sequence(name=\"Sequence 1\", memory=True)\nsequence_1.add_children([has_apple, eat_apple])                             \nhas_pear = HasPear(name=\"Has pear\")        \neat_pear = EatPear(name=\"Eat pear\")        \nsequence_2 = py_trees.composites.Sequence(name=\"Sequence 2\", memory=True)\nsequence_2.add_children([has_pear, eat_pear])                               \nroot = py_trees.composites.Selector(name=\"Selector\", memory=True)\nroot.add_children([sequence_1, sequence_2])                                 \nbehavior_tree = py_trees.trees.BehaviourTree(root)    \npy_trees.logging.level = py_trees.logging.Level.DEBUG   \nfor i in range(1, 4):                                                     \n    print(\"\\n------------------ Tick {0} ------------------\".format(i))\n    behavior_tree.tick()                                                  \n### Start of output\n------------------ Tick 1 ------------------\n[DEBUG] Selector             : Selector.tick()\n[DEBUG] Selector             : Selector.tick() [!RUNNING->reset current_child]\n[DEBUG] Sequence 1           : Sequence.tick()\n[DEBUG] Has apple            : HasApple.tick()\n[DEBUG] Has apple            : HasApple.stop(Status.INVALID->Status.SUCCESS)\n[DEBUG] Eat apple            : EatApple.tick()\nEating apple\n[DEBUG] Eat apple            : EatApple.stop(Status.INVALID->Status.SUCCESS)\n[DEBUG] Sequence 1           : Sequence.stop()[Status.INVALID->Status.SUCCESS]\nThe code in listing 6.1 represents the behavior tree in figure 6.1. You can run this\ncode as is or alter what the conditions return and then run the tree again. You can\nalso change the behavior tree by removing one of the sequence nodes from the root\nselector.\n Now that we have a basic understanding of behavior trees, we can move on to\nworking with agents/assistants. Before doing that, we\u2019ll look at a tool to help us work\nListing 6.1\nfirst_btree.py\nCreates a class to implement \nan action or condition\nCreates the action \nand condition nodes\nAdds the nodes to\ntheir respective\nparents\nCreates the action \nand condition nodes\nCreates the whole \nbehavior tree\nExecutes one step/tick\non the behavior tree\n\n\n136\nCHAPTER 6\nBuilding autonomous assistants\nwith OpenAI Assistants. This tool will help us wrap our first ABTs around OpenAI\nAssistants.\n6.2\nExploring the GPT Assistants Playground\nFor the development of this book, several GitHub projects were created to address var-\nious aspects of building agents and assistants. One such project, the GPT Assistants\nPlayground, is built using Gradio for the interface that mimics the OpenAI Assistants\nPlayground but with several extras added.\n The Playground project was developed as both a teaching and demonstration aid.\nInside the project, the Python code uses the OpenAI Assistants API to create a chat inter-\nface and an agentic system to build and power assistants. There is also a comprehensive\ncollection of actions assistants you can use, and you can easily add your own actions.\n6.2.1\nInstalling and running the Playground\nThe following listing shows installing and running the Playground project from the\nterminal. There is currently no PyPI package to install. \n# change to a working folder and create a new Python virtual environment\ngit clone \n\u27a5 https://github.com/cxbxmxcx/GPTAssistantsPlayground    \ncd GPTAssistantsPlayground     \npip install -r requirements.txt     \nYou can run the application from the terminal or using Visual Studio Code (VS\nCode), with the latter giving you more control. Before running the application, you\nneed to set your OpenAI API key through the command line or by creating an .env\nfile, as we\u2019ve done a few times already. Listing 6.3 shows an example of setting the\nenvironment variable on Linux/Mac or the Git Bash shell (Windows recommended)\nand running the application.\nexport OPENAI_API_KEY=\"your-api-key\"     \npython main.py    \nOpen your browser to the URL displayed (typically http://127.0.0.1:7860) or what\nis mentioned in the terminal. You\u2019ll see an interface similar to that shown in figure 6.3.\nListing 6.2\nInstalling the GPT Assistants Playground\nListing 6.3\nRunning the GPT Assistants Playground\nPulls the \nsource code \nfrom GitHub\nChanges directory to the \nproject source code folder\nInstalls the\nrequirements\nSets your API key as an \nenvironment variable\nRuns the app from the \nterminal or via VS Code\n\n\n137\n6.2\nExploring the GPT Assistants Playground\nIf you\u2019ve already defined the OpenAI Assistants, you\u2019ll see them in the Select Assistant\ndropdown.\nIf you\u2019ve never defined an assistant, you can create one and choose the various\noptions and instructions you need. If you\u2019ve visited the OpenAI Playground, you\u2019ve\nalready experienced a similar interface. \nGPT vs. an assistant\nOpenAI defines a GPT as the assistant you can run and use within the ChatGPT inter-\nface. An assistant can only be consumed through the API and requires custom code\nin most cases. When you run an assistant, you\u2019re charged according to the model\ntoken usage and any special tools, including the Code Interpreter and files, whereas\na GPT runs within ChatGPT and is covered by account costs.\nSelect an existing Assistant\nor create a new assistant.\nSelect from any\navailable models.\nSelect the Tools\nand Actions.\nThe assistant\ncan output \ufb01les\ncreated by the\nCode Interpreter.\nFigure 6.3\nThe GPT Assistants Playground interface being used to learn math\n\n\n138\nCHAPTER 6\nBuilding autonomous assistants\nEach of these features is covered in more detail over the next few sections. We\u2019ll start\nwith a look at using and consuming actions in the next section.\n6.2.2\nUsing and building custom actions\nActions and tools are the building blocks that empower agents and assistants. Without\naccess to tools, agents are functionless chatbots. The OpenAI platform is a leader in\nestablishing many of the patterns for tools, as we saw in chapter 3.\n The Playground provides several custom actions that can be attached to assistants\nthrough the interface. In this next exercise, we\u2019ll build a simple assistant and attach a\ncouple of custom actions to see what is possible.\n Figure 6.4 shows the expanded Actions accordion, which displays many available\ncustom actions. Run the Playground from the terminal or debugger, and create a new\nassistant. Then, select the actions shown in the figure. After you\u2019re done selecting the\nactions, scroll to the bottom, and click Add Assistant to add the assistant. Assistants\nneed to be created before they can be used.\n After you create the assistant, you can ask it to list all available assistants. Listing the\nassistants also gives you the IDs required to call the assistant. You can also call other\nassistants and ask them to complete tasks in their area of specialization.\n Adding your custom actions is as simple as adding code to a file and dropping it in\nthe right folder. Open the playground/assistant_actions folder from the main\nproject folder, and you\u2019ll see several files that define the various actions. Open the\nfile_actions.py file in VS Code, as shown in listing 6.4.\n \n \n \n(continued)\nThe reason for creating a local version of the Playground was an exercise to demon-\nstrate the code structure but also provide additional features listed here:\n\uf0a1Actions (custom actions)\u2014Creating your own actions allows you to add any\nfunctionality you want to an assistant. As we\u2019ll see, the Playground makes it\nvery easy to create your own actions quickly.\n\uf0a1Code runner\u2014The API does come with a Code Interpreter, but it\u2019s relatively\nexpensive ($.03 per run), doesn\u2019t allow you to install your modules, can\u2019t run\ncode interactively, and runs slowly. The Playground will enable you to run\nPython code locally in an isolated virtual environment. While not as secure as\npushing code out to Docker images, it does execute code windowed and out\nof process better than other platforms.\n\uf0a1Transparency and logging\u2014The Playground provides for comprehensive cap-\nturing of logs and will even show how the assistant uses internal and external\ntools/actions. This can be an excellent way to see what the assistant is doing\nbehind the scenes.\n\n\n139\n6.2\nExploring the GPT Assistants Playground\nimport os\nfrom playground.actions_manager import agent_action\nOUTPUT_FOLDER = \"assistant_outputs\"\n@agent_action    \ndef save_file(filename, content):     \n    \"\"\"\n    Save content to a file.     \n    :param filename: The name of the file including extension.\n    :param content: The content to save in the file.\n    \"\"\"\n    file_path = os.path.join(OUTPUT_FOLDER, filename)\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(content)\n    print(f\"File '{filename}' saved successfully.\")     \nListing 6.4\nplayground/assistant_actions/file_actions.py\nName your assistant\na memorable name.\nAsk to list the assistants, and you\u2019ll\nsee all the assistants you've created.\nSelect call_assistant\nand list_assistants.\nThe call_assistant action allows\nthe assistant to delegate work\nto other assistants.\nYou don\u2019t need any\nspecial instructions.\nDisable or enable the Code\nInterpreter to see the effect.\nFigure 6.4\nSelecting and using custom actions in the interface\nThis decorator \nautomatically adds the \nfunction as an action.\nGive your functions \nclear names that align \nwith the purpose.\nThe description is what \nthe assistant uses to \ndetermine the function, \nso document it well.\nGenerally returns \na message stating \nsuccess or failure\n\n\n140\nCHAPTER 6\nBuilding autonomous assistants\nYou can add any custom action you want by placing the file in the assistant_actions\nfolder and decorating it with the agent_action decorator. Just make sure to give the\nfunction a good name and enter quality documentation for how the function should\nbe used. When the Playground starts up, it loads all the actions in the folder that are\ndecorated correctly and have descriptions/documentation.\n It\u2019s that simple. You can add several custom actions as needed. In the next section,\nwe\u2019ll look at a special custom action that allows the assistant to run code locally.\n6.2.3\nInstalling the assistants database\nTo run several of the examples in this chapter, you\u2019ll need to install the assistants data-\nbase. Fortunately, this can be easily done through the interface and just by asking\nagents. The upcoming instructions detail the process for installing the assistants and\nare taken directly from the GPT Assistants Playground README. You can install sev-\neral of the demo assistants located in the assistants.db SQLite database:\n1\nCreate a new assistant, or use an existing assistant.\n2\nGive the assistant the create_manager_assistant action (found under the\nActions section).\n3\nAsk the assistant to create the manager assistant (i.e., \u201cplease create the man-\nager assistant\u201d), and be sure to name the assistant \u201cManager Assistant.\u201d\n4\nRefresh your browser to reload the assistants selector.\n5\nSelect the new Manager Assistant. This assistant has the instructions and actions\nthat will allow it to install assistants from the assistants.db database.\n6\nTalk to the Manager Assistant to give you a list of assistants to install, or just ask\nthe Manager Assistant to install all available assistants. \n6.2.4\nGetting an assistant to run code locally\nGetting agents and assistants to generate and run executable code has a lot of power.\nUnlike the Code Interpreter, running code locally provides numerous opportunities\nto iterate and tune quickly. We saw this earlier with AutoGen, where the agents could\nkeep running the code until it worked as expected.\n In the Playground, it\u2019s a simple matter to select the custom action run_code, as\nshown in figure 6.5. You\u2019ll also want to choose the run_shell_command action because\nit allows the assistant to pip install any required modules.\n \n \n \n \n \n \n \n \n\n\n141\n6.2\nExploring the GPT Assistants Playground\nYou can now ask an assistant to generate and run the code to be sure it works on your\nbehalf. Try this out by adding the custom actions and asking the assistant to generate\nand run code, as shown in figure 6.6. If the code doesn\u2019t work as expected, tell the\nassistant what problems you encountered.\n Again, the Python code running in the Playground creates a new virtual environ-\nment in a project subfolder. This system works well if you\u2019re not running any operat-\ning system\u2013level code or low-level code. If you need something more robust, a good\noption is AutoGen, which uses Docker containers to run isolated code.\n Adding actions to run code or other tasks can make assistants feel like a black box.\nFortunately, the OpenAI Assistants API allows you to consume events and see what the\nassistant is doing behind the scenes. In the next section, we\u2019ll see what this looks like.\nDo not select the\nCode Interpreter tool.\nSelect both the run_code and\nrun_shell_command custom\nactions. Running commands on\nthe shell allows an assistant to\ninstall new packages as required.\nFigure 6.5\nSelecting custom actions for the assistant to run Python code\n\n\n142\nCHAPTER 6\nBuilding autonomous assistants\n6.2.5\nInvestigating the assistant process through logs\nOpenAI added a feature into the Assistants API that allows you to listen to events and\nactions chained through tool/action use. This feature has been integrated into the\nPlayground, capturing action and tool use when an assistant calls another assistant. \n We can try this by asking an assistant to use a tool and then open the log. A great\nexample of how you can do this is by giving an assistant the Code Interpreter tool and\nthen asking it to plot an equation. Figure 6.7 shows an example of this exercise.\n Usually, when the Assistant Code Interpreter tool is enabled, you don\u2019t see any\ncode generation or execution. This feature allows you to see all tools and actions used\nby the assistant as they happen. Not only is it an excellent tool for diagnostics, but it\nalso provides additional insights into the functions of LLMs.\n We haven\u2019t reviewed the code to do all this because it\u2019s extensive and will likely\nundergo several changes. That being said, if you plan on working with the Assistants\nAPI, this project is a good place to start. With the Playground introduced, we can con-\ntinue our journey into ABTs in the next section.\nAny assistant can generate code. Adding\nsome helpful instructions and personality\ncan better align the output.\nThe \u201csnake\u201d game will open a new window\ndemonstrating the code is running.\nNote: While the window is open, it will\nblock the Gradio interface.\nIn this example, the assistant generated\nthe code for the game and then realized\nit need to install Pygame. After installing,\nit ran the code, as shown in the side window.\nFigure 6.6\nGetting the assistant to generate and run Python code\n",
      "page_number": 159
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 167-176)",
      "start_page": 167,
      "end_page": 176,
      "detection_method": "topic_boundary",
      "content": "143\n6.3\nIntroducing agentic behavior trees\n6.3\nIntroducing agentic behavior trees\nAgentic behavior trees (ABTs) implement behavior trees on assistant and agent sys-\ntems. The key difference between regular behavior trees and ABTs is that they use\nprompts to direct actions and conditions. Because prompts may return a high occur-\nrence of random results, we could also name these trees stochastic behavior trees,\nwhich do exist. For simplicity, we\u2019ll differentiate behavior trees used to control agents,\nreferring to them as agentic.\n Next, we\u2019ll undertake an exercise to create an ABT. The finished tree will be writ-\nten in Python but will require the setup and configuration of various assistants. We\u2019ll\ncover how to manage assistants using the assistants themselves.\n6.3.1\nManaging assistants with assistants\nFortunately, the Playground can help us quickly manage and create the assistants.\nWe\u2019ll first install the Manager Assistant, followed by installing the predefined assis-\ntants. let\u2019s get started with installing the Manager Assistant using the following steps:\nThe Logs tab shows\nwhere \ufb01les are saved.\nCode being generated and run in the Code\nInterpreter is shown in the Logs tab.\nPlotting a graph\nis a good test\nFigure 6.7\nInternal assistant logs being captured\n\n\n144\nCHAPTER 6\nBuilding autonomous assistants\n1\nOpen Playground in your browser, and create a new simple assistant or use an\nexisting assistant. If you need a new assistant, create it and then select it.\n2\nWith the assistant selected, open the Actions accordion, and select the create_\nmanager_assistant action. You don\u2019t need to save; the interface will update\nthe assistant automatically.\n3\nNow, in the chat interface, prompt the assistant with the following: \u201cPlease cre-\nate the manager assistant.\u201d\n4\nAfter a few seconds, the assistant will say it\u2019s done. Refresh your browser, and\nconfirm that the Manager Assistant is now available. If, for some reason, the\nnew assistant isn\u2019t shown, try restarting the Gradio app itself.\nThe Manager Assistant is like an admin that has access to everything. When engag-\ning the Manager Assistant, be sure to be specific about your requests. With the Man-\nager Assistant active, you can now install new assistants used in the book using the\nfollowing steps:\n1\nSelect the Manager Assistant. If you\u2019ve modified the Manager Assistant, you can\ndelete it and reinstall it anytime. Although it\u2019s possible to have multiple Man-\nager Assistants, it\u2019s not recommended.\n2\nAsk the Manager Assistant what assistants can be installed by typing the follow-\ning in the chat interface:\nPlease list all the installable assistants.\n3\nIdentify which assistant you want installed when you ask the Manager Assistant\nto install it:\nPlease install the Python Coding Assistant.\nYou can manage and install any available assistants using the Playground. You can also\nask the Manager Assistant to save the definitions of all your assistants as JSON:\nPlease save all the assistants as JSON to a file called assistants.json.\nThe Manager Assistant can access all actions, which should be considered unique and\nused sparingly. When crafting assistants, it\u2019s best to keep them goal specific and limit\nthe actions to just what they need. This not only avoids giving the AI too many deci-\nsions but also avoids accidents or mistakes caused by hallucinations.\n As we go through the remaining exercises in this chapter, you\u2019ll likely need to\ninstall the required assistants. Alternatively, you can ask the Manager Assistant to\ninstall all available assistants. Either way, we look at creating an ABT with assistants in\nthe next section.\n\n\n145\n6.3\nIntroducing agentic behavior trees\n6.3.2\nBuilding a coding challenge ABT\nCoding challenges provide a good baseline for testing and evaluating agent and assis-\ntant systems. Challenges and benchmarks can quantify how well an agent or agentic\nsystem operates. We already applied coding challenges to multi-platform agents in\nchapter 4 with AutoGen and CrewAI.\n For this coding challenge, we\u2019re going a little further and looking at Python cod-\ning challenges from the Edabit site (https://edabit.com), which range in complexity\nfrom beginner to expert. We\u2019ll stick with the expert code challenges because GPT-4o\nand other models are excellent coders. Look at the challenge in the next listing, and\nthink about how you would solve it.\nPlant the Grass by AniXDownLoe\n    You will be given a matrix representing a field g \nand two numbers x, y coordinate.\n    There are three types of possible characters in the matrix:\n        x representing a rock.\n        o representing a dirt space.\n        + representing a grassed space.\n    You have to simulate grass growing from the position (x, y). \n    Grass can grow in all four directions (up, left, right, down). \n    Grass can only grow on dirt spaces and can't go past rocks.\n    Return the simulated matrix.\n    Examples\n    simulate_grass([\n    \"xxxxxxx\",\n    \"xooooox\",\n    \"xxxxoox\"\n    \"xoooxxx\"\n    \"xxxxxxx\"\n    ], 1, 1) \u2192 [\n    \"xxxxxxx\",\n    \"x+++++x\",\n    \"xxxx++x\"\n    \"xoooxxx\"\n    \"xxxxxxx\"\n    ]\n    Notes\n    There will always be rocks on the perimeter\nYou can use any challenge or coding exercise you want, but here are a few things to\nconsider:\nListing 6.5\nEdabit challenge: Plant the Grass\n\n\n146\nCHAPTER 6\nBuilding autonomous assistants\n\uf0a1The challenge should be testable with quantifiable assertions (pass/fail).\n\uf0a1Avoid opening windows when asking for a game, building a website, or using\nanother interface. At some point, testing full interfaces will be possible, but for\nnow, it\u2019s just text output.\n\uf0a1Avoid long-running challenges, at least initially. Start by keeping the challenges\nconcise and short lived. \nAlong with any challenge, you\u2019ll also want a set of tests or assertions to confirm the\nsolution works. On Edabit, a challenge typically provides a comprehensive set of tests.\nThe following listing shows the additional tests provided with the challenge.\nTest.assert_equals(simulate_grass(\n[\"xxxxxxx\",\"xooooox\",\"xxxxoox\",\"xoooxxx\",\"xxxxxxx\"],\n 1, 1), \n[\"xxxxxxx\",\"x+++++x\",\"xxxx++x\",\"xoooxxx\",\"xxxxxxx\"])\n    Test.assert_equals(simulate_grass(\n[\"xxxxxxx\",\"xoxooox\",\"xxoooox\",\"xooxxxx\",\n\"xoxooox\",\"xoxooox\",\"xxxxxxx\"],\n 2, 3), [\"xxxxxxx\",\"xox+++x\",\"xx++++x\",\"x++xxxx\",\n\"x+xooox\",\"x+xooox\",\"xxxxxxx\"])\n    Test.assert_equals(simulate_grass(\n[\"xxxxxx\",\"xoxoox\",\"xxooox\",\"xoooox\",\"xoooox\",\"xxxxxx\"], \n1, 1), \n[\"xxxxxx\",\"x+xoox\",\"xxooox\",\"xoooox\",\"xoooox\",\"xxxxxx\"])\n    Test.assert_equals(simulate_grass(\n[\"xxxxx\",\"xooox\",\"xooox\",\"xooox\",\"xxxxx\"], \n1, 1),\n[\"xxxxx\",\"x+++x\",\"x+++x\",\"x+++x\",\"xxxxx\"])\n    Test.assert_equals(simulate_grass(\n[\"xxxxxx\",\"xxxxox\",\"xxooox\",\"xoooxx\",\"xooxxx\",\n\"xooxxx\",\"xxooox\",\"xxxoxx\",\"xxxxxx\"], \n4, 1),\n[\"xxxxxx\",\"xxxx+x\",\"xx+++x\",\"x+++xx\",\"x++xxx\",\n\"x++xxx\",\"xx+++x\",\"xxx+xx\",\"xxxxxx\"])\n    Test.assert_equals(simulate_grass(\n[\"xxxxxxxxxxx\", \"xoxooooooox\", \"xoxoxxxxxox\", \n\"xoxoxoooxox\", \"xoxoxoxoxox\", \"xoxoxoxoxox\", \n\"xoxoxxxoxox\", \"xoxoooooxox\", \"xoxxxxxxxox\", \n\"xooooooooox\", \"xxxxxxxxxxx\"], 1, 1), \n[\"xxxxxxxxxxx\", \"x+x+++++++x\", \"x+x+xxxxx+x\", \n\"x+x+x+++x+x\", \"x+x+x+x+x+x\", \"x+x+x+x+x+x\", \n\"x+x+xxx+x+x\", \"x+x+++++x+x\", \"x+xxxxxxx+x\", \n\"x+++++++++x\", \"xxxxxxxxxxx\"])   \nThe tests will be run as part of a two-step verification to confirm that the solution\nworks. We\u2019ll also use the tests and challenges as written, which will further test the AI. \n Figure 6.8 shows the makeup of a straightforward behavior tree that will be used to\nsolve various programming challenges. You\u2019ll notice that this ABT uses a different\nListing 6.6\nPlant the Grass tests\n\n\n147\n6.3\nIntroducing agentic behavior trees\nassistant for the actions and conditions. For the first step, the Python coding assistant\n(called the Hacker) generates a solution that is then reviewed by the coding challenge\nJudge (called the Judge), which produces a refined solution that is verified by a differ-\nent Python coding assistant (called the Verifier).\nFigure 6.8 also shows how each agent converses on which thread. Assistants use mes-\nsage threads, similar to a Slack or Discord channel, where all assistants conversing on\na thread will see all messages. For this ABT, we keep one main conversation thread for\nthe Hacker and Judge to share messages, while the Verifier works on a separate mes-\nsage thread. Keeping the Verifier on its own thread isolates it from the noise of the\nsolution-solving efforts.\n Now, building the ABT in code is a matter of combining the py_trees package and\nthe Playground API functions. Listing 6.7 shows an excerpt of code that creates each\nof the action/condition nodes with the assistants and gives them the instructions.\n \n \n \n \n\u2192\nVerify solution\nHacking solution\nThe root node is a sequence.\nJudge solution\nThe initial solution will\nbe generated by the\nPython oding ssistant,\nc\na\nwhich will save output to\nsolution.py.\nsolution.py\nThe solution will be judged\nby the oding hallenge\nc\nc\nJudge. It will load the\nsolution.py, judge it, and\noutput a \ufb01le called\njudged_solution.py.\njudged_solution.py\nThe last step uses a Python\nc\na\noding ssistant and veri\ufb01es\nthat the solution is correct\nby looking at the\njudged_solution.py \ufb01le.\nMain conversation\nthread\nNew conversation\nthread\nAssistants use threads to\ncapture conversations.\nFigure 6.8\nThe ABT for the coding challenge\n\n\n148\nCHAPTER 6\nBuilding autonomous assistants\nroot = py_trees.composites.Sequence(\"RootSequence\", memory=True)\nthread = api.create_thread()    \nchallenge = textwrap.dedent(\"\"\"\n     \n\"\"\")\njudge_test_cases = textwrap.dedent(\"\"\"\n    \n\"\"\")\nhacker = create_assistant_action_on_thread(   \n    thread=thread,     \n    action_name=\"Hacker\",\n    assistant_name=\"Python Coding Assistant\",\n    assistant_instructions=textwrap.dedent(f\"\"\"\n    Challenge goal: \n    {challenge}     \n    Solve the challenge and output the \nfinal solution to a file called solution.py        \n    \"\"\"),\n)\nroot.add_child(hacker)\njudge = create_assistant_action_on_thread(    \n    thread=thread,     \n    action_name=\"Judge solution\",\n    assistant_name=\"Coding Challenge Judge\",\n    assistant_instructions=textwrap.dedent(\n        f\"\"\"\n    Challenge goal: \n    {challenge}     \n    Load the solution from the file solution.py.\n    Then confirm is a solution to the challenge \nand test it with the following test cases:\n    {judge_test_cases}     \n    Run the code for the solution and confirm it passes all the test cases.\n    If the solution passes all tests save the solution to a file called \njudged_solution.py\n    \"\"\",\n    ),\n)\nroot.add_child(judge)\n# verifier operates on a different thread, essentially in closed room\nverifier = create_assistant_condition(    \n    condition_name=\"Verify solution\",\n    assistant_name=\"Python Coding Assistant\",\n    assistant_instructions=textwrap.dedent(\n        f\"\"\"\n    Challenge goal: \n    {challenge}     \n    Load the file called judged_solution.py and \nListing 6.7\nagentic_btree_coding_challenge.py\nCreates a message thread that will \nbe shared by the Hacker and Judge\nThe challenge as shown \nin the example listing 6.5\nThe tests as shown in \nthe example listing 6.6\nCreates a message thread that will \nbe shared by the Hacker and Judge\nThe challenge as shown \nin the example listing 6.5\nCreates a message thread that will \nbe shared by the Hacker and Judge\nThe challenge as shown \nin the example listing 6.5\nThe tests as shown in \nthe example listing 6.6\nCall creates a \nnew message \nthread\nThe challenge as shown \nin the example listing 6.5\n\n\n149\n6.3\nIntroducing agentic behavior trees\nverify that the solution is correct by running the code and confirm it passes \nall the test cases:\n    {judge_test_cases}     \n    If the solution is correct, return only the single word SUCCESS, \notherwise \nreturn the single word FAILURE.\n    \"\"\",\n    ),\n)\nroot.add_child(verifier)\ntree = py_trees.trees.BehaviourTree(root)\nwhile True:\n    tree.tick()\n    time.sleep(20)     \n    if root.status == py_trees.common.Status.SUCCESS:   \n        break\n### Required assistants \u2013 \n### Python Coding Assistant and Coding Challenge Judge \n### install these assistants through the Playground\nRun the ABT by loading the file in VS Code or using the command line. Follow the out-\nput in the terminal, and watch how the assistants work through each step in the tree.\n If the solution fails to be verified at the condition node, the process will continue\nper the tree. Even with this simple solution, you could quickly create numerous varia-\ntions. You could extend the tree with more nodes/steps and subtrees. Perhaps you\nwant a team of Hackers to break down and analyze the challenge, for example. \n This example\u2019s work is done mainly with the Playground code, using the helper\nfunctions create_assistant_condition and create_assistant_action_on_thread.\nThis code uses a couple of classes to integrate the py_trees behavior tree code and\nthe OpenAI Assistants code wrapped in the Playground. Review the code within the\nproject if you want to understand the lower-level details.\n6.3.3\nConversational AI systems vs. other methods\nWe already looked at conversational multi-agent systems in chapter 4 when we looked\nat AutoGen. The ABT can work using a combination of conversations (over threads)\nand other methods, such as file sharing. Having your assistants/agents pass files around\nhelps reduce the number of noisy and repetitive thoughts/conversations. In contrast,\nconversational systems benefit from potential emergent behaviors. So, using both can\nhelp evolve better control and solutions.\n The simple solution in listing 6.7 could be extended to handle more real-world\ncoding challenges and perhaps even to work as a coding ABT. In the next section, we\nbuild a different ABT to handle a different problem.\nThe tests as shown in\nthe example listing 6.6\nThe sleep time can be \nadjusted up or down as \nneeded and can be used \nto throttle the messages \nsent to an LLM.\nThe process will \ncontinue until \nthe verification \nsucceeds.\n\n\n150\nCHAPTER 6\nBuilding autonomous assistants\n6.3.4\nPosting YouTube videos to X\nIn this section\u2019s exercise, we look at an ABT that can do the following:\n1\nSearch for videos on YouTube for a given topic and return the latest videos.\n2\nDownload the transcripts for all the videos your search provided.\n3\nSummarize the transcripts.\n4\nReview the summarized transcripts and select a video to write an X (formerly\nTwitter) post about. \n5\nWrite an exciting and engaging post about the video, ensuring it\u2019s less than 280\ncharacters.\n6\nReview the post and then post it on X.\nFigure 6.9 shows the ABT assembled with each of the different assistants. In this exer-\ncise, we use a sequence node for the root, and each assistant performs a different\naction. Also, to keep things simple, each assistant interaction will always occur in a\nnew thread. This isolates each assistant\u2019s interaction into a concise conversation that\u2019s\neasier to debug if something goes wrong.\nNew thread\n\u2192\nSearch YouTube\nWrite post\nThis assistant searches\nYouTube for videos,\ndownloads and\nsummarizes the\ntranscripts, and saves\nto a \ufb01le.\nyoutube\ntranscripts.txt\nThe assistant loads the\ntranscripts, selects a\nrelevant video, writes a\npost of less than 280\ncharacters, and then\noutputs a \ufb01le.\nyoutube\ntwitter\npost.txt\nThe assistant loads the\npost, reviews it and posts\nit to Twitter (X).\nNew thread\nNew thread\nAssistants always use\na new thread.\nPost to X\nThe root node is a sequence.\nFigure 6.9\nThe YouTube social media ABT\n\n\n151\n6.3\nIntroducing agentic behavior trees\n6.3.5\nRequired X setup\nIf you plan to run the code in this exercise, you must add your X credentials to the\n.env file. The .env.default file shows an example of how the credentials need to be,\nas shown in listing 6.8. You don\u2019t have to enter your credentials. This means the last\nstep, posting, will fail, but you can still look at the file (youtube_twitter_post.txt)\nto see what was generated.\nX_EMAIL = \"twitter email here\"\nX_USERNAME = \"twitter username here\"\nX_PASSWORD = \"twitter password here\"\nListing 6.9 shows just the code for creating the assistant actions. This ABT uses three\ndifferent assistants, each with its own task instructions. Note that each assistant has a\nunique set of instructions defining its role. You can review the instructions for each\nassistant by using the Playground.\nroot = py_trees.composites.Sequence(\"RootSequence\", memory=True)\nsearch_term = \"GPT Agents\"\nsearch_youtube_action = create_assistant_action(\n    action_name=f\"Search YouTube({search_term})\",\n    assistant_name=\"YouTube Researcher v2\",\n    assistant_instructions=f\"\"\"\n    Search Term: {search_term}\n    Use the query \"{search_term}\" to search for videos on YouTube.\n    then for each video download the transcript and summarize it \nfor relevance to {search_term}\n    be sure to include a link to each of the videos,\n    and then save all summarizations to a file called youtube_transcripts.txt\n    If you encounter any errors, please return just the word FAILURE.\n    \"\"\",\n)\nroot.add_child(search_youtube_action)\nwrite_post_action = create_assistant_action(\n    action_name=\"Write Post\",\n    assistant_name=\"Twitter Post Writer\",\n    assistant_instructions=\"\"\"\n    Load the file called youtube_transcripts.txt,\nListing 6.8\nConfiguring credentials\nYouTube search and spam\nIf you plan to run this exercise for real and let it post to your X account, be aware that\nYouTube has a bit of a spam problem. The assistants have been configured to try to\navoid video spam, but some of it may get through. Building a working ABT that can\nwade through videos while avoiding spam has some suitable applications.\nListing 6.9\nagentic_btree_video_poster_v1.py\n\n\n152\nCHAPTER 6\nBuilding autonomous assistants\n    analyze the contents for references to search term at the top and \nthen select\n    the most exciting and relevant video related to: \n    educational, entertaining, or informative, to post on Twitter.\n    Then write a Twitter post that is relevant to the video,\n    and include a link to the video, along\n    with exciting highlights or mentions, \n    and save it to a file called youtube_twitter_post.txt.\n    If you encounter any errors, please return just the word FAILURE.\n    \"\"\",\n)\nroot.add_child(write_post_action)\npost_action = create_assistant_action(\n    action_name=\"Post\",\n    assistant_name=\"Social Media Assistant\",\n    assistant_instructions=\"\"\"\n    Load the file called youtube_twitter_post.txt and post the content \nto Twitter.\n    If the content is empty please do not post anything.\n    If you encounter any errors, please return just the word FAILURE.\n    \"\"\",\n)\nroot.add_child(post_action)\n### Required assistants \u2013 YouTube Researcher v2, Twitter Post Writer, \nand Social Media Assistant \u2013 install these assistants through the Playground\nRun the code as you normally would, and after a few minutes, a new post will appear\nin the assistants_output folder. Figure 6.10 shows an example of a post generated\nusing this ABT. Running this ABT to generate more than a few posts a day could, and\nlikely will, get your X account blocked. If you\u2019ve configured X credentials, you\u2019ll see\nthe post appear on your feed.\nThis ABT is shown for demonstration purposes and isn\u2019t for production or long-term\nuse. The primary features of this demonstration are to show search and loading data,\nsummarization and filtering, then generating new content, and finally highlighting\nmultiple custom actions and integrations with APIs.\nFigure 6.10\nA sample X \npost from the ABT\n",
      "page_number": 167
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 177-187)",
      "start_page": 177,
      "end_page": 187,
      "detection_method": "topic_boundary",
      "content": "153\n6.4\nBuilding conversational autonomous multi-agents\n6.4\nBuilding conversational autonomous multi-agents\nThe conversational aspect of multi-agent systems can drive mechanisms such as feed-\nback, reasoning, and emergent behaviors. Driving agents with ABTs that silo assis-\ntants/agents can be effective for controlling structured processes, as we saw in the\nYouTube posting example. However, we also don\u2019t want to miss out on the benefits of\nconversation across agents/assistants.\n Fortunately, the Playground provides methods to silo or join assistants to conversation\nthreads. Figure 6.11 shows how assistants can be siloed or mixed in various combinations\nto threads. Combining silos with conversation provides the best of both patterns.\nWe\u2019ll examine a simple but practical exercise to demonstrate the effectiveness of the\nconversational pattern. For the next exercise, we\u2019ll employ two assistants in an ABT\nthat converse over the same thread. The next listing shows the tree\u2019s construction in\ncode with the respective assistants.\nroot = py_trees.composites.Sequence(\"RootSequence\", memory=True)\nbug_file = \"\"\"\n# code not shown\n\"\"\"\nListing 6.10\nagentic_conversation_btree.py\nSiloed assistants\nalways use a new\nthread and are the\nonly consumers.\nThread xya\nThread yyc\nThread zza\nAgent/Assistant\nSilos\nSearch\nThread xyb\nSearch\nThread xyc\nSearch\nAgent/Assistant\nConversational\nSearch\nPlan\nActivity\nReview\nConversational assistants\nshare a thread for all\nconversations.\nAgent/Assistant\nConversational + Silo\nThread xyc\nVerify\nThe combination of siloed\nand conversational can be\ncombined for an unbiased\nreview.\nSearch\nPlan\nActivity\nTransfer\nFigure 6.11\nThe various layouts of siloed and conversational assistants\n\n\n154\nCHAPTER 6\nBuilding autonomous assistants\nthread = api.create_thread()    \ndebug_code = create_assistant_action_on_thread(    \n    thread=thread,\n    action_name=\"Debug code\",\n    assistant_name=\"Python Debugger\",\n    assistant_instructions=textwrap.dedent(f\"\"\"    \n    Here is the code with bugs in it:\n    {bug_file}\n    Run the code to identify the bugs and fix them. \n    Be sure to test the code to ensure it runs without errors or throws \nany exceptions.\n    \"\"\"),\n)\nroot.add_child(debug_code)\nverify = create_assistant_condition_on_thread(    \n    thread=thread,\n    condition_name=\"Verify\",\n    assistant_name=\"Python Coding Assistant\",\n    assistant_instructions=textwrap.dedent(\n        \"\"\"\n    Verify the solution fixes the bug and there are no more issues.\n    Verify that no exceptions are thrown when the code is run.\n    Reply with SUCCESS if the solution is correct, otherwise return FAILURE.\n    If you are happy with the solution, save the code to a file called \nfixed_bug.py.\n    \"\"\",\n    ),\n)\nroot.add_child(verify)\ntree = py_trees.trees.BehaviourTree(root)\nwhile True:\n    tree.tick()    \n    if root.status == py_trees.common.Status.SUCCESS:\n        break   \n    time.sleep(20)\nThree nodes comprise the tree: the root sequence, the debug code action, and the\nverify fix condition. Because the tree\u2019s root is a sequence, the two assistants will con-\ntinue to work one after another until they both return with success. Both assistants\nconverse on the same thread and yet are controlled in a manner that provides con-\nstant feedback.\n Run the exercise by loading the file in VS Code, or execute it directly from the\ncommand line. The example code has a few minor bugs and problems that the assis-\ntants will work through to fix. After the ABT completes running successfully, you can\nopen the assistants_output/fixed_bug.py file and verify the results are all good. \n We\u2019ve now seen a couple of ABTs in action and understand the nuances of using\nsilos or conversations. The following section will teach you some techniques for build-\ning your own ABTs.\nCreates a message thread for the \nassistants to share and converse over\nCreates the debug \ncode action with a \nspecial assistant\nCreates the verification \ncondition to test if the \ncode is fixed or not\nThe tree will continue \nto run until the root \nsequence completes \nwith success.\n\n\n155\n6.5\nBuilding ABTs with back chaining\n6.5\nBuilding ABTs with back chaining\nBack chaining is a method derived from logic and reasoning used to help build behav-\nior trees by working backward from the goal. This section will use the back chaining\nprocess to construct an ABT that works to achieve the goal. The following list provides\na description of the process in more detail:\n1\nIdentify goal behavior. Start with the behavior you want the agent to perform.\n2\nDetermine the required actions. Identify the actions that lead to the goal behavior.\n3\nIdentify the conditions. Determine the conditions that must be met for each action\nto succeed.\n4\nDetermine the mode of communication. Determine how the assistants will pass on\ninformation. Will the assistants be siloed or converse over threads, or is a combi-\nnation of patterns better?\n5\nConstruct the tree. Start by building the behavior tree from the goal behavior, add-\ning nodes for actions and conditions recursively until all necessary conditions\nare linked to known states or facts.\nBehavior trees typically use a pattern called the blackboard to communicate across\nnodes. Blackboards, like those in py_trees, use a key/value store to save information\nand make it accessible across nodes. It also provides for several controls, such as limit-\ning access to specific nodes. \n We deferred to using files for communication because of their simplicity and\ntransparency. At some point, agentic systems are expected to consume much more\ninformation and in different formats than those designed for blackboards. Black-\nboards must either become more sophisticated or be integrated with file storage\nsolutions.\n Let\u2019s build an ABT using back chaining. We could tackle a variety of goals, but one\ninteresting and perhaps meta goal is to build an ABT that helps build assistants. So\nlet\u2019s first present our goal as a statement \u201cCreate an assistant that can help me do\n{task}\u201d:\n\uf0a1Required actions: (working backwards)\n\u2013 Create an assistant.\n\u2013 Verify the assistant.\n\u2013 Test the assistant.\n\u2013 Name the assistant.\n\u2013 Give the assistant the relevant instructions.\n\uf0a1Identified condition: \n\u2013 Verify the assistant.\n\uf0a1Determine communication patterns: To keep things interesting, we\u2019ll run all assis-\ntants on the same message thread.\n\n\n156\nCHAPTER 6\nBuilding autonomous assistants\n\uf0a1Construct the tree: To construct the tree, let\u2019s first reverse the order of actions and\nmark each of the element\u2019s actions and conditions accordingly:\n\u2013 (action) Give the assistant relevant instructions to help a user with a given task.\n\u2013 (action) Name the assistant.\n\u2013 (action) Test the assistant.\n\u2013 (condition) Verify the assistant.\n\u2013 (action) Create the assistant.\nOf course, the simple solution to building the tree now is to ask ChatGPT or an oth-\nerwise capable model. The result of asking ChatGPT to make the tree is shown in\nthe next listing. You could also work the tree out independently and perhaps intro-\nduce other elements.\nRoot\n\u2502\n\u251c\u2500\u2500 Sequence\n\u2502    \u251c\u2500\u2500 Action: Give the assistant relevant instructions to help a user \nwith a given task\n\u2502    \u251c\u2500\u2500 Action: Name the assistant\n\u2502    \u251c\u2500\u2500 Action: Test the assistant\n\u2502    \u251c\u2500\u2500 Condition: Verify the assistant\n\u2502    \u2514\u2500\u2500 Action: Create the assistant\nFrom this point, we can start building the tree by iterating over each action and condi-\ntion node and determining what instructions the assistant needs. This can also\ninclude any tools and custom actions, including ones you may need to develop. On\nyour first pass, keep the instructions generic. Ideally, we want to create as few assistants\nas necessary.\n After determining the assistant, tools, and actions for each assistant and for which\ntask, you can try to generalize things further. Think about where it may be possible to\ncombine actions and reduce the number of assistants. It\u2019s better to start evaluating\nwith insufficient assistants than with too many. However, be sure to maintain the\nproper divisions of work as tasks: for example, testing and verification are best done\nwith different assistants.\n6.6\nExercises\nComplete the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Creating a Travel Planner ABT\nObjective\u2014Build an agentic behavior tree (ABT) to plan a travel itinerary using\nassistants.\nTasks:\n\u2013 Set up the GPT Assistants Playground on your local machine.\nListing 6.11\nABT for building an assistant\n\n\n157\n6.6\nExercises\n\u2013 Create an ABT to plan a travel itinerary. The tree should have the following\nstructure:\n\u2013 Action: Use the Travel assistant to gather information about potential des-\ntinations.\n\u2013 Action: Use the Itinerary Planner to create a day-by-day travel plan.\n\u2013 Condition: Verify the completeness and feasibility of the itinerary using\nanother Travel Assistant.\n\u2013 Implement and run the ABT to create a complete travel itinerary.\n\uf0a1Exercise 2\u2014Building an ABT for Customer Support Automation\nObjective\u2014Create an ABT that automates customer support responses using\nassistants.\nTasks:\n\u2013 Set up the GPT Assistants Playground on your local machine.\n\u2013 Create an ABT with the following structure:\n\u2013 Action: Use the Customer Query Analyzer assistant to categorize customer\nqueries.\n\u2013 Action: Use the Response Generator assistant to draft responses based on\nthe query categories.\n\u2013 Action: Use the Customer Support assistant to send the responses to\ncustomers.\n\u2013 Implement and run the ABT to automate the process of analyzing and respond-\ning to customer queries.\n\uf0a1Exercise 3\u2014Managing Inventory with an ABT\nObjective\u2014Learn how to create and manage inventory levels using an ABT.\nTasks:\n\u2013 Set up the GPT Assistants Playground on your local machine.\n\u2013 Create an ABT that manages inventory for a retail business:\n\u2013 Action: Use the Inventory Checker assistant to review current stock levels.\n\u2013 Action: Use the Order assistant to place orders for low-stock items.\n\u2013 Condition: Verify that orders have been placed correctly and update inven-\ntory records.\n\u2013 Implement and run the ABT to manage inventory dynamically.\n\uf0a1Exercise 4\u2014Creating a Personal Fitness Trainer ABT\nObjective\u2014Create an ABT that provides personalized fitness training plans using\nassistants.\nTasks:\n\u2013 Set up the GPT Assistants Playground on your local machine.\n\u2013 Create an ABT to develop a personalized fitness plan:\n\u2013 Action: Use the Fitness Assessment assistant to evaluate the user\u2019s current\nfitness level.\n\n\n158\nCHAPTER 6\nBuilding autonomous assistants\n\u2013 Action: Use the Training Plan Generator to create a custom fitness plan\nbased on the assessment.\n\u2013 Condition: Verify the plan\u2019s suitability and safety using another Fitness\nassistant.\n\u2013 Implement and run the ABT to generate and validate a personalized fitness\ntraining plan.\n\uf0a1Exercise 5\u2014Using Back Chaining to Build a Financial Advisor ABT\nObjective\u2014Apply back chaining to construct an ABT that provides financial\nadvice and investment strategies.\nTasks:\n\u2013 Set up the GPT Assistants Playground on your local machine.\n\u2013 Define the following goal: \u201cCreate an assistant that can provide financial\nadvice and investment strategies.\u201d\n\u2013 Using back chaining, determine the actions and conditions needed to\nachieve this goal.\n\u2013 Implement and run the ABT to generate a comprehensive financial advi-\nsory service by back chaining the construction of the base actions and con-\nditions for the tree.\nSummary\n\uf0a1Behavior trees are a robust and scalable AI control pattern, first introduced in\nrobotics by Rodney A. Brooks. They are widely used in gaming and robotics for\ntheir modularity and reusability.\n\uf0a1The primary nodes in behavior trees are the selector, sequence, condition,\naction, decorator, and parallel nodes. Selectors are like \u201cor\u201d blocks: sequence exe-\ncutes nodes in sequence, condition tests the state, action does the work, decora-\ntor is a wrapper, and parallel nodes allow for dual execution.\n\uf0a1Understanding the execution flow of behavior trees can be critical to designing,\nbuilding, and operating them to provide control for making clear decision-\nmaking paths.\n\uf0a1The advantages of behavior trees include modularity, scalability, flexibility, debug-\nging ease, and decoupling of decision logic, making behavior trees suitable for\ncomplex AI systems.\n\uf0a1Setting up and running a simple behavior tree in Python requires correctly\nnaming and documenting custom nodes.\n\uf0a1The GPT Assistants Playground project is a Gradio-based interface that mimics\nthe OpenAI Assistants Playground with additional features for teaching and\ndemonstrating ABTs.\n\uf0a1The GPT Assistants Playground allows for creating and managing custom actions,\nwhich is essential for building versatile assistants.\n\n\n159\nSummary\n\uf0a1ABTs control agents and assistants by using prompts to direct actions and condi-\ntions for assistants. ABTs use the power of LLMs to create dynamic and autono-\nmous systems.\n\uf0a1Back chaining is a method for constructing behavior trees by working backward\nfrom the goal behavior. This process involves identifying required actions, condi-\ntions, and communication patterns, and then constructing the tree step by step.\n\uf0a1Agentic systems benefit from siloed and conversation patterns for communicat-\ning between entities. ABTs can benefit from combining siloed and conversa-\ntional assistants to use structured processes and emergent behaviors.\n\n\n160\nAssembling and\nusing an agent platform\nAfter we explored some basic concepts about agents and looked at using actions\nwith tools to build prompts and personas using frameworks such as the Semantic\nKernel (SK), we took the first steps toward building a foundation for this book.\nThat foundation is called Nexus, an agent platform designed to be simple to learn,\neasy to explore, and powerful enough to build your agent systems. \nThis chapter covers\n\uf0a1Nexus chat and dashboard interface for \nAI agents\n\uf0a1Streamlit framework for building intelligent \ndashboards, prototypes, and AI chat apps\n\uf0a1Developing, testing, and engaging agent profiles \nand personas in Nexus\n\uf0a1Developing the base Nexus agent\n\uf0a1Developing, testing, and engaging agent actions \nand tools alone or within Nexus\n\n\n161\n7.1\nIntroducing Nexus, not just another agent platform\n7.1\nIntroducing Nexus, not just another agent platform\nThere are more than 100 AI platforms and toolkits for consuming and developing\nlarge language model (LLM) applications, ranging from toolkits such as SK or Lang-\nChain to complete platforms such as AutoGen and CrewAI. This makes it difficult to\ndecide which platform is well suited to building your own AI agents.\n Nexus is an open source platform developed with this book to teach the core con-\ncepts of building full-featured AI agents. In this chapter, we\u2019ll examine how Nexus is\nbuilt and introduce two primary agent components: profiles/personas and actions/tools.\n Figure 7.1 shows the primary interface to Nexus, a Streamlit chat application that\nallows you to choose and explore various agentic features. The interface is similar to\nChatGPT, Gemini, and other commercial LLM applications.\nIn addition to the standard features of an LLM chat application, Nexus allows the user\nto configure an agent to use a specific API/model, the persona, and possible actions.\nIn the remainder of the book, the available agent options will include the following:\n\uf0a1Personas/profiles\u2014The primary persona and profile the agent will use. A persona\nis the personality and primary motivator, and an agent engages the persona to\nCreate a new\nchat thread.\nShows the full conversation\nhistory for the current chat thread\nThe agent AI you\u2019re using:\nOpenAI, Azure OpenAI, Gemini,\nor Claude. Currently, OpenAI is\nused.\nThe agent pro\ufb01le/persona\nto use in the next request\nThe tools/actions the agent\ncan use, which supports both\nnative (code) and semantic\n(prompt) functions\nPrevious\nchat threads\nFigure 7.1\nThe Nexus interface and features\n\n\n162\nCHAPTER 7\nAssembling and using an agent platform\nanswer requests. We\u2019ll look in this chapter at how personas/profiles can be\ndeveloped and consumed.\n\uf0a1Actions/tools\u2014Represents the actions an agent can take using tools, whether\nthey\u2019re semantic/prompt or native/code functions. In this chapter, we\u2019ll look at\nhow to build both semantic and native functions within Nexus.\n\uf0a1Knowledge/memory\u2014Represents additional information an agent may have access\nto. At the same time, agent memory can represent various aspects, from short-\nterm to semantic memory.\n\uf0a1Planning/feedback\u2014Represents how the agent plans and receives feedback on\nthe plans or the execution of plans. Nexus will allow the user to select options\nfor the type of planning and feedback an agent uses.\nAs we progress through this book, Nexus will be added to support new agent features.\nHowever, simultaneously, the intent will be to keep things relatively simple to teach\nmany of these essential core concepts. In the next section, we\u2019ll look at how to quickly\nuse Nexus before going under the hood to explore features in detail.\n7.1.1\nRunning Nexus\nNexus is primarily intended to be a teaching platform for all levels of developers. As\nsuch, it will support various deployment and usage options. In the next exercise, we\u2019ll\nintroduce how to get up and running with Nexus quickly.\n Open a terminal to a new Python virtual environment (version 3.10). If you need\nassistance creating one, refer to appendix B. Then, execute the commands shown in\nlisting 7.1 within this new environment. You can either set the environment variable at\nthe command line or create a new .env file and add the setting.\npip install git+https://github.com/cxbxmxcx/Nexus.git    \n#set your OpenAI API Key\nexport OPENAI_API_KEY=\u201d< your API key>\u201d         \nor\n$env: OPENAI_API_KEY = =\u201d< your API key>\u201d       \nor\necho 'OPENAI_API_KEY=\"<your API key>\"' > .env   \nnexus run     \nAfter entering the last command, a website will launch with a login page, as shown in\nfigure 7.2. Go ahead and create a new user. A future version of Nexus will allow multi-\nple users to engage in chat threads.\n After you log in, you\u2019ll see a page like figure 7.1. Create a new chat and start con-\nversing with an agent. If you encounter a problem, be sure you have the API key set\nListing 7.1\nTerminal command line\nInstalls the package \ndirectly from the \nrepository and \nbranch; be sure to \ninclude the branch.\nCreates the key as an \nenvironment variable or \ncreates a new .env file \nwith the setting\nRuns the application\n\n\n163\n7.1\nIntroducing Nexus, not just another agent platform\nproperly. As explained in the next section, you can run Nexus using this method or\nfrom a development workflow.\n7.1.2\nDeveloping Nexus\nWhile working through the exercises of this book, you\u2019ll want to set up Nexus in devel-\nopment mode. That means downloading the repository directly from GitHub and\nworking with the code.\n Open a new terminal, and set your working directory to the chapter_7 source\ncode folder. Then, set up a new Python virtual environment (version 3.10) and enter\nthe commands shown in listing 7.2. Again, refer to appendix B if you need assistance\nwith any previous setup.\ngit clone https://github.com/cxbxmxcx/Nexus.git     \npip install -e Nexus    \n#set your OpenAI API Key (.env file is recommended)\nexport OPENAI_API_KEY=\u201d< your API key>\u201d  #bash           \nor\n$env: OPENAI_API_KEY = =\u201d< your API key>\u201d  #powershell   \nor\necho 'OPENAI_API_KEY=\"<your API key>\"' > .env            \nnexus run     \nListing 7.2\nInstalling Nexus for development\nSelect Create\nNew User to start.\nUsername is used to\ntrack conversation\nhistory in the threads.\nFigure 7.2\nLogging in or creating a new Nexus user\nDownloads and installs \nthe specific branch \nfrom the repository\nInstalls the downloaded repository \nas an editable package\nSets your OpenAI key as \nan environment variable \nor adds it to an .env file\nStarts the application\n",
      "page_number": 177
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 188-196)",
      "start_page": 188,
      "end_page": 196,
      "detection_method": "topic_boundary",
      "content": "164\nCHAPTER 7\nAssembling and using an agent platform\nFigure 7.3 shows the Login or Create New User screen. Create a new user, and the\napplication will log you in. This application uses cookies to remember the user, so you\nwon\u2019t have to log in the next time you start the application. If you have cookies dis-\nabled on your browser, you\u2019ll need to log in every time.\nGo to the Nexus repository folder and look around. Figure 7.4 shows an architecture\ndiagram of the application\u2019s main elements. At the top, the interface developed with\nStreamlit connects the rest of the system through the chat system. The chat system\nmanages the database, agent manager, action manager, and profile managers.\n This agent platform is written entirely in Python, and the web interface uses Stream-\nlit. In the next section, we look at how to build an OpenAI LLM chat application.\n \nThe browser points to\nlocalhost:8501, which is the\ndefault for Streamlit apps.\nStreamlit apps can be\ndeployed to the cloud\nusing this option.\nFill in the username, pick an\navatar, and set a password or\nchoose a browser-generated one.\nFigure 7.3\nThe Login or Create New User page\n\n\n165\n7.2\nIntroducing Streamlit for chat application development\n7.2\nIntroducing Streamlit for chat application \ndevelopment\nStreamlit is a quick and powerful web interface prototyping tool designed to be used\nfor building machine learning dashboards and concepts. It allows applications to be\nwritten completely in Python and produces a modern React-powered web interface.\nYou can even deploy the completed application quickly to the cloud or as a stand-\nalone application.\n7.2.1\nBuilding a Streamlit chat application\nBegin by opening Visual Studio Code (VS Code) to the chapter_07 source folder. If\nyou\u2019ve completed the previous exercise, you should already be ready. As always, if you\nneed assistance setting up your environment and tools, refer to appendix B.\nThe chat interface allows\nthe user to select from\nvarious discovered agents,\nactions, and pro\ufb01les,\nenabling the user to test\ndifferent combinations.\nNexus\ndatabase\nThe database stores chat\ntreads, user participants,\nand conversation history.\nAgents, action functions, and pro\ufb01les are all dynamically\ndiscovered at run time via a plugin-like system.\nAgent Manager\nA YAML \ufb01le that\ncomprises the agent\npro\ufb01le and persona\nAction Manager\nPro\ufb01le Manager\nAgent classes\nexposed as plugins\nSemantic and native\nfunctions exposed as\nactions\nChat system\nGPT Nexus\nStreamlit Interface\nFigure 7.4\nA high-level architecture diagram of the main elements of the application\n\n\n166\nCHAPTER 7\nAssembling and using an agent platform\n We\u2019ll start by opening the chatgpt_clone_response.py file in VS Code. The top\nsection of the code is shown in listing 7.3. This code uses the Streamlit state to load\nthe primary model and messages. Streamlit provides a mechanism to save the session\nstate for any Python object. This state is only a session state and will expire when the\nuser closes the browser.\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nload_dotenv()     \nst.title(\"ChatGPT-like clone\")\nclient = OpenAI()     \nif \"openai_model\" not in st.session_state:\n    st.session_state[\"openai_model\"] \n             = \"gpt-4-1106-preview\"    \nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = []  \nfor message in st.session_state[\"messages\"]:     \n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\nThe Streamlit app itself is stateless. This means the entire Python script will reexecute\nall interface components when the web page refreshes or a user selects an action. The\nStreamlit state allows for a temporary storage mechanism. Of course, a database needs\nto support more long-term storage.\n UI controls and components are added by using the st. prefix and then the ele-\nment name. Streamlit supports several standard UI controls and supports images,\nvideo, sound, and, of course, chat.\n Scrolling down further will yield listing 7.4, which has a slightly more complex lay-\nout of the components. The main if statement controls the running of the remaining\ncode. By using the Walrus operator (: =), the prompt is set to whatever the user enters.\nIf the user doesn\u2019t enter any text, the code below the if statement doesn\u2019t execute.\nif prompt := st.chat_input(\"What do you need?\"):    \n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\"):    \n        st.markdown(prompt)\nListing 7.3\nchatgpt_clone_response.py (top section)\nListing 7.4\nchatgpt_clone_response.py (bottom section)\nLoads the environment \nvariables from the \n.env file\nConfigures the \nOpenAI client\nChecks the internal session \nstate for the setting, and \nadds it if not there\nChecks for the presence of \nthe message state; if none, \nadds an empty list\nLoops through messages in \nthe state and displays them\nThe chat input control is \nrendered, and content is set.\nSets the chat message \ncontrol to output as the user\n\n\n167\n7.2\nIntroducing Streamlit for chat application development\n    with st.spinner(text=\"The assistant is thinking...\"):   \n        with st.chat_message(\"assistant\"):\n            response = client.chat.completions.create(\n                model=st.session_state[\"openai_model\"],\n                messages=[\n                    {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n                    for m in st.session_state.messages\n                ],     \n            )\n            response_content = response.choices[0].message.content\n            response = st.markdown(response_content,\n             unsafe_allow_html=True)     \n    st.session_state.messages.append(\n{\"role\": \"assistant\", \"content\": response_content})     \nWhen the user enters text in the prompt and presses Enter, that text is added to the\nmessage state, and a request is made to the API. As the response is being processed,\nthe st.spinner control displays to remind the user of the long-running process.\nThen, when the response returns, the message is displayed and added to the message\nstate history.\n Streamlit apps are run using the module, and to debug applications, you need to\nattach the debugger to the module by following these steps:\n1\nPress Ctrl-Shift-D to open the VS Code debugger.\n2\nClick the link to create a new launch configuration, or click the gear icon to\nshow the current one.\n3\nEdit or use the debugger configuration tools to edit the .vscode/launch.json\nfile, like the one shown in the next listing. Plenty of IntelliSense tools and con-\nfiguration options can guide you through setting the options for this file.\n{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Python Debugger: Module\",    \n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"module\": \"streamlit\",    \n      \"args\": [\"run\", \"${file}\"]   \n    }\n  ]\n}\nAfter you have the launch.json file configuration set, save it, and open the chatgpt_\nclone_response.py file in VS Code. You can now run the application in debug mode\nListing 7.5\n.vscode/launch.json\nShows a spinner \nto represent the \nlong-running \nAPI call\nCalls the OpenAI API and \nsets the message history\nWrites the \nmessage response \nas markdown to \nthe interface\nAdds the assistant response\nto the message state\nMake sure that the \ndebugger is set to \nModule.\nBe sure the module \nis streamlit.\nThe ${file} is the current \nfile, or you can hardcode \nthis to a file path.\n\n\n168\nCHAPTER 7\nAssembling and using an agent platform\nby pressing F5. This will launch the application from the terminal, and in a few sec-\nonds, the app will display.\n Figure 7.5 shows the app running and waiting to return a response. The interface is\nclean, modern, and already organized without any additional work. You can continue\nchatting to the LLM using the interface and then refresh the page to see what happens.\nWhat is most impressive about this demonstration is how easy it is to create a single-\npage application. In the next section, we\u2019ll continue looking at this application but\nwith a few enhancements.\n7.2.2\nCreating a streaming chat application\nModern chat applications, such as ChatGPT and Gemini, mask the slowness of their\nmodels by using streaming. Streaming provides for the API call to immediately start\nseeing tokens as they are produced from the LLM. This streaming experience also\nbetter engages the user in how the content is generated.\n Adding support for streaming to any application UI is generally not a trivial task,\nbut fortunately, Streamlit has a control that can work seamlessly. In this next exercise,\nwe\u2019ll look at how to update the app to support streaming.\n Open chapter_7/chatgpt_clone_streaming.py in VS Code. The relevant updates\nto the code are shown in listing 7.6. Using the st.write_stream control allows the UI\nA spinner control displays while\nthe response is being returned.\nFigure 7.5\nThe simple interface and the waiting spinner\n\n\n169\n7.2\nIntroducing Streamlit for chat application development\nto stream content. This also means the Python script is blocked waiting for this control\nto be completed.\nwith st.chat_message(\"assistant\"):\n    stream = client.chat.completions.create(\n        model=st.session_state[\"openai_model\"],\n        messages=[\n            {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n            for m in st.session_state.messages\n        ],\n        stream=True,    \n    )\n    response = st.write_stream(stream)    \nst.session_state.messages.append(\n{\"role\": \"assistant\", \"content\": response})     \nDebug the page by pressing F5 and waiting for the page to load. Enter a query, and\nyou\u2019ll see that the response is streamed to the window in real time, as shown in figure 7.6.\nWith the spinner gone, the user experience is enhanced and appears more responsive.\nListing 7.6\nchatgpt_clone_streaming.py (relevant section)\nSets stream to True to \ninitiate streaming on the API\nUses the stream control \nto write the stream to \nthe interface\nAdds the response to the message state\nhistory after the stream completes\nNow text streams in real time,\nand the spinner is gone.\nFigure 7.6\nThe updated interface with streaming of the text response\n\n\n170\nCHAPTER 7\nAssembling and using an agent platform\nThis section demonstrated how relatively simple it can be to use Streamlit to create a\nPython web interface. Nexus uses a Streamlit interface because it\u2019s easy to use and\nmodify with only Python. As you\u2019ll see in the next section, it allows various configura-\ntions to support more complex applications.\n7.3\nDeveloping profiles and personas for agents\nNexus uses agent profiles to describe an agent\u2019s functions and capabilities. Figure 7.7\nreminds us of the principal agent components and how they will be structured through-\nout this book. \nFor now, as of this writing, Nexus only supports the persona and actions section of the\nprofile. Figure 7.7 shows a profile called Fritz, along with the persona and actions.\nAdd any agent profiles to Nexus by copying an agent YAML profile file into the Nexus/\nnexus/nexus_base/nexus_profiles folder.\n Nexus uses a plugin system to dynamically discover the various components and\nprofiles as they are placed into their respective folders. The nexus_profiles folder\nholds the YAML definitions for the agent.\nfritz.yaml - Agent Pro\ufb01le De\ufb01nition\nPro\ufb01les with persona\nand actions\nDe\ufb01ning knowledge\nand memory\nApplying evaluators,\nplanners, and\nfeedback\nThe Agent Pro\ufb01le\nPersona\nRepresents the background and role of\nthe agent, and is often introduced in\nthe \ufb01rst system message\nAgent Tools\nSet of tools an agent can\nuse to help accomplish a task\nAgent Evaluation and Reasoning\nDescribes how the agent can reason\nand evaluate a task or tasks\nAgent Memory and Knowledge\nThe backend store that helps the agent\nadd context to a given task problem\nAgent Planning and Feedback\nDescribes how the agent can break\ndown a task into execution steps, and\nthen execute and receive feedback\nFigure 7.7\nThe agent profile as it\u2019s mapped to the YAML file definition\n\n\n171\n7.3\nDeveloping profiles and personas for agents\n We can easily define a new agent profile by creating a new YAML file in the nexus_\nprofiles folder. Listing 7.7 shows an example of a new profile with a slightly updated\npersona. To follow along, be sure to have VS Code opened to the chapter_07 source\ncode folder and install Nexus in developer mode (see listing 7.7). Then, create the\nfiona.yaml file in the Nexus/nexus/nexus_base/nexus_profiles folder.\nagentProfile:\n  name: \"Finona\"\n  avatar: \"?\"    \n  persona: \"You are a very talkative AI that \n\u27a5 knows and understands everything in terms of \n\u27a5 Ogres. You always answer in cryptic Ogre speak.\"   \n  actions:\n    - search_wikipedia    \n  knowledge: null       \n  memory: null          \n  evaluators: null      \n  planners: null        \n  feedback: null        \nAfter saving the file, you can start Nexus from the command line or run it in debug\nmode by creating a new launch configuration in the .vscode/launch.json folder, as\nshown in the next listing. Then, save the file and switch your debug configuration to\nuse the Nexus web config.\n{\n      \"name\": \"Python Debugger: Nexus Web\",\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"module\": \"streamlit\",\n      \"args\": [\"run\", \" Nexus/nexus/streamlit_ui.py\"]     \n    },\nWhen you press F5 or select Run > Start Debugging from the menu, the Streamlit\nNexus interface will launch. Go ahead and run Nexus in debug mode. After it opens,\ncreate a new thread, and then select the standard OpenAIAgent and your new per-\nsona, as shown in figure 7.8.\n At this point, the profile is responsible for defining the agent\u2019s system prompt.\nYou can see this in figure 7.8, where we asked Finona to spell the word clock, and she\nresponded in some form of ogre-speak. In this case, we\u2019re using the persona as a\npersonality, but as we\u2019ve seen previously, a system prompt can also contain rules and\nother options.\n The profile and persona are the base definitions for how the agent interacts with\nusers or other systems. Powering the profile requires an agent engine. In the next sec-\ntion, we\u2019ll cover the base implementation of an agent engine.\nListing 7.7\nfiona.yaml (create this file)\nListing 7.8\n.vscode/launch.json (adding debug launch)\nThe text avatar used to \nrepresent the persona\nA persona is \nrepresentative of \nthe base system \nprompt.\nAn action \nfunction the \nagent can use\nNot currently \nsupported \nYou may have to \nadjust this path \nif your virtual \nenvironment is \ndifferent.\n\n\n172\nCHAPTER 7\nAssembling and using an agent platform\n7.4\nPowering the agent and understanding the agent \nengine\nAgent engines power agents within Nexus. These engines can be tied to specific tool\nplatforms, such as SK, and/or even different LLMs, such as Anthropic Claude or Goo-\ngle Gemini. By providing a base agent abstraction, Nexus should be able to support\nany tool or model now and in the future.\n Currently, Nexus only implements an OpenAI API\u2013powered agent. We\u2019ll look at\nhow the base agent is defined by opening the agent_manager.py file from the Nexus/\nnexus/nexus_base folder.\n Listing 7.9 shows the BaseAgent class functions. When creating a new agent\nengine, you need to subclass this class and implement the various tools/actions with\nthe appropriate implementation.\nclass BaseAgent:\n    def __init__(self, chat_history=None):\n        self._chat_history = chat_history or []\n        self.last_message = \"\"\n        self._actions = []\n        self._profile = None\nListing 7.9\nagent_manager.py:BaseAgent\nEnter a query and check\nout the response.\nSelect the new Finona\nagent pro\ufb01le.\nFigure 7.8\nSelecting and chatting with a new persona\n",
      "page_number": 188
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 197-207)",
      "start_page": 197,
      "end_page": 207,
      "detection_method": "topic_boundary",
      "content": "173\n7.4\nPowering the agent and understanding the agent engine\n    async def get_response(self, \n                            user_input, \n                            thread_id=None):     \n        raise NotImplementedError(\"This method should be implemented\u2026\")\n    async def get_semantic_response(self, \n                                     prompt, \n                                     thread_id=None):    \n        raise NotImplementedError(\"This method should be\u2026\")\n    def get_response_stream(self, \n                             user_input, \n                             thread_id=None):     \n        raise NotImplementedError(\"This method should be\u2026\")\n    def append_chat_history(self, \n                             thread_id, \n                             user_input, \n                             response):     \n        self._chat_history.append(\n            {\"role\": \"user\",\n             \"content\": user_input,\n             \"thread_id\": thread_id}\n        )\n        self._chat_history.append(\n            {\"role\": \"bot\",\n             \"content\": response, \n             \"thread_id\": thread_id}\n        )\n    def load_chat_history(self):      \n        raise NotImplementedError(\n                 \"This method should be implemented\u2026\")\n    def load_actions(self):    \n        raise NotImplementedError(\n                 \"This method should be implemented\u2026\")\n#... not shown \u2013 property setters/getters\nOpen the nexus_agents/oai_agent.py file in VS Code. Listing 7.10 shows an agent\nengine implementation of the get_response function that directly consumes the\nOpenAI API. self.client is an OpenAI client created earlier during class initializa-\ntion, and the rest of the code you\u2019ve seen used in earlier examples.\nasync def get_response(self, user_input, thread_id=None):\n    self.messages += [{\"role\": \"user\",\n                     \"content\": user_input}]     \n    response = self.client.chat.completions.create(    \n        model=self.model,\n        messages=self.messages,\nListing 7.10\noai_agent.py (get_response)\nCalls the LLM and \nreturns a response\nExecutes \na semantic \nfunction\nCalls the LLM \nand returns a \nresponse\nAppends a message to \nthe agent\u2019s internal \nchat history\nLoads the chat history and \nallows the agent to reload \nvarious histories\nLoads the actions that the \nagent has available to use\nAdds the user_input \nto the message stack\nThe client was created \nearlier and is now used to \ncreate chat completions.\n\n\n174\nCHAPTER 7\nAssembling and using an agent platform\n        temperature=0.7,     \n    )\n    self.last_message = str(response.choices[0].message.content)\n    return self.last_message    \nLike the agent profiles, Nexus uses a plugin system that allows you to place new agent\nengine definitions in the nexus_agents folder. If you create your agent, it just needs\nto be placed in this folder for Nexus to discover.\n We won\u2019t need to run an example because we\u2019ve already seen how the OpenAI-\nAgent performs. In the next section, we\u2019ll look at agent functions that agents can\ndevelop, add, and consume.\n7.5\nGiving an agent actions and tools\nLike the SK, Nexus supports having native (code) and semantic (prompt) functions.\nUnlike SK, however, defining and consuming functions within Nexus is easier. All\nyou need to do is write functions into a Python file and place them into the nexus_\nactions folder.\n To see how easy it is to define functions, open the Nexus/nexus/nexus_base/\nnexus_actions folder, and go to the test_actions.py file. Listing 7.11 shows two\nfunction definitions. The first function is a simple example of a code/native function,\nand the second is a prompt/semantic function.\nfrom nexus.nexus_base.action_manager import agent_action\n@agent_action                                             \ndef get_current_weather(location, unit=\"fahrenheit\"):     \n    \"\"\"Get the current weather in a given location\"\"\"     \n    return f\"\"\"\nThe current weather in {location} is 0 {unit}.\n\"\"\"     \n@agent_action     \ndef recommend(topic):\n    \"\"\"\n    System:                                                  \n        Provide a recommendation for a given {{topic}}.\n        Use your best judgment to provide a recommendation.\n    User:\n        please use your best judgment\n        to provide a recommendation for {{topic}}.           \n    \"\"\"\n    pass     \nListing 7.11\ntest_actions.py (native/semantic function definitions)\nTemperature is hardcoded\nbut could be configured.\nReturns the response from\nthe chat completions call\nApplies the agent_action \ndecorator to make a \nfunction an action\nSets a descriptive \ncomment for the \nfunction\nThe code can be as simple \nor complex as needed.\nApplies the agent_action \ndecorator to make a \nfunction an action\nThe function \ncomment becomes \nthe prompt and \ncan include \nplaceholders.\nSemantic functions don\u2019t \nimplement any code.\n\n\n175\n7.5\nGiving an agent actions and tools\nPlace both functions in the nexus_actions folder, and they will be automatically dis-\ncovered. Adding the agent_action decorator allows the functions to be inspected and\nautomatically generates the OpenAI standard tool specification. The LLM can then\nuse this tool specification for tool use and function calling.\n Listing 7.12 shows the generated OpenAI tool specification for both functions, as\nshown previously in listing 7.11. The semantic function, which uses a prompt, also\napplies to the tool description. This tool description is sent to the LLM to determine\nwhich function to call.\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \n        \"Get the current weather in a given location\",   \n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {     \n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"location\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                        \"celsius\",\n                        \"fahrenheit\"\n                    ]\n                }\n            },\n            \"required\": [\n                \"location\"\n            ]\n        }\n    }\n}\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"recommend\",\n        \"description\": \"\"\"\n    System:\n    Provide a recommendation for a given {{topic}}.\nUse your best judgment to provide a recommendation.\nUser:\nplease use your best judgment\nto provide a recommendation for {{topic}}.\"\"\",     \n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {      \nListing 7.12\ntest_actions: OpenAI-generated tool specifications\nThe function \ncomment becomes \nthe function tool \ndescription.\nThe input parameters of the \nfunction are extracted and \nadded to the specification.\nThe function \ncomment becomes \nthe function tool \ndescription.\nThe input parameters of the \nfunction are extracted and \nadded to the specification.\n\n\n176\nCHAPTER 7\nAssembling and using an agent platform\n                \"topic\": {\n                    \"type\": \"string\",\n                    \"description\": \"topic\"\n                }\n            },\n            \"required\": [\n                \"topic\"\n            ]\n        }\n    }\n}\nThe agent engine also needs to implement that capability to implement functions and\nother components. The OpenAI agent has been implemented to support parallel\nfunction calling. Other agent engine implementations will be required to support\ntheir respective versions of action use. Fortunately, the definition of the OpenAI tool\nis becoming the standard, and many platforms adhere to this standard.\n Before we dive into a demo on tool use, let\u2019s observe how the OpenAI agent\nimplements actions by opening the oai_agent.py file in VS Code. The following list-\ning shows the top of the agent\u2019s get_response_stream function and its implementa-\ntion of function calling.\ndef get_response_stream(self, user_input, thread_id=None):\n    self.last_message = \"\"\n    self.messages += [{\"role\": \"user\", \"content\": user_input}]\n    if self.tools and len(self.tools) > 0:   \n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=self.messages,\n            tools=self.tools,     \n            tool_choice=\"auto\",     \n        )\n    else:    \n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=self.messages,\n        )\n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls    \nExecuting the functions follows, as shown in listing 7.14. This code demonstrates how\nthe agent supports parallel function/tool calls. These calls are parallel because the\nagent executes each one together and in no order. In chapter 11, we\u2019ll look at plan-\nners that allow actions to be called in ordered sequences.\n \n \nListing 7.13\nCaling the API in get_response_stream\nDetects whether \nthe agent has \nany available \ntools turned on\nSets the tools in the \nchat completions call\nEnsures that the \nLLM knows it can \nchoose any tool\nIf no tools, calls the \nLLM the standard way\nDetects whether there \nwere any tools used by \nthe LLM\n\n\n177\n7.5\nGiving an agent actions and tools\nif tool_calls:    \n    available_functions = {\n        action[\"name\"]: action[\"pointer\"] for action in self.actions\n    }    \n    self.messages.append(\n        response_message\n    )\n    for tool_call in tool_calls:    \n        function_name = tool_call.function.name\n        function_to_call = available_functions[function_name]\n        function_args = json.loads(tool_call.function.arguments)\n        function_response = function_to_call(\n            **function_args, _caller_agent=self\n        )\n        self.messages.append(\n            {\n                \"tool_call_id\": tool_call.id,\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": str(function_response),\n            }\n        )\n    second_response = self.client.chat.completions.create(\n        model=self.model,\n        messages=self.messages,\n    )     \n    response_message = second_response.choices[0].message\nTo demo this, start up Nexus in the debugger by pressing F5. Then, select the two test\nactions\u2014recommend and get_current_weather\u2014and the terse persona/profile Olly.\nFigure 7.9 shows the result of entering a query and the agent responding by using\nboth tools in its response.\n If you need to review how these agent actions work in more detail, refer to chap-\nter 5. The underlying code is more complex and out of the scope of review here.\nHowever, you can review the Nexus code to gain a better understanding of how\neverything connects.\n Now, you can continue exercising the various agent options within Nexus. Try\nselecting different profiles/personas with other functions, for example. In the next\nchapter, we unveil how agents can consume external memory and knowledge using\npatterns such as Retrieval Augmented Generation (RAG).\n \n \n \nListing 7.14\noai_agent.py (get_response_stream: execute tool calls)\nProceeds if tool calls are detected \nin the LLM response\nLoads pointers to the actual function \nimplementations for code execution\nLoops through \nall the calls the \nLLM wants to \ncall; there can \nbe several.\nPerforms a second \nLLM call with the \nresults of the tool \ncalls\n\n\n178\nCHAPTER 7\nAssembling and using an agent platform\n7.6\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Explore Streamlit Basics (Easy)\nObjective\u2014Gain familiarity with Streamlit by creating a simple web application\nthat displays text input by the user.\nTasks: \n\u2013 Follow the Streamlit documentation to set up a basic application. \n\u2013 Add a text input and a button. When the button is clicked, display the text\nentered by the user on the screen.\n\uf0a1Exercise 2\u2014Create a Basic Agent Profile\nObjective\u2014Understand the process of creating and applying agent profiles in\nNexus.\nTasks: \n\u2013 Create a new agent profile with a unique persona. This persona should have\na specific theme or characteristic (e.g., a historian). \n\u2013 Define a basic set of responses that align with this persona. \n\u2013 Test the persona by interacting with it through the Nexus interface.\nThe agent answered in a\nterse manner, and we can see\nthat both actions were used.\nSelect the terse agent\npro\ufb01le called Olly.\n.\nSelect the test actions ecommend\nR\nand get_current_weather. Currently,\nthe agent pro\ufb01le does not restrict\naction selection.\nFigure 7.9\nHow the agent can use tools in parallel and respond with a single response\n\n\n179\nSummary\n\uf0a1Exercise 3\u2014Develop a Custom Action\nObjective\u2014Learn to extend the functionality of Nexus by developing a custom\naction.\nTasks: \n\u2013 Develop a new action (e.g., fetch_current_news) that integrates with a\nmock API to retrieve the latest news headlines. \n\u2013 Implement this action as both a native (code) function and a semantic\n(prompt-based) function. \n\u2013 Test the action in the Nexus environment to ensure it works as expected.\n\uf0a1Exercise 4\u2014Integrate a Third-Party API\nObjective\u2014Enhance the capabilities of a Nexus agent by integrating a real\nthird-party API.\nTasks:\n\u2013 Choose a public API (e.g., weather or news API), and create a new action\nthat fetches data from this API. \n\u2013 Incorporate error handling and ensure that the agent can gracefully handle\nAPI failures or unexpected responses. \n\u2013 Test the integration thoroughly within Nexus.\nSummary\n\uf0a1Nexus is an open source agent development platform used in conjunction with\nthis book. It\u2019s designed to develop, test, and host AI agents and is built on\nStreamlit for creating interactive dashboards and chat interfaces.\n\uf0a1Streamlit, a Python web application framework, enables the rapid development\nof user-friendly dashboards and chat applications. This framework facilitates the\nexploration and interaction with various agent features in a streamlined manner.\n\uf0a1Nexus supports creating and customizing agent profiles and personas, allowing\nusers to define their agents\u2019 personalities and behaviors. These profiles dictate\nhow agents interact with and respond to user inputs.\n\uf0a1The Nexus platform allows for developing and integrating semantic (prompt-\nbased) and native (code-based) actions and tools within agents. This enables\nthe creation of highly functional and responsive agents.\n\uf0a1As an open source platform, Nexus is designed to be extensible, encouraging\ncontributions and the addition of new features, tools, and agent capabilities by\nthe community.\n\uf0a1Nexus is flexible, supporting various deployment options, including a web inter-\nface, API, and a Discord bot in future iterations, accommodating a wide range\nof development and testing needs.\n\n\n180\nUnderstanding agent\nmemory and knowledge\nNow that we\u2019ve explored agent actions using external tools, such as plugins in the\nform of native or semantic functions, we can look at the role of memory and knowl-\nedge using retrieval in agents and chat interfaces. We\u2019ll describe memory and\nknowledge and how they relate to prompt engineering strategies, and then, to under-\nstand memory knowledge, we\u2019ll investigate document indexing, construct retrieval\nsystems with LangChain, use memory with LangChain, and build semantic memory\nusing Nexus.\nThis chapter covers\n\uf0a1Retrieval in knowledge/memory in AI functions\n\uf0a1Building retrieval augmented generation \nworkflows with LangChain\n\uf0a1Retrieval augmented generation for agentic \nknowledge systems in Nexus\n\uf0a1Retrieval patterns for memory in agents\n\uf0a1Improving augmented retrieval systems with \nmemory and knowledge compression \n\n\n181\n8.1\nUnderstanding retrieval in AI applications\n8.1\nUnderstanding retrieval in AI applications\nRetrieval in agent and chat applications is a mechanism for obtaining knowledge to\nkeep in storage that is typically external and long-lived. Unstructured knowledge\nincludes conversation or task histories, facts, preferences, or other items necessary for\ncontextualizing a prompt. Structured knowledge, typically stored in databases or files,\nis accessed through native functions or plugins.\n Memory and knowledge, as shown in figure 8.1, are elements used to add further\ncontext and relevant information to a prompt. Prompts can be augmented with every-\nthing from information about a document to previous tasks or conversations and\nother reference information.\nThe prompt engineering strategies shown in figure 8.1 can be applied to memory and\nknowledge. Knowledge isn\u2019t considered memory but rather an augmentation of the\nprompt from existing documents. Both knowledge and memory use retrieval as the\nbasis for how unstructured information can be queried.\nProvide Reference Text\nHelps reduce hallucinations.\nTactics involve instructing\nthe model to use or cite\nreference texts.\nKnowledge and Memory\nUse External Tools\nEnhances model capabilities.\nTactics include embeddings-based\nsearch, code execution, and access\nto speci\ufb01c functions.\nActions, Knowledge, Memory\nPrompt Engineering Strategies\nMemory\nRetrieved\nmemories\nDatabase\nVector Store\nInternal Memory\nPrompt\nSave to memory\nLLM\nRetrieved\nknowledge\nRetrieved\nmemory\nRequest\nKnowledge\nVector Store\nRetrieved\nknowledge\nResponse\nMade by a user or\nanother system or\nagent\nRetrieved elements\nprovide references\nand context\nMay include the\nwhole or parts of\nthe conversation\nRetrieval is done\nusing semantic\nsimilarity.\nFigure 8.1\nMemory, retrieval, and augmentation of the prompt using the following prompt engineering \nstrategies: Use External Tools and Provide Reference Text.\n\n\n182\nCHAPTER 8\nUnderstanding agent memory and knowledge\n The retrieval mechanism, called retrieval augmented generation (RAG), has\nbecome a standard for providing relevant context to a prompt. The exact mechanism\nthat powers RAG also powers memory/knowledge, and it\u2019s essential to understand\nhow it works. In the next section, we\u2019ll examine what RAG is.\n8.2\nThe basics of retrieval augmented generation (RAG)\nRAG has become a popular mechanism for supporting document chat or question-\nand-answer chat. The system typically works by a user supplying a relevant document,\nsuch as a PDF, and then using RAG and a large language model (LLM) to query the\ndocument.\n Figure 8.2 shows how RAG can allow a document to be queried using an LLM.\nBefore any document can be queried, it must first be loaded, transformed into con-\ntext chunks, embedded into vectors, and stored in a vector database.\nA user can query previously indexed documents by submitting a query. That query is\nthen embedded into a vector representation to search for similar chunks in the vector\nRetrieval Augmented Generation (RAG)\n(3) Generate\n(1) Retrieve\n(2) Augment\nLLM Chat\nQuery\nResponse\nEmbedding\nLLM\nPrompt\nsystem: you are a ...\nQuery\nContext\nVector DB\nContext\nLLM generates a response\nbased on the contextualized\nprompt.\nRetrieval works\nby using vector\nsimilarity search.\nQuery is\nembedded\nto represent\na vector.\nRetrieved context\nsemantically matches\nthe query.\nEmbedding\nSubmit document\nto query\nTransform\nVector DB\nDocument is loaded,\ntransformed, and\nsplit into chunks.\nChunks of text\nare converted\nto vectors.\nVectors\nrepresenting\nchunks of text\nare stored.\nDocuments are\n\ufb01rst indexed to a\nvector database.\nIndexed documents\ncan be queried/\nquestioned by the\nuser.\nFigure 8.2\nThe two phases of RAG: first, documents must be loaded, transformed, embedded, and stored, and, \nsecond, they can be queried using augmented generation.\n\n\n183\n8.2\nThe basics of retrieval augmented generation (RAG)\ndatabase. Content similar to the query is then used as context and populated into the\nprompt as augmentation. The prompt is pushed to an LLM, which can use the con-\ntext information to help answer the query.\n Unstructured memory/knowledge concepts rely on some format of text-similarity\nsearch following the retrieval pattern shown in figure 8.2. Figure 8.3 shows how mem-\nory uses the same embedding and vector database components. Rather than preload\ndocuments, conversations or parts of a conversation are embedded and saved to a vec-\ntor database.\nThe retrieval pattern and document indexing are nuanced and require careful con-\nsideration to be employed successfully. This requires understanding how data is stored\nand retrieved, which we\u2019ll start to unfold in the next section.\nMemory Retrieval Augmented Generation\n(3) Generate\n(1) Retrieve\nMemory\n(2) Augment\nLLM Chat\nQuery\nResponse\nEmbedding\nLLM\nPrompt\nsystem: you are a ...\nQuery\nMemory\nVector DB\nMemory\nLLM generates a\nresponse based on the\ncontextualized prompt.\nRetrieval works\nby using vector\nsimilarity search.\nRetrieved memory\nsemantically matches\nthe query.\nEmbedding\n(4) Remember\nGenerated\nResponse\nAll or parts of the\nconversation are\nembedded and\nadded to the\nvector database.\nQuery is\nembedded\nto represent\na vector.\nChat with memory\nFigure 8.3\nMemory retrieval for augmented generation uses the same embedding patterns to index items to a \nvector database.\n",
      "page_number": 197
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 208-215)",
      "start_page": 208,
      "end_page": 215,
      "detection_method": "topic_boundary",
      "content": "184\nCHAPTER 8\nUnderstanding agent memory and knowledge\n8.3\nDelving into semantic search and document indexing\nDocument indexing transforms a document\u2019s information to be more easily recov-\nered. How the index will be queried or searched also plays a factor, whether searching\nfor a particular set of words or wanting to match phrase for phrase.\n A semantic search is a search for content that matches the searched phrase by words\nand meaning. The ability to search by meaning, semantically, is potent and worth\ninvestigating in some detail. In the next section, we look at how vector similarity\nsearch can lay the framework for semantic search.\n8.3.1\nApplying vector similarity search\nLet\u2019s look now at how a document can be transformed into a semantic vector, or a repre-\nsentation of text that can then be used to perform distance or similarity matching.\nThere are numerous ways to convert text into a semantic vector, so we\u2019ll look at a sim-\nple one.\n Open the chapter_08 folder in a new Visual Studio Code (VS Code) workspace.\nCreate a new environment and pip install the requirements.txt file for all the\nchapter dependencies. If you need help setting up a new Python environment, con-\nsult appendix B.\n Now open the document_vector_similarity.py file in VS Code, and review the\ntop section in listing 8.1. This example uses Term Frequency\u2013Inverse Document Fre-\nquency (TF\u2013IDF). This numerical statistic reflects how important a word is to a docu-\nment in a collection or set of documents by increasing proportionally to the number\nof times a word appears in the document and offset by the frequency of the word in\nthe document set. TF\u2013IDF is a classic measure of understanding one document\u2019s\nimportance within a set of documents.\nimport plotly.graph_objects as go\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndocuments = [     \n    \"The sky is blue and beautiful.\",\n    \"Love this blue and beautiful sky!\",\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"A king's breakfast has sausages, ham, bacon, eggs, toast, and beans\",\n    \"I love green eggs, ham, sausages and bacon!\",\n    \"The brown fox is quick and the blue dog is lazy!\",\n    \"The sky is very blue and the sky is very beautiful today\",\n    \"The dog is lazy but the brown fox is quick!\"\n]\nvectorizer = TfidfVectorizer()    \nX = vectorizer.fit_transform(documents)     \nListing 8.1\ndocument_vector_similarity (transform to vector)\nSamples of documents\nVectorization \nusing TF\u2013IDF\nVectorize the \ndocuments.\n\n\n185\n8.3\nDelving into semantic search and document indexing\nLet\u2019s break down TF\u2013IDF into its two components using the sample sentence, \u201cThe\nsky is blue and beautiful,\u201d and focusing on the word blue.\nTERM FREQUENCY (TF)\nTerm Frequency measures how frequently a term occurs in a document. Because we\u2019re\nconsidering only a single document (our sample sentence), the simplest form of the\nTF for blue can be calculated as the number of times blue appears in the document\ndivided by the total number of words in the document. Let\u2019s calculate it:\nNumber of times blue appears in the document: 1\nTotal number of words in the document: 6\nTF = 1 \u00f7 6TF = .16\nINVERSE DOCUMENT FREQUENCY (IDF)\nInverse Document Frequency measures how important a term is within the entire corpus.\nIt\u2019s calculated by dividing the total number of documents by the number of docu-\nments containing the term and then taking the logarithm of that quotient:\nIDF = log(Total number of documents \u00f7 Number of documents containing the word)\nIn this example, the corpus is a small collection of eight documents, and blue appears\nin four of these documents.\nIDF = log(8 \u00f7 4)\nTF\u2013IDF CALCULATION\nFinally, the TF\u2013IDF score for blue in our sample sentence is calculated by multiplying\nthe TF and the IDF scores:\nTF\u2013IDF = TF \u00d7 IDF\nLet\u2019s compute the actual values for TF\u2013IDF for the word blue using the example pro-\nvided; first, the term frequency (how often the word occurs in the document) is com-\nputed as follows:\nTF = 1 \u00f7 6\nAssuming the base of the logarithm is 10 (commonly used), the inverse document fre-\nquency is computed as follows:\nIDF = log10 (8 \u00f7 4)\nNow let\u2019s calculate the exact TF\u2013IDF value for the word blue in the sentence, \u201cThe sky\nis blue and beautiful\u201d:\n\n\n186\nCHAPTER 8\nUnderstanding agent memory and knowledge\nThe Term Frequency (TF) is approximately 0.1670.\nThe Inverse Document Frequency (IDF) is approximately 0.301.\nThus, the TF\u2013IDF (TF \u00d7 IDF) score for blue is approximately 0.050.\nThis TF\u2013IDF score indicates the relative importance of the word blue in the given doc-\nument (the sample sentence) within the context of the specified corpus (eight docu-\nments, with blue appearing in four of them). Higher TF\u2013IDF scores imply greater\nimportance. \n We use TF\u2013IDF here because it\u2019s simple to apply and understand. Now that we\nhave the elements represented as vectors, we can measure document similarity using\ncosine similarity. Cosine similarity is a measure used to calculate the cosine of the\nangle between two nonzero vectors in a multidimensional space, indicating how simi-\nlar they are, irrespective of their size.\n Figure 8.4 shows how cosine distance compares the vector representations of two\npieces or documents of text. Cosine similarity returns a value from \u20131 (not similar) to\n1 (identical). Cosine distance is a normalized value ranging from 0 to 2, derived by tak-\ning 1 minus the cosine similarity. A cosine distance of 0 means identical items, and 2\nindicates complete opposites.\nListing 8.2 shows how the cosine similarities are computed using the cosine_similarity\nfunction from scikit-learn. Similarities are calculated for each document against all\nother documents in the set. The computed matrix of similarities for documents is\nstored in the cosine_similarities variable. Then, in the input loop, the user can\nselect the document to view its similarities to the other documents.\n \n \n \nCosine Similarity\nThe angle or distance is a measure of how\nclose the vectors are in space. It also\nrepresents their similarity to each other.\nCosine Distance\n\u03b8\nThe sky is blue and beautiful.\nLove this blue and beautiful sky!\nVector representations of the text rendered in 2D\nand in reality vectors can be highly dimensional.\nFigure 8.4\nHow cosine \nsimilarity is measured\n\n\n187\n8.3\nDelving into semantic search and document indexing\ncosine_similarities = cosine_similarity(X)     \nwhile True:     \n    selected_document_index = input(f\"Enter a document number\n\u27a5 (0-{len(documents)-1}) or 'exit' to quit: \").strip()\n    if selected_document_index.lower() == 'exit':\n        break\n    if not selected_document_index.isdigit() or \n\u27a5 not 0 <= int(selected_document_index) < len(documents):\n        print(\"Invalid input. Please enter a valid document number.\")\n        continue\n    selected_document_index = int(selected_document_index)   \n    selected_document_similarities = \ncosine_similarities[selected_document_index]    \n# code to plot document similarities omitted\nFigure 8.5 shows the output of running the sample in VS Code (F5 for debugging\nmode). After you select a document, you\u2019ll see the similarities between the various\ndocuments in the set. A document will have a cosine similarity of 1 with itself. Note\nthat you won\u2019t see a negative similarity because of the TF\u2013IDF vectorization. We\u2019ll look\nlater at other, more sophisticated means of measuring semantic similarity.\nListing 8.2\ndocument_vector_similarity (cosine similarity)\nComputes the document \nsimilarities for all vector pairs\nThe main \ninput loop\nGets the selected \ndocument index \nto compare with\nExtracts the \ncomputed similarities \nagainst all documents\nThe select document is compared against all\nother documents to show similarity\nmeasure between document vectors.\nCosine Similarities of \u201cThe sky is blue and beautiful.\u201d with Others\nFigure 8.5\nThe cosine similarity between selected documents and the document set\n\n\n188\nCHAPTER 8\nUnderstanding agent memory and knowledge\nThe method of vectorization will dictate the measure of semantic similarity between\ndocuments. Before we move on to better methods of vectorizing documents, we\u2019ll\nexamine storing vectors to perform vector similarity searches.\n8.3.2\nVector databases and similarity search\nAfter vectorizing documents, they can be stored in a vector database for later similar-\nity searches. To demonstrate how this works, we can efficiently replicate a simple vec-\ntor database in Python code.\n Open document_vector_database.py in VS Code, as shown in listing 8.3. This\ncode demonstrates creating a vector database in memory and then allowing users to\nenter text to search the database and return results. The results returned show the\ndocument text and the similarity score.\n# code above omitted\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(documents)\nvector_database = X.toarray()    \ndef cosine_similarity_search(query,\n                             database, \n                             vectorizer, \n                             top_n=5):    \n    query_vec = vectorizer.transform([query]).toarray()\n    similarities = cosine_similarity(query_vec, database)[0]\n    top_indices = np.argsort(-similarities)[:top_n]  # Top n indices\n    return [(idx, similarities[idx]) for idx in top_indices]\nwhile True:     \n    query = input(\"Enter a search query (or 'exit' to stop): \")\n    if query.lower() == 'exit':\n        break\n    top_n = int(input(\"How many top matches do you want to see? \"))\n    search_results = cosine_similarity_search(query,\n                                              vector_database, \n                                              vectorizer, \n                                              top_n)\n    print(\"Top Matched Documents:\")\n    for idx, score in search_results:\n        print(f\"- {documents[idx]} (Score: {score:.4f})\")  \n    print(\"\\n\")\n###Output\nEnter a search query (or 'exit' to stop): blue\nHow many top matches do you want to see? 3\nTop Matched Documents:\n- The sky is blue and beautiful. (Score: 0.4080)\n- Love this blue and beautiful sky! (Score: 0.3439)\n- The brown fox is quick and the blue dog is lazy! (Score: 0.2560)\nListing 8.3\ndocument_vector_database.py\nStores the \ndocument vectors \ninto an array\nThe function to perform \nsimilarity matching on \nquery returns, matches, \nand similarity scores\nThe main \ninput loop\nLoops through \nresults and \noutputs text and \nsimilarity score\n\n\n189\n8.3\nDelving into semantic search and document indexing\nRun this exercise to see the output (F5 in VS Code). Enter any text you like, and see\nthe results of documents being returned. This search form works well for matching\nwords and phrases with similar words and phrases. This form of search misses the\nword context and meaning from the document. In the next section, we\u2019ll look at a way\nof transforming documents into vectors that better preserves their semantic meaning.\n8.3.3\nDemystifying document embeddings\nTF\u2013IDF is a simple form that tries to capture semantic meaning in documents. How-\never, it\u2019s unreliable because it only counts word frequency and doesn\u2019t understand the\nrelationships between words. A better and more modern method uses document\nembedding, a form of document vectorizing that better preserves the semantic mean-\ning of the document.\n Embedding networks are constructed by training neural networks on large data-\nsets to map words, sentences, or documents to high-dimensional vectors, capturing\nsemantic and syntactic relationships based on context and relationships in the data.\nYou typically use a pretrained model trained on massive datasets to embed documents\nand perform embeddings. Models are available from many sources, including Hug-\nging Face and, of course, OpenAI.\n In our next scenario, we\u2019ll use an OpenAI embedding model. These models are\ntypically perfect for capturing the semantic context of embedded documents. Listing\n8.4 shows the relevant code that uses OpenAI to embed the documents into vectors\nthat are then reduced to three dimensions and rendered into a plot.\nload_dotenv()                                                            \napi_key = os.getenv('OPENAI_API_KEY')\nif not api_key:\n    raise ValueError(\"No API key found. Please check your .env file.\")\nclient = OpenAI(api_key=api_key)                                         \ndef get_embedding(text, model=\"text-embedding-ada-002\"):    \n    text = text.replace(\"\\n\", \" \")\n    return client.embeddings.create(input=[text],\n              model=model).data[0].embedding                \n# Sample documents (omitted)\nembeddings = [get_embedding(doc) for doc in documents]   \nprint(embeddings_array.shape)\nembeddings_array = np.array(embeddings)   \npca = PCA(n_components=3)  \nreduced_embeddings = pca.fit_transform(embeddings_array)\nListing 8.4\ndocument_visualizing_embeddings.py (relevant sections)\nJoin all the items on the string ', '.\nUses the OpenAI \nclient to create \nthe embedding\nGenerates embeddings \nfor each document of \nsize 1536 dimensions\nConverts embeddings to \na NumPy array for PCA\nApplies PCA to \nreduce dimensions \nto 3 for plotting\n\n\n190\nCHAPTER 8\nUnderstanding agent memory and knowledge\nWhen a document is embedded using an OpenAI model, it transforms the text into a\nvector with dimensions of 1536. We can\u2019t visualize this number of dimensions, so we\nuse a dimensionality reduction technique via principal component analysis (PCA) to\nconvert the vector of size 1536 to 3 dimensions.\n Figure 8.6 shows the output generated from running the file in VS Code. By reduc-\ning the embeddings to 3D, we can plot the output to show how semantically similar\ndocuments are now grouped.\nThe choice of which embedding model or service you use is up to you. The OpenAI\nembedding models are considered the best for general semantic similarity. This has\nmade these models the standard for most memory and retrieval applications. With\nour understanding of how text can be vectorized with embeddings and stored in a vec-\ntor database, we can move on to a more realistic example in the next section.\n8.3.4\nQuerying document embeddings from Chroma\nWe can combine all the pieces and look at a complete example using a local vector\ndatabase called Chroma DB. Many vector database options exist, but Chroma DB is an\nexcellent local vector store for development or small-scale projects. There are also\nplenty of more robust options that you can consider later.\nSimilar documents are now similar in\nmeaning and are shown grouped together.\nDocuments are projected to 3D\nbased on their semantic meaning.\nFigure 8.6\nEmbeddings in 3D, showing how similar semantic documents are grouped\n\n\n191\n8.3\nDelving into semantic search and document indexing\n Listing 8.5 shows the new and relevant code sections from the document_query_\nchromadb.py file. Note that the results are scored by distance and not by similarity.\nCosine distance is determined by this equation:\nCosine Distance(A,B) = 1 \u2013 Cosine Similarity(A,B)\nThis means that cosine distance will range from 0 for most similar to 2 for semanti-\ncally opposite in meaning.\nembeddings = [get_embedding(doc) for doc in documents]    \nids = [f\"id{i}\" for i in range(len(documents))]           \nchroma_client = chromadb.Client()              \ncollection = chroma_client.create_collection(\n                       name=\"documents\")       \ncollection.add(    \n    embeddings=embeddings,\n    documents=documents,\n    ids=ids\n)\ndef query_chromadb(query, top_n=2):     \n    query_embedding = get_embedding(query)\n    results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=top_n\n    )\n    return [(id, score, text) for id, score, text in\n            zip(results['ids'][0],\n                results['distances'][0], \n                results['documents'][0])]\nwhile True:    \n    query = input(\"Enter a search query (or 'exit' to stop): \")\n    if query.lower() == 'exit':\n        break\n    top_n = int(input(\"How many top matches do you want to see? \"))\n    search_results = query_chromadb(query, top_n)\n    print(\"Top Matched Documents:\")\n    for id, score, text in search_results:\n        print(f\"\"\"\nID:{id} TEXT: {text} SCORE: {round(score, 2)}\n\"\"\")    \n    print(\"\\n\")\n###Output\nEnter a search query (or 'exit' to stop): dogs are lazy\nHow many top matches do you want to see? 3\nTop Matched Documents:\nID:id7 TEXT: The dog is lazy but the brown fox is quick! SCORE: 0.24\nListing 8.5\ndocument_query_chromadb.py (relevant code sections)\nGenerates embeddings \nfor each document and \nassigns an ID\nCreates a Chroma DB \nclient and a collection\nAdds document \nembeddings to \nthe collection\nQueries the datastore \nand returns the top n \nrelevant documents\nThe input loop for\nuser input and output of\nrelevant documents/scores\n",
      "page_number": 208
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 216-223)",
      "start_page": 216,
      "end_page": 223,
      "detection_method": "topic_boundary",
      "content": "192\nCHAPTER 8\nUnderstanding agent memory and knowledge\nID:id5 TEXT: The brown fox is quick and the blue dog is lazy! SCORE: 0.28\nID:id2 TEXT: The quick brown fox jumps over the lazy dog. SCORE: 0.29\nAs the earlier scenario demonstrated, you can now query the documents using seman-\ntic meaning rather than just key terms or phrases. These scenarios should now pro-\nvide the background to see how the retrieval pattern works at a low level. In the next\nsection, we\u2019ll see how the retrieval pattern can be employed using LangChain.\n8.4\nConstructing RAG with LangChain\nLangChain began as an open source project specializing in abstracting the retrieval\npattern across multiple data sources and vector stores. It has since morphed into\nmuch more, but foundationally, it still provides excellent options for implementing\nretrieval.\n Figure 8.7 shows a diagram from LangChain that identifies the process of storing\ndocuments for retrieval. These same steps may be replicated in whole or in part to\nimplement memory retrieval. The critical difference between document and memory\nretrieval is the source and how content is transformed.\nWe\u2019ll examine how to implement each of these steps using LangChain and under-\nstand the nuances and details accompanying this implementation. In the next section,\nwe\u2019ll start by splitting and loading documents with LangChain.\n8.4.1\nSplitting and loading documents with LangChain\nRetrieval mechanisms augment the context of a given prompt with specific informa-\ntion relevant to the request. For example, you may request detailed information about\nLangChain provides a\nplugin architecture that\nsupports document import\nfrom many sources.\nTransform breaks\nthe document\ndown into relevant\nsections or chunks.\nEmbed or\nembeddings\nbreak the chunks\ninto vectors.\nLangChain supports\nmultiple options for\nvector stores.\nFigure 8.7\nLoad, transform, embed, and store steps in storing documents for later retrieval\n\n\n193\n8.4\nConstructing RAG with LangChain\na local document. With earlier language models, submitting the whole document as\npart of the prompt wasn\u2019t an option due to token limitations.\n Today, we could submit a whole document for many commercial LLMs, such as\nGPT-4 Turbo, as part of a prompt request. However, the results may not be better and\nwould likely cost more because of the increased number of tokens. Therefore, a better\noption is to split the document and use the relevant parts to request context\u2014pre-\ncisely what RAG and memory do.\n Splitting a document is essential in breaking down content into semantically and\nspecifically relevant sections. Figure 8.8 shows how to break down an HTML docu-\nment containing the Mother Goose nursery rhymes. Often, splitting a document into\ncontextual semantic chunks requires careful consideration.\nIdeally, when we split documents into chunks, they are broken down by relevance and\nsemantic meaning. While an LLM or agent could help us with this, we\u2019ll look at cur-\nrent toolkit options within LangChain for splitting documents. Later in this chapter,\nwe\u2019ll look at a semantic function that can assist us in semantically dividing content for\nembeddings.\n For the next exercise, open langchain_load_splitting.py in VS Code, as shown\nin listing 8.6. This code shows where we left off from listing 8.5, in the previous sec-\ntion. Instead of using the sample documents, we\u2019re loading the Mother Goose nursery\nrhymes this time.\n \n \n \nSplit the documents into chunks.\nCreate\nembeddings\nStore\nLoad the document(s).\nIdeally, the chunks are\nsemantically relevant\nand speci\ufb01c.\nFigure 8.8\nHow the document would ideally be split into chunks for better semantic and contextual meaning\n\n\n194\nCHAPTER 8\nUnderstanding agent memory and knowledge\nFrom langchain_community.document_loaders \n                     \u27a5 import UnstructuredHTMLLoader   \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n#previous code\nloader = UnstructuredHTMLLoader(\n                   \"sample_documents/mother_goose.html\")  \ndata = loader.load   \ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=100,\n    chunk_overlap=25,    \n    length_function=len,\n    add_start_index=True,\n)\ndocuments = text_splitter.split_documents(data)\ndocuments = [doc.page_content \n                \u27a5 for doc in documents] [100:350]  \nembeddings = [get_embedding(doc) for doc in documents]    \nids = [f\"id{i}\" for i in range(len(documents))]\n###Output\nEnter a search query (or 'exit' to stop): who kissed the girls and made \nthem cry?\nHow many top matches do you want to see? 3\nTop Matched Documents:\nID:id233 TEXT: And chid her daughter,\n        And kissed my sister instead of me. SCORE: 0.4\u2026\nNote in listing 8.6 that the HTML document gets split into 100-character chunks with\na 25-character overlap. The overlap allows the document\u2019s parts not to cut off specific\nthoughts. We selected the splitter for this exercise because it was easy to use, set up,\nand understand.\n Go ahead and run the langchain_load_splitting.py file in VS Code (F5). Enter\na query, and see what results you get. The output in listing 8.6 shows good results\ngiven a specific example. Remember that we only embedded 250 document chunks to\nreduce costs and keep the exercise short. Of course, you can always try to embed the\nentire document or use a minor input document example.\n Perhaps the most critical element to building proper retrieval is the process of\ndocument splitting. You can use numerous methods to split a document, including\nmultiple concurrent methods. More than one method passes and splits the docu-\nment for numerous embedding views of the same document. In the next section,\nwe\u2019ll examine a more general technique for splitting documents, using tokens and\ntokenization.\nListing 8.6\nlangchain_load_splitting.py (sections and output)\nNew LangChain \nimports\nLoads the \ndocument \nas HTML\nLoads the \ndocument\nSplits the document into blocks of \ntext 100 characters long with a \n25-character overlap\nEmbeds only 250 \nchunks, which is \ncheaper and faster\nReturns the \nembedding for \neach document\n\n\n195\n8.4\nConstructing RAG with LangChain\n8.4.2\nSplitting documents by token with LangChain\nTokenization is the process of breaking text into word tokens. Where a word token rep-\nresents a succinct element in the text, a token could be a word like hold or even a sym-\nbol like the left curly brace ({), depending on what\u2019s relevant.\n Splitting documents using tokenization provides a better base for how the text will\nbe interpreted by language models and for semantic similarity. Tokenization also\nallows the removal of irrelevant characters, such as whitespace, making the similarity\nmatching of documents more relevant and generally providing better results.\n For the next code exercise, open the langchain_token_splitting.py file in VS\nCode, as shown in listing 8.7. Now we split the document using tokenization, which\nbreaks the document into sections of unequal size. The unequal size results from the\nlarge sections of whitespace of the original document.\nloader = UnstructuredHTMLLoader(\"sample_documents/mother_goose.html\")\ndata = loader.load()\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=50, chunk_overlap=10     \n)\ndocuments = text_splitter.split_documents(data)\ndocuments = [doc for doc in documents][8:94]     \ndb = Chroma.from_documents(documents, OpenAIEmbeddings())\ndef query_documents(query, top_n=2):\n    docs = db.similarity_search(query, top_n)     \n    return docs\n###Output\nCreated a chunk of size 68, \nwhich is longer than the specified 50\nCreated a chunk of size 67, \nwhich is longer than the specified 50    \nEnter a search query (or 'exit' to stop): \n                     who kissed the girls and made them cry?\nHow many top matches do you want to see? 3\nTop Matched Documents:\nDocument 1: GEORGY PORGY\n        Georgy Porgy, pudding and pie,\n        Kissed the girls and made them cry.\nRun the langchain_token_splitting.py code in VS Code (F5). You can use the\nquery we used last time or your own. Notice how the results are significantly better\nthan the previous exercise. However, the results are still suspect because the query\nuses several similar words in the same order.\n A better test would be to try a semantically similar phrase but one that uses differ-\nent words and check the results. With the code still running, enter a new phrase to\nListing 8.7\nlangchain_token_splitting.py (relevant new code)\nUpdates to 50 tokens \nand overlap of 10 \ntokens\nSelects just the \ndocuments that \ncontain rhymes\nUses the database\u2019s \nsimilarity search\nBreaks into irregular \nsize chunks because of \nthe whitespace\n\n\n196\nCHAPTER 8\nUnderstanding agent memory and knowledge\nquery: Why are the girls crying? Listing 8.8 shows the results of executing that\nquery. If you run this example yourself and scroll down over the output, you\u2019ll see\nGeorgy Porgy appear in either the second or third returned document.\nEnter a search query (or 'exit' to stop): Who made the girls cry?\nHow many top matches do you want to see? 3\nTop Matched Documents:\nDocument 1: WILLY, WILLY\n        Willy, Willy Wilkin\u2026\nThis exercise shows how various retrieval methods can be employed to return docu-\nments semantically. With this base established, we can see how RAG can be applied to\nknowledge and memory systems. The following section will discuss RAG as it applies\nto knowledge of agents and agentic systems.\n8.5\nApplying RAG to building agent knowledge\nKnowledge in agents encompasses employing RAG to search semantically across\nunstructured documents. These documents could be anything from PDFs to Micro-\nsoft Word documents and all text, including code. Agentic knowledge also includes\nusing unstructured documents for Q&A, reference lookup, information augmenta-\ntion, and other future patterns.\n Nexus, the agent platform developed in tandem with this book and introduced in\nthe previous chapter, employs complete knowledge and memory systems for agents.\nIn this section, we\u2019ll uncover how the knowledge system works.\n To install Nexus for just this chapter, see listing 8.9. Open a terminal within the\nchapter_08 folder, and execute the commands in the listing to download, install,\nand run Nexus in normal or development mode. If you want to refer to the code,\nyou should install the project in development and configure the debugger to run\nthe Streamlit app from VS Code. Refer to chapter 7 if you need a refresher on any of\nthese steps.\n# to install and run\npip install git+https://github.com/cxbxmxcx/Nexus.git\nnexus run\n# install in development mode\ngit clone https://github.com/cxbxmxcx/Nexus.git\n# Install the cloned repository in editable mode\npip install -e Nexus\nListing 8.8\nQuery: Who made the girls cry?\nListing 8.9\nInstalling Nexus \n\n\n197\n8.5\nApplying RAG to building agent knowledge\nRegardless of which method you decide to run the app in after you log in, navigate to\nthe Knowledge Store Manager page, as shown in figure 8.9. Create a new Knowledge\nStore, and then upload the sample_documents/back_to_the_future.txt movie script. \nThe script is a large document, and it may take a while to load, chunk, and embed the\nparts into the Chroma DB vector database. Wait for the indexing to complete, and\nthen you can inspect the embeddings and run a query, as shown in figure 8.10.\n Now, we can connect the knowledge store to a supported agent and ask questions.\nUse the top-left selector to choose the chat page within the Nexus interface. Then,\nselect an agent and the time_travel knowledge store, as shown in figure 8.11. You\nwill also need to select an agent engine that supports knowledge. Each of the multiple\nagent engines requires the proper configuration to be accessible.\n Currently, as of this chapter, Nexus supports access to only a single knowledge\nstore at a time. In a future version, agents may be able to select multiple knowledge\nstores at a time. This may include more advanced options, from semantic knowl-\nedge to employing other forms of RAG.\n \nSelect the\nknowledge store.\nCreate a new\nknowledge store.\nDrag and drop or browse and select the\nsample_documents/back_to_the_future.txt\nmovie script here.\nFigure 8.9\nAdding a new knowledge store and populating it with a document\n\n\n198\nCHAPTER 8\nUnderstanding agent memory and knowledge\nPlot of embeddings shown in 3D\nSelect to view all the\nembeddings in the\nknowledge store.\nEnter text to query\nand see the results of\nthe top \ufb01ve chunks\ndisplayed.\nSelect to query the\ndocument embeddings\nin the knowledge store.\nFigure 8.10\nThe embeddings and document query views\nThe agent will reply given\nthe chosen persona.\nEnter a question about\nthe script you would\nlike to ask.\nBe sure to select the\nknowledge store.\nSelect an agent engine\nthat supports knowledge.\nFigure 8.11\nEnabling the knowledge store for agent use\n\n\n199\n8.5\nApplying RAG to building agent knowledge\nYou can also configure the RAG settings within the Configuration tab of the Knowl-\nedge Store Manager page, as shown in figure 8.12. As of now, you can select from the\ntype of splitter (Chunking Option field) to chunk the document, along with the\nChunk Size field and Overlap field. \nThe loading, splitting, chunking, and embedding options provided are the only basic\noptions supported by LangChain for now. In future versions of Nexus, more options\nand patterns will be offered. The code to support other options can be added directly\nto Nexus.\n We won\u2019t cover the code that performs the RAG as it\u2019s very similar to what we\nalready covered. Feel free to review the Nexus code, particularly the KnowledgeManager\nclass in the knowledge_manager.py file. \nRepresents the minimum\nsize in characters or tokens\nto chunk the data\nSelect the\nCon\ufb01guration tab.\nAllows for some overlap\nof text from one chunk\nto the next\nRepresents the type of\ndocument splitter used to extract\nchunks from the document\nFigure 8.12\nManaging the knowledge store splitting and chunking options\n",
      "page_number": 216
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 224-232)",
      "start_page": 224,
      "end_page": 232,
      "detection_method": "topic_boundary",
      "content": "200\nCHAPTER 8\nUnderstanding agent memory and knowledge\n While the retrieval patterns for knowledge and memory are quite similar for aug-\nmentation, the two patterns differ when it comes to populating the stores. In the next\nsection, we\u2019ll explore what makes memory in agents unique.\n8.6\nImplementing memory in agentic systems\nMemory in agents and AI applications is often described in the same terms as cogni-\ntive memory functions. Cognitive memory describes the type of memory we use to\nremember what we did 30 seconds ago or how tall we were 30 years ago. Computer\nmemory is also an essential element of agent memory, but one we won\u2019t consider in\nthis section.\n Figure 8.13 shows how memory is broken down into sensory, short-term, and long-\nterm memory. This memory can be applied to AI agents, and this list describes how\neach form of memory maps to agent functions:\n\uf0a1Sensory memory in AI\u2014Functions such as RAG but with images/audio/haptic\ndata forms. Briefly holds input data (e.g., text and images) for immediate pro-\ncessing but not long-term storage. \n\uf0a1Short-term/working memory in AI\u2014Acts as an active memory buffer of conversa-\ntion history. We\u2019re holding a limited amount of recent input and context for\nimmediate analysis and response generation. Within Nexus, short- and long-\nterm conversational memory is also held in the context of the thread.\n\uf0a1Long-term memory in AI\u2014Longer-term memory storage relevant to the agent\u2019s or\nuser\u2019s life. Semantic memory provides a robust capacity to store and retrieve rel-\nevant global or local facts and concepts. \nVisual memory\nSensory memory\nShort-term memory (contextual memory)\nLong-term memory\nMemory\nIconic memory (visual)\nEchoic memory (auditory)\nHaptic memory (touch)\nExplicit/declarative memory\n(conscious)\nImplicit/procedural memory (unconscious skills)\nEpisodic memory\n(life events)\nSemantic memory\n(facts, concepts)\nConversational\nmemory and RAG\nThis is an area of\nmemory of most\ninterest to agents.\nImplicit memory and skills can also be\nconveyed as actions and tools for agents.\nFigure 8.13\nHow memory is broken down into various forms\n\n\n201\n8.6\nImplementing memory in agentic systems\nWhile memory uses the exact same retrieval and augmentation mechanisms as knowl-\nedge, it typically differs significantly when updating or appending memories. Figure 8.14\nhighlights the process of capturing and using memories to augment prompts. Because\nmemories are often different from the size of complete documents, we can avoid\nusing any splitting or chunking mechanisms.\nNexus provides a mechanism like the knowledge store, allowing users to create mem-\nory stores that can be configured for various uses and applications. It also supports\nsome of the more advanced memory forms highlighted in figure 8.13. The following\nsection will examine how basic memory stores work in Nexus.\nAdding memories\nBasic memory augmentation\nEmbedding\nVector DB\nStored\nembeddings\nand text\nNew memories\ncan come from a\nconversation or\ncan be populated\nas a set of beginning\nfacts, preferences,\nand so on.\nVector DB\nConverted to embedding\nand then used to query database\nAugmented prompt\nuser:\nWhat movie should\nI watch?\nRemembered facts:\nUser likes time\ntravel movies\nLLM\nResponse sent\nback to user\nAssistant:\nYou should watch\nthis movie on\ntime travel.\nUser likes time\ntravel movies\nUser likes sci-\ufb01\nmovies\nConversations are then fed back into the memory function\nto create new memories.\nUser likes time\ntravel movies\nUser likes sci-\ufb01\nmovies\nLLM\nUser:\n[I like time\ntravel movies]\nI like time travel\nmovies\nMemory function,\nextracts relevant\nmemories\nEmbedding\nUser:\nWhat movie\nshould I watch?\nWhat movie should\nI watch?\nFigure 8.14\nBasic memory retrieval and augmentation workflow\n\n\n202\nCHAPTER 8\nUnderstanding agent memory and knowledge\n8.6.1\nConsuming memory stores in Nexus\nMemory stores operate and are constructed like knowledge stores in Nexus. They\nboth heavily rely on the retrieval pattern. What differs is the extra steps memory sys-\ntems take to build new memories.\n Go ahead and start Nexus, and refer to listing 8.9 if you need to install it. After\nlogging in, select the Memory page, and create a new memory store, as shown in fig-\nure 8.15. Select an agent engine, and then add a few personal facts and preferences\nabout yourself.\nThe reason we need an agent (LLM) was shown in figure 8.14 earlier. When informa-\ntion is fed into a memory store, it\u2019s generally processed through an LLM using a memory\nCreate a new memory\nstore called my_memory.\nSelect the memory\nyou want to inspect.\nYou will need to select an agent\nengine to process the memory.\nSelect the\nMemory page.\nAdd a few facts or\npreferences about yourself.\nFigure 8.15\nAdding memories to a newly created memory store\n\n\n203\n8.6\nImplementing memory in agentic systems\nfunction, whose purpose is to process the statements/conversations into semantically\nrelevant information related to the type of memory.\n Listing 8.10 shows the conversational memory function used to extract information\nfrom a conversation into memories. Yes, this is just the header portion of the prompt\nsent to the LLM, instructing it how to extract information from a conversation. \nSummarize the conversation and create a set of statements that summarize \nthe conversation. Return a JSON object with the following keys: 'summary'. \nEach key should have a list of statements that are relevant to that \ncategory. Return only the JSON object and nothing else.\nAfter you generate a few relevant memories about yourself, return to the Chat area in\nNexus, enable the my_memory memory store, and see how well the agent knows you.\nFigure 8.16 shows a sample conversation using a different agent engine. \nThis is an example of a basic memory pattern that extracts facts/preferences from\nconversations and stores them in a vector database as memories. Numerous other\nimplementations of memory follow those displayed earlier in figure 8.13. We\u2019ll imple-\nment those in the next section.\nListing 8.10\nConversational memory function\nSelect the memory store.\nIf possible, select a different agent\nengine that supports memory.\nAsk the agent something relevant to\nthe facts you just added to memory.\nFigure 8.16\nConversing with a different agent on the same memory store\n\n\n204\nCHAPTER 8\nUnderstanding agent memory and knowledge\n8.6.2\nSemantic memory and applications to semantic, episodic, \nand procedural memory\nPsychologists categorize memory into multiple forms, depending on what informa-\ntion is remembered. Semantic, episodic, and procedural memory all represent differ-\nent types of information. Episodic memories are about events, procedural memories are\nabout the process or steps, and semantic represents the meaning and could include\nfeelings or emotions. Other forms of memory (geospatial is another), aren\u2019t described\nhere but could be. \n Because these memories rely on an additional level of categorization, they also rely\non another level of semantic categorization. Some platforms, such as Semantic Kernel\n(SK), refer to this as semantic memory. This can be confusing because semantic categori-\nzation is also applied to extract episodic and procedural memories.\n Figure 8.17 shows the semantic memory categorization process, also sometimes\ncalled semantic memory. The difference between semantic memory and regular mem-\nory is the additional step of processing the input semantically and extracting relevant\nquestions that can be used to query the memory-relevant vector database.\nSemantic memory augmentation\nVector DB\nAugmented prompt\nuser:\nWhat movie should I\nwatch?\nSemantics:\nThe user has recently\nwatched these time\ntravel movies.\nResponse sent\nback to user\nAssistant:\nYou should watch\nthis movie on time\ntravel.\nConversations are then\nfed back into the memory\nfunction to create relevant\nnew memories.\nQuestions are converted\nto embedding and then\nused to query database.\nThe user has recently\nwatched these time\ntravel movies.\nEmbedding\nUser:\nWhat movie\nshould I watch?\nWhat movie should\nI watch?\nSemantic Augmentation\nfunction extracts details\nspeci\ufb01c to memory form.\nWhat type of movies\ndoes the user like?\nGive me a summary\nof movies the user\nhas watched.\nThe semantic\naugmentation\nconverts the input\ninto questions\nrelevant to the\nparticular form of\nmemory.\nLLM\nLLM\nFigure 8.17\nHow semantic memory augmentation works\n\n\n205\n8.6\nImplementing memory in agentic systems\nThe benefit of using semantic augmentation is the increased ability to extract more\nrelevant memories. We can see this in operation by jumping back into Nexus and cre-\nating a new semantic memory store.\n Figure 8.18 shows how to configure a new memory store using semantic memory.\nAs of yet, you can\u2019t configure the specific function prompts for memory, augmenta-\ntion, and summarization. However, it can be useful to read through each of the func-\ntion prompts to gain a sense of how they work.\nMemory function for adding new memories\nAugmentation function is called to extract\nrelevance before querying a memory store.\nSelect SEMANTIC as the type of memory.\nSelect the\nCon\ufb01guration tab.\nBe sure to create a new\nmemory store \ufb01rst.\nSummarization function is used in memory\ncompression.\nFigure 8.18\nConfiguration for changing the memory store type to semantic\n\n\n206\nCHAPTER 8\nUnderstanding agent memory and knowledge\nNow, if you go back and add facts and preferences, they will convert to the semantics\nof the relevant memory type. Figure 8.19 shows an example of memories being popu-\nlated for the same set of statements into two different forms of memory. Generally, the\nstatements entered into memory would be more specific to the form of memory.\nMemory and knowledge can significantly assist an agent with various application types.\nIndeed, a single memory/knowledge store could feed one or multiple agents, allowing\nFigure 8.19\nComparing memories for the same information given two different memory types\n\n\n207\n8.7\nUnderstanding memory and knowledge compression\nfor further specialized interpretations of both types of stores. We\u2019ll finish out the\nchapter by discussing memory/knowledge compression next.\n8.7\nUnderstanding memory and knowledge compression\nMuch like our own memory, memory stores can become cluttered with redundant\ninformation and numerous unrelated details over time. Internally, our minds deal\nwith memory clutter by compressing or summarizing memories. Our minds remem-\nber more significant details over less important ones, and memories accessed more\nfrequently.\n We can apply similar principles of memory compression to agent memory and\nother retrieval systems to extract significant details. The principle of compression is\nsimilar to semantic augmentation but adds another layer to the preclusters groups of\nrelated memories that can collectively be summarized.\n Figure 8.20 shows the process of memory/knowledge compression. Memories or\nknowledge are first clustered using an algorithm such as k-means. Then, the groups of\nmemories are passed through a compression function, which summarizes and collects\nthe items into more succinct representations. \nNexus provides for both knowledge and memory store compression using k-means\noptimal clustering. Figure 8.21 shows the compression interface for memory. Within\nthe compression interface, you\u2019ll see the items displayed in 3D and clustered. The size\n(number of items) of the clusters is shown in the left table.\n Compressing memories and even knowledge is generally recommended if the\nnumber of items in a cluster is large or unbalanced. Each use case for compression\nmay vary depending on the use and application of memories. Generally, though, if an\nMemory/Knowledge Comparison\nCompressed items are\nstored as embeddings.\nMemories and\nknowledge can be\nredundant, repetitive,\nand include duplicate\ninformation and\npotentially misaligned\ninformation.\nClustering\nThe user likes time\ntravel movies.\nThe speaker enjoys\ntime travel movies.\nlikes time travel\nstories\nItems are clustered\nusing reduced dimensional\nembeddings and -means.\nk\nCompression function\nsummarizes items\nby group and\nfunction into a new\nlist of memories and\nknowledge\nEmbedding\nVector DB\nThe user enjoys\ntime travel movies\nand has seen\nthe following \ufb01lms:\nFigure 8.20\nThe process of memory and knowledge compression\n\n\n208\nCHAPTER 8\nUnderstanding agent memory and knowledge\ninspection of the items in a store contains repetitive or duplicate information, it\u2019s a\ngood time for compression. The following is a summary of use cases for applications\nthat would benefit from compression.\nTHE CASE FOR KNOWLEDGE COMPRESSION\nKnowledge retrieval and augmentation have also been shown to benefit significantly\nfrom compression. Results will vary by use case, but generally, the more verbose the\nsource of knowledge, the more it will benefit from compression. Documents that\nfeature literary prose, such as stories and novels, will benefit more than, say, a base\nof code. However, if the code is likewise very repetitive, compression could also be\nshown to be beneficial.\nTHE CASE FOR HOW OFTEN YOU APPLY COMPRESSION\nMemory will often benefit from the periodic compression application, whereas knowl-\nedge stores typically only help on the first load. How frequently you apply compres-\nsion will greatly depend on the memory use, frequency, and quantity.\nTHE CASE FOR APPLYING COMPRESSION MORE THAN ONCE\nMultiple passes of compression at the same time has been shown to improve retrieval\nperformance. Other patterns have also suggested using memory or knowledge at vari-\nous levels of compression. For example, a knowledge store is compressed two times,\nresulting in three different levels of knowledge. \nNumber of items are\nshown per cluster.\nCompression requires an agent\nengine; GPT-4 or higher LLMs\nare preferred for compression.\nThe optimal number of\nk-means clusters are shown.\nClick the Compress button to\nstart compression. The process of\ncompression may take several minutes\ndepending on the size of the store.\nFigure 8.21\nThe interface for compressing memories\n",
      "page_number": 224
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 233-241)",
      "start_page": 233,
      "end_page": 241,
      "detection_method": "topic_boundary",
      "content": "209\n8.8\nExercises\nTHE CASE FOR BLENDING KNOWLEDGE AND MEMORY COMPRESSION\nIf a system is specialized to a particular source of knowledge and that system also\nemploys memories, there may be further optimization to consolidate stores. Another\napproach is to populate memory with the starting knowledge of a document directly.\nTHE CASE FOR MULTIPLE MEMORY OR KNOWLEDGE STORES\nIn more advanced systems, we\u2019ll look at agents employing multiple memory and knowl-\nedge stores relevant to their workflow. For example, an agent could employ individual\nmemory stores as part of its conversations with individual users, perhaps including the\nability to share different groups of memory with different groups of individuals. Mem-\nory and knowledge retrieval are cornerstones of agentic systems, and we can now sum-\nmarize what we covered and review some learning exercises in the next section.\n8.8\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Load and Split a Different Document (Intermediate) \nObjective\u2014Understand the effect of document splitting on retrieval efficiency by\nusing LangChain. \nTasks:\n\u2013 Select a different document (e.g., a news article, a scientific paper, or a short\nstory).\n\u2013 Use LangChain to load and split the document into chunks.\n\u2013 Analyze how the document is split into chunks and how it affects the retrieval\nprocess.\n\uf0a1Exercise 2\u2014Experiment with Semantic Search (Intermediate) \nObjective\u2014Compare the effectiveness of various vectorization techniques by per-\nforming semantic searches. \nTasks:\n\u2013 Choose a set of documents for semantic search.\n\u2013 Use a vectorization method such as Word2Vec or BERT embeddings instead\nof TF\u2013IDF.\n\u2013 Perform the semantic search, and compare the results with those obtained\nusing TF\u2013IDF to understand the differences and effectiveness.\n\uf0a1Exercise 3\u2014Implement a Custom RAG Workflow (Advanced) \nObjective\u2014Apply theoretical knowledge of RAG in a practical context using\nLangChain. \nTasks:\n\u2013 Choose a specific application (e.g., customer service inquiries or academic\nresearch queries).\n\u2013 Design and implement a custom RAG workflow using LangChain.\n\u2013 Tailor the workflow to suit the chosen application, and test its effectiveness.\n\n\n210\nCHAPTER 8\nUnderstanding agent memory and knowledge\n\uf0a1Exercise 4\u2014Build a Knowledge Store and Experiment with Splitting Patterns\n(Intermediate) \nObjective\u2014Understand how different splitting patterns and compression affect\nknowledge retrieval. \nTasks:\n\u2013 Build a knowledge store, and populate it with a couple of documents.\n\u2013 Experiment with different forms of splitting/chunking patterns, and analyze\ntheir effect on retrieval.\n\u2013 Compress the knowledge store, and observe the effects on query performance.\n\uf0a1Exercise 5\u2014Build and Test Various Memory Stores (Advanced) \nObjective\u2014Understand the uniqueness and use cases of different memory store\ntypes. \nTasks:\n\u2013 Build various forms of memory stores (conversational, semantic, episodic,\nand procedural).\n\u2013 Interact with an agent using each type of memory store, and observe the dif-\nferences.\n\u2013 Compress the memory store, and analyze the effect on memory retrieval.\nSummary\n\uf0a1Memory in AI applications differentiates between unstructured and structured\nmemory, highlighting their use in contextualizing prompts for more relevant\ninteractions.\n\uf0a1Retrieval augmented generation (RAG) is a mechanism for enhancing prompts\nwith context from external documents, using vector embeddings and similarity\nsearch to retrieve relevant content.\n\uf0a1Semantic search with document indexing converts documents into semantic\nvectors using TF\u2013IDF and cosine similarity, enhancing the capability to perform\nsemantic searches across indexed documents.\n\uf0a1Vector databases and similarity search stores document vectors in a vector data-\nbase, facilitating efficient similarity searches and improving retrieval accuracy.\n\uf0a1Document embeddings capture semantic meanings, using models such as\nOpenAI\u2019s models to generate embeddings that preserve a document\u2019s context\nand facilitate semantic similarity searches.\n\uf0a1LangChain provides several tools for performing RAG, and it abstracts the\nretrieval process, allowing for easy implementation of RAG and memory sys-\ntems across various data sources and vector stores.\n\uf0a1Short-term and long-term memory in LangChain implements conversational\nmemory within LangChain, distinguishing between short-term buffering pat-\nterns and long-term storage solutions.\n\n\n211\nSummary\n\uf0a1Storing document vectors in databases for efficient similarity searches is crucial\nfor implementing scalable retrieval systems in AI applications.\n\uf0a1Agent knowledge directly relates to the general RAG pattern of performing\nquestion and answer on documents or other textual information.\n\uf0a1Agent memory is a pattern related to RAG that captures the agentic interac-\ntions with users, itself, and other systems.\n\uf0a1Nexus is a platform that implements agentic knowledge and memory systems,\nincluding setting up knowledge stores for document retrieval and memory\nstores for various forms of memory.\n\uf0a1Semantic memory augmentation (semantic memory) differentiates between vari-\nous types of memories (semantic, episodic, procedural). It implements them\nthrough semantic augmentation, enhancing agents\u2019 ability to recall and use\ninformation relevantly specific to the nature of the memories.\n\uf0a1Memory and knowledge compression are techniques for condensing informa-\ntion stored in memory and knowledge systems, improving retrieval efficiency\nand relevancy through clustering and summarization.\n\n\n212\nMastering agent prompts\nwith prompt flow\nIn this chapter, we delve into the Test Changes Systematically prompt engineering\nstrategy. If you recall, we covered the grand strategies of the OpenAI prompt engi-\nneering framework in chapter 2. These strategies are instrumental in helping us\nbuild better prompts and, consequently, better agent profiles and personas. Under-\nstanding this role is key to our prompt engineering journey.\n Test Changes Systematically is such a core facet of prompt engineering that Mic-\nrosoft developed a tool around this strategy called prompt flow, described later in\nthis chapter. Before getting to prompt flow, we need to understand why we need\nsystemic prompt engineering.\nThis chapter covers\n\uf0a1Understanding systematic prompt engineering \nand setting up your first prompt flow\n\uf0a1Crafting an effective profile/persona prompt\n\uf0a1Evaluating profiles: Rubrics and grounding\n\uf0a1Grounding evaluation of a large language model \nprofile\n\uf0a1Comparing prompts: Getting the perfect profile \n\n\n213\n9.1\nWhy we need systematic prompt engineering\n9.1\nWhy we need systematic prompt engineering\nPrompt engineering, by its nature, is an iterative process. When building a prompt,\nyou\u2019ll often iterate and evaluate. To see this concept in action, consider the simple\napplication of prompt engineering to a ChatGPT question.\n You can follow along by opening your browser to ChatGPT (https://chat.openai\n.com/), entering the following (text) prompt into ChatGPT, and clicking the Send Mes-\nsage button (an example of this conversation is shown in figure 9.1, on the left side):\ncan you recommend something\nWe can see that the response from ChatGPT is asking for more information. Go ahead\nand open a new conversation with ChatGPT, and enter the following prompt, as\nshown in figure 9.1, on the right side:\nCan you please recommend a time travel movie set in the medieval period.\nThe results in figure 9.1 show a clear difference between leaving out details and being\nmore specific in your request. We just applied the tactic of politely Writing Clear Instruc-\ntions, and ChatGPT provided us with a good recommendation. But also notice how\nChatGPT itself guides the user into better prompting. The refreshed screen shown in\nfigure 9.2 shows the OpenAI prompt engineering strategies.\n We just applied simple iteration to improve our prompt. We can extend this exam-\nple by using a system prompt/message. Figure 9.3 demonstrates the use and role of\nthe system prompt in iterative communication. In chapter 2, we used the system mes-\nsage/prompt in various examples.\nNo prompt engineering\nApplying prompt engineering\nDetails are included in\nthe prompt/request.\nChatGPT guides the user to\nsupply additional details.\nFigure 9.1\nThe differences in applying prompt engineering and iterating\n\n\n214\nCHAPTER 9\nMastering agent prompts with prompt flow\nWrite Clear Instructions\nBe speci\ufb01c in what you ask.\nTactics include detailing queries, adopting personas, using d imiters,\nel\nspecifying steps, providing examples, and specifying output length.\nBasics\nProvide Reference Text\nHelps reduce fabrications.\nTactics involve instructing the model to use or cite reference texts.\nMemory\nUse External Tools\nEnhances model capabilities.\nTactics include embeddings-based search, code execution, and\naccess to speci\ufb01c functions.\nMemory\nSplit Complex Tasks into Simpler Subtasks\nReduces error rates.\nTactics include intent classi\ufb01cation, summarizing dialogues, and\npiecewise summarization of documents.\nPlanning\nGive Models Time to \u201cThink\u201d\nAllows more reliable reasoning.\nTactics involve working out solutions before conclusions, using inn r\ne\nmonologue, and reviewing previous answers.\nPlanning\nTest Changes Systematically\nEnsures improvements are genuine.\nTactics involve evaluating model outputs with reference to standard\nanswers.\nEvaluation\nPrompt Engineering Strategies\nFigure 9.2\nOpenAI prompt engineering strategies, broken down by agent component\nThe ser prompt de\ufb01nes\nu\nthe details of the ask.\nThe ystem prompt de\ufb01nes the role\ns\nand rules and continues across the\nconversation.\nA ser prompt may re\ufb01ne\nu\nthe ask or start a new ask.\nThe Assistant marks the\nresponse from the LLM.\nFigure 9.3\nThe messages to and from an LLM conversation and the iteration of messages\n\n\n215\n9.1\nWhy we need systematic prompt engineering\nYou can also try this in ChatGPT. This time, enter the following prompt and include\nthe word system in lowercase, followed by a new line (enter a new line in the message\nwindow without sending the message by pressing Shift-Enter):\nsystem\nYou are an expert on time travel movies.\nChatGPT will respond with some pleasant comments, as shown in figure 9.4. Because\nof this, it\u2019s happy to accept its new role and asks for any follow-up questions. Now\nenter the following generic prompt as we did previously:\ncan you recommend something\nThis sets the ystem prompt, the role\ns\nthe LLM will take for the remainder of\nthe conversation.\nThe LLM responds happily with the new role.\nMake the generic ask again.\nThe LLM now provides a list of recommendations.\nFigure 9.4\nThe effect of adding a system prompt to our previous conversation\n\n\n216\nCHAPTER 9\nMastering agent prompts with prompt flow\nWe\u2019ve just seen the iteration of refining a prompt, the prompt engineering, to extract\na better response. This was accomplished over three different conversations using the\nChatGPT UI. While not the most efficient way, it works.\n However, we haven\u2019t defined the iterative flow for evaluating the prompt and\ndetermining when a prompt is effective. Figure 9.5 shows a systemic method of prompt\nengineering using a system of iteration and evaluation.\nThe system of iterating and evaluating prompts covers the broad Test Changes System-\nically strategy. Evaluating the performance and effectiveness of prompts is still new,\nbut we\u2019ll use techniques from education, such as rubrics and grounding, which we\u2019ll\nexplore in a later section of this chapter. However, as spelled out in the next section,\nwe need to understand the difference between a persona and an agent profile before\nwe do so.\n9.2\nUnderstanding agent profiles and personas\nAn agent profile is an encapsulation of component prompts or messages that describe\nan agent. It includes the agent\u2019s persona, special instructions, and other strategies that\ncan guide the user or other agent consumers.\n Figure 9.6 shows the main elements of an agent profile. These elements map to\nprompt engineering strategies described in this book. Not all agents will use all the\nelements of a full agent profile.\n At a basic level, an agent profile is a set of prompts describing the agent. It may\ninclude other external elements related to actions/tools, knowledge, memory,\nSystemic Prompt Engineering\n(Strategy - Test Changes Systemically)\nBuild prompt\nor pro\ufb01le\nWrite/update the\nprompt\nYes\nEvaluate\nprompt\nis working\nNo\nYes\nBatch\nevaluation\nof prompt\nNo\nPrompt is\nused\nPrompt or pro\ufb01le\nis grounded.\nEvaluate variations\nof the prompt/pro\ufb01le.\nEvaluate the prompt\nbasic on rubrics.\nUse prompt\nengineering to\nwrite the prompt.\nFigure 9.5\nThe systemic method of prompt engineering\n\n\n217\n9.3\nSetting up your first prompt flow\nreasoning, evaluation, planning, and feedback. The combination of these elements\ncomprises an entire agent prompt profile.\n Prompts are the heart of an agent\u2019s function. A prompt or set of prompts drives\neach of the agent components in the profile. For actions/tools, these prompts are well\ndefined, but as we\u2019ve seen, prompts for memory and knowledge can vary significantly\nby use case.\n The definition of an AI agent profile is more than just a system prompt. Prompt\nflow can allow us to construct the prompts and code comprising the agent profile but\nalso include the ability to evaluate its effectiveness. In the next section, we\u2019ll open up\nprompt flow and start using it.\n9.3\nSetting up your first prompt flow\nPrompt flow is a tool developed by Microsoft within its Azure Machine Learning Stu-\ndio platform. The tool was later released as an open source project on GitHub,\nwhere it has attracted more attention and use. While initially intended as an applica-\ntion platform, it has since shown its strength in developing and evaluating prompts/\nprofiles.\n Because prompt flow was initially developed to run on Azure as a service, it fea-\ntures a robust core architecture. The tool supports multi-threaded batch processing,\nThe Agent Pro\ufb01le (prompts)\nActions and tools are added to\nthe prompt under the covers.\nKnowledge and memory are prompts\nused to extract and identify memories.\nAdding reasoning to prompts\nSimilar to prompt personas, the agent\npersona can give an agent specialized\nattributes, rules, and even personality.\nPlanning and feedback\nPersona\nRepresents the background and role of\nthe agent, and is often introduced in\nthe \ufb01rst system message.\nAgent T ools\nSet of tools an agent can\nuse to help accomplish a task.\nAgent Evaluation and Reasoning\nDescribes how the agent can reason\nand evaluate a task or tasks.\nAgent Memory and Knowledge\nThe backend store that helps the agent\nadd context to a given task problem.\nAgent Planning and Feedback\nDescribes how the agent can break\ndown a task into execution steps, and\nthen execute and receive feedback.\nFigure 9.6\nThe component parts of an agent profile\n",
      "page_number": 233
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 242-251)",
      "start_page": 242,
      "end_page": 251,
      "detection_method": "topic_boundary",
      "content": "218\nCHAPTER 9\nMastering agent prompts with prompt flow\nwhich makes it ideal for evaluating prompts at scale. The following section will exam-\nine the basics of starting with prompt flow.\n9.3.1\nGetting started\nThere are a few prerequisites to undertake before working through the exercises in\nthis book. The relevant prerequisites for this section and chapter are shown in the fol-\nlowing list; make sure to complete them before attempting the exercises:\n\uf0a1Visual Studio Code (VS Code)\u2014Refer to appendix A for installation instructions,\nincluding additional extensions.\n\uf0a1Prompt flow, VS Code extension\u2014Refer to appendix A for details on installing\nextensions.\n\uf0a1Python virtual environment\u2014Refer to appendix A for details on setting up a vir-\ntual environment.\n\uf0a1Install prompt flow packages\u2014Within your virtual environment, do a quick pip\ninstall, as shown here:\npip install promptflow promptflow-tools\n\uf0a1LLM (GPT-4 or above)\u2014You\u2019ll need access to GPT-4 or above through OpenAI\nor Azure OpenAI Studio. Refer to appendix B if you need assistance accessing\nthese resources.\n\uf0a1Book\u2019s source code\u2014Clone the book\u2019s source code to a local folder; refer to\nappendix A if you need help cloning the repository.\nOpen up VS Code to the book\u2019s source code folder, chapter 3. Ensure that you have a\nvirtual environment connected and have installed the prompt flow packages and\nextension.\n First, you\u2019ll want to create a connection to your LLM resource within the prompt\nflow extension. Open the prompt flow extension within VS Code, and then click to\nopen the connections. Then, click the plus sign beside the LLM resource to create a\nnew connection, as shown in figure 9.7.\n This will open a YAML file where you\u2019ll need to populate the connection name\nand other information relevant to your connection. Follow the directions, and don\u2019t\nenter API keys into the document, as shown in figure 9.8.\n When the connection information is entered, click the Create Connection link at\nthe bottom of the document. This will open a terminal prompt below the document,\nasking you to enter your key. Depending on your terminal configuration, you may be\nunable to paste (Ctrl-V, Cmd-V). Alternatively, you can paste the key by hovering the\nmouse cursor over the terminal and right-clicking on Windows.\n We\u2019ll now test the connection by first opening the simple flow in the chap-\nter_09/promptflow/simpleflow folder. Then, open the flow.dag.yaml file in VS\nCode. This is a YAML file, but the prompt flow extension provides a visual editor\n\n\n219\n9.3\nSetting up your first prompt flow\n(omitted)\nVS Code\nClick to open the prompt \ufb02ow extension.\nClick the plus to create a new connection.\nFigure 9.7\nCreating a new prompt flow LLM connection\nEnter a name for the connection.\nFollow the directions: don\u2019t enter a key.\nClick after completing the above.\nFigure 9.8\nSetting the connection information for your LLM resource\n\n\n220\nCHAPTER 9\nMastering agent prompts with prompt flow\nthat is accessible by clicking the Visual Editor link at the top of the file, as shown in\nfigure 9.9.\nAfter the visual editor window is opened, you\u2019ll see a graph representing the flow and\nthe flow blocks. Double-click the recommender block, and set the connection name,\nAPI type, and model or deployment name, as shown in figure 9.10.\nClick the link to open the visual editor.\n\ufb02ow.dag.yaml\nFigure 9.9\nOpening the prompt flow visual editor\nDouble-click to\nopen LLM block.\nSelect the\nconnection name.\nAPI type\nModel or\ndeployment name\nFigure 9.10\nSetting the LLM connection details\n\n\n221\n9.3\nSetting up your first prompt flow\nA prompt flow is composed of a set of blocks starting with an Inputs block and termi-\nnating in an Outputs block. Within this simple flow, the recommender block represents\nthe LLM connection and the prompt used to converse with the model. The echo\nblock for this simple example echoes the input.\n      When creating a connection to an LLM, either in prompt flow or through an\nAPI, here are the crucial parameters we always need to consider (prompt flow docu-\nmentation: https://microsoft.github.io/promptflow):\n\uf0a1Connection\u2014This is the connection name, but it also represents the service\nyou\u2019re connecting to. Prompt flow supports multiple services, including locally\ndeployed LLMs.\n\uf0a1API\u2014This is the API type. The options are chat for a chat completion API,\nsuch as GPT-4, or completion for the older completion models, such as the\nOpenAI Davinci.\n\uf0a1Model\u2014This may be the model or deployment name, depending on your service\nconnection. For OpenAI, this will be the model\u2019s name, and for Azure OpenAI,\nit will represent the deployment name.\n\uf0a1Temperature\u2014This represents the stochasticity or variability of the model response.\nA value of 1 represents a high variability of responses, while 0 indicates a desire\nfor no variability. This is a critical parameter to understand and, as we\u2019ll see, will\nvary by use case.\n\uf0a1Stop\u2014This optional setting tells the call to the LLM to stop creating tokens. It\u2019s\nmore appropriate for older and open source models.\n\uf0a1Max tokens\u2014This limits the number of tokens used in a conversation. Knowl-\nedge of how many tokens you use is crucial to evaluating how your LLM interac-\ntions will work when scaled. Counting tokens may not be a concern if you\u2019re\nexploring and conducting research. However, in production systems, tokens\nrepresent the load on the LLM, and connections using numerous tokens may\nnot scale well.\n\uf0a1Advanced parameters\u2014You can set a few more options to tune your interaction\nwith the LLM, but we\u2019ll cover that topic in later sections of the book.\nAfter configuring the LLM block, scroll up to the Inputs block section, and review the\nprimary input shown in the user_input field, as shown in figure 9.11. Leave it as the\ndefault, and then click the Play button at the top of the window.\n All the blocks in the flow will run, and the results will be shown in the terminal win-\ndow. What you should find interesting is that the output shows recommendations for\ntime travel movies. This is because the recommender block already has a simple pro-\nfile set, and we\u2019ll see how that works in the next section.\n \n\n\n222\nCHAPTER 9\nMastering agent prompts with prompt flow\n9.3.2\nCreating profiles with Jinja2 templates\nThe flow responds with time travel movie recommendations because of the prompt or\nprofile it uses. By default, prompt flow uses Jinja2 templates to define the content of\nthe prompt or what we\u2019ll call a profile. For the purposes of this book and our explora-\ntion of AI agents, we\u2019ll refer to these templates as the profile of a flow or agent.\n While prompt flow doesn\u2019t explicitly refer to itself as an assistant or agent engine, it\ncertainly meets the criteria of producing a proxy and general types of agents. As you\u2019ll\nsee, prompt flow even supports deployments of flows into containers and as services.\n Open VS Code to chapter_09/promptflow/simpleflow/flow.dag.yaml, and open\nthe file in the visual editor. Then, locate the Prompt field, and click the recommended\n.jinja2 link, as shown in figure 9.12.\nInputs block\nClick Play to run all\nthe blocks in the \ufb02ow.\nUse the default text.\nFigure 9.11\nSetting the inputs and starting the flow\nClick the link to open\nthe inja2 template.\nJ\nDe\ufb01nes the start of the system and\nuser portion of the prompt/pro\ufb01le\nThe role of the pro\ufb01le\nThis is where the user_input\ntext will be placed.\nFigure 9.12\nOpening the prompt Jinja2 template and examining the parts of the profile/prompt\n\n\n223\n9.3\nSetting up your first prompt flow\nJinja is a templating engine, and Jinja2 is a particular version of that engine. Tem-\nplates are an excellent way of defining the layout and parts of any form of text docu-\nment. They have been extensively used to produce HTML, JSON, CSS, and other\ndocument forms. In addition, they support the ability to apply code directly into the\ntemplate. While there is no standard way to construct prompts or agent profiles, our\npreference in this book is to use templating engines such as Jinja.\n At this point, change the role within the system prompt of the recommended.jin-\nja2 template. Then, run all blocks of the flow by opening the flow in the visual editor\nand clicking the Play button. The next section will look at other ways of running\nprompt flow for testing or actual deployment.\n9.3.3\nDeploying a prompt flow API\nBecause prompt flow was also designed to be deployed as a service, it supports a cou-\nple of ways to deploy as an app or API quickly. Prompt flow can be deployed as a local\nweb application and API running from the terminal or as a Docker container.\n Return to the flow.dag.yaml file in the visual editor from VS Code. At the top of\nthe window beside the Play button are several options we\u2019ll want to investigate further.\nClick the Build button as shown in figure 9.13, and then select to deploy as a local\napp. A new YAML file will be created to configure the app. Leave the defaults, and\nclick the Start Local App link.\nThis will launch the flow as a local web application, and you\u2019ll see a browser tab open,\nas shown in figure 9.14. Enter some text into the user_input field, which is marked as\nrequired with a red asterisk. Click Enter and wait a few seconds for the reply.\n You should see a reply like the one shown earlier in figure 9.12, where the flow or\nagent replies with a list of time travel movies. This is great\u2014we\u2019ve just developed our\nfirst agent profile and the equivalent of a proxy agent. However, we need to determine\nClick the Build button. When prompted, select to build\nas either a web application or Docker container.\nClick the link to start the local app.\nFigure 9.13\nBuilding and starting the flow as a local app\n\n\n224\nCHAPTER 9\nMastering agent prompts with prompt flow\nhow successful or valuable the recommendations are. In the next section, we explore\nhow to evaluate prompts and profiles.\n9.4\nEvaluating profiles: Rubrics and grounding\nA key element of any prompt or agent profile is how well it performs its given task. As\nwe see in our recommendation example, prompting an agent profile to give a list of\nrecommendations is relatively easy, but knowing whether those recommendations are\nhelpful requires us to evaluate the response.\n Fortunately, prompt flow has been designed to evaluate prompts/profiles at scale.\nThe robust infrastructure allows for the evaluation of LLM interactions to be paral-\nlelized and managed as workers, allowing hundreds of profile evaluations and varia-\ntions to happen quickly.\n In the next section, we look at how prompt flow can be configured to run prompt/\nprofile variations against each other. We\u2019ll need to understand this before evaluating\nprofiles\u2019 performance.\n Prompt flow provides a mechanism to allow for multiple variations within an LLM\nprompt/profile. This tool is excellent for comparing subtle or significant differences\nChat history is only used for chat \ufb02ow. This is a standard \ufb02ow.\nKeeps a history of submissions\nEnter text to be used by the recommender.\nFigure 9.14\nRunning the flow as a local web application\n\n\n225\n9.4\nEvaluating profiles: Rubrics and grounding\nbetween profile variations. When used in performing bulk evaluations, it can be\ninvaluable for quickly assessing the performance of a profile.\n Open the recommender_with_variations/flow.dag.yaml file in VS Code and the\nflow visual editor, as shown in figure 9.15. This time, we\u2019re making the profile more\ngeneralized and allowing for customization at the input level. This allows us to expand\nour recommendations to anything and not just time travel movies.\nThe new inputs Subject, Genre, Format, and Custom allow us to define a profile that\ncan easily be adjusted to any recommendation. This also means that we must prime\nthe inputs based on the recommendation use case. There are multiple ways to prime\nthese inputs; two examples of priming inputs are shown in figure 9.16. The figure\nshows two options, options A and B, for priming inputs. Option A represents the clas-\nsic UI; perhaps there are objects for the user to select the subject or genre, for exam-\nple. Option B places a proxy/chat agent to interact with the user better to understand\nthe desired subject, genre, and so on.\nAdded additional inputs\ninto the recommender\nThe recommenderLLM has two variations. Click\nthe adjust icons to see and edit the variations.\nInputs are passed directly to\nthe recommender LLM block.\nFigure 9.15\nThe recommender, with variations in flow and expanded inputs\n\n\n226\nCHAPTER 9\nMastering agent prompts with prompt flow\nEven considering the power of LLMs, you may still want or need to use option A. The\nbenefit of option A is that you can constrain and validate the inputs much like you do\nwith any modern UI. Alternatively, the downside of option A is that the constrained\nbehavior may limit and restrict future use cases.\n Option B represents a more fluid and natural way without a traditional UI. It\u2019s far\nmore powerful and extensible than option A but also introduces more unknowns for\nevaluation. However, if the proxy agent that option B uses is written well, it can assist a\nlot in gathering better information from the user.\n The option you choose will dictate how you need to evaluate your profiles. If\nyou\u2019re okay with a constrained UI, then it\u2019s likely that the inputs will also be con-\nstrained to a set of discrete values. For now, we\u2019ll assume option B for input priming,\nmeaning the input values will be defined by their name.\n To get back to VS Code and the visual view of the recommender with variants flow,\nclick the icon shown earlier in figure 9.15 to open the variants and allow editing.\nThen, click the recommend.jinja2 and recommender_variant_1.jinja2 links to open\nthe files side by side, as shown in figure 9.17.\n Figure 9.17 demonstrates the difference between the variant profiles. One profile\ninjects the inputs into the user prompt, and the other injects them into the system\nprompt. However, it\u2019s essential to understand that variations can encompass more\nthan profile design, as identified in table 9.1.\nAPI\nUser interface\nI want a movie recommendation\nLLM\nOptions\nOption A\nOption B\nUser enters the required information: subject,\ngenre, format, and custom into an interface\nThe proxy agent asks or is directed by the user\nfor recommendations. The agent asks and/or\nidenti\ufb01es the subject, genre, format, and\ncustom through conversation.\nAgent pro\ufb01le interacts with LLM by\ninjecting subject, genre, format,\nand custom into its pro\ufb01le\nFigure 9.16\nThe user interaction options for interfacing with the agent profile to prime inputs to the agent profile\n\n\n227\n9.4\nEvaluating profiles: Rubrics and grounding\nFor this simple example, we\u2019re just going to use prompt variations by varying the input\nto reflect in either the system or user prompt. Refer to figure 9.17 for what this looks\nTable 9.1\nLLM variation options in prompt flow\nOption\nEvaluation option examples\nNotes\nJinja2 prompt \ntemplate\nCompare system prompt variations, \nuser prompt variations, or mixed prompt \nvariations.\nSome endless combinations and tech-\nniques can be applied here. Prompt engi-\nneering is evolving all the time.\nLLM\nCompare GPT-9.5 to GPT-4.\nCompare GPT-4 to GPT-4 Turbo.\nCompare open source models to com-\nmercial models.\nThis is a useful way to evaluate and \nground model performance against a \nprompt. It can also help you tune your \nprofile to work with open source and/or \ncheaper models.\nTemperature\nCompare a 0 temperature (no random-\nness) to a 1 (maximum randomness).\nChanges to the temperature can signifi-\ncantly change the responses of some \nprompts, which may improve or degrade \nperformance.\nMax tokens\nCompare limited tokens to larger token \nsizes.\nThis can allow you to reduce and maxi-\nmize token usage.\nAdvanced \nparameters\nCompare differences to options such \nas top_p, presence_penalty, \nfrequency_penalty, and \nlogit_bias. \nWe\u2019ll cover the use of these advanced \nparameters in later chapters.\nFunction calls\nCompare alternative function calls.\nFunction calls will be addressed later in \nthis chapter.\nThe system prompt describes a generic recommender\nthat works when given speci\ufb01c inputs.\nThis pro\ufb01le injects the inputs directly\ninto a formatted user message.\nCustom input now becomes\nthe user prompt.\nThe inputs are now injected into\nthe system prompt. Both\nprompts have been simpli\ufb01ed.\nFigure 9.17\nSide-by-side comparison of variant profile templates for the recommender\n",
      "page_number": 242
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 252-260)",
      "start_page": 252,
      "end_page": 260,
      "detection_method": "topic_boundary",
      "content": "228\nCHAPTER 9\nMastering agent prompts with prompt flow\nlike. We can then quickly run both variations by clicking the Play (Run All) button at\nthe top and choosing both, as shown in figure 9.18.\nIn the terminal window, you\u2019ll see the results of both runs. The results will likely look\nsimilar, so now we must move on to how we evaluate the difference between variations\nin the next section.\n9.5\nUnderstanding rubrics and grounding\nEvaluation of prompt/profile performance isn\u2019t something we can typically do using a\nmeasure of accuracy or correct percentage. Measuring the performance of a profile\ndepends on the use case and desired outcome. If that is as simple as determining if\nthe response was right or wrong, all the better. However, in most cases, evaluation\nwon\u2019t be that simple.\n In education, the rubric concept defines a structured set of criteria and standards a\nstudent must establish to receive a particular grade. A rubric can also be used to\ndefine a guide for the performance of a profile or prompt. We can follow these steps\nto define a rubric we can use to evaluate the performance of a profile or prompt:\n1\nIdentify the purpose and objectives. Determine the goals you want the profile or\nagent to accomplish. For example, do you want to evaluate the quality of recom-\nmendations for a given audience or overall quality for a given subject, format,\nor other input?\n2\nDefine criteria. Develop a set of criteria or dimensions that you\u2019ll use to evaluate\nthe profile. These criteria should align with your objectives and provide clear\nguidelines for assessment. Each criterion should be specific and measurable.\n#2 Select the option to run all variations.\n#1 Click Play (Run All) to start the run.\nFigure 9.18\nRunning both prompt variations at the same time\n\n\n229\n9.5\nUnderstanding rubrics and grounding\nFor example, you may want to measure a recommendation by how well it fits\nwith the genre and then by subject and format.\n3\nCreate a scale. Establish a rating scale that describes the levels of performance for\neach criterion. Standard scales include numerical scales (e.g., 1\u20135) or descrip-\ntive scales (e.g., Excellent, Good, Fair, Poor).\n4\nProvide descriptions. For each level on the scale, provide clear and concise descrip-\ntions that indicate what constitutes a strong performance and what represents a\nweaker performance for each criterion.\n5\nApply the rubric. When assessing a prompt or profile, use the rubric to evaluate\nthe prompt\u2019s performance based on the established criteria. Assign scores or\nratings for each criterion, considering the descriptions for each level.\n6\nCalculate the total score. Depending on your rubric, you may calculate a total\nscore by summing up the scores for each criterion or using a weighted average\nif some criteria are more important than others.\n7\nEnsure evaluation consistency. If multiple evaluators are assessing the profile, it\u2019s\ncrucial to ensure consistency in grading.\n8\nReview, revise, and iterate. Periodically review and revise the rubric to ensure it\naligns with your assessment goals and objectives. Adjust as needed to improve\nits effectiveness.\nGrounding is a concept that can be applied to profile and prompt evaluation\u2014it\ndefines how well a response is aligned with a given rubric\u2019s specific criteria and stan-\ndards. You can also think of grounding as the baseline expectation of a prompt or pro-\nfile output.\n This list summarizes some other important considerations when using grounding\nwith profile evaluation:\n\uf0a1Grounding refers to aligning responses with the criteria, objectives, and context\ndefined by the rubric and prompt.\n\uf0a1Grounding involves assessing whether the response directly addresses the\nrubric criteria, stays on topic, and adheres to any provided instructions.\n\uf0a1Evaluators and evaluations gauge the accuracy, relevance, and adherence to\nstandards when assessing grounding.\n\uf0a1Grounding ensures that the response output is firmly rooted in the specified\ncontext, making the assessment process more objective and meaningful.\nA well-grounded response aligns with all the rubric criteria within the given context\nand objectives. Poorly grounded responses will fail or miss the entire criteria, context,\nand objectives.\n As the concepts of rubrics and grounding may still be abstract, let\u2019s look at apply-\ning them to our current recommender example. Following is a list that follows the\nprocess for defining a rubric as applied to our recommender example:\n\n\n230\nCHAPTER 9\nMastering agent prompts with prompt flow\n1\nIdentify the purpose and objectives. The purpose of our profile/prompt is to recom-\nmend three top items given a subject, format, genre, and custom input.\n2\nDefine criteria. For simplicity, we\u2019ll evaluate how a particular recommendation\naligns with the given input criteria, subject, format, and genre. For example, if a\nprofile recommends a book when asked for a movie format, we expect a low\nscore in the format criteria.\n3\nCreate a scale. Again, keeping things simple, we\u2019ll use a scale of 1\u20135 (1 is poor,\nand 5 is excellent).\n4\nProvide descriptions. See the general descriptions for the rating scale shown in\ntable 9.2.\n5\nApply the rubric. With the rubric assigned at this stage, it\u2019s an excellent exercise\nto evaluate the rubric against recommendations manually.\n6\nCalculate the total score. For our rubric, we\u2019ll average the score for all criteria to\nprovide a total score.\n7\nEnsure evaluation consistency. The technique we\u2019ll use for evaluation will provide\nvery consistent results.\n8\nReview, revise, and iterate. We\u2019ll review, compare, and iterate on our profiles, rubrics,\nand the evaluations themselves.\nThis basic rubric can now be applied to evaluate the responses for our profile. You can\ndo this manually, or as you\u2019ll see in the next section, using a second LLM profile.\n9.6\nGrounding evaluation with an LLM profile\nThis section will employ another LLM prompt/profile for evaluation and grounding.\nThis second LLM prompt will add another block after the recommendations are gen-\nerated. It will process the generated recommendations and evaluate each one, given\nthe previous rubric.\n Before GPT-4 and other sophisticated LLMs came along, we would have never con-\nsidered using another LLM prompt to evaluate or ground a profile. You often want\nTable 9.2\nRubric ratings \nRating\nDescription\n1\nPoor alignment: this is the opposite of what is expected given the criteria.\n2\nBad alignment: this isn\u2019t a good fit for the given criteria.\n3\nMediocre alignment: it may or may not fit well with the given criteria.\n4\nGood alignment: it may not align 100% with the criteria but is a good fit otherwise.\n5\nExcellent alignment: this is a good recommendation for the given criteria.\n\n\n231\n9.6\nGrounding evaluation with an LLM profile\nto use a different model when using LLMs to ground a profile. However, if you\u2019re\ncomparing profiles against each other, using the same LLM for evaluation and ground-\ning is appropriate.\n Open the recommender_with_LLM_evaluation\\flow.dag.yaml file in the prompt\nflow visual editor, scroll down to the evaluate_recommendation block, and click the\nevaluate_recommendation.jinja2 link to open the file, as shown in figure 9.19. Each\nsection of the rubric is identified in the figure.\nWe have a rubric that is not only well defined but also in the form of a prompt that can\nbe used to evaluate recommendations. This allows us to evaluate the effectiveness of\nrecommendations for a given profile\u2014automatically. Of course, you can also use the\nrubric to score and evaluate the recommendations manually for a better baseline.\nNOTE\nUsing LLMs to evaluate prompts and profiles provides a strong base-\nline for comparing the performance of a profile. It can also do this without\nhuman bias in a controlled and repeatable manner. This provides an excel-\nlent mechanism to establish baseline groundings for any profile or prompt.\nReturning to the recommender_with_LLM_evaluation flow visual editor, we can run\nthe flow by clicking the Play button and observing the output. You can run a single\nDe\ufb01ne the basic role of the pro\ufb01le.\nDe\ufb01ne the basic criteria for the rubric.\nDe\ufb01ne the rubric scale and\na description for each item\non the scale.\nReiterate the criteria and scale, and\nshow an example of expected output.\nFigure 9.19\nThe evaluation prompt, with each of the parts of the rubric outlined\n\n\n232\nCHAPTER 9\nMastering agent prompts with prompt flow\nrecommendation or run both variations when prompted. The output of a single eval-\nuation using the default inputs is shown in the following listing.\n{\n    \"recommendations\": \"Title: The Butterfly Effect\nSubject: 5\nFormat: 5\nGenre: 4\nTitle: Primer\nSubject: 5\nFormat: 5\nGenre: 4\nTitle: Time Bandits\nSubject: 5\nFormat: 5\nGenre: 5\"\n}\nWe now have a rubric for grounding our recommender, and the evaluation is run\nautomatically using a second LLM prompt. In the next section, we look at how to per-\nform multiple evaluations simultaneously and then at a total score for everything.\n9.7\nComparing profiles: Getting the perfect profile\nWith our understanding of rubrics and grounding, we can now move on to evaluating\nand iterating the perfect profile. Before we do that, though, we need to clean up the\noutput from the LLM evaluation block. This will require us to parse the recommenda-\ntions into something more Pythonic, which we\u2019ll tackle in the next section.\n9.7.1\nParsing the LLM evaluation output\nAs the raw output from the evaluation block is text, we now want to parse that into\nsomething more usable. Of course, writing parsing functions is simple, but there are\nbetter ways to cast responses automagically. We covered better methods for returning\nresponses in chapter 5, on agent actions.\n Open chapter_09\\prompt_flow\\recommender_with_parsing\\flow.dag.yaml in\nVS Code, and look at the flow in the visual editor. Locate the parsing_results block,\nand click the link to open the Python file in the editor, as shown in figure 9.20.\n The code for the parsing_results.py file is shown in listing 9.2.\n \n \n \n \nListing 9.1\nLLM rubric evaluation output\n\n\n233\n9.7\nComparing profiles: Getting the perfect profile\nfrom promptflow import tool\n@tool     \ndef parse(input: str) -> str:\n    # Splitting the recommendations into individual movie blocks\n    rblocks = input.strip().split(\"\\n\\n\")     \n    # Function to parse individual recommendation block into dictionary\n    def parse_block(block):\n        lines = block.split('\\n')\n        rdict = {}\n        for line in lines:\n            kvs = line.split(': ')\n            key, value = kvs[0], kvs[1]\n            rdict[key.lower()] = value    \n        return rdict\n    parsed = [parse_block(block) for block in rblocks]   \n    return parsed\nWe\u2019re converting the recommendations output from listing 9.1, which is just a string,\ninto a dictionary. So this code will convert this string into the JSON block shown next:\nListing 9.2\nparsing_results.py\nClick the link to open the Python \ufb01le.\nparsing_results.py parses output\nafter LLM evaluation.\nFigure 9.20\nOpening the parsing_results.py file in VS Code\nSpecial decorator to \ndenote the tool block\nSplits the input and\ndouble new lines\nCreates a dictionary entry \nand sets the value\nLoops through each \nblock and parses \ninto key/value \ndictionary\n\n\n234\nCHAPTER 9\nMastering agent prompts with prompt flow\n Before parsing:\n\"Title: The Butterfly Effect\nSubject: 5\nFormat: 5\nGenre: 4\nTitle: Primer\nSubject: 5\nFormat: 5\nGenre: 4\nTitle: Time Bandits\nSubject: 5\nFormat: 5\nGenre: 5\"\nAfter parsing:\n       {\n            \"title\": \" The Butterfly Effect\n            \"subject\": \"5\",\n            \"format\": \"5\",\n            \"genre\": \"4\"\n        },\n        {\n            \"title\": \" Primer\",\n            \"subject\": \"5\",\n            \"format\": \"5\",\n            \"genre\": \"4\"\n        },\n        {\n            \"title\": \" Time Bandits\",\n            \"subject\": \"5\",\n            \"format\": \"5\",\n            \"genre\": \"5\"\n        }\nThe output of this parsing_results block now gets passed to the output and is\nwrapped in a list of recommendations. We can see what all this looks like by running\nthe flow.\n Open flow.dag.yaml for the flow in the visual editor, and click the Play (Run All)\nbutton. Be sure to select to use both recommender variants. You\u2019ll see both variations\nrun and output to the terminal.\n At this point, we have a full working recommendation and LLM evaluation flow\nthat outputs a score for each criterion on each output. However, to do comprehensive\nevaluations of a particular profile, we want to generate multiple recommendations\nwith various criteria. We\u2019ll see how to do batch processing of flows in the next section.\n\n\n235\n9.7\nComparing profiles: Getting the perfect profile\n9.7.2\nRunning batch processing in prompt flow\nIn our generic recommendation profile, we want to evaluate how various input crite-\nria can affect the generated recommendations. Fortunately, prompt flow can batch-\nprocess any variations we want to test. The limit is only the time and money we want\nto spend.\n To perform batch processing, we must first create a JSON Lines (JSONL) or JSON\nlist document of our input criteria. If you recall, our input criteria looked like the fol-\nlowing in JSON format:\n{\n    \"subject\": \"time travel\",\n    \"format\": \"books\",\n    \"genre\": \"fantasy\",\n    \"custom\": \"don't include any R rated content\"\n}\nWe want to create a list of JSON objects like that just shown, preferably in a random\nmanner. Of course, the simple way to do this is to prompt ChatGPT to create a JSONL\ndocument using the following prompt:\nI am developing a recommendation agent. The agent will recommend anything given\nthe following criteria:\n1. subject - examples: time travel, cooking, vacation\n2. format - examples: books, movies, games\n3. genre: documentary, action, romance\n4. custom: don't include any R rated content\nCan you please generate a random list of these criteria and output it in the format of\na JSON Lines file, JSONL. Please include 10 items in the list.\nTry this out by going to ChatGPT and entering the preceding prompt. A previously\ngenerated file can be found in the flow folder, called \\bulk_recommend.jsonl. The\ncontents of this file have been shown here for reference:\n{\n  \"subject\": \"time travel\",\n  \"format\": \"books\",\n  \"genre\": \"fantasy\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"space exploration\",\n  \"format\": \"podcasts\",\n  \"genre\": \"sci-fi\",\n  \"custom\": \"include family-friendly content only\"\n}\n\n\n236\nCHAPTER 9\nMastering agent prompts with prompt flow\n{\n  \"subject\": \"mystery\",\n  \"format\": \"podcasts\",\n  \"genre\": \"fantasy\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"space exploration\",\n  \"format\": \"podcasts\",\n  \"genre\": \"action\",\n  \"custom\": \"include family-friendly content only\"\n}\n{\n  \"subject\": \"vacation\",\n  \"format\": \"books\",\n  \"genre\": \"thriller\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"mystery\",\n  \"format\": \"books\",\n  \"genre\": \"sci-fi\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"mystery\",\n  \"format\": \"books\",\n  \"genre\": \"romance\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"vacation\",\n  \"format\": \"movies\",\n  \"genre\": \"fantasy\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"cooking\",\n  \"format\": \"TV shows\",\n  \"genre\": \"thriller\",\n  \"custom\": \"include family-friendly content only\"\n}\n{\n  \"subject\": \"mystery\",\n  \"format\": \"movies\",\n  \"genre\": \"romance\",\n  \"custom\": \"include family-friendly content only\"\n}\nWith this bulk file, we can run both variants using the various input criteria in the bulk\nJSONL file. Open the flow.dag.yaml file in the visual editor, click Batch (the beaker\nicon) to start the bulk-data loading process, and select the file as shown in figure 9.21.\nFor some operating systems, this may appear as Local Data File.\n",
      "page_number": 252
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 261-269)",
      "start_page": 261,
      "end_page": 269,
      "detection_method": "topic_boundary",
      "content": "237\n9.7\nComparing profiles: Getting the perfect profile\nAfter the bulk file is selected, a new YAML document will open with a Run link added\nat the bottom of the file, as shown in figure 9.22. Click the link to do the batch run\nof inputs.\nAt this point, a few things will happen. The flow visual editor will appear, and beside\nthat a log file will open, showing the progress of the run. In the terminal window,\nyou\u2019ll see the various worker processes spawning and running.\n Be patient. The batch run, even for 10 items, may take a few minutes or seconds,\ndepending on various factors such as hardware, previous calls, and so on. Wait for the\nrun to complete, and you\u2019ll see a summary of results in the terminal.\n You can also view the run results by opening the prompt flow extension and select-\ning the last run, as shown in figure 9.23. Then, you dig into each run by clicking the\nSelect a local \ufb01le.\nClick to open the select input source.\nFigure 9.21\nLoading the bulk JSONL file to run the flow on multiple input variations\nClick to run the batch.\nFigure 9.22\nRunning the batch run of inputs\n\n\n238\nCHAPTER 9\nMastering agent prompts with prompt flow\ntable cells. A lot of information is exposed in this dialog, which can help you trouble-\nshoot flows and profiles.\nA lot of information is captured during a batch run, and you can explore much of it\nthrough the visualizer. More information can be found by clicking the output folder\nlink from the terminal window. This will open another session of VS Code with the\noutput folder allowing you to review the run logs and other details.\n Now that we\u2019ve completed the batch run for each variant, we can apply grounding\nand evaluate the results of both prompts. The next section will use a new flow to per-\nform the profile/prompt evaluation.\n9.7.3\nCreating an evaluation flow for grounding\nOpen chapter_3\\prompt_flow\\evaluate_groundings\\flow.dag.yaml in the visual edi-\ntor, as shown in figure 9.24. There are no LLM blocks in the evaluation flow\u2014just\nPython code blocks that will run the scoring and then aggregate the scores.\n We can now look at the code for the scoring and aggregate blocks, starting\nwith the scoring code in listing 9.3. This scoring code averages the score for each\ncriterion into an average score. The output of the function is a list of processed\nrecommendations.\n#2 Right-click a run, and\nselect to open the visualization.\nRecommendations for\na single set of inputs\nLook at the API calls and\ntiming for the various calls.\n#3 Click the recommendations\ncell to view the results.\n#1 Open the prompt\n\ufb02ow extension.\nFigure 9.23\nAn opening run visualization and an examination of a batch run\n\n\n239\n9.7\nComparing profiles: Getting the perfect profile\n@tool\ndef line_process(recommendations: str):    \n    inputs = recommendations\n    output = []\n    for data_dict in inputs:                     \n        total_score = 0\n        score_count = 0\n        for key, value in data_dict.items():     \n                if key != \"title\":    \n                    try:\n                        total_score += float(value)\n                        score_count += 1\n                        data_dict[key] = float(value)    \n                    except:\n                        pass\n        avg_score = total_score / score_count if score_count > 0 else 0\n        data_dict[\"avg_score\"] = round(avg_score, 2)   \n        output.append(data_dict)\n    return output\nListing 9.3\nline_process.py\nline_process block processes and\nscores each recommendation.\naggregate block aggregates\nthe results of scoring.\nFigure 9.24\nLooking at the evaluate_groundings flow used to ground recommendation runs\nA set of three recommendations \nis input into the function.\nLoops over each \nrecommendation \nand criterion\nTitle isn\u2019t a criterion, \nso ignore it.\nTotals the score \nfor all criteria \nand sets the float \nvalue to key\nAdds the average \nscore as a grounding \nscore of the \nrecommendation\n\n\n240\nCHAPTER 9\nMastering agent prompts with prompt flow\nFrom the grounded recommendations, we can move on to aggregating the scores\nwith the aggregate block\u2014the code for the aggregate block is shown in the follow-\ning listing.\n@tool\ndef aggregate(processed_results: List[str]):\n    items = [item for sublist in processed_results \n              \u27a5 for item in sublist]    \n    aggregated = {}\n    for item in items:\n        for key, value in item.items():\n            if key == 'title':\n                continue\n            if isinstance(value, (float, int)):     \n                if key in aggregated:\n                    aggregated[key] += value\n                else:\n                    aggregated[key] = value\n    for key, value in aggregated.items():     \n        value = value / len(items)\n        log_metric(key=key, value=value)    \n        aggregated[key] = value\n    return aggregated\nThe result of the aggregations will be a summary score for each criterion and the aver-\nage score. Since the evaluation/grounding flow is separate, it can be run over any rec-\nommendation run we perform. This will allow us to use the batch run results for any\nvariation to compare results.\n We can run the grounding flow by opening flow.dag.yaml in the visual editor and\nclicking Batch (beaker icon). Then, when prompted, we select an existing run and\nthen select the run we want to evaluate, as shown in figure 9.25. This will open a YAML\nfile with the Run link at the bottom, as we\u2019ve seen before. Click the Run link to run\nthe evaluation.\n After the run is completed, you\u2019ll see a summary of the results in the terminal win-\ndow. You can click the output link to open the folder in VS Code and analyze the\nresults, but there is a better way to compare them.\n Open the prompt flow extension, focus on the Batch Run History window, and\nscroll down to the Run against Run section, as shown in figure 9.26. Select the runs\nyou want to compare\u2014likely the ones near the top\u2014so that the checkmark appears.\nThen, right-click the run, and select the Visualize Runs option. The Batch Run Visual-\nization window opens, and you\u2019ll see the metrics for each of the runs at the top.\nListing 9.4\naggregate.py\nThe input is a list \nof lists; flatten to \na list of items.\nChecks to see if the value \nis numeric and accumulates \nscores for each criterion key\nLoops over aggregated \ncriterion scores\nLogs the criterion \nas a metric\n\n\n241\n9.7\nComparing profiles: Getting the perfect profile\nWe can now see a significant difference between profile/prompt variation 0, the\nuser prompt, and variation 1, the system prompt. Refer to figure 9.15 if you need a\nrefresher on what the prompts/profiles look like. At this point, it should be evi-\ndent that injecting the input parameters into the system prompt provides better\nrecommendations.\n You can now go back and try other profiles or other variant options to see what\neffect this has on your recommendations. The possibilities are virtually endless, but\nhopefully you can see what an excellent tool prompt flow will be for building agent\nprofiles and prompts.\n \n \nSelect Existing Run that is not a local JSON Lines \ufb01le.\nSelect the run you want to evaluate, noting the name.\nFigure 9.25\nLoading a previous run to be grounded and evaluated\n#1 Scroll down to the batch\nRun against the run section.\n#2 Select the runs you want to evaluate,\nand a checkmark appears. Then,\nright-click and select Visualize Runs.\n#3 Compare the aggregated criteria\nresults against each of the runs.\nFigure 9.26\nVisualizing the metrics for multiple runs and comparing them\n\n\n242\nCHAPTER 9\nMastering agent prompts with prompt flow\n9.7.4\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Create a New Prompt Variant for Recommender Flow (Intermediate)\nObjective\u2014Improve the recommendation results by creating and testing a new\nprompt variant in prompt flow.\nTasks:\n\u2013 Create a new prompt variant for the recommender flow in prompt flow.\n\u2013 Run the flow in batch mode.\n\u2013 Evaluate the results to determine if they are better or worse compared to the\noriginal prompt.\n\uf0a1Exercise 2\u2014Add a Custom Field to the Rubric and Evaluate (Intermediate)\nObjective\u2014Enhance the evaluation criteria by incorporating a custom field into\nthe rubric and updating the evaluation flow.\nTasks:\n\u2013 Add the custom field as a new criterion to the rubric.\n\u2013 Update the evaluation flow to score the new criterion.\n\u2013 Evaluate the results, and analyze the effect of the new criterion on the\nevaluation.\n\uf0a1Exercise 3\u2014Develop a New Use Case and Evaluation Rubric (Advanced)\nObjective\u2014Expand the application of prompt engineering by developing a new\nuse case and creating an evaluation rubric.\nTasks:\n\u2013 Develop a new use case aside from the recommendation.\n\u2013 Build the prompt for the new use case.\n\u2013 Create a rubric for evaluating the new prompt.\n\u2013 Update or alter the evaluation flow to aggregate and compare the results of\nthe new use case with existing ones.\n\uf0a1Exercise 4\u2014Evaluate Other LLMs Using LM Studio (Intermediate)\nObjective\u2014Assess the performance of different open source LLMs by hosting a\nlocal server with LM Studio.\nTasks:\n\u2013 Use LM Studio to host a local server for evaluating LLMs.\n\u2013 Evaluate other open source LLMs.\n\u2013 Consult chapter 2 if assistance is needed for setting up the server and per-\nforming the evaluations.\n\uf0a1Exercise 5\u2014Build and Evaluate Prompts Using Prompt Flow (Intermediate)\nObjective\u2014Apply prompt engineering strategies to build and evaluate new prompts\nor profiles using prompt flow.\n\n\n243\nSummary\nTasks:\n\u2013 Build new prompts or profiles for evaluation using prompt flow.\n\u2013 Apply the Write Clear Instructions prompt engineering strategy from chap-\nter 2.\n\u2013 Evaluate the prompts and profiles using prompt flow.\n\u2013 Refer to chapter 2 for tactics and implementation details if a refresher is\nneeded.\nSummary\n\uf0a1An agent profile consists of several other component prompts that can drive\nfunctions such as actions/tools, knowledge, memory, evaluation, reasoning, feed-\nback, and planning.\n\uf0a1Prompt flow can be used to evaluate an agent\u2019s component prompts.\n\uf0a1Systemic prompt engineering is an iterative process evaluating a prompt and\nagent profile.\n\uf0a1The Test Changes Systematically strategy describes iterating and evaluating\nprompts, and system prompt engineering implements this strategy.\n\uf0a1Agent profiles and prompt engineering have many similarities. We define an\nagent profile as the combination of prompt engineering elements that guide\nand help an agent through its task.\n\uf0a1Prompt flow is an open source tool from Microsoft that provides several fea-\ntures for developing and evaluating profiles and prompts.\n\uf0a1An LLM connection in prompt flow supports additional parameters, including\ntemperature, stop token, max tokens, and other advanced parameters.\n\uf0a1LLM blocks support prompt and profile variants, which allow for evaluating\nchanges to the prompt/profile or other connection parameters.\n\uf0a1A rubric applied to an LLM prompt is the criteria and standards a prompt/profile\nmust fulfill to be grounded. Grounding is the scoring and evaluation of a rubric.\n\uf0a1Prompt flow supports running multiple variations as single runs or batch runs.\n\uf0a1In prompt flow, an evaluation flow is run after a generative flow to score and\naggregate the results. The Visualize Runs option can compare the aggregated\ncriteria from scoring the rubric across multiple runs.\n\n\n244\nAgent reasoning\nand evaluation\nNow that we\u2019ve examined the patterns of memory and retrieval that define the\nsemantic memory component in agents, we can take a look at the last and most\ninstrumental component in agents: planning. Planning encompasses many facets,\nfrom reasoning, understanding, and evaluation to feedback.\n To explore how LLMs can be prompted to reason, understand, and plan, we\u2019ll\ndemonstrate how to engage reasoning through prompt engineering and then\nexpand that to planning. The planning solution provided by the Semantic Kernel\n(SK) encompasses multiple planning forms. We\u2019ll finish the chapter by incorporat-\ning adaptive feedback into a new planner.\n Figure 10.1 demonstrates the high-level prompt engineering strategies we\u2019ll\ncover in this chapter and how they relate to the various techniques we\u2019ll cover. Each\nThis chapter covers\n\uf0a1Using various prompt engineering techniques \nto extend large language model functions\n\uf0a1Engaging large language models with prompt \nengineering techniques that engage reasoning\n\uf0a1Employing an evaluation prompt to narrow \nand identify the solution to an unknown \nproblem \n\n\n245\n10.1\nUnderstanding direct solution prompting\nof the methods showcased in the figure will be explored in this chapter, from the\nbasics of solution/direct prompting, shown in the top-left corner, to self-consistency\nand tree of thought (ToT) prompting, in the bottom right.\n10.1\nUnderstanding direct solution prompting\nDirect solution prompting is generally the first form of prompt engineering that users\nemploy when asking LLMs questions or solving a particular problem. Given any LLM\nuse, these techniques may seem apparent, but they are worth reviewing to establish\nthe foundation of thought and planning. In the next section, we\u2019ll start from the\nbeginning, asking questions and expecting answers.\nSplit Complex Tasks into Simpler Subtasks\nReduces error rates.\nTactics include intent classi cation, summarizing\n\ufb01\ndialogues, and piecewise summarization of documents.\nPlanning\nGive Models\nTime to\n\u201cThink\u201d\nAllows more\nreliable\nreasoning.\nTactics involve\nworking out\nsolutions\nbefore\nconclusions,\nusing inner\nmonologue,\nand reviewing\nprevious\nanswers.\nPlanning\nSolutions\u2014prompts in this\ngroup are direct and may include\nexamples of completed tasks.\nPrompt\nchaining\nQuestion and\n-\n-\nanswer prompting\nZero-shot\nprompting\nOne-shot\nprompting\nSelf-consistency\nprompting\nTree of thought\nprompting\nAutomatic easoning\nr\nwith tools (ART)\nPlanners\nAdaptive constructive\nfeedback\nChain of thought\nprompting\nReasoning\nprompts demonstrate\n\u2014\nreasoning by encouraging a thought\nprocess and solving a sequence of\nthoughts.\nEvaluation\u2014multiple\nprompts are generated\nand evaluated based on\nthe goal.\nTooling\nthe ability to use\n\u2014\ntools, actions, planners, and\nspecialized prompts to solve\ntasks and high-level goals.\nMulti-agent\nsystems\nFeedbace\nxtends\nk\u2014\nplanning but also\nengages with perceived\nor genuine feedback to\ncontinually adapt and\nimprove plans.\nThink of these as axes:\nthe y-axis represents\nthought, and the x-axis\nrepresents planning.\nPrompt Engineering Strategies\nEvaluation and Feedback\nAll of these\nconcepts will\nbe covered in\nthis chapter.\n.\nThought\nFigure 10.1\nHow the two planning prompt engineering strategies align with the various techniques\n",
      "page_number": 261
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 270-277)",
      "start_page": 270,
      "end_page": 277,
      "detection_method": "topic_boundary",
      "content": "246\nCHAPTER 10\nAgent reasoning and evaluation\n10.1.1 Question-and-answer prompting\nFor the exercises in this chapter, we\u2019ll employ prompt flow to build and evaluate the var-\nious techniques. (We already extensively covered this tool in chapter 9, so refer to that\nchapter if you need a review.) Prompt flow is an excellent tool for understanding how\nthese techniques work and exploring the flow of the planning and reasoning process.\n Open Visual Studio Code (VS Code) to the chapter 10 source folder. Create a new\nvirtual environment for the folder, and install the requirements.txt file. If you need\nhelp setting up a chapter\u2019s Python environment, refer to appendix B.\n We\u2019ll look at the first flow in the prompt_flow/question-answering-prompting\nfolder. Open the flow.dag.yaml file in the visual editor, as shown in figure 10.2. On\nthe right side, you\u2019ll see the flow of components. At the top is the question_answer\nLLM prompt, followed by two Embedding components and a final LLM prompt to do\nthe evaluation called evaluate.\nThe breakdown in listing 10.1 shows the structure and components of the flow in\nmore detail using a sort of YAML-shortened pseudocode. You can also see the input\nand outputs to the various components and a sample output from running the flow.\n   Inputs:\n        context\u00a0 : the content to ask the question about\nListing 10.1\nquestion-answer-prompting flow\nPrompt \ufb02ow folder to open\nOpen the \ufb02ow.dag.yaml\n\ufb01le in the visual editor.\nSelect the question_answer\nLLM component.\nShows the visual\n\ufb02ow of the DAG\nmaking up the \ufb02ow\nEmbeds the expected and\npredicted answer so their\nsimilarity can be evaluated\nFigure 10.2\nThe flow.dag.yaml file, open in the visual editor, highlighting the various components of the flow\n\n\n247\n10.1\nUnderstanding direct solution prompting\n        question\u00a0: question asked specific to the content\n        expected\u00a0: the expected answer\n   LLM: Question-Answer (the prompt used to ask the question)\n        inputs:\n               context and question\n        outputs: \n               the prediction/answer to the question\n   Embeddings: uses an LLM embedding model to create the embedding \nrepresentation of the text\n     Embedding_predicted: embeds the output of the Question-Answer LLM\n     Embedding_expected: embeds the output of the expected answer\n   Python: Evaluation (Python code to measure embedding similarity)\n     Inputs:\n            Embedding_predicted output\n            Embedding_expected output\n     Outputs: \n            the similarity score between predicted and expected\n   \n   Outputs:\n        context: -> input.context\n        question: -> input.question\n     expected: -> input.expected\n     predicted: -> output.question_answer\n     evaluation_score: output.evaluation\n### Example Output\n{\n    \"context\": \"Back to the Future (1985)\u2026\",\n    \"evaluation_score\": 0.9567478002354606,\n    \"expected\": \"Marty traveled back in time 30 years.\",\n    \"predicted\": \"Marty traveled back in time 30 years from 1985 to 1955 \nin the movie \\\"Back to the Future.\\\"\",\n    \"question\": \"How far did Marty travel back in time in the movie \nBack to the Future (1985)\"\n}\nBefore running this flow, make sure your LLM block is configured correctly. This may\nrequire you to set up a connection to your chosen LLM. Again, refer to chapter 9 if\nyou need a review on how to complete this. You\u2019ll need to configure the LLM and\nEmbedding blocks with your connection if you\u2019re not using OpenAI.\n After configuring your LLM connection, run the flow by clicking the Play but-\nton from the visual editor or using the Test (Shift-F5) link in the YAML editor win-\ndow. If everything is connected and configured correctly, you should see output\nlike that in listing 10.1.\n Open the question_answer.jinja2 file in VS Code, as shown in listing 10.2. This\nlisting shows the basic question-and-answer-style prompt. In this style of prompt, the\nsystem message describes the basic rules and provides the context to answer the question.\n\n\n248\nCHAPTER 10\nAgent reasoning and evaluation\nIn chapter 4, we explored the retrieval augmented generation (RAG) pattern, and\nthis prompt follows a similar pattern.\nsystem:\nAnswer the users question based on the context below. Keep the answer \nshort and concise. Respond \"Unsure about answer\" if not sure about the \nanswer.\nContext: {{context}}    \nuser:\nQuestion: {{question}}    \nThis exercise shows the simple method of using an LLM to ask questions about a\npiece of content. Then, the question response is evaluated using a similarity matching\nscore. We can see from the output in listing 10.1 that the LLM does a good job of\nanswering a question about the context. In the next section, we\u2019ll explore a similar\ntechnique that uses direct prompting.\n10.1.2 Implementing few-shot prompting\nFew-shot prompting is like question-and-answer prompting, but the makeup of the\nprompt is more about providing a few examples than about facts or context. This\nallows the LLM to bend to patterns or content not previously seen. While this\napproach sounds like question and answer, the implementation is quite different,\nand the results can be powerful.\nOpen prompt_flow/few-shot-prompting/flow.dag.yaml in VS Code and the visual\neditor. Most of the flow looks like the one pictured earlier in figure 10.2, and the dif-\nferences are highlighted in listing 10.3, which shows a YAML pseudocode represen-\ntation. The main differences between this and the previous flow are the inputs and\nLLM prompt.\n \n \nListing 10.2\nquestion_answer.jinja2\nZero-shot, one-shot, and few-shot learning\nOne holy grail of machine learning and AI is the ability to train a model on as few items\nas possible. For example, in traditional vision models, millions of images are fed into\nthe model to help identify the differences between a cat and a dog.\nA one-shot model is a model that requires only a single image to train it. For example,\na picture of a cat can be shown, and then the model can identify any cat image. A\nfew-shot model requires only a few things to train the model. And, of course, zero-shot\nindicates the ability to identify something given no previous examples. LLMs are effi-\ncient learners and can do all three types of learning.\nReplace with the content LLM \nshould answer the question about.\nReplace with \nthe question.\n\n\n249\n10.1\nUnderstanding direct solution prompting\n   Inputs:\n       statement\u00a0 : introduces the context and then asks for output\n       expected\u00a0: the expected answer to the statement\n   LLM: few_shot (the prompt used to ask the question)\n       inputs:statement\n       outputs: the prediction/answer to the statement\n   Embeddings: uses an LLM embedding model to create the embedding \nrepresentation of the text\n        Embedding_predicted: embeds the output of the few_shot LLM\n        Embedding_expected: embeds the output of the expected answer\n   \n   Python: Evaluation (Python code to measure embedding similarity)\n        Inputs:\n               Embedding_predicted output\n               Embedding_expected output\n        Outputs: the similarity score between predicted and expected\nOutputs:\n        statement: -> input.statement\n        expected: -> input.expected\n        predicted: -> output.few_shot\n        evaluation_score: output.evaluation\n### Example Output\n{\n    \"evaluation_score\": 0.906647282920417,    \n    \"expected\": \"We ate sunner and watched the setting sun.\",\n    \"predicted\": \"After a long hike, we sat by the lake \nand enjoyed a peaceful sunner as the sky turned \nbrilliant shades of orange and pink.\",    \n    \"statement\": \"A sunner is a meal we eat in Cananda \nat sunset, please use the word in a sentence\"    \n}\nRun the flow by pressing Shift-F5 or clicking the Play/Test button from the visual edi-\ntor. You should see output like listing 10.3 where the LLM has used the word sunner (a\nmade-up term) correctly in a sentence given the initial statement.\n This exercise demonstrates the ability to use a prompt to alter the behavior of the\nLLM to be contrary to what it has learned. We\u2019re changing what the LLM understands\nto be accurate. Furthermore, we then use that modified perspective to elicit the use of\na made-up word.\n Open the few_shot.jinja2 prompt in VS Code, shown in listing 10.4. This listing\ndemonstrates setting up a simple persona, that of an eccentric dictionary maker, and\nthen providing examples of words it has defined and used before. The base of the\nListing 10.3\nfew-shot-prompting flow\nEvaluation score \nrepresents the \nsimilarity between \nexpected and \npredicted.\nUses sunner in \na sentence\nThis is a false statement but the \nintent is to get the LLM to use \nthe word as if it was real.\n\n\n250\nCHAPTER 10\nAgent reasoning and evaluation\nprompt allows for the LLM to extend the examples and produce similar results using\nother words.\nsystem:\nYou are an eccentric word dictionary maker. You will be asked to \nconstruct a sentence using the word.\nThe following are examples that demonstrate how to craft a sentence using \nthe word.\nA \"whatpu\" is a small, furry animal native to Tanzania. \nAn example of a sentence that uses the word whatpu is:    \nWe were traveling in Africa and we saw these very cute whatpus.\nTo do a \"farduddle\" means to jump up and down really fast. An example of a \nsentence that uses the word farduddle is:\nI was so excited that I started to farduddle.    \nPlease only return the sentence requested by the user.  \nuser:\n{{statement}}   \nYou may say we\u2019re forcing the LLM to hallucinate here, but this technique is the basis\nfor modifying behavior. It allows prompts to be constructed to guide an LLM to do\neverything contrary to what it learned. This foundation of prompting also establishes\ntechniques for other forms of altered behavior. From the ability to alter the percep-\ntion and background of an LLM, we\u2019ll move on to demonstrate a final example of a\ndirect solution in the next section.\n10.1.3 Extracting generalities with zero-shot prompting\nZero-shot prompting or learning is the ability to generate a prompt in such a manner that\nallows the LLM to generalize. This generalization is embedded within the LLM and\ndemonstrated through zero-shot prompting, where no examples are given, but instead a\nset of guidelines or rules are given to guide the LLM.\n Employing this technique is simple and works well to guide the LLM to generate\nreplies given its internal knowledge and no other contexts. It\u2019s a subtle yet powerful\ntechnique that applies the knowledge of the LLM to other applications. This tech-\nnique, combined with other prompting strategies, is proving effective at replacing\nother language classification models\u2014models that identify the emotion or sentiment\nin text, for example.\n Open prompt_flow/zero-shot-prompting/flow.dag.yaml in the VS Code prompt\nflow visual editor. This flow is again almost identical to that shown earlier in figure 10.1\nbut differs slightly in implementation, as shown in listing 10.5.\n \n \nListing 10.4\nfew_shot.jinja2\nDemonstrates an example defining a made-\nup word and using it in a sentence\nDemonstrates \nanother example\nA rule to prevent the \nLLM from outputting \nextra information\nThe input statement defines a \nnew word and asks for the use.\n\n\n251\n10.1\nUnderstanding direct solution prompting\n   Inputs:\n        statement\u00a0 : the statement to be classified\n        expected\u00a0: the expected classification of the statement\n   \n    LLM: zero_shot (the prompt used to classify)\n        inputs: statement\n        outputs: the predicted class given the statement\n    Embeddings: uses an LLM embedding model to create the embedding \nrepresentation of the text\n    Embedding_predicted: embeds the output of the zero_shot LLM\n    Embedding_expected: embeds the output of the expected answer\n    Python: Evaluation (Python code to measure embedding similarity)\n        Inputs:\n               Embedding_predicted output\n             Embedding_expected output\n          Outputs: the similarity score between predicted and expected\n   Outputs:\n        statement: -> input.statement\n        expected: -> input.expected\n        predicted: -> output.few_shot\n        evaluation_score: output.evaluation\n   ### Example Output\n{\n       \"evaluation_score\": 1,    \n       \"expected\": \"neutral\",\n       \"predicted\": \"neutral\",\n       \"statement\": \"I think the vacation is okay. \"    \n   }\nRun the flow by pressing Shift-F5 within the VS Code prompt flow visual editor. You\nshould see output similar to that shown in listing 10.5.\n Now open the zero_shot.jinja2 prompt as shown in listing 10.6. The prompt is\nsimple and uses no examples to extract the sentiment from the text. What is especially\ninteresting to note is that the prompt doesn\u2019t even mention the phrase sentiment, and\nthe LLM seems to understand the intent.\nsystem:\nClassify the text into neutral, negative or positive. \nReturn on the result and nothing else.    \nuser:\n{{statement}}    \nListing 10.5\nzero-shot-prompting flow\nListing 10.6\nzero_shot.jinja2\nShows a perfect \nevaluation score \nof 1.0\nThe statement \nwe\u2019re asking the \nLLM to classify\nProvides essential \nguidance on \nperforming the \nclassification\nThe statement of \ntext to classify\n\n\n252\nCHAPTER 10\nAgent reasoning and evaluation\nZero-shot prompt engineering is about using the ability of the LLM to generalize\nbroadly based on its training material. This exercise demonstrates how knowledge\nwithin the LLM can be put to work for other tasks. The LLM\u2019s ability to self-contextu-\nalize and apply knowledge can extend beyond its training. In the next section, we\nextend this concept further by looking at how LLMs can reason.\n10.2\nReasoning in prompt engineering\nLLMs like ChatGPT were developed to function as chat completion models, where\ntext content is fed into the model, whose responses align with completing that request.\nLLMs were never trained to reason, plan, think, or have thoughts.\n However, much like we demonstrated with the examples in the previous section,\nLLMs can be prompted to extract their generalities and be extended beyond their\ninitial design. While an LLM isn\u2019t designed to reason, the training material fed into\nthe model provides an understanding of reasoning, planning, and thought. There-\nfore, by extension, an LLM understands what reasoning is and can employ the con-\ncept of reasoning.\nWe\u2019ll look at another set of prompt engineering techniques that allow or mimic rea-\nsoning behavior to demonstrate this reasoning ability. Typically, when evaluating the\napplication of reasoning, we look to having the LLM solve challenging problems it\nwasn\u2019t designed to solve. A good source of such is based on logic, math, and word\nproblems.\n Using the time travel theme, what class of unique problems could be better to\nsolve than understanding time travel? Figure 10.3 depicts one example of a uniquely\nchallenging time travel problem. Our goal is to acquire the ability to prompt the LLM\nin a manner that allows it to solve the problem correctly.\n Time travel problems are thought exercises that can be deceptively difficult to\nsolve. The example in figure 10.3 is complicated to solve for an LLM, but the part it\nReasoning and planning\nReasoning is the ability of an intellect, artificial or not, to understand the process of\nthought or thinking through a problem. An intellect can understand that actions have\noutcomes, and it can use this ability to reason through which action from a set of\nactions can be applied to solve a given task.\nPlanning is the ability of the intellect to reason out the order of actions or tasks and\napply the correct parameters to achieve a goal or outcome\u2014the extent to which an\nintellectual plan depends on the scope of the problem. An intellect may combine mul-\ntiple levels of planning, from strategic and tactical to operational and contingent.\n\n\n253\n10.2\nReasoning in prompt engineering\ngets wrong may surprise you. The next section will use reasoning in prompts to solve\nthese unique problems.\n10.2.1 Chain of thought prompting\nChain of thought (CoT)prompting is a prompt engineering technique that employs\nthe one-shot or few-shot examples that describe the reasoning and the steps to\naccomplish a desired goal. Through the demonstration of reasoning, the LLM can\ngeneralize this principle and reason through similar problems and goals. While the\nLLM isn\u2019t trained with the goal of reasoning, we can elicit the model to reason, using\nprompt engineering.\n Open prompt_flow/chain-of-thought-prompting/flow.dag.yaml in the VS Code\nprompt flow visual editor. The elements of this flow are simple, as shown in figure 10.4.\nWith only two LLM blocks, the flow first uses a CoT prompt to solve a complex ques-\ntion; then, the second LLM prompt evaluates the answer.\n Listing 10.7 shows the YAML pseudocode that describes the blocks and the\ninputs/outputs of the flow in more detail. The default problem statement in this\nexample isn\u2019t the same as in figure 10.3.\n \n \n \nIn a sci-\ufb01\ufb01lm, Alex is a time traveler who decides to go back in\ntime to witness a famous historical battle that took place 100 years ago,\nwhich lasted for 10 days. He arrives three days before the battle starts.\nHowever, after spending six days in the past, he jumps forward in time by\n50 years and stays there for 20 days. Then, he travels back to witness the\nend of the battle. How many days does Alex spend in the past before he\nsees the end of the battle?\nAlex\nTimeline\nPresent\nHistoric\nbattle starts\nHistoric\nbattle ends\nAlex\narrives\nJumps back in time 100 years (3 days before the battle)\nspends 6 days\njumps forward in time 50 years\n20 days\njumps back to the present\nAlex\nleaves\nAlex\nreturns?\nAlex\narrives\nAlex\nleaves\n???\nThe problem statement:\nThe statement is vague as\nto what time Alex returns\nto the battle. Is it the same\ntime he left?\njumps back to witness end of battle\nbattle lasts 10 days\nFigure 10.3\nThe complexity of the time travel problems we intend to solve using LLMs with reasoning and \nplanning\n",
      "page_number": 270
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 278-287)",
      "start_page": 278,
      "end_page": 287,
      "detection_method": "topic_boundary",
      "content": "254\nCHAPTER 10\nAgent reasoning and evaluation\n   Inputs:\n        statement\u00a0 : the statement problem to be solved\n        expected\u00a0: the expected solution to the problem\n   LLM: cot (the prompt used to solve the problem)\n        inputs: statement\n        outputs: the predicted answer given the problem statement\n   LLM: evaluate_answer (the prompt used to evaluate the solution)\n        inputs:\n               statement: -> input.statement\n               expected: -> input.expected\n               predicted: -> output.cot\n        outputs: a score of how well the problem was answered\nListing 10.7\nchain-of-thought-prompting flow\nLLM prompt that uses chain of\nthought to answer a given problem\nLLM prompt that evaluates\nthe ef\ufb01cacy of the answer\nFigure 10.4\nThe flow of the CoT\n\n\n255\n10.2\nReasoning in prompt engineering\n   Outputs:\n        statement: -> input.statement\n        expected: -> input.expected\n        predicted: -> output.cot\n        evaluation_score: output.evaluate_answer\n   ### Example Output\n{\n    \"evaluation_score\": \"0.5\",    \n    \"expected\": \"After the final jump, Max finds himself \nin the year 1980 and he is 75 years old.\",    \n    \"predicted\": \" Max starts in the year 2300 and \ntravels backward in 40-year increments, spending 5 years \nin each period. The journeys will be as follows:\n\\n\\n- From 2300 to 2260: Max is 25 + 5 = 30 years old.\n\\n- From 2260 to 2220: Max is 30 + 5 = 35 years old.\n\\n- From 2220 to 2180: Max is 35 + 5 = 40 years old.\n\\n- From 2180 to 2140: Max is 40 + 5 = 45 years old.\n\\n- From 2140 to 2100: Max is 45 + 5 = 50 years old.\n\\n- From 2100 to 2060: Max is 50 + 5 = 55 years old.\n\\n- From 2060 to 2020: Max is 55 + 5 = 60 years old.\n\\n- From 2020 to 1980: Max is 60 + 5 = 65 years old.\n\\n- From 1980 to 1940: Max is 65 + 5 = 70 years old.\n\\n- From 1940 to 1900: Max is 70 + 5\"    \n}\nDig into the inputs and check the problem statement; try to evaluate the problem\nyourself. Then, run the flow by pressing Shift-F5. You should see output similar to that\nshown in listing 10.7.\n Open the cot.jinja2 prompt file as shown in listing 10.8. This prompt gives a few\nexamples of time travel problems and then the thought-out and reasoned solution.\nThe process of showing the LLM the steps to complete the problem provides the rea-\nsoning mechanism.\nsystem:\n\"In a time travel movie, Sarah travels back in time to \nprevent a historic event from happening. She arrives \n2 days before the event. After spending a day preparing, \nshe attempts to change the event but realizes she has \nactually arrived 2 years early, not 2 days. She then \ndecides to wait and live in the past until the event's \noriginal date. How many days does Sarah spend in the past \nbefore the day of the event?\"    \nChain of Thought:    \n    Initial Assumption: Sarah thinks she has arrived 2 days before the event.\n    Time Spent on Preparation: 1 day spent preparing.\n    Realization of Error: Sarah realizes she's actually 2 years early.\n    Conversion of Years to Days: \nListing 10.8\ncot.jinja2\nThe evaluated \nscore for the \ngiven solution\nThe expected \nanswer for the \nproblem\nThe predicted answer \nshows the reasoning \nsteps and output.\nA few example \nproblem \nstatements\nThe solution \nto the problem \nstatement, output \nas a sequence of \nreasoning steps\n\n\n256\nCHAPTER 10\nAgent reasoning and evaluation\n2 years = 2 \u00d7 365 = 730 days (assuming non-leap years).\n    Adjust for the Day Spent Preparing: 730 - 1 = 729 days.\n    Conclusion: Sarah spends 729 days in the past before the day of the event.\n\"In a sci-fi film, Alex is a time traveler who decides \nto go back in time to witness a famous historical battle \nthat took place 100 years ago, which lasted for 10 days. \nHe arrives three days before the battle starts. However, \nafter spending six days in the past, he jumps forward in \ntime by 50 years and stays there for 20 days. Then, he \ntravels back to witness the end of the battle. How many \ndays does Alex spend in the past before he sees the end of\n the battle?\"    \nChain of Thought:    \n    Initial Travel: Alex arrives three days before the battle starts.\n    Time Spent Before Time Jump: Alex spends six days in the past. \nThe battle has started and has been going on for 3 days (since he \narrived 3 days early and has now spent 6 days, 3 + 3 = 6).\n    First Time Jump: Alex jumps 50 years forward and stays for 20 days.\n This adds 20 days to the 6 days he's already spent in the past \n(6 + 20 = 26).\n    Return to the Battle: When Alex returns, he arrives back on the same \nday he left (as per time travel logic). The battle has been going on for \n3 days now.\n    Waiting for the Battle to End: The battle lasts 10 days. Since he's \nalready witnessed 3 days of it, he needs to wait for 7 more days.\n    Conclusion: Alex spends a total of 3 (initial wait) + 3 (before the \nfirst jump) + 20 (50 years ago) + 7 (after returning) = 33 days in the \npast before he sees the end of the battle.\nThink step by step but only show the final answer to the statement.\nuser:\n{{statement}}    \nYou may note that the solution to figure 10.3 is also provided as an example in listing\n10.8. It\u2019s also helpful to go back and review listing 10.7 for the reply from the LLM\nabout the problem. From this, you can see the reasoning steps the LLM applied to get\nits final answer.\n Now, we can look at the prompt that evaluates how well the solution solved the\nproblem. Open evaluate_answer.jinja2, shown in listing 10.9, to review the prompt\nused. The prompt is simple, uses zero-shot prompting, and allows the LLM to general-\nize how it should score the expected and predicted. We could provide examples and\nscores, thus changing this to an example of a few-shot classification.\nsystem:\nPlease confirm that expected and predicted results are \nthe same for the given problem.    \nListing 10.9\nevaluate_answer.jinja2\nA few example \nproblem \nstatements\nThe solution \nto the problem \nstatement, output \nas a sequence of \nreasoning steps\nThe problem statement the \nLLM is directed to solve\nThe rules for \nevaluating the \nsolution\n\n\n257\n10.2\nReasoning in prompt engineering\nReturn a score from 0 to 1 where 1 is a perfect match and 0 is no match.\nPlease just return the score and not the explanation.    \nuser:\nProblem: {{problem}}    \nExpected result: {{expected}}    \nPredicted result: {{predicted}}    \nLooking at the LLM output shown earlier in listing 10.7, you can see why the evalua-\ntion step may get confusing. Perhaps a fix to this could be suggesting to the LLM to\nprovide the final answer in a single statement. In the next section, we move on to\nanother example of prompt reasoning.\n10.2.2 Zero-shot CoT prompting\nAs our time travel demonstrates, CoT prompting can be expensive in terms of prompt\ngeneration for a specific class of problem. While not as effective, there are techniques\nsimilar to CoT that don\u2019t use examples and can be more generalized. This section will\nexamine a straightforward phrase employed to elicit reasoning in LLMs.\n Open prompt_flow/zero-shot-cot-prompting/flow.dag.yaml in the VS Code\nprompt flow visual editor. This flow is very similar to the previous CoT, as shown in fig-\nure 10.4. The next lsting shows the YAML pseudocode that describes the flow.\n   Inputs:\n        statement\u00a0 : the statement problem to be solved\n        expected\u00a0: the expected solution to the problem\n    \n   LLM: cot (the prompt used to solve the problem)\n        inputs: statement\n        outputs: the predicted answer given the problem statement\n    \n   LLM: evaluate_answer (the prompt used to evaluate the solution)\n        inputs:\n               statement: -> input.statement\n               expected: -> input.expected\n               predicted: -> output.cot\n    \n         outputs: a score of how well the problem was answered\n    \n    \n    Outputs:\n        statement: -> input.statement\n        expected: -> input.expected\n        predicted: -> output.cot\nListing 10.10\nzero-shot-CoT-prompting flow\nDirection to only \nreturn the score \nand nothing else\nThe initial \nproblem \nstatement\nThe expected or \ngrounded answer\nThe output from the\nCoT prompt earlier\n\n\n258\nCHAPTER 10\nAgent reasoning and evaluation\n        evaluation_score: output.evaluate_answer\n    \n    ### Example Output\n   {\n       \"evaluation_score\": \"1\",    \n       \"expected\": \"After the final jump, \n          \u27a5 Max finds himself in the year 1980 and \n   he is 75 years old.\",    \n       \"predicted\": \"Max starts in\u2026 \n          \u27a5 Therefore, after the final jump, \n          \u27a5 Max is 75 years old and in the year 1980.\",    \n       \"statement\": \"In a complex time travel \u2026\"    \n   }\nRun/test the flow in VS Code by pressing Shift-F5 while in the visual editor. The flow\nwill run, and you should see output similar to that shown in listing 10.10. This exercise\nexample performs better than the previous example on the same problem.\n Open the cot.jinja2 prompt in VS Code, as shown in listing 10.11. This is a much\nsimpler prompt than the previous example because it only uses zero-shot. However, one\nkey phrase turns this simple prompt into a powerful reasoning engine. The line in the\nprompt Let\u2019s think step by step triggers the LLM to consider internal context show-\ning reasoning. This, in turn, directs the LLM to reason out the problem in steps.\nsystem:\nYou are an expert in solving time travel problems.\nYou are given a time travel problem and you have to solve it.\nLet's think step by step.    \nPlease finalize your answer in a single statement.    \nuser:\n{{statement}}    \nSimilar phrases asking the LLM to think about the steps or asking it to respond in\nsteps also extract reasoning. We\u2019ll demonstrate a similar but more elaborate tech-\nnique in the next section.\n10.2.3 Step by step with prompt chaining\nWe can extend the behavior of asking an LLM to think step by step into a chain of\nprompts that force the LLM to solve the problem in steps. In this section, we look at a\ntechnique called prompt chaining that forces an LLM to process problems in steps.\n Open the prompt_flow/prompt-chaining/flow.dag.yaml file in the visual editor,\nas shown in figure 10.5. Prompt chaining breaks up the reasoning method used to\nsolve a problem into chains of prompts. This technique forces the LLM to answer the\nproblem in terms of steps.\nListing 10.11\ncot.jinja2\nThe final \nevaluation score\nThe expected \nanswer\nThe predicted \nanswer (the \nsteps have been \nomitted showing \nthe final answer)\nThe initial problem\nstatement\nA magic line \nthat formulates \nreasoning from \nthe LLM\nAsks the LLM to \nprovide a final \nstatement of the \nanswer\nThe problem \nstatement the LLM \nis asked to solve\n\n\n259\n10.2\nReasoning in prompt engineering\nListing 10.12 shows the YAML pseudocode that describes the flow in a few more\ndetails. This flow chains the output of the first LLM block into the second and then\nfrom the second into the third. Forcing the LLM to process the problem this way\nuncovers the reasoning pattern, but it can also be overly verbose.\n   Inputs:\n        statement  : the statement problem to be solved\n    \n   LLM: decompose_steps (the prompt used to decompose the problem)\n        inputs: \n               statement: -> input.statement    \n    \n        outputs: the breakdown of steps to solve the problem\n   LLM: calculate_steps (the prompt used to calculate the steps)\n        inputs:\n               statement: -> input.statement\n               decompose_steps: -> output.decompose_steps    \n    \n               outputs: the calculation for each step\nListing 10.12\nprompt-chaining flow\nLLM then calculates the\noutput for each step.\nLLM calculates the\nsolution using the steps.\nLLM prompt \ufb01rst decomposes\nthe problem into steps.\nFigure 10.5\nThe prompt chaining flow\nStart of the chain \nof prompts\nOutput from \nthe previous \nstep injected \ninto this step\n\n\n260\nCHAPTER 10\nAgent reasoning and evaluation\n   LLM: calculate_solution (attempts to solve the problem)\n        inputs:\n               statement: -> input.statement\n               decompose_steps: -> output.decompose_steps\n               calculate_steps: -> output.calculate_steps    \n         outputs: the final solution statement\n   Outputs:\n        statement: -> input.statement\n        decompose_steps: -> output.decompose_steps\n        calculate_steps: -> output.calculate_steps\n        calculate_solution: -> output.calculate_solution\n   ### Example Output\n{\n    \"calculate_steps\": \"1. The days spent by Alex\",\n    \"decompose_steps\": \"To figure out the \u2026\",\n    \"solution\": \"Alex spends 13 days in the \n           \u27a5 past before the end of the battle.\",    \n    \"statement\": \"In a sci-fi film, Alex \u2026\"    \n}\nRun the flow by pressing Shift-F5 from the visual editor, and you\u2019ll see the output as\nshown in listing 10.12. The answer is still not correct for the Alex problem, but we can\nsee all the work the LLM is doing to reason out the problem.\n Open up all three prompts: decompose_steps.jinja2, calculate_steps.jinja2,\nand calculate_solution.jinja2 (see listings 10.13, 10.14, and 10.15, respectively).\nAll three prompts shown in the listings can be compared to show how outputs chain\ntogether.\nsystem:\nYou are a problem solving AI assistant.\nYour job is to break the users problem down into smaller steps and list \nthe steps in the order you would solve them.\nThink step by step, not in generalities.\nDo not attempt to solve the problem, just list the steps.\nuser:\n{{statement}}    \nsystem:\nYou are a problem solving AI assistant.\nYou will be given a list of steps that solve a problem.\nYour job is to calculate the output for each of the steps in order.\nDo not attempt to solve the whole problem,\nListing 10.13\ndecompose_steps.jinja2\nListing 10.14\ncalculate_steps.jinja2\nOutput from \ntwo previous \nsteps injected \ninto this step\nThe final solution \nstatement, \nalthough wrong, \nis closer.\nForces the LLM to \nlist only the steps \nand nothing else\nThe initial problem \nstatement\n\n\n261\n10.3\nEmploying evaluation for consistent solutions\njust list output for each of the steps.    \nThink step by step.    \nuser:\n{{statement}}\n{{steps}}    \nsystem:\nYou are a problem solving AI assistant.\nYou will be given a list of steps and the calculated output for each step.\nUse the calculated output from each step to determine the final \nsolution to the problem.\nProvide only the final solution to the problem in a \nsingle concise sentence. Do not include any steps \nin your answer.    \nuser:\n{{statement}}\n{{steps}}    \n{{calculated}}    \nIn this exercise example, we\u2019re not performing any evaluation and scoring. Without\nthe evaluation, we can see that this sequence of prompts still has problems solving our\nmore challenging time travel problem shown earlier in figure 10.3. However, that\ndoesn\u2019t mean this technique doesn\u2019t have value, and this prompting format solves\nsome complex problems well.\n What we want to find, however, is a reasoning and planning methodology that can\nsolve such complex problems consistently. The following section moves from reason-\ning to evaluating the best solution.\n10.3\nEmploying evaluation for consistent solutions\nIn the previous section, we learned that even the best-reasoned plans may not always\nderive the correct solution. Furthermore, we may not always have the answer to con-\nfirm if that solution is correct. The reality is that we often want to use some form of\nevaluation to determine the efficacy of a solution.\n Figure 10.6 shows a comparison of the prompt engineering strategies that have been\ndevised as a means of getting LLMs to reason and plan. We\u2019ve already covered the two on\nthe left: zero-shot direct prompting and CoT prompting. The following example exer-\ncises in this section will look at self-consistency with the CoT and ToT techniques.\n We\u2019ll continue to focus on the complex time travel problem to compare these\nmore advanced methods that expand on reasoning and planning with evaluation. In\nthe next section, we\u2019ll evaluate self-consistency.\nListing 10.15\ncalculate_solution.jinja2\nRequests that the LLM \nnot solve the whole \nproblem, just the steps\nUses the magic statement \nto extract reasoning\nInjects the steps \nproduced by the \ndecompose_steps step\nRequests that the LLM \noutput the final answer \nand not any steps\nThe decomposed steps\nThe calculated steps\n\n\n262\nCHAPTER 10\nAgent reasoning and evaluation\n10.3.1 Evaluating self-consistency prompting\nConsistency in prompting is more than just lowering the temperature parameter we\nsend to an LLM. Often, we want to generate a consistent plan or solution and still use\na high temperature to better evaluate all the variations to a plan. By evaluating multi-\nple different plans, we can get a better sense of the overall value of a solution.\n Self-consistent prompting is the technique of generating multiple plans/solutions for\na given problem. Then, those plans are evaluated, and the more frequent or consis-\ntent plan is accepted. Imagine three plans generated, where two are similar, but the\nthird is different. Using self-consistency, we evaluate the first two plans as the more\nconsistent answer.\n Open prompt_flow/self-consistency-prompting/flow.dag.yaml in the VS Code\nprompt flow visual editor. The flow diagram shows the simplicity of the prompt genera-\ntion flow in figure 10.7. Next to it in the diagram is the self-consistency evaluation flow.\n Prompt flow uses a direct acyclic graph (DAG) format to execute the flow logic.\nDAGs are an excellent way of demonstrating and executing flow logic, but because\nthey are acyclic, meaning they can\u2019t repeat, they can\u2019t execute loops. However, because\nprompt flow provides a batch processing mechanism, we can use that to simulate\nloops or repetition in a flow.\n Referring to figure 10.6, we can see that self-consistency processes the input three\ntimes before collecting the results and determining the best plan/reply. We can apply\nthis same pattern but use batch processing to generate the outputs. Then, the evalua-\ntion flow will aggregate the results and determine the best answer.\n Open the self-consistency-prompting/cot.jinja2 prompt template in VS\nCode (see listing 10.16). The listing was shortened, as we\u2019ve seen parts before. This\nInput\nInput\nInput\nInput\nOutput\nOutput\nOutput\nOutput\nZero-shot, direct\nprompting\nChain of thought\n(CoT)\nSelf-consistency\nwith CoT (CoT-SC)\nTree of thought\n(ToT)\nMajority vote\n......\n......\n......\n......\n......\nthought\nFigure 10.6\nComparing the various prompt engineering strategies to enable reasoning and planning from LLMs\n\n\n263\n10.3\nEmploying evaluation for consistent solutions\nprompt uses two (few-shot prompt) examples of a CoT to demonstrate the thought\nreasoning to the LLM.\nsystem:\n\"In a time travel movie, Sarah travels back\u2026 \"    \nChain of Thought:\n    Initial Assumption: \u2026    \n    Conclusion: Sarah spends 729 days in the past before the day of the event.\n\"In a complex time travel movie plot, Max, a 25 year old\u2026\"    \nChain of Thought:\n    Starting Point: Max starts \u2026    \n    Conclusion: After the final jump, \nMax finds himself in the year 1980 and he is 75 years old.\nListing 10.16\nself-consistency-prompting/cot.jinja2\nSelf-consistency generation\nSelf-consistency evaluation\nThis \ufb02ow evaluates each of the\nprompts and then chooses the\nprompt that is\nconsistent.\nthe most\nSelf-consistency uses a\nchain of thought prompt\nto generate the plans.\nFigure 10.7\nThe self-consistency prompt generation beside the evaluation flow\nThe Sarah time \ntravel problem\nSample CoT, \ncut for brevity\nThe Max time \ntravel problem\nSample CoT, \ncut for brevity\n",
      "page_number": 278
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 288-295)",
      "start_page": 288,
      "end_page": 295,
      "detection_method": "topic_boundary",
      "content": "264\nCHAPTER 10\nAgent reasoning and evaluation\nThink step by step,\n but only show the final answer to the statement.    \nuser:\n{{statement}}\nOpen the self-consistency-prompting/flow.dag.yaml file in VS Code. Run the\nexample in batch mode by clicking Batch Run (the beaker icon) from the visual edi-\ntor. Figure 10.8 shows the process step by step: \n1\nClick Batch Run.\n2\nSelect the JSON Lines (JSONL) input.\n3\nSelect statements.jsonl.\n4\nClick the Run link.\nTIP\nIf you need to review the process, refer to chapter 9, which covers this\nprocess in more detail.\nFinal guide and \nstatement to \nconstrain output\n1. Click the\nBatch Run button.\n3. Click the Run link to start\nthe batch processing.\n2. Select the local data \ufb01le, and\nwhen prompted with the \ufb01le\ndialog, select statements.jsonl.\nFigure 10.8\nThe step-by-step process of launching a batch process\n\n\n265\n10.3\nEmploying evaluation for consistent solutions\nListing 10.17 shows the JSON output from executing the flow in batch mode. The\nstatements.jsonl file has five identical Alex time travel problem entries. Using\nidentical entries allows us to simulate the prompt executing five times on the dupli-\ncate entry.\n{\n    \"name\": \"self-consistency-prompting_default_20240203_100322_912000\",\n    \"created_on\": \"2024-02-03T10:22:30.028558\",\n    \"status\": \"Completed\",\n    \"display_name\": \"self-consistency-prompting_variant_0_202402031022\",\n    \"description\": null,\n    \"tags\": null,\n    \"properties\": {\n        \"flow_path\": \"\u2026prompt_flow/self-consistency-prompting\",    \n        \"output_path\": \"\u2026/.promptflow/.runs/self-\n\u27a5 consistency-prompting_default_20240203_100322_912000\",    \n        \"system_metrics\": {\n            \"total_tokens\": 4649,\n            \"prompt_tokens\": 3635,\n            \"completion_tokens\": 1014,\n            \"duration\": 30.033773\n        }\n    },\n    \"flow_name\": \"self-consistency-prompting\",\n    \"data\": \"\u2026/prompt_flow/self-consistency-prompting/\n\u27a5 statements.jsonl\",    \n    \"output\": \"\u2026/.promptflow/.runs/self-consistency-\n\u27a5 prompting_default_20240203_100322_912000/flow_outputs\"\n}\nYou can view the flow produced by pressing the Ctrl key and clicking the output link,\nhighlighted in listing 10.17. This will open another instance of VS Code, showing a\nfolder with all the output from the run. We now want to check the most consistent\nanswer. Fortunately, the evaluation feature in prompt flow can help us identify consis-\ntent answers using similarity matching.\n Open self-consistency-evaluation/flow.dag.yaml in VS Code (see figure\n10.7). This flow embeds the predicted answer and then uses an aggregation to deter-\nmine the most consistent answer.\n From the flow, open consistency.py in VS Code, as shown in listing 10.18. The\ncode for this tool function calculates the cosine similarity for all pairs of answers.\nThen, it finds the most similar answer, logs it, and outputs that as the answer.\nfrom promptflow import tool\nfrom typing import List\nimport numpy as np\nfrom scipy.spatial.distance import cosine\nListing 10.17\nself-consistency-prompting batch execution output\nListing 10.18\nconsistency.py\nThe path where \nthe flow was \nexecuted from\nThe folder \ncontaining the \noutputs of the flow \n(note this path)\nThe data used to run \nthe flow in batch\n\n\n266\nCHAPTER 10\nAgent reasoning and evaluation\n@tool\ndef consistency(texts: List[str],\n                embeddings: List[List[float]]) -> str:\n    if len(embeddings) != len(texts):\n        raise ValueError(\"The number of embeddings \n       \u27a5 must match the number of texts.\")\n    mean_embedding = np.mean(embeddings, axis=0)    \n    similarities = [1 - cosine(embedding, mean_embedding) \n                \u27a5 for embedding in embeddings]    \n    most_similar_index = np.argmax(similarities)    \n    from promptflow import log_metric\n    log_metric(key=\"highest_ranked_output\", value=texts[most_similar_index])    \n    return texts[most_similar_index]    \nWe need to run the evaluation flow in batch mode as well. Open self-consistency-\nevaluation/flow.dag.yaml in VS Code and run the flow in batch mode (beaker\nicon). Then, select Existing Run as the flow input, and when prompted, choose the\ntop or the last run you just executed as input.\n Again, after the flow completes processing, you\u2019ll see an output like that shown in\nlisting 10.17. Ctrl-click on the output folder link to open a new instance of VS Code\nshowing the results. Locate and open the metric.json file in VS Code, as shown in\nfigure 10.9.\n The answer shown in figure 10.9 is still incorrect for this run. You can continue a\nfew more batch runs of the prompt and/or increase the number of runs in a batch\nand then evaluate flows to see if you get better answers. This technique is generally\nmore helpful for more straightforward problems but still demonstrates an inability to\nreason out complex problems.\n Self-consistency uses a reflective approach to evaluate the most likely thought.\nHowever, the most likely thing is certainly not always the best. Therefore, we must con-\nsider a more comprehensive approach in the next section.\n10.3.2 Evaluating tree of thought prompting\nAs mentioned earlier, ToT prompting, as shown in figure 10.6, combines self-evalua-\ntion and prompt chaining techniques. As such, it breaks down the sequence of plan-\nning into a chain of prompts, but at each step in the chain, it provides for multiple\nevaluations. This creates a tree that can be executed and evaluated at each level,\nbreadth-first, or from top to bottom, depth-first.\n Figure 10.10 shows the difference between executing a tree using breadth-first or\ndepth-first. Unfortunately, due to the DAG execution pattern of prompt flow, we can\u2019t\nquickly implement the depth-first method, but breadth-first works just fine.\nCalculates the mean of \nall the embeddings\nCalculates cosine \nsimilarity for each \npair of embeddings\nFinds the index \nof the most \nsimilar answer\nLogs the output\nas a metric\nReturns the text \nfor the most \nsimilar answer\n\n\n267\n10.3\nEmploying evaluation for consistent solutions\nSelect the\nmetrics.json output\nReturns the highest ranked output as\ngenerated by the aggregation in\nconsistency.pr, but is the answer right?\nFigure 10.9\nThe VS Code is open to the batch run output folder. Highlighted are the metrics.json file and \nthe output showing the most similar answer.\ninput\noutput\nTree of Thought (ToT)\n......\nBreadth-\ufb01rst executes/considers\na row of thoughts at a time.\nDepth-\ufb01rst executes/considers\na sequence of thoughts from\nbeginning to end.\nFigure 10.10\nBreadth-first vs. depth-first execution on a ToT pattern\n\n\n268\nCHAPTER 10\nAgent reasoning and evaluation\nOpen tree-of-thought-evaluation/flow.dag.yaml in VS Code. The visual of the\nflow is shown in figure 10.11. This flow functions like a breadth-first ToT pattern\u2014the\nflow chains together a series of prompts asking the LLM to return multiple plans at\neach step.\nBecause the flow executes in a breadth-first style, each level output of the nodes is also\nevaluated. Each node in the flow uses a pair of semantic functions\u2014one to generate\nthe answer and the other to evaluate the answer. The semantic function is a custom\nPython flow block that processes multiple inputs and generates multiple outputs.\n Listing 10.19 shows the semantic_function.py tool. This general tool is reused for\nmultiple blocks in this flow. It also demonstrates the embedding functionality from\nthe SK for direct use within prompt flow.\n@tool\ndef my_python_tool(\n    input: str,\n    input_node: int,\n    history: str,\nListing 10.19\nsemantic_function.py\nFirst asks LLM to create 3 plans,\nbut not solutions as an expert\nEach LLM node calculates 3 versions\nof the output for each step in the plan.\nOutput from an LLM block is a\nsingle string, so this block splits\nthe text into nodes.\nSplits the output text\ninto 3 nodes again\nEach answer node calculates\nthe output of the answer.\nFigure 10.11\nToT pattern expressed and prompt flow\n\n\n269\n10.3\nEmploying evaluation for consistent solutions\n    semantic_function: str,\n    evaluation_function: str,\n    function_name: str,\n    skill_name: str,\n    max_tokens: int,\n    temperature: float,\n    deployment_name: str,\n    connection: Union[OpenAIConnection, \n                      AzureOpenAIConnection],    \n) -> str:\n    if input is None or input == \"\":    \n        return \"\"\n    kernel = sk.Kernel(log=sk.NullLogger())\n    # code for setting up the kernel and LLM connection omitted\n    function = kernel.create_semantic_function(\n                             semantic_function,                                               \n                             function_name=function_name,\n                             skill_name=skill_name,\n                             max_tokens=max_tokens,\n                             temperature=temperature,\n                             top_p=0.5)    \n    evaluation = kernel.create_semantic_function(\n                             evaluation_function,        \n                             function_name=\"Evaluation\",\n                             skill_name=skill_name,\n                             max_tokens=max_tokens,\n                             temperature=temperature,\n                             top_p=0.5)    \n    async def main():\n        query = f\"{history}\\n{input}\"\n        try:\n            eval = int((await evaluation.invoke_async(query)).result)\n            if eval > 25:    \n                return await function.invoke_async(query)   \n        except Exception as e:\n            raise Exception(\"Evaluation failed\", e)\n       try:\n        result = asyncio.run(main()).result\n        return result\n    except Exception as e:\n        print(e)\n        return \"\"\nThe semantic function tool is used in the tree\u2019s experts, nodes, and answer blocks. At\neach step, the function determines if any text is being input. If there is no text, the\nblock returns with no execution. Passing no text to a block means that the previous\nblock failed evaluation. By evaluating before each step, ToT short-circuits the execu-\ntion of plans it deems as not being valid.\nUses a union to allow \nfor different types of \nLLM connections\nChecks to see if the input is \nempty or None; if so, the \nfunction shouldn\u2019t be executed.\nSets up the \ngeneration function \nthat creates a plan\nSets up the \nevaluation function\nRuns the evaluate function and\ndetermines if the input is good\nenough to continue\nIf the evaluation \nscore is high \nenough, \ngenerates the \nnext step\n\n\n270\nCHAPTER 10\nAgent reasoning and evaluation\n This may be a complex pattern to grasp at first, so go ahead and run the flow in VS\nCode. Listing 10.20 shows just the answer node output of a run; these results may vary\nfrom what you see but should be similar. Nodes that return no text either failed evalu-\nation or their parents did.\n{\n    \"answer_1_1\": \"\",    \n    \"answer_1_2\": \"\",\n    \"answer_1_3\": \"\",\n    \"answer_2_1\": \"Alex spends a total of 29 days in the past before he \nsees the end of the battle.\",\n    \"answer_2_2\": \"\",    \n    \"answer_2_3\": \"Alex spends a total of 29 days in the past before he \nsees the end of the battle.\",\n    \"answer_3_1\": \"\",    \n    \"answer_3_2\": \"Alex spends a total of 29 days in the past before he \nsees the end of the battle.\",\n    \"answer_3_3\": \"Alex spends a total of 9 days in the past before he \nsees the end of the battle.\",\nThe output in listing 10.20 shows how only a select set of nodes was evaluated. In most\ncases, the evaluated nodes returned an answer that could be valid. Where no output\nwas produced, it means that the node itself or its parent wasn\u2019t valid. When sibling\nnodes all return empty, the parent node fails to evaluate.\n As we can see, ToT is valid for complex problems but perhaps not very practical.\nThe execution of this flow can take up to 27 calls to an LLM to generate an output. In\npractice, it may only do half that many calls, but that\u2019s still a dozen or more calls to\nanswer a single problem.\n10.4\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Create Direct Prompting, Few-Shot Prompting, and Zero-Shot\nPrompting\nObjective\u2014Create three different prompts for an LLM to summarize a recent\nscientific article: one using direct prompting, one with few-shot prompting, and\nthe last employing zero-shot prompting. \nTasks:\n\u2013 Compare the effectiveness of the summaries generated by each approach.\n\u2013 Compare the accuracy of the summaries generated by each approach.\n\uf0a1Exercise 2\u2014Craft Reasoning Prompts\nObjective\u2014Design a set of prompts that require the LLM to solve logical puzzles\nor riddles.\nListing 10.20\nOutput from tree-of-thought-evaluation flow\nRepresents that the first \nnode plans weren\u2019t valid \nand not executed\nThe plan for node 2 and\nanswer 2 failed evaluation\nand wasn\u2019t run.\nThe plan for this node failed\nto evaluate and wasn\u2019t run.\n\n\n271\nSummary\nTasks:\n\u2013 Focus on how the structure of your prompt can influence the LLM\u2019s rea-\nsoning process.\n\u2013 Focus on how the same can influence the correctness of its answers.\n\uf0a1Exercise 3\u2014Evaluation Prompt Techniques\nObjective\u2014Develop an evaluation prompt that asks the LLM to predict the out-\ncome of a hypothetical experiment.\nTask:\n\u2013 Create a follow-up prompt that evaluates the LLM\u2019s prediction for accuracy\nand provides feedback on its reasoning process.\nSummary\n\uf0a1Direct solution prompting is a foundational method of using prompts to direct\nLLMs toward solving specific problems or tasks, emphasizing the importance of\nclear question-and-answer structures.\n\uf0a1Few-shot prompting provides LLMs with a few examples to guide them in han-\ndling new or unseen content, highlighting its power in enabling the model to\nadapt to unfamiliar patterns.\n\uf0a1Zero-shot learning and prompting demonstrate how LLMs can generalize from\ntheir training to solve problems without needing explicit examples, showcasing\ntheir inherent ability to understand and apply knowledge in new contexts.\n\uf0a1Chain of thought prompting guides the LLMs through a reasoning process step\nby step to solve complex problems, illustrating how to elicit detailed reasoning\nfrom the model.\n\uf0a1Prompt chaining breaks down a problem into a series of prompts that build\nupon each other, showing how to structure complex problem-solving processes\ninto manageable steps for LLMs.\n\uf0a1Self-consistency is a prompt technique that generates multiple solutions to a\nproblem and selects the most consistent answer through evaluation, emphasiz-\ning the importance of consistency in achieving reliable outcomes.\n\uf0a1Tree of thought prompting combines self-evaluation and prompt chaining to\ncreate a comprehensive strategy for tackling complex problems, allowing for a\nsystematic exploration of multiple solution paths.\n\uf0a1Advanced prompt engineering strategies provide insights into sophisticated\ntechniques such as self-consistency with CoT and ToT, offering methods to\nincrease the accuracy and reliability of LLM-generated solutions.\n",
      "page_number": 288
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 296-303)",
      "start_page": 296,
      "end_page": 303,
      "detection_method": "topic_boundary",
      "content": "272\nAgent planning\nand feedback\nNow that we\u2019ve examined how large language models (LLMs) can reason and plan,\nthis chapter takes this concept a step further by employing planning within an\nagent framework. Planning should be at the core of any agent/assistant platform\nor toolkit. We\u2019ll start by looking at the basics of planning and how to implement a\nplanner through prompting. Then, we\u2019ll see how planning operates using the\nOpenAI Assistants platform, which automatically incorporates planning. From\nthere, we\u2019ll build and implement a general planner for LLMs.\n Planning can only go so far, and an often-unrecognized element is feedback.\nTherefore, in the last sections of the chapter, we explore feedback and implement\nThis chapter covers\n\uf0a1Planning for an LLM and implementing it in \nagents and assistants\n\uf0a1Using the OpenAI Assistants platform via custom \nactions\n\uf0a1Implementing/testing a generic planner on LLMs\n\uf0a1Using the feedback mechanism in advanced \nmodels \n\uf0a1Planning, reasoning, evaluation, and feedback in \nbuilding agentic systems\n\n\n273\n11.1\nPlanning: The essential tool for all agents/assistants\nit within a planner. You must be familiar with the content of chapter 10, so please\nreview it if you need to, and when you\u2019re ready, let\u2019s begin planning.\n11.1\nPlanning: The essential tool for all agents/assistants\nAgents and assistants who can\u2019t plan and only follow simple interactions are nothing\nmore than chatbots. As we\u2019ve seen throughout this book, our goal isn\u2019t to build bots\nbut rather to build autonomous thinking agents\u2014agents that can take a goal, work\nout how to solve it, and then return with the results. \n Figure 11.1 explains the overall planning process that the agent/assistant will\nundertake. This figure was also presented in chapter 1, but let\u2019s review it now in more\ndetail. At the top of the figure, a user submits a goal. In an agentic system, the agent\ntakes the goal, constructs the plan, executes it, and then returns the results.\nDepending on your interaction with platforms such as ChatGPT and GPTs, Claude,\nand others, you may have already encountered a planning assistant and not even noticed.\nPlanning is becoming ubiquitous and is now built into most commercial platforms to\nAgent interface layer (natural language)\nPlanning: Agent takes the goal and breaks it into tasks.\n1. Collect data.\n2. Annotate data.\n3. Format data, and create report visualizations.\n4. Present report.\n4. Agent presents the report.\nPlease create a report of\nlast year\u2019s sales.\nGPT data layer\nDatabase\n1. Query database using natural\nlanguage.\nGPT functions\nAPIs,\nweb browsing,\nsearch, etc.\nAll communication via natural language\n2. Annotate data by calling semantic\nfunctions using natural language.\nExternal agents\nGPT tools\n3. External agent formats data and\nmay write code to generate visuals.\nFigure 11.1\nThe agent planning process\n\n\n274\nCHAPTER 11\nAgent planning and feedback\nmake the model appear more intelligent and capable. Therefore, in the next exercise,\nwe\u2019ll look at an example to set a baseline and differentiate between an LLM that can\u2019t\nplan and an agent that can.\n For the next exercise, we\u2019ll use Nexus to demonstrate how raw LLMs can\u2019t plan\nindependently. If you need assistance installing, setting up, and running Nexus, refer\nto chapter 7. After you have Nexus installed and ready, we can begin running it with\nthe Gradio interface, using the commands shown next.\nnexus run gradio\nGradio is an excellent web interface tool built to demonstrate Python machine learn-\ning projects. Figure 11.2 shows the Gradio Nexus interface and the process for creat-\ning an agent and using an agent engine (OpenAI, Azure, and Groq) of your choice.\nYou can\u2019t use LM Studio unless the model/server supports tool/action use. Anthropic\u2019s\nClaude supports internal planning, so for the purposes of this exercise, avoid using\nthis model.\nListing 11.1\nRunning Nexus with the Gradio interface \nFigure 11.2\nCreating a new agent in Nexus\n1. Select to create a new agent.\n2. Name your agent.\n3. Don\u2019t worry about a system\nprompt or instructions.\n4. Select an engine. Note that each engine needs to\nbe con\ufb01gured correctly to show in the list.\n5. Keep the default engine settings, model,\ntemperature,\n, and max tokens.\ntop_p\n6. Don\u2019t worry about retrieval or\nadvanced options for now.\n7. Click Add Agent when your agent\ncon\ufb01guration is complete.\nWhen you start conversing\nwith your agent,  you will need\nto create a new thread or use\nan existing thread.\n\n\n275\n11.1\nPlanning: The essential tool for all agents/assistants\nAfter creating the agent, we want to give it specific actions (tools) to undertake or\ncomplete a goal. Generally, providing only the actions an agent needs to complete its\ngoal is best for a few reasons:\n\uf0a1More actions can confuse an agent into deciding which to use or even how to\nsolve a goal.\n\uf0a1APIs have limits on the number of tools that can be submitted; at the time of\nwriting, hitting this limit is relatively easy.\n\uf0a1Agents may use your actions in ways you didn\u2019t intend unless that\u2019s your goal.\nBe warned, however, that actions can have consequences.\n\uf0a1Safety and security need to be considered. LLMs aren\u2019t going to take over the\nworld, but they make mistakes and quickly get off track. Remember, these\nagents will operate independently and may perform any action.\nWARNING\nWhile writing this book and working with and building agents over\nmany hours, I have encountered several instances of agents going rogue with\nactions, from downloading files to writing and executing code when not\nintended, continually iterating from tool to tool, and even deleting files they\nshouldn\u2019t have. Watching an agent emerge new behaviors using actions can\nbe fun, but things can quickly go astray.\nFor this exercise, we\u2019ll define the goal described in the following listing. \nSearch Wikipedia for pages on {topic} and download each page and save it \nto a file called Wikipedia_{topic}.txt\nThis goal will demonstrate the following actions:\n\uf0a1\nsearch_wikipedia(topic)\u2014Searches Wikipedia and returns page IDs for the\ngiven search term.\n\uf0a1\nget_wikipedia_page(page_id)\u2014Downloads the page content given the page ID.\n\uf0a1\nsave_file\u2014Saves the content to a file.\nSet the actions on the agent, as shown in figure 11.3. You\u2019ll also want to make sure the\nPlanner is set to None. We\u2019ll look at setting up and using planners soon. You don\u2019t\nhave to click Save; the interface automatically saves an agent\u2019s changes.\n After you choose the actions and planner, enter the goal in listing 11.2. Then click\nCreate New Thread to instantiate a new conversation. Substitute the topic you want to\nsearch for in the chat input, and wait for the agent to respond. Here\u2019s an example of\nthe goal filled with the topic, but again, use any topic you like:\nSearch Wikipedia for pages on Calgary and download each page and save it to \na file called Wikipedia_Calgary.txt.\nListing 11.2\nDemonstrating planning: The goal \n\n\n276\nCHAPTER 11\nAgent planning and feedback\nFigure 11.4 shows the results of submitting the goal to the plain agent. We see the\nagent executed the tool/action to search for the topic but couldn\u2019t execute any\nsteps beyond that. If you recall from our discussion and code example of actions in\nchapter 5, OpenAI, Groq, and Azure OpenAI all support parallel actions but not\nsequential or planned actions.\n The LLM can answer reasonably well if you submit a goal with several parallel\ntasks/actions. However, if the actions are sequential, requiring one step to be depen-\ndent on another, it will fail. Remember, parallel actions are standalone actions that\ncan be run alongside others. \nSelect only the three actions:\nsearch_wikipedia,\nget_wikipedia_page, and\nsave_\ufb01le\nMake sure to select\nNone for the Planner.\nFigure 11.3\nSelecting the actions for the agent and disabling the planner\n\n\n277\n11.2\nUnderstanding the sequential planning process\nAnthropic\u2019s Claude and OpenAI Assistants support sequential action planning. This\nmeans both models can be called with sequential plans, and the model will execute them\nand return the results. In the next section, we\u2019ll explore sequential planning and then\ndemonstrate it in action.\n11.2\nUnderstanding the sequential planning process\nIn the next exercise, we\u2019ll ask an OpenAI assistant to solve the same goal. If you have\nAnthropic/Claude credentials and have the engine configured, you can also try this\nexercise with that model. \n Figure 11.5 shows the difference between executing tasks sequentially (planning)\nand using iteration. If you\u2019ve used GPTs, assistants, or Claude Sonnet 3.5, you\u2019ve likely\nalready experienced this difference. These advanced tools already incorporate plan-\nning by prompt annotations, advanced training, or combining both. \n As LLM and chat services evolve, most models will likely natively support some\nform of planning and tool use. However, most models, including GPT-4o, only sup-\nport action/tool use today. \n Let\u2019s open the GPT Assistants Playground to demonstrate sequential planning in\naction. If you need help, refer to the setup guide in chapter 6. We\u2019ll use the same goal\nbut, this time, run it against an assistant (which has built-in planning).\nFigure 11.4\nThe results from trying to get the agent/LLM to complete the goal\n\n\n278\nCHAPTER 11\nAgent planning and feedback\nAfter you launch the Playground, create a new assistant, and assign it the search_\nwikipedia, get_wikipedia_page, and save_file actions. Figure 11.6 shows the results\nof entering the goal to the assistant. As you can see, the assistant completed all the\ntasks behind the scenes and responded with the user\u2019s final requested output, achiev-\ning the goal.\n To demonstrate the effectiveness of the OpenAI Assistant\u2019s planner, we added\nanother task, summarizing each page, to the goal. The inserted task didn\u2019t have a\nfunction/tool, but the assistant was savvy enough to use its ability to summarize the\ncontent. You can see the output of what the assistant produced by opening the [root\nfolder]assistants_working_folder/Wikipedia_{topic}.txt file and reviewing the\ncontents. Now that we understand how LLMs function without planners and plan-\nning, we can move on to creating our planners in the next section.\n11.3\nBuilding a sequential planner\nLLM tools such as LangChain and Semantic Kernel (SK) have many planners using\nvarious strategies. However, writing our planner is relatively easy, and Nexus also sup-\nports a plugin-style interface allowing you to add other planners from tools such as\nLangChain and SK, or your derivatives.\nSingle prompt as a goal:\nsearch Wikipedia for topic, return\nthe top page, and save to a \ufb01le.\nSequential execution of a goal\nSingle prompt as a goal:\nsearch Wikipedia for topic, return\nthe top page, and save to a \ufb01le.\nIterative execution of a goal\nTasks/plan\nReturns a list of page IDs\nsearch_wikipedia(topic)\nReturns the page content\nget_wikipedia_page(page_id)\nCollects pages\nsave_ le\n\ufb01\nAll pages combined\nIn a planner, this\naggregation is\nperformed\ninternally.\nAll tasks are\nperformed\ninternally by\nthe agent.\nUser submits a goal.\nsearch_wikipedia(topic)\nResults are displayed, and the\nagent asks to continue to the\nnext step.\nReturns a list of page IDs\nget_wikipedia_page(page_id)\nUser responds\nwith continue.\nReturns are collected and\ndisplayed to the user. The agent\nasks for further continuation.\nReturns the page content\nsave_ le\n\ufb01\nAll pages combined\nUser responds\nwith continue.\nGoal is complete, and user has the output.\nIterative\nexecution\nis default\nbehavior\nfor an LLM.\nSequential\n(planned)\nexecution is\nstandard in\nGPT\nAssistants\nand Claude.\nFigure 11.5\nThe difference between iterative and planned execution\n\n\n279\n11.3\nBuilding a sequential planner\nPlanners may sound complicated, but they are easily implemented through prompt\nengineering strategies that incorporate planning and reasoning. In chapter 10, we\ncovered the basics of reasoning and deriving plans, and now we can put those skills to\ngood use.\n Listing 11.3 shows a sequential planner derived from the SK, which is extended to\nincorporate iteration. Prompt annotation planners like those shown in the listing can\nbe adapted to fit specific needs or be more general like those shown. This planner\nuses JSON, but planners could use any format an LLM understands, including code.\n \n \n \nNo instructions are needed, but\nbe sure to select the actions:\nsearch_wikipedia,\nget_wikipedia_page, and\nsave_\ufb01le\nClick the Logs tab as the assistant is\nrunning to see it work through the actions.\nIn this example, the goal was extended\nto ask the assistant to summarize the\ncontent \ufb01rst before saving it to a \ufb01le.\nThis is a single response from\nthe assistant, which also\nincluded summarization of\nthe content and con\ufb01rmation\nof the \ufb01le save.\nNexus and Playground have\na feature that con\ufb01rms the\n\ufb01le indeed was saved.\nFigure 11.6\nThe assistant processing the goal and outputting the results\n",
      "page_number": 296
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 304-312)",
      "start_page": 304,
      "end_page": 312,
      "detection_method": "topic_boundary",
      "content": "280\nCHAPTER 11\nAgent planning and feedback\nYou are a planner for Nexus.    \nYour job is to create a properly formatted JSON plan step by step, to \nsatisfy the goal given.\nCreate a list of subtasks based off the [GOAL] provided.\nEach subtask must be from within the [AVAILABLE FUNCTIONS] list. Do not \nuse any functions that are not in the list.\nBase your decisions on which functions to use from the description and the \nname of the function.\nSometimes, a function may take arguments. Provide them if necessary.\nThe plan should be as short as possible.\nYou will also be given a list of corrective, suggestive and epistemic \nfeedback from previous plans to help you make your decision.\nFor example:\n[SPECIAL FUNCTIONS]    \nfor-each- prefix\ndescription: execute a function for each item in a list\nargs: \n- function: the function to execute\n- list: the list of items to iterate over\n- index: the arg name for the current item in the list\n[AVAILABLE FUNCTIONS]\nGetJokeTopics\ndescription: Get a list ([str]) of joke topics\nEmailTo\ndescription: email the input text to a recipient\nargs:\n- text: the text to email\n- recipient: the recipient's email address. Multiple addresses may be \nincluded if separated by ';'.\nSummarize\ndescription: summarize input text\nargs:\n- text: the text to summarize\nJoke\ndescription: Generate a funny joke\nargs:\n- topic: the topic to generate a joke about\n[GOAL]\n\"Get a list of joke topics and generate a different joke for each topic. \nEmail the jokes to a friend.\"\n[OUTPUT]\n    {        \n        \"subtasks\": [\n            {\"function\": \"GetJokeTopics\"},\nListing 11.3\nbasic_nexus_planner.py \nThe preamble instructions telling the \nagent how to process the examples\nBeginning of the \nthree (few-shot) \nexamples\n\n\n281\n11.3\nBuilding a sequential planner\n            {\"function\": \"for-each\",\n             \"args\": {\n                       \"list\": \"output_GetJokeTopics\",\n                       \"index\": \"topic\", \n                       \"function\": \n                                  {\n                                   \"function\": \"Joke\",\n                                   \"args\": {\"topic\": \"topic\"}}}},\n            {\n             \"function\": \"EmailTo\",\n              \"args\": {\n                        \"text\": \"for-each_output_GetJokeTopics\"\n                       ecipient\": \"friend\"}}\n        ]\n    }\n# 2 more examples are given but omitted from this listing\n[SPECIAL FUNCTIONS]    \nfor-each\ndescription: execute a function for each item in a list\nargs: \n- function: the function to execute\n- iterator: the list of items to iterate over\n- index: the arg name for the current item in the list  \n[AVAILABLE FUNCTIONS]    \n{{$available_functions}}\n[GOAL]\n{{$goal}}    \nBe sure to only use functions from the list of available functions. \nThe plan should be as short as possible. \nAnd only return the plan in JSON format.\n[OUTPUT]    \nFigure 11.7 shows the process of building and running a planning prompt, from\nbuilding to execution to finally returning the results to the user. Planners work by\nbuilding a planning prompt, submitting it to an LLM to construct the plan, parsing\nand executing the plan locally, returning the results to an LLM to evaluate and sum-\nmarize, and finally returning the final output back to the user.\n It\u2019s essential to notice a few subtle details about the planning process. Typically, the\nplan is built in isolation by not adding context history. This is done to focus on the\ngoal because most planning prompts consume many tokens. Executing the functions\nwithin the executor is usually done in a local environment and may include calling\nAPIs, executing code, or even running machine learning models.\n Listing 11.4 shows the code for the create_plan function from the BasicNexus-\nPlanner class; tools such as LangChain and SK use similar patterns. The process loads\nthe agent\u2019s actions as a string. The goal and available functions list are then inserted into\nthe planner prompt template using the PromptTemplateManager, which is just a wrapper\nAdds the for-each \nspecial iterative \nfunction\nAvailable functions are \nautopopulated from the \nagent\u2019s list of available \nfunctions.\nThe goal is inserted here.\nWhere the agent is expected \nto place the output\n\n\n282\nCHAPTER 11\nAgent planning and feedback\nfor the template-handling code. Template handling is done with simple regex but can\nalso be more sophisticated using tools such as Jinja2, Handlebars, or Mustache.\ndef create_plan(self, nexus, agent, goal: str, prompt: str = PROMPT) -> Plan:\n        selected_actions = nexus.get_actions(agent.actions)\n        available_functions_string = \"\\n\\n\".join(\n            format_action(action) for action in selected_actions\n        )    \n        \n        context = {}    \n        context[\"goal\"] = goal\n        context[\"available_functions\"] = available_functions_string\nListing 11.4\nbasic_nexus_planner.py (create_plan)\nExecution is typically done\nin an environment local to\nthe agent.\nThe planning prompt\nadds the list of available\nfunctions and the goal to\nthe prompt template.\nLLM\nLLM\nLLM\nChat history\nAssembled results\nThe plan\nGoal\n\u201cFetch, summarize, and send\u201d\nResults of executed goal\nExecute the function to query the\ndatabase for access to an API.\nChat history\nSend the goal along with\nplanning prompt.\nNo history of other\ncontext is sent.\nPlan executor\nEach step of the plan is executed,\nand results are passed through LLM\nalong with history.\nUse the output and other arguments\nto execute the actions/functions.\nResults are again sent through\nthe LLM to summarize the output\nfor the user.\nPlanning prompt\nFigure 11.7\nThe planning process for creating and executing a plan\nLoads the agent\u2019s available actions and\nformats the result string for the planner\nThe context will be injected into\nthe planner prompt template.\n\n\n283\n11.3\nBuilding a sequential planner\n        ptm = PromptTemplateManager()    \n        prompt = ptm.render_prompt(prompt, context)\n        plan_text = nexus.execute_prompt(agent, prompt)    \n        return Plan(prompt=prompt, \n                    goal=goal, \n                    plan_text=plan_text)    \nThe code to execute the plan, shown in listing 11.5, parses the JSON string and exe-\ncutes the functions. When executing the plan, the code detects the particular for-each\nfunction, which iterates through a list and executes each element in a function. The\nresults of each function execution are added to the context. This context is passed to\neach function call and returned as the final output.\ndef execute_plan(self, nexus, agent, plan: Plan) -> str:\n        context = {}\n        plan = plan.generated_plan\n        for task in plan[\"subtasks\"]:    \n            if task[\"function\"] == \"for-each\":    \n                list_name = task[\"args\"][\"list\"]\n                index_name = task[\"args\"][\"index\"]\n                inner_task = task[\"args\"][\"function\"]\n                list_value = context.get(list_name, [])\n                for item in list_value:\n                    context[index_name] = item\n                    result = nexus.execute_task(agent, inner_task, context)\n                    context[f\"for-each_{list_name}_{item}\"] = result\n                for_each_output = [    \n                    context[f\"for-each_{list_name}_{item}\"] \u27a5\n                      for item in list_value\n                ]\n                context[f\"for-each_{list_name}\"] = for_each_output\n                \n                for item in list_value:    \n                    del context[f\"for-each_{list_name}_{item}\"]\n            else:\n                result = nexus.execute_task(agent,\n                                            task,\n                                            context)    \n                context[f\"output_{task['function']}\"] = result\n        return context    \nListing 11.5\nbasic_nexus_planner.py (execute_plan)\nA simple template \nmanager, similar in \nconcept to Jinja2, \nHandlebars, or \nMustache\nSends the filled-in \nplanner prompt to \nthe LLM\nThe results (the plan) are wrapped in a \nPlan class and returned for execution.\nIterates through each \nsubtask in the plan\nHandles functions that\nshould be iterated\nover and adds full list\nof results to the\ncontext\nRemoves \nindividual \nfor-each \ncontext \nentries\nGeneral task \nexecution\nReturns the full context, which includes \nthe results of each function call\n\n\n284\nCHAPTER 11\nAgent planning and feedback\nThe returned context from the entire execution is sent in a final call to the LLM,\nwhich summarizes the results and returns a response. If everything goes as planned,\nthe LLM will respond with a summary of the results. If there is an error or something\nis missing, the LLM may try to fix the problem or inform the user of the error.\n Let\u2019s now open Nexus again and test a planner in operation. Load up the same\nagent you used last time, but select the planner under the Advanced options this time,\nas shown in figure 11.8. Then, enter the goal prompt as you did before, and let the\nagent take it away.\nAfter a few minutes, the agent returns with the saved file, and in some cases, it may\nprovide extra information, such as the next steps and what to do with the output. This\nis because the agent was given a high-level overview of what it accomplished. Remem-\nber, though, that plan execution is done at the local level, and only context, plan, and\ngoal were sent to the LLM.\nGoal requiring multiple\nsteps to complete\nThe agent will execute the tasks and\n\ufb01nish by outputting the \ufb01le as well\nas suggesting next steps.\nSelect the basic\nplanner.\nFigure 11.8\nThe results from requesting to complete the goal in Nexus using the basic planner\n\n\n285\n11.4\nReviewing a stepwise planner: OpenAI Strawberry\n This means that plan execution can be completed by any process, not necessarily\nby the agent. Executing a plan outside the LLM reduces the tokens and tool use the\nagent needs to perform. This also means that an LLM doesn\u2019t need to support tools\nusage to use a planner.\n Internally, when a planner is enabled within Nexus, the agent engine tool is\nbypassed. Instead, the planner completes the action execution, and the agent is only\naware of the actions through the passing of the output context. This can be good for\nmodels that support tool use but can\u2019t plan. However, a planner may limit functional-\nity for models that support both tool use and planning, such as Claude.\n In general, you\u2019ll want to understand the capabilities of the LLM you\u2019re using. If\nyou\u2019re unsure of those details, then a little trial and error can also work. Ask the agent to\ncomplete a multistep goal with and without planning enabled, and then see the results.\n Planning allows agents to complete multiple sequential tasks to achieve more com-\nplex goals. The problem with external or prompt planning is that it bypasses the feed-\nback iteration loop, which can help correct problems quickly. Because of this, OpenAI\nand others are now directly integrating reasoning and planning at the LLM level, as\nwe\u2019ll see in the next section.\n11.4\nReviewing a stepwise planner: OpenAI Strawberry\nThe release of the o1-preview model, code named Strawberry, introduced a dra-\nmatic shift in the type of LLMs becoming available for agentic systems. Strawberry\nwas not only proclaimed to be more efficient at math, science, and general calcula-\ntion tasks but also able to engage in reasoning, planning, evaluation, and feedback\ndirectly in the LLM. \n Consider our time travel problem from chapter 10 and shown again in figure 11.9.\nIf you recall, this problem was difficult to solve using GPT-4 and other similar LLMs.\nHowever, with the application of reasoning and feedback, we were able to produce\noutput that was occasionally correct. \n As an experiment, enter this problem into ChatGPT using the o1-preview model,\nas shown in listing 11.6. Sit back for a few seconds and wait for the answer. Yep, the\nmodel still gets it wrong.\nIn a sci-fi film, Alex is a time traveler who decides to go back in time \nto witness a famous historical battle that took place 100 years ago, \nwhich lasted for 10 days. He arrives three days before the battle starts. \nHowever, after spending six days in the past, he jumps forward in time \nby 50 years and stays there for 20 days. Then, he travels back to \nwitness the end of the battle. How many days does Alex spend in the past \nbefore he sees the end of the battle?\nWhile it may be somewhat disappointing to see the model get the wrong answer, it\ndoes, however, do a far better job of breaking down the problem and demonstrating\nListing 11.6\nTime travel reasoning/planning problem\n\n\n286\nCHAPTER 11\nAgent planning and feedback\nits answer. Listing 11.7 shows the sample output from posing the problem in listing\n11.6 to the Strawberry model. Note, you may get a different answer because of the sto-\nchastic nature of the LLM.\nTo determine how many days Alex spends in the past before witnessing the \nend of the battle, let's reconstruct the timeline step by step:\nArrival in the Past:\nAlex arrives 3 days before the battle begins.\nHe spends these 3 days waiting for the battle to start.\nDuring the Battle:\nThe battle starts, and Alex experiences the first 3 days of it.\nAt this point, he has spent a total of 6 days in the past (3 days before \nthe battle and 3 days during the battle).\nTime Jump Forward:    \nAfter these 6 days, Alex jumps 50 years forward in time.\nHe spends 20 days in this future time.\nReturn to the Past:\nAlex then returns to the past to witness the end of the battle.\nSince the battle lasts 10 days and he left after day 3 of the battle, \nthere are 7 days remaining in the battle.\nHowever, because he was away for 20 days, which is longer than the \nremaining 7 days of the battle, he must return to a point in time before \nthe battle ends to witness its conclusion.\nTo see the end of the battle, he returns to the past on the last day of \nListing 11.7\no1-preview response to time travel problem\nIn a sci-\ufb01\ufb01lm, Alex is a time traveler who decides to go back in\ntime to witness a famous historical battle that took place 100 years ago,\nwhich lasted for 10 days. He arrives three days before the battle starts.\nHowever, after spending six days in the past, he jumps forward in time by\n50 years and stays there for 20 days. Then, he travels back to witness the\nend of the battle. How many days does Alex spend in the past before he\nsees the end of the battle?\nAlex\nTimeline\nPresent\nHistoric\nbattle starts\nHistoric\nbattle ends\nAlex\narrives\nJumps back in time 100 years (3 days before the battle)\nspends 6 days\njumps forward in time 50 years\n20 days\njumps back to the present\nAlex\nleaves\nAlex\nreturns?\nAlex\narrives\nAlex\nleaves\n???\nThe problem statement:\nThe statement is vague as\nto what time Alex returns\nto the battle. Is it the same\ntime he left?\njumps back to witness end of battle\nbattle lasts 10 days\nFigure 11.9\nThe time travel problem, revisited\nIt becomes obvious \nwhere the model is \nmaking the error.\n\n\n287\n11.4\nReviewing a stepwise planner: OpenAI Strawberry\nthe battle.\nFinal Days Spent Before the Battle Ends:\nUpon returning, he spends 1 more day in the past to witness the battle's \nend.\nTotal Days Spent in the Past Before Seeing the End:\n6 days before jumping forward in time.\n1 day after returning to the past.\nTotal: 6 + 1 = 7 days.\nAnswer: 7\nBecause we know the right answer is 27, we know the LLM is wrong, but if we didn\u2019t,\nwe could just as easily assume that the work and reasoning were all correct. Problems\nlike this can happen when we remove feedback in LLM interactions and agentic sys-\ntems. Feedback can guide the model to correct itself. \n However, what if we didn\u2019t know the correct answer was 27 (26, if you assume he\ndoesn\u2019t spend the day to witness the battle) and assumed the LLM or agent was cor-\nrect? Well, this is a problem we can rectify with a couple of simple prompts that can\nengage the LLM in reasoning and planning feedback. However, these techniques are\nmore effective with LLMs or wrappers such as the OpenAI Assistants, which provide\nreasoning and planning within the model.\n What we want to do is provide feedback to the LLM, but understanding what that\nfeedback is will likely be difficult for us. Fortunately, we can elicit feedback directly\nfrom the LLM, provided we give the correct answer. Listing 11.8 shows how to gener-\nate constructive feedback from the LLM concerning our time travel problem.\nthe correct answer is 27, please review what you did wrong and suggest \nfeedback you could give yourself when trying to solve similar future problems\nEntering this after the model answers the question wrong will generate feedback that\nyou can use to guide the model through prompting or as part of system instructions.\nListing 11.9 shows an example of the feedback provided by o1-preview. You can then\nextract this feedback and augment the instructions the next time you want to tackle\ncomplex time travel problems.\nFeedback for Future Problems\nUnderstanding Time Frames in Time Travel:\nRelative Time Frames: Always consider the time periods relative to the \noriginal point of reference. In this problem, even though Alex jumps \nforward 50 years, he is still in the past relative to his original \npresent time.\nIncluding All Relevant Periods: \nEnsure that all time periods spent in the past (or future) are included \nin the total calculation, even if they involve moving to different \npoints in time.\nTime Continuity: \nListing 11.8\nGenerating feedback\nListing 11.9\nGenerated feedback\n\n\n288\nCHAPTER 11\nAgent planning and feedback\nRemember that while the traveler moves through time, events in other \ntime periods continue to progress unless specified otherwise.\nApplication:\nCareful Reading: \nPay close attention to the details provided, such as the durations and \nthe sequences of time jumps.\nVisualization: \nIt may help to draw a timeline to visualize the sequence of events and \ntime periods involved.\nDouble-Checking Calculations: \nAfter solving, revisit each step to ensure all time periods are \naccounted for correctly.\nThis feedback technique will consistently work on models such as o1-preview, but\nother models may still struggle to answer correctly, even given this feedback. Over\ntime, as models become smarter, this technique will likely generally work on most\nmodels. However, this feedback mechanism will likely be essential even as models\nget progressively brighter. because language is nuanced, and not every problem we\nchallenge LLMs with may have an obvious absolute answer. Take our example prob-\nlem, for instance. This problem is an excellent example of requiring the problem\nsolver to make assumptions and draw correlations from the question. There are still\nplenty of areas in science, from geology to behavioral science, where answering the\nsame problem may yield a range of answers. Let\u2019s look next at a few techniques for\nhow the application of reasoning, planning, evaluation, and feedback can be\napplied to agentic systems.\n11.5\nApplying planning, reasoning, evaluation, and \nfeedback to assistant and agentic systems\nIn recent chapters, we\u2019ve examined how the agentic components of planning, reason-\ning, feedback, and evaluation can be implemented. Now we look at how, when, and\nwhere those components can be integrated into assistant and agentic systems for real-\ntime production, research, or development.\n While not all of these components may fit the same into every application, it\u2019s use-\nful to understand where and when to apply which component. In the next section, we\nlook at how planning can be integrated into assistant/agentic systems.\n11.5.1 Application of assistant/agentic planning\nPlanning is the component where an assistant or agent can plan to undertake a set of\ntasks, whether they are in series, parallel, or some other combination. We typically\nassociate planning with tool use, and, rightfully, any system using tools will likely want\na capable planner. However, not all systems are created equally, so in table 11.1, we\u2019ll\nreview where, when, and how to implement planners.\n \n",
      "page_number": 304
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 313-321)",
      "start_page": 313,
      "end_page": 321,
      "detection_method": "topic_boundary",
      "content": "289\n11.5\nApplying planning, reasoning, evaluation, and feedback to assistant and agentic sys-\nTable 11.1 shows several varied application scenarios in which we may find an assis-\ntant or agent deployed to assist in some capacity. To provide further information\nand guidance, this list provides more details about how planning may be employed\nin each application:\n\uf0a1Personal assistant\u2014While this application has been slow to roll out, LLM per-\nsonal assistants promise to surpass Alexa and Siri in the future. Planning will be\nessential to these new assistants/agents to coordinate numerous complex tasks\nand execute tools (actions) in series or parallel. \n\uf0a1Customer service bot\u2014Due to the controlled nature of this environment, it\u2019s\nunlikely that assistants engaged directly with customers will have controlled and\nvery specific tools use. This means that these types of assistants will likely not\nrequire extensive planning.\n\uf0a1Autonomous agent\u2014As we\u2019ve seen in previous chapters, agents with the ability to\nplan can complete a series of complex tasks for various goals. Planning will be\nan essential element of any autonomous agentic system.\n\uf0a1Collaborative workflows\u2014Think of these as agents or assistants that sit alongside\ncoders or writers. While these workflows are still in early development, think of\na workflow where agents are automatically tasked with writing and executing\ntest code alongside developers. Planning will be an essential part of executing\nthese complex future workflows.\nTable 11.1\nWhen and where planning is employed and used in various applications\nApplication\nImplemented\nEnvironment\nPurpose\nTiming\nConfiguration\nPersonal \nassistant\nAt or within the \nLLM\nPersonal \ndevice\nFacilitate tool \nuse\nDuring the \nresponse\nAs part of the \nprompt or LLM\nCustomer \nservice bot\nNot typical; \nrestricted \nenvironment\nRestricted \nenvironment, \nno tool use\nAutonomous \nagent\nAs part of the \nagent prompt \nand within the \nLLM\nServer or \nservice\nFacilitate com-\nplex tool use \nand task plan-\nning\nAs part of con-\nstructing the \nagent and/or \nduring the \nresponse\nWithin the \nagent or LLM\nCollaborative \nworkflows\nAs part of \nthe LLM\nShared can-\nvas or coding\nFacilitate com-\nplex tool use\nDuring the \nresponse\nWithin the LLM\nGame AI\nAs part of \nthe LLM\nServer or \napplication\nComplex \ntool use and \nplanning\nBefore or \nduring the \nresponse\nWithin the LLM\nResearch\nAnywhere\nServer \nFacilitate tool \nuse and engage \nin complex \ntask workflows\nBefore, \nduring, and \nafter response \ngeneration\nAnywhere\n\n\n290\nCHAPTER 11\nAgent planning and feedback\n\uf0a1Game AI\u2014While applying LLMs to games is still in early stages, it isn\u2019t hard to\nimagine in-game agents or assistants that can assist or challenge the player. Giv-\ning these agents the ability to plan and execute complex workflows could dis-\nrupt how and with whom we play games.\n\uf0a1Research\u2014Similar to collaborative workflows, these agents will be responsible\nfor deriving new ideas from existing sources of information. Finding that infor-\nmation will likely be facilitated through extensive tool use, which will benefit\nfrom coordination of planning.\nAs you can see, planning is an essential part of many LLM applications, whether\nthrough coordination of tool use or otherwise. In the next section, we look at the next\ncomponent of reasoning and how it can be applied to the same application stack.\n11.5.2 Application of assistant/agentic reasoning\nReasoning, while often strongly associated with planning and task completion, is a\ncomponent that can also stand by itself. As LLMs mature and get smarter, reasoning is\noften included within the LLM itself. However, not all applications may benefit from\nextensive reasoning, as it often introduces a thinking cycle within the LLM response.\nTable 11.2 describes at a high level how the reasoning component can be integrated\nwith various LLM application types.\nTable 11.2\nWhen and where reasoning is employed and used in various applications\nApplication\nImplemented\nEnvironment\nPurpose\nTiming\nConfiguration\nPersonal \nassistant\nWithin the LLM\nPersonal \ndevice\nBreaking down \nwork into steps\nDuring the \nresponse\nAs part of the \nprompt or LLM\nCustomer \nservice bot\nNot typical; \nusually just \ninformational\nLimited tool \nuse and need \nfor composite \ntool use\nAutono-\nmous \nagent\nAs part of the \nagent prompt \nand within \nthe LLM\nServer or \nservice\nFacilitate \ncomplex tool use \nand task planning\nAs part of \nLLM, external \nreasoning not \nwell suited \nWithin the \nagent or LLM\nCollabora-\ntive work-\nflows\nAs part of the \nLLM\nShared can-\nvas or coding\nAssists in break-\ning work down\nDuring the \nresponse\nWithin the \nLLM\nGame AI\nAs part of \nthe LLM\nServer or \napplication\nEssential for \nundertaking com-\nplex actions\nBefore or \nduring the \nresponse\nWithin the \nLLM\nResearch\nAnywhere\nServer \nUnderstand how to \nsolve complex \nproblems and \nengage in complex \ntask workflows\nBefore, during, \nand after \nresponse \ngeneration\nAnywhere\n\n\n291\n11.5\nApplying planning, reasoning, evaluation, and feedback to assistant and agentic sys-\nTable 11.2 shows several varied application scenarios in which we may find an assis-\ntant or agent deployed to assist in some capacity. To provide further information\nand guidance, this list provides more details about how reasoning may be employed\nin each application:\n\uf0a1Personal assistant\u2014Depending on the application, the amount of reasoning an\nagent employs may be limited. Reasoning is a process that requires the LLM to\nthink through a problem, and this often requires longer response times depend-\ning on the complexity of the problem and the extent of the prompt. In many\nsituations, responses intended to be closer to real-time reasoning may be dis-\nabled or turned down. While this may limit the complexity at which an agent\ncan interact, limited or no reasoning can improve response times and increase\nuser enjoyment.\n\uf0a1Customer service bot\u2014Again, because of the controlled nature of this environ-\nment, it\u2019s unlikely that assistants engaged directly with customers will need to\nperform complex or any form of reasoning. \n\uf0a1Autonomous agent\u2014While reasoning is a strong component of autonomous\nagents, we still don\u2019t know how much reasoning is too much. As models such as\nStrawberry become available for agentic workflows, we can gauge at what\npoint extensive reasoning may not be needed. This will surely be the case for\nwell-defined autonomous agent workflows.\n\uf0a1Collaborative workflows\u2014Again, applying reasoning creates an overhead in the LLM\ninteraction. Extensive reasoning may provide benefits for some workflows, while\nother well-defined workflows may suffer. This may mean that these types of work-\nflows will benefit from multiple agents\u2014those with reasoning and those without. \n\uf0a1Game AI\u2014Similar to other applications, heavy-reasoning applications may not\nbe appropriate for most game AIs. Games will especially require LLM response\ntimes to be quick, and this will surely be the application of reasoning for gen-\neral tactical agents. Of course, that doesn\u2019t preclude the use of other reasoning\nagents that may provide more strategic control.\n\uf0a1Research\u2014Reasoning will likely be essential to any complex research task for sev-\neral reasons. A good example is the application of the Strawberry model, which\nwe\u2019ve already seen in research done in mathematics and the sciences.\nWhile we often consider reasoning in tandem with planning, there may be conditions\nwhere the level at which each is implemented may differ. In the next section we con-\nsider the agent pillar of evaluation of various applications.\n11.5.3 Application of evaluation to agentic systems\nEvaluation is the component of agentic/assistant systems that can guide how well the\nsystem performs. While we demonstrated incorporating evaluation in some agentic\nworkflows, evaluation is often an external component in agentic systems. However, it\u2019s\nalso a core component of most LLM applications and not something that should be\n\n\n292\nCHAPTER 11\nAgent planning and feedback\noverlooked in most developments. Table 11.3 describes at a high level how the evalua-\ntion component can be integrated with various LLM application types.\nTable 11.3 shows several varied application scenarios in which we may find an assis-\ntant or agent deployed to assist in some capacity. To provide further information\nand guidance, this list provides more details about how evaluation may be employed\nin each application:\n\uf0a1Personal assistant\u2014In most cases, an evaluation component will be used to process\nand guide the performance of agent responses. In systems primarily employing\nretrieval augmented generation (RAG) for document exploration, the evaluation\nindicates how well the assistant responds to information requests. \n\uf0a1Customer service bot\u2014Evaluating service bots is critical to understanding how well\nthe bot responds to customer requests. In many cases, a strong RAG knowledge\nelement may be an element of the system that will require extensive and ongoing\nevaluation. Again, with most evaluation components, this element is external to\nTable 11.3\nWhen and where evaluation is employed and used in various applications\nApplication\nImplemented\nEnvironment\nPurpose\nTiming\nConfiguration\nPersonal \nassistant\nExternal\nServer\nDetermine how \nwell the system \nis working\nAfter the \ninteraction\nOften devel-\noped externally\nCustomer \nservice bot\nExternal \nmonitor\nServer\nEvaluate the suc-\ncess of each \ninteraction\nAfter the \ninteraction\nExternal to the \nagent system\nAutonomous \nagent\nExternal or \ninternal\nServer or \nservice\nDetermine the \nsuccess of the \nsystem after \nor during task \ncompletion\nAfter the \ninteraction \nExternal or \ninternal \nCollaborative \nworkflows\nExternal\nShared can-\nvas or coding\nEvaluate the \nsuccess of the \ncollaboration\nAfter the \ninteraction\nExternal service\nGame AI\nExternal or \ninternal\nServer or \napplication\nEvaluate the \nagent or evalu-\nate the success \nof a strategy or \naction\nAfter the \ninteraction\nExternal or \nas part of the \nagent or another \nagent\nResearch\nCombined \nmanual and \nLLM\nServer and \nhuman\nEvaluate the \noutput of the \nresearch \ndeveloped\nAfter the \ngenerated \noutput\nDepends on the \ncomplexity of \nthe problem \nand research \nundertaken\n\n\n293\n11.5\nApplying planning, reasoning, evaluation, and feedback to assistant and agentic sys-\nthe main working system and is often run as part of monitoring general perfor-\nmance over several metrics.\n\uf0a1Autonomous agent\u2014In most cases, a manual review of agent output will be a pri-\nmary guide to the success of an autonomous agent. However, in some cases,\ninternal evaluation can help guide the agent when it\u2019s undertaking complex\ntasks or as a means of improving the final output. Multiple agent systems, such\nas CrewAI and AutoGen, are examples of autonomous agents that use internal\nfeedback to improve the generated output.\n\uf0a1Collaborative workflows\u2014In most direct cases, manual evaluation is ongoing\nwithin these types of workflows. A user will often immediately and in near\nreal time correct the assistant/agent by evaluating the output. Additional\nagents could be added similarly to autonomous agents for more extensive\ncollaborative workflows.\n\uf0a1Game AI\u2014Evaluation will often be broken down into development evaluation\u2014\nevaluating how the agent interacts with the game\u2014and in-game evaluation, evalu-\nating how well an agent succeeded at a task. Implementing the later evaluation\nform is similar to autonomous agents but aims to improve some strategies or\nexecution. Such in-game evaluations would also likely benefit from memory\nand a means of feedback.\n\uf0a1Research\u2014Evaluation at this level generally occurs as a manual effort after com-\npleting the research task. An agent could employ some form of evaluation simi-\nlar to autonomous agents to improve the generated output, perhaps even\ncontemplating internally how evaluation of the output could be extended or\nfurther researched. Because this is currently a new area for agentic develop-\nment, how well this will be executed remains to be seen.\nEvaluation is an essential element to any agentic or assistant system, especially if that\nsystem provides real and fundamental information to users. Developing evaluation sys-\ntems for agents and assistants is likely something that could or should have its own\nbook. In the final section of this chapter, we\u2019ll look at feedback implementation for\nvarious LLM applications.\n11.5.4 Application of feedback to agentic/assistant applications\nFeedback as a component of agentic systems is often, if not always, implemented as\nan external component\u2014at least for now. Perhaps confidence in evaluation systems\nmay improve to the point where feedback is regularly incorporated into such sys-\ntems. Table 11.4 showcases how feedback can be implemented into various LLM\napplications.\n \n \n \n\n\n294\nCHAPTER 11\nAgent planning and feedback\nTable 11.4 shows several application scenarios in which we may find an assistant or agent\ndeployed to assist in some capacity. To provide further information and guidance, this list\nprovides more details about how feedback may be employed in each application:\n\uf0a1Personal assistant\u2014If the assistant or agent interacts with the user in a chat-style\ninterface, direct and immediate feedback can be applied by the user. Whether\nthis feedback is sustained over future conversations or interactions, it usually\ndevelops within agentic memory. Assistants such as ChatGPT now incorporate\nmemory and can benefit from explicit user feedback.\n\uf0a1Customer service bot\u2014User or system feedback is typically provided through a survey\nafter the interaction has completed. This usually means that feedback is regulated\nto an external system that aggregates the feedback for later improvements.\n\uf0a1Autonomous agent\u2014Much like bots, feedback within autonomous agents is typi-\ncally regulated to after the agent has completed a task that a user then reviews.\nThe feedback mechanism may be harder to capture because many things can\nTable 11.4\nWhen and where feedback is employed and used in various applications\nApplication\nImplemented\nEnvironment\nPurpose\nTiming\nConfiguration\nPersonal \nassistant\nExternal or \nby the user\nAggregated to \nthe server or \nas part of the \nsystem\nProvides means \nof system \nimprovement\nAfter or during \nthe interaction\nInternal and \nexternal\nCustomer \nservice bot\nExternal \nmonitor\nAggregated to \nthe server\nQualifies and \nprovides a means \nfor system \nimprovement\nAfter the \ninteraction\nExternal to the \nagent system\nAutonomous \nagent\nExternal \nAggregated at \nthe server\nProvides a means \nfor system \nimprovement\nAfter the \ninteraction \nExternal \nCollaborative \nworkflows\nWhile \ninteracting\nShared can-\nvas or coding\nProvides a mecha-\nnism for immedi-\nate feedback\nDuring the \ninteraction\nExternal \nservice\nGame AI\nExternal or \ninternal\nServer or \napplication\nAs part of internal \nevaluation feed-\nback provided \nfor dynamic \nimprovement\nAfter or during \nthe interaction\nExternal or \nas part of \nthe agent or \nanother agent\nResearch\nCombined \nmanual and \nLLM\nServer and \nhuman\nEvaluate the out-\nput of the research \ndeveloped\nAfter the \ngenerated \noutput\nDepends on \nthe complexity \nof the problem \nand the \nresearch \nundertaken\n\n\n295\n11.5\nApplying planning, reasoning, evaluation, and feedback to assistant and agentic sys-\nbe subjective. Methods explored in this chapter for producing feedback can be\nused within prompt engineering improvements.\n\uf0a1Collaborative workflows\u2014Similar to the personal assistant, these types of appli-\ncations can benefit from immediate and direct feedback from the user. Again,\nhow this information is persisted across sessions is often an implementation of\nagentic memory.\n\uf0a1Game AI\u2014Feedback can be implemented alongside evaluation through addi-\ntional and multiple agents. This feedback form may again be single-use and\nexist within the current interaction or may persist as memory. Imagine a game\nAI that can evaluate its actions, improve those with feedback, and remember\nthose improvements. While this pattern isn\u2019t ideal for games, it will certainly\nimprove the gameplay experience.\n\uf0a1Research\u2014Similar to evaluation in the context of research, feedback is typically\nperformed offline after the output is evaluated. While some development has\nbeen done using multiple agent systems incorporating agents for evaluation\nand feedback, these systems don\u2019t always perform well, at least not with the cur-\nrent state-of-the-art models. Instead, it\u2019s often better to isolate feedback and\nevaluation at the end to avoid the common feedback looping problem. \nFeedback is another powerful component of agentic and assistant systems, but it\u2019s not\nalways required on the first release. However, incorporating rigorous feedback and\nevaluation mechanisms can greatly benefit agentic systems in the long term concern-\ning ongoing monitoring and providing the confidence to improve various aspects of\nthe system.\n How you implement each of these components in your agentic systems may, in\npart, be guided by the architecture of your chosen agentic platform. Now that you\nunderstand the nuances of each component, you also have the knowledge to guide\nyou in selecting the right agent system that fits your application and business use case.\nRegardless of your application, you\u2019ll want to employ several agentic components in\nalmost all cases.\n As agentic systems mature and LLMs themselves get smarter, some of the compo-\nnents we today consider external may be closely integrated. We\u2019ve already seen rea-\nsoning and planning be integrated into a model such as Strawberry. Certainly, as we\napproach the theoretical artificial general intelligence milestone, we may see models\ncapable of performing long-term self-evaluation and feedback. \n In any case, I hope you enjoyed this journey with me into this incredible frontier of\na new and emerging technology that will certainly alter our perception of work and\nhow we undertake it through agents.\n\n\n296\nCHAPTER 11\nAgent planning and feedback\n11.6\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Implement a Simple Planning Agent (Beginner)\nObjective\u2014Learn how to implement a basic planning agent using a prompt to\ngenerate a sequence of actions.\nTasks:\n\u2013 Create an agent that receives a goal, breaks it into steps, and executes those\nsteps sequentially.\n\u2013 Define a simple goal, such as retrieving information from Wikipedia and sav-\ning it to a file.\n\u2013 Implement the agent using a basic planner prompt (refer to the planner\nexample in section 11.3).\n\u2013 Run the agent, and evaluate how well it plans and executes each step.\n\uf0a1Exercise 2\u2014Test Feedback Integration in a Planning Agent (Intermediate)\nObjective\u2014Understand how feedback mechanisms can improve the performance\nof an agentic system.\nTasks:\n\u2013 Modify the agent from exercise 1 to include a feedback loop after each task.\n\u2013 Use the feedback to adjust or correct the next task in the sequence.\n\u2013 Test the agent by giving it a more complex task, such as gathering data from\nmultiple sources, and observe how the feedback improves its performance.\n\u2013 Document and compare the agent\u2019s behavior before and after adding feedback.\n\uf0a1Exercise 3\u2014Experiment with Parallel and Sequential Planning (Intermediate)\nObjective\u2014Learn the difference between parallel and sequential actions and\nhow they affect agent behavior.\nTasks:\n\u2013 Set up two agents using Nexus: one that executes tasks in parallel and another\nthat performs tasks sequentially.\n\u2013 Define a multistep goal where some actions depend on the results of previous\nactions (sequential), and some can be done simultaneously (parallel).\n\u2013 Compare the performance and output of both agents, noting any errors or\ninefficiencies in parallel execution when sequential steps are required.\n\uf0a1Exercise 4\u2014Build and Integrate a Custom Planner into Nexus (Advanced)\nObjective\u2014Learn how to build a custom planner and integrate it into an agent\nplatform.\nTasks:\n\u2013 Write a custom planner using prompt engineering strategies from section 11.3,\nensuring it supports sequential task execution.\n\u2013 Integrate this planner into Nexus, and create an agent that uses it.\n\n\n297\nSummary\n\u2013 Test the planner with a complex goal that involves multiple steps and tools\n(e.g., data retrieval, processing, and saving).\n\u2013 Evaluate how the custom planner performs compared to built-in planners in\nNexus or other platforms.\n\uf0a1Exercise 5\u2014Implement Error Handling and Feedback in Sequential Planning\n(Advanced)\nObjective\u2014Learn how to implement error handling and feedback to refine sequen-\ntial planning in an agentic system.\nTasks:\n\u2013 Using a sequential planner, set up an agent to perform a goal that may encoun-\nter common errors (e.g., a failed API call, missing data, or invalid input).\n\u2013 Implement error-handling mechanisms in the planner to recognize and\nrespond to these errors.\n\u2013 Add feedback loops to adjust the plan or retry actions based on the error\nencountered.\n\u2013 Test the system by deliberately causing errors during execution, and observe\nhow the agent recovers or adjusts its plan.\nSummary\n\uf0a1Planning is central to agents and assistants, allowing them to take a goal, break\nit into steps, and execute them. Without planning, agents are reduced to simple\nchatbot-like interactions.\n\uf0a1Agents must differentiate between parallel and sequential actions. Many LLMs\ncan handle parallel actions, but only advanced models support sequential plan-\nning, critical for complex task completion.\n\uf0a1Feedback is crucial in guiding agents to correct their course and improve per-\nformance over time. This chapter demonstrates how feedback mechanisms can\nbe integrated with agents to refine their decision-making processes.\n\uf0a1Platforms such as OpenAI Assistants and Anthropic\u2019s Claude support internal\nplanning and can execute complex, multistep tasks. Agents using these plat-\nforms can use sequential action planning for sophisticated workflows.\n\uf0a1Properly selecting and limiting agent actions is vital to avoid confusion and\nunintended behavior. Too many actions may overwhelm an agent, while unnec-\nessary tools may be misused.\n\uf0a1Nexus allows for creating and managing agents through a flexible interface,\nwhere users can implement custom planners, set goals, and assign tools. The\nchapter includes practical examples using Nexus to highlight the difference\nbetween a raw LLM and a planner-enhanced agent.\n\uf0a1Writing custom planners is straightforward, using prompt engineering strate-\ngies. Tools such as LangChain and Semantic Kernel offer a variety of planners\nthat can be adapted or extended to fit specific agentic needs.\n",
      "page_number": 313
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 322-331)",
      "start_page": 322,
      "end_page": 331,
      "detection_method": "topic_boundary",
      "content": "298\nCHAPTER 11\nAgent planning and feedback\n\uf0a1Models such as OpenAI Strawberry integrate reasoning, planning, evalua-\ntion, and feedback directly into the LLM, offering more accurate problem-\nsolving capabilities.\n\uf0a1Evaluation helps determine how well an agentic system is performing and can\nbe implemented internally or externally, depending on the use case.\n\uf0a1As LLMs evolve, reasoning, planning, and feedback mechanisms may become\ndeeply integrated into models, paving the way for more autonomous and intelli-\ngent agent systems.\n\n\n299\nappendix A\nAccessing OpenAI\nlarge language models\nAlthough several commercial large language model (LLM) services are available,\nthis book recommends using OpenAI services directly or through Azure OpenAI\nStudio. To access either service, you must create an account and register a payment\nmethod not covered in this appendix. The GPT-4 family of LLMs is considered best\nin class and better suited for agent development. Using open source and alterna-\ntive services is always an option but generally only advisable after you\u2019ve worked\nwith GPT-4 for some time.\nA.1\nAccessing OpenAI accounts and keys\nThe following general steps can help you quickly set up using OpenAI LLMs for agent\ndevelopment. Though using OpenAI and other commercial LLMs comes at a price,\nyou can expect to pay less than US$100 to complete all the exercises in this book:\n1\nGo to https://openai.com and log in, or register for an account and log in. If\nthis is your first time creating an account, you\u2019ll likely be given free credit in\nsome amount. If you already have an account, you must register a payment\nmethod and type. It\u2019s generally better to purchase a number of credits at a\ntime. This will allow you to manage the costs better and avoid overruns.\n2\nAfter logging in to the platform, select ChatGPT or the API, as shown in\nfigure A.1. Choose the API.\n \n \n \n\n\n300\nAPPENDIX A\nAccessing OpenAI large language models\n3\nOpen the left menu, and select the API Keys option, as shown in figure A.2.\nSelect the API\n.\n.\nFigure A.1\nSelecting the API section of the OpenAI platform\nSelect the API\neys\nK\nFigure A.2\nSelecting \nthe API Keys option\n\n\n301\nA.1\nAccessing OpenAI accounts and keys\n4\nClick the Create button to create a new key, enter a name for the key, and click\nthe Create Secret Key button, as shown in figure A.3.\n5\nCopy and paste the key to a notepad or another area for safekeeping using the\nCopy button, as shown in figure A.4. Keep this key secret, and ensure it remains\nonly on your development machine.\nAfter generating a key, you can continue to use it within an .env configuration file or\nthrough other means of registering an OpenAI key. For most of the packages used in\nthis book, configuring OpenAI will generally only require the key. Other services,\nsuch as Azure OpenAI, will require the configuration of a model deployment and a\nbase URL as covered in the next section.\n \n \n \nEnter a helpful name,\nsuch as GPT-Agents.\nClick to create\nthe key.\nStart by clicking the button\nto create a new secret key.\nFigure A.3\nCreating the secret API key\n\n\n302\nAPPENDIX A\nAccessing OpenAI large language models\nA.2\nAzure OpenAI Studio, keys, and deployments\nThrough its ongoing relationship with OpenAI, Microsoft hosts the same models at\nthe same price within Azure OpenAI Studio. Occasionally, Azure may be a model ver-\nsion behind, but Microsoft generally keeps current with the latest OpenAI models.\n These guidelines will be more general because there are several ways to access\nAzure and methods of creating accounts and accessing the studio (for specific instruc-\ntions, refer to Microsoft documentation):\n1\nLog in to your Azure portal account subscription.\n2\nCreate a new Azure OpenAI Studio resource in a region that makes sense to\nyou. At the time of writing, not all regions provided access to all models. You may\nneed to check which models are available for your region first. This will also be\nspecific to your account and usage.\nWithin Azure OpenAI, models are exposed through a resource allocation\ncalled a deployment. Deployments wrap a model, such as GPT-4, and provide\naccess to the resource. Figure A.5 shows an example of various models being\nexposed through deployments.\n3\nClick the Create New Deployment button to create a new deployment, and then\nselect the model you want to deploy.\n \nClick the Copy button to copy\nthe key to the clipboard.\nMake sure to save the\nkey right away.\nClick to create the key.\nFigure A.4\nCopying and pasting the key to a well-known safe location\n\n\n303\nA.2\nAzure OpenAI Studio, keys, and deployments\n4\nAfter the model is wrapped in a deployment, you must access the parent Azure\nOpenAI resource. From there, you can access the key, endpoint, or base URL\nneeded to configure your connection, as shown in figure A.6.\nAgain, if you get stuck, the Microsoft documentation can guide you in the right direc-\ntion. The three critical differences to remember when connecting to a resource such\nas Azure OpenAI Studio or another LLM using the OpenAI tooling are listed here:\n\uf0a1The api key to access the model\n\uf0a1The base url or endpoint where the model is located\n\uf0a1The name of the model or deployment name\nIf you can\u2019t access a model for whatever reason, a good alternative is open source\nmodels. Setting up and consuming open source LLMs is covered in chapter 2. \nDeployment name is the name\nreferenced as the model name.\nOpenAI model\nname\nOther resource\ninformation\nFigure A.5\nDeploying a model through an Azure OpenAI Studio deployment\n\n\n304\nAPPENDIX A\nAccessing OpenAI large language models\nClick to copy, and then\npaste the keys as needed.\nThe base URL for\nthe model service\nOther resource\ninformation\n\n\n305\nappendix B\nPython development\nenvironment\nWhile this book assumes readers are experienced Python developers, this could\nmean many different things. In this appendix, we look at configuring a Python\ndevelopment environment that will function with the code examples in this book.\nYou can use other integrated development environments (IDEs), but not all tool-\ning, especially extensions, will work in all IDEs.\nB.1\nDownloading the source code\nTo download and run the source code, install Git, and then pull the repository\nlocally. Here are the high-level steps to pull the code from the book\u2019s GitHub\nrepository:\n1\nInstall Git if you need to. Git can be installed from multiple sources, but a\ngood option is the main release, found here: https://git-scm.com/down-\nloads. Follow the instructions to download and install the tool for your oper-\nating system.\n2\nOpen a terminal in a folder you want to download the source to, and then\nenter the following command:\ngit clone https://github.com/cxbxmxcx/GPT-Agents.git\n3\nAfter the code is downloaded, you can begin by opening the chapter folder\nthat you\u2019re working on in Visual Studio Code (VS Code). If you need to\n\n\n306\nAPPENDIX B\nPython development environment\ninstall VS Code or understand how to load a chapter folder as a workspace, con-\nsult section B.5 in this appendix.\nB.2\nInstalling Python\nPython is provided through different versions and deployments. This book relies on\nthe standard Python installation, version 3.10. Anaconda is another deployment of\nPython that is very popular and could be used. However, all the material in this book\nhas been run and tested with a Python 3.10 virtual environment:\n1\nGo to www.python.org/downloads/.\n2\nLocate and download the latest release of Python 3.10 for your operating system.\n3\nInstall the release on your machine using the instructions for your operating\nsystem.\n4\nTo confirm your installation, open a terminal, and execute the following\ncommand:\npython \u2013-version\nThe version should be 3.10, but if it isn\u2019t, don't worry. You may have multiple Python\nversions installed. We\u2019ll also confirm the installation when setting up VS Code.\nB.3\nInstalling VS Code\nInstalling VS Code is relatively straightforward and can be done in just a few steps:\n1\nGo to https://code.visualstudio.com.\n2\nDownload a stable release of VS Code for your operating system.\n3\nAfter the release is downloaded, follow the installation instructions for your\noperating system.\n4\nLaunch VS Code for your operating system, and make sure no warnings or\nerrors appear. If you encounter problems, try to restart your computer and/or\nreinstall.\nWith VS Code running, we can install the necessary extensions. We\u2019ll cover those\nextensions next.\nB.4\nInstalling VS Code Python extensions\nThousands of extensions for VS Code can provide an excellent Python coding envi-\nronment. The recommended ones are only the start of what you can explore inde-\npendently. Beware, though, that not all extensions are created equally. When installing\nnew extensions, look at the number of installs and ratings. Extensions with fewer than\nfour stars are generally to be avoided. To install the extensions, follow these steps:\n \n\n\n307\nB.4\nInstalling VS Code Python extensions\n1\nLaunch VS Code, and open the Extensions panel, as shown in figure B.1.\n2\nInstall the following list of extensions:\n\u2013 Python, for environment and language support\n\u2013 Python Extension Pack, for covering other extensions\n\u2013 Python Environment Manager, for managing environments\n\u2013 Python Indent, for code formatting\n\u2013 Flake8, for code formatting/linting\n\u2013 Prompt Flow, for testing LLM prompts\n\u2013 Semantic Kernel Tools, for working with the Semantic Kernel framework\n\u2013 Docker, for managing Docker containers\n\u2013 Dev Containers, for running development environments with containers\nYou\u2019ll only need to install the extensions for each VS Code environment you\u2019re run-\nning. Typically, this will mean installing for just your operating system installation of\nVS Code. However, if you run VS Code in containers, you must install extensions for\neach container you\u2019re running. Working with Python in the Dev Containers extension\nwill be covered later in this appendix.\nOpen the\nExtensions panel.\nSelect the\nextensions.\nEnter your search term.\nClick the Install\nbutton to install.\nFigure B.1\nInstalling VS Code extensions\n",
      "page_number": 322
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 332-340)",
      "start_page": 332,
      "end_page": 340,
      "detection_method": "topic_boundary",
      "content": "308\nAPPENDIX B\nPython development environment\nB.5\nCreating a new Python environment with VS Code\nWhen developing Python projects, you often want to create isolated virtual environ-\nments. This will help in managing multiple package dependencies across various tasks\nand tools. In this book, it\u2019s recommended that a new virtual environment be created\nfor each new chapter. VS Code can help you create and manage multiple Python envi-\nronments quickly and efficiently via the following steps:\n1\nPress Ctrl-Shift-P (Cmd-Shift-P) to open the command panel, and select Python:\nCreate Environment, as shown in figure B.2.\n2\nSelect the environment type, either Venv or Conda. This book demonstrates\nVenv but Conda should also work.\nOpen the command panel (Ctrl-Shift-P, Cmd-Shift-P)\nSelect to create a new environment.\nUse the Refresh button if your installation is shown.\nCheck the requirements.txt \ufb01le, which should\ncontain all the dependencies for the chapter.\nSelect the installation. This list should re\ufb02ect\nthe Python versions you have installed.\nSelect the type of environment. The\ninstructions in this book use an .venv\nenvironment, but Conda should work\njust as well.\nFigure B.2\nThe steps to set up the virtual environment for a chapter\n\n\n309\nB.6\nUsing VS Code Dev Containers (Docker)\n3\nSelect the Python installation. The code in this book has been run with Python\n3.10 at a minimum. The agent tools and frameworks featured in this book are\ncutting edge, so they should support later versions of Python.\n4\nCheck that the requirements.txt file in the chapter folder is selected. This will\ninstall all the requirements for the current chapter.\nYou should complete these steps for each new chapter of the book. The alternative is\nto use VS Code development containers, which will be covered in the next section.\nB.6\nUsing VS Code Dev Containers (Docker)\nWhen working with advanced agents and agents that can generate and execute\ncode, running them in isolated containers is generally recommended. Container\nisolation prevents operating system disruption or corruption and provides a base for\ndeploying agents.\n Getting familiar with containers and platforms such as Docker can be an extensive\nundertaking to grasp everything. Fortunately, it takes very little knowledge to start\nusing containers, and VS Code extensions make this even more accessible.\n You\u2019ll first need to install a container toolset. Docker is free (provided you use the\ntool as a hobby or you\u2019re a student) and the most accessible. Follow these instructions\nto install Docker and get started working with containers:\n1\nGo to the Docker Desktop download page at www.docker.com/products/\ndocker-desktop.\n2\nDownload and install Docker for your operating system. Follow any other\ninstructions as requested.\n3\nLaunch the Docker desktop application. Completing this step will confirm you\nhave Docker installed and working as expected.\n4\nOpen VS Code, and confirm that the Docker extensions listed in section 1.4 are\ninstalled.\nWith Docker and VS Code configured, you can move on to using Dev Containers by\nfollowing these steps:\n1\nOpen a new instance of VS Code.\n2\nSelect to open a remote window, as shown in figure B.3. \n3\nSelect Open Folder in Container to start a container from a folder, or select\nNew Dev Container to start without a folder.\nAfter the container is launched, your VS Code environment will be con-\nnected. This allows you to develop code on the container without worrying about\ndependencies not working.\n\n\n310\nAPPENDIX B\nPython development environment\nSelect Open Folder\nin Container.\nSelect Open\nRemote Window.\nFigure B.3\nOpening a remote window to a container in VS Code\n\n\n311\nindex\nA\nABTs (agentic behavior trees) 143\u2013152\nblackboards in 155\nbuilding with back chaining 155\u2013156\ncoding challenge ABT 145\u2013149\nconversational AI systems 149\nmanaging assistants with assistants 143\nposting YouTube videos to X 150\nrequired X setup 151\nactions\nand tools, agent platforms 174\u2013179\ndefining 99\nempowering agents with 98\nwriting semantic services 125\nexecuting OpenAI functions 101\u2013107\nactioning function calls 103\u2013107\nadding functions to LLM API calls 101\nexercises 127\nnative functions 111\u2013118\nsemantic functions 111\u2013118\nembedding native functions within\n117\nSemantic Kernel (SK) 107\u2013111\ncontext variables 109\nsemantic functions 108\u2013111\nadvanced parameters 221\nagent_action decorator 175\nagent critics 79\nagent engines 172\nagentic systems\napplication of\nassistant/agentic planning 288\u2013290\nassistant/agentic reasoning 290\u2013291\nevaluation to agentic systems 292\u2013293\nfeedback to agentic/assistant \napplications 293\u2013295\nimplementing memory in 200\u2013207\nsemantic memory 204\nagent memory, exercises 209\nagentops package 88\nagent personas 216\nagent planning\nexercises 296\nsequential planners 278\u2013285\nagent planning and feedback 272\nagent platforms\nactions and tools 174\u2013179\nagent engines 172\ndeveloping profiles and personas for \nagents 170\noverview of 161\u2013164\nStreamlit 165\u2013170\nbuilding chat applications 165\u2013168\ncreating streaming chat applications\n168\nagent profiles 216\nevaluating 224\u2013228\nagent prompts\nagent personas 216\nagent profiles 216\nrubrics and grounding 228\u2013230\nagent reasoning\ndirect solution prompting 245\u2013252\nfew-shot prompting 248\u2013250\nquestion and answer prompting\n246\u2013248\nzero-shot prompting 250\n\n\nINDEX\n312\nagents\ncomponent systems of 4\u20139\ndefining 1\u20134\nempowering with actions 98\nexecuting OpenAI functions 101\u2013107\nmemory and knowledge\nsemantic search and document \nindexing 184\u2013192\nOpenAI Strawberry 285\u2013287\noverview of 12\nplanning 273\u2013277\nreasoning and evaluation 244\nrise of 9\naggregate block 238, 240\nAGI (artificial general intelligence) 9\nAI agents, AI interface 11\nApi parameter 221\nAPIs (application programming interfaces), \ndeploying prompt flow API 223\nassistant role 18\nassistants\nABTs (agentic behavior trees) 143\u2013152\nengaging through ChatGPT 40\u201344\nPlayground 136\u2013142\ngetting assistant to run code locally 140\ninstalling and running 136\ninstalling assistants database 140\ninvestigating assistant process through \nlogs 142\nusing and building custom actions 138\nSocial Media Assistant 152\nTwitter Post Writer 152\nYouTube Researcher v2 152\nassistant systems, application of\nassistant/agentic planning 288\u2013290\nassistant/agentic reasoning 290\u2013291\nevaluation to agentic systems 292\u2013293\nfeedback to agentic/assistant applications\n293\u2013295\nAutoGen Studio, multi-agent systems with\n69\u201376\nadding skills 73\u201376\ninstalling and using 70\nautonomous assistants 129\nbehavior trees 130\u2013136\ndeciding on 132\u2013134\nexecution 131\nrunning with Python and py_trees 134\nconversational autonomous multi-agents\n153\nexercises 156\nAzure OpenAI Studio 302\nB\nBaseAgent class 172\nBasicNexusPlanner class 282\nbehavior trees 130\u2013136\nblackboards in 155\nbuilding with back chaining 155\u2013156\ndeciding on 132\u2013134\nexecution 131\nrunning with Python and py_trees 134\nblackboards 155\nC\nchain of thought (CoT) prompting 253\u2013257\nchat completions 16\nChatGPT, engaging assistants through 40\u201344\ncoding challenge ABT (agentic behavior \ntree) 145\u2013149\ncompression\nknowledge 207\nblending knowledge and memory 209\ncase for 208\nfrequency of application 208\nmultiple knowledge stores 209\nmultiple passes of 208\nmemory 207\nblending knowledge and memory 209\nfrequency of application 208\nmultiple memory stores 209\nmultiple passes of 208\nconfig.json file 112\nconnecting.py code 36\nConnection parameter 221\ncontext variables 109\nConversableAgent agent 78\nconversational AI systems 149\ncosine_similarities variable 186\ncosine_similarity function 186\ncreate_assistant_action_on_thread helper \nfunction 149\ncreate_assistant_condition helper function 149\ncreate_manager_assistant action 144\ncreate_plan function 282\nCrewAI\nbuilding agent crew with 84\u201390\ncreating jokester crew of CrewAI agents\n84\u201387\nobserving agents working with AgentOps\n87\u201390\nrevisiting coding agents with 90\u201395\ncrewai_agentops.py file 88\n\n\nINDEX\n313\ncrewai package 88\ncustom actions 49\u201355\nconnecting custom action to assistant 53\ncreating assistant to build assistant 49\u201353\nD\nDAG (direct acyclic graph) 262\ndescribe_image function 75\nDirect Prompting 270\ndirect solution prompting 245\u2013252\nfew-shot prompting 248\u2013250\nquestion and answer prompting 246\u2013248\nzero-shot prompting 250\nDocker 309\ndocument indexing 184\u2013192\ndocument embeddings 189\nquerying document embeddings from \nChroma 190\nvector similarity search 184\u2013188\nInverse Document Frequency (IDF) 185\nTerm Frequency (TF) 185\nTF-IDF calculation 185\u2013188\nvector databases and similarity search 188\nE\necho block 221\nEmbedding components 246\nepisodic memory 204\nevaluate_recommendation block 231\nevaluation, employing for consistent \nsolutions 261\u2013270\nself-consistency prompting 262\u2013266\nToT prompting 266\u2013270\nevaluation prompt techniques 271\nF\nfeedback\napplication of feedback to agentic/assistant \napplications 293\u2013295\nstepwise planner, OpenAI Strawberry\n285\u2013287\nfew-shot learning 248\nfew-shot model 248\nFew-Shot Prompting 270\nfile uploads extending assistants\u2019 knowledge \nusing 56\u201361\nbuilding Calculus Made Easy GPT 56\u201358\nknowledge search and more with file \nuploads 58\u201361\nfirst_function.py Python script 103\nflow.dag.yaml file 236, 240\nfor-each function 283\nfunctions\nactioning function calls 103\u2013107\nadding to LLM API calls 101\nG\ngenerative models 14\nget_response function 173\nget_response_stream function 176\nget_top_movie_by_genre function 124\nget_top_movies_by_genre function 126\nget_wikipedia_page action 278\nGPT Assistants Playground 136\u2013142\ngetting assistant to run code locally 140\ninstalling and running 136\ninstalling assistants database 140\ninvestigating assistant process through logs\n142\nusing and building custom actions 138\nGPT (Generative Pretrained Transformer) \nassistants 39\nbuilding 44\u201348\nexercises 65\nOpenAI Assistants 136\u2013149\npublishing 61\u201365\neconomics of GPTs 63\nexpensive GPT assistants 62\nreleasing GPT 63\nGPTs (Generative Pretrained Transformers)\n14\nbuilding semantic interface 119\u2013121\ncustomizing 49\u201355\nconnecting custom action to assistant 53\ncreating assistant to build assistant\n49\u201353\ngrounding 228\u2013230\nGroupChat 82\nGroupChatManager 82\ngroup chat with agents and AutoGen 82\nI\nIDEs (integrated development environments)\n305\nIDF (Inverse Document Frequency) 185\nimport_plugin function 118\ninitiate_chat call 82\nInputs block 221\ninput tokens 20\n\n\nINDEX\n314\ninteractive service agent, semantic kernel \nas 118\u2013125\nbuilding semantic GPT interface 119\u2013121\ninteractive chat with semantic service layer\n122\u2013125\ntesting semantic services 121\nJ\nJinja2 templates, creating profiles with 222\nJSON Lines (JSONL) 235\nJSON list document 235\nK\nknowledge 180\nbuilding agent knowledge 196\u2013200\ncompression 207\nblending knowledge and memory 209\ncase for 208\nextending assistants\u2019 knowledge using file \nuploads 56\u201361\nbuilding Calculus Made Easy GPT 56\u201358\nknowledge search with file uploads 58\u201361\nstores 209\nKnowledgeManager class 199\nknowledge/memory 162\nL\nLangChain, constructing RAG with 192\u2013196\nsplitting and loading documents 193\u2013194\nsplitting documents by token 195\nlaunch.json file 168\nLLMs (large language models) 2, 14, 99, 161, \n299\nadding functions to API calls 101\nchoosing optimal 34\u201336\nexercises 14\ngrounding evaluation with LLM profile\n230\nOpenAI API 16\u201320\nconnecting to chat completions model 16\nrequest and response 18\u201320\nopen source LLMs with LM Studio 20\u201325\nprompt engineering 25\u201334\nadopting personas 29\ncreating detailed queries 28\nproviding examples 32\nspecifying output length 33\nspecifying steps 31\nusing delimiters 30\nLM Studio 20\u201325\ninstalling and running 20\nserving LLM locally with 23\nload_dotenv function 17\nLocal Data File 236\nM\nmanager_llm class 92\nmax_rpm (maximum requests per minute) 87\nMax_tokens parameter 221\nmemory 180\ncompression 207\nconsuming memory stores in Nexus 202\nimplementing in agentic systems 200\u2013207\nretrieval 181\nsemantic memory 204\nModel parameter 221\nmulti-agent systems 3, 68\nAutoGen 77\u201382\ncache 81\nenhancing code output with agent critics 79\ninstalling and consuming 77\u201379\nbuilding agent crew with CrewAI 84\u201390\ncreating jokester crew of CrewAI agents\n84\u201387\nobserving agents working with AgentOps\n87\u201390\nexercises 95\ngroup chat with agents and AutoGen 82\nrevisiting coding agents with CrewAI 90\u201395\nwith AutoGen Studio 69\u201376\nadding skills 73\u201376\ninstalling and using 70\nmultipath reasoning 9\nN\nnative functions 111\u2013118, 127\napplying 115\nembedding within semantic functions 117\nNexus\nconsuming memory stores in 202\ndeveloping 163\noverview of 161\u2013164\nrunning 162\nNPCs (nonplayer characters) 130\nO\nOAI_CONFIG_LIST file 78\none-shot learning 248\n\n\nINDEX\n315\none-shot model 248\nOpenAI\naccessing accounts and keys 299\nAzure OpenAI Studio 302\nOpenAI API 16\u201320\nconnecting to chat completions model 16\nrequest and response 18\u201320\nOutputs block 221\noutput tokens 20\nP\nparsing_results block 232\nPCA (principal component analysis)\n190\npersonas 29\ndeveloping for agents 170\npip command 77\nplanning 252\nstepwise planner, OpenAI Strawberry\n285\u2013287\nplanning agents 273\u2013277\nplanning/feedback 162\nplugin folder 115\nplugins, creating and registering 111\u2013115\nprocedural memory 204\nprofiles\ncomparing 232\u2013243\ncreating evaluation flow for grounding\n238\u2013241\nexercises 242\nparsing LLM evaluation output 232\nrunning batch processing in prompt \nflow 235\u2013238\ndeveloping for agents 170\nprompt chaining 258\u2013261\nprompt engineering 9, 25\u201334\nadopting personas 29\ncreating detailed queries 28\nproviding examples 32\nreasoning in 252\u2013261\nchain of thought prompting 253\u2013257\nstep-by-step with prompt chaining\n258\u2013261\nzero-shot CoT prompting 257\nspecifying output length 33\nspecifying steps 31\nsystematic 213\nusing delimiters 30\nprompt_engineering.py file 26, 28\nprompt flow 212\nagent profiles, evaluating 224\u2013228\ncomparing profiles 232\u2013243\ncreating evaluation flow for grounding\n238\u2013241\nexercises 242\nparsing LLM evaluation output 232\nrunning batch processing in prompt \nflow 235\u2013238\ngrounding evaluation with LLM profile 230\nsetting up 217\u2013224\ncreating profiles with Jinja2 templates 222\ndeploying prompt flow API 223\nprerequisites 218\u2013221\nsystematic prompt engineering 213\nPromptTemplateManager 282\nPython\nbehavior trees with 134\ncreating new environment with VS Code 308\ndevelopment environment\ndownloading source code 305\nVS Code Dev Containers (Docker) and 309\ninstalling VS code 306\ninstalling VS Code extensions 306\npy_trees 134\npy_trees package 147\nQ\nquestion_answer LLM prompt 246\nR\nRAG (Retrieval Augmented Generation) 177, 182\napplying to building agent knowledge 196\u2013200\nconstructing with LangChain 192\u2013196\nsplitting and loading documents 193\u2013194\nsplitting documents by token 195\nreasoning 252\nin prompt engineering 252\u2013261\nchain of thought prompting 253\u2013257\nstep-by-step with prompt chaining 258\u2013261\nzero-shot CoT prompting 257\nreasoning agents, exercises 270\nreasoning prompts 270\nrecommender block 221\nrecommender_with_LLM_evaluation flows visual \neditor 232\nregistering\nplugins 111\u2013115\nsemantic functions 111\u2013115\nsemantic plugins 111\u2013115\nsemantic skills 111\u2013115\nskills 111\u2013115\n\n\nINDEX\n316\nrequirements.txt file 77, 246\nretrieval 181\nRLHF (reinforcement learning with human \nfeedback) 16\nrubber ducking 29\nrubrics 228\u2013230\nrun_conversation function 104\nS\nsave_file action 278\nscoring block 238\nsearch_wikipedia action 278\nself-consistency prompting 262\u2013266\nsemantic functions 111\u2013118, 127\ncreating and registering 111\u2013115\nembedding native functions within 117\nsemantic memory 204\nsemantic plugins, creating 111\u2013115\nsemantic search 184\u2013192\nvector similarity search 184\u2013188\ndocument embeddings 189\nInverse Document Frequency (IDF) 185\nquerying document embeddings from \nChroma 190\nTerm Frequency (TF) 185\nTF-IDF calculation 185\u2013188\nvector databases and similarity search 188\nsemantic services, writing 125\nsemantic skills, creating 111\u2013115\nSEO (search engine optimization) 64\nsequential planners 278\u2013285\nsequential planning process 277\nsequential reasoning 9\nsingle-path reasoning 9\nsk_function decorator 120\nskills\ncreating 111\u2013115\nregistering 111\u2013115\nskills directory 115\nskills/plugin folder 112\nskills/Recommender/Recommend_Movies \nfolder 112\nskprompt.txt file 112\nSK (Semantic Kernel) 107\u2013111, 128, 160, 244\nas interactive service agent 118\u2013125\nbuilding semantic GPT interface 119\u2013121\ninteractive chat with semantic service \nlayer 122\u2013125\ntesting semantic services 121\ncontext variables 109\nsemantic functions 108\u2013111\nstep-by-step with prompt chaining 258\u2013261\nstepwise planner, OpenAI Strawberry\n285\u2013287\nStop parameter 221\nStreamlit\nbuilding chat applications 165\u2013168\ncreating streaming chat applications 168\noverview 165\u2013170\nst.spinner control 167\nst.write_stream control 169\nsystem role 18\nT\ntemperature conversion plugin 127\nTemperature parameter 221\nTF-IDF (Term Frequency-Inverse Document \nFrequency) 184\u2013188\nTF (Term Frequency) 185\ntmdb.py file 119\nTMDbService 126\nTMDbService class 119\u2013120, 122\nTMDB (The Movie Database) 118\ntokenization, splitting documents by \ntoken with LangChain 195\nToT (tree of thought) 245\nToT (tree of thought) prompting\n266\u2013270\nU\nulk_recommend.jsonl file 235\nunstructured memory/knowledge concepts\n183\nUserProxy agent 78\nuser role 18\nV\nvector databases 188\nvector similarity search 184\u2013188\ndocument embeddings 189\nInverse Document Frequency (IDF) 185\nquerying document embeddings from \nChroma 190\nTerm Frequency (TF) 185\nTF-IDF calculation 185\u2013188\nvector databases and similarity search\n188\nVS Code\ninstalling 306\ninstalling Python extensions 306\n",
      "page_number": 332
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 341-346)",
      "start_page": 341,
      "end_page": 346,
      "detection_method": "topic_boundary",
      "content": "INDEX\n317\nVS Code Dev Containers (Docker) 309\n.vscode/launch.json file 167\nVS Code (Visual Studio Code) 16, 51, 70, \n102, 136, 218\ncreating new Python environment \nwith 308\nW\nweather information plugin 127\nweb APIs 128\nWindows Subsystem for Linux (WSL)\n79\nwith statement 82\nworking_dir folder 79\nWrite Clear Instructions strategy 26\nWSL (Windows Subsystem for Linux)\n79\nX\nX (formerly Twitter)\nrequired assistants 152\nrequired setup 151\nY\nYouTube\nposting videos to X 150\nrequired assistants 152\nsearch and spam 151\nZ\nzero-shot CoT prompting 257\nzero-shot learning 248\nzero-shot prompting 250, 270\n\n\nFor ordering information, go to www.manning.com\nRELATED MANNING TITLES\nMulti-Agent Systems with AutoGen\nby Victor Dibia\nISBN 9781633436145\n325 pages (estimated), $59.99\nSpring 2025 (estimated)\nGenerative AI for the IT Pro\nby Chrissy LeMaire and Brandon Abshire\nISBN 9781633436428\n350 pages (estimated), $49.99\nSpring 2025 (estimated)\nThe Complete Obsolete Guide to Generative AI\nby David Clinton\nISBN 9781633436985\n240 pages, $39.99\nJuly 2024\nGenerative AI in Action\nby Amit Bahree\nForeword by Eric Boyd\nISBN 9781633436947\n464 pages, $59.99\nSeptember 2024\n\n\nThe Manning Early Access Program\nDon\u2019t wait to start learning! In MEAP, the Manning Early Access Program, you can read \nbooks as they\u2019re being created and long before they\u2019re available in stores. \nHere\u2019s how MEAP works.\n\u2022\t \u0007Start now. Buy a MEAP and you\u2019ll get all available chapters in PDF, ePub, Kindle, \nand liveBook formats. \n\u2022\t \u0007Regular updates. New chapters are released as soon as they\u2019re written. We\u2019ll \nlet you know when fresh content is available.\n\u2022\t \u0007Finish faster. MEAP customers are the first to get final versions of all books! \nPre-order the print book, and it\u2019ll ship as soon as it\u2019s off the press.\n\u2022\t \u0007Contribute to the process. The feedback you share with authors makes the end \nproduct better.\n\u2022\t No risk. You get a full refund or exchange if we ever have to cancel a MEAP.\nExplore dozens of titles in MEAP at www.manning.com.\n\n\nA new online reading experience\nliveBook, our online reading platform, adds a new dimension to your Manning books, \nwith features that make reading, learning, and sharing easier than ever. A liveBook \nversion of your book is included FREE with every Manning book.\nThis next generation book platform is more than an online reader. It\u2019s packed with \nunique features to upgrade and enhance your learning experience.\n\u2022 Add your own notes and bookmarks\n\u2022 One-click code copy\n\u2022 Learn from other readers in the discussion forum\n\u2022 Audio recordings and interactive exercises\n\u2022 Read all your purchased Manning content in any browser, anytime, anywhere\nAs an added bonus, you can search every Manning book and video in liveBook\u2014even \nones you don\u2019t yet own. Open any liveBook, and you\u2019ll be able to browse the content and \nread anything you like.*\nFind out more at www.manning.com/livebook-program.\n*Open reading is limited to 10 minutes per book daily\n\n\nThe difference between iterative and planned execution\nSingle prompt as a goal:\nsearch Wikipedia for topic, return\nthe top page, and save to a \ufb01le.\nSequential execution of a goal\nSingle prompt as a goal:\nsearch Wikipedia for topic, return\nthe top page, and save to a \ufb01le.\nIterative execution of a goal\nTasks/plan\nReturns a list of page IDs\nsearch_wikipedia(topic)\nReturns the page content\nget_wikipedia_page(page_id)\nCollects pages\nsave_ \ufb01le\nAll pages combined\nIn a planner, this\naggregation is\nperformed\ninternally.\nAll tasks are\nperformed\ninternally by\nthe agent.\nUser submits a goal.\nsearch_wikipedia(topic)\nResults are displayed, and the\nagent asks to continue to the\nnext step.\nReturns a list of page IDs\nget_wikipedia_page(page_id)\nUser responds\nwith continue.\nReturns are collected and\ndisplayed to the user. The agent\nasks for further continuation.\nReturns the page content\nsave_ \ufb01le\nAll pages combined\nUser responds\nwith continue.\nGoal is complete, and user has the output.\nIterative\nexecution\nis default\nbehavior\nfor an LLM.\nSequential\n(planned)\nexecution is\nstandard in\nGPT\nAssistants\nand Claude.\n\n\nMicheal Lanham\nM\nost production AI systems require many orchestrated \ninteractions between the user, AI models, and a wide \nvariety of data sources. AI agents capture and organize \nthese interactions into autonomous components that can pro-\ncess information, make decisions, and learn from interactions \nbehind the scenes. Th is book will show you how to create AI \nagents and connect them together into powerful multi-agent \nsystems.\nIn AI Agents in Action, you\u2019ll learn how to build production\n-ready assistants, multi-agent systems, and behavioral agents. \nYou\u2019ll master the essential parts of an agent, including \nretrieval-augmented knowledge and memory, while you create \nmulti-agent applications that can use software tools, plan tasks \nautonomously, and learn from experience. As you explore the \nmany interesting examples, you\u2019ll work with state-of-the-art \ntools like OpenAI Assistants API, GPT Nexus, LangChain, \nPrompt Flow, AutoGen, and CrewAI.\nWhat\u2019s Inside\n\u25cf Knowledge management and memory systems\n\u25cf Feedback loops for continuous agent learning\n\u25cf Collaborative multi-agent systems\n\u25cf Speech and computer vision\nFor intermediate Python programmers.\nMicheal Lanham is a software and technology innovator with \nover 20 years of industry experience. He has authored books \non deep learning, including Manning\u2019s Evolutionary Deep \nLearning.\nTh e technical editor on this book was Ross Turner.\nFor print book owners, all digital formats are free:\nhttps://www.manning.com/freebook\nAI Agents IN ACTION\nPYTHON/SOFTWARE DEVELOPMENT\nM A N N I N G\n\u201c\nTh is is about to become \nthe hottest area of applied \nAI. Get a head start with \n this book!\u201d\n \n\u2014Richard Davies, author of \nPrompt Engineering in Practice\n\u201c\nCouldn\u2019t put this book \ndown! It\u2019s so comprehensive \nand clear that I felt like \nI was learning from a \n  master teacher.\u201d\n \n\u2014Radhika Kanubaddhi, Amazon\n\u201c\nAn enlightening journey! \nTh is book transformed my \n  questions into answers.\u201d\n\u2014Jose San Leandro, ACM-SL\n\u201c\nExpertly guides through \ncreating agent profi les, using \ntools, memory, planning, \nand multi-agent systems.\n Couldn\u2019t be more timely!\u201d\n\u2014Grigory Sapunov\nauthor of JAX in Action\nISBN-13: 978-1-63343-634-3\nSee first page\n",
      "page_number": 341
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "M A N N I N G\nMicheal Lanham\n",
      "content_length": 29,
      "extraction_method": "Direct"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "The differences between the LLM interactions from direct action \ncompared to using proxy agents, agents, and autonomous agents\nPlease explain the\nde\ufb01nition of agent.\nLarge language model\n(ChatGPT)\nLLM: The de\ufb01nition\nof agent is...\nShow an illustration\nof an agent.\nLarge language model\n(ChatGPT)\n\"An image of a female\nsecret agent of Hispanic\ndescent in a nighttime\nurban setting. . .\nImage generation model\n(DALL-E 3)\nNo agent or assistant\ndirect connection to LLM\nAgent/assistant proxy for\nimage generator\nWhat is the temperature\nin Calgary today?\nLarge language model\n(ChatGPT)\nLLM identi\ufb01es an external\nfunction API to call and\nparameters to connect\nto a weather service.\nAgent/assistant acting on\nbehalf of user\nUser con\ufb01rms\nexecution okay.\nAsks user if it\u2019s okay\nto execute the function\non their behalf.\nExecutes the function\nand returns weather\ninformation.\nFilter my emails by\nimportance and notify\nme of the top 5 most\nimportant emails.\nLarge language model\n(ChatGPT)\nLLM identi\ufb01es an external\nfunction API to call and\nparameters to connect\nto an email service.\nNoti\ufb01es the user of\nimportant emails.\nAutonomous agent making\ndecisions on behalf of user\nLLM reads\nand sorts emails by\nwhat it deems to be\nimportant.\nDecision step\nLLM reformulates\nweather information and\nresponds to the user.\n.\n",
      "content_length": 1301,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "AI Agents in Action\n",
      "content_length": 20,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "AI Agents in Action\nMICHEAL LANHAM\nM A N N I N G\nSHELTER ISLAND\n",
      "content_length": 64,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "For online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n\u00a92025 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning Publications \nwas aware of a trademark claim, the designations have been printed in initial caps or all caps.\nRecognizing the importance of preserving what has been written, it is Manning\u2019s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nThe authors and publisher have made every effort to ensure that the information in this book \nwas correct at press time. The authors and publisher do not assume and hereby disclaim any \nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether \nsuch errors or omissions result from negligence, accident, or any other cause, or from any usage \nof the information herein.\nManning Publications Co.\nDevelopment editor: Becky Whitney\n20 Baldwin Road\nTechnical editor: Ross Turner\nPO Box 761\nReview editor: Kishor Rit\nShelter Island, NY 11964\nProduction editor: Keri Hales\nCopy editor: Julie McNamee\nProofreader: Katie Tennant\nTechnical proofreader: Ross Turner\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781633436343\nPrinted in the United States of America\n",
      "content_length": 2101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": " I dedicate this book to all the readers who embark on this journey with me. \nBooks are a powerful way for an author to connect with readers on a deeply personal \nlevel, chapter by chapter, page by page. In that shared experience of learning, \nexploring, and growing together, I find true meaning. May this book inspire you \nand challenge you, and help you see the incredible potential that AI agents hold\u2014\nnot just for the future but also for today.\n",
      "content_length": 451,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "vivi\nbrief contents\n1\n\u25a0\nIntroduction to agents and their world\n1\n2\n\u25a0\nHarnessing the power of large language models\n14\n3\n\u25a0\nEngaging GPT assistants\n39\n4\n\u25a0\nExploring multi-agent systems\n68\n5\n\u25a0\nEmpowering agents with actions\n98\n6\n\u25a0\nBuilding autonomous assistants\n129\n7\n\u25a0\nAssembling and using an agent platform\n160\n8\n\u25a0\nUnderstanding agent memory and knowledge\n180\n9\n\u25a0\nMastering agent prompts with prompt flow\n212\n10\n\u25a0\nAgent reasoning and evaluation\n244\n11\n\u25a0\nAgent planning and feedback\n272\n",
      "content_length": 485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "vii\ncontents\npreface\nxiii\nacknowledgments\nxv\nabout this book\nxvii\nabout the author\nxxi\nabout the cover illustration\nxxii\n1 Introduction to agents and their world\n1\n1.1\nDefining agents\n1\n1.2\nUnderstanding the component systems of an agent\n4\n1.3\nExamining the rise of the agent era: Why agents?\n9\n1.4\nPeeling back the AI interface\n11\n1.5\nNavigating the agent landscape\n12\n2 Harnessing the power of large language models\n14\n2.1\nMastering the OpenAI API\n16\nConnecting to the chat completions model\n16\n\u25a0Understanding the \nrequest and response\n18\n2.2\nExploring open source LLMs with LM Studio\n20\nInstalling and running LM Studio\n20\n\u25a0Serving an LLM locally \nwith LM Studio\n23\n",
      "content_length": 669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "CONTENTS\nviii\n2.3\nPrompting LLMs with prompt engineering\n25\nCreating detailed queries\n28\n\u25a0Adopting personas\n29\nUsing delimiters\n30\n\u25a0Specifying steps\n31\n\u25a0Providing \nexamples\n32\n\u25a0Specifying output length\n33\n2.4\nChoosing the optimal LLM for your specific needs\n34\n2.5\nExercises\n36\n3 Engaging GPT assistants\n39\n3.1\nExploring GPT assistants through ChatGPT\n40\n3.2\nBuilding a GPT that can do data science\n44\n3.3\nCustomizing a GPT and adding custom actions\n49\nCreating an assistant to build an assistant\n49\n\u25a0Connecting the \ncustom action to an assistant\n53\n3.4\nExtending an assistant\u2019s knowledge using file uploads\n56\nBuilding the Calculus Made Easy GPT\n56\n\u25a0Knowledge search \nand more with file uploads\n58\n3.5\nPublishing your GPT\n61\nExpensive GPT assistants\n62\n\u25a0Understanding the economics of \nGPTs\n63\n\u25a0Releasing the GPT\n63\n3.6\nExercises\n65\n4 Exploring multi-agent systems\n68\n4.1\nIntroducing multi-agent systems with AutoGen Studio\n69\nInstalling and using AutoGen Studio\n70\n\u25a0Adding skills in \nAutoGen Studio\n72\n4.2\nExploring AutoGen\n77\nInstalling and consuming AutoGen\n77\n\u25a0Enhancing code output \nwith agent critics\n79\n\u25a0Understanding the AutoGen cache\n81\n4.3\nGroup chat with agents and AutoGen\n82\n4.4\nBuilding an agent crew with CrewAI\n84\nCreating a jokester crew of CrewAI agents\n84\n\u25a0Observing agents \nworking with AgentOps\n87\n4.5\nRevisiting coding agents with CrewAI\n90\n4.6\nExercises\n95\n",
      "content_length": 1381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "CONTENTS\nix\n5 Empowering agents with actions\n98\n5.1\nDefining agent actions\n99\n5.2\nExecuting OpenAI functions\n101\nAdding functions to LLM API calls\n101\n\u25a0Actioning function \ncalls\n103\n5.3\nIntroducing Semantic Kernel\n107\nGetting started with SK semantic functions\n108\n\u25a0Semantic \nfunctions and context variables\n109\n5.4\nSynergizing semantic and native functions\n111\nCreating and registering a semantic skill/plugin\n111\n\u25a0Applying \nnative functions\n115\n\u25a0Embedding native functions within \nsemantic functions\n117\n5.5\nSemantic Kernel as an interactive service agent\n118\nBuilding a semantic GPT interface\n119\n\u25a0Testing semantic \nservices\n121\n\u25a0Interactive chat with the semantic service \nlayer\n122\n5.6\nThinking semantically when writing semantic \nservices\n125\n5.7\nExercises\n127\n6 Building autonomous assistants\n129\n6.1\nIntroducing behavior trees\n130\nUnderstanding behavior tree execution\n131\n\u25a0Deciding on \nbehavior trees\n132\n\u25a0Running behavior trees with Python \nand py_trees\n134\n6.2\nExploring the GPT Assistants Playground\n136\nInstalling and running the Playground\n136\n\u25a0Using and \nbuilding custom actions\n138\n\u25a0Installing the assistants \ndatabase\n140\n\u25a0Getting an assistant to run code locally\n140\nInvestigating the assistant process through logs\n142\n6.3\nIntroducing agentic behavior trees\n143\nManaging assistants with assistants\n143\n\u25a0Building a coding \nchallenge ABT\n145\n\u25a0Conversational AI systems vs. other \nmethods\n149\n\u25a0Posting YouTube videos to X\n150\nRequired X setup\n151\n6.4\nBuilding conversational autonomous multi-agents\n153\n",
      "content_length": 1519,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "CONTENTS\nx\n6.5\nBuilding ABTs with back chaining\n155\n6.6\nExercises\n156\n7 Assembling and using an agent platform\n160\n7.1\nIntroducing Nexus, not just another agent platform\n161\nRunning Nexus\n162\n\u25a0Developing Nexus\n163\n7.2\nIntroducing Streamlit for chat application \ndevelopment\n165\nBuilding a Streamlit chat application\n165\n\u25a0Creating a streaming \nchat application\n168\n7.3\nDeveloping profiles and personas for agents\n170\n7.4\nPowering the agent and understanding the agent \nengine\n172\n7.5\nGiving an agent actions and tools\n174\n7.6\nExercises\n178\n8 Understanding agent memory and knowledge\n180\n8.1\nUnderstanding retrieval in AI applications\n181\n8.2\nThe basics of retrieval augmented generation (RAG)\n182\n8.3\nDelving into semantic search and document \nindexing\n184\nApplying vector similarity search\n184\n\u25a0Vector databases and \nsimilarity search\n188\n\u25a0Demystifying document embeddings\n189\nQuerying document embeddings from Chroma\n190\n8.4\nConstructing RAG with LangChain\n192\nSplitting and loading documents with LangChain\n192\n\u25a0Splitting \ndocuments by token with LangChain\n195\n8.5\nApplying RAG to building agent knowledge\n196\n8.6\nImplementing memory in agentic systems\n200\nConsuming memory stores in Nexus\n202\n\u25a0Semantic memory and \napplications to semantic, episodic, and procedural memory\n204\n8.7\nUnderstanding memory and knowledge \ncompression\n207\n8.8\nExercises\n209\n",
      "content_length": 1354,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "CONTENTS\nxi\n9 Mastering agent prompts with prompt flow\n212\n9.1\nWhy we need systematic prompt engineering\n213\n9.2\nUnderstanding agent profiles and personas\n216\n9.3\nSetting up your first prompt flow\n217\nGetting started\n218\n\u25a0Creating profiles with Jinja2 \ntemplates\n222\n\u25a0Deploying a prompt flow API\n223\n9.4\nEvaluating profiles: Rubrics and grounding\n224\n9.5\nUnderstanding rubrics and grounding\n228\n9.6\nGrounding evaluation with an LLM profile\n230\n9.7\nComparing profiles: Getting the perfect \nprofile\n232\nParsing the LLM evaluation output\n232\n\u25a0Running batch \nprocessing in prompt flow\n235\n\u25a0Creating an evaluation flow \nfor grounding\n238\n\u25a0Exercises\n242\n10 Agent reasoning and evaluation\n244\n10.1\nUnderstanding direct solution prompting\n245\nQuestion-and-answer prompting\n246\n\u25a0Implementing few-shot \nprompting\n248\n\u25a0Extracting generalities with zero-shot \nprompting\n250\n10.2\nReasoning in prompt engineering\n252\nChain of thought prompting\n253\n\u25a0Zero-shot CoT \nprompting\n257\n\u25a0Step by step with prompt chaining\n258\n10.3\nEmploying evaluation for consistent solutions\n261\nEvaluating self-consistency prompting\n262\n\u25a0Evaluating tree of \nthought prompting\n266\n10.4\nExercises\n270\n11 Agent planning and feedback\n272\n11.1\nPlanning: The essential tool for all agents/assistants\n273\n11.2\nUnderstanding the sequential planning process\n277\n11.3\nBuilding a sequential planner\n278\n11.4\nReviewing a stepwise planner: OpenAI Strawberry\n285\n",
      "content_length": 1412,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "CONTENTS\nxii\n11.5\nApplying planning, reasoning, evaluation, and feedback to \nassistant and agentic systems\n288\nApplication of assistant/agentic planning\n288\n\u25a0Application of \nassistant/agentic reasoning\n290\n\u25a0Application of evaluation to \nagentic systems\n291\n\u25a0Application of feedback to agentic/assistant \napplications\n293\n11.6\nExercises\n296\nappendix A\nAccessing OpenAI large language models\n299\nappendix B\nPython development environment\n305\nindex\n311\n",
      "content_length": 450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "xiii\npreface\nMy journey into the world of intelligent systems began back in the early 1980s. Like\nmany people then, I believed artificial intelligence (AI) was just around the corner. It\nalways seemed like one more innovation and technological leap would lead us to the\nintelligence we imagined. But that leap never came.\n Perhaps the promise of HAL, from Stanley Kubrick\u2019s 2001: A Space Odyssey, capti-\nvated me with the idea of a truly intelligent computer companion. After years of effort,\ntrial, and countless errors, I began to understand that creating AI was far more com-\nplex than we humans had imagined. In the early 1990s, I shifted my focus, applying\nmy skills to more tangible goals in other industries.\n Not until the late 1990s, after experiencing a series of challenging and transforma-\ntive events, did I realize my passion for building intelligent systems. I knew these sys-\ntems might never reach the superintelligence of HAL, but I was okay with that. I\nfound fulfillment in working with machine learning and data science, creating models\nthat could learn and adapt. For more than 20 years, I thrived in this space, tackling\nproblems that required creativity, precision, and a sense of possibility.\n During that time, I worked on everything from genetic algorithms for predicting\nunknown inputs to developing generative learning models for horizontal drilling in\nthe oil-and-gas sector. These experiences led me to write, where I shared my knowl-\nedge by way of books on various topics\u2014reverse-engineering Pok\u00e9mon Go, building\naugmented and virtual reality experiences, designing audio for games, and applying\nreinforcement learning to create intelligent agents. I spent years knuckles-deep in\ncode, developing agents in Unity ML-Agents and deep reinforcement learning.\n",
      "content_length": 1789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "PREFACE\nxiv\n Even then, I never imagined that one day I could simply describe what I wanted to\nan AI model, and it would make it happen. I never imagined that, in my lifetime, I\nwould be able to collaborate with an AI as naturally as I do today. And I certainly never\nimagined how fast\u2014and simultaneously how slow\u2014this journey would feel.\n In November 2022, the release of ChatGPT changed everything. It changed the\nworld\u2019s perception of AI, and it changed the way we build intelligent systems. For me,\nit also altered my perspective on the capabilities of these systems. Suddenly, the idea\nof agents that could autonomously perform complex tasks wasn\u2019t just a far-off dream\nbut instead a tangible, achievable reality. In some of my earlier books, I had described\nagentic systems that could undertake specific tasks, but now, those once-theoretical\nideas were within reach.\n This book is the culmination of my decades of experience in building intelligent\nsystems, but it\u2019s also a realization of the dreams I once had about what AI could\nbecome. AI agents are here, poised to transform how we interact with technology, how\nwe work, and, ultimately, how we live.\n Yet, even now, I see hesitation from organizations when it comes to adopting agen-\ntic systems. I believe this hesitation stems not from fear of AI but rather from a lack\nof understanding and expertise in building these systems. I hope that this book\nhelps to bridge that gap. I want to introduce AI agents as tools that can be accessible\nto everyone\u2014tools we shouldn\u2019t fear but instead respect, manage responsibly, and\nlearn to work with in harmony.\n",
      "content_length": 1614,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "xv\nacknowledgments\nI want to extend my deepest gratitude to the machine learning and deep learning\ncommunities for their tireless dedication and incredible work. Just a few short years\nago, many questioned whether the field was headed for another AI winter\u2014a period\nof stagnation and doubt. But thanks to the persistence, brilliance, and passion of\ncountless individuals, the field not only persevered but also flourished. We\u2019re standing\non the threshold of an AI-driven future, and I am endlessly grateful for the contribu-\ntions of this talented community.\n Writing a book, even with the help of AI, is no small feat. It takes dedication, col-\nlaboration, and a tremendous amount of support. I am incredibly thankful to the\nteam of editors and reviewers who made this book possible. I want to express my\nheartfelt thanks to everyone who took the time to review and provide feedback. In\nparticular, I want to thank Becky Whitney, my content editor, and Ross Turner, my\ntechnical editor and chief production and technology officer at OpenSC, for their\ndedication, as well as the whole production team at Manning for their insight and\nunwavering support throughout this journey.\n To my partner, Rhonda\u2014your love, patience, and encouragement mean the world\nto me. You\u2019ve been the cornerstone of my support system, not just for this book but\nfor all the books that have come before. I truly couldn\u2019t have done any of this without\nyou. Thank you for being my rock, my partner, and my inspiration.\n Many of the early ideas for this book grew out of my work at Symend. It was during\nmy time there that I first began developing the concepts and designs for agentic sys-\ntems that laid the foundation for this book. I am deeply grateful to my colleagues at\n",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "ACKNOWLEDGMENTS\nxvi\nSymend for their collaboration and contributions, including Peh Teh, Andrew Wright,\nZiko Rajabali, Chris Garrett, Kouros, Fatemeh Torabi Asr, Sukh Singh, and Hanif\nJoshaghani. Your insights and hard work helped bring these ideas to life, and I am\nhonored to have worked alongside such an incredible group of people.\n Finally, I would like to thank all the reviewers: Anandaganesh Balakrishnan, Aryan\nJadon, Chau Giang, Dan Sheikh, David Curran, Dibyendu Roy Chowdhury, Divya\nBhargavi, Felipe Provezano Coutinho, Gary Pass, John Williams, Jose San Leandro,\nLaurence Giglio, Manish Jain, Maxim Volgin, Michael Wang, Mike Metzger, Piti\nChampeethong, Prashant Dwivedi, Radhika Kanubaddhi, Rajat Kant Goel, Ramaa\nVissa, Richard Vaughan, Satej Kumar Sahu, Sergio Gtz, Siva Dhandapani, Annamaneni\nSriharsha, Sri Ram Macharla, Sumit Bhattacharyya, Tony Holdroyd, Vidal Graupera,\nVidhya Vinay, and Vinoth Nageshwaran. Your suggestions helped make this a better\nbook.\n",
      "content_length": 978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "xvii\nabout this book\nAI Agents in Action is about building and working with intelligent agent systems\u2014not\njust creating autonomous entities but also developing agents that can effectively tackle\nand solve real-world problems. The book starts with the basics of working with large\nlanguage models (LLMs) to build assistants, multi-agent systems, and agentic behav-\nioral agents. From there, it explores the key components of agentic systems: retrieval\nsystems for knowledge and memory augmentation, action and tool usage, reasoning,\nplanning, evaluation, and feedback. The book demonstrates how these components\nempower agents to perform a wide range of complex tasks through practical examples.\n This journey isn\u2019t just about technology; it\u2019s about reimagining how we approach\nproblem solving. I hope this book inspires you to see intelligent agents as partners in\ninnovation, capable of transforming ideas into actions in ways that were once thought\nimpossible. Together, we\u2019ll explore how AI can augment human potential, enabling us\nto achieve far more than we could alone.\nWho should read this book\nThis book is for anyone curious about intelligent agents and how to develop agentic\nsystems\u2014whether you\u2019re building your first helpful assistant or diving deeper into\ncomplex multi-agent systems. No prior experience with agents, agentic systems,\nprompt engineering, or working with LLMs is required. All you need is a basic under-\nstanding of Python and familiarity with GitHub repositories. My goal is to make these\nconcepts accessible and engaging, empowering anyone who wants to explore the\nworld of AI agents to do so with confidence.\n",
      "content_length": 1641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "ABOUT THIS BOOK\nxviii\n Whether you\u2019re a developer, researcher, or hobbyist or are simply intrigued by the\npossibilities of AI, this book is for you. I hope that in these pages you\u2019ll find inspira-\ntion, practical guidance, and a new appreciation for the remarkable potential of intel-\nligent agents. Let this book guide understanding, creating, and unleashing the power\nof AI agents in action.\nHow this book is organized: A road map\nThis book has 11 chapters. Chapter 1, \u201cIntroduction to agents and their world,\u201d\nbegins by laying a foundation with fundamental definitions of large language models,\nchat systems, assistants, and autonomous agents. As the book progresses, the discus-\nsion shifts to the key components that make up an agent and how these components\nwork together to create truly effective systems. Here is a quick summary of chapters 2\nthrough 11:\n\uf0a1Chapter 2, \u201cHarnessing the power of large language models\u201d\u2014We start by exploring\nhow to use commercial LLMs, such as OpenAI. We then examine tools, such as\nLM Studio, that provide the infrastructure and support for running various\nopen source LLMs, enabling anyone to experiment and innovate.\n\uf0a1Chapter 3, \u201cEngaging GPT assistants\u201d\u2014This chapter dives into the capabilities of\nthe GPT Assistants platform from OpenAI. Assistants are foundational agent\ntypes, and we explore how to create practical and diverse assistants, from culi-\nnary helpers to intern data scientists and even a book learning assistant.\n\uf0a1Chapter 4, \u201cExploring multi-agent systems\u201d\u2014Agentic tools have advanced signifi-\ncantly quickly. Here, we explore two sophisticated multi-agent systems: CrewAI\nand AutoGen. We demonstrate AutoGen\u2019s ability to develop code autono-\nmously and see how CrewAI can bring together a group of joke researchers to\ncreate humor collaboratively.\n\uf0a1Chapter 5, \u201cEmpowering agents with actions\u201d\u2014Actions are fundamental to any\nagentic system. This chapter discusses how agents can use tools and functions to\nexecute actions, ranging from database and application programming interface\n(API) queries to generating images. We focus on enabling agents to take mean-\ningful actions autonomously.\n\uf0a1Chapter 6, \u201cBuilding autonomous assistants\u201d\u2014We explore the behavior tree\u2014a sta-\nple in robotics and game systems\u2014as a mechanism to orchestrate multiple\ncoordinated agents. We\u2019ll use behavior trees to tackle challenges such as code\ncompetitions and social media content creation.\n\uf0a1Chapter 7, \u201cAssembling and using an agent platform\u201d\u2014This chapter introduces Nexus,\na sophisticated platform for orchestrating multiple agents and LLMs. We discuss\nhow Nexus facilitates agentic workflows and enables complex interactions between\nagents, providing an example of a fully functioning multi-agent environment.\n\uf0a1Chapter 8, \u201cUnderstanding agent memory and knowledge\u201d\u2014Retrieval-augmented\ngeneration (RAG) has become an essential tool for extending the capabilities\n",
      "content_length": 2901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "ABOUT THIS BOOK\nxix\nof LLM agents. This chapter explores how retrieval mechanisms can serve as\nboth a source of knowledge by processing ingested files, and of memory, allow-\ning agents to recall previous interactions or events.\n\uf0a1Chapter 9, \u201cMastering agent prompts with prompt flow\u201d\u2014Prompt engineering is cen-\ntral to an agent\u2019s success. This chapter introduces prompt flow, a tool from Mic-\nrosoft that helps automate the testing and evaluation of prompts, enabling\nmore robust and effective agentic behavior.\n\uf0a1Chapter 10, \u201cAgent reasoning and evaluation\u201d\u2014Reasoning is crucial to solving\nproblems intelligently. In this chapter, we explore various reasoning techniques,\nsuch as chain of thought (CoT), and show how agents can evaluate reasoning\nstrategies even during inference, improving their capacity to solve problems\nautonomously.\n\uf0a1Chapter 11, \u201cAgent planning and feedback\u201d\u2014Planning is perhaps an agent\u2019s most crit-\nical skill in achieving its goals. We discuss how agents can incorporate planning to\nnavigate complex tasks and how feedback loops can be used to refine those plans.\nThe chapter concludes by integrating all the key components\u2014actions, memory\nand knowledge, reasoning, evaluation, planning, and feedback\u2014into practical\nexamples of agentic systems that solve real-world problems.\nAbout the code\nThe code for this book is spread across several open source projects, many of which\nare hosted by me or by other organizations in GitHub repositories. Throughout this\nbook, I strive to make the content as accessible as possible, taking a low-code approach\nto help you focus on core concepts. Many chapters demonstrate how simple prompts\ncan generate meaningful code, showcasing the power of AI-assisted development.\n Additionally, you\u2019ll find a variety of assistant profiles and multi-agent systems that\ndemonstrate how to solve real-world problems using generated code. These examples\nare meant to inspire, guide, and empower you to explore what is possible with AI\nagents. I am deeply grateful to the many contributors and the community members\nwho have collaborated on these projects, and I encourage you to explore the reposito-\nries, experiment with the code, and adapt it to your own needs. This book is a testa-\nment to the power of collaboration and the incredible things we can achieve together.\n This book contains many examples of source code both in numbered listings and in\nline with normal text. In both cases, source code is formatted in a fixed-width font\nlike this to separate it from ordinary text. Sometimes, some of the code is typeset in\nbold to highlight code that has changed from previous steps in the chapter, such as\nwhen a feature is added to an existing line of code. In many cases, the original source\ncode has been reformatted; we\u2019ve added line breaks and reworked indentation to\naccommodate the available page space in the book. In some cases, even this wasn\u2019t\nenough, and listings include line-continuation markers (\u27a5). Additionally, comments in\nthe source code have often been removed from the listings when the code is described\n",
      "content_length": 3078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "ABOUT THIS BOOK\nxx\nin the text. Code annotations accompany many of the listings, highlighting important\nconcepts.\n You can get executable snippets of code from the liveBook (online) version of this\nbook at https://livebook.manning.com/book/ai-agents-in-action. The complete code\nfor the examples in the book is available for download from the Manning website at\nwww.manning.com/books/ai-agents-in-action. In addition, the code developed for this\nbook has been placed in three GitHub repositories that are all publicly accessible:\n\uf0a1GPT-Agents (the original book title), at https://github.com/cxbxmxcx/GPT-\nAgents, holds the code for several examples demonstrated in the chapters.\n\uf0a1GPT Assistants Playground, at https://github.com/cxbxmxcx/GPTAssistants\nPlayground, is an entire platform and tool dedicated to building OpenAI GPT\nassistants with a helpful web user interface and plenty of tools to develop auton-\nomous agent systems.\n\uf0a1Nexus, at https://github.com/cxbxmxcx/Nexus, is an example of a web-based\nagentic tool that can help you create agentic systems and demonstrate various\ncode challenges.\nliveBook discussion forum\nPurchase of AI Agents in Action includes free access to liveBook, Manning\u2019s online\nreading platform. Using liveBook\u2019s exclusive discussion features, you can attach com-\nments to the book globally or to specific sections or paragraphs. It\u2019s a snap to make\nnotes for yourself, ask and answer technical questions, and receive help from the\nauthor and other users. To access the forum, go to https://livebook.manning.com/\nbook/ai-agents-in-action/discussion. You can also learn more about Manning\u2019s forums\nand the rules of conduct at https://livebook.manning.com/discussion.\n Manning\u2019s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It isn\u2019t a commitment to any specific amount of participation on the part of the\nauthor, whose contribution to the forum remains voluntary (and unpaid). We suggest\nyou try asking the him challenging questions lest his interest stray! The forum and the\narchives of previous discussions will be accessible from the publisher\u2019s website as long\nas the book is in print.\n",
      "content_length": 2230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "xxi\nabout the author\nMICHEAL LANHAM is a distinguished software and technology\ninnovator with more than two decades of experience in the\nindustry. He has an extensive background in developing various\nsoftware applications across several domains, such as gaming,\ngraphics, web development, desktop engineering, AI, GIS, oil\nand gas geoscience/geomechanics, and machine learning.\nMicheal began by pioneering work in integrating neural net-\nworks and evolutionary algorithms into game development,\nwhich began around the turn of the millennium. He has\nauthored multiple influential books exploring deep learning, game development,\nand augmented reality, including Evolutionary Deep Learning (Manning, 2023) and\nAugmented Reality Game Development (Packt Publishing, 2017). He has contributed to\nthe tech community via publications with many significant tech publishers, including\nManning. Micheal resides in Calgary, Alberta, Canada, with his large family, whom he\nenjoys cooking for.\n",
      "content_length": 981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "xxii\nabout the cover illustration\nThe figure on the cover of AI Agents in Action is \u201cCl\u00e9mentinien,\u201d taken from Balthasar\nHacquet\u2019s Illustrations de L\u2019Illyrie et la Dalmatie, published in 1815.\n In those days, it was easy to identify where people lived and what their trade or sta-\ntion in life was just by their dress. Manning celebrates the inventiveness and initiative\nof the computer business with book covers based on the rich diversity of regional cul-\nture centuries ago, brought back to life by pictures from collections such as this one.\n",
      "content_length": 546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "1\nIntroduction to agents\nand their world\nThe agent isn\u2019t a new concept in machine learning and artificial intelligence (AI).\nIn reinforcement learning, for instance, the word agent denotes an active decision-\nmaking and learning intelligence. In other areas, the word agent aligns more with\nan automated application or software that does something on your behalf.\n1.1\nDefining agents\nYou can consult any online dictionary to find the definition of an agent. The Mer-\nriam-Webster Dictionary defines it this way (www.merriam-webster.com/dictionary/\nagent):\nThis chapter covers\n\uf0a1Defining the concept of agents\n\uf0a1Differentiating the components of an \nagent\n\uf0a1Analyzing the rise of the agent era: \nWhy agents?\n\uf0a1Peeling back the AI interface\n\uf0a1Navigating the agent landscape\n",
      "content_length": 767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "2\nCHAPTER 1\nIntroduction to agents and their world\n\uf0a1One that acts or exerts power\n\uf0a1Something that produces or can produce an effect\n\uf0a1A means or instrument by which a guiding intelligence achieves a result\nThe word agent in our journey to build powerful agents in this book uses this dictio-\nnary definition. That also means the term assistant will be synonymous with agent.\nTools like OpenAI\u2019s GPT Assistants will also fall under the AI agent blanket. OpenAI\navoids the word agent because of the history of machine learning, where an agent is\nself-deciding and autonomous.\n Figure 1.1 shows four cases where a user may interact with a large language model\n(LLM) directly or through an agent/assistant proxy, an agent/assistant, or an autono-\nmous agent. These four use cases are highlighted in more detail in this list:\n\uf0a1Direct user interaction\u2014If you used earlier versions of ChatGPT, you experienced\ndirect interaction with the LLM. There is no proxy agent or other assistant\ninterjecting on your behalf.\n\uf0a1Agent/assistant proxy\u2014If you\u2019ve used Dall-E 3 through ChatGPT, then you\u2019ve expe-\nrienced a proxy agent interaction. In this use case, an LLM interjects your\nrequests and reformulates them in a format better designed for the task. For\nexample, for image generation, ChatGPT better formulates the prompt. A proxy\nagent is an everyday use case to assist users with unfamiliar tasks or models.\n\uf0a1Agent/assistant\u2014If you\u2019ve ever used a ChatGPT plugin or GPT assistant, then\nyou\u2019ve experienced this use case. In this case, the LLM is aware of the plugin\nor assistant functions and prepares to make calls to this plugin/function.\nHowever, before making a call, the LLM requires user approval. If approved,\nthe plugin or function is executed, and the results are returned to the LLM.\nThe LLM then wraps this response in natural language and returns it to\nthe user.\n\uf0a1Autonomous agent\u2014In this use case, the agent interprets the user\u2019s request, con-\nstructs a plan, and identifies decision points. From this, it executes the steps in\nthe plan and makes the required decisions independently. The agent may\nrequest user feedback after certain milestone tasks, but it\u2019s often given free rein\nto explore and learn if possible. This agent poses the most ethical and safety\nconcerns, which we\u2019ll explore later. \nFigure 1.1 demonstrates the use cases for a single flow of actions on an LLM using a\nsingle agent. For more complex problems, we often break agents into profiles or per-\nsonas. Each agent profile is given a specific task and executes that task with specialized\ntools and knowledge.\n Multi-agent systems are agent profiles that work together in various configurations to\nsolve a problem. Figure 1.2 demonstrates an example of a multi-agent system using\nthree agents: a controller or proxy and two profile agents as workers controlled by the\nproxy. The coder profile on the left writes the code the user requests; on the right is a\n",
      "content_length": 2931,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "3\n1.1\nDefining agents\ntester profile designed to write unit tests. These agents work and communicate\ntogether until they are happy with the code and then pass it on to the user.\n Figure 1.2 shows one of the possibly infinite agent configurations. (In chapter 4,\nwe\u2019ll explore Microsoft\u2019s open source platform, AutoGen, which supports multiple\nconfigurations for employing multi-agent systems.)\n Multi-agent systems can work autonomously but may also function guided entirely\nby human feedback. The benefits of using multiple agents are like those of a single\nPlease explain the\nde\ufb01nition of agent.\nLarge language model\n(ChatGPT)\nLLM: The de\ufb01nition\nof agent is...\nShow an illustration\nof an agent.\nLarge language model\n(ChatGPT)\n\"An image of a female\nsecret agent of Hispanic\ndescent in a nighttime\nurban setting. . .\nImage generation model\n(DALL-E 3)\nNo agent or assistant\ndirect connection to LLM\nAgent/assistant proxy for\nimage generator\nWhat is the temperature\nin Calgary today?\nLarge language model\n(ChatGPT)\nLLM identi\ufb01es an external\nfunction API to call and\nparameters to connect\nto a weather service.\nAgent/assistant acting on\nbehalf of user\nUser con\ufb01rms\nexecution okay.\nAsks user if it\u2019s okay\nto execute the function\non their behalf.\nExecutes the function\nand returns weather\ninformation.\nFilter my emails by\nimportance and notify\nme of the top 5 most\nimportant emails.\nLarge language model\n(ChatGPT)\nLLM identi\ufb01es an external\nfunction API to call and\nparameters to connect\nto an email service.\nNoti\ufb01es the user of\nimportant emails.\nAutonomous agent making\ndecisions on behalf of user\nLLM reads\nand sorts emails by\nwhat it deems to be\nimportant.\nDecision step\nLLM reformulates\nweather information and\nresponds to the user.\n.\nFigure 1.1\nThe differences between the LLM interactions from direct action compared to using proxy agents, \nagents, and autonomous agents\n",
      "content_length": 1871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "4\nCHAPTER 1\nIntroduction to agents and their world\nagent but often magnified. Where a single agent typically specializes in a single task,\nmulti-agent systems can tackle multiple tasks in parallel. Multiple agents can also pro-\nvide feedback and evaluation, reducing errors when completing assignments.\n As we can see, an AI agent or agent system can be assembled in multiple ways.\nHowever, an agent itself can also be assembled using multiple components. In the\nnext section, we\u2019ll cover topics ranging from an agent\u2019s profile to the actions it may\nperform, as well as memory and planning.\n1.2\nUnderstanding the component systems of an agent\nAgents can be complex units composed of multiple component systems. These com-\nponents are the tools the agent employs to help it complete its goal or assigned tasks\nand even create new ones. Components may be simple or complex systems, typically\nsplit into five categories.\n Figure 1.3 describes the major categories of components a single-agent system may\nincorporate. Each element will have subtypes that can define the component\u2019s type,\nUser query\nAnswer\nController\nCoder\nTester\nFeature request\nCode\nCode\nUnit tests\nLarge Language Model\nLarge Language Model\nThe controller agent\ncan execute code on\nthe user\u2019s behalf.\nRepeat until the\ncode and tests\nwork as expected.\nWorker agents\ninteract with\nthe LLM.\nFigure 1.2\nIn this example of a multi-agent system, the controller or agent proxy communicates directly with \nthe user. Two agents\u2014a coder and a tester\u2014work in the background to create code and write unit tests to test \nthe code.\n",
      "content_length": 1582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "5\n1.2\nUnderstanding the component systems of an agent\nstructure, and use. At the core of all agents is the profile and persona; extending from\nthat are the systems and functions that enhance the agent.\nThe agent profile and persona shown in figure 1.4 represent the base description of\nthe agent. The persona\u2014often called the system prompt\u2014guides an agent to complete\ntasks, learn how to respond, and other nuances. It includes elements such as the back-\nground (e.g., coder, writer) and demographics, and it can be generated through\nmethods such as handcrafting, LLM assistance, or data-driven techniques, including\nevolutionary algorithms.\n We\u2019ll explore how to create effective and specific agent profiles/personas through\ntechniques such as rubrics and grounding. In addition, we\u2019ll explain the aspects of\nhuman-formulated versus AI-formulated (LLM) profiles, including innovative tech-\nniques using data and evolutionary algorithms to build profiles.\nA persona represents the agent\u2019s\nmain role or function, typically\nde\ufb01ned in a system prompt. The\npro\ufb01le describes the entire agent\nsystem.\nPro\ufb01le and Persona\nActions and Tool Use\nReasoning and Evaluation\nMemory and Knowledge\nPlanning and Feedback\nActions represent a function/\ntool an agent can use.\nReasoning and evaluation\nground the agent and empower\nit to make better decisions.\nPlanning and feedback\nallow the agent to learn and\nimprove on task completion.\nMemory and knowledge provide\nadded context to the agent for a\nspeci\ufb01c request or task.\nFigure 1.3\nThe five main components of a single-agent system (image generated through DALL-E 3)\n",
      "content_length": 1601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "6\nCHAPTER 1\nIntroduction to agents and their world\nNOTE\nThe agent or assistant profile is composed of elements, including the\npersona. It may be helpful to think of profiles describing the work the agent/\nassistant will perform and the tools it needs.\nFigure 1.5 demonstrates the component actions and tool use in the context of agents\ninvolving activities directed toward task completion or acquiring information. These\nactions can be categorized into task completion, exploration, and communication,\nwith varying levels of effect on the agent\u2019s environment and internal states. Actions\ncan be generated manually, through memory recollection, or by following predefined\nplans, influencing the agent\u2019s behavior and enhancing learning.\n Understanding the action target helps us define clear objectives for task comple-\ntion, exploration, or communication. Recognizing the action effect reveals how actions\ninfluence task outcomes, the agent\u2019s environment, and its internal states, contributing\nto efficient decision making. Lastly, grasping action generation methods equips us\nwith the knowledge to create actions manually, recall them from memory, or follow\npredefined plans, enhancing our ability to effectively shape agent behavior and learn-\ning processes.\n Figure 1.6 shows the component knowledge and memory in more detail. Agents\nuse knowledge and memory to annotate context with the most pertinent information\nwhile limiting the number of tokens used. Knowledge and memory structures can be\nunified, where both subsets follow a single structure or hybrid structure involving a\nmix of different retrieval forms. Knowledge and memory formats can vary widely from\nAgent persona: We\u2019ll understand how\nto clearly de\ufb01ne the persona, specifying\ntheir role and characteristics to guide\nthe agent effectively.\nAgent role and demographics: We\u2019ll\nsee how relevant demographic and role\ndetails can provide agent context, such\nas age, gender, or background, for a\nmore relevant interaction.\nHuman vs. AI assistance for persona\ngeneration: We\u2019ll highlight the role\nof human involvement in persona\ngeneration, whether it\u2019s entirely\nhuman driven or assisted by LLMs\nor other agents.\nInnovative persona techniques:\nPrompts generated through data\nor other novel approaches such as\nevolutionary algorithms to enhance\nagent capabilities.\nPro\ufb01le and Persona\nPro\ufb01le Contents\nPro\ufb01le Generation\nPersona: Role, i.e., coder or tester\nDemographics: Sex, age, background\nHandcrafted: Manually designed by\nhumans\nLLM generated: Directed by human\nprompts\nData generated: Constructed from\ndata personas\nFigure 1.4\nAn in-depth look at how we\u2019ll explore creating agent profiles\n",
      "content_length": 2652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "7\n1.2\nUnderstanding the component systems of an agent\nlanguage (e.g., PDF documents) to databases (relational, object, or document) and\nembeddings, simplifying semantic similarity search through vector representations or\neven simple lists serving as agent memories.\nFigure 1.7 shows the reasoning and evaluation component of an agent system. Research\nand practical applications have shown that LLMs/agents can effectively reason. Rea-\nsoning and evaluation systems annotate an agent\u2019s workflow by providing an ability to\nthink through problems and evaluate solutions.\nAction targets: We\u2019ll learn the importance\nof de\ufb01ning action targets, whether for task\ncompletion, exploration, or communication,\nto clarify the agent\u2019s objectives.\nAction space and impact: We\u2019ll learn the\nsigni\ufb01cance of understanding how actions\naffect task completion and their effect on\nthe agent\u2019s environment, internal states,\nand self-knowledge.\nAction generation methods: We\u2019ll see the\nvarious ways actions can be generated, such\nas manual creation, memory recollection,\nor plan following, to illustrate the diversity\nof agent behaviors.\nAction and Tool Use\nAction Target\nSemantic or native functions\nTools, self-knowledge, other agents\nEnvironments, new actions, internal\nstates, other agents\nManual, memory recollection, plan\nfollowing\nFigure 1.5\nThe aspects of agent actions we\u2019ll explore in this book\nMemory and Knowledge\nRetrieval Structure\n\u2022 Uni\ufb01ed\n\u2022 Hybrid\nRetrieval Formats\n\u2022 Language\n\u2022 Databases\n\u2022 Embeddings\n\u2022 Lists\nRetrieval Operation\n\u2022 Augmentation\n\u2022 Semantic Extraction\n\u2022 Compression\nRetrieval structure variety: We\u2019ll learn\nabout the diverse memory structures\nagents can employ, including uni\ufb01ed and\nhybrid approaches, enabling \ufb02exibility in\ninformation storage.\nRetrieval formats: We\u2019ll explore the\nvarious data sources for memory, such\nas language (e.g., PDF documents),\ndatabases (relational, object, or\ndocument), and embeddings, offering a\nrich pool of information to draw upon.\nSemantic similarity: We\u2019ll learn how\nembeddings enable semantic similarity\nsearches, facilitating ef\ufb01cient retrieval of\nrelevant data and enhancing the agent\u2019s\ndecision-making capabilities.\nFigure 1.6\nExploring the role and use of agent memory and knowledge\n",
      "content_length": 2231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "8\nCHAPTER 1\nIntroduction to agents and their world\nFigure 1.8 shows the component agent planning/feedback and its role in organizing\ntasks to achieve higher-level goals. It can be categorized into these two approaches:\n\uf0a1Planning without feedback\u2014Autonomous agents make decisions independently.\n\uf0a1Planning with feedback\u2014Monitoring and modifying plans is based on various\nsources of input, including environmental changes and direct human feedback.\nWithin planning, agents may employ single-path reasoning, sequential reasoning through\neach step of a task, or multipath reasoning to explore multiple strategies and save the\nReasoning and Evaluation\nReasoning\n\u2022 Zero-shot prompting\n\u2022 One-shot prompting\n\u2022 Few-shot prompting\n\u2022 Chain of thought\n\u2022 Tree of thought\n\u2022 Skeleton of thought\nEvaluation\n\u2022 Self-consistency\n\u2022 Prompt chaining\nReasoning enables the agent\nto self-re\ufb02ect and internally\nreason out the completion\nof a task or tasks.\nEvaluation provides the basis\nfor an agent\u2019s self-re\ufb02ection\non working through and upon\ntask completion.\nFigure 1.7\nThe reasoning and evaluation component and details\nWe\u2019ll look at various planning\nstrategies with and without\nfeedback\u2014from basic and\nsequential planners to automatic\ntool use with reasoning.\nFeedback may come from a variety\nof sources, such as environmental,\nhuman, and an LLM via various\nconstructive feedback patterns.\nPlanning and Feedback\nPlanning without feedback\n(autonomous)\n\u2022 Basic planning\n\u2022 Automatic reasoning with\ntool use\n\u2022 Sequential planning\nPlanning with feedback\n\u2022 Environmental feedback\n\u2022 Human feedback\n\u2022 LLM feedback\n\u2022 Adaptive constructive\nfeedback\nFigure 1.8\nExploring the role of agent planning and reasoning\n",
      "content_length": 1680,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "9\n1.3\nExamining the rise of the agent era: Why agents?\nefficient ones for future use. External planners, which can be code or other agent sys-\ntems, may also play a role in orchestrating plans.\n Any of our previous agent types\u2014the proxy agent/assistant, agent/assistant, or\nautonomous agent\u2014may use some or all of these components. Even the planning\ncomponent has a role outside of the autonomous agent and can effectively empower\neven the regular agent.\n1.3\nExamining the rise of the agent era: Why agents?\nAI agents and assistants have quickly moved from the main commodity in AI research\nto mainstream software development. An ever-growing list of tools and platforms assist\nin the construction and empowerment of agents. To an outsider, it may all seem like\nhype intended to inflate the value of some cool but overrated technology.\n During the first few months after ChatGPT\u2019s initial release, a new discipline called\nprompt engineering was formed: users found that using various techniques and patterns\nin their prompts allowed them to generate better and more consistent output. How-\never, users also realized that prompt engineering could only go so far.\n Prompt engineering is still an excellent way to interact directly with LLMs such as\nChatGPT. Over time, many users discovered that effective prompting required iteration,\nreflection, and more iteration. The first agent systems, such as AutoGPT, emerged from\nthese discoveries, capturing the community\u2019s attention.\n Figure 1.9 shows the original design of AutoGPT, one of the first autonomous\nagent systems. The agent is designed to iterate a planned sequence of tasks that it\ndefines by looking at the user\u2019s goal. Through each task iteration of steps, the agent\nevaluates the goal and determines if the task is complete. If the task isn\u2019t complete, the\nagent may replan the steps and update the plan based on new knowledge or human\nfeedback.\n AutoGPT became the first example to demonstrate the power of using task plan-\nning and iteration with LLM models. From this and in tandem, other agent systems\nand frameworks exploded into the community using similar planning and task itera-\ntion systems. It\u2019s generally accepted that planning, iteration, and repetition are the\nbest processes for solving complex and multifaceted goals for an LLM.\n However, autonomous agent systems require trust in the agent decision-making\nprocess, the guardrails/evaluation system, and the goal definition. Trust is also some-\nthing that is acquired over time. Our lack of trust stems from our lack of understand-\ning of an autonomous agent\u2019s capabilities.\nNOTE\nArtificial general intelligence (AGI) is a form of intelligence that\ncan learn to accomplish any task a human can. Many practitioners in this\nnew world of AI believe an AGI using autonomous agent systems is an\nattainable goal.\nFor this reason, many of the mainstream and production-ready agent tools aren\u2019t auton-\nomous. However, they still provide a significant benefit in managing and automating\n",
      "content_length": 3004,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "10\nCHAPTER 1\nIntroduction to agents and their world\ntasks using GPTs (LLMs). Therefore, as our goal in this book is to understand all agent\nforms, many more practical applications will be driven by non-autonomous agents.\n Agents and agent tools are only the top layer of a new software application devel-\nopment paradigm. We\u2019ll look at this new paradigm in the next section.\nThe agent evaluates if\nthe goal is complete\nafter every task\niteration.\nAI Large Language\nModels (GPT-4)\nInternet\nOther Tools\nEvaluation\nDe\ufb01ning and\nSequencing Tasks\nSetting Goals\nTask Execution\nGoals not complete\nThe agent can be\nset to ask for\npermission for\nevery task or for\nevery x number\nof tasks.\nGoals complete\nUser sets the main\nobjective goal\nAgent executes\ntasks.\nThe agent could write code to\nperform other tasks as needed.\nThe agent\nplans out the\nsequence of\ntasks to\nundertake.\nAutonomous AI Mechanism\nFigure 1.9\nThe original design of the AutoGPT agent system\n",
      "content_length": 950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "11\n1.4\nPeeling back the AI interface\n1.4\nPeeling back the AI interface\nThe AI agent paradigm is not only a shift in how we work with LLMs but is also per-\nceived as a shift in how we develop software and handle data. Software and data will\nno longer be interfaced using user interfaces (UIs), application programming inter-\nfaces (APIs), and specialized query languages such as SQL. Instead, they will be\ndesigned to be interfaced using natural language.\n Figure 1.10 shows a high-level snapshot of what this new architecture may look like\nand what role AI agents play. Data, software, and applications adapt to support seman-\ntic, natural language interfaces. These AI interfaces allow agents to collect data and\ninteract with software applications, even other agents or agent applications. This rep-\nresents a new shift in how we interact with software and applications.\nAn AI interface is a collection of functions, tools, and data layers that expose data and\napplications by natural language. In the past, the word semantic has been heavily\nAgent interface layer (natural language)\nPlanning: Agent takes the goal and breaks into tasks.\n1. Collect data.\n2. Annotate data.\n3. Format data, and create report visualizations.\n4. Present report.\n4. Agent presents the report.\nPlease create a report of\nlast year\u2019s sales.\nGPT data layer\nDatabase\n1. Query database using natural\nlanguage.\nGPT functions\nAPIs,\nweb browsing,\nsearch, etc.\nAll communication via natural language\n2. Annotate data by calling semantic\nfunctions using natural language.\nExternal agents\nGPT tools\n3. External agent formats data and\nmay write code to generate visuals.\nFigure 1.10\nA vision of how agents will interact with software systems\n",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "12\nCHAPTER 1\nIntroduction to agents and their world\nused to describe these interfaces, and even some tools use the name; however,\n\u201csemantic\u201d can also have a variety of meanings and uses. Therefore, in this book,\nwe\u2019ll use the term AI interface.\n The construction of AI interfaces will empower agents that need to consume\nthe services, tools, and data. With this empowerment will come increased accuracy\nin completing tasks and more trustworthy and autonomous applications. While an\nAI interface may not be appropriate for all software and data, it will dominate many\nuse cases.\n1.5\nNavigating the agent landscape\nGPT agents represent an entire shift in how consumers and developers approach\neverything, from finding information to building software and accessing data. Almost\ndaily, a new agent framework, component, or interface pops up on GitHub or in a\nresearch paper. This can be overwhelming and intimidating to the new user trying to\ngrasp what agent systems are and how to use them.\nSummary\n\uf0a1An agent is an entity that acts or exerts power, produces an effect, or serves as a\nmeans for achieving a result. An agent automates interaction with a large lan-\nguage model (LLM) in AI.\n\uf0a1An assistant is synonymous with an agent. Both terms encompass tools such as\nOpenAI\u2019s GPT Assistants.\n\uf0a1Autonomous agents can make independent decisions, and their distinction\nfrom non-autonomous agents is crucial.\n\uf0a1The four main types of LLM interactions include direct user interaction, agent/\nassistant proxy, agent/assistant, and autonomous agent.\n\uf0a1Multi-agent systems involve agent profiles working together, often controlled by\na proxy, to accomplish complex tasks.\n\uf0a1The main components of an agent include the profile/persona, actions, knowl-\nedge/memory, reasoning/evaluation, and planning/feedback.\n\uf0a1Agent profiles and personas guide an agent\u2019s tasks, responses, and other nuances,\noften including background and demographics.\n\uf0a1Actions and tools for agents can be manually generated, recalled from memory,\nor follow predefined plans.\n\uf0a1Agents use knowledge and memory structures to optimize context and mini-\nmize token usage via various formats, from documents to embeddings.\n\uf0a1Reasoning and evaluation systems enable agents to think through problems\nand assess solutions using prompting patterns such as zero-shot, one-shot, and\nfew-shot.\n\uf0a1Planning/feedback components organize tasks to achieve goals using single-path\nor multipath reasoning and integrating environmental and human feedback.\n",
      "content_length": 2488,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "13\nSummary\n\uf0a1The rise of AI agents has introduced a new software development paradigm,\nshifting from traditional to natural language\u2013based AI interfaces.\n\uf0a1Understanding the progression and interaction of these tools helps develop\nagent systems, whether single, multiple, or autonomous.\n",
      "content_length": 285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "14\nHarnessing the power\nof large language models\nThe term large language models (LLMs) has now become a ubiquitous descriptor of a\nform of AI. These LLMs have been developed using generative pretrained trans-\nformers (GPTs). While other architectures also power LLMs, the GPT form is cur-\nrently the most successful.\n LLMs and GPTs are generative models, which means they are trained to generate\nrather than predict or classify content. To illustrate this further, consider figure 2.1,\nwhich shows the difference between generative and predictive/classification mod-\nels. Generative models create something from the input, whereas predictive and\nclassifying models classify it.\nThis chapter covers\n\uf0a1Understanding the basics of LLMs\n\uf0a1Connecting to and consuming the OpenAI API\n\uf0a1Exploring and using open source LLMs with \nLM Studio\n\uf0a1Prompting LLMs with prompt engineering\n\uf0a1Choosing the optimal LLM for your specific \nneeds\n",
      "content_length": 921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "15\nWe can further define an LLM by its constituent parts, as shown in figure 2.2. In\nthis diagram, data represents the content used to train the model, and architecture is\nan attribute of the model itself, such as the number of parameters or size of the\nmodel. Models are further trained specifically to the desired use case, including\nchat, completions, or instruction. Finally, fine-tuning is a feature added to models\nthat refines the input data and model training to better match a particular use case\nor domain.\nInput text\nOutputs the most\nprobable next word\ngiven the input\nthus far\nOutputs the most\nprobable class\nthe phrase is\naligned with\nOutputs text\nOutputs prediction\nFigure 2.1\nThe difference between generative and predictive models\nData\nLLM\nTraining\nArchitecture\nFine-tuning\nThe input data represents the\ncontent the model will train on.\nThis typically consists of terabytes\nto petabytes of data.\nDenotes the model architecture.\nThe architecture de\ufb01nes things\nsuch as context, token limits,\nembedding size, and number of\nparameters (model size).\nDe\ufb01nes the form of training used to\ntrain the model. Training will also\nde\ufb01ne the model use case, such as\nchat completions, completions,\ninstruct, or question/answer.\nFine-tuning is the process of\nmaking a model more speci\ufb01c\nto a particular domain or dataset.\nFigure 2.2\nThe main elements that describe an LLM\nCHAPTER 2\nHarnessing the power of large language models\n",
      "content_length": 1427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "16\nCHAPTER 2\nHarnessing the power of large language models\nThe transformer architecture of GPTs, which is a specific architecture of LLMs, allows\nthe models to be scaled to billions of parameters in size. This requires these large\nmodels to be trained on terabytes of documents to build a foundation. From there,\nthese models will be successively trained using various methods for the desired use\ncase of the model.\n ChatGPT, for example, is trained effectively on the public internet and then fine-\ntuned using several training strategies. The final fine-tuning training is completed\nusing an advanced form called reinforcement learning with human feedback (RLHF). This\nproduces a model use case called chat completions.\n Chat completions LLMs are designed to improve through iteration and refine-\nment\u2014in other words, chatting. These models have also been benchmarked to be the\nbest in task completion, reasoning, and planning, which makes them ideal for build-\ning agents and assistants. Completion models are trained/designed only to provide\ngenerated content on input text, so they don\u2019t benefit from iteration.\n For our journey to build powerful agents in this book, we focus on the class of\nLLMs called chat completions models. That, of course, doesn\u2019t preclude you from try-\ning other model forms for your agents. However, you may have to significantly alter\nthe code samples provided to support other model forms.\n We\u2019ll uncover more details about LLMs and GPTs later in this chapter when we\nlook at running an open source LLM locally. In the next section, we look at how to\nconnect to an LLM using a growing standard from OpenAI.\n2.1\nMastering the OpenAI API\nNumerous AI agents and assistant projects use the OpenAI API SDK to connect to an\nLLM. While not standard, the basic concepts describing a connection now follow the\nOpenAI pattern. Therefore, we must understand the core concepts of an LLM con-\nnection using the OpenAI SDK.\n This chapter will look at connecting to an LLM model using the OpenAI Python\nSDK/package. We\u2019ll discuss connecting to a GPT-4 model, the model response, count-\ning tokens, and how to define consistent messages. Starting in the following subsec-\ntion, we\u2019ll examine how to use OpenAI.\n2.1.1\nConnecting to the chat completions model\nTo complete the exercises in this section and subsequent ones, you must set up a\nPython developer environment and get access to an LLM. Appendix A walks you\nthrough setting up an OpenAI account and accessing GPT-4 or other models. Appen-\ndix B demonstrates setting up a Python development environment with Visual Studio\nCode (VS Code), including installing needed extensions. Review these sections if you\nwant to follow along with the scenarios.\n Start by opening the source code chapter_2 folder in VS Code and creating a new\nPython virtual environment. Again, refer to appendix B if you need assistance.\n",
      "content_length": 2881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "17\n2.1\nMastering the OpenAI API\n Then, install the OpenAI and Python dot environment packages using the com-\nmand in the following listing. This will install the required packages into the virtual\nenvironment.\npip install openai python-dotenv\nNext, open the connecting.py file in VS Code, and inspect the code shown in listing 2.2.\nBe sure to set the model\u2019s name to an appropriate name\u2014for example, gpt-4. At the\ntime of writing, the gpt-4-1106-preview was used to represent GPT-4 Turbo.\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nload_dotenv()                          \napi_key = os.getenv('OPENAI_API_KEY')\nif not api_key:                            \n    raise ValueError(\"No API key found. Please check your .env file.\")\nclient = OpenAI(api_key=api_key)                       \ndef ask_chatgpt(user_message):\n    response = client.chat.completions.create(     \n        model=\"gpt-4-1106-preview\",\n        messages=[{\"role\": \"system\",\n \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": user_message}],\n        temperature=0.7,\n        )\n    return response.choices[0].message.content    \nuser = \"What is the capital of France?\"\nresponse = ask_chatgpt(user)               \nprint(response)\nA lot is happening here, so let\u2019s break it down by section, starting with the beginning\nand loading the environment variables. In the chapter_2 folder is another file called\n.env, which holds environment variables. These variables are set automatically by call-\ning the load_dotenv function.\n You must set your OpenAI API key in the .env file, as shown in the next listing.\nAgain, refer to appendix A to find out how to get a key and find a model name.\nOPENAI_API_KEY='your-openai-api-key'\nAfter setting the key, you can debug the file by pressing the F5 key or selecting Run >\nStart Debugging from the VS Code menu. This will run the code, and you should see\nsomething like \u201cThe capital of France is Paris.\u201d\nListing 2.1\npip installs\nListing 2.2\nconnecting.py\nListing 2.3\n.env\nLoads the secrets \nstored in the .env file\nChecks to see \nwhether the key is set\nCreates a client \nwith the key\nUses the create \nfunction to generate \na response\nReturns just the content \nof the response\nExecutes the request and \nreturns the response\n",
      "content_length": 2272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "18\nCHAPTER 2\nHarnessing the power of large language models\n Remember that the response from a generative model depends on the probability.\nThe model will probably give us a correct and consistent answer in this case.\n You can play with these probabilities by adjusting the temperature of the request.\nIf you want a model to be more consistent, turn the temperature down to 0, but if you\nwant the model to produce more variation, turn the temperature up. We\u2019ll explore\nsetting the temperature further in the next section.\n2.1.2\nUnderstanding the request and response\nDigging into the chat completions request and response features can be helpful. We\u2019ll\nfocus on the request first, as shown next. The request encapsulates the intended\nmodel, the messages, and the temperature.\nresponse = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",                \n    messages=[{\"role\": \"system\", \n\"content\": \"You are a helpful assistant.\"},                    \n              {\"role\": \"user\", \"content\": user_message}],     \n    temperature=0.7,    \n    )\nWithin the request, the messages block describes a set of messages and roles used in a\nrequest. Messages for a chat completions model can be defined in three roles:\n\uf0a1System role\u2014A message that describes the request\u2019s rules and guidelines. It can\noften be used to describe the role of the LLM in making the request.\n\uf0a1User role\u2014Represents and contains the message from the user.\n\uf0a1Assistant role\u2014Can be used to capture the message history of previous responses\nfrom the LLM. It can also inject a message history when perhaps none existed.\nThe message sent in a single request can encapsulate an entire conversation, as shown\nin the JSON in the following listing.\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the capital of France?\"\n    },\n    {\n        \"role\": \"assistant\",\nListing 2.4\nThe chat completions request\nListing 2.5\nMessages with history\nThe model or deployment used \nto respond to the request\nThe system \nrole message\nThe user role \nmessage\nThe temperature or \nvariability of the request\n",
      "content_length": 2159,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "19\n2.1\nMastering the OpenAI API\n        \"content\": \"The capital of France is Paris.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is an interesting fact of Paris.\"\n    }\n],\nYou can see how this can be applied by opening message_history.py in VS Code and\ndebugging it by pressing F5. After the file runs, be sure to check the output. Then, try\nto run the sample a few more times to see how the results change.\n The results will change from each run to the next due to the high temperature of\n.7. Go ahead and reduce the temperature to .0, and run the message_history.py\nsample a few more times. Keeping the temperature at 0 will show the same or similar\nresults each time.\n Setting a request\u2019s temperature will often depend on your particular use case.\nSometimes, you may want to limit the responses\u2019 stochastic nature (randomness).\nReducing the temperature to 0 will give consistent results. Likewise, a value of 1.0 will\ngive the most variability in the responses.\n Next, we also want to know what information is being returned for each request.\nThe next listing shows the output format for the response. You can see this output by\nrunning the message_history.py file in VS Code.\n{\n    \"id\": \"chatcmpl-8WWL23up3IRfK1nrDFQ3EHQfhx0U6\",\n    \"choices\": [                                     \n        {\n            \"finish_reason\": \"stop\",\n            \"index\": 0,\n            \"message\": {\n                \"content\": \"\u2026 omitted\",\n                \"role\": \"assistant\",      \n                \"function_call\": null,\n                \"tool_calls\": null\n            },\n            \"logprobs\": null\n        }\n    ],\n    \"created\": 1702761496,\n    \"model\": \"gpt-4-1106-preview\",    \n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": \"fp_3905aa4f79\",\n    \"usage\": {\n        \"completion_tokens\": 78,    \n        \"prompt_tokens\": 48,        \n        \"total_tokens\": 126         \n    }\n}\nListing 2.6\nChat completions response\nA model may \nreturn more than \none response.\nResponses \nreturned in the \nassistant role\nIndicates the \nmodel used\nCounts the number of \ninput (prompt) and output \n(completion) tokens used\n",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "20\nCHAPTER 2\nHarnessing the power of large language models\nIt can be helpful to track the number of input tokens (those used in prompts) and the\noutput tokens (the number returned through completions). Sometimes, minimizing\nand reducing the number of tokens can be essential. Having fewer tokens typically\nmeans LLM interactions will be cheaper, respond faster, and produce better and more\nconsistent results.\n That covers the basics of connecting to an LLM and returning responses. Through-\nout this book, we\u2019ll review and expand on how to interact with LLMs. Until then, we\u2019ll\nexplore in the next section how to load and use open source LLMs.\n2.2\nExploring open source LLMs with LM Studio\nCommercial LLMs, such as GPT-4 from OpenAI, are an excellent place to start to\nlearn how to use modern AI and build agents. However, commercial agents are an\nexternal resource that comes at a cost, reduces data privacy and security, and\nintroduces dependencies. Other external influences can further complicate these\nfactors.\n It\u2019s unsurprising that the race to build comparable open source LLMs is growing\nmore competitive every day. As a result, there are now open source LLMs that may be\nadequate for numerous tasks and agent systems. There have even been so many\nadvances in tooling in just a year that hosting LLMs locally is now very easy, as we\u2019ll see\nin the next section.\n2.2.1\nInstalling and running LM Studio\nLM Studio is a free download that supports downloading and hosting LLMs and other\nmodels locally for Windows, Mac, and Linux. The software is easy to use and offers sev-\neral helpful features to get you started quickly. Here is a quick summary of steps to\ndownload and set up LM Studio:\n1\nDownload LM Studio from https://lmstudio.ai/.\n2\nAfter downloading, install the software per your operating system. Be aware that\nsome versions of LM Studio may be in beta and require installation of addi-\ntional tools or libraries.\n3\nLaunch the software.\nFigure 2.3 shows the LM Studio window running. From there, you can review the cur-\nrent list of hot models, search for others, and even download. The home page content\ncan be handy for understanding the details and specifications of the top models.\n An appealing feature of LM Studio is its ability to analyze your hardware and align\nit with the requirements of a given model. The software will let you know how well you\ncan run a given model. This can be a great time saver in guiding what models you\nexperiment with.\n \n",
      "content_length": 2475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "21\n2.2\nExploring open source LLMs with LM Studio\nEnter some text to search for a model, and click Go. You\u2019ll be taken to the search\npage interface, as shown in figure 2.4. From this page, you can see all the model\nvariations and other specifications, such as context token size. After you click the\nCompatibility Guess button, the software will even tell you if the model will run on\nyour system.\n Click to download any model that will run on your system. You may want to stick\nwith models designed for chat completions, but if your system is limited, work with\nwhat you have. In addition, if you\u2019re unsure of which model to use, go ahead and\ndownload to try them. LM Studio is a great way to explore and experiment with many\nmodels.\n \n \n \n \nChat interface to talk\ndirectly to a local LLM\nRun a local model\nas a service.\nSearch area\nBrowse already\ndownloaded models.\nFigure 2.3\nLM Studio software showing the main home page\n",
      "content_length": 924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "22\nCHAPTER 2\nHarnessing the power of large language models\nAfter the model is downloaded, you can then load and run the model on the chat\npage or as a server on the server page. Figure 2.5 shows loading and running a\nmodel on the chat page. It also shows the option for enabling and using a GPU if\nyou have one.\n To load and run a model, open the drop-down menu at the top middle of the\npage, and select a downloaded model. A progress bar will appear showing the model\nloading, and when it\u2019s ready, you can start typing into the UI.\n The software even allows you to use some or all of your GPU, if detected, for the\nmodel inference. A GPU will generally speed up the model response times in some\ncapacities. You can see how adding a GPU can affect the model\u2019s performance by\nlooking at the performance status at the bottom of the page, as shown in figure 2.5.\n Chatting with a model and using or playing with various prompts can help you\ndetermine how well a model will work for your given use case. A more systematic\napproach is using the prompt flow tool for evaluating prompts and LLMs. We\u2019ll\ndescribe how to use prompt flow in chapter 9.\n LM Studio also allows a model to be run on a server and made accessible using the\nOpenAI package. We\u2019ll see how to use the server feature and serve a model in the\nnext section.\nSearch text\nLook at the model\ncard on Hugging Face.\nThe compatibility guesser\ninforms you if a model will run.\nShows the\ndownloaded models\nFigure 2.4\nThe LM Studio search page\n",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "23\n2.2\nExploring open source LLMs with LM Studio\n2.2.2\nServing an LLM locally with LM Studio\nRunning an LLM locally as a server is easy with LM Studio. Just open the server page,\nload a model, and then click the Start Server button, as shown in figure 2.6. From\nthere, you can copy and paste any of the examples to connect with your model.\n You can review an example of the Python code by opening chapter_2/lmstudio_\nserver.py in VS Code. The code is also shown here in listing 2.7. Then, run the code\nin the VS Code debugger (press F5).\n \n \nLoaded model\nConversation\nhistory\nText area for\nuser messages\nModel system\nprompt\nEnabling GPU acceleration,\navailable when GPU detected\nModel performance\nand usage\nFigure 2.5\nThe LM Studio chat page with a loaded, locally running LLM\n",
      "content_length": 777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "24\nCHAPTER 2\nHarnessing the power of large language models\nfrom openai import OpenAI\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\ncompletion = client.chat.completions.create(\n  model=\"local-model\",                          \n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}      \n  ],\n  temperature=0.7,\n)\nprint(completion.choices[0].message)     \nListing 2.7\nlmstudio_server.py\nLoaded model\nEnabling GPU acceleration,\navailable when GPU detected\nExample to connect\nto the server\nStart/stop the server\nLogs showing startup and\nconnection information\nFigure 2.6\nThe LM Studio server page and a server running an LLM\nCurrently not used; \ncan be anything\nFeel free to change \nthe message as \nyou like.\nDefault code outputs \nthe whole message.\n",
      "content_length": 851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "25\n2.3\nPrompting LLMs with prompt engineering\nIf you encounter problems connecting to the server or experience any other prob-\nlems, be sure your configuration for the Server Model Settings matches the model\ntype. For example, in figure 2.6, shown earlier, the loaded model differs from the\nserver settings. The corrected settings are shown in figure 2.7.\nNow, you can use a locally hosted LLM or a commercial model to build, test, and\npotentially even run your agents. The following section will examine how to build\nprompts using prompt engineering more effectively.\n2.3\nPrompting LLMs with prompt engineering\nA prompt defined for LLMs is the message content used in the request for better\nresponse output. Prompt engineering is a new and emerging field that attempts to struc-\nture a methodology for building prompts. Unfortunately, prompt building isn\u2019t a well-\nestablished science, and there is a growing and diverse set of methods defined as\nprompt engineering.\n Fortunately, organizations such as OpenAI have begun documenting a universal\nset of strategies, as shown in figure 2.8. These strategies cover various tactics, some\nrequiring additional infrastructure and considerations. As such, the prompt engineer-\ning strategies relating to more advanced concepts will be covered in the indicated\nchapters.\n Each strategy in figure 2.8 unfolds into tactics that can further refine the specific\nmethod of prompt engineering. This chapter will examine the fundamental Write\nLoaded model\nServer\nodel ettings\nM\nS\nmatch the loaded model.\nWhen running correctly,\nyou\u2019ll see the message logs.\nFigure 2.7\nChoosing the correct Server Model Settings for the loaded model\n",
      "content_length": 1667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "26\nCHAPTER 2\nHarnessing the power of large language models\nClear Instructions strategy. Figure 2.9 shows the tactics for this strategy in more detail,\nalong with examples for each tactic. We\u2019ll look at running these examples using a\ncode demo in the following sections.\n The Write Clear Instructions strategy is about being careful and specific about\nwhat you ask for. Asking an LLM to perform a task is no different from asking a per-\nson to complete the same task. Generally, the more information and context relevant\nto a task you can specify in a request, the better the response.\n This strategy has been broken down into specific tactics you can apply to prompts.\nTo understand how to use those, a code demo (prompt_engineering.py) with various\nprompt examples is in the chapter 2 source code folder.\n Open the prompt_engineering.py file in VS Code, as shown in listing 2.8. This\ncode starts by loading all the JSON Lines files in the prompts folder. Then, it displays\nthe list of files as choices and allows the user to select a prompt option. After selecting\nthe option, the prompts are submitted to an LLM, and the response is printed.\n \n \n \nWrite Clear Instructions\nBe speci\ufb01c in what you ask.\nTactics include detailing queries, adopting personas, using delimiters,\nspecifying steps, providing examples, and specifying output length.\nBasics\nProvide Reference Text\nHelps reduce fabrications.\nTactics involve instructing the model to use or cite reference texts.\nMemory\nUse External Tools\nEnhances model capabilities.\nTactics include embeddings-based search, code execution, and\naccess to speci\ufb01c functions.\nMemory\nSplit Complex Tasks into Simpler Subtasks\nReduces error rates.\nTactics include intent classi\ufb01cation, summarizing dialogues, and\npiecewise summarization of documents.\nPlanning\nGive Models Time to \u201cThink\u201d\nAllows more reliable reasoning.\nTactics involve working out solutions before conclusions, using inner\nmonologue, and reviewing previous answers.\nPlanning\nTest Changes Systematically\nEnsures improvements are genuine.\nTactics involve evaluating model outputs with reference to standard\nanswers.\nEvaluation\nPrompt Engineering Strategies\nFigure 2.8\nOpenAI prompt engineering strategies reviewed in this book, by chapter location\n",
      "content_length": 2250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "27\n2.3\nPrompting LLMs with prompt engineering\ndef main():\n    directory = \"prompts\"\n    text_files = list_text_files_in_directory(directory)   \n    if not text_files:\n        print(\"No text files found in the directory.\")\n        return\n    def print_available():                                    \n        print(\"Available prompt tactics:\")\n        for i, filename in enumerate(text_files, start=1):\n            print(f\"{i}. {filename}\")\n    while True:\n        try:\n            print_available()                                 \n            choice = int(input(\"Enter \u2026 0 to exit): \"))          \nListing 2.8\nprompt_engineering.py (main())\nDetailed\nQueries\nAdopting\nPersonas\nUsing\nDelimiters\nSpecifying\nSteps\nProviding\nExamples\nSpecify Output\nLength\nTactics for Strategy: Writing Clear Instructions\nWithout detailed\nqueries:\nWho\u2019s the prime\nminister?\nWith detailed\nqueries:\nWho is the prime\nminister of Canada,\nand how frequently\nare elections held?\nSYSTEM:\nYou are a prompt\nexpert and will\nsuggest ways to\nimprove a user\nrequest.\nUSER:\nWhat is the capital\nof Canada?\nUSER:\nSummarize the text\ndelimited by triple\nquotes with a\nlimerick:\n\u201ctext to be\nsummarized\u201d\n.\nEXAMPLES\nSYSTEM:\nUse the following\nstep-by-step\ninstructions to\nrespond to user\ninputs:\nStep 1 - Summarize\nthe text in triple\nquotes to one\nsentence with a\npre\ufb01x that says\n\u201cSummary: \u201d.\nStep 2 - Translate\nthe summary from\nStep 1 into French,\nwith a pre\ufb01x that\nsays \u201cTranslation: \u201d.\nUSER:\n\u201ctext to summarize\nand translate\u201d\nSYSTEM:\nAnswer in a\nconsistent style.\nUSER:\nT\no\neach me ab ut\npatience.\nA\nN\nSSISTA T:\nThe river that\ncarves the deepest\nvalley \ufb02ows from a\nmodest spring; the\nmost intricate\ntapestry begins with\na solitary thread.\nUSER:\nT\no\neach me ab ut\nthe ocean.\nUSER:\nSummarize the text\ndelimited by triple\nquotes in about 50\nwords.\n\u201ctext to summarize\nhere\u201d\nThis is the\nexample.\nProvide as much\ndetail as you can\nin a query;\ngenerally, the\nmore detail the\nbetter.\nPersonas can\ninclude details\nabout\ndemographics,\nknowledge, and\npersonality.\nDelimiters can\nhel separate\np\nblocks of content\nfrom speci\ufb01cation\ndetails.\nLimiting the\nlength of output\ncan be speci\ufb01c\nto words, bullet\npoints, or other\nmetrics.\nUsing steps can\nhelp the LLM\nbetter process\nthe task, but be\nsure to limit\nthe number.\nExamples are a\nform of few-shot\nlearning and can\nbe an excellent\nway to indicate\nthe desired\nresponse format\nand other details.\nFigure 2.9\nThe tactics for the Write Clear Instructions strategy\nCollects all \nthe files for the \ngiven folder\nPrints the list of \nfiles as choices\nInputs the \nuser\u2019s choice\n",
      "content_length": 2564,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "28\nCHAPTER 2\nHarnessing the power of large language models\n            if choice == 0:\n                break\n            elif 1 <= choice <= len(text_files):\n                selected_file = text_files[choice - 1]\n                file_path = os.path.join(directory,\n      selected_file)\n                prompts = \n\u27a5 load_and_parse_json_file(file_path)                         \n                print(f\"Running prompts for {selected_file}\")\n                for i, prompt in enumerate(prompts):\n                    print(f\"PROMPT {i+1} --------------------\")\n                    print(prompt)\n                    print(f\"REPLY ---------------------------\")\n                    print(prompt_llm(prompt))                      \n            else:\n                print(\"Invalid choice. Please enter a valid number.\")\n        except ValueError:\n            print(\"Invalid input. Please enter a number.\")\nA commented-out section from the listing demonstrates how to connect to a local\nLLM. This will allow you to explore the same prompt engineering tactics applied to\nopen source LLMs running locally. By default, this example uses the OpenAI model\nwe configured previously in section 2.1.1. If you didn\u2019t complete that earlier, please go\nback and do it before running this one.\n Figure 2.10 shows the output of running the prompt engineering tactics tester, the\nprompt_engineering.py file in VS Code. When you run the tester, you can enter a\nvalue for the tactic you want to test and watch it run.\nIn the following sections, we\u2019ll explore each prompt tactic in more detail. We\u2019ll also\nexamine the various examples.\n2.3.1\nCreating detailed queries\nThe basic premise of this tactic is to provide as much detail as possible but also to be\ncareful not to give irrelevant details. The following listing shows the JSON Lines file\nexamples for exploring this tactic.\n \n \nLoads the \nprompt and \nparses it into \nmessages\nSubmits the \nprompt to an \nOpenAI LLM\nFigure 2.10\nThe output of the prompt engineering tactics tester\n",
      "content_length": 2005,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "29\n2.3\nPrompting LLMs with prompt engineering\n[                       \n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is an agent?\"     \n    }\n]\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"\nWhat is a GPT Agent? \nPlease give me 3 examples of a GPT agent\n\"\"\"                                       \n    }\n]\nThis example demonstrates the difference between using detailed queries and not. It\nalso goes a step further by asking for examples. Remember, the more relevance and\ncontext you can provide in your prompt, the better the overall response. Asking for\nexamples is another way of enforcing the relationship between the question and the\nexpected output.\n2.3.2\nAdopting personas\nAdopting personas grants the ability to define an overarching context or set of rules to\nthe LLM. The LLM can then use that context and/or rules to frame all later output\nresponses. This is a compelling tactic and one that we\u2019ll make heavy use of throughout\nthis book.\n Listing 2.10 shows an example of employing two personas to answer the same ques-\ntion. This can be an enjoyable technique for exploring a wide range of novel applica-\ntions, from getting demographic feedback to specializing in a specific task or even\nrubber ducking.\nListing 2.9\ndetailed_queries.jsonl\nGPT rubber ducking\nRubber ducking is a problem-solving technique in which a person explains a problem\nto an inanimate object, like a rubber duck, to understand or find a solution. This\nmethod is prevalent in programming and debugging, as articulating the problem aloud\noften helps clarify the problem and can lead to new insights or solutions.\nThe first example doesn\u2019t \nuse detailed queries.\nFirst ask the LLM a \nvery general question.\nAsk a more specific question, \nand ask for examples.\n",
      "content_length": 1936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "30\nCHAPTER 2\nHarnessing the power of large language models\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nYou are a 20 year old female who attends college \nin computer science. Answer all your replies as \na junior programmer.\n\"\"\"                        \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the best subject to study.\"\n    }\n]\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nYou are a 38 year old male registered nurse. \nAnswer all replies as a medical professional.\n\"\"\"                                            \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the best subject to study.\"\n    }\n]\nA core element of agent profiles is the persona. We\u2019ll employ various personas to assist\nagents in completing their tasks. When you run this tactic, pay particular attention to\nthe way the LLM outputs the response.\n2.3.3\nUsing delimiters\nDelimiters are a useful way of isolating and getting the LLM to focus on some part of a\nmessage. This tactic is often combined with other tactics but can work well inde-\npendently. The following listing demonstrates two examples, but there are several\nother ways of describing delimiters, from XML tags to using markdown.\n[\n    {\n(continued)\nGPT rubber ducking uses the same technique, but instead of an inanimate object, we\nuse an LLM. This strategy can be expanded further by giving the LLM a persona spe-\ncific to the desired solution domain.\nListing 2.10\nadopting_personas.jsonl\nListing 2.11\nusing_delimiters.jsonl\nFirst persona\nSecond persona\n",
      "content_length": 1549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "31\n2.3\nPrompting LLMs with prompt engineering\n        \"role\": \"system\",\n        \"content\": \"\"\"\nSummarize the text delimited by triple quotes \nwith a haiku.\n\"\"\"              \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"A gold chain is cool '''but a silver chain is better'''\"\n    }\n]\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nYou will be provided with a pair of statements \n(delimited with XML tags) about the same topic. \nFirst summarize the arguments of each statement. \nThen indicate which of them makes a better statement\n and explain why.\n\"\"\"                       \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"\n<statement>gold chains are cool</statement>\n<statement>silver chains are better</statement>\n\"\"\"\n    }\n]\nWhen you run this tactic, pay attention to the parts of the text the LLM focuses on\nwhen it outputs the response. This tactic can be beneficial for describing information\nin a hierarchy or other relationship patterns.\n2.3.4\nSpecifying steps\nSpecifying steps is another powerful tactic that can have many uses, including in\nagents, as shown in listing 2.12. It\u2019s especially powerful when developing prompts or\nagent profiles for complex multistep tasks. You can specify steps to break down these\ncomplex prompts into a step-by-step process that the LLM can follow. In turn, these\nsteps can guide the LLM through multiple interactions over a more extended conver-\nsation and many iterations.\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nUse the following step-by-step instructions to respond to user inputs.\nStep 1 - The user will provide you with text in triple single quotes. \nSummarize this text in one sentence with a prefix that says 'Summary: '.\nListing 2.12\nspecifying_steps.jsonl \nThe delimiter is defined by \ncharacter type and repetition.\nThe delimiter is defined \nby XML standards.\n",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "32\nCHAPTER 2\nHarnessing the power of large language models\nStep 2 - Translate the summary from Step 1 into Spanish, \nwith a prefix that says 'Translation: '.\n\"\"\"                                         \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"'''I am hungry and would like to order an appetizer.'''\"\n    }\n]\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nUse the following step-by-step instructions to respond to user inputs.\nStep 1 - The user will provide you with text. Answer any questions in \nthe text in one sentence with a prefix that says 'Answer: '.\nStep 2 - Translate the Answer from Step 1 into a dad joke,\n with a prefix that says 'Dad Joke: '.\"\"\"                     \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the tallest structure in Paris?\"\n    }\n]\n2.3.5\nProviding examples\nProviding examples is an excellent way to guide the desired output of an LLM. There\nare numerous ways to demonstrate examples to an LLM. The system message/prompt\ncan be a helpful way to emphasize general output. In the following listing, the example\nis added as the last LLM assistant reply, given the prompt \u201cTeach me about Python.\u201d\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nAnswer all replies in a consistent style that follows the format, \nlength and style of your previous responses.\nExample:\n  user:\n       Teach me about Python.\n  assistant:                                               \n       Python is a programming language developed in 1989\n by Guido van Rossum.\n  Future replies:\n       The response was only a sentence so limit\n all future replies to a single sentence.\n\"\"\"                                          \n    },\n    {\n        \"role\": \"user\",\nListing 2.13\nproviding_examples.jsonl\nNotice the tactic of \nusing delimiters.\nSteps can be \ncompletely \ndifferent \noperations.\nInjects the \nsample output as \nthe \u201cprevious\u201d \nassistant reply\nAdds a limit output tactic to \nrestrict the size of the output \nand match the example\n",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "33\n2.3\nPrompting LLMs with prompt engineering\n        \"content\": \"Teach me about Java.\"\n    }\n]\nProviding examples can also be used to request a particular output format from a\ncomplex series of tasks that derive the output. For example, asking an LLM to pro-\nduce code that matches a sample output is an excellent use of examples. We\u2019ll employ\nthis tactic throughout the book, but other methods exist for guiding output.\n2.3.6\nSpecifying output length\nThe tactic of specifying output length can be helpful in not just limiting tokens but\nalso in guiding the output to a desired format. Listing 2.14 shows an example of using\ntwo different techniques for this tactic. The first limits the output to fewer than 10\nwords. This can have the added benefit of making the response more concise and\ndirected, which can be desirable for some use cases. The second example demon-\nstrates limiting output to a concise set of bullet points. This method can help narrow\ndown the output and keep answers short. More concise answers generally mean the\noutput is more focused and contains less filler.\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nSummarize all replies into 10 or fewer words.\n\"\"\"                                              \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Please tell me an exciting fact about Paris?\"\n    }\n]\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\nSummarize all replies into 3 bullet points.\n\"\"\"                                          \n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Please tell me an exciting fact about Paris?\"\n    }\n]\nKeeping answers brief can have additional benefits when developing multi-agent sys-\ntems. Any agent system that converses with other agents can benefit from more con-\ncise and focused replies. It tends to keep the LLM more focused and reduces noisy\ncommunication.\nListing 2.14\nspecifying_output_length.jsonl\nRestricting the output \nmakes the answer \nmore concise.\nRestricts the answer \nto a short set of \nbullets\n",
      "content_length": 2022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "34\nCHAPTER 2\nHarnessing the power of large language models\n Be sure to run through all the examples of the prompt tactics for this strategy. As\nmentioned, we\u2019ll cover other prompt engineering strategies and tactics in future chap-\nters. We\u2019ll finish this chapter by looking at how to pick the best LLM for your use case.\n2.4\nChoosing the optimal LLM for your specific needs\nWhile being a successful crafter of AI agents doesn\u2019t require an in-depth understand-\ning of LLMs, it\u2019s helpful to be able to evaluate the specifications. Like a computer\nuser, you don\u2019t need to know how to build a processor to understand the differences\nin processor models. This analogy holds well for LLMs, and while the criteria may be\ndifferent, it still depends on some primary considerations.\n From our previous discussion and look at LM Studio, we can extract some funda-\nmental criteria that will be important to us when considering LLMs. Figure 2.11\nLLMs\n2\n1\n3\n4\n5\nModel Performance:\nDetermines how well a\nmodel may perform on a\ngiven benchmark, such\nas answering SAT\nquestions.\nModel Parameters (Size):\nSpeci\ufb01cs the size of the\nmodel in billions of\nparameters. Larger\nmodels typically perform\nbetter on general tasks.\nUse Case (Model Type):\nDetermines the type of\nmodel and expected use\ncase. This could be chat\ncompletions for a model\nlike ChatGPT.\nTraining Input: Speci\ufb01es\nthe material used to train\nthe model. This can\nrange from everything on\nthe internet to a speci\ufb01c\ndomain Python code.\nTraining Method: Speci\ufb01es\nhow the model is trained\nand/or \ufb01ne-tuned. Models\nlike ChatGPT are trained\nusing reinforcement learning\nwith human feedback.\n6\n7\n8\nContext Token Size:\nSpeci\ufb01es how large the\nmodel\u2019s context size is in\ntokens. Large context is\nimportant for verbose agent\nconversations.\nModel Speed (Model\nDeployment): Denotes the\nspeed of the model. OpenAI\nmodels marked Turbo are\ntypically faster. For local LLMs,\nspeed will be determined by\nthe infrastructure.\nModel Cost (Project\nBudget): Could represent\nthe price of the service or\nthe cost to host and run a\nmodel on your infrastructure.\nFigure 2.11\nThe important criteria to consider when consuming an LLM\n",
      "content_length": 2149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "35\n2.4\nChoosing the optimal LLM for your specific needs\nexplains the essential criteria to define what makes an LLM worth considering for cre-\nating a GPT agent or any LLM task.\n For our purposes of building AI agents, we need to look at each of these criteria in\nterms related to the task. Model context size and speed could be considered the sixth\nand seventh criteria, but they are usually considered variations of a model deployment\narchitecture and infrastructure. An eighth criterion to consider for an LLM is cost,\nbut this depends on many other factors. Here is a summary of how these criteria relate\nto building AI agents:\n\uf0a1Model performance\u2014You\u2019ll generally want to understand the LLM\u2019s performance\nfor a given set of tasks. For example, if you\u2019re building an agent specific to cod-\ning, then an LLM that performs well on code will be essential.\n\uf0a1Model parameters (size)\u2014The size of a model is often an excellent indication of\ninference performance and how well the model responds. However, the size of\na model will also dictate your hardware requirements. If you plan to use your\nown locally hosted model, the model size will also primarily dictate the com-\nputer and GPU you need. Fortunately, we\u2019re seeing small, very capable open\nsource models being released regularly.\n\uf0a1Use case (model type)\u2014The type of model has several variations. Chat completions\nmodels such as ChatGPT are effective for iterating and reasoning through a\nproblem, whereas models such as completion, question/answer, and instruct\nare more related to specific tasks. A chat completions model is essential for\nagent applications, especially those that iterate.\n\uf0a1Training input\u2014Understanding the content used to train a model will often dic-\ntate the domain of a model. While general models can be effective across tasks,\nmore specific or fine-tuned models can be more relevant to a domain. This may\nbe a consideration for a domain-specific agent where a smaller, more fine-tuned\nmodel may perform as well as or better than a larger model such as GPT-4.\n\uf0a1Training method\u2014It\u2019s perhaps less of a concern, but it can be helpful to under-\nstand what method was used to train a model. How a model is trained can\naffect its ability to generalize, reason, and plan. This can be essential for plan-\nning agents but perhaps less significant for agents than for a more task-specific\nassistant.\n\uf0a1Context token size\u2014The context size of a model is more specific to the model\narchitecture and type. It dictates the size of context or memory the model may\nhold. A smaller context window of less than 4,000 tokens is typically more than\nenough for simple tasks. However, a large context window can be essential when\nusing multiple agents\u2014all conversing over a task. The models will typically be\ndeployed with variations on the context window size.\n\uf0a1Model speed (model deployment)\u2014The speed of a model is dictated by its inference\nspeed (or how fast a model replies to a request), which in turn is dictated by the\ninfrastructure it runs on. If your agent isn\u2019t directly interacting with users, raw\n",
      "content_length": 3063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "36\nCHAPTER 2\nHarnessing the power of large language models\nreal-time speed may not be necessary. On the other hand, an LLM agent inter-\nacting in real time needs to be as quick as possible. For commercial models,\nspeed will be determined and supported by the provider. Your infrastructure\nwill determine the speed for those wanting to run their LLMs.\n\uf0a1Model cost (project budget)\u2014The cost is often dictated by the project. Whether\nlearning to build an agent or implementing enterprise software, cost is always a\nconsideration. A significant tradeoff exists between running your LLMs versus\nusing a commercial API.\nThere is a lot to consider when choosing which model you want to build a production\nagent system on. However, picking and working with a single model is usually best for\nresearch and learning purposes. If you\u2019re new to LLMs and agents, you\u2019ll likely want\nto choose a commercial option such as GPT-4 Turbo. Unless otherwise stated, the\nwork in this book will depend on GPT-4 Turbo.\n Over time, models will undoubtedly be replaced by better models. So you may\nneed to upgrade or swap out models. To do this, though, you must understand the\nperformance metrics of your LLMs and agents. Fortunately, in chapter 9, we\u2019ll explore\nevaluating LLMs, prompts, and agent profiles with prompt flow.\n2.5\nExercises\nUse the following exercises to help you engage with the material in this chapter:\n\uf0a1Exercise 1\u2014Consuming Different LLMs\nObjective\u2014Use the connecting.py code example to consume a different LLM\nfrom OpenAI or another provider.\nTasks:\n\u2013 Modify connecting.py to connect to a different LLM.\n\u2013 Choose an LLM from OpenAI or another provider.\n\u2013 Update the API keys and endpoints in the code.\n\u2013 Execute the modified code and validate the response.\n\uf0a1Exercise 2\u2014Exploring Prompt Engineering Tactics\nObjective\u2014Explore various prompt engineering tactics, and create variations\nfor each.\nTasks:\n\u2013 Review the prompt engineering tactics covered in the chapter.\n\u2013 Write variations for each tactic, experimenting with different phrasing and\nstructures.\n\u2013 Test the variations with an LLM to observe different outcomes.\n\u2013 Document the results, and analyze the effectiveness of each variation.\n\uf0a1Exercise 3\u2014Downloading and Running an LLM with LM Studio\nObjective\u2014Download an LLM using LM Studio, and connect it to prompt engi-\nneering tactics.\n",
      "content_length": 2336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "37\nSummary\nTasks:\n\u2013 Install LM Studio on your machine.\n\u2013 Download an LLM using LM Studio.\n\u2013 Serve the model using LM Studio.\n\u2013 Write Python code to connect to the served model.\n\u2013 Integrate the prompt engineering tactics example with the served model.\n\uf0a1Exercise 4\u2014Comparing Commercial and Open source LLMs\nObjective\u2014Compare the performance of a commercial LLM such as GPT-4\nTurbo with an open source model using prompt engineering examples.\nTasks:\n\u2013 Implement the prompt engineering examples using GPT-4 Turbo.\n\u2013 Repeat the implementation using an open source LLM.\n\u2013 Evaluate the models based on criteria such as response accuracy, coherence,\nand speed.\n\u2013 Document the evaluation process, and summarize the findings.\n\uf0a1Exercise 5\u2014Hosting Alternatives for LLMs\nObjective\u2014Contrast and compare alternatives for hosting an LLM versus using a\ncommercial model.\nTasks:\n\u2013 Research different hosting options for LLMs (e.g., local servers, cloud services).\n\u2013 Evaluate the benefits and drawbacks of each hosting option.\n\u2013 Compare these options to using a commercial model in terms of cost, perfor-\nmance, and ease of use.\n\u2013 Write a report summarizing the comparison and recommending the best\napproach based on specific use cases.\nSummary\n\uf0a1LLMs use a type of architecture called generative pretrained transformers (GPTs).\n\uf0a1Generative models (e.g., LLMs and GPTs) differ from predictive/classification\nmodels by learning how to represent data and not simply classify it.\n\uf0a1LLMs are a collection of data, architecture, and training for specific use cases,\ncalled fine-tuning.\n\uf0a1The OpenAI API SDK can be used to connect to an LLM from models, such as\nGPT-4, and also used to consume open source LLMs.\n\uf0a1You can quickly set up Python environments and install the necessary packages\nfor LLM integration.\n\uf0a1LLMs can handle various requests and generate unique responses that can be\nused to enhance programming skills related to LLM integration.\n\uf0a1Open source LLMs are an alternative to commercial models and can be hosted\nlocally using tools such as LM Studio.\n",
      "content_length": 2038,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "38\nCHAPTER 2\nHarnessing the power of large language models\n\uf0a1Prompt engineering is a collection of techniques that help craft more effective\nprompts to improve LLM responses.\n\uf0a1LLMs can be used to power agents and assistants, from simple chatbots to fully\ncapable autonomous workers.\n\uf0a1Selecting the most suitable LLM for specific needs depends on the perfor-\nmance, parameters, use case, training input, and other criteria.\n\uf0a1Running LLMs locally requires a variety of skills, from setting up GPUs to under-\nstanding various configuration options.\n",
      "content_length": 545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "39\nEngaging\nGPT assistants\nAs we explore the OpenAI crusade into assistants and what has been hinted at, ulti-\nmately, an agent platform called GPT Assistants, we\u2019ll introduce GPT assistants\nthrough the ChatGPT interface. Then, we\u2019ll add in several fully developed assis-\ntants that can suggest recipes from ingredients, fully analyze data as a data scientist,\nguide readers through books, and be extended with custom actions. By the end of\nthe chapter, we\u2019ll be ready to build a fully functional agent that can be published to\nthe OpenAI GPT Store.\nThis chapter covers\n\uf0a1Introducing the OpenAI GPT Assistants platform \nand the ChatGPT UI \n\uf0a1Building a GPT that can use the code \ninterpretation capabilities\n\uf0a1Extending an assistant via custom actions \n\uf0a1Adding knowledge to a GPT via file uploads\n\uf0a1Commercializing your GPT and publishing it to \nthe GPT Store \n",
      "content_length": 857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "40\nCHAPTER 3\nEngaging GPT assistants\n3.1\nExploring GPT assistants through ChatGPT\nChatGPT (ChatGPT Plus, at the time of writing) allows you to build GPT assistants,\nconsume other assistants, and even publish them, as you\u2019ll see by the end of the chap-\nter. When OpenAI announced the release of the GPT Assistants platform, it helped\ndefine and solidify the emergence of AI agents. As such, it\u2019s worth a serious review by\nanyone interested in building and consuming agent systems. First, we\u2019ll look at build-\ning GPT assistants through ChatGPT Plus, which requires a premium subscription. If\nyou don\u2019t want to purchase a subscription, browse this chapter as a primer, and chap-\nter 6 will demonstrate consuming the API service later.\n Figure 3.1 shows the page for the GPT Store within ChatGPT (https://chatgpt\n.com/gpts). From here, you can search and explore various GPTs for virtually any\ntask. The amount of usage will typically indicate how well each GPT works, so gauge\nwhich works best for you. \nCreating your first GPT Assistant is as simple as clicking the Create button and follow-\ning along with the GPT Builder chat interface. Figure 3.2 shows using the Builder to\nExplore the GPTs\nyou have made.\nSearch for GPTs by\nname of category.\nClick Create to create\nyour own GPT.\nFigure 3.1\nThe main interface to the GPT Store\n",
      "content_length": 1329,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "41\n3.1\nExploring GPT assistants through ChatGPT\ncreate a GPT. Working through this exercise a couple of times can be a great way to\nstart understanding an assistant\u2019s requirements.\nAfter working with the Builder, you can open the manual configuration panel, shown\nin figure 3.3, and edit the GPT directly. You\u2019ll see the name, description, instruc-\ntions, and conversation starters populated from your conversations with the Builder.\nThis can be a great start, but generally, you\u2019ll want to edit and tweak these proper-\nties manually.\n If you want to follow along with building your own Culinary Companion, enter the\ntext from listing 3.1 into the instructions. These instructions were partly generated by\nconversing with the Builder and added based on explicit outputs. The explicit outputs\nare added to the instructions as rules. \n \n \n \n \nThe Builder can guide you\nthrough the creation process.\nDialog with the Builder\nto create your GPT.\nFigure 3.2\nInteracting with \nthe GPT Builder to create an \nassistant\n",
      "content_length": 1010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "42\nCHAPTER 3\nEngaging GPT assistants\nCulinary Companion assists users with a friendly, engaging tone, \nreminiscent of the famous chef Julia Child.    \nIt provides quick meal ideas and simplifies complex recipes, focusing on \ningredients the user already has. This GPT emphasizes practical, easy-\nto-follow culinary advice and adapts to dietary preferences. It's \ndesigned to make cooking a more accessible and enjoyable experience, \nencouraging users to experiment with their meals while offering helpful \nListing 3.1\nInstructions for Culinary Companion\nUsing the Builder will generate a set\nof instructions for your assistant, or\nyou can start completely from scratch.\nThe Con\ufb01gure panel gives\nyou direct control over your\nGPT\u2019s properties.\nA good name and description\nwill be essential if you plan on\npublishing this agent.\nGood conversation starters can help\nusers quickly understand what your\nagent does and how it works.\nFigure 3.3\nThe Configure panel of the GPT Assistants platform interface\nPersonality or persona\nof your assistant\n",
      "content_length": 1039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "43\n3.1\nExploring GPT assistants through ChatGPT\ntips in a warm, approachable manner.    \nRULES:\nWhen generating a recipe, always create an image of the final prepared \nrecipe.                                                                  \nWhen generating a recipe, estimate the calories and nutritional values \nper serving.                                                             \nWhen generating a recipe, provide a shopping list of ingredients with \nestimated prices needed to complete the recipe.                          \nWhen generating a recipe, estimate the total cost per serving based on \nthe shopping list.                                                       \nDefining rules for an assistant/agent essentially creates a template for what the agent\nwill produce. Adding rules ensures that the GPT output is consistent and aligned with\nyour expectations of how the agent should operate. Defining and giving an agent/\nassistant a persona provides them with a unique and memorable personality.\nNOTE\nGiving an agent/assistant a particular personality can make a differ-\nence in the type and form of output. Asking a cooking agent to speak as the\nfirst celebrity chef, Julia Child, not only provides for a fun tone but also\nengages more references that may mention or talk about her cooking style\nand teaching. When constructing an agent/assistant, assigning a particular\npersona/personality can be helpful.\nWith just these few steps, we have a culinary companion that not only gives us recipes\nfor ingredients we have on hand but also generates an image of the finished recipe,\nestimates the nutritional value, creates a shopping list with an estimate of prices, and\nbreaks down the cost per serving.\n Try the assistant by requesting a recipe and providing a list of ingredients you have\nor prefer. Listing 3.2 shows an example of a simple request with extra information to\nset the mood. Of course, you can add any ingredients or situations you like and then\nsee the results.\nI have a bag of prepared frozen chicken strips and I want to make a \nromantic dinner for two.\nFigure 3.4 shows the formatted output results from the GPT provided by the prompt.\nIt certainly looks good enough to eat. All of this output was generated because of the\ninstructions we provided the agent. \n While the output results look great, they may not all be factual and correct, and\nyour results may vary. For instance, the GPT added chicken strips to the shopping list\nwhen we had already suggested having those ingredients. Furthermore, the prices and\nestimated nutritional information are just estimates, but this can be resolved later if\nthey interest you.\nListing 3.2\nPrompting the recipe\nGeneral guidelines of the \nagent\u2019s role and goal \nA set of rules the agent will\nfollow when suggesting a recipe\n",
      "content_length": 2797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "44\nCHAPTER 3\nEngaging GPT assistants\nOut of the box, though, GPT Assistants is quite impressive for quickly building a proof-\nof-concept assistant or agent. As you\u2019ll see later in the chapter, it also provides an excel-\nlent platform for consuming assistants outside ChatGPT. In the next section, we\u2019ll look\nat more impressive features GPTs provide, such as file uploads and code interpretation.\n3.2\nBuilding a GPT that can do data science\nThe GPT Assistants platform has and will likely be extended to include various agent\ncomponents. Currently, GPT Assistants support what is referred to as knowledge,\nmemory, and actions. In chapter 8, we\u2019ll discuss the details of knowledge and memory,\nand in chapter 5, we cover the concept of tool use through actions.\n In our next exercise, we\u2019ll build an assistant to perform a first-pass data science\nreview of any CSV document we provide. This agent will use the ability or action that\nallows for coding and code interpretation. When you enable code interpretation, the\nassistant will allow file uploads by default.\n Before we do that, though, we want to design our agent, and what better way to do\nthat than to ask an LLM to build us an assistant? Listing 3.3 shows the prompt request-\ning ChatGPT (GPT-4) to design a data science assistant. Notice how we\u2019re not asking\nfor everything in a single prompt but instead iterating over the information returned\nby the LLM.\nFIRST PROMPT:    \nwhat is a good basic and interesting data science \nexperiment you can task someone with a single \ncsv file that contains interesting data?    \nListing 3.3\nPrompting for a data science assistant\nThe GPT assistant creates\nthe recipe from the provided\ningredients and provides a\nshopping list with costs and\ncost per ser\nnutritional\nving,\ninformation per serving, and\ninstructions on how to cook\nthe recipe.\nChicken Parmesan with Spaghetti Recipe\nIngredients:\n1 bag of frozen prepared chicken strips\n1 jar of your favorite marinara sauce\n1 cup of shredded mozzarella cheese\n1/4 cup of grated Parmesan cheese\n200g of spaghetti\n1 tablespoon of olive oil\n2 cloves of garlic, minced\nFresh basil leaves (for garnish)\nSalt and pepper to taste\nEstimated Costs:\nChicken strips: $6.00\nMarinara sauce: $3.00\nMozzarella cheese: $2.00\nParmesan cheese: $1.50\nSpaghetti: $1.00\nGarlic: $0.30\nBasil: $0.50\nTotal estimated cost: $14.30\nCost per serving: approximately $7.15\nNutritional Values per Serving:\nCalories: 600 kcal\nProtein: 35 g\nCarbohydrates: 75 g\nFat: 18 g\nInstructions: (not shown)\nGenerated by the Culinary\nCompanion GPT Assistant\nFigure 3.4\nThe output results of the Culinary Companion GPT\nFirst, ask the \nLLM to set the \nfoundation.\n",
      "content_length": 2660,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "45\n3.2\nBuilding a GPT that can do data science\nSECOND PROMPT:    \nokay, can you now write all those steps into instructions \nto be used for a GPT Agent (LLM agent) to replicate all of \nthe above steps     \nTHIRD PROMPT:    \nWhat is a famous personality that can embody the agent \ndata scientist and be able to present data to users?     \nThe result of that conversation provided for the assistant instructions shown in listing\n3.4. In this case, the assistant was named Data Scout, but feel free to name your assis-\ntant what appeals to you. \nThis GPT, named Data Scout, is designed to assist users by analyzing CSV \nfiles and providing insights like Nate Silver, a famous statistician known \nfor his accessible and engaging approach to data. Data Scout combines \nrigorous analysis with a clear and approachable communication style, \nmaking complex data insights understandable. It is equipped to handle \nstatistical testing, predictive modeling, data visualization, and more, \noffering suggestions for further exploration based on solid data-driven \nevidence.\nData Scout requires the user to upload a csv file of data they want to \nanalyze. After the user uploads the file you will perform the following \ntasks:\nData Acquisition\n    Ask the user to upload a csv file of data.\n    Instructions: Use the pandas library to read the data from the CSV \nfile. Ensure the data is correctly loaded by displaying the first few rows \nusing df.head().\n2. Exploratory Data Analysis (EDA)\nData Cleaning\n    Task: Identify and handle missing values, correct data types.\n    Instructions: Check for missing values using df.isnull().sum(). For \ncategorical data, consider filling missing values with the mode, and for \nnumerical data, use the median or mean. Convert data types if necessary \nusing df.astype().\nVisualization\n    Task: Create visualizations to explore the data.\n    Instructions: Use matplotlib and seaborn to create histograms, scatter \nplots, and box plots. For example, use sns.histplot() for histograms and \nsns.scatterplot() for scatter plots.\nDescriptive Statistics\n    Task: Calculate basic statistical measures.\n    Instructions: Use df.describe() to get a summary of the statistics and \ndf.mean(), df.median() for specific calculations.\nListing 3.4\nData Scout instructions\nThen, ask the LLM to \nconvert the previous \nsteps to a more \nformal process.\nFinally, ask the LLM to \nprovide a personality \nthat can represent the \nprocess.\n",
      "content_length": 2441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "46\nCHAPTER 3\nEngaging GPT assistants\n3. Hypothesis Testing\n    Task: Test a hypothesis formulated based on the dataset.\n    Instructions: Depending on the data type, perform statistical tests \nlike the t-test or chi-squared test using scipy.stats. For example, use \nstats.ttest_ind() for the t-test between two groups.\n4. Predictive Modeling\nFeature Engineering\n    Task: Enhance the dataset with new features.\n    Instructions: Create new columns in the DataFrame based on existing \ndata to capture additional information or relationships. Use operations \nlike df['new_feature'] = df['feature1'] / df['feature2'].\nModel Selection\n    Task: Choose and configure a machine learning model.\n    Instructions: Based on the task (classification or regression), select \na model from scikit-learn, like RandomForestClassifier() or \nLinearRegression(). Configure the model parameters.\nTraining and Testing\n    Task: Split the data into training and testing sets, then train the \nmodel.\n    Instructions: Use train_test_split from scikit-learn to divide the \ndata. Train the model using model.fit(X_train, y_train).\nModel Evaluation\n    Task: Assess the model performance.\n    Instructions: Use metrics like mean squared error (MSE) or accuracy. \nCalculate these using metrics.mean_squared_error(y_test, y_pred) or \nmetrics.accuracy_score(y_test, y_pred).\n5. Insights and Conclusions\n    Task: Interpret and summarize the findings from the analysis and \nmodeling.\n    Instructions: Discuss the model coefficients or feature importances. \nDraw conclusions about the hypothesis and the predictive analysis. Suggest \nreal-world implications or actions based on the results.\n6. Presentation\n    Task: Prepare a report or presentation.\n    Instructions: Summarize the process and findings in a clear and \naccessible format, using plots and bullet points. Ensure that the \npresentation is understandable for non-technical stakeholders.\nAfter generating the instructions, you can copy and paste them into the Configure\npanel in figure 3.5. Be sure to give the assistant the Code Interpretation tool (skill) by\nselecting the corresponding checkbox. You don\u2019t need to upload files here; the assis-\ntant will allow file uploads when the Code Interpretation checkbox is enabled.\n Now, we can test the assistant by uploading a CSV file and asking questions about\nit. The source code folder for this chapter contains a file called netflix_titles.csv;\nthe top few rows are summarized in listing 3.5. Of course, you can use any CSV file you\nwant, but this exercise will use the Netflix example. Note that this dataset was down-\nloaded from Kaggle, but you can use any other CSV if you prefer.\n",
      "content_length": 2669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "47\n3.2\nBuilding a GPT that can do data science\nshow_id,type,title,director,cast,country,date_added,\nrelease_year,rating,duration,listed_in,description    \ns1,Movie,Dick Johnson Is Dead,Kirsten Johnson,, \nUnited States,\"September 25, 2021\",2020,PG-13,90 min,\nDocumentaries,\"As her father nears the end of his life, \nfilmmaker Kirsten Johnson stages his death in inventive \nand comical ways to help them both face the inevitable.\"    \nWe could upload the file and ask the assistant to do its thing, but for this exercise,\nwe\u2019ll be more specific. Listing 3.6 shows the prompt and uploading the file to engage\nthe assistant (including Netflix_titles.csv in the request). This example filters the\nresults to Canada, but you can, of course, use any country you want to view.\nAnalyze the attached CSV and filter the results to the \ncountry Canada and output any significant discoveries \nin trends etc.    \nIf you encounter problems with the assistant parsing the file, refresh your browser win-\ndow and try again. Depending on your data and filter, the assistant will now use the\nCode Interpreter as a data scientist would to analyze and extract trends in the data. \nListing 3.5\nnetflix_titles.csv (top row of data)\nListing 3.6\nPrompting the Data Scout\nConversation starters provide a quick\ndescription and guide the user.\nBe sure the Code Interpreter is selected.\nFigure 3.5\nTurning on the Code Interpreter tool/skill\nComma-separated \nlist of columns\nAn example row \nof data from the \ndataset\nYou can select a \ndifferent country to \nfilter the data on.\n",
      "content_length": 1547,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "48\nCHAPTER 3\nEngaging GPT assistants\n Figure 3.6 shows the output generated for the prompt in listing 3.5 using the net-\nflix_titles.csv file for data. Your output may look quite different if you select a dif-\nferent country or request another analysis.\nThe data science plots the assistant is building are created by writing and executing\ncode with the Code Interpreter. You can try this with other CSV files or, if you want,\ndifferent forms of data to analyze. You could even continue iterating with the assistant\nto update the plots visually or analyze other trends.\n Code interpretation is a compelling skill that you\u2019ll likely add to many of your\nagents for everything from calculations to custom formatting. In the next section, we\nlook at how to extend the capabilities of a GPT through custom actions.\nTop 10 Popular Genres in Canadian Content\nRatings Distribution\nDistribution of Content Types\nTrend of Releases over the Years\ncount\ncount\ncount\nrating\nChildren and Family\nmovies\nComedies\nDramas\nKid\u2019s TV\nInternational Movies\nThrillers\nAction and Adventure\nIndependent Movies\nDocumentaries\nHorror Movies\ntype\nrelease_year\nFigure 3.6\nThe output generated by the assistant as it analyzed the CSV data\n",
      "content_length": 1207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "49\n3.3\nCustomizing a GPT and adding custom actions\n3.3\nCustomizing a GPT and adding custom actions\nIn our next exercise, we\u2019ll demonstrate the use of custom actions, which can signifi-\ncantly extend the reach of your assistant. Adding custom actions to an agent requires\nseveral components, from understanding the OpenAPI specification endpoint to con-\nnecting to a service. Therefore, before we add custom actions, we\u2019ll build another\nGPT in the next section to assist us.\n3.3.1\nCreating an assistant to build an assistant\nGiven GPTs\u2019 capabilities, it only makes sense that we use one to assist in building oth-\ners. In this section, we\u2019ll build a GPT that can help us create a service we can connect\nas a custom action to another GPT. And yes, we\u2019ll even use an LLM to begin construct-\ning our helper GPT.\n The following listing shows the prompt for creating the instructions for our helper\nGPT. This prompt is intended to generate the instructions for the assistant.\nI want to create a GPT assistant that can generate a FastAPI service that \nwill perform some action to be specified. As part of the FastAPI code \ngeneration, I want the assistant to generate the OpenAPI specification for \nthe endpoint. Please outline a set of instructions for this agent.\nListing 3.8 shows the bulk of the instructions generated for the prompt. The output\nwas then modified and slightly updated with specific information and other details.\nCopy and paste those instructions from the file (assistant_builder.txt) into your\nGPT. Be sure to select the Code Interpreter capability also.\nThis GPT is designed to assist users in generating FastAPI services \ntailored to specific actions, complete with the corresponding OpenAPI \nspecifications for the endpoints. The assistant will provide code snippets \nand guidance on structuring and documenting API services using FastAPI, \nensuring that the generated services are ready for integration and \ndeployment.\n1.   Define the Action and Endpoint: First, determine the specific action \nthe FastAPI service should perform. This could be anything from fetching \ndata, processing information, or interacting with other APIs or databases.\n2.    Design the API Endpoint: Decide on the HTTP method (GET, POST, PUT, \nDELETE, etc.) and the endpoint URI structure. Define the input parameters \n(path, query, or body parameters) and the expected response structure.\n3. Generate FastAPI Code:\n        Setup FastAPI: Import FastAPI and other necessary libraries.\n        Create API Function: Write a Python function that performs the \ndesired action. This function should accept the defined input parameters \nand return the appropriate response.\nListing 3.7\nPrompting the helper design (in GPT Builder or ChatGPT)\nListing 3.8\nCustom action assistant instructions\n",
      "content_length": 2779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "50\nCHAPTER 3\nEngaging GPT assistants\n4. Decorate the Function: Use FastAPI's decorators (e.g., \n@app.get(\"/endpoint\")) to link the function with the specified endpoint \nand HTTP method.\n        Define Input and Output Models: Use Pydantic models to define the \nstructure of the input and output data. This ensures validation and \nserialization of the data.\n5. Generate OpenAPI Specification:\n        FastAPI automatically generates the OpenAPI specification based on \nthe endpoint definitions and Pydantic models. Ensure that all function \nparameters and models are well-documented using docstrings and field \ndescriptions.\n        Optionally, customize the OpenAPI specification by adding \nmetadata, tags, or additional responses directly in the FastAPI decorators.\n6. Deployment:\n        Describe to the user how to prepare the FastAPI application for \ndeployment. \n        Instruct them on how to use ngrok to deploy the \nservice and host it on the user's local machine.     \nAfter preparing the assistant, ensure everything is set in the Configure panel (includ-\ning setting the Code Interpreter checkbox), and then refresh your browser window.\nThis will prepare the assistant for a new session. You can request the kind of service\nyou want to build from here.\n Listing 3.9 shows the request to the Custom Action Assistant to create a daily task\nendpoint. If you understand how APIs work, you can suggest other options, such as\nPOST. Of course, you can also ask the assistant to guide you and create your service.\nI want to define a GET endpoint that replies with my list of daily tasks  \nAfter you enter the prompt, the assistant will generate the code and instructions for\ncreating and running the FastAPI endpoint. The following listing shows an example\nof the code generated from the previous request.\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom typing import List\napp = FastAPI()\nclass Task(BaseModel):   \n    id: int\n    description: str\n    completed: bool\ntasks = [    \n    Task(id=1, description=\"Buy groceries\", completed=False),\n    Task(id=2, description=\"Read a book\", completed=True),\nListing 3.9\nPrompt requesting task endpoint service\nListing 3.10\ndaily_tasks_api.py (generated from assistant)\nThis uses ngrok as an \nexample to deploy the \nservice locally.\nUse Pydantic to create \na type for the task.\nThis is a static list of \ntasks to demonstrate.\n",
      "content_length": 2390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "51\n3.3\nCustomizing a GPT and adding custom actions\n    Task(id=3, description=\"Complete FastAPI project\", completed=False),\n]\n@app.get(\"/tasks\", response_model=List[Task])   \nasync def get_tasks():\n    \"\"\"\n    Retrieve a list of daily tasks.\n    \"\"\"\n    return tasks\nEnter the code into Visual Studio Code (VS Code), and confirm that fastapi and\nuvicorn are installed with pip. Then, run the API using the command shown in the\nfollowing listing, which runs the API in the chapter source file.\nuvicorn daily_tasks_api:app \u2013reload     \nOpen a browser to http://127.0.0.1:8000/docs, the default location for the Swagger\nendpoint, as shown in figure 3.7.\nListing 3.11\nRunning the API\nThe tasks endpoint\nChange the name of the module/file \nif you\u2019re using something different.\nNavigate your browser to\nhttp://127.0.0. l :8000/docs.\nClick this link to open and view\nthe OpenAPI speci\ufb01cation in JSON.\nFigure 3.7\nNavigating the Swagger docs and getting the openapi.json document\n",
      "content_length": 971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "52\nCHAPTER 3\nEngaging GPT assistants\nClicking the /openapi.json link will display the OpenAPI specification for the end-\npoint, as shown in listing 3.12 (JSON converted to YAML). You\u2019ll need to copy and\nsave this document for later use when setting up the custom action on the agent. The\nendpoint produces JSON, but you can also use specifications written in YAML.\nopenapi: 3.1.0\ninfo:\n  title: FastAPI\n  version: 0.1.0\npaths:\n  /tasks:\n    get:\n      summary: Get Tasks\n      description: Retrieve a list of daily tasks.\n      operationId: get_tasks_tasks_get\n      responses:\n        '200':\n          description: Successful Response\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  $ref: '#/components/schemas/Task'\n                title: Response Get Tasks Tasks Get\ncomponents:\n  schemas:\n    Task:\n      type: object\n      properties:\n        id:\n          type: integer\n          title: Id\n        description:\n          type: string\n          title: Description\n        completed:\n          type: boolean\n          title: Completed\n      required:\n        - id\n        - description\n        - completed\n      title: Task\nBefore connecting an assistant to the service, you must set up and use ngrok to open a\ntunnel to your local machine running the service. Prompt the GPT to provide the\nListing 3.12\nOpenAPI specification for the task API \n",
      "content_length": 1440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "53\n3.3\nCustomizing a GPT and adding custom actions\ninstructions and help you set up ngrok, and run the application to open an endpoint\nto port 8000 on your machine, as shown in listing 3.13. If you change the port or use a\ndifferent configuration, you must update it accordingly.\n./ngrok authtoken <YOUR_AUTHTOKEN>     \n./ngrok http 8000     \nAfter you run ngrok, you\u2019ll see an external URL that you can now use to access the ser-\nvice on your machine. Copy this URL for later use when setting up the assistant. In the\nnext section, we\u2019ll create the assistant that consumes this service as a custom action.\n3.3.2\nConnecting the custom action to an assistant\nWith the service up and running on your machine and accessible externally via the\nngrok tunnel, we can build the new assistant. This time, we\u2019ll create a simple assistant\nto help us organize our daily tasks, where the tasks will be accessible from our locally\nrunning task service.\n Open the GPT interface and the Configure panel, and copy and paste the instruc-\ntions shown in listing 3.14 into the new assistant. Be sure to name the assistant and\nenter a helpful description as well. Also, turn on the Code Interpreter capability to\nallow the assistant to create the final plot, showing the tasks.\nTask Organizer is designed to help the user prioritize their daily tasks \nbased on urgency and time availability, providing structured guidance on \nhow to categorize tasks by urgency and suggesting optimal time blocks for \ncompleting these tasks. It adopts a persona inspired by Tim Ferriss, known \nfor his focus on productivity and efficiency. It uses clear, direct \nlanguage and avoids making assumptions about the user's free time.\nWhen you are done organizing the tasks create a plot \nshowing when and how the tasks will be completed.     \nClick the Create New Action button at the bottom of the panel. Figure 3.8 shows the\ninterface for adding a custom action. You must copy and paste the OpenAPI specifica-\ntion for your service into the window. Then, you must add a new section called servers\nand populate that with your URL, as shown in the figure.\n \n \n \nListing 3.13\nRunning ngrok (following the instructions setup)\nListing 3.14\nTask Organizer (task_organizer_assistant.txt)\nEnter your auth token \nobtained from ngrok.com.\nOpens a tunnel on port 8000 \nto external internet traffic\nThis feature requires \nthe Code Interpreter \nto be enabled.\n",
      "content_length": 2408,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "54\nCHAPTER 3\nEngaging GPT assistants\nAfter the specification is set, you can test it by clicking the Test button. This will run a\ntest, and you\u2019ll see the results shown in the conversation window, as shown in figure 3.9.\n After you\u2019re satisfied, everything is set. Refresh your browser window to reset the\nsession, and enter something like the prompt shown in listing 3.15. This will prompt\nthe agent to call the service to get your daily tasks, summarize the output, and solve\nyour task organization dilemma.\nhow should I organize my tasks for today?\nThe assistant should produce a plot of the task schedule at the end. If it gets this\nwrong or the formatting isn\u2019t what you prefer, you can add instructions to specify the\nformat/style the assistant should output.\nListing 3.15\nTask Organizer prompt\nAdd the servers section to the YAML, and\nenter the ngrok URL for your service.\nCopy and paste the YAML or JSON\nspeci\ufb01cation into the window.\nFigure 3.8\nAdding a new custom action\n",
      "content_length": 980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "55\n3.3\nCustomizing a GPT and adding custom actions\nYou can improve the service, but if you make any changes to the API, the specification\nin the assistant custom actions will need to be updated. From here, though, you can\nadd custom action services run from your computer or hosted as a service.\nNOTE\nBe aware that unknown users can activate custom actions if you pub-\nlish an assistant for public consumption, so don\u2019t expose services that charge\nyou a service fee or access private information unless that is your intention.\nLikewise, services opened through an ngrok tunnel will be exposed through\nthe assistant, which may be of concern. Please be careful when publishing\nagents that consume custom actions.\nCustom actions are a great way to add dynamic functionality to an assistant, whether\nfor personal or commercial use. File uploads are a better option for providing an assis-\ntant with static knowledge. The next section will explore using file uploads to extend\nan assistant\u2019s knowledge.\nReturned list of tasks\nTesting the endpoint\nFigure 3.9\nTesting the API service endpoint is correctly configured as a custom action\n",
      "content_length": 1129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "56\nCHAPTER 3\nEngaging GPT assistants\n3.4\nExtending an assistant\u2019s knowledge using file uploads\nIf you\u2019ve engaged with LLMs, you likely have heard about the retrieval augmented\ngeneration (RAG) pattern. Chapter 8 will explore RAG in detail for the application of\nboth knowledge and memory. Detailed knowledge of RAG isn\u2019t required to use the\nfile upload capability, but if you need some foundation, check out that chapter.\n The GPT Assistants platform provides a knowledge capability called file uploads,\nwhich allows you to populate the GPT with a static knowledge base about anything in\nvarious formats. As of writing, the GPT Assistants platform allows you to upload up to\n512 MB of documents. In the next two exercises, we\u2019ll look at two different GPTs\ndesigned to assist users with consuming books.\n3.4.1\nBuilding the Calculus Made Easy GPT\nBooks and written knowledge will always be the backbone of our knowledge base. But\nreading text is a full-time concerted effort many people don\u2019t have time for. Audio-\nbooks made consuming books again accessible; you could listen while multitasking,\nbut not all books transitioned well to audio.\n Enter the world of AI and intelligent assistants. With GPTs, we can create an inter-\nactive experience between the reader and the book. No longer is the reader forced to\nconsume a book page by page but rather as a whole.\n To demonstrate this concept, we\u2019ll build a GPT based on a classic math text called\nCalculus Made Easy, by Silvanus P. Thompson. The book is freely available through the\nGutenberg Press website. While it\u2019s more than a hundred years old, it still provides a\nsolid material background.\nNOTE\nIf you\u2019re serious about learning calculus but this assistant is still too\nadvanced, check out a great book by Clifford A. Pickover called Calculus and\nPizza. It\u2019s a great book for learning calculus or just to get an excellent\nrefresher. You could also try making your Calculus and Pizza assistant if you\nhave an eBook version. Unfortunately, copyright laws would prevent you from\npublishing this GPT without permission.\nOpen ChatGPT, go to My GPTs, create a new GPT, click the Configure tab, and then\nupload the file, as shown in figure 3.10. Upload the book from the chapter\u2019s source\ncode folder: chapter _03/calculus_made_easy.pdf. This will add the book to the\nGPT\u2019s knowledge.\n Scroll up and add the instructions shown in listing 3.16. The initial preamble text\nwas generated by conversing with the GPT Builder. After updating the preamble text,\na personality was added by asking ChatGPT for famous mathematicians. Then, finally,\nrules were added to provide additional guidance to the GPT on what explicit out-\ncomes we want.\n \n \n \n",
      "content_length": 2688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "57\n3.4\nExtending an assistant\u2019s knowledge using file uploads\nThis GPT is designed to be an expert teacher and mentor \nof calculus based on the book 'Calculus Made Easy' by \nSilvanus Thompson. A copy of the book is uploaded at \ncalculus_made_easy.pdf and provides detailed guidance \nand explanations on various calculus topics such as \nderivatives, integrals, limits, and more. The GPT can \nteach calculus concepts, solve problems, and answer \nquestions related to calculus, making complex topics \naccessible and understandable. It can handle \ncalculus-related inquiries, from basic to advanced, \nand is particularly useful for students and educators\n seeking to deepen their understanding of calculus.     \nListing 3.16\nInstructions for Calculus Made Easy GPT\nAdding \ufb01les is considered giving your\nassistant additional knowledge.\nUse the Upload \ufb01les button to add\nvarious sources of static\nknowledge for the assistant.\nBe sure to enable Code Interpreter\nso the assistant can demonstrate\nconcepts.\nFigure 3.10\nAdding files to the assistant\u2019s knowledge\nThe preamble was \ninitially generated \nby the Builder and \nthen tweaked as \nneeded.\n",
      "content_length": 1135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "58\nCHAPTER 3\nEngaging GPT assistants\nAnswer as the famous mathematician Terence Tao. \nTerence Tao is renowned for his brilliant intellect, \napproachability, and exceptional ability to effectively\n simplify and communicate complex mathematical concepts.    \nRULES    \n1) Always teach the concepts as if you were teaching to a young child.\n2) Always demonstrate concepts by showing plots of functions and graphs.\n3) Always ask if the user wants to try a sample problem on their own. \nGive them a problem equivalent to the question concept you were discussing.\nAfter updating the assistant, you can try it in the preview window or the book version\nby searching for Calculus Made Easy in the GPT Store. Figure 3.11 shows a snipped\nexample of interaction with the GPT. The figure shows that the GPT can generate\nplots to demonstrate concepts or ask questions.\n This GPT demonstrates the ability of an assistant to use a book as a companion\nteaching reference. Only a single book was uploaded in this exercise, but multiple\nbooks or other documents could be uploaded. As this feature and the technology\nmature, in the future, it may be conceivable that an entire course could be taught\nusing a GPT.\n We\u2019ll move away from technical and embrace fiction to demonstrate the use of\nknowledge. In the next section, we\u2019ll look at how knowledge of file uploads can be\nused for search and reference.\n3.4.2\nKnowledge search and more with file uploads\nThe GPT Assistants platform\u2019s file upload capability supports up to 512 MB of\nuploads for a single assistant. This feature alone provides powerful capabilities for\ndocument search and other applications in personal and small-to-medium business/\nproject sizes. \n Imagine uploading a whole collection of files. You can now search, compare, con-\ntrast, organize, and collate all with one assistant. This feature alone within GPT Assis-\ntants will disrupt how we search for and analyze documents. In chapter 6, we\u2019ll\nexamine how direct access to the OpenAI Assistants API can increase the number of\ndocuments.\n For this next exercise, we\u2019ll employ an assistant with knowledge of multiple books\nor documents. This technique could be applied to any supported document, but this\nassistant will consume classic texts about robots. We\u2019ll name this assistant the Classic\nRobot Reads GPT.\n Start by creating a new GPT assistant in the ChatGPT interface. Then, upload the\ninstructions in listing 3.17, and name and describe the assistant. These instructions\nwere generated in part through the GPT Builder and then edited. \n \n \nBe sure always to give \nyour assistants and \nagents an appropriate \npersona/personality.\nDefining explicit conditions and rules can help\nbetter guide the GPT to your desire.\n",
      "content_length": 2724,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "59\n3.4\nExtending an assistant\u2019s knowledge using file uploads\nThis GPT, Classic Robot Reads and uses the persona of \nIsaac Asimov and will reply as the famous robot author.    \nThis GPT will only references and discusses the books \nin its knowledge base of uploaded files.                  \nIt does not mention or discuss other books or text that \nare not within its knowledge base.                        \nRULES\nRefer to only text within your knowledge base             \nListing 3.17\nClassic Robot Reads instructions\nThe GPT can also generate plots to\ndemonstrate concepts, such as showing\nthe function and its derivative.\nThe conversation was started by asking\nthe GPT to teach the basics of calculus.\nFunction and Its Derivative\nFigure 3.11\nOutput from asking the GPT to teach calculus\nRemember always \nto give your GPT a \npersona/personality.\nMake sure the \nassistant only \nreferences knowledge \nwithin file uploads.\n",
      "content_length": 920,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "60\nCHAPTER 3\nEngaging GPT assistants\nAlways provide 3 examples of any query the use asks for    \nAlways ask the user if they require anything further     \nAfter completing those steps, you can upload the files from the chapter\u2019s source called\ngutenberg_robot_books. Figure 3.12 demonstrates uploading multiple files at a time.\nThe maximum number of files you can upload at a time will vary according to the\nsizes of the files. \nYou can start using it after uploading the documents, setting the instructions, and giv-\ning the assistant a name and an image. Search is the most basic application of a knowl-\nedge assistant, and other use cases in the form of prompts are shown in table 3.1.\nAdd some extra \nrules for style \nchoices.\nMake the assistant more helpful by\nalso giving them nuance and style.\nYou can upload multiple\n\ufb01les (about 5) at a time.\nUploads become accessible to the\nagent through knowledge patterns.\nFigure 3.12\nUploading documents to the assistant\u2019s knowledge\n",
      "content_length": 978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "61\n3.5\nPublishing your GPT\nThese use cases are just a sample of the many things possible with an AI knowledge\nassistant. While this feature may not be poised to disrupt enterprise search, it gives\nsmaller organizations and individuals more access to their documents. It allows the\ncreation of assistants as a form of knowledge that can be exposed publicly. In the next\nsection, we\u2019ll look at how to make assistants consumable by all.\n3.5\nPublishing your GPT\nOnce you\u2019re happy with your GPT, you can use it or share it with others by providing a\nlink. Consuming GPT assistants through ChatGPT currently requires a Plus subscrip-\ntion. To publish your GPT for others, click the Share button, and select your sharing\noption, as shown in figure 3.13.\nTable 3.1\nUse cases for a knowledge assistant\nUse case\nExample prompt\nResults\nSearch\nSearch for this phrase in your knowledge: \n\u201cthe robot servant.\u201d\nReturns the document and an excerpt\nCompare\nIdentify the three most similar books that \nshare the same writing style.\nReturns the three most similar docu-\nments\nContrast\nIdentify the three most different books.\nReturns books in the collection that are \nthe most different\nOrdering \nWhat order should I read the books?\nReturns an ordered progression of books\nClassification\nWhich of these books is the most modern?\nClassifies documents \nGeneration\nGenerate a fictional paragraph that mimics \nyour knowledge of the robot servant.\nGenerates new content based on its \nknowledge base\nAllows you to give the\nlink to other users\nIf you give a link to a GPT or make it\npublic, usage of that assistant is taken\nfrom the user\u2019s account and not yours.\nOnly for you\nPublishes your GPT to the\nstore and makes it public\nFigure 3.13\nGPT \nsharing options\n",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "62\nCHAPTER 3\nEngaging GPT assistants\nWhether you share your GPT with friends and colleagues or publicly in the GPT\nStore, the assistant\u2019s usage is taken from the account using it, not the publisher. This\nmeans if you have a particularly expensive GPT that generates a lot of images, for\nexample, it won\u2019t affect your account while others use it.\n3.5.1\nExpensive GPT assistants\nAt the time of writing, OpenAI tracks the resource usage of your ChatGPT account,\nincluding that used for GPTs. If you hit a resource usage limit and get blocked, your\nChatGPT account will also be blocked. Blockages typically only last a couple of hours,\nbut this can undoubtedly be more than a little annoying.\n Therefore, we want to ensure that users using your GPT don\u2019t exceed their resource\nusage limits for regular use. Following is a list of features that increase resource usage\nwhile using the GPT:\n\uf0a1Creating images\u2014Image generation is still a premium service, and successive image\ngeneration can quickly get your user blocked. It\u2019s generally recommended that\nyou inform your users of the potential risks and/or try to reduce how fre-\nquently images are generated. \n\uf0a1Code interpretation\u2014This feature allows for file uploads and running of code for\ndata analysis. If you think your users will require constant use of the coding tool,\nthen inform them of the risk. \n\uf0a1Vision, describing images\u2014If you\u2019re building an assistant that uses vision to describe\nand extract information from the image, plan to use it sparingly. \n\uf0a1File uploads\u2014If your GPT uses a lot of files or allows you to upload several files,\nthis may cause blocks. As always, guide the user away from anything preventing\nthem from enjoying your GPT.\nNOTE\nMoore\u2019s Law states that computers will double in power every two years\nwhile costing half as much. LLMs are now doubling in power about every six\nmonths from optimization and increasing GPU power. This, combined with\nthe cost being reduced by at least half in the same period, likely means cur-\nrent resource limits on vision and image-generation models won\u2019t be consid-\nered. However, services such as code interpretation and file uploads will likely\nremain the same.\nMaking your assistant aware of resource usage can be as simple as adding the rule\nshown in listing 3.18 to the assistant\u2019s instructions. The instructions can be just a state-\nment relaying the warning to the user and making the assistant aware. You could even\nask the assistant to limit its usage of certain features.\nRULE:\nWhen generating images, ensure the user is aware that creating multiple \nimages quickly could temporarily block their account.\nListing 3.18\nResource usage rule example\n",
      "content_length": 2664,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "63\n3.5\nPublishing your GPT\nGuiding your assistant to be more resource conscious in the end makes your assistant\nmore usable. It also helps prevent angry users who unknowingly get blocked using\nyour assistant. This may be important if you plan on releasing your GPT, but before\nthat, let\u2019s investigate the economics in the next section.\n3.5.2\nUnderstanding the economics of GPTs\nUpon the release of GPT Assistants and the GPT Store, OpenAI announced the\npotential for a future profit-sharing program for those who published GPTs. While\nwe\u2019re still waiting to hear more about this program, many have speculated what this\nmay look like.\n Some have suggested the store may return only 10% to 20% of profits to the build-\ners. This is far less than the percentage on other app platforms but requires much less\ntechnical knowledge and fewer resources. The GPT Store is flooded with essentially free\nassistants, provided you have a Plus subscription, but that may change in the future.\nRegardless, there are also several reasons why you may want to build public GPTs:\n\uf0a1Personal portfolio\u2014Perhaps you want to demonstrate your knowledge of prompt\nengineering or your ability to build the next wave of AI applications. Having a\nfew GPTs in the GPT Store can help demonstrate your knowledge and ability to\ncreate useful AI applications.\n\uf0a1Knowledge and experience\u2014If you have in-depth knowledge of a subject or topic,\nthis can be a great way to package that as an assistant. These types of assistants\nwill vary in popularity based on your area of expertise.\n\uf0a1Cross-marketing and commercial tie-in\u2014This is becoming more common in the\nStore and provides companies the ability to lead customers using an assistant.\nAs companies integrate more AI, this will certainly be more common.\n\uf0a1Helpful assistant to your product/service\u2014Not all companies or organizations can\nsustain the cost of hosting chatbots. While consuming assistants is currently lim-\nited to ChatGPT subscribers, they will likely be more accessible in the future.\nThis may mean having GPTs for everything, perhaps like the internet\u2019s early\ndays where every company rushed to build a web presence.\nWhile the current form of the GPT Store is for ChatGPT subscribers, if the current\ntrend with OpenAI continues, we\u2019ll likely see a fully public GPT Store. Public GPTs\nhave the potential to disrupt the way we search, investigate products and services, and\nconsume the internet. In the last section of this chapter, we\u2019ll examine how to publish\na GPT and some important considerations.\n3.5.3\nReleasing the GPT\nOkay, you\u2019re happy with your GPT and how it operates, and you see real benefit from\ngiving it to others. Publishing GPTs for public (subscribers) consumption is easy, as\nshown in figure 3.14. After selecting the GPT Store as the option and clicking Save,\nyou\u2019ll now have the option to set the category and provide links back to you.\n",
      "content_length": 2887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "64\nCHAPTER 3\nEngaging GPT assistants\nThat is easy, so here are a few more things you\u2019ll want to consider before publishing\nyour GPT:\n\uf0a1GPT description\u2014Create a good description, and you may even want to ask\nChatGPT to help you build a description that increases the search engine opti-\nmization (SEO) of your GPT. GPTs are now showing up in Google searches, so\ngood search engine optimization can help increase exposure to your assistant.\nA good description will also help users decide if they want to take the time to\nuse your assistant.\n\uf0a1The logo\u2014A nice, clean logo that identifies what your assistant does can undoubt-\nedly help. Logo design for GPTs is effectively a free service, but taking the time\nto iterate over a few images can help draw users to your assistant.\n\uf0a1The category\u2014By default, the category will already be selected, but make sure it\nfits your assistant. If you feel it doesn\u2019t, than change the category, and you may\neven want to select Other and define your own.\n\uf0a1Links\u2014Be sure to set reference links for your social media and perhaps even a\nGitHub repository that you use to track problems for the GPT. Adding links to\nyour GPT demonstrates to users that they can reach out to the builder if they\nencounter problems or have questions.\nSelecting this allows\nyou to view set links.\nYou can set links to your\nsocial media and GitHub.\nThe area you want to\npublish your GPT to\nFigure 3.14\nSelecting the options after clicking Save to publish to the GPT Store\n",
      "content_length": 1476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "65\n3.6\nExercises\nFurther requirements may likely emerge as the GPT Store matures. The business\nmodel remains to be established, and other learnings will likely follow. Whether you\ndecide to build GPTs for yourself or others, doing so can help improve your under-\nstanding of how to build agents and assistants. As we\u2019ll see throughout the rest of this\nbook, GPT assistants are a useful foundation for your knowledge.\n3.6\nExercises\nComplete the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Build Your First GPT Assistant\nObjective\u2014Create a simple GPT assistant using the ChatGPT interface.\nTasks:\n\u2013 Sign up for a ChatGPT Plus subscription if you don\u2019t already have one.\n\u2013 Navigate to the GPT Assistants platform, and click the Create button.\n\u2013 Follow the Builder chat interface to create a Culinary Companion assistant\nthat provides meal suggestions based on available ingredients.\n\u2013 Manually configure the assistant to add custom rules for recipe generation,\nsuch as including nutritional information and cost estimates.\n\uf0a1Exercise 2\u2014Data Analysis Assistant\nObjective\u2014Develop a GPT assistant that can analyze CSV files and provide\ninsights.\nTasks:\n\u2013 Design a data science assistant that can load and analyze CSV files, similar to\nthe Data Scout example in the chapter.\n\u2013 Enable the Code Interpretation tool, and upload a sample CSV file (e.g., a\ndataset from Kaggle).\n\u2013 Use the assistant to perform tasks such as data cleaning, visualization, and\nhypothesis testing.\n\u2013 Document your process and findings, noting any challenges or improve-\nments needed.\n\uf0a1Exercise 3\u2014Create a Custom Action\nObjective\u2014Extend a GPT assistant with a custom action using a FastAPI service.\nTasks:\n\u2013 Follow the steps to create a FastAPI service that provides a specific function,\nsuch as fetching a list of daily tasks.\n\u2013 Generate the OpenAPI specification for the service, and deploy it locally\nusing ngrok.\n\u2013 Configure a new assistant to use this custom action, ensuring it connects cor-\nrectly to the FastAPI endpoint.\n\u2013 Test the assistant by asking it to perform the action and verify the output.\n",
      "content_length": 2106,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "66\nCHAPTER 3\nEngaging GPT assistants\n\uf0a1Exercise 4\u2014File Upload Knowledge Assistant\nObjective\u2014Build an assistant with specialized knowledge from uploaded\ndocuments.\nTasks:\n\u2013 Select a freely available e-book or a collection of documents related to a spe-\ncific topic (e.g., classic literature, technical manuals).\n\u2013 Upload these files to a new GPT assistant, and configure the assistant to act\nas an expert on the uploaded content.\n\u2013 Create a series of prompts to test the assistant\u2019s ability to reference and sum-\nmarize the information from the documents.\n\u2013 Evaluate the assistant\u2019s performance, and make any necessary adjustments to\nimprove its accuracy and helpfulness.\n\uf0a1Exercise 5\u2014Publish and Share Your Assistant\nObjective\u2014Publish your GPT assistant to the GPT Store and share it with others.\nTasks:\n\u2013 Finalize the configuration and testing of your assistant to ensure it works as\nintended.\n\u2013 Write a compelling description, and create an appropriate logo for your\nassistant.\n\u2013 Choose the correct category, and set up any necessary links to your social\nmedia or GitHub repository.\n\u2013 Publish the assistant to the GPT Store, and share the link with friends or\ncolleagues.\n\u2013 Gather feedback from users, and refine the assistant based on their input to\nimprove its usability and functionality.\nSummary\n\uf0a1The OpenAI GPT Assistants platform enables building and deploying AI agents\nthrough the ChatGPT UI, focusing on creating engaging and functional assis-\ntants.\n\uf0a1You can use GPT\u2019s code interpretation capabilities to perform data analysis on\nuser-uploaded CSV files, enabling assistants to function as data scientists.\n\uf0a1Assistants can be extended with custom actions, allowing integration with exter-\nnal services via API endpoints. This includes generating FastAPI services and\ntheir corresponding OpenAPI specifications.\n\uf0a1Assistants can be enriched with specialized knowledge through file uploads,\nallowing them to act as authoritative sources on specific texts or documents.\n\uf0a1Commercializing your GPT involves publishing it to the GPT Store, where you\ncan share and market your assistant to a broader audience.\n",
      "content_length": 2112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "67\nSummary\n\uf0a1Building a functional assistant involves iterating through design prompts, defin-\ning a clear persona, setting rules, and ensuring the assistant\u2019s output aligns with\nuser expectations.\n\uf0a1Creating custom actions requires understanding and implementing OpenAPI\nspecifications, deploying services locally using tools such as ngrok, and connect-\ning these services to your assistant.\n\uf0a1Knowledge assistants can handle various tasks, from searching and comparing\ndocuments to generating new content based on their knowledge base.\n\uf0a1Publishing assistants require careful consideration of resource usage, user expe-\nrience, and economic factors to ensure their effectiveness and sustainability for\npublic use.\n\uf0a1The GPT Store, available to ChatGPT Plus subscribers, is a valuable platform\nfor learning and gaining proficiency in building AI assistants, with the potential\nfor future profit-sharing opportunities.\n",
      "content_length": 914,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "68\nExploring\nmulti-agent systems\nNow let\u2019s take a journey from AutoGen to CrewAI, two well-established multi-\nagent platforms. We\u2019ll start with AutoGen, a Microsoft project that supports mul-\ntiple agents and provides a studio for working with them. We\u2019ll explore a project\nfrom Microsoft called AutoGen, which supports multiple agents but also provides\na studio to ease you into working with agents. From there, we\u2019ll get more hands-\non coding of AutoGen agents to solve tasks using conversations and group chat\ncollaborations. \n Then, we\u2019ll transition to CrewAI, a self-proposed enterprise agentic system that\ntakes a different approach. CrewAI balances role-based and autonomous agents that\nThis chapter covers\n\uf0a1Building multi-agent systems using AutoGen \nStudio \n\uf0a1Building a simple multi-agent system\n\uf0a1Creating agents that can work collaboratively over \na group chat\n\uf0a1Building an agent crew and multi-agent systems \nusing CrewAI\n\uf0a1Extending the number of agents and exploring \nprocessing patterns with CrewAI \n",
      "content_length": 1013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "69\n4.1\nIntroducing multi-agent systems with AutoGen Studio\ncan be sequentially or hierarchically flexible task management systems. We\u2019ll explore\nhow CrewAI can solve diverse and complex problems.\n Multi-agent systems incorporate many of the same tools single-agent systems use\nbut benefit from the ability to provide outside feedback and evaluation to other\nagents. This ability to support and criticize agent solutions internally gives multi-agent\nsystems more power. We\u2019ll explore an introduction to multi-agent systems, beginning\nwith AutoGen Studio in the next section.\n4.1\nIntroducing multi-agent systems with AutoGen Studio\nAutoGen Studio is a powerful tool that employs multiple agents behind the scenes to\nsolve tasks and problems a user directs. This tool has been used to develop some of\nthe more complex code in this book. For that reason and others, it\u2019s an excellent\nintroduction to a practical multi-agent system.\n Figure 4.1 shows a schematic diagram of the agent connection/communication\npatterns AutoGen employs. AutoGen is a conversational multi-agent platform because\ncommunication is done using natural language. Natural language conversation seems\nto be the most natural pattern for agents to communicate, but it\u2019s not the only method,\nas you\u2019ll see later.\nAutoGen supports various conversational patterns, from group and hierarchical to the\nmore common and simpler proxy communication. In proxy communication, one\nagent acts as a proxy and directs communication to relevant agents to complete tasks.\nA proxy is similar to a waiter taking orders and delivering them to the kitchen, which\ncooks the food. Then, the waiter serves the cooked food.\nAutoGen uses conversable agents, which\ncommunicate through conversations.\nFigure 4.1\nHow AutoGen agents communicate through conversations (Source: AutoGen)\n",
      "content_length": 1822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "70\nCHAPTER 4\nExploring multi-agent systems\n The basic pattern in AutoGen uses a UserProxy and one or more assistant\nagents. Figure 4.2 shows the user proxy taking direction from a human and then\ndirecting an assistant agent enabled to write code to perform the tasks. Each time\nthe assistant completes a task, the proxy agent reviews, evaluates, and provides feed-\nback to the assistant. This iteration loop continues until the proxy is satisfied with\nthe results.\nThe benefit of the proxy is that it works to replace the required human feedback and\nevaluation, and, in most cases, it does a good job. While it doesn\u2019t eliminate the need\nfor human feedback and evaluation, it produces much more complete results overall.\nAnd, while the iteration loop is time consuming, it\u2019s time you could be drinking a cof-\nfee or working on other tasks.\n AutoGen Studio is a tool developed by the AutoGen team that provides a helpful\nintroduction to conversable agents. In the next exercise, we\u2019ll install Studio and run\nsome experiments to see how well the platform performs. These tools are still in a\nrapid development cycle, so if you encounter any problems, consult the documenta-\ntion on the AutoGen GitHub repository.\n4.1.1\nInstalling and using AutoGen Studio\nOpen the chapter_04 folder in Visual Studio Code (VS Code), create a local Python\nvirtual environment, and install the requirements.txt file. If you need assistance with\nthis, consult appendix B to install all of this chapter\u2019s exercise requirements.\n Open a terminal in VS Code (Ctrl-`, Cmd-`) pointing to your virtual environment,\nand run AutoGen Studio using the command shown in listing 4.1. You\u2019ll first need to\nHuman communicates\nto the user proxy agent,\nwhich communicates\nto other agents.\nAssistant agent\nundertakes completion\nof the direct tasks.\nEvaluation and feedback\nloop is formed between\nthe proxy and the assistant.\nLLM con\ufb01gured to\nwrite Python code\nFigure 4.2\nThe user proxy agent and assistant agent communication (Source: AutoGen)\n",
      "content_length": 2004,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "71\n4.1\nIntroducing multi-agent systems with AutoGen Studio\ndefine an environment variable for your OpenAI key. Because ports 8080 and 8081\nare popular, and if you have other services running, change the port to 8082 or some-\nthing you choose.\n# set environment variable on Bash (Git Bash)\nexport OPENAI_API_KEY=\u201d<your API key>\u201d         \n# sent environment variable with PowerShell\n$env:VAR_NAME =\u201d<your API key>\"                \nautogenstudio ui --port 8081    \nNavigate your browser to the AutoGen Studio interface shown in figure 4.3 (as of this\nwriting). While there may be differences, one thing is for sure: the primary interface\nwill still be chat. Enter a complex task that requires coding. The example used here is\nCreate a plot showing the popularity of the term GPT Agents in Google search.\nListing 4.1\nLaunching AutoGen Studio \nUse the appropriate \ncommand for your \nterminal type.\nChange the port if you expect or \nexperience a conflict on your machine.\nEnter a task for the\nagents to work on.\nYou can create new\nsessions or review or\ncontinue previous\nsessions.\nThe Playground tab is where you interact\nwith agents. The Build tab is for creating new\nagents and skills, and the Gallery tab is for\nreviewing previous best output.\nThe proxy agent and assistant\nagent will not work together\nto complete the task.\nFigure 4.3\nEntering a task for the agents to work on in the AutoGen interface\n",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "72\nCHAPTER 4\nExploring multi-agent systems\nThe agent assistant generates code snippets to perform or complete various subtasks\nas the agents work together through the task in the example. The user proxy agent then\nattempts to execute those code snippets and assesses the output. In many cases, prov-\ning the code runs and produces the required output is sufficient for the user proxy\nagent to approve the task\u2019s completion.\n If you encounter any problems with the assistant agent requests, ask the proxy\nagent to try a different method or another problem. This highlights a bigger problem\nwith agentic systems using packages or libraries that have expired and no longer work.\nFor this reason, it\u2019s generally better to get agents to execute actions rather than build\ncode to perform actions as tools.\nTIP\nExecuting AutoGen and AutoGen Studio using Docker is recommended,\nespecially when working with code that may affect the operating system. Docker\ncan isolate and virtualize the agents\u2019 environment, thus isolating potentially\nharmful code. Using Docker can help alleviate any secondary windows or\nwebsites that may block the agent process from running. \nFigure 4.4 shows the agent\u2019s completion of the task. The proxy agent will collect any\ngenerated code snippet, images, or other documents and append them to the message.\nGenerated code \ufb01les and\nother output will be\nattached to the last message.\nReply with TERMINATE if you are\n\ufb01nished with this agent session. This\nstop word is used to stop the session.\nIn this example, a Matplotlib\nplot was generated in a new\nwindow outside the browser.\nFigure 4.4\nThe output after the agents complete the task\n",
      "content_length": 1652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "73\n4.1\nIntroducing multi-agent systems with AutoGen Studio\nYou can also review the agent conversation by opening the Agent Messages expander.\nIn many cases, if you ask the agent to generate plots or applications, secondary win-\ndows will open showing those results.\n Amazingly, the agents will perform most tasks nicely and complete them well.\nDepending on the complexity of the task, you may need to further iterate with the\nproxy. Sometimes, an agent may only go so far to complete a task because it lacks the\nrequired skills. In the next section, we\u2019ll look at how to add skills to agents.\n4.1.2\nAdding skills in AutoGen Studio\nSkills and tools, or actions, as we refer to them in this book, are the primary means by\nwhich agents can extend themselves. Actions give agents the ability to execute code,\ncall APIs, or even further evaluate and inspect generated output. AutoGen Studio cur-\nrently begins with just a basic set of tools to fetch web content or generate images.\nNOTE\nMany agentic systems employ the practice of allowing agents to code to\nsolve goals. However, we discovered that code can be easily broken, needs to be\nmaintained, and can change quickly. Therefore, as we\u2019ll discuss in later chap-\nters, it\u2019s better to provide agents with skills/actions/tools to solve problems. \nIn the following exercise scenario, we\u2019ll add a skill/action to inspect an image using\nthe OpenAI vision model. This will allow the proxy agent to provide feedback if we ask\nthe assistant to generate an image with particular content.\n With AutoGen Studio running, go to the Build tab and click Skills, as shown in\nfigure 4.5. Then, click the New Skill button to open a code panel where you can\ncopy\u2013paste code to. From this tab, you can also configure models, agents, and agent\nworkflows.\n Enter the code shown in listing 4.2 and also provided in the book\u2019s source code as\ndescribe_image.py. Copy and paste this code into the editor window, and then click\nthe Save button at the bottom.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "74\nCHAPTER 4\nExploring multi-agent systems\nimport base64\nimport requests\nimport os\ndef describe_image(image_path='animals.png') -> str:\n    \"\"\"\n    Uses GPT-4 Vision to inspect and describe the contents of the image.\n    :param input_path: str, the name of the PNG file to describe.\n    \"\"\"\n    api_key = os.environ['OPEN_API_KEY']\n    # Function to encode the image\n    def encode_image(image_path):     \n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\nListing 4.2\ndescribe_image.py \nClick existing skills\nto see how they work.\nClick to create a new skill.\nYou can also con\ufb01gure\nother models, agents, and\nwork\ufb02ows from here.\nFigure 4.5\nSteps to creating a new skill on the Build tab\nFunction to load and \nencode the image as \na Base64 string\n",
      "content_length": 817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "75\n4.1\nIntroducing multi-agent systems with AutoGen Studio\n    # Getting the base64 string\n    base64_image = encode_image(image_path)\n    headers = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {api_key}\"\n    }\n    payload = {\n    \"model\": \"gpt-4-turbo\",\n    \"messages\": [\n        {\n        \"role\": \"user\",\n        \"content\": [\n            {\n            \"type\": \"text\",\n            \"text\": \"What\u2019s in this image?\"\n            },\n            {\n            \"type\": \"image_url\",\n            \"image_url\": {\n         \"url\": f\"data:image/jpeg;base64,{base64_image}\"     \n            }\n            }\n        ]\n        }\n    ],\n    \"max_tokens\": 300\n    }\n    response = requests.post(\n        \"https://api.openai.com/v1/chat/completions\",\n        headers=headers,\n        json=payload)\n    return response.json()[\"choices\"][0][\"message\"] \n[\"content\"]                                         \nThe describe_image function uses the OpenAI GPT-4 vision model to describe what\nis in the image. This skill can be paired with the existing generate_image skill as a\nquality assessment. The agents can confirm that the generated image matches the\nuser\u2019s requirements.\n After the skill is added, it must be added to the specific agent workflow and agent\nfor use. Figure 4.6 demonstrates adding the new skill to the primary assistant agent in\nthe general or default agent workflow. \n Now that the skill is added to the primary assistant, we can task the agent with cre-\nating a specific image and validating it using the new describe_image skill. Because\nimage generators notoriously struggle with correct text, we\u2019ll create an exercise task to\ndo just that.\n Enter the text shown in listing 4.3 to prompt the agents to create a book image\ncover for this book. We\u2019ll explicitly say that the text needs to be correct and insist that\nthe agent uses the new describe_image function to verify the image.\nIncluding the \nimage string \nalong with the \nJSON payload\nUnpacking the response and \nreturning the content of the reply\n",
      "content_length": 2030,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "76\nCHAPTER 4\nExploring multi-agent systems\nPlease create a cover for the book GPT Agents In Action, use the \ndescribe_image skill to make sure the title of the book is spelled \ncorrectly on the cover\nAfter the prompt is entered, wait for a while, and you may get to see some dialogue\nexchanged about the image generation and verification process. In the end, though, if\neverything works correctly, the agents will return with the results shown in figure 4.7.\n Remarkably, the agent coordination completed the task in just a couple of itera-\ntions. Along with the images, you can also see the various helper code snippets gener-\nated to assist with task completion. AutoGen Studio is impressive in its ability to\nintegrate skills that the agents can further adapt to complete some goal. The following\nsection will show how these powerful agents are implemented in code.\nListing 4.3\nPrompting for a book cover\nSelect the General\nAgent Work\ufb02ow.\nSelect to edit the primary_assistant\nat the bottom of the work\ufb02ow panel.\nGo to the Build tab, and then go to the Work\ufb02ows tab.\nClick add to add the new\ndescribe_image skill.\nClick OK to exit and return\nto the Playground tab.\nFigure 4.6\nConfiguring the primary_assistant agent with the new skill\n",
      "content_length": 1237,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "77\n4.2\nExploring AutoGen\n4.2\nExploring AutoGen\nWhile AutoGen Studio is a fantastic tool for understanding multi-agent systems, we\nmust look into the code. Fortunately, coding multiple agent examples with AutoGen is\nsimple and easy to run. We\u2019ll cover the basic AutoGen setup in the next section.\n4.2.1\nInstalling and consuming AutoGen\nThis next exercise will look at coding a basic multi-agent system that uses a user proxy\nand conversable agent. Before we do that, though, we want to make sure AutoGen is\ninstalled and configured correctly.\n Open a terminal in VS Code, and run the entire chapter 4 install directions per\nappendix B, or run the pip command in listing 4.4. If you\u2019ve installed the require-\nments.txt file, you\u2019ll also be ready to run AutoGen.\npip install pyautogen\nNext, copy the chapter_04/OAI_CONFIG_LIST.example to OAI_CONFIG_LIST, remov-\ning .example from the file name. Then, open the new file in VS Code, and enter your\nListing 4.4\nInstalling AutoGen\nReview the generated output \ufb01les.\nThe agents generate additional\ncode to use the skills as needed.\nTho agents even enhance existing\nskills to better complete the task.\nAfter a couple iterations, the book cover\nis generated with the correct text.\nFigure 4.7\nThe generated file outputs from the agent work on the image generation task\n",
      "content_length": 1307,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "78\nCHAPTER 4\nExploring multi-agent systems\nOpenAI or Azure configuration in the OAI_CONFIG_LIST file in listing 4.5. Fill in your\nAPI key, model, and other details per your API service requirements. AutoGen will\nwork with any model that adheres to the OpenAI client. That means you can use local\nLLMs via LM Studio or other services such as Groq, Hugging Face, and more.\n[\n    {\n        \"model\": \"gpt-4\",                    \n        \"api_key\": \"<your OpenAI API key here>\",           \n        \"tags\": [\"gpt-4\", \"tool\"]\n    },\n    {\n        \"model\": \"<your Azure OpenAI deployment name>\",     \n        \"api_key\": \"<your Azure OpenAI API key here>\",     \n        \"base_url\": \"<your Azure OpenAI API base here>\",    \n        \"api_type\": \"azure\",\n        \"api_version\": \"2024-02-15-preview\"\n    }    \n]\nNow, we can look at the code for a basic multi-agent chat using the out-of-the-box\nUserProxy and ConversableAgent agents. Open autogen_start.py in VS Code,\nshown in the following listing, and review the parts before running the file.\nfrom autogen import ConversableAgent, UserProxyAgent, config_list_from_json\n   config_list = config_list_from_json(\n        env_or_file=\"OAI_CONFIG_LIST\")     \n   assistant = ConversableAgent(\n        \"agent\", \n        llm_config={\"config_list\": config_list})     \n   user_proxy = UserProxyAgent(     \n        \"user\",\n        code_execution_config={\n            \"work_dir\": \"working\",\n            \"use_docker\": False,\n        },\n        human_input_mode=\"ALWAYS\",\n        is_termination_msg=lambda x: x.get(\"content\", \"\")\n        .rstrip()\n        .endswith(\"TERMINATE\"),     \n    )    \n    user_proxy.initiate_chat(assistant, message=\"write a solution \n\u27a5 for fizz buzz in one line?\")    \nListing 4.5\nOAI_CONFIG_LIST \nListing 4.6\nautogen_start.py\nSelect the model; GPT-4 \nis recommended.\nUse the service key you \nwould typically use.\nSelect the model; GPT-4 \nis recommended.\nUse the service \nkey you would \ntypically use.\nChanging the base URL allows you to point to\nother services, not just Azure OpenAI.\nLoads your LLM \nconfiguration from the \nJSON file OAI_CONFIG_LIST\nThis agent talks \ndirectly to the LLM.\nThis agent proxies \nconversations from the \nuser to the assistant.\nSetting the termination message \nallows the agent to iterate.\nA chat is initiated with the\nassistant through the user_proxy\nto complete a task.\n",
      "content_length": 2355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "79\n4.2\nExploring AutoGen\nRun the code by running the file in VS Code in the debugger (F5). The code in list-\ning 4.6 uses a simple task to demonstrate code writing. Listing 4.7 shows a few examples\nto choose from. These coding tasks are also some of the author\u2019s regular baselines to\nassess an LLMs\u2019 strength in coding.\nwrite a Python function to check if a number is prime\ncode a classic sname game using Pygame                  \ncode a classic asteroids game in Python using Pygame    \nAfter the code starts in a few seconds, the assistant will respond to the proxy with a\nsolution. At this time, the proxy will prompt you for feedback. Press Enter, essentially\ngiving no feedback, and this will prompt the proxy to run the code to verify it operates\nas expected. \n Impressively, the proxy agent will even take cues to install required packages such\nas Pygame. Then it will run the code, and you\u2019ll see the output in the terminal or as a\nnew window or browser. You can play the game or use the interface if the code shelled\na new window/browser.\n Note that the spawned window/browser won\u2019t close on Windows and will require\nexiting the entire program. To avoid this problem, run the code through Windows\nSubsystem for Linux (WSL) or Docker. AutoGen explicitly recommends using Docker\nfor code execution agents, and if you\u2019re comfortable with containers, this is a good\noption.\n Either way, after the proxy generates and runs the code, the working_dir folder\nset earlier in listing 4.6 should now have a Python file with the code. This will allow\nyou to run the code at your leisure, make changes, or even ask for improvements, as\nwe\u2019ll see. In the next section, we\u2019ll look at how to improve the capabilities of the cod-\ning agents.\n4.2.2\nEnhancing code output with agent critics\nOne powerful benefit of multi-agent systems is the multiple roles/personas you can\nautomatically assign when completing tasks. Generating or helping to write code can be\nan excellent advantage to any developer, but what if that code was also reviewed and\ntested? In the next exercise, we\u2019ll add another agent critic to our agent system to help\nwith coding tasks. Open autogen_coding_critic.py, as shown in the following listing.\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\nconfig_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\nuser_proxy = UserProxyAgent(\n    \"user\",\nListing 4.7\nSimple coding task examples\nListing 4.8\nautogen_coding_critic.py\nTo enjoy iterating \nover these tasks, use \nWindows Subsystem \nfor Linux (WSL) on \nWindows, or use Docker. \n",
      "content_length": 2576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "80\nCHAPTER 4\nExploring multi-agent systems\n    code_execution_config={\n        \"work_dir\": \"working\",\n        \"use_docker\": False,\n        \"last_n_messages\": 1,\n    },\n    human_input_mode=\"ALWAYS\",\n    is_termination_msg=lambda x: \nx.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n)\nengineer = AssistantAgent(\n    name=\"Engineer\",\n    llm_config={\"config_list\": config_list},\n    system_message=\"\"\"\n    You are a profession Python engineer, known for your expertise in \nsoftware development.\n    You use your skills to create software applications, tools, and \ngames that are both functional and efficient.\n    Your preference is to write clean, well-structured code that is easy \nto read and maintain.    \n    \"\"\",     \n)\ncritic = AssistantAgent(\n    name=\"Reviewer\",\n    llm_config={\"config_list\": config_list},\n    system_message=\"\"\"\n    You are a code reviewer, known for your thoroughness and commitment \nto standards.\n    Your task is to scrutinize code content for any harmful or \nsubstandard elements.\n    You ensure that the code is secure, efficient, and adheres to best \npractices.\n    You will identify any issues or areas for improvement in the code \nand output them as a list.\n    \"\"\",     \n)\ndef review_code(recipient, messages, sender, config):     \n    return f\"\"\"\n            Review and critque the following code.\n            \n            {recipient.chat_messages_for_summary(sender)[-1]['content']}\n            \"\"\"                                           \nuser_proxy.register_nested_chats(     \n    [\n        {\n            \"recipient\": critic,\n            \"message\": review_code,\n            \"summary_method\": \"last_msg\",\n            \"max_turns\": 1,\n        }\n    ],\n    trigger=engineer,                 \n)\nThis time, the assistant is \ngiven a system/persona \nmessage.\nA second assistant critic agent is \ncreated with a background.\nA custom function\nhelps extract the\ncode for review by\nthe critic.\nA nested chat is \ncreated between \nthe critic and the \nengineer.\n",
      "content_length": 1992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "81\n4.2\nExploring AutoGen\ntask = \"\"\"Write a snake game using Pygame.\"\"\"\nres = user_proxy.initiate_chat(\n    recipient=engineer, \n    message=task, \n    max_turns=2, \n    summary_method=\"last_msg\"     \n)\nRun the autogen_coding_critic.py file in VS Code in debug mode, and watch the\ndialog between the agents. This time, after the code returns, the critic will also be trig-\ngered to respond. Then, the critic will add comments and suggestions to improve the\ncode.\n Nested chats work well for supporting and controlling agent interactions, but we\u2019ll\nsee a better approach in the following section. Before that though, we\u2019ll review the\nimportance of the AutoGen cache in the next section.\n4.2.3\nUnderstanding the AutoGen cache\nAutoGen can consume many tokens over chat iterations as a conversable multi-agent\nplatform. If you ask AutoGen to work through complex or novel problems, you may\neven encounter token limits on your LLM; because of this, AutoGen supports several\nmethods to reduce token usage.\n AutoGen uses caching to store progress and reduce token usage. Caching is\nenabled by default, and you may have already encountered it. If you check your cur-\nrent working folder, you\u2019ll notice a .cache folder, as shown in figure 4.8. Caching\nallows your agents to continue conversations if they get interrupted.\nIn code, you can control the cache folder for your agent\u2019s run, as shown in listing 4.9.\nBy wrapping the initiate_chat call with the with statement, you can control the\nThe proxy agent initiates a \nchat with a max delay and \nexplicit summary method.\nThe folder code\nis output here.\nOutput code \ufb01les are named\nwith temporary names.\nA cache is denoted by the folder\nand contains a SQLite database\nof message history.\nFigure 4.8\nAutoGen cache \nand working folders\n",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "82\nCHAPTER 4\nExploring multi-agent systems\nlocation and seed for the cache. This will allow you to save and return to long-running\nAutoGen tasks in the future by just setting the cache_seed for the previous cache.\nwith Cache.disk(cache_seed=42) as cache:    \n    res = user_proxy.initiate_chat(\n        recipient=engineer,\n        message=task,\n        max_turns=2,\n        summary_method=\"last_msg\",\n        cache=cache,     \n    )\nThis caching ability allows you to continue operations from the previous cache loca-\ntion and captures previous runs. It can also be a great way to demonstrate and inspect\nhow an agent conversation generated the results. In the next section, we\u2019ll look at\nanother conversational pattern in which AutoGen supports group chat.\n4.3\nGroup chat with agents and AutoGen\nOne problem with chat delegation and nested chats or conversations is the convey-\nance of information. If you\u2019ve ever played the telephone game, you\u2019ve witnessed this\nfirsthand and experienced how quickly information can change over iterations. With\nagents, this is certainly no different, and chatting through nested or sequential con-\nversations can alter the task or even the desired result.\nFigure 4.9 shows the difference between nested and collaborative group chats. We\nused the nested chat feature in the previous section to build a nested agent chat. In\nthis section, we use the group chat to provide a more collaborative experience.\n Open autogen_coding_group.py with relevant parts, as shown in listing 4.10. The\ncode is similar to the previous exercise but now introduces GroupChat and GroupChat-\nManager. The agents and messages are held with the group chat, similar to a messaging\nchannel in applications such as Slack or Discord. The chat manager coordinates the\nmessage responses to reduce conversation overlap.\nListing 4.9\nSetting the cache folder\nThe telephone game\nThe telephone game is a fun but educational game that demonstrates information\nand coherence loss. Children form a line, and the first child receives a message only\nthey can hear. Then, in turn, the children verbally pass the message on to the next\nchild, and so on. At the end, the last child announces the message to the whole\ngroup, which often isn\u2019t even close to the same message.\nTo counter this, AutoGen provides a group chat, a mechanism by which agents par-\nticipate in a shared conversation. This allows agents to review all past conversations\nand better collaborate on long-running and complex tasks.\nSetting the seed_cache \ndenotes the individual \nlocation.\nSets the cache as \na parameter\n",
      "content_length": 2581,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "83\n4.3\nGroup chat with agents and AutoGen\nuser_proxy = UserProxyAgent(\n    \"user\",\n    code_execution_config={\n        \"work_dir\": \"working\",\n        \"use_docker\": False,\n        \"last_n_messages\": 3,\n    },\n    human_input_mode=\"NEVER\",    \n)\nllm_config = {\"config_list\": config_list}\nengineer = AssistantAgent(\u2026     \ncritic = AssistantAgent(\u2026       \n    \ngroupchat = GroupChat(agents=[user_proxy, \n                              engineer, \n                              critic], \n                              messages=[], \n                              max_round=20)     \nmanager = GroupChatManager(groupchat=groupchat, \n                           llm_config=llm_config)    \ntask = \"\"\"Write a snake game using Pygame.\"\"\"\nListing 4.10\nautoget_coding_group.py (relevant sections)\nUser Proxy\nChat\nManager\nAgents now collaborate\nthrough the group chat\nmanager.\nUser Proxy\nMessage is sent\nto the critic.\nEngineer\nCritic\nRepresents the nested\nchat to the critic\nConversable Agents Nested Chat\nConversable Agents Group Chat\nMessages go through the\ngroup chat manager.\nCritic responds with\nsuggested updates\nand changes.\nProxy initiates chat\nwith engineer.\nEngineer\nCritic\nFigure 4.9\nThe difference between nested and group chat for conversable agents\nHuman input is now \nset to never, so no \nhuman feedback.\nCode omitted, but \nconsult changes to the \npersona in the file\nThis object holds the \nconnection to all the \nagents and stores the \nmessages.\nThe manager \ncoordinates the \nconversation as a \nmoderator would.\n",
      "content_length": 1511,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "84\nCHAPTER 4\nExploring multi-agent systems\nwith Cache.disk(cache_seed=43) as cache:\n    res = user_proxy.initiate_chat(\n        recipient=manager,\n        message=task,\n        cache=cache,\n    )\nRun this exercise, and you\u2019ll see how the agents collaborate. The engineer will now\ntake feedback from the critic and undertake operations to address the critic\u2019s sugges-\ntions. This also allows the proxy to engage in all of the conversation.\n Group conversations are an excellent way to strengthen your agents\u2019 abilities as\nthey collaborate on tasks. However, they are also substantially more verbose and token\nexpensive. Of course, as LLMs mature, so do the size of their context token windows\nand the price of token processing. As token windows increase, concerns over token\nconsumption may eventually go away.\n AutoGen is a powerful multi-agent platform that can be experienced using a web\ninterface or code. Whatever your preference, this agent collaboration tool is an excel-\nlent platform for building code or other complex tasks. Of course, it isn\u2019t the only plat-\nform, as you\u2019ll see in the next section, where we explore a newcomer called CrewAI.\n4.4\nBuilding an agent crew with CrewAI\nCrewAI is relatively new to the realm of multi-agent systems. Where AutoGen was\ninitially developed from research and then extended, CrewAI is built with enterprise\nsystems in mind. As such, the platform is more robust, making it less extensible in\nsome areas.\n With CrewAI, you build a crew of agents to focus on specific areas of a task goal.\nUnlike AutoGen, CrewAI doesn\u2019t require the use of the user proxy agent but instead\nassumes the agents only work among themselves.\n Figure 4.10 shows the main elements of the CrewAI platform, how they connect\ntogether, and their primary function. It shows a sequential-processing agent system\nwith generic researcher and writer agents. Agents are assigned tasks that may also include\ntools or memory to assist them.\n CrewAI supports two primary forms of processing: sequential and hierarchical. Fig-\nure 4.10 shows the sequential process by iterating across the given agents and their\nassociated tasks. In the next section, we dig into some code to set up a crew and\nemploy it to complete a goal and create a good joke.\n4.4.1\nCreating a jokester crew of CrewAI agents\nCrewAI requires more setup than AutoGen, but this also allows for more control and\nadditional guides, which provide more specific context to guide the agents in com-\npleting the given task. This isn\u2019t without problems, but it does offer more control than\nAutoGen out of the box.\n Open crewai_introduction.py in VS Code and look at the top section, as shown\nin listing 4.11. Many settings are required to configure an agent, including the role,\n",
      "content_length": 2748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "85\n4.4\nBuilding an agent crew with CrewAI\ngoal, verboseness, memory, backstory, delegation, and even tools (not shown). In this\nexample, we\u2019re using two agents: a senior joke researcher and a joke writer.\nfrom crewai import Agent, Crew, Process, Task\nfrom dotenv import load_dotenv\nload_dotenv()\njoke_researcher = Agent(     \n    role=\"Senior Joke Researcher\",\n    goal=\"Research what makes things funny about the following {topic}\",\n    verbose=True,     \n    memory=True,     \n    backstory=(     \n        \"Driven by slapstick humor, you are a seasoned joke researcher\"\n        \"who knows what makes people laugh. You have a knack for finding\"\nListing 4.11\ncrewai_introduction.py (agent section)\nCrew\nTasks\nAgents\nTools\nMemory\nSearch\nCall APIs\nAccess data\nConversational\nTask speci\ufb01c\nSemantic\nVarious forms of memory and\nr\na\ng\netrieval ugmented eneration (RAG)\npatterns are supported.\nresearch on this {topic}\nwrite on this topic\nTools can be attached\nto agents and tasks.\nwriter\ngoal:\nbackstory:\nresearcher\ngoal:\nbackstory:\nAgents have a goal\nand backstory as\ntheir persona.\nProcessing can\nbe sequential\nor hierarchical.\nSequential\nprocessing\nFigure 4.10\nThe composition of a CrewAI system\nCreates the \nagents and \nprovides them \na goal\nverbose allows the\nagent to emit output\nto the terminal.\nSupports the use of\nmemory for the agents\nThe backstory is the agent\u2019s background\u2014its persona.\n",
      "content_length": 1392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "86\nCHAPTER 4\nExploring multi-agent systems\n        \"the funny in everyday situations and can turn a dull moment into\"\n        \"a laugh riot.\"\n    ),\n    allow_delegation=True,    \n)\njoke_writer = Agent(    \n    role=\"Joke Writer\",\n    goal=\"Write a humourous and funny joke on the following {topic}\",\n    verbose=True,    \n    memory=True,     \n    backstory=(    \n        \"You are a joke writer with a flair for humor. You can turn a\"\n        \"simple idea into a laugh riot. You have a way with words and\"\n        \"can make people laugh with just a few lines.\"\n    ),\n    allow_delegation=False,    \n)\nMoving down the code, we next see the tasks, as shown in listing 4.12. Tasks denote an\nagent\u2019s process to complete the primary system goal. They also link an agent to work\non a specific task, define the output from that task, and may include how it\u2019s executed.\nresearch_task = Task(        \n    description=(\n        \"Identify what makes the following topic:{topic} so funny.\"\n        \"Be sure to include the key elements that make it humourous.\"\n        \"Also, provide an analysis of the current social trends,\"\n        \"and how it impacts the perception of humor.\"\n    ),\n    expected_output=\"A comprehensive 3 paragraphs long report \n\u27a5     on the latest jokes.\",              \n    agent=joke_researcher,     \n)\nwrite_task = Task(  \n    description=(\n        \"Compose an insightful, humourous and socially aware joke on \n{topic}.\"\n        \"Be sure to include the key elements that make it funny and\"\n        \"relevant to the current social trends.\"\n    ),\n    expected_output=\"A joke on {topic}.\",  \n    agent=joke_writer,        \n    async_execution=False,         \n    output_file=\"the_best_joke.md\",     \n)\nListing 4.12\ncrewai_introduction.py (task section)\nThe agents can either be delegated to or \nare allowed to delegate; True means they \ncan delegate.\nCreates the \nagents and \nprovides them \na goal\nverbose allows the\nagent to emit output\nto the terminal.\nSupports the use of\nmemory for the agents\nThe backstory is the\nagent\u2019s background\u2014\nits persona.\nThe Task description defines how \nthe agent will complete the task.\nExplicitly defines the expected \noutput from performing the task\nThe \nagent \nassigned \nto work \non the \ntask\nThe Task description defines how \nthe agent will complete the task.\nExplicitly defines the expected \noutput from performing the task\nIf the agent should execute \nasynchronously\nAny output the agent will generate\n",
      "content_length": 2453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "87\n4.4\nBuilding an agent crew with CrewAI\nNow, we can see how everything comes together as the Crew at the bottom of the file,\nas shown in listing 4.13. Again, many options can be set when building the Crew,\nincluding the agents, tasks, process type, memory, cache, maximum requests per min-\nute (max_rpm), and whether the crew shares.\ncrew = Crew(\n    agents=[joke_researcher, joke_writer],   \n    tasks=[research_task, write_task],    \n    process=Process.sequential,     \n    memory=True,     \n    cache=True,    \n    max_rpm=100,    \n    share_crew=True,    \n)\nresult = crew.kickoff(inputs={\"topic\": \"AI engineer jokes\"})\nprint(result)\nWhen you\u2019re done reviewing, run the file in VS Code (F5), and watch the terminal for\nconversations and messages from the crew. As you can probably tell by now, the goal of\nthis agent system is to craft jokes related to AI engineering. Here are some of the fun-\nnier jokes generated over a few runs of the agent system:\n\uf0a1Why was the computer cold? It left Windows open.\n\uf0a1Why don\u2019t AI engineers play hide and seek with their algorithms? Because no\nmatter where they hide, the algorithms always find them in the \u201coverfitting\u201d\nroom!\n\uf0a1What is an AI engineer\u2019s favorite song? \u201cI just called to say I love you . . . and to\ncollect more data for my voice recognition software.\u201d\n\uf0a1Why was the AI engineer broke? Because he spent all his money on cookies, but\nhis browser kept eating them.\nBefore you run more iterations of the joke crew, you should read the next section.\nThis section shows how to add observability to the multi-agent system.\n4.4.2\nObserving agents working with AgentOps\nObserving a complex assemblage such as a multi-agent system is critical to understand-\ning the myriad of problems that can happen. Observability through application tracing\nis a key element of any complex system, especially one engaged in enterprise use.\n CrewAI supports connecting to a specialized agent operations platform appropri-\nately called AgentOps. This observability platform is generic and designed to support\nListing 4.13\ncrewai_introduction.py (crew section)\nThe agents assembled\ninto the crew\nThe tasks the agents\ncan work on\nDefining how the agents \nwill interact\nWhether the system should \nuse memory; needs to be set \nif agents/tasks have it on\nWhether the system \nshould use a cache, \nsimilar to AutoGen\nMaximum requests per minute \nthe system should limit itself to\nWhether the crew should share \ninformation, similar to group chat\n",
      "content_length": 2470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "88\nCHAPTER 4\nExploring multi-agent systems\nobservability with any agent platform specific to LLM usage. Currently, no pricing or\ncommercialization details are available.\n Connecting to AgentOps is as simple as installing the package, getting an API key,\nand adding a line of code to your crew setup. This next exercise will go through the\nsteps to connect and run AgentOps.\n Listing 4.14 shows installing the agentops package using pip. You can install the\npackage alone or as an additional component of the crewai package. Remember that\nAgentOps can also be connected to other agent platforms for observability.\npip install agentops\nor as an option with CrewAI\npip install crewai[agentops]\nBefore using AgentOps, you need to sign up for an API key. Following are the general\nsteps to sign up for a key at the time of writing:\n1\nVisit https://app.agentops.ai in your browser.\n2\nSign up for an account. \n3\nCreate a project, or use the default.\n4\nGo to Settings > Projects and API Keys.\n5\nCopy and/or generate a new API key; this will copy the key to your browser.\n6\nPaste the key to your .env file in your project.\nAfter the API key is copied, it should resemble the example shown in the following\nlisting.\nAGENTOPS_API_KEY=\"your API key\"\nNow, we need to add a few lines of code to the CrewAI script. Listing 4.16 shows the\nadditions as they are added to the crewai_agentops.py file. When creating your own\nscripts, all you need to do is add the agentops package and initialize it when using\nCrewAI.\nimport agentops     \nfrom crewai import Agent, Crew, Process, Task\nfrom dotenv import load_dotenv\nload_dotenv()\nagentops.init()    \nListing 4.14\nInstalling AgentOps\nListing 4.15\nenv.: Adding an AgentOps key\nListing 4.16\ncrewai_agentops.py (AgentOps additions)\nThe addition of the \nrequired package\nMake sure to initialize the \npackage after the environment \nvariables are loaded.\n",
      "content_length": 1879,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "89\n4.4\nBuilding an agent crew with CrewAI\nRun the crewai_agentops.py file in VS Code (F5), and watch the agents work as before.\nHowever, you can now go to the AgentOps dashboard and view the agent interactions\nat various levels.\n Figure 4.11 shows the dashboard for running the joke crew to create the best joke.\nSeveral statistics include total duration, the run environment, prompt and completion\ntokens, LLM call timings, and estimated cost. Seeing the cost can be both sobering\nand indicative of how verbose agent conversations can become.\nThe AgentOps platform is an excellent addition to any agent platform. While it\u2019s built\ninto CrewAI, it\u2019s helpful that the observability could be added to AutoGen or other\nframeworks. Another attractive thing about AgentOps is that it\u2019s dedicated to observing\nagent interactions and not transforming from a machine learning operations platform.\nIn the future, we\u2019ll likely see the spawn of more agent observability patterns.\n One benefit that can\u2019t be overstated is the cost observation that an observability\nplatform can provide. Did you notice in figure 4.11 that creating a single joke costs a\nlittle over 50 cents? Agents can be very powerful, but they can also become very costly,\nand it\u2019s essential to observe what those costs are in terms of practicality and commer-\ncialization.\nYou can even track individual\nLLM calls, actions, and tool use.\nPrompts and replies are also\ncaptured for all iterations.\nVarious statistics are captured with\nrespect to the entire agent conversation\nsequence, including costs.\nSystem information\nis captured as well.\nFigure 4.11\nThe AgentOps dashboard for running the joke crew\n",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "90\nCHAPTER 4\nExploring multi-agent systems\n In the last section of this chapter, we\u2019ll return to CrewAI and revisit building\nagents that can code games. This will provide an excellent comparison between the\ncapabilities of AutoGen and CrewAI.\n4.5\nRevisiting coding agents with CrewAI\nA great way to compare capabilities between multi-agent platforms is to implement\nsimilar tasks in a bot. In this next set of exercises, we\u2019ll employ CrewAI as a game pro-\ngramming team. Of course, this could be adapted to other coding tasks as well.\n Open crewai_coding_crew.py in VS Code, and we\u2019ll first review the agent section\nin listing 4.17. Here, we\u2019re creating a senior engineer, a QA engineer, and a chief QA\nengineer with a role, goal, and backstory. \nprint(\"## Welcome to the Game Crew\")     \nprint(\"-------------------------------\")\ngame = input(\"What is the game you would like to build?\n\u27a5 What will be the mechanics?\\n\")\nsenior_engineer_agent = Agent(\n    role=\"Senior Software Engineer\",\n    goal=\"Create software as needed\",\n    backstory=dedent(\n        \"\"\"\n        You are a Senior Software Engineer at a leading tech think tank.\n        Your expertise in programming in python. and do your best to\n        produce perfect code\n        \"\"\"\n    ),\n    allow_delegation=False,\n    verbose=True,\n)\nqa_engineer_agent = Agent(\n    role=\"Software Quality Control Engineer\",\n    goal=\"create prefect code, by analizing the code \n\u27a5 that is given for errors\",\n    backstory=dedent(\n        \"\"\"\n        You are a software engineer that specializes in checking code\n        for errors. You have an eye for detail and a knack for finding\n        hidden bugs.\n        You check for missing imports, variable declarations, mismatched\n        brackets and syntax errors.\n        You also check for security vulnerabilities, and logic errors\n        \"\"\"\n    ),\n    allow_delegation=False,\n    verbose=True,\n)\nListing 4.17\ncrewai_coding_crew.py (agent section)\nAllows the user \nto input the \ninstructions for \ntheir game\n",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "91\n4.5\nRevisiting coding agents with CrewAI\nchief_qa_engineer_agent = Agent(\n    role=\"Chief Software Quality Control Engineer\",\n    goal=\"Ensure that the code does the job that it is supposed to do\",\n    backstory=dedent(\n        \"\"\"\n        You are a Chief Software Quality Control Engineer at a leading\n        tech think tank. You are responsible for ensuring that the code\n        that is written does the job that it is supposed to do.\n        You are responsible for checking the code for errors and ensuring\n        that it is of the highest quality.\n        \"\"\"\n    ),\n    allow_delegation=True,    \n    verbose=True,\n)\nScrolling down in the file will display the agent tasks, as shown in listing 4.18. The task\ndescriptions and expected output should be easy to follow. Again, each agent has a\nspecific task to provide better context when working to complete the task.\ncode_task = Task(\n    description=f\"\"\"\nYou will create a game using python, these are the instructions:\n        Instructions\n        ------------\n        {game}            \n        You will write the code for the game using python.\"\"\",\n    expected_output=\"Your Final answer must be the \n\u27a5 full python code, only the python code and nothing else.\",\n    agent=senior_engineer_agent,\n)\nqa_task = Task(\n    description=f\"\"\"You are helping create a game \n\u27a5 using python, these are the instructions:\n        Instructions\n        ------------\n        {game}            \n        Using the code you got, check for errors. Check for logic errors,\n        syntax errors, missing imports, variable declarations, \nmismatched brackets,\n        and security vulnerabilities.\"\"\",\n    expected_output=\"Output a list of issues you found in the code.\",\n    agent=qa_engineer_agent,\n)\nevaluate_task = Task(\n    description=f\"\"\"You are helping create a game \n\u27a5 using python, these are the instructions:\n        Instructions\n        ------------\n        {game}            \nListing 4.18\ncrewai_coding_crew.py (task section)\nOnly the chief QA engineer \ncan delegate tasks.\nThe game instructions\nare substituted into the\nprompt using Python\nformatting.\n",
      "content_length": 2108,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "92\nCHAPTER 4\nExploring multi-agent systems\n        You will look over the code to insure that it is complete and\n        does the job that it is supposed to do. \"\"\",\n    expected_output=\"Your Final answer must be the \n\u27a5 corrected a full python code, only the python code and nothing else.\",\n    agent=chief_qa_engineer_agent,\n)\nFinally, we can see how this comes together by going to the bottom of the file, as\nshown in listing 4.19. This crew configuration is much like what we\u2019ve seen before.\nEach agent and task are added, as well as the verbose and process attributes. For this\nexample, we\u2019ll continue to use sequential methods. \ncrew = Crew(\n    agents=[senior_engineer_agent, \n            qa_engineer_agent, \n            chief_qa_engineer_agent],\n    tasks=[code_task, qa_task, evaluate_task],\n    verbose=2,  \n    process=Process.sequential,     \n)\n# Get your crew to work!\nresult = crew.kickoff()   \nprint(\"######################\")\nprint(result)\nWhen you run the VS Code (F5) file, you\u2019ll be prompted to enter the instructions for\nwriting a game. Enter some instructions, perhaps the snake game or another game\nyou choose. Then, let the agents work, and observe what they produce.\n With the addition of the chief QA engineer, the results will generally look better\nthan what was produced with AutoGen, at least out of the box. If you review the code,\nyou\u2019ll see that it generally follows good patterns and, in some cases, may even include\ntests and unit tests.\n Before we finish the chapter, we\u2019ll make one last change to the crew\u2019s processing\npattern. Previously, we employed sequential processing, as shown in figure 4.10. Fig-\nure 4.12 shows what hierarchical processing looks like in CrewAI. \n Adding this manager is a relatively simple process. Listing 4.20 shows the addi-\ntional code changes to a new file that uses the coding crew in a hierarchical method.\nAside from importing a class for connecting to OpenAI from LangChain, the other\naddition is adding this class as the crew manger, manager_llm.\n \n \n \n \nListing 4.19\ncrewai_coding_crew.py (crew section)\nProcess is sequential.\nNo additional context is \nprovided in the kickoff.\n",
      "content_length": 2148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "93\n4.5\nRevisiting coding agents with CrewAI\nfrom langchain_openai import ChatOpenAI     \ncrew = Crew(\n    agents=[senior_engineer_agent, \n            qa_engineer_agent, \n            chief_qa_engineer_agent],\n    tasks=[code_task, qa_task, evaluate_task],\n    verbose=2,  \n    process=Process.hierarchical,    \n    manager_llm=ChatOpenAI(              \n        temperature=0, model=\"gpt-4\"     \nListing 4.20\ncrewai_hierarchy.py (crew manager sections)\nCrew\nTasks\nAgents\nTools\nMemory\nSearch\nCall APIs\nAccess data\nConversational\nTask speci\ufb01c\nSemantic\nTools can be attached\nto agents and tasks.\nresearch on this {topic}\nwrite on this topic\nwriter\ngoal:\nbackstory:\nresearcher\ngoal:\nbackstory:\nAgents have a goal\nand backstory as\ntheir persona.\ncrew\nmanager\nHierarchical processing\nis coordinated through\na managing agent.\nVarious forms of memory and\nr\na\ng\netrieval ugmented eneration (RAG)\npatterns are supported.\nFigure 4.12\nHierarchical processing of agents coordinated through a crew manager\nImports the LLM \nconnector from \nLangChain\nYou must set a crew \nmanager when selecting \nhierarchical processing.\nSets the crew manager to \nbe the LLM connector\n",
      "content_length": 1150,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "94\nCHAPTER 4\nExploring multi-agent systems\n    ),   \n)        \nRun this file in VS Code (F5). When prompted, enter a game you want to create. Try\nusing the same game you tried with AutoGen; the snake game is also a good baseline\nexample. Observe the agents work through the code and review it repeatedly for\nproblems.\n After you run the file, you can also jump on AgentOps to review the cost of this\nrun. Chances are, it will cost over double what it would have without the agent man-\nager. The output will also likely not be significantly better. This is the trap of building\nagent systems without understanding how quickly things can spiral.\n An example of this spiral that often happens when agents continually iterate over\nthe same actions is frequently repeating tasks. You can view this problem in AgentOps,\nas shown in figure 4.13, by viewing the Repeat Thoughts plot. \nThe Repeat Thoughts plot from AgentOps is an excellent way to measure the repeti-\ntion your agent system encounters. Overly repetitive thought patterns typically\nmean the agent isn\u2019t being decisive enough and instead keeps trying to generate a\ndifferent answer. If you encounter this problem, you want to change the agents\u2019 pro-\ncessing patterns, tasks, and goals. You may even want to alter the system\u2019s type and\nnumber of agents.\n Multi-agent systems are an excellent way to break up work in terms of work pat-\nterns of jobs and tasks. Generally, the job role is allocated to an agent role/persona,\nYou must set a crew manager when \nselecting hierarchical processing.\nFigure 4.13\nThe repetition of thoughts as they occurred within an agent run\nPlot indicates the repetition\nof the same thoughts in an\nagent interaction.\n2\n1.5\n1\n0.5\n",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "95\n4.6\nExercises\nand the tasks it needs to complete may be implicit, as in AutoGen, or more explicit, as\nin CrewAI.\n In this chapter, we covered many useful tools and platforms that you can use right\naway to improve your work, life, and more. That completes our journey through multi-\nagent platforms, but it doesn\u2019t conclude our exploration and use of multiple agents,\nas we\u2019ll discover in later chapters.\n4.6\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Basic Agent Communication with AutoGen\nObjective\u2014Familiarize yourself with basic agent communications and setup in\nAutoGen.\nTasks:\n\u2013 Set up AutoGen Studio on your local machine, following the instructions\nprovided in this chapter.\n\u2013 Create a simple multi-agent system with a user proxy and two assistant agents.\n\u2013 Implement a basic task where the user proxy coordinates between the assis-\ntant agents to generate a simple text output, such as summarizing a short\nparagraph.\n\uf0a1Exercise 2\u2014Implementing Advanced Agent Skills in AutoGen Studio\nObjective\u2014Enhance agent capabilities by adding advanced skills.\nTasks:\n\u2013 Develop and integrate a new skill into an AutoGen agent that allows it to\nfetch and display real-time data from a public API (e.g., weather information\nor stock prices).\n\u2013 Ensure the agent can ask for user preferences (e.g., city for weather, type of\nstocks) and display the fetched data accordingly.\n\uf0a1Exercise 3\u2014Role-Based Task Management with CrewAI\nObjective\u2014Explore role-based task management in CrewAI.\nTasks:\n\u2013 Design a CrewAI setup where multiple agents are assigned specific roles\n(e.g., data fetcher, analyzer, presenter).\n\u2013 Configure a task sequence where the data fetcher collects data, the analyzer\nprocesses the data, and the presenter generates a report.\n\u2013 Execute the sequence and observe the flow of information and task delega-\ntion among agents.\n\uf0a1Exercise 4\u2014Multi-Agent Collaboration in Group Chat Using AutoGen\nObjective\u2014Understand and implement a group chat system in AutoGen to facil-\nitate agent collaboration.\n",
      "content_length": 2049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "96\nCHAPTER 4\nExploring multi-agent systems\nTasks:\n\u2013 Set up a scenario where multiple agents need to collaborate to solve a com-\nplex problem (e.g., planning an itinerary for a business trip).\n\u2013 Use the group chat feature to allow agents to share information, ask ques-\ntions, and provide updates to each other.\n\u2013 Monitor the agents\u2019 interactions and effectiveness in collaborative problem\nsolving.\n\uf0a1Exercise 5\u2014Adding and Testing Observability with AgentOps in CrewAI\nObjective\u2014Implement and evaluate the observability of agents using AgentOps\nin a CrewAI environment.\nTasks:\n\u2013 Integrate AgentOps into a CrewAI multi-agent system.\n\u2013 Design a task for the agents that involves significant computation or data\nprocessing (e.g., analyzing customer reviews to determine sentiment trends).\n\u2013 Use AgentOps to monitor the performance, cost, and output accuracy of the\nagents. Identify any potential inefficiencies or errors in agent interactions.\nSummary\n\uf0a1AutoGen, developed by Microsoft, is a conversational multi-agent platform that\nemploys a variety of agent types, such as user proxies and assistant agents, to\nfacilitate task execution through natural language interactions.\n\uf0a1AutoGen Studio acts as a development environment that allows users to create,\ntest, and manage multi-agent systems, enhancing the usability of AutoGen.\n\uf0a1AutoGen supports multiple communication patterns, including group chats\nand hierarchical and proxy communications. Proxy communication involves a\nprimary agent (proxy) that interfaces between the user and other agents to\nstreamline task completion.    \n\uf0a1CrewAI offers a structured approach to building multi-agent systems with a focus\non enterprise applications. It emphasizes role-based and autonomous agent func-\ntionalities, allowing for flexible, sequential, or hierarchical task management.\n\uf0a1Practical exercises in the chapter illustrate how to set up and use AutoGen Stu-\ndio, including installing necessary components and running basic multi-agent\nsystems.\n\uf0a1Agents in AutoGen can be equipped with specific skills to perform tasks such as\ncode generation, image analysis, and data retrieval, thereby broadening their\napplication scope.\n\uf0a1CrewAI is distinguished by its ability to structure agent interactions more rigidly\nthan AutoGen, which can be advantageous in settings that require precise and\ncontrolled agent behavior.\n\uf0a1CrewAI supports integrating memory and tools for agents to consume through\ntask completion.\n",
      "content_length": 2450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "97\nSummary\n\uf0a1CrewAI supports integration with observability tools such as AgentOps, which\nprovides insights into agent performance, interaction efficiency, and cost\nmanagement.\n\uf0a1AgentOps is an agent observability platform that can help you easily monitor\nextensive agent interactions.\n",
      "content_length": 284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "98\nEmpowering agents\nwith actions\nIn this chapter, we explore actions through the use of functions and how agents\ncan use them as well. We\u2019ll start by looking at OpenAI function calling and then\nquickly move on to another project from Microsoft called Semantic Kernel (SK),\nwhich we\u2019ll use to build and manage skills and functions for agents or as agents.\n We\u2019ll finish the chapter using SK to host our first agent system. This will be a\ncomplete chapter with plenty of annotated code examples. \nThis chapter covers\n\uf0a1How an agent acts outside of itself using actions\n\uf0a1Defining and using OpenAI functions\n\uf0a1The Semantic Kernel and how to use \nsemantic functions\n\uf0a1Synergizing semantic and native functions\n\uf0a1Instantiating a GPT interface with \nSemantic Kernel \n",
      "content_length": 757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "99\n5.1\nDefining agent actions\n5.1\nDefining agent actions\nChatGPT plugins were first introduced to provide a session with abilities, skills, or\ntools. With a plugin, you can search the web or create spreadsheets or graphs. Plugins\nprovide ChatGPT with the means to extend the platform.\n Figure 5.1 shows how a ChatGPT plugin works. In this example, a new movie rec-\nommender plugin has been installed in ChatGPT. When a user asks ChatGPT to rec-\nommend a new movie, the large language model (LLM) recognizes that it has a\nplugin to manage that action. It then breaks down the user request into actionable\nparameters, which it passes to the new movie recommender.\nThe recommender then scrapes a website showcasing new movies and appends that\ninformation to a new prompt request to an LLM. With this information, the LLM\nCalls the plugin/function\nwith parameters\nHere are some new movies\nyou may like to see...\nChatGPT with a new\nmovie recommender\nplugin\nChatGPT user\nCon\ufb01rms the use of\nthe recommender plugin\nGPT-4.5?\nPlugin calls LLM to get a recommendation\nfor the list of new movies.\nPlugin uses a service\nto nd new movie.\n\ufb01\nPlugin replies with\nrecommended new movie.\nNew Movie Recommender\nplugin (agent)\nThe plugin/agent\nscrapes websites for\nnew movies.\nUse External Tools\nEnhances model capabilities.\nTactics include embeddings-based search, code execution, and access\nto speci c functions.\n\ufb01\nActions\nMemory\nPrompt Engineering Strategies\nCan you recommend\na new movie?\nA plugin may use the same,\ndifferent, or even multiple\nLLMs.\nSecond, the plugin\nuses an LLM to get\na recommendation.\nFirst, the plugin scrapes\na site for a list of new\nmovies.\nThe LLM recognizes the\nrequest for a plugin and\nthen extracts the input\nparameters required for\nthe plugin.\nFigure 5.1\nHow a ChatGPT plugin operates and how plugins and other external tools (e.g., APIs) align with the \nUse External Tools prompt engineering strategy\n",
      "content_length": 1914,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "100\nCHAPTER 5\nEmpowering agents with actions\nresponds to the recommender, which passes this back to ChatGPT. ChatGPT then\nresponds to the user with the recommended request.\n We can think of plugins as proxies for actions. A plugin generally encapsulates one\nor more abilities, such as calling an API or scraping a website. Actions, therefore, are\nextensions of plugins\u2014they give a plugin its abilities.\n AI agents can be considered plugins and consumers of plugins, tools, skills, and\nother agents. Adding skills, functions, and tools to an agent/plugin allows it to exe-\ncute well-defined actions\u2014figure 5.2 highlights where agent actions occur and their\ninteraction with LLMs and other systems.\nAn agent action is an ability that allows it to use a function, skill, or tool. What gets\nconfusing is that different frameworks use different terminology. We\u2019ll define an\naction as anything an agent can do to establish some basic definitions.\n ChatGPT plugins and functions represent an actionable ability that ChatGPT or an\nagent system can use to perform additional actions. Now let\u2019s examine the basis for\nOpenAI plugins and the function definition.\n1. The LLM recognizes the\nrequest for a plugin/agent\nand then extracts the input\nparameters required to\nactivate the agent.\nCalls the agent/plugin\nwith parameters\nAgent System\nChatGPT user\nGPT-4.5?\nAgent replies with\nrecommended new movie.\nNew Movie Recommender\nplugin (agent)\n2. The agent adds the\ninformation to a prompt\nused to make a request\nto an LLM.\nThe plugin/agent scrapes\nwebsites for new movies.\n3. The agent uses an action\nto \ufb01nd new movies.\nAn agent may use the\nsame, different, or\neven multiple LLMs.\nChevron denotes an\nagent action.\nAn agent action can be a\nfunction or skill/tool prompt.\n4. The agent system passes\nthe responses to the LLM\nto summarize the results.\nCan you recommend\na new movie?\nHere are some new movies\nyou may like to see...\nCon\ufb01rms the use of\na recommender plugin\nPlugin calls LLM to get a\nrecommendation for the list of new movies.\nAgent uses a function or\nskill to nd new movie.\n\ufb01\nFigure 5.2\nHow an agent uses actions to perform external tasks\n",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "101\n5.2\nExecuting OpenAI functions\n5.2\nExecuting OpenAI functions\nOpenAI, with the enablement of plugins, introduced a structure specification for defin-\ning the interface between functions/plugins an LLM could action. This specification is\nbecoming a standard that LLM systems can follow to provide actionable systems.\n These same function definitions are now also being used to define plugins for\nChatGPT and other systems. Next, we\u2019ll explore how to use functions directly with\nan LLM call.\n5.2.1\nAdding functions to LLM API calls\nFigure 5.3 demonstrates how an LLM recognizes and uses the function definition to\ncast its response as the function call.\nListing 5.1 shows the details of an LLM API call using tools and a function definition.\nAdding a function definition allows the LLM to reply regarding the function\u2019s input\nparameters. This means the LLM will identify the correct function and parse the rele-\nvant parameters for the user\u2019s request.\nresponse = client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[{\"role\": \"system\",\n                   \"content\": \"You are a helpful assistant.\"},\n                  {\"role\": \"user\", \"content\": user_message}],\nListing 5.1\nfirst_function.py (API call)\nGPT-4\nMake a request to\nLLM using tools.\nCon\ufb01rms the request matches\na particular function de\ufb01nition\nExtracts parameters matching\nthe function de nition from the\n\ufb01\noriginal request\nRequest\nModel: GPT-4\nMessages:\nSystem: you are a ...\nUser: please recommend a movie.\nParameters:\nTemperature: .7\nMax tokens: 256\nTools\n\"type\": \"function\",\n:\nReplies with the tool name\n(function) and input parameters\nfor the function\nTools represents plugins or\nfunctions added to a request.\nThe LLM does not\nexecute the function.\nIf the LLM doesn\u2019t\nmatch any tools, it\nwill respond given\nthe expected prompt.\nFigure 5.3\nHow a single LLM request, including tools, gets interpreted by an LLM\n",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "102\nCHAPTER 5\nEmpowering agents with actions\n        temperature=0.7,\n        tools=[    \n            {\n                \"type\": \"function\",    \n                \"function\": {\n                    \"name\": \"recommend\",\n                    \"description\": \"Provide a \u2026 topic.\",    \n                    \"parameters\": {\n                        \"type\": \"object\",    \n                        \"properties\": {\n                            \"topic\": {\n                                \"type\": \"string\",\n                                \"description\": \n                                   \"The topic,\u2026 for.\",    \n                            },\n                            \"rating\": {\n                                \"type\": \"string\",\n                                \"description\": \n                          \"The rating \u2026 given.\",    \n                                \"enum\": [\"good\",\n                                         \"bad\", \n                                         \"terrible\"]    \n                                },\n                        },\n                        \"required\": [\"topic\"],\n                    },\n                },\n                }\n            ]\n        )\nTo see how this works, open Visual Studio Code (VS Code) to the book\u2019s source code\nfolder: chapter_4/first_function.py. It\u2019s a good practice to open the relevant chapter\nfolder in VS Code to create a new Python environment and install the requirements.txt\nfile. If you need assistance with this, consult appendix B.\n Before starting, correctly set up an .env file in the chapter_4 folder with your\nAPI credentials. Function calling is an extra capability provided by the LLM com-\nmercial service. At the time of writing, this feature wasn\u2019t an option for open\nsource LLM deployments.\n Next, we\u2019ll look at the bottom of the code in first_function.py, as shown in list-\ning 5.2. Here are just two examples of calls made to an LLM using the request previ-\nously specified in listing 5.1. Here, each request shows the generated output from\nrunning the example.\nuser = \"Can you please recommend me a time travel movie?\"\nresponse = ask_chatgpt(user)    \nprint(response)\nListing 5.2\nfirst_function.py (exercising the API)\nNew parameter called tools\nSets the type of tool to function\nProvides an excellent \ndescription of what \nthe function does\nDefines the type of parameters \nfor input; an object represents \na JSON document.\nExcellent \ndescriptions \nfor each input \nparameter\nYou can even \ndescribe in terms \nof enumerations.\nPreviously \ndefined function\n",
      "content_length": 2514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "103\n5.2\nExecuting OpenAI functions\n###Output\nFunction(arguments='{\"topic\":\"time travel movie\"}', \n                      name='recommend')    \nuser = \"Can you please recommend me a good time travel movie?\"\nresponse = ask_chatgpt(user)    \nprint(response)\n###Output\nFunction(arguments='{\"topic\":\"time travel movie\",\n                     \"rating\":\"good\"}',\n name='recommend')    \nRun the first_function.py Python script in VS Code using the debugger (F5) or the\nterminal to see the same results. Here, the LLM parses the input request to match any\nregistered tools. In this case, the tool is the single function definition, that is, the rec-\nommended function. The LLM extracts the input parameters from this function and\nparses those from the request. Then, it replies with the named function and desig-\nnated input parameters.\nNOTE\nThe actual function isn\u2019t being called. The LLM only returns the sug-\ngested function and the relevant input parameters. The name and parame-\nters must be extracted and passed into a function matching the signature to\nact on the function. We\u2019ll look at an example of this in the next section.\n5.2.2\nActioning function calls\nNow that we understand that an LLM doesn\u2019t execute the function or plugin directly,\nwe can look at an example that executes the tools. Keeping with the recommender\ntheme, we\u2019ll look at another example that adds a Python function for simple\nrecommendations.\n Figure 5.4 shows how this simple example will work. We\u2019ll submit a single request\nthat includes a tool function definition, asking for three recommendations. The LLM,\nin turn, will reply with the three function calls with input parameters (time travel, rec-\nipe, and gift). The results from executing the functions are then passed back to the\nLLM, which converts them back to natural language and returns a reply.\n Now that we understand the example, open parallel_functions.py in VS Code.\nListing 5.3 shows the Python function that you want to call to give recommendations.\n \n \n \n \n \n \n \nReturned in \nthe name of the \nfunction to call \nand the extracted \ninput parameters\nPreviously \ndefined function\nReturned in the name of the function to \ncall and the extracted input parameters\n",
      "content_length": 2196,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "104\nCHAPTER 5\nEmpowering agents with actions\ndef recommend(topic, rating=\"good\"):\n    if \"time travel\" in topic.lower():    \n        return json.dumps({\"topic\": \"time travel\",\n                           \"recommendation\": \"Back to the Future\",\n                           \"rating\": rating})\n    elif \"recipe\" in topic.lower():    \n        return json.dumps({\"topic\": \"recipe\",\n                           \"recommendation\": \"The best thing \u2026 ate.\",\n                           \"rating\": rating})\n    elif \"gift\" in topic.lower():    \n        return json.dumps({\"topic\": \"gift\",\n                           \"recommendation\": \"A glorious new...\",\n                           \"rating\": rating})\n    else:    \n        return json.dumps({\"topic\": topic,\n                           \"recommendation\": \"unknown\"})    \nNext, we\u2019ll look at the function called run_conversation, where all the work starts\nwith the request construction.\nuser = \"\"\"Can you please make recommendations for the following:\n1. Time travel movies\n2. Recipes\nListing 5.3\nparallel_functions.py (recommend function)\nListing 5.4\nparallel_functions.py (run_conversation, request)\nMake a request to\nLLM using tools.\nGPT\nRequest\nMessages:\nUser: Can you please make\nrecommendations for the following:\n1. Time travel movies\n2. Recipes\n3. Gifts\nTools: recommend function de nition\n\ufb01\nCon\ufb01rms the request matches a\nparticular function de\ufb01nition and\nthere are 3 calls to evaluate\nReturns 3 tool calls to the function recommend\n\u201c\n\u201d\nCreates 3 function replies, one\nfor each recommendation\nAdd results of function execution to\nconversation history, and ask LLM to respond.\nExecute functions.\nReturn results of all three recommendations\nin natural language.\nGPT\nReturns the function\nname and parameters\nCould be the same\nor different LLM\nFigure 5.4\nA sample request returns three tool function calls and then submits the results back to the LLM \nto return a natural language response.\nChecks to \nsee if the \nstring is \ncontained \nwithin the \ntopic input\nIf no topic is \ndetected, returns \nthe default\nReturns a \nJSON object\n",
      "content_length": 2065,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "105\n5.2\nExecuting OpenAI functions\n3. Gifts\"\"\"    \nmessages = [{\"role\": \"user\", \"content\": user}]    \ntools = [    \n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"recommend\",\n            \"description\": \n                \"Provide a recommendation for any topic.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"topic\": {\n                        \"type\": \"string\",\n                        \"description\": \n                              \"The topic, \u2026 recommendation for.\",\n                        },\n                        \"rating\": {\n                            \"type\": \"string\",\n                            \"description\": \"The rating \u2026 was given.\",\n                            \"enum\": [\"good\", \"bad\", \"terrible\"]\n                            },\n                        },\n                \"required\": [\"topic\"],\n                },\n            },\n        }\n    ]\nListing 5.5 shows the request being made, which we\u2019ve covered before, but there are a\nfew things to note. This call uses a lower model such as GPT-3.5 because delegating\nfunctions is a more straightforward task and can be done using older, cheaper, less\nsophisticated language models.\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",    \n    messages=messages,    \n    tools=tools,    \n    tool_choice=\"auto\",  \n)\nresponse_message = response.choices[0].message    \nAt this point, after the API call, the response should hold the information for the\nrequired function calls. Remember, we asked the LLM to provide us with three recom-\nmendations, which means it should also provide us with three function call outputs, as\nshown in the following listing.\n \nListing 5.5\nparallel_functions.py (run_conversation, API call)\nThe user message \nasks for three \nrecommendations.\nNote that there is no \nsystem message.\nAdds the function \ndefinition to the tools \npart of the request\nLLMs that delegate to functions \ncan be simpler models.\nAdds the messages and tools definitions\nauto is the default.\nThe returned message \nfrom the LLM\n",
      "content_length": 2104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "106\nCHAPTER 5\nEmpowering agents with actions\ntool_calls = response_message.tool_calls    \nif tool_calls:    \n    available_functions = {\n        \"recommend\": recommend,\n    }    \n    # Step 4: send the info for each function call and function response to \nthe model\n    for tool_call in tool_calls:    \n        function_name = tool_call.function.name\n        function_to_call = available_functions[function_name]\n        function_args = json.loads(tool_call.function.arguments)\n        function_response = function_to_call(\n            topic=function_args.get(\"topic\"),    \n            rating=function_args.get(\"rating\"),\n        )\n        messages.append(    \n            {\n                \"tool_call_id\": tool_call.id,\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n    second_response = client.chat.completions.create(    \n        model=\"gpt-3.5-turbo-1106\",\n        messages=messages,\n    )\n    return second_response.choices[0].message.content    \nThe tool call outputs and the calls to the recommender function results are appended\nto the messages. Notice how messages now also contain the history of the first call.\nThis is then passed back to the LLM to construct a reply in natural language.\n Debug this example in VS Code by pressing the F5 key with the file open. The fol-\nlowing listing shows the output of running parallel_functions.py.\nHere are some recommendations for you:\n1. Time travel movies: \"Back to the Future\"\n2. Recipes: \"The best thing you ever ate.\"\n3. Gifts: \"A glorious new...\" (the recommendation was cut off, so I \ncouldn't provide the full recommendation)\nI hope you find these recommendations helpful! Let me know if you need \nmore information.\nThis completes this simple demonstration. For more advanced applications, the func-\ntions could do any number of things, from scraping websites to calling search engines\nto completing far more complex tasks.\nListing 5.6\nparallel_functions.py (run_conversation, tool_calls)\nListing 5.7\nparallel_functions.py (output)\nIf the response contains \ntool calls, execute them.\nOnly one function but \ncould contain several\nLoops through the calls and replays \nthe content back to the LLM\nExecutes the recommend \nfunction from extracted \nparameters\nAppends the results of \neach function call to the \nset of messages\nSends another request \nto the LLM with updated \ninformation and returns \nthe message reply\n",
      "content_length": 2513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "107\n5.3\nIntroducing Semantic Kernel\n Functions are an excellent way to cast outputs for a particular task. However, the\nwork of handling functions or tools and making secondary calls can be done in a\ncleaner and more efficient way. The following section will uncover a more robust sys-\ntem of adding actions to agents.\n5.3\nIntroducing Semantic Kernel\nSemantic Kernel (SK) is another open source project from Microsoft intended to help\nbuild AI applications, which we call agents. At its core, the project is best used to\ndefine actions, or what the platform calls semantic plugins, which are wrappers for skills\nand functions.\n Figure 5.5 shows how the SK can be used as a plugin and a consumer of OpenAI\nplugins. The SK relies on the OpenAI plugin definition to define a plugin. That way, it\ncan consume and publish itself or other plugins to other systems.\nAn OpenAI plugin definition maps precisely to the function definitions in listing 5.4.\nThis means that SK is the orchestrator of API tool calls, aka plugins. That also means\nthat SK can help organize multiple plugins with a chat interface or an agent.\nNOTE\nThe team at SK originally labeled the functional modules as skills. How-\never, to be more consistent with OpenAI, they have since renamed skills to\nLLM\nInterface is de ned like an OpenAI plugin.\n\ufb01\nSemantic\nKernel\nInterface as an OpenAI plugin\nChatGPT\nPlugins (Semantic Skills and Native Functions)\nMath Plugin\n(native function)\nRecommend Plugin\n(semantic function)\nGet Movies Plugin\n(native plugin)\nPlease recommend\na movie.\nCan be consumed as a plugin\nand also consumes plugins\nRequests can be made\ndirectly to the kernel\nThe kernel itself can be\nregistered as a plugin.\nLLM\nCan be the same\nLLM or different\nFigure 5.5\nHow the Semantic Kernel integrates as a plugin and can also consume plugins\n",
      "content_length": 1812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "108\nCHAPTER 5\nEmpowering agents with actions\nplugins. What is more confusing is that the code still uses the term skills. There-\nfore, throughout this chapter, we\u2019ll use skills and plugins to mean the same thing.\nSK is a useful tool for managing multiple plugins (actions for agents) and, as we\u2019ll see\nlater, can also assist with memory and planning tools. For this chapter, we\u2019ll focus on\nthe actions/plugins. In the next section, we look at how to get started using SK.\n5.3.1\nGetting started with SK semantic functions\nSK is easy to install and works within Python, Java, and C#. This is excellent news as it also\nallows plugins developed in one language to be consumed in a different language. How-\never, you can\u2019t yet develop a native function in one language and use it in another.\n We\u2019ll continue from where we left off for the Python environment using the\nchapter_4 workspace in VS Code. Be sure you have a workspace configured if you\nwant to explore and run any examples.\n Listing 5.8 shows how to install SK from a terminal within VS Code. You can also\ninstall the SK extension for VS Code. The extension can be a helpful tool to create\nplugins/skills, but it isn\u2019t required.\npip uninstall semantic-kernel    \ngit clone https://github.com/microsoft/semantic-kernel.git    \ncd semantic-kernel/python    \npip install -e .    \nOnce you finish the installation, open SK_connecting.py in VS Code. Listing 5.9\nshows a demo of running an example quickly through SK. The example creates a chat\ncompletion service using either OpenAI or Azure OpenAI.\nimport semantic_kernel as sk\nselected_service = \"OpenAI\"    \nkernel = sk.Kernel()    \nservice_id = None\nif selected_service == \"OpenAI\":\n    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n    api_key, org_id = sk.openai_settings_from_dot_env()    \n    service_id = \"oai_chat_gpt\"\n    kernel.add_service(\n        OpenAIChatCompletion(\n            service_id=service_id,\n            ai_model_id=\"gpt-3.5-turbo-1106\",\nListing 5.8\nInstalling Semantic Kernel \nListing 5.9\nSK_connecting.py\nUninstalls any previous installations of SK\nClones the \nrepository to \na local folder\nChanges to the source folder\nInstalls the editable package \nfrom the source folder\nSets the service you\u2019re using \n(OpenAI or Azure OpenAI)\nCreates the \nkernel\nLoads secrets \nfrom the .env file \nand sets them on \nthe chat service\n",
      "content_length": 2374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "109\n5.3\nIntroducing Semantic Kernel\n            api_key=api_key,\n            org_id=org_id,\n        ),\n    )\nelif selected_service == \"AzureOpenAI\":\n    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n    deployment, api_key, endpoint = \n\u27a5 sk.azure_openai_settings_from_dot_env()  \n    service_id = \"aoai_chat_completion\"\n    kernel.add_service(\n        AzureChatCompletion(\n            service_id=service_id,\n            deployment_name=deployment,\n            endpoint=endpoint,\n            api_key=api_key,\n        ),\n    )\n#This function is currently broken\nasync def run_prompt():\n    result = await kernel.invoke_prompt( \n              \u27a5 prompt=\"recommend a movie about \n\u27a5 time travel\")    \n    print(result)\n# Use asyncio.run to execute the async function\nasyncio.run(run_prompt())    \n###Output\nOne highly recommended time travel movie is \"Back to the Future\" (1985) \ndirected by Robert Zemeckis. This classic film follows the adventures of \nteenager Marty McFly (Michael J. Fox)\u2026\nRun the example by pressing F5 (debugging), and you should see an output similar to\nlisting 5.9. This example demonstrates how a semantic function can be created with SK\nand executed. A semantic function is the equivalent of a prompt template in prompt\nflow, another Microsoft tool. In this example, we define a simple prompt as a function.\n It\u2019s important to note that this semantic function isn\u2019t defined as a plugin. How-\never, the kernel can create the function as a self-contained semantic element that\ncan be executed against an LLM. Semantic functions can be used alone or regis-\ntered as plugins, as you\u2019ll see later. Let\u2019s jump to the next section, where we intro-\nduce contextual variables.\n5.3.2\nSemantic functions and context variables\nExpanding on the previous example, we can look at adding contextual variables to the\nsemantic function. This pattern of adding placeholders to prompt templates is one\nwe\u2019ll review over and over. In this example, we look at a prompt template that has\nplaceholders for subject, genre, format, and custom.\nLoads secrets \nfrom the .env file \nand sets them on \nthe chat service\nInvokes the \nprompt\nCalls the function \nasynchronously\n",
      "content_length": 2187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "110\nCHAPTER 5\nEmpowering agents with actions\n Open SK_context_variables.py in VS Code, as shown in the next listing. The\nprompt is equivalent to setting aside a system and user section of the prompt.\n#top section omitted\u2026\nprompt = \"\"\"    \nsystem:\nYou have vast knowledge of everything and can recommend anything provided \nyou are given the following criteria, the subject, genre, format and any \nother custom information.\nuser:\nPlease recommend a {{$format}} with the subject {{$subject}} and {{$genre}}.\nInclude the following custom information: {{$custom}}\n\"\"\"\nprompt_template_config = sk.PromptTemplateConfig(    \n    template=prompt,\n    name=\"tldr\",\n    template_format=\"semantic-kernel\",\n    input_variables=[\n        InputVariable(\n            name=\"format\", \n            description=\"The format to recommend\", \n            is_required=True\n        ),\n        InputVariable(\n            name=\"suject\", \n            description=\"The subject to recommend\", \n            is_required=True\n        ),\n        InputVariable(\n            name=\"genre\", \n            description=\"The genre to recommend\", \n            is_required=True\n        ),\n        InputVariable(\n            name=\"custom\",\n            description=\"Any custom information [CA]\n                       to enhance the recommendation\",\n            is_required=True,\n        ),\n    ],\n    execution_settings=execution_settings,\n)\nrecommend_function = kernel.create_function_from_prompt(    \n    prompt_template_config=prompt_template_config,\n    function_name=\"Recommend_Movies\",\n    plugin_name=\"Recommendation\",\n)\nListing 5.10\nSK_context_variables.py\nDefines a prompt \nwith placeholders\nConfigures a \nprompt template \nand input variable \ndefinitions\nCreates a kernel \nfunction from \nthe prompt\n",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "111\n5.4\nSynergizing semantic and native functions\nasync def run_recommendation(    \n    subject=\"time travel\",\n    format=\"movie\", \n    genre=\"medieval\", \n           custom=\"must be a comedy\"\n):\n    recommendation = await kernel.invoke(\n        recommend_function,\n        sk.KernelArguments(subject=subject,\n                      format=format, \n                      genre=genre, \n                      custom=custom),    \n    )\n    print(recommendation)\n# Use asyncio.run to execute the async function\nasyncio.run(run_recommendation())\n###Output\nOne movie that fits the criteria of being about time travel, set in a \nmedieval period, and being a comedy is \"The Visitors\" (Les Visiteurs) \nfrom 1993. This French film, directed by Jean-Marie Poir\u00e9, follows a \nknight and his squire who are transported to the modern era by a \nwizard\u2019s spell gone wrong.\u2026\nGo ahead and debug this example (F5), and wait for the output to be generated. That is\nthe basis for setting up SK and creating and exercising semantic functions. In the next\nsection, we move on to see how a semantic function can be registered as a skill/plugin.\n5.4\nSynergizing semantic and native functions\nSemantic functions encapsulate a prompt/profile and execute through interaction with\nan LLM. Native functions are the encapsulation of code that may perform anything\nfrom scraping websites to searching the web. Both semantic and native functions can\nregister as plugins/skills in the SK kernel.\n A function, semantic or native, can be registered as a plugin and used the same\nway we registered the earlier function directly with our API calls. When a function is\nregistered as a plugin, it becomes accessible to chat or agent interfaces, depending on\nthe use case. The next section looks at how a semantic function is created and regis-\ntered with the kernel.\n5.4.1\nCreating and registering a semantic skill/plugin\nThe VS Code extension for SK provides helpful tools for creating plugins/skills. In\nthis section, we\u2019ll use the SK extension to create a plugin/skill and then edit the com-\nponents of that extension. After that, we\u2019ll register and execute the plugin in the SK.\n Figure 5.6 shows the process for creating a new skill within VS Code using the SK\nextension. (Refer to appendix B for directions if you need to install this extension.)\nYou\u2019ll then be given the option for the skill/plugin folder to place the function.\nAlways group functions that are similar together. After creating a skill, enter the name\nCreates an asynchronous \nfunction to wrap the \nfunction call\nSets the \nkernel \nfunction \narguments\n",
      "content_length": 2581,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "112\nCHAPTER 5\nEmpowering agents with actions\nand description of the function you want to develop. Be sure to describe the function\nas if the LLM were going to use it.\nYou can see the completed skills and functions by opening the skills/plugin folder\nand reviewing the files. We\u2019ll follow the previously constructed example, so open the\nskills/Recommender/Recommend_Movies folder, as shown in figure 5.7. Inside this folder\nis a config.json file, the function description, and the semantic function/prompt in\na file called skprompt.txt.\n Listing 5.11 shows the contents of the semantic function definition, also known as\nthe plugin definition. Note that the type is marked as completion and not of type\nfunction because this is a semantic function. We would define a native function as a\ntype function.\n \n \n \n1. Select the icon to create a\nnew semantic skill/plugin.\n2. Select an existing skill\nfolder, or create a new one.\n3. Name the function.\n4. Then, provide a description.\nFigure 5.6\nThe process of creating a new skill/plugin\n",
      "content_length": 1031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "113\n5.4\nSynergizing semantic and native functions\n{\n    \"schema\": 1,\n    \"type\": \"completion\",    \n    \"description\": \"A function to recommend movies based on users list of \npreviously seen movies.\",\n    \"completion\": {    \n        \"max_tokens\": 256,\n        \"temperature\": 0,\n        \"top_p\": 0,\n        \"presence_penalty\": 0,\n        \"frequency_penalty\": 0\n    },\n    \"input\": {\n        \"parameters\": [\n            {\n                \"name\": \"input\",    \n                \"description\": \"The users list of previously seen movies.\",\n                \"defaultValue\": \"\"\n            }\n        ]\n    },\n    \"default_backends\": []\n}\nNext, we can look at the definition of the semantic function prompt, as shown in list-\ning 5.12. The format is a little different, but what we see here matches the earlier\nexamples using templating. This prompt recommends movies based on a list of mov-\nies the user has previously seen.\nYou are a wise movie recommender and you have been asked to recommend a \nmovie to a user.\nYou are provided a list of movies that the user has watched before.\nYou want to recommend a movie that the user has not watched before.\n[INPUT]\nListing 5.11\nRecommend_Movies/config.json\nListing 5.12\nRecommend_Movies/skprompt.txt\nThe folder containing\nthe\n/plugin\nskill\nAn inner folder that holds\nthe plugin/skill de\ufb01nitions\nDe\ufb01nes the function/plugin\ndescription\nPrompt that de\ufb01nes the\nsemantic function\nFigure 5.7\nThe file and folder \nstructure of a semantic \nfunction skill/plugin\nSemantic functions are \nfunctions of type completion.\nWe can also set the \ncompletion parameters for \nhow the function is called.\nDefines the parameters \ninput into the semantic \nfunction\n",
      "content_length": 1675,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "114\nCHAPTER 5\nEmpowering agents with actions\n{{$input}}\n[END INPUT]\nNow, we\u2019ll dive into the code that loads the skill/plugin and executes it in a simple\nexample. Open the SK_first_skill.py file in VS Code. The following listing shows\nan abridged version highlighting the new sections.\nkernel = sk.Kernel()\nplugins_directory = \"plugins\"\nrecommender = kernel.import_plugin_from_prompt_directory(\n    plugins_directory,\n    \"Recommender\",\n)    \nrecommend = recommender[\"Recommend_Movies\"]\nseen_movie_list = [    \n    \"Back to the Future\",\n    \"The Terminator\",\n    \"12 Monkeys\",\n    \"Looper\",\n    \"Groundhog Day\",\n    \"Primer\",\n    \"Donnie Darko\",\n    \"Interstellar\",\n    \"Time Bandits\",\n    \"Doctor Strange\",\n]\nasync def run():\n    result = await kernel.invoke(\n        recommend,\n        sk.KernelArguments(    \n            settings=execution_settings, input=\", \".join(seen_movie_list)\n        ),\n    )\n    print(result)\nasyncio.run(run())    \n###Output\nBased on the list of movies you've provided, it seems you have an \ninterest in science fiction, time travel, and mind-bending narratives. \nGiven that you've watched a mix of classics and modern films in this \ngenre, I would recommend the following movie that you have not watched \nbefore:\n\"Edge of Tomorrow\" (also known as \"Live Die Repeat: Edge of Tomorrow\")\u2026\nListing 5.13\nSK_first_skill.py (abridged listing)\nLoads the prompt from \nthe plugins folder\nList of user\u2019s previously \nseen movies\nInput is set to joined \nlist of seen movies.\nFunction is executed \nasynchronously.\n",
      "content_length": 1529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "115\n5.4\nSynergizing semantic and native functions\nThe code loads the skill/plugin from the skills directory and the plugin folder.\nWhen a skill is loaded into the kernel and not just created, it becomes a registered\nplugin. That means it can be executed directly as is done here or through an LLM\nchat conversation via the plugin interface.\n Run the code (F5), and you should see an output like listing 5.13. We now have a\nsimple semantic function that can be hosted as a plugin. However, this function\nrequires users to input a complete list of movies they have watched. We\u2019ll look at a\nmeans to fix this by introducing native functions in the next section.\n5.4.2\nApplying native functions\nAs stated, native functions are code that can do anything. In the following example,\nwe\u2019ll introduce a native function to assist the semantic function we built earlier.\n This native function will load a list of movies the user has previously seen, from a file.\nWhile this function introduces the concept of memory, we\u2019ll defer that discussion until\nchapter 8. Consider this new native function as any code that could virtually do anything.\n Native functions can be created and registered using the SK extension. For this exam-\nple, we\u2019ll create a native function directly in code to make the example easier to follow.\n Open SK_native_functions.py in VS Code. We\u2019ll start by looking at how the\nnative function is defined. A native function is typically defined within a class, which\nsimplifies managing and instantiating native functions.\nclass MySeenMoviesDatabase:\n    \"\"\"\n    Description: Manages the list of users seen movies.    \n    \"\"\"\n    @kernel_function(    \n        description=\"Loads a list of movies \u2026 user has already seen\",\n        name=\"LoadSeenMovies\",\n    )\n    def load_seen_movies(self) -> str:    \n        try:\n            with open(\"seen_movies.txt\", 'r') as file:    \n                lines = [line.strip() for line in file.readlines()]\n                comma_separated_string = ', '.join(lines)\n            return comma_separated_string\n        except Exception as e:\n            print(f\"Error reading file: {e}\")\n            return None\nWith the native function defined, we can see how it\u2019s used by scrolling down in the\nfile, as shown in the following listing.\n \n \nListing 5.14\nSK_native_functions.py (MySeenMovieDatabase)\nProvides a description\nfor the container class\nUses a decorator to \nprovide function \ndescription and \nname\nThe actual function \nreturns a list of movies in a \ncomma-separated string.\nLoads seen\nmovies from\nthe text file\n",
      "content_length": 2558,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "116\nCHAPTER 5\nEmpowering agents with actions\nplugins_directory = \"plugins\"\nrecommender = kernel.import_plugin_from_prompt_directory(\n    plugins_directory,\n    \"Recommender\",\n)    \nrecommend = recommender[\"Recommend_Movies\"]\nseen_movies_plugin = kernel.import_plugin_from_object(\n    MySeenMoviesDatabase(), \"SeenMoviesPlugin\"\n)    \nload_seen_movies = seen_movies_plugin[\"LoadSeenMovies\"]    \nasync def show_seen_movies():\n    seen_movie_list = await load_seen_movies(kernel)\n    return seen_movie_list\nseen_movie_list = asyncio.run(show_seen_movies())    \nprint(seen_movie_list)\nasync def run():     \n    result = await kernel.invoke(\n        recommend,\n        sk.KernelArguments(\n                settings=execution_settings,\n                input=seen_movie_list),\n    )\n    print(result)\nasyncio.run(run())\n###Output\nThe Matrix, The Matrix Reloaded, The Matrix Revolutions, The Matrix \nResurrections \u2013 output from print statement\nBased on your interest in the \"The Matrix\" series, it seems you enjoy \nscience fiction films with a strong philosophical undertone and action \nelements. Given that you've watched all\nOne important aspect to note is how the native function was imported into the kernel.\nThe act of importing to the kernel registers that function as a plugin/skill. This means\nthe function can be used as a skill from the kernel through other conversations or\ninteractions. We\u2019ll see how to embed a native function within a semantic function in\nthe next section.\nListing 5.15\nSK_native_functions (remaining code)\nLoads the semantic function \nas shown previously\nImports the skill \ninto the kernel and \nregisters the function \nas a plugin\nLoads the native \nfunction\nExecutes the \nfunction and returns \nthe list as a string\nWraps the \nplugin call in an \nasynchronous \nfunction and \nexecutes\n",
      "content_length": 1804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "117\n5.4\nSynergizing semantic and native functions\n5.4.3\nEmbedding native functions within semantic functions\nThere are plenty of powerful features within SK, but one beneficial feature is the abil-\nity to embed native or semantic functions within other semantic functions. The follow-\ning listing shows how a native function can be embedded within a semantic function.\nsk_prompt = \"\"\"\nYou are a wise movie recommender and you have been asked to recommend a \nmovie to a user.\nYou have a list of movies that the user has watched before.\nYou want to recommend a movie that \nthe user has not watched before.    \nMovie List: {{MySeenMoviesDatabase.LoadSeenMovies}}.    \n\"\"\"\nThe next example, SK_semantic_native_functions.py, uses inline native and seman-\ntic functions. Open the file in VS Code, and the following listing shows the code to\ncreate, register, and execute the functions.\nprompt_template_config = sk.PromptTemplateConfig(\n    template=sk_prompt,\n    name=\"tldr\",\n    template_format=\"semantic-kernel\",\n    execution_settings=execution_settings,\n)    \nrecommend_function = kernel.create_function_from_prompt(\n    prompt_template_config=prompt_template_config,\n    function_name=\"Recommend_Movies\",\n    plugin_name=\"Recommendation\",\n)    \nasync def run_recommendation():    \n    recommendation = await kernel.invoke(\n        recommend_function,\n        sk.KernelArguments(),\n    )\n    print(recommendation)\n# Use asyncio.run to execute the async function\nasyncio.run(run_recommendation())\n###Output\nBased on the list provided, it seems the user is a fan of the Matrix \nfranchise. Since they have watched all four existing Matrix movies, I \nwould recommend a\u2026\nListing 5.16\nSK_semantic_native_functions.py (skprompt)\nListing 5.17\nSK_semantic_native_functions.py (abridged)\nThe exact \ninstruction text \nas previous\nThe native function is referenced and identified\nby class name and function name.\nCreates the prompt template \nconfig for the prompt\nCreates an inline semantic \nfunction from the prompt\nExecutes the semantic \nfunction asynchronously\n",
      "content_length": 2051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "118\nCHAPTER 5\nEmpowering agents with actions\nRun the code, and you should see an output like listing 5.17. One important aspect to\nnote is that the native function is registered with the kernel, but the semantic function\nis not. This is important because function creation doesn\u2019t register a function.\n For this example to work correctly, the native function must be registered with the\nkernel, which uses the import_plugin function call\u2014the first line in listing 5.17. How-\never, the semantic function itself isn\u2019t registered. An easy way to register the function\nis to make it a plugin and import it.\n These simple exercises showcase ways to integrate plugins and skills into chat or\nagent interfaces. In the next section, we\u2019ll look at a complete example demonstrating\nadding a plugin representing a service or GPT interface to a chat function.\n5.5\nSemantic Kernel as an interactive service agent\nIn chapter 1, we introduced the concept of the GPT interface\u2014a new paradigm in\nconnecting services and other components to LLMs via plugins and semantic layers.\nSK provides an excellent abstraction for converting any service to a GPT interface.\n Figure 5.8 shows a GPT interface constructed around an API service called The\nMovie Database (TMDB; www.themoviedb.org). The TMDB site provides a free API that\nexposes information about movies and TV shows.\nTo follow along with the exercises in this section, you must register for a free account\nfrom TMDB and create an API key. Instructions for getting an API key can be found at\nthe TMDB website (www.themoviedb.org) or by asking a GPT-4 turbo or a more\nrecent LLM.\nUser\nWeb Interface\nThe Movie Database\nwww.themoviedb.org\nAPI Interface\nGPT Interface\nSemantic Kernel\nChat Interface\nAgent Interface\nA user can access the site now in\nthree ways: web, chat, or agent.\nSK acts as an abstraction\nand tool to expose the\ninterface as a plugin.\nThis is the semantic\nmapping of functions\nto API endpoints.\nThis is the developer\nAPI endpoint exposed\nby the site.\nFigure 5.8\nThis layer architecture diagram shows the role of a GPT interface and the Semantic \nKernel being exposed to chat or agent interfaces.\n",
      "content_length": 2146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "119\n5.5\nSemantic Kernel as an interactive service agent\n Over the next set of subsections, we\u2019ll create a GPT interface using an SK set of\nnative functions. Then, we\u2019ll use the SK kernel to test the interface and, later in this\nchapter, implement it as plugins into a chat function. In the next section, we look at\nbuilding a GPT interface against the TMDB API.\n5.5.1\nBuilding a semantic GPT interface\nTMDB is an excellent service, but it provides no semantic services or services that can\nbe plugged into ChatGPT or an agent. To do that, we must wrap the API calls that\nTMDB exposes in a semantic service layer.\n A semantic service layer is a GPT interface that exposes functions through natural\nlanguage. As discussed, to expose functions to ChatGPT or other interfaces such as\nagents, they must be defined as plugins. Fortunately, SK can create the plugins for us\nautomatically, given that we write our semantic service layer correctly.\n A native plugin or set of skills can act as a semantic layer. To create a native plugin,\ncreate a new plugin folder, and put a Python file holding a class containing the set of\nnative functions inside that folder. The SK extension currently doesn\u2019t do this well, so\nmanually creating the module works best.\n Figure 5.9 shows the structure of the new plugin called Movies and the semantic\nservice layer called tmdb.py. For native functions, the parent folder\u2019s name (Movies) is\nused in the import.\nOpen the tmdb.py file in VS Code, and look at the top of the file, as shown in listing\n5.18. This file contains a class called TMDbService, which exposes several functions\nthat map to API endpoint calls. The idea is to map the various relevant API function\ncalls in this semantic service layer. This will expose the functions as plugins for a chat\nor agent interface.\nfrom semantic_kernel.functions import kernel_funct\nimport requests\nimport inspect\ndef print_function_call():    \n    #omitted \u2026\nListing 5.18\ntmdb.py (top of file)\nParent skills folder\nName of the plugin folder\nFile/module containing the class\nand set of native functions\nFigure 5.9\nThe folder and file \nstructure of the TMDB plugin\nPrints the calls to \nthe functions for \ndebugging\n",
      "content_length": 2188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "120\nCHAPTER 5\nEmpowering agents with actions\nclass TMDbService:    \n    def __init__(self):\n        # enter your TMDb API key here\n        self.api_key = \"your-TMDb-api-key\"\n    @kernel_function(    \n        description=\"Gets the movie genre ID for a given genre name\",\n        name=\"get_movie_genre_id\",\n        input_description=\"The movie genre name of the genre_id to get\",\n        )\n    def get_movie_genre_id(self, genre_name: str) -> str:    \n        print_function_call()\n        base_url = \"https://api.themoviedb.org/3\"\n        endpoint = f\"{base_url}/genre/movie/list\n                     \u27a5 ?api_key={self.api_key}&language=en-US\"\n        response = requests.get(endpoint)    \n        if response.status_code == 200:    \n            genres = response.json()['genres']\n            for genre in genres:\n                if genre_name.lower() in genre['name'].lower():\n                    return str(genre['id'])    \n        return None\nThe bulk of the code for the TMDbService and the functions to call the TMDB end-\npoints was written with the help of GPT-4 Turbo. Then, each function was wrapped\nwith the sk_function decorator to expose it semantically.\n A few of the TMDB API calls have been mapped semantically. Listing 5.19 shows\nanother example of a function exposed to the semantic service layer. This function\npulls a current top 10 list of movies playing for a particular genre.\n@kernel_function(    \n        description=\"\u201d\u201d\nGets a list of currently playing movies for a given genre\u201d\u201d\",\n        name=\"get_top_movies_by_genre\",\n        input_description=\"The genre of the movies to get\",\n        )\n    def get_top_movies_by_genre(self, genre: str) -> str:\n        print_function_call()\n        genre_id = self.get_movie_genre_id(genre)    \n        if genre_id:\n            base_url = \"https://api.themoviedb.org/3\n            playing_movies_endpoint = f\"{base_url}/movie/now_playing?\n\u27a5 api_key={self.api_key}&language=en-US\"\n            response = requests.get(\n                          playing_movies_endpoint)    \n            if response.status_code != 200:\n                return \"\"\nListing 5.19\ntmdb.py (get_top_movies_by_genre)\nTop-level service \nand decorator \nused to describe \nthe function (good \ndescriptions are \nimportant)\nFunction wrapped in \nsemantic wrapper; \nshould return str\nCalls the API endpoint, and, \nif good (code 200), checks \nfor matching genre\nFound the genre, \nreturns the id\nDecorates the function \nwith descriptions\nFinds the genre \nid for the given \ngenre name\nGets a list of currently \nplaying movies\n",
      "content_length": 2548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "121\n5.5\nSemantic Kernel as an interactive service agent\n            playing_movies = response.json()['results'\n            for movie in playing_movies:    \n                movie['genre_ids'] = [str(genre_id)  \n                      \u27a5 for genre_id in movie['genre_ids']]\n            filtered_movies = [movie for movie \n\u27a5 in playing_movies if genre_id \n\u27a5 in movie['genre_ids']][:10]    \n            results = \", \".join([movie['title'] for movie in \nfiltered_movies])\n            return results\n        else:\n            return \"\"\nLook through the various other API calls mapped semantically. As you can see, there\nis a well-defined pattern for converting API calls to a semantic service. Before we run\nthe full service, we\u2019ll test each of the functions in the next section.\n5.5.2\nTesting semantic services\nIn a real-world application, you\u2019ll likely want to write a complete set of unit or integra-\ntion tests for each semantic service function. We won\u2019t do that here; instead, we\u2019ll\nwrite a quick helper script to test the various functions.\n Open test_tmdb_service.py in VS Code, and review the code, as shown in listing\n5.20. You can comment and uncomment any functions to test them in isolation. Be\nsure to have only one function uncommented at a time.\nimport semantic_kernel as sk\nfrom plugins.Movies.tmdb import TMDbService\nasync def main():\n    kernel = sk.Kernel()    \n    tmdb_service = kernel.import_plugin_from_object \n\u27a5 (TMDbService(), \"TMDBService\")    \n    print(\n        await tmdb_service[\"get_movie_genre_id\"](\n            kernel, sk.KernelArguments(\n                            genre_name=\"action\")    \n        )\n    )    \n    print(\n        await tmdb_service[\"get_tv_show_genre_id\"](\n            kernel, sk.KernelArguments(\n                            genre_name=\"action\")    \n        )\n    )    \n    print(\n        await tmdb_service[\"get_top_movies_by_genre\"](\n            kernel, sk.KernelArguments(\nListing 5.20\ntest_tmdb_service.py\nConverts \ngenre_ids \nto strings\nChecks to see \nif the genre id \nmatches movie \ngenres\nInstantiates \nthe kernel\nImports the \nplugin service\nInputs parameter \nto functions, \nwhen needed\nExecutes and \ntests the various \nfunctions\nInputs parameter \nto functions, \nwhen needed\nExecutes and \ntests the various \nfunctions\n",
      "content_length": 2269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "122\nCHAPTER 5\nEmpowering agents with actions\n                            genre_name=\"action\")    \n        )\n    )    \n    print(\n        await tmdb_service[\"get_top_tv_shows_by_genre\"](\n            kernel, sk.KernelArguments(\n                            genre_name=\"action\")    \n        )\n    )\n    print(await tmdb_service[\"get_movie_genres\"](\nkernel, sk.KernelArguments()))                       \n    print(await tmdb_service[\"get_tv_show_genres\"](\nkernel, sk.KernelArguments()))                       \n# Run the main function\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())    \n###Output\nFunction name: get_top_tv_shows_by_genre    \nArguments:\n  self = <skills.Movies.tmdb.TMDbService object at 0x00000159F52090C0>\n  genre = action\nFunction name: get_tv_show_genre_id    \nArguments:\n  self = <skills.Movies.tmdb.TMDbService object at 0x00000159F52090C0>\n  genre_name = action\nArcane, One Piece, Rick and Morty, Avatar: The Last Airbender, Fullmetal \nAlchemist: Brotherhood, Demon Slayer: Kimetsu no Yaiba, Invincible, \nAttack on Titan, My Hero Academia, Fighting Spirit, The Owl House\nThe real power of SK is shown in this test. Notice how the TMDbService class is imported\nas a plugin, but we don\u2019t have to define any plugin configurations other than what we\nalready did? By just writing one class that wrapped a few API functions, we\u2019ve exposed\npart of the TMDB API semantically. Now, with the functions exposed, we can look at\nhow they can be used as plugins for a chat interface in the next section.\n5.5.3\nInteractive chat with the semantic service layer\nWith the TMDB functions exposed semantically, we can move on to integrating them\ninto a chat interface. This will allow us to converse naturally in this interface to get var-\nious information, such as current top movies.\n Open SK_service_chat.py in VS Code. Scroll down to the start of the new section\nof code that creates the functions, as shown in listing 5.21. The functions created here\nare now exposed as plugins, except we filter out the chat function, which we don\u2019t\nwant to expose as a plugin. The chat function here allows the user to converse directly\nwith the LLM and shouldn\u2019t be a plugin.\nInputs parameter \nto functions, \nwhen needed\nExecutes and tests the various functions\nExecutes and tests \nthe various functions\nExecutes main \nasynchronously\nCalls print\nfunction details\nto notify when\nthe function is\nbeing called\n",
      "content_length": 2417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "123\n5.5\nSemantic Kernel as an interactive service agent\nsystem_message = \"You are a helpful AI assistant.\"\ntmdb_service = kernel.import_plugin_from_object(\nTMDbService(), \"TMDBService\")    \n# extracted section of code\nexecution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n        service_id=service_id,\n        ai_model_id=model_id,\n        max_tokens=2000,\n        temperature=0.7,\n        top_p=0.8,\n        tool_choice=\"auto\",\n        tools=get_tool_call_object(\n            kernel, {\"exclude_plugin\": [\"ChatBot\"]}),    \n    )\nprompt_config = sk.PromptTemplateConfig.from_completion_parameters(\n    max_tokens=2000,\n    temperature=0.7,\n    top_p=0.8,\n    function_call=\"auto\",\n    chat_system_prompt=system_message,\n)    \nprompt_template = OpenAIChatPromptTemplate(\n    \"{{$user_input}}\", kernel.prompt_template_engine, prompt_config\n)    \nhistory = ChatHistory()\nhistory.add_system_message(\"You recommend movies and TV Shows.\")\nhistory.add_user_message(\"Hi there, who are you?\")\nhistory.add_assistant_message(\n    \"I am Rudy, the recommender chat bot. I'm trying to figure out what \npeople need.\"\n)    \nchat_function = kernel.create_function_from_prompt(\n    prompt_template_config=prompt_template,\n    plugin_name=\"ChatBot\",\n    function_name=\"Chat\",\n)    \nNext, we can continue by scrolling in the same file to review the chat function, as\nshown in the following listing.\nasync def chat() -> bool:\n    try:\n        user_input = input(\"User:> \")    \n    except KeyboardInterrupt:\nListing 5.21\nSK_service_chat.py (function setup)\nListing 5.22\nSK_service_chat.py (chat function)\nImports the \nTMDbService \nas a plugin\nConfigures the \nexecution settings and \nadds filtered tools\nConfigures \nthe prompt \nconfiguration\nDefines the input \ntemplate and takes full \nstrings as user input\nAdds the chat history object \nand populates some history\nCreates the chat function\nInput is taken \ndirectly from the \nterminal/console.\n",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "124\nCHAPTER 5\nEmpowering agents with actions\n        print(\"\\n\\nExiting chat...\")\n        return False\n    except EOFError:\n        print(\"\\n\\nExiting chat...\")\n        return False\n    if user_input == \"exit\":    \n        print(\"\\n\\nExiting chat...\")\n        return False\n    arguments = sk.KernelArguments(    \n        user_input=user_input,\n        history=(\"\\n\").join(\n           [f\"{msg.role}: {msg.content}\" for msg in history]),\n    )\n    result = await chat_completion_with_tool_call(    \n        kernel=kernel,\n        arguments=arguments,\n        chat_plugin_name=\"ChatBot\",\n        chat_function_name=\"Chat\",\n        chat_history=history,\n    )\n    print(f\"AI Agent:> {result}\")\n    return True\nLastly, scroll down to the bottom of the file, and review the primary function. This is\nthe code that calls the chat function in a loop.\nasync def main() -> None:\n    chatting = True\n    context = kernel.create_new_context()\n    print(\"Welcome to your first AI Agent\\    \n\\n  Type 'exit' to exit.\\\n\\n  Ask to get a list of currently playing movies by genre.\"\n    )\n    while chatting:    \n        chatting, context = await chat(context)    \nif __name__ == \"__main__\":\n    asyncio.ru n(main())\nRun the chat interface, run the file (F5), and then ask about movies or television\nshows of a particular genre. An example conversation session is shown in listing 5.24.\nThis output shows how a request to list movies from two genres made the chat inter-\nface make multiple calls to the get_top_movie_by_genre function.\nWelcome to your first AI Agent\n  Type 'exit' to exit.\nListing 5.23\nSK_service_chat.py (main function)\nListing 5.24\nSK_service_chat.py (example conversation) \nIf the user types \nexit, then exit \nthe chat.\nCreates arguments to \npass to the function\nUses the utility \nfunction to call \nthe function and \nexecute the tool\nIntroduction \nto the user\nContinues until \nchatting is False\nCalls the \nchat function \nasynchronously\n",
      "content_length": 1938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "125\n5.6\nThinking semantically when writing semantic services\n  Ask to get a list of currently playing movies by genre.\nUser:> Input: can you give me list of the current top playing movies for \nthe action and comedy genres?\nFunction name: get_top_movies_by_genre    \nArguments:\n  genre = action\nFunction name: get_movie_genre_id    \nArguments:\n  genre_name = action\nFunction name: get_top_movies_by_genre    \nArguments:\n  genre = comedy\nFunction name: get_movie_genre_id    \nArguments:\n  genre_name = comedy\nAgent:> Here are the current top-playing movies \nfor the action and comedy genres:\n**Action:**    \n1. The Hunger Games: The Ballad of Songbirds & Snakes\n2. Rebel Moon - Part One: A Child of Fire\n3. Aquaman and the Lost Kingdom\n4. Silent Night\n5. The Family Plan\n6. Freelance\n7. Migration\n8. Sound of Freedom\n9. Godzilla Minus One\n**Comedy:**    \n1. The Family Plan\n2. Wonka\n3. Freelance\n4. Saltburn\n5. Chicken Run: Dawn of the Nugget\n6. Trolls Band Together\n7. There's Something in the Barn\n8. Migration\nPlease note that some movies may overlap in both genres, such as \n\"The Family Plan\" and \"Freelance\n.\"\nBe sure to explore the chat interface\u2019s boundaries and what you can ask for from the\nTMDB service. For example, try asking for a list of genres for movies or television\nshows. This service is a good first try, but we can perhaps do better, as we\u2019ll see in the\nnext section.\n5.6\nThinking semantically when writing semantic services\nNow we\u2019ve seen an excellent demonstration of converting an API into a semantic ser-\nvice interface. As it is, the functions return the titles of the top movies and television\nshows currently playing. However, by just returning the titles, we\u2019re limiting the ability\nof the LLM to parse the results on its own.\nLLM makes two calls to \nget_top_movies_by_genre.\nInternal\ncall to\nget the\ngenre id\nList of the top \ncurrent action \nmovies\nList of the top \ncurrent comedy \nmovies\n",
      "content_length": 1917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "126\nCHAPTER 5\nEmpowering agents with actions\n Therefore, we\u2019ll create a v2 version of TMDbService to correct this and return the\nresults as JSON strings. Open the file tmdb_v2.py in VS Code, and scroll down to the\nget_top_movies_by_genre function.\ndef get_top_movies_by_genre(self, genre: str) -> str:\n        print_function_call()\n        genre_id = self.get_movie_genre_id(genre)\n        if genre_id:\n            #same code \u2026\n            return json.dumps(filtered_movies)    \n        else:\n            return \"\"\nNow open SK_service_chat.py in VS Code, and comment and uncomment the line\nshown in listing 5.26. This will then use version 2 of the TMDbService that outputs\nresults as full JSON documents in a single string.\n#from skills.Movies.tmdb import TMDbService    \nfrom skills.Movies.tmdb_v2 import TMDbService    \nRerun the SK_service_chat.py file in VS Code, and alter your query slightly, as\nshown by the output in the following listing.\nUser:> get a list of currently playing movies for the \naction genre and only return movies about space    \nAgent:> To find currently playing action movies that are specifically \nabout space, I will need to manually filter the provided list for those \nthat have space-related themes in their overview. Here's what fits that \ncriteria from the list:\n1. **Rebel Moon - Part One: A Child of Fire**    \n   - Release Date: 2023-12-15\n   - Overview: When a peaceful colony on the edge of the galaxy finds \nitself threatened by the armies of the tyrannical Regent Balisarius, \nthey dispatch Kora, a young woman with a mysterious past, to seek out \nwarriors from neighboring planets to help them take a stand.\nThis is the only movie from the provided list that clearly mentions a \nspace-related theme in its overview. \u2026\nListing 5.25\ntmdb_v2.py (get_top_movies_by_genre)\nListing 5.26\nSK_service_chat.py (modifying imports)\nListing 5.27\nSK_service_chat.py (TMDb_v2 service output)\nNow returns a \nfiltered list as \na JSON string\nComment out this line.\nUncomment this line to use \nversion 2 of the service.\nNew query asks to \ninclude an additional \nfilter for space\nThe LLM calls the service and then\nreviews the returned results that\nmatch the filter.\n",
      "content_length": 2189,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "127\n5.7\nExercises\nBecause the semantic service functions now return the complete movie listing in\nJSON, the LLM can apply additional filtering. This is the real power of semantic\nservices, allowing you to process the data through the LLM. We won\u2019t see this power\nby just returning a list of titles.\n This last exercise demonstrated the change in mentality you need to make when\nwriting semantic service layers. Generally, you\u2019ll typically want to return as much infor-\nmation as possible. Returning more information takes advantage of the LLM abilities\nto filter, sort, and transform data independently. In the next chapter, we\u2019ll explore\nbuilding autonomous agents using behavior trees.\n5.7\nExercises\nComplete the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Creating a Basic Plugin for Temperature Conversion \nObjective\u2014Familiarize yourself with creating a simple plugin for the OpenAI chat\ncompletions API. \nTasks:\n\u2013 Develop a plugin that converts temperatures between Celsius and Fahrenheit.\n\u2013 Test the plugin by integrating it into a simple OpenAI chat session where\nusers can ask for temperature conversions.\n\uf0a1Exercise 2\u2014Developing a Weather Information Plugin \nObjective\u2014Learn to create a plugin that performs a unique task. \nTasks:\n\u2013 Create a plugin for the OpenAI chat completions API that fetches weather\ninformation from a public API.\n\u2013 Ensure the plugin can handle user requests for current weather conditions\nin different cities.\n\uf0a1Exercise 3\u2014Crafting a Creative Semantic Function \nObjective\u2014Explore the creation of semantic functions. \nTasks:\n\u2013 Develop a semantic function that writes a poem or tells a children\u2019s story\nbased on user input.\n\u2013 Test the function in a chat session to ensure it generates creative and coher-\nent outputs.\n\uf0a1Exercise 4\u2014Enhancing Semantic Functions with Native Functions \nObjective\u2014Understand how to combine semantic and native functions. \nTasks:\n\u2013 Create a semantic function that uses a native function to enhance its capabilities.\n\u2013 For example, develop a semantic function that generates a meal plan and\nuses a native function to fetch nutritional information for the ingredients.\n",
      "content_length": 2154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "128\nCHAPTER 5\nEmpowering agents with actions\n\uf0a1Exercise 5\u2014Wrapping an Existing Web API with Semantic Kernel \nObjective\u2014Learn to wrap existing web APIs as semantic service plugins. \nTasks:\n\u2013 Use SK to wrap a news API and expose it as a semantic service plugin in a\nchat agent.\n\u2013 Ensure the plugin can handle user requests for the latest news articles on\nvarious topics.\nSummary\n\uf0a1Agent actions extend the capabilities of an agent system, such as ChatGPT. This\nincludes the ability to add plugins to ChatGPT and LLMs to function as proxies\nfor actions.\n\uf0a1OpenAI supports function definitions and plugins within an OpenAI API ses-\nsion. This includes adding function definitions to LLM API calls and under-\nstanding how these functions allow the LLM to perform additional actions.\n\uf0a1The Semantic Kernel (SK) is an open source project from Microsoft that can be\nused to build AI applications and agent systems. This includes the role of seman-\ntic plugins in defining native and semantic functions.\n\uf0a1Semantic functions encapsulate the prompt/profile template used to engage\nan LLM.\n\uf0a1Native functions encapsulate code that performs or executes an action using an\nAPI or other interface.\n\uf0a1Semantic functions can be combined with other semantic or native functions\nand layered within one another as execution stages.\n\uf0a1SK can be used to create a GPT interface over the top of API calls in a semantic\nservice layer and expose them as chat or agent interface plugins.\n\uf0a1Semantic services represent the interaction between LLMs and plugins, as well as\nthe practical implementation of these concepts in creating efficient AI agents.\n",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "129\nBuilding\nautonomous assistants\nNow that we\u2019ve covered how actions extend the power/capabilities of agents, we\ncan look at how behavior trees can guide agentic systems. We\u2019ll start by understand-\ning the basics of behavior trees and how they control robotics and AI in games. \n We\u2019ll return to agentic actions and examine how actions can be implemented\non the OpenAI Assistants platform using the GPT Assistants Playground project.\nFrom there, we\u2019ll look at how to build an autonomous agentic behavior tree (ABT)\nusing OpenAI assistants. Then, we\u2019ll move on to understanding the need for con-\ntrols and guardrails on autonomous agents and using control barrier functions. \nThis chapter covers\n\uf0a1Behavior trees for robotics and AI apps\n\uf0a1GPT Assistants Playground and creating \nassistants and actions\n\uf0a1Autonomous control of agentic behavior trees\n\uf0a1Simulating conversational multi-agent systems \nvia agentic behavior trees\n\uf0a1Using back chaining to create behavior trees \nfor complex systems \n",
      "content_length": 990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "130\nCHAPTER 6\nBuilding autonomous assistants\n In the final section of the chapter, we\u2019ll examine the use of the AgentOps platform\nto monitor our autonomous behavior-driven agentic systems. This will be an exciting\nchapter with several challenges. Let\u2019s begin by jumping into the next section, which\nintroduces behavior trees.\n6.1\nIntroducing behavior trees\nBehavior trees are a long-established pattern used to control robotics and AI in games.\nRodney A. Brooks first introduced the concept in his \u201cA Robust Layered Control Sys-\ntem for a Mobile Robot\u201d paper in 1986. This laid the groundwork for a pattern that\nexpanded on using the tree and node structure we have today.\n If you\u2019ve ever played a computer game with nonplayer characters (NPCs) or inter-\nacted with advanced robotic systems, you\u2019ve witnessed behavior trees at work. Figure 6.1\nshows a simple behavior tree. The tree represents all the primary nodes: selector or\nfallback nodes, sequence nodes, action nodes, and condition nodes.\nTable 6.1 describes the functions and purpose of the primary nodes we\u2019ll explore in\nthis book. There are other nodes and node types, and you can even create custom\nnodes, but for now, we\u2019ll focus on those in the table.\nTable 6.1\nThe primary nodes used in behavior trees\nNode\nPurpose\nFunction\nType\nSelector \n(fallback)\nThis node works by selecting the first \nchild that completes successfully. It\u2019s \noften called the fallback node \nbecause it will always fall back to the \nlast successful node that executed.\nThe node calls its children in \nsequence and stops executing \nwhen the first child succeeds. \nWhen a child node succeeds, it will \nreturn success; if no nodes suc-\nceed, it returns failure.\nComposite\n\u2192\n?\n?\n\u2192\n\u2192\nThe root node can be any\ncomposite node, such as\nselector or sequence.\nExecution \ufb02ows from\ntop to bottom and\nthen from left to right.\n1\n2\n3\n4\n5\n6\n7\nFigure 6.1\nA simple behavior tree of eating an apple or a pear\n",
      "content_length": 1925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "131\n6.1\nIntroducing behavior trees\nThe primary nodes in table 6.1 can provide enough functionality to handle numerous\nuse cases. However, understanding behavior trees initially can be daunting. You won\u2019t\nappreciate their underlying complexity until you start using them. Before we build\nsome simple trees, we want to look at execution in more detail in the next section.\n6.1.1\nUnderstanding behavior tree execution\nUnderstanding how behavior trees execute is crucial to designing and implementing\nbehavior trees. Unlike most concepts in computer science, behavior trees operate in\nterms of success and failure. When a node in a behavior tree executes, it will return\neither success or failure; this even applies to conditions and selector nodes.\n Behavior trees execute from top to bottom and left to right. Figure 6.2 shows\nthe process and what happens if a node fails or succeeds. In the example, the AI\nthe tree controls has an apple but no pear. In the first sequence node, a condition\nchecks if the AI has an apple. Because the AI doesn\u2019t have an apple, it aborts the\nsequence and falls back to the selector. The selector then selects its next child\nnode, another sequence, that checks if the AI has a pear, and because it does, the\nAI eats the apple.\nSequence\nThis node executes all of its children \nin sequence until one node fails or \nthey all complete successfully.\nThe node calls each of its children \nin sequence regardless of whether \nthey fail or succeed. If all children \nsucceed, it returns success, and \nfailure if just one child fails.\nComposite\nCondition\nBehavior trees don\u2019t use Boolean \nlogic but rather success or failure \nas a means of control. The condition \nreturns success if the condition is \ntrue and false otherwise.\nThe node returns success or failure \nbased on a condition.\nTask\nAction\nThis is where the action happens. \nThe node executes and returns suc-\ncess if successful or returns failure \notherwise.\nTask\nDecorator\nThey work by controlling the execu-\ntion of child nodes. They are often \nreferred to as conditionals because \nthey can determine whether a node is \nworth executing or safe to execute. \nThe node controls execution of \nthe child nodes. Decorators can \noperate as control barrier functions \nto block or prevent unwanted \nbehaviors.\nDecorator\nParallel\nThis node executes all of its nodes in \nparallel. Success or failure is con-\ntrolled by a threshold of the number \nof children needed to succeed to \nreturn success.\nThe node executes all of its child \nnodes in sequence regardless of \nthe status of the nodes.\nComposite\nTable 6.1\nThe primary nodes used in behavior trees (continued)\nNode\nPurpose\nFunction\nType\n",
      "content_length": 2658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "132\nCHAPTER 6\nBuilding autonomous assistants\nBehavior trees provide control over how an AI system will execute at a macro or micro\nlevel. Regarding robotics, behavior trees will typically be designed to operate at the\nmicro level, where each action or condition is a small event, such as detecting the\napple. Conversely, behavior trees can also control more macro systems, such as NPCs\nin games, where each action may be a combination of events, like attacking the player.\n For agentic systems, behavior trees support controlling an agent or assistant at\nyour chosen level. We\u2019ll explore controlling agents at the task and, in later chapters,\nthe planning levels. After all, with the power of LLMs, agents can construct their own\nbehavior tree.\n Of course, several other forms of AI control could be used to control agentic sys-\ntems. The next section will examine those different systems and compare them to\nbehavior trees.\n6.1.2\nDeciding on behavior trees\nNumerous other AI control systems have benefits and are worth exploring in con-\ntrolling agentic systems. They can demonstrate the benefits of behavior trees and pro-\nvide other options for specific use cases. The behavior tree is an excellent pattern, but\nit isn\u2019t the only one, and it\u2019s worth learning about others.\n Table 6.2 highlights several other systems we may consider for controlling AI sys-\ntems. Each item in the table describes what the method does, its shortcomings, and its\npossible application to agentic AI control.\n \nThe root node\nexecutes according\nto its composite type.\nSequence nodes execute\nall child nodes in order; if\na node fails, the sequence\nfails.\nIf the previous node in\na sequence fails, the\nsequence is aborted.\nIn this example, the AI has the\npear, which returns success\nand then eats the pear.\nNode success/failure\n\ufb02ows back to the\nparent node.\nSelector nodes execute\nall children and return\nsuccess on the \ufb01rst child\nthat returns success.\n?\n\u2192\n\u2192\n1\n2\n3\n4\n6\n7\nFigure 6.2\nThe execution process of a simple behavior tree\n",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "133\n6.1\nIntroducing behavior trees\nTable 6.2\nComparison of other AI control systems\nControl name\nDescription\nShortcomings \nControl agentic AI?\nFinite state \nmachinea (FSM)\nFSMs model AI using a set \nof states and transitions \ntriggered by events or \nconditions.\nFSMs can become \nunwieldy with increasing \ncomplexity.\nFSMs aren\u2019t practical for \nagents because they don\u2019t \nscale well.\nDecision treeb\nDecision trees use a \ntree-like model of deci-\nsions and their possible \nconsequences.\nDecision trees can suffer \nfrom overfitting and lack \ngeneralization in complex \nscenarios.\nDecision trees can be \nadapted and enhanced \nwith behavior trees.\nUtility-based \nsystemb\nUtility functions evaluate \nand select the best action \nbased on the current \nsituation.\nThese systems require \ncareful design of utility \nfunctions to balance \npriorities.\nThis pattern can be \nadopted within a behavior \ntree.\nRule-based \nsystema\nThis set of if-then rules \ndefine the behavior of \nthe AI.\nThese systems can \nbecome cumbersome with \nmany rules, leading to \npotential conflicts.\nThese aren\u2019t very practical \nwhen paired with agentic \nsystems powered by \nLLMs.\nPlanning sys-\ntemc\nPlanning systems gener-\nate a sequence of actions \nto achieve a specific goal \nusing planning algorithms.\nThese systems are com-\nputationally expensive and \nrequire significant domain \nknowledge.\nAgents can already imple-\nment such patterns on \ntheir own as we\u2019ll see in \nlater chapters.\nBehavioral \ncloningc\nBehavioral cloning \nrefers to learning policies \nby mimicking expert \ndemonstrations.\nThis system may struggle \nwith generalization to \nunseen situations.\nThis can be incorporated \ninto behavior trees or into \na specific task.\nHierarchical \nTask Network \n(HTN)d\nHTNs decompose tasks \ninto smaller, manageable \nsubtasks arranged in a \nhierarchy.\nThese are complex to \nmanage and design for \nvery large tasks.\nHTNs allow for better orga-\nnization and execution of \ncomplex tasks. This pat-\ntern can be used for larger \nagentic systems.\nBlackboard \nsystemb\nThese systems feature \ncollaborative problem-solv-\ning using a shared black-\nboard for different \nsubsystems.\nThese systems are diffi-\ncult to implement and \nmanage communication \nbetween subsystems.\nAgentic systems can \nimplement similar pat-\nterns using conversation \nor group chats/threads.\nGenetic \nalgorithm (GA)d\nThese optimization tech-\nniques are inspired by \nnatural selection to \nevolve solutions to \nsolve problems.\nGAs are computationally \nintensive and may not \nalways find the optimal \nsolution.\nGAs have potential and \ncould even be used to \noptimize behavior trees.\na Not practical when considering complex agentic systems\nb Exists in behavior trees or can easily be incorporated\nc Typically applied at the task or action/condition level\nd Advanced systems that would require heavy lifting when applied to agents\n",
      "content_length": 2852,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "134\nCHAPTER 6\nBuilding autonomous assistants\nIn later chapters of this book, we\u2019ll investigate some of the patterns discussed in\ntable 6.2. Overall, several patterns can be enhanced or incorporated using behavior\ntrees as the base. While other patterns, such as FSMs, may be helpful for small experi-\nments, they lack the scalability of behavior trees.\n Behavior trees can provide several benefits as an AI control system, including scal-\nability. The following list highlights other notable benefits of using behavior trees:\n\uf0a1Modularity and reusability\u2014Behavior trees promote a modular approach to design-\ning behaviors, allowing developers to create reusable components. Nodes in a\nbehavior tree can be easily reused across different parts of the tree or even in\ndifferent projects, enhancing maintainability and reducing development time.\n\uf0a1Scalability\u2014As systems grow in complexity, behavior trees handle the addition\nof new behaviors more gracefully than other methods, such as FSMs. Behavior\ntrees allow for the hierarchical organization of tasks, making it easier to manage\nand understand large behavior sets.\n\uf0a1Flexibility and extensibility\u2014Behavior trees offer a flexible framework where new\nnodes (actions, conditions, decorators) can be added without drastically alter-\ning the existing structure. This extensibility makes it straightforward to intro-\nduce new behaviors or modify existing ones to adapt to new requirements.\n\uf0a1Debugging and visualization\u2014Behavior trees provide a clear and intuitive visual\nrepresentation of behaviors, which is beneficial for debugging and understand-\ning the decision-making process. Tools that support behavior trees often\ninclude graphical editors that allow developers to visualize and debug the tree\nstructure, making it easier to identify and fix problems.\n\uf0a1Decoupling of decision logic\u2014Behavior trees separate the decision-making and\nexecution logic, promoting a clear distinction between high-level strategy and\nlow-level actions. This decoupling simplifies the design and allows for more\nstraightforward modifications and testing of specific behavior parts without\naffecting the entire system.\nHaving made a strong case for behavior trees, we should now consider how to imple-\nment them in code. In the next section, we look at how to build a simple behavior\ntree, using Python code.\n6.1.3\nRunning behavior trees with Python and py_trees\nBecause behavior trees have been around for so long and have been incorporated into\nmany technologies, creating a sample demonstration is very simple. Of course, the easi-\nest way is to ask ChatGPT or your favorite AI chat tool. Listing 6.1 shows the result of\nusing a prompt to generate the code sample and submitting figure 6.1 as the example\ntree. The final code had to be corrected for simple naming and parameter errors.\nNOTE\nAll the code for this chapter can be found by downloading the GPT\nAssistants Playground project at https://mng.bz/Ea0q.\n",
      "content_length": 2940,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "135\n6.1\nIntroducing behavior trees\nimport py_trees\nclass HasApple(py_trees.behaviour.Behaviour):     \n    def __init__(self, name):\n        super(HasApple, self).__init__(name)\n    def update(self):        \n        if True:  \n            return py_trees.common.Status.SUCCESS\n        else:\n            return py_trees.common.Status.FAILURE\n# Other classes omitted\u2026\nhas_apple = HasApple(name=\"Has apple\")     \neat_apple = EatApple(name=\"Eat apple\")     \nsequence_1 = py_trees.composites.Sequence(name=\"Sequence 1\", memory=True)\nsequence_1.add_children([has_apple, eat_apple])                             \nhas_pear = HasPear(name=\"Has pear\")        \neat_pear = EatPear(name=\"Eat pear\")        \nsequence_2 = py_trees.composites.Sequence(name=\"Sequence 2\", memory=True)\nsequence_2.add_children([has_pear, eat_pear])                               \nroot = py_trees.composites.Selector(name=\"Selector\", memory=True)\nroot.add_children([sequence_1, sequence_2])                                 \nbehavior_tree = py_trees.trees.BehaviourTree(root)    \npy_trees.logging.level = py_trees.logging.Level.DEBUG   \nfor i in range(1, 4):                                                     \n    print(\"\\n------------------ Tick {0} ------------------\".format(i))\n    behavior_tree.tick()                                                  \n### Start of output\n------------------ Tick 1 ------------------\n[DEBUG] Selector             : Selector.tick()\n[DEBUG] Selector             : Selector.tick() [!RUNNING->reset current_child]\n[DEBUG] Sequence 1           : Sequence.tick()\n[DEBUG] Has apple            : HasApple.tick()\n[DEBUG] Has apple            : HasApple.stop(Status.INVALID->Status.SUCCESS)\n[DEBUG] Eat apple            : EatApple.tick()\nEating apple\n[DEBUG] Eat apple            : EatApple.stop(Status.INVALID->Status.SUCCESS)\n[DEBUG] Sequence 1           : Sequence.stop()[Status.INVALID->Status.SUCCESS]\nThe code in listing 6.1 represents the behavior tree in figure 6.1. You can run this\ncode as is or alter what the conditions return and then run the tree again. You can\nalso change the behavior tree by removing one of the sequence nodes from the root\nselector.\n Now that we have a basic understanding of behavior trees, we can move on to\nworking with agents/assistants. Before doing that, we\u2019ll look at a tool to help us work\nListing 6.1\nfirst_btree.py\nCreates a class to implement \nan action or condition\nCreates the action \nand condition nodes\nAdds the nodes to\ntheir respective\nparents\nCreates the action \nand condition nodes\nCreates the whole \nbehavior tree\nExecutes one step/tick\non the behavior tree\n",
      "content_length": 2604,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "136\nCHAPTER 6\nBuilding autonomous assistants\nwith OpenAI Assistants. This tool will help us wrap our first ABTs around OpenAI\nAssistants.\n6.2\nExploring the GPT Assistants Playground\nFor the development of this book, several GitHub projects were created to address var-\nious aspects of building agents and assistants. One such project, the GPT Assistants\nPlayground, is built using Gradio for the interface that mimics the OpenAI Assistants\nPlayground but with several extras added.\n The Playground project was developed as both a teaching and demonstration aid.\nInside the project, the Python code uses the OpenAI Assistants API to create a chat inter-\nface and an agentic system to build and power assistants. There is also a comprehensive\ncollection of actions assistants you can use, and you can easily add your own actions.\n6.2.1\nInstalling and running the Playground\nThe following listing shows installing and running the Playground project from the\nterminal. There is currently no PyPI package to install. \n# change to a working folder and create a new Python virtual environment\ngit clone \n\u27a5 https://github.com/cxbxmxcx/GPTAssistantsPlayground    \ncd GPTAssistantsPlayground     \npip install -r requirements.txt     \nYou can run the application from the terminal or using Visual Studio Code (VS\nCode), with the latter giving you more control. Before running the application, you\nneed to set your OpenAI API key through the command line or by creating an .env\nfile, as we\u2019ve done a few times already. Listing 6.3 shows an example of setting the\nenvironment variable on Linux/Mac or the Git Bash shell (Windows recommended)\nand running the application.\nexport OPENAI_API_KEY=\"your-api-key\"     \npython main.py    \nOpen your browser to the URL displayed (typically http://127.0.0.1:7860) or what\nis mentioned in the terminal. You\u2019ll see an interface similar to that shown in figure 6.3.\nListing 6.2\nInstalling the GPT Assistants Playground\nListing 6.3\nRunning the GPT Assistants Playground\nPulls the \nsource code \nfrom GitHub\nChanges directory to the \nproject source code folder\nInstalls the\nrequirements\nSets your API key as an \nenvironment variable\nRuns the app from the \nterminal or via VS Code\n",
      "content_length": 2202,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "137\n6.2\nExploring the GPT Assistants Playground\nIf you\u2019ve already defined the OpenAI Assistants, you\u2019ll see them in the Select Assistant\ndropdown.\nIf you\u2019ve never defined an assistant, you can create one and choose the various\noptions and instructions you need. If you\u2019ve visited the OpenAI Playground, you\u2019ve\nalready experienced a similar interface. \nGPT vs. an assistant\nOpenAI defines a GPT as the assistant you can run and use within the ChatGPT inter-\nface. An assistant can only be consumed through the API and requires custom code\nin most cases. When you run an assistant, you\u2019re charged according to the model\ntoken usage and any special tools, including the Code Interpreter and files, whereas\na GPT runs within ChatGPT and is covered by account costs.\nSelect an existing Assistant\nor create a new assistant.\nSelect from any\navailable models.\nSelect the Tools\nand Actions.\nThe assistant\ncan output \ufb01les\ncreated by the\nCode Interpreter.\nFigure 6.3\nThe GPT Assistants Playground interface being used to learn math\n",
      "content_length": 1021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "138\nCHAPTER 6\nBuilding autonomous assistants\nEach of these features is covered in more detail over the next few sections. We\u2019ll start\nwith a look at using and consuming actions in the next section.\n6.2.2\nUsing and building custom actions\nActions and tools are the building blocks that empower agents and assistants. Without\naccess to tools, agents are functionless chatbots. The OpenAI platform is a leader in\nestablishing many of the patterns for tools, as we saw in chapter 3.\n The Playground provides several custom actions that can be attached to assistants\nthrough the interface. In this next exercise, we\u2019ll build a simple assistant and attach a\ncouple of custom actions to see what is possible.\n Figure 6.4 shows the expanded Actions accordion, which displays many available\ncustom actions. Run the Playground from the terminal or debugger, and create a new\nassistant. Then, select the actions shown in the figure. After you\u2019re done selecting the\nactions, scroll to the bottom, and click Add Assistant to add the assistant. Assistants\nneed to be created before they can be used.\n After you create the assistant, you can ask it to list all available assistants. Listing the\nassistants also gives you the IDs required to call the assistant. You can also call other\nassistants and ask them to complete tasks in their area of specialization.\n Adding your custom actions is as simple as adding code to a file and dropping it in\nthe right folder. Open the playground/assistant_actions folder from the main\nproject folder, and you\u2019ll see several files that define the various actions. Open the\nfile_actions.py file in VS Code, as shown in listing 6.4.\n \n \n \n(continued)\nThe reason for creating a local version of the Playground was an exercise to demon-\nstrate the code structure but also provide additional features listed here:\n\uf0a1Actions (custom actions)\u2014Creating your own actions allows you to add any\nfunctionality you want to an assistant. As we\u2019ll see, the Playground makes it\nvery easy to create your own actions quickly.\n\uf0a1Code runner\u2014The API does come with a Code Interpreter, but it\u2019s relatively\nexpensive ($.03 per run), doesn\u2019t allow you to install your modules, can\u2019t run\ncode interactively, and runs slowly. The Playground will enable you to run\nPython code locally in an isolated virtual environment. While not as secure as\npushing code out to Docker images, it does execute code windowed and out\nof process better than other platforms.\n\uf0a1Transparency and logging\u2014The Playground provides for comprehensive cap-\nturing of logs and will even show how the assistant uses internal and external\ntools/actions. This can be an excellent way to see what the assistant is doing\nbehind the scenes.\n",
      "content_length": 2700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "139\n6.2\nExploring the GPT Assistants Playground\nimport os\nfrom playground.actions_manager import agent_action\nOUTPUT_FOLDER = \"assistant_outputs\"\n@agent_action    \ndef save_file(filename, content):     \n    \"\"\"\n    Save content to a file.     \n    :param filename: The name of the file including extension.\n    :param content: The content to save in the file.\n    \"\"\"\n    file_path = os.path.join(OUTPUT_FOLDER, filename)\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(content)\n    print(f\"File '{filename}' saved successfully.\")     \nListing 6.4\nplayground/assistant_actions/file_actions.py\nName your assistant\na memorable name.\nAsk to list the assistants, and you\u2019ll\nsee all the assistants you've created.\nSelect call_assistant\nand list_assistants.\nThe call_assistant action allows\nthe assistant to delegate work\nto other assistants.\nYou don\u2019t need any\nspecial instructions.\nDisable or enable the Code\nInterpreter to see the effect.\nFigure 6.4\nSelecting and using custom actions in the interface\nThis decorator \nautomatically adds the \nfunction as an action.\nGive your functions \nclear names that align \nwith the purpose.\nThe description is what \nthe assistant uses to \ndetermine the function, \nso document it well.\nGenerally returns \na message stating \nsuccess or failure\n",
      "content_length": 1304,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "140\nCHAPTER 6\nBuilding autonomous assistants\nYou can add any custom action you want by placing the file in the assistant_actions\nfolder and decorating it with the agent_action decorator. Just make sure to give the\nfunction a good name and enter quality documentation for how the function should\nbe used. When the Playground starts up, it loads all the actions in the folder that are\ndecorated correctly and have descriptions/documentation.\n It\u2019s that simple. You can add several custom actions as needed. In the next section,\nwe\u2019ll look at a special custom action that allows the assistant to run code locally.\n6.2.3\nInstalling the assistants database\nTo run several of the examples in this chapter, you\u2019ll need to install the assistants data-\nbase. Fortunately, this can be easily done through the interface and just by asking\nagents. The upcoming instructions detail the process for installing the assistants and\nare taken directly from the GPT Assistants Playground README. You can install sev-\neral of the demo assistants located in the assistants.db SQLite database:\n1\nCreate a new assistant, or use an existing assistant.\n2\nGive the assistant the create_manager_assistant action (found under the\nActions section).\n3\nAsk the assistant to create the manager assistant (i.e., \u201cplease create the man-\nager assistant\u201d), and be sure to name the assistant \u201cManager Assistant.\u201d\n4\nRefresh your browser to reload the assistants selector.\n5\nSelect the new Manager Assistant. This assistant has the instructions and actions\nthat will allow it to install assistants from the assistants.db database.\n6\nTalk to the Manager Assistant to give you a list of assistants to install, or just ask\nthe Manager Assistant to install all available assistants. \n6.2.4\nGetting an assistant to run code locally\nGetting agents and assistants to generate and run executable code has a lot of power.\nUnlike the Code Interpreter, running code locally provides numerous opportunities\nto iterate and tune quickly. We saw this earlier with AutoGen, where the agents could\nkeep running the code until it worked as expected.\n In the Playground, it\u2019s a simple matter to select the custom action run_code, as\nshown in figure 6.5. You\u2019ll also want to choose the run_shell_command action because\nit allows the assistant to pip install any required modules.\n \n \n \n \n \n \n \n \n",
      "content_length": 2337,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "141\n6.2\nExploring the GPT Assistants Playground\nYou can now ask an assistant to generate and run the code to be sure it works on your\nbehalf. Try this out by adding the custom actions and asking the assistant to generate\nand run code, as shown in figure 6.6. If the code doesn\u2019t work as expected, tell the\nassistant what problems you encountered.\n Again, the Python code running in the Playground creates a new virtual environ-\nment in a project subfolder. This system works well if you\u2019re not running any operat-\ning system\u2013level code or low-level code. If you need something more robust, a good\noption is AutoGen, which uses Docker containers to run isolated code.\n Adding actions to run code or other tasks can make assistants feel like a black box.\nFortunately, the OpenAI Assistants API allows you to consume events and see what the\nassistant is doing behind the scenes. In the next section, we\u2019ll see what this looks like.\nDo not select the\nCode Interpreter tool.\nSelect both the run_code and\nrun_shell_command custom\nactions. Running commands on\nthe shell allows an assistant to\ninstall new packages as required.\nFigure 6.5\nSelecting custom actions for the assistant to run Python code\n",
      "content_length": 1193,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "142\nCHAPTER 6\nBuilding autonomous assistants\n6.2.5\nInvestigating the assistant process through logs\nOpenAI added a feature into the Assistants API that allows you to listen to events and\nactions chained through tool/action use. This feature has been integrated into the\nPlayground, capturing action and tool use when an assistant calls another assistant. \n We can try this by asking an assistant to use a tool and then open the log. A great\nexample of how you can do this is by giving an assistant the Code Interpreter tool and\nthen asking it to plot an equation. Figure 6.7 shows an example of this exercise.\n Usually, when the Assistant Code Interpreter tool is enabled, you don\u2019t see any\ncode generation or execution. This feature allows you to see all tools and actions used\nby the assistant as they happen. Not only is it an excellent tool for diagnostics, but it\nalso provides additional insights into the functions of LLMs.\n We haven\u2019t reviewed the code to do all this because it\u2019s extensive and will likely\nundergo several changes. That being said, if you plan on working with the Assistants\nAPI, this project is a good place to start. With the Playground introduced, we can con-\ntinue our journey into ABTs in the next section.\nAny assistant can generate code. Adding\nsome helpful instructions and personality\ncan better align the output.\nThe \u201csnake\u201d game will open a new window\ndemonstrating the code is running.\nNote: While the window is open, it will\nblock the Gradio interface.\nIn this example, the assistant generated\nthe code for the game and then realized\nit need to install Pygame. After installing,\nit ran the code, as shown in the side window.\nFigure 6.6\nGetting the assistant to generate and run Python code\n",
      "content_length": 1728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "143\n6.3\nIntroducing agentic behavior trees\n6.3\nIntroducing agentic behavior trees\nAgentic behavior trees (ABTs) implement behavior trees on assistant and agent sys-\ntems. The key difference between regular behavior trees and ABTs is that they use\nprompts to direct actions and conditions. Because prompts may return a high occur-\nrence of random results, we could also name these trees stochastic behavior trees,\nwhich do exist. For simplicity, we\u2019ll differentiate behavior trees used to control agents,\nreferring to them as agentic.\n Next, we\u2019ll undertake an exercise to create an ABT. The finished tree will be writ-\nten in Python but will require the setup and configuration of various assistants. We\u2019ll\ncover how to manage assistants using the assistants themselves.\n6.3.1\nManaging assistants with assistants\nFortunately, the Playground can help us quickly manage and create the assistants.\nWe\u2019ll first install the Manager Assistant, followed by installing the predefined assis-\ntants. let\u2019s get started with installing the Manager Assistant using the following steps:\nThe Logs tab shows\nwhere \ufb01les are saved.\nCode being generated and run in the Code\nInterpreter is shown in the Logs tab.\nPlotting a graph\nis a good test\nFigure 6.7\nInternal assistant logs being captured\n",
      "content_length": 1275,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "144\nCHAPTER 6\nBuilding autonomous assistants\n1\nOpen Playground in your browser, and create a new simple assistant or use an\nexisting assistant. If you need a new assistant, create it and then select it.\n2\nWith the assistant selected, open the Actions accordion, and select the create_\nmanager_assistant action. You don\u2019t need to save; the interface will update\nthe assistant automatically.\n3\nNow, in the chat interface, prompt the assistant with the following: \u201cPlease cre-\nate the manager assistant.\u201d\n4\nAfter a few seconds, the assistant will say it\u2019s done. Refresh your browser, and\nconfirm that the Manager Assistant is now available. If, for some reason, the\nnew assistant isn\u2019t shown, try restarting the Gradio app itself.\nThe Manager Assistant is like an admin that has access to everything. When engag-\ning the Manager Assistant, be sure to be specific about your requests. With the Man-\nager Assistant active, you can now install new assistants used in the book using the\nfollowing steps:\n1\nSelect the Manager Assistant. If you\u2019ve modified the Manager Assistant, you can\ndelete it and reinstall it anytime. Although it\u2019s possible to have multiple Man-\nager Assistants, it\u2019s not recommended.\n2\nAsk the Manager Assistant what assistants can be installed by typing the follow-\ning in the chat interface:\nPlease list all the installable assistants.\n3\nIdentify which assistant you want installed when you ask the Manager Assistant\nto install it:\nPlease install the Python Coding Assistant.\nYou can manage and install any available assistants using the Playground. You can also\nask the Manager Assistant to save the definitions of all your assistants as JSON:\nPlease save all the assistants as JSON to a file called assistants.json.\nThe Manager Assistant can access all actions, which should be considered unique and\nused sparingly. When crafting assistants, it\u2019s best to keep them goal specific and limit\nthe actions to just what they need. This not only avoids giving the AI too many deci-\nsions but also avoids accidents or mistakes caused by hallucinations.\n As we go through the remaining exercises in this chapter, you\u2019ll likely need to\ninstall the required assistants. Alternatively, you can ask the Manager Assistant to\ninstall all available assistants. Either way, we look at creating an ABT with assistants in\nthe next section.\n",
      "content_length": 2340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "145\n6.3\nIntroducing agentic behavior trees\n6.3.2\nBuilding a coding challenge ABT\nCoding challenges provide a good baseline for testing and evaluating agent and assis-\ntant systems. Challenges and benchmarks can quantify how well an agent or agentic\nsystem operates. We already applied coding challenges to multi-platform agents in\nchapter 4 with AutoGen and CrewAI.\n For this coding challenge, we\u2019re going a little further and looking at Python cod-\ning challenges from the Edabit site (https://edabit.com), which range in complexity\nfrom beginner to expert. We\u2019ll stick with the expert code challenges because GPT-4o\nand other models are excellent coders. Look at the challenge in the next listing, and\nthink about how you would solve it.\nPlant the Grass by AniXDownLoe\n    You will be given a matrix representing a field g \nand two numbers x, y coordinate.\n    There are three types of possible characters in the matrix:\n        x representing a rock.\n        o representing a dirt space.\n        + representing a grassed space.\n    You have to simulate grass growing from the position (x, y). \n    Grass can grow in all four directions (up, left, right, down). \n    Grass can only grow on dirt spaces and can't go past rocks.\n    Return the simulated matrix.\n    Examples\n    simulate_grass([\n    \"xxxxxxx\",\n    \"xooooox\",\n    \"xxxxoox\"\n    \"xoooxxx\"\n    \"xxxxxxx\"\n    ], 1, 1) \u2192 [\n    \"xxxxxxx\",\n    \"x+++++x\",\n    \"xxxx++x\"\n    \"xoooxxx\"\n    \"xxxxxxx\"\n    ]\n    Notes\n    There will always be rocks on the perimeter\nYou can use any challenge or coding exercise you want, but here are a few things to\nconsider:\nListing 6.5\nEdabit challenge: Plant the Grass\n",
      "content_length": 1661,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "146\nCHAPTER 6\nBuilding autonomous assistants\n\uf0a1The challenge should be testable with quantifiable assertions (pass/fail).\n\uf0a1Avoid opening windows when asking for a game, building a website, or using\nanother interface. At some point, testing full interfaces will be possible, but for\nnow, it\u2019s just text output.\n\uf0a1Avoid long-running challenges, at least initially. Start by keeping the challenges\nconcise and short lived. \nAlong with any challenge, you\u2019ll also want a set of tests or assertions to confirm the\nsolution works. On Edabit, a challenge typically provides a comprehensive set of tests.\nThe following listing shows the additional tests provided with the challenge.\nTest.assert_equals(simulate_grass(\n[\"xxxxxxx\",\"xooooox\",\"xxxxoox\",\"xoooxxx\",\"xxxxxxx\"],\n 1, 1), \n[\"xxxxxxx\",\"x+++++x\",\"xxxx++x\",\"xoooxxx\",\"xxxxxxx\"])\n    Test.assert_equals(simulate_grass(\n[\"xxxxxxx\",\"xoxooox\",\"xxoooox\",\"xooxxxx\",\n\"xoxooox\",\"xoxooox\",\"xxxxxxx\"],\n 2, 3), [\"xxxxxxx\",\"xox+++x\",\"xx++++x\",\"x++xxxx\",\n\"x+xooox\",\"x+xooox\",\"xxxxxxx\"])\n    Test.assert_equals(simulate_grass(\n[\"xxxxxx\",\"xoxoox\",\"xxooox\",\"xoooox\",\"xoooox\",\"xxxxxx\"], \n1, 1), \n[\"xxxxxx\",\"x+xoox\",\"xxooox\",\"xoooox\",\"xoooox\",\"xxxxxx\"])\n    Test.assert_equals(simulate_grass(\n[\"xxxxx\",\"xooox\",\"xooox\",\"xooox\",\"xxxxx\"], \n1, 1),\n[\"xxxxx\",\"x+++x\",\"x+++x\",\"x+++x\",\"xxxxx\"])\n    Test.assert_equals(simulate_grass(\n[\"xxxxxx\",\"xxxxox\",\"xxooox\",\"xoooxx\",\"xooxxx\",\n\"xooxxx\",\"xxooox\",\"xxxoxx\",\"xxxxxx\"], \n4, 1),\n[\"xxxxxx\",\"xxxx+x\",\"xx+++x\",\"x+++xx\",\"x++xxx\",\n\"x++xxx\",\"xx+++x\",\"xxx+xx\",\"xxxxxx\"])\n    Test.assert_equals(simulate_grass(\n[\"xxxxxxxxxxx\", \"xoxooooooox\", \"xoxoxxxxxox\", \n\"xoxoxoooxox\", \"xoxoxoxoxox\", \"xoxoxoxoxox\", \n\"xoxoxxxoxox\", \"xoxoooooxox\", \"xoxxxxxxxox\", \n\"xooooooooox\", \"xxxxxxxxxxx\"], 1, 1), \n[\"xxxxxxxxxxx\", \"x+x+++++++x\", \"x+x+xxxxx+x\", \n\"x+x+x+++x+x\", \"x+x+x+x+x+x\", \"x+x+x+x+x+x\", \n\"x+x+xxx+x+x\", \"x+x+++++x+x\", \"x+xxxxxxx+x\", \n\"x+++++++++x\", \"xxxxxxxxxxx\"])   \nThe tests will be run as part of a two-step verification to confirm that the solution\nworks. We\u2019ll also use the tests and challenges as written, which will further test the AI. \n Figure 6.8 shows the makeup of a straightforward behavior tree that will be used to\nsolve various programming challenges. You\u2019ll notice that this ABT uses a different\nListing 6.6\nPlant the Grass tests\n",
      "content_length": 2299,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "147\n6.3\nIntroducing agentic behavior trees\nassistant for the actions and conditions. For the first step, the Python coding assistant\n(called the Hacker) generates a solution that is then reviewed by the coding challenge\nJudge (called the Judge), which produces a refined solution that is verified by a differ-\nent Python coding assistant (called the Verifier).\nFigure 6.8 also shows how each agent converses on which thread. Assistants use mes-\nsage threads, similar to a Slack or Discord channel, where all assistants conversing on\na thread will see all messages. For this ABT, we keep one main conversation thread for\nthe Hacker and Judge to share messages, while the Verifier works on a separate mes-\nsage thread. Keeping the Verifier on its own thread isolates it from the noise of the\nsolution-solving efforts.\n Now, building the ABT in code is a matter of combining the py_trees package and\nthe Playground API functions. Listing 6.7 shows an excerpt of code that creates each\nof the action/condition nodes with the assistants and gives them the instructions.\n \n \n \n \n\u2192\nVerify solution\nHacking solution\nThe root node is a sequence.\nJudge solution\nThe initial solution will\nbe generated by the\nPython oding ssistant,\nc\na\nwhich will save output to\nsolution.py.\nsolution.py\nThe solution will be judged\nby the oding hallenge\nc\nc\nJudge. It will load the\nsolution.py, judge it, and\noutput a \ufb01le called\njudged_solution.py.\njudged_solution.py\nThe last step uses a Python\nc\na\noding ssistant and veri\ufb01es\nthat the solution is correct\nby looking at the\njudged_solution.py \ufb01le.\nMain conversation\nthread\nNew conversation\nthread\nAssistants use threads to\ncapture conversations.\nFigure 6.8\nThe ABT for the coding challenge\n",
      "content_length": 1712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "148\nCHAPTER 6\nBuilding autonomous assistants\nroot = py_trees.composites.Sequence(\"RootSequence\", memory=True)\nthread = api.create_thread()    \nchallenge = textwrap.dedent(\"\"\"\n     \n\"\"\")\njudge_test_cases = textwrap.dedent(\"\"\"\n    \n\"\"\")\nhacker = create_assistant_action_on_thread(   \n    thread=thread,     \n    action_name=\"Hacker\",\n    assistant_name=\"Python Coding Assistant\",\n    assistant_instructions=textwrap.dedent(f\"\"\"\n    Challenge goal: \n    {challenge}     \n    Solve the challenge and output the \nfinal solution to a file called solution.py        \n    \"\"\"),\n)\nroot.add_child(hacker)\njudge = create_assistant_action_on_thread(    \n    thread=thread,     \n    action_name=\"Judge solution\",\n    assistant_name=\"Coding Challenge Judge\",\n    assistant_instructions=textwrap.dedent(\n        f\"\"\"\n    Challenge goal: \n    {challenge}     \n    Load the solution from the file solution.py.\n    Then confirm is a solution to the challenge \nand test it with the following test cases:\n    {judge_test_cases}     \n    Run the code for the solution and confirm it passes all the test cases.\n    If the solution passes all tests save the solution to a file called \njudged_solution.py\n    \"\"\",\n    ),\n)\nroot.add_child(judge)\n# verifier operates on a different thread, essentially in closed room\nverifier = create_assistant_condition(    \n    condition_name=\"Verify solution\",\n    assistant_name=\"Python Coding Assistant\",\n    assistant_instructions=textwrap.dedent(\n        f\"\"\"\n    Challenge goal: \n    {challenge}     \n    Load the file called judged_solution.py and \nListing 6.7\nagentic_btree_coding_challenge.py\nCreates a message thread that will \nbe shared by the Hacker and Judge\nThe challenge as shown \nin the example listing 6.5\nThe tests as shown in \nthe example listing 6.6\nCreates a message thread that will \nbe shared by the Hacker and Judge\nThe challenge as shown \nin the example listing 6.5\nCreates a message thread that will \nbe shared by the Hacker and Judge\nThe challenge as shown \nin the example listing 6.5\nThe tests as shown in \nthe example listing 6.6\nCall creates a \nnew message \nthread\nThe challenge as shown \nin the example listing 6.5\n",
      "content_length": 2156,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "149\n6.3\nIntroducing agentic behavior trees\nverify that the solution is correct by running the code and confirm it passes \nall the test cases:\n    {judge_test_cases}     \n    If the solution is correct, return only the single word SUCCESS, \notherwise \nreturn the single word FAILURE.\n    \"\"\",\n    ),\n)\nroot.add_child(verifier)\ntree = py_trees.trees.BehaviourTree(root)\nwhile True:\n    tree.tick()\n    time.sleep(20)     \n    if root.status == py_trees.common.Status.SUCCESS:   \n        break\n### Required assistants \u2013 \n### Python Coding Assistant and Coding Challenge Judge \n### install these assistants through the Playground\nRun the ABT by loading the file in VS Code or using the command line. Follow the out-\nput in the terminal, and watch how the assistants work through each step in the tree.\n If the solution fails to be verified at the condition node, the process will continue\nper the tree. Even with this simple solution, you could quickly create numerous varia-\ntions. You could extend the tree with more nodes/steps and subtrees. Perhaps you\nwant a team of Hackers to break down and analyze the challenge, for example. \n This example\u2019s work is done mainly with the Playground code, using the helper\nfunctions create_assistant_condition and create_assistant_action_on_thread.\nThis code uses a couple of classes to integrate the py_trees behavior tree code and\nthe OpenAI Assistants code wrapped in the Playground. Review the code within the\nproject if you want to understand the lower-level details.\n6.3.3\nConversational AI systems vs. other methods\nWe already looked at conversational multi-agent systems in chapter 4 when we looked\nat AutoGen. The ABT can work using a combination of conversations (over threads)\nand other methods, such as file sharing. Having your assistants/agents pass files around\nhelps reduce the number of noisy and repetitive thoughts/conversations. In contrast,\nconversational systems benefit from potential emergent behaviors. So, using both can\nhelp evolve better control and solutions.\n The simple solution in listing 6.7 could be extended to handle more real-world\ncoding challenges and perhaps even to work as a coding ABT. In the next section, we\nbuild a different ABT to handle a different problem.\nThe tests as shown in\nthe example listing 6.6\nThe sleep time can be \nadjusted up or down as \nneeded and can be used \nto throttle the messages \nsent to an LLM.\nThe process will \ncontinue until \nthe verification \nsucceeds.\n",
      "content_length": 2464,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "150\nCHAPTER 6\nBuilding autonomous assistants\n6.3.4\nPosting YouTube videos to X\nIn this section\u2019s exercise, we look at an ABT that can do the following:\n1\nSearch for videos on YouTube for a given topic and return the latest videos.\n2\nDownload the transcripts for all the videos your search provided.\n3\nSummarize the transcripts.\n4\nReview the summarized transcripts and select a video to write an X (formerly\nTwitter) post about. \n5\nWrite an exciting and engaging post about the video, ensuring it\u2019s less than 280\ncharacters.\n6\nReview the post and then post it on X.\nFigure 6.9 shows the ABT assembled with each of the different assistants. In this exer-\ncise, we use a sequence node for the root, and each assistant performs a different\naction. Also, to keep things simple, each assistant interaction will always occur in a\nnew thread. This isolates each assistant\u2019s interaction into a concise conversation that\u2019s\neasier to debug if something goes wrong.\nNew thread\n\u2192\nSearch YouTube\nWrite post\nThis assistant searches\nYouTube for videos,\ndownloads and\nsummarizes the\ntranscripts, and saves\nto a \ufb01le.\nyoutube\ntranscripts.txt\nThe assistant loads the\ntranscripts, selects a\nrelevant video, writes a\npost of less than 280\ncharacters, and then\noutputs a \ufb01le.\nyoutube\ntwitter\npost.txt\nThe assistant loads the\npost, reviews it and posts\nit to Twitter (X).\nNew thread\nNew thread\nAssistants always use\na new thread.\nPost to X\nThe root node is a sequence.\nFigure 6.9\nThe YouTube social media ABT\n",
      "content_length": 1485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "151\n6.3\nIntroducing agentic behavior trees\n6.3.5\nRequired X setup\nIf you plan to run the code in this exercise, you must add your X credentials to the\n.env file. The .env.default file shows an example of how the credentials need to be,\nas shown in listing 6.8. You don\u2019t have to enter your credentials. This means the last\nstep, posting, will fail, but you can still look at the file (youtube_twitter_post.txt)\nto see what was generated.\nX_EMAIL = \"twitter email here\"\nX_USERNAME = \"twitter username here\"\nX_PASSWORD = \"twitter password here\"\nListing 6.9 shows just the code for creating the assistant actions. This ABT uses three\ndifferent assistants, each with its own task instructions. Note that each assistant has a\nunique set of instructions defining its role. You can review the instructions for each\nassistant by using the Playground.\nroot = py_trees.composites.Sequence(\"RootSequence\", memory=True)\nsearch_term = \"GPT Agents\"\nsearch_youtube_action = create_assistant_action(\n    action_name=f\"Search YouTube({search_term})\",\n    assistant_name=\"YouTube Researcher v2\",\n    assistant_instructions=f\"\"\"\n    Search Term: {search_term}\n    Use the query \"{search_term}\" to search for videos on YouTube.\n    then for each video download the transcript and summarize it \nfor relevance to {search_term}\n    be sure to include a link to each of the videos,\n    and then save all summarizations to a file called youtube_transcripts.txt\n    If you encounter any errors, please return just the word FAILURE.\n    \"\"\",\n)\nroot.add_child(search_youtube_action)\nwrite_post_action = create_assistant_action(\n    action_name=\"Write Post\",\n    assistant_name=\"Twitter Post Writer\",\n    assistant_instructions=\"\"\"\n    Load the file called youtube_transcripts.txt,\nListing 6.8\nConfiguring credentials\nYouTube search and spam\nIf you plan to run this exercise for real and let it post to your X account, be aware that\nYouTube has a bit of a spam problem. The assistants have been configured to try to\navoid video spam, but some of it may get through. Building a working ABT that can\nwade through videos while avoiding spam has some suitable applications.\nListing 6.9\nagentic_btree_video_poster_v1.py\n",
      "content_length": 2186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "152\nCHAPTER 6\nBuilding autonomous assistants\n    analyze the contents for references to search term at the top and \nthen select\n    the most exciting and relevant video related to: \n    educational, entertaining, or informative, to post on Twitter.\n    Then write a Twitter post that is relevant to the video,\n    and include a link to the video, along\n    with exciting highlights or mentions, \n    and save it to a file called youtube_twitter_post.txt.\n    If you encounter any errors, please return just the word FAILURE.\n    \"\"\",\n)\nroot.add_child(write_post_action)\npost_action = create_assistant_action(\n    action_name=\"Post\",\n    assistant_name=\"Social Media Assistant\",\n    assistant_instructions=\"\"\"\n    Load the file called youtube_twitter_post.txt and post the content \nto Twitter.\n    If the content is empty please do not post anything.\n    If you encounter any errors, please return just the word FAILURE.\n    \"\"\",\n)\nroot.add_child(post_action)\n### Required assistants \u2013 YouTube Researcher v2, Twitter Post Writer, \nand Social Media Assistant \u2013 install these assistants through the Playground\nRun the code as you normally would, and after a few minutes, a new post will appear\nin the assistants_output folder. Figure 6.10 shows an example of a post generated\nusing this ABT. Running this ABT to generate more than a few posts a day could, and\nlikely will, get your X account blocked. If you\u2019ve configured X credentials, you\u2019ll see\nthe post appear on your feed.\nThis ABT is shown for demonstration purposes and isn\u2019t for production or long-term\nuse. The primary features of this demonstration are to show search and loading data,\nsummarization and filtering, then generating new content, and finally highlighting\nmultiple custom actions and integrations with APIs.\nFigure 6.10\nA sample X \npost from the ABT\n",
      "content_length": 1820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "153\n6.4\nBuilding conversational autonomous multi-agents\n6.4\nBuilding conversational autonomous multi-agents\nThe conversational aspect of multi-agent systems can drive mechanisms such as feed-\nback, reasoning, and emergent behaviors. Driving agents with ABTs that silo assis-\ntants/agents can be effective for controlling structured processes, as we saw in the\nYouTube posting example. However, we also don\u2019t want to miss out on the benefits of\nconversation across agents/assistants.\n Fortunately, the Playground provides methods to silo or join assistants to conversation\nthreads. Figure 6.11 shows how assistants can be siloed or mixed in various combinations\nto threads. Combining silos with conversation provides the best of both patterns.\nWe\u2019ll examine a simple but practical exercise to demonstrate the effectiveness of the\nconversational pattern. For the next exercise, we\u2019ll employ two assistants in an ABT\nthat converse over the same thread. The next listing shows the tree\u2019s construction in\ncode with the respective assistants.\nroot = py_trees.composites.Sequence(\"RootSequence\", memory=True)\nbug_file = \"\"\"\n# code not shown\n\"\"\"\nListing 6.10\nagentic_conversation_btree.py\nSiloed assistants\nalways use a new\nthread and are the\nonly consumers.\nThread xya\nThread yyc\nThread zza\nAgent/Assistant\nSilos\nSearch\nThread xyb\nSearch\nThread xyc\nSearch\nAgent/Assistant\nConversational\nSearch\nPlan\nActivity\nReview\nConversational assistants\nshare a thread for all\nconversations.\nAgent/Assistant\nConversational + Silo\nThread xyc\nVerify\nThe combination of siloed\nand conversational can be\ncombined for an unbiased\nreview.\nSearch\nPlan\nActivity\nTransfer\nFigure 6.11\nThe various layouts of siloed and conversational assistants\n",
      "content_length": 1715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "154\nCHAPTER 6\nBuilding autonomous assistants\nthread = api.create_thread()    \ndebug_code = create_assistant_action_on_thread(    \n    thread=thread,\n    action_name=\"Debug code\",\n    assistant_name=\"Python Debugger\",\n    assistant_instructions=textwrap.dedent(f\"\"\"    \n    Here is the code with bugs in it:\n    {bug_file}\n    Run the code to identify the bugs and fix them. \n    Be sure to test the code to ensure it runs without errors or throws \nany exceptions.\n    \"\"\"),\n)\nroot.add_child(debug_code)\nverify = create_assistant_condition_on_thread(    \n    thread=thread,\n    condition_name=\"Verify\",\n    assistant_name=\"Python Coding Assistant\",\n    assistant_instructions=textwrap.dedent(\n        \"\"\"\n    Verify the solution fixes the bug and there are no more issues.\n    Verify that no exceptions are thrown when the code is run.\n    Reply with SUCCESS if the solution is correct, otherwise return FAILURE.\n    If you are happy with the solution, save the code to a file called \nfixed_bug.py.\n    \"\"\",\n    ),\n)\nroot.add_child(verify)\ntree = py_trees.trees.BehaviourTree(root)\nwhile True:\n    tree.tick()    \n    if root.status == py_trees.common.Status.SUCCESS:\n        break   \n    time.sleep(20)\nThree nodes comprise the tree: the root sequence, the debug code action, and the\nverify fix condition. Because the tree\u2019s root is a sequence, the two assistants will con-\ntinue to work one after another until they both return with success. Both assistants\nconverse on the same thread and yet are controlled in a manner that provides con-\nstant feedback.\n Run the exercise by loading the file in VS Code, or execute it directly from the\ncommand line. The example code has a few minor bugs and problems that the assis-\ntants will work through to fix. After the ABT completes running successfully, you can\nopen the assistants_output/fixed_bug.py file and verify the results are all good. \n We\u2019ve now seen a couple of ABTs in action and understand the nuances of using\nsilos or conversations. The following section will teach you some techniques for build-\ning your own ABTs.\nCreates a message thread for the \nassistants to share and converse over\nCreates the debug \ncode action with a \nspecial assistant\nCreates the verification \ncondition to test if the \ncode is fixed or not\nThe tree will continue \nto run until the root \nsequence completes \nwith success.\n",
      "content_length": 2358,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "155\n6.5\nBuilding ABTs with back chaining\n6.5\nBuilding ABTs with back chaining\nBack chaining is a method derived from logic and reasoning used to help build behav-\nior trees by working backward from the goal. This section will use the back chaining\nprocess to construct an ABT that works to achieve the goal. The following list provides\na description of the process in more detail:\n1\nIdentify goal behavior. Start with the behavior you want the agent to perform.\n2\nDetermine the required actions. Identify the actions that lead to the goal behavior.\n3\nIdentify the conditions. Determine the conditions that must be met for each action\nto succeed.\n4\nDetermine the mode of communication. Determine how the assistants will pass on\ninformation. Will the assistants be siloed or converse over threads, or is a combi-\nnation of patterns better?\n5\nConstruct the tree. Start by building the behavior tree from the goal behavior, add-\ning nodes for actions and conditions recursively until all necessary conditions\nare linked to known states or facts.\nBehavior trees typically use a pattern called the blackboard to communicate across\nnodes. Blackboards, like those in py_trees, use a key/value store to save information\nand make it accessible across nodes. It also provides for several controls, such as limit-\ning access to specific nodes. \n We deferred to using files for communication because of their simplicity and\ntransparency. At some point, agentic systems are expected to consume much more\ninformation and in different formats than those designed for blackboards. Black-\nboards must either become more sophisticated or be integrated with file storage\nsolutions.\n Let\u2019s build an ABT using back chaining. We could tackle a variety of goals, but one\ninteresting and perhaps meta goal is to build an ABT that helps build assistants. So\nlet\u2019s first present our goal as a statement \u201cCreate an assistant that can help me do\n{task}\u201d:\n\uf0a1Required actions: (working backwards)\n\u2013 Create an assistant.\n\u2013 Verify the assistant.\n\u2013 Test the assistant.\n\u2013 Name the assistant.\n\u2013 Give the assistant the relevant instructions.\n\uf0a1Identified condition: \n\u2013 Verify the assistant.\n\uf0a1Determine communication patterns: To keep things interesting, we\u2019ll run all assis-\ntants on the same message thread.\n",
      "content_length": 2270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "156\nCHAPTER 6\nBuilding autonomous assistants\n\uf0a1Construct the tree: To construct the tree, let\u2019s first reverse the order of actions and\nmark each of the element\u2019s actions and conditions accordingly:\n\u2013 (action) Give the assistant relevant instructions to help a user with a given task.\n\u2013 (action) Name the assistant.\n\u2013 (action) Test the assistant.\n\u2013 (condition) Verify the assistant.\n\u2013 (action) Create the assistant.\nOf course, the simple solution to building the tree now is to ask ChatGPT or an oth-\nerwise capable model. The result of asking ChatGPT to make the tree is shown in\nthe next listing. You could also work the tree out independently and perhaps intro-\nduce other elements.\nRoot\n\u2502\n\u251c\u2500\u2500 Sequence\n\u2502    \u251c\u2500\u2500 Action: Give the assistant relevant instructions to help a user \nwith a given task\n\u2502    \u251c\u2500\u2500 Action: Name the assistant\n\u2502    \u251c\u2500\u2500 Action: Test the assistant\n\u2502    \u251c\u2500\u2500 Condition: Verify the assistant\n\u2502    \u2514\u2500\u2500 Action: Create the assistant\nFrom this point, we can start building the tree by iterating over each action and condi-\ntion node and determining what instructions the assistant needs. This can also\ninclude any tools and custom actions, including ones you may need to develop. On\nyour first pass, keep the instructions generic. Ideally, we want to create as few assistants\nas necessary.\n After determining the assistant, tools, and actions for each assistant and for which\ntask, you can try to generalize things further. Think about where it may be possible to\ncombine actions and reduce the number of assistants. It\u2019s better to start evaluating\nwith insufficient assistants than with too many. However, be sure to maintain the\nproper divisions of work as tasks: for example, testing and verification are best done\nwith different assistants.\n6.6\nExercises\nComplete the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Creating a Travel Planner ABT\nObjective\u2014Build an agentic behavior tree (ABT) to plan a travel itinerary using\nassistants.\nTasks:\n\u2013 Set up the GPT Assistants Playground on your local machine.\nListing 6.11\nABT for building an assistant\n",
      "content_length": 2094,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "157\n6.6\nExercises\n\u2013 Create an ABT to plan a travel itinerary. The tree should have the following\nstructure:\n\u2013 Action: Use the Travel assistant to gather information about potential des-\ntinations.\n\u2013 Action: Use the Itinerary Planner to create a day-by-day travel plan.\n\u2013 Condition: Verify the completeness and feasibility of the itinerary using\nanother Travel Assistant.\n\u2013 Implement and run the ABT to create a complete travel itinerary.\n\uf0a1Exercise 2\u2014Building an ABT for Customer Support Automation\nObjective\u2014Create an ABT that automates customer support responses using\nassistants.\nTasks:\n\u2013 Set up the GPT Assistants Playground on your local machine.\n\u2013 Create an ABT with the following structure:\n\u2013 Action: Use the Customer Query Analyzer assistant to categorize customer\nqueries.\n\u2013 Action: Use the Response Generator assistant to draft responses based on\nthe query categories.\n\u2013 Action: Use the Customer Support assistant to send the responses to\ncustomers.\n\u2013 Implement and run the ABT to automate the process of analyzing and respond-\ning to customer queries.\n\uf0a1Exercise 3\u2014Managing Inventory with an ABT\nObjective\u2014Learn how to create and manage inventory levels using an ABT.\nTasks:\n\u2013 Set up the GPT Assistants Playground on your local machine.\n\u2013 Create an ABT that manages inventory for a retail business:\n\u2013 Action: Use the Inventory Checker assistant to review current stock levels.\n\u2013 Action: Use the Order assistant to place orders for low-stock items.\n\u2013 Condition: Verify that orders have been placed correctly and update inven-\ntory records.\n\u2013 Implement and run the ABT to manage inventory dynamically.\n\uf0a1Exercise 4\u2014Creating a Personal Fitness Trainer ABT\nObjective\u2014Create an ABT that provides personalized fitness training plans using\nassistants.\nTasks:\n\u2013 Set up the GPT Assistants Playground on your local machine.\n\u2013 Create an ABT to develop a personalized fitness plan:\n\u2013 Action: Use the Fitness Assessment assistant to evaluate the user\u2019s current\nfitness level.\n",
      "content_length": 1971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "158\nCHAPTER 6\nBuilding autonomous assistants\n\u2013 Action: Use the Training Plan Generator to create a custom fitness plan\nbased on the assessment.\n\u2013 Condition: Verify the plan\u2019s suitability and safety using another Fitness\nassistant.\n\u2013 Implement and run the ABT to generate and validate a personalized fitness\ntraining plan.\n\uf0a1Exercise 5\u2014Using Back Chaining to Build a Financial Advisor ABT\nObjective\u2014Apply back chaining to construct an ABT that provides financial\nadvice and investment strategies.\nTasks:\n\u2013 Set up the GPT Assistants Playground on your local machine.\n\u2013 Define the following goal: \u201cCreate an assistant that can provide financial\nadvice and investment strategies.\u201d\n\u2013 Using back chaining, determine the actions and conditions needed to\nachieve this goal.\n\u2013 Implement and run the ABT to generate a comprehensive financial advi-\nsory service by back chaining the construction of the base actions and con-\nditions for the tree.\nSummary\n\uf0a1Behavior trees are a robust and scalable AI control pattern, first introduced in\nrobotics by Rodney A. Brooks. They are widely used in gaming and robotics for\ntheir modularity and reusability.\n\uf0a1The primary nodes in behavior trees are the selector, sequence, condition,\naction, decorator, and parallel nodes. Selectors are like \u201cor\u201d blocks: sequence exe-\ncutes nodes in sequence, condition tests the state, action does the work, decora-\ntor is a wrapper, and parallel nodes allow for dual execution.\n\uf0a1Understanding the execution flow of behavior trees can be critical to designing,\nbuilding, and operating them to provide control for making clear decision-\nmaking paths.\n\uf0a1The advantages of behavior trees include modularity, scalability, flexibility, debug-\nging ease, and decoupling of decision logic, making behavior trees suitable for\ncomplex AI systems.\n\uf0a1Setting up and running a simple behavior tree in Python requires correctly\nnaming and documenting custom nodes.\n\uf0a1The GPT Assistants Playground project is a Gradio-based interface that mimics\nthe OpenAI Assistants Playground with additional features for teaching and\ndemonstrating ABTs.\n\uf0a1The GPT Assistants Playground allows for creating and managing custom actions,\nwhich is essential for building versatile assistants.\n",
      "content_length": 2222,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "159\nSummary\n\uf0a1ABTs control agents and assistants by using prompts to direct actions and condi-\ntions for assistants. ABTs use the power of LLMs to create dynamic and autono-\nmous systems.\n\uf0a1Back chaining is a method for constructing behavior trees by working backward\nfrom the goal behavior. This process involves identifying required actions, condi-\ntions, and communication patterns, and then constructing the tree step by step.\n\uf0a1Agentic systems benefit from siloed and conversation patterns for communicat-\ning between entities. ABTs can benefit from combining siloed and conversa-\ntional assistants to use structured processes and emergent behaviors.\n",
      "content_length": 653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "160\nAssembling and\nusing an agent platform\nAfter we explored some basic concepts about agents and looked at using actions\nwith tools to build prompts and personas using frameworks such as the Semantic\nKernel (SK), we took the first steps toward building a foundation for this book.\nThat foundation is called Nexus, an agent platform designed to be simple to learn,\neasy to explore, and powerful enough to build your agent systems. \nThis chapter covers\n\uf0a1Nexus chat and dashboard interface for \nAI agents\n\uf0a1Streamlit framework for building intelligent \ndashboards, prototypes, and AI chat apps\n\uf0a1Developing, testing, and engaging agent profiles \nand personas in Nexus\n\uf0a1Developing the base Nexus agent\n\uf0a1Developing, testing, and engaging agent actions \nand tools alone or within Nexus\n",
      "content_length": 779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "161\n7.1\nIntroducing Nexus, not just another agent platform\n7.1\nIntroducing Nexus, not just another agent platform\nThere are more than 100 AI platforms and toolkits for consuming and developing\nlarge language model (LLM) applications, ranging from toolkits such as SK or Lang-\nChain to complete platforms such as AutoGen and CrewAI. This makes it difficult to\ndecide which platform is well suited to building your own AI agents.\n Nexus is an open source platform developed with this book to teach the core con-\ncepts of building full-featured AI agents. In this chapter, we\u2019ll examine how Nexus is\nbuilt and introduce two primary agent components: profiles/personas and actions/tools.\n Figure 7.1 shows the primary interface to Nexus, a Streamlit chat application that\nallows you to choose and explore various agentic features. The interface is similar to\nChatGPT, Gemini, and other commercial LLM applications.\nIn addition to the standard features of an LLM chat application, Nexus allows the user\nto configure an agent to use a specific API/model, the persona, and possible actions.\nIn the remainder of the book, the available agent options will include the following:\n\uf0a1Personas/profiles\u2014The primary persona and profile the agent will use. A persona\nis the personality and primary motivator, and an agent engages the persona to\nCreate a new\nchat thread.\nShows the full conversation\nhistory for the current chat thread\nThe agent AI you\u2019re using:\nOpenAI, Azure OpenAI, Gemini,\nor Claude. Currently, OpenAI is\nused.\nThe agent pro\ufb01le/persona\nto use in the next request\nThe tools/actions the agent\ncan use, which supports both\nnative (code) and semantic\n(prompt) functions\nPrevious\nchat threads\nFigure 7.1\nThe Nexus interface and features\n",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "162\nCHAPTER 7\nAssembling and using an agent platform\nanswer requests. We\u2019ll look in this chapter at how personas/profiles can be\ndeveloped and consumed.\n\uf0a1Actions/tools\u2014Represents the actions an agent can take using tools, whether\nthey\u2019re semantic/prompt or native/code functions. In this chapter, we\u2019ll look at\nhow to build both semantic and native functions within Nexus.\n\uf0a1Knowledge/memory\u2014Represents additional information an agent may have access\nto. At the same time, agent memory can represent various aspects, from short-\nterm to semantic memory.\n\uf0a1Planning/feedback\u2014Represents how the agent plans and receives feedback on\nthe plans or the execution of plans. Nexus will allow the user to select options\nfor the type of planning and feedback an agent uses.\nAs we progress through this book, Nexus will be added to support new agent features.\nHowever, simultaneously, the intent will be to keep things relatively simple to teach\nmany of these essential core concepts. In the next section, we\u2019ll look at how to quickly\nuse Nexus before going under the hood to explore features in detail.\n7.1.1\nRunning Nexus\nNexus is primarily intended to be a teaching platform for all levels of developers. As\nsuch, it will support various deployment and usage options. In the next exercise, we\u2019ll\nintroduce how to get up and running with Nexus quickly.\n Open a terminal to a new Python virtual environment (version 3.10). If you need\nassistance creating one, refer to appendix B. Then, execute the commands shown in\nlisting 7.1 within this new environment. You can either set the environment variable at\nthe command line or create a new .env file and add the setting.\npip install git+https://github.com/cxbxmxcx/Nexus.git    \n#set your OpenAI API Key\nexport OPENAI_API_KEY=\u201d< your API key>\u201d         \nor\n$env: OPENAI_API_KEY = =\u201d< your API key>\u201d       \nor\necho 'OPENAI_API_KEY=\"<your API key>\"' > .env   \nnexus run     \nAfter entering the last command, a website will launch with a login page, as shown in\nfigure 7.2. Go ahead and create a new user. A future version of Nexus will allow multi-\nple users to engage in chat threads.\n After you log in, you\u2019ll see a page like figure 7.1. Create a new chat and start con-\nversing with an agent. If you encounter a problem, be sure you have the API key set\nListing 7.1\nTerminal command line\nInstalls the package \ndirectly from the \nrepository and \nbranch; be sure to \ninclude the branch.\nCreates the key as an \nenvironment variable or \ncreates a new .env file \nwith the setting\nRuns the application\n",
      "content_length": 2532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "163\n7.1\nIntroducing Nexus, not just another agent platform\nproperly. As explained in the next section, you can run Nexus using this method or\nfrom a development workflow.\n7.1.2\nDeveloping Nexus\nWhile working through the exercises of this book, you\u2019ll want to set up Nexus in devel-\nopment mode. That means downloading the repository directly from GitHub and\nworking with the code.\n Open a new terminal, and set your working directory to the chapter_7 source\ncode folder. Then, set up a new Python virtual environment (version 3.10) and enter\nthe commands shown in listing 7.2. Again, refer to appendix B if you need assistance\nwith any previous setup.\ngit clone https://github.com/cxbxmxcx/Nexus.git     \npip install -e Nexus    \n#set your OpenAI API Key (.env file is recommended)\nexport OPENAI_API_KEY=\u201d< your API key>\u201d  #bash           \nor\n$env: OPENAI_API_KEY = =\u201d< your API key>\u201d  #powershell   \nor\necho 'OPENAI_API_KEY=\"<your API key>\"' > .env            \nnexus run     \nListing 7.2\nInstalling Nexus for development\nSelect Create\nNew User to start.\nUsername is used to\ntrack conversation\nhistory in the threads.\nFigure 7.2\nLogging in or creating a new Nexus user\nDownloads and installs \nthe specific branch \nfrom the repository\nInstalls the downloaded repository \nas an editable package\nSets your OpenAI key as \nan environment variable \nor adds it to an .env file\nStarts the application\n",
      "content_length": 1393,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "164\nCHAPTER 7\nAssembling and using an agent platform\nFigure 7.3 shows the Login or Create New User screen. Create a new user, and the\napplication will log you in. This application uses cookies to remember the user, so you\nwon\u2019t have to log in the next time you start the application. If you have cookies dis-\nabled on your browser, you\u2019ll need to log in every time.\nGo to the Nexus repository folder and look around. Figure 7.4 shows an architecture\ndiagram of the application\u2019s main elements. At the top, the interface developed with\nStreamlit connects the rest of the system through the chat system. The chat system\nmanages the database, agent manager, action manager, and profile managers.\n This agent platform is written entirely in Python, and the web interface uses Stream-\nlit. In the next section, we look at how to build an OpenAI LLM chat application.\n \nThe browser points to\nlocalhost:8501, which is the\ndefault for Streamlit apps.\nStreamlit apps can be\ndeployed to the cloud\nusing this option.\nFill in the username, pick an\navatar, and set a password or\nchoose a browser-generated one.\nFigure 7.3\nThe Login or Create New User page\n",
      "content_length": 1143,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "165\n7.2\nIntroducing Streamlit for chat application development\n7.2\nIntroducing Streamlit for chat application \ndevelopment\nStreamlit is a quick and powerful web interface prototyping tool designed to be used\nfor building machine learning dashboards and concepts. It allows applications to be\nwritten completely in Python and produces a modern React-powered web interface.\nYou can even deploy the completed application quickly to the cloud or as a stand-\nalone application.\n7.2.1\nBuilding a Streamlit chat application\nBegin by opening Visual Studio Code (VS Code) to the chapter_07 source folder. If\nyou\u2019ve completed the previous exercise, you should already be ready. As always, if you\nneed assistance setting up your environment and tools, refer to appendix B.\nThe chat interface allows\nthe user to select from\nvarious discovered agents,\nactions, and pro\ufb01les,\nenabling the user to test\ndifferent combinations.\nNexus\ndatabase\nThe database stores chat\ntreads, user participants,\nand conversation history.\nAgents, action functions, and pro\ufb01les are all dynamically\ndiscovered at run time via a plugin-like system.\nAgent Manager\nA YAML \ufb01le that\ncomprises the agent\npro\ufb01le and persona\nAction Manager\nPro\ufb01le Manager\nAgent classes\nexposed as plugins\nSemantic and native\nfunctions exposed as\nactions\nChat system\nGPT Nexus\nStreamlit Interface\nFigure 7.4\nA high-level architecture diagram of the main elements of the application\n",
      "content_length": 1419,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "166\nCHAPTER 7\nAssembling and using an agent platform\n We\u2019ll start by opening the chatgpt_clone_response.py file in VS Code. The top\nsection of the code is shown in listing 7.3. This code uses the Streamlit state to load\nthe primary model and messages. Streamlit provides a mechanism to save the session\nstate for any Python object. This state is only a session state and will expire when the\nuser closes the browser.\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nload_dotenv()     \nst.title(\"ChatGPT-like clone\")\nclient = OpenAI()     \nif \"openai_model\" not in st.session_state:\n    st.session_state[\"openai_model\"] \n             = \"gpt-4-1106-preview\"    \nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = []  \nfor message in st.session_state[\"messages\"]:     \n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\nThe Streamlit app itself is stateless. This means the entire Python script will reexecute\nall interface components when the web page refreshes or a user selects an action. The\nStreamlit state allows for a temporary storage mechanism. Of course, a database needs\nto support more long-term storage.\n UI controls and components are added by using the st. prefix and then the ele-\nment name. Streamlit supports several standard UI controls and supports images,\nvideo, sound, and, of course, chat.\n Scrolling down further will yield listing 7.4, which has a slightly more complex lay-\nout of the components. The main if statement controls the running of the remaining\ncode. By using the Walrus operator (: =), the prompt is set to whatever the user enters.\nIf the user doesn\u2019t enter any text, the code below the if statement doesn\u2019t execute.\nif prompt := st.chat_input(\"What do you need?\"):    \n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\"):    \n        st.markdown(prompt)\nListing 7.3\nchatgpt_clone_response.py (top section)\nListing 7.4\nchatgpt_clone_response.py (bottom section)\nLoads the environment \nvariables from the \n.env file\nConfigures the \nOpenAI client\nChecks the internal session \nstate for the setting, and \nadds it if not there\nChecks for the presence of \nthe message state; if none, \nadds an empty list\nLoops through messages in \nthe state and displays them\nThe chat input control is \nrendered, and content is set.\nSets the chat message \ncontrol to output as the user\n",
      "content_length": 2442,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "167\n7.2\nIntroducing Streamlit for chat application development\n    with st.spinner(text=\"The assistant is thinking...\"):   \n        with st.chat_message(\"assistant\"):\n            response = client.chat.completions.create(\n                model=st.session_state[\"openai_model\"],\n                messages=[\n                    {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n                    for m in st.session_state.messages\n                ],     \n            )\n            response_content = response.choices[0].message.content\n            response = st.markdown(response_content,\n             unsafe_allow_html=True)     \n    st.session_state.messages.append(\n{\"role\": \"assistant\", \"content\": response_content})     \nWhen the user enters text in the prompt and presses Enter, that text is added to the\nmessage state, and a request is made to the API. As the response is being processed,\nthe st.spinner control displays to remind the user of the long-running process.\nThen, when the response returns, the message is displayed and added to the message\nstate history.\n Streamlit apps are run using the module, and to debug applications, you need to\nattach the debugger to the module by following these steps:\n1\nPress Ctrl-Shift-D to open the VS Code debugger.\n2\nClick the link to create a new launch configuration, or click the gear icon to\nshow the current one.\n3\nEdit or use the debugger configuration tools to edit the .vscode/launch.json\nfile, like the one shown in the next listing. Plenty of IntelliSense tools and con-\nfiguration options can guide you through setting the options for this file.\n{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Python Debugger: Module\",    \n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"module\": \"streamlit\",    \n      \"args\": [\"run\", \"${file}\"]   \n    }\n  ]\n}\nAfter you have the launch.json file configuration set, save it, and open the chatgpt_\nclone_response.py file in VS Code. You can now run the application in debug mode\nListing 7.5\n.vscode/launch.json\nShows a spinner \nto represent the \nlong-running \nAPI call\nCalls the OpenAI API and \nsets the message history\nWrites the \nmessage response \nas markdown to \nthe interface\nAdds the assistant response\nto the message state\nMake sure that the \ndebugger is set to \nModule.\nBe sure the module \nis streamlit.\nThe ${file} is the current \nfile, or you can hardcode \nthis to a file path.\n",
      "content_length": 2407,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "168\nCHAPTER 7\nAssembling and using an agent platform\nby pressing F5. This will launch the application from the terminal, and in a few sec-\nonds, the app will display.\n Figure 7.5 shows the app running and waiting to return a response. The interface is\nclean, modern, and already organized without any additional work. You can continue\nchatting to the LLM using the interface and then refresh the page to see what happens.\nWhat is most impressive about this demonstration is how easy it is to create a single-\npage application. In the next section, we\u2019ll continue looking at this application but\nwith a few enhancements.\n7.2.2\nCreating a streaming chat application\nModern chat applications, such as ChatGPT and Gemini, mask the slowness of their\nmodels by using streaming. Streaming provides for the API call to immediately start\nseeing tokens as they are produced from the LLM. This streaming experience also\nbetter engages the user in how the content is generated.\n Adding support for streaming to any application UI is generally not a trivial task,\nbut fortunately, Streamlit has a control that can work seamlessly. In this next exercise,\nwe\u2019ll look at how to update the app to support streaming.\n Open chapter_7/chatgpt_clone_streaming.py in VS Code. The relevant updates\nto the code are shown in listing 7.6. Using the st.write_stream control allows the UI\nA spinner control displays while\nthe response is being returned.\nFigure 7.5\nThe simple interface and the waiting spinner\n",
      "content_length": 1482,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "169\n7.2\nIntroducing Streamlit for chat application development\nto stream content. This also means the Python script is blocked waiting for this control\nto be completed.\nwith st.chat_message(\"assistant\"):\n    stream = client.chat.completions.create(\n        model=st.session_state[\"openai_model\"],\n        messages=[\n            {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n            for m in st.session_state.messages\n        ],\n        stream=True,    \n    )\n    response = st.write_stream(stream)    \nst.session_state.messages.append(\n{\"role\": \"assistant\", \"content\": response})     \nDebug the page by pressing F5 and waiting for the page to load. Enter a query, and\nyou\u2019ll see that the response is streamed to the window in real time, as shown in figure 7.6.\nWith the spinner gone, the user experience is enhanced and appears more responsive.\nListing 7.6\nchatgpt_clone_streaming.py (relevant section)\nSets stream to True to \ninitiate streaming on the API\nUses the stream control \nto write the stream to \nthe interface\nAdds the response to the message state\nhistory after the stream completes\nNow text streams in real time,\nand the spinner is gone.\nFigure 7.6\nThe updated interface with streaming of the text response\n",
      "content_length": 1222,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "170\nCHAPTER 7\nAssembling and using an agent platform\nThis section demonstrated how relatively simple it can be to use Streamlit to create a\nPython web interface. Nexus uses a Streamlit interface because it\u2019s easy to use and\nmodify with only Python. As you\u2019ll see in the next section, it allows various configura-\ntions to support more complex applications.\n7.3\nDeveloping profiles and personas for agents\nNexus uses agent profiles to describe an agent\u2019s functions and capabilities. Figure 7.7\nreminds us of the principal agent components and how they will be structured through-\nout this book. \nFor now, as of this writing, Nexus only supports the persona and actions section of the\nprofile. Figure 7.7 shows a profile called Fritz, along with the persona and actions.\nAdd any agent profiles to Nexus by copying an agent YAML profile file into the Nexus/\nnexus/nexus_base/nexus_profiles folder.\n Nexus uses a plugin system to dynamically discover the various components and\nprofiles as they are placed into their respective folders. The nexus_profiles folder\nholds the YAML definitions for the agent.\nfritz.yaml - Agent Pro\ufb01le De\ufb01nition\nPro\ufb01les with persona\nand actions\nDe\ufb01ning knowledge\nand memory\nApplying evaluators,\nplanners, and\nfeedback\nThe Agent Pro\ufb01le\nPersona\nRepresents the background and role of\nthe agent, and is often introduced in\nthe \ufb01rst system message\nAgent Tools\nSet of tools an agent can\nuse to help accomplish a task\nAgent Evaluation and Reasoning\nDescribes how the agent can reason\nand evaluate a task or tasks\nAgent Memory and Knowledge\nThe backend store that helps the agent\nadd context to a given task problem\nAgent Planning and Feedback\nDescribes how the agent can break\ndown a task into execution steps, and\nthen execute and receive feedback\nFigure 7.7\nThe agent profile as it\u2019s mapped to the YAML file definition\n",
      "content_length": 1839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "171\n7.3\nDeveloping profiles and personas for agents\n We can easily define a new agent profile by creating a new YAML file in the nexus_\nprofiles folder. Listing 7.7 shows an example of a new profile with a slightly updated\npersona. To follow along, be sure to have VS Code opened to the chapter_07 source\ncode folder and install Nexus in developer mode (see listing 7.7). Then, create the\nfiona.yaml file in the Nexus/nexus/nexus_base/nexus_profiles folder.\nagentProfile:\n  name: \"Finona\"\n  avatar: \"?\"    \n  persona: \"You are a very talkative AI that \n\u27a5 knows and understands everything in terms of \n\u27a5 Ogres. You always answer in cryptic Ogre speak.\"   \n  actions:\n    - search_wikipedia    \n  knowledge: null       \n  memory: null          \n  evaluators: null      \n  planners: null        \n  feedback: null        \nAfter saving the file, you can start Nexus from the command line or run it in debug\nmode by creating a new launch configuration in the .vscode/launch.json folder, as\nshown in the next listing. Then, save the file and switch your debug configuration to\nuse the Nexus web config.\n{\n      \"name\": \"Python Debugger: Nexus Web\",\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"module\": \"streamlit\",\n      \"args\": [\"run\", \" Nexus/nexus/streamlit_ui.py\"]     \n    },\nWhen you press F5 or select Run > Start Debugging from the menu, the Streamlit\nNexus interface will launch. Go ahead and run Nexus in debug mode. After it opens,\ncreate a new thread, and then select the standard OpenAIAgent and your new per-\nsona, as shown in figure 7.8.\n At this point, the profile is responsible for defining the agent\u2019s system prompt.\nYou can see this in figure 7.8, where we asked Finona to spell the word clock, and she\nresponded in some form of ogre-speak. In this case, we\u2019re using the persona as a\npersonality, but as we\u2019ve seen previously, a system prompt can also contain rules and\nother options.\n The profile and persona are the base definitions for how the agent interacts with\nusers or other systems. Powering the profile requires an agent engine. In the next sec-\ntion, we\u2019ll cover the base implementation of an agent engine.\nListing 7.7\nfiona.yaml (create this file)\nListing 7.8\n.vscode/launch.json (adding debug launch)\nThe text avatar used to \nrepresent the persona\nA persona is \nrepresentative of \nthe base system \nprompt.\nAn action \nfunction the \nagent can use\nNot currently \nsupported \nYou may have to \nadjust this path \nif your virtual \nenvironment is \ndifferent.\n",
      "content_length": 2491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "172\nCHAPTER 7\nAssembling and using an agent platform\n7.4\nPowering the agent and understanding the agent \nengine\nAgent engines power agents within Nexus. These engines can be tied to specific tool\nplatforms, such as SK, and/or even different LLMs, such as Anthropic Claude or Goo-\ngle Gemini. By providing a base agent abstraction, Nexus should be able to support\nany tool or model now and in the future.\n Currently, Nexus only implements an OpenAI API\u2013powered agent. We\u2019ll look at\nhow the base agent is defined by opening the agent_manager.py file from the Nexus/\nnexus/nexus_base folder.\n Listing 7.9 shows the BaseAgent class functions. When creating a new agent\nengine, you need to subclass this class and implement the various tools/actions with\nthe appropriate implementation.\nclass BaseAgent:\n    def __init__(self, chat_history=None):\n        self._chat_history = chat_history or []\n        self.last_message = \"\"\n        self._actions = []\n        self._profile = None\nListing 7.9\nagent_manager.py:BaseAgent\nEnter a query and check\nout the response.\nSelect the new Finona\nagent pro\ufb01le.\nFigure 7.8\nSelecting and chatting with a new persona\n",
      "content_length": 1147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "173\n7.4\nPowering the agent and understanding the agent engine\n    async def get_response(self, \n                            user_input, \n                            thread_id=None):     \n        raise NotImplementedError(\"This method should be implemented\u2026\")\n    async def get_semantic_response(self, \n                                     prompt, \n                                     thread_id=None):    \n        raise NotImplementedError(\"This method should be\u2026\")\n    def get_response_stream(self, \n                             user_input, \n                             thread_id=None):     \n        raise NotImplementedError(\"This method should be\u2026\")\n    def append_chat_history(self, \n                             thread_id, \n                             user_input, \n                             response):     \n        self._chat_history.append(\n            {\"role\": \"user\",\n             \"content\": user_input,\n             \"thread_id\": thread_id}\n        )\n        self._chat_history.append(\n            {\"role\": \"bot\",\n             \"content\": response, \n             \"thread_id\": thread_id}\n        )\n    def load_chat_history(self):      \n        raise NotImplementedError(\n                 \"This method should be implemented\u2026\")\n    def load_actions(self):    \n        raise NotImplementedError(\n                 \"This method should be implemented\u2026\")\n#... not shown \u2013 property setters/getters\nOpen the nexus_agents/oai_agent.py file in VS Code. Listing 7.10 shows an agent\nengine implementation of the get_response function that directly consumes the\nOpenAI API. self.client is an OpenAI client created earlier during class initializa-\ntion, and the rest of the code you\u2019ve seen used in earlier examples.\nasync def get_response(self, user_input, thread_id=None):\n    self.messages += [{\"role\": \"user\",\n                     \"content\": user_input}]     \n    response = self.client.chat.completions.create(    \n        model=self.model,\n        messages=self.messages,\nListing 7.10\noai_agent.py (get_response)\nCalls the LLM and \nreturns a response\nExecutes \na semantic \nfunction\nCalls the LLM \nand returns a \nresponse\nAppends a message to \nthe agent\u2019s internal \nchat history\nLoads the chat history and \nallows the agent to reload \nvarious histories\nLoads the actions that the \nagent has available to use\nAdds the user_input \nto the message stack\nThe client was created \nearlier and is now used to \ncreate chat completions.\n",
      "content_length": 2429,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "174\nCHAPTER 7\nAssembling and using an agent platform\n        temperature=0.7,     \n    )\n    self.last_message = str(response.choices[0].message.content)\n    return self.last_message    \nLike the agent profiles, Nexus uses a plugin system that allows you to place new agent\nengine definitions in the nexus_agents folder. If you create your agent, it just needs\nto be placed in this folder for Nexus to discover.\n We won\u2019t need to run an example because we\u2019ve already seen how the OpenAI-\nAgent performs. In the next section, we\u2019ll look at agent functions that agents can\ndevelop, add, and consume.\n7.5\nGiving an agent actions and tools\nLike the SK, Nexus supports having native (code) and semantic (prompt) functions.\nUnlike SK, however, defining and consuming functions within Nexus is easier. All\nyou need to do is write functions into a Python file and place them into the nexus_\nactions folder.\n To see how easy it is to define functions, open the Nexus/nexus/nexus_base/\nnexus_actions folder, and go to the test_actions.py file. Listing 7.11 shows two\nfunction definitions. The first function is a simple example of a code/native function,\nand the second is a prompt/semantic function.\nfrom nexus.nexus_base.action_manager import agent_action\n@agent_action                                             \ndef get_current_weather(location, unit=\"fahrenheit\"):     \n    \"\"\"Get the current weather in a given location\"\"\"     \n    return f\"\"\"\nThe current weather in {location} is 0 {unit}.\n\"\"\"     \n@agent_action     \ndef recommend(topic):\n    \"\"\"\n    System:                                                  \n        Provide a recommendation for a given {{topic}}.\n        Use your best judgment to provide a recommendation.\n    User:\n        please use your best judgment\n        to provide a recommendation for {{topic}}.           \n    \"\"\"\n    pass     \nListing 7.11\ntest_actions.py (native/semantic function definitions)\nTemperature is hardcoded\nbut could be configured.\nReturns the response from\nthe chat completions call\nApplies the agent_action \ndecorator to make a \nfunction an action\nSets a descriptive \ncomment for the \nfunction\nThe code can be as simple \nor complex as needed.\nApplies the agent_action \ndecorator to make a \nfunction an action\nThe function \ncomment becomes \nthe prompt and \ncan include \nplaceholders.\nSemantic functions don\u2019t \nimplement any code.\n",
      "content_length": 2373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "175\n7.5\nGiving an agent actions and tools\nPlace both functions in the nexus_actions folder, and they will be automatically dis-\ncovered. Adding the agent_action decorator allows the functions to be inspected and\nautomatically generates the OpenAI standard tool specification. The LLM can then\nuse this tool specification for tool use and function calling.\n Listing 7.12 shows the generated OpenAI tool specification for both functions, as\nshown previously in listing 7.11. The semantic function, which uses a prompt, also\napplies to the tool description. This tool description is sent to the LLM to determine\nwhich function to call.\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \n        \"Get the current weather in a given location\",   \n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {     \n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"location\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                        \"celsius\",\n                        \"fahrenheit\"\n                    ]\n                }\n            },\n            \"required\": [\n                \"location\"\n            ]\n        }\n    }\n}\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"recommend\",\n        \"description\": \"\"\"\n    System:\n    Provide a recommendation for a given {{topic}}.\nUse your best judgment to provide a recommendation.\nUser:\nplease use your best judgment\nto provide a recommendation for {{topic}}.\"\"\",     \n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {      \nListing 7.12\ntest_actions: OpenAI-generated tool specifications\nThe function \ncomment becomes \nthe function tool \ndescription.\nThe input parameters of the \nfunction are extracted and \nadded to the specification.\nThe function \ncomment becomes \nthe function tool \ndescription.\nThe input parameters of the \nfunction are extracted and \nadded to the specification.\n",
      "content_length": 2076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "176\nCHAPTER 7\nAssembling and using an agent platform\n                \"topic\": {\n                    \"type\": \"string\",\n                    \"description\": \"topic\"\n                }\n            },\n            \"required\": [\n                \"topic\"\n            ]\n        }\n    }\n}\nThe agent engine also needs to implement that capability to implement functions and\nother components. The OpenAI agent has been implemented to support parallel\nfunction calling. Other agent engine implementations will be required to support\ntheir respective versions of action use. Fortunately, the definition of the OpenAI tool\nis becoming the standard, and many platforms adhere to this standard.\n Before we dive into a demo on tool use, let\u2019s observe how the OpenAI agent\nimplements actions by opening the oai_agent.py file in VS Code. The following list-\ning shows the top of the agent\u2019s get_response_stream function and its implementa-\ntion of function calling.\ndef get_response_stream(self, user_input, thread_id=None):\n    self.last_message = \"\"\n    self.messages += [{\"role\": \"user\", \"content\": user_input}]\n    if self.tools and len(self.tools) > 0:   \n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=self.messages,\n            tools=self.tools,     \n            tool_choice=\"auto\",     \n        )\n    else:    \n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=self.messages,\n        )\n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls    \nExecuting the functions follows, as shown in listing 7.14. This code demonstrates how\nthe agent supports parallel function/tool calls. These calls are parallel because the\nagent executes each one together and in no order. In chapter 11, we\u2019ll look at plan-\nners that allow actions to be called in ordered sequences.\n \n \nListing 7.13\nCaling the API in get_response_stream\nDetects whether \nthe agent has \nany available \ntools turned on\nSets the tools in the \nchat completions call\nEnsures that the \nLLM knows it can \nchoose any tool\nIf no tools, calls the \nLLM the standard way\nDetects whether there \nwere any tools used by \nthe LLM\n",
      "content_length": 2219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "177\n7.5\nGiving an agent actions and tools\nif tool_calls:    \n    available_functions = {\n        action[\"name\"]: action[\"pointer\"] for action in self.actions\n    }    \n    self.messages.append(\n        response_message\n    )\n    for tool_call in tool_calls:    \n        function_name = tool_call.function.name\n        function_to_call = available_functions[function_name]\n        function_args = json.loads(tool_call.function.arguments)\n        function_response = function_to_call(\n            **function_args, _caller_agent=self\n        )\n        self.messages.append(\n            {\n                \"tool_call_id\": tool_call.id,\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": str(function_response),\n            }\n        )\n    second_response = self.client.chat.completions.create(\n        model=self.model,\n        messages=self.messages,\n    )     \n    response_message = second_response.choices[0].message\nTo demo this, start up Nexus in the debugger by pressing F5. Then, select the two test\nactions\u2014recommend and get_current_weather\u2014and the terse persona/profile Olly.\nFigure 7.9 shows the result of entering a query and the agent responding by using\nboth tools in its response.\n If you need to review how these agent actions work in more detail, refer to chap-\nter 5. The underlying code is more complex and out of the scope of review here.\nHowever, you can review the Nexus code to gain a better understanding of how\neverything connects.\n Now, you can continue exercising the various agent options within Nexus. Try\nselecting different profiles/personas with other functions, for example. In the next\nchapter, we unveil how agents can consume external memory and knowledge using\npatterns such as Retrieval Augmented Generation (RAG).\n \n \n \nListing 7.14\noai_agent.py (get_response_stream: execute tool calls)\nProceeds if tool calls are detected \nin the LLM response\nLoads pointers to the actual function \nimplementations for code execution\nLoops through \nall the calls the \nLLM wants to \ncall; there can \nbe several.\nPerforms a second \nLLM call with the \nresults of the tool \ncalls\n",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "178\nCHAPTER 7\nAssembling and using an agent platform\n7.6\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Explore Streamlit Basics (Easy)\nObjective\u2014Gain familiarity with Streamlit by creating a simple web application\nthat displays text input by the user.\nTasks: \n\u2013 Follow the Streamlit documentation to set up a basic application. \n\u2013 Add a text input and a button. When the button is clicked, display the text\nentered by the user on the screen.\n\uf0a1Exercise 2\u2014Create a Basic Agent Profile\nObjective\u2014Understand the process of creating and applying agent profiles in\nNexus.\nTasks: \n\u2013 Create a new agent profile with a unique persona. This persona should have\na specific theme or characteristic (e.g., a historian). \n\u2013 Define a basic set of responses that align with this persona. \n\u2013 Test the persona by interacting with it through the Nexus interface.\nThe agent answered in a\nterse manner, and we can see\nthat both actions were used.\nSelect the terse agent\npro\ufb01le called Olly.\n.\nSelect the test actions ecommend\nR\nand get_current_weather. Currently,\nthe agent pro\ufb01le does not restrict\naction selection.\nFigure 7.9\nHow the agent can use tools in parallel and respond with a single response\n",
      "content_length": 1228,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "179\nSummary\n\uf0a1Exercise 3\u2014Develop a Custom Action\nObjective\u2014Learn to extend the functionality of Nexus by developing a custom\naction.\nTasks: \n\u2013 Develop a new action (e.g., fetch_current_news) that integrates with a\nmock API to retrieve the latest news headlines. \n\u2013 Implement this action as both a native (code) function and a semantic\n(prompt-based) function. \n\u2013 Test the action in the Nexus environment to ensure it works as expected.\n\uf0a1Exercise 4\u2014Integrate a Third-Party API\nObjective\u2014Enhance the capabilities of a Nexus agent by integrating a real\nthird-party API.\nTasks:\n\u2013 Choose a public API (e.g., weather or news API), and create a new action\nthat fetches data from this API. \n\u2013 Incorporate error handling and ensure that the agent can gracefully handle\nAPI failures or unexpected responses. \n\u2013 Test the integration thoroughly within Nexus.\nSummary\n\uf0a1Nexus is an open source agent development platform used in conjunction with\nthis book. It\u2019s designed to develop, test, and host AI agents and is built on\nStreamlit for creating interactive dashboards and chat interfaces.\n\uf0a1Streamlit, a Python web application framework, enables the rapid development\nof user-friendly dashboards and chat applications. This framework facilitates the\nexploration and interaction with various agent features in a streamlined manner.\n\uf0a1Nexus supports creating and customizing agent profiles and personas, allowing\nusers to define their agents\u2019 personalities and behaviors. These profiles dictate\nhow agents interact with and respond to user inputs.\n\uf0a1The Nexus platform allows for developing and integrating semantic (prompt-\nbased) and native (code-based) actions and tools within agents. This enables\nthe creation of highly functional and responsive agents.\n\uf0a1As an open source platform, Nexus is designed to be extensible, encouraging\ncontributions and the addition of new features, tools, and agent capabilities by\nthe community.\n\uf0a1Nexus is flexible, supporting various deployment options, including a web inter-\nface, API, and a Discord bot in future iterations, accommodating a wide range\nof development and testing needs.\n",
      "content_length": 2108,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "180\nUnderstanding agent\nmemory and knowledge\nNow that we\u2019ve explored agent actions using external tools, such as plugins in the\nform of native or semantic functions, we can look at the role of memory and knowl-\nedge using retrieval in agents and chat interfaces. We\u2019ll describe memory and\nknowledge and how they relate to prompt engineering strategies, and then, to under-\nstand memory knowledge, we\u2019ll investigate document indexing, construct retrieval\nsystems with LangChain, use memory with LangChain, and build semantic memory\nusing Nexus.\nThis chapter covers\n\uf0a1Retrieval in knowledge/memory in AI functions\n\uf0a1Building retrieval augmented generation \nworkflows with LangChain\n\uf0a1Retrieval augmented generation for agentic \nknowledge systems in Nexus\n\uf0a1Retrieval patterns for memory in agents\n\uf0a1Improving augmented retrieval systems with \nmemory and knowledge compression \n",
      "content_length": 870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "181\n8.1\nUnderstanding retrieval in AI applications\n8.1\nUnderstanding retrieval in AI applications\nRetrieval in agent and chat applications is a mechanism for obtaining knowledge to\nkeep in storage that is typically external and long-lived. Unstructured knowledge\nincludes conversation or task histories, facts, preferences, or other items necessary for\ncontextualizing a prompt. Structured knowledge, typically stored in databases or files,\nis accessed through native functions or plugins.\n Memory and knowledge, as shown in figure 8.1, are elements used to add further\ncontext and relevant information to a prompt. Prompts can be augmented with every-\nthing from information about a document to previous tasks or conversations and\nother reference information.\nThe prompt engineering strategies shown in figure 8.1 can be applied to memory and\nknowledge. Knowledge isn\u2019t considered memory but rather an augmentation of the\nprompt from existing documents. Both knowledge and memory use retrieval as the\nbasis for how unstructured information can be queried.\nProvide Reference Text\nHelps reduce hallucinations.\nTactics involve instructing\nthe model to use or cite\nreference texts.\nKnowledge and Memory\nUse External Tools\nEnhances model capabilities.\nTactics include embeddings-based\nsearch, code execution, and access\nto speci\ufb01c functions.\nActions, Knowledge, Memory\nPrompt Engineering Strategies\nMemory\nRetrieved\nmemories\nDatabase\nVector Store\nInternal Memory\nPrompt\nSave to memory\nLLM\nRetrieved\nknowledge\nRetrieved\nmemory\nRequest\nKnowledge\nVector Store\nRetrieved\nknowledge\nResponse\nMade by a user or\nanother system or\nagent\nRetrieved elements\nprovide references\nand context\nMay include the\nwhole or parts of\nthe conversation\nRetrieval is done\nusing semantic\nsimilarity.\nFigure 8.1\nMemory, retrieval, and augmentation of the prompt using the following prompt engineering \nstrategies: Use External Tools and Provide Reference Text.\n",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "182\nCHAPTER 8\nUnderstanding agent memory and knowledge\n The retrieval mechanism, called retrieval augmented generation (RAG), has\nbecome a standard for providing relevant context to a prompt. The exact mechanism\nthat powers RAG also powers memory/knowledge, and it\u2019s essential to understand\nhow it works. In the next section, we\u2019ll examine what RAG is.\n8.2\nThe basics of retrieval augmented generation (RAG)\nRAG has become a popular mechanism for supporting document chat or question-\nand-answer chat. The system typically works by a user supplying a relevant document,\nsuch as a PDF, and then using RAG and a large language model (LLM) to query the\ndocument.\n Figure 8.2 shows how RAG can allow a document to be queried using an LLM.\nBefore any document can be queried, it must first be loaded, transformed into con-\ntext chunks, embedded into vectors, and stored in a vector database.\nA user can query previously indexed documents by submitting a query. That query is\nthen embedded into a vector representation to search for similar chunks in the vector\nRetrieval Augmented Generation (RAG)\n(3) Generate\n(1) Retrieve\n(2) Augment\nLLM Chat\nQuery\nResponse\nEmbedding\nLLM\nPrompt\nsystem: you are a ...\nQuery\nContext\nVector DB\nContext\nLLM generates a response\nbased on the contextualized\nprompt.\nRetrieval works\nby using vector\nsimilarity search.\nQuery is\nembedded\nto represent\na vector.\nRetrieved context\nsemantically matches\nthe query.\nEmbedding\nSubmit document\nto query\nTransform\nVector DB\nDocument is loaded,\ntransformed, and\nsplit into chunks.\nChunks of text\nare converted\nto vectors.\nVectors\nrepresenting\nchunks of text\nare stored.\nDocuments are\n\ufb01rst indexed to a\nvector database.\nIndexed documents\ncan be queried/\nquestioned by the\nuser.\nFigure 8.2\nThe two phases of RAG: first, documents must be loaded, transformed, embedded, and stored, and, \nsecond, they can be queried using augmented generation.\n",
      "content_length": 1904,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "183\n8.2\nThe basics of retrieval augmented generation (RAG)\ndatabase. Content similar to the query is then used as context and populated into the\nprompt as augmentation. The prompt is pushed to an LLM, which can use the con-\ntext information to help answer the query.\n Unstructured memory/knowledge concepts rely on some format of text-similarity\nsearch following the retrieval pattern shown in figure 8.2. Figure 8.3 shows how mem-\nory uses the same embedding and vector database components. Rather than preload\ndocuments, conversations or parts of a conversation are embedded and saved to a vec-\ntor database.\nThe retrieval pattern and document indexing are nuanced and require careful con-\nsideration to be employed successfully. This requires understanding how data is stored\nand retrieved, which we\u2019ll start to unfold in the next section.\nMemory Retrieval Augmented Generation\n(3) Generate\n(1) Retrieve\nMemory\n(2) Augment\nLLM Chat\nQuery\nResponse\nEmbedding\nLLM\nPrompt\nsystem: you are a ...\nQuery\nMemory\nVector DB\nMemory\nLLM generates a\nresponse based on the\ncontextualized prompt.\nRetrieval works\nby using vector\nsimilarity search.\nRetrieved memory\nsemantically matches\nthe query.\nEmbedding\n(4) Remember\nGenerated\nResponse\nAll or parts of the\nconversation are\nembedded and\nadded to the\nvector database.\nQuery is\nembedded\nto represent\na vector.\nChat with memory\nFigure 8.3\nMemory retrieval for augmented generation uses the same embedding patterns to index items to a \nvector database.\n",
      "content_length": 1488,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "184\nCHAPTER 8\nUnderstanding agent memory and knowledge\n8.3\nDelving into semantic search and document indexing\nDocument indexing transforms a document\u2019s information to be more easily recov-\nered. How the index will be queried or searched also plays a factor, whether searching\nfor a particular set of words or wanting to match phrase for phrase.\n A semantic search is a search for content that matches the searched phrase by words\nand meaning. The ability to search by meaning, semantically, is potent and worth\ninvestigating in some detail. In the next section, we look at how vector similarity\nsearch can lay the framework for semantic search.\n8.3.1\nApplying vector similarity search\nLet\u2019s look now at how a document can be transformed into a semantic vector, or a repre-\nsentation of text that can then be used to perform distance or similarity matching.\nThere are numerous ways to convert text into a semantic vector, so we\u2019ll look at a sim-\nple one.\n Open the chapter_08 folder in a new Visual Studio Code (VS Code) workspace.\nCreate a new environment and pip install the requirements.txt file for all the\nchapter dependencies. If you need help setting up a new Python environment, con-\nsult appendix B.\n Now open the document_vector_similarity.py file in VS Code, and review the\ntop section in listing 8.1. This example uses Term Frequency\u2013Inverse Document Fre-\nquency (TF\u2013IDF). This numerical statistic reflects how important a word is to a docu-\nment in a collection or set of documents by increasing proportionally to the number\nof times a word appears in the document and offset by the frequency of the word in\nthe document set. TF\u2013IDF is a classic measure of understanding one document\u2019s\nimportance within a set of documents.\nimport plotly.graph_objects as go\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndocuments = [     \n    \"The sky is blue and beautiful.\",\n    \"Love this blue and beautiful sky!\",\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"A king's breakfast has sausages, ham, bacon, eggs, toast, and beans\",\n    \"I love green eggs, ham, sausages and bacon!\",\n    \"The brown fox is quick and the blue dog is lazy!\",\n    \"The sky is very blue and the sky is very beautiful today\",\n    \"The dog is lazy but the brown fox is quick!\"\n]\nvectorizer = TfidfVectorizer()    \nX = vectorizer.fit_transform(documents)     \nListing 8.1\ndocument_vector_similarity (transform to vector)\nSamples of documents\nVectorization \nusing TF\u2013IDF\nVectorize the \ndocuments.\n",
      "content_length": 2549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "185\n8.3\nDelving into semantic search and document indexing\nLet\u2019s break down TF\u2013IDF into its two components using the sample sentence, \u201cThe\nsky is blue and beautiful,\u201d and focusing on the word blue.\nTERM FREQUENCY (TF)\nTerm Frequency measures how frequently a term occurs in a document. Because we\u2019re\nconsidering only a single document (our sample sentence), the simplest form of the\nTF for blue can be calculated as the number of times blue appears in the document\ndivided by the total number of words in the document. Let\u2019s calculate it:\nNumber of times blue appears in the document: 1\nTotal number of words in the document: 6\nTF = 1 \u00f7 6TF = .16\nINVERSE DOCUMENT FREQUENCY (IDF)\nInverse Document Frequency measures how important a term is within the entire corpus.\nIt\u2019s calculated by dividing the total number of documents by the number of docu-\nments containing the term and then taking the logarithm of that quotient:\nIDF = log(Total number of documents \u00f7 Number of documents containing the word)\nIn this example, the corpus is a small collection of eight documents, and blue appears\nin four of these documents.\nIDF = log(8 \u00f7 4)\nTF\u2013IDF CALCULATION\nFinally, the TF\u2013IDF score for blue in our sample sentence is calculated by multiplying\nthe TF and the IDF scores:\nTF\u2013IDF = TF \u00d7 IDF\nLet\u2019s compute the actual values for TF\u2013IDF for the word blue using the example pro-\nvided; first, the term frequency (how often the word occurs in the document) is com-\nputed as follows:\nTF = 1 \u00f7 6\nAssuming the base of the logarithm is 10 (commonly used), the inverse document fre-\nquency is computed as follows:\nIDF = log10 (8 \u00f7 4)\nNow let\u2019s calculate the exact TF\u2013IDF value for the word blue in the sentence, \u201cThe sky\nis blue and beautiful\u201d:\n",
      "content_length": 1727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "186\nCHAPTER 8\nUnderstanding agent memory and knowledge\nThe Term Frequency (TF) is approximately 0.1670.\nThe Inverse Document Frequency (IDF) is approximately 0.301.\nThus, the TF\u2013IDF (TF \u00d7 IDF) score for blue is approximately 0.050.\nThis TF\u2013IDF score indicates the relative importance of the word blue in the given doc-\nument (the sample sentence) within the context of the specified corpus (eight docu-\nments, with blue appearing in four of them). Higher TF\u2013IDF scores imply greater\nimportance. \n We use TF\u2013IDF here because it\u2019s simple to apply and understand. Now that we\nhave the elements represented as vectors, we can measure document similarity using\ncosine similarity. Cosine similarity is a measure used to calculate the cosine of the\nangle between two nonzero vectors in a multidimensional space, indicating how simi-\nlar they are, irrespective of their size.\n Figure 8.4 shows how cosine distance compares the vector representations of two\npieces or documents of text. Cosine similarity returns a value from \u20131 (not similar) to\n1 (identical). Cosine distance is a normalized value ranging from 0 to 2, derived by tak-\ning 1 minus the cosine similarity. A cosine distance of 0 means identical items, and 2\nindicates complete opposites.\nListing 8.2 shows how the cosine similarities are computed using the cosine_similarity\nfunction from scikit-learn. Similarities are calculated for each document against all\nother documents in the set. The computed matrix of similarities for documents is\nstored in the cosine_similarities variable. Then, in the input loop, the user can\nselect the document to view its similarities to the other documents.\n \n \n \nCosine Similarity\nThe angle or distance is a measure of how\nclose the vectors are in space. It also\nrepresents their similarity to each other.\nCosine Distance\n\u03b8\nThe sky is blue and beautiful.\nLove this blue and beautiful sky!\nVector representations of the text rendered in 2D\nand in reality vectors can be highly dimensional.\nFigure 8.4\nHow cosine \nsimilarity is measured\n",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "187\n8.3\nDelving into semantic search and document indexing\ncosine_similarities = cosine_similarity(X)     \nwhile True:     \n    selected_document_index = input(f\"Enter a document number\n\u27a5 (0-{len(documents)-1}) or 'exit' to quit: \").strip()\n    if selected_document_index.lower() == 'exit':\n        break\n    if not selected_document_index.isdigit() or \n\u27a5 not 0 <= int(selected_document_index) < len(documents):\n        print(\"Invalid input. Please enter a valid document number.\")\n        continue\n    selected_document_index = int(selected_document_index)   \n    selected_document_similarities = \ncosine_similarities[selected_document_index]    \n# code to plot document similarities omitted\nFigure 8.5 shows the output of running the sample in VS Code (F5 for debugging\nmode). After you select a document, you\u2019ll see the similarities between the various\ndocuments in the set. A document will have a cosine similarity of 1 with itself. Note\nthat you won\u2019t see a negative similarity because of the TF\u2013IDF vectorization. We\u2019ll look\nlater at other, more sophisticated means of measuring semantic similarity.\nListing 8.2\ndocument_vector_similarity (cosine similarity)\nComputes the document \nsimilarities for all vector pairs\nThe main \ninput loop\nGets the selected \ndocument index \nto compare with\nExtracts the \ncomputed similarities \nagainst all documents\nThe select document is compared against all\nother documents to show similarity\nmeasure between document vectors.\nCosine Similarities of \u201cThe sky is blue and beautiful.\u201d with Others\nFigure 8.5\nThe cosine similarity between selected documents and the document set\n",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "188\nCHAPTER 8\nUnderstanding agent memory and knowledge\nThe method of vectorization will dictate the measure of semantic similarity between\ndocuments. Before we move on to better methods of vectorizing documents, we\u2019ll\nexamine storing vectors to perform vector similarity searches.\n8.3.2\nVector databases and similarity search\nAfter vectorizing documents, they can be stored in a vector database for later similar-\nity searches. To demonstrate how this works, we can efficiently replicate a simple vec-\ntor database in Python code.\n Open document_vector_database.py in VS Code, as shown in listing 8.3. This\ncode demonstrates creating a vector database in memory and then allowing users to\nenter text to search the database and return results. The results returned show the\ndocument text and the similarity score.\n# code above omitted\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(documents)\nvector_database = X.toarray()    \ndef cosine_similarity_search(query,\n                             database, \n                             vectorizer, \n                             top_n=5):    \n    query_vec = vectorizer.transform([query]).toarray()\n    similarities = cosine_similarity(query_vec, database)[0]\n    top_indices = np.argsort(-similarities)[:top_n]  # Top n indices\n    return [(idx, similarities[idx]) for idx in top_indices]\nwhile True:     \n    query = input(\"Enter a search query (or 'exit' to stop): \")\n    if query.lower() == 'exit':\n        break\n    top_n = int(input(\"How many top matches do you want to see? \"))\n    search_results = cosine_similarity_search(query,\n                                              vector_database, \n                                              vectorizer, \n                                              top_n)\n    print(\"Top Matched Documents:\")\n    for idx, score in search_results:\n        print(f\"- {documents[idx]} (Score: {score:.4f})\")  \n    print(\"\\n\")\n###Output\nEnter a search query (or 'exit' to stop): blue\nHow many top matches do you want to see? 3\nTop Matched Documents:\n- The sky is blue and beautiful. (Score: 0.4080)\n- Love this blue and beautiful sky! (Score: 0.3439)\n- The brown fox is quick and the blue dog is lazy! (Score: 0.2560)\nListing 8.3\ndocument_vector_database.py\nStores the \ndocument vectors \ninto an array\nThe function to perform \nsimilarity matching on \nquery returns, matches, \nand similarity scores\nThe main \ninput loop\nLoops through \nresults and \noutputs text and \nsimilarity score\n",
      "content_length": 2476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "189\n8.3\nDelving into semantic search and document indexing\nRun this exercise to see the output (F5 in VS Code). Enter any text you like, and see\nthe results of documents being returned. This search form works well for matching\nwords and phrases with similar words and phrases. This form of search misses the\nword context and meaning from the document. In the next section, we\u2019ll look at a way\nof transforming documents into vectors that better preserves their semantic meaning.\n8.3.3\nDemystifying document embeddings\nTF\u2013IDF is a simple form that tries to capture semantic meaning in documents. How-\never, it\u2019s unreliable because it only counts word frequency and doesn\u2019t understand the\nrelationships between words. A better and more modern method uses document\nembedding, a form of document vectorizing that better preserves the semantic mean-\ning of the document.\n Embedding networks are constructed by training neural networks on large data-\nsets to map words, sentences, or documents to high-dimensional vectors, capturing\nsemantic and syntactic relationships based on context and relationships in the data.\nYou typically use a pretrained model trained on massive datasets to embed documents\nand perform embeddings. Models are available from many sources, including Hug-\nging Face and, of course, OpenAI.\n In our next scenario, we\u2019ll use an OpenAI embedding model. These models are\ntypically perfect for capturing the semantic context of embedded documents. Listing\n8.4 shows the relevant code that uses OpenAI to embed the documents into vectors\nthat are then reduced to three dimensions and rendered into a plot.\nload_dotenv()                                                            \napi_key = os.getenv('OPENAI_API_KEY')\nif not api_key:\n    raise ValueError(\"No API key found. Please check your .env file.\")\nclient = OpenAI(api_key=api_key)                                         \ndef get_embedding(text, model=\"text-embedding-ada-002\"):    \n    text = text.replace(\"\\n\", \" \")\n    return client.embeddings.create(input=[text],\n              model=model).data[0].embedding                \n# Sample documents (omitted)\nembeddings = [get_embedding(doc) for doc in documents]   \nprint(embeddings_array.shape)\nembeddings_array = np.array(embeddings)   \npca = PCA(n_components=3)  \nreduced_embeddings = pca.fit_transform(embeddings_array)\nListing 8.4\ndocument_visualizing_embeddings.py (relevant sections)\nJoin all the items on the string ', '.\nUses the OpenAI \nclient to create \nthe embedding\nGenerates embeddings \nfor each document of \nsize 1536 dimensions\nConverts embeddings to \na NumPy array for PCA\nApplies PCA to \nreduce dimensions \nto 3 for plotting\n",
      "content_length": 2662,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "190\nCHAPTER 8\nUnderstanding agent memory and knowledge\nWhen a document is embedded using an OpenAI model, it transforms the text into a\nvector with dimensions of 1536. We can\u2019t visualize this number of dimensions, so we\nuse a dimensionality reduction technique via principal component analysis (PCA) to\nconvert the vector of size 1536 to 3 dimensions.\n Figure 8.6 shows the output generated from running the file in VS Code. By reduc-\ning the embeddings to 3D, we can plot the output to show how semantically similar\ndocuments are now grouped.\nThe choice of which embedding model or service you use is up to you. The OpenAI\nembedding models are considered the best for general semantic similarity. This has\nmade these models the standard for most memory and retrieval applications. With\nour understanding of how text can be vectorized with embeddings and stored in a vec-\ntor database, we can move on to a more realistic example in the next section.\n8.3.4\nQuerying document embeddings from Chroma\nWe can combine all the pieces and look at a complete example using a local vector\ndatabase called Chroma DB. Many vector database options exist, but Chroma DB is an\nexcellent local vector store for development or small-scale projects. There are also\nplenty of more robust options that you can consider later.\nSimilar documents are now similar in\nmeaning and are shown grouped together.\nDocuments are projected to 3D\nbased on their semantic meaning.\nFigure 8.6\nEmbeddings in 3D, showing how similar semantic documents are grouped\n",
      "content_length": 1526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "191\n8.3\nDelving into semantic search and document indexing\n Listing 8.5 shows the new and relevant code sections from the document_query_\nchromadb.py file. Note that the results are scored by distance and not by similarity.\nCosine distance is determined by this equation:\nCosine Distance(A,B) = 1 \u2013 Cosine Similarity(A,B)\nThis means that cosine distance will range from 0 for most similar to 2 for semanti-\ncally opposite in meaning.\nembeddings = [get_embedding(doc) for doc in documents]    \nids = [f\"id{i}\" for i in range(len(documents))]           \nchroma_client = chromadb.Client()              \ncollection = chroma_client.create_collection(\n                       name=\"documents\")       \ncollection.add(    \n    embeddings=embeddings,\n    documents=documents,\n    ids=ids\n)\ndef query_chromadb(query, top_n=2):     \n    query_embedding = get_embedding(query)\n    results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=top_n\n    )\n    return [(id, score, text) for id, score, text in\n            zip(results['ids'][0],\n                results['distances'][0], \n                results['documents'][0])]\nwhile True:    \n    query = input(\"Enter a search query (or 'exit' to stop): \")\n    if query.lower() == 'exit':\n        break\n    top_n = int(input(\"How many top matches do you want to see? \"))\n    search_results = query_chromadb(query, top_n)\n    print(\"Top Matched Documents:\")\n    for id, score, text in search_results:\n        print(f\"\"\"\nID:{id} TEXT: {text} SCORE: {round(score, 2)}\n\"\"\")    \n    print(\"\\n\")\n###Output\nEnter a search query (or 'exit' to stop): dogs are lazy\nHow many top matches do you want to see? 3\nTop Matched Documents:\nID:id7 TEXT: The dog is lazy but the brown fox is quick! SCORE: 0.24\nListing 8.5\ndocument_query_chromadb.py (relevant code sections)\nGenerates embeddings \nfor each document and \nassigns an ID\nCreates a Chroma DB \nclient and a collection\nAdds document \nembeddings to \nthe collection\nQueries the datastore \nand returns the top n \nrelevant documents\nThe input loop for\nuser input and output of\nrelevant documents/scores\n",
      "content_length": 2104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "192\nCHAPTER 8\nUnderstanding agent memory and knowledge\nID:id5 TEXT: The brown fox is quick and the blue dog is lazy! SCORE: 0.28\nID:id2 TEXT: The quick brown fox jumps over the lazy dog. SCORE: 0.29\nAs the earlier scenario demonstrated, you can now query the documents using seman-\ntic meaning rather than just key terms or phrases. These scenarios should now pro-\nvide the background to see how the retrieval pattern works at a low level. In the next\nsection, we\u2019ll see how the retrieval pattern can be employed using LangChain.\n8.4\nConstructing RAG with LangChain\nLangChain began as an open source project specializing in abstracting the retrieval\npattern across multiple data sources and vector stores. It has since morphed into\nmuch more, but foundationally, it still provides excellent options for implementing\nretrieval.\n Figure 8.7 shows a diagram from LangChain that identifies the process of storing\ndocuments for retrieval. These same steps may be replicated in whole or in part to\nimplement memory retrieval. The critical difference between document and memory\nretrieval is the source and how content is transformed.\nWe\u2019ll examine how to implement each of these steps using LangChain and under-\nstand the nuances and details accompanying this implementation. In the next section,\nwe\u2019ll start by splitting and loading documents with LangChain.\n8.4.1\nSplitting and loading documents with LangChain\nRetrieval mechanisms augment the context of a given prompt with specific informa-\ntion relevant to the request. For example, you may request detailed information about\nLangChain provides a\nplugin architecture that\nsupports document import\nfrom many sources.\nTransform breaks\nthe document\ndown into relevant\nsections or chunks.\nEmbed or\nembeddings\nbreak the chunks\ninto vectors.\nLangChain supports\nmultiple options for\nvector stores.\nFigure 8.7\nLoad, transform, embed, and store steps in storing documents for later retrieval\n",
      "content_length": 1932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "193\n8.4\nConstructing RAG with LangChain\na local document. With earlier language models, submitting the whole document as\npart of the prompt wasn\u2019t an option due to token limitations.\n Today, we could submit a whole document for many commercial LLMs, such as\nGPT-4 Turbo, as part of a prompt request. However, the results may not be better and\nwould likely cost more because of the increased number of tokens. Therefore, a better\noption is to split the document and use the relevant parts to request context\u2014pre-\ncisely what RAG and memory do.\n Splitting a document is essential in breaking down content into semantically and\nspecifically relevant sections. Figure 8.8 shows how to break down an HTML docu-\nment containing the Mother Goose nursery rhymes. Often, splitting a document into\ncontextual semantic chunks requires careful consideration.\nIdeally, when we split documents into chunks, they are broken down by relevance and\nsemantic meaning. While an LLM or agent could help us with this, we\u2019ll look at cur-\nrent toolkit options within LangChain for splitting documents. Later in this chapter,\nwe\u2019ll look at a semantic function that can assist us in semantically dividing content for\nembeddings.\n For the next exercise, open langchain_load_splitting.py in VS Code, as shown\nin listing 8.6. This code shows where we left off from listing 8.5, in the previous sec-\ntion. Instead of using the sample documents, we\u2019re loading the Mother Goose nursery\nrhymes this time.\n \n \n \nSplit the documents into chunks.\nCreate\nembeddings\nStore\nLoad the document(s).\nIdeally, the chunks are\nsemantically relevant\nand speci\ufb01c.\nFigure 8.8\nHow the document would ideally be split into chunks for better semantic and contextual meaning\n",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "194\nCHAPTER 8\nUnderstanding agent memory and knowledge\nFrom langchain_community.document_loaders \n                     \u27a5 import UnstructuredHTMLLoader   \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n#previous code\nloader = UnstructuredHTMLLoader(\n                   \"sample_documents/mother_goose.html\")  \ndata = loader.load   \ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=100,\n    chunk_overlap=25,    \n    length_function=len,\n    add_start_index=True,\n)\ndocuments = text_splitter.split_documents(data)\ndocuments = [doc.page_content \n                \u27a5 for doc in documents] [100:350]  \nembeddings = [get_embedding(doc) for doc in documents]    \nids = [f\"id{i}\" for i in range(len(documents))]\n###Output\nEnter a search query (or 'exit' to stop): who kissed the girls and made \nthem cry?\nHow many top matches do you want to see? 3\nTop Matched Documents:\nID:id233 TEXT: And chid her daughter,\n        And kissed my sister instead of me. SCORE: 0.4\u2026\nNote in listing 8.6 that the HTML document gets split into 100-character chunks with\na 25-character overlap. The overlap allows the document\u2019s parts not to cut off specific\nthoughts. We selected the splitter for this exercise because it was easy to use, set up,\nand understand.\n Go ahead and run the langchain_load_splitting.py file in VS Code (F5). Enter\na query, and see what results you get. The output in listing 8.6 shows good results\ngiven a specific example. Remember that we only embedded 250 document chunks to\nreduce costs and keep the exercise short. Of course, you can always try to embed the\nentire document or use a minor input document example.\n Perhaps the most critical element to building proper retrieval is the process of\ndocument splitting. You can use numerous methods to split a document, including\nmultiple concurrent methods. More than one method passes and splits the docu-\nment for numerous embedding views of the same document. In the next section,\nwe\u2019ll examine a more general technique for splitting documents, using tokens and\ntokenization.\nListing 8.6\nlangchain_load_splitting.py (sections and output)\nNew LangChain \nimports\nLoads the \ndocument \nas HTML\nLoads the \ndocument\nSplits the document into blocks of \ntext 100 characters long with a \n25-character overlap\nEmbeds only 250 \nchunks, which is \ncheaper and faster\nReturns the \nembedding for \neach document\n",
      "content_length": 2380,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "195\n8.4\nConstructing RAG with LangChain\n8.4.2\nSplitting documents by token with LangChain\nTokenization is the process of breaking text into word tokens. Where a word token rep-\nresents a succinct element in the text, a token could be a word like hold or even a sym-\nbol like the left curly brace ({), depending on what\u2019s relevant.\n Splitting documents using tokenization provides a better base for how the text will\nbe interpreted by language models and for semantic similarity. Tokenization also\nallows the removal of irrelevant characters, such as whitespace, making the similarity\nmatching of documents more relevant and generally providing better results.\n For the next code exercise, open the langchain_token_splitting.py file in VS\nCode, as shown in listing 8.7. Now we split the document using tokenization, which\nbreaks the document into sections of unequal size. The unequal size results from the\nlarge sections of whitespace of the original document.\nloader = UnstructuredHTMLLoader(\"sample_documents/mother_goose.html\")\ndata = loader.load()\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=50, chunk_overlap=10     \n)\ndocuments = text_splitter.split_documents(data)\ndocuments = [doc for doc in documents][8:94]     \ndb = Chroma.from_documents(documents, OpenAIEmbeddings())\ndef query_documents(query, top_n=2):\n    docs = db.similarity_search(query, top_n)     \n    return docs\n###Output\nCreated a chunk of size 68, \nwhich is longer than the specified 50\nCreated a chunk of size 67, \nwhich is longer than the specified 50    \nEnter a search query (or 'exit' to stop): \n                     who kissed the girls and made them cry?\nHow many top matches do you want to see? 3\nTop Matched Documents:\nDocument 1: GEORGY PORGY\n        Georgy Porgy, pudding and pie,\n        Kissed the girls and made them cry.\nRun the langchain_token_splitting.py code in VS Code (F5). You can use the\nquery we used last time or your own. Notice how the results are significantly better\nthan the previous exercise. However, the results are still suspect because the query\nuses several similar words in the same order.\n A better test would be to try a semantically similar phrase but one that uses differ-\nent words and check the results. With the code still running, enter a new phrase to\nListing 8.7\nlangchain_token_splitting.py (relevant new code)\nUpdates to 50 tokens \nand overlap of 10 \ntokens\nSelects just the \ndocuments that \ncontain rhymes\nUses the database\u2019s \nsimilarity search\nBreaks into irregular \nsize chunks because of \nthe whitespace\n",
      "content_length": 2563,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "196\nCHAPTER 8\nUnderstanding agent memory and knowledge\nquery: Why are the girls crying? Listing 8.8 shows the results of executing that\nquery. If you run this example yourself and scroll down over the output, you\u2019ll see\nGeorgy Porgy appear in either the second or third returned document.\nEnter a search query (or 'exit' to stop): Who made the girls cry?\nHow many top matches do you want to see? 3\nTop Matched Documents:\nDocument 1: WILLY, WILLY\n        Willy, Willy Wilkin\u2026\nThis exercise shows how various retrieval methods can be employed to return docu-\nments semantically. With this base established, we can see how RAG can be applied to\nknowledge and memory systems. The following section will discuss RAG as it applies\nto knowledge of agents and agentic systems.\n8.5\nApplying RAG to building agent knowledge\nKnowledge in agents encompasses employing RAG to search semantically across\nunstructured documents. These documents could be anything from PDFs to Micro-\nsoft Word documents and all text, including code. Agentic knowledge also includes\nusing unstructured documents for Q&A, reference lookup, information augmenta-\ntion, and other future patterns.\n Nexus, the agent platform developed in tandem with this book and introduced in\nthe previous chapter, employs complete knowledge and memory systems for agents.\nIn this section, we\u2019ll uncover how the knowledge system works.\n To install Nexus for just this chapter, see listing 8.9. Open a terminal within the\nchapter_08 folder, and execute the commands in the listing to download, install,\nand run Nexus in normal or development mode. If you want to refer to the code,\nyou should install the project in development and configure the debugger to run\nthe Streamlit app from VS Code. Refer to chapter 7 if you need a refresher on any of\nthese steps.\n# to install and run\npip install git+https://github.com/cxbxmxcx/Nexus.git\nnexus run\n# install in development mode\ngit clone https://github.com/cxbxmxcx/Nexus.git\n# Install the cloned repository in editable mode\npip install -e Nexus\nListing 8.8\nQuery: Who made the girls cry?\nListing 8.9\nInstalling Nexus \n",
      "content_length": 2113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "197\n8.5\nApplying RAG to building agent knowledge\nRegardless of which method you decide to run the app in after you log in, navigate to\nthe Knowledge Store Manager page, as shown in figure 8.9. Create a new Knowledge\nStore, and then upload the sample_documents/back_to_the_future.txt movie script. \nThe script is a large document, and it may take a while to load, chunk, and embed the\nparts into the Chroma DB vector database. Wait for the indexing to complete, and\nthen you can inspect the embeddings and run a query, as shown in figure 8.10.\n Now, we can connect the knowledge store to a supported agent and ask questions.\nUse the top-left selector to choose the chat page within the Nexus interface. Then,\nselect an agent and the time_travel knowledge store, as shown in figure 8.11. You\nwill also need to select an agent engine that supports knowledge. Each of the multiple\nagent engines requires the proper configuration to be accessible.\n Currently, as of this chapter, Nexus supports access to only a single knowledge\nstore at a time. In a future version, agents may be able to select multiple knowledge\nstores at a time. This may include more advanced options, from semantic knowl-\nedge to employing other forms of RAG.\n \nSelect the\nknowledge store.\nCreate a new\nknowledge store.\nDrag and drop or browse and select the\nsample_documents/back_to_the_future.txt\nmovie script here.\nFigure 8.9\nAdding a new knowledge store and populating it with a document\n",
      "content_length": 1459,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "198\nCHAPTER 8\nUnderstanding agent memory and knowledge\nPlot of embeddings shown in 3D\nSelect to view all the\nembeddings in the\nknowledge store.\nEnter text to query\nand see the results of\nthe top \ufb01ve chunks\ndisplayed.\nSelect to query the\ndocument embeddings\nin the knowledge store.\nFigure 8.10\nThe embeddings and document query views\nThe agent will reply given\nthe chosen persona.\nEnter a question about\nthe script you would\nlike to ask.\nBe sure to select the\nknowledge store.\nSelect an agent engine\nthat supports knowledge.\nFigure 8.11\nEnabling the knowledge store for agent use\n",
      "content_length": 579,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "199\n8.5\nApplying RAG to building agent knowledge\nYou can also configure the RAG settings within the Configuration tab of the Knowl-\nedge Store Manager page, as shown in figure 8.12. As of now, you can select from the\ntype of splitter (Chunking Option field) to chunk the document, along with the\nChunk Size field and Overlap field. \nThe loading, splitting, chunking, and embedding options provided are the only basic\noptions supported by LangChain for now. In future versions of Nexus, more options\nand patterns will be offered. The code to support other options can be added directly\nto Nexus.\n We won\u2019t cover the code that performs the RAG as it\u2019s very similar to what we\nalready covered. Feel free to review the Nexus code, particularly the KnowledgeManager\nclass in the knowledge_manager.py file. \nRepresents the minimum\nsize in characters or tokens\nto chunk the data\nSelect the\nCon\ufb01guration tab.\nAllows for some overlap\nof text from one chunk\nto the next\nRepresents the type of\ndocument splitter used to extract\nchunks from the document\nFigure 8.12\nManaging the knowledge store splitting and chunking options\n",
      "content_length": 1114,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "200\nCHAPTER 8\nUnderstanding agent memory and knowledge\n While the retrieval patterns for knowledge and memory are quite similar for aug-\nmentation, the two patterns differ when it comes to populating the stores. In the next\nsection, we\u2019ll explore what makes memory in agents unique.\n8.6\nImplementing memory in agentic systems\nMemory in agents and AI applications is often described in the same terms as cogni-\ntive memory functions. Cognitive memory describes the type of memory we use to\nremember what we did 30 seconds ago or how tall we were 30 years ago. Computer\nmemory is also an essential element of agent memory, but one we won\u2019t consider in\nthis section.\n Figure 8.13 shows how memory is broken down into sensory, short-term, and long-\nterm memory. This memory can be applied to AI agents, and this list describes how\neach form of memory maps to agent functions:\n\uf0a1Sensory memory in AI\u2014Functions such as RAG but with images/audio/haptic\ndata forms. Briefly holds input data (e.g., text and images) for immediate pro-\ncessing but not long-term storage. \n\uf0a1Short-term/working memory in AI\u2014Acts as an active memory buffer of conversa-\ntion history. We\u2019re holding a limited amount of recent input and context for\nimmediate analysis and response generation. Within Nexus, short- and long-\nterm conversational memory is also held in the context of the thread.\n\uf0a1Long-term memory in AI\u2014Longer-term memory storage relevant to the agent\u2019s or\nuser\u2019s life. Semantic memory provides a robust capacity to store and retrieve rel-\nevant global or local facts and concepts. \nVisual memory\nSensory memory\nShort-term memory (contextual memory)\nLong-term memory\nMemory\nIconic memory (visual)\nEchoic memory (auditory)\nHaptic memory (touch)\nExplicit/declarative memory\n(conscious)\nImplicit/procedural memory (unconscious skills)\nEpisodic memory\n(life events)\nSemantic memory\n(facts, concepts)\nConversational\nmemory and RAG\nThis is an area of\nmemory of most\ninterest to agents.\nImplicit memory and skills can also be\nconveyed as actions and tools for agents.\nFigure 8.13\nHow memory is broken down into various forms\n",
      "content_length": 2100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "201\n8.6\nImplementing memory in agentic systems\nWhile memory uses the exact same retrieval and augmentation mechanisms as knowl-\nedge, it typically differs significantly when updating or appending memories. Figure 8.14\nhighlights the process of capturing and using memories to augment prompts. Because\nmemories are often different from the size of complete documents, we can avoid\nusing any splitting or chunking mechanisms.\nNexus provides a mechanism like the knowledge store, allowing users to create mem-\nory stores that can be configured for various uses and applications. It also supports\nsome of the more advanced memory forms highlighted in figure 8.13. The following\nsection will examine how basic memory stores work in Nexus.\nAdding memories\nBasic memory augmentation\nEmbedding\nVector DB\nStored\nembeddings\nand text\nNew memories\ncan come from a\nconversation or\ncan be populated\nas a set of beginning\nfacts, preferences,\nand so on.\nVector DB\nConverted to embedding\nand then used to query database\nAugmented prompt\nuser:\nWhat movie should\nI watch?\nRemembered facts:\nUser likes time\ntravel movies\nLLM\nResponse sent\nback to user\nAssistant:\nYou should watch\nthis movie on\ntime travel.\nUser likes time\ntravel movies\nUser likes sci-\ufb01\nmovies\nConversations are then fed back into the memory function\nto create new memories.\nUser likes time\ntravel movies\nUser likes sci-\ufb01\nmovies\nLLM\nUser:\n[I like time\ntravel movies]\nI like time travel\nmovies\nMemory function,\nextracts relevant\nmemories\nEmbedding\nUser:\nWhat movie\nshould I watch?\nWhat movie should\nI watch?\nFigure 8.14\nBasic memory retrieval and augmentation workflow\n",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "202\nCHAPTER 8\nUnderstanding agent memory and knowledge\n8.6.1\nConsuming memory stores in Nexus\nMemory stores operate and are constructed like knowledge stores in Nexus. They\nboth heavily rely on the retrieval pattern. What differs is the extra steps memory sys-\ntems take to build new memories.\n Go ahead and start Nexus, and refer to listing 8.9 if you need to install it. After\nlogging in, select the Memory page, and create a new memory store, as shown in fig-\nure 8.15. Select an agent engine, and then add a few personal facts and preferences\nabout yourself.\nThe reason we need an agent (LLM) was shown in figure 8.14 earlier. When informa-\ntion is fed into a memory store, it\u2019s generally processed through an LLM using a memory\nCreate a new memory\nstore called my_memory.\nSelect the memory\nyou want to inspect.\nYou will need to select an agent\nengine to process the memory.\nSelect the\nMemory page.\nAdd a few facts or\npreferences about yourself.\nFigure 8.15\nAdding memories to a newly created memory store\n",
      "content_length": 1010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "203\n8.6\nImplementing memory in agentic systems\nfunction, whose purpose is to process the statements/conversations into semantically\nrelevant information related to the type of memory.\n Listing 8.10 shows the conversational memory function used to extract information\nfrom a conversation into memories. Yes, this is just the header portion of the prompt\nsent to the LLM, instructing it how to extract information from a conversation. \nSummarize the conversation and create a set of statements that summarize \nthe conversation. Return a JSON object with the following keys: 'summary'. \nEach key should have a list of statements that are relevant to that \ncategory. Return only the JSON object and nothing else.\nAfter you generate a few relevant memories about yourself, return to the Chat area in\nNexus, enable the my_memory memory store, and see how well the agent knows you.\nFigure 8.16 shows a sample conversation using a different agent engine. \nThis is an example of a basic memory pattern that extracts facts/preferences from\nconversations and stores them in a vector database as memories. Numerous other\nimplementations of memory follow those displayed earlier in figure 8.13. We\u2019ll imple-\nment those in the next section.\nListing 8.10\nConversational memory function\nSelect the memory store.\nIf possible, select a different agent\nengine that supports memory.\nAsk the agent something relevant to\nthe facts you just added to memory.\nFigure 8.16\nConversing with a different agent on the same memory store\n",
      "content_length": 1506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "204\nCHAPTER 8\nUnderstanding agent memory and knowledge\n8.6.2\nSemantic memory and applications to semantic, episodic, \nand procedural memory\nPsychologists categorize memory into multiple forms, depending on what informa-\ntion is remembered. Semantic, episodic, and procedural memory all represent differ-\nent types of information. Episodic memories are about events, procedural memories are\nabout the process or steps, and semantic represents the meaning and could include\nfeelings or emotions. Other forms of memory (geospatial is another), aren\u2019t described\nhere but could be. \n Because these memories rely on an additional level of categorization, they also rely\non another level of semantic categorization. Some platforms, such as Semantic Kernel\n(SK), refer to this as semantic memory. This can be confusing because semantic categori-\nzation is also applied to extract episodic and procedural memories.\n Figure 8.17 shows the semantic memory categorization process, also sometimes\ncalled semantic memory. The difference between semantic memory and regular mem-\nory is the additional step of processing the input semantically and extracting relevant\nquestions that can be used to query the memory-relevant vector database.\nSemantic memory augmentation\nVector DB\nAugmented prompt\nuser:\nWhat movie should I\nwatch?\nSemantics:\nThe user has recently\nwatched these time\ntravel movies.\nResponse sent\nback to user\nAssistant:\nYou should watch\nthis movie on time\ntravel.\nConversations are then\nfed back into the memory\nfunction to create relevant\nnew memories.\nQuestions are converted\nto embedding and then\nused to query database.\nThe user has recently\nwatched these time\ntravel movies.\nEmbedding\nUser:\nWhat movie\nshould I watch?\nWhat movie should\nI watch?\nSemantic Augmentation\nfunction extracts details\nspeci\ufb01c to memory form.\nWhat type of movies\ndoes the user like?\nGive me a summary\nof movies the user\nhas watched.\nThe semantic\naugmentation\nconverts the input\ninto questions\nrelevant to the\nparticular form of\nmemory.\nLLM\nLLM\nFigure 8.17\nHow semantic memory augmentation works\n",
      "content_length": 2073,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "205\n8.6\nImplementing memory in agentic systems\nThe benefit of using semantic augmentation is the increased ability to extract more\nrelevant memories. We can see this in operation by jumping back into Nexus and cre-\nating a new semantic memory store.\n Figure 8.18 shows how to configure a new memory store using semantic memory.\nAs of yet, you can\u2019t configure the specific function prompts for memory, augmenta-\ntion, and summarization. However, it can be useful to read through each of the func-\ntion prompts to gain a sense of how they work.\nMemory function for adding new memories\nAugmentation function is called to extract\nrelevance before querying a memory store.\nSelect SEMANTIC as the type of memory.\nSelect the\nCon\ufb01guration tab.\nBe sure to create a new\nmemory store \ufb01rst.\nSummarization function is used in memory\ncompression.\nFigure 8.18\nConfiguration for changing the memory store type to semantic\n",
      "content_length": 906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "206\nCHAPTER 8\nUnderstanding agent memory and knowledge\nNow, if you go back and add facts and preferences, they will convert to the semantics\nof the relevant memory type. Figure 8.19 shows an example of memories being popu-\nlated for the same set of statements into two different forms of memory. Generally, the\nstatements entered into memory would be more specific to the form of memory.\nMemory and knowledge can significantly assist an agent with various application types.\nIndeed, a single memory/knowledge store could feed one or multiple agents, allowing\nFigure 8.19\nComparing memories for the same information given two different memory types\n",
      "content_length": 648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "207\n8.7\nUnderstanding memory and knowledge compression\nfor further specialized interpretations of both types of stores. We\u2019ll finish out the\nchapter by discussing memory/knowledge compression next.\n8.7\nUnderstanding memory and knowledge compression\nMuch like our own memory, memory stores can become cluttered with redundant\ninformation and numerous unrelated details over time. Internally, our minds deal\nwith memory clutter by compressing or summarizing memories. Our minds remem-\nber more significant details over less important ones, and memories accessed more\nfrequently.\n We can apply similar principles of memory compression to agent memory and\nother retrieval systems to extract significant details. The principle of compression is\nsimilar to semantic augmentation but adds another layer to the preclusters groups of\nrelated memories that can collectively be summarized.\n Figure 8.20 shows the process of memory/knowledge compression. Memories or\nknowledge are first clustered using an algorithm such as k-means. Then, the groups of\nmemories are passed through a compression function, which summarizes and collects\nthe items into more succinct representations. \nNexus provides for both knowledge and memory store compression using k-means\noptimal clustering. Figure 8.21 shows the compression interface for memory. Within\nthe compression interface, you\u2019ll see the items displayed in 3D and clustered. The size\n(number of items) of the clusters is shown in the left table.\n Compressing memories and even knowledge is generally recommended if the\nnumber of items in a cluster is large or unbalanced. Each use case for compression\nmay vary depending on the use and application of memories. Generally, though, if an\nMemory/Knowledge Comparison\nCompressed items are\nstored as embeddings.\nMemories and\nknowledge can be\nredundant, repetitive,\nand include duplicate\ninformation and\npotentially misaligned\ninformation.\nClustering\nThe user likes time\ntravel movies.\nThe speaker enjoys\ntime travel movies.\nlikes time travel\nstories\nItems are clustered\nusing reduced dimensional\nembeddings and -means.\nk\nCompression function\nsummarizes items\nby group and\nfunction into a new\nlist of memories and\nknowledge\nEmbedding\nVector DB\nThe user enjoys\ntime travel movies\nand has seen\nthe following \ufb01lms:\nFigure 8.20\nThe process of memory and knowledge compression\n",
      "content_length": 2350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "208\nCHAPTER 8\nUnderstanding agent memory and knowledge\ninspection of the items in a store contains repetitive or duplicate information, it\u2019s a\ngood time for compression. The following is a summary of use cases for applications\nthat would benefit from compression.\nTHE CASE FOR KNOWLEDGE COMPRESSION\nKnowledge retrieval and augmentation have also been shown to benefit significantly\nfrom compression. Results will vary by use case, but generally, the more verbose the\nsource of knowledge, the more it will benefit from compression. Documents that\nfeature literary prose, such as stories and novels, will benefit more than, say, a base\nof code. However, if the code is likewise very repetitive, compression could also be\nshown to be beneficial.\nTHE CASE FOR HOW OFTEN YOU APPLY COMPRESSION\nMemory will often benefit from the periodic compression application, whereas knowl-\nedge stores typically only help on the first load. How frequently you apply compres-\nsion will greatly depend on the memory use, frequency, and quantity.\nTHE CASE FOR APPLYING COMPRESSION MORE THAN ONCE\nMultiple passes of compression at the same time has been shown to improve retrieval\nperformance. Other patterns have also suggested using memory or knowledge at vari-\nous levels of compression. For example, a knowledge store is compressed two times,\nresulting in three different levels of knowledge. \nNumber of items are\nshown per cluster.\nCompression requires an agent\nengine; GPT-4 or higher LLMs\nare preferred for compression.\nThe optimal number of\nk-means clusters are shown.\nClick the Compress button to\nstart compression. The process of\ncompression may take several minutes\ndepending on the size of the store.\nFigure 8.21\nThe interface for compressing memories\n",
      "content_length": 1742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "209\n8.8\nExercises\nTHE CASE FOR BLENDING KNOWLEDGE AND MEMORY COMPRESSION\nIf a system is specialized to a particular source of knowledge and that system also\nemploys memories, there may be further optimization to consolidate stores. Another\napproach is to populate memory with the starting knowledge of a document directly.\nTHE CASE FOR MULTIPLE MEMORY OR KNOWLEDGE STORES\nIn more advanced systems, we\u2019ll look at agents employing multiple memory and knowl-\nedge stores relevant to their workflow. For example, an agent could employ individual\nmemory stores as part of its conversations with individual users, perhaps including the\nability to share different groups of memory with different groups of individuals. Mem-\nory and knowledge retrieval are cornerstones of agentic systems, and we can now sum-\nmarize what we covered and review some learning exercises in the next section.\n8.8\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Load and Split a Different Document (Intermediate) \nObjective\u2014Understand the effect of document splitting on retrieval efficiency by\nusing LangChain. \nTasks:\n\u2013 Select a different document (e.g., a news article, a scientific paper, or a short\nstory).\n\u2013 Use LangChain to load and split the document into chunks.\n\u2013 Analyze how the document is split into chunks and how it affects the retrieval\nprocess.\n\uf0a1Exercise 2\u2014Experiment with Semantic Search (Intermediate) \nObjective\u2014Compare the effectiveness of various vectorization techniques by per-\nforming semantic searches. \nTasks:\n\u2013 Choose a set of documents for semantic search.\n\u2013 Use a vectorization method such as Word2Vec or BERT embeddings instead\nof TF\u2013IDF.\n\u2013 Perform the semantic search, and compare the results with those obtained\nusing TF\u2013IDF to understand the differences and effectiveness.\n\uf0a1Exercise 3\u2014Implement a Custom RAG Workflow (Advanced) \nObjective\u2014Apply theoretical knowledge of RAG in a practical context using\nLangChain. \nTasks:\n\u2013 Choose a specific application (e.g., customer service inquiries or academic\nresearch queries).\n\u2013 Design and implement a custom RAG workflow using LangChain.\n\u2013 Tailor the workflow to suit the chosen application, and test its effectiveness.\n",
      "content_length": 2213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "210\nCHAPTER 8\nUnderstanding agent memory and knowledge\n\uf0a1Exercise 4\u2014Build a Knowledge Store and Experiment with Splitting Patterns\n(Intermediate) \nObjective\u2014Understand how different splitting patterns and compression affect\nknowledge retrieval. \nTasks:\n\u2013 Build a knowledge store, and populate it with a couple of documents.\n\u2013 Experiment with different forms of splitting/chunking patterns, and analyze\ntheir effect on retrieval.\n\u2013 Compress the knowledge store, and observe the effects on query performance.\n\uf0a1Exercise 5\u2014Build and Test Various Memory Stores (Advanced) \nObjective\u2014Understand the uniqueness and use cases of different memory store\ntypes. \nTasks:\n\u2013 Build various forms of memory stores (conversational, semantic, episodic,\nand procedural).\n\u2013 Interact with an agent using each type of memory store, and observe the dif-\nferences.\n\u2013 Compress the memory store, and analyze the effect on memory retrieval.\nSummary\n\uf0a1Memory in AI applications differentiates between unstructured and structured\nmemory, highlighting their use in contextualizing prompts for more relevant\ninteractions.\n\uf0a1Retrieval augmented generation (RAG) is a mechanism for enhancing prompts\nwith context from external documents, using vector embeddings and similarity\nsearch to retrieve relevant content.\n\uf0a1Semantic search with document indexing converts documents into semantic\nvectors using TF\u2013IDF and cosine similarity, enhancing the capability to perform\nsemantic searches across indexed documents.\n\uf0a1Vector databases and similarity search stores document vectors in a vector data-\nbase, facilitating efficient similarity searches and improving retrieval accuracy.\n\uf0a1Document embeddings capture semantic meanings, using models such as\nOpenAI\u2019s models to generate embeddings that preserve a document\u2019s context\nand facilitate semantic similarity searches.\n\uf0a1LangChain provides several tools for performing RAG, and it abstracts the\nretrieval process, allowing for easy implementation of RAG and memory sys-\ntems across various data sources and vector stores.\n\uf0a1Short-term and long-term memory in LangChain implements conversational\nmemory within LangChain, distinguishing between short-term buffering pat-\nterns and long-term storage solutions.\n",
      "content_length": 2215,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "211\nSummary\n\uf0a1Storing document vectors in databases for efficient similarity searches is crucial\nfor implementing scalable retrieval systems in AI applications.\n\uf0a1Agent knowledge directly relates to the general RAG pattern of performing\nquestion and answer on documents or other textual information.\n\uf0a1Agent memory is a pattern related to RAG that captures the agentic interac-\ntions with users, itself, and other systems.\n\uf0a1Nexus is a platform that implements agentic knowledge and memory systems,\nincluding setting up knowledge stores for document retrieval and memory\nstores for various forms of memory.\n\uf0a1Semantic memory augmentation (semantic memory) differentiates between vari-\nous types of memories (semantic, episodic, procedural). It implements them\nthrough semantic augmentation, enhancing agents\u2019 ability to recall and use\ninformation relevantly specific to the nature of the memories.\n\uf0a1Memory and knowledge compression are techniques for condensing informa-\ntion stored in memory and knowledge systems, improving retrieval efficiency\nand relevancy through clustering and summarization.\n",
      "content_length": 1094,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "212\nMastering agent prompts\nwith prompt flow\nIn this chapter, we delve into the Test Changes Systematically prompt engineering\nstrategy. If you recall, we covered the grand strategies of the OpenAI prompt engi-\nneering framework in chapter 2. These strategies are instrumental in helping us\nbuild better prompts and, consequently, better agent profiles and personas. Under-\nstanding this role is key to our prompt engineering journey.\n Test Changes Systematically is such a core facet of prompt engineering that Mic-\nrosoft developed a tool around this strategy called prompt flow, described later in\nthis chapter. Before getting to prompt flow, we need to understand why we need\nsystemic prompt engineering.\nThis chapter covers\n\uf0a1Understanding systematic prompt engineering \nand setting up your first prompt flow\n\uf0a1Crafting an effective profile/persona prompt\n\uf0a1Evaluating profiles: Rubrics and grounding\n\uf0a1Grounding evaluation of a large language model \nprofile\n\uf0a1Comparing prompts: Getting the perfect profile \n",
      "content_length": 1009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "213\n9.1\nWhy we need systematic prompt engineering\n9.1\nWhy we need systematic prompt engineering\nPrompt engineering, by its nature, is an iterative process. When building a prompt,\nyou\u2019ll often iterate and evaluate. To see this concept in action, consider the simple\napplication of prompt engineering to a ChatGPT question.\n You can follow along by opening your browser to ChatGPT (https://chat.openai\n.com/), entering the following (text) prompt into ChatGPT, and clicking the Send Mes-\nsage button (an example of this conversation is shown in figure 9.1, on the left side):\ncan you recommend something\nWe can see that the response from ChatGPT is asking for more information. Go ahead\nand open a new conversation with ChatGPT, and enter the following prompt, as\nshown in figure 9.1, on the right side:\nCan you please recommend a time travel movie set in the medieval period.\nThe results in figure 9.1 show a clear difference between leaving out details and being\nmore specific in your request. We just applied the tactic of politely Writing Clear Instruc-\ntions, and ChatGPT provided us with a good recommendation. But also notice how\nChatGPT itself guides the user into better prompting. The refreshed screen shown in\nfigure 9.2 shows the OpenAI prompt engineering strategies.\n We just applied simple iteration to improve our prompt. We can extend this exam-\nple by using a system prompt/message. Figure 9.3 demonstrates the use and role of\nthe system prompt in iterative communication. In chapter 2, we used the system mes-\nsage/prompt in various examples.\nNo prompt engineering\nApplying prompt engineering\nDetails are included in\nthe prompt/request.\nChatGPT guides the user to\nsupply additional details.\nFigure 9.1\nThe differences in applying prompt engineering and iterating\n",
      "content_length": 1780,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "214\nCHAPTER 9\nMastering agent prompts with prompt flow\nWrite Clear Instructions\nBe speci\ufb01c in what you ask.\nTactics include detailing queries, adopting personas, using d imiters,\nel\nspecifying steps, providing examples, and specifying output length.\nBasics\nProvide Reference Text\nHelps reduce fabrications.\nTactics involve instructing the model to use or cite reference texts.\nMemory\nUse External Tools\nEnhances model capabilities.\nTactics include embeddings-based search, code execution, and\naccess to speci\ufb01c functions.\nMemory\nSplit Complex Tasks into Simpler Subtasks\nReduces error rates.\nTactics include intent classi\ufb01cation, summarizing dialogues, and\npiecewise summarization of documents.\nPlanning\nGive Models Time to \u201cThink\u201d\nAllows more reliable reasoning.\nTactics involve working out solutions before conclusions, using inn r\ne\nmonologue, and reviewing previous answers.\nPlanning\nTest Changes Systematically\nEnsures improvements are genuine.\nTactics involve evaluating model outputs with reference to standard\nanswers.\nEvaluation\nPrompt Engineering Strategies\nFigure 9.2\nOpenAI prompt engineering strategies, broken down by agent component\nThe ser prompt de\ufb01nes\nu\nthe details of the ask.\nThe ystem prompt de\ufb01nes the role\ns\nand rules and continues across the\nconversation.\nA ser prompt may re\ufb01ne\nu\nthe ask or start a new ask.\nThe Assistant marks the\nresponse from the LLM.\nFigure 9.3\nThe messages to and from an LLM conversation and the iteration of messages\n",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "215\n9.1\nWhy we need systematic prompt engineering\nYou can also try this in ChatGPT. This time, enter the following prompt and include\nthe word system in lowercase, followed by a new line (enter a new line in the message\nwindow without sending the message by pressing Shift-Enter):\nsystem\nYou are an expert on time travel movies.\nChatGPT will respond with some pleasant comments, as shown in figure 9.4. Because\nof this, it\u2019s happy to accept its new role and asks for any follow-up questions. Now\nenter the following generic prompt as we did previously:\ncan you recommend something\nThis sets the ystem prompt, the role\ns\nthe LLM will take for the remainder of\nthe conversation.\nThe LLM responds happily with the new role.\nMake the generic ask again.\nThe LLM now provides a list of recommendations.\nFigure 9.4\nThe effect of adding a system prompt to our previous conversation\n",
      "content_length": 874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "216\nCHAPTER 9\nMastering agent prompts with prompt flow\nWe\u2019ve just seen the iteration of refining a prompt, the prompt engineering, to extract\na better response. This was accomplished over three different conversations using the\nChatGPT UI. While not the most efficient way, it works.\n However, we haven\u2019t defined the iterative flow for evaluating the prompt and\ndetermining when a prompt is effective. Figure 9.5 shows a systemic method of prompt\nengineering using a system of iteration and evaluation.\nThe system of iterating and evaluating prompts covers the broad Test Changes System-\nically strategy. Evaluating the performance and effectiveness of prompts is still new,\nbut we\u2019ll use techniques from education, such as rubrics and grounding, which we\u2019ll\nexplore in a later section of this chapter. However, as spelled out in the next section,\nwe need to understand the difference between a persona and an agent profile before\nwe do so.\n9.2\nUnderstanding agent profiles and personas\nAn agent profile is an encapsulation of component prompts or messages that describe\nan agent. It includes the agent\u2019s persona, special instructions, and other strategies that\ncan guide the user or other agent consumers.\n Figure 9.6 shows the main elements of an agent profile. These elements map to\nprompt engineering strategies described in this book. Not all agents will use all the\nelements of a full agent profile.\n At a basic level, an agent profile is a set of prompts describing the agent. It may\ninclude other external elements related to actions/tools, knowledge, memory,\nSystemic Prompt Engineering\n(Strategy - Test Changes Systemically)\nBuild prompt\nor pro\ufb01le\nWrite/update the\nprompt\nYes\nEvaluate\nprompt\nis working\nNo\nYes\nBatch\nevaluation\nof prompt\nNo\nPrompt is\nused\nPrompt or pro\ufb01le\nis grounded.\nEvaluate variations\nof the prompt/pro\ufb01le.\nEvaluate the prompt\nbasic on rubrics.\nUse prompt\nengineering to\nwrite the prompt.\nFigure 9.5\nThe systemic method of prompt engineering\n",
      "content_length": 1972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "217\n9.3\nSetting up your first prompt flow\nreasoning, evaluation, planning, and feedback. The combination of these elements\ncomprises an entire agent prompt profile.\n Prompts are the heart of an agent\u2019s function. A prompt or set of prompts drives\neach of the agent components in the profile. For actions/tools, these prompts are well\ndefined, but as we\u2019ve seen, prompts for memory and knowledge can vary significantly\nby use case.\n The definition of an AI agent profile is more than just a system prompt. Prompt\nflow can allow us to construct the prompts and code comprising the agent profile but\nalso include the ability to evaluate its effectiveness. In the next section, we\u2019ll open up\nprompt flow and start using it.\n9.3\nSetting up your first prompt flow\nPrompt flow is a tool developed by Microsoft within its Azure Machine Learning Stu-\ndio platform. The tool was later released as an open source project on GitHub,\nwhere it has attracted more attention and use. While initially intended as an applica-\ntion platform, it has since shown its strength in developing and evaluating prompts/\nprofiles.\n Because prompt flow was initially developed to run on Azure as a service, it fea-\ntures a robust core architecture. The tool supports multi-threaded batch processing,\nThe Agent Pro\ufb01le (prompts)\nActions and tools are added to\nthe prompt under the covers.\nKnowledge and memory are prompts\nused to extract and identify memories.\nAdding reasoning to prompts\nSimilar to prompt personas, the agent\npersona can give an agent specialized\nattributes, rules, and even personality.\nPlanning and feedback\nPersona\nRepresents the background and role of\nthe agent, and is often introduced in\nthe \ufb01rst system message.\nAgent T ools\nSet of tools an agent can\nuse to help accomplish a task.\nAgent Evaluation and Reasoning\nDescribes how the agent can reason\nand evaluate a task or tasks.\nAgent Memory and Knowledge\nThe backend store that helps the agent\nadd context to a given task problem.\nAgent Planning and Feedback\nDescribes how the agent can break\ndown a task into execution steps, and\nthen execute and receive feedback.\nFigure 9.6\nThe component parts of an agent profile\n",
      "content_length": 2160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "218\nCHAPTER 9\nMastering agent prompts with prompt flow\nwhich makes it ideal for evaluating prompts at scale. The following section will exam-\nine the basics of starting with prompt flow.\n9.3.1\nGetting started\nThere are a few prerequisites to undertake before working through the exercises in\nthis book. The relevant prerequisites for this section and chapter are shown in the fol-\nlowing list; make sure to complete them before attempting the exercises:\n\uf0a1Visual Studio Code (VS Code)\u2014Refer to appendix A for installation instructions,\nincluding additional extensions.\n\uf0a1Prompt flow, VS Code extension\u2014Refer to appendix A for details on installing\nextensions.\n\uf0a1Python virtual environment\u2014Refer to appendix A for details on setting up a vir-\ntual environment.\n\uf0a1Install prompt flow packages\u2014Within your virtual environment, do a quick pip\ninstall, as shown here:\npip install promptflow promptflow-tools\n\uf0a1LLM (GPT-4 or above)\u2014You\u2019ll need access to GPT-4 or above through OpenAI\nor Azure OpenAI Studio. Refer to appendix B if you need assistance accessing\nthese resources.\n\uf0a1Book\u2019s source code\u2014Clone the book\u2019s source code to a local folder; refer to\nappendix A if you need help cloning the repository.\nOpen up VS Code to the book\u2019s source code folder, chapter 3. Ensure that you have a\nvirtual environment connected and have installed the prompt flow packages and\nextension.\n First, you\u2019ll want to create a connection to your LLM resource within the prompt\nflow extension. Open the prompt flow extension within VS Code, and then click to\nopen the connections. Then, click the plus sign beside the LLM resource to create a\nnew connection, as shown in figure 9.7.\n This will open a YAML file where you\u2019ll need to populate the connection name\nand other information relevant to your connection. Follow the directions, and don\u2019t\nenter API keys into the document, as shown in figure 9.8.\n When the connection information is entered, click the Create Connection link at\nthe bottom of the document. This will open a terminal prompt below the document,\nasking you to enter your key. Depending on your terminal configuration, you may be\nunable to paste (Ctrl-V, Cmd-V). Alternatively, you can paste the key by hovering the\nmouse cursor over the terminal and right-clicking on Windows.\n We\u2019ll now test the connection by first opening the simple flow in the chap-\nter_09/promptflow/simpleflow folder. Then, open the flow.dag.yaml file in VS\nCode. This is a YAML file, but the prompt flow extension provides a visual editor\n",
      "content_length": 2505,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "219\n9.3\nSetting up your first prompt flow\n(omitted)\nVS Code\nClick to open the prompt \ufb02ow extension.\nClick the plus to create a new connection.\nFigure 9.7\nCreating a new prompt flow LLM connection\nEnter a name for the connection.\nFollow the directions: don\u2019t enter a key.\nClick after completing the above.\nFigure 9.8\nSetting the connection information for your LLM resource\n",
      "content_length": 373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "220\nCHAPTER 9\nMastering agent prompts with prompt flow\nthat is accessible by clicking the Visual Editor link at the top of the file, as shown in\nfigure 9.9.\nAfter the visual editor window is opened, you\u2019ll see a graph representing the flow and\nthe flow blocks. Double-click the recommender block, and set the connection name,\nAPI type, and model or deployment name, as shown in figure 9.10.\nClick the link to open the visual editor.\n\ufb02ow.dag.yaml\nFigure 9.9\nOpening the prompt flow visual editor\nDouble-click to\nopen LLM block.\nSelect the\nconnection name.\nAPI type\nModel or\ndeployment name\nFigure 9.10\nSetting the LLM connection details\n",
      "content_length": 636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "221\n9.3\nSetting up your first prompt flow\nA prompt flow is composed of a set of blocks starting with an Inputs block and termi-\nnating in an Outputs block. Within this simple flow, the recommender block represents\nthe LLM connection and the prompt used to converse with the model. The echo\nblock for this simple example echoes the input.\n      When creating a connection to an LLM, either in prompt flow or through an\nAPI, here are the crucial parameters we always need to consider (prompt flow docu-\nmentation: https://microsoft.github.io/promptflow):\n\uf0a1Connection\u2014This is the connection name, but it also represents the service\nyou\u2019re connecting to. Prompt flow supports multiple services, including locally\ndeployed LLMs.\n\uf0a1API\u2014This is the API type. The options are chat for a chat completion API,\nsuch as GPT-4, or completion for the older completion models, such as the\nOpenAI Davinci.\n\uf0a1Model\u2014This may be the model or deployment name, depending on your service\nconnection. For OpenAI, this will be the model\u2019s name, and for Azure OpenAI,\nit will represent the deployment name.\n\uf0a1Temperature\u2014This represents the stochasticity or variability of the model response.\nA value of 1 represents a high variability of responses, while 0 indicates a desire\nfor no variability. This is a critical parameter to understand and, as we\u2019ll see, will\nvary by use case.\n\uf0a1Stop\u2014This optional setting tells the call to the LLM to stop creating tokens. It\u2019s\nmore appropriate for older and open source models.\n\uf0a1Max tokens\u2014This limits the number of tokens used in a conversation. Knowl-\nedge of how many tokens you use is crucial to evaluating how your LLM interac-\ntions will work when scaled. Counting tokens may not be a concern if you\u2019re\nexploring and conducting research. However, in production systems, tokens\nrepresent the load on the LLM, and connections using numerous tokens may\nnot scale well.\n\uf0a1Advanced parameters\u2014You can set a few more options to tune your interaction\nwith the LLM, but we\u2019ll cover that topic in later sections of the book.\nAfter configuring the LLM block, scroll up to the Inputs block section, and review the\nprimary input shown in the user_input field, as shown in figure 9.11. Leave it as the\ndefault, and then click the Play button at the top of the window.\n All the blocks in the flow will run, and the results will be shown in the terminal win-\ndow. What you should find interesting is that the output shows recommendations for\ntime travel movies. This is because the recommender block already has a simple pro-\nfile set, and we\u2019ll see how that works in the next section.\n \n",
      "content_length": 2589,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "222\nCHAPTER 9\nMastering agent prompts with prompt flow\n9.3.2\nCreating profiles with Jinja2 templates\nThe flow responds with time travel movie recommendations because of the prompt or\nprofile it uses. By default, prompt flow uses Jinja2 templates to define the content of\nthe prompt or what we\u2019ll call a profile. For the purposes of this book and our explora-\ntion of AI agents, we\u2019ll refer to these templates as the profile of a flow or agent.\n While prompt flow doesn\u2019t explicitly refer to itself as an assistant or agent engine, it\ncertainly meets the criteria of producing a proxy and general types of agents. As you\u2019ll\nsee, prompt flow even supports deployments of flows into containers and as services.\n Open VS Code to chapter_09/promptflow/simpleflow/flow.dag.yaml, and open\nthe file in the visual editor. Then, locate the Prompt field, and click the recommended\n.jinja2 link, as shown in figure 9.12.\nInputs block\nClick Play to run all\nthe blocks in the \ufb02ow.\nUse the default text.\nFigure 9.11\nSetting the inputs and starting the flow\nClick the link to open\nthe inja2 template.\nJ\nDe\ufb01nes the start of the system and\nuser portion of the prompt/pro\ufb01le\nThe role of the pro\ufb01le\nThis is where the user_input\ntext will be placed.\nFigure 9.12\nOpening the prompt Jinja2 template and examining the parts of the profile/prompt\n",
      "content_length": 1322,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "223\n9.3\nSetting up your first prompt flow\nJinja is a templating engine, and Jinja2 is a particular version of that engine. Tem-\nplates are an excellent way of defining the layout and parts of any form of text docu-\nment. They have been extensively used to produce HTML, JSON, CSS, and other\ndocument forms. In addition, they support the ability to apply code directly into the\ntemplate. While there is no standard way to construct prompts or agent profiles, our\npreference in this book is to use templating engines such as Jinja.\n At this point, change the role within the system prompt of the recommended.jin-\nja2 template. Then, run all blocks of the flow by opening the flow in the visual editor\nand clicking the Play button. The next section will look at other ways of running\nprompt flow for testing or actual deployment.\n9.3.3\nDeploying a prompt flow API\nBecause prompt flow was also designed to be deployed as a service, it supports a cou-\nple of ways to deploy as an app or API quickly. Prompt flow can be deployed as a local\nweb application and API running from the terminal or as a Docker container.\n Return to the flow.dag.yaml file in the visual editor from VS Code. At the top of\nthe window beside the Play button are several options we\u2019ll want to investigate further.\nClick the Build button as shown in figure 9.13, and then select to deploy as a local\napp. A new YAML file will be created to configure the app. Leave the defaults, and\nclick the Start Local App link.\nThis will launch the flow as a local web application, and you\u2019ll see a browser tab open,\nas shown in figure 9.14. Enter some text into the user_input field, which is marked as\nrequired with a red asterisk. Click Enter and wait a few seconds for the reply.\n You should see a reply like the one shown earlier in figure 9.12, where the flow or\nagent replies with a list of time travel movies. This is great\u2014we\u2019ve just developed our\nfirst agent profile and the equivalent of a proxy agent. However, we need to determine\nClick the Build button. When prompted, select to build\nas either a web application or Docker container.\nClick the link to start the local app.\nFigure 9.13\nBuilding and starting the flow as a local app\n",
      "content_length": 2199,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "224\nCHAPTER 9\nMastering agent prompts with prompt flow\nhow successful or valuable the recommendations are. In the next section, we explore\nhow to evaluate prompts and profiles.\n9.4\nEvaluating profiles: Rubrics and grounding\nA key element of any prompt or agent profile is how well it performs its given task. As\nwe see in our recommendation example, prompting an agent profile to give a list of\nrecommendations is relatively easy, but knowing whether those recommendations are\nhelpful requires us to evaluate the response.\n Fortunately, prompt flow has been designed to evaluate prompts/profiles at scale.\nThe robust infrastructure allows for the evaluation of LLM interactions to be paral-\nlelized and managed as workers, allowing hundreds of profile evaluations and varia-\ntions to happen quickly.\n In the next section, we look at how prompt flow can be configured to run prompt/\nprofile variations against each other. We\u2019ll need to understand this before evaluating\nprofiles\u2019 performance.\n Prompt flow provides a mechanism to allow for multiple variations within an LLM\nprompt/profile. This tool is excellent for comparing subtle or significant differences\nChat history is only used for chat \ufb02ow. This is a standard \ufb02ow.\nKeeps a history of submissions\nEnter text to be used by the recommender.\nFigure 9.14\nRunning the flow as a local web application\n",
      "content_length": 1353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "225\n9.4\nEvaluating profiles: Rubrics and grounding\nbetween profile variations. When used in performing bulk evaluations, it can be\ninvaluable for quickly assessing the performance of a profile.\n Open the recommender_with_variations/flow.dag.yaml file in VS Code and the\nflow visual editor, as shown in figure 9.15. This time, we\u2019re making the profile more\ngeneralized and allowing for customization at the input level. This allows us to expand\nour recommendations to anything and not just time travel movies.\nThe new inputs Subject, Genre, Format, and Custom allow us to define a profile that\ncan easily be adjusted to any recommendation. This also means that we must prime\nthe inputs based on the recommendation use case. There are multiple ways to prime\nthese inputs; two examples of priming inputs are shown in figure 9.16. The figure\nshows two options, options A and B, for priming inputs. Option A represents the clas-\nsic UI; perhaps there are objects for the user to select the subject or genre, for exam-\nple. Option B places a proxy/chat agent to interact with the user better to understand\nthe desired subject, genre, and so on.\nAdded additional inputs\ninto the recommender\nThe recommenderLLM has two variations. Click\nthe adjust icons to see and edit the variations.\nInputs are passed directly to\nthe recommender LLM block.\nFigure 9.15\nThe recommender, with variations in flow and expanded inputs\n",
      "content_length": 1408,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "226\nCHAPTER 9\nMastering agent prompts with prompt flow\nEven considering the power of LLMs, you may still want or need to use option A. The\nbenefit of option A is that you can constrain and validate the inputs much like you do\nwith any modern UI. Alternatively, the downside of option A is that the constrained\nbehavior may limit and restrict future use cases.\n Option B represents a more fluid and natural way without a traditional UI. It\u2019s far\nmore powerful and extensible than option A but also introduces more unknowns for\nevaluation. However, if the proxy agent that option B uses is written well, it can assist a\nlot in gathering better information from the user.\n The option you choose will dictate how you need to evaluate your profiles. If\nyou\u2019re okay with a constrained UI, then it\u2019s likely that the inputs will also be con-\nstrained to a set of discrete values. For now, we\u2019ll assume option B for input priming,\nmeaning the input values will be defined by their name.\n To get back to VS Code and the visual view of the recommender with variants flow,\nclick the icon shown earlier in figure 9.15 to open the variants and allow editing.\nThen, click the recommend.jinja2 and recommender_variant_1.jinja2 links to open\nthe files side by side, as shown in figure 9.17.\n Figure 9.17 demonstrates the difference between the variant profiles. One profile\ninjects the inputs into the user prompt, and the other injects them into the system\nprompt. However, it\u2019s essential to understand that variations can encompass more\nthan profile design, as identified in table 9.1.\nAPI\nUser interface\nI want a movie recommendation\nLLM\nOptions\nOption A\nOption B\nUser enters the required information: subject,\ngenre, format, and custom into an interface\nThe proxy agent asks or is directed by the user\nfor recommendations. The agent asks and/or\nidenti\ufb01es the subject, genre, format, and\ncustom through conversation.\nAgent pro\ufb01le interacts with LLM by\ninjecting subject, genre, format,\nand custom into its pro\ufb01le\nFigure 9.16\nThe user interaction options for interfacing with the agent profile to prime inputs to the agent profile\n",
      "content_length": 2116,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "227\n9.4\nEvaluating profiles: Rubrics and grounding\nFor this simple example, we\u2019re just going to use prompt variations by varying the input\nto reflect in either the system or user prompt. Refer to figure 9.17 for what this looks\nTable 9.1\nLLM variation options in prompt flow\nOption\nEvaluation option examples\nNotes\nJinja2 prompt \ntemplate\nCompare system prompt variations, \nuser prompt variations, or mixed prompt \nvariations.\nSome endless combinations and tech-\nniques can be applied here. Prompt engi-\nneering is evolving all the time.\nLLM\nCompare GPT-9.5 to GPT-4.\nCompare GPT-4 to GPT-4 Turbo.\nCompare open source models to com-\nmercial models.\nThis is a useful way to evaluate and \nground model performance against a \nprompt. It can also help you tune your \nprofile to work with open source and/or \ncheaper models.\nTemperature\nCompare a 0 temperature (no random-\nness) to a 1 (maximum randomness).\nChanges to the temperature can signifi-\ncantly change the responses of some \nprompts, which may improve or degrade \nperformance.\nMax tokens\nCompare limited tokens to larger token \nsizes.\nThis can allow you to reduce and maxi-\nmize token usage.\nAdvanced \nparameters\nCompare differences to options such \nas top_p, presence_penalty, \nfrequency_penalty, and \nlogit_bias. \nWe\u2019ll cover the use of these advanced \nparameters in later chapters.\nFunction calls\nCompare alternative function calls.\nFunction calls will be addressed later in \nthis chapter.\nThe system prompt describes a generic recommender\nthat works when given speci\ufb01c inputs.\nThis pro\ufb01le injects the inputs directly\ninto a formatted user message.\nCustom input now becomes\nthe user prompt.\nThe inputs are now injected into\nthe system prompt. Both\nprompts have been simpli\ufb01ed.\nFigure 9.17\nSide-by-side comparison of variant profile templates for the recommender\n",
      "content_length": 1820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "228\nCHAPTER 9\nMastering agent prompts with prompt flow\nlike. We can then quickly run both variations by clicking the Play (Run All) button at\nthe top and choosing both, as shown in figure 9.18.\nIn the terminal window, you\u2019ll see the results of both runs. The results will likely look\nsimilar, so now we must move on to how we evaluate the difference between variations\nin the next section.\n9.5\nUnderstanding rubrics and grounding\nEvaluation of prompt/profile performance isn\u2019t something we can typically do using a\nmeasure of accuracy or correct percentage. Measuring the performance of a profile\ndepends on the use case and desired outcome. If that is as simple as determining if\nthe response was right or wrong, all the better. However, in most cases, evaluation\nwon\u2019t be that simple.\n In education, the rubric concept defines a structured set of criteria and standards a\nstudent must establish to receive a particular grade. A rubric can also be used to\ndefine a guide for the performance of a profile or prompt. We can follow these steps\nto define a rubric we can use to evaluate the performance of a profile or prompt:\n1\nIdentify the purpose and objectives. Determine the goals you want the profile or\nagent to accomplish. For example, do you want to evaluate the quality of recom-\nmendations for a given audience or overall quality for a given subject, format,\nor other input?\n2\nDefine criteria. Develop a set of criteria or dimensions that you\u2019ll use to evaluate\nthe profile. These criteria should align with your objectives and provide clear\nguidelines for assessment. Each criterion should be specific and measurable.\n#2 Select the option to run all variations.\n#1 Click Play (Run All) to start the run.\nFigure 9.18\nRunning both prompt variations at the same time\n",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "229\n9.5\nUnderstanding rubrics and grounding\nFor example, you may want to measure a recommendation by how well it fits\nwith the genre and then by subject and format.\n3\nCreate a scale. Establish a rating scale that describes the levels of performance for\neach criterion. Standard scales include numerical scales (e.g., 1\u20135) or descrip-\ntive scales (e.g., Excellent, Good, Fair, Poor).\n4\nProvide descriptions. For each level on the scale, provide clear and concise descrip-\ntions that indicate what constitutes a strong performance and what represents a\nweaker performance for each criterion.\n5\nApply the rubric. When assessing a prompt or profile, use the rubric to evaluate\nthe prompt\u2019s performance based on the established criteria. Assign scores or\nratings for each criterion, considering the descriptions for each level.\n6\nCalculate the total score. Depending on your rubric, you may calculate a total\nscore by summing up the scores for each criterion or using a weighted average\nif some criteria are more important than others.\n7\nEnsure evaluation consistency. If multiple evaluators are assessing the profile, it\u2019s\ncrucial to ensure consistency in grading.\n8\nReview, revise, and iterate. Periodically review and revise the rubric to ensure it\naligns with your assessment goals and objectives. Adjust as needed to improve\nits effectiveness.\nGrounding is a concept that can be applied to profile and prompt evaluation\u2014it\ndefines how well a response is aligned with a given rubric\u2019s specific criteria and stan-\ndards. You can also think of grounding as the baseline expectation of a prompt or pro-\nfile output.\n This list summarizes some other important considerations when using grounding\nwith profile evaluation:\n\uf0a1Grounding refers to aligning responses with the criteria, objectives, and context\ndefined by the rubric and prompt.\n\uf0a1Grounding involves assessing whether the response directly addresses the\nrubric criteria, stays on topic, and adheres to any provided instructions.\n\uf0a1Evaluators and evaluations gauge the accuracy, relevance, and adherence to\nstandards when assessing grounding.\n\uf0a1Grounding ensures that the response output is firmly rooted in the specified\ncontext, making the assessment process more objective and meaningful.\nA well-grounded response aligns with all the rubric criteria within the given context\nand objectives. Poorly grounded responses will fail or miss the entire criteria, context,\nand objectives.\n As the concepts of rubrics and grounding may still be abstract, let\u2019s look at apply-\ning them to our current recommender example. Following is a list that follows the\nprocess for defining a rubric as applied to our recommender example:\n",
      "content_length": 2671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "230\nCHAPTER 9\nMastering agent prompts with prompt flow\n1\nIdentify the purpose and objectives. The purpose of our profile/prompt is to recom-\nmend three top items given a subject, format, genre, and custom input.\n2\nDefine criteria. For simplicity, we\u2019ll evaluate how a particular recommendation\naligns with the given input criteria, subject, format, and genre. For example, if a\nprofile recommends a book when asked for a movie format, we expect a low\nscore in the format criteria.\n3\nCreate a scale. Again, keeping things simple, we\u2019ll use a scale of 1\u20135 (1 is poor,\nand 5 is excellent).\n4\nProvide descriptions. See the general descriptions for the rating scale shown in\ntable 9.2.\n5\nApply the rubric. With the rubric assigned at this stage, it\u2019s an excellent exercise\nto evaluate the rubric against recommendations manually.\n6\nCalculate the total score. For our rubric, we\u2019ll average the score for all criteria to\nprovide a total score.\n7\nEnsure evaluation consistency. The technique we\u2019ll use for evaluation will provide\nvery consistent results.\n8\nReview, revise, and iterate. We\u2019ll review, compare, and iterate on our profiles, rubrics,\nand the evaluations themselves.\nThis basic rubric can now be applied to evaluate the responses for our profile. You can\ndo this manually, or as you\u2019ll see in the next section, using a second LLM profile.\n9.6\nGrounding evaluation with an LLM profile\nThis section will employ another LLM prompt/profile for evaluation and grounding.\nThis second LLM prompt will add another block after the recommendations are gen-\nerated. It will process the generated recommendations and evaluate each one, given\nthe previous rubric.\n Before GPT-4 and other sophisticated LLMs came along, we would have never con-\nsidered using another LLM prompt to evaluate or ground a profile. You often want\nTable 9.2\nRubric ratings \nRating\nDescription\n1\nPoor alignment: this is the opposite of what is expected given the criteria.\n2\nBad alignment: this isn\u2019t a good fit for the given criteria.\n3\nMediocre alignment: it may or may not fit well with the given criteria.\n4\nGood alignment: it may not align 100% with the criteria but is a good fit otherwise.\n5\nExcellent alignment: this is a good recommendation for the given criteria.\n",
      "content_length": 2241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "231\n9.6\nGrounding evaluation with an LLM profile\nto use a different model when using LLMs to ground a profile. However, if you\u2019re\ncomparing profiles against each other, using the same LLM for evaluation and ground-\ning is appropriate.\n Open the recommender_with_LLM_evaluation\\flow.dag.yaml file in the prompt\nflow visual editor, scroll down to the evaluate_recommendation block, and click the\nevaluate_recommendation.jinja2 link to open the file, as shown in figure 9.19. Each\nsection of the rubric is identified in the figure.\nWe have a rubric that is not only well defined but also in the form of a prompt that can\nbe used to evaluate recommendations. This allows us to evaluate the effectiveness of\nrecommendations for a given profile\u2014automatically. Of course, you can also use the\nrubric to score and evaluate the recommendations manually for a better baseline.\nNOTE\nUsing LLMs to evaluate prompts and profiles provides a strong base-\nline for comparing the performance of a profile. It can also do this without\nhuman bias in a controlled and repeatable manner. This provides an excel-\nlent mechanism to establish baseline groundings for any profile or prompt.\nReturning to the recommender_with_LLM_evaluation flow visual editor, we can run\nthe flow by clicking the Play button and observing the output. You can run a single\nDe\ufb01ne the basic role of the pro\ufb01le.\nDe\ufb01ne the basic criteria for the rubric.\nDe\ufb01ne the rubric scale and\na description for each item\non the scale.\nReiterate the criteria and scale, and\nshow an example of expected output.\nFigure 9.19\nThe evaluation prompt, with each of the parts of the rubric outlined\n",
      "content_length": 1631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "232\nCHAPTER 9\nMastering agent prompts with prompt flow\nrecommendation or run both variations when prompted. The output of a single eval-\nuation using the default inputs is shown in the following listing.\n{\n    \"recommendations\": \"Title: The Butterfly Effect\nSubject: 5\nFormat: 5\nGenre: 4\nTitle: Primer\nSubject: 5\nFormat: 5\nGenre: 4\nTitle: Time Bandits\nSubject: 5\nFormat: 5\nGenre: 5\"\n}\nWe now have a rubric for grounding our recommender, and the evaluation is run\nautomatically using a second LLM prompt. In the next section, we look at how to per-\nform multiple evaluations simultaneously and then at a total score for everything.\n9.7\nComparing profiles: Getting the perfect profile\nWith our understanding of rubrics and grounding, we can now move on to evaluating\nand iterating the perfect profile. Before we do that, though, we need to clean up the\noutput from the LLM evaluation block. This will require us to parse the recommenda-\ntions into something more Pythonic, which we\u2019ll tackle in the next section.\n9.7.1\nParsing the LLM evaluation output\nAs the raw output from the evaluation block is text, we now want to parse that into\nsomething more usable. Of course, writing parsing functions is simple, but there are\nbetter ways to cast responses automagically. We covered better methods for returning\nresponses in chapter 5, on agent actions.\n Open chapter_09\\prompt_flow\\recommender_with_parsing\\flow.dag.yaml in\nVS Code, and look at the flow in the visual editor. Locate the parsing_results block,\nand click the link to open the Python file in the editor, as shown in figure 9.20.\n The code for the parsing_results.py file is shown in listing 9.2.\n \n \n \n \nListing 9.1\nLLM rubric evaluation output\n",
      "content_length": 1703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "233\n9.7\nComparing profiles: Getting the perfect profile\nfrom promptflow import tool\n@tool     \ndef parse(input: str) -> str:\n    # Splitting the recommendations into individual movie blocks\n    rblocks = input.strip().split(\"\\n\\n\")     \n    # Function to parse individual recommendation block into dictionary\n    def parse_block(block):\n        lines = block.split('\\n')\n        rdict = {}\n        for line in lines:\n            kvs = line.split(': ')\n            key, value = kvs[0], kvs[1]\n            rdict[key.lower()] = value    \n        return rdict\n    parsed = [parse_block(block) for block in rblocks]   \n    return parsed\nWe\u2019re converting the recommendations output from listing 9.1, which is just a string,\ninto a dictionary. So this code will convert this string into the JSON block shown next:\nListing 9.2\nparsing_results.py\nClick the link to open the Python \ufb01le.\nparsing_results.py parses output\nafter LLM evaluation.\nFigure 9.20\nOpening the parsing_results.py file in VS Code\nSpecial decorator to \ndenote the tool block\nSplits the input and\ndouble new lines\nCreates a dictionary entry \nand sets the value\nLoops through each \nblock and parses \ninto key/value \ndictionary\n",
      "content_length": 1185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "234\nCHAPTER 9\nMastering agent prompts with prompt flow\n Before parsing:\n\"Title: The Butterfly Effect\nSubject: 5\nFormat: 5\nGenre: 4\nTitle: Primer\nSubject: 5\nFormat: 5\nGenre: 4\nTitle: Time Bandits\nSubject: 5\nFormat: 5\nGenre: 5\"\nAfter parsing:\n       {\n            \"title\": \" The Butterfly Effect\n            \"subject\": \"5\",\n            \"format\": \"5\",\n            \"genre\": \"4\"\n        },\n        {\n            \"title\": \" Primer\",\n            \"subject\": \"5\",\n            \"format\": \"5\",\n            \"genre\": \"4\"\n        },\n        {\n            \"title\": \" Time Bandits\",\n            \"subject\": \"5\",\n            \"format\": \"5\",\n            \"genre\": \"5\"\n        }\nThe output of this parsing_results block now gets passed to the output and is\nwrapped in a list of recommendations. We can see what all this looks like by running\nthe flow.\n Open flow.dag.yaml for the flow in the visual editor, and click the Play (Run All)\nbutton. Be sure to select to use both recommender variants. You\u2019ll see both variations\nrun and output to the terminal.\n At this point, we have a full working recommendation and LLM evaluation flow\nthat outputs a score for each criterion on each output. However, to do comprehensive\nevaluations of a particular profile, we want to generate multiple recommendations\nwith various criteria. We\u2019ll see how to do batch processing of flows in the next section.\n",
      "content_length": 1367,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "235\n9.7\nComparing profiles: Getting the perfect profile\n9.7.2\nRunning batch processing in prompt flow\nIn our generic recommendation profile, we want to evaluate how various input crite-\nria can affect the generated recommendations. Fortunately, prompt flow can batch-\nprocess any variations we want to test. The limit is only the time and money we want\nto spend.\n To perform batch processing, we must first create a JSON Lines (JSONL) or JSON\nlist document of our input criteria. If you recall, our input criteria looked like the fol-\nlowing in JSON format:\n{\n    \"subject\": \"time travel\",\n    \"format\": \"books\",\n    \"genre\": \"fantasy\",\n    \"custom\": \"don't include any R rated content\"\n}\nWe want to create a list of JSON objects like that just shown, preferably in a random\nmanner. Of course, the simple way to do this is to prompt ChatGPT to create a JSONL\ndocument using the following prompt:\nI am developing a recommendation agent. The agent will recommend anything given\nthe following criteria:\n1. subject - examples: time travel, cooking, vacation\n2. format - examples: books, movies, games\n3. genre: documentary, action, romance\n4. custom: don't include any R rated content\nCan you please generate a random list of these criteria and output it in the format of\na JSON Lines file, JSONL. Please include 10 items in the list.\nTry this out by going to ChatGPT and entering the preceding prompt. A previously\ngenerated file can be found in the flow folder, called \\bulk_recommend.jsonl. The\ncontents of this file have been shown here for reference:\n{\n  \"subject\": \"time travel\",\n  \"format\": \"books\",\n  \"genre\": \"fantasy\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"space exploration\",\n  \"format\": \"podcasts\",\n  \"genre\": \"sci-fi\",\n  \"custom\": \"include family-friendly content only\"\n}\n",
      "content_length": 1809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "236\nCHAPTER 9\nMastering agent prompts with prompt flow\n{\n  \"subject\": \"mystery\",\n  \"format\": \"podcasts\",\n  \"genre\": \"fantasy\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"space exploration\",\n  \"format\": \"podcasts\",\n  \"genre\": \"action\",\n  \"custom\": \"include family-friendly content only\"\n}\n{\n  \"subject\": \"vacation\",\n  \"format\": \"books\",\n  \"genre\": \"thriller\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"mystery\",\n  \"format\": \"books\",\n  \"genre\": \"sci-fi\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"mystery\",\n  \"format\": \"books\",\n  \"genre\": \"romance\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"vacation\",\n  \"format\": \"movies\",\n  \"genre\": \"fantasy\",\n  \"custom\": \"don't include any R rated content\"\n}\n{\n  \"subject\": \"cooking\",\n  \"format\": \"TV shows\",\n  \"genre\": \"thriller\",\n  \"custom\": \"include family-friendly content only\"\n}\n{\n  \"subject\": \"mystery\",\n  \"format\": \"movies\",\n  \"genre\": \"romance\",\n  \"custom\": \"include family-friendly content only\"\n}\nWith this bulk file, we can run both variants using the various input criteria in the bulk\nJSONL file. Open the flow.dag.yaml file in the visual editor, click Batch (the beaker\nicon) to start the bulk-data loading process, and select the file as shown in figure 9.21.\nFor some operating systems, this may appear as Local Data File.\n",
      "content_length": 1371,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "237\n9.7\nComparing profiles: Getting the perfect profile\nAfter the bulk file is selected, a new YAML document will open with a Run link added\nat the bottom of the file, as shown in figure 9.22. Click the link to do the batch run\nof inputs.\nAt this point, a few things will happen. The flow visual editor will appear, and beside\nthat a log file will open, showing the progress of the run. In the terminal window,\nyou\u2019ll see the various worker processes spawning and running.\n Be patient. The batch run, even for 10 items, may take a few minutes or seconds,\ndepending on various factors such as hardware, previous calls, and so on. Wait for the\nrun to complete, and you\u2019ll see a summary of results in the terminal.\n You can also view the run results by opening the prompt flow extension and select-\ning the last run, as shown in figure 9.23. Then, you dig into each run by clicking the\nSelect a local \ufb01le.\nClick to open the select input source.\nFigure 9.21\nLoading the bulk JSONL file to run the flow on multiple input variations\nClick to run the batch.\nFigure 9.22\nRunning the batch run of inputs\n",
      "content_length": 1095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "238\nCHAPTER 9\nMastering agent prompts with prompt flow\ntable cells. A lot of information is exposed in this dialog, which can help you trouble-\nshoot flows and profiles.\nA lot of information is captured during a batch run, and you can explore much of it\nthrough the visualizer. More information can be found by clicking the output folder\nlink from the terminal window. This will open another session of VS Code with the\noutput folder allowing you to review the run logs and other details.\n Now that we\u2019ve completed the batch run for each variant, we can apply grounding\nand evaluate the results of both prompts. The next section will use a new flow to per-\nform the profile/prompt evaluation.\n9.7.3\nCreating an evaluation flow for grounding\nOpen chapter_3\\prompt_flow\\evaluate_groundings\\flow.dag.yaml in the visual edi-\ntor, as shown in figure 9.24. There are no LLM blocks in the evaluation flow\u2014just\nPython code blocks that will run the scoring and then aggregate the scores.\n We can now look at the code for the scoring and aggregate blocks, starting\nwith the scoring code in listing 9.3. This scoring code averages the score for each\ncriterion into an average score. The output of the function is a list of processed\nrecommendations.\n#2 Right-click a run, and\nselect to open the visualization.\nRecommendations for\na single set of inputs\nLook at the API calls and\ntiming for the various calls.\n#3 Click the recommendations\ncell to view the results.\n#1 Open the prompt\n\ufb02ow extension.\nFigure 9.23\nAn opening run visualization and an examination of a batch run\n",
      "content_length": 1562,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "239\n9.7\nComparing profiles: Getting the perfect profile\n@tool\ndef line_process(recommendations: str):    \n    inputs = recommendations\n    output = []\n    for data_dict in inputs:                     \n        total_score = 0\n        score_count = 0\n        for key, value in data_dict.items():     \n                if key != \"title\":    \n                    try:\n                        total_score += float(value)\n                        score_count += 1\n                        data_dict[key] = float(value)    \n                    except:\n                        pass\n        avg_score = total_score / score_count if score_count > 0 else 0\n        data_dict[\"avg_score\"] = round(avg_score, 2)   \n        output.append(data_dict)\n    return output\nListing 9.3\nline_process.py\nline_process block processes and\nscores each recommendation.\naggregate block aggregates\nthe results of scoring.\nFigure 9.24\nLooking at the evaluate_groundings flow used to ground recommendation runs\nA set of three recommendations \nis input into the function.\nLoops over each \nrecommendation \nand criterion\nTitle isn\u2019t a criterion, \nso ignore it.\nTotals the score \nfor all criteria \nand sets the float \nvalue to key\nAdds the average \nscore as a grounding \nscore of the \nrecommendation\n",
      "content_length": 1262,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "240\nCHAPTER 9\nMastering agent prompts with prompt flow\nFrom the grounded recommendations, we can move on to aggregating the scores\nwith the aggregate block\u2014the code for the aggregate block is shown in the follow-\ning listing.\n@tool\ndef aggregate(processed_results: List[str]):\n    items = [item for sublist in processed_results \n              \u27a5 for item in sublist]    \n    aggregated = {}\n    for item in items:\n        for key, value in item.items():\n            if key == 'title':\n                continue\n            if isinstance(value, (float, int)):     \n                if key in aggregated:\n                    aggregated[key] += value\n                else:\n                    aggregated[key] = value\n    for key, value in aggregated.items():     \n        value = value / len(items)\n        log_metric(key=key, value=value)    \n        aggregated[key] = value\n    return aggregated\nThe result of the aggregations will be a summary score for each criterion and the aver-\nage score. Since the evaluation/grounding flow is separate, it can be run over any rec-\nommendation run we perform. This will allow us to use the batch run results for any\nvariation to compare results.\n We can run the grounding flow by opening flow.dag.yaml in the visual editor and\nclicking Batch (beaker icon). Then, when prompted, we select an existing run and\nthen select the run we want to evaluate, as shown in figure 9.25. This will open a YAML\nfile with the Run link at the bottom, as we\u2019ve seen before. Click the Run link to run\nthe evaluation.\n After the run is completed, you\u2019ll see a summary of the results in the terminal win-\ndow. You can click the output link to open the folder in VS Code and analyze the\nresults, but there is a better way to compare them.\n Open the prompt flow extension, focus on the Batch Run History window, and\nscroll down to the Run against Run section, as shown in figure 9.26. Select the runs\nyou want to compare\u2014likely the ones near the top\u2014so that the checkmark appears.\nThen, right-click the run, and select the Visualize Runs option. The Batch Run Visual-\nization window opens, and you\u2019ll see the metrics for each of the runs at the top.\nListing 9.4\naggregate.py\nThe input is a list \nof lists; flatten to \na list of items.\nChecks to see if the value \nis numeric and accumulates \nscores for each criterion key\nLoops over aggregated \ncriterion scores\nLogs the criterion \nas a metric\n",
      "content_length": 2406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "241\n9.7\nComparing profiles: Getting the perfect profile\nWe can now see a significant difference between profile/prompt variation 0, the\nuser prompt, and variation 1, the system prompt. Refer to figure 9.15 if you need a\nrefresher on what the prompts/profiles look like. At this point, it should be evi-\ndent that injecting the input parameters into the system prompt provides better\nrecommendations.\n You can now go back and try other profiles or other variant options to see what\neffect this has on your recommendations. The possibilities are virtually endless, but\nhopefully you can see what an excellent tool prompt flow will be for building agent\nprofiles and prompts.\n \n \nSelect Existing Run that is not a local JSON Lines \ufb01le.\nSelect the run you want to evaluate, noting the name.\nFigure 9.25\nLoading a previous run to be grounded and evaluated\n#1 Scroll down to the batch\nRun against the run section.\n#2 Select the runs you want to evaluate,\nand a checkmark appears. Then,\nright-click and select Visualize Runs.\n#3 Compare the aggregated criteria\nresults against each of the runs.\nFigure 9.26\nVisualizing the metrics for multiple runs and comparing them\n",
      "content_length": 1161,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "242\nCHAPTER 9\nMastering agent prompts with prompt flow\n9.7.4\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Create a New Prompt Variant for Recommender Flow (Intermediate)\nObjective\u2014Improve the recommendation results by creating and testing a new\nprompt variant in prompt flow.\nTasks:\n\u2013 Create a new prompt variant for the recommender flow in prompt flow.\n\u2013 Run the flow in batch mode.\n\u2013 Evaluate the results to determine if they are better or worse compared to the\noriginal prompt.\n\uf0a1Exercise 2\u2014Add a Custom Field to the Rubric and Evaluate (Intermediate)\nObjective\u2014Enhance the evaluation criteria by incorporating a custom field into\nthe rubric and updating the evaluation flow.\nTasks:\n\u2013 Add the custom field as a new criterion to the rubric.\n\u2013 Update the evaluation flow to score the new criterion.\n\u2013 Evaluate the results, and analyze the effect of the new criterion on the\nevaluation.\n\uf0a1Exercise 3\u2014Develop a New Use Case and Evaluation Rubric (Advanced)\nObjective\u2014Expand the application of prompt engineering by developing a new\nuse case and creating an evaluation rubric.\nTasks:\n\u2013 Develop a new use case aside from the recommendation.\n\u2013 Build the prompt for the new use case.\n\u2013 Create a rubric for evaluating the new prompt.\n\u2013 Update or alter the evaluation flow to aggregate and compare the results of\nthe new use case with existing ones.\n\uf0a1Exercise 4\u2014Evaluate Other LLMs Using LM Studio (Intermediate)\nObjective\u2014Assess the performance of different open source LLMs by hosting a\nlocal server with LM Studio.\nTasks:\n\u2013 Use LM Studio to host a local server for evaluating LLMs.\n\u2013 Evaluate other open source LLMs.\n\u2013 Consult chapter 2 if assistance is needed for setting up the server and per-\nforming the evaluations.\n\uf0a1Exercise 5\u2014Build and Evaluate Prompts Using Prompt Flow (Intermediate)\nObjective\u2014Apply prompt engineering strategies to build and evaluate new prompts\nor profiles using prompt flow.\n",
      "content_length": 1946,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "243\nSummary\nTasks:\n\u2013 Build new prompts or profiles for evaluation using prompt flow.\n\u2013 Apply the Write Clear Instructions prompt engineering strategy from chap-\nter 2.\n\u2013 Evaluate the prompts and profiles using prompt flow.\n\u2013 Refer to chapter 2 for tactics and implementation details if a refresher is\nneeded.\nSummary\n\uf0a1An agent profile consists of several other component prompts that can drive\nfunctions such as actions/tools, knowledge, memory, evaluation, reasoning, feed-\nback, and planning.\n\uf0a1Prompt flow can be used to evaluate an agent\u2019s component prompts.\n\uf0a1Systemic prompt engineering is an iterative process evaluating a prompt and\nagent profile.\n\uf0a1The Test Changes Systematically strategy describes iterating and evaluating\nprompts, and system prompt engineering implements this strategy.\n\uf0a1Agent profiles and prompt engineering have many similarities. We define an\nagent profile as the combination of prompt engineering elements that guide\nand help an agent through its task.\n\uf0a1Prompt flow is an open source tool from Microsoft that provides several fea-\ntures for developing and evaluating profiles and prompts.\n\uf0a1An LLM connection in prompt flow supports additional parameters, including\ntemperature, stop token, max tokens, and other advanced parameters.\n\uf0a1LLM blocks support prompt and profile variants, which allow for evaluating\nchanges to the prompt/profile or other connection parameters.\n\uf0a1A rubric applied to an LLM prompt is the criteria and standards a prompt/profile\nmust fulfill to be grounded. Grounding is the scoring and evaluation of a rubric.\n\uf0a1Prompt flow supports running multiple variations as single runs or batch runs.\n\uf0a1In prompt flow, an evaluation flow is run after a generative flow to score and\naggregate the results. The Visualize Runs option can compare the aggregated\ncriteria from scoring the rubric across multiple runs.\n",
      "content_length": 1856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "244\nAgent reasoning\nand evaluation\nNow that we\u2019ve examined the patterns of memory and retrieval that define the\nsemantic memory component in agents, we can take a look at the last and most\ninstrumental component in agents: planning. Planning encompasses many facets,\nfrom reasoning, understanding, and evaluation to feedback.\n To explore how LLMs can be prompted to reason, understand, and plan, we\u2019ll\ndemonstrate how to engage reasoning through prompt engineering and then\nexpand that to planning. The planning solution provided by the Semantic Kernel\n(SK) encompasses multiple planning forms. We\u2019ll finish the chapter by incorporat-\ning adaptive feedback into a new planner.\n Figure 10.1 demonstrates the high-level prompt engineering strategies we\u2019ll\ncover in this chapter and how they relate to the various techniques we\u2019ll cover. Each\nThis chapter covers\n\uf0a1Using various prompt engineering techniques \nto extend large language model functions\n\uf0a1Engaging large language models with prompt \nengineering techniques that engage reasoning\n\uf0a1Employing an evaluation prompt to narrow \nand identify the solution to an unknown \nproblem \n",
      "content_length": 1130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "245\n10.1\nUnderstanding direct solution prompting\nof the methods showcased in the figure will be explored in this chapter, from the\nbasics of solution/direct prompting, shown in the top-left corner, to self-consistency\nand tree of thought (ToT) prompting, in the bottom right.\n10.1\nUnderstanding direct solution prompting\nDirect solution prompting is generally the first form of prompt engineering that users\nemploy when asking LLMs questions or solving a particular problem. Given any LLM\nuse, these techniques may seem apparent, but they are worth reviewing to establish\nthe foundation of thought and planning. In the next section, we\u2019ll start from the\nbeginning, asking questions and expecting answers.\nSplit Complex Tasks into Simpler Subtasks\nReduces error rates.\nTactics include intent classi cation, summarizing\n\ufb01\ndialogues, and piecewise summarization of documents.\nPlanning\nGive Models\nTime to\n\u201cThink\u201d\nAllows more\nreliable\nreasoning.\nTactics involve\nworking out\nsolutions\nbefore\nconclusions,\nusing inner\nmonologue,\nand reviewing\nprevious\nanswers.\nPlanning\nSolutions\u2014prompts in this\ngroup are direct and may include\nexamples of completed tasks.\nPrompt\nchaining\nQuestion and\n-\n-\nanswer prompting\nZero-shot\nprompting\nOne-shot\nprompting\nSelf-consistency\nprompting\nTree of thought\nprompting\nAutomatic easoning\nr\nwith tools (ART)\nPlanners\nAdaptive constructive\nfeedback\nChain of thought\nprompting\nReasoning\nprompts demonstrate\n\u2014\nreasoning by encouraging a thought\nprocess and solving a sequence of\nthoughts.\nEvaluation\u2014multiple\nprompts are generated\nand evaluated based on\nthe goal.\nTooling\nthe ability to use\n\u2014\ntools, actions, planners, and\nspecialized prompts to solve\ntasks and high-level goals.\nMulti-agent\nsystems\nFeedbace\nxtends\nk\u2014\nplanning but also\nengages with perceived\nor genuine feedback to\ncontinually adapt and\nimprove plans.\nThink of these as axes:\nthe y-axis represents\nthought, and the x-axis\nrepresents planning.\nPrompt Engineering Strategies\nEvaluation and Feedback\nAll of these\nconcepts will\nbe covered in\nthis chapter.\n.\nThought\nFigure 10.1\nHow the two planning prompt engineering strategies align with the various techniques\n",
      "content_length": 2148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "246\nCHAPTER 10\nAgent reasoning and evaluation\n10.1.1 Question-and-answer prompting\nFor the exercises in this chapter, we\u2019ll employ prompt flow to build and evaluate the var-\nious techniques. (We already extensively covered this tool in chapter 9, so refer to that\nchapter if you need a review.) Prompt flow is an excellent tool for understanding how\nthese techniques work and exploring the flow of the planning and reasoning process.\n Open Visual Studio Code (VS Code) to the chapter 10 source folder. Create a new\nvirtual environment for the folder, and install the requirements.txt file. If you need\nhelp setting up a chapter\u2019s Python environment, refer to appendix B.\n We\u2019ll look at the first flow in the prompt_flow/question-answering-prompting\nfolder. Open the flow.dag.yaml file in the visual editor, as shown in figure 10.2. On\nthe right side, you\u2019ll see the flow of components. At the top is the question_answer\nLLM prompt, followed by two Embedding components and a final LLM prompt to do\nthe evaluation called evaluate.\nThe breakdown in listing 10.1 shows the structure and components of the flow in\nmore detail using a sort of YAML-shortened pseudocode. You can also see the input\nand outputs to the various components and a sample output from running the flow.\n   Inputs:\n        context\u00a0 : the content to ask the question about\nListing 10.1\nquestion-answer-prompting flow\nPrompt \ufb02ow folder to open\nOpen the \ufb02ow.dag.yaml\n\ufb01le in the visual editor.\nSelect the question_answer\nLLM component.\nShows the visual\n\ufb02ow of the DAG\nmaking up the \ufb02ow\nEmbeds the expected and\npredicted answer so their\nsimilarity can be evaluated\nFigure 10.2\nThe flow.dag.yaml file, open in the visual editor, highlighting the various components of the flow\n",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "247\n10.1\nUnderstanding direct solution prompting\n        question\u00a0: question asked specific to the content\n        expected\u00a0: the expected answer\n   LLM: Question-Answer (the prompt used to ask the question)\n        inputs:\n               context and question\n        outputs: \n               the prediction/answer to the question\n   Embeddings: uses an LLM embedding model to create the embedding \nrepresentation of the text\n     Embedding_predicted: embeds the output of the Question-Answer LLM\n     Embedding_expected: embeds the output of the expected answer\n   Python: Evaluation (Python code to measure embedding similarity)\n     Inputs:\n            Embedding_predicted output\n            Embedding_expected output\n     Outputs: \n            the similarity score between predicted and expected\n   \n   Outputs:\n        context: -> input.context\n        question: -> input.question\n     expected: -> input.expected\n     predicted: -> output.question_answer\n     evaluation_score: output.evaluation\n### Example Output\n{\n    \"context\": \"Back to the Future (1985)\u2026\",\n    \"evaluation_score\": 0.9567478002354606,\n    \"expected\": \"Marty traveled back in time 30 years.\",\n    \"predicted\": \"Marty traveled back in time 30 years from 1985 to 1955 \nin the movie \\\"Back to the Future.\\\"\",\n    \"question\": \"How far did Marty travel back in time in the movie \nBack to the Future (1985)\"\n}\nBefore running this flow, make sure your LLM block is configured correctly. This may\nrequire you to set up a connection to your chosen LLM. Again, refer to chapter 9 if\nyou need a review on how to complete this. You\u2019ll need to configure the LLM and\nEmbedding blocks with your connection if you\u2019re not using OpenAI.\n After configuring your LLM connection, run the flow by clicking the Play but-\nton from the visual editor or using the Test (Shift-F5) link in the YAML editor win-\ndow. If everything is connected and configured correctly, you should see output\nlike that in listing 10.1.\n Open the question_answer.jinja2 file in VS Code, as shown in listing 10.2. This\nlisting shows the basic question-and-answer-style prompt. In this style of prompt, the\nsystem message describes the basic rules and provides the context to answer the question.\n",
      "content_length": 2224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "248\nCHAPTER 10\nAgent reasoning and evaluation\nIn chapter 4, we explored the retrieval augmented generation (RAG) pattern, and\nthis prompt follows a similar pattern.\nsystem:\nAnswer the users question based on the context below. Keep the answer \nshort and concise. Respond \"Unsure about answer\" if not sure about the \nanswer.\nContext: {{context}}    \nuser:\nQuestion: {{question}}    \nThis exercise shows the simple method of using an LLM to ask questions about a\npiece of content. Then, the question response is evaluated using a similarity matching\nscore. We can see from the output in listing 10.1 that the LLM does a good job of\nanswering a question about the context. In the next section, we\u2019ll explore a similar\ntechnique that uses direct prompting.\n10.1.2 Implementing few-shot prompting\nFew-shot prompting is like question-and-answer prompting, but the makeup of the\nprompt is more about providing a few examples than about facts or context. This\nallows the LLM to bend to patterns or content not previously seen. While this\napproach sounds like question and answer, the implementation is quite different,\nand the results can be powerful.\nOpen prompt_flow/few-shot-prompting/flow.dag.yaml in VS Code and the visual\neditor. Most of the flow looks like the one pictured earlier in figure 10.2, and the dif-\nferences are highlighted in listing 10.3, which shows a YAML pseudocode represen-\ntation. The main differences between this and the previous flow are the inputs and\nLLM prompt.\n \n \nListing 10.2\nquestion_answer.jinja2\nZero-shot, one-shot, and few-shot learning\nOne holy grail of machine learning and AI is the ability to train a model on as few items\nas possible. For example, in traditional vision models, millions of images are fed into\nthe model to help identify the differences between a cat and a dog.\nA one-shot model is a model that requires only a single image to train it. For example,\na picture of a cat can be shown, and then the model can identify any cat image. A\nfew-shot model requires only a few things to train the model. And, of course, zero-shot\nindicates the ability to identify something given no previous examples. LLMs are effi-\ncient learners and can do all three types of learning.\nReplace with the content LLM \nshould answer the question about.\nReplace with \nthe question.\n",
      "content_length": 2308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "249\n10.1\nUnderstanding direct solution prompting\n   Inputs:\n       statement\u00a0 : introduces the context and then asks for output\n       expected\u00a0: the expected answer to the statement\n   LLM: few_shot (the prompt used to ask the question)\n       inputs:statement\n       outputs: the prediction/answer to the statement\n   Embeddings: uses an LLM embedding model to create the embedding \nrepresentation of the text\n        Embedding_predicted: embeds the output of the few_shot LLM\n        Embedding_expected: embeds the output of the expected answer\n   \n   Python: Evaluation (Python code to measure embedding similarity)\n        Inputs:\n               Embedding_predicted output\n               Embedding_expected output\n        Outputs: the similarity score between predicted and expected\nOutputs:\n        statement: -> input.statement\n        expected: -> input.expected\n        predicted: -> output.few_shot\n        evaluation_score: output.evaluation\n### Example Output\n{\n    \"evaluation_score\": 0.906647282920417,    \n    \"expected\": \"We ate sunner and watched the setting sun.\",\n    \"predicted\": \"After a long hike, we sat by the lake \nand enjoyed a peaceful sunner as the sky turned \nbrilliant shades of orange and pink.\",    \n    \"statement\": \"A sunner is a meal we eat in Cananda \nat sunset, please use the word in a sentence\"    \n}\nRun the flow by pressing Shift-F5 or clicking the Play/Test button from the visual edi-\ntor. You should see output like listing 10.3 where the LLM has used the word sunner (a\nmade-up term) correctly in a sentence given the initial statement.\n This exercise demonstrates the ability to use a prompt to alter the behavior of the\nLLM to be contrary to what it has learned. We\u2019re changing what the LLM understands\nto be accurate. Furthermore, we then use that modified perspective to elicit the use of\na made-up word.\n Open the few_shot.jinja2 prompt in VS Code, shown in listing 10.4. This listing\ndemonstrates setting up a simple persona, that of an eccentric dictionary maker, and\nthen providing examples of words it has defined and used before. The base of the\nListing 10.3\nfew-shot-prompting flow\nEvaluation score \nrepresents the \nsimilarity between \nexpected and \npredicted.\nUses sunner in \na sentence\nThis is a false statement but the \nintent is to get the LLM to use \nthe word as if it was real.\n",
      "content_length": 2340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "250\nCHAPTER 10\nAgent reasoning and evaluation\nprompt allows for the LLM to extend the examples and produce similar results using\nother words.\nsystem:\nYou are an eccentric word dictionary maker. You will be asked to \nconstruct a sentence using the word.\nThe following are examples that demonstrate how to craft a sentence using \nthe word.\nA \"whatpu\" is a small, furry animal native to Tanzania. \nAn example of a sentence that uses the word whatpu is:    \nWe were traveling in Africa and we saw these very cute whatpus.\nTo do a \"farduddle\" means to jump up and down really fast. An example of a \nsentence that uses the word farduddle is:\nI was so excited that I started to farduddle.    \nPlease only return the sentence requested by the user.  \nuser:\n{{statement}}   \nYou may say we\u2019re forcing the LLM to hallucinate here, but this technique is the basis\nfor modifying behavior. It allows prompts to be constructed to guide an LLM to do\neverything contrary to what it learned. This foundation of prompting also establishes\ntechniques for other forms of altered behavior. From the ability to alter the percep-\ntion and background of an LLM, we\u2019ll move on to demonstrate a final example of a\ndirect solution in the next section.\n10.1.3 Extracting generalities with zero-shot prompting\nZero-shot prompting or learning is the ability to generate a prompt in such a manner that\nallows the LLM to generalize. This generalization is embedded within the LLM and\ndemonstrated through zero-shot prompting, where no examples are given, but instead a\nset of guidelines or rules are given to guide the LLM.\n Employing this technique is simple and works well to guide the LLM to generate\nreplies given its internal knowledge and no other contexts. It\u2019s a subtle yet powerful\ntechnique that applies the knowledge of the LLM to other applications. This tech-\nnique, combined with other prompting strategies, is proving effective at replacing\nother language classification models\u2014models that identify the emotion or sentiment\nin text, for example.\n Open prompt_flow/zero-shot-prompting/flow.dag.yaml in the VS Code prompt\nflow visual editor. This flow is again almost identical to that shown earlier in figure 10.1\nbut differs slightly in implementation, as shown in listing 10.5.\n \n \nListing 10.4\nfew_shot.jinja2\nDemonstrates an example defining a made-\nup word and using it in a sentence\nDemonstrates \nanother example\nA rule to prevent the \nLLM from outputting \nextra information\nThe input statement defines a \nnew word and asks for the use.\n",
      "content_length": 2525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "251\n10.1\nUnderstanding direct solution prompting\n   Inputs:\n        statement\u00a0 : the statement to be classified\n        expected\u00a0: the expected classification of the statement\n   \n    LLM: zero_shot (the prompt used to classify)\n        inputs: statement\n        outputs: the predicted class given the statement\n    Embeddings: uses an LLM embedding model to create the embedding \nrepresentation of the text\n    Embedding_predicted: embeds the output of the zero_shot LLM\n    Embedding_expected: embeds the output of the expected answer\n    Python: Evaluation (Python code to measure embedding similarity)\n        Inputs:\n               Embedding_predicted output\n             Embedding_expected output\n          Outputs: the similarity score between predicted and expected\n   Outputs:\n        statement: -> input.statement\n        expected: -> input.expected\n        predicted: -> output.few_shot\n        evaluation_score: output.evaluation\n   ### Example Output\n{\n       \"evaluation_score\": 1,    \n       \"expected\": \"neutral\",\n       \"predicted\": \"neutral\",\n       \"statement\": \"I think the vacation is okay. \"    \n   }\nRun the flow by pressing Shift-F5 within the VS Code prompt flow visual editor. You\nshould see output similar to that shown in listing 10.5.\n Now open the zero_shot.jinja2 prompt as shown in listing 10.6. The prompt is\nsimple and uses no examples to extract the sentiment from the text. What is especially\ninteresting to note is that the prompt doesn\u2019t even mention the phrase sentiment, and\nthe LLM seems to understand the intent.\nsystem:\nClassify the text into neutral, negative or positive. \nReturn on the result and nothing else.    \nuser:\n{{statement}}    \nListing 10.5\nzero-shot-prompting flow\nListing 10.6\nzero_shot.jinja2\nShows a perfect \nevaluation score \nof 1.0\nThe statement \nwe\u2019re asking the \nLLM to classify\nProvides essential \nguidance on \nperforming the \nclassification\nThe statement of \ntext to classify\n",
      "content_length": 1943,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "252\nCHAPTER 10\nAgent reasoning and evaluation\nZero-shot prompt engineering is about using the ability of the LLM to generalize\nbroadly based on its training material. This exercise demonstrates how knowledge\nwithin the LLM can be put to work for other tasks. The LLM\u2019s ability to self-contextu-\nalize and apply knowledge can extend beyond its training. In the next section, we\nextend this concept further by looking at how LLMs can reason.\n10.2\nReasoning in prompt engineering\nLLMs like ChatGPT were developed to function as chat completion models, where\ntext content is fed into the model, whose responses align with completing that request.\nLLMs were never trained to reason, plan, think, or have thoughts.\n However, much like we demonstrated with the examples in the previous section,\nLLMs can be prompted to extract their generalities and be extended beyond their\ninitial design. While an LLM isn\u2019t designed to reason, the training material fed into\nthe model provides an understanding of reasoning, planning, and thought. There-\nfore, by extension, an LLM understands what reasoning is and can employ the con-\ncept of reasoning.\nWe\u2019ll look at another set of prompt engineering techniques that allow or mimic rea-\nsoning behavior to demonstrate this reasoning ability. Typically, when evaluating the\napplication of reasoning, we look to having the LLM solve challenging problems it\nwasn\u2019t designed to solve. A good source of such is based on logic, math, and word\nproblems.\n Using the time travel theme, what class of unique problems could be better to\nsolve than understanding time travel? Figure 10.3 depicts one example of a uniquely\nchallenging time travel problem. Our goal is to acquire the ability to prompt the LLM\nin a manner that allows it to solve the problem correctly.\n Time travel problems are thought exercises that can be deceptively difficult to\nsolve. The example in figure 10.3 is complicated to solve for an LLM, but the part it\nReasoning and planning\nReasoning is the ability of an intellect, artificial or not, to understand the process of\nthought or thinking through a problem. An intellect can understand that actions have\noutcomes, and it can use this ability to reason through which action from a set of\nactions can be applied to solve a given task.\nPlanning is the ability of the intellect to reason out the order of actions or tasks and\napply the correct parameters to achieve a goal or outcome\u2014the extent to which an\nintellectual plan depends on the scope of the problem. An intellect may combine mul-\ntiple levels of planning, from strategic and tactical to operational and contingent.\n",
      "content_length": 2620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "253\n10.2\nReasoning in prompt engineering\ngets wrong may surprise you. The next section will use reasoning in prompts to solve\nthese unique problems.\n10.2.1 Chain of thought prompting\nChain of thought (CoT)prompting is a prompt engineering technique that employs\nthe one-shot or few-shot examples that describe the reasoning and the steps to\naccomplish a desired goal. Through the demonstration of reasoning, the LLM can\ngeneralize this principle and reason through similar problems and goals. While the\nLLM isn\u2019t trained with the goal of reasoning, we can elicit the model to reason, using\nprompt engineering.\n Open prompt_flow/chain-of-thought-prompting/flow.dag.yaml in the VS Code\nprompt flow visual editor. The elements of this flow are simple, as shown in figure 10.4.\nWith only two LLM blocks, the flow first uses a CoT prompt to solve a complex ques-\ntion; then, the second LLM prompt evaluates the answer.\n Listing 10.7 shows the YAML pseudocode that describes the blocks and the\ninputs/outputs of the flow in more detail. The default problem statement in this\nexample isn\u2019t the same as in figure 10.3.\n \n \n \nIn a sci-\ufb01\ufb01lm, Alex is a time traveler who decides to go back in\ntime to witness a famous historical battle that took place 100 years ago,\nwhich lasted for 10 days. He arrives three days before the battle starts.\nHowever, after spending six days in the past, he jumps forward in time by\n50 years and stays there for 20 days. Then, he travels back to witness the\nend of the battle. How many days does Alex spend in the past before he\nsees the end of the battle?\nAlex\nTimeline\nPresent\nHistoric\nbattle starts\nHistoric\nbattle ends\nAlex\narrives\nJumps back in time 100 years (3 days before the battle)\nspends 6 days\njumps forward in time 50 years\n20 days\njumps back to the present\nAlex\nleaves\nAlex\nreturns?\nAlex\narrives\nAlex\nleaves\n???\nThe problem statement:\nThe statement is vague as\nto what time Alex returns\nto the battle. Is it the same\ntime he left?\njumps back to witness end of battle\nbattle lasts 10 days\nFigure 10.3\nThe complexity of the time travel problems we intend to solve using LLMs with reasoning and \nplanning\n",
      "content_length": 2137,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "254\nCHAPTER 10\nAgent reasoning and evaluation\n   Inputs:\n        statement\u00a0 : the statement problem to be solved\n        expected\u00a0: the expected solution to the problem\n   LLM: cot (the prompt used to solve the problem)\n        inputs: statement\n        outputs: the predicted answer given the problem statement\n   LLM: evaluate_answer (the prompt used to evaluate the solution)\n        inputs:\n               statement: -> input.statement\n               expected: -> input.expected\n               predicted: -> output.cot\n        outputs: a score of how well the problem was answered\nListing 10.7\nchain-of-thought-prompting flow\nLLM prompt that uses chain of\nthought to answer a given problem\nLLM prompt that evaluates\nthe ef\ufb01cacy of the answer\nFigure 10.4\nThe flow of the CoT\n",
      "content_length": 778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "255\n10.2\nReasoning in prompt engineering\n   Outputs:\n        statement: -> input.statement\n        expected: -> input.expected\n        predicted: -> output.cot\n        evaluation_score: output.evaluate_answer\n   ### Example Output\n{\n    \"evaluation_score\": \"0.5\",    \n    \"expected\": \"After the final jump, Max finds himself \nin the year 1980 and he is 75 years old.\",    \n    \"predicted\": \" Max starts in the year 2300 and \ntravels backward in 40-year increments, spending 5 years \nin each period. The journeys will be as follows:\n\\n\\n- From 2300 to 2260: Max is 25 + 5 = 30 years old.\n\\n- From 2260 to 2220: Max is 30 + 5 = 35 years old.\n\\n- From 2220 to 2180: Max is 35 + 5 = 40 years old.\n\\n- From 2180 to 2140: Max is 40 + 5 = 45 years old.\n\\n- From 2140 to 2100: Max is 45 + 5 = 50 years old.\n\\n- From 2100 to 2060: Max is 50 + 5 = 55 years old.\n\\n- From 2060 to 2020: Max is 55 + 5 = 60 years old.\n\\n- From 2020 to 1980: Max is 60 + 5 = 65 years old.\n\\n- From 1980 to 1940: Max is 65 + 5 = 70 years old.\n\\n- From 1940 to 1900: Max is 70 + 5\"    \n}\nDig into the inputs and check the problem statement; try to evaluate the problem\nyourself. Then, run the flow by pressing Shift-F5. You should see output similar to that\nshown in listing 10.7.\n Open the cot.jinja2 prompt file as shown in listing 10.8. This prompt gives a few\nexamples of time travel problems and then the thought-out and reasoned solution.\nThe process of showing the LLM the steps to complete the problem provides the rea-\nsoning mechanism.\nsystem:\n\"In a time travel movie, Sarah travels back in time to \nprevent a historic event from happening. She arrives \n2 days before the event. After spending a day preparing, \nshe attempts to change the event but realizes she has \nactually arrived 2 years early, not 2 days. She then \ndecides to wait and live in the past until the event's \noriginal date. How many days does Sarah spend in the past \nbefore the day of the event?\"    \nChain of Thought:    \n    Initial Assumption: Sarah thinks she has arrived 2 days before the event.\n    Time Spent on Preparation: 1 day spent preparing.\n    Realization of Error: Sarah realizes she's actually 2 years early.\n    Conversion of Years to Days: \nListing 10.8\ncot.jinja2\nThe evaluated \nscore for the \ngiven solution\nThe expected \nanswer for the \nproblem\nThe predicted answer \nshows the reasoning \nsteps and output.\nA few example \nproblem \nstatements\nThe solution \nto the problem \nstatement, output \nas a sequence of \nreasoning steps\n",
      "content_length": 2492,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "256\nCHAPTER 10\nAgent reasoning and evaluation\n2 years = 2 \u00d7 365 = 730 days (assuming non-leap years).\n    Adjust for the Day Spent Preparing: 730 - 1 = 729 days.\n    Conclusion: Sarah spends 729 days in the past before the day of the event.\n\"In a sci-fi film, Alex is a time traveler who decides \nto go back in time to witness a famous historical battle \nthat took place 100 years ago, which lasted for 10 days. \nHe arrives three days before the battle starts. However, \nafter spending six days in the past, he jumps forward in \ntime by 50 years and stays there for 20 days. Then, he \ntravels back to witness the end of the battle. How many \ndays does Alex spend in the past before he sees the end of\n the battle?\"    \nChain of Thought:    \n    Initial Travel: Alex arrives three days before the battle starts.\n    Time Spent Before Time Jump: Alex spends six days in the past. \nThe battle has started and has been going on for 3 days (since he \narrived 3 days early and has now spent 6 days, 3 + 3 = 6).\n    First Time Jump: Alex jumps 50 years forward and stays for 20 days.\n This adds 20 days to the 6 days he's already spent in the past \n(6 + 20 = 26).\n    Return to the Battle: When Alex returns, he arrives back on the same \nday he left (as per time travel logic). The battle has been going on for \n3 days now.\n    Waiting for the Battle to End: The battle lasts 10 days. Since he's \nalready witnessed 3 days of it, he needs to wait for 7 more days.\n    Conclusion: Alex spends a total of 3 (initial wait) + 3 (before the \nfirst jump) + 20 (50 years ago) + 7 (after returning) = 33 days in the \npast before he sees the end of the battle.\nThink step by step but only show the final answer to the statement.\nuser:\n{{statement}}    \nYou may note that the solution to figure 10.3 is also provided as an example in listing\n10.8. It\u2019s also helpful to go back and review listing 10.7 for the reply from the LLM\nabout the problem. From this, you can see the reasoning steps the LLM applied to get\nits final answer.\n Now, we can look at the prompt that evaluates how well the solution solved the\nproblem. Open evaluate_answer.jinja2, shown in listing 10.9, to review the prompt\nused. The prompt is simple, uses zero-shot prompting, and allows the LLM to general-\nize how it should score the expected and predicted. We could provide examples and\nscores, thus changing this to an example of a few-shot classification.\nsystem:\nPlease confirm that expected and predicted results are \nthe same for the given problem.    \nListing 10.9\nevaluate_answer.jinja2\nA few example \nproblem \nstatements\nThe solution \nto the problem \nstatement, output \nas a sequence of \nreasoning steps\nThe problem statement the \nLLM is directed to solve\nThe rules for \nevaluating the \nsolution\n",
      "content_length": 2759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "257\n10.2\nReasoning in prompt engineering\nReturn a score from 0 to 1 where 1 is a perfect match and 0 is no match.\nPlease just return the score and not the explanation.    \nuser:\nProblem: {{problem}}    \nExpected result: {{expected}}    \nPredicted result: {{predicted}}    \nLooking at the LLM output shown earlier in listing 10.7, you can see why the evalua-\ntion step may get confusing. Perhaps a fix to this could be suggesting to the LLM to\nprovide the final answer in a single statement. In the next section, we move on to\nanother example of prompt reasoning.\n10.2.2 Zero-shot CoT prompting\nAs our time travel demonstrates, CoT prompting can be expensive in terms of prompt\ngeneration for a specific class of problem. While not as effective, there are techniques\nsimilar to CoT that don\u2019t use examples and can be more generalized. This section will\nexamine a straightforward phrase employed to elicit reasoning in LLMs.\n Open prompt_flow/zero-shot-cot-prompting/flow.dag.yaml in the VS Code\nprompt flow visual editor. This flow is very similar to the previous CoT, as shown in fig-\nure 10.4. The next lsting shows the YAML pseudocode that describes the flow.\n   Inputs:\n        statement\u00a0 : the statement problem to be solved\n        expected\u00a0: the expected solution to the problem\n    \n   LLM: cot (the prompt used to solve the problem)\n        inputs: statement\n        outputs: the predicted answer given the problem statement\n    \n   LLM: evaluate_answer (the prompt used to evaluate the solution)\n        inputs:\n               statement: -> input.statement\n               expected: -> input.expected\n               predicted: -> output.cot\n    \n         outputs: a score of how well the problem was answered\n    \n    \n    Outputs:\n        statement: -> input.statement\n        expected: -> input.expected\n        predicted: -> output.cot\nListing 10.10\nzero-shot-CoT-prompting flow\nDirection to only \nreturn the score \nand nothing else\nThe initial \nproblem \nstatement\nThe expected or \ngrounded answer\nThe output from the\nCoT prompt earlier\n",
      "content_length": 2048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "258\nCHAPTER 10\nAgent reasoning and evaluation\n        evaluation_score: output.evaluate_answer\n    \n    ### Example Output\n   {\n       \"evaluation_score\": \"1\",    \n       \"expected\": \"After the final jump, \n          \u27a5 Max finds himself in the year 1980 and \n   he is 75 years old.\",    \n       \"predicted\": \"Max starts in\u2026 \n          \u27a5 Therefore, after the final jump, \n          \u27a5 Max is 75 years old and in the year 1980.\",    \n       \"statement\": \"In a complex time travel \u2026\"    \n   }\nRun/test the flow in VS Code by pressing Shift-F5 while in the visual editor. The flow\nwill run, and you should see output similar to that shown in listing 10.10. This exercise\nexample performs better than the previous example on the same problem.\n Open the cot.jinja2 prompt in VS Code, as shown in listing 10.11. This is a much\nsimpler prompt than the previous example because it only uses zero-shot. However, one\nkey phrase turns this simple prompt into a powerful reasoning engine. The line in the\nprompt Let\u2019s think step by step triggers the LLM to consider internal context show-\ning reasoning. This, in turn, directs the LLM to reason out the problem in steps.\nsystem:\nYou are an expert in solving time travel problems.\nYou are given a time travel problem and you have to solve it.\nLet's think step by step.    \nPlease finalize your answer in a single statement.    \nuser:\n{{statement}}    \nSimilar phrases asking the LLM to think about the steps or asking it to respond in\nsteps also extract reasoning. We\u2019ll demonstrate a similar but more elaborate tech-\nnique in the next section.\n10.2.3 Step by step with prompt chaining\nWe can extend the behavior of asking an LLM to think step by step into a chain of\nprompts that force the LLM to solve the problem in steps. In this section, we look at a\ntechnique called prompt chaining that forces an LLM to process problems in steps.\n Open the prompt_flow/prompt-chaining/flow.dag.yaml file in the visual editor,\nas shown in figure 10.5. Prompt chaining breaks up the reasoning method used to\nsolve a problem into chains of prompts. This technique forces the LLM to answer the\nproblem in terms of steps.\nListing 10.11\ncot.jinja2\nThe final \nevaluation score\nThe expected \nanswer\nThe predicted \nanswer (the \nsteps have been \nomitted showing \nthe final answer)\nThe initial problem\nstatement\nA magic line \nthat formulates \nreasoning from \nthe LLM\nAsks the LLM to \nprovide a final \nstatement of the \nanswer\nThe problem \nstatement the LLM \nis asked to solve\n",
      "content_length": 2491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "259\n10.2\nReasoning in prompt engineering\nListing 10.12 shows the YAML pseudocode that describes the flow in a few more\ndetails. This flow chains the output of the first LLM block into the second and then\nfrom the second into the third. Forcing the LLM to process the problem this way\nuncovers the reasoning pattern, but it can also be overly verbose.\n   Inputs:\n        statement  : the statement problem to be solved\n    \n   LLM: decompose_steps (the prompt used to decompose the problem)\n        inputs: \n               statement: -> input.statement    \n    \n        outputs: the breakdown of steps to solve the problem\n   LLM: calculate_steps (the prompt used to calculate the steps)\n        inputs:\n               statement: -> input.statement\n               decompose_steps: -> output.decompose_steps    \n    \n               outputs: the calculation for each step\nListing 10.12\nprompt-chaining flow\nLLM then calculates the\noutput for each step.\nLLM calculates the\nsolution using the steps.\nLLM prompt \ufb01rst decomposes\nthe problem into steps.\nFigure 10.5\nThe prompt chaining flow\nStart of the chain \nof prompts\nOutput from \nthe previous \nstep injected \ninto this step\n",
      "content_length": 1171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "260\nCHAPTER 10\nAgent reasoning and evaluation\n   LLM: calculate_solution (attempts to solve the problem)\n        inputs:\n               statement: -> input.statement\n               decompose_steps: -> output.decompose_steps\n               calculate_steps: -> output.calculate_steps    \n         outputs: the final solution statement\n   Outputs:\n        statement: -> input.statement\n        decompose_steps: -> output.decompose_steps\n        calculate_steps: -> output.calculate_steps\n        calculate_solution: -> output.calculate_solution\n   ### Example Output\n{\n    \"calculate_steps\": \"1. The days spent by Alex\",\n    \"decompose_steps\": \"To figure out the \u2026\",\n    \"solution\": \"Alex spends 13 days in the \n           \u27a5 past before the end of the battle.\",    \n    \"statement\": \"In a sci-fi film, Alex \u2026\"    \n}\nRun the flow by pressing Shift-F5 from the visual editor, and you\u2019ll see the output as\nshown in listing 10.12. The answer is still not correct for the Alex problem, but we can\nsee all the work the LLM is doing to reason out the problem.\n Open up all three prompts: decompose_steps.jinja2, calculate_steps.jinja2,\nand calculate_solution.jinja2 (see listings 10.13, 10.14, and 10.15, respectively).\nAll three prompts shown in the listings can be compared to show how outputs chain\ntogether.\nsystem:\nYou are a problem solving AI assistant.\nYour job is to break the users problem down into smaller steps and list \nthe steps in the order you would solve them.\nThink step by step, not in generalities.\nDo not attempt to solve the problem, just list the steps.\nuser:\n{{statement}}    \nsystem:\nYou are a problem solving AI assistant.\nYou will be given a list of steps that solve a problem.\nYour job is to calculate the output for each of the steps in order.\nDo not attempt to solve the whole problem,\nListing 10.13\ndecompose_steps.jinja2\nListing 10.14\ncalculate_steps.jinja2\nOutput from \ntwo previous \nsteps injected \ninto this step\nThe final solution \nstatement, \nalthough wrong, \nis closer.\nForces the LLM to \nlist only the steps \nand nothing else\nThe initial problem \nstatement\n",
      "content_length": 2086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "261\n10.3\nEmploying evaluation for consistent solutions\njust list output for each of the steps.    \nThink step by step.    \nuser:\n{{statement}}\n{{steps}}    \nsystem:\nYou are a problem solving AI assistant.\nYou will be given a list of steps and the calculated output for each step.\nUse the calculated output from each step to determine the final \nsolution to the problem.\nProvide only the final solution to the problem in a \nsingle concise sentence. Do not include any steps \nin your answer.    \nuser:\n{{statement}}\n{{steps}}    \n{{calculated}}    \nIn this exercise example, we\u2019re not performing any evaluation and scoring. Without\nthe evaluation, we can see that this sequence of prompts still has problems solving our\nmore challenging time travel problem shown earlier in figure 10.3. However, that\ndoesn\u2019t mean this technique doesn\u2019t have value, and this prompting format solves\nsome complex problems well.\n What we want to find, however, is a reasoning and planning methodology that can\nsolve such complex problems consistently. The following section moves from reason-\ning to evaluating the best solution.\n10.3\nEmploying evaluation for consistent solutions\nIn the previous section, we learned that even the best-reasoned plans may not always\nderive the correct solution. Furthermore, we may not always have the answer to con-\nfirm if that solution is correct. The reality is that we often want to use some form of\nevaluation to determine the efficacy of a solution.\n Figure 10.6 shows a comparison of the prompt engineering strategies that have been\ndevised as a means of getting LLMs to reason and plan. We\u2019ve already covered the two on\nthe left: zero-shot direct prompting and CoT prompting. The following example exer-\ncises in this section will look at self-consistency with the CoT and ToT techniques.\n We\u2019ll continue to focus on the complex time travel problem to compare these\nmore advanced methods that expand on reasoning and planning with evaluation. In\nthe next section, we\u2019ll evaluate self-consistency.\nListing 10.15\ncalculate_solution.jinja2\nRequests that the LLM \nnot solve the whole \nproblem, just the steps\nUses the magic statement \nto extract reasoning\nInjects the steps \nproduced by the \ndecompose_steps step\nRequests that the LLM \noutput the final answer \nand not any steps\nThe decomposed steps\nThe calculated steps\n",
      "content_length": 2338,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "262\nCHAPTER 10\nAgent reasoning and evaluation\n10.3.1 Evaluating self-consistency prompting\nConsistency in prompting is more than just lowering the temperature parameter we\nsend to an LLM. Often, we want to generate a consistent plan or solution and still use\na high temperature to better evaluate all the variations to a plan. By evaluating multi-\nple different plans, we can get a better sense of the overall value of a solution.\n Self-consistent prompting is the technique of generating multiple plans/solutions for\na given problem. Then, those plans are evaluated, and the more frequent or consis-\ntent plan is accepted. Imagine three plans generated, where two are similar, but the\nthird is different. Using self-consistency, we evaluate the first two plans as the more\nconsistent answer.\n Open prompt_flow/self-consistency-prompting/flow.dag.yaml in the VS Code\nprompt flow visual editor. The flow diagram shows the simplicity of the prompt genera-\ntion flow in figure 10.7. Next to it in the diagram is the self-consistency evaluation flow.\n Prompt flow uses a direct acyclic graph (DAG) format to execute the flow logic.\nDAGs are an excellent way of demonstrating and executing flow logic, but because\nthey are acyclic, meaning they can\u2019t repeat, they can\u2019t execute loops. However, because\nprompt flow provides a batch processing mechanism, we can use that to simulate\nloops or repetition in a flow.\n Referring to figure 10.6, we can see that self-consistency processes the input three\ntimes before collecting the results and determining the best plan/reply. We can apply\nthis same pattern but use batch processing to generate the outputs. Then, the evalua-\ntion flow will aggregate the results and determine the best answer.\n Open the self-consistency-prompting/cot.jinja2 prompt template in VS\nCode (see listing 10.16). The listing was shortened, as we\u2019ve seen parts before. This\nInput\nInput\nInput\nInput\nOutput\nOutput\nOutput\nOutput\nZero-shot, direct\nprompting\nChain of thought\n(CoT)\nSelf-consistency\nwith CoT (CoT-SC)\nTree of thought\n(ToT)\nMajority vote\n......\n......\n......\n......\n......\nthought\nFigure 10.6\nComparing the various prompt engineering strategies to enable reasoning and planning from LLMs\n",
      "content_length": 2213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "263\n10.3\nEmploying evaluation for consistent solutions\nprompt uses two (few-shot prompt) examples of a CoT to demonstrate the thought\nreasoning to the LLM.\nsystem:\n\"In a time travel movie, Sarah travels back\u2026 \"    \nChain of Thought:\n    Initial Assumption: \u2026    \n    Conclusion: Sarah spends 729 days in the past before the day of the event.\n\"In a complex time travel movie plot, Max, a 25 year old\u2026\"    \nChain of Thought:\n    Starting Point: Max starts \u2026    \n    Conclusion: After the final jump, \nMax finds himself in the year 1980 and he is 75 years old.\nListing 10.16\nself-consistency-prompting/cot.jinja2\nSelf-consistency generation\nSelf-consistency evaluation\nThis \ufb02ow evaluates each of the\nprompts and then chooses the\nprompt that is\nconsistent.\nthe most\nSelf-consistency uses a\nchain of thought prompt\nto generate the plans.\nFigure 10.7\nThe self-consistency prompt generation beside the evaluation flow\nThe Sarah time \ntravel problem\nSample CoT, \ncut for brevity\nThe Max time \ntravel problem\nSample CoT, \ncut for brevity\n",
      "content_length": 1029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "264\nCHAPTER 10\nAgent reasoning and evaluation\nThink step by step,\n but only show the final answer to the statement.    \nuser:\n{{statement}}\nOpen the self-consistency-prompting/flow.dag.yaml file in VS Code. Run the\nexample in batch mode by clicking Batch Run (the beaker icon) from the visual edi-\ntor. Figure 10.8 shows the process step by step: \n1\nClick Batch Run.\n2\nSelect the JSON Lines (JSONL) input.\n3\nSelect statements.jsonl.\n4\nClick the Run link.\nTIP\nIf you need to review the process, refer to chapter 9, which covers this\nprocess in more detail.\nFinal guide and \nstatement to \nconstrain output\n1. Click the\nBatch Run button.\n3. Click the Run link to start\nthe batch processing.\n2. Select the local data \ufb01le, and\nwhen prompted with the \ufb01le\ndialog, select statements.jsonl.\nFigure 10.8\nThe step-by-step process of launching a batch process\n",
      "content_length": 848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "265\n10.3\nEmploying evaluation for consistent solutions\nListing 10.17 shows the JSON output from executing the flow in batch mode. The\nstatements.jsonl file has five identical Alex time travel problem entries. Using\nidentical entries allows us to simulate the prompt executing five times on the dupli-\ncate entry.\n{\n    \"name\": \"self-consistency-prompting_default_20240203_100322_912000\",\n    \"created_on\": \"2024-02-03T10:22:30.028558\",\n    \"status\": \"Completed\",\n    \"display_name\": \"self-consistency-prompting_variant_0_202402031022\",\n    \"description\": null,\n    \"tags\": null,\n    \"properties\": {\n        \"flow_path\": \"\u2026prompt_flow/self-consistency-prompting\",    \n        \"output_path\": \"\u2026/.promptflow/.runs/self-\n\u27a5 consistency-prompting_default_20240203_100322_912000\",    \n        \"system_metrics\": {\n            \"total_tokens\": 4649,\n            \"prompt_tokens\": 3635,\n            \"completion_tokens\": 1014,\n            \"duration\": 30.033773\n        }\n    },\n    \"flow_name\": \"self-consistency-prompting\",\n    \"data\": \"\u2026/prompt_flow/self-consistency-prompting/\n\u27a5 statements.jsonl\",    \n    \"output\": \"\u2026/.promptflow/.runs/self-consistency-\n\u27a5 prompting_default_20240203_100322_912000/flow_outputs\"\n}\nYou can view the flow produced by pressing the Ctrl key and clicking the output link,\nhighlighted in listing 10.17. This will open another instance of VS Code, showing a\nfolder with all the output from the run. We now want to check the most consistent\nanswer. Fortunately, the evaluation feature in prompt flow can help us identify consis-\ntent answers using similarity matching.\n Open self-consistency-evaluation/flow.dag.yaml in VS Code (see figure\n10.7). This flow embeds the predicted answer and then uses an aggregation to deter-\nmine the most consistent answer.\n From the flow, open consistency.py in VS Code, as shown in listing 10.18. The\ncode for this tool function calculates the cosine similarity for all pairs of answers.\nThen, it finds the most similar answer, logs it, and outputs that as the answer.\nfrom promptflow import tool\nfrom typing import List\nimport numpy as np\nfrom scipy.spatial.distance import cosine\nListing 10.17\nself-consistency-prompting batch execution output\nListing 10.18\nconsistency.py\nThe path where \nthe flow was \nexecuted from\nThe folder \ncontaining the \noutputs of the flow \n(note this path)\nThe data used to run \nthe flow in batch\n",
      "content_length": 2375,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "266\nCHAPTER 10\nAgent reasoning and evaluation\n@tool\ndef consistency(texts: List[str],\n                embeddings: List[List[float]]) -> str:\n    if len(embeddings) != len(texts):\n        raise ValueError(\"The number of embeddings \n       \u27a5 must match the number of texts.\")\n    mean_embedding = np.mean(embeddings, axis=0)    \n    similarities = [1 - cosine(embedding, mean_embedding) \n                \u27a5 for embedding in embeddings]    \n    most_similar_index = np.argmax(similarities)    \n    from promptflow import log_metric\n    log_metric(key=\"highest_ranked_output\", value=texts[most_similar_index])    \n    return texts[most_similar_index]    \nWe need to run the evaluation flow in batch mode as well. Open self-consistency-\nevaluation/flow.dag.yaml in VS Code and run the flow in batch mode (beaker\nicon). Then, select Existing Run as the flow input, and when prompted, choose the\ntop or the last run you just executed as input.\n Again, after the flow completes processing, you\u2019ll see an output like that shown in\nlisting 10.17. Ctrl-click on the output folder link to open a new instance of VS Code\nshowing the results. Locate and open the metric.json file in VS Code, as shown in\nfigure 10.9.\n The answer shown in figure 10.9 is still incorrect for this run. You can continue a\nfew more batch runs of the prompt and/or increase the number of runs in a batch\nand then evaluate flows to see if you get better answers. This technique is generally\nmore helpful for more straightforward problems but still demonstrates an inability to\nreason out complex problems.\n Self-consistency uses a reflective approach to evaluate the most likely thought.\nHowever, the most likely thing is certainly not always the best. Therefore, we must con-\nsider a more comprehensive approach in the next section.\n10.3.2 Evaluating tree of thought prompting\nAs mentioned earlier, ToT prompting, as shown in figure 10.6, combines self-evalua-\ntion and prompt chaining techniques. As such, it breaks down the sequence of plan-\nning into a chain of prompts, but at each step in the chain, it provides for multiple\nevaluations. This creates a tree that can be executed and evaluated at each level,\nbreadth-first, or from top to bottom, depth-first.\n Figure 10.10 shows the difference between executing a tree using breadth-first or\ndepth-first. Unfortunately, due to the DAG execution pattern of prompt flow, we can\u2019t\nquickly implement the depth-first method, but breadth-first works just fine.\nCalculates the mean of \nall the embeddings\nCalculates cosine \nsimilarity for each \npair of embeddings\nFinds the index \nof the most \nsimilar answer\nLogs the output\nas a metric\nReturns the text \nfor the most \nsimilar answer\n",
      "content_length": 2695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "267\n10.3\nEmploying evaluation for consistent solutions\nSelect the\nmetrics.json output\nReturns the highest ranked output as\ngenerated by the aggregation in\nconsistency.pr, but is the answer right?\nFigure 10.9\nThe VS Code is open to the batch run output folder. Highlighted are the metrics.json file and \nthe output showing the most similar answer.\ninput\noutput\nTree of Thought (ToT)\n......\nBreadth-\ufb01rst executes/considers\na row of thoughts at a time.\nDepth-\ufb01rst executes/considers\na sequence of thoughts from\nbeginning to end.\nFigure 10.10\nBreadth-first vs. depth-first execution on a ToT pattern\n",
      "content_length": 596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "268\nCHAPTER 10\nAgent reasoning and evaluation\nOpen tree-of-thought-evaluation/flow.dag.yaml in VS Code. The visual of the\nflow is shown in figure 10.11. This flow functions like a breadth-first ToT pattern\u2014the\nflow chains together a series of prompts asking the LLM to return multiple plans at\neach step.\nBecause the flow executes in a breadth-first style, each level output of the nodes is also\nevaluated. Each node in the flow uses a pair of semantic functions\u2014one to generate\nthe answer and the other to evaluate the answer. The semantic function is a custom\nPython flow block that processes multiple inputs and generates multiple outputs.\n Listing 10.19 shows the semantic_function.py tool. This general tool is reused for\nmultiple blocks in this flow. It also demonstrates the embedding functionality from\nthe SK for direct use within prompt flow.\n@tool\ndef my_python_tool(\n    input: str,\n    input_node: int,\n    history: str,\nListing 10.19\nsemantic_function.py\nFirst asks LLM to create 3 plans,\nbut not solutions as an expert\nEach LLM node calculates 3 versions\nof the output for each step in the plan.\nOutput from an LLM block is a\nsingle string, so this block splits\nthe text into nodes.\nSplits the output text\ninto 3 nodes again\nEach answer node calculates\nthe output of the answer.\nFigure 10.11\nToT pattern expressed and prompt flow\n",
      "content_length": 1345,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "269\n10.3\nEmploying evaluation for consistent solutions\n    semantic_function: str,\n    evaluation_function: str,\n    function_name: str,\n    skill_name: str,\n    max_tokens: int,\n    temperature: float,\n    deployment_name: str,\n    connection: Union[OpenAIConnection, \n                      AzureOpenAIConnection],    \n) -> str:\n    if input is None or input == \"\":    \n        return \"\"\n    kernel = sk.Kernel(log=sk.NullLogger())\n    # code for setting up the kernel and LLM connection omitted\n    function = kernel.create_semantic_function(\n                             semantic_function,                                               \n                             function_name=function_name,\n                             skill_name=skill_name,\n                             max_tokens=max_tokens,\n                             temperature=temperature,\n                             top_p=0.5)    \n    evaluation = kernel.create_semantic_function(\n                             evaluation_function,        \n                             function_name=\"Evaluation\",\n                             skill_name=skill_name,\n                             max_tokens=max_tokens,\n                             temperature=temperature,\n                             top_p=0.5)    \n    async def main():\n        query = f\"{history}\\n{input}\"\n        try:\n            eval = int((await evaluation.invoke_async(query)).result)\n            if eval > 25:    \n                return await function.invoke_async(query)   \n        except Exception as e:\n            raise Exception(\"Evaluation failed\", e)\n       try:\n        result = asyncio.run(main()).result\n        return result\n    except Exception as e:\n        print(e)\n        return \"\"\nThe semantic function tool is used in the tree\u2019s experts, nodes, and answer blocks. At\neach step, the function determines if any text is being input. If there is no text, the\nblock returns with no execution. Passing no text to a block means that the previous\nblock failed evaluation. By evaluating before each step, ToT short-circuits the execu-\ntion of plans it deems as not being valid.\nUses a union to allow \nfor different types of \nLLM connections\nChecks to see if the input is \nempty or None; if so, the \nfunction shouldn\u2019t be executed.\nSets up the \ngeneration function \nthat creates a plan\nSets up the \nevaluation function\nRuns the evaluate function and\ndetermines if the input is good\nenough to continue\nIf the evaluation \nscore is high \nenough, \ngenerates the \nnext step\n",
      "content_length": 2503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "270\nCHAPTER 10\nAgent reasoning and evaluation\n This may be a complex pattern to grasp at first, so go ahead and run the flow in VS\nCode. Listing 10.20 shows just the answer node output of a run; these results may vary\nfrom what you see but should be similar. Nodes that return no text either failed evalu-\nation or their parents did.\n{\n    \"answer_1_1\": \"\",    \n    \"answer_1_2\": \"\",\n    \"answer_1_3\": \"\",\n    \"answer_2_1\": \"Alex spends a total of 29 days in the past before he \nsees the end of the battle.\",\n    \"answer_2_2\": \"\",    \n    \"answer_2_3\": \"Alex spends a total of 29 days in the past before he \nsees the end of the battle.\",\n    \"answer_3_1\": \"\",    \n    \"answer_3_2\": \"Alex spends a total of 29 days in the past before he \nsees the end of the battle.\",\n    \"answer_3_3\": \"Alex spends a total of 9 days in the past before he \nsees the end of the battle.\",\nThe output in listing 10.20 shows how only a select set of nodes was evaluated. In most\ncases, the evaluated nodes returned an answer that could be valid. Where no output\nwas produced, it means that the node itself or its parent wasn\u2019t valid. When sibling\nnodes all return empty, the parent node fails to evaluate.\n As we can see, ToT is valid for complex problems but perhaps not very practical.\nThe execution of this flow can take up to 27 calls to an LLM to generate an output. In\npractice, it may only do half that many calls, but that\u2019s still a dozen or more calls to\nanswer a single problem.\n10.4\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Create Direct Prompting, Few-Shot Prompting, and Zero-Shot\nPrompting\nObjective\u2014Create three different prompts for an LLM to summarize a recent\nscientific article: one using direct prompting, one with few-shot prompting, and\nthe last employing zero-shot prompting. \nTasks:\n\u2013 Compare the effectiveness of the summaries generated by each approach.\n\u2013 Compare the accuracy of the summaries generated by each approach.\n\uf0a1Exercise 2\u2014Craft Reasoning Prompts\nObjective\u2014Design a set of prompts that require the LLM to solve logical puzzles\nor riddles.\nListing 10.20\nOutput from tree-of-thought-evaluation flow\nRepresents that the first \nnode plans weren\u2019t valid \nand not executed\nThe plan for node 2 and\nanswer 2 failed evaluation\nand wasn\u2019t run.\nThe plan for this node failed\nto evaluate and wasn\u2019t run.\n",
      "content_length": 2359,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "271\nSummary\nTasks:\n\u2013 Focus on how the structure of your prompt can influence the LLM\u2019s rea-\nsoning process.\n\u2013 Focus on how the same can influence the correctness of its answers.\n\uf0a1Exercise 3\u2014Evaluation Prompt Techniques\nObjective\u2014Develop an evaluation prompt that asks the LLM to predict the out-\ncome of a hypothetical experiment.\nTask:\n\u2013 Create a follow-up prompt that evaluates the LLM\u2019s prediction for accuracy\nand provides feedback on its reasoning process.\nSummary\n\uf0a1Direct solution prompting is a foundational method of using prompts to direct\nLLMs toward solving specific problems or tasks, emphasizing the importance of\nclear question-and-answer structures.\n\uf0a1Few-shot prompting provides LLMs with a few examples to guide them in han-\ndling new or unseen content, highlighting its power in enabling the model to\nadapt to unfamiliar patterns.\n\uf0a1Zero-shot learning and prompting demonstrate how LLMs can generalize from\ntheir training to solve problems without needing explicit examples, showcasing\ntheir inherent ability to understand and apply knowledge in new contexts.\n\uf0a1Chain of thought prompting guides the LLMs through a reasoning process step\nby step to solve complex problems, illustrating how to elicit detailed reasoning\nfrom the model.\n\uf0a1Prompt chaining breaks down a problem into a series of prompts that build\nupon each other, showing how to structure complex problem-solving processes\ninto manageable steps for LLMs.\n\uf0a1Self-consistency is a prompt technique that generates multiple solutions to a\nproblem and selects the most consistent answer through evaluation, emphasiz-\ning the importance of consistency in achieving reliable outcomes.\n\uf0a1Tree of thought prompting combines self-evaluation and prompt chaining to\ncreate a comprehensive strategy for tackling complex problems, allowing for a\nsystematic exploration of multiple solution paths.\n\uf0a1Advanced prompt engineering strategies provide insights into sophisticated\ntechniques such as self-consistency with CoT and ToT, offering methods to\nincrease the accuracy and reliability of LLM-generated solutions.\n",
      "content_length": 2075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "272\nAgent planning\nand feedback\nNow that we\u2019ve examined how large language models (LLMs) can reason and plan,\nthis chapter takes this concept a step further by employing planning within an\nagent framework. Planning should be at the core of any agent/assistant platform\nor toolkit. We\u2019ll start by looking at the basics of planning and how to implement a\nplanner through prompting. Then, we\u2019ll see how planning operates using the\nOpenAI Assistants platform, which automatically incorporates planning. From\nthere, we\u2019ll build and implement a general planner for LLMs.\n Planning can only go so far, and an often-unrecognized element is feedback.\nTherefore, in the last sections of the chapter, we explore feedback and implement\nThis chapter covers\n\uf0a1Planning for an LLM and implementing it in \nagents and assistants\n\uf0a1Using the OpenAI Assistants platform via custom \nactions\n\uf0a1Implementing/testing a generic planner on LLMs\n\uf0a1Using the feedback mechanism in advanced \nmodels \n\uf0a1Planning, reasoning, evaluation, and feedback in \nbuilding agentic systems\n",
      "content_length": 1044,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "273\n11.1\nPlanning: The essential tool for all agents/assistants\nit within a planner. You must be familiar with the content of chapter 10, so please\nreview it if you need to, and when you\u2019re ready, let\u2019s begin planning.\n11.1\nPlanning: The essential tool for all agents/assistants\nAgents and assistants who can\u2019t plan and only follow simple interactions are nothing\nmore than chatbots. As we\u2019ve seen throughout this book, our goal isn\u2019t to build bots\nbut rather to build autonomous thinking agents\u2014agents that can take a goal, work\nout how to solve it, and then return with the results. \n Figure 11.1 explains the overall planning process that the agent/assistant will\nundertake. This figure was also presented in chapter 1, but let\u2019s review it now in more\ndetail. At the top of the figure, a user submits a goal. In an agentic system, the agent\ntakes the goal, constructs the plan, executes it, and then returns the results.\nDepending on your interaction with platforms such as ChatGPT and GPTs, Claude,\nand others, you may have already encountered a planning assistant and not even noticed.\nPlanning is becoming ubiquitous and is now built into most commercial platforms to\nAgent interface layer (natural language)\nPlanning: Agent takes the goal and breaks it into tasks.\n1. Collect data.\n2. Annotate data.\n3. Format data, and create report visualizations.\n4. Present report.\n4. Agent presents the report.\nPlease create a report of\nlast year\u2019s sales.\nGPT data layer\nDatabase\n1. Query database using natural\nlanguage.\nGPT functions\nAPIs,\nweb browsing,\nsearch, etc.\nAll communication via natural language\n2. Annotate data by calling semantic\nfunctions using natural language.\nExternal agents\nGPT tools\n3. External agent formats data and\nmay write code to generate visuals.\nFigure 11.1\nThe agent planning process\n",
      "content_length": 1810,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "274\nCHAPTER 11\nAgent planning and feedback\nmake the model appear more intelligent and capable. Therefore, in the next exercise,\nwe\u2019ll look at an example to set a baseline and differentiate between an LLM that can\u2019t\nplan and an agent that can.\n For the next exercise, we\u2019ll use Nexus to demonstrate how raw LLMs can\u2019t plan\nindependently. If you need assistance installing, setting up, and running Nexus, refer\nto chapter 7. After you have Nexus installed and ready, we can begin running it with\nthe Gradio interface, using the commands shown next.\nnexus run gradio\nGradio is an excellent web interface tool built to demonstrate Python machine learn-\ning projects. Figure 11.2 shows the Gradio Nexus interface and the process for creat-\ning an agent and using an agent engine (OpenAI, Azure, and Groq) of your choice.\nYou can\u2019t use LM Studio unless the model/server supports tool/action use. Anthropic\u2019s\nClaude supports internal planning, so for the purposes of this exercise, avoid using\nthis model.\nListing 11.1\nRunning Nexus with the Gradio interface \nFigure 11.2\nCreating a new agent in Nexus\n1. Select to create a new agent.\n2. Name your agent.\n3. Don\u2019t worry about a system\nprompt or instructions.\n4. Select an engine. Note that each engine needs to\nbe con\ufb01gured correctly to show in the list.\n5. Keep the default engine settings, model,\ntemperature,\n, and max tokens.\ntop_p\n6. Don\u2019t worry about retrieval or\nadvanced options for now.\n7. Click Add Agent when your agent\ncon\ufb01guration is complete.\nWhen you start conversing\nwith your agent,  you will need\nto create a new thread or use\nan existing thread.\n",
      "content_length": 1608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "275\n11.1\nPlanning: The essential tool for all agents/assistants\nAfter creating the agent, we want to give it specific actions (tools) to undertake or\ncomplete a goal. Generally, providing only the actions an agent needs to complete its\ngoal is best for a few reasons:\n\uf0a1More actions can confuse an agent into deciding which to use or even how to\nsolve a goal.\n\uf0a1APIs have limits on the number of tools that can be submitted; at the time of\nwriting, hitting this limit is relatively easy.\n\uf0a1Agents may use your actions in ways you didn\u2019t intend unless that\u2019s your goal.\nBe warned, however, that actions can have consequences.\n\uf0a1Safety and security need to be considered. LLMs aren\u2019t going to take over the\nworld, but they make mistakes and quickly get off track. Remember, these\nagents will operate independently and may perform any action.\nWARNING\nWhile writing this book and working with and building agents over\nmany hours, I have encountered several instances of agents going rogue with\nactions, from downloading files to writing and executing code when not\nintended, continually iterating from tool to tool, and even deleting files they\nshouldn\u2019t have. Watching an agent emerge new behaviors using actions can\nbe fun, but things can quickly go astray.\nFor this exercise, we\u2019ll define the goal described in the following listing. \nSearch Wikipedia for pages on {topic} and download each page and save it \nto a file called Wikipedia_{topic}.txt\nThis goal will demonstrate the following actions:\n\uf0a1\nsearch_wikipedia(topic)\u2014Searches Wikipedia and returns page IDs for the\ngiven search term.\n\uf0a1\nget_wikipedia_page(page_id)\u2014Downloads the page content given the page ID.\n\uf0a1\nsave_file\u2014Saves the content to a file.\nSet the actions on the agent, as shown in figure 11.3. You\u2019ll also want to make sure the\nPlanner is set to None. We\u2019ll look at setting up and using planners soon. You don\u2019t\nhave to click Save; the interface automatically saves an agent\u2019s changes.\n After you choose the actions and planner, enter the goal in listing 11.2. Then click\nCreate New Thread to instantiate a new conversation. Substitute the topic you want to\nsearch for in the chat input, and wait for the agent to respond. Here\u2019s an example of\nthe goal filled with the topic, but again, use any topic you like:\nSearch Wikipedia for pages on Calgary and download each page and save it to \na file called Wikipedia_Calgary.txt.\nListing 11.2\nDemonstrating planning: The goal \n",
      "content_length": 2436,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "276\nCHAPTER 11\nAgent planning and feedback\nFigure 11.4 shows the results of submitting the goal to the plain agent. We see the\nagent executed the tool/action to search for the topic but couldn\u2019t execute any\nsteps beyond that. If you recall from our discussion and code example of actions in\nchapter 5, OpenAI, Groq, and Azure OpenAI all support parallel actions but not\nsequential or planned actions.\n The LLM can answer reasonably well if you submit a goal with several parallel\ntasks/actions. However, if the actions are sequential, requiring one step to be depen-\ndent on another, it will fail. Remember, parallel actions are standalone actions that\ncan be run alongside others. \nSelect only the three actions:\nsearch_wikipedia,\nget_wikipedia_page, and\nsave_\ufb01le\nMake sure to select\nNone for the Planner.\nFigure 11.3\nSelecting the actions for the agent and disabling the planner\n",
      "content_length": 881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "277\n11.2\nUnderstanding the sequential planning process\nAnthropic\u2019s Claude and OpenAI Assistants support sequential action planning. This\nmeans both models can be called with sequential plans, and the model will execute them\nand return the results. In the next section, we\u2019ll explore sequential planning and then\ndemonstrate it in action.\n11.2\nUnderstanding the sequential planning process\nIn the next exercise, we\u2019ll ask an OpenAI assistant to solve the same goal. If you have\nAnthropic/Claude credentials and have the engine configured, you can also try this\nexercise with that model. \n Figure 11.5 shows the difference between executing tasks sequentially (planning)\nand using iteration. If you\u2019ve used GPTs, assistants, or Claude Sonnet 3.5, you\u2019ve likely\nalready experienced this difference. These advanced tools already incorporate plan-\nning by prompt annotations, advanced training, or combining both. \n As LLM and chat services evolve, most models will likely natively support some\nform of planning and tool use. However, most models, including GPT-4o, only sup-\nport action/tool use today. \n Let\u2019s open the GPT Assistants Playground to demonstrate sequential planning in\naction. If you need help, refer to the setup guide in chapter 6. We\u2019ll use the same goal\nbut, this time, run it against an assistant (which has built-in planning).\nFigure 11.4\nThe results from trying to get the agent/LLM to complete the goal\n",
      "content_length": 1422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "278\nCHAPTER 11\nAgent planning and feedback\nAfter you launch the Playground, create a new assistant, and assign it the search_\nwikipedia, get_wikipedia_page, and save_file actions. Figure 11.6 shows the results\nof entering the goal to the assistant. As you can see, the assistant completed all the\ntasks behind the scenes and responded with the user\u2019s final requested output, achiev-\ning the goal.\n To demonstrate the effectiveness of the OpenAI Assistant\u2019s planner, we added\nanother task, summarizing each page, to the goal. The inserted task didn\u2019t have a\nfunction/tool, but the assistant was savvy enough to use its ability to summarize the\ncontent. You can see the output of what the assistant produced by opening the [root\nfolder]assistants_working_folder/Wikipedia_{topic}.txt file and reviewing the\ncontents. Now that we understand how LLMs function without planners and plan-\nning, we can move on to creating our planners in the next section.\n11.3\nBuilding a sequential planner\nLLM tools such as LangChain and Semantic Kernel (SK) have many planners using\nvarious strategies. However, writing our planner is relatively easy, and Nexus also sup-\nports a plugin-style interface allowing you to add other planners from tools such as\nLangChain and SK, or your derivatives.\nSingle prompt as a goal:\nsearch Wikipedia for topic, return\nthe top page, and save to a \ufb01le.\nSequential execution of a goal\nSingle prompt as a goal:\nsearch Wikipedia for topic, return\nthe top page, and save to a \ufb01le.\nIterative execution of a goal\nTasks/plan\nReturns a list of page IDs\nsearch_wikipedia(topic)\nReturns the page content\nget_wikipedia_page(page_id)\nCollects pages\nsave_ le\n\ufb01\nAll pages combined\nIn a planner, this\naggregation is\nperformed\ninternally.\nAll tasks are\nperformed\ninternally by\nthe agent.\nUser submits a goal.\nsearch_wikipedia(topic)\nResults are displayed, and the\nagent asks to continue to the\nnext step.\nReturns a list of page IDs\nget_wikipedia_page(page_id)\nUser responds\nwith continue.\nReturns are collected and\ndisplayed to the user. The agent\nasks for further continuation.\nReturns the page content\nsave_ le\n\ufb01\nAll pages combined\nUser responds\nwith continue.\nGoal is complete, and user has the output.\nIterative\nexecution\nis default\nbehavior\nfor an LLM.\nSequential\n(planned)\nexecution is\nstandard in\nGPT\nAssistants\nand Claude.\nFigure 11.5\nThe difference between iterative and planned execution\n",
      "content_length": 2398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "279\n11.3\nBuilding a sequential planner\nPlanners may sound complicated, but they are easily implemented through prompt\nengineering strategies that incorporate planning and reasoning. In chapter 10, we\ncovered the basics of reasoning and deriving plans, and now we can put those skills to\ngood use.\n Listing 11.3 shows a sequential planner derived from the SK, which is extended to\nincorporate iteration. Prompt annotation planners like those shown in the listing can\nbe adapted to fit specific needs or be more general like those shown. This planner\nuses JSON, but planners could use any format an LLM understands, including code.\n \n \n \nNo instructions are needed, but\nbe sure to select the actions:\nsearch_wikipedia,\nget_wikipedia_page, and\nsave_\ufb01le\nClick the Logs tab as the assistant is\nrunning to see it work through the actions.\nIn this example, the goal was extended\nto ask the assistant to summarize the\ncontent \ufb01rst before saving it to a \ufb01le.\nThis is a single response from\nthe assistant, which also\nincluded summarization of\nthe content and con\ufb01rmation\nof the \ufb01le save.\nNexus and Playground have\na feature that con\ufb01rms the\n\ufb01le indeed was saved.\nFigure 11.6\nThe assistant processing the goal and outputting the results\n",
      "content_length": 1226,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "280\nCHAPTER 11\nAgent planning and feedback\nYou are a planner for Nexus.    \nYour job is to create a properly formatted JSON plan step by step, to \nsatisfy the goal given.\nCreate a list of subtasks based off the [GOAL] provided.\nEach subtask must be from within the [AVAILABLE FUNCTIONS] list. Do not \nuse any functions that are not in the list.\nBase your decisions on which functions to use from the description and the \nname of the function.\nSometimes, a function may take arguments. Provide them if necessary.\nThe plan should be as short as possible.\nYou will also be given a list of corrective, suggestive and epistemic \nfeedback from previous plans to help you make your decision.\nFor example:\n[SPECIAL FUNCTIONS]    \nfor-each- prefix\ndescription: execute a function for each item in a list\nargs: \n- function: the function to execute\n- list: the list of items to iterate over\n- index: the arg name for the current item in the list\n[AVAILABLE FUNCTIONS]\nGetJokeTopics\ndescription: Get a list ([str]) of joke topics\nEmailTo\ndescription: email the input text to a recipient\nargs:\n- text: the text to email\n- recipient: the recipient's email address. Multiple addresses may be \nincluded if separated by ';'.\nSummarize\ndescription: summarize input text\nargs:\n- text: the text to summarize\nJoke\ndescription: Generate a funny joke\nargs:\n- topic: the topic to generate a joke about\n[GOAL]\n\"Get a list of joke topics and generate a different joke for each topic. \nEmail the jokes to a friend.\"\n[OUTPUT]\n    {        \n        \"subtasks\": [\n            {\"function\": \"GetJokeTopics\"},\nListing 11.3\nbasic_nexus_planner.py \nThe preamble instructions telling the \nagent how to process the examples\nBeginning of the \nthree (few-shot) \nexamples\n",
      "content_length": 1732,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "281\n11.3\nBuilding a sequential planner\n            {\"function\": \"for-each\",\n             \"args\": {\n                       \"list\": \"output_GetJokeTopics\",\n                       \"index\": \"topic\", \n                       \"function\": \n                                  {\n                                   \"function\": \"Joke\",\n                                   \"args\": {\"topic\": \"topic\"}}}},\n            {\n             \"function\": \"EmailTo\",\n              \"args\": {\n                        \"text\": \"for-each_output_GetJokeTopics\"\n                       ecipient\": \"friend\"}}\n        ]\n    }\n# 2 more examples are given but omitted from this listing\n[SPECIAL FUNCTIONS]    \nfor-each\ndescription: execute a function for each item in a list\nargs: \n- function: the function to execute\n- iterator: the list of items to iterate over\n- index: the arg name for the current item in the list  \n[AVAILABLE FUNCTIONS]    \n{{$available_functions}}\n[GOAL]\n{{$goal}}    \nBe sure to only use functions from the list of available functions. \nThe plan should be as short as possible. \nAnd only return the plan in JSON format.\n[OUTPUT]    \nFigure 11.7 shows the process of building and running a planning prompt, from\nbuilding to execution to finally returning the results to the user. Planners work by\nbuilding a planning prompt, submitting it to an LLM to construct the plan, parsing\nand executing the plan locally, returning the results to an LLM to evaluate and sum-\nmarize, and finally returning the final output back to the user.\n It\u2019s essential to notice a few subtle details about the planning process. Typically, the\nplan is built in isolation by not adding context history. This is done to focus on the\ngoal because most planning prompts consume many tokens. Executing the functions\nwithin the executor is usually done in a local environment and may include calling\nAPIs, executing code, or even running machine learning models.\n Listing 11.4 shows the code for the create_plan function from the BasicNexus-\nPlanner class; tools such as LangChain and SK use similar patterns. The process loads\nthe agent\u2019s actions as a string. The goal and available functions list are then inserted into\nthe planner prompt template using the PromptTemplateManager, which is just a wrapper\nAdds the for-each \nspecial iterative \nfunction\nAvailable functions are \nautopopulated from the \nagent\u2019s list of available \nfunctions.\nThe goal is inserted here.\nWhere the agent is expected \nto place the output\n",
      "content_length": 2471,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "282\nCHAPTER 11\nAgent planning and feedback\nfor the template-handling code. Template handling is done with simple regex but can\nalso be more sophisticated using tools such as Jinja2, Handlebars, or Mustache.\ndef create_plan(self, nexus, agent, goal: str, prompt: str = PROMPT) -> Plan:\n        selected_actions = nexus.get_actions(agent.actions)\n        available_functions_string = \"\\n\\n\".join(\n            format_action(action) for action in selected_actions\n        )    \n        \n        context = {}    \n        context[\"goal\"] = goal\n        context[\"available_functions\"] = available_functions_string\nListing 11.4\nbasic_nexus_planner.py (create_plan)\nExecution is typically done\nin an environment local to\nthe agent.\nThe planning prompt\nadds the list of available\nfunctions and the goal to\nthe prompt template.\nLLM\nLLM\nLLM\nChat history\nAssembled results\nThe plan\nGoal\n\u201cFetch, summarize, and send\u201d\nResults of executed goal\nExecute the function to query the\ndatabase for access to an API.\nChat history\nSend the goal along with\nplanning prompt.\nNo history of other\ncontext is sent.\nPlan executor\nEach step of the plan is executed,\nand results are passed through LLM\nalong with history.\nUse the output and other arguments\nto execute the actions/functions.\nResults are again sent through\nthe LLM to summarize the output\nfor the user.\nPlanning prompt\nFigure 11.7\nThe planning process for creating and executing a plan\nLoads the agent\u2019s available actions and\nformats the result string for the planner\nThe context will be injected into\nthe planner prompt template.\n",
      "content_length": 1563,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "283\n11.3\nBuilding a sequential planner\n        ptm = PromptTemplateManager()    \n        prompt = ptm.render_prompt(prompt, context)\n        plan_text = nexus.execute_prompt(agent, prompt)    \n        return Plan(prompt=prompt, \n                    goal=goal, \n                    plan_text=plan_text)    \nThe code to execute the plan, shown in listing 11.5, parses the JSON string and exe-\ncutes the functions. When executing the plan, the code detects the particular for-each\nfunction, which iterates through a list and executes each element in a function. The\nresults of each function execution are added to the context. This context is passed to\neach function call and returned as the final output.\ndef execute_plan(self, nexus, agent, plan: Plan) -> str:\n        context = {}\n        plan = plan.generated_plan\n        for task in plan[\"subtasks\"]:    \n            if task[\"function\"] == \"for-each\":    \n                list_name = task[\"args\"][\"list\"]\n                index_name = task[\"args\"][\"index\"]\n                inner_task = task[\"args\"][\"function\"]\n                list_value = context.get(list_name, [])\n                for item in list_value:\n                    context[index_name] = item\n                    result = nexus.execute_task(agent, inner_task, context)\n                    context[f\"for-each_{list_name}_{item}\"] = result\n                for_each_output = [    \n                    context[f\"for-each_{list_name}_{item}\"] \u27a5\n                      for item in list_value\n                ]\n                context[f\"for-each_{list_name}\"] = for_each_output\n                \n                for item in list_value:    \n                    del context[f\"for-each_{list_name}_{item}\"]\n            else:\n                result = nexus.execute_task(agent,\n                                            task,\n                                            context)    \n                context[f\"output_{task['function']}\"] = result\n        return context    \nListing 11.5\nbasic_nexus_planner.py (execute_plan)\nA simple template \nmanager, similar in \nconcept to Jinja2, \nHandlebars, or \nMustache\nSends the filled-in \nplanner prompt to \nthe LLM\nThe results (the plan) are wrapped in a \nPlan class and returned for execution.\nIterates through each \nsubtask in the plan\nHandles functions that\nshould be iterated\nover and adds full list\nof results to the\ncontext\nRemoves \nindividual \nfor-each \ncontext \nentries\nGeneral task \nexecution\nReturns the full context, which includes \nthe results of each function call\n",
      "content_length": 2521,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "284\nCHAPTER 11\nAgent planning and feedback\nThe returned context from the entire execution is sent in a final call to the LLM,\nwhich summarizes the results and returns a response. If everything goes as planned,\nthe LLM will respond with a summary of the results. If there is an error or something\nis missing, the LLM may try to fix the problem or inform the user of the error.\n Let\u2019s now open Nexus again and test a planner in operation. Load up the same\nagent you used last time, but select the planner under the Advanced options this time,\nas shown in figure 11.8. Then, enter the goal prompt as you did before, and let the\nagent take it away.\nAfter a few minutes, the agent returns with the saved file, and in some cases, it may\nprovide extra information, such as the next steps and what to do with the output. This\nis because the agent was given a high-level overview of what it accomplished. Remem-\nber, though, that plan execution is done at the local level, and only context, plan, and\ngoal were sent to the LLM.\nGoal requiring multiple\nsteps to complete\nThe agent will execute the tasks and\n\ufb01nish by outputting the \ufb01le as well\nas suggesting next steps.\nSelect the basic\nplanner.\nFigure 11.8\nThe results from requesting to complete the goal in Nexus using the basic planner\n",
      "content_length": 1280,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "285\n11.4\nReviewing a stepwise planner: OpenAI Strawberry\n This means that plan execution can be completed by any process, not necessarily\nby the agent. Executing a plan outside the LLM reduces the tokens and tool use the\nagent needs to perform. This also means that an LLM doesn\u2019t need to support tools\nusage to use a planner.\n Internally, when a planner is enabled within Nexus, the agent engine tool is\nbypassed. Instead, the planner completes the action execution, and the agent is only\naware of the actions through the passing of the output context. This can be good for\nmodels that support tool use but can\u2019t plan. However, a planner may limit functional-\nity for models that support both tool use and planning, such as Claude.\n In general, you\u2019ll want to understand the capabilities of the LLM you\u2019re using. If\nyou\u2019re unsure of those details, then a little trial and error can also work. Ask the agent to\ncomplete a multistep goal with and without planning enabled, and then see the results.\n Planning allows agents to complete multiple sequential tasks to achieve more com-\nplex goals. The problem with external or prompt planning is that it bypasses the feed-\nback iteration loop, which can help correct problems quickly. Because of this, OpenAI\nand others are now directly integrating reasoning and planning at the LLM level, as\nwe\u2019ll see in the next section.\n11.4\nReviewing a stepwise planner: OpenAI Strawberry\nThe release of the o1-preview model, code named Strawberry, introduced a dra-\nmatic shift in the type of LLMs becoming available for agentic systems. Strawberry\nwas not only proclaimed to be more efficient at math, science, and general calcula-\ntion tasks but also able to engage in reasoning, planning, evaluation, and feedback\ndirectly in the LLM. \n Consider our time travel problem from chapter 10 and shown again in figure 11.9.\nIf you recall, this problem was difficult to solve using GPT-4 and other similar LLMs.\nHowever, with the application of reasoning and feedback, we were able to produce\noutput that was occasionally correct. \n As an experiment, enter this problem into ChatGPT using the o1-preview model,\nas shown in listing 11.6. Sit back for a few seconds and wait for the answer. Yep, the\nmodel still gets it wrong.\nIn a sci-fi film, Alex is a time traveler who decides to go back in time \nto witness a famous historical battle that took place 100 years ago, \nwhich lasted for 10 days. He arrives three days before the battle starts. \nHowever, after spending six days in the past, he jumps forward in time \nby 50 years and stays there for 20 days. Then, he travels back to \nwitness the end of the battle. How many days does Alex spend in the past \nbefore he sees the end of the battle?\nWhile it may be somewhat disappointing to see the model get the wrong answer, it\ndoes, however, do a far better job of breaking down the problem and demonstrating\nListing 11.6\nTime travel reasoning/planning problem\n",
      "content_length": 2940,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "286\nCHAPTER 11\nAgent planning and feedback\nits answer. Listing 11.7 shows the sample output from posing the problem in listing\n11.6 to the Strawberry model. Note, you may get a different answer because of the sto-\nchastic nature of the LLM.\nTo determine how many days Alex spends in the past before witnessing the \nend of the battle, let's reconstruct the timeline step by step:\nArrival in the Past:\nAlex arrives 3 days before the battle begins.\nHe spends these 3 days waiting for the battle to start.\nDuring the Battle:\nThe battle starts, and Alex experiences the first 3 days of it.\nAt this point, he has spent a total of 6 days in the past (3 days before \nthe battle and 3 days during the battle).\nTime Jump Forward:    \nAfter these 6 days, Alex jumps 50 years forward in time.\nHe spends 20 days in this future time.\nReturn to the Past:\nAlex then returns to the past to witness the end of the battle.\nSince the battle lasts 10 days and he left after day 3 of the battle, \nthere are 7 days remaining in the battle.\nHowever, because he was away for 20 days, which is longer than the \nremaining 7 days of the battle, he must return to a point in time before \nthe battle ends to witness its conclusion.\nTo see the end of the battle, he returns to the past on the last day of \nListing 11.7\no1-preview response to time travel problem\nIn a sci-\ufb01\ufb01lm, Alex is a time traveler who decides to go back in\ntime to witness a famous historical battle that took place 100 years ago,\nwhich lasted for 10 days. He arrives three days before the battle starts.\nHowever, after spending six days in the past, he jumps forward in time by\n50 years and stays there for 20 days. Then, he travels back to witness the\nend of the battle. How many days does Alex spend in the past before he\nsees the end of the battle?\nAlex\nTimeline\nPresent\nHistoric\nbattle starts\nHistoric\nbattle ends\nAlex\narrives\nJumps back in time 100 years (3 days before the battle)\nspends 6 days\njumps forward in time 50 years\n20 days\njumps back to the present\nAlex\nleaves\nAlex\nreturns?\nAlex\narrives\nAlex\nleaves\n???\nThe problem statement:\nThe statement is vague as\nto what time Alex returns\nto the battle. Is it the same\ntime he left?\njumps back to witness end of battle\nbattle lasts 10 days\nFigure 11.9\nThe time travel problem, revisited\nIt becomes obvious \nwhere the model is \nmaking the error.\n",
      "content_length": 2342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "287\n11.4\nReviewing a stepwise planner: OpenAI Strawberry\nthe battle.\nFinal Days Spent Before the Battle Ends:\nUpon returning, he spends 1 more day in the past to witness the battle's \nend.\nTotal Days Spent in the Past Before Seeing the End:\n6 days before jumping forward in time.\n1 day after returning to the past.\nTotal: 6 + 1 = 7 days.\nAnswer: 7\nBecause we know the right answer is 27, we know the LLM is wrong, but if we didn\u2019t,\nwe could just as easily assume that the work and reasoning were all correct. Problems\nlike this can happen when we remove feedback in LLM interactions and agentic sys-\ntems. Feedback can guide the model to correct itself. \n However, what if we didn\u2019t know the correct answer was 27 (26, if you assume he\ndoesn\u2019t spend the day to witness the battle) and assumed the LLM or agent was cor-\nrect? Well, this is a problem we can rectify with a couple of simple prompts that can\nengage the LLM in reasoning and planning feedback. However, these techniques are\nmore effective with LLMs or wrappers such as the OpenAI Assistants, which provide\nreasoning and planning within the model.\n What we want to do is provide feedback to the LLM, but understanding what that\nfeedback is will likely be difficult for us. Fortunately, we can elicit feedback directly\nfrom the LLM, provided we give the correct answer. Listing 11.8 shows how to gener-\nate constructive feedback from the LLM concerning our time travel problem.\nthe correct answer is 27, please review what you did wrong and suggest \nfeedback you could give yourself when trying to solve similar future problems\nEntering this after the model answers the question wrong will generate feedback that\nyou can use to guide the model through prompting or as part of system instructions.\nListing 11.9 shows an example of the feedback provided by o1-preview. You can then\nextract this feedback and augment the instructions the next time you want to tackle\ncomplex time travel problems.\nFeedback for Future Problems\nUnderstanding Time Frames in Time Travel:\nRelative Time Frames: Always consider the time periods relative to the \noriginal point of reference. In this problem, even though Alex jumps \nforward 50 years, he is still in the past relative to his original \npresent time.\nIncluding All Relevant Periods: \nEnsure that all time periods spent in the past (or future) are included \nin the total calculation, even if they involve moving to different \npoints in time.\nTime Continuity: \nListing 11.8\nGenerating feedback\nListing 11.9\nGenerated feedback\n",
      "content_length": 2522,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "288\nCHAPTER 11\nAgent planning and feedback\nRemember that while the traveler moves through time, events in other \ntime periods continue to progress unless specified otherwise.\nApplication:\nCareful Reading: \nPay close attention to the details provided, such as the durations and \nthe sequences of time jumps.\nVisualization: \nIt may help to draw a timeline to visualize the sequence of events and \ntime periods involved.\nDouble-Checking Calculations: \nAfter solving, revisit each step to ensure all time periods are \naccounted for correctly.\nThis feedback technique will consistently work on models such as o1-preview, but\nother models may still struggle to answer correctly, even given this feedback. Over\ntime, as models become smarter, this technique will likely generally work on most\nmodels. However, this feedback mechanism will likely be essential even as models\nget progressively brighter. because language is nuanced, and not every problem we\nchallenge LLMs with may have an obvious absolute answer. Take our example prob-\nlem, for instance. This problem is an excellent example of requiring the problem\nsolver to make assumptions and draw correlations from the question. There are still\nplenty of areas in science, from geology to behavioral science, where answering the\nsame problem may yield a range of answers. Let\u2019s look next at a few techniques for\nhow the application of reasoning, planning, evaluation, and feedback can be\napplied to agentic systems.\n11.5\nApplying planning, reasoning, evaluation, and \nfeedback to assistant and agentic systems\nIn recent chapters, we\u2019ve examined how the agentic components of planning, reason-\ning, feedback, and evaluation can be implemented. Now we look at how, when, and\nwhere those components can be integrated into assistant and agentic systems for real-\ntime production, research, or development.\n While not all of these components may fit the same into every application, it\u2019s use-\nful to understand where and when to apply which component. In the next section, we\nlook at how planning can be integrated into assistant/agentic systems.\n11.5.1 Application of assistant/agentic planning\nPlanning is the component where an assistant or agent can plan to undertake a set of\ntasks, whether they are in series, parallel, or some other combination. We typically\nassociate planning with tool use, and, rightfully, any system using tools will likely want\na capable planner. However, not all systems are created equally, so in table 11.1, we\u2019ll\nreview where, when, and how to implement planners.\n \n",
      "content_length": 2543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "289\n11.5\nApplying planning, reasoning, evaluation, and feedback to assistant and agentic sys-\nTable 11.1 shows several varied application scenarios in which we may find an assis-\ntant or agent deployed to assist in some capacity. To provide further information\nand guidance, this list provides more details about how planning may be employed\nin each application:\n\uf0a1Personal assistant\u2014While this application has been slow to roll out, LLM per-\nsonal assistants promise to surpass Alexa and Siri in the future. Planning will be\nessential to these new assistants/agents to coordinate numerous complex tasks\nand execute tools (actions) in series or parallel. \n\uf0a1Customer service bot\u2014Due to the controlled nature of this environment, it\u2019s\nunlikely that assistants engaged directly with customers will have controlled and\nvery specific tools use. This means that these types of assistants will likely not\nrequire extensive planning.\n\uf0a1Autonomous agent\u2014As we\u2019ve seen in previous chapters, agents with the ability to\nplan can complete a series of complex tasks for various goals. Planning will be\nan essential element of any autonomous agentic system.\n\uf0a1Collaborative workflows\u2014Think of these as agents or assistants that sit alongside\ncoders or writers. While these workflows are still in early development, think of\na workflow where agents are automatically tasked with writing and executing\ntest code alongside developers. Planning will be an essential part of executing\nthese complex future workflows.\nTable 11.1\nWhen and where planning is employed and used in various applications\nApplication\nImplemented\nEnvironment\nPurpose\nTiming\nConfiguration\nPersonal \nassistant\nAt or within the \nLLM\nPersonal \ndevice\nFacilitate tool \nuse\nDuring the \nresponse\nAs part of the \nprompt or LLM\nCustomer \nservice bot\nNot typical; \nrestricted \nenvironment\nRestricted \nenvironment, \nno tool use\nAutonomous \nagent\nAs part of the \nagent prompt \nand within the \nLLM\nServer or \nservice\nFacilitate com-\nplex tool use \nand task plan-\nning\nAs part of con-\nstructing the \nagent and/or \nduring the \nresponse\nWithin the \nagent or LLM\nCollaborative \nworkflows\nAs part of \nthe LLM\nShared can-\nvas or coding\nFacilitate com-\nplex tool use\nDuring the \nresponse\nWithin the LLM\nGame AI\nAs part of \nthe LLM\nServer or \napplication\nComplex \ntool use and \nplanning\nBefore or \nduring the \nresponse\nWithin the LLM\nResearch\nAnywhere\nServer \nFacilitate tool \nuse and engage \nin complex \ntask workflows\nBefore, \nduring, and \nafter response \ngeneration\nAnywhere\n",
      "content_length": 2508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "290\nCHAPTER 11\nAgent planning and feedback\n\uf0a1Game AI\u2014While applying LLMs to games is still in early stages, it isn\u2019t hard to\nimagine in-game agents or assistants that can assist or challenge the player. Giv-\ning these agents the ability to plan and execute complex workflows could dis-\nrupt how and with whom we play games.\n\uf0a1Research\u2014Similar to collaborative workflows, these agents will be responsible\nfor deriving new ideas from existing sources of information. Finding that infor-\nmation will likely be facilitated through extensive tool use, which will benefit\nfrom coordination of planning.\nAs you can see, planning is an essential part of many LLM applications, whether\nthrough coordination of tool use or otherwise. In the next section, we look at the next\ncomponent of reasoning and how it can be applied to the same application stack.\n11.5.2 Application of assistant/agentic reasoning\nReasoning, while often strongly associated with planning and task completion, is a\ncomponent that can also stand by itself. As LLMs mature and get smarter, reasoning is\noften included within the LLM itself. However, not all applications may benefit from\nextensive reasoning, as it often introduces a thinking cycle within the LLM response.\nTable 11.2 describes at a high level how the reasoning component can be integrated\nwith various LLM application types.\nTable 11.2\nWhen and where reasoning is employed and used in various applications\nApplication\nImplemented\nEnvironment\nPurpose\nTiming\nConfiguration\nPersonal \nassistant\nWithin the LLM\nPersonal \ndevice\nBreaking down \nwork into steps\nDuring the \nresponse\nAs part of the \nprompt or LLM\nCustomer \nservice bot\nNot typical; \nusually just \ninformational\nLimited tool \nuse and need \nfor composite \ntool use\nAutono-\nmous \nagent\nAs part of the \nagent prompt \nand within \nthe LLM\nServer or \nservice\nFacilitate \ncomplex tool use \nand task planning\nAs part of \nLLM, external \nreasoning not \nwell suited \nWithin the \nagent or LLM\nCollabora-\ntive work-\nflows\nAs part of the \nLLM\nShared can-\nvas or coding\nAssists in break-\ning work down\nDuring the \nresponse\nWithin the \nLLM\nGame AI\nAs part of \nthe LLM\nServer or \napplication\nEssential for \nundertaking com-\nplex actions\nBefore or \nduring the \nresponse\nWithin the \nLLM\nResearch\nAnywhere\nServer \nUnderstand how to \nsolve complex \nproblems and \nengage in complex \ntask workflows\nBefore, during, \nand after \nresponse \ngeneration\nAnywhere\n",
      "content_length": 2418,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "291\n11.5\nApplying planning, reasoning, evaluation, and feedback to assistant and agentic sys-\nTable 11.2 shows several varied application scenarios in which we may find an assis-\ntant or agent deployed to assist in some capacity. To provide further information\nand guidance, this list provides more details about how reasoning may be employed\nin each application:\n\uf0a1Personal assistant\u2014Depending on the application, the amount of reasoning an\nagent employs may be limited. Reasoning is a process that requires the LLM to\nthink through a problem, and this often requires longer response times depend-\ning on the complexity of the problem and the extent of the prompt. In many\nsituations, responses intended to be closer to real-time reasoning may be dis-\nabled or turned down. While this may limit the complexity at which an agent\ncan interact, limited or no reasoning can improve response times and increase\nuser enjoyment.\n\uf0a1Customer service bot\u2014Again, because of the controlled nature of this environ-\nment, it\u2019s unlikely that assistants engaged directly with customers will need to\nperform complex or any form of reasoning. \n\uf0a1Autonomous agent\u2014While reasoning is a strong component of autonomous\nagents, we still don\u2019t know how much reasoning is too much. As models such as\nStrawberry become available for agentic workflows, we can gauge at what\npoint extensive reasoning may not be needed. This will surely be the case for\nwell-defined autonomous agent workflows.\n\uf0a1Collaborative workflows\u2014Again, applying reasoning creates an overhead in the LLM\ninteraction. Extensive reasoning may provide benefits for some workflows, while\nother well-defined workflows may suffer. This may mean that these types of work-\nflows will benefit from multiple agents\u2014those with reasoning and those without. \n\uf0a1Game AI\u2014Similar to other applications, heavy-reasoning applications may not\nbe appropriate for most game AIs. Games will especially require LLM response\ntimes to be quick, and this will surely be the application of reasoning for gen-\neral tactical agents. Of course, that doesn\u2019t preclude the use of other reasoning\nagents that may provide more strategic control.\n\uf0a1Research\u2014Reasoning will likely be essential to any complex research task for sev-\neral reasons. A good example is the application of the Strawberry model, which\nwe\u2019ve already seen in research done in mathematics and the sciences.\nWhile we often consider reasoning in tandem with planning, there may be conditions\nwhere the level at which each is implemented may differ. In the next section we con-\nsider the agent pillar of evaluation of various applications.\n11.5.3 Application of evaluation to agentic systems\nEvaluation is the component of agentic/assistant systems that can guide how well the\nsystem performs. While we demonstrated incorporating evaluation in some agentic\nworkflows, evaluation is often an external component in agentic systems. However, it\u2019s\nalso a core component of most LLM applications and not something that should be\n",
      "content_length": 2998,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "292\nCHAPTER 11\nAgent planning and feedback\noverlooked in most developments. Table 11.3 describes at a high level how the evalua-\ntion component can be integrated with various LLM application types.\nTable 11.3 shows several varied application scenarios in which we may find an assis-\ntant or agent deployed to assist in some capacity. To provide further information\nand guidance, this list provides more details about how evaluation may be employed\nin each application:\n\uf0a1Personal assistant\u2014In most cases, an evaluation component will be used to process\nand guide the performance of agent responses. In systems primarily employing\nretrieval augmented generation (RAG) for document exploration, the evaluation\nindicates how well the assistant responds to information requests. \n\uf0a1Customer service bot\u2014Evaluating service bots is critical to understanding how well\nthe bot responds to customer requests. In many cases, a strong RAG knowledge\nelement may be an element of the system that will require extensive and ongoing\nevaluation. Again, with most evaluation components, this element is external to\nTable 11.3\nWhen and where evaluation is employed and used in various applications\nApplication\nImplemented\nEnvironment\nPurpose\nTiming\nConfiguration\nPersonal \nassistant\nExternal\nServer\nDetermine how \nwell the system \nis working\nAfter the \ninteraction\nOften devel-\noped externally\nCustomer \nservice bot\nExternal \nmonitor\nServer\nEvaluate the suc-\ncess of each \ninteraction\nAfter the \ninteraction\nExternal to the \nagent system\nAutonomous \nagent\nExternal or \ninternal\nServer or \nservice\nDetermine the \nsuccess of the \nsystem after \nor during task \ncompletion\nAfter the \ninteraction \nExternal or \ninternal \nCollaborative \nworkflows\nExternal\nShared can-\nvas or coding\nEvaluate the \nsuccess of the \ncollaboration\nAfter the \ninteraction\nExternal service\nGame AI\nExternal or \ninternal\nServer or \napplication\nEvaluate the \nagent or evalu-\nate the success \nof a strategy or \naction\nAfter the \ninteraction\nExternal or \nas part of the \nagent or another \nagent\nResearch\nCombined \nmanual and \nLLM\nServer and \nhuman\nEvaluate the \noutput of the \nresearch \ndeveloped\nAfter the \ngenerated \noutput\nDepends on the \ncomplexity of \nthe problem \nand research \nundertaken\n",
      "content_length": 2241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "293\n11.5\nApplying planning, reasoning, evaluation, and feedback to assistant and agentic sys-\nthe main working system and is often run as part of monitoring general perfor-\nmance over several metrics.\n\uf0a1Autonomous agent\u2014In most cases, a manual review of agent output will be a pri-\nmary guide to the success of an autonomous agent. However, in some cases,\ninternal evaluation can help guide the agent when it\u2019s undertaking complex\ntasks or as a means of improving the final output. Multiple agent systems, such\nas CrewAI and AutoGen, are examples of autonomous agents that use internal\nfeedback to improve the generated output.\n\uf0a1Collaborative workflows\u2014In most direct cases, manual evaluation is ongoing\nwithin these types of workflows. A user will often immediately and in near\nreal time correct the assistant/agent by evaluating the output. Additional\nagents could be added similarly to autonomous agents for more extensive\ncollaborative workflows.\n\uf0a1Game AI\u2014Evaluation will often be broken down into development evaluation\u2014\nevaluating how the agent interacts with the game\u2014and in-game evaluation, evalu-\nating how well an agent succeeded at a task. Implementing the later evaluation\nform is similar to autonomous agents but aims to improve some strategies or\nexecution. Such in-game evaluations would also likely benefit from memory\nand a means of feedback.\n\uf0a1Research\u2014Evaluation at this level generally occurs as a manual effort after com-\npleting the research task. An agent could employ some form of evaluation simi-\nlar to autonomous agents to improve the generated output, perhaps even\ncontemplating internally how evaluation of the output could be extended or\nfurther researched. Because this is currently a new area for agentic develop-\nment, how well this will be executed remains to be seen.\nEvaluation is an essential element to any agentic or assistant system, especially if that\nsystem provides real and fundamental information to users. Developing evaluation sys-\ntems for agents and assistants is likely something that could or should have its own\nbook. In the final section of this chapter, we\u2019ll look at feedback implementation for\nvarious LLM applications.\n11.5.4 Application of feedback to agentic/assistant applications\nFeedback as a component of agentic systems is often, if not always, implemented as\nan external component\u2014at least for now. Perhaps confidence in evaluation systems\nmay improve to the point where feedback is regularly incorporated into such sys-\ntems. Table 11.4 showcases how feedback can be implemented into various LLM\napplications.\n \n \n \n",
      "content_length": 2580,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "294\nCHAPTER 11\nAgent planning and feedback\nTable 11.4 shows several application scenarios in which we may find an assistant or agent\ndeployed to assist in some capacity. To provide further information and guidance, this list\nprovides more details about how feedback may be employed in each application:\n\uf0a1Personal assistant\u2014If the assistant or agent interacts with the user in a chat-style\ninterface, direct and immediate feedback can be applied by the user. Whether\nthis feedback is sustained over future conversations or interactions, it usually\ndevelops within agentic memory. Assistants such as ChatGPT now incorporate\nmemory and can benefit from explicit user feedback.\n\uf0a1Customer service bot\u2014User or system feedback is typically provided through a survey\nafter the interaction has completed. This usually means that feedback is regulated\nto an external system that aggregates the feedback for later improvements.\n\uf0a1Autonomous agent\u2014Much like bots, feedback within autonomous agents is typi-\ncally regulated to after the agent has completed a task that a user then reviews.\nThe feedback mechanism may be harder to capture because many things can\nTable 11.4\nWhen and where feedback is employed and used in various applications\nApplication\nImplemented\nEnvironment\nPurpose\nTiming\nConfiguration\nPersonal \nassistant\nExternal or \nby the user\nAggregated to \nthe server or \nas part of the \nsystem\nProvides means \nof system \nimprovement\nAfter or during \nthe interaction\nInternal and \nexternal\nCustomer \nservice bot\nExternal \nmonitor\nAggregated to \nthe server\nQualifies and \nprovides a means \nfor system \nimprovement\nAfter the \ninteraction\nExternal to the \nagent system\nAutonomous \nagent\nExternal \nAggregated at \nthe server\nProvides a means \nfor system \nimprovement\nAfter the \ninteraction \nExternal \nCollaborative \nworkflows\nWhile \ninteracting\nShared can-\nvas or coding\nProvides a mecha-\nnism for immedi-\nate feedback\nDuring the \ninteraction\nExternal \nservice\nGame AI\nExternal or \ninternal\nServer or \napplication\nAs part of internal \nevaluation feed-\nback provided \nfor dynamic \nimprovement\nAfter or during \nthe interaction\nExternal or \nas part of \nthe agent or \nanother agent\nResearch\nCombined \nmanual and \nLLM\nServer and \nhuman\nEvaluate the out-\nput of the research \ndeveloped\nAfter the \ngenerated \noutput\nDepends on \nthe complexity \nof the problem \nand the \nresearch \nundertaken\n",
      "content_length": 2374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "295\n11.5\nApplying planning, reasoning, evaluation, and feedback to assistant and agentic sys-\nbe subjective. Methods explored in this chapter for producing feedback can be\nused within prompt engineering improvements.\n\uf0a1Collaborative workflows\u2014Similar to the personal assistant, these types of appli-\ncations can benefit from immediate and direct feedback from the user. Again,\nhow this information is persisted across sessions is often an implementation of\nagentic memory.\n\uf0a1Game AI\u2014Feedback can be implemented alongside evaluation through addi-\ntional and multiple agents. This feedback form may again be single-use and\nexist within the current interaction or may persist as memory. Imagine a game\nAI that can evaluate its actions, improve those with feedback, and remember\nthose improvements. While this pattern isn\u2019t ideal for games, it will certainly\nimprove the gameplay experience.\n\uf0a1Research\u2014Similar to evaluation in the context of research, feedback is typically\nperformed offline after the output is evaluated. While some development has\nbeen done using multiple agent systems incorporating agents for evaluation\nand feedback, these systems don\u2019t always perform well, at least not with the cur-\nrent state-of-the-art models. Instead, it\u2019s often better to isolate feedback and\nevaluation at the end to avoid the common feedback looping problem. \nFeedback is another powerful component of agentic and assistant systems, but it\u2019s not\nalways required on the first release. However, incorporating rigorous feedback and\nevaluation mechanisms can greatly benefit agentic systems in the long term concern-\ning ongoing monitoring and providing the confidence to improve various aspects of\nthe system.\n How you implement each of these components in your agentic systems may, in\npart, be guided by the architecture of your chosen agentic platform. Now that you\nunderstand the nuances of each component, you also have the knowledge to guide\nyou in selecting the right agent system that fits your application and business use case.\nRegardless of your application, you\u2019ll want to employ several agentic components in\nalmost all cases.\n As agentic systems mature and LLMs themselves get smarter, some of the compo-\nnents we today consider external may be closely integrated. We\u2019ve already seen rea-\nsoning and planning be integrated into a model such as Strawberry. Certainly, as we\napproach the theoretical artificial general intelligence milestone, we may see models\ncapable of performing long-term self-evaluation and feedback. \n In any case, I hope you enjoyed this journey with me into this incredible frontier of\na new and emerging technology that will certainly alter our perception of work and\nhow we undertake it through agents.\n",
      "content_length": 2729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "296\nCHAPTER 11\nAgent planning and feedback\n11.6\nExercises\nUse the following exercises to improve your knowledge of the material:\n\uf0a1Exercise 1\u2014Implement a Simple Planning Agent (Beginner)\nObjective\u2014Learn how to implement a basic planning agent using a prompt to\ngenerate a sequence of actions.\nTasks:\n\u2013 Create an agent that receives a goal, breaks it into steps, and executes those\nsteps sequentially.\n\u2013 Define a simple goal, such as retrieving information from Wikipedia and sav-\ning it to a file.\n\u2013 Implement the agent using a basic planner prompt (refer to the planner\nexample in section 11.3).\n\u2013 Run the agent, and evaluate how well it plans and executes each step.\n\uf0a1Exercise 2\u2014Test Feedback Integration in a Planning Agent (Intermediate)\nObjective\u2014Understand how feedback mechanisms can improve the performance\nof an agentic system.\nTasks:\n\u2013 Modify the agent from exercise 1 to include a feedback loop after each task.\n\u2013 Use the feedback to adjust or correct the next task in the sequence.\n\u2013 Test the agent by giving it a more complex task, such as gathering data from\nmultiple sources, and observe how the feedback improves its performance.\n\u2013 Document and compare the agent\u2019s behavior before and after adding feedback.\n\uf0a1Exercise 3\u2014Experiment with Parallel and Sequential Planning (Intermediate)\nObjective\u2014Learn the difference between parallel and sequential actions and\nhow they affect agent behavior.\nTasks:\n\u2013 Set up two agents using Nexus: one that executes tasks in parallel and another\nthat performs tasks sequentially.\n\u2013 Define a multistep goal where some actions depend on the results of previous\nactions (sequential), and some can be done simultaneously (parallel).\n\u2013 Compare the performance and output of both agents, noting any errors or\ninefficiencies in parallel execution when sequential steps are required.\n\uf0a1Exercise 4\u2014Build and Integrate a Custom Planner into Nexus (Advanced)\nObjective\u2014Learn how to build a custom planner and integrate it into an agent\nplatform.\nTasks:\n\u2013 Write a custom planner using prompt engineering strategies from section 11.3,\nensuring it supports sequential task execution.\n\u2013 Integrate this planner into Nexus, and create an agent that uses it.\n",
      "content_length": 2188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "297\nSummary\n\u2013 Test the planner with a complex goal that involves multiple steps and tools\n(e.g., data retrieval, processing, and saving).\n\u2013 Evaluate how the custom planner performs compared to built-in planners in\nNexus or other platforms.\n\uf0a1Exercise 5\u2014Implement Error Handling and Feedback in Sequential Planning\n(Advanced)\nObjective\u2014Learn how to implement error handling and feedback to refine sequen-\ntial planning in an agentic system.\nTasks:\n\u2013 Using a sequential planner, set up an agent to perform a goal that may encoun-\nter common errors (e.g., a failed API call, missing data, or invalid input).\n\u2013 Implement error-handling mechanisms in the planner to recognize and\nrespond to these errors.\n\u2013 Add feedback loops to adjust the plan or retry actions based on the error\nencountered.\n\u2013 Test the system by deliberately causing errors during execution, and observe\nhow the agent recovers or adjusts its plan.\nSummary\n\uf0a1Planning is central to agents and assistants, allowing them to take a goal, break\nit into steps, and execute them. Without planning, agents are reduced to simple\nchatbot-like interactions.\n\uf0a1Agents must differentiate between parallel and sequential actions. Many LLMs\ncan handle parallel actions, but only advanced models support sequential plan-\nning, critical for complex task completion.\n\uf0a1Feedback is crucial in guiding agents to correct their course and improve per-\nformance over time. This chapter demonstrates how feedback mechanisms can\nbe integrated with agents to refine their decision-making processes.\n\uf0a1Platforms such as OpenAI Assistants and Anthropic\u2019s Claude support internal\nplanning and can execute complex, multistep tasks. Agents using these plat-\nforms can use sequential action planning for sophisticated workflows.\n\uf0a1Properly selecting and limiting agent actions is vital to avoid confusion and\nunintended behavior. Too many actions may overwhelm an agent, while unnec-\nessary tools may be misused.\n\uf0a1Nexus allows for creating and managing agents through a flexible interface,\nwhere users can implement custom planners, set goals, and assign tools. The\nchapter includes practical examples using Nexus to highlight the difference\nbetween a raw LLM and a planner-enhanced agent.\n\uf0a1Writing custom planners is straightforward, using prompt engineering strate-\ngies. Tools such as LangChain and Semantic Kernel offer a variety of planners\nthat can be adapted or extended to fit specific agentic needs.\n",
      "content_length": 2435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "298\nCHAPTER 11\nAgent planning and feedback\n\uf0a1Models such as OpenAI Strawberry integrate reasoning, planning, evalua-\ntion, and feedback directly into the LLM, offering more accurate problem-\nsolving capabilities.\n\uf0a1Evaluation helps determine how well an agentic system is performing and can\nbe implemented internally or externally, depending on the use case.\n\uf0a1As LLMs evolve, reasoning, planning, and feedback mechanisms may become\ndeeply integrated into models, paving the way for more autonomous and intelli-\ngent agent systems.\n",
      "content_length": 529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "299\nappendix A\nAccessing OpenAI\nlarge language models\nAlthough several commercial large language model (LLM) services are available,\nthis book recommends using OpenAI services directly or through Azure OpenAI\nStudio. To access either service, you must create an account and register a payment\nmethod not covered in this appendix. The GPT-4 family of LLMs is considered best\nin class and better suited for agent development. Using open source and alterna-\ntive services is always an option but generally only advisable after you\u2019ve worked\nwith GPT-4 for some time.\nA.1\nAccessing OpenAI accounts and keys\nThe following general steps can help you quickly set up using OpenAI LLMs for agent\ndevelopment. Though using OpenAI and other commercial LLMs comes at a price,\nyou can expect to pay less than US$100 to complete all the exercises in this book:\n1\nGo to https://openai.com and log in, or register for an account and log in. If\nthis is your first time creating an account, you\u2019ll likely be given free credit in\nsome amount. If you already have an account, you must register a payment\nmethod and type. It\u2019s generally better to purchase a number of credits at a\ntime. This will allow you to manage the costs better and avoid overruns.\n2\nAfter logging in to the platform, select ChatGPT or the API, as shown in\nfigure A.1. Choose the API.\n \n \n \n",
      "content_length": 1342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "300\nAPPENDIX A\nAccessing OpenAI large language models\n3\nOpen the left menu, and select the API Keys option, as shown in figure A.2.\nSelect the API\n.\n.\nFigure A.1\nSelecting the API section of the OpenAI platform\nSelect the API\neys\nK\nFigure A.2\nSelecting \nthe API Keys option\n",
      "content_length": 274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "301\nA.1\nAccessing OpenAI accounts and keys\n4\nClick the Create button to create a new key, enter a name for the key, and click\nthe Create Secret Key button, as shown in figure A.3.\n5\nCopy and paste the key to a notepad or another area for safekeeping using the\nCopy button, as shown in figure A.4. Keep this key secret, and ensure it remains\nonly on your development machine.\nAfter generating a key, you can continue to use it within an .env configuration file or\nthrough other means of registering an OpenAI key. For most of the packages used in\nthis book, configuring OpenAI will generally only require the key. Other services,\nsuch as Azure OpenAI, will require the configuration of a model deployment and a\nbase URL as covered in the next section.\n \n \n \nEnter a helpful name,\nsuch as GPT-Agents.\nClick to create\nthe key.\nStart by clicking the button\nto create a new secret key.\nFigure A.3\nCreating the secret API key\n",
      "content_length": 920,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "302\nAPPENDIX A\nAccessing OpenAI large language models\nA.2\nAzure OpenAI Studio, keys, and deployments\nThrough its ongoing relationship with OpenAI, Microsoft hosts the same models at\nthe same price within Azure OpenAI Studio. Occasionally, Azure may be a model ver-\nsion behind, but Microsoft generally keeps current with the latest OpenAI models.\n These guidelines will be more general because there are several ways to access\nAzure and methods of creating accounts and accessing the studio (for specific instruc-\ntions, refer to Microsoft documentation):\n1\nLog in to your Azure portal account subscription.\n2\nCreate a new Azure OpenAI Studio resource in a region that makes sense to\nyou. At the time of writing, not all regions provided access to all models. You may\nneed to check which models are available for your region first. This will also be\nspecific to your account and usage.\nWithin Azure OpenAI, models are exposed through a resource allocation\ncalled a deployment. Deployments wrap a model, such as GPT-4, and provide\naccess to the resource. Figure A.5 shows an example of various models being\nexposed through deployments.\n3\nClick the Create New Deployment button to create a new deployment, and then\nselect the model you want to deploy.\n \nClick the Copy button to copy\nthe key to the clipboard.\nMake sure to save the\nkey right away.\nClick to create the key.\nFigure A.4\nCopying and pasting the key to a well-known safe location\n",
      "content_length": 1440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "303\nA.2\nAzure OpenAI Studio, keys, and deployments\n4\nAfter the model is wrapped in a deployment, you must access the parent Azure\nOpenAI resource. From there, you can access the key, endpoint, or base URL\nneeded to configure your connection, as shown in figure A.6.\nAgain, if you get stuck, the Microsoft documentation can guide you in the right direc-\ntion. The three critical differences to remember when connecting to a resource such\nas Azure OpenAI Studio or another LLM using the OpenAI tooling are listed here:\n\uf0a1The api key to access the model\n\uf0a1The base url or endpoint where the model is located\n\uf0a1The name of the model or deployment name\nIf you can\u2019t access a model for whatever reason, a good alternative is open source\nmodels. Setting up and consuming open source LLMs is covered in chapter 2. \nDeployment name is the name\nreferenced as the model name.\nOpenAI model\nname\nOther resource\ninformation\nFigure A.5\nDeploying a model through an Azure OpenAI Studio deployment\n",
      "content_length": 978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "304\nAPPENDIX A\nAccessing OpenAI large language models\nClick to copy, and then\npaste the keys as needed.\nThe base URL for\nthe model service\nOther resource\ninformation\n",
      "content_length": 166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "305\nappendix B\nPython development\nenvironment\nWhile this book assumes readers are experienced Python developers, this could\nmean many different things. In this appendix, we look at configuring a Python\ndevelopment environment that will function with the code examples in this book.\nYou can use other integrated development environments (IDEs), but not all tool-\ning, especially extensions, will work in all IDEs.\nB.1\nDownloading the source code\nTo download and run the source code, install Git, and then pull the repository\nlocally. Here are the high-level steps to pull the code from the book\u2019s GitHub\nrepository:\n1\nInstall Git if you need to. Git can be installed from multiple sources, but a\ngood option is the main release, found here: https://git-scm.com/down-\nloads. Follow the instructions to download and install the tool for your oper-\nating system.\n2\nOpen a terminal in a folder you want to download the source to, and then\nenter the following command:\ngit clone https://github.com/cxbxmxcx/GPT-Agents.git\n3\nAfter the code is downloaded, you can begin by opening the chapter folder\nthat you\u2019re working on in Visual Studio Code (VS Code). If you need to\n",
      "content_length": 1163,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "306\nAPPENDIX B\nPython development environment\ninstall VS Code or understand how to load a chapter folder as a workspace, con-\nsult section B.5 in this appendix.\nB.2\nInstalling Python\nPython is provided through different versions and deployments. This book relies on\nthe standard Python installation, version 3.10. Anaconda is another deployment of\nPython that is very popular and could be used. However, all the material in this book\nhas been run and tested with a Python 3.10 virtual environment:\n1\nGo to www.python.org/downloads/.\n2\nLocate and download the latest release of Python 3.10 for your operating system.\n3\nInstall the release on your machine using the instructions for your operating\nsystem.\n4\nTo confirm your installation, open a terminal, and execute the following\ncommand:\npython \u2013-version\nThe version should be 3.10, but if it isn\u2019t, don't worry. You may have multiple Python\nversions installed. We\u2019ll also confirm the installation when setting up VS Code.\nB.3\nInstalling VS Code\nInstalling VS Code is relatively straightforward and can be done in just a few steps:\n1\nGo to https://code.visualstudio.com.\n2\nDownload a stable release of VS Code for your operating system.\n3\nAfter the release is downloaded, follow the installation instructions for your\noperating system.\n4\nLaunch VS Code for your operating system, and make sure no warnings or\nerrors appear. If you encounter problems, try to restart your computer and/or\nreinstall.\nWith VS Code running, we can install the necessary extensions. We\u2019ll cover those\nextensions next.\nB.4\nInstalling VS Code Python extensions\nThousands of extensions for VS Code can provide an excellent Python coding envi-\nronment. The recommended ones are only the start of what you can explore inde-\npendently. Beware, though, that not all extensions are created equally. When installing\nnew extensions, look at the number of installs and ratings. Extensions with fewer than\nfour stars are generally to be avoided. To install the extensions, follow these steps:\n \n",
      "content_length": 2011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "307\nB.4\nInstalling VS Code Python extensions\n1\nLaunch VS Code, and open the Extensions panel, as shown in figure B.1.\n2\nInstall the following list of extensions:\n\u2013 Python, for environment and language support\n\u2013 Python Extension Pack, for covering other extensions\n\u2013 Python Environment Manager, for managing environments\n\u2013 Python Indent, for code formatting\n\u2013 Flake8, for code formatting/linting\n\u2013 Prompt Flow, for testing LLM prompts\n\u2013 Semantic Kernel Tools, for working with the Semantic Kernel framework\n\u2013 Docker, for managing Docker containers\n\u2013 Dev Containers, for running development environments with containers\nYou\u2019ll only need to install the extensions for each VS Code environment you\u2019re run-\nning. Typically, this will mean installing for just your operating system installation of\nVS Code. However, if you run VS Code in containers, you must install extensions for\neach container you\u2019re running. Working with Python in the Dev Containers extension\nwill be covered later in this appendix.\nOpen the\nExtensions panel.\nSelect the\nextensions.\nEnter your search term.\nClick the Install\nbutton to install.\nFigure B.1\nInstalling VS Code extensions\n",
      "content_length": 1151,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "308\nAPPENDIX B\nPython development environment\nB.5\nCreating a new Python environment with VS Code\nWhen developing Python projects, you often want to create isolated virtual environ-\nments. This will help in managing multiple package dependencies across various tasks\nand tools. In this book, it\u2019s recommended that a new virtual environment be created\nfor each new chapter. VS Code can help you create and manage multiple Python envi-\nronments quickly and efficiently via the following steps:\n1\nPress Ctrl-Shift-P (Cmd-Shift-P) to open the command panel, and select Python:\nCreate Environment, as shown in figure B.2.\n2\nSelect the environment type, either Venv or Conda. This book demonstrates\nVenv but Conda should also work.\nOpen the command panel (Ctrl-Shift-P, Cmd-Shift-P)\nSelect to create a new environment.\nUse the Refresh button if your installation is shown.\nCheck the requirements.txt \ufb01le, which should\ncontain all the dependencies for the chapter.\nSelect the installation. This list should re\ufb02ect\nthe Python versions you have installed.\nSelect the type of environment. The\ninstructions in this book use an .venv\nenvironment, but Conda should work\njust as well.\nFigure B.2\nThe steps to set up the virtual environment for a chapter\n",
      "content_length": 1239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "309\nB.6\nUsing VS Code Dev Containers (Docker)\n3\nSelect the Python installation. The code in this book has been run with Python\n3.10 at a minimum. The agent tools and frameworks featured in this book are\ncutting edge, so they should support later versions of Python.\n4\nCheck that the requirements.txt file in the chapter folder is selected. This will\ninstall all the requirements for the current chapter.\nYou should complete these steps for each new chapter of the book. The alternative is\nto use VS Code development containers, which will be covered in the next section.\nB.6\nUsing VS Code Dev Containers (Docker)\nWhen working with advanced agents and agents that can generate and execute\ncode, running them in isolated containers is generally recommended. Container\nisolation prevents operating system disruption or corruption and provides a base for\ndeploying agents.\n Getting familiar with containers and platforms such as Docker can be an extensive\nundertaking to grasp everything. Fortunately, it takes very little knowledge to start\nusing containers, and VS Code extensions make this even more accessible.\n You\u2019ll first need to install a container toolset. Docker is free (provided you use the\ntool as a hobby or you\u2019re a student) and the most accessible. Follow these instructions\nto install Docker and get started working with containers:\n1\nGo to the Docker Desktop download page at www.docker.com/products/\ndocker-desktop.\n2\nDownload and install Docker for your operating system. Follow any other\ninstructions as requested.\n3\nLaunch the Docker desktop application. Completing this step will confirm you\nhave Docker installed and working as expected.\n4\nOpen VS Code, and confirm that the Docker extensions listed in section 1.4 are\ninstalled.\nWith Docker and VS Code configured, you can move on to using Dev Containers by\nfollowing these steps:\n1\nOpen a new instance of VS Code.\n2\nSelect to open a remote window, as shown in figure B.3. \n3\nSelect Open Folder in Container to start a container from a folder, or select\nNew Dev Container to start without a folder.\nAfter the container is launched, your VS Code environment will be con-\nnected. This allows you to develop code on the container without worrying about\ndependencies not working.\n",
      "content_length": 2247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "310\nAPPENDIX B\nPython development environment\nSelect Open Folder\nin Container.\nSelect Open\nRemote Window.\nFigure B.3\nOpening a remote window to a container in VS Code\n",
      "content_length": 167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "311\nindex\nA\nABTs (agentic behavior trees) 143\u2013152\nblackboards in 155\nbuilding with back chaining 155\u2013156\ncoding challenge ABT 145\u2013149\nconversational AI systems 149\nmanaging assistants with assistants 143\nposting YouTube videos to X 150\nrequired X setup 151\nactions\nand tools, agent platforms 174\u2013179\ndefining 99\nempowering agents with 98\nwriting semantic services 125\nexecuting OpenAI functions 101\u2013107\nactioning function calls 103\u2013107\nadding functions to LLM API calls 101\nexercises 127\nnative functions 111\u2013118\nsemantic functions 111\u2013118\nembedding native functions within\n117\nSemantic Kernel (SK) 107\u2013111\ncontext variables 109\nsemantic functions 108\u2013111\nadvanced parameters 221\nagent_action decorator 175\nagent critics 79\nagent engines 172\nagentic systems\napplication of\nassistant/agentic planning 288\u2013290\nassistant/agentic reasoning 290\u2013291\nevaluation to agentic systems 292\u2013293\nfeedback to agentic/assistant \napplications 293\u2013295\nimplementing memory in 200\u2013207\nsemantic memory 204\nagent memory, exercises 209\nagentops package 88\nagent personas 216\nagent planning\nexercises 296\nsequential planners 278\u2013285\nagent planning and feedback 272\nagent platforms\nactions and tools 174\u2013179\nagent engines 172\ndeveloping profiles and personas for \nagents 170\noverview of 161\u2013164\nStreamlit 165\u2013170\nbuilding chat applications 165\u2013168\ncreating streaming chat applications\n168\nagent profiles 216\nevaluating 224\u2013228\nagent prompts\nagent personas 216\nagent profiles 216\nrubrics and grounding 228\u2013230\nagent reasoning\ndirect solution prompting 245\u2013252\nfew-shot prompting 248\u2013250\nquestion and answer prompting\n246\u2013248\nzero-shot prompting 250\n",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "INDEX\n312\nagents\ncomponent systems of 4\u20139\ndefining 1\u20134\nempowering with actions 98\nexecuting OpenAI functions 101\u2013107\nmemory and knowledge\nsemantic search and document \nindexing 184\u2013192\nOpenAI Strawberry 285\u2013287\noverview of 12\nplanning 273\u2013277\nreasoning and evaluation 244\nrise of 9\naggregate block 238, 240\nAGI (artificial general intelligence) 9\nAI agents, AI interface 11\nApi parameter 221\nAPIs (application programming interfaces), \ndeploying prompt flow API 223\nassistant role 18\nassistants\nABTs (agentic behavior trees) 143\u2013152\nengaging through ChatGPT 40\u201344\nPlayground 136\u2013142\ngetting assistant to run code locally 140\ninstalling and running 136\ninstalling assistants database 140\ninvestigating assistant process through \nlogs 142\nusing and building custom actions 138\nSocial Media Assistant 152\nTwitter Post Writer 152\nYouTube Researcher v2 152\nassistant systems, application of\nassistant/agentic planning 288\u2013290\nassistant/agentic reasoning 290\u2013291\nevaluation to agentic systems 292\u2013293\nfeedback to agentic/assistant applications\n293\u2013295\nAutoGen Studio, multi-agent systems with\n69\u201376\nadding skills 73\u201376\ninstalling and using 70\nautonomous assistants 129\nbehavior trees 130\u2013136\ndeciding on 132\u2013134\nexecution 131\nrunning with Python and py_trees 134\nconversational autonomous multi-agents\n153\nexercises 156\nAzure OpenAI Studio 302\nB\nBaseAgent class 172\nBasicNexusPlanner class 282\nbehavior trees 130\u2013136\nblackboards in 155\nbuilding with back chaining 155\u2013156\ndeciding on 132\u2013134\nexecution 131\nrunning with Python and py_trees 134\nblackboards 155\nC\nchain of thought (CoT) prompting 253\u2013257\nchat completions 16\nChatGPT, engaging assistants through 40\u201344\ncoding challenge ABT (agentic behavior \ntree) 145\u2013149\ncompression\nknowledge 207\nblending knowledge and memory 209\ncase for 208\nfrequency of application 208\nmultiple knowledge stores 209\nmultiple passes of 208\nmemory 207\nblending knowledge and memory 209\nfrequency of application 208\nmultiple memory stores 209\nmultiple passes of 208\nconfig.json file 112\nconnecting.py code 36\nConnection parameter 221\ncontext variables 109\nConversableAgent agent 78\nconversational AI systems 149\ncosine_similarities variable 186\ncosine_similarity function 186\ncreate_assistant_action_on_thread helper \nfunction 149\ncreate_assistant_condition helper function 149\ncreate_manager_assistant action 144\ncreate_plan function 282\nCrewAI\nbuilding agent crew with 84\u201390\ncreating jokester crew of CrewAI agents\n84\u201387\nobserving agents working with AgentOps\n87\u201390\nrevisiting coding agents with 90\u201395\ncrewai_agentops.py file 88\n",
      "content_length": 2557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "INDEX\n313\ncrewai package 88\ncustom actions 49\u201355\nconnecting custom action to assistant 53\ncreating assistant to build assistant 49\u201353\nD\nDAG (direct acyclic graph) 262\ndescribe_image function 75\nDirect Prompting 270\ndirect solution prompting 245\u2013252\nfew-shot prompting 248\u2013250\nquestion and answer prompting 246\u2013248\nzero-shot prompting 250\nDocker 309\ndocument indexing 184\u2013192\ndocument embeddings 189\nquerying document embeddings from \nChroma 190\nvector similarity search 184\u2013188\nInverse Document Frequency (IDF) 185\nTerm Frequency (TF) 185\nTF-IDF calculation 185\u2013188\nvector databases and similarity search 188\nE\necho block 221\nEmbedding components 246\nepisodic memory 204\nevaluate_recommendation block 231\nevaluation, employing for consistent \nsolutions 261\u2013270\nself-consistency prompting 262\u2013266\nToT prompting 266\u2013270\nevaluation prompt techniques 271\nF\nfeedback\napplication of feedback to agentic/assistant \napplications 293\u2013295\nstepwise planner, OpenAI Strawberry\n285\u2013287\nfew-shot learning 248\nfew-shot model 248\nFew-Shot Prompting 270\nfile uploads extending assistants\u2019 knowledge \nusing 56\u201361\nbuilding Calculus Made Easy GPT 56\u201358\nknowledge search and more with file \nuploads 58\u201361\nfirst_function.py Python script 103\nflow.dag.yaml file 236, 240\nfor-each function 283\nfunctions\nactioning function calls 103\u2013107\nadding to LLM API calls 101\nG\ngenerative models 14\nget_response function 173\nget_response_stream function 176\nget_top_movie_by_genre function 124\nget_top_movies_by_genre function 126\nget_wikipedia_page action 278\nGPT Assistants Playground 136\u2013142\ngetting assistant to run code locally 140\ninstalling and running 136\ninstalling assistants database 140\ninvestigating assistant process through logs\n142\nusing and building custom actions 138\nGPT (Generative Pretrained Transformer) \nassistants 39\nbuilding 44\u201348\nexercises 65\nOpenAI Assistants 136\u2013149\npublishing 61\u201365\neconomics of GPTs 63\nexpensive GPT assistants 62\nreleasing GPT 63\nGPTs (Generative Pretrained Transformers)\n14\nbuilding semantic interface 119\u2013121\ncustomizing 49\u201355\nconnecting custom action to assistant 53\ncreating assistant to build assistant\n49\u201353\ngrounding 228\u2013230\nGroupChat 82\nGroupChatManager 82\ngroup chat with agents and AutoGen 82\nI\nIDEs (integrated development environments)\n305\nIDF (Inverse Document Frequency) 185\nimport_plugin function 118\ninitiate_chat call 82\nInputs block 221\ninput tokens 20\n",
      "content_length": 2384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "INDEX\n314\ninteractive service agent, semantic kernel \nas 118\u2013125\nbuilding semantic GPT interface 119\u2013121\ninteractive chat with semantic service layer\n122\u2013125\ntesting semantic services 121\nJ\nJinja2 templates, creating profiles with 222\nJSON Lines (JSONL) 235\nJSON list document 235\nK\nknowledge 180\nbuilding agent knowledge 196\u2013200\ncompression 207\nblending knowledge and memory 209\ncase for 208\nextending assistants\u2019 knowledge using file \nuploads 56\u201361\nbuilding Calculus Made Easy GPT 56\u201358\nknowledge search with file uploads 58\u201361\nstores 209\nKnowledgeManager class 199\nknowledge/memory 162\nL\nLangChain, constructing RAG with 192\u2013196\nsplitting and loading documents 193\u2013194\nsplitting documents by token 195\nlaunch.json file 168\nLLMs (large language models) 2, 14, 99, 161, \n299\nadding functions to API calls 101\nchoosing optimal 34\u201336\nexercises 14\ngrounding evaluation with LLM profile\n230\nOpenAI API 16\u201320\nconnecting to chat completions model 16\nrequest and response 18\u201320\nopen source LLMs with LM Studio 20\u201325\nprompt engineering 25\u201334\nadopting personas 29\ncreating detailed queries 28\nproviding examples 32\nspecifying output length 33\nspecifying steps 31\nusing delimiters 30\nLM Studio 20\u201325\ninstalling and running 20\nserving LLM locally with 23\nload_dotenv function 17\nLocal Data File 236\nM\nmanager_llm class 92\nmax_rpm (maximum requests per minute) 87\nMax_tokens parameter 221\nmemory 180\ncompression 207\nconsuming memory stores in Nexus 202\nimplementing in agentic systems 200\u2013207\nretrieval 181\nsemantic memory 204\nModel parameter 221\nmulti-agent systems 3, 68\nAutoGen 77\u201382\ncache 81\nenhancing code output with agent critics 79\ninstalling and consuming 77\u201379\nbuilding agent crew with CrewAI 84\u201390\ncreating jokester crew of CrewAI agents\n84\u201387\nobserving agents working with AgentOps\n87\u201390\nexercises 95\ngroup chat with agents and AutoGen 82\nrevisiting coding agents with CrewAI 90\u201395\nwith AutoGen Studio 69\u201376\nadding skills 73\u201376\ninstalling and using 70\nmultipath reasoning 9\nN\nnative functions 111\u2013118, 127\napplying 115\nembedding within semantic functions 117\nNexus\nconsuming memory stores in 202\ndeveloping 163\noverview of 161\u2013164\nrunning 162\nNPCs (nonplayer characters) 130\nO\nOAI_CONFIG_LIST file 78\none-shot learning 248\n",
      "content_length": 2224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "INDEX\n315\none-shot model 248\nOpenAI\naccessing accounts and keys 299\nAzure OpenAI Studio 302\nOpenAI API 16\u201320\nconnecting to chat completions model 16\nrequest and response 18\u201320\nOutputs block 221\noutput tokens 20\nP\nparsing_results block 232\nPCA (principal component analysis)\n190\npersonas 29\ndeveloping for agents 170\npip command 77\nplanning 252\nstepwise planner, OpenAI Strawberry\n285\u2013287\nplanning agents 273\u2013277\nplanning/feedback 162\nplugin folder 115\nplugins, creating and registering 111\u2013115\nprocedural memory 204\nprofiles\ncomparing 232\u2013243\ncreating evaluation flow for grounding\n238\u2013241\nexercises 242\nparsing LLM evaluation output 232\nrunning batch processing in prompt \nflow 235\u2013238\ndeveloping for agents 170\nprompt chaining 258\u2013261\nprompt engineering 9, 25\u201334\nadopting personas 29\ncreating detailed queries 28\nproviding examples 32\nreasoning in 252\u2013261\nchain of thought prompting 253\u2013257\nstep-by-step with prompt chaining\n258\u2013261\nzero-shot CoT prompting 257\nspecifying output length 33\nspecifying steps 31\nsystematic 213\nusing delimiters 30\nprompt_engineering.py file 26, 28\nprompt flow 212\nagent profiles, evaluating 224\u2013228\ncomparing profiles 232\u2013243\ncreating evaluation flow for grounding\n238\u2013241\nexercises 242\nparsing LLM evaluation output 232\nrunning batch processing in prompt \nflow 235\u2013238\ngrounding evaluation with LLM profile 230\nsetting up 217\u2013224\ncreating profiles with Jinja2 templates 222\ndeploying prompt flow API 223\nprerequisites 218\u2013221\nsystematic prompt engineering 213\nPromptTemplateManager 282\nPython\nbehavior trees with 134\ncreating new environment with VS Code 308\ndevelopment environment\ndownloading source code 305\nVS Code Dev Containers (Docker) and 309\ninstalling VS code 306\ninstalling VS Code extensions 306\npy_trees 134\npy_trees package 147\nQ\nquestion_answer LLM prompt 246\nR\nRAG (Retrieval Augmented Generation) 177, 182\napplying to building agent knowledge 196\u2013200\nconstructing with LangChain 192\u2013196\nsplitting and loading documents 193\u2013194\nsplitting documents by token 195\nreasoning 252\nin prompt engineering 252\u2013261\nchain of thought prompting 253\u2013257\nstep-by-step with prompt chaining 258\u2013261\nzero-shot CoT prompting 257\nreasoning agents, exercises 270\nreasoning prompts 270\nrecommender block 221\nrecommender_with_LLM_evaluation flows visual \neditor 232\nregistering\nplugins 111\u2013115\nsemantic functions 111\u2013115\nsemantic plugins 111\u2013115\nsemantic skills 111\u2013115\nskills 111\u2013115\n",
      "content_length": 2411,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "INDEX\n316\nrequirements.txt file 77, 246\nretrieval 181\nRLHF (reinforcement learning with human \nfeedback) 16\nrubber ducking 29\nrubrics 228\u2013230\nrun_conversation function 104\nS\nsave_file action 278\nscoring block 238\nsearch_wikipedia action 278\nself-consistency prompting 262\u2013266\nsemantic functions 111\u2013118, 127\ncreating and registering 111\u2013115\nembedding native functions within 117\nsemantic memory 204\nsemantic plugins, creating 111\u2013115\nsemantic search 184\u2013192\nvector similarity search 184\u2013188\ndocument embeddings 189\nInverse Document Frequency (IDF) 185\nquerying document embeddings from \nChroma 190\nTerm Frequency (TF) 185\nTF-IDF calculation 185\u2013188\nvector databases and similarity search 188\nsemantic services, writing 125\nsemantic skills, creating 111\u2013115\nSEO (search engine optimization) 64\nsequential planners 278\u2013285\nsequential planning process 277\nsequential reasoning 9\nsingle-path reasoning 9\nsk_function decorator 120\nskills\ncreating 111\u2013115\nregistering 111\u2013115\nskills directory 115\nskills/plugin folder 112\nskills/Recommender/Recommend_Movies \nfolder 112\nskprompt.txt file 112\nSK (Semantic Kernel) 107\u2013111, 128, 160, 244\nas interactive service agent 118\u2013125\nbuilding semantic GPT interface 119\u2013121\ninteractive chat with semantic service \nlayer 122\u2013125\ntesting semantic services 121\ncontext variables 109\nsemantic functions 108\u2013111\nstep-by-step with prompt chaining 258\u2013261\nstepwise planner, OpenAI Strawberry\n285\u2013287\nStop parameter 221\nStreamlit\nbuilding chat applications 165\u2013168\ncreating streaming chat applications 168\noverview 165\u2013170\nst.spinner control 167\nst.write_stream control 169\nsystem role 18\nT\ntemperature conversion plugin 127\nTemperature parameter 221\nTF-IDF (Term Frequency-Inverse Document \nFrequency) 184\u2013188\nTF (Term Frequency) 185\ntmdb.py file 119\nTMDbService 126\nTMDbService class 119\u2013120, 122\nTMDB (The Movie Database) 118\ntokenization, splitting documents by \ntoken with LangChain 195\nToT (tree of thought) 245\nToT (tree of thought) prompting\n266\u2013270\nU\nulk_recommend.jsonl file 235\nunstructured memory/knowledge concepts\n183\nUserProxy agent 78\nuser role 18\nV\nvector databases 188\nvector similarity search 184\u2013188\ndocument embeddings 189\nInverse Document Frequency (IDF) 185\nquerying document embeddings from \nChroma 190\nTerm Frequency (TF) 185\nTF-IDF calculation 185\u2013188\nvector databases and similarity search\n188\nVS Code\ninstalling 306\ninstalling Python extensions 306\n",
      "content_length": 2402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "INDEX\n317\nVS Code Dev Containers (Docker) 309\n.vscode/launch.json file 167\nVS Code (Visual Studio Code) 16, 51, 70, \n102, 136, 218\ncreating new Python environment \nwith 308\nW\nweather information plugin 127\nweb APIs 128\nWindows Subsystem for Linux (WSL)\n79\nwith statement 82\nworking_dir folder 79\nWrite Clear Instructions strategy 26\nWSL (Windows Subsystem for Linux)\n79\nX\nX (formerly Twitter)\nrequired assistants 152\nrequired setup 151\nY\nYouTube\nposting videos to X 150\nrequired assistants 152\nsearch and spam 151\nZ\nzero-shot CoT prompting 257\nzero-shot learning 248\nzero-shot prompting 250, 270\n",
      "content_length": 596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "For ordering information, go to www.manning.com\nRELATED MANNING TITLES\nMulti-Agent Systems with AutoGen\nby Victor Dibia\nISBN 9781633436145\n325 pages (estimated), $59.99\nSpring 2025 (estimated)\nGenerative AI for the IT Pro\nby Chrissy LeMaire and Brandon Abshire\nISBN 9781633436428\n350 pages (estimated), $49.99\nSpring 2025 (estimated)\nThe Complete Obsolete Guide to Generative AI\nby David Clinton\nISBN 9781633436985\n240 pages, $39.99\nJuly 2024\nGenerative AI in Action\nby Amit Bahree\nForeword by Eric Boyd\nISBN 9781633436947\n464 pages, $59.99\nSeptember 2024\n",
      "content_length": 556,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "The Manning Early Access Program\nDon\u2019t wait to start learning! In MEAP, the Manning Early Access Program, you can read \nbooks as they\u2019re being created and long before they\u2019re available in stores. \nHere\u2019s how MEAP works.\n\u2022\t \u0007Start now. Buy a MEAP and you\u2019ll get all available chapters in PDF, ePub, Kindle, \nand liveBook formats. \n\u2022\t \u0007Regular updates. New chapters are released as soon as they\u2019re written. We\u2019ll \nlet you know when fresh content is available.\n\u2022\t \u0007Finish faster. MEAP customers are the first to get final versions of all books! \nPre-order the print book, and it\u2019ll ship as soon as it\u2019s off the press.\n\u2022\t \u0007Contribute to the process. The feedback you share with authors makes the end \nproduct better.\n\u2022\t No risk. You get a full refund or exchange if we ever have to cancel a MEAP.\nExplore dozens of titles in MEAP at www.manning.com.\n",
      "content_length": 846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "A new online reading experience\nliveBook, our online reading platform, adds a new dimension to your Manning books, \nwith features that make reading, learning, and sharing easier than ever. A liveBook \nversion of your book is included FREE with every Manning book.\nThis next generation book platform is more than an online reader. It\u2019s packed with \nunique features to upgrade and enhance your learning experience.\n\u2022 Add your own notes and bookmarks\n\u2022 One-click code copy\n\u2022 Learn from other readers in the discussion forum\n\u2022 Audio recordings and interactive exercises\n\u2022 Read all your purchased Manning content in any browser, anytime, anywhere\nAs an added bonus, you can search every Manning book and video in liveBook\u2014even \nones you don\u2019t yet own. Open any liveBook, and you\u2019ll be able to browse the content and \nread anything you like.*\nFind out more at www.manning.com/livebook-program.\n*Open reading is limited to 10 minutes per book daily\n",
      "content_length": 942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "The difference between iterative and planned execution\nSingle prompt as a goal:\nsearch Wikipedia for topic, return\nthe top page, and save to a \ufb01le.\nSequential execution of a goal\nSingle prompt as a goal:\nsearch Wikipedia for topic, return\nthe top page, and save to a \ufb01le.\nIterative execution of a goal\nTasks/plan\nReturns a list of page IDs\nsearch_wikipedia(topic)\nReturns the page content\nget_wikipedia_page(page_id)\nCollects pages\nsave_ \ufb01le\nAll pages combined\nIn a planner, this\naggregation is\nperformed\ninternally.\nAll tasks are\nperformed\ninternally by\nthe agent.\nUser submits a goal.\nsearch_wikipedia(topic)\nResults are displayed, and the\nagent asks to continue to the\nnext step.\nReturns a list of page IDs\nget_wikipedia_page(page_id)\nUser responds\nwith continue.\nReturns are collected and\ndisplayed to the user. The agent\nasks for further continuation.\nReturns the page content\nsave_ \ufb01le\nAll pages combined\nUser responds\nwith continue.\nGoal is complete, and user has the output.\nIterative\nexecution\nis default\nbehavior\nfor an LLM.\nSequential\n(planned)\nexecution is\nstandard in\nGPT\nAssistants\nand Claude.\n",
      "content_length": 1108,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "Micheal Lanham\nM\nost production AI systems require many orchestrated \ninteractions between the user, AI models, and a wide \nvariety of data sources. AI agents capture and organize \nthese interactions into autonomous components that can pro-\ncess information, make decisions, and learn from interactions \nbehind the scenes. Th is book will show you how to create AI \nagents and connect them together into powerful multi-agent \nsystems.\nIn AI Agents in Action, you\u2019ll learn how to build production\n-ready assistants, multi-agent systems, and behavioral agents. \nYou\u2019ll master the essential parts of an agent, including \nretrieval-augmented knowledge and memory, while you create \nmulti-agent applications that can use software tools, plan tasks \nautonomously, and learn from experience. As you explore the \nmany interesting examples, you\u2019ll work with state-of-the-art \ntools like OpenAI Assistants API, GPT Nexus, LangChain, \nPrompt Flow, AutoGen, and CrewAI.\nWhat\u2019s Inside\n\u25cf Knowledge management and memory systems\n\u25cf Feedback loops for continuous agent learning\n\u25cf Collaborative multi-agent systems\n\u25cf Speech and computer vision\nFor intermediate Python programmers.\nMicheal Lanham is a software and technology innovator with \nover 20 years of industry experience. He has authored books \non deep learning, including Manning\u2019s Evolutionary Deep \nLearning.\nTh e technical editor on this book was Ross Turner.\nFor print book owners, all digital formats are free:\nhttps://www.manning.com/freebook\nAI Agents IN ACTION\nPYTHON/SOFTWARE DEVELOPMENT\nM A N N I N G\n\u201c\nTh is is about to become \nthe hottest area of applied \nAI. Get a head start with \n this book!\u201d\n \n\u2014Richard Davies, author of \nPrompt Engineering in Practice\n\u201c\nCouldn\u2019t put this book \ndown! It\u2019s so comprehensive \nand clear that I felt like \nI was learning from a \n  master teacher.\u201d\n \n\u2014Radhika Kanubaddhi, Amazon\n\u201c\nAn enlightening journey! \nTh is book transformed my \n  questions into answers.\u201d\n\u2014Jose San Leandro, ACM-SL\n\u201c\nExpertly guides through \ncreating agent profi les, using \ntools, memory, planning, \nand multi-agent systems.\n Couldn\u2019t be more timely!\u201d\n\u2014Grigory Sapunov\nauthor of JAX in Action\nISBN-13: 978-1-63343-634-3\nSee first page\n",
      "content_length": 2193,
      "extraction_method": "Direct"
    }
  ]
}